
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.3
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507657231

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T12:40:36.738109: step 1, loss 8.96572, acc 0.1875, learning_rate 0.005
2017-10-10T12:40:37.029763: step 2, loss 7.93937, acc 0.3125, learning_rate 0.00498
2017-10-10T12:40:37.318794: step 3, loss 7.81514, acc 0.34375, learning_rate 0.00496008
2017-10-10T12:40:37.609906: step 4, loss 7.23362, acc 0.390625, learning_rate 0.00494024
2017-10-10T12:40:37.892179: step 5, loss 7.00021, acc 0.28125, learning_rate 0.00492049
2017-10-10T12:40:38.181832: step 6, loss 6.52625, acc 0.28125, learning_rate 0.00490081
2017-10-10T12:40:38.466410: step 7, loss 5.83979, acc 0.296875, learning_rate 0.00488121
2017-10-10T12:40:38.752585: step 8, loss 7.33574, acc 0.234375, learning_rate 0.0048617
2017-10-10T12:40:39.036106: step 9, loss 6.47722, acc 0.375, learning_rate 0.00484226
2017-10-10T12:40:39.310074: step 10, loss 4.85967, acc 0.46875, learning_rate 0.00482291
2017-10-10T12:40:39.585027: step 11, loss 4.80921, acc 0.375, learning_rate 0.00480363
2017-10-10T12:40:39.874121: step 12, loss 4.19918, acc 0.4375, learning_rate 0.00478443
2017-10-10T12:40:40.176760: step 13, loss 5.93849, acc 0.421875, learning_rate 0.00476531
2017-10-10T12:40:40.467724: step 14, loss 2.97268, acc 0.578125, learning_rate 0.00474627
2017-10-10T12:40:40.766613: step 15, loss 5.18812, acc 0.265625, learning_rate 0.0047273
2017-10-10T12:40:41.060324: step 16, loss 3.81148, acc 0.515625, learning_rate 0.00470841
2017-10-10T12:40:41.347457: step 17, loss 5.10026, acc 0.390625, learning_rate 0.0046896
2017-10-10T12:40:41.662315: step 18, loss 4.27787, acc 0.5, learning_rate 0.00467087
2017-10-10T12:40:42.027563: step 19, loss 4.70164, acc 0.421875, learning_rate 0.00465221
2017-10-10T12:40:42.201857: step 20, loss 3.50878, acc 0.515625, learning_rate 0.00463363
2017-10-10T12:40:42.374324: step 21, loss 3.89829, acc 0.5625, learning_rate 0.00461513
2017-10-10T12:40:42.549420: step 22, loss 3.58662, acc 0.453125, learning_rate 0.0045967
2017-10-10T12:40:42.728038: step 23, loss 3.2156, acc 0.5625, learning_rate 0.00457834
2017-10-10T12:40:43.010448: step 24, loss 3.97849, acc 0.4375, learning_rate 0.00456006
2017-10-10T12:40:43.261656: step 25, loss 2.95249, acc 0.609375, learning_rate 0.00454186
2017-10-10T12:40:43.557008: step 26, loss 2.30416, acc 0.5625, learning_rate 0.00452373
2017-10-10T12:40:43.834602: step 27, loss 1.83031, acc 0.734375, learning_rate 0.00450567
2017-10-10T12:40:44.115613: step 28, loss 2.43569, acc 0.578125, learning_rate 0.00448769
2017-10-10T12:40:44.406258: step 29, loss 1.85088, acc 0.609375, learning_rate 0.00446978
2017-10-10T12:40:44.702322: step 30, loss 2.24288, acc 0.59375, learning_rate 0.00445194
2017-10-10T12:40:44.989346: step 31, loss 2.31296, acc 0.703125, learning_rate 0.00443418
2017-10-10T12:40:45.280885: step 32, loss 2.20719, acc 0.609375, learning_rate 0.00441649
2017-10-10T12:40:45.552887: step 33, loss 2.35662, acc 0.53125, learning_rate 0.00439887
2017-10-10T12:40:45.822068: step 34, loss 3.15473, acc 0.546875, learning_rate 0.00438132
2017-10-10T12:40:46.094549: step 35, loss 2.22188, acc 0.609375, learning_rate 0.00436385
2017-10-10T12:40:46.374854: step 36, loss 1.48522, acc 0.71875, learning_rate 0.00434644
2017-10-10T12:40:46.661345: step 37, loss 2.51098, acc 0.515625, learning_rate 0.00432911
2017-10-10T12:40:46.976515: step 38, loss 3.03165, acc 0.625, learning_rate 0.00431185
2017-10-10T12:40:47.261279: step 39, loss 2.11237, acc 0.625, learning_rate 0.00429465
2017-10-10T12:40:47.543974: step 40, loss 2.02238, acc 0.640625, learning_rate 0.00427753

Evaluation:
2017-10-10T12:40:48.189298: step 40, loss 0.578422, acc 0.857554

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-40

2017-10-10T12:40:49.352756: step 41, loss 2.01471, acc 0.65625, learning_rate 0.00426048
2017-10-10T12:40:49.653557: step 42, loss 2.3243, acc 0.703125, learning_rate 0.0042435
2017-10-10T12:40:49.942784: step 43, loss 1.82849, acc 0.703125, learning_rate 0.00422659
2017-10-10T12:40:50.221794: step 44, loss 1.67202, acc 0.71875, learning_rate 0.00420974
2017-10-10T12:40:50.492904: step 45, loss 1.4387, acc 0.71875, learning_rate 0.00419297
2017-10-10T12:40:50.754797: step 46, loss 2.93695, acc 0.65625, learning_rate 0.00417626
2017-10-10T12:40:51.038529: step 47, loss 2.12136, acc 0.703125, learning_rate 0.00415962
2017-10-10T12:40:51.320576: step 48, loss 2.96385, acc 0.578125, learning_rate 0.00414305
2017-10-10T12:40:51.623138: step 49, loss 1.70426, acc 0.65625, learning_rate 0.00412655
2017-10-10T12:40:51.910410: step 50, loss 1.87934, acc 0.71875, learning_rate 0.00411011
2017-10-10T12:40:52.196876: step 51, loss 1.27204, acc 0.765625, learning_rate 0.00409375
2017-10-10T12:40:52.481519: step 52, loss 1.15361, acc 0.703125, learning_rate 0.00407744
2017-10-10T12:40:52.777044: step 53, loss 1.44736, acc 0.640625, learning_rate 0.00406121
2017-10-10T12:40:53.076934: step 54, loss 1.53775, acc 0.75, learning_rate 0.00404504
2017-10-10T12:40:53.362702: step 55, loss 1.37788, acc 0.75, learning_rate 0.00402894
2017-10-10T12:40:53.638034: step 56, loss 0.879622, acc 0.84375, learning_rate 0.0040129
2017-10-10T12:40:53.904889: step 57, loss 1.2375, acc 0.828125, learning_rate 0.00399693
2017-10-10T12:40:54.258094: step 58, loss 1.49382, acc 0.765625, learning_rate 0.00398102
2017-10-10T12:40:54.460968: step 59, loss 1.36099, acc 0.703125, learning_rate 0.00396518
2017-10-10T12:40:54.639313: step 60, loss 1.43239, acc 0.71875, learning_rate 0.00394941
2017-10-10T12:40:54.809309: step 61, loss 0.912238, acc 0.765625, learning_rate 0.00393369
2017-10-10T12:40:54.985778: step 62, loss 1.87543, acc 0.703125, learning_rate 0.00391804
2017-10-10T12:40:55.157156: step 63, loss 1.68202, acc 0.6875, learning_rate 0.00390246
2017-10-10T12:40:55.424861: step 64, loss 1.66106, acc 0.71875, learning_rate 0.00388694
2017-10-10T12:40:55.713645: step 65, loss 1.24945, acc 0.75, learning_rate 0.00387148
2017-10-10T12:40:56.012652: step 66, loss 1.18312, acc 0.71875, learning_rate 0.00385609
2017-10-10T12:40:56.306052: step 67, loss 0.941189, acc 0.75, learning_rate 0.00384076
2017-10-10T12:40:56.591014: step 68, loss 1.52203, acc 0.78125, learning_rate 0.00382549
2017-10-10T12:40:56.884694: step 69, loss 1.13227, acc 0.75, learning_rate 0.00381028
2017-10-10T12:40:57.178924: step 70, loss 1.3189, acc 0.75, learning_rate 0.00379514
2017-10-10T12:40:57.452954: step 71, loss 1.39514, acc 0.75, learning_rate 0.00378005
2017-10-10T12:40:57.728840: step 72, loss 1.08204, acc 0.796875, learning_rate 0.00376503
2017-10-10T12:40:58.016423: step 73, loss 1.36336, acc 0.828125, learning_rate 0.00375007
2017-10-10T12:40:58.298511: step 74, loss 0.705509, acc 0.8125, learning_rate 0.00373517
2017-10-10T12:40:58.584850: step 75, loss 0.512139, acc 0.859375, learning_rate 0.00372034
2017-10-10T12:40:58.867495: step 76, loss 1.41766, acc 0.71875, learning_rate 0.00370556
2017-10-10T12:40:59.169572: step 77, loss 1.25307, acc 0.75, learning_rate 0.00369084
2017-10-10T12:40:59.448940: step 78, loss 1.14921, acc 0.78125, learning_rate 0.00367619
2017-10-10T12:40:59.742969: step 79, loss 1.09044, acc 0.75, learning_rate 0.00366159
2017-10-10T12:41:00.027546: step 80, loss 1.22691, acc 0.6875, learning_rate 0.00364705

Evaluation:
2017-10-10T12:41:00.713848: step 80, loss 0.470214, acc 0.87482

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-80

2017-10-10T12:41:01.904493: step 81, loss 1.62557, acc 0.71875, learning_rate 0.00363257
2017-10-10T12:41:02.189918: step 82, loss 0.789877, acc 0.75, learning_rate 0.00361815
2017-10-10T12:41:02.481165: step 83, loss 0.925759, acc 0.734375, learning_rate 0.00360379
2017-10-10T12:41:02.750292: step 84, loss 1.35312, acc 0.734375, learning_rate 0.00358949
2017-10-10T12:41:02.998851: step 85, loss 1.12982, acc 0.703125, learning_rate 0.00357525
2017-10-10T12:41:03.260103: step 86, loss 0.60797, acc 0.828125, learning_rate 0.00356106
2017-10-10T12:41:03.558396: step 87, loss 0.846157, acc 0.796875, learning_rate 0.00354694
2017-10-10T12:41:03.826905: step 88, loss 1.01781, acc 0.828125, learning_rate 0.00353287
2017-10-10T12:41:04.120632: step 89, loss 1.27633, acc 0.703125, learning_rate 0.00351885
2017-10-10T12:41:04.411479: step 90, loss 1.30555, acc 0.671875, learning_rate 0.0035049
2017-10-10T12:41:04.693496: step 91, loss 0.848667, acc 0.765625, learning_rate 0.003491
2017-10-10T12:41:04.963618: step 92, loss 0.512306, acc 0.875, learning_rate 0.00347716
2017-10-10T12:41:05.248137: step 93, loss 0.891463, acc 0.78125, learning_rate 0.00346338
2017-10-10T12:41:05.529354: step 94, loss 1.00856, acc 0.71875, learning_rate 0.00344965
2017-10-10T12:41:05.820776: step 95, loss 1.03476, acc 0.765625, learning_rate 0.00343597
2017-10-10T12:41:06.102754: step 96, loss 0.867813, acc 0.796875, learning_rate 0.00342236
2017-10-10T12:41:06.468944: step 97, loss 1.26645, acc 0.75, learning_rate 0.0034088
2017-10-10T12:41:06.748179: step 98, loss 1.44104, acc 0.72549, learning_rate 0.00339529
2017-10-10T12:41:06.930776: step 99, loss 1.03147, acc 0.71875, learning_rate 0.00338184
2017-10-10T12:41:07.115616: step 100, loss 1.71704, acc 0.671875, learning_rate 0.00336844
2017-10-10T12:41:07.307047: step 101, loss 1.02315, acc 0.8125, learning_rate 0.0033551
2017-10-10T12:41:07.479928: step 102, loss 1.11173, acc 0.75, learning_rate 0.00334182
2017-10-10T12:41:07.721469: step 103, loss 0.87334, acc 0.78125, learning_rate 0.00332858
2017-10-10T12:41:08.011700: step 104, loss 0.823598, acc 0.75, learning_rate 0.00331541
2017-10-10T12:41:08.284847: step 105, loss 1.17143, acc 0.703125, learning_rate 0.00330228
2017-10-10T12:41:08.564770: step 106, loss 1.05982, acc 0.734375, learning_rate 0.00328921
2017-10-10T12:41:08.834800: step 107, loss 1.01447, acc 0.703125, learning_rate 0.00327619
2017-10-10T12:41:09.119022: step 108, loss 0.989019, acc 0.75, learning_rate 0.00326323
2017-10-10T12:41:09.389594: step 109, loss 0.676463, acc 0.828125, learning_rate 0.00325032
2017-10-10T12:41:09.675722: step 110, loss 1.08545, acc 0.78125, learning_rate 0.00323746
2017-10-10T12:41:09.968873: step 111, loss 0.844843, acc 0.78125, learning_rate 0.00322465
2017-10-10T12:41:10.259495: step 112, loss 0.370424, acc 0.890625, learning_rate 0.0032119
2017-10-10T12:41:10.555333: step 113, loss 0.659231, acc 0.796875, learning_rate 0.0031992
2017-10-10T12:41:10.849058: step 114, loss 0.684754, acc 0.84375, learning_rate 0.00318655
2017-10-10T12:41:11.143204: step 115, loss 0.513469, acc 0.859375, learning_rate 0.00317395
2017-10-10T12:41:11.426342: step 116, loss 0.994836, acc 0.734375, learning_rate 0.0031614
2017-10-10T12:41:11.726497: step 117, loss 0.967526, acc 0.78125, learning_rate 0.0031489
2017-10-10T12:41:12.016257: step 118, loss 0.902447, acc 0.78125, learning_rate 0.00313646
2017-10-10T12:41:12.312922: step 119, loss 1.02223, acc 0.734375, learning_rate 0.00312407
2017-10-10T12:41:12.562151: step 120, loss 0.782064, acc 0.828125, learning_rate 0.00311172

Evaluation:
2017-10-10T12:41:13.260241: step 120, loss 0.360298, acc 0.87482

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-120

2017-10-10T12:41:14.413123: step 121, loss 0.150524, acc 0.953125, learning_rate 0.00309943
2017-10-10T12:41:14.706832: step 122, loss 0.720127, acc 0.859375, learning_rate 0.00308719
2017-10-10T12:41:14.944892: step 123, loss 0.407185, acc 0.890625, learning_rate 0.00307499
2017-10-10T12:41:15.216851: step 124, loss 0.648899, acc 0.796875, learning_rate 0.00306285
2017-10-10T12:41:15.476584: step 125, loss 0.48254, acc 0.84375, learning_rate 0.00305076
2017-10-10T12:41:15.750289: step 126, loss 0.740628, acc 0.84375, learning_rate 0.00303871
2017-10-10T12:41:16.040722: step 127, loss 1.03525, acc 0.78125, learning_rate 0.00302672
2017-10-10T12:41:16.321132: step 128, loss 0.700801, acc 0.796875, learning_rate 0.00301477
2017-10-10T12:41:16.611845: step 129, loss 0.874996, acc 0.75, learning_rate 0.00300287
2017-10-10T12:41:16.911405: step 130, loss 0.509115, acc 0.828125, learning_rate 0.00299102
2017-10-10T12:41:17.218616: step 131, loss 0.602825, acc 0.8125, learning_rate 0.00297922
2017-10-10T12:41:17.467588: step 132, loss 0.970069, acc 0.765625, learning_rate 0.00296747
2017-10-10T12:41:17.738924: step 133, loss 0.847159, acc 0.828125, learning_rate 0.00295577
2017-10-10T12:41:17.976594: step 134, loss 0.625527, acc 0.75, learning_rate 0.00294411
2017-10-10T12:41:18.229006: step 135, loss 0.389499, acc 0.890625, learning_rate 0.0029325
2017-10-10T12:41:18.629013: step 136, loss 0.469812, acc 0.859375, learning_rate 0.00292094
2017-10-10T12:41:18.890348: step 137, loss 0.930025, acc 0.78125, learning_rate 0.00290943
2017-10-10T12:41:19.066066: step 138, loss 0.834256, acc 0.75, learning_rate 0.00289796
2017-10-10T12:41:19.277108: step 139, loss 0.707393, acc 0.796875, learning_rate 0.00288654
2017-10-10T12:41:19.463372: step 140, loss 0.436059, acc 0.859375, learning_rate 0.00287516
2017-10-10T12:41:19.646257: step 141, loss 0.479762, acc 0.875, learning_rate 0.00286384
2017-10-10T12:41:19.932148: step 142, loss 0.908118, acc 0.78125, learning_rate 0.00285256
2017-10-10T12:41:20.220113: step 143, loss 0.727041, acc 0.796875, learning_rate 0.00284132
2017-10-10T12:41:20.505967: step 144, loss 0.516479, acc 0.8125, learning_rate 0.00283013
2017-10-10T12:41:20.805213: step 145, loss 0.441174, acc 0.90625, learning_rate 0.00281899
2017-10-10T12:41:21.078726: step 146, loss 0.944556, acc 0.796875, learning_rate 0.00280789
2017-10-10T12:41:21.355902: step 147, loss 1.04943, acc 0.765625, learning_rate 0.00279684
2017-10-10T12:41:21.630545: step 148, loss 0.456243, acc 0.859375, learning_rate 0.00278583
2017-10-10T12:41:21.908332: step 149, loss 0.502718, acc 0.859375, learning_rate 0.00277486
2017-10-10T12:41:22.179170: step 150, loss 0.874537, acc 0.796875, learning_rate 0.00276395
2017-10-10T12:41:22.466457: step 151, loss 0.454445, acc 0.828125, learning_rate 0.00275307
2017-10-10T12:41:22.744295: step 152, loss 0.48591, acc 0.859375, learning_rate 0.00274224
2017-10-10T12:41:23.041514: step 153, loss 0.700275, acc 0.875, learning_rate 0.00273146
2017-10-10T12:41:23.310184: step 154, loss 0.56702, acc 0.84375, learning_rate 0.00272072
2017-10-10T12:41:23.598228: step 155, loss 0.423057, acc 0.859375, learning_rate 0.00271002
2017-10-10T12:41:23.883437: step 156, loss 0.841169, acc 0.78125, learning_rate 0.00269937
2017-10-10T12:41:24.161318: step 157, loss 0.249924, acc 0.9375, learning_rate 0.00268876
2017-10-10T12:41:24.443629: step 158, loss 0.573347, acc 0.828125, learning_rate 0.00267819
2017-10-10T12:41:24.741314: step 159, loss 0.575962, acc 0.8125, learning_rate 0.00266767
2017-10-10T12:41:25.040931: step 160, loss 0.426902, acc 0.8125, learning_rate 0.00265719

Evaluation:
2017-10-10T12:41:25.712362: step 160, loss 0.320884, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-160

2017-10-10T12:41:26.855653: step 161, loss 0.499789, acc 0.890625, learning_rate 0.00264675
2017-10-10T12:41:27.127967: step 162, loss 0.47909, acc 0.84375, learning_rate 0.00263635
2017-10-10T12:41:27.434297: step 163, loss 0.885035, acc 0.75, learning_rate 0.002626
2017-10-10T12:41:27.761061: step 164, loss 1.03532, acc 0.78125, learning_rate 0.00261569
2017-10-10T12:41:28.068015: step 165, loss 0.496619, acc 0.828125, learning_rate 0.00260542
2017-10-10T12:41:28.367471: step 166, loss 0.671076, acc 0.828125, learning_rate 0.0025952
2017-10-10T12:41:28.650173: step 167, loss 0.26098, acc 0.921875, learning_rate 0.00258501
2017-10-10T12:41:28.948203: step 168, loss 0.330248, acc 0.890625, learning_rate 0.00257487
2017-10-10T12:41:29.236906: step 169, loss 0.448454, acc 0.828125, learning_rate 0.00256477
2017-10-10T12:41:29.531105: step 170, loss 0.55615, acc 0.796875, learning_rate 0.0025547
2017-10-10T12:41:29.806968: step 171, loss 0.818241, acc 0.84375, learning_rate 0.00254469
2017-10-10T12:41:30.090900: step 172, loss 0.614006, acc 0.859375, learning_rate 0.00253471
2017-10-10T12:41:30.404224: step 173, loss 0.41422, acc 0.875, learning_rate 0.00252477
2017-10-10T12:41:30.719160: step 174, loss 0.771749, acc 0.8125, learning_rate 0.00251487
2017-10-10T12:41:31.076941: step 175, loss 0.562392, acc 0.859375, learning_rate 0.00250501
2017-10-10T12:41:31.399606: step 176, loss 0.37992, acc 0.890625, learning_rate 0.0024952
2017-10-10T12:41:31.587443: step 177, loss 0.682685, acc 0.828125, learning_rate 0.00248542
2017-10-10T12:41:31.761595: step 178, loss 0.291084, acc 0.859375, learning_rate 0.00247568
2017-10-10T12:41:31.932838: step 179, loss 0.529369, acc 0.8125, learning_rate 0.00246599
2017-10-10T12:41:32.104484: step 180, loss 0.840873, acc 0.84375, learning_rate 0.00245633
2017-10-10T12:41:32.375196: step 181, loss 0.804534, acc 0.84375, learning_rate 0.00244671
2017-10-10T12:41:32.654581: step 182, loss 0.998459, acc 0.71875, learning_rate 0.00243713
2017-10-10T12:41:32.947140: step 183, loss 0.573819, acc 0.828125, learning_rate 0.00242759
2017-10-10T12:41:33.236258: step 184, loss 0.779578, acc 0.84375, learning_rate 0.00241809
2017-10-10T12:41:33.526248: step 185, loss 0.592973, acc 0.84375, learning_rate 0.00240863
2017-10-10T12:41:33.809021: step 186, loss 0.132424, acc 0.9375, learning_rate 0.00239921
2017-10-10T12:41:34.096775: step 187, loss 0.354255, acc 0.890625, learning_rate 0.00238982
2017-10-10T12:41:34.382537: step 188, loss 0.467137, acc 0.84375, learning_rate 0.00238048
2017-10-10T12:41:34.678709: step 189, loss 0.327448, acc 0.875, learning_rate 0.00237117
2017-10-10T12:41:34.983918: step 190, loss 0.637865, acc 0.796875, learning_rate 0.0023619
2017-10-10T12:41:35.251559: step 191, loss 0.578768, acc 0.828125, learning_rate 0.00235267
2017-10-10T12:41:35.522135: step 192, loss 0.740058, acc 0.8125, learning_rate 0.00234347
2017-10-10T12:41:35.792513: step 193, loss 0.352274, acc 0.875, learning_rate 0.00233431
2017-10-10T12:41:36.085275: step 194, loss 0.442089, acc 0.828125, learning_rate 0.00232519
2017-10-10T12:41:36.344845: step 195, loss 0.331986, acc 0.875, learning_rate 0.00231611
2017-10-10T12:41:36.591082: step 196, loss 0.753185, acc 0.843137, learning_rate 0.00230707
2017-10-10T12:41:36.867409: step 197, loss 0.240776, acc 0.921875, learning_rate 0.00229806
2017-10-10T12:41:37.153853: step 198, loss 0.454002, acc 0.78125, learning_rate 0.00228908
2017-10-10T12:41:37.455737: step 199, loss 0.337516, acc 0.90625, learning_rate 0.00228015
2017-10-10T12:41:37.736799: step 200, loss 0.653634, acc 0.828125, learning_rate 0.00227125

Evaluation:
2017-10-10T12:41:38.393947: step 200, loss 0.293867, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-200

2017-10-10T12:41:39.565431: step 201, loss 0.584917, acc 0.828125, learning_rate 0.00226239
2017-10-10T12:41:39.855887: step 202, loss 0.469615, acc 0.890625, learning_rate 0.00225356
2017-10-10T12:41:40.127571: step 203, loss 0.177338, acc 0.953125, learning_rate 0.00224477
2017-10-10T12:41:40.410175: step 204, loss 0.517008, acc 0.828125, learning_rate 0.00223602
2017-10-10T12:41:40.689588: step 205, loss 0.190619, acc 0.90625, learning_rate 0.0022273
2017-10-10T12:41:40.983011: step 206, loss 0.401689, acc 0.8125, learning_rate 0.00221862
2017-10-10T12:41:41.253581: step 207, loss 0.489597, acc 0.875, learning_rate 0.00220997
2017-10-10T12:41:41.526108: step 208, loss 0.198322, acc 0.953125, learning_rate 0.00220136
2017-10-10T12:41:41.799733: step 209, loss 0.192731, acc 0.9375, learning_rate 0.00219278
2017-10-10T12:41:42.096106: step 210, loss 0.787773, acc 0.796875, learning_rate 0.00218424
2017-10-10T12:41:42.367141: step 211, loss 0.386381, acc 0.90625, learning_rate 0.00217573
2017-10-10T12:41:42.640967: step 212, loss 0.28256, acc 0.921875, learning_rate 0.00216726
2017-10-10T12:41:42.915258: step 213, loss 0.506517, acc 0.859375, learning_rate 0.00215882
2017-10-10T12:41:43.260878: step 214, loss 0.417826, acc 0.890625, learning_rate 0.00215041
2017-10-10T12:41:43.612285: step 215, loss 0.502798, acc 0.921875, learning_rate 0.00214204
2017-10-10T12:41:43.786033: step 216, loss 0.431766, acc 0.84375, learning_rate 0.00213371
2017-10-10T12:41:43.962123: step 217, loss 0.314415, acc 0.90625, learning_rate 0.00212541
2017-10-10T12:41:44.143997: step 218, loss 0.590231, acc 0.859375, learning_rate 0.00211714
2017-10-10T12:41:44.321304: step 219, loss 0.580837, acc 0.875, learning_rate 0.00210891
2017-10-10T12:41:44.599497: step 220, loss 0.50788, acc 0.84375, learning_rate 0.00210071
2017-10-10T12:41:44.880902: step 221, loss 0.307311, acc 0.859375, learning_rate 0.00209254
2017-10-10T12:41:45.106214: step 222, loss 0.592498, acc 0.875, learning_rate 0.00208441
2017-10-10T12:41:45.339364: step 223, loss 0.244021, acc 0.90625, learning_rate 0.00207631
2017-10-10T12:41:45.602114: step 224, loss 0.508806, acc 0.875, learning_rate 0.00206824
2017-10-10T12:41:45.847198: step 225, loss 0.747501, acc 0.8125, learning_rate 0.00206021
2017-10-10T12:41:46.122256: step 226, loss 0.415433, acc 0.84375, learning_rate 0.00205221
2017-10-10T12:41:46.410362: step 227, loss 0.381472, acc 0.859375, learning_rate 0.00204424
2017-10-10T12:41:46.694141: step 228, loss 0.725209, acc 0.734375, learning_rate 0.0020363
2017-10-10T12:41:46.974458: step 229, loss 0.521562, acc 0.84375, learning_rate 0.0020284
2017-10-10T12:41:47.260146: step 230, loss 0.516923, acc 0.84375, learning_rate 0.00202053
2017-10-10T12:41:47.529071: step 231, loss 0.721655, acc 0.8125, learning_rate 0.00201269
2017-10-10T12:41:47.811182: step 232, loss 0.482451, acc 0.859375, learning_rate 0.00200488
2017-10-10T12:41:48.089987: step 233, loss 0.380537, acc 0.875, learning_rate 0.00199711
2017-10-10T12:41:48.367303: step 234, loss 0.284063, acc 0.921875, learning_rate 0.00198936
2017-10-10T12:41:48.638677: step 235, loss 0.284593, acc 0.890625, learning_rate 0.00198165
2017-10-10T12:41:48.932658: step 236, loss 0.230319, acc 0.921875, learning_rate 0.00197397
2017-10-10T12:41:49.215574: step 237, loss 0.467634, acc 0.828125, learning_rate 0.00196632
2017-10-10T12:41:49.505701: step 238, loss 0.472155, acc 0.90625, learning_rate 0.0019587
2017-10-10T12:41:49.795181: step 239, loss 0.409136, acc 0.84375, learning_rate 0.00195112
2017-10-10T12:41:50.083186: step 240, loss 0.362984, acc 0.90625, learning_rate 0.00194356

Evaluation:
2017-10-10T12:41:50.736824: step 240, loss 0.28476, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-240

2017-10-10T12:41:51.913493: step 241, loss 0.257448, acc 0.890625, learning_rate 0.00193604
2017-10-10T12:41:52.225801: step 242, loss 0.417295, acc 0.90625, learning_rate 0.00192854
2017-10-10T12:41:52.509678: step 243, loss 0.498942, acc 0.828125, learning_rate 0.00192108
2017-10-10T12:41:52.805616: step 244, loss 0.439178, acc 0.90625, learning_rate 0.00191364
2017-10-10T12:41:53.096031: step 245, loss 0.496577, acc 0.8125, learning_rate 0.00190624
2017-10-10T12:41:53.376929: step 246, loss 0.355914, acc 0.921875, learning_rate 0.00189887
2017-10-10T12:41:53.667150: step 247, loss 0.593503, acc 0.875, learning_rate 0.00189153
2017-10-10T12:41:53.980998: step 248, loss 0.42287, acc 0.890625, learning_rate 0.00188421
2017-10-10T12:41:54.279771: step 249, loss 0.480696, acc 0.828125, learning_rate 0.00187693
2017-10-10T12:41:54.562117: step 250, loss 0.543038, acc 0.796875, learning_rate 0.00186968
2017-10-10T12:41:54.845800: step 251, loss 0.415698, acc 0.84375, learning_rate 0.00186245
2017-10-10T12:41:55.124174: step 252, loss 0.215874, acc 0.921875, learning_rate 0.00185526
2017-10-10T12:41:55.409639: step 253, loss 0.521676, acc 0.828125, learning_rate 0.0018481
2017-10-10T12:41:55.777286: step 254, loss 0.323554, acc 0.90625, learning_rate 0.00184096
2017-10-10T12:41:56.098697: step 255, loss 0.518187, acc 0.8125, learning_rate 0.00183385
2017-10-10T12:41:56.288787: step 256, loss 0.593547, acc 0.8125, learning_rate 0.00182678
2017-10-10T12:41:56.475082: step 257, loss 0.33082, acc 0.90625, learning_rate 0.00181973
2017-10-10T12:41:56.671006: step 258, loss 0.358099, acc 0.890625, learning_rate 0.00181271
2017-10-10T12:41:56.871314: step 259, loss 0.224934, acc 0.921875, learning_rate 0.00180572
2017-10-10T12:41:57.094340: step 260, loss 0.268924, acc 0.9375, learning_rate 0.00179876
2017-10-10T12:41:57.408929: step 261, loss 0.671473, acc 0.859375, learning_rate 0.00179182
2017-10-10T12:41:57.709987: step 262, loss 0.484089, acc 0.84375, learning_rate 0.00178492
2017-10-10T12:41:57.995084: step 263, loss 0.430401, acc 0.859375, learning_rate 0.00177804
2017-10-10T12:41:58.283334: step 264, loss 0.567073, acc 0.84375, learning_rate 0.00177119
2017-10-10T12:41:58.556625: step 265, loss 0.261283, acc 0.9375, learning_rate 0.00176437
2017-10-10T12:41:58.826476: step 266, loss 0.335705, acc 0.875, learning_rate 0.00175758
2017-10-10T12:41:59.138311: step 267, loss 0.28773, acc 0.890625, learning_rate 0.00175081
2017-10-10T12:41:59.408950: step 268, loss 0.389535, acc 0.90625, learning_rate 0.00174407
2017-10-10T12:41:59.690067: step 269, loss 0.483, acc 0.8125, learning_rate 0.00173736
2017-10-10T12:41:59.977104: step 270, loss 0.281874, acc 0.890625, learning_rate 0.00173068
2017-10-10T12:42:00.260107: step 271, loss 0.318819, acc 0.90625, learning_rate 0.00172402
2017-10-10T12:42:00.534853: step 272, loss 0.324813, acc 0.828125, learning_rate 0.00171739
2017-10-10T12:42:00.808528: step 273, loss 0.21679, acc 0.90625, learning_rate 0.00171079
2017-10-10T12:42:01.121142: step 274, loss 0.597724, acc 0.890625, learning_rate 0.00170422
2017-10-10T12:42:01.403187: step 275, loss 0.477366, acc 0.875, learning_rate 0.00169767
2017-10-10T12:42:01.689368: step 276, loss 0.382557, acc 0.890625, learning_rate 0.00169115
2017-10-10T12:42:01.978273: step 277, loss 0.561322, acc 0.796875, learning_rate 0.00168465
2017-10-10T12:42:02.259960: step 278, loss 0.589989, acc 0.859375, learning_rate 0.00167818
2017-10-10T12:42:02.553394: step 279, loss 0.606774, acc 0.828125, learning_rate 0.00167174
2017-10-10T12:42:02.832517: step 280, loss 0.363221, acc 0.859375, learning_rate 0.00166533

Evaluation:
2017-10-10T12:42:03.482985: step 280, loss 0.270987, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-280

2017-10-10T12:42:04.652901: step 281, loss 0.432152, acc 0.84375, learning_rate 0.00165894
2017-10-10T12:42:04.922896: step 282, loss 0.757465, acc 0.75, learning_rate 0.00165257
2017-10-10T12:42:05.201519: step 283, loss 0.214276, acc 0.921875, learning_rate 0.00164624
2017-10-10T12:42:05.491361: step 284, loss 0.564141, acc 0.75, learning_rate 0.00163993
2017-10-10T12:42:05.774840: step 285, loss 0.57523, acc 0.828125, learning_rate 0.00163364
2017-10-10T12:42:06.074420: step 286, loss 0.323549, acc 0.890625, learning_rate 0.00162738
2017-10-10T12:42:06.368710: step 287, loss 0.192765, acc 0.921875, learning_rate 0.00162115
2017-10-10T12:42:06.646581: step 288, loss 0.449649, acc 0.859375, learning_rate 0.00161494
2017-10-10T12:42:06.935692: step 289, loss 0.518354, acc 0.90625, learning_rate 0.00160875
2017-10-10T12:42:07.223188: step 290, loss 0.419422, acc 0.84375, learning_rate 0.00160259
2017-10-10T12:42:07.508866: step 291, loss 0.364218, acc 0.828125, learning_rate 0.00159646
2017-10-10T12:42:07.789962: step 292, loss 0.243041, acc 0.921875, learning_rate 0.00159035
2017-10-10T12:42:08.108832: step 293, loss 0.233761, acc 0.921875, learning_rate 0.00158427
2017-10-10T12:42:08.399768: step 294, loss 0.226157, acc 0.980392, learning_rate 0.00157821
2017-10-10T12:42:08.602187: step 295, loss 0.529499, acc 0.84375, learning_rate 0.00157218
2017-10-10T12:42:08.776362: step 296, loss 0.408475, acc 0.859375, learning_rate 0.00156617
2017-10-10T12:42:08.955022: step 297, loss 0.319069, acc 0.859375, learning_rate 0.00156018
2017-10-10T12:42:09.125328: step 298, loss 0.238377, acc 0.9375, learning_rate 0.00155422
2017-10-10T12:42:09.299693: step 299, loss 0.331999, acc 0.890625, learning_rate 0.00154829
2017-10-10T12:42:09.592529: step 300, loss 0.260996, acc 0.921875, learning_rate 0.00154238
2017-10-10T12:42:09.881273: step 301, loss 0.490003, acc 0.890625, learning_rate 0.00153649
2017-10-10T12:42:10.174029: step 302, loss 0.227223, acc 0.921875, learning_rate 0.00153063
2017-10-10T12:42:10.466164: step 303, loss 0.24448, acc 0.921875, learning_rate 0.00152479
2017-10-10T12:42:10.763542: step 304, loss 0.247343, acc 0.921875, learning_rate 0.00151897
2017-10-10T12:42:11.050715: step 305, loss 0.304182, acc 0.890625, learning_rate 0.00151318
2017-10-10T12:42:11.330246: step 306, loss 0.512479, acc 0.828125, learning_rate 0.00150741
2017-10-10T12:42:11.622031: step 307, loss 0.561807, acc 0.84375, learning_rate 0.00150167
2017-10-10T12:42:11.936477: step 308, loss 0.32695, acc 0.859375, learning_rate 0.00149594
2017-10-10T12:42:12.259861: step 309, loss 0.361748, acc 0.859375, learning_rate 0.00149025
2017-10-10T12:42:12.576677: step 310, loss 0.316507, acc 0.90625, learning_rate 0.00148457
2017-10-10T12:42:12.885131: step 311, loss 0.21906, acc 0.890625, learning_rate 0.00147892
2017-10-10T12:42:13.197161: step 312, loss 0.214855, acc 0.859375, learning_rate 0.00147329
2017-10-10T12:42:13.503545: step 313, loss 0.275807, acc 0.90625, learning_rate 0.00146769
2017-10-10T12:42:13.813722: step 314, loss 0.524339, acc 0.84375, learning_rate 0.0014621
2017-10-10T12:42:14.110985: step 315, loss 0.315151, acc 0.9375, learning_rate 0.00145654
2017-10-10T12:42:14.420525: step 316, loss 0.545177, acc 0.84375, learning_rate 0.00145101
2017-10-10T12:42:14.711238: step 317, loss 0.327947, acc 0.84375, learning_rate 0.00144549
2017-10-10T12:42:15.029771: step 318, loss 0.470584, acc 0.84375, learning_rate 0.00144
2017-10-10T12:42:15.361072: step 319, loss 0.440141, acc 0.859375, learning_rate 0.00143453
2017-10-10T12:42:15.674467: step 320, loss 0.38505, acc 0.890625, learning_rate 0.00142908

Evaluation:
2017-10-10T12:42:16.408890: step 320, loss 0.274067, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-320

2017-10-10T12:42:17.624881: step 321, loss 0.55771, acc 0.84375, learning_rate 0.00142366
2017-10-10T12:42:17.922562: step 322, loss 0.255251, acc 0.890625, learning_rate 0.00141826
2017-10-10T12:42:18.177882: step 323, loss 0.313609, acc 0.875, learning_rate 0.00141288
2017-10-10T12:42:18.516830: step 324, loss 0.437636, acc 0.84375, learning_rate 0.00140752
2017-10-10T12:42:18.781454: step 325, loss 0.530176, acc 0.84375, learning_rate 0.00140218
2017-10-10T12:42:19.088907: step 326, loss 0.239751, acc 0.921875, learning_rate 0.00139686
2017-10-10T12:42:19.406710: step 327, loss 0.161429, acc 0.9375, learning_rate 0.00139157
2017-10-10T12:42:19.672032: step 328, loss 0.52694, acc 0.828125, learning_rate 0.0013863
2017-10-10T12:42:19.952934: step 329, loss 0.233827, acc 0.90625, learning_rate 0.00138105
2017-10-10T12:42:20.287690: step 330, loss 0.246354, acc 0.921875, learning_rate 0.00137582
2017-10-10T12:42:20.584834: step 331, loss 0.585463, acc 0.8125, learning_rate 0.00137061
2017-10-10T12:42:20.904152: step 332, loss 0.342756, acc 0.890625, learning_rate 0.00136543
2017-10-10T12:42:21.272866: step 333, loss 0.380742, acc 0.875, learning_rate 0.00136026
2017-10-10T12:42:21.657472: step 334, loss 0.589853, acc 0.828125, learning_rate 0.00135512
2017-10-10T12:42:21.886465: step 335, loss 0.177341, acc 0.9375, learning_rate 0.00134999
2017-10-10T12:42:22.072905: step 336, loss 0.327332, acc 0.84375, learning_rate 0.00134489
2017-10-10T12:42:22.264724: step 337, loss 0.406838, acc 0.859375, learning_rate 0.00133981
2017-10-10T12:42:22.450795: step 338, loss 0.685397, acc 0.84375, learning_rate 0.00133475
2017-10-10T12:42:22.747966: step 339, loss 0.362219, acc 0.875, learning_rate 0.00132971
2017-10-10T12:42:23.075838: step 340, loss 0.627931, acc 0.890625, learning_rate 0.00132469
2017-10-10T12:42:23.403469: step 341, loss 0.24443, acc 0.921875, learning_rate 0.00131969
2017-10-10T12:42:23.718681: step 342, loss 0.233343, acc 0.890625, learning_rate 0.00131471
2017-10-10T12:42:24.027515: step 343, loss 0.186573, acc 0.90625, learning_rate 0.00130975
2017-10-10T12:42:24.345337: step 344, loss 0.18091, acc 0.90625, learning_rate 0.00130482
2017-10-10T12:42:24.634348: step 345, loss 0.209539, acc 0.90625, learning_rate 0.0012999
2017-10-10T12:42:24.962059: step 346, loss 0.330041, acc 0.859375, learning_rate 0.001295
2017-10-10T12:42:25.250550: step 347, loss 0.446921, acc 0.84375, learning_rate 0.00129012
2017-10-10T12:42:25.532290: step 348, loss 0.221028, acc 0.921875, learning_rate 0.00128527
2017-10-10T12:42:25.804295: step 349, loss 0.272686, acc 0.9375, learning_rate 0.00128043
2017-10-10T12:42:26.128847: step 350, loss 0.224107, acc 0.953125, learning_rate 0.00127561
2017-10-10T12:42:26.432848: step 351, loss 0.215852, acc 0.953125, learning_rate 0.00127081
2017-10-10T12:42:26.716612: step 352, loss 0.319806, acc 0.890625, learning_rate 0.00126603
2017-10-10T12:42:27.025601: step 353, loss 0.570049, acc 0.84375, learning_rate 0.00126127
2017-10-10T12:42:27.324976: step 354, loss 0.316967, acc 0.859375, learning_rate 0.00125653
2017-10-10T12:42:27.638559: step 355, loss 0.25752, acc 0.890625, learning_rate 0.00125181
2017-10-10T12:42:27.945902: step 356, loss 0.243238, acc 0.921875, learning_rate 0.00124711
2017-10-10T12:42:28.264911: step 357, loss 0.409599, acc 0.921875, learning_rate 0.00124243
2017-10-10T12:42:28.584704: step 358, loss 0.210377, acc 0.953125, learning_rate 0.00123777
2017-10-10T12:42:28.884166: step 359, loss 0.292567, acc 0.890625, learning_rate 0.00123312
2017-10-10T12:42:29.212176: step 360, loss 0.430186, acc 0.90625, learning_rate 0.0012285

Evaluation:
2017-10-10T12:42:29.938668: step 360, loss 0.267328, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-360

2017-10-10T12:42:31.199925: step 361, loss 0.196262, acc 0.921875, learning_rate 0.00122389
2017-10-10T12:42:31.502537: step 362, loss 0.330397, acc 0.875, learning_rate 0.0012193
2017-10-10T12:42:31.803042: step 363, loss 0.351793, acc 0.890625, learning_rate 0.00121473
2017-10-10T12:42:32.111126: step 364, loss 0.439199, acc 0.84375, learning_rate 0.00121018
2017-10-10T12:42:32.421409: step 365, loss 0.259656, acc 0.875, learning_rate 0.00120565
2017-10-10T12:42:32.736750: step 366, loss 0.281367, acc 0.921875, learning_rate 0.00120114
2017-10-10T12:42:33.055089: step 367, loss 0.461983, acc 0.84375, learning_rate 0.00119664
2017-10-10T12:42:33.366954: step 368, loss 0.144398, acc 0.953125, learning_rate 0.00119217
2017-10-10T12:42:33.680552: step 369, loss 0.236894, acc 0.90625, learning_rate 0.00118771
2017-10-10T12:42:33.990524: step 370, loss 0.387272, acc 0.859375, learning_rate 0.00118327
2017-10-10T12:42:34.297183: step 371, loss 0.23329, acc 0.921875, learning_rate 0.00117885
2017-10-10T12:42:34.636941: step 372, loss 0.228752, acc 0.890625, learning_rate 0.00117445
2017-10-10T12:42:34.985215: step 373, loss 0.263144, acc 0.921875, learning_rate 0.00117006
2017-10-10T12:42:35.215057: step 374, loss 0.713696, acc 0.84375, learning_rate 0.00116569
2017-10-10T12:42:35.406347: step 375, loss 0.284721, acc 0.875, learning_rate 0.00116134
2017-10-10T12:42:35.593248: step 376, loss 0.18562, acc 0.921875, learning_rate 0.00115701
2017-10-10T12:42:35.780183: step 377, loss 0.269723, acc 0.890625, learning_rate 0.0011527
2017-10-10T12:42:35.973458: step 378, loss 0.190454, acc 0.921875, learning_rate 0.0011484
2017-10-10T12:42:36.166298: step 379, loss 0.152105, acc 0.9375, learning_rate 0.00114412
2017-10-10T12:42:36.470440: step 380, loss 0.185778, acc 0.921875, learning_rate 0.00113986
2017-10-10T12:42:36.756807: step 381, loss 0.216828, acc 0.9375, learning_rate 0.00113561
2017-10-10T12:42:37.074641: step 382, loss 0.262763, acc 0.890625, learning_rate 0.00113139
2017-10-10T12:42:37.379063: step 383, loss 0.320802, acc 0.890625, learning_rate 0.00112718
2017-10-10T12:42:37.699223: step 384, loss 0.21336, acc 0.921875, learning_rate 0.00112298
2017-10-10T12:42:38.015101: step 385, loss 0.301817, acc 0.90625, learning_rate 0.00111881
2017-10-10T12:42:38.312015: step 386, loss 0.101754, acc 0.96875, learning_rate 0.00111465
2017-10-10T12:42:38.613471: step 387, loss 0.216949, acc 0.921875, learning_rate 0.00111051
2017-10-10T12:42:38.923763: step 388, loss 0.284292, acc 0.859375, learning_rate 0.00110638
2017-10-10T12:42:39.223551: step 389, loss 0.383624, acc 0.828125, learning_rate 0.00110228
2017-10-10T12:42:39.559822: step 390, loss 0.16771, acc 0.921875, learning_rate 0.00109818
2017-10-10T12:42:39.890392: step 391, loss 0.302019, acc 0.90625, learning_rate 0.00109411
2017-10-10T12:42:40.170389: step 392, loss 0.382087, acc 0.823529, learning_rate 0.00109005
2017-10-10T12:42:40.485608: step 393, loss 0.300933, acc 0.90625, learning_rate 0.00108601
2017-10-10T12:42:40.786526: step 394, loss 0.277626, acc 0.921875, learning_rate 0.00108199
2017-10-10T12:42:41.103595: step 395, loss 0.44088, acc 0.890625, learning_rate 0.00107798
2017-10-10T12:42:41.405556: step 396, loss 0.136228, acc 0.953125, learning_rate 0.00107399
2017-10-10T12:42:41.708451: step 397, loss 0.174675, acc 0.921875, learning_rate 0.00107001
2017-10-10T12:42:42.013560: step 398, loss 0.329706, acc 0.890625, learning_rate 0.00106605
2017-10-10T12:42:42.344701: step 399, loss 0.117474, acc 0.984375, learning_rate 0.00106211
2017-10-10T12:42:42.720020: step 400, loss 0.454243, acc 0.859375, learning_rate 0.00105818

Evaluation:
2017-10-10T12:42:43.740434: step 400, loss 0.256166, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-400

2017-10-10T12:42:45.024542: step 401, loss 0.188366, acc 0.921875, learning_rate 0.00105427
2017-10-10T12:42:45.437472: step 402, loss 0.557322, acc 0.8125, learning_rate 0.00105037
2017-10-10T12:42:45.856832: step 403, loss 0.355685, acc 0.9375, learning_rate 0.0010465
2017-10-10T12:42:46.274073: step 404, loss 0.234335, acc 0.921875, learning_rate 0.00104263
2017-10-10T12:42:46.692421: step 405, loss 0.423425, acc 0.859375, learning_rate 0.00103878
2017-10-10T12:42:47.106664: step 406, loss 0.323567, acc 0.875, learning_rate 0.00103495
2017-10-10T12:42:47.525268: step 407, loss 0.476284, acc 0.890625, learning_rate 0.00103114
2017-10-10T12:42:47.923569: step 408, loss 0.199642, acc 0.9375, learning_rate 0.00102734
2017-10-10T12:42:48.341154: step 409, loss 0.359505, acc 0.875, learning_rate 0.00102355
2017-10-10T12:42:48.688332: step 410, loss 0.35019, acc 0.90625, learning_rate 0.00101978
2017-10-10T12:42:49.093170: step 411, loss 0.376123, acc 0.890625, learning_rate 0.00101603
2017-10-10T12:42:49.480369: step 412, loss 0.238111, acc 0.953125, learning_rate 0.00101229
2017-10-10T12:42:49.914289: step 413, loss 0.360128, acc 0.859375, learning_rate 0.00100856
2017-10-10T12:42:50.321139: step 414, loss 0.174051, acc 0.9375, learning_rate 0.00100486
2017-10-10T12:42:50.756015: step 415, loss 0.160133, acc 0.953125, learning_rate 0.00100116
2017-10-10T12:42:51.217153: step 416, loss 0.25391, acc 0.890625, learning_rate 0.000997483
2017-10-10T12:42:51.530557: step 417, loss 0.155215, acc 0.953125, learning_rate 0.00099382
2017-10-10T12:42:51.853175: step 418, loss 0.373307, acc 0.90625, learning_rate 0.000990172
2017-10-10T12:42:52.159978: step 419, loss 0.214112, acc 0.921875, learning_rate 0.000986538
2017-10-10T12:42:52.578912: step 420, loss 0.428478, acc 0.875, learning_rate 0.00098292
2017-10-10T12:42:53.014379: step 421, loss 0.187438, acc 0.953125, learning_rate 0.000979316
2017-10-10T12:42:53.445582: step 422, loss 0.301734, acc 0.90625, learning_rate 0.000975727
2017-10-10T12:42:53.878963: step 423, loss 0.283579, acc 0.921875, learning_rate 0.000972152
2017-10-10T12:42:54.300957: step 424, loss 0.315238, acc 0.875, learning_rate 0.000968592
2017-10-10T12:42:54.710984: step 425, loss 0.203793, acc 0.953125, learning_rate 0.000965047
2017-10-10T12:42:55.129597: step 426, loss 0.203505, acc 0.953125, learning_rate 0.000961516
2017-10-10T12:42:55.543131: step 427, loss 0.234686, acc 0.921875, learning_rate 0.000958
2017-10-10T12:42:55.899666: step 428, loss 0.271024, acc 0.90625, learning_rate 0.000954497
2017-10-10T12:42:56.241066: step 429, loss 0.48179, acc 0.875, learning_rate 0.00095101
2017-10-10T12:42:56.557082: step 430, loss 0.464406, acc 0.84375, learning_rate 0.000947536
2017-10-10T12:42:56.943845: step 431, loss 0.352319, acc 0.890625, learning_rate 0.000944076
2017-10-10T12:42:57.326288: step 432, loss 0.235156, acc 0.921875, learning_rate 0.000940631
2017-10-10T12:42:57.741123: step 433, loss 0.205707, acc 0.890625, learning_rate 0.0009372
2017-10-10T12:42:58.092531: step 434, loss 0.403391, acc 0.90625, learning_rate 0.000933783
2017-10-10T12:42:58.509958: step 435, loss 0.255207, acc 0.921875, learning_rate 0.000930379
2017-10-10T12:42:58.932181: step 436, loss 0.238725, acc 0.890625, learning_rate 0.00092699
2017-10-10T12:42:59.412384: step 437, loss 0.270744, acc 0.90625, learning_rate 0.000923614
2017-10-10T12:42:59.712761: step 438, loss 0.104138, acc 0.984375, learning_rate 0.000920253
2017-10-10T12:43:00.024275: step 439, loss 0.17804, acc 0.9375, learning_rate 0.000916905
2017-10-10T12:43:00.332399: step 440, loss 0.454848, acc 0.84375, learning_rate 0.00091357

Evaluation:
2017-10-10T12:43:01.272287: step 440, loss 0.255144, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-440

2017-10-10T12:43:02.649045: step 441, loss 0.262985, acc 0.90625, learning_rate 0.000910249
2017-10-10T12:43:03.045099: step 442, loss 0.195683, acc 0.9375, learning_rate 0.000906942
2017-10-10T12:43:03.433924: step 443, loss 0.221185, acc 0.921875, learning_rate 0.000903648
2017-10-10T12:43:03.812967: step 444, loss 0.409139, acc 0.859375, learning_rate 0.000900368
2017-10-10T12:43:04.196307: step 445, loss 0.373527, acc 0.890625, learning_rate 0.000897101
2017-10-10T12:43:04.568972: step 446, loss 0.35298, acc 0.890625, learning_rate 0.000893848
2017-10-10T12:43:05.001271: step 447, loss 0.273059, acc 0.9375, learning_rate 0.000890607
2017-10-10T12:43:05.456823: step 448, loss 0.492802, acc 0.875, learning_rate 0.00088738
2017-10-10T12:43:05.896843: step 449, loss 0.268355, acc 0.890625, learning_rate 0.000884166
2017-10-10T12:43:06.330271: step 450, loss 0.34667, acc 0.859375, learning_rate 0.000880966
2017-10-10T12:43:06.736106: step 451, loss 0.193653, acc 0.953125, learning_rate 0.000877778
2017-10-10T12:43:07.155832: step 452, loss 0.459649, acc 0.859375, learning_rate 0.000874603
2017-10-10T12:43:07.557063: step 453, loss 0.505247, acc 0.90625, learning_rate 0.000871441
2017-10-10T12:43:07.976891: step 454, loss 0.254932, acc 0.890625, learning_rate 0.000868293
2017-10-10T12:43:08.505872: step 455, loss 0.487599, acc 0.828125, learning_rate 0.000865157
2017-10-10T12:43:08.952204: step 456, loss 0.418359, acc 0.90625, learning_rate 0.000862033
2017-10-10T12:43:09.251415: step 457, loss 0.147219, acc 0.953125, learning_rate 0.000858923
2017-10-10T12:43:09.555256: step 458, loss 0.211125, acc 0.90625, learning_rate 0.000855825
2017-10-10T12:43:09.876769: step 459, loss 0.163392, acc 0.921875, learning_rate 0.00085274
2017-10-10T12:43:10.299791: step 460, loss 0.215972, acc 0.953125, learning_rate 0.000849668
2017-10-10T12:43:10.729820: step 461, loss 0.192303, acc 0.9375, learning_rate 0.000846608
2017-10-10T12:43:11.176972: step 462, loss 0.22502, acc 0.9375, learning_rate 0.00084356
2017-10-10T12:43:11.600854: step 463, loss 0.405601, acc 0.90625, learning_rate 0.000840525
2017-10-10T12:43:12.036668: step 464, loss 0.230949, acc 0.9375, learning_rate 0.000837502
2017-10-10T12:43:12.399496: step 465, loss 0.322276, acc 0.859375, learning_rate 0.000834492
2017-10-10T12:43:12.773887: step 466, loss 0.395464, acc 0.859375, learning_rate 0.000831494
2017-10-10T12:43:13.203579: step 467, loss 0.240438, acc 0.90625, learning_rate 0.000828508
2017-10-10T12:43:13.611795: step 468, loss 0.366634, acc 0.859375, learning_rate 0.000825535
2017-10-10T12:43:13.980833: step 469, loss 0.365666, acc 0.875, learning_rate 0.000822573
2017-10-10T12:43:14.377155: step 470, loss 0.162776, acc 0.9375, learning_rate 0.000819624
2017-10-10T12:43:14.772838: step 471, loss 0.334574, acc 0.84375, learning_rate 0.000816687
2017-10-10T12:43:15.143483: step 472, loss 0.11172, acc 0.984375, learning_rate 0.000813761
2017-10-10T12:43:15.483777: step 473, loss 0.363521, acc 0.875, learning_rate 0.000810848
2017-10-10T12:43:15.858750: step 474, loss 0.196415, acc 0.9375, learning_rate 0.000807946
2017-10-10T12:43:16.292850: step 475, loss 0.398896, acc 0.859375, learning_rate 0.000805057
2017-10-10T12:43:16.645105: step 476, loss 0.209331, acc 0.90625, learning_rate 0.000802179
2017-10-10T12:43:17.154774: step 477, loss 0.213081, acc 0.90625, learning_rate 0.000799313
2017-10-10T12:43:17.489071: step 478, loss 0.264604, acc 0.890625, learning_rate 0.000796458
2017-10-10T12:43:17.788610: step 479, loss 0.489457, acc 0.875, learning_rate 0.000793616
2017-10-10T12:43:18.054180: step 480, loss 0.190812, acc 0.96875, learning_rate 0.000790784

Evaluation:
2017-10-10T12:43:18.839465: step 480, loss 0.251626, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-480

2017-10-10T12:43:20.463526: step 481, loss 0.349808, acc 0.90625, learning_rate 0.000787965
2017-10-10T12:43:20.719337: step 482, loss 0.366091, acc 0.859375, learning_rate 0.000785157
2017-10-10T12:43:21.110839: step 483, loss 0.236943, acc 0.90625, learning_rate 0.00078236
2017-10-10T12:43:21.528921: step 484, loss 0.164855, acc 0.9375, learning_rate 0.000779575
2017-10-10T12:43:21.934595: step 485, loss 0.0874748, acc 0.984375, learning_rate 0.000776801
2017-10-10T12:43:22.337132: step 486, loss 0.23392, acc 0.953125, learning_rate 0.000774038
2017-10-10T12:43:22.765252: step 487, loss 0.230782, acc 0.921875, learning_rate 0.000771287
2017-10-10T12:43:23.124280: step 488, loss 0.218976, acc 0.90625, learning_rate 0.000768547
2017-10-10T12:43:23.513195: step 489, loss 0.385339, acc 0.828125, learning_rate 0.000765818
2017-10-10T12:43:23.860911: step 490, loss 0.366118, acc 0.921569, learning_rate 0.000763101
2017-10-10T12:43:24.213275: step 491, loss 0.118879, acc 0.96875, learning_rate 0.000760394
2017-10-10T12:43:24.608431: step 492, loss 0.338081, acc 0.859375, learning_rate 0.000757698
2017-10-10T12:43:24.993131: step 493, loss 0.199857, acc 0.953125, learning_rate 0.000755014
2017-10-10T12:43:25.443120: step 494, loss 0.347142, acc 0.875, learning_rate 0.00075234
2017-10-10T12:43:25.873428: step 495, loss 0.387194, acc 0.828125, learning_rate 0.000749677
2017-10-10T12:43:26.169787: step 496, loss 0.244008, acc 0.90625, learning_rate 0.000747026
2017-10-10T12:43:26.455809: step 497, loss 0.321759, acc 0.890625, learning_rate 0.000744385
2017-10-10T12:43:26.754141: step 498, loss 0.280498, acc 0.890625, learning_rate 0.000741754
2017-10-10T12:43:27.064938: step 499, loss 0.160164, acc 0.9375, learning_rate 0.000739135
2017-10-10T12:43:27.508086: step 500, loss 0.150169, acc 0.953125, learning_rate 0.000736526
2017-10-10T12:43:27.910060: step 501, loss 0.243804, acc 0.90625, learning_rate 0.000733928
2017-10-10T12:43:28.313502: step 502, loss 0.329262, acc 0.9375, learning_rate 0.00073134
2017-10-10T12:43:28.734554: step 503, loss 0.387555, acc 0.875, learning_rate 0.000728763
2017-10-10T12:43:29.116413: step 504, loss 0.147685, acc 0.953125, learning_rate 0.000726197
2017-10-10T12:43:29.516848: step 505, loss 0.216377, acc 0.90625, learning_rate 0.000723641
2017-10-10T12:43:29.916824: step 506, loss 0.138427, acc 0.953125, learning_rate 0.000721095
2017-10-10T12:43:30.260969: step 507, loss 0.245954, acc 0.9375, learning_rate 0.00071856
2017-10-10T12:43:30.715893: step 508, loss 0.185844, acc 0.90625, learning_rate 0.000716036
2017-10-10T12:43:31.120849: step 509, loss 0.191667, acc 0.9375, learning_rate 0.000713521
2017-10-10T12:43:31.460901: step 510, loss 0.459537, acc 0.890625, learning_rate 0.000711017
2017-10-10T12:43:31.884837: step 511, loss 0.251792, acc 0.90625, learning_rate 0.000708523
2017-10-10T12:43:32.285761: step 512, loss 0.372631, acc 0.875, learning_rate 0.000706039
2017-10-10T12:43:32.692851: step 513, loss 0.356292, acc 0.875, learning_rate 0.000703565
2017-10-10T12:43:33.109152: step 514, loss 0.106801, acc 0.96875, learning_rate 0.000701102
2017-10-10T12:43:33.509284: step 515, loss 0.326278, acc 0.890625, learning_rate 0.000698648
2017-10-10T12:43:33.908826: step 516, loss 0.345665, acc 0.90625, learning_rate 0.000696204
2017-10-10T12:43:34.357071: step 517, loss 0.220921, acc 0.921875, learning_rate 0.000693771
2017-10-10T12:43:34.778139: step 518, loss 0.261309, acc 0.859375, learning_rate 0.000691347
2017-10-10T12:43:35.124883: step 519, loss 0.325671, acc 0.90625, learning_rate 0.000688934
2017-10-10T12:43:35.449999: step 520, loss 0.40706, acc 0.890625, learning_rate 0.00068653

Evaluation:
2017-10-10T12:43:36.245080: step 520, loss 0.244483, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-520

2017-10-10T12:43:37.466509: step 521, loss 0.191438, acc 0.96875, learning_rate 0.000684136
2017-10-10T12:43:37.893583: step 522, loss 0.329571, acc 0.890625, learning_rate 0.000681751
2017-10-10T12:43:38.288848: step 523, loss 0.466054, acc 0.859375, learning_rate 0.000679377
2017-10-10T12:43:38.688841: step 524, loss 0.238135, acc 0.921875, learning_rate 0.000677012
2017-10-10T12:43:39.096855: step 525, loss 0.279074, acc 0.921875, learning_rate 0.000674657
2017-10-10T12:43:39.462181: step 526, loss 0.335494, acc 0.890625, learning_rate 0.000672311
2017-10-10T12:43:39.802924: step 527, loss 0.295133, acc 0.90625, learning_rate 0.000669975
2017-10-10T12:43:40.228116: step 528, loss 0.462345, acc 0.828125, learning_rate 0.000667648
2017-10-10T12:43:40.651550: step 529, loss 0.445677, acc 0.828125, learning_rate 0.000665331
2017-10-10T12:43:41.070806: step 530, loss 0.216205, acc 0.953125, learning_rate 0.000663024
2017-10-10T12:43:41.488974: step 531, loss 0.285004, acc 0.90625, learning_rate 0.000660726
2017-10-10T12:43:41.913409: step 532, loss 0.335822, acc 0.890625, learning_rate 0.000658437
2017-10-10T12:43:42.345813: step 533, loss 0.341738, acc 0.84375, learning_rate 0.000656158
2017-10-10T12:43:42.765324: step 534, loss 0.402559, acc 0.859375, learning_rate 0.000653888
2017-10-10T12:43:43.181018: step 535, loss 0.219896, acc 0.9375, learning_rate 0.000651627
2017-10-10T12:43:43.686189: step 536, loss 0.234006, acc 0.90625, learning_rate 0.000649375
2017-10-10T12:43:44.008167: step 537, loss 0.231945, acc 0.90625, learning_rate 0.000647133
2017-10-10T12:43:44.320652: step 538, loss 0.170191, acc 0.953125, learning_rate 0.000644899
2017-10-10T12:43:44.617881: step 539, loss 0.341137, acc 0.875, learning_rate 0.000642675
2017-10-10T12:43:44.936039: step 540, loss 0.200771, acc 0.9375, learning_rate 0.00064046
2017-10-10T12:43:45.284840: step 541, loss 0.410569, acc 0.875, learning_rate 0.000638254
2017-10-10T12:43:45.640836: step 542, loss 0.209844, acc 0.9375, learning_rate 0.000636057
2017-10-10T12:43:46.088972: step 543, loss 0.0720617, acc 1, learning_rate 0.000633869
2017-10-10T12:43:46.504751: step 544, loss 0.399412, acc 0.859375, learning_rate 0.00063169
2017-10-10T12:43:46.928464: step 545, loss 0.229882, acc 0.9375, learning_rate 0.00062952
2017-10-10T12:43:47.300500: step 546, loss 0.304227, acc 0.875, learning_rate 0.000627358
2017-10-10T12:43:47.691071: step 547, loss 0.255995, acc 0.90625, learning_rate 0.000625206
2017-10-10T12:43:48.118058: step 548, loss 0.194387, acc 0.90625, learning_rate 0.000623062
2017-10-10T12:43:48.539364: step 549, loss 0.377862, acc 0.890625, learning_rate 0.000620927
2017-10-10T12:43:48.986923: step 550, loss 0.168227, acc 0.921875, learning_rate 0.000618801
2017-10-10T12:43:49.424920: step 551, loss 0.298579, acc 0.90625, learning_rate 0.000616683
2017-10-10T12:43:49.824839: step 552, loss 0.199798, acc 0.9375, learning_rate 0.000614574
2017-10-10T12:43:50.225945: step 553, loss 0.247549, acc 0.90625, learning_rate 0.000612474
2017-10-10T12:43:50.640215: step 554, loss 0.302849, acc 0.90625, learning_rate 0.000610382
2017-10-10T12:43:51.058755: step 555, loss 0.129112, acc 0.953125, learning_rate 0.000608299
2017-10-10T12:43:51.469560: step 556, loss 0.227207, acc 0.875, learning_rate 0.000606224
2017-10-10T12:43:51.932304: step 557, loss 0.214723, acc 0.921875, learning_rate 0.000604158
2017-10-10T12:43:52.387500: step 558, loss 0.223831, acc 0.953125, learning_rate 0.0006021
2017-10-10T12:43:52.667706: step 559, loss 0.186259, acc 0.921875, learning_rate 0.00060005
2017-10-10T12:43:52.980348: step 560, loss 0.228598, acc 0.9375, learning_rate 0.000598009

Evaluation:
2017-10-10T12:43:53.798044: step 560, loss 0.254034, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-560

2017-10-10T12:43:55.247577: step 561, loss 0.273833, acc 0.9375, learning_rate 0.000595977
2017-10-10T12:43:55.576937: step 562, loss 0.389046, acc 0.875, learning_rate 0.000593952
2017-10-10T12:43:55.973028: step 563, loss 0.188654, acc 0.953125, learning_rate 0.000591936
2017-10-10T12:43:56.348396: step 564, loss 0.245862, acc 0.890625, learning_rate 0.000589928
2017-10-10T12:43:56.749040: step 565, loss 0.229299, acc 0.90625, learning_rate 0.000587928
2017-10-10T12:43:57.176581: step 566, loss 0.379246, acc 0.828125, learning_rate 0.000585937
2017-10-10T12:43:57.523966: step 567, loss 0.291218, acc 0.875, learning_rate 0.000583953
2017-10-10T12:43:57.897008: step 568, loss 0.315224, acc 0.890625, learning_rate 0.000581978
2017-10-10T12:43:58.305005: step 569, loss 0.195723, acc 0.90625, learning_rate 0.00058001
2017-10-10T12:43:58.665083: step 570, loss 0.340078, acc 0.90625, learning_rate 0.000578051
2017-10-10T12:43:59.006330: step 571, loss 0.11427, acc 0.96875, learning_rate 0.0005761
2017-10-10T12:43:59.454618: step 572, loss 0.258287, acc 0.921875, learning_rate 0.000574157
2017-10-10T12:43:59.877150: step 573, loss 0.17717, acc 0.953125, learning_rate 0.000572221
2017-10-10T12:44:00.340891: step 574, loss 0.0883045, acc 0.96875, learning_rate 0.000570294
2017-10-10T12:44:00.780854: step 575, loss 0.204274, acc 0.90625, learning_rate 0.000568374
2017-10-10T12:44:01.272826: step 576, loss 0.350241, acc 0.890625, learning_rate 0.000566462
2017-10-10T12:44:01.552881: step 577, loss 0.232067, acc 0.890625, learning_rate 0.000564558
2017-10-10T12:44:01.866578: step 578, loss 0.17054, acc 0.953125, learning_rate 0.000562662
2017-10-10T12:44:02.170574: step 579, loss 0.175645, acc 0.921875, learning_rate 0.000560774
2017-10-10T12:44:02.596693: step 580, loss 0.286257, acc 0.90625, learning_rate 0.000558893
2017-10-10T12:44:03.022279: step 581, loss 0.253004, acc 0.9375, learning_rate 0.00055702
2017-10-10T12:44:03.445917: step 582, loss 0.1711, acc 0.921875, learning_rate 0.000555154
2017-10-10T12:44:03.842347: step 583, loss 0.279002, acc 0.90625, learning_rate 0.000553296
2017-10-10T12:44:04.257683: step 584, loss 0.307036, acc 0.890625, learning_rate 0.000551446
2017-10-10T12:44:04.658198: step 585, loss 0.23152, acc 0.890625, learning_rate 0.000549604
2017-10-10T12:44:05.075474: step 586, loss 0.207821, acc 0.90625, learning_rate 0.000547768
2017-10-10T12:44:05.481017: step 587, loss 0.111711, acc 0.953125, learning_rate 0.000545941
2017-10-10T12:44:05.868798: step 588, loss 0.520557, acc 0.843137, learning_rate 0.00054412
2017-10-10T12:44:06.274466: step 589, loss 0.3749, acc 0.84375, learning_rate 0.000542308
2017-10-10T12:44:06.781724: step 590, loss 0.193769, acc 0.9375, learning_rate 0.000540502
2017-10-10T12:44:07.195851: step 591, loss 0.167002, acc 0.9375, learning_rate 0.000538704
2017-10-10T12:44:07.614802: step 592, loss 0.246185, acc 0.90625, learning_rate 0.000536914
2017-10-10T12:44:08.044397: step 593, loss 0.32924, acc 0.859375, learning_rate 0.00053513
2017-10-10T12:44:08.452845: step 594, loss 0.208995, acc 0.9375, learning_rate 0.000533354
2017-10-10T12:44:08.860860: step 595, loss 0.238904, acc 0.953125, learning_rate 0.000531585
2017-10-10T12:44:09.344237: step 596, loss 0.258212, acc 0.890625, learning_rate 0.000529824
2017-10-10T12:44:09.757480: step 597, loss 0.288237, acc 0.9375, learning_rate 0.000528069
2017-10-10T12:44:10.094434: step 598, loss 0.188782, acc 0.953125, learning_rate 0.000526322
2017-10-10T12:44:10.419569: step 599, loss 0.380343, acc 0.875, learning_rate 0.000524582
2017-10-10T12:44:10.748422: step 600, loss 0.162854, acc 0.890625, learning_rate 0.000522849

Evaluation:
2017-10-10T12:44:11.696654: step 600, loss 0.243415, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-600

2017-10-10T12:44:13.342460: step 601, loss 0.265996, acc 0.9375, learning_rate 0.000521123
2017-10-10T12:44:13.746688: step 602, loss 0.238843, acc 0.921875, learning_rate 0.000519404
2017-10-10T12:44:14.161803: step 603, loss 0.0726402, acc 0.984375, learning_rate 0.000517692
2017-10-10T12:44:14.573523: step 604, loss 0.168701, acc 0.921875, learning_rate 0.000515987
2017-10-10T12:44:14.981762: step 605, loss 0.137654, acc 0.96875, learning_rate 0.000514289
2017-10-10T12:44:15.379632: step 606, loss 0.171361, acc 0.953125, learning_rate 0.000512598
2017-10-10T12:44:15.745292: step 607, loss 0.215041, acc 0.9375, learning_rate 0.000510914
2017-10-10T12:44:16.045127: step 608, loss 0.269458, acc 0.921875, learning_rate 0.000509237
2017-10-10T12:44:16.457570: step 609, loss 0.369106, acc 0.828125, learning_rate 0.000507566
2017-10-10T12:44:16.765805: step 610, loss 0.1073, acc 0.96875, learning_rate 0.000505903
2017-10-10T12:44:17.158497: step 611, loss 0.294793, acc 0.90625, learning_rate 0.000504246
2017-10-10T12:44:17.569663: step 612, loss 0.164225, acc 0.921875, learning_rate 0.000502596
2017-10-10T12:44:17.985451: step 613, loss 0.233909, acc 0.921875, learning_rate 0.000500953
2017-10-10T12:44:18.402872: step 614, loss 0.387521, acc 0.859375, learning_rate 0.000499316
2017-10-10T12:44:18.826035: step 615, loss 0.289286, acc 0.890625, learning_rate 0.000497686
2017-10-10T12:44:19.267946: step 616, loss 0.167414, acc 0.9375, learning_rate 0.000496063
2017-10-10T12:44:19.612832: step 617, loss 0.368246, acc 0.890625, learning_rate 0.000494446
2017-10-10T12:44:19.948925: step 618, loss 0.274458, acc 0.90625, learning_rate 0.000492836
2017-10-10T12:44:20.248262: step 619, loss 0.241018, acc 0.9375, learning_rate 0.000491233
2017-10-10T12:44:20.636226: step 620, loss 0.32713, acc 0.90625, learning_rate 0.000489636
2017-10-10T12:44:21.010599: step 621, loss 0.160036, acc 0.921875, learning_rate 0.000488045
2017-10-10T12:44:21.448864: step 622, loss 0.354112, acc 0.859375, learning_rate 0.000486461
2017-10-10T12:44:21.876670: step 623, loss 0.248907, acc 0.9375, learning_rate 0.000484884
2017-10-10T12:44:22.295542: step 624, loss 0.199857, acc 0.9375, learning_rate 0.000483313
2017-10-10T12:44:22.684820: step 625, loss 0.177539, acc 0.984375, learning_rate 0.000481748
2017-10-10T12:44:23.114913: step 626, loss 0.354849, acc 0.84375, learning_rate 0.00048019
2017-10-10T12:44:23.511674: step 627, loss 0.328623, acc 0.921875, learning_rate 0.000478638
2017-10-10T12:44:23.888597: step 628, loss 0.369368, acc 0.859375, learning_rate 0.000477093
2017-10-10T12:44:24.232894: step 629, loss 0.147984, acc 0.9375, learning_rate 0.000475554
2017-10-10T12:44:24.632237: step 630, loss 0.105058, acc 0.96875, learning_rate 0.000474021
2017-10-10T12:44:25.048868: step 631, loss 0.287549, acc 0.859375, learning_rate 0.000472494
2017-10-10T12:44:25.417014: step 632, loss 0.158054, acc 0.9375, learning_rate 0.000470974
2017-10-10T12:44:25.864843: step 633, loss 0.267857, acc 0.921875, learning_rate 0.000469459
2017-10-10T12:44:26.297650: step 634, loss 0.313558, acc 0.90625, learning_rate 0.000467951
2017-10-10T12:44:26.701485: step 635, loss 0.191304, acc 0.953125, learning_rate 0.000466449
2017-10-10T12:44:27.114713: step 636, loss 0.163978, acc 0.9375, learning_rate 0.000464954
2017-10-10T12:44:27.590110: step 637, loss 0.306109, acc 0.90625, learning_rate 0.000463464
2017-10-10T12:44:27.949360: step 638, loss 0.224469, acc 0.953125, learning_rate 0.00046198
2017-10-10T12:44:28.262653: step 639, loss 0.15264, acc 0.9375, learning_rate 0.000460503
2017-10-10T12:44:28.565491: step 640, loss 0.256127, acc 0.90625, learning_rate 0.000459031

Evaluation:
2017-10-10T12:44:29.424947: step 640, loss 0.249406, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-640

2017-10-10T12:44:30.636951: step 641, loss 0.173375, acc 0.953125, learning_rate 0.000457566
2017-10-10T12:44:31.049043: step 642, loss 0.200663, acc 0.921875, learning_rate 0.000456106
2017-10-10T12:44:31.332824: step 643, loss 0.32833, acc 0.90625, learning_rate 0.000454653
2017-10-10T12:44:31.746047: step 644, loss 0.338672, acc 0.890625, learning_rate 0.000453205
2017-10-10T12:44:32.168860: step 645, loss 0.320305, acc 0.859375, learning_rate 0.000451764
2017-10-10T12:44:32.585300: step 646, loss 0.360313, acc 0.890625, learning_rate 0.000450328
2017-10-10T12:44:33.045034: step 647, loss 0.104059, acc 0.96875, learning_rate 0.000448898
2017-10-10T12:44:33.476956: step 648, loss 0.398728, acc 0.890625, learning_rate 0.000447474
2017-10-10T12:44:33.871691: step 649, loss 0.302684, acc 0.90625, learning_rate 0.000446055
2017-10-10T12:44:34.285855: step 650, loss 0.156747, acc 0.9375, learning_rate 0.000444643
2017-10-10T12:44:34.686269: step 651, loss 0.390291, acc 0.890625, learning_rate 0.000443236
2017-10-10T12:44:35.090179: step 652, loss 0.145691, acc 0.96875, learning_rate 0.000441835
2017-10-10T12:44:35.506255: step 653, loss 0.164098, acc 0.9375, learning_rate 0.00044044
2017-10-10T12:44:35.957609: step 654, loss 0.215344, acc 0.921875, learning_rate 0.00043905
2017-10-10T12:44:36.480856: step 655, loss 0.320293, acc 0.921875, learning_rate 0.000437666
2017-10-10T12:44:36.958532: step 656, loss 0.14549, acc 0.921875, learning_rate 0.000436288
2017-10-10T12:44:37.298966: step 657, loss 0.202373, acc 0.90625, learning_rate 0.000434915
2017-10-10T12:44:37.608735: step 658, loss 0.216367, acc 0.90625, learning_rate 0.000433548
2017-10-10T12:44:37.921794: step 659, loss 0.168541, acc 0.9375, learning_rate 0.000432187
2017-10-10T12:44:38.318398: step 660, loss 0.312419, acc 0.890625, learning_rate 0.000430831
2017-10-10T12:44:38.743355: step 661, loss 0.105712, acc 0.953125, learning_rate 0.000429481
2017-10-10T12:44:39.166153: step 662, loss 0.23748, acc 0.9375, learning_rate 0.000428136
2017-10-10T12:44:39.601179: step 663, loss 0.148525, acc 0.9375, learning_rate 0.000426796
2017-10-10T12:44:40.007251: step 664, loss 0.321694, acc 0.875, learning_rate 0.000425463
2017-10-10T12:44:40.425032: step 665, loss 0.147426, acc 0.9375, learning_rate 0.000424134
2017-10-10T12:44:40.892832: step 666, loss 0.299606, acc 0.890625, learning_rate 0.000422811
2017-10-10T12:44:41.336928: step 667, loss 0.200303, acc 0.921875, learning_rate 0.000421493
2017-10-10T12:44:41.779387: step 668, loss 0.16382, acc 0.9375, learning_rate 0.000420181
2017-10-10T12:44:42.241077: step 669, loss 0.210962, acc 0.9375, learning_rate 0.000418874
2017-10-10T12:44:42.615732: step 670, loss 0.305966, acc 0.875, learning_rate 0.000417573
2017-10-10T12:44:43.016888: step 671, loss 0.239288, acc 0.90625, learning_rate 0.000416276
2017-10-10T12:44:43.429052: step 672, loss 0.196738, acc 0.890625, learning_rate 0.000414985
2017-10-10T12:44:43.824841: step 673, loss 0.312658, acc 0.84375, learning_rate 0.0004137
2017-10-10T12:44:44.216409: step 674, loss 0.223936, acc 0.90625, learning_rate 0.000412419
2017-10-10T12:44:44.644963: step 675, loss 0.216136, acc 0.921875, learning_rate 0.000411144
2017-10-10T12:44:45.120857: step 676, loss 0.233006, acc 0.9375, learning_rate 0.000409874
2017-10-10T12:44:45.605127: step 677, loss 0.187606, acc 0.90625, learning_rate 0.000408609
2017-10-10T12:44:45.876846: step 678, loss 0.252188, acc 0.9375, learning_rate 0.00040735
2017-10-10T12:44:46.180879: step 679, loss 0.332607, acc 0.875, learning_rate 0.000406095
2017-10-10T12:44:46.478208: step 680, loss 0.258067, acc 0.90625, learning_rate 0.000404846

Evaluation:
2017-10-10T12:44:47.396817: step 680, loss 0.24339, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-680

2017-10-10T12:44:48.798651: step 681, loss 0.207532, acc 0.9375, learning_rate 0.000403601
2017-10-10T12:44:49.223478: step 682, loss 0.205555, acc 0.9375, learning_rate 0.000402362
2017-10-10T12:44:49.664112: step 683, loss 0.244036, acc 0.90625, learning_rate 0.000401128
2017-10-10T12:44:50.101198: step 684, loss 0.320711, acc 0.90625, learning_rate 0.000399899
2017-10-10T12:44:50.544896: step 685, loss 0.19735, acc 0.921875, learning_rate 0.000398675
2017-10-10T12:44:50.952250: step 686, loss 0.290174, acc 0.921569, learning_rate 0.000397456
2017-10-10T12:44:51.432217: step 687, loss 0.234335, acc 0.921875, learning_rate 0.000396241
2017-10-10T12:44:51.832599: step 688, loss 0.217321, acc 0.90625, learning_rate 0.000395032
2017-10-10T12:44:52.213456: step 689, loss 0.0956856, acc 0.96875, learning_rate 0.000393828
2017-10-10T12:44:52.661141: step 690, loss 0.177322, acc 0.953125, learning_rate 0.000392629
2017-10-10T12:44:53.088150: step 691, loss 0.282785, acc 0.90625, learning_rate 0.000391434
2017-10-10T12:44:53.516949: step 692, loss 0.197242, acc 0.921875, learning_rate 0.000390245
2017-10-10T12:44:53.946136: step 693, loss 0.231518, acc 0.9375, learning_rate 0.00038906
2017-10-10T12:44:54.375054: step 694, loss 0.151514, acc 0.9375, learning_rate 0.00038788
2017-10-10T12:44:54.808856: step 695, loss 0.168565, acc 0.953125, learning_rate 0.000386705
2017-10-10T12:44:55.285144: step 696, loss 0.312639, acc 0.890625, learning_rate 0.000385535
2017-10-10T12:44:55.647476: step 697, loss 0.155172, acc 0.9375, learning_rate 0.000384369
2017-10-10T12:44:55.928505: step 698, loss 0.152474, acc 0.953125, learning_rate 0.000383209
2017-10-10T12:44:56.212579: step 699, loss 0.258756, acc 0.921875, learning_rate 0.000382053
2017-10-10T12:44:56.612912: step 700, loss 0.310146, acc 0.875, learning_rate 0.000380901
2017-10-10T12:44:57.089675: step 701, loss 0.158954, acc 0.9375, learning_rate 0.000379755
2017-10-10T12:44:57.516446: step 702, loss 0.169778, acc 0.9375, learning_rate 0.000378613
2017-10-10T12:44:57.915441: step 703, loss 0.180622, acc 0.953125, learning_rate 0.000377476
2017-10-10T12:44:58.272900: step 704, loss 0.185075, acc 0.9375, learning_rate 0.000376343
2017-10-10T12:44:58.739336: step 705, loss 0.224103, acc 0.90625, learning_rate 0.000375215
2017-10-10T12:44:59.160910: step 706, loss 0.236109, acc 0.90625, learning_rate 0.000374092
2017-10-10T12:44:59.597988: step 707, loss 0.262444, acc 0.890625, learning_rate 0.000372973
2017-10-10T12:45:00.045134: step 708, loss 0.242561, acc 0.9375, learning_rate 0.000371859
2017-10-10T12:45:00.470645: step 709, loss 0.175048, acc 0.9375, learning_rate 0.000370749
2017-10-10T12:45:00.916375: step 710, loss 0.164272, acc 0.953125, learning_rate 0.000369644
2017-10-10T12:45:01.379096: step 711, loss 0.256262, acc 0.859375, learning_rate 0.000368543
2017-10-10T12:45:01.884820: step 712, loss 0.194753, acc 0.953125, learning_rate 0.000367447
2017-10-10T12:45:02.313116: step 713, loss 0.208358, acc 0.921875, learning_rate 0.000366356
2017-10-10T12:45:02.737101: step 714, loss 0.296377, acc 0.921875, learning_rate 0.000365268
2017-10-10T12:45:03.158087: step 715, loss 0.228572, acc 0.90625, learning_rate 0.000364186
2017-10-10T12:45:03.618154: step 716, loss 0.281154, acc 0.90625, learning_rate 0.000363107
2017-10-10T12:45:04.069121: step 717, loss 0.313099, acc 0.921875, learning_rate 0.000362033
2017-10-10T12:45:04.437516: step 718, loss 0.278388, acc 0.875, learning_rate 0.000360964
2017-10-10T12:45:04.762790: step 719, loss 0.197176, acc 0.90625, learning_rate 0.000359899
2017-10-10T12:45:05.057003: step 720, loss 0.244435, acc 0.90625, learning_rate 0.000358838

Evaluation:
2017-10-10T12:45:05.992598: step 720, loss 0.241563, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-720

2017-10-10T12:45:07.571017: step 721, loss 0.377198, acc 0.890625, learning_rate 0.000357781
2017-10-10T12:45:08.017098: step 722, loss 0.317074, acc 0.90625, learning_rate 0.000356729
2017-10-10T12:45:08.353260: step 723, loss 0.144848, acc 0.953125, learning_rate 0.000355681
2017-10-10T12:45:08.704845: step 724, loss 0.194725, acc 0.90625, learning_rate 0.000354637
2017-10-10T12:45:09.143835: step 725, loss 0.223089, acc 0.921875, learning_rate 0.000353598
2017-10-10T12:45:09.563227: step 726, loss 0.172012, acc 0.9375, learning_rate 0.000352563
2017-10-10T12:45:09.973579: step 727, loss 0.266619, acc 0.9375, learning_rate 0.000351532
2017-10-10T12:45:10.371655: step 728, loss 0.494978, acc 0.859375, learning_rate 0.000350505
2017-10-10T12:45:10.786540: step 729, loss 0.20412, acc 0.921875, learning_rate 0.000349483
2017-10-10T12:45:11.248954: step 730, loss 0.22624, acc 0.921875, learning_rate 0.000348465
2017-10-10T12:45:11.716818: step 731, loss 0.222561, acc 0.90625, learning_rate 0.00034745
2017-10-10T12:45:12.121198: step 732, loss 0.134093, acc 0.953125, learning_rate 0.00034644
2017-10-10T12:45:12.584827: step 733, loss 0.18732, acc 0.96875, learning_rate 0.000345434
2017-10-10T12:45:13.062935: step 734, loss 0.159771, acc 0.953125, learning_rate 0.000344433
2017-10-10T12:45:13.559320: step 735, loss 0.225962, acc 0.90625, learning_rate 0.000343435
2017-10-10T12:45:13.885215: step 736, loss 0.277866, acc 0.90625, learning_rate 0.000342441
2017-10-10T12:45:14.241945: step 737, loss 0.191795, acc 0.9375, learning_rate 0.000341452
2017-10-10T12:45:14.556802: step 738, loss 0.182726, acc 0.921875, learning_rate 0.000340466
2017-10-10T12:45:14.985411: step 739, loss 0.230333, acc 0.921875, learning_rate 0.000339485
2017-10-10T12:45:15.440830: step 740, loss 0.116548, acc 0.96875, learning_rate 0.000338507
2017-10-10T12:45:15.890958: step 741, loss 0.230346, acc 0.921875, learning_rate 0.000337534
2017-10-10T12:45:16.356313: step 742, loss 0.141685, acc 0.953125, learning_rate 0.000336564
2017-10-10T12:45:16.815327: step 743, loss 0.300515, acc 0.953125, learning_rate 0.000335598
2017-10-10T12:45:17.300828: step 744, loss 0.221725, acc 0.90625, learning_rate 0.000334637
2017-10-10T12:45:17.732715: step 745, loss 0.283304, acc 0.9375, learning_rate 0.000333679
2017-10-10T12:45:18.164808: step 746, loss 0.145667, acc 0.953125, learning_rate 0.000332725
2017-10-10T12:45:18.636759: step 747, loss 0.179308, acc 0.921875, learning_rate 0.000331775
2017-10-10T12:45:19.032267: step 748, loss 0.221248, acc 0.921875, learning_rate 0.000330829
2017-10-10T12:45:19.412896: step 749, loss 0.153002, acc 0.953125, learning_rate 0.000329887
2017-10-10T12:45:19.834860: step 750, loss 0.143574, acc 0.96875, learning_rate 0.000328949
2017-10-10T12:45:20.205329: step 751, loss 0.267804, acc 0.90625, learning_rate 0.000328014
2017-10-10T12:45:20.685191: step 752, loss 0.163358, acc 0.9375, learning_rate 0.000327083
2017-10-10T12:45:21.120927: step 753, loss 0.213, acc 0.953125, learning_rate 0.000326157
2017-10-10T12:45:21.616929: step 754, loss 0.34334, acc 0.890625, learning_rate 0.000325233
2017-10-10T12:45:22.003309: step 755, loss 0.236599, acc 0.9375, learning_rate 0.000324314
2017-10-10T12:45:22.505745: step 756, loss 0.210923, acc 0.9375, learning_rate 0.000323399
2017-10-10T12:45:22.883975: step 757, loss 0.234809, acc 0.90625, learning_rate 0.000322487
2017-10-10T12:45:23.206013: step 758, loss 0.204348, acc 0.9375, learning_rate 0.000321579
2017-10-10T12:45:23.541334: step 759, loss 0.245402, acc 0.90625, learning_rate 0.000320674
2017-10-10T12:45:23.945191: step 760, loss 0.184904, acc 0.9375, learning_rate 0.000319773

Evaluation:
2017-10-10T12:45:24.954818: step 760, loss 0.247694, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-760

2017-10-10T12:45:26.140989: step 761, loss 0.167807, acc 0.9375, learning_rate 0.000318876
2017-10-10T12:45:26.684159: step 762, loss 0.201942, acc 0.9375, learning_rate 0.000317983
2017-10-10T12:45:27.244852: step 763, loss 0.167835, acc 0.9375, learning_rate 0.000317093
2017-10-10T12:45:27.764201: step 764, loss 0.149565, acc 0.984375, learning_rate 0.000316207
2017-10-10T12:45:28.324859: step 765, loss 0.261933, acc 0.921875, learning_rate 0.000315325
2017-10-10T12:45:28.874215: step 766, loss 0.139759, acc 0.953125, learning_rate 0.000314446
2017-10-10T12:45:29.452853: step 767, loss 0.0963655, acc 0.96875, learning_rate 0.00031357
2017-10-10T12:45:30.109037: step 768, loss 0.20542, acc 0.90625, learning_rate 0.000312699
2017-10-10T12:45:30.631129: step 769, loss 0.333644, acc 0.859375, learning_rate 0.00031183
2017-10-10T12:45:31.140882: step 770, loss 0.341469, acc 0.875, learning_rate 0.000310966
2017-10-10T12:45:31.617127: step 771, loss 0.268832, acc 0.921875, learning_rate 0.000310105
2017-10-10T12:45:32.108908: step 772, loss 0.289308, acc 0.90625, learning_rate 0.000309247
2017-10-10T12:45:32.683097: step 773, loss 0.33235, acc 0.859375, learning_rate 0.000308393
2017-10-10T12:45:33.247899: step 774, loss 0.324859, acc 0.875, learning_rate 0.000307542
2017-10-10T12:45:33.868933: step 775, loss 0.382541, acc 0.90625, learning_rate 0.000306695
2017-10-10T12:45:34.270717: step 776, loss 0.0797951, acc 0.984375, learning_rate 0.000305852
2017-10-10T12:45:34.677719: step 777, loss 0.16385, acc 0.96875, learning_rate 0.000305011
2017-10-10T12:45:35.069252: step 778, loss 0.183278, acc 0.9375, learning_rate 0.000304174
2017-10-10T12:45:35.580934: step 779, loss 0.235965, acc 0.921875, learning_rate 0.000303341
2017-10-10T12:45:36.039209: step 780, loss 0.128801, acc 0.953125, learning_rate 0.000302511
2017-10-10T12:45:36.490958: step 781, loss 0.198872, acc 0.921875, learning_rate 0.000301684
2017-10-10T12:45:37.001514: step 782, loss 0.39413, acc 0.875, learning_rate 0.000300861
2017-10-10T12:45:37.485264: step 783, loss 0.0990682, acc 0.96875, learning_rate 0.000300041
2017-10-10T12:45:37.980162: step 784, loss 0.148157, acc 0.921569, learning_rate 0.000299225
2017-10-10T12:45:38.461445: step 785, loss 0.152825, acc 0.9375, learning_rate 0.000298412
2017-10-10T12:45:38.997866: step 786, loss 0.125563, acc 0.953125, learning_rate 0.000297602
2017-10-10T12:45:39.448984: step 787, loss 0.109508, acc 0.953125, learning_rate 0.000296795
2017-10-10T12:45:39.876911: step 788, loss 0.251746, acc 0.9375, learning_rate 0.000295992
2017-10-10T12:45:40.392897: step 789, loss 0.18767, acc 0.953125, learning_rate 0.000295192
2017-10-10T12:45:40.992960: step 790, loss 0.277964, acc 0.890625, learning_rate 0.000294395
2017-10-10T12:45:41.508902: step 791, loss 0.162179, acc 0.9375, learning_rate 0.000293602
2017-10-10T12:45:42.020323: step 792, loss 0.172523, acc 0.921875, learning_rate 0.000292812
2017-10-10T12:45:42.596323: step 793, loss 0.307234, acc 0.875, learning_rate 0.000292025
2017-10-10T12:45:43.098958: step 794, loss 0.244137, acc 0.921875, learning_rate 0.000291241
2017-10-10T12:45:43.633275: step 795, loss 0.178928, acc 0.9375, learning_rate 0.00029046
2017-10-10T12:45:44.200897: step 796, loss 0.166381, acc 0.953125, learning_rate 0.000289683
2017-10-10T12:45:44.752844: step 797, loss 0.153473, acc 0.9375, learning_rate 0.000288908
2017-10-10T12:45:45.110769: step 798, loss 0.180463, acc 0.953125, learning_rate 0.000288137
2017-10-10T12:45:45.609036: step 799, loss 0.163231, acc 0.9375, learning_rate 0.000287369
2017-10-10T12:45:46.162556: step 800, loss 0.263967, acc 0.90625, learning_rate 0.000286605

Evaluation:
2017-10-10T12:45:47.304895: step 800, loss 0.24622, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-800

2017-10-10T12:45:49.110699: step 801, loss 0.258332, acc 0.890625, learning_rate 0.000285843
2017-10-10T12:45:50.076841: step 802, loss 0.151068, acc 0.9375, learning_rate 0.000285084
2017-10-10T12:45:50.448595: step 803, loss 0.229312, acc 0.90625, learning_rate 0.000284329
2017-10-10T12:45:50.852960: step 804, loss 0.243598, acc 0.9375, learning_rate 0.000283577
2017-10-10T12:45:51.304894: step 805, loss 0.336314, acc 0.921875, learning_rate 0.000282827
2017-10-10T12:45:51.760932: step 806, loss 0.168331, acc 0.921875, learning_rate 0.000282081
2017-10-10T12:45:52.236058: step 807, loss 0.172828, acc 0.953125, learning_rate 0.000281338
2017-10-10T12:45:52.760845: step 808, loss 0.196404, acc 0.9375, learning_rate 0.000280598
2017-10-10T12:45:53.292787: step 809, loss 0.163314, acc 0.984375, learning_rate 0.00027986
2017-10-10T12:45:53.773969: step 810, loss 0.206815, acc 0.90625, learning_rate 0.000279126
2017-10-10T12:45:54.277162: step 811, loss 0.118337, acc 0.953125, learning_rate 0.000278395
2017-10-10T12:45:54.826671: step 812, loss 0.206068, acc 0.9375, learning_rate 0.000277667
2017-10-10T12:45:55.344833: step 813, loss 0.350732, acc 0.90625, learning_rate 0.000276942
2017-10-10T12:45:55.925004: step 814, loss 0.120424, acc 0.96875, learning_rate 0.00027622
2017-10-10T12:45:56.465116: step 815, loss 0.186287, acc 0.9375, learning_rate 0.0002755
2017-10-10T12:45:57.003431: step 816, loss 0.272859, acc 0.921875, learning_rate 0.000274784
2017-10-10T12:45:57.555624: step 817, loss 0.129797, acc 0.984375, learning_rate 0.000274071
2017-10-10T12:45:58.072424: step 818, loss 0.19082, acc 0.9375, learning_rate 0.00027336
2017-10-10T12:45:58.524806: step 819, loss 0.346136, acc 0.890625, learning_rate 0.000272652
2017-10-10T12:45:58.999171: step 820, loss 0.209878, acc 0.890625, learning_rate 0.000271948
2017-10-10T12:45:59.527687: step 821, loss 0.274195, acc 0.890625, learning_rate 0.000271246
2017-10-10T12:46:00.052959: step 822, loss 0.239159, acc 0.921875, learning_rate 0.000270547
2017-10-10T12:46:00.613017: step 823, loss 0.262268, acc 0.921875, learning_rate 0.000269851
2017-10-10T12:46:01.110177: step 824, loss 0.224722, acc 0.953125, learning_rate 0.000269157
2017-10-10T12:46:01.629078: step 825, loss 0.296464, acc 0.90625, learning_rate 0.000268467
2017-10-10T12:46:02.164898: step 826, loss 0.413862, acc 0.875, learning_rate 0.000267779
2017-10-10T12:46:02.693442: step 827, loss 0.323196, acc 0.890625, learning_rate 0.000267094
2017-10-10T12:46:03.163050: step 828, loss 0.138039, acc 0.921875, learning_rate 0.000266412
2017-10-10T12:46:03.666282: step 829, loss 0.0844301, acc 0.96875, learning_rate 0.000265733
2017-10-10T12:46:04.236500: step 830, loss 0.0976867, acc 0.984375, learning_rate 0.000265057
2017-10-10T12:46:04.671653: step 831, loss 0.393811, acc 0.890625, learning_rate 0.000264383
2017-10-10T12:46:05.180942: step 832, loss 0.155551, acc 0.9375, learning_rate 0.000263712
2017-10-10T12:46:05.740135: step 833, loss 0.0860239, acc 0.96875, learning_rate 0.000263044
2017-10-10T12:46:06.246463: step 834, loss 0.0937034, acc 0.984375, learning_rate 0.000262378
2017-10-10T12:46:06.877142: step 835, loss 0.143001, acc 0.9375, learning_rate 0.000261715
2017-10-10T12:46:07.328826: step 836, loss 0.160694, acc 0.9375, learning_rate 0.000261055
2017-10-10T12:46:07.838253: step 837, loss 0.191785, acc 0.9375, learning_rate 0.000260398
2017-10-10T12:46:08.302419: step 838, loss 0.151664, acc 0.96875, learning_rate 0.000259743
2017-10-10T12:46:08.760815: step 839, loss 0.168827, acc 0.921875, learning_rate 0.000259091
2017-10-10T12:46:09.262478: step 840, loss 0.264821, acc 0.921875, learning_rate 0.000258442

Evaluation:
2017-10-10T12:46:10.476829: step 840, loss 0.242229, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-840

2017-10-10T12:46:12.244982: step 841, loss 0.212018, acc 0.921875, learning_rate 0.000257795
2017-10-10T12:46:12.728919: step 842, loss 0.246386, acc 0.921875, learning_rate 0.000257151
2017-10-10T12:46:13.321106: step 843, loss 0.246381, acc 0.890625, learning_rate 0.00025651
2017-10-10T12:46:13.900932: step 844, loss 0.14846, acc 0.9375, learning_rate 0.000255871
2017-10-10T12:46:14.371920: step 845, loss 0.111545, acc 0.953125, learning_rate 0.000255235
2017-10-10T12:46:14.816814: step 846, loss 0.253195, acc 0.9375, learning_rate 0.000254601
2017-10-10T12:46:15.364973: step 847, loss 0.159805, acc 0.9375, learning_rate 0.00025397
2017-10-10T12:46:15.858903: step 848, loss 0.223007, acc 0.921875, learning_rate 0.000253341
2017-10-10T12:46:16.420830: step 849, loss 0.319112, acc 0.921875, learning_rate 0.000252716
2017-10-10T12:46:16.984843: step 850, loss 0.134985, acc 0.921875, learning_rate 0.000252092
2017-10-10T12:46:17.517107: step 851, loss 0.134084, acc 0.96875, learning_rate 0.000251471
2017-10-10T12:46:18.035993: step 852, loss 0.173402, acc 0.953125, learning_rate 0.000250853
2017-10-10T12:46:18.579461: step 853, loss 0.185359, acc 0.96875, learning_rate 0.000250237
2017-10-10T12:46:19.142602: step 854, loss 0.159666, acc 0.9375, learning_rate 0.000249624
2017-10-10T12:46:19.744918: step 855, loss 0.164163, acc 0.921875, learning_rate 0.000249013
2017-10-10T12:46:20.384085: step 856, loss 0.130149, acc 0.9375, learning_rate 0.000248405
2017-10-10T12:46:20.775022: step 857, loss 0.333908, acc 0.890625, learning_rate 0.000247799
2017-10-10T12:46:21.054363: step 858, loss 0.178349, acc 0.9375, learning_rate 0.000247196
2017-10-10T12:46:21.609815: step 859, loss 0.17806, acc 0.921875, learning_rate 0.000246595
2017-10-10T12:46:22.094484: step 860, loss 0.163609, acc 0.953125, learning_rate 0.000245997
2017-10-10T12:46:22.588901: step 861, loss 0.124753, acc 0.953125, learning_rate 0.000245401
2017-10-10T12:46:23.141023: step 862, loss 0.336208, acc 0.875, learning_rate 0.000244808
2017-10-10T12:46:23.669001: step 863, loss 0.122579, acc 0.96875, learning_rate 0.000244216
2017-10-10T12:46:24.196347: step 864, loss 0.293034, acc 0.875, learning_rate 0.000243628
2017-10-10T12:46:24.672882: step 865, loss 0.223738, acc 0.90625, learning_rate 0.000243042
2017-10-10T12:46:25.137026: step 866, loss 0.274309, acc 0.9375, learning_rate 0.000242458
2017-10-10T12:46:25.661203: step 867, loss 0.131791, acc 0.953125, learning_rate 0.000241876
2017-10-10T12:46:26.195307: step 868, loss 0.0754557, acc 1, learning_rate 0.000241297
2017-10-10T12:46:26.782077: step 869, loss 0.0921433, acc 0.984375, learning_rate 0.00024072
2017-10-10T12:46:27.336901: step 870, loss 0.373675, acc 0.84375, learning_rate 0.000240146
2017-10-10T12:46:27.845014: step 871, loss 0.262528, acc 0.890625, learning_rate 0.000239574
2017-10-10T12:46:28.373395: step 872, loss 0.0905726, acc 0.953125, learning_rate 0.000239004
2017-10-10T12:46:28.860737: step 873, loss 0.279089, acc 0.9375, learning_rate 0.000238437
2017-10-10T12:46:29.355858: step 874, loss 0.190207, acc 0.9375, learning_rate 0.000237872
2017-10-10T12:46:29.948970: step 875, loss 0.192524, acc 0.90625, learning_rate 0.000237309
2017-10-10T12:46:30.573026: step 876, loss 0.138595, acc 0.921875, learning_rate 0.000236749
2017-10-10T12:46:30.979374: step 877, loss 0.245255, acc 0.890625, learning_rate 0.00023619
2017-10-10T12:46:31.432970: step 878, loss 0.149655, acc 0.921875, learning_rate 0.000235635
2017-10-10T12:46:31.835945: step 879, loss 0.276913, acc 0.890625, learning_rate 0.000235081
2017-10-10T12:46:32.320875: step 880, loss 0.24309, acc 0.890625, learning_rate 0.00023453

Evaluation:
2017-10-10T12:46:33.528439: step 880, loss 0.239671, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-880

2017-10-10T12:46:35.493179: step 881, loss 0.223294, acc 0.921875, learning_rate 0.00023398
2017-10-10T12:46:36.044340: step 882, loss 0.110085, acc 0.941176, learning_rate 0.000233434
2017-10-10T12:46:36.521285: step 883, loss 0.108722, acc 0.984375, learning_rate 0.000232889
2017-10-10T12:46:37.179097: step 884, loss 0.385986, acc 0.859375, learning_rate 0.000232346
2017-10-10T12:46:37.708199: step 885, loss 0.192373, acc 0.9375, learning_rate 0.000231806
2017-10-10T12:46:38.146044: step 886, loss 0.118787, acc 0.96875, learning_rate 0.000231268
2017-10-10T12:46:38.606213: step 887, loss 0.136281, acc 0.9375, learning_rate 0.000230732
2017-10-10T12:46:39.160602: step 888, loss 0.189455, acc 0.9375, learning_rate 0.000230199
2017-10-10T12:46:39.690792: step 889, loss 0.350865, acc 0.90625, learning_rate 0.000229667
2017-10-10T12:46:40.187315: step 890, loss 0.152324, acc 0.953125, learning_rate 0.000229138
2017-10-10T12:46:40.720419: step 891, loss 0.248098, acc 0.953125, learning_rate 0.000228611
2017-10-10T12:46:41.268893: step 892, loss 0.374472, acc 0.84375, learning_rate 0.000228086
2017-10-10T12:46:41.827480: step 893, loss 0.107351, acc 0.96875, learning_rate 0.000227563
2017-10-10T12:46:42.377517: step 894, loss 0.309154, acc 0.921875, learning_rate 0.000227043
2017-10-10T12:46:42.924970: step 895, loss 0.199189, acc 0.921875, learning_rate 0.000226524
2017-10-10T12:46:43.458946: step 896, loss 0.107853, acc 0.96875, learning_rate 0.000226008
2017-10-10T12:46:44.012833: step 897, loss 0.213308, acc 0.9375, learning_rate 0.000225493
2017-10-10T12:46:44.511513: step 898, loss 0.11626, acc 0.953125, learning_rate 0.000224981
2017-10-10T12:46:44.982266: step 899, loss 0.133633, acc 0.96875, learning_rate 0.000224471
2017-10-10T12:46:45.408568: step 900, loss 0.270801, acc 0.890625, learning_rate 0.000223963
2017-10-10T12:46:45.872884: step 901, loss 0.193944, acc 0.9375, learning_rate 0.000223457
2017-10-10T12:46:46.401393: step 902, loss 0.111635, acc 0.96875, learning_rate 0.000222953
2017-10-10T12:46:46.955029: step 903, loss 0.185173, acc 0.9375, learning_rate 0.000222451
2017-10-10T12:46:47.456988: step 904, loss 0.228375, acc 0.90625, learning_rate 0.000221951
2017-10-10T12:46:48.009342: step 905, loss 0.227377, acc 0.890625, learning_rate 0.000221453
2017-10-10T12:46:48.580195: step 906, loss 0.192844, acc 0.921875, learning_rate 0.000220958
2017-10-10T12:46:49.159402: step 907, loss 0.234683, acc 0.9375, learning_rate 0.000220464
2017-10-10T12:46:49.667124: step 908, loss 0.265661, acc 0.890625, learning_rate 0.000219972
2017-10-10T12:46:50.112990: step 909, loss 0.152415, acc 0.921875, learning_rate 0.000219483
2017-10-10T12:46:50.691242: step 910, loss 0.260809, acc 0.921875, learning_rate 0.000218995
2017-10-10T12:46:51.162662: step 911, loss 0.32487, acc 0.890625, learning_rate 0.000218509
2017-10-10T12:46:51.653630: step 912, loss 0.221485, acc 0.890625, learning_rate 0.000218025
2017-10-10T12:46:52.178037: step 913, loss 0.272736, acc 0.921875, learning_rate 0.000217544
2017-10-10T12:46:52.672907: step 914, loss 0.222327, acc 0.90625, learning_rate 0.000217064
2017-10-10T12:46:53.252865: step 915, loss 0.220038, acc 0.921875, learning_rate 0.000216586
2017-10-10T12:46:53.821085: step 916, loss 0.192323, acc 0.9375, learning_rate 0.00021611
2017-10-10T12:46:54.248916: step 917, loss 0.252585, acc 0.90625, learning_rate 0.000215636
2017-10-10T12:46:54.712936: step 918, loss 0.109547, acc 0.9375, learning_rate 0.000215164
2017-10-10T12:46:55.265288: step 919, loss 0.12396, acc 0.96875, learning_rate 0.000214694
2017-10-10T12:46:55.752604: step 920, loss 0.324383, acc 0.875, learning_rate 0.000214226

Evaluation:
2017-10-10T12:46:57.048995: step 920, loss 0.239762, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-920

2017-10-10T12:46:58.627345: step 921, loss 0.123758, acc 0.953125, learning_rate 0.00021376
2017-10-10T12:46:59.148872: step 922, loss 0.362366, acc 0.921875, learning_rate 0.000213295
2017-10-10T12:46:59.672853: step 923, loss 0.189945, acc 0.90625, learning_rate 0.000212833
2017-10-10T12:47:00.208705: step 924, loss 0.156824, acc 0.9375, learning_rate 0.000212372
2017-10-10T12:47:00.823969: step 925, loss 0.190182, acc 0.921875, learning_rate 0.000211914
2017-10-10T12:47:01.303333: step 926, loss 0.0760736, acc 0.984375, learning_rate 0.000211457
2017-10-10T12:47:01.615749: step 927, loss 0.0716615, acc 0.984375, learning_rate 0.000211002
2017-10-10T12:47:02.104944: step 928, loss 0.124444, acc 0.96875, learning_rate 0.000210549
2017-10-10T12:47:02.645128: step 929, loss 0.169064, acc 0.9375, learning_rate 0.000210098
2017-10-10T12:47:03.145460: step 930, loss 0.206841, acc 0.953125, learning_rate 0.000209648
2017-10-10T12:47:03.732978: step 931, loss 0.284299, acc 0.90625, learning_rate 0.000209201
2017-10-10T12:47:04.249043: step 932, loss 0.154534, acc 0.96875, learning_rate 0.000208755
2017-10-10T12:47:04.757126: step 933, loss 0.154499, acc 0.9375, learning_rate 0.000208311
2017-10-10T12:47:05.312259: step 934, loss 0.186185, acc 0.921875, learning_rate 0.000207869
2017-10-10T12:47:05.760552: step 935, loss 0.219077, acc 0.890625, learning_rate 0.000207429
2017-10-10T12:47:06.410171: step 936, loss 0.28158, acc 0.890625, learning_rate 0.00020699
2017-10-10T12:47:06.916203: step 937, loss 0.0502469, acc 0.984375, learning_rate 0.000206554
2017-10-10T12:47:07.409619: step 938, loss 0.148352, acc 0.9375, learning_rate 0.000206119
2017-10-10T12:47:07.963580: step 939, loss 0.247819, acc 0.859375, learning_rate 0.000205685
2017-10-10T12:47:08.488968: step 940, loss 0.200437, acc 0.921875, learning_rate 0.000205254
2017-10-10T12:47:09.048598: step 941, loss 0.141359, acc 0.9375, learning_rate 0.000204824
2017-10-10T12:47:09.552440: step 942, loss 0.326111, acc 0.90625, learning_rate 0.000204397
2017-10-10T12:47:10.103855: step 943, loss 0.179112, acc 0.9375, learning_rate 0.00020397
2017-10-10T12:47:10.671757: step 944, loss 0.162459, acc 0.9375, learning_rate 0.000203546
2017-10-10T12:47:11.304998: step 945, loss 0.178429, acc 0.9375, learning_rate 0.000203123
2017-10-10T12:47:11.820867: step 946, loss 0.215683, acc 0.9375, learning_rate 0.000202702
2017-10-10T12:47:12.369876: step 947, loss 0.168376, acc 0.96875, learning_rate 0.000202283
2017-10-10T12:47:12.924185: step 948, loss 0.0999561, acc 0.953125, learning_rate 0.000201866
2017-10-10T12:47:13.454915: step 949, loss 0.110677, acc 0.96875, learning_rate 0.00020145
2017-10-10T12:47:13.972857: step 950, loss 0.162889, acc 0.921875, learning_rate 0.000201036
2017-10-10T12:47:14.333259: step 951, loss 0.217508, acc 0.9375, learning_rate 0.000200623
2017-10-10T12:47:14.841051: step 952, loss 0.181365, acc 0.9375, learning_rate 0.000200213
2017-10-10T12:47:15.361148: step 953, loss 0.223792, acc 0.90625, learning_rate 0.000199804
2017-10-10T12:47:15.832921: step 954, loss 0.422779, acc 0.859375, learning_rate 0.000199396
2017-10-10T12:47:16.465203: step 955, loss 0.253563, acc 0.875, learning_rate 0.000198991
2017-10-10T12:47:16.936869: step 956, loss 0.216349, acc 0.9375, learning_rate 0.000198587
2017-10-10T12:47:17.395262: step 957, loss 0.152286, acc 0.953125, learning_rate 0.000198184
2017-10-10T12:47:17.884262: step 958, loss 0.23205, acc 0.9375, learning_rate 0.000197783
2017-10-10T12:47:18.387702: step 959, loss 0.140512, acc 0.9375, learning_rate 0.000197384
2017-10-10T12:47:18.913031: step 960, loss 0.324487, acc 0.921875, learning_rate 0.000196987

Evaluation:
2017-10-10T12:47:20.022869: step 960, loss 0.24075, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-960

2017-10-10T12:47:21.505099: step 961, loss 0.193279, acc 0.96875, learning_rate 0.000196591
2017-10-10T12:47:22.092702: step 962, loss 0.180757, acc 0.96875, learning_rate 0.000196197
2017-10-10T12:47:22.533617: step 963, loss 0.0894285, acc 0.984375, learning_rate 0.000195804
2017-10-10T12:47:23.067523: step 964, loss 0.160725, acc 0.9375, learning_rate 0.000195413
2017-10-10T12:47:23.609158: step 965, loss 0.201667, acc 0.90625, learning_rate 0.000195023
2017-10-10T12:47:24.169963: step 966, loss 0.14195, acc 0.9375, learning_rate 0.000194636
2017-10-10T12:47:24.577798: step 967, loss 0.190096, acc 0.921875, learning_rate 0.000194249
2017-10-10T12:47:25.056427: step 968, loss 0.239248, acc 0.921875, learning_rate 0.000193865
2017-10-10T12:47:25.544930: step 969, loss 0.241191, acc 0.90625, learning_rate 0.000193482
2017-10-10T12:47:26.105364: step 970, loss 0.190085, acc 0.9375, learning_rate 0.0001931
2017-10-10T12:47:26.588950: step 971, loss 0.13393, acc 0.9375, learning_rate 0.00019272
2017-10-10T12:47:27.105584: step 972, loss 0.272135, acc 0.921875, learning_rate 0.000192341
2017-10-10T12:47:27.584860: step 973, loss 0.113837, acc 0.96875, learning_rate 0.000191965
2017-10-10T12:47:28.056893: step 974, loss 0.175912, acc 0.96875, learning_rate 0.000191589
2017-10-10T12:47:28.558000: step 975, loss 0.175396, acc 0.953125, learning_rate 0.000191215
2017-10-10T12:47:29.087129: step 976, loss 0.104983, acc 0.96875, learning_rate 0.000190843
2017-10-10T12:47:29.572893: step 977, loss 0.240507, acc 0.921875, learning_rate 0.000190472
2017-10-10T12:47:29.983517: step 978, loss 0.243912, acc 0.9375, learning_rate 0.000190103
2017-10-10T12:47:30.427270: step 979, loss 0.138937, acc 0.96875, learning_rate 0.000189735
2017-10-10T12:47:30.957136: step 980, loss 0.37289, acc 0.843137, learning_rate 0.000189369
2017-10-10T12:47:31.503243: step 981, loss 0.242014, acc 0.921875, learning_rate 0.000189004
2017-10-10T12:47:32.032736: step 982, loss 0.295673, acc 0.875, learning_rate 0.000188641
2017-10-10T12:47:32.544914: step 983, loss 0.160014, acc 0.953125, learning_rate 0.000188279
2017-10-10T12:47:33.102794: step 984, loss 0.0845161, acc 1, learning_rate 0.000187919
2017-10-10T12:47:33.620859: step 985, loss 0.121656, acc 0.953125, learning_rate 0.00018756
2017-10-10T12:47:34.148300: step 986, loss 0.187403, acc 0.96875, learning_rate 0.000187202
2017-10-10T12:47:34.727655: step 987, loss 0.155159, acc 0.953125, learning_rate 0.000186846
2017-10-10T12:47:35.191441: step 988, loss 0.136182, acc 0.953125, learning_rate 0.000186492
2017-10-10T12:47:35.714075: step 989, loss 0.151345, acc 0.921875, learning_rate 0.000186139
2017-10-10T12:47:36.209237: step 990, loss 0.315836, acc 0.875, learning_rate 0.000185787
2017-10-10T12:47:36.663633: step 991, loss 0.276416, acc 0.90625, learning_rate 0.000185437
2017-10-10T12:47:37.175923: step 992, loss 0.247935, acc 0.890625, learning_rate 0.000185088
2017-10-10T12:47:37.644231: step 993, loss 0.114946, acc 0.984375, learning_rate 0.000184741
2017-10-10T12:47:38.187152: step 994, loss 0.153188, acc 0.953125, learning_rate 0.000184395
2017-10-10T12:47:38.720911: step 995, loss 0.271418, acc 0.90625, learning_rate 0.000184051
2017-10-10T12:47:39.280854: step 996, loss 0.317818, acc 0.875, learning_rate 0.000183708
2017-10-10T12:47:39.843999: step 997, loss 0.21762, acc 0.921875, learning_rate 0.000183366
2017-10-10T12:47:40.224525: step 998, loss 0.284849, acc 0.84375, learning_rate 0.000183026
2017-10-10T12:47:40.680849: step 999, loss 0.281672, acc 0.890625, learning_rate 0.000182687
2017-10-10T12:47:41.144963: step 1000, loss 0.153781, acc 0.953125, learning_rate 0.000182349

Evaluation:
2017-10-10T12:47:42.256952: step 1000, loss 0.239937, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1000

2017-10-10T12:47:44.078021: step 1001, loss 0.113997, acc 0.984375, learning_rate 0.000182013
2017-10-10T12:47:44.499658: step 1002, loss 0.124716, acc 0.96875, learning_rate 0.000181678
2017-10-10T12:47:45.023148: step 1003, loss 0.172705, acc 0.921875, learning_rate 0.000181345
2017-10-10T12:47:45.516858: step 1004, loss 0.261555, acc 0.921875, learning_rate 0.000181013
2017-10-10T12:47:46.040839: step 1005, loss 0.0840445, acc 0.984375, learning_rate 0.000180682
2017-10-10T12:47:46.645006: step 1006, loss 0.330741, acc 0.859375, learning_rate 0.000180353
2017-10-10T12:47:47.216338: step 1007, loss 0.188296, acc 0.921875, learning_rate 0.000180025
2017-10-10T12:47:47.676820: step 1008, loss 0.185827, acc 0.90625, learning_rate 0.000179698
2017-10-10T12:47:48.120608: step 1009, loss 0.121303, acc 0.953125, learning_rate 0.000179373
2017-10-10T12:47:48.581640: step 1010, loss 0.239233, acc 0.9375, learning_rate 0.000179049
2017-10-10T12:47:49.103032: step 1011, loss 0.116584, acc 0.96875, learning_rate 0.000178726
2017-10-10T12:47:49.645025: step 1012, loss 0.0961351, acc 0.96875, learning_rate 0.000178405
2017-10-10T12:47:50.184954: step 1013, loss 0.256936, acc 0.90625, learning_rate 0.000178085
2017-10-10T12:47:50.692892: step 1014, loss 0.188358, acc 0.9375, learning_rate 0.000177766
2017-10-10T12:47:51.153058: step 1015, loss 0.235652, acc 0.9375, learning_rate 0.000177449
2017-10-10T12:47:51.744965: step 1016, loss 0.115297, acc 0.96875, learning_rate 0.000177133
2017-10-10T12:47:52.316871: step 1017, loss 0.128862, acc 0.921875, learning_rate 0.000176818
2017-10-10T12:47:52.882474: step 1018, loss 0.244464, acc 0.890625, learning_rate 0.000176504
2017-10-10T12:47:53.280871: step 1019, loss 0.152888, acc 0.953125, learning_rate 0.000176192
2017-10-10T12:47:53.732833: step 1020, loss 0.112779, acc 0.96875, learning_rate 0.000175881
2017-10-10T12:47:54.228902: step 1021, loss 0.142855, acc 0.984375, learning_rate 0.000175571
2017-10-10T12:47:54.796533: step 1022, loss 0.32769, acc 0.9375, learning_rate 0.000175263
2017-10-10T12:47:55.324719: step 1023, loss 0.140371, acc 0.953125, learning_rate 0.000174956
2017-10-10T12:47:55.835881: step 1024, loss 0.135532, acc 0.953125, learning_rate 0.00017465
2017-10-10T12:47:56.357044: step 1025, loss 0.265642, acc 0.890625, learning_rate 0.000174345
2017-10-10T12:47:56.892852: step 1026, loss 0.168969, acc 0.921875, learning_rate 0.000174042
2017-10-10T12:47:57.361084: step 1027, loss 0.248692, acc 0.90625, learning_rate 0.000173739
2017-10-10T12:47:57.847625: step 1028, loss 0.136086, acc 0.953125, learning_rate 0.000173438
2017-10-10T12:47:58.350592: step 1029, loss 0.0847191, acc 0.953125, learning_rate 0.000173139
2017-10-10T12:47:58.892560: step 1030, loss 0.200221, acc 0.921875, learning_rate 0.00017284
2017-10-10T12:47:59.469187: step 1031, loss 0.173239, acc 0.9375, learning_rate 0.000172543
2017-10-10T12:48:00.024884: step 1032, loss 0.26834, acc 0.921875, learning_rate 0.000172247
2017-10-10T12:48:00.598701: step 1033, loss 0.228069, acc 0.90625, learning_rate 0.000171952
2017-10-10T12:48:01.155547: step 1034, loss 0.177129, acc 0.921875, learning_rate 0.000171658
2017-10-10T12:48:01.673127: step 1035, loss 0.270799, acc 0.84375, learning_rate 0.000171366
2017-10-10T12:48:02.188873: step 1036, loss 0.251526, acc 0.921875, learning_rate 0.000171074
2017-10-10T12:48:02.831844: step 1037, loss 0.121103, acc 0.953125, learning_rate 0.000170784
2017-10-10T12:48:03.286776: step 1038, loss 0.218526, acc 0.90625, learning_rate 0.000170495
2017-10-10T12:48:03.717389: step 1039, loss 0.277342, acc 0.890625, learning_rate 0.000170208
2017-10-10T12:48:04.272700: step 1040, loss 0.163381, acc 0.9375, learning_rate 0.000169921

Evaluation:
2017-10-10T12:48:05.320941: step 1040, loss 0.238489, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1040

2017-10-10T12:48:06.743417: step 1041, loss 0.167248, acc 0.9375, learning_rate 0.000169636
2017-10-10T12:48:07.199784: step 1042, loss 0.116821, acc 0.96875, learning_rate 0.000169351
2017-10-10T12:48:07.684904: step 1043, loss 0.145621, acc 0.921875, learning_rate 0.000169068
2017-10-10T12:48:08.224005: step 1044, loss 0.223694, acc 0.90625, learning_rate 0.000168786
2017-10-10T12:48:08.689145: step 1045, loss 0.351807, acc 0.875, learning_rate 0.000168506
2017-10-10T12:48:09.143675: step 1046, loss 0.329234, acc 0.875, learning_rate 0.000168226
2017-10-10T12:48:09.739407: step 1047, loss 0.235128, acc 0.9375, learning_rate 0.000167947
2017-10-10T12:48:10.265746: step 1048, loss 0.206908, acc 0.90625, learning_rate 0.00016767
2017-10-10T12:48:10.732742: step 1049, loss 0.295531, acc 0.890625, learning_rate 0.000167394
2017-10-10T12:48:11.205007: step 1050, loss 0.145587, acc 0.9375, learning_rate 0.000167119
2017-10-10T12:48:11.721825: step 1051, loss 0.152016, acc 0.921875, learning_rate 0.000166845
2017-10-10T12:48:12.286503: step 1052, loss 0.194381, acc 0.953125, learning_rate 0.000166572
2017-10-10T12:48:12.820839: step 1053, loss 0.155235, acc 0.9375, learning_rate 0.0001663
2017-10-10T12:48:13.288843: step 1054, loss 0.126775, acc 0.96875, learning_rate 0.00016603
2017-10-10T12:48:13.705982: step 1055, loss 0.182556, acc 0.9375, learning_rate 0.00016576
2017-10-10T12:48:14.257017: step 1056, loss 0.101585, acc 0.96875, learning_rate 0.000165492
2017-10-10T12:48:14.733352: step 1057, loss 0.205509, acc 0.90625, learning_rate 0.000165224
2017-10-10T12:48:15.264976: step 1058, loss 0.320129, acc 0.875, learning_rate 0.000164958
2017-10-10T12:48:15.836968: step 1059, loss 0.260832, acc 0.921875, learning_rate 0.000164693
2017-10-10T12:48:16.492880: step 1060, loss 0.264788, acc 0.90625, learning_rate 0.000164429
2017-10-10T12:48:16.960823: step 1061, loss 0.225874, acc 0.921875, learning_rate 0.000164166
2017-10-10T12:48:17.421136: step 1062, loss 0.193594, acc 0.890625, learning_rate 0.000163904
2017-10-10T12:48:18.069219: step 1063, loss 0.230436, acc 0.90625, learning_rate 0.000163643
2017-10-10T12:48:18.585632: step 1064, loss 0.0745216, acc 0.984375, learning_rate 0.000163383
2017-10-10T12:48:19.122856: step 1065, loss 0.081952, acc 0.96875, learning_rate 0.000163125
2017-10-10T12:48:19.652921: step 1066, loss 0.219952, acc 0.90625, learning_rate 0.000162867
2017-10-10T12:48:20.185465: step 1067, loss 0.138399, acc 0.953125, learning_rate 0.00016261
2017-10-10T12:48:20.700855: step 1068, loss 0.172125, acc 0.9375, learning_rate 0.000162355
2017-10-10T12:48:21.208878: step 1069, loss 0.147186, acc 0.921875, learning_rate 0.0001621
2017-10-10T12:48:21.739329: step 1070, loss 0.217081, acc 0.921875, learning_rate 0.000161847
2017-10-10T12:48:22.267847: step 1071, loss 0.192893, acc 0.9375, learning_rate 0.000161594
2017-10-10T12:48:22.800364: step 1072, loss 0.119065, acc 0.953125, learning_rate 0.000161343
2017-10-10T12:48:23.355177: step 1073, loss 0.232757, acc 0.9375, learning_rate 0.000161093
2017-10-10T12:48:23.924262: step 1074, loss 0.27162, acc 0.90625, learning_rate 0.000160843
2017-10-10T12:48:24.496846: step 1075, loss 0.247772, acc 0.875, learning_rate 0.000160595
2017-10-10T12:48:25.016879: step 1076, loss 0.191048, acc 0.921875, learning_rate 0.000160348
2017-10-10T12:48:25.610419: step 1077, loss 0.247071, acc 0.9375, learning_rate 0.000160101
2017-10-10T12:48:26.101121: step 1078, loss 0.170659, acc 0.960784, learning_rate 0.000159856
2017-10-10T12:48:26.478766: step 1079, loss 0.355733, acc 0.90625, learning_rate 0.000159612
2017-10-10T12:48:26.907503: step 1080, loss 0.277592, acc 0.9375, learning_rate 0.000159368

Evaluation:
2017-10-10T12:48:27.892912: step 1080, loss 0.235478, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1080

2017-10-10T12:48:29.473050: step 1081, loss 0.224866, acc 0.921875, learning_rate 0.000159126
2017-10-10T12:48:29.973518: step 1082, loss 0.135685, acc 0.953125, learning_rate 0.000158885
2017-10-10T12:48:30.485769: step 1083, loss 0.121633, acc 0.96875, learning_rate 0.000158644
2017-10-10T12:48:30.987107: step 1084, loss 0.08673, acc 0.984375, learning_rate 0.000158405
2017-10-10T12:48:31.522637: step 1085, loss 0.212089, acc 0.9375, learning_rate 0.000158167
2017-10-10T12:48:32.073069: step 1086, loss 0.178247, acc 0.9375, learning_rate 0.000157929
2017-10-10T12:48:32.492956: step 1087, loss 0.215203, acc 0.890625, learning_rate 0.000157693
2017-10-10T12:48:33.061001: step 1088, loss 0.152848, acc 0.921875, learning_rate 0.000157457
2017-10-10T12:48:33.636445: step 1089, loss 0.215283, acc 0.90625, learning_rate 0.000157223
2017-10-10T12:48:34.143960: step 1090, loss 0.26558, acc 0.90625, learning_rate 0.000156989
2017-10-10T12:48:34.592937: step 1091, loss 0.202113, acc 0.96875, learning_rate 0.000156757
2017-10-10T12:48:35.144868: step 1092, loss 0.18279, acc 0.9375, learning_rate 0.000156525
2017-10-10T12:48:35.668924: step 1093, loss 0.139444, acc 0.953125, learning_rate 0.000156294
2017-10-10T12:48:36.167149: step 1094, loss 0.246364, acc 0.90625, learning_rate 0.000156064
2017-10-10T12:48:36.684857: step 1095, loss 0.233222, acc 0.890625, learning_rate 0.000155836
2017-10-10T12:48:37.161033: step 1096, loss 0.434589, acc 0.84375, learning_rate 0.000155608
2017-10-10T12:48:37.732891: step 1097, loss 0.0916328, acc 0.96875, learning_rate 0.000155381
2017-10-10T12:48:38.268909: step 1098, loss 0.234655, acc 0.953125, learning_rate 0.000155155
2017-10-10T12:48:38.820883: step 1099, loss 0.281204, acc 0.921875, learning_rate 0.000154929
2017-10-10T12:48:39.336693: step 1100, loss 0.431679, acc 0.84375, learning_rate 0.000154705
2017-10-10T12:48:39.823309: step 1101, loss 0.261634, acc 0.90625, learning_rate 0.000154482
2017-10-10T12:48:40.307122: step 1102, loss 0.210104, acc 0.921875, learning_rate 0.00015426
2017-10-10T12:48:40.815849: step 1103, loss 0.422609, acc 0.828125, learning_rate 0.000154038
2017-10-10T12:48:41.372841: step 1104, loss 0.202756, acc 0.9375, learning_rate 0.000153818
2017-10-10T12:48:41.936988: step 1105, loss 0.132673, acc 0.96875, learning_rate 0.000153598
2017-10-10T12:48:42.441980: step 1106, loss 0.315284, acc 0.859375, learning_rate 0.000153379
2017-10-10T12:48:42.987891: step 1107, loss 0.265332, acc 0.90625, learning_rate 0.000153161
2017-10-10T12:48:43.508705: step 1108, loss 0.199421, acc 0.90625, learning_rate 0.000152944
2017-10-10T12:48:44.050370: step 1109, loss 0.0830546, acc 0.96875, learning_rate 0.000152728
2017-10-10T12:48:44.591735: step 1110, loss 0.191009, acc 0.921875, learning_rate 0.000152513
2017-10-10T12:48:45.143465: step 1111, loss 0.099287, acc 0.984375, learning_rate 0.000152299
2017-10-10T12:48:45.660229: step 1112, loss 0.392491, acc 0.84375, learning_rate 0.000152085
2017-10-10T12:48:46.194342: step 1113, loss 0.166927, acc 0.953125, learning_rate 0.000151872
2017-10-10T12:48:46.716901: step 1114, loss 0.391922, acc 0.828125, learning_rate 0.000151661
2017-10-10T12:48:47.272894: step 1115, loss 0.136339, acc 0.9375, learning_rate 0.00015145
2017-10-10T12:48:47.838165: step 1116, loss 0.218914, acc 0.9375, learning_rate 0.00015124
2017-10-10T12:48:48.451395: step 1117, loss 0.191326, acc 0.953125, learning_rate 0.000151031
2017-10-10T12:48:48.944903: step 1118, loss 0.198208, acc 0.90625, learning_rate 0.000150822
2017-10-10T12:48:49.414631: step 1119, loss 0.270716, acc 0.90625, learning_rate 0.000150615
2017-10-10T12:48:49.866980: step 1120, loss 0.169061, acc 0.9375, learning_rate 0.000150408

Evaluation:
2017-10-10T12:48:51.023046: step 1120, loss 0.23989, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1120

2017-10-10T12:48:52.878960: step 1121, loss 0.109493, acc 1, learning_rate 0.000150203
2017-10-10T12:48:53.404838: step 1122, loss 0.0926545, acc 0.96875, learning_rate 0.000149998
2017-10-10T12:48:54.027246: step 1123, loss 0.111795, acc 0.953125, learning_rate 0.000149794
2017-10-10T12:48:54.580167: step 1124, loss 0.23226, acc 0.890625, learning_rate 0.00014959
2017-10-10T12:48:55.121954: step 1125, loss 0.27494, acc 0.875, learning_rate 0.000149388
2017-10-10T12:48:55.621223: step 1126, loss 0.173293, acc 0.9375, learning_rate 0.000149186
2017-10-10T12:48:56.163251: step 1127, loss 0.27314, acc 0.90625, learning_rate 0.000148986
2017-10-10T12:48:56.776908: step 1128, loss 0.163126, acc 0.9375, learning_rate 0.000148786
2017-10-10T12:48:57.188920: step 1129, loss 0.280022, acc 0.90625, learning_rate 0.000148587
2017-10-10T12:48:57.608971: step 1130, loss 0.13686, acc 0.953125, learning_rate 0.000148388
2017-10-10T12:48:58.073108: step 1131, loss 0.280151, acc 0.875, learning_rate 0.000148191
2017-10-10T12:48:58.585169: step 1132, loss 0.269827, acc 0.90625, learning_rate 0.000147994
2017-10-10T12:48:59.137239: step 1133, loss 0.24481, acc 0.921875, learning_rate 0.000147798
2017-10-10T12:48:59.593222: step 1134, loss 0.296881, acc 0.890625, learning_rate 0.000147603
2017-10-10T12:49:00.169158: step 1135, loss 0.187842, acc 0.9375, learning_rate 0.000147409
2017-10-10T12:49:00.748196: step 1136, loss 0.11761, acc 0.96875, learning_rate 0.000147215
2017-10-10T12:49:01.277862: step 1137, loss 0.152098, acc 0.96875, learning_rate 0.000147022
2017-10-10T12:49:01.836932: step 1138, loss 0.176163, acc 0.9375, learning_rate 0.000146831
2017-10-10T12:49:02.468864: step 1139, loss 0.148034, acc 0.921875, learning_rate 0.000146639
2017-10-10T12:49:02.826371: step 1140, loss 0.181285, acc 0.921875, learning_rate 0.000146449
2017-10-10T12:49:03.197026: step 1141, loss 0.260651, acc 0.921875, learning_rate 0.000146259
2017-10-10T12:49:03.693213: step 1142, loss 0.133433, acc 0.96875, learning_rate 0.000146071
2017-10-10T12:49:04.241202: step 1143, loss 0.0770822, acc 0.984375, learning_rate 0.000145883
2017-10-10T12:49:04.760720: step 1144, loss 0.139413, acc 0.9375, learning_rate 0.000145695
2017-10-10T12:49:05.291300: step 1145, loss 0.371255, acc 0.859375, learning_rate 0.000145509
2017-10-10T12:49:05.833029: step 1146, loss 0.19371, acc 0.953125, learning_rate 0.000145323
2017-10-10T12:49:06.368523: step 1147, loss 0.298315, acc 0.90625, learning_rate 0.000145138
2017-10-10T12:49:06.879039: step 1148, loss 0.103254, acc 0.96875, learning_rate 0.000144954
2017-10-10T12:49:07.449164: step 1149, loss 0.226807, acc 0.9375, learning_rate 0.00014477
2017-10-10T12:49:08.029111: step 1150, loss 0.225235, acc 0.921875, learning_rate 0.000144588
2017-10-10T12:49:08.612613: step 1151, loss 0.20449, acc 0.90625, learning_rate 0.000144406
2017-10-10T12:49:09.212974: step 1152, loss 0.30958, acc 0.890625, learning_rate 0.000144224
2017-10-10T12:49:09.729127: step 1153, loss 0.11886, acc 0.984375, learning_rate 0.000144044
2017-10-10T12:49:10.215684: step 1154, loss 0.106512, acc 0.96875, learning_rate 0.000143864
2017-10-10T12:49:10.792992: step 1155, loss 0.199693, acc 0.953125, learning_rate 0.000143685
2017-10-10T12:49:11.448975: step 1156, loss 0.254618, acc 0.90625, learning_rate 0.000143507
2017-10-10T12:49:11.908854: step 1157, loss 0.210621, acc 0.90625, learning_rate 0.000143329
2017-10-10T12:49:12.312874: step 1158, loss 0.0966669, acc 0.96875, learning_rate 0.000143152
2017-10-10T12:49:12.769076: step 1159, loss 0.167489, acc 0.953125, learning_rate 0.000142976
2017-10-10T12:49:13.211217: step 1160, loss 0.144523, acc 0.9375, learning_rate 0.000142801

Evaluation:
2017-10-10T12:49:14.345043: step 1160, loss 0.237646, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1160

2017-10-10T12:49:15.785131: step 1161, loss 0.128274, acc 0.953125, learning_rate 0.000142626
2017-10-10T12:49:16.303789: step 1162, loss 0.114129, acc 0.921875, learning_rate 0.000142452
2017-10-10T12:49:16.785293: step 1163, loss 0.195616, acc 0.9375, learning_rate 0.000142279
2017-10-10T12:49:17.286384: step 1164, loss 0.180915, acc 0.9375, learning_rate 0.000142106
2017-10-10T12:49:17.792931: step 1165, loss 0.154813, acc 0.96875, learning_rate 0.000141934
2017-10-10T12:49:18.360038: step 1166, loss 0.111833, acc 0.984375, learning_rate 0.000141763
2017-10-10T12:49:18.936875: step 1167, loss 0.126456, acc 0.953125, learning_rate 0.000141593
2017-10-10T12:49:19.535374: step 1168, loss 0.203503, acc 0.921875, learning_rate 0.000141423
2017-10-10T12:49:19.894924: step 1169, loss 0.123545, acc 0.953125, learning_rate 0.000141254
2017-10-10T12:49:20.304821: step 1170, loss 0.186319, acc 0.9375, learning_rate 0.000141085
2017-10-10T12:49:20.789036: step 1171, loss 0.115679, acc 0.953125, learning_rate 0.000140918
2017-10-10T12:49:21.392838: step 1172, loss 0.232387, acc 0.953125, learning_rate 0.000140751
2017-10-10T12:49:21.912841: step 1173, loss 0.0970251, acc 0.96875, learning_rate 0.000140584
2017-10-10T12:49:22.448884: step 1174, loss 0.25184, acc 0.875, learning_rate 0.000140419
2017-10-10T12:49:23.000862: step 1175, loss 0.108287, acc 0.953125, learning_rate 0.000140254
2017-10-10T12:49:23.476863: step 1176, loss 0.18427, acc 0.882353, learning_rate 0.000140089
2017-10-10T12:49:24.020859: step 1177, loss 0.225809, acc 0.90625, learning_rate 0.000139926
2017-10-10T12:49:24.365131: step 1178, loss 0.150224, acc 0.9375, learning_rate 0.000139763
2017-10-10T12:49:24.992968: step 1179, loss 0.118645, acc 0.96875, learning_rate 0.0001396
2017-10-10T12:49:25.473448: step 1180, loss 0.222921, acc 0.921875, learning_rate 0.000139439
2017-10-10T12:49:25.929008: step 1181, loss 0.192065, acc 0.921875, learning_rate 0.000139278
2017-10-10T12:49:26.296317: step 1182, loss 0.272852, acc 0.90625, learning_rate 0.000139118
2017-10-10T12:49:26.664962: step 1183, loss 0.143207, acc 0.953125, learning_rate 0.000138958
2017-10-10T12:49:27.321014: step 1184, loss 0.19087, acc 0.953125, learning_rate 0.000138799
2017-10-10T12:49:27.821211: step 1185, loss 0.373166, acc 0.890625, learning_rate 0.00013864
2017-10-10T12:49:28.328882: step 1186, loss 0.125636, acc 0.9375, learning_rate 0.000138483
2017-10-10T12:49:28.809128: step 1187, loss 0.32089, acc 0.921875, learning_rate 0.000138326
2017-10-10T12:49:29.340954: step 1188, loss 0.262401, acc 0.921875, learning_rate 0.000138169
2017-10-10T12:49:29.948867: step 1189, loss 0.133706, acc 0.96875, learning_rate 0.000138013
2017-10-10T12:49:30.520854: step 1190, loss 0.112918, acc 0.953125, learning_rate 0.000137858
2017-10-10T12:49:31.132769: step 1191, loss 0.158013, acc 0.953125, learning_rate 0.000137704
2017-10-10T12:49:31.688845: step 1192, loss 0.249407, acc 0.9375, learning_rate 0.00013755
2017-10-10T12:49:32.248752: step 1193, loss 0.249566, acc 0.921875, learning_rate 0.000137397
2017-10-10T12:49:32.796916: step 1194, loss 0.217955, acc 0.921875, learning_rate 0.000137244
2017-10-10T12:49:33.329192: step 1195, loss 0.172975, acc 0.953125, learning_rate 0.000137092
2017-10-10T12:49:33.985099: step 1196, loss 0.224797, acc 0.890625, learning_rate 0.000136941
2017-10-10T12:49:34.576851: step 1197, loss 0.222525, acc 0.90625, learning_rate 0.00013679
2017-10-10T12:49:35.007993: step 1198, loss 0.153925, acc 0.96875, learning_rate 0.00013664
2017-10-10T12:49:35.443716: step 1199, loss 0.286614, acc 0.890625, learning_rate 0.00013649
2017-10-10T12:49:35.940954: step 1200, loss 0.258053, acc 0.890625, learning_rate 0.000136341

Evaluation:
2017-10-10T12:49:37.132810: step 1200, loss 0.238107, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1200

2017-10-10T12:49:38.637761: step 1201, loss 0.242254, acc 0.953125, learning_rate 0.000136193
2017-10-10T12:49:39.176892: step 1202, loss 0.12842, acc 0.96875, learning_rate 0.000136045
2017-10-10T12:49:39.712853: step 1203, loss 0.199758, acc 0.921875, learning_rate 0.000135898
2017-10-10T12:49:40.280828: step 1204, loss 0.149306, acc 0.9375, learning_rate 0.000135751
2017-10-10T12:49:40.798916: step 1205, loss 0.25873, acc 0.859375, learning_rate 0.000135605
2017-10-10T12:49:41.316033: step 1206, loss 0.200636, acc 0.9375, learning_rate 0.00013546
2017-10-10T12:49:41.933199: step 1207, loss 0.188817, acc 0.921875, learning_rate 0.000135315
2017-10-10T12:49:42.506219: step 1208, loss 0.207279, acc 0.921875, learning_rate 0.000135171
2017-10-10T12:49:42.964972: step 1209, loss 0.238763, acc 0.921875, learning_rate 0.000135028
2017-10-10T12:49:43.396446: step 1210, loss 0.150615, acc 0.9375, learning_rate 0.000134885
2017-10-10T12:49:43.956119: step 1211, loss 0.16611, acc 0.953125, learning_rate 0.000134742
2017-10-10T12:49:44.501970: step 1212, loss 0.155479, acc 0.9375, learning_rate 0.0001346
2017-10-10T12:49:45.065280: step 1213, loss 0.0789089, acc 0.984375, learning_rate 0.000134459
2017-10-10T12:49:45.607492: step 1214, loss 0.252322, acc 0.890625, learning_rate 0.000134319
2017-10-10T12:49:46.118380: step 1215, loss 0.345114, acc 0.84375, learning_rate 0.000134178
2017-10-10T12:49:46.568949: step 1216, loss 0.110905, acc 0.953125, learning_rate 0.000134039
2017-10-10T12:49:47.086143: step 1217, loss 0.239702, acc 0.921875, learning_rate 0.0001339
2017-10-10T12:49:47.624915: step 1218, loss 0.1985, acc 0.921875, learning_rate 0.000133762
2017-10-10T12:49:48.180829: step 1219, loss 0.24731, acc 0.890625, learning_rate 0.000133624
2017-10-10T12:49:48.695792: step 1220, loss 0.106545, acc 1, learning_rate 0.000133487
2017-10-10T12:49:49.124853: step 1221, loss 0.250295, acc 0.9375, learning_rate 0.00013335
2017-10-10T12:49:49.620962: step 1222, loss 0.16961, acc 0.921875, learning_rate 0.000133214
2017-10-10T12:49:50.168946: step 1223, loss 0.202506, acc 0.90625, learning_rate 0.000133078
2017-10-10T12:49:50.739726: step 1224, loss 0.156013, acc 0.953125, learning_rate 0.000132943
2017-10-10T12:49:51.181874: step 1225, loss 0.120154, acc 0.96875, learning_rate 0.000132809
2017-10-10T12:49:51.754512: step 1226, loss 0.0972658, acc 0.984375, learning_rate 0.000132675
2017-10-10T12:49:52.239619: step 1227, loss 0.160879, acc 0.953125, learning_rate 0.000132541
2017-10-10T12:49:52.776350: step 1228, loss 0.107018, acc 0.96875, learning_rate 0.000132409
2017-10-10T12:49:53.368870: step 1229, loss 0.16477, acc 0.953125, learning_rate 0.000132276
2017-10-10T12:49:53.981573: step 1230, loss 0.300854, acc 0.875, learning_rate 0.000132145
2017-10-10T12:49:54.527219: step 1231, loss 0.186124, acc 0.90625, learning_rate 0.000132013
2017-10-10T12:49:55.140657: step 1232, loss 0.171267, acc 0.96875, learning_rate 0.000131883
2017-10-10T12:49:55.678963: step 1233, loss 0.519535, acc 0.828125, learning_rate 0.000131753
2017-10-10T12:49:56.185105: step 1234, loss 0.126898, acc 0.9375, learning_rate 0.000131623
2017-10-10T12:49:56.780876: step 1235, loss 0.129975, acc 0.9375, learning_rate 0.000131494
2017-10-10T12:49:57.422930: step 1236, loss 0.289712, acc 0.90625, learning_rate 0.000131365
2017-10-10T12:49:57.891883: step 1237, loss 0.157939, acc 0.921875, learning_rate 0.000131237
2017-10-10T12:49:58.360832: step 1238, loss 0.156774, acc 0.96875, learning_rate 0.00013111
2017-10-10T12:49:58.835773: step 1239, loss 0.214374, acc 0.921875, learning_rate 0.000130983
2017-10-10T12:49:59.363218: step 1240, loss 0.143323, acc 0.953125, learning_rate 0.000130856

Evaluation:
2017-10-10T12:50:00.646022: step 1240, loss 0.236133, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1240

2017-10-10T12:50:02.376982: step 1241, loss 0.155049, acc 0.953125, learning_rate 0.00013073
2017-10-10T12:50:02.946169: step 1242, loss 0.261957, acc 0.890625, learning_rate 0.000130605
2017-10-10T12:50:03.409753: step 1243, loss 0.159517, acc 0.9375, learning_rate 0.00013048
2017-10-10T12:50:03.948417: step 1244, loss 0.16777, acc 0.953125, learning_rate 0.000130356
2017-10-10T12:50:04.465505: step 1245, loss 0.252547, acc 0.875, learning_rate 0.000130232
2017-10-10T12:50:04.999888: step 1246, loss 0.118117, acc 0.96875, learning_rate 0.000130108
2017-10-10T12:50:05.539945: step 1247, loss 0.108131, acc 0.9375, learning_rate 0.000129985
2017-10-10T12:50:06.127859: step 1248, loss 0.287134, acc 0.90625, learning_rate 0.000129863
2017-10-10T12:50:06.566755: step 1249, loss 0.173266, acc 0.921875, learning_rate 0.000129741
2017-10-10T12:50:07.008856: step 1250, loss 0.156567, acc 0.9375, learning_rate 0.00012962
2017-10-10T12:50:07.381545: step 1251, loss 0.190793, acc 0.9375, learning_rate 0.000129499
2017-10-10T12:50:07.975453: step 1252, loss 0.140334, acc 0.9375, learning_rate 0.000129378
2017-10-10T12:50:08.392988: step 1253, loss 0.223084, acc 0.9375, learning_rate 0.000129259
2017-10-10T12:50:08.833299: step 1254, loss 0.176398, acc 0.953125, learning_rate 0.000129139
2017-10-10T12:50:09.319581: step 1255, loss 0.124891, acc 0.953125, learning_rate 0.00012902
2017-10-10T12:50:09.798149: step 1256, loss 0.18983, acc 0.921875, learning_rate 0.000128902
2017-10-10T12:50:10.442939: step 1257, loss 0.158566, acc 0.921875, learning_rate 0.000128784
2017-10-10T12:50:11.063083: step 1258, loss 0.143688, acc 0.953125, learning_rate 0.000128666
2017-10-10T12:50:11.522909: step 1259, loss 0.232056, acc 0.9375, learning_rate 0.000128549
2017-10-10T12:50:11.967707: step 1260, loss 0.337198, acc 0.890625, learning_rate 0.000128433
2017-10-10T12:50:12.541749: step 1261, loss 0.141681, acc 0.96875, learning_rate 0.000128317
2017-10-10T12:50:13.044929: step 1262, loss 0.133956, acc 0.9375, learning_rate 0.000128201
2017-10-10T12:50:13.564874: step 1263, loss 0.210049, acc 0.90625, learning_rate 0.000128086
2017-10-10T12:50:14.112678: step 1264, loss 0.212038, acc 0.90625, learning_rate 0.000127971
2017-10-10T12:50:14.643919: step 1265, loss 0.110032, acc 0.96875, learning_rate 0.000127857
2017-10-10T12:50:15.168981: step 1266, loss 0.338438, acc 0.875, learning_rate 0.000127743
2017-10-10T12:50:15.615623: step 1267, loss 0.20945, acc 0.9375, learning_rate 0.00012763
2017-10-10T12:50:16.145008: step 1268, loss 0.248513, acc 0.9375, learning_rate 0.000127517
2017-10-10T12:50:16.660833: step 1269, loss 0.205136, acc 0.9375, learning_rate 0.000127405
2017-10-10T12:50:17.227509: step 1270, loss 0.226385, acc 0.90625, learning_rate 0.000127293
2017-10-10T12:50:17.768819: step 1271, loss 0.186911, acc 0.921875, learning_rate 0.000127182
2017-10-10T12:50:18.304988: step 1272, loss 0.184566, acc 0.921875, learning_rate 0.000127071
2017-10-10T12:50:18.848943: step 1273, loss 0.166927, acc 0.9375, learning_rate 0.00012696
2017-10-10T12:50:19.311051: step 1274, loss 0.252727, acc 0.921569, learning_rate 0.00012685
2017-10-10T12:50:19.908911: step 1275, loss 0.130077, acc 0.9375, learning_rate 0.000126741
2017-10-10T12:50:20.440862: step 1276, loss 0.128254, acc 0.96875, learning_rate 0.000126632
2017-10-10T12:50:21.009135: step 1277, loss 0.195902, acc 0.9375, learning_rate 0.000126523
2017-10-10T12:50:21.427318: step 1278, loss 0.165241, acc 0.953125, learning_rate 0.000126415
2017-10-10T12:50:21.838467: step 1279, loss 0.238783, acc 0.90625, learning_rate 0.000126307
2017-10-10T12:50:22.352921: step 1280, loss 0.0872809, acc 0.984375, learning_rate 0.000126199

Evaluation:
2017-10-10T12:50:23.401013: step 1280, loss 0.238883, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1280

2017-10-10T12:50:24.917143: step 1281, loss 0.16072, acc 0.9375, learning_rate 0.000126093
2017-10-10T12:50:25.488599: step 1282, loss 0.20439, acc 0.90625, learning_rate 0.000125986
2017-10-10T12:50:26.021048: step 1283, loss 0.278452, acc 0.921875, learning_rate 0.00012588
2017-10-10T12:50:26.546653: step 1284, loss 0.111741, acc 0.953125, learning_rate 0.000125774
2017-10-10T12:50:27.098059: step 1285, loss 0.084099, acc 0.984375, learning_rate 0.000125669
2017-10-10T12:50:27.612967: step 1286, loss 0.143358, acc 0.953125, learning_rate 0.000125564
2017-10-10T12:50:28.165742: step 1287, loss 0.0708467, acc 0.96875, learning_rate 0.00012546
2017-10-10T12:50:28.636910: step 1288, loss 0.212348, acc 0.96875, learning_rate 0.000125356
2017-10-10T12:50:29.276745: step 1289, loss 0.168133, acc 0.953125, learning_rate 0.000125253
2017-10-10T12:50:29.799459: step 1290, loss 0.143167, acc 0.9375, learning_rate 0.00012515
2017-10-10T12:50:30.202380: step 1291, loss 0.274169, acc 0.90625, learning_rate 0.000125047
2017-10-10T12:50:30.597028: step 1292, loss 0.329641, acc 0.90625, learning_rate 0.000124945
2017-10-10T12:50:31.104951: step 1293, loss 0.298719, acc 0.875, learning_rate 0.000124843
2017-10-10T12:50:31.628869: step 1294, loss 0.171907, acc 0.96875, learning_rate 0.000124741
2017-10-10T12:50:32.069203: step 1295, loss 0.242545, acc 0.90625, learning_rate 0.00012464
2017-10-10T12:50:32.617128: step 1296, loss 0.309097, acc 0.921875, learning_rate 0.00012454
2017-10-10T12:50:33.093753: step 1297, loss 0.117853, acc 0.984375, learning_rate 0.00012444
2017-10-10T12:50:33.644820: step 1298, loss 0.208578, acc 0.890625, learning_rate 0.00012434
2017-10-10T12:50:34.234676: step 1299, loss 0.207964, acc 0.921875, learning_rate 0.000124241
2017-10-10T12:50:34.688826: step 1300, loss 0.139205, acc 0.9375, learning_rate 0.000124142
2017-10-10T12:50:35.118995: step 1301, loss 0.0819228, acc 0.984375, learning_rate 0.000124043
2017-10-10T12:50:35.605328: step 1302, loss 0.133281, acc 0.9375, learning_rate 0.000123945
2017-10-10T12:50:36.211904: step 1303, loss 0.136716, acc 0.921875, learning_rate 0.000123847
2017-10-10T12:50:36.743920: step 1304, loss 0.191315, acc 0.921875, learning_rate 0.00012375
2017-10-10T12:50:37.300010: step 1305, loss 0.260819, acc 0.90625, learning_rate 0.000123653
2017-10-10T12:50:37.817071: step 1306, loss 0.159154, acc 0.953125, learning_rate 0.000123556
2017-10-10T12:50:38.357891: step 1307, loss 0.196379, acc 0.9375, learning_rate 0.00012346
2017-10-10T12:50:38.848969: step 1308, loss 0.132727, acc 0.953125, learning_rate 0.000123364
2017-10-10T12:50:39.374376: step 1309, loss 0.102652, acc 0.953125, learning_rate 0.000123269
2017-10-10T12:50:39.856557: step 1310, loss 0.22381, acc 0.9375, learning_rate 0.000123174
2017-10-10T12:50:40.461320: step 1311, loss 0.262878, acc 0.921875, learning_rate 0.00012308
2017-10-10T12:50:40.980892: step 1312, loss 0.113742, acc 0.96875, learning_rate 0.000122985
2017-10-10T12:50:41.505078: step 1313, loss 0.232096, acc 0.953125, learning_rate 0.000122892
2017-10-10T12:50:41.992958: step 1314, loss 0.200361, acc 0.9375, learning_rate 0.000122798
2017-10-10T12:50:42.433017: step 1315, loss 0.215378, acc 0.953125, learning_rate 0.000122705
2017-10-10T12:50:43.044998: step 1316, loss 0.220414, acc 0.921875, learning_rate 0.000122612
2017-10-10T12:50:43.493920: step 1317, loss 0.0993495, acc 0.96875, learning_rate 0.00012252
2017-10-10T12:50:43.934632: step 1318, loss 0.272654, acc 0.90625, learning_rate 0.000122428
2017-10-10T12:50:44.272873: step 1319, loss 0.176559, acc 0.953125, learning_rate 0.000122337
2017-10-10T12:50:44.845645: step 1320, loss 0.191499, acc 0.90625, learning_rate 0.000122245

Evaluation:
2017-10-10T12:50:45.975726: step 1320, loss 0.233544, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1320

2017-10-10T12:50:47.656611: step 1321, loss 0.159768, acc 0.9375, learning_rate 0.000122155
2017-10-10T12:50:48.191587: step 1322, loss 0.304756, acc 0.90625, learning_rate 0.000122064
2017-10-10T12:50:48.706148: step 1323, loss 0.215082, acc 0.9375, learning_rate 0.000121974
2017-10-10T12:50:49.223987: step 1324, loss 0.282728, acc 0.875, learning_rate 0.000121884
2017-10-10T12:50:49.746157: step 1325, loss 0.183803, acc 0.921875, learning_rate 0.000121795
2017-10-10T12:50:50.267706: step 1326, loss 0.191674, acc 0.9375, learning_rate 0.000121706
2017-10-10T12:50:50.863314: step 1327, loss 0.207252, acc 0.953125, learning_rate 0.000121618
2017-10-10T12:50:51.368852: step 1328, loss 0.136892, acc 0.953125, learning_rate 0.000121529
2017-10-10T12:50:51.928200: step 1329, loss 0.221566, acc 0.90625, learning_rate 0.000121441
2017-10-10T12:50:52.525751: step 1330, loss 0.15751, acc 0.9375, learning_rate 0.000121354
2017-10-10T12:50:52.979560: step 1331, loss 0.120048, acc 0.96875, learning_rate 0.000121267
2017-10-10T12:50:53.437163: step 1332, loss 0.206679, acc 0.96875, learning_rate 0.00012118
2017-10-10T12:50:54.003361: step 1333, loss 0.103821, acc 0.984375, learning_rate 0.000121093
2017-10-10T12:50:54.564846: step 1334, loss 0.148763, acc 0.9375, learning_rate 0.000121007
2017-10-10T12:50:55.116830: step 1335, loss 0.28709, acc 0.90625, learning_rate 0.000120922
2017-10-10T12:50:55.660827: step 1336, loss 0.112028, acc 0.984375, learning_rate 0.000120836
2017-10-10T12:50:56.208562: step 1337, loss 0.138598, acc 0.953125, learning_rate 0.000120751
2017-10-10T12:50:56.801004: step 1338, loss 0.192631, acc 0.9375, learning_rate 0.000120666
2017-10-10T12:50:57.378104: step 1339, loss 0.210592, acc 0.9375, learning_rate 0.000120582
2017-10-10T12:50:57.774956: step 1340, loss 0.318808, acc 0.875, learning_rate 0.000120498
2017-10-10T12:50:58.212878: step 1341, loss 0.173914, acc 0.9375, learning_rate 0.000120414
2017-10-10T12:50:58.755405: step 1342, loss 0.106635, acc 0.96875, learning_rate 0.000120331
2017-10-10T12:50:59.245033: step 1343, loss 0.141859, acc 0.953125, learning_rate 0.000120248
2017-10-10T12:50:59.712738: step 1344, loss 0.13149, acc 0.96875, learning_rate 0.000120165
2017-10-10T12:51:00.212946: step 1345, loss 0.0884313, acc 0.984375, learning_rate 0.000120083
2017-10-10T12:51:00.744918: step 1346, loss 0.193847, acc 0.90625, learning_rate 0.000120001
2017-10-10T12:51:01.251824: step 1347, loss 0.0898374, acc 0.984375, learning_rate 0.00011992
2017-10-10T12:51:01.749903: step 1348, loss 0.192637, acc 0.9375, learning_rate 0.000119838
2017-10-10T12:51:02.261137: step 1349, loss 0.179853, acc 0.921875, learning_rate 0.000119757
2017-10-10T12:51:02.888112: step 1350, loss 0.194564, acc 0.9375, learning_rate 0.000119677
2017-10-10T12:51:03.423727: step 1351, loss 0.178374, acc 0.90625, learning_rate 0.000119596
2017-10-10T12:51:03.976752: step 1352, loss 0.121676, acc 0.953125, learning_rate 0.000119516
2017-10-10T12:51:04.520915: step 1353, loss 0.0772219, acc 0.96875, learning_rate 0.000119437
2017-10-10T12:51:05.080996: step 1354, loss 0.167613, acc 0.9375, learning_rate 0.000119357
2017-10-10T12:51:05.573895: step 1355, loss 0.24137, acc 0.9375, learning_rate 0.000119278
2017-10-10T12:51:06.225054: step 1356, loss 0.151793, acc 0.953125, learning_rate 0.0001192
2017-10-10T12:51:06.666429: step 1357, loss 0.195906, acc 0.90625, learning_rate 0.000119121
2017-10-10T12:51:07.000994: step 1358, loss 0.163861, acc 0.953125, learning_rate 0.000119043
2017-10-10T12:51:07.465162: step 1359, loss 0.202794, acc 0.9375, learning_rate 0.000118965
2017-10-10T12:51:08.009301: step 1360, loss 0.197465, acc 0.953125, learning_rate 0.000118888

Evaluation:
2017-10-10T12:51:09.100928: step 1360, loss 0.233551, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1360

2017-10-10T12:51:10.681012: step 1361, loss 0.24677, acc 0.890625, learning_rate 0.000118811
2017-10-10T12:51:11.252940: step 1362, loss 0.165376, acc 0.953125, learning_rate 0.000118734
2017-10-10T12:51:11.732848: step 1363, loss 0.252978, acc 0.9375, learning_rate 0.000118658
2017-10-10T12:51:12.243185: step 1364, loss 0.267209, acc 0.890625, learning_rate 0.000118582
2017-10-10T12:51:12.781027: step 1365, loss 0.196071, acc 0.96875, learning_rate 0.000118506
2017-10-10T12:51:13.336934: step 1366, loss 0.117499, acc 0.953125, learning_rate 0.00011843
2017-10-10T12:51:13.833646: step 1367, loss 0.208606, acc 0.9375, learning_rate 0.000118355
2017-10-10T12:51:14.341152: step 1368, loss 0.102279, acc 0.953125, learning_rate 0.00011828
2017-10-10T12:51:14.968972: step 1369, loss 0.259508, acc 0.953125, learning_rate 0.000118205
2017-10-10T12:51:15.632923: step 1370, loss 0.294065, acc 0.96875, learning_rate 0.000118131
2017-10-10T12:51:16.028978: step 1371, loss 0.160912, acc 0.953125, learning_rate 0.000118057
2017-10-10T12:51:16.376841: step 1372, loss 0.110699, acc 0.960784, learning_rate 0.000117983
2017-10-10T12:51:16.833084: step 1373, loss 0.256712, acc 0.875, learning_rate 0.00011791
2017-10-10T12:51:17.389076: step 1374, loss 0.128839, acc 0.96875, learning_rate 0.000117837
2017-10-10T12:51:17.905049: step 1375, loss 0.118697, acc 0.953125, learning_rate 0.000117764
2017-10-10T12:51:18.452035: step 1376, loss 0.269277, acc 0.890625, learning_rate 0.000117692
2017-10-10T12:51:18.925893: step 1377, loss 0.174706, acc 0.953125, learning_rate 0.000117619
2017-10-10T12:51:19.460927: step 1378, loss 0.189127, acc 0.9375, learning_rate 0.000117547
2017-10-10T12:51:20.041016: step 1379, loss 0.148514, acc 0.9375, learning_rate 0.000117476
2017-10-10T12:51:20.582148: step 1380, loss 0.168381, acc 0.9375, learning_rate 0.000117404
2017-10-10T12:51:20.990492: step 1381, loss 0.0906887, acc 0.96875, learning_rate 0.000117333
2017-10-10T12:51:21.368104: step 1382, loss 0.186594, acc 0.9375, learning_rate 0.000117263
2017-10-10T12:51:21.892793: step 1383, loss 0.150238, acc 0.953125, learning_rate 0.000117192
2017-10-10T12:51:22.410288: step 1384, loss 0.182888, acc 0.921875, learning_rate 0.000117122
2017-10-10T12:51:22.873147: step 1385, loss 0.23444, acc 0.9375, learning_rate 0.000117052
2017-10-10T12:51:23.406990: step 1386, loss 0.126993, acc 0.984375, learning_rate 0.000116983
2017-10-10T12:51:23.873250: step 1387, loss 0.224874, acc 0.890625, learning_rate 0.000116913
2017-10-10T12:51:24.441089: step 1388, loss 0.138489, acc 0.953125, learning_rate 0.000116844
2017-10-10T12:51:25.009238: step 1389, loss 0.119854, acc 0.953125, learning_rate 0.000116775
2017-10-10T12:51:25.568978: step 1390, loss 0.194179, acc 0.921875, learning_rate 0.000116707
2017-10-10T12:51:26.013050: step 1391, loss 0.278472, acc 0.90625, learning_rate 0.000116639
2017-10-10T12:51:26.531955: step 1392, loss 0.221359, acc 0.890625, learning_rate 0.000116571
2017-10-10T12:51:27.008789: step 1393, loss 0.18168, acc 0.9375, learning_rate 0.000116503
2017-10-10T12:51:27.552254: step 1394, loss 0.348768, acc 0.875, learning_rate 0.000116436
2017-10-10T12:51:28.213693: step 1395, loss 0.172622, acc 0.9375, learning_rate 0.000116369
2017-10-10T12:51:28.688998: step 1396, loss 0.176887, acc 0.9375, learning_rate 0.000116302
2017-10-10T12:51:29.080802: step 1397, loss 0.153378, acc 0.953125, learning_rate 0.000116235
2017-10-10T12:51:29.443540: step 1398, loss 0.240206, acc 0.875, learning_rate 0.000116169
2017-10-10T12:51:30.024868: step 1399, loss 0.182063, acc 0.9375, learning_rate 0.000116103
2017-10-10T12:51:30.527420: step 1400, loss 0.191301, acc 0.90625, learning_rate 0.000116037

Evaluation:
2017-10-10T12:51:31.860906: step 1400, loss 0.234064, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1400

2017-10-10T12:51:33.741105: step 1401, loss 0.186472, acc 0.921875, learning_rate 0.000115972
2017-10-10T12:51:34.272998: step 1402, loss 0.143314, acc 0.953125, learning_rate 0.000115907
2017-10-10T12:51:34.796227: step 1403, loss 0.344924, acc 0.875, learning_rate 0.000115842
2017-10-10T12:51:35.375943: step 1404, loss 0.124611, acc 0.9375, learning_rate 0.000115777
2017-10-10T12:51:35.870569: step 1405, loss 0.290465, acc 0.875, learning_rate 0.000115713
2017-10-10T12:51:36.397102: step 1406, loss 0.0940364, acc 0.953125, learning_rate 0.000115649
2017-10-10T12:51:36.907546: step 1407, loss 0.255404, acc 0.90625, learning_rate 0.000115585
2017-10-10T12:51:37.528935: step 1408, loss 0.383715, acc 0.890625, learning_rate 0.000115521
2017-10-10T12:51:38.094246: step 1409, loss 0.194942, acc 0.9375, learning_rate 0.000115458
2017-10-10T12:51:38.512836: step 1410, loss 0.118526, acc 0.953125, learning_rate 0.000115395
2017-10-10T12:51:38.912825: step 1411, loss 0.187791, acc 0.953125, learning_rate 0.000115332
2017-10-10T12:51:39.412438: step 1412, loss 0.153358, acc 0.9375, learning_rate 0.000115269
2017-10-10T12:51:39.844980: step 1413, loss 0.117216, acc 0.953125, learning_rate 0.000115207
2017-10-10T12:51:40.364943: step 1414, loss 0.138533, acc 0.96875, learning_rate 0.000115145
2017-10-10T12:51:40.845954: step 1415, loss 0.116571, acc 0.9375, learning_rate 0.000115083
2017-10-10T12:51:41.369198: step 1416, loss 0.201911, acc 0.953125, learning_rate 0.000115022
2017-10-10T12:51:41.884246: step 1417, loss 0.230508, acc 0.953125, learning_rate 0.00011496
2017-10-10T12:51:42.436874: step 1418, loss 0.160745, acc 0.953125, learning_rate 0.000114899
2017-10-10T12:51:43.052881: step 1419, loss 0.368292, acc 0.875, learning_rate 0.000114838
2017-10-10T12:51:43.580866: step 1420, loss 0.167392, acc 0.90625, learning_rate 0.000114778
2017-10-10T12:51:43.996274: step 1421, loss 0.312589, acc 0.921875, learning_rate 0.000114717
2017-10-10T12:51:44.377967: step 1422, loss 0.0793541, acc 0.984375, learning_rate 0.000114657
2017-10-10T12:51:44.812947: step 1423, loss 0.0557341, acc 0.984375, learning_rate 0.000114598
2017-10-10T12:51:45.321082: step 1424, loss 0.134328, acc 0.9375, learning_rate 0.000114538
2017-10-10T12:51:45.897108: step 1425, loss 0.252189, acc 0.921875, learning_rate 0.000114479
2017-10-10T12:51:46.345122: step 1426, loss 0.0831224, acc 0.96875, learning_rate 0.00011442
2017-10-10T12:51:46.845813: step 1427, loss 0.142129, acc 0.96875, learning_rate 0.000114361
2017-10-10T12:51:47.334428: step 1428, loss 0.123797, acc 0.96875, learning_rate 0.000114302
2017-10-10T12:51:47.816003: step 1429, loss 0.175599, acc 0.9375, learning_rate 0.000114244
2017-10-10T12:51:48.351507: step 1430, loss 0.128688, acc 0.96875, learning_rate 0.000114186
2017-10-10T12:51:48.960402: step 1431, loss 0.259736, acc 0.921875, learning_rate 0.000114128
2017-10-10T12:51:49.476593: step 1432, loss 0.130113, acc 0.96875, learning_rate 0.00011407
2017-10-10T12:51:49.992904: step 1433, loss 0.198266, acc 0.9375, learning_rate 0.000114013
2017-10-10T12:51:50.466500: step 1434, loss 0.314362, acc 0.90625, learning_rate 0.000113955
2017-10-10T12:51:51.037537: step 1435, loss 0.154443, acc 0.953125, learning_rate 0.000113898
2017-10-10T12:51:51.628397: step 1436, loss 0.101628, acc 0.96875, learning_rate 0.000113842
2017-10-10T12:51:52.175435: step 1437, loss 0.220626, acc 0.9375, learning_rate 0.000113785
2017-10-10T12:51:52.624094: step 1438, loss 0.151254, acc 0.9375, learning_rate 0.000113729
2017-10-10T12:51:53.124480: step 1439, loss 0.359952, acc 0.90625, learning_rate 0.000113673
2017-10-10T12:51:53.648839: step 1440, loss 0.290139, acc 0.9375, learning_rate 0.000113617

Evaluation:
2017-10-10T12:51:54.800496: step 1440, loss 0.235009, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1440

2017-10-10T12:51:56.164926: step 1441, loss 0.375889, acc 0.90625, learning_rate 0.000113561
2017-10-10T12:51:56.685021: step 1442, loss 0.14526, acc 0.9375, learning_rate 0.000113506
2017-10-10T12:51:57.229027: step 1443, loss 0.236637, acc 0.953125, learning_rate 0.000113451
2017-10-10T12:51:57.756904: step 1444, loss 0.207795, acc 0.9375, learning_rate 0.000113396
2017-10-10T12:51:58.258127: step 1445, loss 0.30546, acc 0.9375, learning_rate 0.000113341
2017-10-10T12:51:58.734761: step 1446, loss 0.230752, acc 0.921875, learning_rate 0.000113287
2017-10-10T12:51:59.290836: step 1447, loss 0.241143, acc 0.921875, learning_rate 0.000113233
2017-10-10T12:51:59.820898: step 1448, loss 0.272034, acc 0.890625, learning_rate 0.000113179
2017-10-10T12:52:00.413089: step 1449, loss 0.224916, acc 0.890625, learning_rate 0.000113125
2017-10-10T12:52:01.037053: step 1450, loss 0.293877, acc 0.921875, learning_rate 0.000113071
2017-10-10T12:52:01.521141: step 1451, loss 0.112693, acc 0.9375, learning_rate 0.000113018
2017-10-10T12:52:01.990686: step 1452, loss 0.19256, acc 0.96875, learning_rate 0.000112965
2017-10-10T12:52:02.426958: step 1453, loss 0.17832, acc 0.953125, learning_rate 0.000112912
2017-10-10T12:52:02.992582: step 1454, loss 0.227207, acc 0.953125, learning_rate 0.000112859
2017-10-10T12:52:03.561080: step 1455, loss 0.233416, acc 0.890625, learning_rate 0.000112807
2017-10-10T12:52:04.105113: step 1456, loss 0.0970001, acc 0.96875, learning_rate 0.000112754
2017-10-10T12:52:04.598516: step 1457, loss 0.261929, acc 0.890625, learning_rate 0.000112702
2017-10-10T12:52:05.111562: step 1458, loss 0.195401, acc 0.921875, learning_rate 0.000112651
2017-10-10T12:52:05.624931: step 1459, loss 0.259623, acc 0.921875, learning_rate 0.000112599
2017-10-10T12:52:06.161207: step 1460, loss 0.251523, acc 0.90625, learning_rate 0.000112547
2017-10-10T12:52:06.597124: step 1461, loss 0.286807, acc 0.90625, learning_rate 0.000112496
2017-10-10T12:52:07.035102: step 1462, loss 0.119596, acc 0.953125, learning_rate 0.000112445
2017-10-10T12:52:07.568955: step 1463, loss 0.217397, acc 0.921875, learning_rate 0.000112394
2017-10-10T12:52:08.068842: step 1464, loss 0.18845, acc 0.9375, learning_rate 0.000112344
2017-10-10T12:52:08.577009: step 1465, loss 0.171033, acc 0.9375, learning_rate 0.000112293
2017-10-10T12:52:09.024895: step 1466, loss 0.352285, acc 0.890625, learning_rate 0.000112243
2017-10-10T12:52:09.551375: step 1467, loss 0.20884, acc 0.9375, learning_rate 0.000112193
2017-10-10T12:52:10.154918: step 1468, loss 0.099159, acc 0.953125, learning_rate 0.000112144
2017-10-10T12:52:10.665113: step 1469, loss 0.200184, acc 0.921875, learning_rate 0.000112094
2017-10-10T12:52:11.087588: step 1470, loss 0.123431, acc 0.980392, learning_rate 0.000112045
2017-10-10T12:52:11.604260: step 1471, loss 0.214894, acc 0.9375, learning_rate 0.000111995
2017-10-10T12:52:12.106342: step 1472, loss 0.1828, acc 0.953125, learning_rate 0.000111946
2017-10-10T12:52:12.564871: step 1473, loss 0.150461, acc 0.9375, learning_rate 0.000111898
2017-10-10T12:52:13.079393: step 1474, loss 0.221431, acc 0.953125, learning_rate 0.000111849
2017-10-10T12:52:13.653033: step 1475, loss 0.133945, acc 0.9375, learning_rate 0.000111801
2017-10-10T12:52:14.199383: step 1476, loss 0.112116, acc 0.96875, learning_rate 0.000111753
2017-10-10T12:52:14.893042: step 1477, loss 0.142231, acc 0.9375, learning_rate 0.000111705
2017-10-10T12:52:15.264087: step 1478, loss 0.198297, acc 0.90625, learning_rate 0.000111657
2017-10-10T12:52:15.692222: step 1479, loss 0.172964, acc 0.9375, learning_rate 0.000111609
2017-10-10T12:52:16.252989: step 1480, loss 0.222959, acc 0.90625, learning_rate 0.000111562

Evaluation:
2017-10-10T12:52:17.388855: step 1480, loss 0.231787, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1480

2017-10-10T12:52:19.005229: step 1481, loss 0.175654, acc 0.9375, learning_rate 0.000111515
2017-10-10T12:52:19.378206: step 1482, loss 0.206041, acc 0.921875, learning_rate 0.000111468
2017-10-10T12:52:19.908910: step 1483, loss 0.206073, acc 0.953125, learning_rate 0.000111421
2017-10-10T12:52:20.415910: step 1484, loss 0.0726434, acc 0.984375, learning_rate 0.000111374
2017-10-10T12:52:20.936005: step 1485, loss 0.282229, acc 0.90625, learning_rate 0.000111328
2017-10-10T12:52:21.433583: step 1486, loss 0.229167, acc 0.953125, learning_rate 0.000111282
2017-10-10T12:52:21.980031: step 1487, loss 0.125607, acc 0.9375, learning_rate 0.000111236
2017-10-10T12:52:22.532153: step 1488, loss 0.147798, acc 0.953125, learning_rate 0.00011119
2017-10-10T12:52:23.079847: step 1489, loss 0.125274, acc 0.96875, learning_rate 0.000111144
2017-10-10T12:52:23.676896: step 1490, loss 0.191262, acc 0.9375, learning_rate 0.000111099
2017-10-10T12:52:24.364654: step 1491, loss 0.103447, acc 0.953125, learning_rate 0.000111053
2017-10-10T12:52:24.772436: step 1492, loss 0.162831, acc 0.9375, learning_rate 0.000111008
2017-10-10T12:52:25.204198: step 1493, loss 0.344772, acc 0.921875, learning_rate 0.000110963
2017-10-10T12:52:25.669512: step 1494, loss 0.375229, acc 0.890625, learning_rate 0.000110918
2017-10-10T12:52:26.173734: step 1495, loss 0.113542, acc 0.96875, learning_rate 0.000110874
2017-10-10T12:52:26.688313: step 1496, loss 0.132359, acc 0.96875, learning_rate 0.00011083
2017-10-10T12:52:27.243408: step 1497, loss 0.241645, acc 0.921875, learning_rate 0.000110785
2017-10-10T12:52:27.836965: step 1498, loss 0.181429, acc 0.921875, learning_rate 0.000110741
2017-10-10T12:52:28.447261: step 1499, loss 0.0985673, acc 0.984375, learning_rate 0.000110697
2017-10-10T12:52:29.045081: step 1500, loss 0.221525, acc 0.890625, learning_rate 0.000110654
2017-10-10T12:52:29.444832: step 1501, loss 0.205121, acc 0.9375, learning_rate 0.00011061
2017-10-10T12:52:29.912600: step 1502, loss 0.0703694, acc 0.984375, learning_rate 0.000110567
2017-10-10T12:52:30.476571: step 1503, loss 0.157623, acc 0.9375, learning_rate 0.000110524
2017-10-10T12:52:30.965322: step 1504, loss 0.153366, acc 0.953125, learning_rate 0.000110481
2017-10-10T12:52:31.492844: step 1505, loss 0.0863867, acc 0.984375, learning_rate 0.000110438
2017-10-10T12:52:31.966188: step 1506, loss 0.163732, acc 0.9375, learning_rate 0.000110396
2017-10-10T12:52:32.441091: step 1507, loss 0.125679, acc 0.96875, learning_rate 0.000110353
2017-10-10T12:52:32.989019: step 1508, loss 0.102319, acc 0.96875, learning_rate 0.000110311
2017-10-10T12:52:33.588880: step 1509, loss 0.139568, acc 0.953125, learning_rate 0.000110269
2017-10-10T12:52:34.057121: step 1510, loss 0.251757, acc 0.90625, learning_rate 0.000110227
2017-10-10T12:52:34.491557: step 1511, loss 0.154937, acc 0.953125, learning_rate 0.000110185
2017-10-10T12:52:34.980702: step 1512, loss 0.217692, acc 0.9375, learning_rate 0.000110144
2017-10-10T12:52:35.517099: step 1513, loss 0.1175, acc 0.9375, learning_rate 0.000110102
2017-10-10T12:52:36.051909: step 1514, loss 0.173581, acc 0.9375, learning_rate 0.000110061
2017-10-10T12:52:36.505024: step 1515, loss 0.179614, acc 0.9375, learning_rate 0.00011002
2017-10-10T12:52:37.161001: step 1516, loss 0.252421, acc 0.90625, learning_rate 0.000109979
2017-10-10T12:52:37.629134: step 1517, loss 0.1499, acc 0.9375, learning_rate 0.000109938
2017-10-10T12:52:38.079037: step 1518, loss 0.1709, acc 0.96875, learning_rate 0.000109898
2017-10-10T12:52:38.524972: step 1519, loss 0.198158, acc 0.921875, learning_rate 0.000109857
2017-10-10T12:52:38.988855: step 1520, loss 0.206983, acc 0.9375, learning_rate 0.000109817

Evaluation:
2017-10-10T12:52:40.206887: step 1520, loss 0.232978, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1520

2017-10-10T12:52:41.940527: step 1521, loss 0.26047, acc 0.921875, learning_rate 0.000109777
2017-10-10T12:52:42.437022: step 1522, loss 0.166623, acc 0.90625, learning_rate 0.000109737
2017-10-10T12:52:42.916446: step 1523, loss 0.112571, acc 0.953125, learning_rate 0.000109697
2017-10-10T12:52:43.454542: step 1524, loss 0.171438, acc 0.953125, learning_rate 0.000109658
2017-10-10T12:52:44.020926: step 1525, loss 0.171335, acc 0.953125, learning_rate 0.000109618
2017-10-10T12:52:44.568848: step 1526, loss 0.123635, acc 0.96875, learning_rate 0.000109579
2017-10-10T12:52:45.098245: step 1527, loss 0.206867, acc 0.953125, learning_rate 0.00010954
2017-10-10T12:52:45.654212: step 1528, loss 0.172201, acc 0.953125, learning_rate 0.000109501
2017-10-10T12:52:46.201245: step 1529, loss 0.168479, acc 0.953125, learning_rate 0.000109462
2017-10-10T12:52:46.737249: step 1530, loss 0.264845, acc 0.890625, learning_rate 0.000109424
2017-10-10T12:52:47.300824: step 1531, loss 0.219591, acc 0.90625, learning_rate 0.000109385
2017-10-10T12:52:47.740821: step 1532, loss 0.245994, acc 0.921875, learning_rate 0.000109347
2017-10-10T12:52:48.207214: step 1533, loss 0.144367, acc 0.96875, learning_rate 0.000109309
2017-10-10T12:52:48.728818: step 1534, loss 0.240444, acc 0.9375, learning_rate 0.000109271
2017-10-10T12:52:49.296877: step 1535, loss 0.117265, acc 0.96875, learning_rate 0.000109233
2017-10-10T12:52:49.873273: step 1536, loss 0.233537, acc 0.9375, learning_rate 0.000109195
2017-10-10T12:52:50.413683: step 1537, loss 0.151635, acc 0.921875, learning_rate 0.000109158
2017-10-10T12:52:50.943393: step 1538, loss 0.138896, acc 0.953125, learning_rate 0.00010912
2017-10-10T12:52:51.452918: step 1539, loss 0.209874, acc 0.921875, learning_rate 0.000109083
2017-10-10T12:52:52.032988: step 1540, loss 0.116272, acc 0.953125, learning_rate 0.000109046
2017-10-10T12:52:52.554588: step 1541, loss 0.254878, acc 0.921875, learning_rate 0.000109009
2017-10-10T12:52:53.019619: step 1542, loss 0.266278, acc 0.90625, learning_rate 0.000108972
2017-10-10T12:52:53.465532: step 1543, loss 0.135072, acc 0.953125, learning_rate 0.000108936
2017-10-10T12:52:53.951931: step 1544, loss 0.110934, acc 0.96875, learning_rate 0.000108899
2017-10-10T12:52:54.516366: step 1545, loss 0.205679, acc 0.953125, learning_rate 0.000108863
2017-10-10T12:52:55.045160: step 1546, loss 0.323001, acc 0.9375, learning_rate 0.000108827
2017-10-10T12:52:55.574968: step 1547, loss 0.176918, acc 0.953125, learning_rate 0.000108791
2017-10-10T12:52:56.090060: step 1548, loss 0.199222, acc 0.953125, learning_rate 0.000108755
2017-10-10T12:52:56.659091: step 1549, loss 0.0809043, acc 0.984375, learning_rate 0.000108719
2017-10-10T12:52:57.232862: step 1550, loss 0.128659, acc 0.96875, learning_rate 0.000108683
2017-10-10T12:52:57.776586: step 1551, loss 0.175848, acc 0.9375, learning_rate 0.000108648
2017-10-10T12:52:58.288680: step 1552, loss 0.132007, acc 0.953125, learning_rate 0.000108613
2017-10-10T12:52:58.836928: step 1553, loss 0.202191, acc 0.90625, learning_rate 0.000108577
2017-10-10T12:52:59.368278: step 1554, loss 0.238642, acc 0.90625, learning_rate 0.000108542
2017-10-10T12:52:59.890762: step 1555, loss 0.210183, acc 0.921875, learning_rate 0.000108508
2017-10-10T12:53:00.440927: step 1556, loss 0.105926, acc 0.953125, learning_rate 0.000108473
2017-10-10T12:53:01.101003: step 1557, loss 0.101419, acc 0.953125, learning_rate 0.000108438
2017-10-10T12:53:01.496856: step 1558, loss 0.108366, acc 0.953125, learning_rate 0.000108404
2017-10-10T12:53:01.948822: step 1559, loss 0.232515, acc 0.9375, learning_rate 0.00010837
2017-10-10T12:53:02.472845: step 1560, loss 0.119813, acc 0.96875, learning_rate 0.000108335

Evaluation:
2017-10-10T12:53:03.747015: step 1560, loss 0.2343, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1560

2017-10-10T12:53:05.195099: step 1561, loss 0.0695452, acc 1, learning_rate 0.000108301
2017-10-10T12:53:05.791946: step 1562, loss 0.219369, acc 0.921875, learning_rate 0.000108267
2017-10-10T12:53:06.291863: step 1563, loss 0.210162, acc 0.953125, learning_rate 0.000108234
2017-10-10T12:53:06.732175: step 1564, loss 0.369017, acc 0.890625, learning_rate 0.0001082
2017-10-10T12:53:07.249062: step 1565, loss 0.0613539, acc 0.984375, learning_rate 0.000108167
2017-10-10T12:53:07.757765: step 1566, loss 0.170232, acc 0.9375, learning_rate 0.000108133
2017-10-10T12:53:08.269022: step 1567, loss 0.200216, acc 0.953125, learning_rate 0.0001081
2017-10-10T12:53:08.734670: step 1568, loss 0.144491, acc 0.960784, learning_rate 0.000108067
2017-10-10T12:53:09.256450: step 1569, loss 0.108127, acc 0.984375, learning_rate 0.000108034
2017-10-10T12:53:09.790416: step 1570, loss 0.198062, acc 0.96875, learning_rate 0.000108001
2017-10-10T12:53:10.455564: step 1571, loss 0.326236, acc 0.875, learning_rate 0.000107969
2017-10-10T12:53:10.916954: step 1572, loss 0.166622, acc 0.96875, learning_rate 0.000107936
2017-10-10T12:53:11.356848: step 1573, loss 0.216165, acc 0.875, learning_rate 0.000107904
2017-10-10T12:53:11.902409: step 1574, loss 0.122488, acc 0.953125, learning_rate 0.000107871
2017-10-10T12:53:12.421759: step 1575, loss 0.126839, acc 0.953125, learning_rate 0.000107839
2017-10-10T12:53:12.860177: step 1576, loss 0.183336, acc 0.9375, learning_rate 0.000107807
2017-10-10T12:53:13.382066: step 1577, loss 0.196272, acc 0.90625, learning_rate 0.000107775
2017-10-10T12:53:13.908376: step 1578, loss 0.250659, acc 0.90625, learning_rate 0.000107744
2017-10-10T12:53:14.465208: step 1579, loss 0.130387, acc 0.96875, learning_rate 0.000107712
2017-10-10T12:53:14.968954: step 1580, loss 0.16303, acc 0.953125, learning_rate 0.000107681
2017-10-10T12:53:15.537067: step 1581, loss 0.132944, acc 0.953125, learning_rate 0.000107649
2017-10-10T12:53:15.948082: step 1582, loss 0.0989219, acc 0.953125, learning_rate 0.000107618
2017-10-10T12:53:16.414632: step 1583, loss 0.269478, acc 0.890625, learning_rate 0.000107587
2017-10-10T12:53:16.822805: step 1584, loss 0.144627, acc 0.9375, learning_rate 0.000107556
2017-10-10T12:53:17.392217: step 1585, loss 0.214777, acc 0.921875, learning_rate 0.000107525
2017-10-10T12:53:17.920362: step 1586, loss 0.131399, acc 0.953125, learning_rate 0.000107494
2017-10-10T12:53:18.475895: step 1587, loss 0.181769, acc 0.96875, learning_rate 0.000107464
2017-10-10T12:53:19.029208: step 1588, loss 0.119563, acc 0.9375, learning_rate 0.000107433
2017-10-10T12:53:19.540881: step 1589, loss 0.192841, acc 0.921875, learning_rate 0.000107403
2017-10-10T12:53:20.062753: step 1590, loss 0.0599375, acc 1, learning_rate 0.000107373
2017-10-10T12:53:20.569252: step 1591, loss 0.165751, acc 0.96875, learning_rate 0.000107343
2017-10-10T12:53:21.121351: step 1592, loss 0.198945, acc 0.90625, learning_rate 0.000107313
2017-10-10T12:53:21.639354: step 1593, loss 0.143125, acc 0.9375, learning_rate 0.000107283
2017-10-10T12:53:22.113053: step 1594, loss 0.178822, acc 0.9375, learning_rate 0.000107253
2017-10-10T12:53:22.637883: step 1595, loss 0.103962, acc 0.96875, learning_rate 0.000107224
2017-10-10T12:53:23.208856: step 1596, loss 0.210445, acc 0.921875, learning_rate 0.000107194
2017-10-10T12:53:23.808858: step 1597, loss 0.150713, acc 0.921875, learning_rate 0.000107165
2017-10-10T12:53:24.244864: step 1598, loss 0.127707, acc 0.9375, learning_rate 0.000107136
2017-10-10T12:53:24.686396: step 1599, loss 0.236408, acc 0.921875, learning_rate 0.000107106
2017-10-10T12:53:25.165129: step 1600, loss 0.143931, acc 0.953125, learning_rate 0.000107077

Evaluation:
2017-10-10T12:53:26.314213: step 1600, loss 0.232746, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1600

2017-10-10T12:53:27.906245: step 1601, loss 0.247557, acc 0.9375, learning_rate 0.000107048
2017-10-10T12:53:28.441013: step 1602, loss 0.195405, acc 0.953125, learning_rate 0.00010702
2017-10-10T12:53:29.005321: step 1603, loss 0.192035, acc 0.9375, learning_rate 0.000106991
2017-10-10T12:53:29.548882: step 1604, loss 0.210058, acc 0.875, learning_rate 0.000106963
2017-10-10T12:53:30.020980: step 1605, loss 0.11339, acc 0.984375, learning_rate 0.000106934
2017-10-10T12:53:30.587519: step 1606, loss 0.100211, acc 0.984375, learning_rate 0.000106906
2017-10-10T12:53:31.094547: step 1607, loss 0.202558, acc 0.921875, learning_rate 0.000106878
2017-10-10T12:53:31.592826: step 1608, loss 0.207043, acc 0.9375, learning_rate 0.00010685
2017-10-10T12:53:32.128856: step 1609, loss 0.145091, acc 0.921875, learning_rate 0.000106822
2017-10-10T12:53:32.677060: step 1610, loss 0.157205, acc 0.953125, learning_rate 0.000106794
2017-10-10T12:53:33.252945: step 1611, loss 0.174972, acc 0.9375, learning_rate 0.000106766
2017-10-10T12:53:33.744393: step 1612, loss 0.244653, acc 0.90625, learning_rate 0.000106738
2017-10-10T12:53:34.224926: step 1613, loss 0.250312, acc 0.921875, learning_rate 0.000106711
2017-10-10T12:53:34.669924: step 1614, loss 0.0563898, acc 1, learning_rate 0.000106684
2017-10-10T12:53:35.231311: step 1615, loss 0.197134, acc 0.9375, learning_rate 0.000106656
2017-10-10T12:53:35.785974: step 1616, loss 0.145377, acc 0.9375, learning_rate 0.000106629
2017-10-10T12:53:36.320838: step 1617, loss 0.157743, acc 0.953125, learning_rate 0.000106602
2017-10-10T12:53:36.885010: step 1618, loss 0.230228, acc 0.921875, learning_rate 0.000106575
2017-10-10T12:53:37.442280: step 1619, loss 0.169943, acc 0.9375, learning_rate 0.000106548
2017-10-10T12:53:37.972993: step 1620, loss 0.183831, acc 0.921875, learning_rate 0.000106521
2017-10-10T12:53:38.544924: step 1621, loss 0.18001, acc 0.953125, learning_rate 0.000106495
2017-10-10T12:53:39.052894: step 1622, loss 0.233509, acc 0.90625, learning_rate 0.000106468
2017-10-10T12:53:39.544785: step 1623, loss 0.266362, acc 0.9375, learning_rate 0.000106442
2017-10-10T12:53:40.100824: step 1624, loss 0.249072, acc 0.890625, learning_rate 0.000106416
2017-10-10T12:53:40.642064: step 1625, loss 0.120658, acc 0.953125, learning_rate 0.000106389
2017-10-10T12:53:41.151230: step 1626, loss 0.082507, acc 0.984375, learning_rate 0.000106363
2017-10-10T12:53:41.680984: step 1627, loss 0.131769, acc 0.96875, learning_rate 0.000106337
2017-10-10T12:53:42.232124: step 1628, loss 0.16539, acc 0.96875, learning_rate 0.000106312
2017-10-10T12:53:42.798852: step 1629, loss 0.178014, acc 0.9375, learning_rate 0.000106286
2017-10-10T12:53:43.330430: step 1630, loss 0.0984455, acc 0.96875, learning_rate 0.00010626
2017-10-10T12:53:43.834408: step 1631, loss 0.252701, acc 0.921875, learning_rate 0.000106235
2017-10-10T12:53:44.358420: step 1632, loss 0.215329, acc 0.90625, learning_rate 0.000106209
2017-10-10T12:53:44.833005: step 1633, loss 0.178851, acc 0.9375, learning_rate 0.000106184
2017-10-10T12:53:45.261226: step 1634, loss 0.193745, acc 0.921875, learning_rate 0.000106159
2017-10-10T12:53:45.768869: step 1635, loss 0.0838536, acc 1, learning_rate 0.000106133
2017-10-10T12:53:46.333016: step 1636, loss 0.102951, acc 0.953125, learning_rate 0.000106108
2017-10-10T12:53:46.872967: step 1637, loss 0.279541, acc 0.890625, learning_rate 0.000106083
2017-10-10T12:53:47.452847: step 1638, loss 0.246391, acc 0.875, learning_rate 0.000106059
2017-10-10T12:53:47.905318: step 1639, loss 0.158802, acc 0.9375, learning_rate 0.000106034
2017-10-10T12:53:48.345477: step 1640, loss 0.146414, acc 0.9375, learning_rate 0.000106009

Evaluation:
2017-10-10T12:53:49.524923: step 1640, loss 0.23003, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1640

2017-10-10T12:53:51.399922: step 1641, loss 0.21849, acc 0.9375, learning_rate 0.000105985
2017-10-10T12:53:51.884827: step 1642, loss 0.166919, acc 0.953125, learning_rate 0.00010596
2017-10-10T12:53:52.423954: step 1643, loss 0.237093, acc 0.84375, learning_rate 0.000105936
2017-10-10T12:53:52.924917: step 1644, loss 0.114126, acc 0.96875, learning_rate 0.000105912
2017-10-10T12:53:53.499213: step 1645, loss 0.137688, acc 0.96875, learning_rate 0.000105888
2017-10-10T12:53:54.017119: step 1646, loss 0.178536, acc 0.90625, learning_rate 0.000105864
2017-10-10T12:53:54.564978: step 1647, loss 0.0869959, acc 0.984375, learning_rate 0.00010584
2017-10-10T12:53:55.097299: step 1648, loss 0.350645, acc 0.890625, learning_rate 0.000105816
2017-10-10T12:53:55.647348: step 1649, loss 0.109899, acc 0.953125, learning_rate 0.000105792
2017-10-10T12:53:56.169060: step 1650, loss 0.219256, acc 0.921875, learning_rate 0.000105768
2017-10-10T12:53:56.656855: step 1651, loss 0.0584319, acc 0.96875, learning_rate 0.000105745
2017-10-10T12:53:57.345155: step 1652, loss 0.149024, acc 0.921875, learning_rate 0.000105721
2017-10-10T12:53:57.792921: step 1653, loss 0.121973, acc 0.984375, learning_rate 0.000105698
2017-10-10T12:53:58.259798: step 1654, loss 0.126425, acc 0.96875, learning_rate 0.000105675
2017-10-10T12:53:58.792940: step 1655, loss 0.188662, acc 0.9375, learning_rate 0.000105652
2017-10-10T12:53:59.343206: step 1656, loss 0.19262, acc 0.90625, learning_rate 0.000105629
2017-10-10T12:53:59.910196: step 1657, loss 0.119641, acc 0.96875, learning_rate 0.000105606
2017-10-10T12:54:00.508253: step 1658, loss 0.28215, acc 0.890625, learning_rate 0.000105583
2017-10-10T12:54:01.196778: step 1659, loss 0.237713, acc 0.9375, learning_rate 0.00010556
2017-10-10T12:54:01.768849: step 1660, loss 0.249345, acc 0.90625, learning_rate 0.000105537
2017-10-10T12:54:02.212840: step 1661, loss 0.148924, acc 0.953125, learning_rate 0.000105515
2017-10-10T12:54:02.656920: step 1662, loss 0.0537713, acc 1, learning_rate 0.000105492
2017-10-10T12:54:03.173212: step 1663, loss 0.159458, acc 0.9375, learning_rate 0.00010547
2017-10-10T12:54:03.673019: step 1664, loss 0.267753, acc 0.890625, learning_rate 0.000105447
2017-10-10T12:54:04.124952: step 1665, loss 0.0725944, acc 0.984375, learning_rate 0.000105425
2017-10-10T12:54:04.549108: step 1666, loss 0.165317, acc 0.921569, learning_rate 0.000105403
2017-10-10T12:54:05.117022: step 1667, loss 0.0918632, acc 0.984375, learning_rate 0.000105381
2017-10-10T12:54:05.527880: step 1668, loss 0.288965, acc 0.921875, learning_rate 0.000105359
2017-10-10T12:54:06.097234: step 1669, loss 0.201328, acc 0.921875, learning_rate 0.000105337
2017-10-10T12:54:06.603122: step 1670, loss 0.20328, acc 0.9375, learning_rate 0.000105315
2017-10-10T12:54:07.084072: step 1671, loss 0.115829, acc 0.9375, learning_rate 0.000105294
2017-10-10T12:54:07.615848: step 1672, loss 0.168263, acc 0.953125, learning_rate 0.000105272
2017-10-10T12:54:08.173010: step 1673, loss 0.144798, acc 0.953125, learning_rate 0.000105251
2017-10-10T12:54:08.702405: step 1674, loss 0.177142, acc 0.9375, learning_rate 0.000105229
2017-10-10T12:54:09.271241: step 1675, loss 0.219282, acc 0.90625, learning_rate 0.000105208
2017-10-10T12:54:09.854428: step 1676, loss 0.22935, acc 0.921875, learning_rate 0.000105186
2017-10-10T12:54:10.357126: step 1677, loss 0.0656401, acc 0.96875, learning_rate 0.000105165
2017-10-10T12:54:10.845511: step 1678, loss 0.0873672, acc 0.953125, learning_rate 0.000105144
2017-10-10T12:54:11.308571: step 1679, loss 0.154286, acc 0.953125, learning_rate 0.000105123
2017-10-10T12:54:11.851354: step 1680, loss 0.159658, acc 0.953125, learning_rate 0.000105102

Evaluation:
2017-10-10T12:54:12.968876: step 1680, loss 0.232499, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1680

2017-10-10T12:54:14.600092: step 1681, loss 0.241165, acc 0.921875, learning_rate 0.000105081
2017-10-10T12:54:15.118911: step 1682, loss 0.114989, acc 0.96875, learning_rate 0.000105061
2017-10-10T12:54:15.561124: step 1683, loss 0.0767064, acc 0.984375, learning_rate 0.00010504
2017-10-10T12:54:16.061646: step 1684, loss 0.139283, acc 0.984375, learning_rate 0.00010502
2017-10-10T12:54:16.536510: step 1685, loss 0.197611, acc 0.9375, learning_rate 0.000104999
2017-10-10T12:54:17.082784: step 1686, loss 0.146932, acc 0.9375, learning_rate 0.000104979
2017-10-10T12:54:17.625824: step 1687, loss 0.165025, acc 0.9375, learning_rate 0.000104958
2017-10-10T12:54:18.182274: step 1688, loss 0.233134, acc 0.953125, learning_rate 0.000104938
2017-10-10T12:54:18.672854: step 1689, loss 0.199948, acc 0.9375, learning_rate 0.000104918
2017-10-10T12:54:19.227839: step 1690, loss 0.189412, acc 0.90625, learning_rate 0.000104898
2017-10-10T12:54:19.714820: step 1691, loss 0.0822104, acc 0.984375, learning_rate 0.000104878
2017-10-10T12:54:20.322677: step 1692, loss 0.180801, acc 0.921875, learning_rate 0.000104858
2017-10-10T12:54:20.739963: step 1693, loss 0.291352, acc 0.890625, learning_rate 0.000104838
2017-10-10T12:54:21.126529: step 1694, loss 0.12284, acc 0.984375, learning_rate 0.000104818
2017-10-10T12:54:21.527050: step 1695, loss 0.228404, acc 0.921875, learning_rate 0.000104799
2017-10-10T12:54:22.069561: step 1696, loss 0.168107, acc 0.96875, learning_rate 0.000104779
2017-10-10T12:54:22.576849: step 1697, loss 0.143429, acc 0.953125, learning_rate 0.00010476
2017-10-10T12:54:23.125144: step 1698, loss 0.0975745, acc 0.96875, learning_rate 0.00010474
2017-10-10T12:54:23.656951: step 1699, loss 0.0850634, acc 0.96875, learning_rate 0.000104721
2017-10-10T12:54:24.148474: step 1700, loss 0.137332, acc 0.9375, learning_rate 0.000104702
2017-10-10T12:54:24.732919: step 1701, loss 0.198249, acc 0.921875, learning_rate 0.000104682
2017-10-10T12:54:25.207775: step 1702, loss 0.236367, acc 0.953125, learning_rate 0.000104663
2017-10-10T12:54:25.581182: step 1703, loss 0.104939, acc 0.953125, learning_rate 0.000104644
2017-10-10T12:54:25.973882: step 1704, loss 0.225137, acc 0.90625, learning_rate 0.000104625
2017-10-10T12:54:26.444857: step 1705, loss 0.214429, acc 0.921875, learning_rate 0.000104606
2017-10-10T12:54:27.026742: step 1706, loss 0.113745, acc 0.96875, learning_rate 0.000104588
2017-10-10T12:54:27.572909: step 1707, loss 0.165793, acc 0.96875, learning_rate 0.000104569
2017-10-10T12:54:28.136838: step 1708, loss 0.151558, acc 0.96875, learning_rate 0.00010455
2017-10-10T12:54:28.694682: step 1709, loss 0.12068, acc 0.96875, learning_rate 0.000104532
2017-10-10T12:54:29.224934: step 1710, loss 0.228553, acc 0.90625, learning_rate 0.000104513
2017-10-10T12:54:29.704910: step 1711, loss 0.117367, acc 0.984375, learning_rate 0.000104495
2017-10-10T12:54:30.192150: step 1712, loss 0.157002, acc 0.9375, learning_rate 0.000104476
2017-10-10T12:54:30.657074: step 1713, loss 0.171215, acc 0.9375, learning_rate 0.000104458
2017-10-10T12:54:31.116867: step 1714, loss 0.113989, acc 0.953125, learning_rate 0.00010444
2017-10-10T12:54:31.579286: step 1715, loss 0.159438, acc 0.96875, learning_rate 0.000104422
2017-10-10T12:54:32.124955: step 1716, loss 0.0952231, acc 0.96875, learning_rate 0.000104404
2017-10-10T12:54:32.682114: step 1717, loss 0.304207, acc 0.890625, learning_rate 0.000104386
2017-10-10T12:54:33.205893: step 1718, loss 0.197042, acc 0.9375, learning_rate 0.000104368
2017-10-10T12:54:33.750478: step 1719, loss 0.361596, acc 0.890625, learning_rate 0.00010435
2017-10-10T12:54:34.184949: step 1720, loss 0.233962, acc 0.9375, learning_rate 0.000104332

Evaluation:
2017-10-10T12:54:35.260877: step 1720, loss 0.235956, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1720

2017-10-10T12:54:36.813068: step 1721, loss 0.209215, acc 0.953125, learning_rate 0.000104315
2017-10-10T12:54:37.326311: step 1722, loss 0.187924, acc 0.9375, learning_rate 0.000104297
2017-10-10T12:54:37.792972: step 1723, loss 0.290625, acc 0.90625, learning_rate 0.000104279
2017-10-10T12:54:38.320238: step 1724, loss 0.0964593, acc 0.953125, learning_rate 0.000104262
2017-10-10T12:54:38.849005: step 1725, loss 0.315816, acc 0.859375, learning_rate 0.000104245
2017-10-10T12:54:39.461828: step 1726, loss 0.116354, acc 0.96875, learning_rate 0.000104227
2017-10-10T12:54:39.988782: step 1727, loss 0.129632, acc 0.96875, learning_rate 0.00010421
2017-10-10T12:54:40.540898: step 1728, loss 0.211437, acc 0.890625, learning_rate 0.000104193
2017-10-10T12:54:41.105006: step 1729, loss 0.200858, acc 0.953125, learning_rate 0.000104176
2017-10-10T12:54:41.612781: step 1730, loss 0.153536, acc 0.9375, learning_rate 0.000104159
2017-10-10T12:54:42.166071: step 1731, loss 0.0731014, acc 0.984375, learning_rate 0.000104142
2017-10-10T12:54:42.641241: step 1732, loss 0.205008, acc 0.90625, learning_rate 0.000104125
2017-10-10T12:54:43.144884: step 1733, loss 0.116766, acc 0.96875, learning_rate 0.000104108
2017-10-10T12:54:43.733123: step 1734, loss 0.1397, acc 0.953125, learning_rate 0.000104091
2017-10-10T12:54:44.198733: step 1735, loss 0.282312, acc 0.921875, learning_rate 0.000104074
2017-10-10T12:54:44.644701: step 1736, loss 0.145359, acc 0.9375, learning_rate 0.000104058
2017-10-10T12:54:45.152262: step 1737, loss 0.0899495, acc 0.953125, learning_rate 0.000104041
2017-10-10T12:54:45.637057: step 1738, loss 0.219315, acc 0.921875, learning_rate 0.000104025
2017-10-10T12:54:46.201162: step 1739, loss 0.254588, acc 0.90625, learning_rate 0.000104008
2017-10-10T12:54:46.749759: step 1740, loss 0.232845, acc 0.90625, learning_rate 0.000103992
2017-10-10T12:54:47.197006: step 1741, loss 0.0790702, acc 0.984375, learning_rate 0.000103976
2017-10-10T12:54:47.856678: step 1742, loss 0.196199, acc 0.9375, learning_rate 0.000103959
2017-10-10T12:54:48.315543: step 1743, loss 0.162367, acc 0.953125, learning_rate 0.000103943
2017-10-10T12:54:48.692858: step 1744, loss 0.248558, acc 0.90625, learning_rate 0.000103927
2017-10-10T12:54:49.169671: step 1745, loss 0.230043, acc 0.90625, learning_rate 0.000103911
2017-10-10T12:54:49.681228: step 1746, loss 0.277569, acc 0.890625, learning_rate 0.000103895
2017-10-10T12:54:50.204890: step 1747, loss 0.216387, acc 0.921875, learning_rate 0.000103879
2017-10-10T12:54:50.721508: step 1748, loss 0.14696, acc 0.96875, learning_rate 0.000103863
2017-10-10T12:54:51.296951: step 1749, loss 0.0935173, acc 0.96875, learning_rate 0.000103848
2017-10-10T12:54:51.803707: step 1750, loss 0.266921, acc 0.90625, learning_rate 0.000103832
2017-10-10T12:54:52.346166: step 1751, loss 0.24019, acc 0.90625, learning_rate 0.000103816
2017-10-10T12:54:52.893094: step 1752, loss 0.214664, acc 0.90625, learning_rate 0.000103801
2017-10-10T12:54:53.489278: step 1753, loss 0.233241, acc 0.953125, learning_rate 0.000103785
2017-10-10T12:54:53.924898: step 1754, loss 0.123847, acc 0.953125, learning_rate 0.00010377
2017-10-10T12:54:54.463295: step 1755, loss 0.089336, acc 0.984375, learning_rate 0.000103754
2017-10-10T12:54:55.020157: step 1756, loss 0.0848923, acc 0.96875, learning_rate 0.000103739
2017-10-10T12:54:55.527355: step 1757, loss 0.277972, acc 0.890625, learning_rate 0.000103724
2017-10-10T12:54:56.029283: step 1758, loss 0.201022, acc 0.90625, learning_rate 0.000103709
2017-10-10T12:54:56.581136: step 1759, loss 0.118424, acc 0.953125, learning_rate 0.000103694
2017-10-10T12:54:57.059223: step 1760, loss 0.148292, acc 0.953125, learning_rate 0.000103678

Evaluation:
2017-10-10T12:54:58.118682: step 1760, loss 0.229898, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1760

2017-10-10T12:54:59.897031: step 1761, loss 0.110223, acc 0.96875, learning_rate 0.000103663
2017-10-10T12:55:00.481907: step 1762, loss 0.0934825, acc 0.96875, learning_rate 0.000103648
2017-10-10T12:55:01.056962: step 1763, loss 0.216005, acc 0.921875, learning_rate 0.000103634
2017-10-10T12:55:01.492982: step 1764, loss 0.263943, acc 0.901961, learning_rate 0.000103619
2017-10-10T12:55:02.048964: step 1765, loss 0.193881, acc 0.921875, learning_rate 0.000103604
2017-10-10T12:55:02.593027: step 1766, loss 0.160365, acc 0.953125, learning_rate 0.000103589
2017-10-10T12:55:03.052953: step 1767, loss 0.301961, acc 0.890625, learning_rate 0.000103575
2017-10-10T12:55:03.601029: step 1768, loss 0.0887075, acc 0.96875, learning_rate 0.00010356
2017-10-10T12:55:04.133927: step 1769, loss 0.23576, acc 0.953125, learning_rate 0.000103545
2017-10-10T12:55:04.615365: step 1770, loss 0.109435, acc 0.953125, learning_rate 0.000103531
2017-10-10T12:55:05.159839: step 1771, loss 0.274437, acc 0.890625, learning_rate 0.000103517
2017-10-10T12:55:05.732944: step 1772, loss 0.267747, acc 0.875, learning_rate 0.000103502
2017-10-10T12:55:06.234653: step 1773, loss 0.136499, acc 0.96875, learning_rate 0.000103488
2017-10-10T12:55:06.904723: step 1774, loss 0.107089, acc 0.984375, learning_rate 0.000103474
2017-10-10T12:55:07.289005: step 1775, loss 0.112176, acc 0.953125, learning_rate 0.00010346
2017-10-10T12:55:07.721092: step 1776, loss 0.125607, acc 0.96875, learning_rate 0.000103445
2017-10-10T12:55:08.138659: step 1777, loss 0.0466516, acc 1, learning_rate 0.000103431
2017-10-10T12:55:08.681373: step 1778, loss 0.353132, acc 0.875, learning_rate 0.000103417
2017-10-10T12:55:09.228374: step 1779, loss 0.122375, acc 0.953125, learning_rate 0.000103403
2017-10-10T12:55:09.773104: step 1780, loss 0.193383, acc 0.859375, learning_rate 0.00010339
2017-10-10T12:55:10.400952: step 1781, loss 0.125714, acc 0.953125, learning_rate 0.000103376
2017-10-10T12:55:10.973281: step 1782, loss 0.0873009, acc 0.984375, learning_rate 0.000103362
2017-10-10T12:55:11.390340: step 1783, loss 0.176526, acc 0.953125, learning_rate 0.000103348
2017-10-10T12:55:11.764504: step 1784, loss 0.329677, acc 0.921875, learning_rate 0.000103335
2017-10-10T12:55:12.385128: step 1785, loss 0.151487, acc 0.9375, learning_rate 0.000103321
2017-10-10T12:55:12.936941: step 1786, loss 0.279382, acc 0.890625, learning_rate 0.000103307
2017-10-10T12:55:13.430413: step 1787, loss 0.111792, acc 0.96875, learning_rate 0.000103294
2017-10-10T12:55:13.992984: step 1788, loss 0.161149, acc 0.9375, learning_rate 0.00010328
2017-10-10T12:55:14.533026: step 1789, loss 0.160943, acc 0.921875, learning_rate 0.000103267
2017-10-10T12:55:15.072826: step 1790, loss 0.102885, acc 0.96875, learning_rate 0.000103254
2017-10-10T12:55:15.637584: step 1791, loss 0.207667, acc 0.9375, learning_rate 0.00010324
2017-10-10T12:55:16.163543: step 1792, loss 0.158524, acc 0.9375, learning_rate 0.000103227
2017-10-10T12:55:16.718247: step 1793, loss 0.135703, acc 0.9375, learning_rate 0.000103214
2017-10-10T12:55:17.237248: step 1794, loss 0.226504, acc 0.921875, learning_rate 0.000103201
2017-10-10T12:55:17.778884: step 1795, loss 0.140278, acc 0.953125, learning_rate 0.000103188
2017-10-10T12:55:18.310068: step 1796, loss 0.139534, acc 0.96875, learning_rate 0.000103175
2017-10-10T12:55:18.776995: step 1797, loss 0.160219, acc 0.96875, learning_rate 0.000103162
2017-10-10T12:55:19.445184: step 1798, loss 0.159728, acc 0.90625, learning_rate 0.000103149
2017-10-10T12:55:19.880104: step 1799, loss 0.265243, acc 0.90625, learning_rate 0.000103136
2017-10-10T12:55:20.367002: step 1800, loss 0.126947, acc 0.921875, learning_rate 0.000103123

Evaluation:
2017-10-10T12:55:21.572395: step 1800, loss 0.231639, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1800

2017-10-10T12:55:22.998718: step 1801, loss 0.183315, acc 0.921875, learning_rate 0.000103111
2017-10-10T12:55:23.457963: step 1802, loss 0.422424, acc 0.953125, learning_rate 0.000103098
2017-10-10T12:55:24.045293: step 1803, loss 0.138491, acc 0.9375, learning_rate 0.000103085
2017-10-10T12:55:24.580999: step 1804, loss 0.136389, acc 0.9375, learning_rate 0.000103073
2017-10-10T12:55:25.064846: step 1805, loss 0.123221, acc 0.96875, learning_rate 0.00010306
2017-10-10T12:55:25.620867: step 1806, loss 0.110562, acc 0.984375, learning_rate 0.000103048
2017-10-10T12:55:26.172277: step 1807, loss 0.297867, acc 0.90625, learning_rate 0.000103035
2017-10-10T12:55:26.712859: step 1808, loss 0.167413, acc 0.953125, learning_rate 0.000103023
2017-10-10T12:55:27.299619: step 1809, loss 0.186487, acc 0.921875, learning_rate 0.00010301
2017-10-10T12:55:27.803496: step 1810, loss 0.256031, acc 0.90625, learning_rate 0.000102998
2017-10-10T12:55:28.307729: step 1811, loss 0.207573, acc 0.890625, learning_rate 0.000102986
2017-10-10T12:55:28.828962: step 1812, loss 0.126829, acc 0.984375, learning_rate 0.000102974
2017-10-10T12:55:29.379915: step 1813, loss 0.258918, acc 0.9375, learning_rate 0.000102962
2017-10-10T12:55:29.901174: step 1814, loss 0.0991878, acc 0.984375, learning_rate 0.000102949
2017-10-10T12:55:30.416818: step 1815, loss 0.128538, acc 0.96875, learning_rate 0.000102937
2017-10-10T12:55:30.868527: step 1816, loss 0.0943736, acc 0.984375, learning_rate 0.000102925
2017-10-10T12:55:31.439753: step 1817, loss 0.189847, acc 0.9375, learning_rate 0.000102913
2017-10-10T12:55:31.957116: step 1818, loss 0.186471, acc 0.953125, learning_rate 0.000102902
2017-10-10T12:55:32.497257: step 1819, loss 0.375523, acc 0.90625, learning_rate 0.00010289
2017-10-10T12:55:33.048866: step 1820, loss 0.153466, acc 0.953125, learning_rate 0.000102878
2017-10-10T12:55:33.704890: step 1821, loss 0.127803, acc 0.953125, learning_rate 0.000102866
2017-10-10T12:55:34.264956: step 1822, loss 0.185052, acc 0.90625, learning_rate 0.000102855
2017-10-10T12:55:34.692853: step 1823, loss 0.179614, acc 0.953125, learning_rate 0.000102843
2017-10-10T12:55:35.144360: step 1824, loss 0.225023, acc 0.921875, learning_rate 0.000102831
2017-10-10T12:55:35.645010: step 1825, loss 0.130751, acc 0.953125, learning_rate 0.00010282
2017-10-10T12:55:36.145641: step 1826, loss 0.326868, acc 0.875, learning_rate 0.000102808
2017-10-10T12:55:36.720936: step 1827, loss 0.110898, acc 0.984375, learning_rate 0.000102797
2017-10-10T12:55:37.320957: step 1828, loss 0.0942985, acc 1, learning_rate 0.000102785
2017-10-10T12:55:37.832876: step 1829, loss 0.0735299, acc 0.984375, learning_rate 0.000102774
2017-10-10T12:55:38.349024: step 1830, loss 0.144088, acc 0.984375, learning_rate 0.000102763
2017-10-10T12:55:38.864945: step 1831, loss 0.104366, acc 0.96875, learning_rate 0.000102751
2017-10-10T12:55:39.407880: step 1832, loss 0.237898, acc 0.921875, learning_rate 0.00010274
2017-10-10T12:55:39.935995: step 1833, loss 0.240845, acc 0.921875, learning_rate 0.000102729
2017-10-10T12:55:40.497764: step 1834, loss 0.0575913, acc 0.984375, learning_rate 0.000102718
2017-10-10T12:55:41.090096: step 1835, loss 0.171506, acc 0.921875, learning_rate 0.000102707
2017-10-10T12:55:41.637884: step 1836, loss 0.23289, acc 0.921875, learning_rate 0.000102696
2017-10-10T12:55:42.157079: step 1837, loss 0.329243, acc 0.890625, learning_rate 0.000102685
2017-10-10T12:55:42.721104: step 1838, loss 0.282304, acc 0.90625, learning_rate 0.000102674
2017-10-10T12:55:43.191481: step 1839, loss 0.318767, acc 0.859375, learning_rate 0.000102663
2017-10-10T12:55:43.635385: step 1840, loss 0.207066, acc 0.921875, learning_rate 0.000102652

Evaluation:
2017-10-10T12:55:44.795361: step 1840, loss 0.229498, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1840

2017-10-10T12:55:46.343419: step 1841, loss 0.172308, acc 0.96875, learning_rate 0.000102641
2017-10-10T12:55:46.864882: step 1842, loss 0.241411, acc 0.890625, learning_rate 0.00010263
2017-10-10T12:55:47.397077: step 1843, loss 0.204763, acc 0.921875, learning_rate 0.00010262
2017-10-10T12:55:47.908431: step 1844, loss 0.103482, acc 0.984375, learning_rate 0.000102609
2017-10-10T12:55:48.473153: step 1845, loss 0.15541, acc 0.953125, learning_rate 0.000102598
2017-10-10T12:55:49.073056: step 1846, loss 0.313889, acc 0.890625, learning_rate 0.000102588
2017-10-10T12:55:49.569296: step 1847, loss 0.118989, acc 0.984375, learning_rate 0.000102577
2017-10-10T12:55:50.136977: step 1848, loss 0.10624, acc 0.96875, learning_rate 0.000102567
2017-10-10T12:55:50.663656: step 1849, loss 0.118432, acc 0.953125, learning_rate 0.000102556
2017-10-10T12:55:51.189075: step 1850, loss 0.142415, acc 0.90625, learning_rate 0.000102546
2017-10-10T12:55:51.692914: step 1851, loss 0.147498, acc 0.96875, learning_rate 0.000102535
2017-10-10T12:55:52.148834: step 1852, loss 0.342607, acc 0.875, learning_rate 0.000102525
2017-10-10T12:55:52.654170: step 1853, loss 0.205514, acc 0.921875, learning_rate 0.000102515
2017-10-10T12:55:53.257127: step 1854, loss 0.207086, acc 0.921875, learning_rate 0.000102504
2017-10-10T12:55:53.716841: step 1855, loss 0.111342, acc 0.953125, learning_rate 0.000102494
2017-10-10T12:55:54.211031: step 1856, loss 0.146204, acc 0.953125, learning_rate 0.000102484
2017-10-10T12:55:54.754971: step 1857, loss 0.187924, acc 0.921875, learning_rate 0.000102474
2017-10-10T12:55:55.299334: step 1858, loss 0.158327, acc 0.953125, learning_rate 0.000102464
2017-10-10T12:55:55.737047: step 1859, loss 0.153373, acc 0.9375, learning_rate 0.000102454
2017-10-10T12:55:56.244995: step 1860, loss 0.246659, acc 0.921875, learning_rate 0.000102444
2017-10-10T12:55:56.830495: step 1861, loss 0.109979, acc 0.984375, learning_rate 0.000102434
2017-10-10T12:55:57.236042: step 1862, loss 0.292018, acc 0.862745, learning_rate 0.000102424
2017-10-10T12:55:57.668797: step 1863, loss 0.12481, acc 0.96875, learning_rate 0.000102414
2017-10-10T12:55:58.165103: step 1864, loss 0.2429, acc 0.90625, learning_rate 0.000102404
2017-10-10T12:55:58.745138: step 1865, loss 0.0579844, acc 1, learning_rate 0.000102394
2017-10-10T12:55:59.228927: step 1866, loss 0.165457, acc 0.953125, learning_rate 0.000102384
2017-10-10T12:55:59.752859: step 1867, loss 0.139146, acc 0.9375, learning_rate 0.000102375
2017-10-10T12:56:00.252876: step 1868, loss 0.250346, acc 0.90625, learning_rate 0.000102365
2017-10-10T12:56:00.781214: step 1869, loss 0.226535, acc 0.9375, learning_rate 0.000102355
2017-10-10T12:56:01.305222: step 1870, loss 0.0516489, acc 0.984375, learning_rate 0.000102346
2017-10-10T12:56:01.873155: step 1871, loss 0.149874, acc 0.9375, learning_rate 0.000102336
2017-10-10T12:56:02.421133: step 1872, loss 0.154723, acc 0.921875, learning_rate 0.000102327
2017-10-10T12:56:02.941394: step 1873, loss 0.0647213, acc 1, learning_rate 0.000102317
2017-10-10T12:56:03.454319: step 1874, loss 0.319332, acc 0.921875, learning_rate 0.000102308
2017-10-10T12:56:03.961161: step 1875, loss 0.117592, acc 0.9375, learning_rate 0.000102298
2017-10-10T12:56:04.442953: step 1876, loss 0.160872, acc 0.96875, learning_rate 0.000102289
2017-10-10T12:56:04.999371: step 1877, loss 0.0996676, acc 0.96875, learning_rate 0.000102279
2017-10-10T12:56:05.528827: step 1878, loss 0.298333, acc 0.90625, learning_rate 0.00010227
2017-10-10T12:56:05.990313: step 1879, loss 0.0880458, acc 0.96875, learning_rate 0.000102261
2017-10-10T12:56:06.455260: step 1880, loss 0.144212, acc 0.921875, learning_rate 0.000102252

Evaluation:
2017-10-10T12:56:07.677054: step 1880, loss 0.227948, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1880

2017-10-10T12:56:09.437098: step 1881, loss 0.0927074, acc 0.96875, learning_rate 0.000102242
2017-10-10T12:56:09.996960: step 1882, loss 0.21523, acc 0.921875, learning_rate 0.000102233
2017-10-10T12:56:10.556512: step 1883, loss 0.0971399, acc 0.984375, learning_rate 0.000102224
2017-10-10T12:56:11.078190: step 1884, loss 0.322023, acc 0.859375, learning_rate 0.000102215
2017-10-10T12:56:11.594606: step 1885, loss 0.0881928, acc 0.984375, learning_rate 0.000102206
2017-10-10T12:56:12.112929: step 1886, loss 0.147028, acc 0.953125, learning_rate 0.000102197
2017-10-10T12:56:12.708961: step 1887, loss 0.0943117, acc 0.96875, learning_rate 0.000102188
2017-10-10T12:56:13.236872: step 1888, loss 0.237595, acc 0.921875, learning_rate 0.000102179
2017-10-10T12:56:13.776908: step 1889, loss 0.138527, acc 0.921875, learning_rate 0.00010217
2017-10-10T12:56:14.268886: step 1890, loss 0.221872, acc 0.9375, learning_rate 0.000102161
2017-10-10T12:56:14.798944: step 1891, loss 0.0994627, acc 0.96875, learning_rate 0.000102153
2017-10-10T12:56:15.324873: step 1892, loss 0.0779298, acc 0.984375, learning_rate 0.000102144
2017-10-10T12:56:15.972387: step 1893, loss 0.155748, acc 0.96875, learning_rate 0.000102135
2017-10-10T12:56:16.468842: step 1894, loss 0.210904, acc 0.921875, learning_rate 0.000102126
2017-10-10T12:56:16.926906: step 1895, loss 0.0623073, acc 1, learning_rate 0.000102118
2017-10-10T12:56:17.375824: step 1896, loss 0.103574, acc 0.953125, learning_rate 0.000102109
2017-10-10T12:56:17.916916: step 1897, loss 0.248731, acc 0.921875, learning_rate 0.0001021
2017-10-10T12:56:18.451158: step 1898, loss 0.235876, acc 0.890625, learning_rate 0.000102092
2017-10-10T12:56:18.940547: step 1899, loss 0.289142, acc 0.890625, learning_rate 0.000102083
2017-10-10T12:56:19.480851: step 1900, loss 0.157159, acc 0.984375, learning_rate 0.000102075
2017-10-10T12:56:19.998320: step 1901, loss 0.233301, acc 0.90625, learning_rate 0.000102066
2017-10-10T12:56:20.389790: step 1902, loss 0.0791669, acc 0.984375, learning_rate 0.000102058
2017-10-10T12:56:20.689115: step 1903, loss 0.1972, acc 0.9375, learning_rate 0.00010205
2017-10-10T12:56:21.254930: step 1904, loss 0.191241, acc 0.921875, learning_rate 0.000102041
2017-10-10T12:56:21.764623: step 1905, loss 0.175215, acc 0.9375, learning_rate 0.000102033
2017-10-10T12:56:22.325289: step 1906, loss 0.158625, acc 0.96875, learning_rate 0.000102025
2017-10-10T12:56:22.934944: step 1907, loss 0.0824717, acc 0.96875, learning_rate 0.000102016
2017-10-10T12:56:23.540816: step 1908, loss 0.185614, acc 0.9375, learning_rate 0.000102008
2017-10-10T12:56:24.119695: step 1909, loss 0.095022, acc 0.96875, learning_rate 0.000102
2017-10-10T12:56:24.696828: step 1910, loss 0.123654, acc 0.9375, learning_rate 0.000101992
2017-10-10T12:56:25.268867: step 1911, loss 0.322993, acc 0.875, learning_rate 0.000101984
2017-10-10T12:56:25.836957: step 1912, loss 0.0896715, acc 0.984375, learning_rate 0.000101975
2017-10-10T12:56:26.355612: step 1913, loss 0.198203, acc 0.953125, learning_rate 0.000101967
2017-10-10T12:56:26.839349: step 1914, loss 0.148717, acc 0.953125, learning_rate 0.000101959
2017-10-10T12:56:27.340893: step 1915, loss 0.204942, acc 0.9375, learning_rate 0.000101951
2017-10-10T12:56:27.849462: step 1916, loss 0.221582, acc 0.9375, learning_rate 0.000101943
2017-10-10T12:56:28.458062: step 1917, loss 0.173588, acc 0.9375, learning_rate 0.000101935
2017-10-10T12:56:28.935152: step 1918, loss 0.143476, acc 0.96875, learning_rate 0.000101928
2017-10-10T12:56:29.309190: step 1919, loss 0.163497, acc 0.953125, learning_rate 0.00010192
2017-10-10T12:56:29.732107: step 1920, loss 0.28333, acc 0.890625, learning_rate 0.000101912

Evaluation:
2017-10-10T12:56:30.874980: step 1920, loss 0.230403, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1920

2017-10-10T12:56:32.421630: step 1921, loss 0.155378, acc 0.984375, learning_rate 0.000101904
2017-10-10T12:56:32.994318: step 1922, loss 0.280453, acc 0.890625, learning_rate 0.000101896
2017-10-10T12:56:33.568329: step 1923, loss 0.223889, acc 0.9375, learning_rate 0.000101889
2017-10-10T12:56:34.031765: step 1924, loss 0.189492, acc 0.921875, learning_rate 0.000101881
2017-10-10T12:56:34.664799: step 1925, loss 0.0933448, acc 0.984375, learning_rate 0.000101873
2017-10-10T12:56:35.244514: step 1926, loss 0.239956, acc 0.9375, learning_rate 0.000101865
2017-10-10T12:56:35.780360: step 1927, loss 0.349117, acc 0.875, learning_rate 0.000101858
2017-10-10T12:56:36.350554: step 1928, loss 0.233009, acc 0.890625, learning_rate 0.00010185
2017-10-10T12:56:36.873788: step 1929, loss 0.103449, acc 0.96875, learning_rate 0.000101843
2017-10-10T12:56:37.424486: step 1930, loss 0.0697844, acc 0.984375, learning_rate 0.000101835
2017-10-10T12:56:37.980382: step 1931, loss 0.123011, acc 0.96875, learning_rate 0.000101828
2017-10-10T12:56:38.520053: step 1932, loss 0.178729, acc 0.9375, learning_rate 0.00010182
2017-10-10T12:56:39.070980: step 1933, loss 0.148435, acc 0.921875, learning_rate 0.000101813
2017-10-10T12:56:39.706654: step 1934, loss 0.209658, acc 0.921875, learning_rate 0.000101805
2017-10-10T12:56:40.112826: step 1935, loss 0.205422, acc 0.9375, learning_rate 0.000101798
2017-10-10T12:56:40.496166: step 1936, loss 0.147833, acc 0.953125, learning_rate 0.000101791
2017-10-10T12:56:40.980912: step 1937, loss 0.117965, acc 0.953125, learning_rate 0.000101783
2017-10-10T12:56:41.441035: step 1938, loss 0.0976504, acc 0.96875, learning_rate 0.000101776
2017-10-10T12:56:41.964895: step 1939, loss 0.176973, acc 0.921875, learning_rate 0.000101769
2017-10-10T12:56:42.493023: step 1940, loss 0.156676, acc 0.9375, learning_rate 0.000101762
2017-10-10T12:56:43.116959: step 1941, loss 0.0845296, acc 0.984375, learning_rate 0.000101754
2017-10-10T12:56:43.516908: step 1942, loss 0.117709, acc 0.96875, learning_rate 0.000101747
2017-10-10T12:56:43.917048: step 1943, loss 0.245281, acc 0.9375, learning_rate 0.00010174
2017-10-10T12:56:44.388938: step 1944, loss 0.101805, acc 0.984375, learning_rate 0.000101733
2017-10-10T12:56:44.901104: step 1945, loss 0.203932, acc 0.9375, learning_rate 0.000101726
2017-10-10T12:56:45.448852: step 1946, loss 0.186907, acc 0.9375, learning_rate 0.000101719
2017-10-10T12:56:45.934565: step 1947, loss 0.282973, acc 0.921875, learning_rate 0.000101712
2017-10-10T12:56:46.391833: step 1948, loss 0.186062, acc 0.953125, learning_rate 0.000101705
2017-10-10T12:56:46.980929: step 1949, loss 0.190024, acc 0.9375, learning_rate 0.000101698
2017-10-10T12:56:47.467572: step 1950, loss 0.256351, acc 0.921875, learning_rate 0.000101691
2017-10-10T12:56:48.065224: step 1951, loss 0.170768, acc 0.921875, learning_rate 0.000101684
2017-10-10T12:56:48.584303: step 1952, loss 0.225438, acc 0.921875, learning_rate 0.000101677
2017-10-10T12:56:49.044864: step 1953, loss 0.08167, acc 0.953125, learning_rate 0.00010167
2017-10-10T12:56:49.524892: step 1954, loss 0.169817, acc 0.96875, learning_rate 0.000101664
2017-10-10T12:56:50.053878: step 1955, loss 0.188693, acc 0.90625, learning_rate 0.000101657
2017-10-10T12:56:50.588524: step 1956, loss 0.0895568, acc 0.984375, learning_rate 0.00010165
2017-10-10T12:56:51.144877: step 1957, loss 0.0917068, acc 0.96875, learning_rate 0.000101643
2017-10-10T12:56:51.785018: step 1958, loss 0.223595, acc 0.921875, learning_rate 0.000101637
2017-10-10T12:56:52.256875: step 1959, loss 0.188023, acc 0.921875, learning_rate 0.00010163
2017-10-10T12:56:52.644338: step 1960, loss 0.181817, acc 0.921569, learning_rate 0.000101623

Evaluation:
2017-10-10T12:56:53.747576: step 1960, loss 0.23067, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-1960

2017-10-10T12:56:55.359506: step 1961, loss 0.0891927, acc 0.953125, learning_rate 0.000101617
2017-10-10T12:56:55.891512: step 1962, loss 0.207413, acc 0.953125, learning_rate 0.00010161
2017-10-10T12:56:56.388488: step 1963, loss 0.225161, acc 0.90625, learning_rate 0.000101604
2017-10-10T12:56:56.873014: step 1964, loss 0.282264, acc 0.875, learning_rate 0.000101597
2017-10-10T12:56:57.418884: step 1965, loss 0.192743, acc 0.90625, learning_rate 0.00010159
2017-10-10T12:56:58.012105: step 1966, loss 0.203047, acc 0.90625, learning_rate 0.000101584
2017-10-10T12:56:58.546625: step 1967, loss 0.158279, acc 0.921875, learning_rate 0.000101577
2017-10-10T12:56:59.040138: step 1968, loss 0.172809, acc 0.96875, learning_rate 0.000101571
2017-10-10T12:56:59.531231: step 1969, loss 0.228089, acc 0.890625, learning_rate 0.000101565
2017-10-10T12:57:00.084959: step 1970, loss 0.082786, acc 0.96875, learning_rate 0.000101558
2017-10-10T12:57:00.676414: step 1971, loss 0.137907, acc 0.953125, learning_rate 0.000101552
2017-10-10T12:57:01.160061: step 1972, loss 0.165395, acc 0.953125, learning_rate 0.000101546
2017-10-10T12:57:01.649006: step 1973, loss 0.419316, acc 0.875, learning_rate 0.000101539
2017-10-10T12:57:02.264895: step 1974, loss 0.140736, acc 0.9375, learning_rate 0.000101533
2017-10-10T12:57:02.808505: step 1975, loss 0.293264, acc 0.921875, learning_rate 0.000101527
2017-10-10T12:57:03.254072: step 1976, loss 0.225792, acc 0.890625, learning_rate 0.00010152
2017-10-10T12:57:03.670811: step 1977, loss 0.0987677, acc 0.984375, learning_rate 0.000101514
2017-10-10T12:57:04.107831: step 1978, loss 0.0830567, acc 0.953125, learning_rate 0.000101508
2017-10-10T12:57:04.628886: step 1979, loss 0.252181, acc 0.921875, learning_rate 0.000101502
2017-10-10T12:57:05.124954: step 1980, loss 0.237658, acc 0.90625, learning_rate 0.000101496
2017-10-10T12:57:05.713568: step 1981, loss 0.166072, acc 0.9375, learning_rate 0.00010149
2017-10-10T12:57:06.201337: step 1982, loss 0.145285, acc 0.953125, learning_rate 0.000101484
2017-10-10T12:57:06.593701: step 1983, loss 0.213956, acc 0.90625, learning_rate 0.000101478
2017-10-10T12:57:07.101146: step 1984, loss 0.23243, acc 0.890625, learning_rate 0.000101472
2017-10-10T12:57:07.609205: step 1985, loss 0.143742, acc 0.9375, learning_rate 0.000101466
2017-10-10T12:57:08.050930: step 1986, loss 0.225964, acc 0.890625, learning_rate 0.00010146
2017-10-10T12:57:08.598124: step 1987, loss 0.163558, acc 0.9375, learning_rate 0.000101454
2017-10-10T12:57:09.098261: step 1988, loss 0.277721, acc 0.953125, learning_rate 0.000101448
2017-10-10T12:57:09.647964: step 1989, loss 0.179071, acc 0.921875, learning_rate 0.000101442
2017-10-10T12:57:10.201301: step 1990, loss 0.115218, acc 0.96875, learning_rate 0.000101436
2017-10-10T12:57:10.764808: step 1991, loss 0.101268, acc 0.96875, learning_rate 0.00010143
2017-10-10T12:57:11.330973: step 1992, loss 0.119078, acc 0.984375, learning_rate 0.000101424
2017-10-10T12:57:11.883395: step 1993, loss 0.266696, acc 0.890625, learning_rate 0.000101418
2017-10-10T12:57:12.479962: step 1994, loss 0.241301, acc 0.875, learning_rate 0.000101413
2017-10-10T12:57:13.044857: step 1995, loss 0.103119, acc 0.953125, learning_rate 0.000101407
2017-10-10T12:57:13.645843: step 1996, loss 0.115866, acc 0.984375, learning_rate 0.000101401
2017-10-10T12:57:14.208881: step 1997, loss 0.184875, acc 0.921875, learning_rate 0.000101395
2017-10-10T12:57:14.825069: step 1998, loss 0.146737, acc 0.9375, learning_rate 0.00010139
2017-10-10T12:57:15.284835: step 1999, loss 0.165423, acc 0.9375, learning_rate 0.000101384
2017-10-10T12:57:15.740264: step 2000, loss 0.147545, acc 0.96875, learning_rate 0.000101378

Evaluation:
2017-10-10T12:57:16.928360: step 2000, loss 0.232672, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2000

2017-10-10T12:57:18.460982: step 2001, loss 0.0942356, acc 0.96875, learning_rate 0.000101373
2017-10-10T12:57:18.941068: step 2002, loss 0.233201, acc 0.890625, learning_rate 0.000101367
2017-10-10T12:57:19.409118: step 2003, loss 0.141349, acc 0.953125, learning_rate 0.000101362
2017-10-10T12:57:19.916497: step 2004, loss 0.291871, acc 0.90625, learning_rate 0.000101356
2017-10-10T12:57:20.422647: step 2005, loss 0.228619, acc 0.90625, learning_rate 0.00010135
2017-10-10T12:57:20.944938: step 2006, loss 0.133464, acc 0.9375, learning_rate 0.000101345
2017-10-10T12:57:21.560875: step 2007, loss 0.179522, acc 0.890625, learning_rate 0.000101339
2017-10-10T12:57:22.088834: step 2008, loss 0.239858, acc 0.90625, learning_rate 0.000101334
2017-10-10T12:57:22.635868: step 2009, loss 0.125198, acc 0.96875, learning_rate 0.000101328
2017-10-10T12:57:23.214259: step 2010, loss 0.355502, acc 0.859375, learning_rate 0.000101323
2017-10-10T12:57:23.704910: step 2011, loss 0.256192, acc 0.921875, learning_rate 0.000101318
2017-10-10T12:57:24.252276: step 2012, loss 0.1969, acc 0.90625, learning_rate 0.000101312
2017-10-10T12:57:24.730663: step 2013, loss 0.168623, acc 0.9375, learning_rate 0.000101307
2017-10-10T12:57:25.197392: step 2014, loss 0.119834, acc 0.96875, learning_rate 0.000101302
2017-10-10T12:57:25.773052: step 2015, loss 0.213575, acc 0.921875, learning_rate 0.000101296
2017-10-10T12:57:26.365001: step 2016, loss 0.172942, acc 0.953125, learning_rate 0.000101291
2017-10-10T12:57:26.797041: step 2017, loss 0.253051, acc 0.9375, learning_rate 0.000101286
2017-10-10T12:57:27.277113: step 2018, loss 0.165463, acc 0.953125, learning_rate 0.00010128
2017-10-10T12:57:27.784955: step 2019, loss 0.0549948, acc 1, learning_rate 0.000101275
2017-10-10T12:57:28.256980: step 2020, loss 0.174945, acc 0.953125, learning_rate 0.00010127
2017-10-10T12:57:28.777174: step 2021, loss 0.262442, acc 0.890625, learning_rate 0.000101265
2017-10-10T12:57:29.275050: step 2022, loss 0.161404, acc 0.9375, learning_rate 0.00010126
2017-10-10T12:57:29.671929: step 2023, loss 0.138548, acc 0.96875, learning_rate 0.000101255
2017-10-10T12:57:30.068818: step 2024, loss 0.101632, acc 0.96875, learning_rate 0.000101249
2017-10-10T12:57:30.625573: step 2025, loss 0.185574, acc 0.921875, learning_rate 0.000101244
2017-10-10T12:57:31.217348: step 2026, loss 0.093912, acc 0.96875, learning_rate 0.000101239
2017-10-10T12:57:31.694678: step 2027, loss 0.10377, acc 0.953125, learning_rate 0.000101234
2017-10-10T12:57:32.257078: step 2028, loss 0.0977784, acc 0.96875, learning_rate 0.000101229
2017-10-10T12:57:32.738707: step 2029, loss 0.0954214, acc 0.96875, learning_rate 0.000101224
2017-10-10T12:57:33.279180: step 2030, loss 0.206718, acc 0.90625, learning_rate 0.000101219
2017-10-10T12:57:33.856914: step 2031, loss 0.13709, acc 0.921875, learning_rate 0.000101214
2017-10-10T12:57:34.330947: step 2032, loss 0.146865, acc 0.9375, learning_rate 0.000101209
2017-10-10T12:57:34.885042: step 2033, loss 0.0877978, acc 0.953125, learning_rate 0.000101204
2017-10-10T12:57:35.432502: step 2034, loss 0.184757, acc 0.96875, learning_rate 0.000101199
2017-10-10T12:57:35.958731: step 2035, loss 0.0851299, acc 0.96875, learning_rate 0.000101194
2017-10-10T12:57:36.488947: step 2036, loss 0.126433, acc 0.96875, learning_rate 0.00010119
2017-10-10T12:57:37.096939: step 2037, loss 0.220282, acc 0.9375, learning_rate 0.000101185
2017-10-10T12:57:37.708157: step 2038, loss 0.226451, acc 0.90625, learning_rate 0.00010118
2017-10-10T12:57:38.160233: step 2039, loss 0.153235, acc 0.9375, learning_rate 0.000101175
2017-10-10T12:57:38.553000: step 2040, loss 0.294104, acc 0.90625, learning_rate 0.00010117

Evaluation:
2017-10-10T12:57:39.820904: step 2040, loss 0.230834, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2040

2017-10-10T12:57:41.616993: step 2041, loss 0.191282, acc 0.9375, learning_rate 0.000101166
2017-10-10T12:57:42.144901: step 2042, loss 0.23464, acc 0.921875, learning_rate 0.000101161
2017-10-10T12:57:42.649069: step 2043, loss 0.229178, acc 0.90625, learning_rate 0.000101156
2017-10-10T12:57:43.173166: step 2044, loss 0.0889905, acc 0.96875, learning_rate 0.000101151
2017-10-10T12:57:43.721061: step 2045, loss 0.166441, acc 0.96875, learning_rate 0.000101147
2017-10-10T12:57:44.275515: step 2046, loss 0.0787885, acc 0.984375, learning_rate 0.000101142
2017-10-10T12:57:44.798680: step 2047, loss 0.148391, acc 0.953125, learning_rate 0.000101137
2017-10-10T12:57:45.172893: step 2048, loss 0.18846, acc 0.921875, learning_rate 0.000101133
2017-10-10T12:57:45.720934: step 2049, loss 0.260152, acc 0.890625, learning_rate 0.000101128
2017-10-10T12:57:46.176905: step 2050, loss 0.235077, acc 0.921875, learning_rate 0.000101123
2017-10-10T12:57:46.728880: step 2051, loss 0.249983, acc 0.90625, learning_rate 0.000101119
2017-10-10T12:57:47.236126: step 2052, loss 0.0799174, acc 1, learning_rate 0.000101114
2017-10-10T12:57:47.798536: step 2053, loss 0.118911, acc 0.953125, learning_rate 0.00010111
2017-10-10T12:57:48.345095: step 2054, loss 0.121175, acc 0.953125, learning_rate 0.000101105
2017-10-10T12:57:48.919636: step 2055, loss 0.342839, acc 0.859375, learning_rate 0.000101101
2017-10-10T12:57:49.425402: step 2056, loss 0.129722, acc 0.984375, learning_rate 0.000101096
2017-10-10T12:57:49.883468: step 2057, loss 0.150512, acc 0.96875, learning_rate 0.000101092
2017-10-10T12:57:50.340860: step 2058, loss 0.282693, acc 0.882353, learning_rate 0.000101087
2017-10-10T12:57:50.854398: step 2059, loss 0.0622391, acc 1, learning_rate 0.000101083
2017-10-10T12:57:51.305045: step 2060, loss 0.0993504, acc 0.96875, learning_rate 0.000101078
2017-10-10T12:57:51.912871: step 2061, loss 0.29067, acc 0.90625, learning_rate 0.000101074
2017-10-10T12:57:52.412854: step 2062, loss 0.241188, acc 0.90625, learning_rate 0.00010107
2017-10-10T12:57:52.900916: step 2063, loss 0.192051, acc 0.9375, learning_rate 0.000101065
2017-10-10T12:57:53.376831: step 2064, loss 0.0696687, acc 0.984375, learning_rate 0.000101061
2017-10-10T12:57:53.902419: step 2065, loss 0.139864, acc 0.953125, learning_rate 0.000101057
2017-10-10T12:57:54.385356: step 2066, loss 0.144003, acc 0.96875, learning_rate 0.000101052
2017-10-10T12:57:54.892000: step 2067, loss 0.270651, acc 0.9375, learning_rate 0.000101048
2017-10-10T12:57:55.400908: step 2068, loss 0.0805172, acc 0.984375, learning_rate 0.000101044
2017-10-10T12:57:55.893079: step 2069, loss 0.143776, acc 0.953125, learning_rate 0.000101039
2017-10-10T12:57:56.384436: step 2070, loss 0.19174, acc 0.953125, learning_rate 0.000101035
2017-10-10T12:57:56.915954: step 2071, loss 0.236547, acc 0.90625, learning_rate 0.000101031
2017-10-10T12:57:57.428857: step 2072, loss 0.185062, acc 0.953125, learning_rate 0.000101027
2017-10-10T12:57:57.943292: step 2073, loss 0.21819, acc 0.9375, learning_rate 0.000101023
2017-10-10T12:57:58.444831: step 2074, loss 0.195396, acc 0.921875, learning_rate 0.000101018
2017-10-10T12:57:59.022318: step 2075, loss 0.156723, acc 0.9375, learning_rate 0.000101014
2017-10-10T12:57:59.545804: step 2076, loss 0.2203, acc 0.9375, learning_rate 0.00010101
2017-10-10T12:58:00.066732: step 2077, loss 0.14367, acc 0.953125, learning_rate 0.000101006
2017-10-10T12:58:00.628894: step 2078, loss 0.352602, acc 0.875, learning_rate 0.000101002
2017-10-10T12:58:01.225048: step 2079, loss 0.159493, acc 0.96875, learning_rate 0.000100998
2017-10-10T12:58:01.697185: step 2080, loss 0.288966, acc 0.890625, learning_rate 0.000100994

Evaluation:
2017-10-10T12:58:02.756868: step 2080, loss 0.230044, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2080

2017-10-10T12:58:04.226932: step 2081, loss 0.294832, acc 0.921875, learning_rate 0.00010099
2017-10-10T12:58:04.760890: step 2082, loss 0.152945, acc 0.9375, learning_rate 0.000100986
2017-10-10T12:58:05.300941: step 2083, loss 0.289651, acc 0.921875, learning_rate 0.000100982
2017-10-10T12:58:05.827723: step 2084, loss 0.0801602, acc 0.96875, learning_rate 0.000100978
2017-10-10T12:58:06.354423: step 2085, loss 0.129684, acc 0.96875, learning_rate 0.000100974
2017-10-10T12:58:06.872806: step 2086, loss 0.161923, acc 0.953125, learning_rate 0.00010097
2017-10-10T12:58:07.368761: step 2087, loss 0.118402, acc 0.96875, learning_rate 0.000100966
2017-10-10T12:58:07.902917: step 2088, loss 0.148936, acc 0.921875, learning_rate 0.000100962
2017-10-10T12:58:08.384140: step 2089, loss 0.191027, acc 0.984375, learning_rate 0.000100958
2017-10-10T12:58:08.925137: step 2090, loss 0.157609, acc 0.9375, learning_rate 0.000100954
2017-10-10T12:58:09.484901: step 2091, loss 0.205602, acc 0.921875, learning_rate 0.00010095
2017-10-10T12:58:10.032834: step 2092, loss 0.117521, acc 0.953125, learning_rate 0.000100946
2017-10-10T12:58:10.565507: step 2093, loss 0.213972, acc 0.90625, learning_rate 0.000100942
2017-10-10T12:58:11.012949: step 2094, loss 0.260431, acc 0.875, learning_rate 0.000100938
2017-10-10T12:58:11.593207: step 2095, loss 0.190498, acc 0.96875, learning_rate 0.000100935
2017-10-10T12:58:12.106973: step 2096, loss 0.135448, acc 0.953125, learning_rate 0.000100931
2017-10-10T12:58:12.455429: step 2097, loss 0.179502, acc 0.9375, learning_rate 0.000100927
2017-10-10T12:58:12.881728: step 2098, loss 0.246496, acc 0.890625, learning_rate 0.000100923
2017-10-10T12:58:13.438663: step 2099, loss 0.232283, acc 0.90625, learning_rate 0.000100919
2017-10-10T12:58:13.964954: step 2100, loss 0.21167, acc 0.890625, learning_rate 0.000100916
2017-10-10T12:58:14.433449: step 2101, loss 0.042248, acc 1, learning_rate 0.000100912
2017-10-10T12:58:14.944957: step 2102, loss 0.108398, acc 0.96875, learning_rate 0.000100908
2017-10-10T12:58:15.556823: step 2103, loss 0.129096, acc 0.921875, learning_rate 0.000100904
2017-10-10T12:58:15.963456: step 2104, loss 0.18855, acc 0.9375, learning_rate 0.000100901
2017-10-10T12:58:16.376114: step 2105, loss 0.219313, acc 0.90625, learning_rate 0.000100897
2017-10-10T12:58:16.865120: step 2106, loss 0.121199, acc 0.96875, learning_rate 0.000100893
2017-10-10T12:58:17.307994: step 2107, loss 0.223365, acc 0.921875, learning_rate 0.00010089
2017-10-10T12:58:17.779659: step 2108, loss 0.202922, acc 0.9375, learning_rate 0.000100886
2017-10-10T12:58:18.260807: step 2109, loss 0.120321, acc 0.96875, learning_rate 0.000100883
2017-10-10T12:58:18.772814: step 2110, loss 0.20284, acc 0.9375, learning_rate 0.000100879
2017-10-10T12:58:19.291132: step 2111, loss 0.283282, acc 0.953125, learning_rate 0.000100875
2017-10-10T12:58:19.839425: step 2112, loss 0.276976, acc 0.953125, learning_rate 0.000100872
2017-10-10T12:58:20.348492: step 2113, loss 0.132476, acc 0.953125, learning_rate 0.000100868
2017-10-10T12:58:20.859155: step 2114, loss 0.222026, acc 0.9375, learning_rate 0.000100865
2017-10-10T12:58:21.384844: step 2115, loss 0.164134, acc 0.9375, learning_rate 0.000100861
2017-10-10T12:58:21.884433: step 2116, loss 0.131401, acc 0.921875, learning_rate 0.000100858
2017-10-10T12:58:22.353044: step 2117, loss 0.136594, acc 0.96875, learning_rate 0.000100854
2017-10-10T12:58:22.864940: step 2118, loss 0.266393, acc 0.921875, learning_rate 0.000100851
2017-10-10T12:58:23.420944: step 2119, loss 0.252302, acc 0.90625, learning_rate 0.000100847
2017-10-10T12:58:24.017019: step 2120, loss 0.268577, acc 0.921875, learning_rate 0.000100844

Evaluation:
2017-10-10T12:58:24.980886: step 2120, loss 0.230594, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2120

2017-10-10T12:58:26.628841: step 2121, loss 0.0820886, acc 0.984375, learning_rate 0.00010084
2017-10-10T12:58:27.182668: step 2122, loss 0.223119, acc 0.90625, learning_rate 0.000100837
2017-10-10T12:58:27.708878: step 2123, loss 0.0437063, acc 1, learning_rate 0.000100833
2017-10-10T12:58:28.292947: step 2124, loss 0.127532, acc 0.953125, learning_rate 0.00010083
2017-10-10T12:58:28.805111: step 2125, loss 0.0984364, acc 0.953125, learning_rate 0.000100827
2017-10-10T12:58:29.309685: step 2126, loss 0.186615, acc 0.921875, learning_rate 0.000100823
2017-10-10T12:58:29.873005: step 2127, loss 0.163562, acc 0.96875, learning_rate 0.00010082
2017-10-10T12:58:30.426882: step 2128, loss 0.142731, acc 0.953125, learning_rate 0.000100817
2017-10-10T12:58:31.045088: step 2129, loss 0.141793, acc 0.953125, learning_rate 0.000100813
2017-10-10T12:58:31.610629: step 2130, loss 0.139016, acc 0.96875, learning_rate 0.00010081
2017-10-10T12:58:32.100197: step 2131, loss 0.146144, acc 0.921875, learning_rate 0.000100807
2017-10-10T12:58:32.620777: step 2132, loss 0.157405, acc 0.953125, learning_rate 0.000100803
2017-10-10T12:58:33.120882: step 2133, loss 0.159718, acc 0.9375, learning_rate 0.0001008
2017-10-10T12:58:33.592909: step 2134, loss 0.147304, acc 0.953125, learning_rate 0.000100797
2017-10-10T12:58:34.036059: step 2135, loss 0.141372, acc 0.921875, learning_rate 0.000100793
2017-10-10T12:58:34.579794: step 2136, loss 0.171141, acc 0.9375, learning_rate 0.00010079
2017-10-10T12:58:35.131901: step 2137, loss 0.295331, acc 0.890625, learning_rate 0.000100787
2017-10-10T12:58:35.574590: step 2138, loss 0.148715, acc 0.953125, learning_rate 0.000100784
2017-10-10T12:58:36.046140: step 2139, loss 0.148257, acc 0.953125, learning_rate 0.000100781
2017-10-10T12:58:36.595993: step 2140, loss 0.132843, acc 0.9375, learning_rate 0.000100777
2017-10-10T12:58:37.093211: step 2141, loss 0.109264, acc 0.96875, learning_rate 0.000100774
2017-10-10T12:58:37.640946: step 2142, loss 0.0886004, acc 0.984375, learning_rate 0.000100771
2017-10-10T12:58:38.233083: step 2143, loss 0.256064, acc 0.921875, learning_rate 0.000100768
2017-10-10T12:58:38.734093: step 2144, loss 0.16103, acc 0.9375, learning_rate 0.000100765
2017-10-10T12:58:39.137768: step 2145, loss 0.187214, acc 0.921875, learning_rate 0.000100762
2017-10-10T12:58:39.496893: step 2146, loss 0.196398, acc 0.953125, learning_rate 0.000100759
2017-10-10T12:58:40.057028: step 2147, loss 0.08843, acc 0.984375, learning_rate 0.000100755
2017-10-10T12:58:40.524944: step 2148, loss 0.188455, acc 0.9375, learning_rate 0.000100752
2017-10-10T12:58:41.080973: step 2149, loss 0.202712, acc 0.9375, learning_rate 0.000100749
2017-10-10T12:58:41.623581: step 2150, loss 0.217688, acc 0.921875, learning_rate 0.000100746
2017-10-10T12:58:42.192957: step 2151, loss 0.2842, acc 0.90625, learning_rate 0.000100743
2017-10-10T12:58:42.696952: step 2152, loss 0.264579, acc 0.90625, learning_rate 0.00010074
2017-10-10T12:58:43.216781: step 2153, loss 0.292375, acc 0.890625, learning_rate 0.000100737
2017-10-10T12:58:43.757379: step 2154, loss 0.330061, acc 0.90625, learning_rate 0.000100734
2017-10-10T12:58:44.304914: step 2155, loss 0.146516, acc 0.9375, learning_rate 0.000100731
2017-10-10T12:58:44.773075: step 2156, loss 0.128285, acc 0.960784, learning_rate 0.000100728
2017-10-10T12:58:45.216254: step 2157, loss 0.131645, acc 0.953125, learning_rate 0.000100725
2017-10-10T12:58:45.697623: step 2158, loss 0.166605, acc 0.9375, learning_rate 0.000100722
2017-10-10T12:58:46.240990: step 2159, loss 0.19281, acc 0.9375, learning_rate 0.000100719
2017-10-10T12:58:46.916523: step 2160, loss 0.221685, acc 0.90625, learning_rate 0.000100716

Evaluation:
2017-10-10T12:58:47.794347: step 2160, loss 0.227746, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2160

2017-10-10T12:58:49.481570: step 2161, loss 0.172161, acc 0.953125, learning_rate 0.000100713
2017-10-10T12:58:50.028964: step 2162, loss 0.14372, acc 0.96875, learning_rate 0.000100711
2017-10-10T12:58:50.557056: step 2163, loss 0.100013, acc 0.953125, learning_rate 0.000100708
2017-10-10T12:58:51.001146: step 2164, loss 0.216675, acc 0.921875, learning_rate 0.000100705
2017-10-10T12:58:51.467278: step 2165, loss 0.198498, acc 0.921875, learning_rate 0.000100702
2017-10-10T12:58:51.943949: step 2166, loss 0.182548, acc 0.9375, learning_rate 0.000100699
2017-10-10T12:58:52.523261: step 2167, loss 0.185524, acc 0.90625, learning_rate 0.000100696
2017-10-10T12:58:53.054484: step 2168, loss 0.11959, acc 0.984375, learning_rate 0.000100693
2017-10-10T12:58:53.591688: step 2169, loss 0.267753, acc 0.90625, learning_rate 0.00010069
2017-10-10T12:58:54.109930: step 2170, loss 0.115112, acc 0.953125, learning_rate 0.000100688
2017-10-10T12:58:54.604922: step 2171, loss 0.149529, acc 0.921875, learning_rate 0.000100685
2017-10-10T12:58:55.148868: step 2172, loss 0.197701, acc 0.90625, learning_rate 0.000100682
2017-10-10T12:58:55.628059: step 2173, loss 0.198088, acc 0.9375, learning_rate 0.000100679
2017-10-10T12:58:56.160857: step 2174, loss 0.169362, acc 0.953125, learning_rate 0.000100677
2017-10-10T12:58:56.673153: step 2175, loss 0.197825, acc 0.9375, learning_rate 0.000100674
2017-10-10T12:58:57.229215: step 2176, loss 0.170199, acc 0.9375, learning_rate 0.000100671
2017-10-10T12:58:57.860196: step 2177, loss 0.0985296, acc 0.953125, learning_rate 0.000100668
2017-10-10T12:58:58.316950: step 2178, loss 0.155386, acc 0.921875, learning_rate 0.000100666
2017-10-10T12:58:58.737027: step 2179, loss 0.175288, acc 0.921875, learning_rate 0.000100663
2017-10-10T12:58:59.186066: step 2180, loss 0.131803, acc 0.984375, learning_rate 0.00010066
2017-10-10T12:58:59.752947: step 2181, loss 0.310628, acc 0.90625, learning_rate 0.000100657
2017-10-10T12:59:00.320934: step 2182, loss 0.224242, acc 0.921875, learning_rate 0.000100655
2017-10-10T12:59:00.896165: step 2183, loss 0.119352, acc 0.9375, learning_rate 0.000100652
2017-10-10T12:59:01.345411: step 2184, loss 0.195782, acc 0.90625, learning_rate 0.000100649
2017-10-10T12:59:01.794196: step 2185, loss 0.0618709, acc 1, learning_rate 0.000100647
2017-10-10T12:59:02.356915: step 2186, loss 0.236564, acc 0.90625, learning_rate 0.000100644
2017-10-10T12:59:02.909938: step 2187, loss 0.0821371, acc 1, learning_rate 0.000100641
2017-10-10T12:59:03.400856: step 2188, loss 0.483105, acc 0.84375, learning_rate 0.000100639
2017-10-10T12:59:03.944795: step 2189, loss 0.0522504, acc 1, learning_rate 0.000100636
2017-10-10T12:59:04.473026: step 2190, loss 0.13289, acc 0.953125, learning_rate 0.000100634
2017-10-10T12:59:04.980933: step 2191, loss 0.16734, acc 0.9375, learning_rate 0.000100631
2017-10-10T12:59:05.481006: step 2192, loss 0.235942, acc 0.890625, learning_rate 0.000100628
2017-10-10T12:59:05.969125: step 2193, loss 0.220619, acc 0.890625, learning_rate 0.000100626
2017-10-10T12:59:06.427740: step 2194, loss 0.0484934, acc 1, learning_rate 0.000100623
2017-10-10T12:59:06.933138: step 2195, loss 0.116489, acc 0.953125, learning_rate 0.000100621
2017-10-10T12:59:07.453931: step 2196, loss 0.114721, acc 0.953125, learning_rate 0.000100618
2017-10-10T12:59:07.956902: step 2197, loss 0.0804395, acc 1, learning_rate 0.000100616
2017-10-10T12:59:08.439353: step 2198, loss 0.132541, acc 0.96875, learning_rate 0.000100613
2017-10-10T12:59:08.988986: step 2199, loss 0.0947618, acc 0.96875, learning_rate 0.000100611
2017-10-10T12:59:09.620878: step 2200, loss 0.205585, acc 0.921875, learning_rate 0.000100608

Evaluation:
2017-10-10T12:59:10.803734: step 2200, loss 0.227817, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2200

2017-10-10T12:59:12.305193: step 2201, loss 0.106308, acc 0.953125, learning_rate 0.000100606
2017-10-10T12:59:12.803531: step 2202, loss 0.315138, acc 0.875, learning_rate 0.000100603
2017-10-10T12:59:13.356882: step 2203, loss 0.156753, acc 0.953125, learning_rate 0.000100601
2017-10-10T12:59:13.942395: step 2204, loss 0.0919409, acc 0.96875, learning_rate 0.000100598
2017-10-10T12:59:14.461125: step 2205, loss 0.122617, acc 0.96875, learning_rate 0.000100596
2017-10-10T12:59:15.041056: step 2206, loss 0.122281, acc 0.96875, learning_rate 0.000100594
2017-10-10T12:59:15.615047: step 2207, loss 0.145462, acc 0.984375, learning_rate 0.000100591
2017-10-10T12:59:16.092362: step 2208, loss 0.187235, acc 0.953125, learning_rate 0.000100589
2017-10-10T12:59:16.568947: step 2209, loss 0.260666, acc 0.9375, learning_rate 0.000100586
2017-10-10T12:59:17.137505: step 2210, loss 0.150625, acc 0.953125, learning_rate 0.000100584
2017-10-10T12:59:17.572964: step 2211, loss 0.164891, acc 0.921875, learning_rate 0.000100581
2017-10-10T12:59:18.118972: step 2212, loss 0.145168, acc 0.953125, learning_rate 0.000100579
2017-10-10T12:59:18.612253: step 2213, loss 0.243838, acc 0.90625, learning_rate 0.000100577
2017-10-10T12:59:19.164980: step 2214, loss 0.0680879, acc 0.984375, learning_rate 0.000100574
2017-10-10T12:59:19.671525: step 2215, loss 0.1652, acc 0.953125, learning_rate 0.000100572
2017-10-10T12:59:20.124845: step 2216, loss 0.0904004, acc 0.984375, learning_rate 0.00010057
2017-10-10T12:59:20.692923: step 2217, loss 0.0767264, acc 0.984375, learning_rate 0.000100567
2017-10-10T12:59:21.236194: step 2218, loss 0.224807, acc 0.96875, learning_rate 0.000100565
2017-10-10T12:59:21.632952: step 2219, loss 0.137357, acc 0.984375, learning_rate 0.000100563
2017-10-10T12:59:22.040875: step 2220, loss 0.106621, acc 0.953125, learning_rate 0.00010056
2017-10-10T12:59:22.493052: step 2221, loss 0.107757, acc 0.953125, learning_rate 0.000100558
2017-10-10T12:59:23.033206: step 2222, loss 0.1942, acc 0.921875, learning_rate 0.000100556
2017-10-10T12:59:23.495869: step 2223, loss 0.352633, acc 0.890625, learning_rate 0.000100554
2017-10-10T12:59:23.960593: step 2224, loss 0.164935, acc 0.953125, learning_rate 0.000100551
2017-10-10T12:59:24.405032: step 2225, loss 0.153981, acc 0.953125, learning_rate 0.000100549
2017-10-10T12:59:24.944848: step 2226, loss 0.161226, acc 0.9375, learning_rate 0.000100547
2017-10-10T12:59:25.416493: step 2227, loss 0.15582, acc 0.9375, learning_rate 0.000100545
2017-10-10T12:59:25.909326: step 2228, loss 0.130422, acc 0.96875, learning_rate 0.000100542
2017-10-10T12:59:26.512906: step 2229, loss 0.131313, acc 0.984375, learning_rate 0.00010054
2017-10-10T12:59:26.989239: step 2230, loss 0.189207, acc 0.953125, learning_rate 0.000100538
2017-10-10T12:59:27.500997: step 2231, loss 0.210495, acc 0.9375, learning_rate 0.000100536
2017-10-10T12:59:27.969695: step 2232, loss 0.200629, acc 0.9375, learning_rate 0.000100534
2017-10-10T12:59:28.477184: step 2233, loss 0.148326, acc 0.921875, learning_rate 0.000100531
2017-10-10T12:59:29.010403: step 2234, loss 0.239274, acc 0.90625, learning_rate 0.000100529
2017-10-10T12:59:29.528850: step 2235, loss 0.164434, acc 0.921875, learning_rate 0.000100527
2017-10-10T12:59:30.080955: step 2236, loss 0.19955, acc 0.921875, learning_rate 0.000100525
2017-10-10T12:59:30.632875: step 2237, loss 0.175887, acc 0.953125, learning_rate 0.000100523
2017-10-10T12:59:31.208636: step 2238, loss 0.210229, acc 0.9375, learning_rate 0.000100521
2017-10-10T12:59:31.700790: step 2239, loss 0.128219, acc 0.953125, learning_rate 0.000100519
2017-10-10T12:59:32.308046: step 2240, loss 0.168099, acc 0.9375, learning_rate 0.000100516

Evaluation:
2017-10-10T12:59:46.218906: step 2240, loss 0.227307, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2240

2017-10-10T12:59:48.689045: step 2241, loss 0.144683, acc 0.9375, learning_rate 0.000100514
2017-10-10T12:59:49.232905: step 2242, loss 0.120243, acc 0.96875, learning_rate 0.000100512
2017-10-10T12:59:49.741296: step 2243, loss 0.18831, acc 0.9375, learning_rate 0.00010051
2017-10-10T12:59:50.224397: step 2244, loss 0.0567461, acc 0.984375, learning_rate 0.000100508
2017-10-10T12:59:50.796892: step 2245, loss 0.218959, acc 0.890625, learning_rate 0.000100506
2017-10-10T12:59:51.300821: step 2246, loss 0.106879, acc 0.96875, learning_rate 0.000100504
2017-10-10T12:59:51.818463: step 2247, loss 0.0931135, acc 0.96875, learning_rate 0.000100502
2017-10-10T12:59:52.353023: step 2248, loss 0.293246, acc 0.890625, learning_rate 0.0001005
2017-10-10T12:59:52.890107: step 2249, loss 0.194573, acc 0.921875, learning_rate 0.000100498
2017-10-10T12:59:53.412830: step 2250, loss 0.0757775, acc 0.984375, learning_rate 0.000100496
2017-10-10T12:59:53.857325: step 2251, loss 0.334529, acc 0.90625, learning_rate 0.000100494
2017-10-10T12:59:54.376882: step 2252, loss 0.154796, acc 0.96875, learning_rate 0.000100492
2017-10-10T12:59:54.871967: step 2253, loss 0.209666, acc 0.9375, learning_rate 0.00010049
2017-10-10T12:59:55.336692: step 2254, loss 0.200772, acc 0.960784, learning_rate 0.000100488
2017-10-10T12:59:55.981577: step 2255, loss 0.183264, acc 0.9375, learning_rate 0.000100486
2017-10-10T12:59:56.597072: step 2256, loss 0.28209, acc 0.890625, learning_rate 0.000100484
2017-10-10T12:59:57.120835: step 2257, loss 0.285968, acc 0.921875, learning_rate 0.000100482
2017-10-10T12:59:57.551200: step 2258, loss 0.0857092, acc 0.984375, learning_rate 0.00010048
2017-10-10T12:59:58.069182: step 2259, loss 0.0973501, acc 0.984375, learning_rate 0.000100478
2017-10-10T12:59:58.563602: step 2260, loss 0.121477, acc 0.953125, learning_rate 0.000100476
2017-10-10T12:59:58.993323: step 2261, loss 0.11236, acc 0.96875, learning_rate 0.000100474
2017-10-10T12:59:59.516820: step 2262, loss 0.190706, acc 0.921875, learning_rate 0.000100472
2017-10-10T13:00:00.038175: step 2263, loss 0.160918, acc 0.9375, learning_rate 0.00010047
2017-10-10T13:00:00.606733: step 2264, loss 0.0734267, acc 0.96875, learning_rate 0.000100468
2017-10-10T13:00:01.132853: step 2265, loss 0.230464, acc 0.890625, learning_rate 0.000100466
2017-10-10T13:00:01.675652: step 2266, loss 0.242028, acc 0.921875, learning_rate 0.000100464
2017-10-10T13:00:02.228082: step 2267, loss 0.179839, acc 0.90625, learning_rate 0.000100462
2017-10-10T13:00:02.747484: step 2268, loss 0.097307, acc 0.984375, learning_rate 0.000100461
2017-10-10T13:00:03.300850: step 2269, loss 0.291194, acc 0.90625, learning_rate 0.000100459
2017-10-10T13:00:03.816843: step 2270, loss 0.171031, acc 0.921875, learning_rate 0.000100457
2017-10-10T13:00:04.330540: step 2271, loss 0.133728, acc 0.9375, learning_rate 0.000100455
2017-10-10T13:00:04.865599: step 2272, loss 0.100824, acc 0.96875, learning_rate 0.000100453
2017-10-10T13:00:05.427237: step 2273, loss 0.183392, acc 0.9375, learning_rate 0.000100451
2017-10-10T13:00:06.320657: step 2274, loss 0.183661, acc 0.953125, learning_rate 0.000100449
2017-10-10T13:00:06.862223: step 2275, loss 0.230006, acc 0.921875, learning_rate 0.000100448
2017-10-10T13:00:07.417338: step 2276, loss 0.101589, acc 0.953125, learning_rate 0.000100446
2017-10-10T13:00:07.952801: step 2277, loss 0.185873, acc 0.9375, learning_rate 0.000100444
2017-10-10T13:00:08.497351: step 2278, loss 0.133171, acc 0.96875, learning_rate 0.000100442
2017-10-10T13:00:09.036848: step 2279, loss 0.100902, acc 0.984375, learning_rate 0.00010044
2017-10-10T13:00:09.564975: step 2280, loss 0.262849, acc 0.9375, learning_rate 0.000100439

Evaluation:
2017-10-10T13:00:11.108884: step 2280, loss 0.225323, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2280

2017-10-10T13:00:12.925186: step 2281, loss 0.293643, acc 0.9375, learning_rate 0.000100437
2017-10-10T13:00:13.453020: step 2282, loss 0.168388, acc 0.96875, learning_rate 0.000100435
2017-10-10T13:00:14.020936: step 2283, loss 0.0888507, acc 0.984375, learning_rate 0.000100433
2017-10-10T13:00:14.504980: step 2284, loss 0.109783, acc 0.984375, learning_rate 0.000100431
2017-10-10T13:00:15.045017: step 2285, loss 0.242667, acc 0.890625, learning_rate 0.00010043
2017-10-10T13:00:15.598444: step 2286, loss 0.338197, acc 0.890625, learning_rate 0.000100428
2017-10-10T13:00:16.172933: step 2287, loss 0.0973727, acc 0.96875, learning_rate 0.000100426
2017-10-10T13:00:16.664943: step 2288, loss 0.241428, acc 0.90625, learning_rate 0.000100424
2017-10-10T13:00:17.163926: step 2289, loss 0.244165, acc 0.921875, learning_rate 0.000100423
2017-10-10T13:00:17.671567: step 2290, loss 0.171027, acc 0.953125, learning_rate 0.000100421
2017-10-10T13:00:18.213128: step 2291, loss 0.183333, acc 0.96875, learning_rate 0.000100419
2017-10-10T13:00:18.737845: step 2292, loss 0.120783, acc 0.953125, learning_rate 0.000100418
2017-10-10T13:00:19.332889: step 2293, loss 0.213758, acc 0.921875, learning_rate 0.000100416
2017-10-10T13:00:19.953074: step 2294, loss 0.125714, acc 0.953125, learning_rate 0.000100414
2017-10-10T13:00:20.424881: step 2295, loss 0.064657, acc 0.984375, learning_rate 0.000100412
2017-10-10T13:00:20.928924: step 2296, loss 0.0984319, acc 0.96875, learning_rate 0.000100411
2017-10-10T13:00:21.284431: step 2297, loss 0.101595, acc 0.953125, learning_rate 0.000100409
2017-10-10T13:00:21.736835: step 2298, loss 0.213055, acc 0.921875, learning_rate 0.000100407
2017-10-10T13:00:22.201999: step 2299, loss 0.211601, acc 0.90625, learning_rate 0.000100406
2017-10-10T13:00:22.755009: step 2300, loss 0.172467, acc 0.9375, learning_rate 0.000100404
2017-10-10T13:00:23.317027: step 2301, loss 0.166002, acc 0.953125, learning_rate 0.000100402
2017-10-10T13:00:23.852737: step 2302, loss 0.211524, acc 0.921875, learning_rate 0.000100401
2017-10-10T13:00:24.351918: step 2303, loss 0.0850122, acc 0.96875, learning_rate 0.000100399
2017-10-10T13:00:24.857380: step 2304, loss 0.0901396, acc 0.96875, learning_rate 0.000100398
2017-10-10T13:00:25.345742: step 2305, loss 0.104535, acc 0.96875, learning_rate 0.000100396
2017-10-10T13:00:25.856922: step 2306, loss 0.189067, acc 0.953125, learning_rate 0.000100394
2017-10-10T13:00:26.348739: step 2307, loss 0.10827, acc 0.953125, learning_rate 0.000100393
2017-10-10T13:00:26.932962: step 2308, loss 0.198261, acc 0.9375, learning_rate 0.000100391
2017-10-10T13:00:27.492608: step 2309, loss 0.192088, acc 0.921875, learning_rate 0.000100389
2017-10-10T13:00:28.018008: step 2310, loss 0.131363, acc 0.96875, learning_rate 0.000100388
2017-10-10T13:00:28.432861: step 2311, loss 0.165401, acc 0.96875, learning_rate 0.000100386
2017-10-10T13:00:28.960881: step 2312, loss 0.0832929, acc 0.953125, learning_rate 0.000100385
2017-10-10T13:00:29.505024: step 2313, loss 0.194291, acc 0.9375, learning_rate 0.000100383
2017-10-10T13:00:30.028839: step 2314, loss 0.13025, acc 0.96875, learning_rate 0.000100382
2017-10-10T13:00:30.596803: step 2315, loss 0.112992, acc 0.9375, learning_rate 0.00010038
2017-10-10T13:00:31.181017: step 2316, loss 0.277908, acc 0.90625, learning_rate 0.000100378
2017-10-10T13:00:31.721267: step 2317, loss 0.0855011, acc 0.984375, learning_rate 0.000100377
2017-10-10T13:00:32.201192: step 2318, loss 0.159206, acc 0.9375, learning_rate 0.000100375
2017-10-10T13:00:32.649075: step 2319, loss 0.105831, acc 0.96875, learning_rate 0.000100374
2017-10-10T13:00:33.153845: step 2320, loss 0.129722, acc 0.96875, learning_rate 0.000100372

Evaluation:
2017-10-10T13:00:34.582498: step 2320, loss 0.226611, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2320

2017-10-10T13:00:35.957011: step 2321, loss 0.126787, acc 0.96875, learning_rate 0.000100371
2017-10-10T13:00:36.484261: step 2322, loss 0.18719, acc 0.953125, learning_rate 0.000100369
2017-10-10T13:00:36.973995: step 2323, loss 0.0747798, acc 0.96875, learning_rate 0.000100368
2017-10-10T13:00:37.479995: step 2324, loss 0.122065, acc 0.96875, learning_rate 0.000100366
2017-10-10T13:00:37.976878: step 2325, loss 0.173627, acc 0.9375, learning_rate 0.000100365
2017-10-10T13:00:38.509012: step 2326, loss 0.111552, acc 0.984375, learning_rate 0.000100363
2017-10-10T13:00:38.985135: step 2327, loss 0.200206, acc 0.9375, learning_rate 0.000100362
2017-10-10T13:00:39.509126: step 2328, loss 0.100847, acc 0.96875, learning_rate 0.00010036
2017-10-10T13:00:40.119706: step 2329, loss 0.156665, acc 0.953125, learning_rate 0.000100359
2017-10-10T13:00:40.617052: step 2330, loss 0.250169, acc 0.875, learning_rate 0.000100357
2017-10-10T13:00:41.144761: step 2331, loss 0.135278, acc 0.953125, learning_rate 0.000100356
2017-10-10T13:00:41.644952: step 2332, loss 0.071325, acc 0.984375, learning_rate 0.000100354
2017-10-10T13:00:42.200905: step 2333, loss 0.16145, acc 0.953125, learning_rate 0.000100353
2017-10-10T13:00:42.720168: step 2334, loss 0.357775, acc 0.890625, learning_rate 0.000100352
2017-10-10T13:00:43.405112: step 2335, loss 0.118887, acc 0.953125, learning_rate 0.00010035
2017-10-10T13:00:43.802036: step 2336, loss 0.204509, acc 0.9375, learning_rate 0.000100349
2017-10-10T13:00:44.329685: step 2337, loss 0.209662, acc 0.90625, learning_rate 0.000100347
2017-10-10T13:00:44.782267: step 2338, loss 0.202355, acc 0.96875, learning_rate 0.000100346
2017-10-10T13:00:45.201671: step 2339, loss 0.225545, acc 0.9375, learning_rate 0.000100344
2017-10-10T13:00:45.757087: step 2340, loss 0.124831, acc 0.953125, learning_rate 0.000100343
2017-10-10T13:00:46.254324: step 2341, loss 0.0542505, acc 1, learning_rate 0.000100342
2017-10-10T13:00:46.800833: step 2342, loss 0.0807563, acc 0.96875, learning_rate 0.00010034
2017-10-10T13:00:47.303505: step 2343, loss 0.139384, acc 0.9375, learning_rate 0.000100339
2017-10-10T13:00:47.812868: step 2344, loss 0.1427, acc 0.9375, learning_rate 0.000100338
2017-10-10T13:00:48.371492: step 2345, loss 0.168231, acc 0.9375, learning_rate 0.000100336
2017-10-10T13:00:48.922944: step 2346, loss 0.305731, acc 0.890625, learning_rate 0.000100335
2017-10-10T13:00:49.920429: step 2347, loss 0.0976332, acc 0.984375, learning_rate 0.000100333
2017-10-10T13:00:50.475362: step 2348, loss 0.107475, acc 0.96875, learning_rate 0.000100332
2017-10-10T13:00:51.040011: step 2349, loss 0.197679, acc 0.9375, learning_rate 0.000100331
2017-10-10T13:00:51.558780: step 2350, loss 0.185086, acc 0.90625, learning_rate 0.000100329
2017-10-10T13:00:52.056846: step 2351, loss 0.133758, acc 0.96875, learning_rate 0.000100328
2017-10-10T13:00:52.528883: step 2352, loss 0.0996178, acc 0.980392, learning_rate 0.000100327
2017-10-10T13:00:53.034095: step 2353, loss 0.112594, acc 0.984375, learning_rate 0.000100325
2017-10-10T13:00:53.483881: step 2354, loss 0.234278, acc 0.90625, learning_rate 0.000100324
2017-10-10T13:00:54.002797: step 2355, loss 0.152314, acc 0.921875, learning_rate 0.000100323
2017-10-10T13:00:54.521569: step 2356, loss 0.172053, acc 0.953125, learning_rate 0.000100321
2017-10-10T13:00:55.111659: step 2357, loss 0.156509, acc 0.953125, learning_rate 0.00010032
2017-10-10T13:00:55.632918: step 2358, loss 0.252701, acc 0.90625, learning_rate 0.000100319
2017-10-10T13:00:56.200847: step 2359, loss 0.157846, acc 0.953125, learning_rate 0.000100317
2017-10-10T13:00:56.753060: step 2360, loss 0.119603, acc 0.953125, learning_rate 0.000100316

Evaluation:
2017-10-10T13:00:58.137496: step 2360, loss 0.223708, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2360

2017-10-10T13:00:59.768883: step 2361, loss 0.162557, acc 0.953125, learning_rate 0.000100315
2017-10-10T13:01:00.240846: step 2362, loss 0.218546, acc 0.9375, learning_rate 0.000100314
2017-10-10T13:01:00.743770: step 2363, loss 0.187282, acc 0.921875, learning_rate 0.000100312
2017-10-10T13:01:01.300912: step 2364, loss 0.264526, acc 0.9375, learning_rate 0.000100311
2017-10-10T13:01:01.850889: step 2365, loss 0.100817, acc 0.96875, learning_rate 0.00010031
2017-10-10T13:01:02.391127: step 2366, loss 0.186544, acc 0.9375, learning_rate 0.000100308
2017-10-10T13:01:02.906680: step 2367, loss 0.201444, acc 0.921875, learning_rate 0.000100307
2017-10-10T13:01:03.423371: step 2368, loss 0.0923566, acc 0.984375, learning_rate 0.000100306
2017-10-10T13:01:03.959987: step 2369, loss 0.0642148, acc 1, learning_rate 0.000100305
2017-10-10T13:01:04.530308: step 2370, loss 0.203356, acc 0.9375, learning_rate 0.000100303
2017-10-10T13:01:05.122160: step 2371, loss 0.153561, acc 0.953125, learning_rate 0.000100302
2017-10-10T13:01:05.623984: step 2372, loss 0.0870784, acc 0.96875, learning_rate 0.000100301
2017-10-10T13:01:06.153197: step 2373, loss 0.297286, acc 0.90625, learning_rate 0.0001003
2017-10-10T13:01:06.796872: step 2374, loss 0.0945583, acc 0.984375, learning_rate 0.000100299
2017-10-10T13:01:07.357012: step 2375, loss 0.221194, acc 0.9375, learning_rate 0.000100297
2017-10-10T13:01:07.634232: step 2376, loss 0.097391, acc 0.96875, learning_rate 0.000100296
2017-10-10T13:01:07.991461: step 2377, loss 0.15089, acc 0.9375, learning_rate 0.000100295
2017-10-10T13:01:08.496905: step 2378, loss 0.174206, acc 0.9375, learning_rate 0.000100294
2017-10-10T13:01:09.056908: step 2379, loss 0.249664, acc 0.9375, learning_rate 0.000100292
2017-10-10T13:01:09.560941: step 2380, loss 0.144215, acc 0.953125, learning_rate 0.000100291
2017-10-10T13:01:10.112038: step 2381, loss 0.191299, acc 0.953125, learning_rate 0.00010029
2017-10-10T13:01:10.632479: step 2382, loss 0.110646, acc 0.96875, learning_rate 0.000100289
2017-10-10T13:01:11.167363: step 2383, loss 0.131867, acc 0.984375, learning_rate 0.000100288
2017-10-10T13:01:11.675117: step 2384, loss 0.182294, acc 0.9375, learning_rate 0.000100287
2017-10-10T13:01:12.259354: step 2385, loss 0.125981, acc 0.921875, learning_rate 0.000100285
2017-10-10T13:01:12.732991: step 2386, loss 0.0569683, acc 0.984375, learning_rate 0.000100284
2017-10-10T13:01:13.292257: step 2387, loss 0.361035, acc 0.921875, learning_rate 0.000100283
2017-10-10T13:01:13.767625: step 2388, loss 0.0932732, acc 0.984375, learning_rate 0.000100282
2017-10-10T13:01:14.216884: step 2389, loss 0.195404, acc 0.9375, learning_rate 0.000100281
2017-10-10T13:01:14.768867: step 2390, loss 0.221441, acc 0.859375, learning_rate 0.00010028
2017-10-10T13:01:15.287757: step 2391, loss 0.12976, acc 0.9375, learning_rate 0.000100278
2017-10-10T13:01:15.789825: step 2392, loss 0.136346, acc 0.9375, learning_rate 0.000100277
2017-10-10T13:01:16.279782: step 2393, loss 0.149032, acc 0.921875, learning_rate 0.000100276
2017-10-10T13:01:16.818482: step 2394, loss 0.140804, acc 0.96875, learning_rate 0.000100275
2017-10-10T13:01:17.305061: step 2395, loss 0.136494, acc 0.9375, learning_rate 0.000100274
2017-10-10T13:01:17.901119: step 2396, loss 0.119338, acc 0.953125, learning_rate 0.000100273
2017-10-10T13:01:18.468844: step 2397, loss 0.145891, acc 0.9375, learning_rate 0.000100272
2017-10-10T13:01:19.024854: step 2398, loss 0.203189, acc 0.9375, learning_rate 0.000100271
2017-10-10T13:01:19.566763: step 2399, loss 0.135424, acc 0.921875, learning_rate 0.00010027
2017-10-10T13:01:20.048858: step 2400, loss 0.163398, acc 0.96875, learning_rate 0.000100268

Evaluation:
2017-10-10T13:01:21.268333: step 2400, loss 0.223573, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2400

2017-10-10T13:01:22.843100: step 2401, loss 0.118734, acc 0.96875, learning_rate 0.000100267
2017-10-10T13:01:23.340898: step 2402, loss 0.196696, acc 0.90625, learning_rate 0.000100266
2017-10-10T13:01:23.887971: step 2403, loss 0.0675581, acc 1, learning_rate 0.000100265
2017-10-10T13:01:24.404842: step 2404, loss 0.132444, acc 0.9375, learning_rate 0.000100264
2017-10-10T13:01:24.916702: step 2405, loss 0.090287, acc 0.96875, learning_rate 0.000100263
2017-10-10T13:01:25.425195: step 2406, loss 0.213264, acc 0.9375, learning_rate 0.000100262
2017-10-10T13:01:25.961172: step 2407, loss 0.197727, acc 0.953125, learning_rate 0.000100261
2017-10-10T13:01:26.489128: step 2408, loss 0.186701, acc 0.9375, learning_rate 0.00010026
2017-10-10T13:01:27.006151: step 2409, loss 0.217525, acc 0.9375, learning_rate 0.000100259
2017-10-10T13:01:27.512890: step 2410, loss 0.182995, acc 0.9375, learning_rate 0.000100258
2017-10-10T13:01:28.044486: step 2411, loss 0.259381, acc 0.90625, learning_rate 0.000100257
2017-10-10T13:01:28.632813: step 2412, loss 0.182408, acc 0.921875, learning_rate 0.000100256
2017-10-10T13:01:29.172984: step 2413, loss 0.238289, acc 0.875, learning_rate 0.000100255
2017-10-10T13:01:29.777020: step 2414, loss 0.185913, acc 0.921875, learning_rate 0.000100253
2017-10-10T13:01:30.236881: step 2415, loss 0.0821537, acc 0.984375, learning_rate 0.000100252
2017-10-10T13:01:30.568827: step 2416, loss 0.212672, acc 0.90625, learning_rate 0.000100251
2017-10-10T13:01:30.848953: step 2417, loss 0.238991, acc 0.9375, learning_rate 0.00010025
2017-10-10T13:01:31.285201: step 2418, loss 0.103454, acc 0.96875, learning_rate 0.000100249
2017-10-10T13:01:31.727850: step 2419, loss 0.143388, acc 0.9375, learning_rate 0.000100248
2017-10-10T13:01:32.188940: step 2420, loss 0.158831, acc 0.9375, learning_rate 0.000100247
2017-10-10T13:01:32.725319: step 2421, loss 0.0631325, acc 0.984375, learning_rate 0.000100246
2017-10-10T13:01:33.350738: step 2422, loss 0.140383, acc 0.9375, learning_rate 0.000100245
2017-10-10T13:01:33.877089: step 2423, loss 0.336152, acc 0.90625, learning_rate 0.000100244
2017-10-10T13:01:34.327044: step 2424, loss 0.143651, acc 0.9375, learning_rate 0.000100243
2017-10-10T13:01:34.825968: step 2425, loss 0.179603, acc 0.953125, learning_rate 0.000100242
2017-10-10T13:01:35.336638: step 2426, loss 0.0723261, acc 0.984375, learning_rate 0.000100241
2017-10-10T13:01:35.876971: step 2427, loss 0.247999, acc 0.90625, learning_rate 0.00010024
2017-10-10T13:01:36.461013: step 2428, loss 0.167931, acc 0.921875, learning_rate 0.000100239
2017-10-10T13:01:37.008854: step 2429, loss 0.17428, acc 0.953125, learning_rate 0.000100238
2017-10-10T13:01:37.472502: step 2430, loss 0.127721, acc 0.96875, learning_rate 0.000100237
2017-10-10T13:01:37.985081: step 2431, loss 0.164061, acc 0.953125, learning_rate 0.000100236
2017-10-10T13:01:38.532862: step 2432, loss 0.165601, acc 0.953125, learning_rate 0.000100235
2017-10-10T13:01:39.041093: step 2433, loss 0.0635998, acc 0.984375, learning_rate 0.000100235
2017-10-10T13:01:39.613010: step 2434, loss 0.132877, acc 0.953125, learning_rate 0.000100234
2017-10-10T13:01:39.992121: step 2435, loss 0.174123, acc 0.953125, learning_rate 0.000100233
2017-10-10T13:01:40.510073: step 2436, loss 0.0835123, acc 0.984375, learning_rate 0.000100232
2017-10-10T13:01:41.000964: step 2437, loss 0.124573, acc 0.953125, learning_rate 0.000100231
2017-10-10T13:01:41.533791: step 2438, loss 0.136435, acc 0.953125, learning_rate 0.00010023
2017-10-10T13:01:42.064635: step 2439, loss 0.189596, acc 0.921875, learning_rate 0.000100229
2017-10-10T13:01:42.552884: step 2440, loss 0.289774, acc 0.90625, learning_rate 0.000100228

Evaluation:
2017-10-10T13:01:43.713610: step 2440, loss 0.226261, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2440

2017-10-10T13:01:45.773168: step 2441, loss 0.136815, acc 0.96875, learning_rate 0.000100227
2017-10-10T13:01:46.295217: step 2442, loss 0.227368, acc 0.90625, learning_rate 0.000100226
2017-10-10T13:01:46.832850: step 2443, loss 0.134501, acc 0.9375, learning_rate 0.000100225
2017-10-10T13:01:47.369355: step 2444, loss 0.192076, acc 0.921875, learning_rate 0.000100224
2017-10-10T13:01:47.884851: step 2445, loss 0.173591, acc 0.9375, learning_rate 0.000100223
2017-10-10T13:01:48.480877: step 2446, loss 0.195289, acc 0.90625, learning_rate 0.000100222
2017-10-10T13:01:48.981587: step 2447, loss 0.161601, acc 0.921875, learning_rate 0.000100221
2017-10-10T13:01:49.480842: step 2448, loss 0.118615, acc 0.984375, learning_rate 0.000100221
2017-10-10T13:01:50.003308: step 2449, loss 0.275373, acc 0.875, learning_rate 0.00010022
2017-10-10T13:01:50.464836: step 2450, loss 0.278597, acc 0.882353, learning_rate 0.000100219
2017-10-10T13:01:50.998662: step 2451, loss 0.093034, acc 0.96875, learning_rate 0.000100218
2017-10-10T13:01:51.528856: step 2452, loss 0.141181, acc 0.953125, learning_rate 0.000100217
2017-10-10T13:01:52.140193: step 2453, loss 0.165121, acc 0.953125, learning_rate 0.000100216
2017-10-10T13:01:52.744872: step 2454, loss 0.242138, acc 0.953125, learning_rate 0.000100215
2017-10-10T13:01:53.312845: step 2455, loss 0.0767407, acc 0.984375, learning_rate 0.000100214
2017-10-10T13:01:53.755814: step 2456, loss 0.129676, acc 0.953125, learning_rate 0.000100213
2017-10-10T13:01:54.137634: step 2457, loss 0.161519, acc 0.984375, learning_rate 0.000100213
2017-10-10T13:01:54.629687: step 2458, loss 0.116599, acc 0.96875, learning_rate 0.000100212
2017-10-10T13:01:55.173008: step 2459, loss 0.0648378, acc 0.984375, learning_rate 0.000100211
2017-10-10T13:01:55.684935: step 2460, loss 0.141936, acc 0.953125, learning_rate 0.00010021
2017-10-10T13:01:56.250863: step 2461, loss 0.189332, acc 0.96875, learning_rate 0.000100209
2017-10-10T13:01:56.804829: step 2462, loss 0.242307, acc 0.921875, learning_rate 0.000100208
2017-10-10T13:01:57.326691: step 2463, loss 0.105777, acc 0.984375, learning_rate 0.000100207
2017-10-10T13:01:57.843755: step 2464, loss 0.177666, acc 0.953125, learning_rate 0.000100207
2017-10-10T13:01:58.358727: step 2465, loss 0.284607, acc 0.875, learning_rate 0.000100206
2017-10-10T13:01:58.947872: step 2466, loss 0.124186, acc 0.96875, learning_rate 0.000100205
2017-10-10T13:01:59.479840: step 2467, loss 0.146226, acc 0.9375, learning_rate 0.000100204
2017-10-10T13:01:59.982250: step 2468, loss 0.179102, acc 0.9375, learning_rate 0.000100203
2017-10-10T13:02:00.444980: step 2469, loss 0.0720889, acc 0.984375, learning_rate 0.000100202
2017-10-10T13:02:00.998213: step 2470, loss 0.219858, acc 0.90625, learning_rate 0.000100202
2017-10-10T13:02:01.539639: step 2471, loss 0.103129, acc 0.96875, learning_rate 0.000100201
2017-10-10T13:02:01.950597: step 2472, loss 0.167446, acc 0.90625, learning_rate 0.0001002
2017-10-10T13:02:02.436705: step 2473, loss 0.100156, acc 0.96875, learning_rate 0.000100199
2017-10-10T13:02:02.926035: step 2474, loss 0.251521, acc 0.921875, learning_rate 0.000100198
2017-10-10T13:02:03.422736: step 2475, loss 0.229025, acc 0.9375, learning_rate 0.000100198
2017-10-10T13:02:03.958848: step 2476, loss 0.103292, acc 0.96875, learning_rate 0.000100197
2017-10-10T13:02:04.496826: step 2477, loss 0.252725, acc 0.921875, learning_rate 0.000100196
2017-10-10T13:02:05.085101: step 2478, loss 0.209262, acc 0.921875, learning_rate 0.000100195
2017-10-10T13:02:05.609230: step 2479, loss 0.139899, acc 0.9375, learning_rate 0.000100194
2017-10-10T13:02:06.129125: step 2480, loss 0.148355, acc 0.953125, learning_rate 0.000100194

Evaluation:
2017-10-10T13:02:07.406299: step 2480, loss 0.224637, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2480

2017-10-10T13:02:08.673889: step 2481, loss 0.149255, acc 0.9375, learning_rate 0.000100193
2017-10-10T13:02:09.217868: step 2482, loss 0.24327, acc 0.9375, learning_rate 0.000100192
2017-10-10T13:02:09.765848: step 2483, loss 0.289472, acc 0.859375, learning_rate 0.000100191
2017-10-10T13:02:10.337002: step 2484, loss 0.101077, acc 0.96875, learning_rate 0.00010019
2017-10-10T13:02:10.896917: step 2485, loss 0.139927, acc 0.96875, learning_rate 0.00010019
2017-10-10T13:02:11.384185: step 2486, loss 0.207556, acc 0.921875, learning_rate 0.000100189
2017-10-10T13:02:12.000544: step 2487, loss 0.25262, acc 0.90625, learning_rate 0.000100188
2017-10-10T13:02:12.512963: step 2488, loss 0.194079, acc 0.9375, learning_rate 0.000100187
2017-10-10T13:02:13.064970: step 2489, loss 0.179303, acc 0.921875, learning_rate 0.000100187
2017-10-10T13:02:13.585230: step 2490, loss 0.220143, acc 0.921875, learning_rate 0.000100186
2017-10-10T13:02:14.112849: step 2491, loss 0.178747, acc 0.921875, learning_rate 0.000100185
2017-10-10T13:02:14.632853: step 2492, loss 0.145417, acc 0.921875, learning_rate 0.000100184
2017-10-10T13:02:15.267677: step 2493, loss 0.0748356, acc 1, learning_rate 0.000100183
2017-10-10T13:02:15.644866: step 2494, loss 0.279992, acc 0.890625, learning_rate 0.000100183
2017-10-10T13:02:16.132943: step 2495, loss 0.138303, acc 0.9375, learning_rate 0.000100182
2017-10-10T13:02:16.640841: step 2496, loss 0.106859, acc 0.984375, learning_rate 0.000100181
2017-10-10T13:02:17.082229: step 2497, loss 0.155308, acc 0.90625, learning_rate 0.000100181
2017-10-10T13:02:17.517224: step 2498, loss 0.256946, acc 0.890625, learning_rate 0.00010018
2017-10-10T13:02:18.106607: step 2499, loss 0.209385, acc 0.9375, learning_rate 0.000100179
2017-10-10T13:02:18.610103: step 2500, loss 0.28896, acc 0.90625, learning_rate 0.000100178
2017-10-10T13:02:19.148336: step 2501, loss 0.201284, acc 0.953125, learning_rate 0.000100178
2017-10-10T13:02:19.724393: step 2502, loss 0.230141, acc 0.9375, learning_rate 0.000100177
2017-10-10T13:02:20.281246: step 2503, loss 0.206397, acc 0.890625, learning_rate 0.000100176
2017-10-10T13:02:20.820852: step 2504, loss 0.215017, acc 0.90625, learning_rate 0.000100175
2017-10-10T13:02:21.356201: step 2505, loss 0.132163, acc 0.96875, learning_rate 0.000100175
2017-10-10T13:02:21.922894: step 2506, loss 0.0685475, acc 0.96875, learning_rate 0.000100174
2017-10-10T13:02:22.437231: step 2507, loss 0.170197, acc 0.9375, learning_rate 0.000100173
2017-10-10T13:02:22.995548: step 2508, loss 0.121395, acc 0.96875, learning_rate 0.000100173
2017-10-10T13:02:23.560036: step 2509, loss 0.209954, acc 0.921875, learning_rate 0.000100172
2017-10-10T13:02:24.117109: step 2510, loss 0.125741, acc 0.953125, learning_rate 0.000100171
2017-10-10T13:02:24.715276: step 2511, loss 0.243044, acc 0.90625, learning_rate 0.00010017
2017-10-10T13:02:25.276933: step 2512, loss 0.0911407, acc 0.984375, learning_rate 0.00010017
2017-10-10T13:02:25.860566: step 2513, loss 0.175979, acc 0.9375, learning_rate 0.000100169
2017-10-10T13:02:26.399982: step 2514, loss 0.157307, acc 0.984375, learning_rate 0.000100168
2017-10-10T13:02:26.936097: step 2515, loss 0.159526, acc 0.9375, learning_rate 0.000100168
2017-10-10T13:02:27.441041: step 2516, loss 0.318956, acc 0.890625, learning_rate 0.000100167
2017-10-10T13:02:27.932991: step 2517, loss 0.309774, acc 0.90625, learning_rate 0.000100166
2017-10-10T13:02:28.458179: step 2518, loss 0.12606, acc 0.953125, learning_rate 0.000100166
2017-10-10T13:02:28.994454: step 2519, loss 0.258314, acc 0.875, learning_rate 0.000100165
2017-10-10T13:02:29.550680: step 2520, loss 0.14158, acc 0.953125, learning_rate 0.000100164

Evaluation:
2017-10-10T13:02:30.604814: step 2520, loss 0.22319, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2520

2017-10-10T13:02:32.133085: step 2521, loss 0.170091, acc 0.921875, learning_rate 0.000100164
2017-10-10T13:02:32.613283: step 2522, loss 0.290519, acc 0.90625, learning_rate 0.000100163
2017-10-10T13:02:33.141058: step 2523, loss 0.104699, acc 0.96875, learning_rate 0.000100162
2017-10-10T13:02:33.639544: step 2524, loss 0.117737, acc 0.96875, learning_rate 0.000100162
2017-10-10T13:02:34.156990: step 2525, loss 0.170854, acc 0.9375, learning_rate 0.000100161
2017-10-10T13:02:34.679641: step 2526, loss 0.0677721, acc 1, learning_rate 0.00010016
2017-10-10T13:02:35.164877: step 2527, loss 0.12674, acc 0.96875, learning_rate 0.00010016
2017-10-10T13:02:35.600208: step 2528, loss 0.0972799, acc 0.96875, learning_rate 0.000100159
2017-10-10T13:02:36.104983: step 2529, loss 0.191814, acc 0.96875, learning_rate 0.000100158
2017-10-10T13:02:36.601000: step 2530, loss 0.210663, acc 0.921875, learning_rate 0.000100158
2017-10-10T13:02:37.060961: step 2531, loss 0.0688535, acc 1, learning_rate 0.000100157
2017-10-10T13:02:37.617063: step 2532, loss 0.157067, acc 0.953125, learning_rate 0.000100156
2017-10-10T13:02:38.121819: step 2533, loss 0.109216, acc 0.96875, learning_rate 0.000100156
2017-10-10T13:02:38.557833: step 2534, loss 0.151173, acc 0.953125, learning_rate 0.000100155
2017-10-10T13:02:39.061761: step 2535, loss 0.278781, acc 0.90625, learning_rate 0.000100155
2017-10-10T13:02:39.669131: step 2536, loss 0.22603, acc 0.953125, learning_rate 0.000100154
2017-10-10T13:02:40.191741: step 2537, loss 0.12237, acc 0.953125, learning_rate 0.000100153
2017-10-10T13:02:40.608006: step 2538, loss 0.163943, acc 0.953125, learning_rate 0.000100153
2017-10-10T13:02:41.037380: step 2539, loss 0.0473969, acc 1, learning_rate 0.000100152
2017-10-10T13:02:41.517080: step 2540, loss 0.128236, acc 0.953125, learning_rate 0.000100151
2017-10-10T13:02:41.953176: step 2541, loss 0.143952, acc 0.953125, learning_rate 0.000100151
2017-10-10T13:02:42.521095: step 2542, loss 0.0922715, acc 0.953125, learning_rate 0.00010015
2017-10-10T13:02:43.064432: step 2543, loss 0.294721, acc 0.875, learning_rate 0.00010015
2017-10-10T13:02:43.660946: step 2544, loss 0.0704952, acc 0.984375, learning_rate 0.000100149
2017-10-10T13:02:44.224837: step 2545, loss 0.20246, acc 0.9375, learning_rate 0.000100148
2017-10-10T13:02:44.776836: step 2546, loss 0.307149, acc 0.90625, learning_rate 0.000100148
2017-10-10T13:02:45.291715: step 2547, loss 0.136446, acc 0.953125, learning_rate 0.000100147
2017-10-10T13:02:45.725319: step 2548, loss 0.112219, acc 0.980392, learning_rate 0.000100147
2017-10-10T13:02:46.224616: step 2549, loss 0.234756, acc 0.890625, learning_rate 0.000100146
2017-10-10T13:02:46.733064: step 2550, loss 0.179382, acc 0.953125, learning_rate 0.000100145
2017-10-10T13:02:47.201188: step 2551, loss 0.152712, acc 0.953125, learning_rate 0.000100145
2017-10-10T13:02:47.783326: step 2552, loss 0.095225, acc 0.96875, learning_rate 0.000100144
2017-10-10T13:02:48.301496: step 2553, loss 0.113206, acc 0.96875, learning_rate 0.000100144
2017-10-10T13:02:48.805153: step 2554, loss 0.220032, acc 0.90625, learning_rate 0.000100143
2017-10-10T13:02:49.315360: step 2555, loss 0.0979441, acc 0.984375, learning_rate 0.000100142
2017-10-10T13:02:49.834366: step 2556, loss 0.13869, acc 0.96875, learning_rate 0.000100142
2017-10-10T13:02:50.377054: step 2557, loss 0.150824, acc 0.953125, learning_rate 0.000100141
2017-10-10T13:02:50.851983: step 2558, loss 0.131855, acc 0.9375, learning_rate 0.000100141
2017-10-10T13:02:51.340920: step 2559, loss 0.146136, acc 0.96875, learning_rate 0.00010014
2017-10-10T13:02:51.876961: step 2560, loss 0.253121, acc 0.90625, learning_rate 0.00010014

Evaluation:
2017-10-10T13:02:53.247957: step 2560, loss 0.226415, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2560

2017-10-10T13:02:54.832874: step 2561, loss 0.209263, acc 0.921875, learning_rate 0.000100139
2017-10-10T13:02:55.331184: step 2562, loss 0.118431, acc 0.953125, learning_rate 0.000100138
2017-10-10T13:02:55.885529: step 2563, loss 0.12936, acc 0.9375, learning_rate 0.000100138
2017-10-10T13:02:56.416968: step 2564, loss 0.145382, acc 0.9375, learning_rate 0.000100137
2017-10-10T13:02:56.845092: step 2565, loss 0.0562822, acc 0.984375, learning_rate 0.000100137
2017-10-10T13:02:57.365062: step 2566, loss 0.279865, acc 0.90625, learning_rate 0.000100136
2017-10-10T13:02:57.901042: step 2567, loss 0.161557, acc 0.953125, learning_rate 0.000100136
2017-10-10T13:02:58.357676: step 2568, loss 0.192366, acc 0.9375, learning_rate 0.000100135
2017-10-10T13:02:58.939692: step 2569, loss 0.203663, acc 0.90625, learning_rate 0.000100134
2017-10-10T13:02:59.512030: step 2570, loss 0.108083, acc 0.984375, learning_rate 0.000100134
2017-10-10T13:03:00.018046: step 2571, loss 0.109853, acc 0.9375, learning_rate 0.000100133
2017-10-10T13:03:00.445508: step 2572, loss 0.131803, acc 0.953125, learning_rate 0.000100133
2017-10-10T13:03:00.888409: step 2573, loss 0.229043, acc 0.921875, learning_rate 0.000100132
2017-10-10T13:03:01.440847: step 2574, loss 0.0900066, acc 0.984375, learning_rate 0.000100132
2017-10-10T13:03:01.980659: step 2575, loss 0.149576, acc 0.953125, learning_rate 0.000100131
2017-10-10T13:03:02.574018: step 2576, loss 0.116059, acc 0.96875, learning_rate 0.000100131
2017-10-10T13:03:03.150652: step 2577, loss 0.126753, acc 0.96875, learning_rate 0.00010013
2017-10-10T13:03:03.585052: step 2578, loss 0.224792, acc 0.9375, learning_rate 0.00010013
2017-10-10T13:03:04.016631: step 2579, loss 0.163544, acc 0.921875, learning_rate 0.000100129
2017-10-10T13:03:04.558880: step 2580, loss 0.122772, acc 0.96875, learning_rate 0.000100129
2017-10-10T13:03:05.089034: step 2581, loss 0.225837, acc 0.921875, learning_rate 0.000100128
2017-10-10T13:03:05.632822: step 2582, loss 0.124702, acc 0.953125, learning_rate 0.000100128
2017-10-10T13:03:06.175172: step 2583, loss 0.302536, acc 0.90625, learning_rate 0.000100127
2017-10-10T13:03:06.671102: step 2584, loss 0.241934, acc 0.921875, learning_rate 0.000100126
2017-10-10T13:03:07.156857: step 2585, loss 0.0819509, acc 0.984375, learning_rate 0.000100126
2017-10-10T13:03:07.569250: step 2586, loss 0.121756, acc 0.953125, learning_rate 0.000100125
2017-10-10T13:03:08.044978: step 2587, loss 0.0832738, acc 0.984375, learning_rate 0.000100125
2017-10-10T13:03:08.558043: step 2588, loss 0.113658, acc 0.984375, learning_rate 0.000100124
2017-10-10T13:03:09.127339: step 2589, loss 0.189314, acc 0.921875, learning_rate 0.000100124
2017-10-10T13:03:09.649062: step 2590, loss 0.187412, acc 0.921875, learning_rate 0.000100123
2017-10-10T13:03:10.181018: step 2591, loss 0.247726, acc 0.90625, learning_rate 0.000100123
2017-10-10T13:03:10.740406: step 2592, loss 0.0941571, acc 0.984375, learning_rate 0.000100122
2017-10-10T13:03:11.196908: step 2593, loss 0.114388, acc 0.953125, learning_rate 0.000100122
2017-10-10T13:03:11.752942: step 2594, loss 0.0958838, acc 0.984375, learning_rate 0.000100121
2017-10-10T13:03:12.328966: step 2595, loss 0.125177, acc 0.96875, learning_rate 0.000100121
2017-10-10T13:03:12.836918: step 2596, loss 0.266582, acc 0.9375, learning_rate 0.00010012
2017-10-10T13:03:13.284897: step 2597, loss 0.0824849, acc 0.96875, learning_rate 0.00010012
2017-10-10T13:03:13.833049: step 2598, loss 0.206708, acc 0.953125, learning_rate 0.000100119
2017-10-10T13:03:14.329124: step 2599, loss 0.098531, acc 0.984375, learning_rate 0.000100119
2017-10-10T13:03:14.924808: step 2600, loss 0.12206, acc 0.9375, learning_rate 0.000100118

Evaluation:
2017-10-10T13:03:16.245115: step 2600, loss 0.231233, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2600

2017-10-10T13:03:17.627406: step 2601, loss 0.217112, acc 0.953125, learning_rate 0.000100118
2017-10-10T13:03:18.202894: step 2602, loss 0.176298, acc 0.9375, learning_rate 0.000100117
2017-10-10T13:03:18.772770: step 2603, loss 0.11663, acc 0.953125, learning_rate 0.000100117
2017-10-10T13:03:19.297755: step 2604, loss 0.138903, acc 0.9375, learning_rate 0.000100117
2017-10-10T13:03:19.793496: step 2605, loss 0.108178, acc 0.96875, learning_rate 0.000100116
2017-10-10T13:03:20.300231: step 2606, loss 0.099632, acc 0.953125, learning_rate 0.000100116
2017-10-10T13:03:20.867511: step 2607, loss 0.260542, acc 0.875, learning_rate 0.000100115
2017-10-10T13:03:21.384846: step 2608, loss 0.171999, acc 0.921875, learning_rate 0.000100115
2017-10-10T13:03:21.921073: step 2609, loss 0.253105, acc 0.921875, learning_rate 0.000100114
2017-10-10T13:03:22.426169: step 2610, loss 0.12676, acc 0.96875, learning_rate 0.000100114
2017-10-10T13:03:23.060845: step 2611, loss 0.199343, acc 0.921875, learning_rate 0.000100113
2017-10-10T13:03:23.760170: step 2612, loss 0.15265, acc 0.9375, learning_rate 0.000100113
2017-10-10T13:03:24.221036: step 2613, loss 0.279315, acc 0.890625, learning_rate 0.000100112
2017-10-10T13:03:24.711814: step 2614, loss 0.162186, acc 0.9375, learning_rate 0.000100112
2017-10-10T13:03:25.220874: step 2615, loss 0.240509, acc 0.875, learning_rate 0.000100111
2017-10-10T13:03:25.805434: step 2616, loss 0.143171, acc 0.9375, learning_rate 0.000100111
2017-10-10T13:03:26.305006: step 2617, loss 0.135668, acc 0.953125, learning_rate 0.000100111
2017-10-10T13:03:26.747253: step 2618, loss 0.15747, acc 0.96875, learning_rate 0.00010011
2017-10-10T13:03:27.285349: step 2619, loss 0.22289, acc 0.9375, learning_rate 0.00010011
2017-10-10T13:03:27.793056: step 2620, loss 0.123566, acc 0.9375, learning_rate 0.000100109
2017-10-10T13:03:28.280915: step 2621, loss 0.185522, acc 0.9375, learning_rate 0.000100109
2017-10-10T13:03:28.768858: step 2622, loss 0.2491, acc 0.9375, learning_rate 0.000100108
2017-10-10T13:03:29.284636: step 2623, loss 0.192792, acc 0.90625, learning_rate 0.000100108
2017-10-10T13:03:29.785087: step 2624, loss 0.130058, acc 0.96875, learning_rate 0.000100107
2017-10-10T13:03:30.388844: step 2625, loss 0.0689206, acc 0.984375, learning_rate 0.000100107
2017-10-10T13:03:30.923085: step 2626, loss 0.114697, acc 0.984375, learning_rate 0.000100107
2017-10-10T13:03:31.464853: step 2627, loss 0.137782, acc 0.953125, learning_rate 0.000100106
2017-10-10T13:03:31.970520: step 2628, loss 0.207148, acc 0.9375, learning_rate 0.000100106
2017-10-10T13:03:32.538222: step 2629, loss 0.196298, acc 0.9375, learning_rate 0.000100105
2017-10-10T13:03:33.108846: step 2630, loss 0.35225, acc 0.875, learning_rate 0.000100105
2017-10-10T13:03:33.669057: step 2631, loss 0.229937, acc 0.921875, learning_rate 0.000100104
2017-10-10T13:03:34.196847: step 2632, loss 0.218387, acc 0.90625, learning_rate 0.000100104
2017-10-10T13:03:34.766539: step 2633, loss 0.129423, acc 0.9375, learning_rate 0.000100104
2017-10-10T13:03:35.275955: step 2634, loss 0.137412, acc 0.953125, learning_rate 0.000100103
2017-10-10T13:03:35.685034: step 2635, loss 0.121034, acc 0.9375, learning_rate 0.000100103
2017-10-10T13:03:36.236439: step 2636, loss 0.164997, acc 0.953125, learning_rate 0.000100102
2017-10-10T13:03:36.732866: step 2637, loss 0.100354, acc 0.96875, learning_rate 0.000100102
2017-10-10T13:03:37.264157: step 2638, loss 0.169908, acc 0.953125, learning_rate 0.000100101
2017-10-10T13:03:37.756869: step 2639, loss 0.173789, acc 0.9375, learning_rate 0.000100101
2017-10-10T13:03:38.193860: step 2640, loss 0.143216, acc 0.9375, learning_rate 0.000100101

Evaluation:
2017-10-10T13:03:39.554415: step 2640, loss 0.225407, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2640

2017-10-10T13:03:41.128256: step 2641, loss 0.168304, acc 0.9375, learning_rate 0.0001001
2017-10-10T13:03:41.704893: step 2642, loss 0.117492, acc 0.9375, learning_rate 0.0001001
2017-10-10T13:03:42.153034: step 2643, loss 0.0937382, acc 0.96875, learning_rate 0.000100099
2017-10-10T13:03:42.688930: step 2644, loss 0.140216, acc 0.96875, learning_rate 0.000100099
2017-10-10T13:03:43.188732: step 2645, loss 0.166828, acc 0.9375, learning_rate 0.000100099
2017-10-10T13:03:43.592428: step 2646, loss 0.142033, acc 0.960784, learning_rate 0.000100098
2017-10-10T13:03:44.129304: step 2647, loss 0.203031, acc 0.9375, learning_rate 0.000100098
2017-10-10T13:03:44.669430: step 2648, loss 0.064748, acc 0.953125, learning_rate 0.000100097
2017-10-10T13:03:45.217572: step 2649, loss 0.108046, acc 0.953125, learning_rate 0.000100097
2017-10-10T13:03:45.758590: step 2650, loss 0.198475, acc 0.90625, learning_rate 0.000100097
2017-10-10T13:03:46.309126: step 2651, loss 0.172414, acc 0.9375, learning_rate 0.000100096
2017-10-10T13:03:46.786555: step 2652, loss 0.161314, acc 0.953125, learning_rate 0.000100096
2017-10-10T13:03:47.235475: step 2653, loss 0.198202, acc 0.96875, learning_rate 0.000100095
2017-10-10T13:03:47.756827: step 2654, loss 0.0735951, acc 0.984375, learning_rate 0.000100095
2017-10-10T13:03:48.343759: step 2655, loss 0.0950326, acc 0.984375, learning_rate 0.000100095
2017-10-10T13:03:48.933228: step 2656, loss 0.157851, acc 0.9375, learning_rate 0.000100094
2017-10-10T13:03:49.409956: step 2657, loss 0.190225, acc 0.90625, learning_rate 0.000100094
2017-10-10T13:03:49.829015: step 2658, loss 0.330576, acc 0.875, learning_rate 0.000100093
2017-10-10T13:03:50.188830: step 2659, loss 0.082091, acc 0.96875, learning_rate 0.000100093
2017-10-10T13:03:50.686054: step 2660, loss 0.108814, acc 0.953125, learning_rate 0.000100093
2017-10-10T13:03:51.213405: step 2661, loss 0.222247, acc 0.90625, learning_rate 0.000100092
2017-10-10T13:03:51.729396: step 2662, loss 0.229641, acc 0.953125, learning_rate 0.000100092
2017-10-10T13:03:52.223179: step 2663, loss 0.097282, acc 0.953125, learning_rate 0.000100092
2017-10-10T13:03:52.774395: step 2664, loss 0.271082, acc 0.90625, learning_rate 0.000100091
2017-10-10T13:03:53.329105: step 2665, loss 0.134429, acc 0.9375, learning_rate 0.000100091
2017-10-10T13:03:53.872884: step 2666, loss 0.278023, acc 0.890625, learning_rate 0.00010009
2017-10-10T13:03:54.410451: step 2667, loss 0.133123, acc 0.9375, learning_rate 0.00010009
2017-10-10T13:03:54.916835: step 2668, loss 0.197806, acc 0.921875, learning_rate 0.00010009
2017-10-10T13:03:55.468854: step 2669, loss 0.124367, acc 0.96875, learning_rate 0.000100089
2017-10-10T13:03:56.025341: step 2670, loss 0.207926, acc 0.953125, learning_rate 0.000100089
2017-10-10T13:03:56.596168: step 2671, loss 0.23777, acc 0.921875, learning_rate 0.000100089
2017-10-10T13:03:57.062575: step 2672, loss 0.111166, acc 0.984375, learning_rate 0.000100088
2017-10-10T13:03:57.610253: step 2673, loss 0.181341, acc 0.9375, learning_rate 0.000100088
2017-10-10T13:03:58.143555: step 2674, loss 0.077879, acc 0.96875, learning_rate 0.000100088
2017-10-10T13:03:58.671278: step 2675, loss 0.205183, acc 0.9375, learning_rate 0.000100087
2017-10-10T13:03:59.125897: step 2676, loss 0.196189, acc 0.96875, learning_rate 0.000100087
2017-10-10T13:03:59.599623: step 2677, loss 0.147453, acc 0.96875, learning_rate 0.000100086
2017-10-10T13:04:00.097195: step 2678, loss 0.180154, acc 0.921875, learning_rate 0.000100086
2017-10-10T13:04:00.600805: step 2679, loss 0.176381, acc 0.9375, learning_rate 0.000100086
2017-10-10T13:04:01.121257: step 2680, loss 0.142075, acc 0.9375, learning_rate 0.000100085

Evaluation:
2017-10-10T13:04:02.432315: step 2680, loss 0.222363, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2680

2017-10-10T13:04:04.490853: step 2681, loss 0.0660278, acc 0.984375, learning_rate 0.000100085
2017-10-10T13:04:04.975445: step 2682, loss 0.15737, acc 0.9375, learning_rate 0.000100085
2017-10-10T13:04:05.509017: step 2683, loss 0.0726878, acc 0.96875, learning_rate 0.000100084
2017-10-10T13:04:06.079618: step 2684, loss 0.100384, acc 0.96875, learning_rate 0.000100084
2017-10-10T13:04:06.599943: step 2685, loss 0.198764, acc 0.90625, learning_rate 0.000100084
2017-10-10T13:04:07.148120: step 2686, loss 0.129801, acc 0.953125, learning_rate 0.000100083
2017-10-10T13:04:07.745067: step 2687, loss 0.15017, acc 0.9375, learning_rate 0.000100083
2017-10-10T13:04:08.377826: step 2688, loss 0.0688865, acc 0.984375, learning_rate 0.000100083
2017-10-10T13:04:08.843757: step 2689, loss 0.131764, acc 0.953125, learning_rate 0.000100082
2017-10-10T13:04:09.220262: step 2690, loss 0.14754, acc 0.953125, learning_rate 0.000100082
2017-10-10T13:04:09.672972: step 2691, loss 0.109216, acc 0.984375, learning_rate 0.000100082
2017-10-10T13:04:10.218855: step 2692, loss 0.147401, acc 0.9375, learning_rate 0.000100081
2017-10-10T13:04:10.688601: step 2693, loss 0.170064, acc 0.921875, learning_rate 0.000100081
2017-10-10T13:04:11.127654: step 2694, loss 0.208248, acc 0.9375, learning_rate 0.000100081
2017-10-10T13:04:11.761080: step 2695, loss 0.139429, acc 0.96875, learning_rate 0.00010008
2017-10-10T13:04:12.221443: step 2696, loss 0.160459, acc 0.921875, learning_rate 0.00010008
2017-10-10T13:04:12.656255: step 2697, loss 0.194269, acc 0.921875, learning_rate 0.00010008
2017-10-10T13:04:13.176821: step 2698, loss 0.2102, acc 0.875, learning_rate 0.000100079
2017-10-10T13:04:13.689607: step 2699, loss 0.175273, acc 0.921875, learning_rate 0.000100079
2017-10-10T13:04:14.254295: step 2700, loss 0.137117, acc 0.9375, learning_rate 0.000100079
2017-10-10T13:04:14.774604: step 2701, loss 0.0997537, acc 0.96875, learning_rate 0.000100078
2017-10-10T13:04:15.283287: step 2702, loss 0.299769, acc 0.90625, learning_rate 0.000100078
2017-10-10T13:04:15.831419: step 2703, loss 0.165458, acc 0.9375, learning_rate 0.000100078
2017-10-10T13:04:16.372622: step 2704, loss 0.138853, acc 0.9375, learning_rate 0.000100077
2017-10-10T13:04:16.871165: step 2705, loss 0.14853, acc 0.96875, learning_rate 0.000100077
2017-10-10T13:04:17.364884: step 2706, loss 0.211958, acc 0.921875, learning_rate 0.000100077
2017-10-10T13:04:17.872861: step 2707, loss 0.113635, acc 0.96875, learning_rate 0.000100076
2017-10-10T13:04:18.400032: step 2708, loss 0.141814, acc 0.96875, learning_rate 0.000100076
2017-10-10T13:04:18.907266: step 2709, loss 0.166787, acc 0.953125, learning_rate 0.000100076
2017-10-10T13:04:19.440863: step 2710, loss 0.0683761, acc 0.984375, learning_rate 0.000100076
2017-10-10T13:04:19.948891: step 2711, loss 0.144141, acc 0.953125, learning_rate 0.000100075
2017-10-10T13:04:20.467464: step 2712, loss 0.155349, acc 0.9375, learning_rate 0.000100075
2017-10-10T13:04:21.044146: step 2713, loss 0.184578, acc 0.9375, learning_rate 0.000100075
2017-10-10T13:04:21.541968: step 2714, loss 0.156253, acc 0.96875, learning_rate 0.000100074
2017-10-10T13:04:22.072839: step 2715, loss 0.0961217, acc 0.984375, learning_rate 0.000100074
2017-10-10T13:04:22.630530: step 2716, loss 0.14132, acc 0.9375, learning_rate 0.000100074
2017-10-10T13:04:23.172893: step 2717, loss 0.100432, acc 0.984375, learning_rate 0.000100073
2017-10-10T13:04:23.757121: step 2718, loss 0.0865297, acc 0.984375, learning_rate 0.000100073
2017-10-10T13:04:24.337009: step 2719, loss 0.218465, acc 0.890625, learning_rate 0.000100073
2017-10-10T13:04:24.832299: step 2720, loss 0.141633, acc 0.953125, learning_rate 0.000100073

Evaluation:
2017-10-10T13:04:26.124709: step 2720, loss 0.224696, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2720

2017-10-10T13:04:27.592813: step 2721, loss 0.167301, acc 0.953125, learning_rate 0.000100072
2017-10-10T13:04:28.084325: step 2722, loss 0.184963, acc 0.9375, learning_rate 0.000100072
2017-10-10T13:04:28.627644: step 2723, loss 0.0521377, acc 1, learning_rate 0.000100072
2017-10-10T13:04:29.153432: step 2724, loss 0.139099, acc 0.953125, learning_rate 0.000100071
2017-10-10T13:04:29.716077: step 2725, loss 0.130682, acc 0.96875, learning_rate 0.000100071
2017-10-10T13:04:30.228972: step 2726, loss 0.186114, acc 0.953125, learning_rate 0.000100071
2017-10-10T13:04:30.796906: step 2727, loss 0.0803285, acc 0.984375, learning_rate 0.00010007
2017-10-10T13:04:31.431610: step 2728, loss 0.042445, acc 1, learning_rate 0.00010007
2017-10-10T13:04:31.884950: step 2729, loss 0.182289, acc 0.921875, learning_rate 0.00010007
2017-10-10T13:04:32.400545: step 2730, loss 0.321295, acc 0.90625, learning_rate 0.00010007
2017-10-10T13:04:32.888952: step 2731, loss 0.123665, acc 0.953125, learning_rate 0.000100069
2017-10-10T13:04:33.388463: step 2732, loss 0.12666, acc 0.953125, learning_rate 0.000100069
2017-10-10T13:04:33.986976: step 2733, loss 0.120129, acc 0.953125, learning_rate 0.000100069
2017-10-10T13:04:34.445050: step 2734, loss 0.258462, acc 0.90625, learning_rate 0.000100068
2017-10-10T13:04:34.896823: step 2735, loss 0.174848, acc 0.953125, learning_rate 0.000100068
2017-10-10T13:04:35.332955: step 2736, loss 0.0839855, acc 0.96875, learning_rate 0.000100068
2017-10-10T13:04:35.812732: step 2737, loss 0.128642, acc 0.953125, learning_rate 0.000100068
2017-10-10T13:04:36.280828: step 2738, loss 0.264062, acc 0.90625, learning_rate 0.000100067
2017-10-10T13:04:36.820940: step 2739, loss 0.163573, acc 0.953125, learning_rate 0.000100067
2017-10-10T13:04:37.302602: step 2740, loss 0.221092, acc 0.890625, learning_rate 0.000100067
2017-10-10T13:04:37.840119: step 2741, loss 0.131669, acc 0.953125, learning_rate 0.000100067
2017-10-10T13:04:38.393209: step 2742, loss 0.280035, acc 0.9375, learning_rate 0.000100066
2017-10-10T13:04:38.895559: step 2743, loss 0.141659, acc 0.96875, learning_rate 0.000100066
2017-10-10T13:04:39.340883: step 2744, loss 0.165299, acc 0.960784, learning_rate 0.000100066
2017-10-10T13:04:39.852890: step 2745, loss 0.117251, acc 0.953125, learning_rate 0.000100065
2017-10-10T13:04:40.407712: step 2746, loss 0.247554, acc 0.9375, learning_rate 0.000100065
2017-10-10T13:04:40.925604: step 2747, loss 0.0981687, acc 0.96875, learning_rate 0.000100065
2017-10-10T13:04:41.424861: step 2748, loss 0.189024, acc 0.921875, learning_rate 0.000100065
2017-10-10T13:04:41.940205: step 2749, loss 0.109829, acc 0.96875, learning_rate 0.000100064
2017-10-10T13:04:42.451137: step 2750, loss 0.124044, acc 0.96875, learning_rate 0.000100064
2017-10-10T13:04:43.021243: step 2751, loss 0.183212, acc 0.9375, learning_rate 0.000100064
2017-10-10T13:04:43.482345: step 2752, loss 0.293152, acc 0.921875, learning_rate 0.000100064
2017-10-10T13:04:44.016883: step 2753, loss 0.122292, acc 0.96875, learning_rate 0.000100063
2017-10-10T13:04:44.476366: step 2754, loss 0.215117, acc 0.921875, learning_rate 0.000100063
2017-10-10T13:04:44.979394: step 2755, loss 0.139535, acc 0.953125, learning_rate 0.000100063
2017-10-10T13:04:45.456871: step 2756, loss 0.341677, acc 0.90625, learning_rate 0.000100063
2017-10-10T13:04:46.032947: step 2757, loss 0.118372, acc 0.953125, learning_rate 0.000100062
2017-10-10T13:04:46.563026: step 2758, loss 0.205125, acc 0.953125, learning_rate 0.000100062
2017-10-10T13:04:47.052958: step 2759, loss 0.202814, acc 0.953125, learning_rate 0.000100062
2017-10-10T13:04:47.596951: step 2760, loss 0.103958, acc 0.96875, learning_rate 0.000100062

Evaluation:
2017-10-10T13:04:48.823476: step 2760, loss 0.223056, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2760

2017-10-10T13:04:50.284917: step 2761, loss 0.0804774, acc 0.96875, learning_rate 0.000100061
2017-10-10T13:04:50.777144: step 2762, loss 0.0838743, acc 1, learning_rate 0.000100061
2017-10-10T13:04:51.194161: step 2763, loss 0.143529, acc 0.921875, learning_rate 0.000100061
2017-10-10T13:04:51.697110: step 2764, loss 0.176179, acc 0.953125, learning_rate 0.000100061
2017-10-10T13:04:52.269255: step 2765, loss 0.100672, acc 0.96875, learning_rate 0.00010006
2017-10-10T13:04:52.804826: step 2766, loss 0.149801, acc 0.953125, learning_rate 0.00010006
2017-10-10T13:04:53.316324: step 2767, loss 0.192567, acc 0.953125, learning_rate 0.00010006
2017-10-10T13:04:53.869283: step 2768, loss 0.15637, acc 0.953125, learning_rate 0.00010006
2017-10-10T13:04:54.500684: step 2769, loss 0.276403, acc 0.890625, learning_rate 0.000100059
2017-10-10T13:04:54.875411: step 2770, loss 0.186222, acc 0.9375, learning_rate 0.000100059
2017-10-10T13:04:55.305234: step 2771, loss 0.218073, acc 0.9375, learning_rate 0.000100059
2017-10-10T13:04:55.792364: step 2772, loss 0.0503839, acc 0.984375, learning_rate 0.000100059
2017-10-10T13:04:56.402168: step 2773, loss 0.154602, acc 0.921875, learning_rate 0.000100058
2017-10-10T13:04:56.980879: step 2774, loss 0.227297, acc 0.90625, learning_rate 0.000100058
2017-10-10T13:04:57.425552: step 2775, loss 0.137011, acc 0.9375, learning_rate 0.000100058
2017-10-10T13:04:57.873285: step 2776, loss 0.172276, acc 0.921875, learning_rate 0.000100058
2017-10-10T13:04:58.404689: step 2777, loss 0.0982842, acc 0.953125, learning_rate 0.000100057
2017-10-10T13:04:58.949042: step 2778, loss 0.151002, acc 0.953125, learning_rate 0.000100057
2017-10-10T13:04:59.493199: step 2779, loss 0.159579, acc 0.9375, learning_rate 0.000100057
2017-10-10T13:05:00.050844: step 2780, loss 0.199846, acc 0.890625, learning_rate 0.000100057
2017-10-10T13:05:00.594792: step 2781, loss 0.233316, acc 0.90625, learning_rate 0.000100056
2017-10-10T13:05:01.121107: step 2782, loss 0.202604, acc 0.953125, learning_rate 0.000100056
2017-10-10T13:05:01.612953: step 2783, loss 0.122962, acc 0.96875, learning_rate 0.000100056
2017-10-10T13:05:02.162582: step 2784, loss 0.133016, acc 0.953125, learning_rate 0.000100056
2017-10-10T13:05:02.648877: step 2785, loss 0.1037, acc 0.96875, learning_rate 0.000100056
2017-10-10T13:05:03.160500: step 2786, loss 0.234143, acc 0.90625, learning_rate 0.000100055
2017-10-10T13:05:03.700869: step 2787, loss 0.0752424, acc 0.984375, learning_rate 0.000100055
2017-10-10T13:05:04.239872: step 2788, loss 0.197444, acc 0.921875, learning_rate 0.000100055
2017-10-10T13:05:04.688997: step 2789, loss 0.232934, acc 0.921875, learning_rate 0.000100055
2017-10-10T13:05:05.157160: step 2790, loss 0.197634, acc 0.953125, learning_rate 0.000100054
2017-10-10T13:05:05.741613: step 2791, loss 0.127134, acc 0.953125, learning_rate 0.000100054
2017-10-10T13:05:06.237061: step 2792, loss 0.0560613, acc 1, learning_rate 0.000100054
2017-10-10T13:05:06.871581: step 2793, loss 0.231726, acc 0.875, learning_rate 0.000100054
2017-10-10T13:05:07.341189: step 2794, loss 0.108557, acc 0.96875, learning_rate 0.000100054
2017-10-10T13:05:07.745160: step 2795, loss 0.148049, acc 0.953125, learning_rate 0.000100053
2017-10-10T13:05:08.309034: step 2796, loss 0.189926, acc 0.9375, learning_rate 0.000100053
2017-10-10T13:05:08.829281: step 2797, loss 0.0529725, acc 0.96875, learning_rate 0.000100053
2017-10-10T13:05:09.408965: step 2798, loss 0.137523, acc 0.96875, learning_rate 0.000100053
2017-10-10T13:05:10.034609: step 2799, loss 0.187896, acc 0.953125, learning_rate 0.000100052
2017-10-10T13:05:10.593005: step 2800, loss 0.114717, acc 0.96875, learning_rate 0.000100052

Evaluation:
2017-10-10T13:05:11.823432: step 2800, loss 0.223495, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2800

2017-10-10T13:05:13.672842: step 2801, loss 0.0861601, acc 0.984375, learning_rate 0.000100052
2017-10-10T13:05:14.170874: step 2802, loss 0.101832, acc 0.953125, learning_rate 0.000100052
2017-10-10T13:05:14.679972: step 2803, loss 0.113167, acc 0.96875, learning_rate 0.000100052
2017-10-10T13:05:15.156921: step 2804, loss 0.112091, acc 0.953125, learning_rate 0.000100051
2017-10-10T13:05:15.684894: step 2805, loss 0.163162, acc 0.9375, learning_rate 0.000100051
2017-10-10T13:05:16.141039: step 2806, loss 0.147061, acc 0.96875, learning_rate 0.000100051
2017-10-10T13:05:16.709045: step 2807, loss 0.158552, acc 0.9375, learning_rate 0.000100051
2017-10-10T13:05:17.171691: step 2808, loss 0.233069, acc 0.921875, learning_rate 0.000100051
2017-10-10T13:05:17.589124: step 2809, loss 0.142327, acc 0.96875, learning_rate 0.00010005
2017-10-10T13:05:18.049816: step 2810, loss 0.122927, acc 0.953125, learning_rate 0.00010005
2017-10-10T13:05:18.551344: step 2811, loss 0.192019, acc 0.921875, learning_rate 0.00010005
2017-10-10T13:05:19.040850: step 2812, loss 0.141629, acc 0.9375, learning_rate 0.00010005
2017-10-10T13:05:19.623168: step 2813, loss 0.101873, acc 0.984375, learning_rate 0.00010005
2017-10-10T13:05:20.233676: step 2814, loss 0.220048, acc 0.921875, learning_rate 0.000100049
2017-10-10T13:05:20.699308: step 2815, loss 0.154521, acc 0.9375, learning_rate 0.000100049
2017-10-10T13:05:21.145279: step 2816, loss 0.105204, acc 0.96875, learning_rate 0.000100049
2017-10-10T13:05:21.584853: step 2817, loss 0.117563, acc 0.96875, learning_rate 0.000100049
2017-10-10T13:05:22.093001: step 2818, loss 0.198728, acc 0.9375, learning_rate 0.000100049
2017-10-10T13:05:22.559197: step 2819, loss 0.128863, acc 0.9375, learning_rate 0.000100048
2017-10-10T13:05:23.102181: step 2820, loss 0.0503439, acc 1, learning_rate 0.000100048
2017-10-10T13:05:23.575637: step 2821, loss 0.299902, acc 0.90625, learning_rate 0.000100048
2017-10-10T13:05:24.123973: step 2822, loss 0.231355, acc 0.953125, learning_rate 0.000100048
2017-10-10T13:05:24.693576: step 2823, loss 0.0731115, acc 0.984375, learning_rate 0.000100048
2017-10-10T13:05:25.229558: step 2824, loss 0.279313, acc 0.921875, learning_rate 0.000100047
2017-10-10T13:05:25.844845: step 2825, loss 0.225008, acc 0.9375, learning_rate 0.000100047
2017-10-10T13:05:26.336990: step 2826, loss 0.144744, acc 0.96875, learning_rate 0.000100047
2017-10-10T13:05:26.888854: step 2827, loss 0.115655, acc 0.953125, learning_rate 0.000100047
2017-10-10T13:05:27.356898: step 2828, loss 0.167177, acc 0.9375, learning_rate 0.000100047
2017-10-10T13:05:27.853132: step 2829, loss 0.194253, acc 0.953125, learning_rate 0.000100046
2017-10-10T13:05:28.317277: step 2830, loss 0.192723, acc 0.9375, learning_rate 0.000100046
2017-10-10T13:05:28.812105: step 2831, loss 0.097873, acc 0.984375, learning_rate 0.000100046
2017-10-10T13:05:29.268810: step 2832, loss 0.180908, acc 0.9375, learning_rate 0.000100046
2017-10-10T13:05:29.848694: step 2833, loss 0.109249, acc 0.96875, learning_rate 0.000100046
2017-10-10T13:05:30.405144: step 2834, loss 0.193018, acc 0.953125, learning_rate 0.000100045
2017-10-10T13:05:30.897815: step 2835, loss 0.159605, acc 0.953125, learning_rate 0.000100045
2017-10-10T13:05:31.464061: step 2836, loss 0.151995, acc 0.9375, learning_rate 0.000100045
2017-10-10T13:05:31.947487: step 2837, loss 0.265133, acc 0.890625, learning_rate 0.000100045
2017-10-10T13:05:32.441030: step 2838, loss 0.129629, acc 0.953125, learning_rate 0.000100045
2017-10-10T13:05:32.942781: step 2839, loss 0.137559, acc 0.921875, learning_rate 0.000100045
2017-10-10T13:05:33.497028: step 2840, loss 0.0808117, acc 0.953125, learning_rate 0.000100044

Evaluation:
2017-10-10T13:05:34.566843: step 2840, loss 0.222369, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2840

2017-10-10T13:05:36.005611: step 2841, loss 0.192804, acc 0.921875, learning_rate 0.000100044
2017-10-10T13:05:36.352892: step 2842, loss 0.192068, acc 0.960784, learning_rate 0.000100044
2017-10-10T13:05:36.880957: step 2843, loss 0.15626, acc 0.9375, learning_rate 0.000100044
2017-10-10T13:05:37.288092: step 2844, loss 0.0732075, acc 1, learning_rate 0.000100044
2017-10-10T13:05:37.736627: step 2845, loss 0.283674, acc 0.890625, learning_rate 0.000100043
2017-10-10T13:05:38.236837: step 2846, loss 0.0625737, acc 1, learning_rate 0.000100043
2017-10-10T13:05:38.812915: step 2847, loss 0.060227, acc 0.984375, learning_rate 0.000100043
2017-10-10T13:05:39.373321: step 2848, loss 0.171458, acc 0.9375, learning_rate 0.000100043
2017-10-10T13:05:39.969168: step 2849, loss 0.196511, acc 0.9375, learning_rate 0.000100043
2017-10-10T13:05:40.376774: step 2850, loss 0.201293, acc 0.9375, learning_rate 0.000100043
2017-10-10T13:05:40.869560: step 2851, loss 0.119985, acc 0.9375, learning_rate 0.000100042
2017-10-10T13:05:41.296892: step 2852, loss 0.111092, acc 0.96875, learning_rate 0.000100042
2017-10-10T13:05:41.905068: step 2853, loss 0.135579, acc 0.9375, learning_rate 0.000100042
2017-10-10T13:05:42.444896: step 2854, loss 0.111446, acc 0.96875, learning_rate 0.000100042
2017-10-10T13:05:43.105595: step 2855, loss 0.134347, acc 0.96875, learning_rate 0.000100042
2017-10-10T13:05:43.608797: step 2856, loss 0.183723, acc 0.9375, learning_rate 0.000100042
2017-10-10T13:05:44.058419: step 2857, loss 0.138425, acc 0.984375, learning_rate 0.000100041
2017-10-10T13:05:44.546002: step 2858, loss 0.169853, acc 0.9375, learning_rate 0.000100041
2017-10-10T13:05:45.093145: step 2859, loss 0.119683, acc 0.953125, learning_rate 0.000100041
2017-10-10T13:05:45.582439: step 2860, loss 0.0487877, acc 1, learning_rate 0.000100041
2017-10-10T13:05:46.087112: step 2861, loss 0.102018, acc 0.984375, learning_rate 0.000100041
2017-10-10T13:05:46.567190: step 2862, loss 0.109388, acc 0.96875, learning_rate 0.000100041
2017-10-10T13:05:47.143082: step 2863, loss 0.13739, acc 0.9375, learning_rate 0.00010004
2017-10-10T13:05:47.691715: step 2864, loss 0.196945, acc 0.953125, learning_rate 0.00010004
2017-10-10T13:05:48.226534: step 2865, loss 0.0224532, acc 1, learning_rate 0.00010004
2017-10-10T13:05:48.761360: step 2866, loss 0.166777, acc 0.921875, learning_rate 0.00010004
2017-10-10T13:05:49.279004: step 2867, loss 0.179635, acc 0.921875, learning_rate 0.00010004
2017-10-10T13:05:49.823827: step 2868, loss 0.115184, acc 0.984375, learning_rate 0.00010004
2017-10-10T13:05:50.340962: step 2869, loss 0.256828, acc 0.90625, learning_rate 0.000100039
2017-10-10T13:05:50.883724: step 2870, loss 0.0864157, acc 0.984375, learning_rate 0.000100039
2017-10-10T13:05:51.395369: step 2871, loss 0.136778, acc 0.96875, learning_rate 0.000100039
2017-10-10T13:05:51.965051: step 2872, loss 0.101394, acc 0.96875, learning_rate 0.000100039
2017-10-10T13:05:52.564772: step 2873, loss 0.163939, acc 0.953125, learning_rate 0.000100039
2017-10-10T13:05:53.084747: step 2874, loss 0.11491, acc 0.9375, learning_rate 0.000100039
2017-10-10T13:05:53.589091: step 2875, loss 0.229361, acc 0.90625, learning_rate 0.000100038
2017-10-10T13:05:54.172746: step 2876, loss 0.0985396, acc 0.953125, learning_rate 0.000100038
2017-10-10T13:05:54.659911: step 2877, loss 0.135596, acc 0.953125, learning_rate 0.000100038
2017-10-10T13:05:55.183873: step 2878, loss 0.113628, acc 0.96875, learning_rate 0.000100038
2017-10-10T13:05:55.687112: step 2879, loss 0.137979, acc 0.9375, learning_rate 0.000100038
2017-10-10T13:05:56.192837: step 2880, loss 0.141278, acc 0.953125, learning_rate 0.000100038

Evaluation:
2017-10-10T13:05:57.374400: step 2880, loss 0.223953, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2880

2017-10-10T13:05:58.788982: step 2881, loss 0.189321, acc 0.9375, learning_rate 0.000100038
2017-10-10T13:05:59.251892: step 2882, loss 0.105226, acc 0.96875, learning_rate 0.000100037
2017-10-10T13:05:59.808928: step 2883, loss 0.247351, acc 0.921875, learning_rate 0.000100037
2017-10-10T13:06:00.356846: step 2884, loss 0.0905554, acc 0.96875, learning_rate 0.000100037
2017-10-10T13:06:00.883024: step 2885, loss 0.105971, acc 0.96875, learning_rate 0.000100037
2017-10-10T13:06:01.488839: step 2886, loss 0.189016, acc 0.9375, learning_rate 0.000100037
2017-10-10T13:06:02.016922: step 2887, loss 0.113079, acc 0.984375, learning_rate 0.000100037
2017-10-10T13:06:02.564931: step 2888, loss 0.113307, acc 0.953125, learning_rate 0.000100036
2017-10-10T13:06:03.136872: step 2889, loss 0.104407, acc 0.96875, learning_rate 0.000100036
2017-10-10T13:06:03.617755: step 2890, loss 0.126689, acc 0.953125, learning_rate 0.000100036
2017-10-10T13:06:04.051170: step 2891, loss 0.153123, acc 0.9375, learning_rate 0.000100036
2017-10-10T13:06:04.625177: step 2892, loss 0.0843033, acc 0.984375, learning_rate 0.000100036
2017-10-10T13:06:05.197357: step 2893, loss 0.147505, acc 0.9375, learning_rate 0.000100036
2017-10-10T13:06:05.787289: step 2894, loss 0.220497, acc 0.921875, learning_rate 0.000100036
2017-10-10T13:06:06.390712: step 2895, loss 0.175384, acc 0.953125, learning_rate 0.000100035
2017-10-10T13:06:06.773133: step 2896, loss 0.144232, acc 0.953125, learning_rate 0.000100035
2017-10-10T13:06:07.196893: step 2897, loss 0.106453, acc 0.984375, learning_rate 0.000100035
2017-10-10T13:06:07.813087: step 2898, loss 0.214552, acc 0.921875, learning_rate 0.000100035
2017-10-10T13:06:08.384917: step 2899, loss 0.122042, acc 0.96875, learning_rate 0.000100035
2017-10-10T13:06:08.921006: step 2900, loss 0.180569, acc 0.953125, learning_rate 0.000100035
2017-10-10T13:06:09.444979: step 2901, loss 0.123211, acc 0.953125, learning_rate 0.000100035
2017-10-10T13:06:09.912964: step 2902, loss 0.070967, acc 0.984375, learning_rate 0.000100034
2017-10-10T13:06:10.433083: step 2903, loss 0.115867, acc 0.96875, learning_rate 0.000100034
2017-10-10T13:06:10.936959: step 2904, loss 0.129605, acc 0.953125, learning_rate 0.000100034
2017-10-10T13:06:11.464935: step 2905, loss 0.142293, acc 0.9375, learning_rate 0.000100034
2017-10-10T13:06:11.988788: step 2906, loss 0.186926, acc 0.953125, learning_rate 0.000100034
2017-10-10T13:06:12.485010: step 2907, loss 0.246045, acc 0.90625, learning_rate 0.000100034
2017-10-10T13:06:13.017016: step 2908, loss 0.159877, acc 0.9375, learning_rate 0.000100034
2017-10-10T13:06:13.536768: step 2909, loss 0.17575, acc 0.9375, learning_rate 0.000100033
2017-10-10T13:06:14.169240: step 2910, loss 0.0807564, acc 0.984375, learning_rate 0.000100033
2017-10-10T13:06:14.679682: step 2911, loss 0.168922, acc 0.9375, learning_rate 0.000100033
2017-10-10T13:06:15.239117: step 2912, loss 0.165049, acc 0.90625, learning_rate 0.000100033
2017-10-10T13:06:15.829143: step 2913, loss 0.176744, acc 0.96875, learning_rate 0.000100033
2017-10-10T13:06:16.296851: step 2914, loss 0.160148, acc 0.9375, learning_rate 0.000100033
2017-10-10T13:06:16.856997: step 2915, loss 0.179149, acc 0.96875, learning_rate 0.000100033
2017-10-10T13:06:17.395684: step 2916, loss 0.138621, acc 0.9375, learning_rate 0.000100033
2017-10-10T13:06:17.880906: step 2917, loss 0.127981, acc 0.953125, learning_rate 0.000100032
2017-10-10T13:06:18.400874: step 2918, loss 0.118299, acc 0.953125, learning_rate 0.000100032
2017-10-10T13:06:18.925293: step 2919, loss 0.149808, acc 0.953125, learning_rate 0.000100032
2017-10-10T13:06:19.483612: step 2920, loss 0.128986, acc 0.9375, learning_rate 0.000100032

Evaluation:
2017-10-10T13:06:20.666440: step 2920, loss 0.222535, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2920

2017-10-10T13:06:22.464913: step 2921, loss 0.192714, acc 0.921875, learning_rate 0.000100032
2017-10-10T13:06:22.982313: step 2922, loss 0.140587, acc 0.953125, learning_rate 0.000100032
2017-10-10T13:06:23.425765: step 2923, loss 0.0924904, acc 0.984375, learning_rate 0.000100032
2017-10-10T13:06:23.948074: step 2924, loss 0.064459, acc 0.984375, learning_rate 0.000100031
2017-10-10T13:06:24.468825: step 2925, loss 0.143003, acc 0.9375, learning_rate 0.000100031
2017-10-10T13:06:24.958288: step 2926, loss 0.177393, acc 0.9375, learning_rate 0.000100031
2017-10-10T13:06:25.512990: step 2927, loss 0.152742, acc 0.953125, learning_rate 0.000100031
2017-10-10T13:06:25.945045: step 2928, loss 0.193398, acc 0.921875, learning_rate 0.000100031
2017-10-10T13:06:26.425001: step 2929, loss 0.161007, acc 0.9375, learning_rate 0.000100031
2017-10-10T13:06:26.787642: step 2930, loss 0.19701, acc 0.921875, learning_rate 0.000100031
2017-10-10T13:06:27.339574: step 2931, loss 0.244208, acc 0.890625, learning_rate 0.000100031
2017-10-10T13:06:27.828527: step 2932, loss 0.224267, acc 0.90625, learning_rate 0.00010003
2017-10-10T13:06:28.367973: step 2933, loss 0.17318, acc 0.953125, learning_rate 0.00010003
2017-10-10T13:06:28.929001: step 2934, loss 0.173715, acc 0.9375, learning_rate 0.00010003
2017-10-10T13:06:29.397151: step 2935, loss 0.161034, acc 0.953125, learning_rate 0.00010003
2017-10-10T13:06:29.844589: step 2936, loss 0.155661, acc 0.9375, learning_rate 0.00010003
2017-10-10T13:06:30.251574: step 2937, loss 0.149374, acc 0.953125, learning_rate 0.00010003
2017-10-10T13:06:30.793446: step 2938, loss 0.0790608, acc 1, learning_rate 0.00010003
2017-10-10T13:06:31.337033: step 2939, loss 0.259321, acc 0.9375, learning_rate 0.00010003
2017-10-10T13:06:31.800049: step 2940, loss 0.119991, acc 0.960784, learning_rate 0.000100029
2017-10-10T13:06:32.272333: step 2941, loss 0.157358, acc 0.984375, learning_rate 0.000100029
2017-10-10T13:06:32.808005: step 2942, loss 0.226343, acc 0.90625, learning_rate 0.000100029
2017-10-10T13:06:33.352151: step 2943, loss 0.15937, acc 0.953125, learning_rate 0.000100029
2017-10-10T13:06:33.856861: step 2944, loss 0.213765, acc 0.90625, learning_rate 0.000100029
2017-10-10T13:06:34.377794: step 2945, loss 0.211863, acc 0.921875, learning_rate 0.000100029
2017-10-10T13:06:34.866708: step 2946, loss 0.131746, acc 0.953125, learning_rate 0.000100029
2017-10-10T13:06:35.331851: step 2947, loss 0.176752, acc 0.921875, learning_rate 0.000100029
2017-10-10T13:06:35.756251: step 2948, loss 0.171706, acc 0.9375, learning_rate 0.000100029
2017-10-10T13:06:36.219831: step 2949, loss 0.155049, acc 0.9375, learning_rate 0.000100028
2017-10-10T13:06:36.705111: step 2950, loss 0.253306, acc 0.890625, learning_rate 0.000100028
2017-10-10T13:06:37.237132: step 2951, loss 0.121294, acc 0.953125, learning_rate 0.000100028
2017-10-10T13:06:37.729002: step 2952, loss 0.181178, acc 0.90625, learning_rate 0.000100028
2017-10-10T13:06:38.301068: step 2953, loss 0.0941923, acc 0.984375, learning_rate 0.000100028
2017-10-10T13:06:38.844961: step 2954, loss 0.0793303, acc 0.984375, learning_rate 0.000100028
2017-10-10T13:06:39.328875: step 2955, loss 0.134205, acc 0.96875, learning_rate 0.000100028
2017-10-10T13:06:39.860829: step 2956, loss 0.26708, acc 0.921875, learning_rate 0.000100028
2017-10-10T13:06:40.408859: step 2957, loss 0.144388, acc 0.953125, learning_rate 0.000100028
2017-10-10T13:06:40.965004: step 2958, loss 0.124329, acc 0.921875, learning_rate 0.000100027
2017-10-10T13:06:41.504967: step 2959, loss 0.240004, acc 0.90625, learning_rate 0.000100027
2017-10-10T13:06:41.997488: step 2960, loss 0.0931524, acc 0.96875, learning_rate 0.000100027

Evaluation:
2017-10-10T13:06:43.174416: step 2960, loss 0.221328, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-2960

2017-10-10T13:06:44.582128: step 2961, loss 0.128407, acc 0.9375, learning_rate 0.000100027
2017-10-10T13:06:44.994990: step 2962, loss 0.216947, acc 0.953125, learning_rate 0.000100027
2017-10-10T13:06:45.430987: step 2963, loss 0.143481, acc 0.96875, learning_rate 0.000100027
2017-10-10T13:06:45.893651: step 2964, loss 0.122495, acc 0.953125, learning_rate 0.000100027
2017-10-10T13:06:46.439096: step 2965, loss 0.153492, acc 0.953125, learning_rate 0.000100027
2017-10-10T13:06:46.984569: step 2966, loss 0.179704, acc 0.921875, learning_rate 0.000100027
2017-10-10T13:06:47.542711: step 2967, loss 0.259919, acc 0.90625, learning_rate 0.000100026
2017-10-10T13:06:48.139334: step 2968, loss 0.049738, acc 0.984375, learning_rate 0.000100026
2017-10-10T13:06:48.763313: step 2969, loss 0.149245, acc 0.96875, learning_rate 0.000100026
2017-10-10T13:06:49.253833: step 2970, loss 0.137876, acc 0.953125, learning_rate 0.000100026
2017-10-10T13:06:49.692876: step 2971, loss 0.0769219, acc 0.96875, learning_rate 0.000100026
2017-10-10T13:06:50.146735: step 2972, loss 0.23784, acc 0.921875, learning_rate 0.000100026
2017-10-10T13:06:50.628924: step 2973, loss 0.0599317, acc 0.984375, learning_rate 0.000100026
2017-10-10T13:06:51.168907: step 2974, loss 0.12913, acc 0.96875, learning_rate 0.000100026
2017-10-10T13:06:51.820317: step 2975, loss 0.268106, acc 0.90625, learning_rate 0.000100026
2017-10-10T13:06:52.282908: step 2976, loss 0.0618861, acc 0.96875, learning_rate 0.000100025
2017-10-10T13:06:52.719578: step 2977, loss 0.156255, acc 0.953125, learning_rate 0.000100025
2017-10-10T13:06:53.244182: step 2978, loss 0.169626, acc 0.953125, learning_rate 0.000100025
2017-10-10T13:06:53.737944: step 2979, loss 0.163203, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:06:54.315495: step 2980, loss 0.0716755, acc 0.984375, learning_rate 0.000100025
2017-10-10T13:06:54.863820: step 2981, loss 0.172853, acc 0.9375, learning_rate 0.000100025
2017-10-10T13:06:55.456821: step 2982, loss 0.157971, acc 0.9375, learning_rate 0.000100025
2017-10-10T13:06:56.039967: step 2983, loss 0.217472, acc 0.90625, learning_rate 0.000100025
2017-10-10T13:06:56.575510: step 2984, loss 0.110825, acc 0.96875, learning_rate 0.000100025
2017-10-10T13:06:57.100339: step 2985, loss 0.0821362, acc 0.96875, learning_rate 0.000100025
2017-10-10T13:06:57.609231: step 2986, loss 0.156442, acc 0.9375, learning_rate 0.000100024
2017-10-10T13:06:58.096000: step 2987, loss 0.110146, acc 0.953125, learning_rate 0.000100024
2017-10-10T13:06:58.589060: step 2988, loss 0.186677, acc 0.9375, learning_rate 0.000100024
2017-10-10T13:06:59.062217: step 2989, loss 0.2229, acc 0.890625, learning_rate 0.000100024
2017-10-10T13:06:59.537688: step 2990, loss 0.0931456, acc 0.953125, learning_rate 0.000100024
2017-10-10T13:07:00.085832: step 2991, loss 0.0995138, acc 0.953125, learning_rate 0.000100024
2017-10-10T13:07:00.672947: step 2992, loss 0.209264, acc 0.921875, learning_rate 0.000100024
2017-10-10T13:07:01.212752: step 2993, loss 0.12618, acc 0.96875, learning_rate 0.000100024
2017-10-10T13:07:01.745542: step 2994, loss 0.0316153, acc 1, learning_rate 0.000100024
2017-10-10T13:07:02.249281: step 2995, loss 0.0545565, acc 0.984375, learning_rate 0.000100024
2017-10-10T13:07:02.777024: step 2996, loss 0.118346, acc 0.96875, learning_rate 0.000100023
2017-10-10T13:07:03.308919: step 2997, loss 0.0963567, acc 0.96875, learning_rate 0.000100023
2017-10-10T13:07:03.880931: step 2998, loss 0.174993, acc 0.9375, learning_rate 0.000100023
2017-10-10T13:07:04.409634: step 2999, loss 0.100063, acc 0.96875, learning_rate 0.000100023
2017-10-10T13:07:04.888868: step 3000, loss 0.138051, acc 0.96875, learning_rate 0.000100023

Evaluation:
2017-10-10T13:07:05.958752: step 3000, loss 0.221442, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3000

2017-10-10T13:07:07.627679: step 3001, loss 0.150461, acc 0.9375, learning_rate 0.000100023
2017-10-10T13:07:08.107808: step 3002, loss 0.150623, acc 0.953125, learning_rate 0.000100023
2017-10-10T13:07:08.484990: step 3003, loss 0.160355, acc 0.953125, learning_rate 0.000100023
2017-10-10T13:07:08.861995: step 3004, loss 0.0964033, acc 0.984375, learning_rate 0.000100023
2017-10-10T13:07:09.405579: step 3005, loss 0.101448, acc 0.96875, learning_rate 0.000100023
2017-10-10T13:07:09.988962: step 3006, loss 0.127689, acc 0.953125, learning_rate 0.000100023
2017-10-10T13:07:10.464904: step 3007, loss 0.184438, acc 0.953125, learning_rate 0.000100022
2017-10-10T13:07:11.099242: step 3008, loss 0.207981, acc 0.9375, learning_rate 0.000100022
2017-10-10T13:07:11.546043: step 3009, loss 0.0539923, acc 0.984375, learning_rate 0.000100022
2017-10-10T13:07:11.989225: step 3010, loss 0.202446, acc 0.921875, learning_rate 0.000100022
2017-10-10T13:07:12.449233: step 3011, loss 0.0711436, acc 0.96875, learning_rate 0.000100022
2017-10-10T13:07:12.975839: step 3012, loss 0.183873, acc 0.921875, learning_rate 0.000100022
2017-10-10T13:07:13.434692: step 3013, loss 0.115673, acc 0.953125, learning_rate 0.000100022
2017-10-10T13:07:13.969101: step 3014, loss 0.0965701, acc 0.984375, learning_rate 0.000100022
2017-10-10T13:07:14.580915: step 3015, loss 0.212443, acc 0.9375, learning_rate 0.000100022
2017-10-10T13:07:15.165875: step 3016, loss 0.184071, acc 0.953125, learning_rate 0.000100022
2017-10-10T13:07:15.593026: step 3017, loss 0.166321, acc 0.9375, learning_rate 0.000100022
2017-10-10T13:07:16.020052: step 3018, loss 0.128737, acc 0.984375, learning_rate 0.000100021
2017-10-10T13:07:16.539266: step 3019, loss 0.13123, acc 0.96875, learning_rate 0.000100021
2017-10-10T13:07:17.018031: step 3020, loss 0.103802, acc 0.96875, learning_rate 0.000100021
2017-10-10T13:07:17.498675: step 3021, loss 0.111458, acc 0.96875, learning_rate 0.000100021
2017-10-10T13:07:17.972440: step 3022, loss 0.185081, acc 0.9375, learning_rate 0.000100021
2017-10-10T13:07:18.561078: step 3023, loss 0.189988, acc 0.953125, learning_rate 0.000100021
2017-10-10T13:07:19.096498: step 3024, loss 0.10651, acc 0.96875, learning_rate 0.000100021
2017-10-10T13:07:19.580890: step 3025, loss 0.203743, acc 0.90625, learning_rate 0.000100021
2017-10-10T13:07:20.088547: step 3026, loss 0.17149, acc 0.9375, learning_rate 0.000100021
2017-10-10T13:07:20.981933: step 3027, loss 0.0674459, acc 0.984375, learning_rate 0.000100021
2017-10-10T13:07:21.548825: step 3028, loss 0.213035, acc 0.953125, learning_rate 0.000100021
2017-10-10T13:07:22.116894: step 3029, loss 0.0819366, acc 0.96875, learning_rate 0.00010002
2017-10-10T13:07:22.715502: step 3030, loss 0.117836, acc 0.96875, learning_rate 0.00010002
2017-10-10T13:07:23.197092: step 3031, loss 0.134345, acc 0.921875, learning_rate 0.00010002
2017-10-10T13:07:23.765006: step 3032, loss 0.0722096, acc 0.96875, learning_rate 0.00010002
2017-10-10T13:07:24.359891: step 3033, loss 0.104777, acc 0.96875, learning_rate 0.00010002
2017-10-10T13:07:24.839760: step 3034, loss 0.261855, acc 0.921875, learning_rate 0.00010002
2017-10-10T13:07:25.332963: step 3035, loss 0.154598, acc 0.953125, learning_rate 0.00010002
2017-10-10T13:07:25.885948: step 3036, loss 0.216917, acc 0.90625, learning_rate 0.00010002
2017-10-10T13:07:26.345044: step 3037, loss 0.0407192, acc 1, learning_rate 0.00010002
2017-10-10T13:07:26.837087: step 3038, loss 0.263835, acc 0.901961, learning_rate 0.00010002
2017-10-10T13:07:27.407222: step 3039, loss 0.134218, acc 0.9375, learning_rate 0.00010002
2017-10-10T13:07:27.878019: step 3040, loss 0.162637, acc 0.9375, learning_rate 0.00010002

Evaluation:
2017-10-10T13:07:28.980863: step 3040, loss 0.221073, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3040

2017-10-10T13:07:30.644896: step 3041, loss 0.111749, acc 0.96875, learning_rate 0.00010002
2017-10-10T13:07:31.240851: step 3042, loss 0.182241, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:07:31.704239: step 3043, loss 0.141509, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:07:32.140885: step 3044, loss 0.1665, acc 0.921875, learning_rate 0.000100019
2017-10-10T13:07:32.612556: step 3045, loss 0.174549, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:07:33.164978: step 3046, loss 0.0846941, acc 0.96875, learning_rate 0.000100019
2017-10-10T13:07:33.724968: step 3047, loss 0.245669, acc 0.921875, learning_rate 0.000100019
2017-10-10T13:07:34.317050: step 3048, loss 0.106464, acc 0.984375, learning_rate 0.000100019
2017-10-10T13:07:34.736845: step 3049, loss 0.190179, acc 0.921875, learning_rate 0.000100019
2017-10-10T13:07:35.184913: step 3050, loss 0.0629823, acc 0.96875, learning_rate 0.000100019
2017-10-10T13:07:35.615356: step 3051, loss 0.133458, acc 0.953125, learning_rate 0.000100019
2017-10-10T13:07:36.060587: step 3052, loss 0.0840946, acc 0.96875, learning_rate 0.000100019
2017-10-10T13:07:36.538370: step 3053, loss 0.0725985, acc 0.984375, learning_rate 0.000100019
2017-10-10T13:07:37.105017: step 3054, loss 0.164347, acc 0.921875, learning_rate 0.000100018
2017-10-10T13:07:37.608872: step 3055, loss 0.0591761, acc 0.984375, learning_rate 0.000100018
2017-10-10T13:07:38.161248: step 3056, loss 0.112431, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:07:38.698064: step 3057, loss 0.102086, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:07:39.152268: step 3058, loss 0.147048, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:07:39.600839: step 3059, loss 0.0875804, acc 1, learning_rate 0.000100018
2017-10-10T13:07:40.109041: step 3060, loss 0.171695, acc 0.9375, learning_rate 0.000100018
2017-10-10T13:07:40.618464: step 3061, loss 0.139808, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:07:41.112105: step 3062, loss 0.202171, acc 0.921875, learning_rate 0.000100018
2017-10-10T13:07:41.689118: step 3063, loss 0.187004, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:07:42.228050: step 3064, loss 0.159578, acc 0.9375, learning_rate 0.000100018
2017-10-10T13:07:42.801127: step 3065, loss 0.128647, acc 0.9375, learning_rate 0.000100018
2017-10-10T13:07:43.389767: step 3066, loss 0.140544, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:07:43.956971: step 3067, loss 0.213249, acc 0.9375, learning_rate 0.000100018
2017-10-10T13:07:44.448592: step 3068, loss 0.0676712, acc 0.984375, learning_rate 0.000100017
2017-10-10T13:07:44.961095: step 3069, loss 0.168882, acc 0.953125, learning_rate 0.000100017
2017-10-10T13:07:45.496923: step 3070, loss 0.0529775, acc 1, learning_rate 0.000100017
2017-10-10T13:07:46.036953: step 3071, loss 0.163762, acc 0.953125, learning_rate 0.000100017
2017-10-10T13:07:46.546024: step 3072, loss 0.160307, acc 0.953125, learning_rate 0.000100017
2017-10-10T13:07:47.039060: step 3073, loss 0.18455, acc 0.90625, learning_rate 0.000100017
2017-10-10T13:07:47.558248: step 3074, loss 0.309907, acc 0.875, learning_rate 0.000100017
2017-10-10T13:07:48.074985: step 3075, loss 0.181827, acc 0.953125, learning_rate 0.000100017
2017-10-10T13:07:48.596417: step 3076, loss 0.25078, acc 0.90625, learning_rate 0.000100017
2017-10-10T13:07:49.088880: step 3077, loss 0.322879, acc 0.875, learning_rate 0.000100017
2017-10-10T13:07:49.539119: step 3078, loss 0.159073, acc 0.890625, learning_rate 0.000100017
2017-10-10T13:07:50.006420: step 3079, loss 0.198286, acc 0.953125, learning_rate 0.000100017
2017-10-10T13:07:50.496951: step 3080, loss 0.184199, acc 0.96875, learning_rate 0.000100017

Evaluation:
2017-10-10T13:07:51.680835: step 3080, loss 0.22104, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3080

2017-10-10T13:07:53.234691: step 3081, loss 0.203844, acc 0.921875, learning_rate 0.000100017
2017-10-10T13:07:53.768853: step 3082, loss 0.122592, acc 0.9375, learning_rate 0.000100016
2017-10-10T13:07:54.377601: step 3083, loss 0.100387, acc 0.984375, learning_rate 0.000100016
2017-10-10T13:07:54.824842: step 3084, loss 0.211704, acc 0.9375, learning_rate 0.000100016
2017-10-10T13:07:55.287151: step 3085, loss 0.150791, acc 0.9375, learning_rate 0.000100016
2017-10-10T13:07:55.785498: step 3086, loss 0.140022, acc 0.953125, learning_rate 0.000100016
2017-10-10T13:07:56.329113: step 3087, loss 0.119015, acc 0.9375, learning_rate 0.000100016
2017-10-10T13:07:56.989049: step 3088, loss 0.244362, acc 0.890625, learning_rate 0.000100016
2017-10-10T13:07:57.394176: step 3089, loss 0.119582, acc 0.96875, learning_rate 0.000100016
2017-10-10T13:07:57.774454: step 3090, loss 0.141503, acc 0.953125, learning_rate 0.000100016
2017-10-10T13:07:58.170210: step 3091, loss 0.249379, acc 0.90625, learning_rate 0.000100016
2017-10-10T13:07:58.724950: step 3092, loss 0.202059, acc 0.9375, learning_rate 0.000100016
2017-10-10T13:07:59.269165: step 3093, loss 0.0748569, acc 0.984375, learning_rate 0.000100016
2017-10-10T13:07:59.841204: step 3094, loss 0.176571, acc 0.953125, learning_rate 0.000100016
2017-10-10T13:08:00.244256: step 3095, loss 0.156833, acc 0.9375, learning_rate 0.000100016
2017-10-10T13:08:00.729020: step 3096, loss 0.177977, acc 0.890625, learning_rate 0.000100016
2017-10-10T13:08:01.283381: step 3097, loss 0.106085, acc 0.984375, learning_rate 0.000100016
2017-10-10T13:08:01.845006: step 3098, loss 0.148887, acc 0.953125, learning_rate 0.000100015
2017-10-10T13:08:02.380265: step 3099, loss 0.229798, acc 0.921875, learning_rate 0.000100015
2017-10-10T13:08:02.844907: step 3100, loss 0.250958, acc 0.890625, learning_rate 0.000100015
2017-10-10T13:08:03.332334: step 3101, loss 0.15929, acc 0.953125, learning_rate 0.000100015
2017-10-10T13:08:03.832855: step 3102, loss 0.0629739, acc 0.96875, learning_rate 0.000100015
2017-10-10T13:08:04.359721: step 3103, loss 0.0986815, acc 0.96875, learning_rate 0.000100015
2017-10-10T13:08:04.897531: step 3104, loss 0.123584, acc 0.96875, learning_rate 0.000100015
2017-10-10T13:08:05.431116: step 3105, loss 0.0869854, acc 1, learning_rate 0.000100015
2017-10-10T13:08:05.946706: step 3106, loss 0.121845, acc 0.96875, learning_rate 0.000100015
2017-10-10T13:08:06.496255: step 3107, loss 0.117688, acc 0.9375, learning_rate 0.000100015
2017-10-10T13:08:07.055251: step 3108, loss 0.0590301, acc 0.953125, learning_rate 0.000100015
2017-10-10T13:08:07.601676: step 3109, loss 0.16753, acc 0.921875, learning_rate 0.000100015
2017-10-10T13:08:08.137114: step 3110, loss 0.235991, acc 0.96875, learning_rate 0.000100015
2017-10-10T13:08:08.744885: step 3111, loss 0.22685, acc 0.921875, learning_rate 0.000100015
2017-10-10T13:08:09.268808: step 3112, loss 0.139135, acc 0.9375, learning_rate 0.000100015
2017-10-10T13:08:09.856622: step 3113, loss 0.0726219, acc 0.984375, learning_rate 0.000100015
2017-10-10T13:08:10.381987: step 3114, loss 0.139502, acc 0.90625, learning_rate 0.000100014
2017-10-10T13:08:10.913958: step 3115, loss 0.178603, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:08:11.450024: step 3116, loss 0.156705, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:08:11.996052: step 3117, loss 0.325997, acc 0.9375, learning_rate 0.000100014
2017-10-10T13:08:12.524169: step 3118, loss 0.0889604, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:08:13.039280: step 3119, loss 0.0662108, acc 0.984375, learning_rate 0.000100014
2017-10-10T13:08:13.551472: step 3120, loss 0.220402, acc 0.9375, learning_rate 0.000100014

Evaluation:
2017-10-10T13:08:14.644944: step 3120, loss 0.22191, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3120

2017-10-10T13:08:16.184817: step 3121, loss 0.0997296, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:08:16.748845: step 3122, loss 0.0914224, acc 0.96875, learning_rate 0.000100014
2017-10-10T13:08:17.351524: step 3123, loss 0.23083, acc 0.9375, learning_rate 0.000100014
2017-10-10T13:08:17.789849: step 3124, loss 0.20892, acc 0.96875, learning_rate 0.000100014
2017-10-10T13:08:18.235217: step 3125, loss 0.129668, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:08:18.798890: step 3126, loss 0.12663, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:08:19.360414: step 3127, loss 0.100263, acc 0.96875, learning_rate 0.000100014
2017-10-10T13:08:19.928827: step 3128, loss 0.135415, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:08:20.532833: step 3129, loss 0.151524, acc 0.96875, learning_rate 0.000100014
2017-10-10T13:08:20.957411: step 3130, loss 0.093879, acc 0.984375, learning_rate 0.000100014
2017-10-10T13:08:21.415788: step 3131, loss 0.0871145, acc 0.96875, learning_rate 0.000100014
2017-10-10T13:08:21.973203: step 3132, loss 0.0835465, acc 0.984375, learning_rate 0.000100013
2017-10-10T13:08:22.500876: step 3133, loss 0.190264, acc 0.9375, learning_rate 0.000100013
2017-10-10T13:08:22.964979: step 3134, loss 0.157584, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:08:23.470137: step 3135, loss 0.0970917, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:08:23.812335: step 3136, loss 0.088271, acc 0.980392, learning_rate 0.000100013
2017-10-10T13:08:24.335835: step 3137, loss 0.162352, acc 0.9375, learning_rate 0.000100013
2017-10-10T13:08:24.828114: step 3138, loss 0.207717, acc 0.90625, learning_rate 0.000100013
2017-10-10T13:08:25.337093: step 3139, loss 0.121777, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:08:25.938986: step 3140, loss 0.145963, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:08:26.323556: step 3141, loss 0.190513, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:08:26.768841: step 3142, loss 0.0940775, acc 1, learning_rate 0.000100013
2017-10-10T13:08:27.273100: step 3143, loss 0.105345, acc 0.9375, learning_rate 0.000100013
2017-10-10T13:08:27.796633: step 3144, loss 0.121689, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:08:28.305284: step 3145, loss 0.0230782, acc 1, learning_rate 0.000100013
2017-10-10T13:08:28.848943: step 3146, loss 0.163943, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:08:29.389510: step 3147, loss 0.118522, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:08:29.917496: step 3148, loss 0.137118, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:08:30.396131: step 3149, loss 0.149078, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:08:30.891030: step 3150, loss 0.105898, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:08:31.456363: step 3151, loss 0.129369, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:08:32.008143: step 3152, loss 0.0780854, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:08:32.543107: step 3153, loss 0.133649, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:08:33.084702: step 3154, loss 0.159404, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:08:34.004301: step 3155, loss 0.102766, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:08:34.564848: step 3156, loss 0.117596, acc 0.984375, learning_rate 0.000100012
2017-10-10T13:08:35.098387: step 3157, loss 0.199403, acc 0.90625, learning_rate 0.000100012
2017-10-10T13:08:35.649148: step 3158, loss 0.123061, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:08:36.268092: step 3159, loss 0.175217, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:08:36.812863: step 3160, loss 0.065052, acc 0.984375, learning_rate 0.000100012

Evaluation:
2017-10-10T13:08:37.922869: step 3160, loss 0.222561, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3160

2017-10-10T13:08:39.480499: step 3161, loss 0.158554, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:08:40.014319: step 3162, loss 0.156081, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:08:40.541070: step 3163, loss 0.137007, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:08:41.136860: step 3164, loss 0.187951, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:08:41.596789: step 3165, loss 0.158346, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:08:42.046402: step 3166, loss 0.222019, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:08:42.591717: step 3167, loss 0.317394, acc 0.890625, learning_rate 0.000100012
2017-10-10T13:08:43.088860: step 3168, loss 0.173576, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:08:43.680797: step 3169, loss 0.196702, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:08:44.029131: step 3170, loss 0.0836409, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:08:44.435043: step 3171, loss 0.127403, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:08:44.880184: step 3172, loss 0.107243, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:08:45.438613: step 3173, loss 0.112192, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:08:45.918372: step 3174, loss 0.173724, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:08:46.425935: step 3175, loss 0.141812, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:08:46.953476: step 3176, loss 0.10245, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:08:47.460733: step 3177, loss 0.0758629, acc 1, learning_rate 0.000100011
2017-10-10T13:08:47.973278: step 3178, loss 0.10125, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:08:48.498626: step 3179, loss 0.240273, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:08:49.116915: step 3180, loss 0.148712, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:08:49.704896: step 3181, loss 0.0582269, acc 1, learning_rate 0.000100011
2017-10-10T13:08:50.100861: step 3182, loss 0.126539, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:08:50.488341: step 3183, loss 0.105482, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:08:50.992239: step 3184, loss 0.206626, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:08:51.492415: step 3185, loss 0.139976, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:08:52.107940: step 3186, loss 0.117089, acc 0.984375, learning_rate 0.000100011
2017-10-10T13:08:52.666782: step 3187, loss 0.162503, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:08:53.132869: step 3188, loss 0.22662, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:08:53.608873: step 3189, loss 0.0745005, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:08:54.061454: step 3190, loss 0.236574, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:08:54.543549: step 3191, loss 0.34083, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:08:55.030081: step 3192, loss 0.171993, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:08:55.565203: step 3193, loss 0.13901, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:08:56.076445: step 3194, loss 0.152106, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:08:56.609314: step 3195, loss 0.24185, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:08:57.208168: step 3196, loss 0.196221, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:08:57.772976: step 3197, loss 0.19964, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:08:58.311677: step 3198, loss 0.213797, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:08:58.872869: step 3199, loss 0.0807034, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:08:59.340826: step 3200, loss 0.115852, acc 0.953125, learning_rate 0.00010001

Evaluation:
2017-10-10T13:09:00.533710: step 3200, loss 0.221884, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3200

2017-10-10T13:09:02.191009: step 3201, loss 0.289597, acc 0.90625, learning_rate 0.00010001
2017-10-10T13:09:02.683280: step 3202, loss 0.0973317, acc 0.984375, learning_rate 0.00010001
2017-10-10T13:09:03.201443: step 3203, loss 0.0591837, acc 1, learning_rate 0.00010001
2017-10-10T13:09:03.740814: step 3204, loss 0.279955, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:09:04.351802: step 3205, loss 0.0970715, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:09:04.790514: step 3206, loss 0.227577, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:09:05.343091: step 3207, loss 0.111827, acc 0.984375, learning_rate 0.00010001
2017-10-10T13:09:05.817062: step 3208, loss 0.122356, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:09:06.300833: step 3209, loss 0.239341, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:09:06.728964: step 3210, loss 0.180738, acc 0.90625, learning_rate 0.00010001
2017-10-10T13:09:07.189113: step 3211, loss 0.247182, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:09:07.801505: step 3212, loss 0.0606342, acc 0.984375, learning_rate 0.00010001
2017-10-10T13:09:08.269170: step 3213, loss 0.103204, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:09:08.856911: step 3214, loss 0.263165, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:09:09.368497: step 3215, loss 0.115562, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:09:09.955960: step 3216, loss 0.10654, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:09:10.477016: step 3217, loss 0.232559, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:09:10.968849: step 3218, loss 0.116769, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:09:11.477407: step 3219, loss 0.142185, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:09:11.999537: step 3220, loss 0.186065, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:09:12.594834: step 3221, loss 0.138758, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:09:13.164827: step 3222, loss 0.129352, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:09:13.569434: step 3223, loss 0.0658697, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:09:14.024897: step 3224, loss 0.0765645, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:09:14.514354: step 3225, loss 0.201836, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:15.094982: step 3226, loss 0.0826505, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:09:15.652602: step 3227, loss 0.265666, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:09:16.184588: step 3228, loss 0.080223, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:09:16.729331: step 3229, loss 0.156187, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:09:17.224979: step 3230, loss 0.242605, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:17.710748: step 3231, loss 0.0964931, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:18.189005: step 3232, loss 0.2547, acc 0.875, learning_rate 0.000100009
2017-10-10T13:09:18.576259: step 3233, loss 0.0881902, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:19.084030: step 3234, loss 0.143127, acc 0.960784, learning_rate 0.000100009
2017-10-10T13:09:19.572941: step 3235, loss 0.100423, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:20.181035: step 3236, loss 0.147905, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:20.723325: step 3237, loss 0.132972, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:21.248939: step 3238, loss 0.106654, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:21.813057: step 3239, loss 0.0957673, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:09:22.318637: step 3240, loss 0.196642, acc 0.9375, learning_rate 0.000100009

Evaluation:
2017-10-10T13:09:23.428939: step 3240, loss 0.221023, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3240

2017-10-10T13:09:24.883524: step 3241, loss 0.10398, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:09:25.424231: step 3242, loss 0.0799087, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:09:26.029157: step 3243, loss 0.193721, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:26.533246: step 3244, loss 0.128304, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:09:27.073268: step 3245, loss 0.130477, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:09:27.453385: step 3246, loss 0.0690327, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:09:27.916956: step 3247, loss 0.218367, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:09:28.577152: step 3248, loss 0.145617, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:09:29.085225: step 3249, loss 0.162322, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:29.530993: step 3250, loss 0.13576, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:09:29.987044: step 3251, loss 0.121484, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:09:30.490864: step 3252, loss 0.184272, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:31.027308: step 3253, loss 0.176066, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:09:31.589173: step 3254, loss 0.188906, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:32.061308: step 3255, loss 0.117124, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:09:32.517038: step 3256, loss 0.132704, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:32.998911: step 3257, loss 0.141577, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:33.501152: step 3258, loss 0.105999, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:09:33.933898: step 3259, loss 0.201106, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:09:34.460622: step 3260, loss 0.122472, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:09:34.962784: step 3261, loss 0.140983, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:35.545064: step 3262, loss 0.153465, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:36.102622: step 3263, loss 0.0804855, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:09:36.540861: step 3264, loss 0.104677, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:09:37.000844: step 3265, loss 0.16929, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:37.548236: step 3266, loss 0.0476351, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:09:38.105948: step 3267, loss 0.106084, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:09:38.628988: step 3268, loss 0.153473, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:09:39.162543: step 3269, loss 0.104307, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:09:39.620186: step 3270, loss 0.267741, acc 0.890625, learning_rate 0.000100008
2017-10-10T13:09:40.088556: step 3271, loss 0.111958, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:09:40.565117: step 3272, loss 0.182064, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:09:41.021825: step 3273, loss 0.118011, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:09:41.505220: step 3274, loss 0.179623, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:09:42.057216: step 3275, loss 0.110147, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:09:42.594422: step 3276, loss 0.0851053, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:43.136809: step 3277, loss 0.246232, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:09:43.698707: step 3278, loss 0.150175, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:09:44.216915: step 3279, loss 0.231647, acc 0.875, learning_rate 0.000100007
2017-10-10T13:09:44.754030: step 3280, loss 0.113734, acc 0.96875, learning_rate 0.000100007

Evaluation:
2017-10-10T13:09:45.952022: step 3280, loss 0.221113, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3280

2017-10-10T13:09:47.575805: step 3281, loss 0.0851766, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:48.064052: step 3282, loss 0.0939206, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:09:48.629833: step 3283, loss 0.0964689, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:49.160935: step 3284, loss 0.1527, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:09:49.733149: step 3285, loss 0.127972, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:50.172944: step 3286, loss 0.102461, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:50.608827: step 3287, loss 0.109727, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:51.054629: step 3288, loss 0.135254, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:09:51.636888: step 3289, loss 0.178335, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:09:52.203306: step 3290, loss 0.0788255, acc 1, learning_rate 0.000100007
2017-10-10T13:09:52.616827: step 3291, loss 0.143946, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:53.088888: step 3292, loss 0.208434, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:09:53.654117: step 3293, loss 0.130658, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:09:54.191512: step 3294, loss 0.128318, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:54.758867: step 3295, loss 0.0805156, acc 1, learning_rate 0.000100007
2017-10-10T13:09:55.311296: step 3296, loss 0.174802, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:09:55.885523: step 3297, loss 0.139506, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:56.430834: step 3298, loss 0.207064, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:09:56.988389: step 3299, loss 0.164064, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:09:57.539038: step 3300, loss 0.138637, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:09:58.092584: step 3301, loss 0.0671555, acc 0.984375, learning_rate 0.000100007
2017-10-10T13:09:58.700858: step 3302, loss 0.156092, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:09:59.312944: step 3303, loss 0.196033, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:09:59.755365: step 3304, loss 0.11852, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:10:00.214780: step 3305, loss 0.176419, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:10:00.772842: step 3306, loss 0.168718, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:10:01.280132: step 3307, loss 0.0344254, acc 1, learning_rate 0.000100007
2017-10-10T13:10:01.835671: step 3308, loss 0.132098, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:10:02.405256: step 3309, loss 0.209462, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:10:02.966836: step 3310, loss 0.218714, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:10:03.477302: step 3311, loss 0.0746347, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:04.019450: step 3312, loss 0.231527, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:10:04.581939: step 3313, loss 0.145342, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:10:05.095101: step 3314, loss 0.105755, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:10:05.676990: step 3315, loss 0.175069, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:10:06.256911: step 3316, loss 0.169263, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:10:06.792889: step 3317, loss 0.140493, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:07.216810: step 3318, loss 0.152303, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:10:07.645019: step 3319, loss 0.0732067, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:10:08.133940: step 3320, loss 0.139083, acc 0.96875, learning_rate 0.000100006

Evaluation:
2017-10-10T13:10:09.373138: step 3320, loss 0.217001, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3320

2017-10-10T13:10:11.131189: step 3321, loss 0.230564, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:10:11.682049: step 3322, loss 0.12047, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:12.216305: step 3323, loss 0.134013, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:10:12.748811: step 3324, loss 0.156036, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:13.361010: step 3325, loss 0.110833, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:13.887565: step 3326, loss 0.143739, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:14.348530: step 3327, loss 0.152156, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:14.849314: step 3328, loss 0.0495011, acc 1, learning_rate 0.000100006
2017-10-10T13:10:15.432898: step 3329, loss 0.149671, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:10:15.864860: step 3330, loss 0.25787, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:10:16.332839: step 3331, loss 0.185779, acc 0.90625, learning_rate 0.000100006
2017-10-10T13:10:16.752210: step 3332, loss 0.175661, acc 0.941176, learning_rate 0.000100006
2017-10-10T13:10:17.332859: step 3333, loss 0.213127, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:10:17.864230: step 3334, loss 0.130496, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:10:18.410859: step 3335, loss 0.107254, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:18.952209: step 3336, loss 0.116087, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:10:19.511078: step 3337, loss 0.14052, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:10:20.055755: step 3338, loss 0.124966, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:10:20.602579: step 3339, loss 0.108537, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:10:21.157043: step 3340, loss 0.0989119, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:10:21.713690: step 3341, loss 0.0807071, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:10:22.364877: step 3342, loss 0.0848087, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:10:22.960388: step 3343, loss 0.118585, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:10:23.437717: step 3344, loss 0.0722133, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:10:23.900849: step 3345, loss 0.139068, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:24.484796: step 3346, loss 0.217374, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:10:25.028975: step 3347, loss 0.193276, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:10:25.581853: step 3348, loss 0.0734955, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:10:26.131879: step 3349, loss 0.172542, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:10:26.666908: step 3350, loss 0.0703955, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:10:27.183485: step 3351, loss 0.1918, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:27.727932: step 3352, loss 0.252674, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:10:28.264586: step 3353, loss 0.117665, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:28.795609: step 3354, loss 0.09424, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:29.242705: step 3355, loss 0.147046, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:10:29.733105: step 3356, loss 0.228435, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:10:30.187982: step 3357, loss 0.141813, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:10:30.685224: step 3358, loss 0.238006, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:10:31.139455: step 3359, loss 0.0687582, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:10:31.660419: step 3360, loss 0.148985, acc 0.9375, learning_rate 0.000100005

Evaluation:
2017-10-10T13:10:32.927158: step 3360, loss 0.21898, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3360

2017-10-10T13:10:34.485300: step 3361, loss 0.141367, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:34.989915: step 3362, loss 0.144554, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:35.422833: step 3363, loss 0.0932065, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:10:35.860889: step 3364, loss 0.259454, acc 0.875, learning_rate 0.000100005
2017-10-10T13:10:36.452910: step 3365, loss 0.142949, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:36.980841: step 3366, loss 0.182444, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:10:37.417276: step 3367, loss 0.149315, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:37.973265: step 3368, loss 0.186712, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:38.592892: step 3369, loss 0.0489832, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:10:39.040845: step 3370, loss 0.108909, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:39.440841: step 3371, loss 0.301646, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:10:39.896116: step 3372, loss 0.179881, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:10:40.376403: step 3373, loss 0.263541, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:10:40.908927: step 3374, loss 0.1574, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:10:41.436868: step 3375, loss 0.124604, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:10:41.956838: step 3376, loss 0.114421, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:42.471170: step 3377, loss 0.146195, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:10:42.960103: step 3378, loss 0.277613, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:10:43.531271: step 3379, loss 0.0989077, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:10:44.046813: step 3380, loss 0.217929, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:10:44.475920: step 3381, loss 0.210405, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:10:44.987812: step 3382, loss 0.126555, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:45.496322: step 3383, loss 0.0981736, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:46.080922: step 3384, loss 0.140292, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:46.697015: step 3385, loss 0.130555, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:47.158244: step 3386, loss 0.154271, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:10:47.591373: step 3387, loss 0.119972, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:48.154914: step 3388, loss 0.106942, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:10:48.724842: step 3389, loss 0.15708, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:49.241159: step 3390, loss 0.109253, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:49.766760: step 3391, loss 0.0701353, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:10:50.273396: step 3392, loss 0.211498, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:10:50.792894: step 3393, loss 0.196004, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:10:51.292846: step 3394, loss 0.11586, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:51.810412: step 3395, loss 0.0805316, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:52.397007: step 3396, loss 0.093042, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:10:52.889454: step 3397, loss 0.171581, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:10:53.416854: step 3398, loss 0.135104, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:10:53.997316: step 3399, loss 0.0655976, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:10:54.533830: step 3400, loss 0.200642, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-10T13:10:55.716820: step 3400, loss 0.221882, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3400

2017-10-10T13:10:57.369623: step 3401, loss 0.0971346, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:10:57.921219: step 3402, loss 0.148547, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:10:58.449113: step 3403, loss 0.151507, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:10:59.021000: step 3404, loss 0.157614, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:10:59.646140: step 3405, loss 0.238722, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:00.051933: step 3406, loss 0.233484, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:11:00.465192: step 3407, loss 0.152147, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:00.962523: step 3408, loss 0.0592758, acc 1, learning_rate 0.000100004
2017-10-10T13:11:01.485059: step 3409, loss 0.104018, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:01.861002: step 3410, loss 0.224879, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:02.282958: step 3411, loss 0.130571, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:11:02.898899: step 3412, loss 0.163666, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:03.444862: step 3413, loss 0.0749915, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:03.992854: step 3414, loss 0.0620837, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:11:04.600858: step 3415, loss 0.109659, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:05.180871: step 3416, loss 0.168708, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:05.707896: step 3417, loss 0.0989225, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:11:06.261788: step 3418, loss 0.110162, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:06.836428: step 3419, loss 0.164955, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:07.369079: step 3420, loss 0.131219, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:07.985547: step 3421, loss 0.146543, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:08.544413: step 3422, loss 0.095579, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:11:09.112860: step 3423, loss 0.232221, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:11:09.721267: step 3424, loss 0.111825, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:10.146805: step 3425, loss 0.139137, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:10.544721: step 3426, loss 0.188918, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:10.932906: step 3427, loss 0.199064, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:11:11.408920: step 3428, loss 0.127176, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:11.966807: step 3429, loss 0.082623, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:12.368389: step 3430, loss 0.222533, acc 0.921569, learning_rate 0.000100004
2017-10-10T13:11:12.965022: step 3431, loss 0.0692172, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:11:13.508866: step 3432, loss 0.125624, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:14.065529: step 3433, loss 0.233602, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:11:14.618187: step 3434, loss 0.0765176, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:15.198012: step 3435, loss 0.296769, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:11:15.783593: step 3436, loss 0.153541, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:16.268923: step 3437, loss 0.130865, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:16.835098: step 3438, loss 0.248311, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:11:17.358037: step 3439, loss 0.137635, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:17.840923: step 3440, loss 0.0749122, acc 0.984375, learning_rate 0.000100004

Evaluation:
2017-10-10T13:11:18.990908: step 3440, loss 0.219078, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3440

2017-10-10T13:11:20.569059: step 3441, loss 0.237335, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:11:21.118986: step 3442, loss 0.178016, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:21.780870: step 3443, loss 0.149638, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:22.292828: step 3444, loss 0.148377, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:22.753183: step 3445, loss 0.100892, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:23.301038: step 3446, loss 0.207455, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:23.900205: step 3447, loss 0.168607, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:11:24.297952: step 3448, loss 0.175848, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:11:24.732274: step 3449, loss 0.110963, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:25.152860: step 3450, loss 0.151581, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:25.669021: step 3451, loss 0.0772393, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:26.223166: step 3452, loss 0.0749691, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:26.772857: step 3453, loss 0.150493, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:11:27.274938: step 3454, loss 0.105888, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:27.817041: step 3455, loss 0.191311, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:11:28.325830: step 3456, loss 0.178937, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:11:28.916935: step 3457, loss 0.0738947, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:29.472917: step 3458, loss 0.137367, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:11:29.996728: step 3459, loss 0.0585869, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:11:30.435488: step 3460, loss 0.251497, acc 0.875, learning_rate 0.000100004
2017-10-10T13:11:30.982867: step 3461, loss 0.176376, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:11:31.488870: step 3462, loss 0.133831, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:31.991487: step 3463, loss 0.15254, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:32.565137: step 3464, loss 0.186903, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:33.121702: step 3465, loss 0.127546, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:11:33.548917: step 3466, loss 0.165663, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:33.996082: step 3467, loss 0.272588, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:11:34.489002: step 3468, loss 0.139705, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:35.052994: step 3469, loss 0.112838, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:35.616851: step 3470, loss 0.131253, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:36.142224: step 3471, loss 0.253134, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:11:36.664839: step 3472, loss 0.107447, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:37.241854: step 3473, loss 0.105232, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:37.754235: step 3474, loss 0.217417, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:38.236096: step 3475, loss 0.0627497, acc 1, learning_rate 0.000100003
2017-10-10T13:11:38.788870: step 3476, loss 0.129116, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:11:39.317162: step 3477, loss 0.162036, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:39.800953: step 3478, loss 0.117286, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:40.385407: step 3479, loss 0.107654, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:11:40.860672: step 3480, loss 0.136587, acc 0.953125, learning_rate 0.000100003

Evaluation:
2017-10-10T13:11:41.932950: step 3480, loss 0.216499, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3480

2017-10-10T13:11:43.707968: step 3481, loss 0.125419, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:11:44.252902: step 3482, loss 0.186114, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:44.883806: step 3483, loss 0.221103, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:45.333912: step 3484, loss 0.14541, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:45.781152: step 3485, loss 0.168678, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:11:46.292832: step 3486, loss 0.0377973, acc 1, learning_rate 0.000100003
2017-10-10T13:11:46.877369: step 3487, loss 0.0803954, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:11:47.344731: step 3488, loss 0.111358, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:47.772851: step 3489, loss 0.050785, acc 1, learning_rate 0.000100003
2017-10-10T13:11:48.259234: step 3490, loss 0.146626, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:48.773583: step 3491, loss 0.106413, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:49.307614: step 3492, loss 0.122242, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:49.811146: step 3493, loss 0.0935682, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:50.342345: step 3494, loss 0.119009, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:11:50.907844: step 3495, loss 0.230912, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:11:51.449187: step 3496, loss 0.0751818, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:11:51.940825: step 3497, loss 0.0935032, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:52.521316: step 3498, loss 0.28082, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:11:53.077053: step 3499, loss 0.127155, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:11:53.651064: step 3500, loss 0.145531, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:11:54.204533: step 3501, loss 0.22831, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:54.746647: step 3502, loss 0.155486, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:11:55.353062: step 3503, loss 0.1683, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:11:55.989161: step 3504, loss 0.239557, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:11:56.380533: step 3505, loss 0.127126, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:56.786342: step 3506, loss 0.120322, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:57.309002: step 3507, loss 0.117407, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:57.832371: step 3508, loss 0.0907907, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:11:58.348955: step 3509, loss 0.144189, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:11:58.909152: step 3510, loss 0.198502, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:11:59.412998: step 3511, loss 0.255944, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:11:59.936509: step 3512, loss 0.153947, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:12:00.346328: step 3513, loss 0.0759088, acc 1, learning_rate 0.000100003
2017-10-10T13:12:00.858976: step 3514, loss 0.192245, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:12:01.452944: step 3515, loss 0.222524, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:12:01.940951: step 3516, loss 0.0536765, acc 1, learning_rate 0.000100003
2017-10-10T13:12:02.429096: step 3517, loss 0.087706, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:12:02.891452: step 3518, loss 0.0686732, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:12:03.408082: step 3519, loss 0.149043, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:12:03.948818: step 3520, loss 0.13104, acc 0.96875, learning_rate 0.000100003

Evaluation:
2017-10-10T13:12:05.087452: step 3520, loss 0.21924, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3520

2017-10-10T13:12:06.546792: step 3521, loss 0.140409, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:12:07.145046: step 3522, loss 0.18672, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:12:07.749757: step 3523, loss 0.0720698, acc 1, learning_rate 0.000100003
2017-10-10T13:12:08.235497: step 3524, loss 0.147676, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:12:08.649660: step 3525, loss 0.154828, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:12:09.087191: step 3526, loss 0.0499878, acc 1, learning_rate 0.000100003
2017-10-10T13:12:09.609776: step 3527, loss 0.0973336, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:12:10.119674: step 3528, loss 0.0270987, acc 1, learning_rate 0.000100003
2017-10-10T13:12:10.613603: step 3529, loss 0.210308, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:12:11.079683: step 3530, loss 0.0989803, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:12:11.647034: step 3531, loss 0.125807, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:12:12.231681: step 3532, loss 0.0410505, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:12:12.804994: step 3533, loss 0.231787, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:12:13.331807: step 3534, loss 0.123308, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:12:13.834946: step 3535, loss 0.12266, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:12:14.332861: step 3536, loss 0.0582362, acc 1, learning_rate 0.000100003
2017-10-10T13:12:14.876873: step 3537, loss 0.119583, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:12:15.393192: step 3538, loss 0.229779, acc 0.875, learning_rate 0.000100003
2017-10-10T13:12:15.958082: step 3539, loss 0.118346, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:12:16.486454: step 3540, loss 0.177751, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:12:17.027389: step 3541, loss 0.228188, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:12:17.520813: step 3542, loss 0.115534, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:12:18.076584: step 3543, loss 0.137222, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:12:18.607150: step 3544, loss 0.195883, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:12:19.246979: step 3545, loss 0.0661854, acc 1, learning_rate 0.000100002
2017-10-10T13:12:19.700863: step 3546, loss 0.216167, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:12:20.148340: step 3547, loss 0.114042, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:20.680936: step 3548, loss 0.0637026, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:21.220934: step 3549, loss 0.12312, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:21.739564: step 3550, loss 0.156213, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:22.303065: step 3551, loss 0.182123, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:22.816818: step 3552, loss 0.112294, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:23.385117: step 3553, loss 0.172561, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:23.886885: step 3554, loss 0.0485492, acc 1, learning_rate 0.000100002
2017-10-10T13:12:24.377187: step 3555, loss 0.130626, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:24.888437: step 3556, loss 0.0848733, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:25.385044: step 3557, loss 0.152288, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:25.913100: step 3558, loss 0.106204, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:26.463612: step 3559, loss 0.087574, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:26.970745: step 3560, loss 0.100955, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T13:12:28.116912: step 3560, loss 0.221776, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3560

2017-10-10T13:12:29.772027: step 3561, loss 0.0711973, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:30.262598: step 3562, loss 0.174116, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:30.774938: step 3563, loss 0.210838, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:12:31.406442: step 3564, loss 0.120488, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:31.823222: step 3565, loss 0.103407, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:32.380837: step 3566, loss 0.0722237, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:32.988474: step 3567, loss 0.0640816, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:33.431800: step 3568, loss 0.226989, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:12:33.896159: step 3569, loss 0.207653, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:34.441557: step 3570, loss 0.114084, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:35.036817: step 3571, loss 0.18398, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:35.575990: step 3572, loss 0.12148, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:12:36.092873: step 3573, loss 0.215368, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:36.604861: step 3574, loss 0.361322, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:12:37.068895: step 3575, loss 0.13807, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:37.556505: step 3576, loss 0.0937387, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:38.142796: step 3577, loss 0.196569, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:12:38.645036: step 3578, loss 0.109344, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:39.067313: step 3579, loss 0.16584, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:39.536629: step 3580, loss 0.0611558, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:40.056976: step 3581, loss 0.309876, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:12:40.617768: step 3582, loss 0.0922404, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:41.189048: step 3583, loss 0.196445, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:12:41.657214: step 3584, loss 0.10232, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:42.213075: step 3585, loss 0.0606594, acc 1, learning_rate 0.000100002
2017-10-10T13:12:42.792200: step 3586, loss 0.184094, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:43.193133: step 3587, loss 0.0471243, acc 1, learning_rate 0.000100002
2017-10-10T13:12:43.621228: step 3588, loss 0.110354, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:44.115588: step 3589, loss 0.188425, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:12:44.657042: step 3590, loss 0.209208, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:12:45.165853: step 3591, loss 0.0794087, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:45.733349: step 3592, loss 0.146511, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:46.292830: step 3593, loss 0.110174, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:46.846658: step 3594, loss 0.119985, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:47.377610: step 3595, loss 0.122152, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:47.902821: step 3596, loss 0.18484, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:48.461918: step 3597, loss 0.158051, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:49.032917: step 3598, loss 0.093674, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:49.610375: step 3599, loss 0.0555174, acc 1, learning_rate 0.000100002
2017-10-10T13:12:50.135948: step 3600, loss 0.182039, acc 0.90625, learning_rate 0.000100002

Evaluation:
2017-10-10T13:12:51.357277: step 3600, loss 0.219915, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3600

2017-10-10T13:12:53.189075: step 3601, loss 0.306192, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:12:53.824873: step 3602, loss 0.133433, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:54.401846: step 3603, loss 0.107688, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:54.908847: step 3604, loss 0.0591028, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:55.416541: step 3605, loss 0.109977, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:55.889680: step 3606, loss 0.085049, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:56.359080: step 3607, loss 0.218073, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:12:56.872972: step 3608, loss 0.170942, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:12:57.380968: step 3609, loss 0.095596, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:57.878911: step 3610, loss 0.121091, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:12:58.370947: step 3611, loss 0.124855, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:12:58.913425: step 3612, loss 0.0522151, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:12:59.479589: step 3613, loss 0.145016, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:00.125001: step 3614, loss 0.10902, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:00.609156: step 3615, loss 0.300619, acc 0.875, learning_rate 0.000100002
2017-10-10T13:13:01.181074: step 3616, loss 0.288332, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:13:01.733168: step 3617, loss 0.0920964, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:13:02.188874: step 3618, loss 0.109206, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:02.660852: step 3619, loss 0.0887061, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:13:03.200886: step 3620, loss 0.159766, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:03.749709: step 3621, loss 0.199902, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:04.280682: step 3622, loss 0.152946, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:04.844340: step 3623, loss 0.0955541, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:05.374631: step 3624, loss 0.184643, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:13:05.938866: step 3625, loss 0.100891, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:06.453213: step 3626, loss 0.245324, acc 0.901961, learning_rate 0.000100002
2017-10-10T13:13:06.936819: step 3627, loss 0.270962, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:13:07.382357: step 3628, loss 0.184464, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:13:07.812899: step 3629, loss 0.0609244, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:13:08.305129: step 3630, loss 0.162545, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:08.904868: step 3631, loss 0.0961404, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:09.292321: step 3632, loss 0.139181, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:09.869040: step 3633, loss 0.0686249, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:13:10.296923: step 3634, loss 0.197629, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:13:10.740973: step 3635, loss 0.167182, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:11.280862: step 3636, loss 0.310085, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:13:11.811818: step 3637, loss 0.105147, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:12.419616: step 3638, loss 0.118558, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:12.976574: step 3639, loss 0.104376, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:13.521250: step 3640, loss 0.0638304, acc 1, learning_rate 0.000100002

Evaluation:
2017-10-10T13:13:14.697580: step 3640, loss 0.219228, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3640

2017-10-10T13:13:16.152066: step 3641, loss 0.244548, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:16.686249: step 3642, loss 0.222529, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:13:17.137017: step 3643, loss 0.189153, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:17.752901: step 3644, loss 0.152622, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:13:18.001070: step 3645, loss 0.108855, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:18.565024: step 3646, loss 0.107032, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:19.000434: step 3647, loss 0.193064, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:13:19.335779: step 3648, loss 0.0615485, acc 1, learning_rate 0.000100002
2017-10-10T13:13:19.796966: step 3649, loss 0.205652, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:20.257054: step 3650, loss 0.124329, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:20.810122: step 3651, loss 0.120305, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:21.321655: step 3652, loss 0.203066, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:21.848782: step 3653, loss 0.054101, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:13:22.449314: step 3654, loss 0.148272, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:22.984881: step 3655, loss 0.145741, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:13:23.500860: step 3656, loss 0.205099, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:24.042149: step 3657, loss 0.0885627, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:24.556810: step 3658, loss 0.138122, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:25.081656: step 3659, loss 0.124006, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:25.556858: step 3660, loss 0.158555, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:26.064908: step 3661, loss 0.124708, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:26.647516: step 3662, loss 0.239211, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:13:27.180196: step 3663, loss 0.102756, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:27.640170: step 3664, loss 0.161858, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:28.211236: step 3665, loss 0.0970167, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:13:28.789137: step 3666, loss 0.123042, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:13:29.244867: step 3667, loss 0.132446, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:29.684817: step 3668, loss 0.11164, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:13:30.145238: step 3669, loss 0.0522537, acc 1, learning_rate 0.000100001
2017-10-10T13:13:30.697678: step 3670, loss 0.105405, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:13:31.231375: step 3671, loss 0.206067, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:13:31.784986: step 3672, loss 0.151772, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:32.376910: step 3673, loss 0.258701, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:13:32.920976: step 3674, loss 0.173755, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:13:33.461001: step 3675, loss 0.087082, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:33.967926: step 3676, loss 0.0914348, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:13:34.494792: step 3677, loss 0.0584707, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:35.022320: step 3678, loss 0.199079, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:35.487410: step 3679, loss 0.0768094, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:13:35.994896: step 3680, loss 0.130581, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T13:13:37.130335: step 3680, loss 0.2179, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3680

2017-10-10T13:13:38.755331: step 3681, loss 0.118445, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:39.361029: step 3682, loss 0.0485435, acc 1, learning_rate 0.000100001
2017-10-10T13:13:39.906270: step 3683, loss 0.110445, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:40.518009: step 3684, loss 0.166117, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:13:41.031081: step 3685, loss 0.179051, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:13:41.483713: step 3686, loss 0.168968, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:13:41.938877: step 3687, loss 0.344637, acc 0.875, learning_rate 0.000100001
2017-10-10T13:13:42.382164: step 3688, loss 0.219511, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:13:42.829000: step 3689, loss 0.129949, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:43.283963: step 3690, loss 0.165297, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:13:43.829300: step 3691, loss 0.200486, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:13:44.336464: step 3692, loss 0.188275, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:13:44.956908: step 3693, loss 0.0727484, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:45.457221: step 3694, loss 0.289819, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:13:45.953329: step 3695, loss 0.0979452, acc 1, learning_rate 0.000100001
2017-10-10T13:13:46.504929: step 3696, loss 0.145288, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:13:47.039849: step 3697, loss 0.256003, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:13:47.516910: step 3698, loss 0.0829629, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:48.120942: step 3699, loss 0.281971, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:13:48.582965: step 3700, loss 0.140791, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:49.107613: step 3701, loss 0.169302, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:13:49.596856: step 3702, loss 0.151045, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:13:50.068467: step 3703, loss 0.239581, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:13:50.588944: step 3704, loss 0.171941, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:13:51.035759: step 3705, loss 0.0921148, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:13:51.656971: step 3706, loss 0.109347, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:52.173353: step 3707, loss 0.119394, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:52.653048: step 3708, loss 0.126728, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:13:52.965523: step 3709, loss 0.102093, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:53.395837: step 3710, loss 0.0453679, acc 1, learning_rate 0.000100001
2017-10-10T13:13:53.933815: step 3711, loss 0.135464, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:54.512205: step 3712, loss 0.139539, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:13:55.102387: step 3713, loss 0.0402576, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:55.641243: step 3714, loss 0.0901041, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:56.181280: step 3715, loss 0.176573, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:13:56.650229: step 3716, loss 0.105205, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:57.223313: step 3717, loss 0.140657, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:13:57.688951: step 3718, loss 0.0752247, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:13:58.301318: step 3719, loss 0.113934, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:13:58.842498: step 3720, loss 0.135582, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:14:00.148623: step 3720, loss 0.21596, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3720

2017-10-10T13:14:01.881227: step 3721, loss 0.128004, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:02.389084: step 3722, loss 0.164935, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:02.945051: step 3723, loss 0.0996876, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:03.497116: step 3724, loss 0.0859471, acc 1, learning_rate 0.000100001
2017-10-10T13:14:03.960482: step 3725, loss 0.068947, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:04.375400: step 3726, loss 0.157497, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:04.831339: step 3727, loss 0.156379, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:05.217202: step 3728, loss 0.121111, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:05.660875: step 3729, loss 0.150699, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:06.130932: step 3730, loss 0.133253, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:06.624838: step 3731, loss 0.0935703, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:07.183007: step 3732, loss 0.20005, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:14:07.670762: step 3733, loss 0.0519318, acc 1, learning_rate 0.000100001
2017-10-10T13:14:08.209282: step 3734, loss 0.0788305, acc 1, learning_rate 0.000100001
2017-10-10T13:14:08.713209: step 3735, loss 0.145684, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:09.208754: step 3736, loss 0.0618024, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:09.686736: step 3737, loss 0.124188, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:10.159558: step 3738, loss 0.0907313, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:10.700795: step 3739, loss 0.163474, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:11.159947: step 3740, loss 0.12189, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:11.676283: step 3741, loss 0.159067, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:12.237285: step 3742, loss 0.224908, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:12.752883: step 3743, loss 0.0870053, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:13.314641: step 3744, loss 0.156471, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:13.812156: step 3745, loss 0.0978119, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:14.433459: step 3746, loss 0.073951, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:15.044905: step 3747, loss 0.0710138, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:15.554467: step 3748, loss 0.100972, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:15.997873: step 3749, loss 0.13854, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:16.460232: step 3750, loss 0.0871755, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:16.984922: step 3751, loss 0.0939194, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:17.518509: step 3752, loss 0.137908, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:18.007447: step 3753, loss 0.0736952, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:18.500463: step 3754, loss 0.115764, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:19.032873: step 3755, loss 0.124875, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:19.539143: step 3756, loss 0.108545, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:20.085965: step 3757, loss 0.134071, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:20.632834: step 3758, loss 0.0654672, acc 1, learning_rate 0.000100001
2017-10-10T13:14:21.170393: step 3759, loss 0.172849, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:21.705212: step 3760, loss 0.230239, acc 0.90625, learning_rate 0.000100001

Evaluation:
2017-10-10T13:14:23.033717: step 3760, loss 0.218537, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3760

2017-10-10T13:14:24.503181: step 3761, loss 0.128551, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:25.031149: step 3762, loss 0.0565714, acc 1, learning_rate 0.000100001
2017-10-10T13:14:25.572826: step 3763, loss 0.125982, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:26.068605: step 3764, loss 0.266142, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:14:26.569209: step 3765, loss 0.0953174, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:27.196328: step 3766, loss 0.130763, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:27.683989: step 3767, loss 0.0549261, acc 1, learning_rate 0.000100001
2017-10-10T13:14:28.045125: step 3768, loss 0.130315, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:28.375402: step 3769, loss 0.108662, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:28.817204: step 3770, loss 0.228577, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:29.333174: step 3771, loss 0.122371, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:29.936706: step 3772, loss 0.154208, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:30.463652: step 3773, loss 0.052659, acc 1, learning_rate 0.000100001
2017-10-10T13:14:31.033085: step 3774, loss 0.194715, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:31.509226: step 3775, loss 0.108535, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:32.093034: step 3776, loss 0.119696, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:32.532929: step 3777, loss 0.0771651, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:33.095399: step 3778, loss 0.103946, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:33.594675: step 3779, loss 0.156755, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:34.142671: step 3780, loss 0.1317, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:34.692986: step 3781, loss 0.102251, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:35.268349: step 3782, loss 0.0852956, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:35.812130: step 3783, loss 0.12971, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:36.335404: step 3784, loss 0.0390572, acc 1, learning_rate 0.000100001
2017-10-10T13:14:36.881805: step 3785, loss 0.0884594, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:37.457051: step 3786, loss 0.129642, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:38.023713: step 3787, loss 0.0742404, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:38.412837: step 3788, loss 0.0907232, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:38.813620: step 3789, loss 0.156586, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:39.332967: step 3790, loss 0.192695, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:39.881133: step 3791, loss 0.178808, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:40.441128: step 3792, loss 0.234332, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:41.006009: step 3793, loss 0.14938, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:41.493104: step 3794, loss 0.108794, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:42.001028: step 3795, loss 0.0636932, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:42.560024: step 3796, loss 0.212608, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:43.068814: step 3797, loss 0.294663, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:14:43.634024: step 3798, loss 0.0974279, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:44.195293: step 3799, loss 0.0919712, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:44.725751: step 3800, loss 0.139683, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:14:45.922163: step 3800, loss 0.215693, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3800

2017-10-10T13:14:47.582113: step 3801, loss 0.0992576, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:48.088949: step 3802, loss 0.228276, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:48.683276: step 3803, loss 0.102081, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:49.252359: step 3804, loss 0.127193, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:49.777281: step 3805, loss 0.0823807, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:50.236603: step 3806, loss 0.13921, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:50.843204: step 3807, loss 0.105229, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:51.294071: step 3808, loss 0.0490163, acc 1, learning_rate 0.000100001
2017-10-10T13:14:51.732847: step 3809, loss 0.200017, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:52.287727: step 3810, loss 0.16369, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:14:52.836904: step 3811, loss 0.203321, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:53.413519: step 3812, loss 0.0898346, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:53.982595: step 3813, loss 0.170223, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:54.500084: step 3814, loss 0.096917, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:55.020816: step 3815, loss 0.0911054, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:55.564865: step 3816, loss 0.105282, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:56.164954: step 3817, loss 0.0951033, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:56.674906: step 3818, loss 0.155414, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:14:57.221590: step 3819, loss 0.133732, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:14:57.796856: step 3820, loss 0.0676208, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:14:58.336837: step 3821, loss 0.230673, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:14:58.801228: step 3822, loss 0.153147, acc 0.941176, learning_rate 0.000100001
2017-10-10T13:14:59.272548: step 3823, loss 0.253788, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:14:59.834271: step 3824, loss 0.137396, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:00.388995: step 3825, loss 0.211536, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:15:00.948500: step 3826, loss 0.0973477, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:01.541215: step 3827, loss 0.174756, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:02.033470: step 3828, loss 0.116786, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:02.485890: step 3829, loss 0.0748455, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:02.927907: step 3830, loss 0.158636, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:03.403155: step 3831, loss 0.0591112, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:03.910651: step 3832, loss 0.0679413, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:04.414660: step 3833, loss 0.101667, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:04.976989: step 3834, loss 0.0931325, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:05.428162: step 3835, loss 0.121306, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:05.975614: step 3836, loss 0.189266, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:15:06.539389: step 3837, loss 0.129505, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:07.061864: step 3838, loss 0.156307, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:07.639260: step 3839, loss 0.105364, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:08.116832: step 3840, loss 0.0650591, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:15:09.401258: step 3840, loss 0.216777, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3840

2017-10-10T13:15:11.196975: step 3841, loss 0.0698783, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:11.793804: step 3842, loss 0.197688, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:12.245329: step 3843, loss 0.0974972, acc 1, learning_rate 0.000100001
2017-10-10T13:15:12.692742: step 3844, loss 0.13011, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:13.153635: step 3845, loss 0.208767, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:15:13.717564: step 3846, loss 0.11292, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:14.151366: step 3847, loss 0.32647, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:15:14.549637: step 3848, loss 0.114538, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:15.052428: step 3849, loss 0.0923656, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:15.565115: step 3850, loss 0.20579, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:16.068236: step 3851, loss 0.178583, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:16.716459: step 3852, loss 0.10236, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:17.240887: step 3853, loss 0.141241, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:17.788478: step 3854, loss 0.156149, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:18.352946: step 3855, loss 0.121358, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:18.897282: step 3856, loss 0.168369, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:19.313018: step 3857, loss 0.109771, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:19.836963: step 3858, loss 0.115203, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:20.425020: step 3859, loss 0.0449992, acc 1, learning_rate 0.000100001
2017-10-10T13:15:20.941050: step 3860, loss 0.0541969, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:21.540847: step 3861, loss 0.174301, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:22.029033: step 3862, loss 0.111403, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:22.632220: step 3863, loss 0.11036, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:23.126045: step 3864, loss 0.22541, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:15:23.584582: step 3865, loss 0.156267, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:24.037116: step 3866, loss 0.117132, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:24.668917: step 3867, loss 0.197752, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:25.304546: step 3868, loss 0.12105, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:25.764922: step 3869, loss 0.239613, acc 0.875, learning_rate 0.000100001
2017-10-10T13:15:26.172953: step 3870, loss 0.0958087, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:26.588919: step 3871, loss 0.0811801, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:27.176838: step 3872, loss 0.114777, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:27.779748: step 3873, loss 0.0894467, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:28.282867: step 3874, loss 0.185883, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:28.832571: step 3875, loss 0.1338, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:29.380107: step 3876, loss 0.140348, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:29.861994: step 3877, loss 0.0946959, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:30.336629: step 3878, loss 0.103094, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:30.828304: step 3879, loss 0.227979, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:31.328993: step 3880, loss 0.147999, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:15:32.625704: step 3880, loss 0.221109, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3880

2017-10-10T13:15:34.112863: step 3881, loss 0.0974595, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:34.637082: step 3882, loss 0.172697, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:35.156977: step 3883, loss 0.146187, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:35.584015: step 3884, loss 0.102153, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:36.028828: step 3885, loss 0.16541, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:36.533267: step 3886, loss 0.101291, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:36.807028: step 3887, loss 0.0878695, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:36.969648: step 3888, loss 0.113117, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:37.380833: step 3889, loss 0.157529, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:37.842349: step 3890, loss 0.077269, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:38.292302: step 3891, loss 0.131671, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:38.828216: step 3892, loss 0.229033, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:15:39.340875: step 3893, loss 0.118607, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:15:39.797351: step 3894, loss 0.105795, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:40.288994: step 3895, loss 0.151474, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:40.764341: step 3896, loss 0.18179, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:41.281075: step 3897, loss 0.223503, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:41.845020: step 3898, loss 0.141351, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:42.452827: step 3899, loss 0.130276, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:42.980893: step 3900, loss 0.163036, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:15:43.576833: step 3901, loss 0.120062, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:44.128719: step 3902, loss 0.17155, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:44.677712: step 3903, loss 0.227137, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:15:45.211032: step 3904, loss 0.189161, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:15:45.775079: step 3905, loss 0.0770219, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:46.305036: step 3906, loss 0.0983869, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:46.849028: step 3907, loss 0.197066, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:15:47.384909: step 3908, loss 0.147249, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:47.960938: step 3909, loss 0.0731035, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:48.586879: step 3910, loss 0.100875, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:49.054410: step 3911, loss 0.214262, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:15:49.511899: step 3912, loss 0.10049, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:50.105871: step 3913, loss 0.0997602, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:50.656840: step 3914, loss 0.19631, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:15:51.200988: step 3915, loss 0.177549, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:15:51.791457: step 3916, loss 0.0480072, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:52.322132: step 3917, loss 0.0906374, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:15:52.847048: step 3918, loss 0.124271, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:15:53.344850: step 3919, loss 0.130667, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:53.784869: step 3920, loss 0.158428, acc 0.960784, learning_rate 0.000100001

Evaluation:
2017-10-10T13:15:54.997504: step 3920, loss 0.216446, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3920

2017-10-10T13:15:56.562446: step 3921, loss 0.136061, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:57.049600: step 3922, loss 0.122458, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:57.521131: step 3923, loss 0.106466, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:58.096902: step 3924, loss 0.145938, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:15:58.734328: step 3925, loss 0.158111, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:15:59.206505: step 3926, loss 0.142414, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:15:59.685334: step 3927, loss 0.183864, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:16:00.227931: step 3928, loss 0.163036, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:16:00.825521: step 3929, loss 0.185293, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:16:01.299204: step 3930, loss 0.141708, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:16:01.738206: step 3931, loss 0.14535, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:16:02.280493: step 3932, loss 0.200325, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:16:02.784894: step 3933, loss 0.168294, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:16:03.317510: step 3934, loss 0.158443, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:16:03.827890: step 3935, loss 0.119554, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:16:04.374795: step 3936, loss 0.113367, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:16:04.911469: step 3937, loss 0.161574, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:05.477930: step 3938, loss 0.218887, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:06.040422: step 3939, loss 0.094984, acc 1, learning_rate 0.0001
2017-10-10T13:16:06.594303: step 3940, loss 0.087649, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:07.084865: step 3941, loss 0.0477095, acc 1, learning_rate 0.0001
2017-10-10T13:16:07.629764: step 3942, loss 0.267697, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:08.184822: step 3943, loss 0.144181, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:08.744755: step 3944, loss 0.108471, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:09.264857: step 3945, loss 0.104419, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:09.811935: step 3946, loss 0.168712, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:10.345275: step 3947, loss 0.0810156, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:10.872832: step 3948, loss 0.112835, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:11.429087: step 3949, loss 0.191546, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:11.984817: step 3950, loss 0.160198, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:12.380852: step 3951, loss 0.158312, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:12.868849: step 3952, loss 0.148744, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:13.419130: step 3953, loss 0.121866, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:13.966507: step 3954, loss 0.096149, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:14.504993: step 3955, loss 0.185941, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:14.993043: step 3956, loss 0.107412, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:15.513118: step 3957, loss 0.117978, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:16.012973: step 3958, loss 0.159563, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:16.449119: step 3959, loss 0.122486, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:16.904567: step 3960, loss 0.0927873, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:16:18.185359: step 3960, loss 0.216038, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-3960

2017-10-10T13:16:19.913678: step 3961, loss 0.194382, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:20.443704: step 3962, loss 0.0795769, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:21.006447: step 3963, loss 0.0562906, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:21.592921: step 3964, loss 0.234187, acc 0.90625, learning_rate 0.0001
2017-10-10T13:16:22.188810: step 3965, loss 0.163248, acc 0.90625, learning_rate 0.0001
2017-10-10T13:16:22.635257: step 3966, loss 0.140211, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:23.236867: step 3967, loss 0.09051, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:23.786450: step 3968, loss 0.0942032, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:24.249283: step 3969, loss 0.150719, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:24.701611: step 3970, loss 0.0743006, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:25.248857: step 3971, loss 0.258196, acc 0.890625, learning_rate 0.0001
2017-10-10T13:16:25.849078: step 3972, loss 0.0874756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:26.436854: step 3973, loss 0.13615, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:26.994088: step 3974, loss 0.161746, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:27.500592: step 3975, loss 0.0776826, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:28.040366: step 3976, loss 0.0818636, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:28.536862: step 3977, loss 0.0937237, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:29.048266: step 3978, loss 0.121542, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:29.553010: step 3979, loss 0.0708451, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:30.034405: step 3980, loss 0.129848, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:30.581551: step 3981, loss 0.0645759, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:31.128862: step 3982, loss 0.0504748, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:31.700554: step 3983, loss 0.171573, acc 0.90625, learning_rate 0.0001
2017-10-10T13:16:32.258263: step 3984, loss 0.114468, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:32.792961: step 3985, loss 0.0843111, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:33.376874: step 3986, loss 0.0926345, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:33.952958: step 3987, loss 0.0852489, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:34.505538: step 3988, loss 0.154121, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:35.084609: step 3989, loss 0.224821, acc 0.875, learning_rate 0.0001
2017-10-10T13:16:35.536884: step 3990, loss 0.161074, acc 0.890625, learning_rate 0.0001
2017-10-10T13:16:36.036869: step 3991, loss 0.143342, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:36.556891: step 3992, loss 0.125755, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:37.124832: step 3993, loss 0.189665, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:37.684841: step 3994, loss 0.302033, acc 0.890625, learning_rate 0.0001
2017-10-10T13:16:38.226530: step 3995, loss 0.154055, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:38.796830: step 3996, loss 0.224051, acc 0.90625, learning_rate 0.0001
2017-10-10T13:16:39.371036: step 3997, loss 0.166989, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:39.908278: step 3998, loss 0.0753514, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:40.464723: step 3999, loss 0.207944, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:41.011847: step 4000, loss 0.059233, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:16:42.381027: step 4000, loss 0.214479, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4000

2017-10-10T13:16:43.786857: step 4001, loss 0.102527, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:44.298672: step 4002, loss 0.140435, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:44.844072: step 4003, loss 0.0983834, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:45.377040: step 4004, loss 0.0877054, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:45.932832: step 4005, loss 0.205679, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:46.413118: step 4006, loss 0.137931, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:46.777583: step 4007, loss 0.118825, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:47.114775: step 4008, loss 0.202064, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:47.549637: step 4009, loss 0.147427, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:48.096546: step 4010, loss 0.183772, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:48.568951: step 4011, loss 0.171539, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:49.128891: step 4012, loss 0.153666, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:49.659988: step 4013, loss 0.135147, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:50.184875: step 4014, loss 0.0826967, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:50.728964: step 4015, loss 0.250198, acc 0.90625, learning_rate 0.0001
2017-10-10T13:16:51.272908: step 4016, loss 0.0882224, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:51.748998: step 4017, loss 0.130329, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:52.209052: step 4018, loss 0.0798113, acc 0.980392, learning_rate 0.0001
2017-10-10T13:16:52.724947: step 4019, loss 0.0754316, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:53.213142: step 4020, loss 0.08573, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:53.703649: step 4021, loss 0.172546, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:54.176513: step 4022, loss 0.218036, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:54.797099: step 4023, loss 0.0429032, acc 1, learning_rate 0.0001
2017-10-10T13:16:55.361078: step 4024, loss 0.0803795, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:55.800329: step 4025, loss 0.111756, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:56.289253: step 4026, loss 0.18557, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:56.885063: step 4027, loss 0.143678, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:57.373149: step 4028, loss 0.165402, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:57.961099: step 4029, loss 0.123222, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:58.442823: step 4030, loss 0.151712, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:58.888036: step 4031, loss 0.0905784, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:59.341987: step 4032, loss 0.192529, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:59.866997: step 4033, loss 0.102321, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:00.383011: step 4034, loss 0.105004, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:00.899345: step 4035, loss 0.132027, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:01.483409: step 4036, loss 0.182851, acc 0.921875, learning_rate 0.0001
2017-10-10T13:17:02.040407: step 4037, loss 0.0868827, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:02.593717: step 4038, loss 0.139823, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:03.124192: step 4039, loss 0.0809357, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:03.648133: step 4040, loss 0.131062, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:17:04.953027: step 4040, loss 0.212956, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4040

2017-10-10T13:17:06.668328: step 4041, loss 0.105781, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:07.285705: step 4042, loss 0.0842252, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:07.886461: step 4043, loss 0.146691, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:08.432957: step 4044, loss 0.0904368, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:09.040002: step 4045, loss 0.0886169, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:09.588834: step 4046, loss 0.124962, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:10.068823: step 4047, loss 0.206887, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:10.488995: step 4048, loss 0.101555, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:10.899615: step 4049, loss 0.119008, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:11.437054: step 4050, loss 0.265846, acc 0.921875, learning_rate 0.0001
2017-10-10T13:17:11.932667: step 4051, loss 0.127416, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:12.444834: step 4052, loss 0.121843, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:12.948194: step 4053, loss 0.202835, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:13.524317: step 4054, loss 0.144785, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:14.061134: step 4055, loss 0.192961, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:14.544846: step 4056, loss 0.118614, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:15.024839: step 4057, loss 0.124668, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:15.604876: step 4058, loss 0.121968, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:16.193017: step 4059, loss 0.0857156, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:16.717180: step 4060, loss 0.0685646, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:17.258547: step 4061, loss 0.158948, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:17.779727: step 4062, loss 0.18625, acc 0.90625, learning_rate 0.0001
2017-10-10T13:17:18.335008: step 4063, loss 0.165181, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:18.844935: step 4064, loss 0.204553, acc 0.90625, learning_rate 0.0001
2017-10-10T13:17:19.384864: step 4065, loss 0.119761, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:19.872972: step 4066, loss 0.119792, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:20.384984: step 4067, loss 0.160216, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:20.837086: step 4068, loss 0.0960016, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:21.437173: step 4069, loss 0.0466605, acc 1, learning_rate 0.0001
2017-10-10T13:17:21.936885: step 4070, loss 0.120568, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:22.321037: step 4071, loss 0.229201, acc 0.90625, learning_rate 0.0001
2017-10-10T13:17:22.751849: step 4072, loss 0.212887, acc 0.90625, learning_rate 0.0001
2017-10-10T13:17:23.305001: step 4073, loss 0.11661, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:23.782772: step 4074, loss 0.132229, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:24.377040: step 4075, loss 0.247564, acc 0.90625, learning_rate 0.0001
2017-10-10T13:17:24.941036: step 4076, loss 0.1051, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:25.499033: step 4077, loss 0.0836268, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:25.936566: step 4078, loss 0.140201, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:26.450326: step 4079, loss 0.236526, acc 0.921875, learning_rate 0.0001
2017-10-10T13:17:26.929282: step 4080, loss 0.18808, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:17:28.193678: step 4080, loss 0.217366, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4080

2017-10-10T13:17:29.764883: step 4081, loss 0.207911, acc 0.90625, learning_rate 0.0001
2017-10-10T13:17:30.293369: step 4082, loss 0.0774283, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:30.825715: step 4083, loss 0.0887376, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:31.333378: step 4084, loss 0.179159, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:31.918328: step 4085, loss 0.186575, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:32.503767: step 4086, loss 0.047948, acc 1, learning_rate 0.0001
2017-10-10T13:17:32.927741: step 4087, loss 0.130831, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:33.262653: step 4088, loss 0.0830797, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:33.718225: step 4089, loss 0.123812, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:34.292968: step 4090, loss 0.0949196, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:34.848846: step 4091, loss 0.136868, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:35.436933: step 4092, loss 0.116482, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:36.012864: step 4093, loss 0.242804, acc 0.921875, learning_rate 0.0001
2017-10-10T13:17:36.534892: step 4094, loss 0.0848088, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:37.073004: step 4095, loss 0.160124, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:37.633025: step 4096, loss 0.0530319, acc 1, learning_rate 0.0001
2017-10-10T13:17:38.122884: step 4097, loss 0.10792, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:38.697697: step 4098, loss 0.182145, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:39.259881: step 4099, loss 0.0651492, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:39.818170: step 4100, loss 0.0458113, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:40.352821: step 4101, loss 0.160228, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:40.921435: step 4102, loss 0.092483, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:41.472813: step 4103, loss 0.071489, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:42.004836: step 4104, loss 0.118268, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:42.544849: step 4105, loss 0.158989, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:43.089416: step 4106, loss 0.11213, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:43.630378: step 4107, loss 0.180084, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:44.227009: step 4108, loss 0.146128, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:44.784998: step 4109, loss 0.253587, acc 0.921875, learning_rate 0.0001
2017-10-10T13:17:45.252992: step 4110, loss 0.0755079, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:45.705257: step 4111, loss 0.149757, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:46.168858: step 4112, loss 0.208667, acc 0.890625, learning_rate 0.0001
2017-10-10T13:17:46.732919: step 4113, loss 0.128743, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:47.188933: step 4114, loss 0.150435, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:47.677456: step 4115, loss 0.194992, acc 0.921875, learning_rate 0.0001
2017-10-10T13:17:48.109579: step 4116, loss 0.107692, acc 0.980392, learning_rate 0.0001
2017-10-10T13:17:48.670239: step 4117, loss 0.09787, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:49.100084: step 4118, loss 0.0516029, acc 1, learning_rate 0.0001
2017-10-10T13:17:49.598988: step 4119, loss 0.0870809, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:50.115864: step 4120, loss 0.0377879, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:17:51.437660: step 4120, loss 0.214219, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4120

2017-10-10T13:17:53.224983: step 4121, loss 0.195811, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:53.738467: step 4122, loss 0.0595441, acc 1, learning_rate 0.0001
2017-10-10T13:17:54.146665: step 4123, loss 0.141951, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:54.760997: step 4124, loss 0.141634, acc 0.921875, learning_rate 0.0001
2017-10-10T13:17:55.384536: step 4125, loss 0.192801, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:55.844848: step 4126, loss 0.0396501, acc 1, learning_rate 0.0001
2017-10-10T13:17:56.184609: step 4127, loss 0.199145, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:56.535901: step 4128, loss 0.238176, acc 0.90625, learning_rate 0.0001
2017-10-10T13:17:56.962355: step 4129, loss 0.125476, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:57.460891: step 4130, loss 0.0934884, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:57.967398: step 4131, loss 0.116056, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:58.513052: step 4132, loss 0.105567, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:59.037105: step 4133, loss 0.21469, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:59.574978: step 4134, loss 0.16475, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:00.100918: step 4135, loss 0.0806202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:00.560895: step 4136, loss 0.213442, acc 0.90625, learning_rate 0.0001
2017-10-10T13:18:01.128839: step 4137, loss 0.0651788, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:01.597513: step 4138, loss 0.0610075, acc 1, learning_rate 0.0001
2017-10-10T13:18:02.132991: step 4139, loss 0.188309, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:02.656881: step 4140, loss 0.103873, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:03.187970: step 4141, loss 0.102042, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:03.760558: step 4142, loss 0.189894, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:04.322726: step 4143, loss 0.0889133, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:04.875396: step 4144, loss 0.158503, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:05.395980: step 4145, loss 0.176326, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:05.958445: step 4146, loss 0.168919, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:06.499551: step 4147, loss 0.0559996, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:07.067109: step 4148, loss 0.175429, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:07.575323: step 4149, loss 0.140123, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:08.036953: step 4150, loss 0.136982, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:08.513132: step 4151, loss 0.0553689, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:09.065067: step 4152, loss 0.0915851, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:09.628954: step 4153, loss 0.0961815, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:10.165018: step 4154, loss 0.146548, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:10.778602: step 4155, loss 0.0813722, acc 1, learning_rate 0.0001
2017-10-10T13:18:11.318246: step 4156, loss 0.0441993, acc 1, learning_rate 0.0001
2017-10-10T13:18:11.843776: step 4157, loss 0.127699, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:12.400699: step 4158, loss 0.199513, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:12.948129: step 4159, loss 0.226328, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:13.484867: step 4160, loss 0.188261, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:18:14.752984: step 4160, loss 0.216363, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4160

2017-10-10T13:18:16.244865: step 4161, loss 0.0566662, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:16.765069: step 4162, loss 0.115083, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:17.296949: step 4163, loss 0.0607561, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:17.885089: step 4164, loss 0.10129, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:18.508878: step 4165, loss 0.056376, acc 1, learning_rate 0.0001
2017-10-10T13:18:19.024966: step 4166, loss 0.107841, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:19.480984: step 4167, loss 0.0417059, acc 1, learning_rate 0.0001
2017-10-10T13:18:19.857782: step 4168, loss 0.100705, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:20.246461: step 4169, loss 0.163287, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:20.883322: step 4170, loss 0.0486608, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:21.331322: step 4171, loss 0.130443, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:21.861339: step 4172, loss 0.136777, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:22.403103: step 4173, loss 0.0650201, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:22.976842: step 4174, loss 0.102854, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:23.534665: step 4175, loss 0.139051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:24.056889: step 4176, loss 0.174397, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:24.595902: step 4177, loss 0.168032, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:25.096840: step 4178, loss 0.206602, acc 0.90625, learning_rate 0.0001
2017-10-10T13:18:25.656942: step 4179, loss 0.168858, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:26.187432: step 4180, loss 0.171706, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:26.654322: step 4181, loss 0.184688, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:27.172998: step 4182, loss 0.212569, acc 0.90625, learning_rate 0.0001
2017-10-10T13:18:27.739863: step 4183, loss 0.0649921, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:28.292243: step 4184, loss 0.107565, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:28.808044: step 4185, loss 0.0918821, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:29.351647: step 4186, loss 0.0681459, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:29.925620: step 4187, loss 0.169515, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:30.450579: step 4188, loss 0.153348, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:31.008727: step 4189, loss 0.0669867, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:31.500895: step 4190, loss 0.274162, acc 0.890625, learning_rate 0.0001
2017-10-10T13:18:31.917053: step 4191, loss 0.126868, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:32.347254: step 4192, loss 0.113569, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:32.920121: step 4193, loss 0.141181, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:33.424866: step 4194, loss 0.143909, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:33.968031: step 4195, loss 0.193305, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:34.482924: step 4196, loss 0.0934029, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:35.044021: step 4197, loss 0.135929, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:35.565092: step 4198, loss 0.125485, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:36.116143: step 4199, loss 0.102335, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:36.698899: step 4200, loss 0.0961841, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:18:38.032893: step 4200, loss 0.216243, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4200

2017-10-10T13:18:39.623304: step 4201, loss 0.149787, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:40.238625: step 4202, loss 0.17769, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:40.741190: step 4203, loss 0.0555411, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:41.271202: step 4204, loss 0.107368, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:41.708824: step 4205, loss 0.240665, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:42.237202: step 4206, loss 0.122678, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:42.693649: step 4207, loss 0.0719843, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:43.113525: step 4208, loss 0.100087, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:43.524407: step 4209, loss 0.205098, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:44.047915: step 4210, loss 0.0987502, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:44.576387: step 4211, loss 0.0930322, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:45.128705: step 4212, loss 0.0919984, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:45.666799: step 4213, loss 0.0973516, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:46.052863: step 4214, loss 0.144696, acc 0.941176, learning_rate 0.0001
2017-10-10T13:18:46.601088: step 4215, loss 0.132007, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:47.092934: step 4216, loss 0.112145, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:47.540941: step 4217, loss 0.172321, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:48.076846: step 4218, loss 0.213876, acc 0.90625, learning_rate 0.0001
2017-10-10T13:18:48.612764: step 4219, loss 0.163343, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:49.160853: step 4220, loss 0.0987535, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:49.685093: step 4221, loss 0.128142, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:50.168990: step 4222, loss 0.0817673, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:50.689012: step 4223, loss 0.0969892, acc 1, learning_rate 0.0001
2017-10-10T13:18:51.240070: step 4224, loss 0.225787, acc 0.90625, learning_rate 0.0001
2017-10-10T13:18:51.737626: step 4225, loss 0.145286, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:52.234314: step 4226, loss 0.117125, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:52.801214: step 4227, loss 0.168502, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:53.305041: step 4228, loss 0.132713, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:53.852892: step 4229, loss 0.0834919, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:54.379329: step 4230, loss 0.14584, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:54.907416: step 4231, loss 0.121184, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:55.328847: step 4232, loss 0.0907918, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:55.780072: step 4233, loss 0.100372, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:56.241683: step 4234, loss 0.0452815, acc 1, learning_rate 0.0001
2017-10-10T13:18:56.720106: step 4235, loss 0.038074, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:57.357209: step 4236, loss 0.10843, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:57.896968: step 4237, loss 0.157671, acc 0.90625, learning_rate 0.0001
2017-10-10T13:18:58.364294: step 4238, loss 0.244508, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:58.886561: step 4239, loss 0.136017, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:59.369127: step 4240, loss 0.147201, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:19:00.568499: step 4240, loss 0.213233, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4240

2017-10-10T13:19:02.312874: step 4241, loss 0.15888, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:02.894651: step 4242, loss 0.0782998, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:03.488219: step 4243, loss 0.135096, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:03.920808: step 4244, loss 0.0747683, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:04.370606: step 4245, loss 0.169625, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:04.932832: step 4246, loss 0.141759, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:05.588092: step 4247, loss 0.164365, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:06.056270: step 4248, loss 0.0958041, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:06.526876: step 4249, loss 0.117433, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:06.979944: step 4250, loss 0.119607, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:07.554539: step 4251, loss 0.165825, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:08.141017: step 4252, loss 0.0261239, acc 1, learning_rate 0.0001
2017-10-10T13:19:08.681856: step 4253, loss 0.118193, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:09.232895: step 4254, loss 0.289464, acc 0.90625, learning_rate 0.0001
2017-10-10T13:19:09.756250: step 4255, loss 0.133923, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:10.348897: step 4256, loss 0.157161, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:10.847288: step 4257, loss 0.146323, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:11.393915: step 4258, loss 0.139104, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:11.908435: step 4259, loss 0.121728, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:12.455567: step 4260, loss 0.199976, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:13.023937: step 4261, loss 0.0799174, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:13.507410: step 4262, loss 0.140052, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:14.021206: step 4263, loss 0.11008, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:14.504980: step 4264, loss 0.162194, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:15.061000: step 4265, loss 0.0689199, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:15.608903: step 4266, loss 0.0802144, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:16.108859: step 4267, loss 0.113758, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:16.642061: step 4268, loss 0.158032, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:17.151839: step 4269, loss 0.159177, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:17.701352: step 4270, loss 0.123985, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:18.220956: step 4271, loss 0.104781, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:18.765089: step 4272, loss 0.0942309, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:19.239109: step 4273, loss 0.160867, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:19.660834: step 4274, loss 0.0798369, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:20.224847: step 4275, loss 0.22345, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:20.738216: step 4276, loss 0.101434, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:21.216866: step 4277, loss 0.238499, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:21.746550: step 4278, loss 0.206517, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:22.272907: step 4279, loss 0.144344, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:22.793041: step 4280, loss 0.31668, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:19:24.077879: step 4280, loss 0.217044, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4280

2017-10-10T13:19:25.569133: step 4281, loss 0.122127, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:26.164906: step 4282, loss 0.18989, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:26.625034: step 4283, loss 0.0916175, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:27.091840: step 4284, loss 0.0940291, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:27.660824: step 4285, loss 0.115878, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:28.134891: step 4286, loss 0.0872014, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:28.676864: step 4287, loss 0.192524, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:29.249025: step 4288, loss 0.111958, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:29.656870: step 4289, loss 0.0969566, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:30.036911: step 4290, loss 0.0747868, acc 1, learning_rate 0.0001
2017-10-10T13:19:30.500954: step 4291, loss 0.098155, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:31.096878: step 4292, loss 0.0698215, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:31.596834: step 4293, loss 0.119875, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:32.171975: step 4294, loss 0.152952, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:32.645066: step 4295, loss 0.188423, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:33.204865: step 4296, loss 0.0556402, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:33.737240: step 4297, loss 0.0486542, acc 1, learning_rate 0.0001
2017-10-10T13:19:34.268849: step 4298, loss 0.102329, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:34.738644: step 4299, loss 0.247458, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:35.277093: step 4300, loss 0.0747356, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:35.805994: step 4301, loss 0.0853095, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:36.400487: step 4302, loss 0.109325, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:36.961880: step 4303, loss 0.0457239, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:37.521669: step 4304, loss 0.147663, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:38.116928: step 4305, loss 0.0600281, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:38.747556: step 4306, loss 0.0916486, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:39.210571: step 4307, loss 0.108398, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:39.737024: step 4308, loss 0.172988, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:40.285057: step 4309, loss 0.159919, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:40.815323: step 4310, loss 0.134322, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:41.438000: step 4311, loss 0.106502, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:41.930630: step 4312, loss 0.188922, acc 0.960784, learning_rate 0.0001
2017-10-10T13:19:42.411458: step 4313, loss 0.0994906, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:42.872891: step 4314, loss 0.16081, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:43.396848: step 4315, loss 0.195948, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:43.976102: step 4316, loss 0.0502724, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:44.494307: step 4317, loss 0.107235, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:44.980892: step 4318, loss 0.0823551, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:45.547015: step 4319, loss 0.115069, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:46.073130: step 4320, loss 0.222314, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:19:47.348918: step 4320, loss 0.216913, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4320

2017-10-10T13:19:48.989479: step 4321, loss 0.238701, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:49.399300: step 4322, loss 0.119078, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:49.772661: step 4323, loss 0.118634, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:50.265068: step 4324, loss 0.191335, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:50.773795: step 4325, loss 0.122622, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:51.343690: step 4326, loss 0.124466, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:51.893914: step 4327, loss 0.0997327, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:52.364747: step 4328, loss 0.0731023, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:52.790469: step 4329, loss 0.109348, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:53.217772: step 4330, loss 0.19717, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:53.792860: step 4331, loss 0.213768, acc 0.90625, learning_rate 0.0001
2017-10-10T13:19:54.303149: step 4332, loss 0.246937, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:54.837691: step 4333, loss 0.114735, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:55.324850: step 4334, loss 0.0878974, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:55.824865: step 4335, loss 0.125536, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:56.325050: step 4336, loss 0.184642, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:56.866105: step 4337, loss 0.0909826, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:57.371489: step 4338, loss 0.295992, acc 0.890625, learning_rate 0.0001
2017-10-10T13:19:57.908457: step 4339, loss 0.0491829, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:58.441508: step 4340, loss 0.0588175, acc 1, learning_rate 0.0001
2017-10-10T13:19:58.957768: step 4341, loss 0.138505, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:59.476963: step 4342, loss 0.143574, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:00.032925: step 4343, loss 0.0727607, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:00.507122: step 4344, loss 0.159469, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:00.985572: step 4345, loss 0.0314403, acc 1, learning_rate 0.0001
2017-10-10T13:20:01.450364: step 4346, loss 0.0542586, acc 1, learning_rate 0.0001
2017-10-10T13:20:01.922195: step 4347, loss 0.0882645, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:02.451777: step 4348, loss 0.150428, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:02.920084: step 4349, loss 0.212782, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:03.445207: step 4350, loss 0.118102, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:03.993184: step 4351, loss 0.0480561, acc 1, learning_rate 0.0001
2017-10-10T13:20:04.552374: step 4352, loss 0.139253, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:05.072902: step 4353, loss 0.0939611, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:05.649154: step 4354, loss 0.0995718, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:06.076211: step 4355, loss 0.197842, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:06.491847: step 4356, loss 0.182618, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:07.053627: step 4357, loss 0.172582, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:07.566455: step 4358, loss 0.0424541, acc 1, learning_rate 0.0001
2017-10-10T13:20:08.099162: step 4359, loss 0.171467, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:08.640956: step 4360, loss 0.150243, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:20:09.868906: step 4360, loss 0.214459, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4360

2017-10-10T13:20:11.748771: step 4361, loss 0.0691775, acc 1, learning_rate 0.0001
2017-10-10T13:20:12.291187: step 4362, loss 0.0695851, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:12.714591: step 4363, loss 0.122475, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:13.165723: step 4364, loss 0.107367, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:13.733255: step 4365, loss 0.114187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:14.316854: step 4366, loss 0.126573, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:14.969001: step 4367, loss 0.128156, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:15.498680: step 4368, loss 0.13817, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:15.972831: step 4369, loss 0.0660779, acc 1, learning_rate 0.0001
2017-10-10T13:20:16.426922: step 4370, loss 0.0953265, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:16.881281: step 4371, loss 0.116459, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:17.442958: step 4372, loss 0.180497, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:18.028850: step 4373, loss 0.160769, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:18.628957: step 4374, loss 0.0679402, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:19.169009: step 4375, loss 0.126345, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:19.665288: step 4376, loss 0.111209, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:20.161775: step 4377, loss 0.1203, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:20.693181: step 4378, loss 0.0467847, acc 1, learning_rate 0.0001
2017-10-10T13:20:21.305181: step 4379, loss 0.125748, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:21.795173: step 4380, loss 0.059495, acc 1, learning_rate 0.0001
2017-10-10T13:20:22.293085: step 4381, loss 0.0738171, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:22.792867: step 4382, loss 0.203433, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:23.331392: step 4383, loss 0.045332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:23.817918: step 4384, loss 0.0653122, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:24.341848: step 4385, loss 0.135927, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:24.882383: step 4386, loss 0.133158, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:25.464929: step 4387, loss 0.120544, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:26.005482: step 4388, loss 0.155187, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:26.540859: step 4389, loss 0.0611459, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:27.072717: step 4390, loss 0.0626574, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:27.679079: step 4391, loss 0.134085, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:28.203665: step 4392, loss 0.201403, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:28.575565: step 4393, loss 0.167613, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:28.972997: step 4394, loss 0.21948, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:29.325936: step 4395, loss 0.149566, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:29.846761: step 4396, loss 0.188819, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:30.353814: step 4397, loss 0.0853814, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:30.867210: step 4398, loss 0.14223, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:31.368529: step 4399, loss 0.060425, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:31.852891: step 4400, loss 0.168388, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:20:33.054081: step 4400, loss 0.215034, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4400

2017-10-10T13:20:34.351923: step 4401, loss 0.113778, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:34.923476: step 4402, loss 0.0735711, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:35.452104: step 4403, loss 0.10724, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:35.924849: step 4404, loss 0.0698595, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:36.326694: step 4405, loss 0.0492578, acc 1, learning_rate 0.0001
2017-10-10T13:20:36.780507: step 4406, loss 0.183385, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:37.308430: step 4407, loss 0.0959442, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:37.845087: step 4408, loss 0.0943553, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:38.409868: step 4409, loss 0.0451743, acc 1, learning_rate 0.0001
2017-10-10T13:20:38.840982: step 4410, loss 0.24765, acc 0.901961, learning_rate 0.0001
2017-10-10T13:20:39.251316: step 4411, loss 0.113521, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:39.696158: step 4412, loss 0.0821034, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:40.078623: step 4413, loss 0.161141, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:40.656842: step 4414, loss 0.137507, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:41.132099: step 4415, loss 0.0815678, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:41.717001: step 4416, loss 0.0645364, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:42.173088: step 4417, loss 0.058988, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:42.705103: step 4418, loss 0.107624, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:43.255889: step 4419, loss 0.160531, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:43.822436: step 4420, loss 0.0685441, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:44.382035: step 4421, loss 0.117009, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:44.936909: step 4422, loss 0.0807028, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:45.497016: step 4423, loss 0.18722, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:45.993372: step 4424, loss 0.188653, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:46.594891: step 4425, loss 0.070914, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:47.128842: step 4426, loss 0.109144, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:47.616891: step 4427, loss 0.0576721, acc 1, learning_rate 0.0001
2017-10-10T13:20:48.140893: step 4428, loss 0.109762, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:48.577062: step 4429, loss 0.147787, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:49.029033: step 4430, loss 0.222953, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:49.521149: step 4431, loss 0.1603, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:50.117664: step 4432, loss 0.199398, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:50.722585: step 4433, loss 0.0857409, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:51.250778: step 4434, loss 0.145568, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:51.792756: step 4435, loss 0.114621, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:52.224489: step 4436, loss 0.225139, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:52.620981: step 4437, loss 0.127959, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:53.200998: step 4438, loss 0.0621549, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:53.796840: step 4439, loss 0.189709, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:54.336515: step 4440, loss 0.231923, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:20:55.428025: step 4440, loss 0.216009, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4440

2017-10-10T13:20:57.145196: step 4441, loss 0.0711981, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:57.653107: step 4442, loss 0.190751, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:58.190755: step 4443, loss 0.0558732, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:58.704938: step 4444, loss 0.263614, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:59.267433: step 4445, loss 0.138223, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:59.905203: step 4446, loss 0.226319, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:00.324941: step 4447, loss 0.215126, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:00.844689: step 4448, loss 0.081091, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:01.348886: step 4449, loss 0.102436, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:01.909147: step 4450, loss 0.0922503, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:02.471238: step 4451, loss 0.132472, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:03.032858: step 4452, loss 0.14958, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:03.524236: step 4453, loss 0.0318152, acc 1, learning_rate 0.0001
2017-10-10T13:21:03.936882: step 4454, loss 0.132928, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:04.292908: step 4455, loss 0.11709, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:04.821102: step 4456, loss 0.128822, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:05.405870: step 4457, loss 0.141049, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:05.927481: step 4458, loss 0.064395, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:06.439575: step 4459, loss 0.251229, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:06.904844: step 4460, loss 0.0375676, acc 1, learning_rate 0.0001
2017-10-10T13:21:07.448854: step 4461, loss 0.218893, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:08.003322: step 4462, loss 0.124494, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:08.565107: step 4463, loss 0.0818334, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:09.152341: step 4464, loss 0.142054, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:09.660841: step 4465, loss 0.143882, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:10.192912: step 4466, loss 0.113696, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:10.777001: step 4467, loss 0.0604873, acc 1, learning_rate 0.0001
2017-10-10T13:21:11.313168: step 4468, loss 0.14095, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:11.844966: step 4469, loss 0.0748923, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:12.322726: step 4470, loss 0.168939, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:12.836969: step 4471, loss 0.0573144, acc 1, learning_rate 0.0001
2017-10-10T13:21:13.369082: step 4472, loss 0.0957287, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:13.908111: step 4473, loss 0.0922385, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:14.482242: step 4474, loss 0.148074, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:14.977263: step 4475, loss 0.0761626, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:15.482635: step 4476, loss 0.0762596, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:15.936672: step 4477, loss 0.124942, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:16.406955: step 4478, loss 0.197378, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:16.924480: step 4479, loss 0.107469, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:17.448844: step 4480, loss 0.113523, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:21:18.869574: step 4480, loss 0.210854, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4480

2017-10-10T13:21:20.398751: step 4481, loss 0.139554, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:20.952501: step 4482, loss 0.142138, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:21.492919: step 4483, loss 0.0758295, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:22.149243: step 4484, loss 0.157463, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:22.602968: step 4485, loss 0.0648262, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:23.030874: step 4486, loss 0.131507, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:23.580869: step 4487, loss 0.196788, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:24.116857: step 4488, loss 0.251501, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:24.596836: step 4489, loss 0.126408, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:25.085106: step 4490, loss 0.138483, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:25.528918: step 4491, loss 0.138015, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:26.116960: step 4492, loss 0.0951277, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:26.625135: step 4493, loss 0.192732, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:27.036260: step 4494, loss 0.13543, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:27.390462: step 4495, loss 0.223808, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:27.876819: step 4496, loss 0.0709131, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:28.411831: step 4497, loss 0.128955, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:28.950627: step 4498, loss 0.0849824, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:29.408979: step 4499, loss 0.0839805, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:29.936852: step 4500, loss 0.0382217, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:30.436195: step 4501, loss 0.126335, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:30.940878: step 4502, loss 0.15817, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:31.450461: step 4503, loss 0.212325, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:31.889031: step 4504, loss 0.121653, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:32.426094: step 4505, loss 0.0924606, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:32.884851: step 4506, loss 0.271984, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:33.397079: step 4507, loss 0.171644, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:33.873111: step 4508, loss 0.0705928, acc 0.960784, learning_rate 0.0001
2017-10-10T13:21:34.410892: step 4509, loss 0.0737708, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:34.971989: step 4510, loss 0.169022, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:35.532181: step 4511, loss 0.0628304, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:36.094116: step 4512, loss 0.0911744, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:36.641910: step 4513, loss 0.145065, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:37.140370: step 4514, loss 0.118373, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:37.719258: step 4515, loss 0.0450814, acc 1, learning_rate 0.0001
2017-10-10T13:21:38.345692: step 4516, loss 0.103728, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:38.805100: step 4517, loss 0.225124, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:39.243723: step 4518, loss 0.141729, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:39.807800: step 4519, loss 0.13888, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:40.353032: step 4520, loss 0.0836704, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:21:41.586404: step 4520, loss 0.213765, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4520

2017-10-10T13:21:43.305138: step 4521, loss 0.144804, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:43.878469: step 4522, loss 0.219434, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:44.372858: step 4523, loss 0.162627, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:44.904920: step 4524, loss 0.0196718, acc 1, learning_rate 0.0001
2017-10-10T13:21:45.473012: step 4525, loss 0.113183, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:45.971650: step 4526, loss 0.242111, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:46.372959: step 4527, loss 0.155366, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:46.845253: step 4528, loss 0.0894508, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:47.317044: step 4529, loss 0.148386, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:47.826970: step 4530, loss 0.0982291, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:48.333052: step 4531, loss 0.119818, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:48.889359: step 4532, loss 0.114407, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:49.426824: step 4533, loss 0.132917, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:50.096610: step 4534, loss 0.0749032, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:50.660873: step 4535, loss 0.0650602, acc 1, learning_rate 0.0001
2017-10-10T13:21:51.073039: step 4536, loss 0.223061, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:51.525030: step 4537, loss 0.0565414, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:52.044085: step 4538, loss 0.105333, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:52.597035: step 4539, loss 0.0905137, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:53.124962: step 4540, loss 0.0719977, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:53.677620: step 4541, loss 0.0633047, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:54.209042: step 4542, loss 0.134122, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:54.780904: step 4543, loss 0.113235, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:55.292842: step 4544, loss 0.24076, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:55.864341: step 4545, loss 0.151202, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:56.371474: step 4546, loss 0.0646594, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:56.909148: step 4547, loss 0.219486, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:57.397084: step 4548, loss 0.135051, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:57.893107: step 4549, loss 0.108166, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:58.364970: step 4550, loss 0.160842, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:58.961178: step 4551, loss 0.101127, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:59.400984: step 4552, loss 0.353679, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:59.888887: step 4553, loss 0.138292, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:00.469001: step 4554, loss 0.10847, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:01.039296: step 4555, loss 0.0934577, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:01.500827: step 4556, loss 0.0893377, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:01.896969: step 4557, loss 0.182012, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:02.377111: step 4558, loss 0.0854619, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:02.905055: step 4559, loss 0.166979, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:03.469094: step 4560, loss 0.278669, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:22:04.836880: step 4560, loss 0.211332, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4560

2017-10-10T13:22:06.141012: step 4561, loss 0.0682234, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:06.729110: step 4562, loss 0.0694975, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:07.291020: step 4563, loss 0.173274, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:07.756867: step 4564, loss 0.127443, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:08.275472: step 4565, loss 0.108597, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:08.877160: step 4566, loss 0.110239, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:09.300859: step 4567, loss 0.0709318, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:09.715628: step 4568, loss 0.150512, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:10.243982: step 4569, loss 0.101366, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:10.786483: step 4570, loss 0.178209, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:11.300139: step 4571, loss 0.144965, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:11.765846: step 4572, loss 0.109858, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:12.309489: step 4573, loss 0.107509, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:12.856827: step 4574, loss 0.113303, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:13.464878: step 4575, loss 0.0888675, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:14.013975: step 4576, loss 0.0936559, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:14.484132: step 4577, loss 0.205207, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:15.092633: step 4578, loss 0.196707, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:15.654761: step 4579, loss 0.146708, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:16.184135: step 4580, loss 0.0667128, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:16.732930: step 4581, loss 0.113162, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:17.305738: step 4582, loss 0.0947411, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:17.881825: step 4583, loss 0.193151, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:18.419844: step 4584, loss 0.0664472, acc 1, learning_rate 0.0001
2017-10-10T13:22:18.915757: step 4585, loss 0.186113, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:19.436824: step 4586, loss 0.107965, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:19.957921: step 4587, loss 0.186071, acc 0.875, learning_rate 0.0001
2017-10-10T13:22:20.442551: step 4588, loss 0.127686, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:20.968930: step 4589, loss 0.123079, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:21.557453: step 4590, loss 0.0743338, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:22.108091: step 4591, loss 0.0946542, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:22.631967: step 4592, loss 0.106079, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:23.145103: step 4593, loss 0.132714, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:23.716380: step 4594, loss 0.0920502, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:24.300910: step 4595, loss 0.15051, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:24.895371: step 4596, loss 0.176038, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:25.373738: step 4597, loss 0.0999659, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:25.796837: step 4598, loss 0.115787, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:26.209759: step 4599, loss 0.28609, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:26.674739: step 4600, loss 0.198443, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:22:28.097215: step 4600, loss 0.211039, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4600

2017-10-10T13:22:30.292319: step 4601, loss 0.167776, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:30.826912: step 4602, loss 0.142694, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:31.428445: step 4603, loss 0.132539, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:32.032934: step 4604, loss 0.16872, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:32.489009: step 4605, loss 0.0711978, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:32.904993: step 4606, loss 0.109349, acc 0.960784, learning_rate 0.0001
2017-10-10T13:22:33.337111: step 4607, loss 0.0799894, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:33.840955: step 4608, loss 0.0693587, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:34.378349: step 4609, loss 0.0899565, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:34.932664: step 4610, loss 0.0629934, acc 1, learning_rate 0.0001
2017-10-10T13:22:35.488843: step 4611, loss 0.187749, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:36.043525: step 4612, loss 0.11326, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:36.453180: step 4613, loss 0.049018, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:36.966308: step 4614, loss 0.154767, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:37.561316: step 4615, loss 0.080629, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:38.124536: step 4616, loss 0.156658, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:38.568347: step 4617, loss 0.078165, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:39.021319: step 4618, loss 0.152102, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:39.587422: step 4619, loss 0.189913, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:40.138997: step 4620, loss 0.142908, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:40.678215: step 4621, loss 0.100172, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:41.231967: step 4622, loss 0.0974214, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:41.813803: step 4623, loss 0.115669, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:42.375481: step 4624, loss 0.166677, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:42.914090: step 4625, loss 0.0840723, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:43.386849: step 4626, loss 0.248165, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:43.910372: step 4627, loss 0.0786645, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:44.412875: step 4628, loss 0.0845662, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:45.000877: step 4629, loss 0.0990429, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:45.559154: step 4630, loss 0.115879, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:46.067484: step 4631, loss 0.245608, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:46.487366: step 4632, loss 0.113366, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:47.006845: step 4633, loss 0.15695, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:47.497426: step 4634, loss 0.146277, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:48.014125: step 4635, loss 0.0767929, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:48.605065: step 4636, loss 0.124477, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:49.098849: step 4637, loss 0.141047, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:49.572490: step 4638, loss 0.13239, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:50.070163: step 4639, loss 0.0866646, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:50.605318: step 4640, loss 0.0494568, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:22:52.226787: step 4640, loss 0.210369, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4640

2017-10-10T13:22:53.974008: step 4641, loss 0.156185, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:54.557002: step 4642, loss 0.138617, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:55.126021: step 4643, loss 0.118999, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:55.573541: step 4644, loss 0.0540114, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:56.000416: step 4645, loss 0.0933177, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:56.415673: step 4646, loss 0.19479, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:56.857170: step 4647, loss 0.109097, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:57.369298: step 4648, loss 0.0894804, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:57.851585: step 4649, loss 0.0376028, acc 1, learning_rate 0.0001
2017-10-10T13:22:58.367025: step 4650, loss 0.0971415, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:58.857750: step 4651, loss 0.135971, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:59.402822: step 4652, loss 0.0597825, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:59.952473: step 4653, loss 0.0886906, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:00.498770: step 4654, loss 0.0939735, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:01.124255: step 4655, loss 0.0827183, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:01.575033: step 4656, loss 0.0605348, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:02.052781: step 4657, loss 0.145236, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:02.581192: step 4658, loss 0.0565573, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:03.118327: step 4659, loss 0.149356, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:03.716860: step 4660, loss 0.0511572, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:04.282509: step 4661, loss 0.0938546, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:04.812800: step 4662, loss 0.0968391, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:05.359241: step 4663, loss 0.127262, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:05.899756: step 4664, loss 0.167021, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:06.341069: step 4665, loss 0.0734328, acc 1, learning_rate 0.0001
2017-10-10T13:23:06.863706: step 4666, loss 0.205108, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:07.285180: step 4667, loss 0.257118, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:07.788607: step 4668, loss 0.162966, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:08.270927: step 4669, loss 0.1241, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:08.756905: step 4670, loss 0.0852843, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:09.315636: step 4671, loss 0.150388, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:09.872345: step 4672, loss 0.0659599, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:10.407951: step 4673, loss 0.167302, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:10.922599: step 4674, loss 0.130214, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:11.450044: step 4675, loss 0.113061, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:11.993614: step 4676, loss 0.0784293, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:12.513029: step 4677, loss 0.105649, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:12.975940: step 4678, loss 0.221221, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:13.492854: step 4679, loss 0.124623, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:14.023538: step 4680, loss 0.0694088, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:23:15.277985: step 4680, loss 0.211171, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4680

2017-10-10T13:23:16.727208: step 4681, loss 0.0742802, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:17.277988: step 4682, loss 0.0779166, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:17.842250: step 4683, loss 0.0767574, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:18.280819: step 4684, loss 0.0991741, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:18.637082: step 4685, loss 0.196044, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:19.120036: step 4686, loss 0.0394307, acc 1, learning_rate 0.0001
2017-10-10T13:23:19.656888: step 4687, loss 0.0942223, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:20.126334: step 4688, loss 0.121456, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:20.693014: step 4689, loss 0.10906, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:21.191617: step 4690, loss 0.0637403, acc 1, learning_rate 0.0001
2017-10-10T13:23:21.661114: step 4691, loss 0.0590678, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:22.219241: step 4692, loss 0.0570738, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:22.753122: step 4693, loss 0.174682, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:23.223593: step 4694, loss 0.0579472, acc 1, learning_rate 0.0001
2017-10-10T13:23:23.764352: step 4695, loss 0.0912223, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:24.269007: step 4696, loss 0.0995426, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:24.868863: step 4697, loss 0.142592, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:25.431578: step 4698, loss 0.177584, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:25.914885: step 4699, loss 0.106444, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:26.390899: step 4700, loss 0.0893543, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:26.930113: step 4701, loss 0.0862306, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:27.440475: step 4702, loss 0.0887485, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:28.001654: step 4703, loss 0.0933874, acc 1, learning_rate 0.0001
2017-10-10T13:23:28.453898: step 4704, loss 0.238719, acc 0.901961, learning_rate 0.0001
2017-10-10T13:23:28.928003: step 4705, loss 0.22552, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:29.454351: step 4706, loss 0.282906, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:29.978422: step 4707, loss 0.137071, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:30.465844: step 4708, loss 0.13429, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:31.000838: step 4709, loss 0.175908, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:31.492852: step 4710, loss 0.112388, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:32.020881: step 4711, loss 0.0719496, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:32.572979: step 4712, loss 0.0916362, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:33.133207: step 4713, loss 0.105779, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:33.618174: step 4714, loss 0.0933471, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:34.100827: step 4715, loss 0.101969, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:34.668868: step 4716, loss 0.0966802, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:35.293144: step 4717, loss 0.110076, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:35.770053: step 4718, loss 0.0540946, acc 1, learning_rate 0.0001
2017-10-10T13:23:36.183828: step 4719, loss 0.218954, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:36.719049: step 4720, loss 0.109665, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:23:38.051151: step 4720, loss 0.210527, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4720

2017-10-10T13:23:39.713152: step 4721, loss 0.113053, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:40.259973: step 4722, loss 0.178518, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:40.799706: step 4723, loss 0.155791, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:41.279856: step 4724, loss 0.194447, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:41.697214: step 4725, loss 0.0656178, acc 1, learning_rate 0.0001
2017-10-10T13:23:42.144973: step 4726, loss 0.0646458, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:42.628888: step 4727, loss 0.10956, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:43.140912: step 4728, loss 0.0391044, acc 1, learning_rate 0.0001
2017-10-10T13:23:43.753121: step 4729, loss 0.129799, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:44.264368: step 4730, loss 0.121543, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:44.861133: step 4731, loss 0.106726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:45.448826: step 4732, loss 0.0978069, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:45.969971: step 4733, loss 0.118589, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:46.401634: step 4734, loss 0.0982136, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:46.838450: step 4735, loss 0.177545, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:47.305209: step 4736, loss 0.180207, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:47.864405: step 4737, loss 0.0646813, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:48.406577: step 4738, loss 0.109248, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:48.880843: step 4739, loss 0.135248, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:49.337746: step 4740, loss 0.137398, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:49.843807: step 4741, loss 0.0350624, acc 1, learning_rate 0.0001
2017-10-10T13:23:50.312932: step 4742, loss 0.24669, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:50.888832: step 4743, loss 0.156712, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:51.453396: step 4744, loss 0.254094, acc 0.890625, learning_rate 0.0001
2017-10-10T13:23:51.960558: step 4745, loss 0.108969, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:52.508863: step 4746, loss 0.196252, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:53.059976: step 4747, loss 0.112579, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:53.568886: step 4748, loss 0.040557, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:54.131991: step 4749, loss 0.228996, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:54.680895: step 4750, loss 0.0889041, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:55.252716: step 4751, loss 0.0558501, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:55.747618: step 4752, loss 0.0906733, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:56.276735: step 4753, loss 0.122678, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:56.743537: step 4754, loss 0.217608, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:57.322473: step 4755, loss 0.216798, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:57.872950: step 4756, loss 0.19782, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:58.458931: step 4757, loss 0.0519207, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:59.049163: step 4758, loss 0.0923294, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:59.522300: step 4759, loss 0.216427, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:59.964925: step 4760, loss 0.172327, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:01.367381: step 4760, loss 0.210728, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4760

2017-10-10T13:24:03.046033: step 4761, loss 0.0956307, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:03.661112: step 4762, loss 0.0663218, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:04.172639: step 4763, loss 0.152189, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:04.556115: step 4764, loss 0.118599, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:05.001183: step 4765, loss 0.111819, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:05.480888: step 4766, loss 0.0335093, acc 1, learning_rate 0.0001
2017-10-10T13:24:06.108069: step 4767, loss 0.151332, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:06.633553: step 4768, loss 0.0635089, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:07.172874: step 4769, loss 0.142888, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:07.685182: step 4770, loss 0.13675, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:08.176173: step 4771, loss 0.156485, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:08.702791: step 4772, loss 0.116063, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:09.294638: step 4773, loss 0.130917, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:09.821089: step 4774, loss 0.160997, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:10.336588: step 4775, loss 0.134726, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:10.985104: step 4776, loss 0.133095, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:11.437195: step 4777, loss 0.0900627, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:11.900249: step 4778, loss 0.132096, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:12.326374: step 4779, loss 0.0890878, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:12.741168: step 4780, loss 0.158856, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:13.321183: step 4781, loss 0.0756258, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:13.769104: step 4782, loss 0.231935, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:14.300921: step 4783, loss 0.114994, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:14.801190: step 4784, loss 0.215004, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:15.292942: step 4785, loss 0.131279, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:15.869443: step 4786, loss 0.136437, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:16.332439: step 4787, loss 0.0668901, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:16.920939: step 4788, loss 0.14642, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:17.425091: step 4789, loss 0.143028, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:17.936897: step 4790, loss 0.0858985, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:18.493126: step 4791, loss 0.0538974, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:19.013037: step 4792, loss 0.111939, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:19.633451: step 4793, loss 0.0870218, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:20.171271: step 4794, loss 0.137334, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:20.681037: step 4795, loss 0.161871, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:21.076985: step 4796, loss 0.0541848, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:21.605141: step 4797, loss 0.18689, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:22.256946: step 4798, loss 0.0872019, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:22.671276: step 4799, loss 0.060921, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:23.100918: step 4800, loss 0.125415, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:24.550653: step 4800, loss 0.208639, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4800

2017-10-10T13:24:25.896966: step 4801, loss 0.107795, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:26.402322: step 4802, loss 0.0969859, acc 0.941176, learning_rate 0.0001
2017-10-10T13:24:26.997469: step 4803, loss 0.115527, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:27.496853: step 4804, loss 0.174203, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:27.951330: step 4805, loss 0.258667, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:28.279062: step 4806, loss 0.041171, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:28.824204: step 4807, loss 0.0622181, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:29.323087: step 4808, loss 0.082917, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:29.892850: step 4809, loss 0.198453, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:30.413222: step 4810, loss 0.156725, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:30.957235: step 4811, loss 0.0821109, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:31.473316: step 4812, loss 0.111006, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:32.009242: step 4813, loss 0.157049, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:32.508886: step 4814, loss 0.108758, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:33.052862: step 4815, loss 0.100322, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:33.568867: step 4816, loss 0.149034, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:34.161177: step 4817, loss 0.107552, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:34.684988: step 4818, loss 0.0809156, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:35.077591: step 4819, loss 0.110777, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:35.509089: step 4820, loss 0.194688, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:36.074013: step 4821, loss 0.0426476, acc 1, learning_rate 0.0001
2017-10-10T13:24:36.627008: step 4822, loss 0.111425, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:37.175864: step 4823, loss 0.0739833, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:37.734977: step 4824, loss 0.0877072, acc 1, learning_rate 0.0001
2017-10-10T13:24:38.248840: step 4825, loss 0.08026, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:38.740467: step 4826, loss 0.130703, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:39.099581: step 4827, loss 0.148633, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:39.586367: step 4828, loss 0.11412, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:40.053219: step 4829, loss 0.117887, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:40.577255: step 4830, loss 0.0773806, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:41.116833: step 4831, loss 0.113693, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:41.652536: step 4832, loss 0.141613, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:42.204530: step 4833, loss 0.151, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:42.676300: step 4834, loss 0.14452, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:43.236909: step 4835, loss 0.106539, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:43.796200: step 4836, loss 0.128769, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:44.373169: step 4837, loss 0.0887204, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:44.930820: step 4838, loss 0.0624453, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:45.412948: step 4839, loss 0.102412, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:45.868974: step 4840, loss 0.181519, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:47.200592: step 4840, loss 0.210564, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4840

2017-10-10T13:24:48.890317: step 4841, loss 0.17387, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:49.412104: step 4842, loss 0.0338453, acc 1, learning_rate 0.0001
2017-10-10T13:24:50.006655: step 4843, loss 0.0916885, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:50.569069: step 4844, loss 0.0840981, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:51.020890: step 4845, loss 0.0752249, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:51.430663: step 4846, loss 0.123354, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:51.988884: step 4847, loss 0.15494, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:52.523003: step 4848, loss 0.103445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:53.028521: step 4849, loss 0.170042, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:53.612860: step 4850, loss 0.1656, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:54.156947: step 4851, loss 0.110607, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:54.627733: step 4852, loss 0.0713255, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:55.121072: step 4853, loss 0.25633, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:55.641035: step 4854, loss 0.0980541, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:56.149419: step 4855, loss 0.125909, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:56.752957: step 4856, loss 0.0910597, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:57.303572: step 4857, loss 0.145547, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:57.732661: step 4858, loss 0.124735, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:58.207782: step 4859, loss 0.0665104, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:58.732881: step 4860, loss 0.0748485, acc 1, learning_rate 0.0001
2017-10-10T13:24:59.204887: step 4861, loss 0.164636, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:59.675032: step 4862, loss 0.105328, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:00.157330: step 4863, loss 0.0424457, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:00.635282: step 4864, loss 0.11051, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:01.091097: step 4865, loss 0.126991, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:01.719145: step 4866, loss 0.127321, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:02.220337: step 4867, loss 0.197908, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:02.751514: step 4868, loss 0.0534777, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:03.252897: step 4869, loss 0.072642, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:03.806110: step 4870, loss 0.118784, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:04.337995: step 4871, loss 0.169772, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:04.915928: step 4872, loss 0.0866178, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:05.433309: step 4873, loss 0.0544744, acc 1, learning_rate 0.0001
2017-10-10T13:25:05.941792: step 4874, loss 0.146869, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:06.474840: step 4875, loss 0.127088, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:07.053122: step 4876, loss 0.287448, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:07.724999: step 4877, loss 0.0483147, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:08.302540: step 4878, loss 0.0652977, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:08.656873: step 4879, loss 0.11744, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:09.058856: step 4880, loss 0.0922116, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:10.395926: step 4880, loss 0.215328, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4880

2017-10-10T13:25:12.204960: step 4881, loss 0.0739181, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:12.773189: step 4882, loss 0.130447, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:13.393103: step 4883, loss 0.112521, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:13.832851: step 4884, loss 0.112367, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:14.284296: step 4885, loss 0.0956685, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:14.717203: step 4886, loss 0.117686, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:15.285369: step 4887, loss 0.209894, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:15.888551: step 4888, loss 0.0567626, acc 1, learning_rate 0.0001
2017-10-10T13:25:16.427399: step 4889, loss 0.194476, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:16.969229: step 4890, loss 0.10491, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:17.484384: step 4891, loss 0.069153, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:18.033041: step 4892, loss 0.110177, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:18.596944: step 4893, loss 0.100806, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:19.085680: step 4894, loss 0.112073, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:19.564896: step 4895, loss 0.147872, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:20.111060: step 4896, loss 0.0779756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:20.504840: step 4897, loss 0.12692, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:20.935901: step 4898, loss 0.0476547, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:21.375508: step 4899, loss 0.117362, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:21.881587: step 4900, loss 0.110579, acc 0.960784, learning_rate 0.0001
2017-10-10T13:25:22.384875: step 4901, loss 0.161323, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:22.912989: step 4902, loss 0.0991065, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:23.346896: step 4903, loss 0.0788564, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:23.770337: step 4904, loss 0.0353613, acc 1, learning_rate 0.0001
2017-10-10T13:25:24.330099: step 4905, loss 0.0828933, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:24.816917: step 4906, loss 0.0983412, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:25.330430: step 4907, loss 0.148814, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:25.894469: step 4908, loss 0.0703309, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:26.415431: step 4909, loss 0.168043, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:26.955048: step 4910, loss 0.121622, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:27.503632: step 4911, loss 0.0878763, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:28.068913: step 4912, loss 0.0810087, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:28.636973: step 4913, loss 0.136089, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:29.202054: step 4914, loss 0.0461691, acc 1, learning_rate 0.0001
2017-10-10T13:25:29.763662: step 4915, loss 0.111064, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:30.319755: step 4916, loss 0.118981, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:30.871494: step 4917, loss 0.169436, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:31.416905: step 4918, loss 0.0448176, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:31.954574: step 4919, loss 0.0761487, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:32.406057: step 4920, loss 0.0809543, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:33.648980: step 4920, loss 0.208388, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4920

2017-10-10T13:25:35.112973: step 4921, loss 0.0792366, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:35.640969: step 4922, loss 0.217259, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:36.243850: step 4923, loss 0.168032, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:36.711516: step 4924, loss 0.0949599, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:37.184181: step 4925, loss 0.130839, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:37.657644: step 4926, loss 0.0769177, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:38.210234: step 4927, loss 0.126444, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:38.758873: step 4928, loss 0.133169, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:39.305131: step 4929, loss 0.217686, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:39.864258: step 4930, loss 0.1111, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:40.411279: step 4931, loss 0.220072, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:40.961045: step 4932, loss 0.0990194, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:41.520051: step 4933, loss 0.0756108, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:42.028163: step 4934, loss 0.101183, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:42.603516: step 4935, loss 0.238937, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:43.147861: step 4936, loss 0.0536985, acc 1, learning_rate 0.0001
2017-10-10T13:25:43.757410: step 4937, loss 0.109496, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:44.210254: step 4938, loss 0.0887481, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:44.702923: step 4939, loss 0.0971859, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:45.188986: step 4940, loss 0.126374, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:45.684936: step 4941, loss 0.108159, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:46.140730: step 4942, loss 0.132767, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:46.649082: step 4943, loss 0.139991, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:47.188966: step 4944, loss 0.192447, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:47.758241: step 4945, loss 0.24615, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:48.233475: step 4946, loss 0.140946, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:48.711294: step 4947, loss 0.157537, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:49.168873: step 4948, loss 0.122959, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:49.648286: step 4949, loss 0.0452859, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:50.163968: step 4950, loss 0.0704598, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:50.689078: step 4951, loss 0.140846, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:51.286972: step 4952, loss 0.0979253, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:51.823756: step 4953, loss 0.0597627, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:52.377837: step 4954, loss 0.0895422, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:52.915136: step 4955, loss 0.0805718, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:53.456987: step 4956, loss 0.159709, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:53.960900: step 4957, loss 0.0701421, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:54.401145: step 4958, loss 0.138279, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:54.985003: step 4959, loss 0.138826, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:55.542100: step 4960, loss 0.101382, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:56.781455: step 4960, loss 0.20804, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-4960

2017-10-10T13:25:58.416921: step 4961, loss 0.0896717, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:59.008842: step 4962, loss 0.196835, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:59.583984: step 4963, loss 0.103504, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:00.132878: step 4964, loss 0.0372956, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:00.596706: step 4965, loss 0.0779443, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:01.084915: step 4966, loss 0.0764871, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:01.604627: step 4967, loss 0.0627416, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:02.080884: step 4968, loss 0.0950293, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:02.551825: step 4969, loss 0.0964338, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:03.060870: step 4970, loss 0.137118, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:03.544894: step 4971, loss 0.150598, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:04.128219: step 4972, loss 0.0914312, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:04.690705: step 4973, loss 0.0492517, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:05.176104: step 4974, loss 0.149283, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:05.660822: step 4975, loss 0.188838, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:06.224930: step 4976, loss 0.07776, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:06.721746: step 4977, loss 0.101478, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:07.287857: step 4978, loss 0.128212, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:07.735296: step 4979, loss 0.134727, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:08.192096: step 4980, loss 0.160857, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:08.761530: step 4981, loss 0.0865303, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:09.326451: step 4982, loss 0.106784, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:09.888879: step 4983, loss 0.0897603, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:10.418665: step 4984, loss 0.105351, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:10.964847: step 4985, loss 0.228107, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:11.452905: step 4986, loss 0.133822, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:11.956998: step 4987, loss 0.0587864, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:12.460992: step 4988, loss 0.11644, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:13.052899: step 4989, loss 0.140421, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:13.593135: step 4990, loss 0.215035, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:14.096940: step 4991, loss 0.0648956, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:14.598144: step 4992, loss 0.161759, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:15.089851: step 4993, loss 0.0629383, acc 1, learning_rate 0.0001
2017-10-10T13:26:15.570045: step 4994, loss 0.172542, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:16.076860: step 4995, loss 0.174384, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:16.628137: step 4996, loss 0.141721, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:17.260880: step 4997, loss 0.0992578, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:17.785235: step 4998, loss 0.105355, acc 0.960784, learning_rate 0.0001
2017-10-10T13:26:18.282745: step 4999, loss 0.109389, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:18.751452: step 5000, loss 0.104147, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:20.112894: step 5000, loss 0.209695, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5000

2017-10-10T13:26:21.784862: step 5001, loss 0.0955824, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:22.325385: step 5002, loss 0.106081, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:22.806422: step 5003, loss 0.0995608, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:23.237237: step 5004, loss 0.107124, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:23.616843: step 5005, loss 0.0860703, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:24.160832: step 5006, loss 0.084068, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:24.688802: step 5007, loss 0.100663, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:25.246928: step 5008, loss 0.118214, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:25.760549: step 5009, loss 0.0909142, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:26.297250: step 5010, loss 0.111996, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:26.857193: step 5011, loss 0.0800298, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:27.353020: step 5012, loss 0.0841813, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:27.905252: step 5013, loss 0.0608899, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:28.406671: step 5014, loss 0.122859, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:28.901040: step 5015, loss 0.133062, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:29.388216: step 5016, loss 0.0701638, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:29.898499: step 5017, loss 0.150141, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:30.433403: step 5018, loss 0.145641, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:30.893506: step 5019, loss 0.100638, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:31.323240: step 5020, loss 0.163125, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:31.787041: step 5021, loss 0.156705, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:32.296880: step 5022, loss 0.0835986, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:32.924988: step 5023, loss 0.0720648, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:33.513320: step 5024, loss 0.0362896, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:34.022794: step 5025, loss 0.110907, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:34.513060: step 5026, loss 0.187178, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:35.025224: step 5027, loss 0.219562, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:35.587898: step 5028, loss 0.12947, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:36.109101: step 5029, loss 0.141344, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:36.630667: step 5030, loss 0.0704412, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:37.222433: step 5031, loss 0.100527, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:37.769393: step 5032, loss 0.0635317, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:38.341678: step 5033, loss 0.126996, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:38.891580: step 5034, loss 0.133482, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:39.456603: step 5035, loss 0.0978394, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:39.968882: step 5036, loss 0.0813914, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:40.552082: step 5037, loss 0.0524648, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:41.146344: step 5038, loss 0.11836, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:41.593259: step 5039, loss 0.0857653, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:42.055345: step 5040, loss 0.135583, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:43.288828: step 5040, loss 0.210775, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5040

2017-10-10T13:26:44.764904: step 5041, loss 0.140756, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:45.357459: step 5042, loss 0.122754, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:45.819044: step 5043, loss 0.0793138, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:46.234132: step 5044, loss 0.135124, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:46.728824: step 5045, loss 0.181508, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:47.243791: step 5046, loss 0.0523265, acc 1, learning_rate 0.0001
2017-10-10T13:26:47.720961: step 5047, loss 0.0853878, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:48.221142: step 5048, loss 0.071726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:48.661452: step 5049, loss 0.131144, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:49.139292: step 5050, loss 0.0926926, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:49.611282: step 5051, loss 0.180251, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:50.200000: step 5052, loss 0.109088, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:50.661163: step 5053, loss 0.0881032, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:51.190356: step 5054, loss 0.0445046, acc 1, learning_rate 0.0001
2017-10-10T13:26:51.737300: step 5055, loss 0.101515, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:52.242734: step 5056, loss 0.0752858, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:52.750469: step 5057, loss 0.109163, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:53.305822: step 5058, loss 0.0550782, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:53.947729: step 5059, loss 0.0794061, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:54.460867: step 5060, loss 0.16127, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:54.894783: step 5061, loss 0.0518352, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:55.320938: step 5062, loss 0.0717348, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:55.856856: step 5063, loss 0.104812, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:56.396908: step 5064, loss 0.191713, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:56.917578: step 5065, loss 0.135047, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:57.425381: step 5066, loss 0.172735, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:57.973235: step 5067, loss 0.106947, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:58.553072: step 5068, loss 0.127501, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:59.144181: step 5069, loss 0.0586725, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:59.626516: step 5070, loss 0.0782529, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:00.090957: step 5071, loss 0.121824, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:00.625154: step 5072, loss 0.115943, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:01.123958: step 5073, loss 0.0967746, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:01.569585: step 5074, loss 0.184377, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:02.054525: step 5075, loss 0.132386, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:02.557811: step 5076, loss 0.0714429, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:03.133015: step 5077, loss 0.160272, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:03.788876: step 5078, loss 0.0558026, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:04.345123: step 5079, loss 0.0515908, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:04.736836: step 5080, loss 0.0910325, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:06.044863: step 5080, loss 0.212724, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5080

2017-10-10T13:27:07.580847: step 5081, loss 0.0793456, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:08.068838: step 5082, loss 0.105134, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:08.713276: step 5083, loss 0.150316, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:09.152831: step 5084, loss 0.115842, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:09.610883: step 5085, loss 0.102154, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:10.172979: step 5086, loss 0.108582, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:10.653866: step 5087, loss 0.152485, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:11.160891: step 5088, loss 0.0662477, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:11.644934: step 5089, loss 0.0682814, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:12.194922: step 5090, loss 0.27098, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:12.668961: step 5091, loss 0.084269, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:13.149128: step 5092, loss 0.107638, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:13.608863: step 5093, loss 0.132572, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:14.108884: step 5094, loss 0.151277, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:14.672789: step 5095, loss 0.177867, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:15.108908: step 5096, loss 0.0788911, acc 0.980392, learning_rate 0.0001
2017-10-10T13:27:15.671219: step 5097, loss 0.0576029, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:16.261790: step 5098, loss 0.168956, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:16.815110: step 5099, loss 0.107197, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:17.336910: step 5100, loss 0.0893679, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:17.779453: step 5101, loss 0.0290495, acc 1, learning_rate 0.0001
2017-10-10T13:27:18.216955: step 5102, loss 0.108881, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:18.759084: step 5103, loss 0.179821, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:19.289008: step 5104, loss 0.0819814, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:19.684836: step 5105, loss 0.107207, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:20.194430: step 5106, loss 0.146534, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:20.728763: step 5107, loss 0.145655, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:21.243988: step 5108, loss 0.147982, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:21.700532: step 5109, loss 0.181943, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:22.251413: step 5110, loss 0.137692, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:22.699510: step 5111, loss 0.260905, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:23.258125: step 5112, loss 0.058195, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:23.772961: step 5113, loss 0.179478, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:24.302323: step 5114, loss 0.14826, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:24.834187: step 5115, loss 0.040642, acc 1, learning_rate 0.0001
2017-10-10T13:27:25.350125: step 5116, loss 0.103344, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:25.880915: step 5117, loss 0.0912539, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:26.486577: step 5118, loss 0.156044, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:27.055994: step 5119, loss 0.165341, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:27.529287: step 5120, loss 0.16919, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:28.816906: step 5120, loss 0.208434, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5120

2017-10-10T13:27:30.396117: step 5121, loss 0.0894737, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:30.960892: step 5122, loss 0.1228, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:31.596897: step 5123, loss 0.0727999, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:32.012973: step 5124, loss 0.0586937, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:32.489041: step 5125, loss 0.156816, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:33.017006: step 5126, loss 0.259749, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:33.620821: step 5127, loss 0.108145, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:34.178282: step 5128, loss 0.0971731, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:34.739985: step 5129, loss 0.147745, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:35.293187: step 5130, loss 0.0578392, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:35.804892: step 5131, loss 0.0742273, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:36.336872: step 5132, loss 0.240329, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:36.858368: step 5133, loss 0.123266, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:37.403182: step 5134, loss 0.0615822, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:37.908923: step 5135, loss 0.137801, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:38.436993: step 5136, loss 0.0755682, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:38.850173: step 5137, loss 0.065418, acc 1, learning_rate 0.0001
2017-10-10T13:27:39.293076: step 5138, loss 0.127334, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:39.837859: step 5139, loss 0.0495584, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:40.422324: step 5140, loss 0.101533, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:40.835850: step 5141, loss 0.0424112, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:41.299052: step 5142, loss 0.111316, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:41.727670: step 5143, loss 0.114521, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:42.197052: step 5144, loss 0.134784, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:42.744962: step 5145, loss 0.0663536, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:43.328889: step 5146, loss 0.139832, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:43.888906: step 5147, loss 0.117458, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:44.361808: step 5148, loss 0.0539443, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:44.841086: step 5149, loss 0.0743548, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:45.324936: step 5150, loss 0.0714926, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:45.832893: step 5151, loss 0.0568082, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:46.354813: step 5152, loss 0.0941497, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:46.888842: step 5153, loss 0.140658, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:47.393887: step 5154, loss 0.0889099, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:47.872915: step 5155, loss 0.122831, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:48.389922: step 5156, loss 0.0565688, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:48.892306: step 5157, loss 0.0845876, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:49.336925: step 5158, loss 0.134011, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:50.011208: step 5159, loss 0.0342792, acc 1, learning_rate 0.0001
2017-10-10T13:27:50.336838: step 5160, loss 0.167108, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:51.636845: step 5160, loss 0.211173, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5160

2017-10-10T13:27:53.384963: step 5161, loss 0.0940721, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:53.957159: step 5162, loss 0.0706158, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:54.468389: step 5163, loss 0.0632763, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:54.911145: step 5164, loss 0.0550411, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:55.339557: step 5165, loss 0.163989, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:55.828888: step 5166, loss 0.0605522, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:56.366755: step 5167, loss 0.106412, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:56.917025: step 5168, loss 0.0647973, acc 1, learning_rate 0.0001
2017-10-10T13:27:57.563158: step 5169, loss 0.0813619, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:58.084376: step 5170, loss 0.142753, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:58.600927: step 5171, loss 0.147102, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:59.104955: step 5172, loss 0.0772877, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:59.690371: step 5173, loss 0.176017, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:00.199934: step 5174, loss 0.0872441, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:00.660938: step 5175, loss 0.159092, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:01.164003: step 5176, loss 0.0609819, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:01.579190: step 5177, loss 0.104487, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:02.136999: step 5178, loss 0.0681984, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:02.624988: step 5179, loss 0.167436, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:03.238169: step 5180, loss 0.228998, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:03.805881: step 5181, loss 0.141945, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:04.281096: step 5182, loss 0.0675135, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:04.656855: step 5183, loss 0.171376, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:05.087040: step 5184, loss 0.129032, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:05.500169: step 5185, loss 0.113414, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:06.056702: step 5186, loss 0.202631, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:06.607237: step 5187, loss 0.064842, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:07.165881: step 5188, loss 0.0576194, acc 1, learning_rate 0.0001
2017-10-10T13:28:07.688926: step 5189, loss 0.0818724, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:08.217569: step 5190, loss 0.169242, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:08.734821: step 5191, loss 0.0481479, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:09.267932: step 5192, loss 0.128839, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:09.760415: step 5193, loss 0.0790467, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:10.217475: step 5194, loss 0.0966322, acc 0.960784, learning_rate 0.0001
2017-10-10T13:28:10.707514: step 5195, loss 0.140555, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:11.181188: step 5196, loss 0.16185, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:11.765023: step 5197, loss 0.0917282, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:12.297011: step 5198, loss 0.0795873, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:12.852157: step 5199, loss 0.191708, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:13.341078: step 5200, loss 0.081789, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:14.996994: step 5200, loss 0.207958, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5200

2017-10-10T13:28:16.476898: step 5201, loss 0.0501366, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:17.031307: step 5202, loss 0.0482158, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:17.439374: step 5203, loss 0.124197, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:17.893003: step 5204, loss 0.0747742, acc 1, learning_rate 0.0001
2017-10-10T13:28:18.421665: step 5205, loss 0.0986753, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:18.880861: step 5206, loss 0.123314, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:19.392868: step 5207, loss 0.10303, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:19.930128: step 5208, loss 0.110787, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:20.461673: step 5209, loss 0.130238, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:21.036515: step 5210, loss 0.0911707, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:21.573481: step 5211, loss 0.0784009, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:22.133063: step 5212, loss 0.0609803, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:22.692905: step 5213, loss 0.164922, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:23.280818: step 5214, loss 0.0950722, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:23.804984: step 5215, loss 0.197838, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:24.229861: step 5216, loss 0.142473, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:24.716675: step 5217, loss 0.128689, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:25.085188: step 5218, loss 0.163097, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:25.571251: step 5219, loss 0.121438, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:26.112826: step 5220, loss 0.0752995, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:26.660880: step 5221, loss 0.131605, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:27.168906: step 5222, loss 0.0913142, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:27.716894: step 5223, loss 0.152356, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:28.106502: step 5224, loss 0.0575506, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:28.505151: step 5225, loss 0.170319, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:29.121092: step 5226, loss 0.217834, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:29.660291: step 5227, loss 0.122785, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:30.220846: step 5228, loss 0.0940317, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:30.720997: step 5229, loss 0.118794, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:31.283436: step 5230, loss 0.057642, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:31.815933: step 5231, loss 0.103158, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:32.368595: step 5232, loss 0.0955556, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:32.908837: step 5233, loss 0.0680022, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:33.436828: step 5234, loss 0.148495, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:33.964833: step 5235, loss 0.226086, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:34.460920: step 5236, loss 0.216906, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:34.952855: step 5237, loss 0.0819719, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:35.580913: step 5238, loss 0.0649927, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:36.188824: step 5239, loss 0.147642, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:36.645290: step 5240, loss 0.0802385, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:37.864898: step 5240, loss 0.207769, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5240

2017-10-10T13:28:39.530354: step 5241, loss 0.0714419, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:40.104867: step 5242, loss 0.153876, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:40.653861: step 5243, loss 0.083556, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:41.131430: step 5244, loss 0.0810432, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:41.589667: step 5245, loss 0.209084, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:42.155146: step 5246, loss 0.0682986, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:42.731695: step 5247, loss 0.108479, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:43.316870: step 5248, loss 0.139621, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:43.818822: step 5249, loss 0.0357516, acc 1, learning_rate 0.0001
2017-10-10T13:28:44.376865: step 5250, loss 0.184147, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:44.945029: step 5251, loss 0.104329, acc 1, learning_rate 0.0001
2017-10-10T13:28:45.467157: step 5252, loss 0.0202394, acc 1, learning_rate 0.0001
2017-10-10T13:28:46.016602: step 5253, loss 0.1764, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:46.529109: step 5254, loss 0.123552, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:47.080844: step 5255, loss 0.152068, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:47.607715: step 5256, loss 0.18862, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:48.157701: step 5257, loss 0.0803535, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:48.740835: step 5258, loss 0.0654142, acc 1, learning_rate 0.0001
2017-10-10T13:28:49.269422: step 5259, loss 0.0823454, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:49.836920: step 5260, loss 0.0690937, acc 1, learning_rate 0.0001
2017-10-10T13:28:50.500828: step 5261, loss 0.0743418, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:51.072772: step 5262, loss 0.140612, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:51.505032: step 5263, loss 0.0782242, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:51.957087: step 5264, loss 0.106825, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:52.484346: step 5265, loss 0.0798464, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:53.041682: step 5266, loss 0.0858224, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:53.588942: step 5267, loss 0.0944685, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:54.147121: step 5268, loss 0.0951093, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:54.660844: step 5269, loss 0.0906108, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:55.216908: step 5270, loss 0.191475, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:55.705594: step 5271, loss 0.184901, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:56.139907: step 5272, loss 0.118049, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:56.720889: step 5273, loss 0.154498, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:57.272868: step 5274, loss 0.101788, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:57.769194: step 5275, loss 0.0846224, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:58.245125: step 5276, loss 0.139499, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:58.806032: step 5277, loss 0.121788, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:59.365006: step 5278, loss 0.207118, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:59.899054: step 5279, loss 0.0766138, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:00.353011: step 5280, loss 0.104545, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:01.595915: step 5280, loss 0.213336, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5280

2017-10-10T13:29:03.329322: step 5281, loss 0.0595496, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:03.858874: step 5282, loss 0.100564, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:04.328529: step 5283, loss 0.130387, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:04.749060: step 5284, loss 0.146764, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:05.221957: step 5285, loss 0.0662778, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:05.736844: step 5286, loss 0.135059, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:06.267344: step 5287, loss 0.134591, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:06.816794: step 5288, loss 0.135915, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:07.377222: step 5289, loss 0.120607, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:07.909133: step 5290, loss 0.0683002, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:08.451783: step 5291, loss 0.0378568, acc 1, learning_rate 0.0001
2017-10-10T13:29:08.925638: step 5292, loss 0.188072, acc 0.901961, learning_rate 0.0001
2017-10-10T13:29:09.439834: step 5293, loss 0.270543, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:09.978377: step 5294, loss 0.0348372, acc 1, learning_rate 0.0001
2017-10-10T13:29:10.404938: step 5295, loss 0.0712575, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:10.911460: step 5296, loss 0.0623274, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:11.417039: step 5297, loss 0.0692756, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:11.900957: step 5298, loss 0.15675, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:12.416911: step 5299, loss 0.149355, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:12.846142: step 5300, loss 0.0857459, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:13.304517: step 5301, loss 0.0670278, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:13.892900: step 5302, loss 0.182604, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:14.456989: step 5303, loss 0.172641, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:14.928935: step 5304, loss 0.169422, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:15.367612: step 5305, loss 0.103903, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:15.815048: step 5306, loss 0.0640298, acc 1, learning_rate 0.0001
2017-10-10T13:29:16.279057: step 5307, loss 0.121606, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:16.762379: step 5308, loss 0.0621094, acc 1, learning_rate 0.0001
2017-10-10T13:29:17.368866: step 5309, loss 0.0619254, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:17.933253: step 5310, loss 0.100717, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:18.533073: step 5311, loss 0.0587708, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:19.067534: step 5312, loss 0.0523856, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:19.592856: step 5313, loss 0.110907, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:20.136976: step 5314, loss 0.0924679, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:20.699350: step 5315, loss 0.030761, acc 1, learning_rate 0.0001
2017-10-10T13:29:21.192903: step 5316, loss 0.0566953, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:21.646110: step 5317, loss 0.193744, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:22.116848: step 5318, loss 0.122988, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:22.596931: step 5319, loss 0.125422, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:23.200904: step 5320, loss 0.0849274, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:24.418750: step 5320, loss 0.208743, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5320

2017-10-10T13:29:26.353000: step 5321, loss 0.167499, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:26.973076: step 5322, loss 0.209808, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:27.376820: step 5323, loss 0.116441, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:27.829767: step 5324, loss 0.0450455, acc 1, learning_rate 0.0001
2017-10-10T13:29:28.281323: step 5325, loss 0.125661, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:28.834914: step 5326, loss 0.0879062, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:29.369309: step 5327, loss 0.185241, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:29.922228: step 5328, loss 0.0684676, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:30.479862: step 5329, loss 0.161501, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:30.987416: step 5330, loss 0.127345, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:31.514382: step 5331, loss 0.0223653, acc 1, learning_rate 0.0001
2017-10-10T13:29:32.049056: step 5332, loss 0.0743146, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:32.560015: step 5333, loss 0.103059, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:32.993575: step 5334, loss 0.083627, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:33.529145: step 5335, loss 0.14018, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:34.132871: step 5336, loss 0.126035, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:34.601033: step 5337, loss 0.204916, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:35.180984: step 5338, loss 0.0850767, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:35.690306: step 5339, loss 0.105068, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:36.184928: step 5340, loss 0.133891, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:36.661104: step 5341, loss 0.117627, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:37.032980: step 5342, loss 0.0759272, acc 1, learning_rate 0.0001
2017-10-10T13:29:37.601085: step 5343, loss 0.162172, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:38.182215: step 5344, loss 0.0992295, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:38.683012: step 5345, loss 0.15727, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:39.128896: step 5346, loss 0.172357, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:39.658219: step 5347, loss 0.0667445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:40.208873: step 5348, loss 0.126059, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:40.708704: step 5349, loss 0.0426583, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:41.258337: step 5350, loss 0.164324, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:41.799410: step 5351, loss 0.143295, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:42.313151: step 5352, loss 0.147556, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:42.816934: step 5353, loss 0.0800698, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:43.376552: step 5354, loss 0.133836, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:43.900846: step 5355, loss 0.129571, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:44.461898: step 5356, loss 0.119618, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:44.897008: step 5357, loss 0.0562593, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:45.352975: step 5358, loss 0.102397, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:45.964908: step 5359, loss 0.110169, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:46.607572: step 5360, loss 0.289562, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:47.817948: step 5360, loss 0.206465, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5360

2017-10-10T13:29:49.428873: step 5361, loss 0.112392, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:49.997023: step 5362, loss 0.105004, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:50.566986: step 5363, loss 0.128969, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:50.993090: step 5364, loss 0.0709394, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:51.444986: step 5365, loss 0.145874, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:51.968957: step 5366, loss 0.0612561, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:52.526110: step 5367, loss 0.0585817, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:53.064894: step 5368, loss 0.171886, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:53.614173: step 5369, loss 0.0803692, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:54.194498: step 5370, loss 0.146846, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:54.757960: step 5371, loss 0.0849863, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:55.302503: step 5372, loss 0.191204, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:55.897122: step 5373, loss 0.108681, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:56.397753: step 5374, loss 0.099295, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:56.897164: step 5375, loss 0.150722, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:57.424078: step 5376, loss 0.211123, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:58.016902: step 5377, loss 0.0589365, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:58.599407: step 5378, loss 0.0536525, acc 1, learning_rate 0.0001
2017-10-10T13:29:59.105725: step 5379, loss 0.152971, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:59.653064: step 5380, loss 0.135882, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:00.244857: step 5381, loss 0.227401, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:00.800852: step 5382, loss 0.113864, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:01.360629: step 5383, loss 0.066676, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:01.820830: step 5384, loss 0.160413, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:02.262103: step 5385, loss 0.0708951, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:02.832890: step 5386, loss 0.0646019, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:03.360922: step 5387, loss 0.152532, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:03.903637: step 5388, loss 0.156993, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:04.404863: step 5389, loss 0.0875676, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:04.785188: step 5390, loss 0.079191, acc 0.980392, learning_rate 0.0001
2017-10-10T13:30:05.324951: step 5391, loss 0.151623, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:05.847249: step 5392, loss 0.13502, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:06.381114: step 5393, loss 0.0686322, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:06.918479: step 5394, loss 0.196225, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:07.453850: step 5395, loss 0.129294, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:07.979149: step 5396, loss 0.0889657, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:08.489705: step 5397, loss 0.054955, acc 1, learning_rate 0.0001
2017-10-10T13:30:09.016988: step 5398, loss 0.147845, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:09.486250: step 5399, loss 0.128494, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:09.828833: step 5400, loss 0.183308, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:11.174919: step 5400, loss 0.211041, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5400

2017-10-10T13:30:12.942718: step 5401, loss 0.0731919, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:13.525337: step 5402, loss 0.118053, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:13.926422: step 5403, loss 0.170496, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:14.365847: step 5404, loss 0.134841, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:14.877170: step 5405, loss 0.133521, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:15.465228: step 5406, loss 0.0281368, acc 1, learning_rate 0.0001
2017-10-10T13:30:15.947852: step 5407, loss 0.0962085, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:16.490095: step 5408, loss 0.100154, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:16.982905: step 5409, loss 0.0292138, acc 1, learning_rate 0.0001
2017-10-10T13:30:17.488402: step 5410, loss 0.0996766, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:18.021030: step 5411, loss 0.153429, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:18.520349: step 5412, loss 0.101737, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:19.056813: step 5413, loss 0.152105, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:19.576695: step 5414, loss 0.122621, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:20.124938: step 5415, loss 0.0851043, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:20.656866: step 5416, loss 0.055338, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:21.171475: step 5417, loss 0.126557, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:21.624951: step 5418, loss 0.160443, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:22.176333: step 5419, loss 0.0955597, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:22.671957: step 5420, loss 0.0839303, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:23.165862: step 5421, loss 0.0490595, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:23.589447: step 5422, loss 0.0976955, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:24.063571: step 5423, loss 0.0807318, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:24.626801: step 5424, loss 0.0356288, acc 1, learning_rate 0.0001
2017-10-10T13:30:25.072602: step 5425, loss 0.108058, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:25.500820: step 5426, loss 0.0487353, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:25.965892: step 5427, loss 0.102397, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:26.516740: step 5428, loss 0.10358, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:27.061063: step 5429, loss 0.177899, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:27.656866: step 5430, loss 0.0698135, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:28.176821: step 5431, loss 0.21678, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:28.720827: step 5432, loss 0.11909, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:29.276830: step 5433, loss 0.0711388, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:29.813490: step 5434, loss 0.0856092, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:30.406678: step 5435, loss 0.0795852, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:30.962290: step 5436, loss 0.0605097, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:31.488780: step 5437, loss 0.109978, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:32.049048: step 5438, loss 0.069031, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:32.592324: step 5439, loss 0.0531002, acc 1, learning_rate 0.0001
2017-10-10T13:30:33.077142: step 5440, loss 0.153628, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:34.420443: step 5440, loss 0.207625, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5440

2017-10-10T13:30:35.825107: step 5441, loss 0.0581863, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:36.371938: step 5442, loss 0.0487763, acc 1, learning_rate 0.0001
2017-10-10T13:30:36.871890: step 5443, loss 0.0768339, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:37.256961: step 5444, loss 0.189542, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:37.700852: step 5445, loss 0.123483, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:38.166015: step 5446, loss 0.179541, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:38.637079: step 5447, loss 0.170555, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:39.181472: step 5448, loss 0.0924196, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:39.585079: step 5449, loss 0.0719227, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:40.082300: step 5450, loss 0.078203, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:40.628180: step 5451, loss 0.149642, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:41.172466: step 5452, loss 0.134596, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:41.702044: step 5453, loss 0.0895928, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:42.254679: step 5454, loss 0.170682, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:42.800625: step 5455, loss 0.0911221, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:43.424412: step 5456, loss 0.133008, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:43.960885: step 5457, loss 0.309368, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:44.487554: step 5458, loss 0.0976149, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:45.036876: step 5459, loss 0.0446587, acc 1, learning_rate 0.0001
2017-10-10T13:30:45.554851: step 5460, loss 0.191435, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:46.098823: step 5461, loss 0.124061, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:46.645239: step 5462, loss 0.121326, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:47.256861: step 5463, loss 0.12656, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:47.906462: step 5464, loss 0.0388393, acc 1, learning_rate 0.0001
2017-10-10T13:30:48.344981: step 5465, loss 0.115948, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:48.815860: step 5466, loss 0.0724368, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:49.349216: step 5467, loss 0.0651444, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:49.896339: step 5468, loss 0.0405628, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:50.409024: step 5469, loss 0.107961, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:50.945037: step 5470, loss 0.134921, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:51.540276: step 5471, loss 0.108735, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:52.092235: step 5472, loss 0.0463901, acc 1, learning_rate 0.0001
2017-10-10T13:30:52.648755: step 5473, loss 0.134025, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:53.167331: step 5474, loss 0.0868633, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:53.644934: step 5475, loss 0.134938, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:54.200906: step 5476, loss 0.176687, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:54.680951: step 5477, loss 0.0559571, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:55.287235: step 5478, loss 0.0708147, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:55.721057: step 5479, loss 0.127391, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:56.284980: step 5480, loss 0.109395, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:57.613015: step 5480, loss 0.206378, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5480

2017-10-10T13:30:59.132840: step 5481, loss 0.0597281, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:59.637030: step 5482, loss 0.0925555, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:00.228994: step 5483, loss 0.102033, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:00.642951: step 5484, loss 0.122332, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:01.060801: step 5485, loss 0.0934313, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:01.605087: step 5486, loss 0.0695706, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:02.069176: step 5487, loss 0.136309, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:02.553012: step 5488, loss 0.199627, acc 0.960784, learning_rate 0.0001
2017-10-10T13:31:03.459203: step 5489, loss 0.101458, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:04.064869: step 5490, loss 0.176926, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:04.564847: step 5491, loss 0.13797, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:05.161111: step 5492, loss 0.0511481, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:05.702432: step 5493, loss 0.182421, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:06.207143: step 5494, loss 0.0471189, acc 1, learning_rate 0.0001
2017-10-10T13:31:06.730876: step 5495, loss 0.161268, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:07.296521: step 5496, loss 0.136613, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:07.770731: step 5497, loss 0.0994111, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:08.280816: step 5498, loss 0.192676, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:08.841358: step 5499, loss 0.0979652, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:09.345122: step 5500, loss 0.0803912, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:09.878407: step 5501, loss 0.183055, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:10.480051: step 5502, loss 0.069438, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:11.012881: step 5503, loss 0.111706, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:11.461303: step 5504, loss 0.201724, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:11.924843: step 5505, loss 0.0734688, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:12.461090: step 5506, loss 0.11931, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:13.008880: step 5507, loss 0.116286, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:13.497091: step 5508, loss 0.0623539, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:14.096860: step 5509, loss 0.0579092, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:14.632971: step 5510, loss 0.102627, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:15.162149: step 5511, loss 0.261328, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:15.731054: step 5512, loss 0.074376, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:16.276850: step 5513, loss 0.0331568, acc 1, learning_rate 0.0001
2017-10-10T13:31:16.761079: step 5514, loss 0.0792478, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:17.363634: step 5515, loss 0.166516, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:17.852890: step 5516, loss 0.195405, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:18.281154: step 5517, loss 0.129286, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:18.796975: step 5518, loss 0.105761, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:19.380904: step 5519, loss 0.0854275, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:19.884940: step 5520, loss 0.125329, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:21.111552: step 5520, loss 0.20593, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5520

2017-10-10T13:31:22.776467: step 5521, loss 0.114997, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:23.372878: step 5522, loss 0.128815, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:23.788931: step 5523, loss 0.0714396, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:24.209453: step 5524, loss 0.0464557, acc 1, learning_rate 0.0001
2017-10-10T13:31:24.792882: step 5525, loss 0.0792229, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:25.385068: step 5526, loss 0.0983193, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:25.893169: step 5527, loss 0.0954298, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:26.401049: step 5528, loss 0.207769, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:26.848904: step 5529, loss 0.105861, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:27.412181: step 5530, loss 0.118171, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:27.916726: step 5531, loss 0.0953088, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:28.540937: step 5532, loss 0.0755763, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:29.020367: step 5533, loss 0.126678, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:29.563632: step 5534, loss 0.0646036, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:30.113568: step 5535, loss 0.152373, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:30.644959: step 5536, loss 0.0831395, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:31.241157: step 5537, loss 0.0758409, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:31.748303: step 5538, loss 0.0735198, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:32.289849: step 5539, loss 0.0824786, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:32.764990: step 5540, loss 0.0892129, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:33.401366: step 5541, loss 0.0569793, acc 1, learning_rate 0.0001
2017-10-10T13:31:33.816989: step 5542, loss 0.0569631, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:34.285039: step 5543, loss 0.1384, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:34.718631: step 5544, loss 0.0667079, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:35.285830: step 5545, loss 0.0915364, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:35.741101: step 5546, loss 0.126029, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:36.240944: step 5547, loss 0.0736709, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:36.783918: step 5548, loss 0.0474337, acc 1, learning_rate 0.0001
2017-10-10T13:31:37.302680: step 5549, loss 0.169559, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:37.840892: step 5550, loss 0.148567, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:38.388892: step 5551, loss 0.19647, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:38.905529: step 5552, loss 0.263744, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:39.360844: step 5553, loss 0.10114, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:39.828832: step 5554, loss 0.112027, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:40.349235: step 5555, loss 0.0854183, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:40.856675: step 5556, loss 0.109376, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:41.309593: step 5557, loss 0.114794, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:41.833105: step 5558, loss 0.0548029, acc 1, learning_rate 0.0001
2017-10-10T13:31:42.401252: step 5559, loss 0.19233, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:42.970991: step 5560, loss 0.0636606, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:44.320847: step 5560, loss 0.207332, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5560

2017-10-10T13:31:46.013002: step 5561, loss 0.0829407, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:46.431179: step 5562, loss 0.244212, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:46.809357: step 5563, loss 0.117076, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:47.211648: step 5564, loss 0.0736024, acc 1, learning_rate 0.0001
2017-10-10T13:31:47.686391: step 5565, loss 0.136982, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:48.233553: step 5566, loss 0.102934, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:48.813108: step 5567, loss 0.18603, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:49.306393: step 5568, loss 0.0524372, acc 1, learning_rate 0.0001
2017-10-10T13:31:49.792490: step 5569, loss 0.145891, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:50.286656: step 5570, loss 0.0677107, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:50.852293: step 5571, loss 0.118917, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:51.384625: step 5572, loss 0.0822449, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:51.913169: step 5573, loss 0.129392, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:52.491264: step 5574, loss 0.124026, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:53.024898: step 5575, loss 0.112069, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:53.556009: step 5576, loss 0.0665634, acc 1, learning_rate 0.0001
2017-10-10T13:31:54.071924: step 5577, loss 0.10105, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:54.645269: step 5578, loss 0.176995, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:55.106639: step 5579, loss 0.102833, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:55.629188: step 5580, loss 0.104748, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:56.205129: step 5581, loss 0.166742, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:56.789313: step 5582, loss 0.0832148, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:57.304854: step 5583, loss 0.0483571, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:57.760379: step 5584, loss 0.0582415, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:58.228967: step 5585, loss 0.0572244, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:58.728212: step 5586, loss 0.174286, acc 0.941176, learning_rate 0.0001
2017-10-10T13:31:59.264387: step 5587, loss 0.0959389, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:59.817756: step 5588, loss 0.254254, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:00.376587: step 5589, loss 0.0920352, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:00.938924: step 5590, loss 0.114972, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:01.494314: step 5591, loss 0.112812, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:02.032833: step 5592, loss 0.0629984, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:02.536931: step 5593, loss 0.142565, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:03.043986: step 5594, loss 0.0837325, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:03.567264: step 5595, loss 0.130869, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:03.969663: step 5596, loss 0.0593532, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:04.464092: step 5597, loss 0.174114, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:04.992981: step 5598, loss 0.165414, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:05.541238: step 5599, loss 0.0808311, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:06.019103: step 5600, loss 0.123233, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:07.431922: step 5600, loss 0.207135, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5600

2017-10-10T13:32:08.893049: step 5601, loss 0.0785801, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:09.285276: step 5602, loss 0.20625, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:09.736912: step 5603, loss 0.118536, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:10.349936: step 5604, loss 0.0618974, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:10.841183: step 5605, loss 0.134324, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:11.388972: step 5606, loss 0.224729, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:11.928919: step 5607, loss 0.071434, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:12.429029: step 5608, loss 0.131835, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:12.968744: step 5609, loss 0.140335, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:13.484766: step 5610, loss 0.166037, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:13.995948: step 5611, loss 0.0975636, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:14.512973: step 5612, loss 0.104521, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:15.003988: step 5613, loss 0.156442, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:15.488880: step 5614, loss 0.0854327, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:16.050962: step 5615, loss 0.0355237, acc 1, learning_rate 0.0001
2017-10-10T13:32:16.568856: step 5616, loss 0.0894737, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:17.109798: step 5617, loss 0.17028, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:17.659768: step 5618, loss 0.195988, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:18.200828: step 5619, loss 0.0973986, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:18.702584: step 5620, loss 0.118477, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:19.189900: step 5621, loss 0.139527, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:19.809248: step 5622, loss 0.0566567, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:20.173775: step 5623, loss 0.17694, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:20.595518: step 5624, loss 0.36808, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:21.050164: step 5625, loss 0.0664211, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:21.628849: step 5626, loss 0.138021, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:22.211594: step 5627, loss 0.109348, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:22.718399: step 5628, loss 0.126438, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:23.189187: step 5629, loss 0.120389, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:23.704958: step 5630, loss 0.0455098, acc 1, learning_rate 0.0001
2017-10-10T13:32:24.200970: step 5631, loss 0.133465, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:24.709433: step 5632, loss 0.0743935, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:25.208892: step 5633, loss 0.0359857, acc 1, learning_rate 0.0001
2017-10-10T13:32:25.776259: step 5634, loss 0.0595551, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:26.278397: step 5635, loss 0.122021, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:26.817121: step 5636, loss 0.0652046, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:27.295614: step 5637, loss 0.0781675, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:27.838647: step 5638, loss 0.142258, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:28.396920: step 5639, loss 0.0743852, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:28.928975: step 5640, loss 0.217015, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:30.731661: step 5640, loss 0.207625, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5640

2017-10-10T13:32:32.224960: step 5641, loss 0.189286, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:32.682998: step 5642, loss 0.020763, acc 1, learning_rate 0.0001
2017-10-10T13:32:32.983070: step 5643, loss 0.0977298, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:33.573072: step 5644, loss 0.147147, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:33.885324: step 5645, loss 0.0599776, acc 1, learning_rate 0.0001
2017-10-10T13:32:34.369199: step 5646, loss 0.102686, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:34.872841: step 5647, loss 0.169833, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:35.416364: step 5648, loss 0.106624, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:35.964049: step 5649, loss 0.151852, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:36.508818: step 5650, loss 0.0987248, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:37.069894: step 5651, loss 0.0903354, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:37.612741: step 5652, loss 0.049098, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:38.131679: step 5653, loss 0.10135, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:38.683591: step 5654, loss 0.10931, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:39.228861: step 5655, loss 0.0714152, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:39.741004: step 5656, loss 0.0844711, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:40.364506: step 5657, loss 0.126524, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:40.975271: step 5658, loss 0.144523, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:41.434427: step 5659, loss 0.147849, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:41.973084: step 5660, loss 0.123158, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:42.525539: step 5661, loss 0.088343, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:43.033156: step 5662, loss 0.0733899, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:43.521835: step 5663, loss 0.131043, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:44.012525: step 5664, loss 0.0703572, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:44.584881: step 5665, loss 0.0759271, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:45.083840: step 5666, loss 0.131947, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:45.607969: step 5667, loss 0.0917109, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:46.161458: step 5668, loss 0.0783319, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:46.753085: step 5669, loss 0.10192, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:47.292777: step 5670, loss 0.115007, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:47.809375: step 5671, loss 0.104804, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:48.304976: step 5672, loss 0.199218, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:48.764594: step 5673, loss 0.0936291, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:49.220984: step 5674, loss 0.0921163, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:49.711692: step 5675, loss 0.0670955, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:50.170142: step 5676, loss 0.0935022, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:50.746661: step 5677, loss 0.17352, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:51.227999: step 5678, loss 0.0389103, acc 1, learning_rate 0.0001
2017-10-10T13:32:51.792973: step 5679, loss 0.0609078, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:52.326054: step 5680, loss 0.284104, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:53.516990: step 5680, loss 0.203723, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5680

2017-10-10T13:32:55.393159: step 5681, loss 0.0292738, acc 1, learning_rate 0.0001
2017-10-10T13:32:55.848043: step 5682, loss 0.0360785, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:56.200914: step 5683, loss 0.0970006, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:56.536831: step 5684, loss 0.0459377, acc 1, learning_rate 0.0001
2017-10-10T13:32:57.160776: step 5685, loss 0.185701, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:57.713215: step 5686, loss 0.162348, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:58.232902: step 5687, loss 0.0954419, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:58.668856: step 5688, loss 0.0848302, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:59.212856: step 5689, loss 0.170653, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:59.773199: step 5690, loss 0.0833152, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:00.281709: step 5691, loss 0.0495469, acc 1, learning_rate 0.0001
2017-10-10T13:33:00.828912: step 5692, loss 0.0654135, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:01.420168: step 5693, loss 0.140589, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:01.946773: step 5694, loss 0.0547804, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:02.484958: step 5695, loss 0.0547569, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:03.033228: step 5696, loss 0.0468921, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:03.533139: step 5697, loss 0.230928, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:03.909038: step 5698, loss 0.101953, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:04.304915: step 5699, loss 0.19446, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:04.847697: step 5700, loss 0.0759196, acc 1, learning_rate 0.0001
2017-10-10T13:33:05.362543: step 5701, loss 0.207252, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:05.904724: step 5702, loss 0.173196, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:06.450866: step 5703, loss 0.178384, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:06.990143: step 5704, loss 0.0876418, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:07.641043: step 5705, loss 0.124072, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:08.145603: step 5706, loss 0.0916568, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:08.632240: step 5707, loss 0.129307, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:09.141132: step 5708, loss 0.0547133, acc 1, learning_rate 0.0001
2017-10-10T13:33:09.687273: step 5709, loss 0.205546, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:10.193253: step 5710, loss 0.112461, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:10.721002: step 5711, loss 0.073179, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:11.172841: step 5712, loss 0.130547, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:11.704845: step 5713, loss 0.128245, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:12.249031: step 5714, loss 0.111126, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:12.809369: step 5715, loss 0.0672089, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:13.317603: step 5716, loss 0.10122, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:13.866800: step 5717, loss 0.0852872, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:14.440771: step 5718, loss 0.0848735, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:14.940983: step 5719, loss 0.0670548, acc 1, learning_rate 0.0001
2017-10-10T13:33:15.447267: step 5720, loss 0.0857363, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:16.637687: step 5720, loss 0.206054, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5720

2017-10-10T13:33:18.040912: step 5721, loss 0.0411655, acc 1, learning_rate 0.0001
2017-10-10T13:33:18.487056: step 5722, loss 0.0673461, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:18.901348: step 5723, loss 0.0828968, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:19.304905: step 5724, loss 0.184555, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:19.813300: step 5725, loss 0.162522, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:20.491905: step 5726, loss 0.193132, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:20.987610: step 5727, loss 0.0907904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:21.379282: step 5728, loss 0.0431096, acc 1, learning_rate 0.0001
2017-10-10T13:33:21.835563: step 5729, loss 0.0678125, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:22.393002: step 5730, loss 0.127333, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:22.894484: step 5731, loss 0.139044, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:23.417478: step 5732, loss 0.0629685, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:23.944939: step 5733, loss 0.0759898, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:24.471908: step 5734, loss 0.0343273, acc 1, learning_rate 0.0001
2017-10-10T13:33:25.032055: step 5735, loss 0.131299, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:25.553776: step 5736, loss 0.0729474, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:26.119516: step 5737, loss 0.0510594, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:26.620561: step 5738, loss 0.0434731, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:27.047659: step 5739, loss 0.116591, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:27.524853: step 5740, loss 0.214441, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:27.993407: step 5741, loss 0.0737951, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:28.526218: step 5742, loss 0.175024, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:29.050478: step 5743, loss 0.0790096, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:29.575257: step 5744, loss 0.174662, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:30.168908: step 5745, loss 0.0942339, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:30.728853: step 5746, loss 0.0326452, acc 1, learning_rate 0.0001
2017-10-10T13:33:31.269303: step 5747, loss 0.186876, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:31.796436: step 5748, loss 0.159904, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:32.356693: step 5749, loss 0.0822969, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:32.910623: step 5750, loss 0.0662028, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:33.472226: step 5751, loss 0.160037, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:34.067079: step 5752, loss 0.114796, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:34.581051: step 5753, loss 0.0447534, acc 1, learning_rate 0.0001
2017-10-10T13:33:35.131821: step 5754, loss 0.0678428, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:35.740812: step 5755, loss 0.137722, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:36.311197: step 5756, loss 0.0737628, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:36.822110: step 5757, loss 0.077926, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:37.352855: step 5758, loss 0.0716302, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:37.882098: step 5759, loss 0.1498, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:38.408058: step 5760, loss 0.0763633, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:39.389082: step 5760, loss 0.202057, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5760

2017-10-10T13:33:40.952930: step 5761, loss 0.076136, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:41.523039: step 5762, loss 0.0706557, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:42.078954: step 5763, loss 0.191018, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:42.400387: step 5764, loss 0.180588, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:42.893020: step 5765, loss 0.137503, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:43.533785: step 5766, loss 0.158335, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:44.130793: step 5767, loss 0.130518, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:44.631296: step 5768, loss 0.225242, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:45.002564: step 5769, loss 0.0365527, acc 1, learning_rate 0.0001
2017-10-10T13:33:45.504830: step 5770, loss 0.174351, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:46.032870: step 5771, loss 0.0785619, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:46.541051: step 5772, loss 0.0997683, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:47.120877: step 5773, loss 0.0747979, acc 1, learning_rate 0.0001
2017-10-10T13:33:47.647714: step 5774, loss 0.087584, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:48.203208: step 5775, loss 0.0285664, acc 1, learning_rate 0.0001
2017-10-10T13:33:48.748858: step 5776, loss 0.0696984, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:49.377236: step 5777, loss 0.120628, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:49.897294: step 5778, loss 0.110237, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:50.345675: step 5779, loss 0.0318917, acc 1, learning_rate 0.0001
2017-10-10T13:33:50.770393: step 5780, loss 0.0653083, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:51.299593: step 5781, loss 0.110142, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:51.809817: step 5782, loss 0.0307916, acc 1, learning_rate 0.0001
2017-10-10T13:33:52.324866: step 5783, loss 0.154093, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:52.755497: step 5784, loss 0.127465, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:53.269042: step 5785, loss 0.113025, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:53.753335: step 5786, loss 0.123903, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:54.288953: step 5787, loss 0.0398872, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:54.780180: step 5788, loss 0.117259, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:55.232848: step 5789, loss 0.173481, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:55.793659: step 5790, loss 0.168807, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:56.329062: step 5791, loss 0.128504, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:56.885966: step 5792, loss 0.101001, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:57.408986: step 5793, loss 0.0970165, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:57.889193: step 5794, loss 0.0913484, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:58.371532: step 5795, loss 0.0664359, acc 1, learning_rate 0.0001
2017-10-10T13:33:58.896884: step 5796, loss 0.0791087, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:59.428107: step 5797, loss 0.14638, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:59.965411: step 5798, loss 0.080341, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:00.479446: step 5799, loss 0.0920775, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:01.053778: step 5800, loss 0.0751846, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:02.183397: step 5800, loss 0.205411, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5800

2017-10-10T13:34:03.904584: step 5801, loss 0.156037, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:04.477112: step 5802, loss 0.0927783, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:04.886498: step 5803, loss 0.124917, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:05.298875: step 5804, loss 0.0315709, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:05.859059: step 5805, loss 0.0981462, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:06.430209: step 5806, loss 0.108174, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:06.919905: step 5807, loss 0.140362, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:07.310855: step 5808, loss 0.0897417, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:07.728961: step 5809, loss 0.147668, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:08.148121: step 5810, loss 0.165245, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:08.676349: step 5811, loss 0.128932, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:09.281085: step 5812, loss 0.0546196, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:09.874121: step 5813, loss 0.0508406, acc 1, learning_rate 0.0001
2017-10-10T13:34:10.353041: step 5814, loss 0.113983, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:10.858563: step 5815, loss 0.0995221, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:11.337871: step 5816, loss 0.0895414, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:11.784090: step 5817, loss 0.0737891, acc 1, learning_rate 0.0001
2017-10-10T13:34:12.452463: step 5818, loss 0.171962, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:13.041096: step 5819, loss 0.133796, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:13.492806: step 5820, loss 0.15693, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:13.947764: step 5821, loss 0.0688505, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:14.475621: step 5822, loss 0.130364, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:14.984904: step 5823, loss 0.10451, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:15.527636: step 5824, loss 0.10686, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:16.059653: step 5825, loss 0.116714, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:16.607813: step 5826, loss 0.0409021, acc 1, learning_rate 0.0001
2017-10-10T13:34:17.161195: step 5827, loss 0.0607572, acc 1, learning_rate 0.0001
2017-10-10T13:34:17.689154: step 5828, loss 0.0914297, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:18.240140: step 5829, loss 0.0691244, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:18.748041: step 5830, loss 0.0635275, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:19.244979: step 5831, loss 0.0868258, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:19.719079: step 5832, loss 0.108782, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:20.255594: step 5833, loss 0.132874, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:20.693486: step 5834, loss 0.165278, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:21.159911: step 5835, loss 0.0737736, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:21.688525: step 5836, loss 0.159642, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:22.166088: step 5837, loss 0.0531589, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:22.675538: step 5838, loss 0.0576408, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:23.176970: step 5839, loss 0.0448265, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:23.708896: step 5840, loss 0.136805, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:24.873005: step 5840, loss 0.205146, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5840

2017-10-10T13:34:26.247615: step 5841, loss 0.0710493, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:26.788983: step 5842, loss 0.0545369, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:27.394128: step 5843, loss 0.230557, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:27.849682: step 5844, loss 0.128485, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:28.303880: step 5845, loss 0.105191, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:28.662186: step 5846, loss 0.0664416, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:29.163523: step 5847, loss 0.0884454, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:29.668961: step 5848, loss 0.0626653, acc 1, learning_rate 0.0001
2017-10-10T13:34:30.364825: step 5849, loss 0.104099, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:30.795076: step 5850, loss 0.0974902, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:31.256861: step 5851, loss 0.0928726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:31.764956: step 5852, loss 0.185062, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:32.236976: step 5853, loss 0.120699, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:32.812865: step 5854, loss 0.0893837, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:33.368242: step 5855, loss 0.134363, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:33.839561: step 5856, loss 0.0743093, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:34.375269: step 5857, loss 0.0937095, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:34.908837: step 5858, loss 0.0694726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:35.472980: step 5859, loss 0.0451197, acc 1, learning_rate 0.0001
2017-10-10T13:34:36.133027: step 5860, loss 0.158092, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:36.583611: step 5861, loss 0.114285, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:36.896952: step 5862, loss 0.101586, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:37.350304: step 5863, loss 0.0782964, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:37.880959: step 5864, loss 0.0838539, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:38.368930: step 5865, loss 0.0582765, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:38.877144: step 5866, loss 0.193252, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:39.413982: step 5867, loss 0.128633, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:39.921657: step 5868, loss 0.155489, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:40.458388: step 5869, loss 0.0524094, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:40.993444: step 5870, loss 0.141553, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:41.549325: step 5871, loss 0.123618, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:42.144878: step 5872, loss 0.110314, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:42.749002: step 5873, loss 0.139988, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:43.248865: step 5874, loss 0.101935, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:43.787483: step 5875, loss 0.0955611, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:44.300910: step 5876, loss 0.132108, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:44.820142: step 5877, loss 0.046097, acc 1, learning_rate 0.0001
2017-10-10T13:34:45.333260: step 5878, loss 0.0677151, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:45.928959: step 5879, loss 0.107757, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:46.369204: step 5880, loss 0.103318, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:47.583980: step 5880, loss 0.204562, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5880

2017-10-10T13:34:49.199296: step 5881, loss 0.0777997, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:49.724971: step 5882, loss 0.0999775, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:50.240918: step 5883, loss 0.0331131, acc 1, learning_rate 0.0001
2017-10-10T13:34:50.820887: step 5884, loss 0.103827, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:51.312124: step 5885, loss 0.0920907, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:51.747167: step 5886, loss 0.0833156, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:52.220844: step 5887, loss 0.100486, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:52.828520: step 5888, loss 0.084546, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:53.280874: step 5889, loss 0.0887651, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:53.690345: step 5890, loss 0.104658, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:54.148914: step 5891, loss 0.0911338, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:54.733073: step 5892, loss 0.109392, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:55.264915: step 5893, loss 0.0966147, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:55.831021: step 5894, loss 0.0311879, acc 1, learning_rate 0.0001
2017-10-10T13:34:56.368868: step 5895, loss 0.0551145, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:56.928612: step 5896, loss 0.165655, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:57.466627: step 5897, loss 0.12008, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:58.003683: step 5898, loss 0.0998557, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:58.518072: step 5899, loss 0.136192, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:59.095534: step 5900, loss 0.142571, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:59.509134: step 5901, loss 0.128961, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:59.816905: step 5902, loss 0.106859, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:00.281146: step 5903, loss 0.112775, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:00.844857: step 5904, loss 0.120537, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:01.421658: step 5905, loss 0.0953523, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:01.952893: step 5906, loss 0.0840582, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:02.456873: step 5907, loss 0.039497, acc 1, learning_rate 0.0001
2017-10-10T13:35:03.000868: step 5908, loss 0.140078, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:03.496473: step 5909, loss 0.0701699, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:04.008835: step 5910, loss 0.143201, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:04.609169: step 5911, loss 0.101219, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:05.144840: step 5912, loss 0.172672, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:05.611430: step 5913, loss 0.0692775, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:06.104515: step 5914, loss 0.176187, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:06.687712: step 5915, loss 0.0515637, acc 1, learning_rate 0.0001
2017-10-10T13:35:07.201893: step 5916, loss 0.0748227, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:07.725208: step 5917, loss 0.0728147, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:08.220914: step 5918, loss 0.0715013, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:08.707698: step 5919, loss 0.0933456, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:09.272022: step 5920, loss 0.0915264, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:10.416864: step 5920, loss 0.203747, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5920

2017-10-10T13:35:12.096345: step 5921, loss 0.175858, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:12.537067: step 5922, loss 0.138522, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:13.144978: step 5923, loss 0.138823, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:13.705377: step 5924, loss 0.0328393, acc 1, learning_rate 0.0001
2017-10-10T13:35:14.174046: step 5925, loss 0.117553, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:14.628682: step 5926, loss 0.172271, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:15.081039: step 5927, loss 0.141004, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:15.601089: step 5928, loss 0.0897026, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:16.197118: step 5929, loss 0.0899029, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:16.644939: step 5930, loss 0.140686, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:17.083637: step 5931, loss 0.0695452, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:17.541011: step 5932, loss 0.239404, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:18.101058: step 5933, loss 0.146674, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:18.629109: step 5934, loss 0.0398648, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:19.224928: step 5935, loss 0.104432, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:19.653240: step 5936, loss 0.151038, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:20.237043: step 5937, loss 0.123367, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:20.814726: step 5938, loss 0.212134, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:21.337933: step 5939, loss 0.129045, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:21.845075: step 5940, loss 0.126039, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:22.265239: step 5941, loss 0.154757, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:22.654127: step 5942, loss 0.0479572, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:23.208876: step 5943, loss 0.139311, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:23.727902: step 5944, loss 0.0662482, acc 1, learning_rate 0.0001
2017-10-10T13:35:24.216843: step 5945, loss 0.15891, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:24.692952: step 5946, loss 0.0415722, acc 1, learning_rate 0.0001
2017-10-10T13:35:25.293295: step 5947, loss 0.0539287, acc 1, learning_rate 0.0001
2017-10-10T13:35:25.800876: step 5948, loss 0.164362, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:26.372361: step 5949, loss 0.1103, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:26.904700: step 5950, loss 0.0913436, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:27.449191: step 5951, loss 0.028147, acc 1, learning_rate 0.0001
2017-10-10T13:35:27.928915: step 5952, loss 0.0613712, acc 1, learning_rate 0.0001
2017-10-10T13:35:28.386598: step 5953, loss 0.0592533, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:28.943286: step 5954, loss 0.0968831, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:29.489070: step 5955, loss 0.0988571, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:29.993152: step 5956, loss 0.0606934, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:30.477262: step 5957, loss 0.120221, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:31.017197: step 5958, loss 0.114756, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:31.484950: step 5959, loss 0.0975278, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:31.991627: step 5960, loss 0.0334361, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:33.118408: step 5960, loss 0.20081, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-5960

2017-10-10T13:35:34.521059: step 5961, loss 0.0919679, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:35.098992: step 5962, loss 0.0759268, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:35.675431: step 5963, loss 0.167602, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:36.215534: step 5964, loss 0.143445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:36.824936: step 5965, loss 0.137588, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:37.412921: step 5966, loss 0.0493725, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:37.829818: step 5967, loss 0.12763, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:38.295047: step 5968, loss 0.0650812, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:38.804945: step 5969, loss 0.0619245, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:39.406417: step 5970, loss 0.116987, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:39.814085: step 5971, loss 0.0336462, acc 1, learning_rate 0.0001
2017-10-10T13:35:40.251399: step 5972, loss 0.172764, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:40.670549: step 5973, loss 0.0466281, acc 1, learning_rate 0.0001
2017-10-10T13:35:41.184180: step 5974, loss 0.0523364, acc 1, learning_rate 0.0001
2017-10-10T13:35:41.660033: step 5975, loss 0.128226, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:42.158124: step 5976, loss 0.167973, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:42.706739: step 5977, loss 0.111171, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:43.159629: step 5978, loss 0.0499721, acc 0.980392, learning_rate 0.0001
2017-10-10T13:35:43.556496: step 5979, loss 0.129256, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:44.100199: step 5980, loss 0.0730981, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:44.620883: step 5981, loss 0.125997, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:45.204672: step 5982, loss 0.0524924, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:45.649025: step 5983, loss 0.0604481, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:46.124843: step 5984, loss 0.105827, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:46.593169: step 5985, loss 0.0952927, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:47.110279: step 5986, loss 0.119959, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:47.513117: step 5987, loss 0.227258, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:48.006390: step 5988, loss 0.138637, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:48.502992: step 5989, loss 0.153535, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:48.994388: step 5990, loss 0.0678174, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:49.523456: step 5991, loss 0.106642, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:50.060371: step 5992, loss 0.0624467, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:50.611886: step 5993, loss 0.0582189, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:51.153497: step 5994, loss 0.0400572, acc 1, learning_rate 0.0001
2017-10-10T13:35:51.665012: step 5995, loss 0.185128, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:52.148863: step 5996, loss 0.140688, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:52.724535: step 5997, loss 0.101167, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:53.196575: step 5998, loss 0.0560605, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:53.740854: step 5999, loss 0.117413, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:54.326708: step 6000, loss 0.105155, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:55.508990: step 6000, loss 0.20486, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6000

2017-10-10T13:35:58.017213: step 6001, loss 0.130305, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:58.637136: step 6002, loss 0.100556, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:59.184914: step 6003, loss 0.0679929, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:59.781047: step 6004, loss 0.273629, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:00.284889: step 6005, loss 0.095083, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:00.649061: step 6006, loss 0.0419256, acc 1, learning_rate 0.0001
2017-10-10T13:36:01.096811: step 6007, loss 0.0892743, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:01.704956: step 6008, loss 0.0763715, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:02.268832: step 6009, loss 0.0850957, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:02.740765: step 6010, loss 0.125561, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:03.270293: step 6011, loss 0.165989, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:03.744903: step 6012, loss 0.133245, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:04.232912: step 6013, loss 0.155755, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:04.724899: step 6014, loss 0.0989939, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:05.276952: step 6015, loss 0.0922802, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:05.796959: step 6016, loss 0.100504, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:06.169028: step 6017, loss 0.142827, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:06.808935: step 6018, loss 0.0807277, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:07.396925: step 6019, loss 0.0584659, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:08.040941: step 6020, loss 0.083218, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:08.474476: step 6021, loss 0.104455, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:08.845161: step 6022, loss 0.117503, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:09.401624: step 6023, loss 0.121188, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:09.899376: step 6024, loss 0.0421937, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:10.400833: step 6025, loss 0.0871938, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:10.943175: step 6026, loss 0.0920956, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:11.472823: step 6027, loss 0.103626, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:12.021648: step 6028, loss 0.125412, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:12.580042: step 6029, loss 0.0717331, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:13.126763: step 6030, loss 0.0881952, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:13.724855: step 6031, loss 0.129165, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:14.240849: step 6032, loss 0.120635, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:14.777573: step 6033, loss 0.160785, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:15.336848: step 6034, loss 0.067846, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:15.880242: step 6035, loss 0.0671496, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:16.388543: step 6036, loss 0.116908, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:16.977607: step 6037, loss 0.0582969, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:17.508849: step 6038, loss 0.125131, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:18.043941: step 6039, loss 0.175502, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:18.596380: step 6040, loss 0.105463, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:19.745287: step 6040, loss 0.203442, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6040

2017-10-10T13:36:21.643114: step 6041, loss 0.100643, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:22.193108: step 6042, loss 0.0561623, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:22.695410: step 6043, loss 0.165252, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:23.212439: step 6044, loss 0.131364, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:23.856854: step 6045, loss 0.121141, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:24.389395: step 6046, loss 0.0691753, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:24.902448: step 6047, loss 0.0562634, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:25.388528: step 6048, loss 0.145865, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:25.788003: step 6049, loss 0.101835, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:26.356136: step 6050, loss 0.115988, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:26.860130: step 6051, loss 0.0759251, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:27.383202: step 6052, loss 0.0627735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:27.936803: step 6053, loss 0.138418, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:28.502799: step 6054, loss 0.136724, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:29.054785: step 6055, loss 0.0751183, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:29.573683: step 6056, loss 0.0959855, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:30.076860: step 6057, loss 0.0378335, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:30.651353: step 6058, loss 0.142551, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:31.302717: step 6059, loss 0.0645077, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:31.768523: step 6060, loss 0.108479, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:32.221551: step 6061, loss 0.143702, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:32.756365: step 6062, loss 0.0786631, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:33.285859: step 6063, loss 0.167106, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:33.821553: step 6064, loss 0.127494, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:34.371357: step 6065, loss 0.157137, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:34.948805: step 6066, loss 0.0956134, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:35.490199: step 6067, loss 0.0788886, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:36.032793: step 6068, loss 0.0653006, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:36.562757: step 6069, loss 0.0477829, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:37.114672: step 6070, loss 0.115489, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:37.671257: step 6071, loss 0.0887444, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:38.236182: step 6072, loss 0.0503889, acc 1, learning_rate 0.0001
2017-10-10T13:36:38.768875: step 6073, loss 0.044552, acc 1, learning_rate 0.0001
2017-10-10T13:36:39.330099: step 6074, loss 0.210342, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:39.876828: step 6075, loss 0.122712, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:40.309464: step 6076, loss 0.0946742, acc 0.980392, learning_rate 0.0001
2017-10-10T13:36:40.775957: step 6077, loss 0.11861, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:41.220961: step 6078, loss 0.0947729, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:41.721090: step 6079, loss 0.106177, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:42.190291: step 6080, loss 0.0281301, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:43.425735: step 6080, loss 0.207268, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6080

2017-10-10T13:36:44.870934: step 6081, loss 0.113625, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:45.356966: step 6082, loss 0.0470352, acc 1, learning_rate 0.0001
2017-10-10T13:36:45.941097: step 6083, loss 0.139114, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:46.600725: step 6084, loss 0.0930348, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:47.024935: step 6085, loss 0.117719, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:47.460882: step 6086, loss 0.0507662, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:47.965339: step 6087, loss 0.128561, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:48.436006: step 6088, loss 0.0397067, acc 1, learning_rate 0.0001
2017-10-10T13:36:48.924927: step 6089, loss 0.0303544, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:49.360916: step 6090, loss 0.0429357, acc 1, learning_rate 0.0001
2017-10-10T13:36:49.862603: step 6091, loss 0.0701797, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:50.393261: step 6092, loss 0.200417, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:50.928117: step 6093, loss 0.135935, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:51.455394: step 6094, loss 0.226199, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:51.980213: step 6095, loss 0.160876, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:52.515049: step 6096, loss 0.0714841, acc 1, learning_rate 0.0001
2017-10-10T13:36:53.072786: step 6097, loss 0.175607, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:53.578818: step 6098, loss 0.148043, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:54.134493: step 6099, loss 0.177801, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:54.689097: step 6100, loss 0.0279035, acc 1, learning_rate 0.0001
2017-10-10T13:36:55.152837: step 6101, loss 0.0489092, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:55.575016: step 6102, loss 0.117671, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:56.028359: step 6103, loss 0.160863, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:56.589110: step 6104, loss 0.0636704, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:57.141009: step 6105, loss 0.0735849, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:57.716898: step 6106, loss 0.0525115, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:58.205043: step 6107, loss 0.176444, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:58.760871: step 6108, loss 0.0721864, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:59.398685: step 6109, loss 0.106425, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:59.908143: step 6110, loss 0.143965, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:00.539628: step 6111, loss 0.120235, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:01.140037: step 6112, loss 0.174571, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:01.681665: step 6113, loss 0.0976595, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:02.199021: step 6114, loss 0.0781344, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:02.716614: step 6115, loss 0.0599122, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:03.192885: step 6116, loss 0.131207, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:03.702407: step 6117, loss 0.0715165, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:04.258802: step 6118, loss 0.0847499, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:04.772885: step 6119, loss 0.102034, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:05.296961: step 6120, loss 0.151102, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:06.344859: step 6120, loss 0.206508, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6120

2017-10-10T13:37:08.069064: step 6121, loss 0.162281, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:08.636881: step 6122, loss 0.0974415, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:09.327006: step 6123, loss 0.0900631, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:09.807859: step 6124, loss 0.0526787, acc 1, learning_rate 0.0001
2017-10-10T13:37:10.172888: step 6125, loss 0.073784, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:10.788987: step 6126, loss 0.0805443, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:11.233113: step 6127, loss 0.0874261, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:11.668965: step 6128, loss 0.0427579, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:12.144089: step 6129, loss 0.0494831, acc 1, learning_rate 0.0001
2017-10-10T13:37:12.561896: step 6130, loss 0.118635, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:13.049627: step 6131, loss 0.129441, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:13.609132: step 6132, loss 0.112036, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:14.164818: step 6133, loss 0.126913, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:14.717378: step 6134, loss 0.0972913, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:15.269371: step 6135, loss 0.0736435, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:15.864896: step 6136, loss 0.0802247, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:16.404568: step 6137, loss 0.139522, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:16.870934: step 6138, loss 0.072268, acc 1, learning_rate 0.0001
2017-10-10T13:37:17.340881: step 6139, loss 0.135119, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:17.981124: step 6140, loss 0.158918, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:18.484807: step 6141, loss 0.0841466, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:18.972832: step 6142, loss 0.109284, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:19.525021: step 6143, loss 0.0848222, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:20.064802: step 6144, loss 0.154202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:20.504874: step 6145, loss 0.0906999, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:21.039893: step 6146, loss 0.101453, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:21.537709: step 6147, loss 0.122238, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:22.110768: step 6148, loss 0.0758163, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:22.689201: step 6149, loss 0.0714568, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:23.179206: step 6150, loss 0.153559, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:23.645043: step 6151, loss 0.110631, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:24.146878: step 6152, loss 0.0589438, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:24.625127: step 6153, loss 0.100121, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:25.108165: step 6154, loss 0.166111, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:25.541348: step 6155, loss 0.080346, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:26.033350: step 6156, loss 0.0727356, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:26.593118: step 6157, loss 0.0408185, acc 1, learning_rate 0.0001
2017-10-10T13:37:27.150003: step 6158, loss 0.100159, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:27.705007: step 6159, loss 0.09285, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:28.211975: step 6160, loss 0.0904398, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:29.317022: step 6160, loss 0.204574, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6160

2017-10-10T13:37:30.912883: step 6161, loss 0.0870126, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:31.453013: step 6162, loss 0.0553287, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:32.128845: step 6163, loss 0.0289888, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:32.555004: step 6164, loss 0.123286, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:33.056568: step 6165, loss 0.180065, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:33.609709: step 6166, loss 0.0815466, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:34.088884: step 6167, loss 0.0551434, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:34.561719: step 6168, loss 0.0356399, acc 1, learning_rate 0.0001
2017-10-10T13:37:35.012107: step 6169, loss 0.0864055, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:35.542430: step 6170, loss 0.0928015, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:36.120911: step 6171, loss 0.181098, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:36.668738: step 6172, loss 0.0813523, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:37.207761: step 6173, loss 0.0961403, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:37.640872: step 6174, loss 0.0438757, acc 1, learning_rate 0.0001
2017-10-10T13:37:38.148435: step 6175, loss 0.0829381, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:38.621018: step 6176, loss 0.166743, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:39.115154: step 6177, loss 0.0737655, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:39.665110: step 6178, loss 0.143432, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:40.285789: step 6179, loss 0.0655929, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:41.008915: step 6180, loss 0.141834, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:41.387178: step 6181, loss 0.158841, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:41.819197: step 6182, loss 0.0962455, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:42.266719: step 6183, loss 0.0725298, acc 1, learning_rate 0.0001
2017-10-10T13:37:42.824469: step 6184, loss 0.0883133, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:43.368179: step 6185, loss 0.149436, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:43.938522: step 6186, loss 0.0613565, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:44.485174: step 6187, loss 0.116782, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:45.012858: step 6188, loss 0.181867, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:45.600411: step 6189, loss 0.109559, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:46.165019: step 6190, loss 0.0977161, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:46.688985: step 6191, loss 0.0889599, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:47.159346: step 6192, loss 0.0851498, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:47.695142: step 6193, loss 0.171378, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:48.255466: step 6194, loss 0.125897, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:48.795975: step 6195, loss 0.11762, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:49.345202: step 6196, loss 0.119934, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:49.862272: step 6197, loss 0.0605219, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:50.396301: step 6198, loss 0.0586258, acc 1, learning_rate 0.0001
2017-10-10T13:37:50.999849: step 6199, loss 0.0425937, acc 1, learning_rate 0.0001
2017-10-10T13:37:51.551056: step 6200, loss 0.119489, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:52.733547: step 6200, loss 0.204775, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6200

2017-10-10T13:37:54.608842: step 6201, loss 0.110479, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:55.234521: step 6202, loss 0.0628701, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:55.681747: step 6203, loss 0.152529, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:56.121319: step 6204, loss 0.130476, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:56.553431: step 6205, loss 0.0420212, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:57.118221: step 6206, loss 0.184054, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:57.616851: step 6207, loss 0.125915, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:58.098916: step 6208, loss 0.13171, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:58.522400: step 6209, loss 0.127622, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:59.055853: step 6210, loss 0.0602175, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:59.611276: step 6211, loss 0.114179, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:00.154405: step 6212, loss 0.0858219, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:00.700832: step 6213, loss 0.0873932, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:01.207044: step 6214, loss 0.0460099, acc 1, learning_rate 0.0001
2017-10-10T13:38:01.717470: step 6215, loss 0.119098, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:02.289284: step 6216, loss 0.0482268, acc 1, learning_rate 0.0001
2017-10-10T13:38:02.848573: step 6217, loss 0.110209, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:03.509100: step 6218, loss 0.0517138, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:04.081429: step 6219, loss 0.177128, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:04.501472: step 6220, loss 0.0910496, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:04.961124: step 6221, loss 0.154292, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:05.415379: step 6222, loss 0.0527318, acc 1, learning_rate 0.0001
2017-10-10T13:38:05.906708: step 6223, loss 0.170955, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:06.409008: step 6224, loss 0.0913002, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:06.945114: step 6225, loss 0.11516, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:07.545628: step 6226, loss 0.0970734, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:08.128200: step 6227, loss 0.108904, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:08.656810: step 6228, loss 0.0756481, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:09.156920: step 6229, loss 0.270082, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:09.692666: step 6230, loss 0.104904, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:10.251568: step 6231, loss 0.0759951, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:10.803267: step 6232, loss 0.0988635, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:11.373109: step 6233, loss 0.131646, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:11.916878: step 6234, loss 0.0682942, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:12.496371: step 6235, loss 0.0343461, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:12.992058: step 6236, loss 0.0990794, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:13.512372: step 6237, loss 0.0641294, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:14.032867: step 6238, loss 0.148286, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:14.544860: step 6239, loss 0.0331775, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:15.092859: step 6240, loss 0.0934291, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:16.257086: step 6240, loss 0.202501, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6240

2017-10-10T13:38:17.564973: step 6241, loss 0.0441672, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:18.129010: step 6242, loss 0.156535, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:18.695503: step 6243, loss 0.045617, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:19.096884: step 6244, loss 0.0717435, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:19.521371: step 6245, loss 0.0707029, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:20.031580: step 6246, loss 0.0544774, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:20.540732: step 6247, loss 0.0605228, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:21.001036: step 6248, loss 0.161947, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:21.416335: step 6249, loss 0.0945451, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:21.944909: step 6250, loss 0.0751922, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:22.477016: step 6251, loss 0.0470078, acc 1, learning_rate 0.0001
2017-10-10T13:38:22.948098: step 6252, loss 0.115715, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:23.445737: step 6253, loss 0.094278, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:23.928342: step 6254, loss 0.121643, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:24.449398: step 6255, loss 0.137511, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:24.998549: step 6256, loss 0.136637, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:25.504943: step 6257, loss 0.0815362, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:26.061087: step 6258, loss 0.081361, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:26.541087: step 6259, loss 0.0547734, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:27.104001: step 6260, loss 0.0485896, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:27.622047: step 6261, loss 0.109097, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:28.063057: step 6262, loss 0.279468, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:28.627283: step 6263, loss 0.0783257, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:29.143515: step 6264, loss 0.115033, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:29.637145: step 6265, loss 0.148454, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:30.131225: step 6266, loss 0.189244, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:30.644904: step 6267, loss 0.0907329, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:31.138558: step 6268, loss 0.102537, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:31.644372: step 6269, loss 0.088898, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:32.212890: step 6270, loss 0.106891, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:32.726243: step 6271, loss 0.122275, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:33.216898: step 6272, loss 0.131249, acc 0.960784, learning_rate 0.0001
2017-10-10T13:38:33.740650: step 6273, loss 0.0813776, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:34.243418: step 6274, loss 0.184586, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:34.700970: step 6275, loss 0.0841469, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:35.201106: step 6276, loss 0.19843, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:35.664756: step 6277, loss 0.110904, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:36.168884: step 6278, loss 0.130299, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:36.591130: step 6279, loss 0.0600101, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:37.136528: step 6280, loss 0.112221, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:38.311845: step 6280, loss 0.202166, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6280

2017-10-10T13:38:39.910992: step 6281, loss 0.125107, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:40.346045: step 6282, loss 0.0757396, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:40.870045: step 6283, loss 0.112652, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:41.548912: step 6284, loss 0.0419541, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:42.000930: step 6285, loss 0.214137, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:42.426989: step 6286, loss 0.0389255, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:42.912396: step 6287, loss 0.131379, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:43.615527: step 6288, loss 0.11885, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:44.213007: step 6289, loss 0.108109, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:44.539037: step 6290, loss 0.118327, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:44.944234: step 6291, loss 0.0656865, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:45.355588: step 6292, loss 0.0432358, acc 1, learning_rate 0.0001
2017-10-10T13:38:45.893475: step 6293, loss 0.066248, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:46.447533: step 6294, loss 0.255814, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:46.987681: step 6295, loss 0.0345463, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:47.625098: step 6296, loss 0.110633, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:48.132897: step 6297, loss 0.103988, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:48.590825: step 6298, loss 0.0729424, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:49.096976: step 6299, loss 0.173011, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:49.548302: step 6300, loss 0.0600301, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:50.068849: step 6301, loss 0.126437, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:50.639723: step 6302, loss 0.0349456, acc 1, learning_rate 0.0001
2017-10-10T13:38:51.080853: step 6303, loss 0.0426913, acc 1, learning_rate 0.0001
2017-10-10T13:38:51.600878: step 6304, loss 0.0248743, acc 1, learning_rate 0.0001
2017-10-10T13:38:52.093042: step 6305, loss 0.0617192, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:52.736857: step 6306, loss 0.130914, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:53.280934: step 6307, loss 0.0523084, acc 1, learning_rate 0.0001
2017-10-10T13:38:53.781045: step 6308, loss 0.0682435, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:54.286981: step 6309, loss 0.0522406, acc 1, learning_rate 0.0001
2017-10-10T13:38:54.826589: step 6310, loss 0.055391, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:55.320899: step 6311, loss 0.121679, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:55.845799: step 6312, loss 0.0943628, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:56.273016: step 6313, loss 0.0396796, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:56.759402: step 6314, loss 0.0725458, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:57.313256: step 6315, loss 0.136103, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:57.800964: step 6316, loss 0.1026, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:58.377124: step 6317, loss 0.0956505, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:58.930787: step 6318, loss 0.134114, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:59.397025: step 6319, loss 0.0670884, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:59.868857: step 6320, loss 0.124668, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:01.071607: step 6320, loss 0.201029, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6320

2017-10-10T13:39:02.919657: step 6321, loss 0.0922934, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:03.474184: step 6322, loss 0.0791879, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:04.052887: step 6323, loss 0.0863639, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:04.628572: step 6324, loss 0.0771003, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:05.104838: step 6325, loss 0.0886215, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:05.553014: step 6326, loss 0.0499084, acc 1, learning_rate 0.0001
2017-10-10T13:39:06.053641: step 6327, loss 0.151594, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:06.589216: step 6328, loss 0.0533456, acc 1, learning_rate 0.0001
2017-10-10T13:39:07.265023: step 6329, loss 0.0511374, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:07.672835: step 6330, loss 0.0667547, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:08.144843: step 6331, loss 0.193191, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:08.652953: step 6332, loss 0.115725, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:09.139635: step 6333, loss 0.132464, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:09.633107: step 6334, loss 0.174695, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:10.165032: step 6335, loss 0.0625134, acc 1, learning_rate 0.0001
2017-10-10T13:39:10.655590: step 6336, loss 0.0240265, acc 1, learning_rate 0.0001
2017-10-10T13:39:11.236915: step 6337, loss 0.0525363, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:11.777908: step 6338, loss 0.0867916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:12.277092: step 6339, loss 0.0388312, acc 1, learning_rate 0.0001
2017-10-10T13:39:12.910217: step 6340, loss 0.142467, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:13.414084: step 6341, loss 0.102198, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:13.988908: step 6342, loss 0.0762239, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:14.522104: step 6343, loss 0.165982, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:14.983744: step 6344, loss 0.155049, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:15.411063: step 6345, loss 0.0668717, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:15.920877: step 6346, loss 0.0986811, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:16.433738: step 6347, loss 0.0606635, acc 1, learning_rate 0.0001
2017-10-10T13:39:16.942369: step 6348, loss 0.0809661, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:17.385046: step 6349, loss 0.0915358, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:17.891090: step 6350, loss 0.128723, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:18.431064: step 6351, loss 0.0951749, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:18.932938: step 6352, loss 0.155434, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:19.372986: step 6353, loss 0.134125, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:19.899501: step 6354, loss 0.100505, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:20.414709: step 6355, loss 0.073251, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:20.904506: step 6356, loss 0.337055, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:21.424924: step 6357, loss 0.0556542, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:22.007816: step 6358, loss 0.0918316, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:22.572835: step 6359, loss 0.0851199, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:23.074020: step 6360, loss 0.11186, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:24.234778: step 6360, loss 0.203928, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6360

2017-10-10T13:39:25.701479: step 6361, loss 0.262716, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:26.248997: step 6362, loss 0.0572205, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:26.787376: step 6363, loss 0.111246, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:27.369251: step 6364, loss 0.105957, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:27.904858: step 6365, loss 0.117157, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:28.383054: step 6366, loss 0.0753756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:28.831851: step 6367, loss 0.0486467, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:29.420000: step 6368, loss 0.0591091, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:30.002467: step 6369, loss 0.0930249, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:30.425337: step 6370, loss 0.0313754, acc 1, learning_rate 0.0001
2017-10-10T13:39:30.908051: step 6371, loss 0.119261, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:31.378247: step 6372, loss 0.0625741, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:31.872909: step 6373, loss 0.049672, acc 1, learning_rate 0.0001
2017-10-10T13:39:32.419423: step 6374, loss 0.121437, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:32.985810: step 6375, loss 0.0765767, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:33.529822: step 6376, loss 0.108784, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:34.070820: step 6377, loss 0.148053, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:34.563554: step 6378, loss 0.137478, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:35.061444: step 6379, loss 0.120612, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:35.597262: step 6380, loss 0.112271, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:36.084039: step 6381, loss 0.10205, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:36.708985: step 6382, loss 0.111425, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:37.340854: step 6383, loss 0.0910141, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:37.757676: step 6384, loss 0.0858614, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:38.143491: step 6385, loss 0.171052, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:38.756873: step 6386, loss 0.0289814, acc 1, learning_rate 0.0001
2017-10-10T13:39:39.295518: step 6387, loss 0.10622, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:39.807556: step 6388, loss 0.0529257, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:40.360270: step 6389, loss 0.158288, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:40.906619: step 6390, loss 0.0813663, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:41.426020: step 6391, loss 0.0551154, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:41.946225: step 6392, loss 0.134765, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:42.489050: step 6393, loss 0.0926421, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:43.047789: step 6394, loss 0.0986565, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:43.497335: step 6395, loss 0.0850091, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:43.927683: step 6396, loss 0.276201, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:44.475909: step 6397, loss 0.0862908, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:44.933351: step 6398, loss 0.0617668, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:45.421722: step 6399, loss 0.0818168, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:45.876163: step 6400, loss 0.136535, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:47.183632: step 6400, loss 0.204841, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6400

2017-10-10T13:39:48.847967: step 6401, loss 0.10575, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:49.408896: step 6402, loss 0.1094, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:50.108886: step 6403, loss 0.140426, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:50.636182: step 6404, loss 0.153094, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:51.108203: step 6405, loss 0.0801015, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:51.586510: step 6406, loss 0.0512128, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:52.180958: step 6407, loss 0.108726, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:52.763876: step 6408, loss 0.0483706, acc 1, learning_rate 0.0001
2017-10-10T13:39:53.398713: step 6409, loss 0.0909429, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:53.906342: step 6410, loss 0.107813, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:54.331016: step 6411, loss 0.0532174, acc 1, learning_rate 0.0001
2017-10-10T13:39:54.841002: step 6412, loss 0.0835513, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:55.359062: step 6413, loss 0.0944642, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:55.948591: step 6414, loss 0.0376809, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:56.519039: step 6415, loss 0.217214, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:57.075092: step 6416, loss 0.0923632, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:57.632903: step 6417, loss 0.0666226, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:58.159763: step 6418, loss 0.206927, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:58.692848: step 6419, loss 0.0667368, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:59.216833: step 6420, loss 0.158761, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:59.749162: step 6421, loss 0.0735099, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:00.385333: step 6422, loss 0.0598717, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:00.772988: step 6423, loss 0.130541, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:01.116246: step 6424, loss 0.0726881, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:01.536941: step 6425, loss 0.167269, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:02.108972: step 6426, loss 0.144519, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:02.624968: step 6427, loss 0.0288712, acc 1, learning_rate 0.0001
2017-10-10T13:40:03.224035: step 6428, loss 0.0477392, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:03.772898: step 6429, loss 0.0362508, acc 1, learning_rate 0.0001
2017-10-10T13:40:04.376844: step 6430, loss 0.0769932, acc 1, learning_rate 0.0001
2017-10-10T13:40:04.923013: step 6431, loss 0.139073, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:05.467385: step 6432, loss 0.0181653, acc 1, learning_rate 0.0001
2017-10-10T13:40:06.006512: step 6433, loss 0.0962798, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:06.513867: step 6434, loss 0.0364403, acc 1, learning_rate 0.0001
2017-10-10T13:40:07.053888: step 6435, loss 0.103964, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:07.568899: step 6436, loss 0.0492355, acc 1, learning_rate 0.0001
2017-10-10T13:40:08.096561: step 6437, loss 0.0876468, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:08.577158: step 6438, loss 0.0831578, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:09.096628: step 6439, loss 0.0366836, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:09.581529: step 6440, loss 0.0798688, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:10.793111: step 6440, loss 0.202862, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6440

2017-10-10T13:40:12.557037: step 6441, loss 0.0316433, acc 1, learning_rate 0.0001
2017-10-10T13:40:13.097017: step 6442, loss 0.0885202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:13.532316: step 6443, loss 0.0746627, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:13.974044: step 6444, loss 0.114343, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:14.516913: step 6445, loss 0.120297, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:15.072835: step 6446, loss 0.0454916, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:15.600853: step 6447, loss 0.0302452, acc 1, learning_rate 0.0001
2017-10-10T13:40:16.130947: step 6448, loss 0.107659, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:16.688912: step 6449, loss 0.163465, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:17.239369: step 6450, loss 0.0732387, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:17.743728: step 6451, loss 0.0793734, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:18.206119: step 6452, loss 0.0660592, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:18.657687: step 6453, loss 0.0585689, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:19.192356: step 6454, loss 0.0759145, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:19.716884: step 6455, loss 0.116476, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:20.268857: step 6456, loss 0.0629724, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:20.849200: step 6457, loss 0.0764767, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:21.425356: step 6458, loss 0.0236891, acc 1, learning_rate 0.0001
2017-10-10T13:40:22.016996: step 6459, loss 0.137723, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:22.660981: step 6460, loss 0.130536, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:23.188874: step 6461, loss 0.240372, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:23.615510: step 6462, loss 0.14778, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:24.008995: step 6463, loss 0.106643, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:24.324833: step 6464, loss 0.0976405, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:24.924887: step 6465, loss 0.107313, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:25.445005: step 6466, loss 0.0475875, acc 1, learning_rate 0.0001
2017-10-10T13:40:25.964502: step 6467, loss 0.0979168, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:26.396827: step 6468, loss 0.0699929, acc 0.980392, learning_rate 0.0001
2017-10-10T13:40:26.945028: step 6469, loss 0.0666971, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:27.541551: step 6470, loss 0.242702, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:28.045064: step 6471, loss 0.0629324, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:28.558229: step 6472, loss 0.227155, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:29.146738: step 6473, loss 0.0439826, acc 1, learning_rate 0.0001
2017-10-10T13:40:29.697048: step 6474, loss 0.116231, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:30.195013: step 6475, loss 0.0539294, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:30.784185: step 6476, loss 0.122076, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:31.322736: step 6477, loss 0.15522, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:31.804829: step 6478, loss 0.0974787, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:32.360946: step 6479, loss 0.130991, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:32.879534: step 6480, loss 0.095826, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:34.020859: step 6480, loss 0.203151, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6480

2017-10-10T13:40:35.404886: step 6481, loss 0.119112, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:36.023493: step 6482, loss 0.174213, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:36.591638: step 6483, loss 0.160029, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:37.048194: step 6484, loss 0.0548946, acc 1, learning_rate 0.0001
2017-10-10T13:40:37.496811: step 6485, loss 0.0509089, acc 1, learning_rate 0.0001
2017-10-10T13:40:37.988868: step 6486, loss 0.0798938, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:38.492947: step 6487, loss 0.0821264, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:38.999460: step 6488, loss 0.144901, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:39.392986: step 6489, loss 0.0892162, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:39.954210: step 6490, loss 0.102542, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:40.550936: step 6491, loss 0.0893233, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:41.038013: step 6492, loss 0.0762291, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:41.520827: step 6493, loss 0.0879927, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:42.012506: step 6494, loss 0.0711874, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:42.512861: step 6495, loss 0.135533, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:42.984941: step 6496, loss 0.154009, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:43.485488: step 6497, loss 0.0704308, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:44.016989: step 6498, loss 0.0661376, acc 1, learning_rate 0.0001
2017-10-10T13:40:44.630463: step 6499, loss 0.11122, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:45.167342: step 6500, loss 0.0279107, acc 1, learning_rate 0.0001
2017-10-10T13:40:45.657443: step 6501, loss 0.0378872, acc 1, learning_rate 0.0001
2017-10-10T13:40:46.283881: step 6502, loss 0.0388068, acc 1, learning_rate 0.0001
2017-10-10T13:40:46.802310: step 6503, loss 0.0418282, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:47.239716: step 6504, loss 0.0699765, acc 1, learning_rate 0.0001
2017-10-10T13:40:47.736844: step 6505, loss 0.135479, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:48.223905: step 6506, loss 0.10698, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:48.663944: step 6507, loss 0.103604, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:49.160683: step 6508, loss 0.104079, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:49.724312: step 6509, loss 0.0430764, acc 1, learning_rate 0.0001
2017-10-10T13:40:50.261137: step 6510, loss 0.0880317, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:50.759589: step 6511, loss 0.0766318, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:51.251104: step 6512, loss 0.100716, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:51.744967: step 6513, loss 0.154097, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:52.280952: step 6514, loss 0.124721, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:52.760237: step 6515, loss 0.0943141, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:53.266792: step 6516, loss 0.0545713, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:53.853136: step 6517, loss 0.208137, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:54.405087: step 6518, loss 0.139154, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:54.924994: step 6519, loss 0.0357183, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:55.434066: step 6520, loss 0.0932327, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:56.644059: step 6520, loss 0.201496, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6520

2017-10-10T13:40:58.259487: step 6521, loss 0.12997, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:58.862061: step 6522, loss 0.105126, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:59.306417: step 6523, loss 0.0633851, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:59.832219: step 6524, loss 0.114809, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:00.228814: step 6525, loss 0.12754, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:00.638375: step 6526, loss 0.1391, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:01.086959: step 6527, loss 0.0838663, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:01.691224: step 6528, loss 0.0350027, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:02.249459: step 6529, loss 0.104153, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:02.653890: step 6530, loss 0.12541, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:03.265062: step 6531, loss 0.0928804, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:03.836371: step 6532, loss 0.175196, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:04.281340: step 6533, loss 0.0712667, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:04.711935: step 6534, loss 0.109898, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:05.248890: step 6535, loss 0.0551612, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:05.812979: step 6536, loss 0.0617156, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:06.339062: step 6537, loss 0.0195915, acc 1, learning_rate 0.0001
2017-10-10T13:41:06.928868: step 6538, loss 0.0590797, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:07.500628: step 6539, loss 0.0572712, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:08.056980: step 6540, loss 0.09978, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:08.659698: step 6541, loss 0.0309838, acc 1, learning_rate 0.0001
2017-10-10T13:41:09.217061: step 6542, loss 0.0306678, acc 1, learning_rate 0.0001
2017-10-10T13:41:09.652816: step 6543, loss 0.0619887, acc 1, learning_rate 0.0001
2017-10-10T13:41:10.062496: step 6544, loss 0.140154, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:10.632261: step 6545, loss 0.150179, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:11.164819: step 6546, loss 0.119072, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:11.699245: step 6547, loss 0.148134, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:12.236098: step 6548, loss 0.0587615, acc 1, learning_rate 0.0001
2017-10-10T13:41:12.748843: step 6549, loss 0.0675329, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:13.288425: step 6550, loss 0.0817399, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:13.784832: step 6551, loss 0.113039, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:14.299535: step 6552, loss 0.110545, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:14.774448: step 6553, loss 0.112309, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:15.317030: step 6554, loss 0.0565379, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:15.920852: step 6555, loss 0.124553, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:16.398950: step 6556, loss 0.050006, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:16.920466: step 6557, loss 0.0560192, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:17.420975: step 6558, loss 0.0681344, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:17.939129: step 6559, loss 0.0319861, acc 1, learning_rate 0.0001
2017-10-10T13:41:18.441001: step 6560, loss 0.0773783, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:19.592825: step 6560, loss 0.202009, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6560

2017-10-10T13:41:21.277198: step 6561, loss 0.13663, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:21.824823: step 6562, loss 0.130059, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:22.441699: step 6563, loss 0.0678246, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:22.907117: step 6564, loss 0.0822711, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:23.341369: step 6565, loss 0.0563177, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:23.678563: step 6566, loss 0.101448, acc 0.960784, learning_rate 0.0001
2017-10-10T13:41:24.165080: step 6567, loss 0.0755568, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:24.630233: step 6568, loss 0.220582, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:25.086569: step 6569, loss 0.0736031, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:25.637882: step 6570, loss 0.106907, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:26.300838: step 6571, loss 0.148253, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:26.852865: step 6572, loss 0.135035, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:27.272165: step 6573, loss 0.0504657, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:27.692760: step 6574, loss 0.0327499, acc 1, learning_rate 0.0001
2017-10-10T13:41:28.188905: step 6575, loss 0.0270068, acc 1, learning_rate 0.0001
2017-10-10T13:41:28.691305: step 6576, loss 0.0891802, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:29.211864: step 6577, loss 0.117399, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:29.773148: step 6578, loss 0.128145, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:30.284605: step 6579, loss 0.0806092, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:30.804936: step 6580, loss 0.110822, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:31.325576: step 6581, loss 0.0708595, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:31.935916: step 6582, loss 0.0769106, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:32.511272: step 6583, loss 0.077355, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:32.979396: step 6584, loss 0.0592146, acc 1, learning_rate 0.0001
2017-10-10T13:41:33.402674: step 6585, loss 0.0587831, acc 1, learning_rate 0.0001
2017-10-10T13:41:33.933139: step 6586, loss 0.0772362, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:34.489340: step 6587, loss 0.0696291, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:35.013177: step 6588, loss 0.146839, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:35.556920: step 6589, loss 0.108768, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:36.094755: step 6590, loss 0.148945, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:36.646791: step 6591, loss 0.137102, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:37.189319: step 6592, loss 0.0863629, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:37.745617: step 6593, loss 0.102838, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:38.338998: step 6594, loss 0.0934397, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:38.861176: step 6595, loss 0.115759, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:39.341257: step 6596, loss 0.0889243, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:39.862017: step 6597, loss 0.0775958, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:40.414999: step 6598, loss 0.151085, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:40.877624: step 6599, loss 0.0790505, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:41.386111: step 6600, loss 0.0700726, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:42.566956: step 6600, loss 0.201509, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6600

2017-10-10T13:41:44.276758: step 6601, loss 0.0736223, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:44.889167: step 6602, loss 0.104129, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:45.556595: step 6603, loss 0.0581763, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:46.008849: step 6604, loss 0.0782093, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:46.469762: step 6605, loss 0.098764, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:46.984064: step 6606, loss 0.11836, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:47.480870: step 6607, loss 0.116924, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:48.005958: step 6608, loss 0.112237, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:48.459133: step 6609, loss 0.0606689, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:48.928180: step 6610, loss 0.141871, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:49.413920: step 6611, loss 0.130493, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:50.011356: step 6612, loss 0.0579881, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:50.578500: step 6613, loss 0.098031, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:51.048998: step 6614, loss 0.148107, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:51.528950: step 6615, loss 0.100891, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:52.023979: step 6616, loss 0.0862449, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:52.604355: step 6617, loss 0.0592968, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:53.088932: step 6618, loss 0.0405824, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:53.608975: step 6619, loss 0.0998372, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:54.121029: step 6620, loss 0.0488584, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:54.698787: step 6621, loss 0.100017, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:55.333224: step 6622, loss 0.100839, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:55.796947: step 6623, loss 0.112616, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:56.189533: step 6624, loss 0.0427175, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:56.662575: step 6625, loss 0.135094, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:57.191852: step 6626, loss 0.16683, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:57.669717: step 6627, loss 0.129783, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:58.215540: step 6628, loss 0.127055, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:58.718966: step 6629, loss 0.0665918, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:59.316177: step 6630, loss 0.0731379, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:59.856876: step 6631, loss 0.108963, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:00.317028: step 6632, loss 0.114019, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:00.821078: step 6633, loss 0.0818435, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:01.346783: step 6634, loss 0.106542, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:01.847366: step 6635, loss 0.138626, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:02.364916: step 6636, loss 0.132236, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:02.912856: step 6637, loss 0.0898029, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:03.420856: step 6638, loss 0.0793583, acc 1, learning_rate 0.0001
2017-10-10T13:42:03.960880: step 6639, loss 0.052203, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:04.486136: step 6640, loss 0.0225457, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:05.479925: step 6640, loss 0.202348, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6640

2017-10-10T13:42:07.069510: step 6641, loss 0.0564021, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:07.589751: step 6642, loss 0.222073, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:08.209010: step 6643, loss 0.165148, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:08.657333: step 6644, loss 0.0616569, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:09.144916: step 6645, loss 0.0822157, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:09.740852: step 6646, loss 0.11041, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:10.340898: step 6647, loss 0.119234, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:11.148839: step 6648, loss 0.138546, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:11.709107: step 6649, loss 0.117148, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:12.272097: step 6650, loss 0.127618, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:12.840443: step 6651, loss 0.0700892, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:13.436915: step 6652, loss 0.0357973, acc 1, learning_rate 0.0001
2017-10-10T13:42:13.994209: step 6653, loss 0.101874, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:14.440247: step 6654, loss 0.0755483, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:14.894751: step 6655, loss 0.0705482, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:15.372945: step 6656, loss 0.0907888, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:15.885106: step 6657, loss 0.0283731, acc 1, learning_rate 0.0001
2017-10-10T13:42:16.424895: step 6658, loss 0.122493, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:16.962397: step 6659, loss 0.0459917, acc 1, learning_rate 0.0001
2017-10-10T13:42:17.431543: step 6660, loss 0.0620332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:17.949034: step 6661, loss 0.128767, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:18.557028: step 6662, loss 0.0446233, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:18.988853: step 6663, loss 0.139353, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:19.362364: step 6664, loss 0.0967283, acc 0.941176, learning_rate 0.0001
2017-10-10T13:42:19.793153: step 6665, loss 0.180993, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:20.348998: step 6666, loss 0.0778482, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:20.892841: step 6667, loss 0.076687, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:21.492881: step 6668, loss 0.0813754, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:22.060928: step 6669, loss 0.120999, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:22.564869: step 6670, loss 0.162271, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:23.029193: step 6671, loss 0.0973561, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:23.581113: step 6672, loss 0.0994731, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:24.156905: step 6673, loss 0.0405831, acc 1, learning_rate 0.0001
2017-10-10T13:42:24.728884: step 6674, loss 0.143175, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:25.248976: step 6675, loss 0.108293, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:25.717083: step 6676, loss 0.0967156, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:26.216176: step 6677, loss 0.0818865, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:26.612889: step 6678, loss 0.207664, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:27.136293: step 6679, loss 0.0379988, acc 1, learning_rate 0.0001
2017-10-10T13:42:27.698306: step 6680, loss 0.103376, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:28.884829: step 6680, loss 0.206101, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6680

2017-10-10T13:42:30.613312: step 6681, loss 0.107071, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:31.076127: step 6682, loss 0.178815, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:31.461005: step 6683, loss 0.140573, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:31.903444: step 6684, loss 0.164401, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:32.451022: step 6685, loss 0.0937024, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:32.981682: step 6686, loss 0.127482, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:33.494583: step 6687, loss 0.0868808, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:33.965544: step 6688, loss 0.0691418, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:34.428960: step 6689, loss 0.0944872, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:35.032863: step 6690, loss 0.0938774, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:35.541094: step 6691, loss 0.0607438, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:36.050855: step 6692, loss 0.089365, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:36.721189: step 6693, loss 0.0521838, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:37.204822: step 6694, loss 0.0609724, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:37.676777: step 6695, loss 0.0853944, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:38.225808: step 6696, loss 0.0930191, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:38.750924: step 6697, loss 0.141389, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:39.279303: step 6698, loss 0.0789498, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:39.772832: step 6699, loss 0.0636969, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:40.328837: step 6700, loss 0.0688368, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:40.876440: step 6701, loss 0.052954, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:41.524931: step 6702, loss 0.0737972, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:41.952510: step 6703, loss 0.0519463, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:42.380483: step 6704, loss 0.0978269, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:42.856336: step 6705, loss 0.0296927, acc 1, learning_rate 0.0001
2017-10-10T13:42:43.409575: step 6706, loss 0.100993, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:43.928929: step 6707, loss 0.0530993, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:44.508849: step 6708, loss 0.100211, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:45.049145: step 6709, loss 0.144223, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:45.569013: step 6710, loss 0.0315197, acc 1, learning_rate 0.0001
2017-10-10T13:42:46.099854: step 6711, loss 0.0657266, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:46.664886: step 6712, loss 0.0324934, acc 1, learning_rate 0.0001
2017-10-10T13:42:47.173076: step 6713, loss 0.0648466, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:47.689356: step 6714, loss 0.0954318, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:48.232259: step 6715, loss 0.160036, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:48.744147: step 6716, loss 0.112259, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:49.233031: step 6717, loss 0.0909869, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:49.753076: step 6718, loss 0.0718268, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:50.305122: step 6719, loss 0.0422499, acc 1, learning_rate 0.0001
2017-10-10T13:42:50.811619: step 6720, loss 0.0913836, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:51.957027: step 6720, loss 0.204406, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6720

2017-10-10T13:42:53.700115: step 6721, loss 0.063332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:54.104595: step 6722, loss 0.0635466, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:54.540471: step 6723, loss 0.0852933, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:54.932844: step 6724, loss 0.139483, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:55.451355: step 6725, loss 0.0169631, acc 1, learning_rate 0.0001
2017-10-10T13:42:56.060923: step 6726, loss 0.0563429, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:56.613017: step 6727, loss 0.0469948, acc 1, learning_rate 0.0001
2017-10-10T13:42:57.158466: step 6728, loss 0.0875231, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:57.727004: step 6729, loss 0.101775, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:58.265459: step 6730, loss 0.0584924, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:58.764985: step 6731, loss 0.10525, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:59.421233: step 6732, loss 0.0668051, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:59.991487: step 6733, loss 0.122557, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:00.439589: step 6734, loss 0.0350798, acc 1, learning_rate 0.0001
2017-10-10T13:43:00.910512: step 6735, loss 0.127894, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:01.476504: step 6736, loss 0.116081, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:01.993830: step 6737, loss 0.117761, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:02.525994: step 6738, loss 0.0806044, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:03.114726: step 6739, loss 0.0644727, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:03.688866: step 6740, loss 0.0401966, acc 1, learning_rate 0.0001
2017-10-10T13:43:04.277145: step 6741, loss 0.0682774, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:04.751062: step 6742, loss 0.0712596, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:05.229061: step 6743, loss 0.0518316, acc 1, learning_rate 0.0001
2017-10-10T13:43:05.773435: step 6744, loss 0.169196, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:06.284883: step 6745, loss 0.0944462, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:06.847088: step 6746, loss 0.097221, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:07.325060: step 6747, loss 0.211518, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:07.734994: step 6748, loss 0.044412, acc 1, learning_rate 0.0001
2017-10-10T13:43:08.205159: step 6749, loss 0.0870723, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:08.740987: step 6750, loss 0.142098, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:09.249161: step 6751, loss 0.0629539, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:09.766902: step 6752, loss 0.103755, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:10.240535: step 6753, loss 0.0298356, acc 1, learning_rate 0.0001
2017-10-10T13:43:10.792037: step 6754, loss 0.110558, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:11.361199: step 6755, loss 0.042068, acc 1, learning_rate 0.0001
2017-10-10T13:43:11.852577: step 6756, loss 0.0750194, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:12.391495: step 6757, loss 0.081816, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:12.857380: step 6758, loss 0.1105, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:13.402394: step 6759, loss 0.186418, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:13.845130: step 6760, loss 0.146989, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:15.020980: step 6760, loss 0.20057, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6760

2017-10-10T13:43:16.430140: step 6761, loss 0.0878541, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:16.770492: step 6762, loss 0.186127, acc 0.941176, learning_rate 0.0001
2017-10-10T13:43:17.227675: step 6763, loss 0.0927627, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:17.733188: step 6764, loss 0.138995, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:18.232911: step 6765, loss 0.0792555, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:18.795730: step 6766, loss 0.111086, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:19.332704: step 6767, loss 0.0358873, acc 1, learning_rate 0.0001
2017-10-10T13:43:19.862617: step 6768, loss 0.104625, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:20.429370: step 6769, loss 0.165832, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:21.014757: step 6770, loss 0.105137, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:21.508181: step 6771, loss 0.0639027, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:22.147655: step 6772, loss 0.0801579, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:22.716905: step 6773, loss 0.0622809, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:23.307113: step 6774, loss 0.172307, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:23.742064: step 6775, loss 0.0307791, acc 1, learning_rate 0.0001
2017-10-10T13:43:24.185078: step 6776, loss 0.0799162, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:24.676894: step 6777, loss 0.0703953, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:25.156925: step 6778, loss 0.101352, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:25.707081: step 6779, loss 0.0440809, acc 1, learning_rate 0.0001
2017-10-10T13:43:26.224853: step 6780, loss 0.186658, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:26.748955: step 6781, loss 0.0577587, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:27.266205: step 6782, loss 0.124735, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:27.792884: step 6783, loss 0.0865094, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:28.212823: step 6784, loss 0.0906455, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:28.720110: step 6785, loss 0.069161, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:29.301129: step 6786, loss 0.0900113, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:29.849127: step 6787, loss 0.0361907, acc 1, learning_rate 0.0001
2017-10-10T13:43:30.397474: step 6788, loss 0.0573896, acc 1, learning_rate 0.0001
2017-10-10T13:43:30.881090: step 6789, loss 0.149132, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:31.376978: step 6790, loss 0.0437959, acc 1, learning_rate 0.0001
2017-10-10T13:43:31.916462: step 6791, loss 0.0508993, acc 1, learning_rate 0.0001
2017-10-10T13:43:32.407371: step 6792, loss 0.132168, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:32.936995: step 6793, loss 0.160018, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:33.521355: step 6794, loss 0.113189, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:34.024279: step 6795, loss 0.152403, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:34.513215: step 6796, loss 0.0925386, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:35.017317: step 6797, loss 0.0820445, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:35.507232: step 6798, loss 0.0276432, acc 1, learning_rate 0.0001
2017-10-10T13:43:36.048415: step 6799, loss 0.131266, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:36.561136: step 6800, loss 0.0989264, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:37.905124: step 6800, loss 0.200679, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6800

2017-10-10T13:43:39.448968: step 6801, loss 0.0457016, acc 1, learning_rate 0.0001
2017-10-10T13:43:39.894763: step 6802, loss 0.0828879, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:40.389079: step 6803, loss 0.0562741, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:40.956966: step 6804, loss 0.066486, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:41.512862: step 6805, loss 0.0544726, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:42.031464: step 6806, loss 0.147825, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:42.572934: step 6807, loss 0.0874285, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:43.112453: step 6808, loss 0.0742265, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:43.644152: step 6809, loss 0.12923, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:44.205120: step 6810, loss 0.0697311, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:44.712919: step 6811, loss 0.0475672, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:45.276850: step 6812, loss 0.1213, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:45.804132: step 6813, loss 0.0613744, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:46.265068: step 6814, loss 0.050438, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:46.884870: step 6815, loss 0.0687952, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:47.411873: step 6816, loss 0.0708546, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:47.864088: step 6817, loss 0.1498, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:48.358998: step 6818, loss 0.066064, acc 1, learning_rate 0.0001
2017-10-10T13:43:48.881013: step 6819, loss 0.0775222, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:49.419814: step 6820, loss 0.0542062, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:50.061397: step 6821, loss 0.0881009, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:50.504871: step 6822, loss 0.0688011, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:50.944186: step 6823, loss 0.0991548, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:51.351598: step 6824, loss 0.0870323, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:51.814692: step 6825, loss 0.0965218, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:52.326090: step 6826, loss 0.168845, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:52.869574: step 6827, loss 0.0964903, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:53.424754: step 6828, loss 0.0451058, acc 1, learning_rate 0.0001
2017-10-10T13:43:53.953451: step 6829, loss 0.0560201, acc 1, learning_rate 0.0001
2017-10-10T13:43:54.501202: step 6830, loss 0.0510504, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:55.048441: step 6831, loss 0.0467043, acc 1, learning_rate 0.0001
2017-10-10T13:43:55.597236: step 6832, loss 0.101934, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:56.108920: step 6833, loss 0.0551178, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:56.664831: step 6834, loss 0.0804568, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:57.220300: step 6835, loss 0.048337, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:57.776382: step 6836, loss 0.0585548, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:58.337579: step 6837, loss 0.147255, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:58.883860: step 6838, loss 0.0542168, acc 1, learning_rate 0.0001
2017-10-10T13:43:59.424651: step 6839, loss 0.07878, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:59.945104: step 6840, loss 0.192375, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:44:01.116780: step 6840, loss 0.201796, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6840

2017-10-10T13:44:03.024872: step 6841, loss 0.105653, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:03.530019: step 6842, loss 0.0817248, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:04.052959: step 6843, loss 0.134487, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:04.622489: step 6844, loss 0.0868602, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:05.208945: step 6845, loss 0.100865, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:05.756258: step 6846, loss 0.106511, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:06.280836: step 6847, loss 0.0684433, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:06.867880: step 6848, loss 0.110876, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:07.364811: step 6849, loss 0.11663, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:07.900850: step 6850, loss 0.187346, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:08.460877: step 6851, loss 0.204318, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:08.981723: step 6852, loss 0.052101, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:09.460895: step 6853, loss 0.0479915, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:10.054409: step 6854, loss 0.0943136, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:10.506500: step 6855, loss 0.203025, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:10.997175: step 6856, loss 0.116555, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:11.397761: step 6857, loss 0.0737574, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:11.893959: step 6858, loss 0.0991003, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:12.405027: step 6859, loss 0.100216, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:12.833185: step 6860, loss 0.0361498, acc 1, learning_rate 0.0001
2017-10-10T13:44:13.405824: step 6861, loss 0.0759855, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:13.844182: step 6862, loss 0.122506, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:14.268807: step 6863, loss 0.11232, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:14.816923: step 6864, loss 0.0475862, acc 1, learning_rate 0.0001
2017-10-10T13:44:15.352344: step 6865, loss 0.100186, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:15.832896: step 6866, loss 0.0485776, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:16.384910: step 6867, loss 0.102695, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:16.900846: step 6868, loss 0.11869, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:17.426539: step 6869, loss 0.105702, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:17.935684: step 6870, loss 0.0838172, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:18.457529: step 6871, loss 0.0387885, acc 1, learning_rate 0.0001
2017-10-10T13:44:18.936864: step 6872, loss 0.0683723, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:19.465067: step 6873, loss 0.111278, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:19.976851: step 6874, loss 0.0823055, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:20.480912: step 6875, loss 0.165551, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:21.021721: step 6876, loss 0.173569, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:21.500173: step 6877, loss 0.0828325, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:22.036965: step 6878, loss 0.1719, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:22.564870: step 6879, loss 0.095833, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:23.007039: step 6880, loss 0.0655667, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:44:43.095654: step 6880, loss 0.203542, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6880

2017-10-10T13:45:31.527321: step 6881, loss 0.0540578, acc 1, learning_rate 0.0001
2017-10-10T13:45:38.097416: step 6882, loss 0.102884, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:39.999656: step 6883, loss 0.116952, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:40.550125: step 6884, loss 0.107372, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:40.805069: step 6885, loss 0.0952748, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:42.870836: step 6886, loss 0.118565, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:43.033661: step 6887, loss 0.116445, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:43.680325: step 6888, loss 0.145303, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:43.849130: step 6889, loss 0.167519, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:44.198316: step 6890, loss 0.14631, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:44.362804: step 6891, loss 0.0618301, acc 1, learning_rate 0.0001
2017-10-10T13:45:44.530694: step 6892, loss 0.017529, acc 1, learning_rate 0.0001
2017-10-10T13:45:44.697971: step 6893, loss 0.0743969, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:45.329958: step 6894, loss 0.115014, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:45.545618: step 6895, loss 0.070838, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:45.712495: step 6896, loss 0.0204494, acc 1, learning_rate 0.0001
2017-10-10T13:45:46.089591: step 6897, loss 0.0854891, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:46.687365: step 6898, loss 0.07172, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:47.010837: step 6899, loss 0.0347036, acc 1, learning_rate 0.0001
2017-10-10T13:45:47.173600: step 6900, loss 0.0857487, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:47.471635: step 6901, loss 0.076619, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:47.635035: step 6902, loss 0.1278, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:48.299383: step 6903, loss 0.134, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:48.522268: step 6904, loss 0.0616483, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:48.689146: step 6905, loss 0.0706115, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:48.852512: step 6906, loss 0.121404, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:49.124854: step 6907, loss 0.077398, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:49.402923: step 6908, loss 0.0717216, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:49.572561: step 6909, loss 0.132813, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:50.316858: step 6910, loss 0.0735654, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:50.592149: step 6911, loss 0.088094, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:50.853239: step 6912, loss 0.112147, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:51.034148: step 6913, loss 0.107533, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:51.592226: step 6914, loss 0.077118, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:51.756174: step 6915, loss 0.074996, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:52.044394: step 6916, loss 0.0793924, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:52.316894: step 6917, loss 0.0520871, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:52.508697: step 6918, loss 0.0744194, acc 1, learning_rate 0.0001
2017-10-10T13:45:52.672396: step 6919, loss 0.0713374, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:52.835811: step 6920, loss 0.124775, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:45:53.440925: step 6920, loss 0.202411, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6920

2017-10-10T13:46:20.309983: step 6921, loss 0.0764976, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:21.470741: step 6922, loss 0.0450507, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:21.717435: step 6923, loss 0.137335, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:21.953092: step 6924, loss 0.0472469, acc 1, learning_rate 0.0001
2017-10-10T13:46:22.226875: step 6925, loss 0.108414, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:22.984984: step 6926, loss 0.0666472, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:23.273007: step 6927, loss 0.158666, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:23.489109: step 6928, loss 0.0464137, acc 1, learning_rate 0.0001
2017-10-10T13:46:23.847045: step 6929, loss 0.0818268, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:24.272893: step 6930, loss 0.134904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:24.686395: step 6931, loss 0.0435608, acc 1, learning_rate 0.0001
2017-10-10T13:46:24.871921: step 6932, loss 0.0826483, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:25.048962: step 6933, loss 0.175555, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:25.236278: step 6934, loss 0.0938883, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:25.416863: step 6935, loss 0.0899079, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:25.587685: step 6936, loss 0.0602889, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:25.892376: step 6937, loss 0.0941552, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:26.185192: step 6938, loss 0.0803298, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:26.463078: step 6939, loss 0.110134, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:26.769694: step 6940, loss 0.0866881, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:27.067604: step 6941, loss 0.0869459, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:27.354542: step 6942, loss 0.0735201, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:27.748847: step 6943, loss 0.177246, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:27.997021: step 6944, loss 0.0982306, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:28.324611: step 6945, loss 0.122486, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:28.625680: step 6946, loss 0.0868719, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:28.900499: step 6947, loss 0.0559327, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:29.170689: step 6948, loss 0.0295975, acc 1, learning_rate 0.0001
2017-10-10T13:46:29.444814: step 6949, loss 0.0661935, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:29.727545: step 6950, loss 0.172733, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:30.015770: step 6951, loss 0.0603645, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:30.301755: step 6952, loss 0.112403, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:30.584783: step 6953, loss 0.136618, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:30.862842: step 6954, loss 0.109957, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:31.138901: step 6955, loss 0.0631492, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:31.425287: step 6956, loss 0.0641932, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:31.708377: step 6957, loss 0.0500869, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:31.967559: step 6958, loss 0.129867, acc 0.901961, learning_rate 0.0001
2017-10-10T13:46:32.249151: step 6959, loss 0.0704692, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:32.552520: step 6960, loss 0.1002, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:46:33.187734: step 6960, loss 0.203132, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-6960

2017-10-10T13:46:34.416915: step 6961, loss 0.109055, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:34.706083: step 6962, loss 0.171749, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:35.007706: step 6963, loss 0.0736662, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:35.293302: step 6964, loss 0.104761, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:35.592509: step 6965, loss 0.124472, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:35.892348: step 6966, loss 0.129736, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:36.185505: step 6967, loss 0.0831326, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:36.477444: step 6968, loss 0.0253624, acc 1, learning_rate 0.0001
2017-10-10T13:46:36.856865: step 6969, loss 0.117366, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:37.159645: step 6970, loss 0.0786588, acc 1, learning_rate 0.0001
2017-10-10T13:46:37.350691: step 6971, loss 0.0621639, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:37.531101: step 6972, loss 0.0834392, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:37.709134: step 6973, loss 0.0378263, acc 1, learning_rate 0.0001
2017-10-10T13:46:37.886338: step 6974, loss 0.0144919, acc 1, learning_rate 0.0001
2017-10-10T13:46:38.060648: step 6975, loss 0.0511365, acc 1, learning_rate 0.0001
2017-10-10T13:46:38.335349: step 6976, loss 0.0275175, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:38.629636: step 6977, loss 0.0855003, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:38.925888: step 6978, loss 0.120449, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:39.222548: step 6979, loss 0.140067, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:39.517790: step 6980, loss 0.143456, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:39.788210: step 6981, loss 0.0690332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:40.075408: step 6982, loss 0.130951, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:40.351443: step 6983, loss 0.123018, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:40.631038: step 6984, loss 0.0925748, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:40.916474: step 6985, loss 0.133221, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:41.195262: step 6986, loss 0.0499991, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:41.471869: step 6987, loss 0.0377321, acc 1, learning_rate 0.0001
2017-10-10T13:46:41.772545: step 6988, loss 0.0724584, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:42.060954: step 6989, loss 0.11419, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:42.348002: step 6990, loss 0.105687, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:42.648488: step 6991, loss 0.0813729, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:42.934494: step 6992, loss 0.152192, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:43.211554: step 6993, loss 0.0255354, acc 1, learning_rate 0.0001
2017-10-10T13:46:43.494772: step 6994, loss 0.103097, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:43.777711: step 6995, loss 0.0480253, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:44.068317: step 6996, loss 0.0439709, acc 1, learning_rate 0.0001
2017-10-10T13:46:44.348060: step 6997, loss 0.179395, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:44.631364: step 6998, loss 0.0853853, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:45.220347: step 6999, loss 0.0882225, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:45.495286: step 7000, loss 0.0987408, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:46:46.164351: step 7000, loss 0.203712, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7000

2017-10-10T13:46:47.046825: step 7001, loss 0.068602, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:47.305517: step 7002, loss 0.0233733, acc 1, learning_rate 0.0001
2017-10-10T13:46:47.585549: step 7003, loss 0.125292, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:47.883624: step 7004, loss 0.125408, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:48.183378: step 7005, loss 0.0373114, acc 1, learning_rate 0.0001
2017-10-10T13:46:48.462249: step 7006, loss 0.068305, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:48.728131: step 7007, loss 0.0372491, acc 1, learning_rate 0.0001
2017-10-10T13:46:49.026926: step 7008, loss 0.0474695, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:49.364121: step 7009, loss 0.107844, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:49.713048: step 7010, loss 0.0357753, acc 1, learning_rate 0.0001
2017-10-10T13:46:49.888176: step 7011, loss 0.101582, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:50.063878: step 7012, loss 0.0709591, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:50.238434: step 7013, loss 0.0753771, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:50.411816: step 7014, loss 0.0974955, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:50.662024: step 7015, loss 0.0849451, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:50.890651: step 7016, loss 0.0896493, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:51.153835: step 7017, loss 0.0505933, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:51.407244: step 7018, loss 0.115484, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:51.664872: step 7019, loss 0.10894, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:51.930660: step 7020, loss 0.0420067, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:52.211122: step 7021, loss 0.137863, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:52.509832: step 7022, loss 0.0675704, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:52.786395: step 7023, loss 0.0974253, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:53.099825: step 7024, loss 0.092373, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:53.421944: step 7025, loss 0.131144, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:53.830671: step 7026, loss 0.0565449, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:54.259827: step 7027, loss 0.169199, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:54.656366: step 7028, loss 0.146649, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:54.927474: step 7029, loss 0.0784178, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:55.350040: step 7030, loss 0.0530022, acc 1, learning_rate 0.0001
2017-10-10T13:46:55.769460: step 7031, loss 0.129481, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:56.171130: step 7032, loss 0.0404689, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:56.593377: step 7033, loss 0.0987811, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:56.998914: step 7034, loss 0.15846, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:57.413052: step 7035, loss 0.095021, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:57.780671: step 7036, loss 0.131529, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:58.087453: step 7037, loss 0.0665851, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:58.469070: step 7038, loss 0.0801912, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:58.843058: step 7039, loss 0.0791869, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:59.255993: step 7040, loss 0.0477768, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:00.155993: step 7040, loss 0.203094, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7040

2017-10-10T13:47:01.572983: step 7041, loss 0.0530011, acc 1, learning_rate 0.0001
2017-10-10T13:47:02.023195: step 7042, loss 0.0512611, acc 1, learning_rate 0.0001
2017-10-10T13:47:02.403242: step 7043, loss 0.0639371, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:02.803991: step 7044, loss 0.0662818, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:03.528358: step 7045, loss 0.117851, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:03.938003: step 7046, loss 0.118656, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:04.351120: step 7047, loss 0.0632761, acc 1, learning_rate 0.0001
2017-10-10T13:47:04.736882: step 7048, loss 0.057691, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:05.072987: step 7049, loss 0.0560123, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:05.569114: step 7050, loss 0.0535402, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:05.909048: step 7051, loss 0.112461, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:06.154104: step 7052, loss 0.0395516, acc 1, learning_rate 0.0001
2017-10-10T13:47:06.449398: step 7053, loss 0.0758742, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:06.747739: step 7054, loss 0.118906, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:07.173937: step 7055, loss 0.216565, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:07.555832: step 7056, loss 0.0905176, acc 0.960784, learning_rate 0.0001
2017-10-10T13:47:07.965203: step 7057, loss 0.0607041, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:08.377204: step 7058, loss 0.129101, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:08.798839: step 7059, loss 0.0415283, acc 1, learning_rate 0.0001
2017-10-10T13:47:09.190849: step 7060, loss 0.104931, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:09.592838: step 7061, loss 0.0493722, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:10.004096: step 7062, loss 0.0177815, acc 1, learning_rate 0.0001
2017-10-10T13:47:10.409418: step 7063, loss 0.0220591, acc 1, learning_rate 0.0001
2017-10-10T13:47:10.887174: step 7064, loss 0.0257417, acc 1, learning_rate 0.0001
2017-10-10T13:47:11.184842: step 7065, loss 0.0906563, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:11.460805: step 7066, loss 0.0339889, acc 1, learning_rate 0.0001
2017-10-10T13:47:11.746819: step 7067, loss 0.117237, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:12.007238: step 7068, loss 0.10209, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:12.452943: step 7069, loss 0.0927624, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:12.804929: step 7070, loss 0.0671731, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:13.196126: step 7071, loss 0.0993974, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:13.565049: step 7072, loss 0.0419309, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:13.942697: step 7073, loss 0.0831435, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:14.319372: step 7074, loss 0.0654449, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:14.672814: step 7075, loss 0.136858, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:15.043019: step 7076, loss 0.0507499, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:15.505162: step 7077, loss 0.0364684, acc 1, learning_rate 0.0001
2017-10-10T13:47:15.868677: step 7078, loss 0.087073, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:16.236286: step 7079, loss 0.106282, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:16.650975: step 7080, loss 0.218004, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:17.449201: step 7080, loss 0.202055, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7080

2017-10-10T13:47:18.986032: step 7081, loss 0.0645822, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:19.404631: step 7082, loss 0.0807929, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:19.804982: step 7083, loss 0.065771, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:20.252832: step 7084, loss 0.0749636, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:20.644875: step 7085, loss 0.0747285, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:21.066721: step 7086, loss 0.106073, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:21.456839: step 7087, loss 0.0585295, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:21.870444: step 7088, loss 0.0476838, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:22.284148: step 7089, loss 0.0810524, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:22.691525: step 7090, loss 0.0940312, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:23.178263: step 7091, loss 0.0818801, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:23.516393: step 7092, loss 0.110454, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:23.825578: step 7093, loss 0.063489, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:24.147623: step 7094, loss 0.0591536, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:24.465990: step 7095, loss 0.0683272, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:24.890552: step 7096, loss 0.0711188, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:25.287460: step 7097, loss 0.0773568, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:25.700137: step 7098, loss 0.126752, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:26.094994: step 7099, loss 0.130114, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:26.480820: step 7100, loss 0.0669735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:26.904920: step 7101, loss 0.101811, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:27.324955: step 7102, loss 0.0412205, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:27.730733: step 7103, loss 0.100376, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:28.137018: step 7104, loss 0.114535, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:28.600883: step 7105, loss 0.0487058, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:28.934314: step 7106, loss 0.140963, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:29.250535: step 7107, loss 0.0734751, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:29.588300: step 7108, loss 0.0357588, acc 1, learning_rate 0.0001
2017-10-10T13:47:29.897672: step 7109, loss 0.12306, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:30.321212: step 7110, loss 0.111584, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:30.737066: step 7111, loss 0.0395929, acc 1, learning_rate 0.0001
2017-10-10T13:47:31.137081: step 7112, loss 0.0638676, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:31.532836: step 7113, loss 0.124103, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:31.936319: step 7114, loss 0.132243, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:32.264077: step 7115, loss 0.0810341, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:32.591190: step 7116, loss 0.143619, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:32.995325: step 7117, loss 0.046077, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:33.352857: step 7118, loss 0.0642967, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:33.745888: step 7119, loss 0.0843179, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:34.157119: step 7120, loss 0.0865895, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:35.039533: step 7120, loss 0.200502, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7120

2017-10-10T13:47:36.213118: step 7121, loss 0.071199, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:36.572993: step 7122, loss 0.116021, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:37.025135: step 7123, loss 0.0679496, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:37.468904: step 7124, loss 0.033904, acc 1, learning_rate 0.0001
2017-10-10T13:47:37.864532: step 7125, loss 0.108679, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:38.286856: step 7126, loss 0.10655, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:38.673513: step 7127, loss 0.0628612, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:39.072933: step 7128, loss 0.0459982, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:39.482550: step 7129, loss 0.10524, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:39.954901: step 7130, loss 0.0783968, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:40.368912: step 7131, loss 0.135337, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:40.868463: step 7132, loss 0.0527718, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:41.280373: step 7133, loss 0.105966, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:41.541953: step 7134, loss 0.0993313, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:41.857709: step 7135, loss 0.136109, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:42.215494: step 7136, loss 0.12573, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:42.614320: step 7137, loss 0.111945, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:43.026547: step 7138, loss 0.113806, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:43.432836: step 7139, loss 0.0547405, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:43.888852: step 7140, loss 0.0527229, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:44.308128: step 7141, loss 0.139298, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:44.704999: step 7142, loss 0.110886, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:45.114544: step 7143, loss 0.106546, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:45.527519: step 7144, loss 0.103575, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:45.955136: step 7145, loss 0.089111, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:46.436019: step 7146, loss 0.141965, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:46.764816: step 7147, loss 0.10782, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:47.068249: step 7148, loss 0.092954, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:47.464171: step 7149, loss 0.108139, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:47.845810: step 7150, loss 0.0714135, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:48.163985: step 7151, loss 0.106339, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:48.534323: step 7152, loss 0.150691, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:48.921257: step 7153, loss 0.0926832, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:49.212865: step 7154, loss 0.0879113, acc 0.960784, learning_rate 0.0001
2017-10-10T13:47:49.617215: step 7155, loss 0.11896, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:50.069281: step 7156, loss 0.124419, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:50.434706: step 7157, loss 0.0643615, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:50.868919: step 7158, loss 0.0552287, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:51.250494: step 7159, loss 0.114005, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:51.652183: step 7160, loss 0.0815155, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:52.552793: step 7160, loss 0.203933, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7160

2017-10-10T13:47:53.951771: step 7161, loss 0.0407142, acc 1, learning_rate 0.0001
2017-10-10T13:47:54.341970: step 7162, loss 0.0854534, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:54.748855: step 7163, loss 0.076421, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:55.161715: step 7164, loss 0.0399383, acc 1, learning_rate 0.0001
2017-10-10T13:47:55.564206: step 7165, loss 0.107938, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:55.981794: step 7166, loss 0.0937019, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:56.390759: step 7167, loss 0.11747, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:56.803536: step 7168, loss 0.100214, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:57.210121: step 7169, loss 0.0856516, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:57.626397: step 7170, loss 0.115356, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:58.049964: step 7171, loss 0.13516, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:58.528865: step 7172, loss 0.0700808, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:58.972789: step 7173, loss 0.0362387, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:59.276857: step 7174, loss 0.0371469, acc 1, learning_rate 0.0001
2017-10-10T13:47:59.573679: step 7175, loss 0.0401148, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:59.883360: step 7176, loss 0.0369514, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:00.325224: step 7177, loss 0.0322002, acc 1, learning_rate 0.0001
2017-10-10T13:48:00.733928: step 7178, loss 0.0578667, acc 1, learning_rate 0.0001
2017-10-10T13:48:01.147722: step 7179, loss 0.0691645, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:01.569154: step 7180, loss 0.0582162, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:01.994446: step 7181, loss 0.139644, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:02.394234: step 7182, loss 0.0886153, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:02.799599: step 7183, loss 0.0715594, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:03.225613: step 7184, loss 0.0751456, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:03.749460: step 7185, loss 0.0713767, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:04.156450: step 7186, loss 0.0531944, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:04.482631: step 7187, loss 0.115685, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:04.791076: step 7188, loss 0.0610442, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:05.133054: step 7189, loss 0.0651244, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:05.555728: step 7190, loss 0.105214, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:05.950099: step 7191, loss 0.121445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:06.289029: step 7192, loss 0.156885, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:06.728903: step 7193, loss 0.162919, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:07.128917: step 7194, loss 0.05085, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:07.516969: step 7195, loss 0.0780121, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:07.924847: step 7196, loss 0.135257, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:08.276923: step 7197, loss 0.135436, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:08.596996: step 7198, loss 0.114122, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:08.982506: step 7199, loss 0.0769443, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:09.369105: step 7200, loss 0.0968251, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:10.222398: step 7200, loss 0.201105, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7200

2017-10-10T13:48:11.671221: step 7201, loss 0.0751777, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:12.066712: step 7202, loss 0.135439, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:12.481796: step 7203, loss 0.10137, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:12.864838: step 7204, loss 0.114303, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:13.244934: step 7205, loss 0.079597, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:13.674559: step 7206, loss 0.152375, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:14.111204: step 7207, loss 0.058082, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:14.521878: step 7208, loss 0.0429722, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:14.924576: step 7209, loss 0.0412723, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:15.324769: step 7210, loss 0.120755, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:15.692856: step 7211, loss 0.12278, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:16.088854: step 7212, loss 0.0872565, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:16.480913: step 7213, loss 0.102445, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:16.937258: step 7214, loss 0.0953437, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:17.268999: step 7215, loss 0.146795, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:17.600840: step 7216, loss 0.136275, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:17.915062: step 7217, loss 0.185115, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:18.220293: step 7218, loss 0.106125, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:18.575182: step 7219, loss 0.0965812, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:18.990201: step 7220, loss 0.104877, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:19.362169: step 7221, loss 0.0543012, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:19.780840: step 7222, loss 0.101345, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:20.207615: step 7223, loss 0.0531656, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:20.588839: step 7224, loss 0.145145, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:21.084987: step 7225, loss 0.0356199, acc 1, learning_rate 0.0001
2017-10-10T13:48:21.366846: step 7226, loss 0.124215, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:21.657461: step 7227, loss 0.0476527, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:21.933986: step 7228, loss 0.0509738, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:22.192404: step 7229, loss 0.0700565, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:22.556861: step 7230, loss 0.103584, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:22.976851: step 7231, loss 0.0390755, acc 1, learning_rate 0.0001
2017-10-10T13:48:23.392252: step 7232, loss 0.074141, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:23.811010: step 7233, loss 0.0726586, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:24.247395: step 7234, loss 0.170246, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:24.657389: step 7235, loss 0.116432, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:25.063322: step 7236, loss 0.126119, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:25.471415: step 7237, loss 0.121363, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:25.840821: step 7238, loss 0.0688116, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:26.192936: step 7239, loss 0.119549, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:26.605693: step 7240, loss 0.0424984, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:27.468900: step 7240, loss 0.202847, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7240

2017-10-10T13:48:28.994015: step 7241, loss 0.049181, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:29.392892: step 7242, loss 0.105245, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:29.819555: step 7243, loss 0.10076, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:30.227792: step 7244, loss 0.0790355, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:30.642087: step 7245, loss 0.0749171, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:31.064845: step 7246, loss 0.165075, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:31.488899: step 7247, loss 0.055877, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:31.880995: step 7248, loss 0.0820736, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:32.286125: step 7249, loss 0.0701687, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:32.715464: step 7250, loss 0.095187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:33.137884: step 7251, loss 0.0544952, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:33.513018: step 7252, loss 0.0724561, acc 0.960784, learning_rate 0.0001
2017-10-10T13:48:33.922432: step 7253, loss 0.104752, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:34.336683: step 7254, loss 0.167751, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:34.740883: step 7255, loss 0.115043, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:35.289119: step 7256, loss 0.0611578, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:35.595658: step 7257, loss 0.245783, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:35.906308: step 7258, loss 0.0679178, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:36.210500: step 7259, loss 0.0778771, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:36.624843: step 7260, loss 0.161323, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:37.028836: step 7261, loss 0.0383997, acc 1, learning_rate 0.0001
2017-10-10T13:48:37.416975: step 7262, loss 0.0552513, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:37.804849: step 7263, loss 0.0574848, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:38.210260: step 7264, loss 0.0462447, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:38.616875: step 7265, loss 0.138667, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:39.147367: step 7266, loss 0.135777, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:39.413978: step 7267, loss 0.0318637, acc 1, learning_rate 0.0001
2017-10-10T13:48:39.701246: step 7268, loss 0.124035, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:40.023758: step 7269, loss 0.0812729, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:40.496175: step 7270, loss 0.0546053, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:40.922115: step 7271, loss 0.0898364, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:41.332843: step 7272, loss 0.124573, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:41.684235: step 7273, loss 0.046264, acc 1, learning_rate 0.0001
2017-10-10T13:48:42.079576: step 7274, loss 0.0858635, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:42.457935: step 7275, loss 0.122445, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:42.785453: step 7276, loss 0.0574273, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:43.181274: step 7277, loss 0.116222, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:43.587693: step 7278, loss 0.125809, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:44.000848: step 7279, loss 0.0152685, acc 1, learning_rate 0.0001
2017-10-10T13:48:44.424363: step 7280, loss 0.127687, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:45.320674: step 7280, loss 0.197947, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7280

2017-10-10T13:48:46.523271: step 7281, loss 0.0456298, acc 1, learning_rate 0.0001
2017-10-10T13:48:46.886282: step 7282, loss 0.0688807, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:47.243739: step 7283, loss 0.050537, acc 1, learning_rate 0.0001
2017-10-10T13:48:47.637133: step 7284, loss 0.0502153, acc 1, learning_rate 0.0001
2017-10-10T13:48:48.063736: step 7285, loss 0.0971194, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:48.507430: step 7286, loss 0.224023, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:48.923372: step 7287, loss 0.0895108, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:49.341939: step 7288, loss 0.0821198, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:49.742504: step 7289, loss 0.135862, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:50.152837: step 7290, loss 0.138151, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:50.561926: step 7291, loss 0.143977, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:50.984799: step 7292, loss 0.0999762, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:51.413789: step 7293, loss 0.192951, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:51.848750: step 7294, loss 0.0289628, acc 1, learning_rate 0.0001
2017-10-10T13:48:52.244034: step 7295, loss 0.104418, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:52.745560: step 7296, loss 0.0767939, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:53.055367: step 7297, loss 0.0910278, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:53.378262: step 7298, loss 0.0472477, acc 1, learning_rate 0.0001
2017-10-10T13:48:53.695419: step 7299, loss 0.0797432, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:54.088836: step 7300, loss 0.107471, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:54.480946: step 7301, loss 0.0794994, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:54.885899: step 7302, loss 0.0843611, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:55.298377: step 7303, loss 0.0372774, acc 1, learning_rate 0.0001
2017-10-10T13:48:55.702279: step 7304, loss 0.0385674, acc 1, learning_rate 0.0001
2017-10-10T13:48:56.100850: step 7305, loss 0.0468797, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:56.528960: step 7306, loss 0.140931, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:57.050741: step 7307, loss 0.0691538, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:57.378140: step 7308, loss 0.0887071, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:57.680175: step 7309, loss 0.0411747, acc 1, learning_rate 0.0001
2017-10-10T13:48:57.991833: step 7310, loss 0.0380627, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:58.402873: step 7311, loss 0.100757, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:58.835420: step 7312, loss 0.0456082, acc 1, learning_rate 0.0001
2017-10-10T13:48:59.266390: step 7313, loss 0.0827226, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:59.687024: step 7314, loss 0.0843425, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:00.103173: step 7315, loss 0.0746052, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:00.507227: step 7316, loss 0.0704854, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:00.936113: step 7317, loss 0.136751, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:01.368628: step 7318, loss 0.0860068, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:01.792710: step 7319, loss 0.0825785, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:02.192430: step 7320, loss 0.15681, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:03.088856: step 7320, loss 0.198648, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7320

2017-10-10T13:49:04.528872: step 7321, loss 0.0634497, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:04.928098: step 7322, loss 0.0676611, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:05.359104: step 7323, loss 0.104004, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:05.772932: step 7324, loss 0.0651194, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:06.205178: step 7325, loss 0.0778788, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:06.619875: step 7326, loss 0.0585271, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:07.036423: step 7327, loss 0.150996, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:07.446919: step 7328, loss 0.0864533, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:07.836687: step 7329, loss 0.127866, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:08.236147: step 7330, loss 0.0512586, acc 1, learning_rate 0.0001
2017-10-10T13:49:08.663224: step 7331, loss 0.0912329, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:09.061157: step 7332, loss 0.135659, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:09.491350: step 7333, loss 0.0508054, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:09.894266: step 7334, loss 0.132792, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:10.381004: step 7335, loss 0.0870173, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:10.834823: step 7336, loss 0.0605594, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:11.160465: step 7337, loss 0.0727097, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:11.468335: step 7338, loss 0.201158, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:11.858110: step 7339, loss 0.0549796, acc 1, learning_rate 0.0001
2017-10-10T13:49:12.244030: step 7340, loss 0.0961298, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:12.652856: step 7341, loss 0.0716101, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:12.985844: step 7342, loss 0.318163, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:13.368873: step 7343, loss 0.0976251, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:13.764326: step 7344, loss 0.0795334, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:14.142102: step 7345, loss 0.0493047, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:14.576518: step 7346, loss 0.0849359, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:15.028987: step 7347, loss 0.168387, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:15.308842: step 7348, loss 0.0615768, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:15.600881: step 7349, loss 0.0579044, acc 1, learning_rate 0.0001
2017-10-10T13:49:15.854417: step 7350, loss 0.0772598, acc 0.980392, learning_rate 0.0001
2017-10-10T13:49:16.107095: step 7351, loss 0.0814375, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:16.460854: step 7352, loss 0.186546, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:16.880020: step 7353, loss 0.0407477, acc 1, learning_rate 0.0001
2017-10-10T13:49:17.285069: step 7354, loss 0.121454, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:17.691350: step 7355, loss 0.0481598, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:18.113060: step 7356, loss 0.0660972, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:18.532136: step 7357, loss 0.129414, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:18.939350: step 7358, loss 0.157367, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:19.358706: step 7359, loss 0.172072, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:19.767464: step 7360, loss 0.0184092, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:20.675399: step 7360, loss 0.198535, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7360

2017-10-10T13:49:22.234148: step 7361, loss 0.0738711, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:22.691030: step 7362, loss 0.0452628, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:23.091753: step 7363, loss 0.0401589, acc 1, learning_rate 0.0001
2017-10-10T13:49:23.510175: step 7364, loss 0.100576, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:23.907026: step 7365, loss 0.0760405, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:24.303519: step 7366, loss 0.0522044, acc 1, learning_rate 0.0001
2017-10-10T13:49:24.696850: step 7367, loss 0.0837882, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:25.124059: step 7368, loss 0.0705703, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:25.528815: step 7369, loss 0.0276964, acc 1, learning_rate 0.0001
2017-10-10T13:49:25.938926: step 7370, loss 0.0229721, acc 1, learning_rate 0.0001
2017-10-10T13:49:26.365346: step 7371, loss 0.12176, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:26.797235: step 7372, loss 0.0610775, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:27.205550: step 7373, loss 0.072538, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:27.584918: step 7374, loss 0.0802547, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:28.088947: step 7375, loss 0.0787528, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:28.346814: step 7376, loss 0.0418581, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:28.644361: step 7377, loss 0.0965595, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:28.912946: step 7378, loss 0.0558201, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:29.196833: step 7379, loss 0.0654297, acc 1, learning_rate 0.0001
2017-10-10T13:49:29.592999: step 7380, loss 0.0530966, acc 1, learning_rate 0.0001
2017-10-10T13:49:29.999673: step 7381, loss 0.0858373, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:30.414872: step 7382, loss 0.0550415, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:30.813152: step 7383, loss 0.131099, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:31.312862: step 7384, loss 0.0568781, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:31.723814: step 7385, loss 0.050239, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:32.157175: step 7386, loss 0.0611235, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:32.637132: step 7387, loss 0.0860414, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:32.922371: step 7388, loss 0.120905, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:33.198142: step 7389, loss 0.0242994, acc 1, learning_rate 0.0001
2017-10-10T13:49:33.469011: step 7390, loss 0.0171867, acc 1, learning_rate 0.0001
2017-10-10T13:49:33.873801: step 7391, loss 0.0846542, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:34.287586: step 7392, loss 0.243175, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:34.686895: step 7393, loss 0.0773858, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:35.083925: step 7394, loss 0.102863, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:35.493452: step 7395, loss 0.116863, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:35.912152: step 7396, loss 0.111823, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:36.334646: step 7397, loss 0.127239, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:36.748872: step 7398, loss 0.130999, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:37.143668: step 7399, loss 0.11225, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:37.564931: step 7400, loss 0.0414512, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:38.444901: step 7400, loss 0.1993, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7400

2017-10-10T13:49:39.657080: step 7401, loss 0.0568916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:40.024939: step 7402, loss 0.0700504, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:40.401898: step 7403, loss 0.129961, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:40.853284: step 7404, loss 0.0572201, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:41.152682: step 7405, loss 0.158909, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:41.552638: step 7406, loss 0.090627, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:41.949519: step 7407, loss 0.0912902, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:42.368221: step 7408, loss 0.0551316, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:42.762983: step 7409, loss 0.12551, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:43.172143: step 7410, loss 0.0593076, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:43.586398: step 7411, loss 0.121347, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:43.980392: step 7412, loss 0.16069, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:44.404853: step 7413, loss 0.0726189, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:44.812530: step 7414, loss 0.151584, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:45.236867: step 7415, loss 0.0405071, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:45.764988: step 7416, loss 0.0842569, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:46.208995: step 7417, loss 0.152861, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:46.473151: step 7418, loss 0.0429202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:46.740853: step 7419, loss 0.112764, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:47.042714: step 7420, loss 0.029874, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:47.464891: step 7421, loss 0.0502046, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:47.872442: step 7422, loss 0.0770916, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:48.279073: step 7423, loss 0.118873, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:48.688828: step 7424, loss 0.0956214, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:49.104837: step 7425, loss 0.0498152, acc 1, learning_rate 0.0001
2017-10-10T13:49:49.521850: step 7426, loss 0.0472416, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:49.960568: step 7427, loss 0.0509987, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:50.428617: step 7428, loss 0.0362595, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:50.744318: step 7429, loss 0.0805971, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:51.066383: step 7430, loss 0.0338176, acc 1, learning_rate 0.0001
2017-10-10T13:49:51.356558: step 7431, loss 0.117809, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:51.736843: step 7432, loss 0.145495, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:52.140868: step 7433, loss 0.112879, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:52.555140: step 7434, loss 0.138014, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:52.960827: step 7435, loss 0.0562814, acc 1, learning_rate 0.0001
2017-10-10T13:49:53.379266: step 7436, loss 0.0892855, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:53.790452: step 7437, loss 0.171944, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:54.210848: step 7438, loss 0.101067, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:54.620396: step 7439, loss 0.112202, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:55.017416: step 7440, loss 0.104654, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:55.883639: step 7440, loss 0.198164, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7440

2017-10-10T13:49:57.315015: step 7441, loss 0.0335647, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:57.735675: step 7442, loss 0.0360769, acc 1, learning_rate 0.0001
2017-10-10T13:49:58.143840: step 7443, loss 0.114637, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:58.560311: step 7444, loss 0.0644701, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:58.982229: step 7445, loss 0.0229266, acc 1, learning_rate 0.0001
2017-10-10T13:49:59.383560: step 7446, loss 0.0244094, acc 1, learning_rate 0.0001
2017-10-10T13:49:59.798123: step 7447, loss 0.205049, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:00.159502: step 7448, loss 0.0591978, acc 0.980392, learning_rate 0.0001
2017-10-10T13:50:00.536865: step 7449, loss 0.0332783, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:00.869086: step 7450, loss 0.0696765, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:01.233072: step 7451, loss 0.0508276, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:01.635726: step 7452, loss 0.0613308, acc 1, learning_rate 0.0001
2017-10-10T13:50:02.002262: step 7453, loss 0.0837741, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:02.348462: step 7454, loss 0.109449, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:03.653157: step 7455, loss 0.0942867, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:04.014619: step 7456, loss 0.133987, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:04.300349: step 7457, loss 0.103169, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:04.601000: step 7458, loss 0.122275, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:04.937104: step 7459, loss 0.0668538, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:05.297037: step 7460, loss 0.0403376, acc 1, learning_rate 0.0001
2017-10-10T13:50:05.775845: step 7461, loss 0.0545862, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:06.095803: step 7462, loss 0.0804682, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:06.509894: step 7463, loss 0.247257, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:06.921046: step 7464, loss 0.0847711, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:07.411976: step 7465, loss 0.140877, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:07.701202: step 7466, loss 0.0693209, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:08.015442: step 7467, loss 0.122611, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:08.328996: step 7468, loss 0.0661644, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:08.638314: step 7469, loss 0.0785582, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:09.086333: step 7470, loss 0.0540474, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:09.510122: step 7471, loss 0.0553736, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:09.928545: step 7472, loss 0.0575126, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:10.332859: step 7473, loss 0.101278, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:10.731080: step 7474, loss 0.0380827, acc 1, learning_rate 0.0001
2017-10-10T13:50:11.153998: step 7475, loss 0.0815825, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:11.564860: step 7476, loss 0.0410396, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:11.978894: step 7477, loss 0.0249459, acc 1, learning_rate 0.0001
2017-10-10T13:50:12.421043: step 7478, loss 0.145103, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:12.874780: step 7479, loss 0.0289051, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:13.240407: step 7480, loss 0.0623334, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:14.106833: step 7480, loss 0.201121, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7480

2017-10-10T13:50:15.625009: step 7481, loss 0.0930765, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:16.037040: step 7482, loss 0.0617778, acc 1, learning_rate 0.0001
2017-10-10T13:50:16.349388: step 7483, loss 0.0411392, acc 1, learning_rate 0.0001
2017-10-10T13:50:16.772959: step 7484, loss 0.173222, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:17.205015: step 7485, loss 0.0615552, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:17.649011: step 7486, loss 0.0428903, acc 1, learning_rate 0.0001
2017-10-10T13:50:17.973100: step 7487, loss 0.118792, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:18.373295: step 7488, loss 0.0774091, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:18.772234: step 7489, loss 0.064024, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:19.118522: step 7490, loss 0.075499, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:19.476957: step 7491, loss 0.115675, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:19.892868: step 7492, loss 0.145546, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:20.333060: step 7493, loss 0.0528899, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:20.892980: step 7494, loss 0.0999155, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:21.284583: step 7495, loss 0.0416232, acc 1, learning_rate 0.0001
2017-10-10T13:50:21.600212: step 7496, loss 0.0980612, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:21.912233: step 7497, loss 0.0746603, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:22.257353: step 7498, loss 0.0843628, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:22.692761: step 7499, loss 0.0958191, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:23.081980: step 7500, loss 0.176614, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:23.404302: step 7501, loss 0.0942149, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:23.797003: step 7502, loss 0.111038, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:24.177075: step 7503, loss 0.17095, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:24.544976: step 7504, loss 0.0806934, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:25.067491: step 7505, loss 0.0259445, acc 1, learning_rate 0.0001
2017-10-10T13:50:25.384818: step 7506, loss 0.122019, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:25.688286: step 7507, loss 0.128555, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:26.015174: step 7508, loss 0.0954499, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:26.441654: step 7509, loss 0.103529, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:26.857292: step 7510, loss 0.110836, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:27.264472: step 7511, loss 0.106095, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:27.677907: step 7512, loss 0.109358, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:28.088497: step 7513, loss 0.0481264, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:28.495622: step 7514, loss 0.0402474, acc 1, learning_rate 0.0001
2017-10-10T13:50:28.872898: step 7515, loss 0.057795, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:29.303713: step 7516, loss 0.0892448, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:29.722028: step 7517, loss 0.0615952, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:30.116792: step 7518, loss 0.108409, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:30.517303: step 7519, loss 0.0547074, acc 1, learning_rate 0.0001
2017-10-10T13:50:30.861018: step 7520, loss 0.118025, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:31.624053: step 7520, loss 0.199965, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7520

2017-10-10T13:50:32.952871: step 7521, loss 0.0381941, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:33.360975: step 7522, loss 0.102274, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:33.790527: step 7523, loss 0.154407, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:34.215254: step 7524, loss 0.105528, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:34.638758: step 7525, loss 0.0672068, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:35.064638: step 7526, loss 0.135602, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:35.481383: step 7527, loss 0.0628185, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:35.884972: step 7528, loss 0.0796919, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:36.281700: step 7529, loss 0.218655, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:36.704939: step 7530, loss 0.111816, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:37.110752: step 7531, loss 0.114185, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:37.536568: step 7532, loss 0.0463905, acc 1, learning_rate 0.0001
2017-10-10T13:50:37.977504: step 7533, loss 0.103744, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:38.344867: step 7534, loss 0.154128, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:38.827778: step 7535, loss 0.0742805, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:39.236508: step 7536, loss 0.0527161, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:39.554754: step 7537, loss 0.0963365, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:39.866929: step 7538, loss 0.0474762, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:40.287184: step 7539, loss 0.0414117, acc 1, learning_rate 0.0001
2017-10-10T13:50:40.675118: step 7540, loss 0.120242, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:41.092718: step 7541, loss 0.0651541, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:41.481440: step 7542, loss 0.0891478, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:41.894475: step 7543, loss 0.112269, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:42.309934: step 7544, loss 0.0977527, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:42.714682: step 7545, loss 0.0807656, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:43.085479: step 7546, loss 0.11936, acc 0.960784, learning_rate 0.0001
2017-10-10T13:50:43.418643: step 7547, loss 0.0784548, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:43.677136: step 7548, loss 0.133866, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:44.154815: step 7549, loss 0.133965, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:44.526300: step 7550, loss 0.0621973, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:44.928771: step 7551, loss 0.0980904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:45.344949: step 7552, loss 0.13328, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:45.721127: step 7553, loss 0.0951006, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:46.133095: step 7554, loss 0.0530673, acc 1, learning_rate 0.0001
2017-10-10T13:50:46.450765: step 7555, loss 0.0728069, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:46.798018: step 7556, loss 0.0674463, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:47.205780: step 7557, loss 0.124994, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:47.619851: step 7558, loss 0.0499757, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:48.028889: step 7559, loss 0.0501326, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:48.457196: step 7560, loss 0.0712794, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:49.358601: step 7560, loss 0.199402, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7560

2017-10-10T13:50:50.788125: step 7561, loss 0.0554202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:51.212999: step 7562, loss 0.0893556, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:51.627606: step 7563, loss 0.104374, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:52.010349: step 7564, loss 0.0656278, acc 1, learning_rate 0.0001
2017-10-10T13:50:52.440901: step 7565, loss 0.0919658, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:52.845277: step 7566, loss 0.108698, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:53.212865: step 7567, loss 0.0657678, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:53.559417: step 7568, loss 0.0407104, acc 1, learning_rate 0.0001
2017-10-10T13:50:53.957064: step 7569, loss 0.0826065, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:54.321264: step 7570, loss 0.0829782, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:54.772041: step 7571, loss 0.0741972, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:55.246388: step 7572, loss 0.0382877, acc 1, learning_rate 0.0001
2017-10-10T13:50:55.640858: step 7573, loss 0.0455187, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:56.148750: step 7574, loss 0.0647535, acc 1, learning_rate 0.0001
2017-10-10T13:50:56.476875: step 7575, loss 0.0632801, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:56.784481: step 7576, loss 0.14623, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:57.097671: step 7577, loss 0.138097, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:57.500669: step 7578, loss 0.0447664, acc 1, learning_rate 0.0001
2017-10-10T13:50:57.915732: step 7579, loss 0.0635574, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:58.364818: step 7580, loss 0.0545738, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:58.772831: step 7581, loss 0.0558892, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:59.194238: step 7582, loss 0.0249047, acc 1, learning_rate 0.0001
2017-10-10T13:50:59.614154: step 7583, loss 0.0663793, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:00.033903: step 7584, loss 0.104138, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:00.438538: step 7585, loss 0.162875, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:00.921002: step 7586, loss 0.231064, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:01.235239: step 7587, loss 0.0888511, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:01.587395: step 7588, loss 0.0642444, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:01.917026: step 7589, loss 0.0354681, acc 1, learning_rate 0.0001
2017-10-10T13:51:02.333922: step 7590, loss 0.0492531, acc 1, learning_rate 0.0001
2017-10-10T13:51:02.752859: step 7591, loss 0.0357761, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:03.154397: step 7592, loss 0.0565158, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:03.580865: step 7593, loss 0.116298, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:03.964446: step 7594, loss 0.0962727, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:04.370983: step 7595, loss 0.113058, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:04.789934: step 7596, loss 0.0847564, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:05.197896: step 7597, loss 0.131111, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:05.623252: step 7598, loss 0.0808088, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:06.037135: step 7599, loss 0.0752353, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:06.461981: step 7600, loss 0.0690142, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:07.361793: step 7600, loss 0.200093, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7600

2017-10-10T13:51:08.728306: step 7601, loss 0.0668247, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:09.200871: step 7602, loss 0.111893, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:09.620849: step 7603, loss 0.0436062, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:10.101012: step 7604, loss 0.125645, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:10.524886: step 7605, loss 0.0381216, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:10.929036: step 7606, loss 0.0379545, acc 1, learning_rate 0.0001
2017-10-10T13:51:11.235398: step 7607, loss 0.0885559, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:11.624988: step 7608, loss 0.0237671, acc 1, learning_rate 0.0001
2017-10-10T13:51:12.030163: step 7609, loss 0.0808565, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:12.375640: step 7610, loss 0.0931924, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:12.782463: step 7611, loss 0.0696316, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:13.212827: step 7612, loss 0.0288435, acc 1, learning_rate 0.0001
2017-10-10T13:51:13.669149: step 7613, loss 0.0858304, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:14.158384: step 7614, loss 0.0561866, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:14.473348: step 7615, loss 0.106326, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:14.788611: step 7616, loss 0.0885283, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:15.098404: step 7617, loss 0.0222614, acc 1, learning_rate 0.0001
2017-10-10T13:51:15.477899: step 7618, loss 0.0908198, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:15.849290: step 7619, loss 0.0421587, acc 1, learning_rate 0.0001
2017-10-10T13:51:16.276902: step 7620, loss 0.0582952, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:16.693479: step 7621, loss 0.0614458, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:17.089054: step 7622, loss 0.0816751, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:17.445033: step 7623, loss 0.146631, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:17.856889: step 7624, loss 0.0535676, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:18.416863: step 7625, loss 0.0532886, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:18.724406: step 7626, loss 0.0677163, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:19.038301: step 7627, loss 0.101174, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:19.452862: step 7628, loss 0.077867, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:19.883766: step 7629, loss 0.138311, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:20.279278: step 7630, loss 0.0637284, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:20.664870: step 7631, loss 0.0839465, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:21.052842: step 7632, loss 0.0807419, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:21.444895: step 7633, loss 0.16279, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:21.788937: step 7634, loss 0.157049, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:22.207701: step 7635, loss 0.0960109, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:22.575485: step 7636, loss 0.100495, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:22.970760: step 7637, loss 0.135145, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:23.346538: step 7638, loss 0.0386601, acc 1, learning_rate 0.0001
2017-10-10T13:51:23.719444: step 7639, loss 0.0851736, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:24.148960: step 7640, loss 0.052763, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:25.016470: step 7640, loss 0.199232, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7640

2017-10-10T13:51:26.574648: step 7641, loss 0.0913071, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:26.976824: step 7642, loss 0.0160564, acc 1, learning_rate 0.0001
2017-10-10T13:51:27.374541: step 7643, loss 0.102433, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:27.761422: step 7644, loss 0.209876, acc 0.941176, learning_rate 0.0001
2017-10-10T13:51:28.176238: step 7645, loss 0.083897, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:28.602077: step 7646, loss 0.107465, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:29.027721: step 7647, loss 0.0947716, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:29.448741: step 7648, loss 0.0216185, acc 1, learning_rate 0.0001
2017-10-10T13:51:29.896601: step 7649, loss 0.179006, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:30.315497: step 7650, loss 0.0410705, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:30.682157: step 7651, loss 0.0588683, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:31.067986: step 7652, loss 0.0992625, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:31.556876: step 7653, loss 0.0391063, acc 1, learning_rate 0.0001
2017-10-10T13:51:32.032452: step 7654, loss 0.0868286, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:32.342543: step 7655, loss 0.0322274, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:32.619798: step 7656, loss 0.0705218, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:33.042679: step 7657, loss 0.0689979, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:33.412973: step 7658, loss 0.107383, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:33.828869: step 7659, loss 0.113461, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:34.148055: step 7660, loss 0.0832386, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:34.600864: step 7661, loss 0.118481, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:34.986759: step 7662, loss 0.0654739, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:35.456876: step 7663, loss 0.0409387, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:35.943210: step 7664, loss 0.0586735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:36.249509: step 7665, loss 0.153453, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:36.560699: step 7666, loss 0.0526354, acc 1, learning_rate 0.0001
2017-10-10T13:51:36.869771: step 7667, loss 0.0180397, acc 1, learning_rate 0.0001
2017-10-10T13:51:37.293475: step 7668, loss 0.0921016, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:37.674119: step 7669, loss 0.0775355, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:38.048853: step 7670, loss 0.0726108, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:38.427292: step 7671, loss 0.122401, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:38.779787: step 7672, loss 0.102097, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:39.166712: step 7673, loss 0.119901, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:39.575431: step 7674, loss 0.0840281, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:39.967503: step 7675, loss 0.0365026, acc 1, learning_rate 0.0001
2017-10-10T13:51:40.334861: step 7676, loss 0.0951205, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:40.742793: step 7677, loss 0.0483138, acc 1, learning_rate 0.0001
2017-10-10T13:51:41.159748: step 7678, loss 0.0537371, acc 1, learning_rate 0.0001
2017-10-10T13:51:41.567650: step 7679, loss 0.0749176, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:42.005741: step 7680, loss 0.0593254, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:42.891516: step 7680, loss 0.200725, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7680

2017-10-10T13:51:44.195415: step 7681, loss 0.0652356, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:44.627389: step 7682, loss 0.136452, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:45.039035: step 7683, loss 0.0366467, acc 1, learning_rate 0.0001
2017-10-10T13:51:45.433779: step 7684, loss 0.0780555, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:45.838232: step 7685, loss 0.0389086, acc 1, learning_rate 0.0001
2017-10-10T13:51:46.247956: step 7686, loss 0.0558515, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:46.660177: step 7687, loss 0.0479605, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:47.079710: step 7688, loss 0.0516772, acc 1, learning_rate 0.0001
2017-10-10T13:51:47.503338: step 7689, loss 0.13878, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:47.923159: step 7690, loss 0.0760615, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:48.350288: step 7691, loss 0.132974, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:48.768866: step 7692, loss 0.132396, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:49.257104: step 7693, loss 0.123492, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:49.586693: step 7694, loss 0.149251, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:49.897155: step 7695, loss 0.144478, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:50.215267: step 7696, loss 0.113056, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:50.608932: step 7697, loss 0.0634646, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:51.026047: step 7698, loss 0.0781333, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:51.443147: step 7699, loss 0.10714, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:51.876043: step 7700, loss 0.169011, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:52.293582: step 7701, loss 0.1198, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:52.725697: step 7702, loss 0.170822, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:53.152489: step 7703, loss 0.0585802, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:53.592862: step 7704, loss 0.104543, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:54.062850: step 7705, loss 0.123242, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:54.391862: step 7706, loss 0.07462, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:54.709677: step 7707, loss 0.0946312, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:55.031404: step 7708, loss 0.144411, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:55.452184: step 7709, loss 0.0981185, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:55.870776: step 7710, loss 0.0790795, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:56.269081: step 7711, loss 0.0540955, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:56.639746: step 7712, loss 0.0997041, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:56.960744: step 7713, loss 0.0298105, acc 1, learning_rate 0.0001
2017-10-10T13:51:57.393057: step 7714, loss 0.0521291, acc 1, learning_rate 0.0001
2017-10-10T13:51:57.781070: step 7715, loss 0.10306, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:58.219667: step 7716, loss 0.0356311, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:58.639377: step 7717, loss 0.0481916, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:59.009085: step 7718, loss 0.0599845, acc 1, learning_rate 0.0001
2017-10-10T13:51:59.424976: step 7719, loss 0.0426428, acc 1, learning_rate 0.0001
2017-10-10T13:51:59.812039: step 7720, loss 0.13689, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:00.595359: step 7720, loss 0.198092, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7720

2017-10-10T13:52:01.992057: step 7721, loss 0.0757099, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:02.350789: step 7722, loss 0.096594, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:02.718381: step 7723, loss 0.02291, acc 1, learning_rate 0.0001
2017-10-10T13:52:03.128883: step 7724, loss 0.233165, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:03.524919: step 7725, loss 0.0346711, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:03.914877: step 7726, loss 0.0590898, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:04.335441: step 7727, loss 0.130932, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:04.772845: step 7728, loss 0.0597537, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:05.196841: step 7729, loss 0.0322568, acc 1, learning_rate 0.0001
2017-10-10T13:52:05.656851: step 7730, loss 0.0568261, acc 1, learning_rate 0.0001
2017-10-10T13:52:06.074518: step 7731, loss 0.0668932, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:06.537039: step 7732, loss 0.0579352, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:07.041105: step 7733, loss 0.0572478, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:07.364024: step 7734, loss 0.194258, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:07.655115: step 7735, loss 0.0676277, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:07.963148: step 7736, loss 0.103122, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:08.218294: step 7737, loss 0.047827, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:08.572823: step 7738, loss 0.0525681, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:08.968806: step 7739, loss 0.0405576, acc 1, learning_rate 0.0001
2017-10-10T13:52:09.383815: step 7740, loss 0.0727367, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:09.808222: step 7741, loss 0.0946467, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:10.170998: step 7742, loss 0.0844379, acc 0.960784, learning_rate 0.0001
2017-10-10T13:52:10.587063: step 7743, loss 0.0364141, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:11.002416: step 7744, loss 0.130158, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:11.460957: step 7745, loss 0.113767, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:11.953045: step 7746, loss 0.0977331, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:12.277053: step 7747, loss 0.0971933, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:12.584365: step 7748, loss 0.0378119, acc 1, learning_rate 0.0001
2017-10-10T13:52:12.916245: step 7749, loss 0.12446, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:13.305899: step 7750, loss 0.0404093, acc 1, learning_rate 0.0001
2017-10-10T13:52:13.739379: step 7751, loss 0.0288153, acc 1, learning_rate 0.0001
2017-10-10T13:52:14.137498: step 7752, loss 0.0721797, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:14.504878: step 7753, loss 0.0783492, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:14.872775: step 7754, loss 0.0416425, acc 1, learning_rate 0.0001
2017-10-10T13:52:15.273077: step 7755, loss 0.0427312, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:15.656949: step 7756, loss 0.0509537, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:16.044562: step 7757, loss 0.11944, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:16.378689: step 7758, loss 0.234116, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:16.764795: step 7759, loss 0.0436771, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:17.112977: step 7760, loss 0.110291, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:17.989251: step 7760, loss 0.198275, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7760

2017-10-10T13:52:19.602841: step 7761, loss 0.0565997, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:20.028518: step 7762, loss 0.0722033, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:20.439168: step 7763, loss 0.0690047, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:20.844718: step 7764, loss 0.0954138, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:21.216839: step 7765, loss 0.0741812, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:21.588926: step 7766, loss 0.0969147, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:21.976930: step 7767, loss 0.148254, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:22.348013: step 7768, loss 0.153228, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:22.709306: step 7769, loss 0.0745072, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:23.172932: step 7770, loss 0.0824039, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:23.549009: step 7771, loss 0.0457296, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:23.928112: step 7772, loss 0.0193135, acc 1, learning_rate 0.0001
2017-10-10T13:52:24.353073: step 7773, loss 0.102124, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:24.810318: step 7774, loss 0.108094, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:25.152789: step 7775, loss 0.161454, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:25.464166: step 7776, loss 0.115331, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:25.774146: step 7777, loss 0.0641686, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:26.149924: step 7778, loss 0.175682, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:26.544190: step 7779, loss 0.124361, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:26.894618: step 7780, loss 0.128638, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:27.291723: step 7781, loss 0.0919835, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:27.670273: step 7782, loss 0.0755092, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:28.078026: step 7783, loss 0.0421239, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:28.433206: step 7784, loss 0.0867604, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:28.820019: step 7785, loss 0.0466244, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:29.261234: step 7786, loss 0.105059, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:29.785554: step 7787, loss 0.101328, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:30.100844: step 7788, loss 0.0602115, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:30.420817: step 7789, loss 0.0223278, acc 1, learning_rate 0.0001
2017-10-10T13:52:30.732059: step 7790, loss 0.0650068, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:31.108840: step 7791, loss 0.10787, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:31.456887: step 7792, loss 0.0548996, acc 1, learning_rate 0.0001
2017-10-10T13:52:31.840688: step 7793, loss 0.063426, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:32.279285: step 7794, loss 0.148734, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:32.658607: step 7795, loss 0.0213485, acc 1, learning_rate 0.0001
2017-10-10T13:52:33.050670: step 7796, loss 0.0624932, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:33.476848: step 7797, loss 0.0397788, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:33.878962: step 7798, loss 0.0477887, acc 1, learning_rate 0.0001
2017-10-10T13:52:34.308443: step 7799, loss 0.0893464, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:34.730904: step 7800, loss 0.104071, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:35.552857: step 7800, loss 0.197523, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7800

2017-10-10T13:52:36.839209: step 7801, loss 0.0902316, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:37.217412: step 7802, loss 0.0564209, acc 1, learning_rate 0.0001
2017-10-10T13:52:37.569755: step 7803, loss 0.0561681, acc 1, learning_rate 0.0001
2017-10-10T13:52:37.921726: step 7804, loss 0.0312978, acc 1, learning_rate 0.0001
2017-10-10T13:52:38.338437: step 7805, loss 0.0885946, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:38.770298: step 7806, loss 0.0699313, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:39.186706: step 7807, loss 0.0433338, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:39.599885: step 7808, loss 0.166399, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:40.020344: step 7809, loss 0.0654303, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:40.453648: step 7810, loss 0.056894, acc 1, learning_rate 0.0001
2017-10-10T13:52:40.868779: step 7811, loss 0.136267, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:41.288560: step 7812, loss 0.0586917, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:41.697221: step 7813, loss 0.0428814, acc 1, learning_rate 0.0001
2017-10-10T13:52:42.096356: step 7814, loss 0.070479, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:42.494676: step 7815, loss 0.0449162, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:42.907446: step 7816, loss 0.141826, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:43.213112: step 7817, loss 0.047129, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:43.524716: step 7818, loss 0.0492997, acc 1, learning_rate 0.0001
2017-10-10T13:52:43.889165: step 7819, loss 0.142695, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:44.315778: step 7820, loss 0.0973712, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:44.674680: step 7821, loss 0.0323615, acc 1, learning_rate 0.0001
2017-10-10T13:52:45.056983: step 7822, loss 0.074202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:45.508889: step 7823, loss 0.105782, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:45.937574: step 7824, loss 0.0742399, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:46.338277: step 7825, loss 0.0347232, acc 1, learning_rate 0.0001
2017-10-10T13:52:46.740847: step 7826, loss 0.0684026, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:47.128868: step 7827, loss 0.0471798, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:47.607941: step 7828, loss 0.058601, acc 1, learning_rate 0.0001
2017-10-10T13:52:47.985597: step 7829, loss 0.0327327, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:48.315894: step 7830, loss 0.0404486, acc 1, learning_rate 0.0001
2017-10-10T13:52:48.622646: step 7831, loss 0.051787, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:49.028877: step 7832, loss 0.0385718, acc 1, learning_rate 0.0001
2017-10-10T13:52:49.388845: step 7833, loss 0.0779663, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:49.768902: step 7834, loss 0.13111, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:50.109133: step 7835, loss 0.0686688, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:50.560433: step 7836, loss 0.153926, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:50.968955: step 7837, loss 0.0703002, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:51.395341: step 7838, loss 0.107899, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:51.756899: step 7839, loss 0.0631196, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:52.064557: step 7840, loss 0.101916, acc 0.960784, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:52.905824: step 7840, loss 0.197607, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657231/checkpoints/model-7840

