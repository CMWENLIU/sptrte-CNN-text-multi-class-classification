
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507664326

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T14:38:56.202962: step 1, loss 8.46522, acc 0.140625, learning_rate 0.005
2017-10-10T14:38:56.388794: step 2, loss 4.92195, acc 0.28125, learning_rate 0.00498
2017-10-10T14:38:56.552826: step 3, loss 4.28176, acc 0.453125, learning_rate 0.00496008
2017-10-10T14:38:56.710428: step 4, loss 6.58224, acc 0.34375, learning_rate 0.00494024
2017-10-10T14:38:56.887734: step 5, loss 6.32024, acc 0.34375, learning_rate 0.00492049
2017-10-10T14:38:57.078323: step 6, loss 5.70352, acc 0.25, learning_rate 0.00490081
2017-10-10T14:38:57.280241: step 7, loss 3.97367, acc 0.453125, learning_rate 0.00488121
2017-10-10T14:38:57.474101: step 8, loss 3.2002, acc 0.421875, learning_rate 0.0048617
2017-10-10T14:38:57.661459: step 9, loss 3.5195, acc 0.453125, learning_rate 0.00484226
2017-10-10T14:38:57.850812: step 10, loss 4.57315, acc 0.453125, learning_rate 0.00482291
2017-10-10T14:38:58.028810: step 11, loss 3.27278, acc 0.515625, learning_rate 0.00480363
2017-10-10T14:38:58.187595: step 12, loss 4.35625, acc 0.359375, learning_rate 0.00478443
2017-10-10T14:38:58.357100: step 13, loss 2.85513, acc 0.5, learning_rate 0.00476531
2017-10-10T14:38:58.530774: step 14, loss 1.86444, acc 0.6875, learning_rate 0.00474627
2017-10-10T14:38:58.719795: step 15, loss 2.88347, acc 0.453125, learning_rate 0.0047273
2017-10-10T14:38:58.914738: step 16, loss 2.56029, acc 0.578125, learning_rate 0.00470841
2017-10-10T14:38:59.116525: step 17, loss 2.60444, acc 0.546875, learning_rate 0.0046896
2017-10-10T14:38:59.317317: step 18, loss 2.22689, acc 0.5, learning_rate 0.00467087
2017-10-10T14:38:59.516751: step 19, loss 2.68841, acc 0.421875, learning_rate 0.00465221
2017-10-10T14:38:59.723183: step 20, loss 1.91558, acc 0.578125, learning_rate 0.00463363
2017-10-10T14:38:59.925880: step 21, loss 2.30521, acc 0.546875, learning_rate 0.00461513
2017-10-10T14:39:00.132926: step 22, loss 2.43819, acc 0.625, learning_rate 0.0045967
2017-10-10T14:39:00.329049: step 23, loss 1.53426, acc 0.75, learning_rate 0.00457834
2017-10-10T14:39:00.509545: step 24, loss 1.85067, acc 0.640625, learning_rate 0.00456006
2017-10-10T14:39:00.709882: step 25, loss 2.77365, acc 0.453125, learning_rate 0.00454186
2017-10-10T14:39:00.911036: step 26, loss 1.22344, acc 0.671875, learning_rate 0.00452373
2017-10-10T14:39:01.094711: step 27, loss 1.92665, acc 0.578125, learning_rate 0.00450567
2017-10-10T14:39:01.277575: step 28, loss 1.39585, acc 0.6875, learning_rate 0.00448769
2017-10-10T14:39:01.472822: step 29, loss 2.23505, acc 0.5625, learning_rate 0.00446978
2017-10-10T14:39:01.649937: step 30, loss 1.55088, acc 0.671875, learning_rate 0.00445194
2017-10-10T14:39:01.844837: step 31, loss 1.50178, acc 0.65625, learning_rate 0.00443418
2017-10-10T14:39:02.024859: step 32, loss 2.13618, acc 0.625, learning_rate 0.00441649
2017-10-10T14:39:02.216847: step 33, loss 2.03495, acc 0.6875, learning_rate 0.00439887
2017-10-10T14:39:02.433973: step 34, loss 1.34276, acc 0.734375, learning_rate 0.00438132
2017-10-10T14:39:02.632913: step 35, loss 1.13372, acc 0.765625, learning_rate 0.00436385
2017-10-10T14:39:02.827246: step 36, loss 1.73803, acc 0.671875, learning_rate 0.00434644
2017-10-10T14:39:02.997370: step 37, loss 1.34629, acc 0.703125, learning_rate 0.00432911
2017-10-10T14:39:03.160771: step 38, loss 1.75444, acc 0.59375, learning_rate 0.00431185
2017-10-10T14:39:03.352883: step 39, loss 1.30711, acc 0.6875, learning_rate 0.00429465
2017-10-10T14:39:03.505403: step 40, loss 1.55866, acc 0.59375, learning_rate 0.00427753

Evaluation:
2017-10-10T14:39:03.928001: step 40, loss 0.421745, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-40

2017-10-10T14:39:04.719194: step 41, loss 1.9092, acc 0.734375, learning_rate 0.00426048
2017-10-10T14:39:04.847989: step 42, loss 1.25204, acc 0.734375, learning_rate 0.0042435
2017-10-10T14:39:04.966810: step 43, loss 1.28829, acc 0.703125, learning_rate 0.00422659
2017-10-10T14:39:05.145545: step 44, loss 1.18355, acc 0.765625, learning_rate 0.00420974
2017-10-10T14:39:05.336196: step 45, loss 0.912949, acc 0.78125, learning_rate 0.00419297
2017-10-10T14:39:05.524816: step 46, loss 1.58174, acc 0.703125, learning_rate 0.00417626
2017-10-10T14:39:05.719767: step 47, loss 1.05094, acc 0.765625, learning_rate 0.00415962
2017-10-10T14:39:05.928477: step 48, loss 1.09672, acc 0.71875, learning_rate 0.00414305
2017-10-10T14:39:06.126031: step 49, loss 1.09767, acc 0.8125, learning_rate 0.00412655
2017-10-10T14:39:06.311066: step 50, loss 0.998416, acc 0.796875, learning_rate 0.00411011
2017-10-10T14:39:06.505572: step 51, loss 1.13223, acc 0.75, learning_rate 0.00409375
2017-10-10T14:39:06.678811: step 52, loss 1.69288, acc 0.671875, learning_rate 0.00407744
2017-10-10T14:39:06.878450: step 53, loss 0.812598, acc 0.828125, learning_rate 0.00406121
2017-10-10T14:39:07.064861: step 54, loss 0.950855, acc 0.734375, learning_rate 0.00404504
2017-10-10T14:39:07.240868: step 55, loss 1.36534, acc 0.71875, learning_rate 0.00402894
2017-10-10T14:39:07.420888: step 56, loss 1.18259, acc 0.8125, learning_rate 0.0040129
2017-10-10T14:39:07.628346: step 57, loss 1.30497, acc 0.734375, learning_rate 0.00399693
2017-10-10T14:39:07.831648: step 58, loss 1.28158, acc 0.796875, learning_rate 0.00398102
2017-10-10T14:39:08.048010: step 59, loss 0.589582, acc 0.78125, learning_rate 0.00396518
2017-10-10T14:39:08.240589: step 60, loss 1.49978, acc 0.65625, learning_rate 0.00394941
2017-10-10T14:39:08.426493: step 61, loss 0.790153, acc 0.796875, learning_rate 0.00393369
2017-10-10T14:39:08.608848: step 62, loss 0.647414, acc 0.828125, learning_rate 0.00391804
2017-10-10T14:39:08.809922: step 63, loss 0.961408, acc 0.765625, learning_rate 0.00390246
2017-10-10T14:39:08.985895: step 64, loss 0.951024, acc 0.75, learning_rate 0.00388694
2017-10-10T14:39:09.189004: step 65, loss 0.789792, acc 0.75, learning_rate 0.00387148
2017-10-10T14:39:09.396205: step 66, loss 0.895985, acc 0.78125, learning_rate 0.00385609
2017-10-10T14:39:09.602618: step 67, loss 0.646093, acc 0.765625, learning_rate 0.00384076
2017-10-10T14:39:09.805426: step 68, loss 0.941354, acc 0.8125, learning_rate 0.00382549
2017-10-10T14:39:10.004762: step 69, loss 0.52245, acc 0.875, learning_rate 0.00381028
2017-10-10T14:39:10.198815: step 70, loss 0.822735, acc 0.75, learning_rate 0.00379514
2017-10-10T14:39:10.402570: step 71, loss 0.803568, acc 0.75, learning_rate 0.00378005
2017-10-10T14:39:10.609973: step 72, loss 0.58795, acc 0.84375, learning_rate 0.00376503
2017-10-10T14:39:10.788820: step 73, loss 1.16835, acc 0.796875, learning_rate 0.00375007
2017-10-10T14:39:10.969217: step 74, loss 0.533763, acc 0.84375, learning_rate 0.00373517
2017-10-10T14:39:11.153288: step 75, loss 0.397983, acc 0.859375, learning_rate 0.00372034
2017-10-10T14:39:11.326838: step 76, loss 0.580758, acc 0.828125, learning_rate 0.00370556
2017-10-10T14:39:11.497314: step 77, loss 0.391863, acc 0.90625, learning_rate 0.00369084
2017-10-10T14:39:11.668592: step 78, loss 0.860624, acc 0.75, learning_rate 0.00367619
2017-10-10T14:39:11.864070: step 79, loss 0.665462, acc 0.8125, learning_rate 0.00366159
2017-10-10T14:39:12.068936: step 80, loss 0.781812, acc 0.796875, learning_rate 0.00364705

Evaluation:
2017-10-10T14:39:12.441408: step 80, loss 0.361388, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-80

2017-10-10T14:39:13.318498: step 81, loss 0.879973, acc 0.765625, learning_rate 0.00363257
2017-10-10T14:39:13.450078: step 82, loss 0.915399, acc 0.78125, learning_rate 0.00361815
2017-10-10T14:39:13.573241: step 83, loss 0.927212, acc 0.765625, learning_rate 0.00360379
2017-10-10T14:39:13.689377: step 84, loss 0.786317, acc 0.828125, learning_rate 0.00358949
2017-10-10T14:39:13.805574: step 85, loss 0.681948, acc 0.8125, learning_rate 0.00357525
2017-10-10T14:39:13.980847: step 86, loss 0.416054, acc 0.84375, learning_rate 0.00356106
2017-10-10T14:39:14.173469: step 87, loss 1.00968, acc 0.78125, learning_rate 0.00354694
2017-10-10T14:39:14.340864: step 88, loss 0.956708, acc 0.75, learning_rate 0.00353287
2017-10-10T14:39:14.508966: step 89, loss 0.710196, acc 0.765625, learning_rate 0.00351885
2017-10-10T14:39:14.676843: step 90, loss 0.539627, acc 0.90625, learning_rate 0.0035049
2017-10-10T14:39:14.858898: step 91, loss 0.831028, acc 0.796875, learning_rate 0.003491
2017-10-10T14:39:15.073409: step 92, loss 0.590917, acc 0.859375, learning_rate 0.00347716
2017-10-10T14:39:15.267128: step 93, loss 0.67589, acc 0.8125, learning_rate 0.00346338
2017-10-10T14:39:15.460849: step 94, loss 0.72857, acc 0.78125, learning_rate 0.00344965
2017-10-10T14:39:15.657037: step 95, loss 0.59319, acc 0.796875, learning_rate 0.00343597
2017-10-10T14:39:15.867036: step 96, loss 0.543829, acc 0.828125, learning_rate 0.00342236
2017-10-10T14:39:16.077695: step 97, loss 1.26983, acc 0.71875, learning_rate 0.0034088
2017-10-10T14:39:16.254738: step 98, loss 0.979181, acc 0.745098, learning_rate 0.00339529
2017-10-10T14:39:16.460433: step 99, loss 0.471793, acc 0.8125, learning_rate 0.00338184
2017-10-10T14:39:16.657891: step 100, loss 0.724497, acc 0.78125, learning_rate 0.00336844
2017-10-10T14:39:16.862259: step 101, loss 0.648556, acc 0.828125, learning_rate 0.0033551
2017-10-10T14:39:17.060151: step 102, loss 0.609608, acc 0.84375, learning_rate 0.00334182
2017-10-10T14:39:17.255008: step 103, loss 0.777798, acc 0.734375, learning_rate 0.00332858
2017-10-10T14:39:17.447154: step 104, loss 0.590445, acc 0.828125, learning_rate 0.00331541
2017-10-10T14:39:17.647822: step 105, loss 1.2003, acc 0.6875, learning_rate 0.00330228
2017-10-10T14:39:17.843660: step 106, loss 0.600995, acc 0.828125, learning_rate 0.00328921
2017-10-10T14:39:18.026492: step 107, loss 0.814873, acc 0.78125, learning_rate 0.00327619
2017-10-10T14:39:18.207051: step 108, loss 0.62639, acc 0.8125, learning_rate 0.00326323
2017-10-10T14:39:18.397446: step 109, loss 0.35477, acc 0.921875, learning_rate 0.00325032
2017-10-10T14:39:18.560875: step 110, loss 1.07196, acc 0.71875, learning_rate 0.00323746
2017-10-10T14:39:18.716875: step 111, loss 0.673876, acc 0.859375, learning_rate 0.00322465
2017-10-10T14:39:18.901928: step 112, loss 0.502172, acc 0.890625, learning_rate 0.0032119
2017-10-10T14:39:19.088858: step 113, loss 0.574731, acc 0.796875, learning_rate 0.0031992
2017-10-10T14:39:19.269875: step 114, loss 0.631853, acc 0.875, learning_rate 0.00318655
2017-10-10T14:39:19.458874: step 115, loss 0.562425, acc 0.859375, learning_rate 0.00317395
2017-10-10T14:39:19.657548: step 116, loss 0.976994, acc 0.71875, learning_rate 0.0031614
2017-10-10T14:39:19.867930: step 117, loss 0.800027, acc 0.859375, learning_rate 0.0031489
2017-10-10T14:39:20.062185: step 118, loss 0.691558, acc 0.84375, learning_rate 0.00313646
2017-10-10T14:39:20.245041: step 119, loss 0.447316, acc 0.890625, learning_rate 0.00312407
2017-10-10T14:39:20.432737: step 120, loss 0.951486, acc 0.765625, learning_rate 0.00311172

Evaluation:
2017-10-10T14:39:20.821176: step 120, loss 0.33342, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-120

2017-10-10T14:39:21.844846: step 121, loss 0.325826, acc 0.921875, learning_rate 0.00309943
2017-10-10T14:39:22.071219: step 122, loss 0.451318, acc 0.828125, learning_rate 0.00308719
2017-10-10T14:39:22.193583: step 123, loss 0.169161, acc 0.9375, learning_rate 0.00307499
2017-10-10T14:39:22.316708: step 124, loss 0.476543, acc 0.8125, learning_rate 0.00306285
2017-10-10T14:39:22.438270: step 125, loss 0.446912, acc 0.859375, learning_rate 0.00305076
2017-10-10T14:39:22.558140: step 126, loss 0.459442, acc 0.890625, learning_rate 0.00303871
2017-10-10T14:39:22.677896: step 127, loss 0.649415, acc 0.875, learning_rate 0.00302672
2017-10-10T14:39:22.795844: step 128, loss 0.409757, acc 0.921875, learning_rate 0.00301477
2017-10-10T14:39:23.000414: step 129, loss 0.530797, acc 0.8125, learning_rate 0.00300287
2017-10-10T14:39:23.197305: step 130, loss 0.58978, acc 0.875, learning_rate 0.00299102
2017-10-10T14:39:23.399322: step 131, loss 0.348784, acc 0.859375, learning_rate 0.00297922
2017-10-10T14:39:23.598151: step 132, loss 0.629194, acc 0.828125, learning_rate 0.00296747
2017-10-10T14:39:23.796171: step 133, loss 0.470681, acc 0.859375, learning_rate 0.00295577
2017-10-10T14:39:24.011041: step 134, loss 0.511291, acc 0.828125, learning_rate 0.00294411
2017-10-10T14:39:24.201022: step 135, loss 0.191816, acc 0.9375, learning_rate 0.0029325
2017-10-10T14:39:24.388038: step 136, loss 0.338053, acc 0.890625, learning_rate 0.00292094
2017-10-10T14:39:24.564031: step 137, loss 0.651546, acc 0.828125, learning_rate 0.00290943
2017-10-10T14:39:24.730100: step 138, loss 0.61883, acc 0.828125, learning_rate 0.00289796
2017-10-10T14:39:24.911893: step 139, loss 0.35387, acc 0.875, learning_rate 0.00288654
2017-10-10T14:39:25.076996: step 140, loss 0.211181, acc 0.921875, learning_rate 0.00287516
2017-10-10T14:39:25.259974: step 141, loss 0.474071, acc 0.84375, learning_rate 0.00286384
2017-10-10T14:39:25.454029: step 142, loss 0.395403, acc 0.84375, learning_rate 0.00285256
2017-10-10T14:39:25.659895: step 143, loss 0.659379, acc 0.796875, learning_rate 0.00284132
2017-10-10T14:39:25.865931: step 144, loss 0.443726, acc 0.859375, learning_rate 0.00283013
2017-10-10T14:39:26.054365: step 145, loss 0.317374, acc 0.875, learning_rate 0.00281899
2017-10-10T14:39:26.249078: step 146, loss 0.685557, acc 0.828125, learning_rate 0.00280789
2017-10-10T14:39:26.452235: step 147, loss 0.494877, acc 0.859375, learning_rate 0.00279684
2017-10-10T14:39:26.640403: step 148, loss 0.447405, acc 0.84375, learning_rate 0.00278583
2017-10-10T14:39:26.840440: step 149, loss 0.296995, acc 0.90625, learning_rate 0.00277486
2017-10-10T14:39:27.036336: step 150, loss 0.55133, acc 0.8125, learning_rate 0.00276395
2017-10-10T14:39:27.232124: step 151, loss 0.541039, acc 0.90625, learning_rate 0.00275307
2017-10-10T14:39:27.434397: step 152, loss 0.539747, acc 0.8125, learning_rate 0.00274224
2017-10-10T14:39:27.631555: step 153, loss 0.297813, acc 0.921875, learning_rate 0.00273146
2017-10-10T14:39:27.827885: step 154, loss 0.318995, acc 0.859375, learning_rate 0.00272072
2017-10-10T14:39:28.046471: step 155, loss 0.126788, acc 0.984375, learning_rate 0.00271002
2017-10-10T14:39:28.255949: step 156, loss 0.778074, acc 0.828125, learning_rate 0.00269937
2017-10-10T14:39:28.465094: step 157, loss 0.415084, acc 0.875, learning_rate 0.00268876
2017-10-10T14:39:28.657227: step 158, loss 0.281014, acc 0.90625, learning_rate 0.00267819
2017-10-10T14:39:28.861213: step 159, loss 1.13889, acc 0.78125, learning_rate 0.00266767
2017-10-10T14:39:29.061879: step 160, loss 0.627464, acc 0.828125, learning_rate 0.00265719

Evaluation:
2017-10-10T14:39:29.445819: step 160, loss 0.340741, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-160

2017-10-10T14:39:30.376816: step 161, loss 0.292895, acc 0.90625, learning_rate 0.00264675
2017-10-10T14:39:30.576403: step 162, loss 0.361604, acc 0.84375, learning_rate 0.00263635
2017-10-10T14:39:30.788874: step 163, loss 0.733102, acc 0.796875, learning_rate 0.002626
2017-10-10T14:39:31.045509: step 164, loss 0.516001, acc 0.875, learning_rate 0.00261569
2017-10-10T14:39:31.182129: step 165, loss 0.869185, acc 0.78125, learning_rate 0.00260542
2017-10-10T14:39:31.311503: step 166, loss 0.49408, acc 0.859375, learning_rate 0.0025952
2017-10-10T14:39:31.446412: step 167, loss 0.248479, acc 0.921875, learning_rate 0.00258501
2017-10-10T14:39:31.583762: step 168, loss 0.252785, acc 0.875, learning_rate 0.00257487
2017-10-10T14:39:31.733223: step 169, loss 0.199212, acc 0.921875, learning_rate 0.00256477
2017-10-10T14:39:31.906999: step 170, loss 0.511159, acc 0.859375, learning_rate 0.0025547
2017-10-10T14:39:32.094659: step 171, loss 0.485239, acc 0.875, learning_rate 0.00254469
2017-10-10T14:39:32.293565: step 172, loss 0.451386, acc 0.890625, learning_rate 0.00253471
2017-10-10T14:39:32.486815: step 173, loss 0.40053, acc 0.90625, learning_rate 0.00252477
2017-10-10T14:39:32.673739: step 174, loss 0.574707, acc 0.859375, learning_rate 0.00251487
2017-10-10T14:39:32.871968: step 175, loss 0.274376, acc 0.90625, learning_rate 0.00250501
2017-10-10T14:39:33.062894: step 176, loss 0.383081, acc 0.828125, learning_rate 0.0024952
2017-10-10T14:39:33.256982: step 177, loss 0.398992, acc 0.921875, learning_rate 0.00248542
2017-10-10T14:39:33.458652: step 178, loss 0.230749, acc 0.90625, learning_rate 0.00247568
2017-10-10T14:39:33.658519: step 179, loss 0.379979, acc 0.890625, learning_rate 0.00246599
2017-10-10T14:39:33.835347: step 180, loss 0.613618, acc 0.828125, learning_rate 0.00245633
2017-10-10T14:39:34.013170: step 181, loss 0.692664, acc 0.8125, learning_rate 0.00244671
2017-10-10T14:39:34.183125: step 182, loss 0.516367, acc 0.828125, learning_rate 0.00243713
2017-10-10T14:39:34.333486: step 183, loss 0.636796, acc 0.84375, learning_rate 0.00242759
2017-10-10T14:39:34.522605: step 184, loss 0.417428, acc 0.859375, learning_rate 0.00241809
2017-10-10T14:39:34.730453: step 185, loss 0.563616, acc 0.828125, learning_rate 0.00240863
2017-10-10T14:39:34.926143: step 186, loss 0.133603, acc 0.953125, learning_rate 0.00239921
2017-10-10T14:39:35.131225: step 187, loss 0.427929, acc 0.875, learning_rate 0.00238982
2017-10-10T14:39:35.316632: step 188, loss 0.316901, acc 0.921875, learning_rate 0.00238048
2017-10-10T14:39:35.494191: step 189, loss 0.226526, acc 0.890625, learning_rate 0.00237117
2017-10-10T14:39:35.684838: step 190, loss 0.320306, acc 0.875, learning_rate 0.0023619
2017-10-10T14:39:35.832286: step 191, loss 0.37277, acc 0.875, learning_rate 0.00235267
2017-10-10T14:39:36.033052: step 192, loss 0.464044, acc 0.84375, learning_rate 0.00234347
2017-10-10T14:39:36.184881: step 193, loss 0.464475, acc 0.859375, learning_rate 0.00233431
2017-10-10T14:39:36.373121: step 194, loss 0.315786, acc 0.875, learning_rate 0.00232519
2017-10-10T14:39:36.571952: step 195, loss 0.352031, acc 0.90625, learning_rate 0.00231611
2017-10-10T14:39:36.756950: step 196, loss 0.660677, acc 0.823529, learning_rate 0.00230707
2017-10-10T14:39:36.981658: step 197, loss 0.122208, acc 0.921875, learning_rate 0.00229806
2017-10-10T14:39:37.186475: step 198, loss 0.462785, acc 0.84375, learning_rate 0.00228908
2017-10-10T14:39:37.377126: step 199, loss 0.255351, acc 0.875, learning_rate 0.00228015
2017-10-10T14:39:37.572495: step 200, loss 0.414092, acc 0.875, learning_rate 0.00227125

Evaluation:
2017-10-10T14:39:37.954172: step 200, loss 0.284952, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-200

2017-10-10T14:39:38.878497: step 201, loss 0.481573, acc 0.78125, learning_rate 0.00226239
2017-10-10T14:39:39.077541: step 202, loss 0.354311, acc 0.890625, learning_rate 0.00225356
2017-10-10T14:39:39.282547: step 203, loss 0.368186, acc 0.890625, learning_rate 0.00224477
2017-10-10T14:39:39.477432: step 204, loss 0.249337, acc 0.859375, learning_rate 0.00223602
2017-10-10T14:39:39.700874: step 205, loss 0.288848, acc 0.921875, learning_rate 0.0022273
2017-10-10T14:39:39.945319: step 206, loss 0.519193, acc 0.84375, learning_rate 0.00221862
2017-10-10T14:39:40.067227: step 207, loss 0.412477, acc 0.90625, learning_rate 0.00220997
2017-10-10T14:39:40.190312: step 208, loss 0.158553, acc 0.9375, learning_rate 0.00220136
2017-10-10T14:39:40.313420: step 209, loss 0.178174, acc 0.921875, learning_rate 0.00219278
2017-10-10T14:39:40.432410: step 210, loss 0.336643, acc 0.890625, learning_rate 0.00218424
2017-10-10T14:39:40.563641: step 211, loss 0.227924, acc 0.9375, learning_rate 0.00217573
2017-10-10T14:39:40.683850: step 212, loss 0.213315, acc 0.953125, learning_rate 0.00216726
2017-10-10T14:39:40.890797: step 213, loss 0.243312, acc 0.90625, learning_rate 0.00215882
2017-10-10T14:39:41.091536: step 214, loss 0.437401, acc 0.890625, learning_rate 0.00215041
2017-10-10T14:39:41.281044: step 215, loss 0.206595, acc 0.921875, learning_rate 0.00214204
2017-10-10T14:39:41.478695: step 216, loss 0.259777, acc 0.921875, learning_rate 0.00213371
2017-10-10T14:39:41.689298: step 217, loss 0.301148, acc 0.890625, learning_rate 0.00212541
2017-10-10T14:39:41.896092: step 218, loss 0.357453, acc 0.890625, learning_rate 0.00211714
2017-10-10T14:39:42.079489: step 219, loss 0.407629, acc 0.875, learning_rate 0.00210891
2017-10-10T14:39:42.261245: step 220, loss 0.693096, acc 0.78125, learning_rate 0.00210071
2017-10-10T14:39:42.437817: step 221, loss 0.433248, acc 0.859375, learning_rate 0.00209254
2017-10-10T14:39:42.611168: step 222, loss 0.196162, acc 0.921875, learning_rate 0.00208441
2017-10-10T14:39:42.787790: step 223, loss 0.408951, acc 0.875, learning_rate 0.00207631
2017-10-10T14:39:42.952854: step 224, loss 0.326341, acc 0.890625, learning_rate 0.00206824
2017-10-10T14:39:43.134551: step 225, loss 0.199392, acc 0.921875, learning_rate 0.00206021
2017-10-10T14:39:43.323958: step 226, loss 0.318802, acc 0.859375, learning_rate 0.00205221
2017-10-10T14:39:43.531251: step 227, loss 0.30618, acc 0.921875, learning_rate 0.00204424
2017-10-10T14:39:43.730180: step 228, loss 0.439278, acc 0.875, learning_rate 0.0020363
2017-10-10T14:39:43.927832: step 229, loss 0.282974, acc 0.890625, learning_rate 0.0020284
2017-10-10T14:39:44.096244: step 230, loss 0.4999, acc 0.8125, learning_rate 0.00202053
2017-10-10T14:39:44.267370: step 231, loss 0.551559, acc 0.8125, learning_rate 0.00201269
2017-10-10T14:39:44.422454: step 232, loss 0.519338, acc 0.8125, learning_rate 0.00200488
2017-10-10T14:39:44.603819: step 233, loss 0.267319, acc 0.921875, learning_rate 0.00199711
2017-10-10T14:39:44.808351: step 234, loss 0.329181, acc 0.890625, learning_rate 0.00198936
2017-10-10T14:39:45.019356: step 235, loss 0.38658, acc 0.84375, learning_rate 0.00198165
2017-10-10T14:39:45.215480: step 236, loss 0.279736, acc 0.890625, learning_rate 0.00197397
2017-10-10T14:39:45.419233: step 237, loss 0.408938, acc 0.875, learning_rate 0.00196632
2017-10-10T14:39:45.615292: step 238, loss 0.359179, acc 0.875, learning_rate 0.0019587
2017-10-10T14:39:45.828533: step 239, loss 0.402772, acc 0.875, learning_rate 0.00195112
2017-10-10T14:39:46.032521: step 240, loss 0.294918, acc 0.8125, learning_rate 0.00194356

Evaluation:
2017-10-10T14:39:46.422473: step 240, loss 0.277517, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-240

2017-10-10T14:39:47.381559: step 241, loss 0.324426, acc 0.90625, learning_rate 0.00193604
2017-10-10T14:39:47.563138: step 242, loss 0.257559, acc 0.875, learning_rate 0.00192854
2017-10-10T14:39:47.744854: step 243, loss 0.491173, acc 0.859375, learning_rate 0.00192108
2017-10-10T14:39:47.924912: step 244, loss 0.269598, acc 0.859375, learning_rate 0.00191364
2017-10-10T14:39:48.093244: step 245, loss 0.58089, acc 0.875, learning_rate 0.00190624
2017-10-10T14:39:48.272902: step 246, loss 0.414208, acc 0.859375, learning_rate 0.00189887
2017-10-10T14:39:48.432879: step 247, loss 0.660973, acc 0.78125, learning_rate 0.00189153
2017-10-10T14:39:48.623394: step 248, loss 0.439821, acc 0.84375, learning_rate 0.00188421
2017-10-10T14:39:48.904568: step 249, loss 0.490469, acc 0.828125, learning_rate 0.00187693
2017-10-10T14:39:49.106148: step 250, loss 0.23195, acc 0.9375, learning_rate 0.00186968
2017-10-10T14:39:49.236111: step 251, loss 0.296645, acc 0.90625, learning_rate 0.00186245
2017-10-10T14:39:49.356971: step 252, loss 0.155289, acc 0.9375, learning_rate 0.00185526
2017-10-10T14:39:49.475284: step 253, loss 0.386273, acc 0.890625, learning_rate 0.0018481
2017-10-10T14:39:49.594801: step 254, loss 0.365286, acc 0.90625, learning_rate 0.00184096
2017-10-10T14:39:49.715583: step 255, loss 0.253064, acc 0.90625, learning_rate 0.00183385
2017-10-10T14:39:49.848954: step 256, loss 0.430412, acc 0.859375, learning_rate 0.00182678
2017-10-10T14:39:50.028845: step 257, loss 0.455646, acc 0.859375, learning_rate 0.00181973
2017-10-10T14:39:50.209004: step 258, loss 0.195282, acc 0.921875, learning_rate 0.00181271
2017-10-10T14:39:50.412214: step 259, loss 0.274973, acc 0.90625, learning_rate 0.00180572
2017-10-10T14:39:50.617241: step 260, loss 0.415099, acc 0.859375, learning_rate 0.00179876
2017-10-10T14:39:50.829087: step 261, loss 0.526282, acc 0.859375, learning_rate 0.00179182
2017-10-10T14:39:51.031362: step 262, loss 0.278256, acc 0.921875, learning_rate 0.00178492
2017-10-10T14:39:51.234140: step 263, loss 0.502778, acc 0.84375, learning_rate 0.00177804
2017-10-10T14:39:51.431193: step 264, loss 0.333991, acc 0.890625, learning_rate 0.00177119
2017-10-10T14:39:51.624055: step 265, loss 0.205328, acc 0.921875, learning_rate 0.00176437
2017-10-10T14:39:51.813888: step 266, loss 0.132005, acc 0.96875, learning_rate 0.00175758
2017-10-10T14:39:52.005247: step 267, loss 0.388122, acc 0.859375, learning_rate 0.00175081
2017-10-10T14:39:52.192007: step 268, loss 0.206175, acc 0.90625, learning_rate 0.00174407
2017-10-10T14:39:52.386856: step 269, loss 0.488423, acc 0.8125, learning_rate 0.00173736
2017-10-10T14:39:52.573772: step 270, loss 0.213586, acc 0.90625, learning_rate 0.00173068
2017-10-10T14:39:52.775258: step 271, loss 0.136164, acc 0.90625, learning_rate 0.00172402
2017-10-10T14:39:52.966825: step 272, loss 0.422502, acc 0.84375, learning_rate 0.00171739
2017-10-10T14:39:53.163007: step 273, loss 0.18533, acc 0.921875, learning_rate 0.00171079
2017-10-10T14:39:53.352423: step 274, loss 0.644099, acc 0.84375, learning_rate 0.00170422
2017-10-10T14:39:53.534345: step 275, loss 0.291667, acc 0.921875, learning_rate 0.00169767
2017-10-10T14:39:53.744089: step 276, loss 0.218517, acc 0.921875, learning_rate 0.00169115
2017-10-10T14:39:53.960777: step 277, loss 0.407517, acc 0.875, learning_rate 0.00168465
2017-10-10T14:39:54.161164: step 278, loss 0.325037, acc 0.890625, learning_rate 0.00167818
2017-10-10T14:39:54.377406: step 279, loss 0.520135, acc 0.828125, learning_rate 0.00167174
2017-10-10T14:39:54.585363: step 280, loss 0.192, acc 0.90625, learning_rate 0.00166533

Evaluation:
2017-10-10T14:39:54.969542: step 280, loss 0.268601, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-280

2017-10-10T14:39:55.963257: step 281, loss 0.365374, acc 0.890625, learning_rate 0.00165894
2017-10-10T14:39:56.129219: step 282, loss 0.614339, acc 0.8125, learning_rate 0.00165257
2017-10-10T14:39:56.318686: step 283, loss 0.386577, acc 0.890625, learning_rate 0.00164624
2017-10-10T14:39:56.516249: step 284, loss 0.350276, acc 0.890625, learning_rate 0.00163993
2017-10-10T14:39:56.716771: step 285, loss 0.258638, acc 0.90625, learning_rate 0.00163364
2017-10-10T14:39:56.914910: step 286, loss 0.18401, acc 0.921875, learning_rate 0.00162738
2017-10-10T14:39:57.097296: step 287, loss 0.280957, acc 0.875, learning_rate 0.00162115
2017-10-10T14:39:57.295704: step 288, loss 0.317228, acc 0.90625, learning_rate 0.00161494
2017-10-10T14:39:57.491145: step 289, loss 0.302519, acc 0.828125, learning_rate 0.00160875
2017-10-10T14:39:57.716869: step 290, loss 0.435783, acc 0.84375, learning_rate 0.00160259
2017-10-10T14:39:57.975230: step 291, loss 0.31517, acc 0.921875, learning_rate 0.00159646
2017-10-10T14:39:58.094704: step 292, loss 0.434503, acc 0.875, learning_rate 0.00159035
2017-10-10T14:39:58.222246: step 293, loss 0.316367, acc 0.859375, learning_rate 0.00158427
2017-10-10T14:39:58.328776: step 294, loss 0.21586, acc 0.960784, learning_rate 0.00157821
2017-10-10T14:39:58.454857: step 295, loss 0.161899, acc 0.953125, learning_rate 0.00157218
2017-10-10T14:39:58.581493: step 296, loss 0.408915, acc 0.859375, learning_rate 0.00156617
2017-10-10T14:39:58.712821: step 297, loss 0.233534, acc 0.921875, learning_rate 0.00156018
2017-10-10T14:39:58.935106: step 298, loss 0.0848367, acc 0.953125, learning_rate 0.00155422
2017-10-10T14:39:59.145136: step 299, loss 0.475511, acc 0.828125, learning_rate 0.00154829
2017-10-10T14:39:59.352456: step 300, loss 0.139878, acc 0.921875, learning_rate 0.00154238
2017-10-10T14:39:59.549100: step 301, loss 0.530566, acc 0.84375, learning_rate 0.00153649
2017-10-10T14:39:59.751308: step 302, loss 0.2609, acc 0.90625, learning_rate 0.00153063
2017-10-10T14:39:59.952991: step 303, loss 0.200448, acc 0.9375, learning_rate 0.00152479
2017-10-10T14:40:00.150654: step 304, loss 0.296664, acc 0.890625, learning_rate 0.00151897
2017-10-10T14:40:00.313520: step 305, loss 0.254586, acc 0.90625, learning_rate 0.00151318
2017-10-10T14:40:00.484874: step 306, loss 0.494846, acc 0.8125, learning_rate 0.00150741
2017-10-10T14:40:00.672868: step 307, loss 0.258541, acc 0.9375, learning_rate 0.00150167
2017-10-10T14:40:00.860862: step 308, loss 0.209854, acc 0.9375, learning_rate 0.00149594
2017-10-10T14:40:01.046341: step 309, loss 0.239633, acc 0.890625, learning_rate 0.00149025
2017-10-10T14:40:01.243889: step 310, loss 0.21322, acc 0.953125, learning_rate 0.00148457
2017-10-10T14:40:01.439781: step 311, loss 0.226593, acc 0.921875, learning_rate 0.00147892
2017-10-10T14:40:01.625903: step 312, loss 0.134899, acc 0.953125, learning_rate 0.00147329
2017-10-10T14:40:01.793027: step 313, loss 0.100553, acc 0.9375, learning_rate 0.00146769
2017-10-10T14:40:01.959197: step 314, loss 0.368496, acc 0.859375, learning_rate 0.0014621
2017-10-10T14:40:02.137058: step 315, loss 0.222282, acc 0.921875, learning_rate 0.00145654
2017-10-10T14:40:02.309208: step 316, loss 0.411036, acc 0.84375, learning_rate 0.00145101
2017-10-10T14:40:02.484857: step 317, loss 0.332785, acc 0.84375, learning_rate 0.00144549
2017-10-10T14:40:02.671898: step 318, loss 0.433471, acc 0.796875, learning_rate 0.00144
2017-10-10T14:40:02.872717: step 319, loss 0.310912, acc 0.921875, learning_rate 0.00143453
2017-10-10T14:40:03.074230: step 320, loss 0.310636, acc 0.875, learning_rate 0.00142908

Evaluation:
2017-10-10T14:40:03.444794: step 320, loss 0.245961, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-320

2017-10-10T14:40:04.309044: step 321, loss 0.260621, acc 0.890625, learning_rate 0.00142366
2017-10-10T14:40:04.515364: step 322, loss 0.206765, acc 0.921875, learning_rate 0.00141826
2017-10-10T14:40:04.722414: step 323, loss 0.521262, acc 0.859375, learning_rate 0.00141288
2017-10-10T14:40:04.926158: step 324, loss 0.520888, acc 0.84375, learning_rate 0.00140752
2017-10-10T14:40:05.126282: step 325, loss 0.284565, acc 0.890625, learning_rate 0.00140218
2017-10-10T14:40:05.322226: step 326, loss 0.296997, acc 0.890625, learning_rate 0.00139686
2017-10-10T14:40:05.507174: step 327, loss 0.265082, acc 0.90625, learning_rate 0.00139157
2017-10-10T14:40:05.694670: step 328, loss 0.307182, acc 0.890625, learning_rate 0.0013863
2017-10-10T14:40:05.894647: step 329, loss 0.124261, acc 0.96875, learning_rate 0.00138105
2017-10-10T14:40:06.084295: step 330, loss 0.216567, acc 0.90625, learning_rate 0.00137582
2017-10-10T14:40:06.278488: step 331, loss 0.372333, acc 0.890625, learning_rate 0.00137061
2017-10-10T14:40:06.464518: step 332, loss 0.270301, acc 0.90625, learning_rate 0.00136543
2017-10-10T14:40:06.672875: step 333, loss 0.395653, acc 0.890625, learning_rate 0.00136026
2017-10-10T14:40:06.943937: step 334, loss 0.260422, acc 0.9375, learning_rate 0.00135512
2017-10-10T14:40:07.064581: step 335, loss 0.128354, acc 0.9375, learning_rate 0.00134999
2017-10-10T14:40:07.183468: step 336, loss 0.430333, acc 0.875, learning_rate 0.00134489
2017-10-10T14:40:07.304791: step 337, loss 0.1849, acc 0.90625, learning_rate 0.00133981
2017-10-10T14:40:07.434062: step 338, loss 0.410266, acc 0.875, learning_rate 0.00133475
2017-10-10T14:40:07.577483: step 339, loss 0.173841, acc 0.9375, learning_rate 0.00132971
2017-10-10T14:40:07.753899: step 340, loss 0.268203, acc 0.921875, learning_rate 0.00132469
2017-10-10T14:40:07.931251: step 341, loss 0.165385, acc 0.9375, learning_rate 0.00131969
2017-10-10T14:40:08.129970: step 342, loss 0.109065, acc 0.953125, learning_rate 0.00131471
2017-10-10T14:40:08.329416: step 343, loss 0.21817, acc 0.90625, learning_rate 0.00130975
2017-10-10T14:40:08.531049: step 344, loss 0.179411, acc 0.9375, learning_rate 0.00130482
2017-10-10T14:40:08.718150: step 345, loss 0.452619, acc 0.859375, learning_rate 0.0012999
2017-10-10T14:40:08.904830: step 346, loss 0.300761, acc 0.90625, learning_rate 0.001295
2017-10-10T14:40:09.094023: step 347, loss 0.351178, acc 0.875, learning_rate 0.00129012
2017-10-10T14:40:09.288484: step 348, loss 0.258659, acc 0.921875, learning_rate 0.00128527
2017-10-10T14:40:09.464825: step 349, loss 0.245664, acc 0.921875, learning_rate 0.00128043
2017-10-10T14:40:09.640968: step 350, loss 0.281107, acc 0.890625, learning_rate 0.00127561
2017-10-10T14:40:09.829010: step 351, loss 0.219441, acc 0.953125, learning_rate 0.00127081
2017-10-10T14:40:09.997019: step 352, loss 0.0741467, acc 0.984375, learning_rate 0.00126603
2017-10-10T14:40:10.196868: step 353, loss 0.462165, acc 0.859375, learning_rate 0.00126127
2017-10-10T14:40:10.399210: step 354, loss 0.385381, acc 0.890625, learning_rate 0.00125653
2017-10-10T14:40:10.568934: step 355, loss 0.110125, acc 0.96875, learning_rate 0.00125181
2017-10-10T14:40:10.720358: step 356, loss 0.23596, acc 0.890625, learning_rate 0.00124711
2017-10-10T14:40:10.901542: step 357, loss 0.359866, acc 0.9375, learning_rate 0.00124243
2017-10-10T14:40:11.105632: step 358, loss 0.292764, acc 0.90625, learning_rate 0.00123777
2017-10-10T14:40:11.303822: step 359, loss 0.234397, acc 0.9375, learning_rate 0.00123312
2017-10-10T14:40:11.504869: step 360, loss 0.467077, acc 0.859375, learning_rate 0.0012285

Evaluation:
2017-10-10T14:40:11.898164: step 360, loss 0.246473, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-360

2017-10-10T14:40:12.819554: step 361, loss 0.136708, acc 0.9375, learning_rate 0.00122389
2017-10-10T14:40:13.024467: step 362, loss 0.184458, acc 0.890625, learning_rate 0.0012193
2017-10-10T14:40:13.230964: step 363, loss 0.218116, acc 0.921875, learning_rate 0.00121473
2017-10-10T14:40:13.440380: step 364, loss 0.277649, acc 0.90625, learning_rate 0.00121018
2017-10-10T14:40:13.646862: step 365, loss 0.343387, acc 0.890625, learning_rate 0.00120565
2017-10-10T14:40:13.841978: step 366, loss 0.237907, acc 0.921875, learning_rate 0.00120114
2017-10-10T14:40:14.051235: step 367, loss 0.32719, acc 0.890625, learning_rate 0.00119664
2017-10-10T14:40:14.249338: step 368, loss 0.174255, acc 0.921875, learning_rate 0.00119217
2017-10-10T14:40:14.447503: step 369, loss 0.275638, acc 0.90625, learning_rate 0.00118771
2017-10-10T14:40:14.650396: step 370, loss 0.296267, acc 0.921875, learning_rate 0.00118327
2017-10-10T14:40:14.850954: step 371, loss 0.155316, acc 0.96875, learning_rate 0.00117885
2017-10-10T14:40:15.057323: step 372, loss 0.261312, acc 0.921875, learning_rate 0.00117445
2017-10-10T14:40:15.255654: step 373, loss 0.150443, acc 0.9375, learning_rate 0.00117006
2017-10-10T14:40:15.483802: step 374, loss 0.306718, acc 0.890625, learning_rate 0.00116569
2017-10-10T14:40:15.723913: step 375, loss 0.302094, acc 0.90625, learning_rate 0.00116134
2017-10-10T14:40:15.850731: step 376, loss 0.0946134, acc 0.953125, learning_rate 0.00115701
2017-10-10T14:40:15.976026: step 377, loss 0.284038, acc 0.890625, learning_rate 0.0011527
2017-10-10T14:40:16.096276: step 378, loss 0.175277, acc 0.953125, learning_rate 0.0011484
2017-10-10T14:40:16.213977: step 379, loss 0.170196, acc 0.90625, learning_rate 0.00114412
2017-10-10T14:40:16.341706: step 380, loss 0.149598, acc 0.953125, learning_rate 0.00113986
2017-10-10T14:40:16.516885: step 381, loss 0.323252, acc 0.90625, learning_rate 0.00113561
2017-10-10T14:40:16.703729: step 382, loss 0.230702, acc 0.90625, learning_rate 0.00113139
2017-10-10T14:40:16.873086: step 383, loss 0.353578, acc 0.890625, learning_rate 0.00112718
2017-10-10T14:40:17.040837: step 384, loss 0.254516, acc 0.90625, learning_rate 0.00112298
2017-10-10T14:40:17.227099: step 385, loss 0.209845, acc 0.921875, learning_rate 0.00111881
2017-10-10T14:40:17.430132: step 386, loss 0.203254, acc 0.9375, learning_rate 0.00111465
2017-10-10T14:40:17.632762: step 387, loss 0.237714, acc 0.9375, learning_rate 0.00111051
2017-10-10T14:40:17.839024: step 388, loss 0.227066, acc 0.921875, learning_rate 0.00110638
2017-10-10T14:40:18.045775: step 389, loss 0.364351, acc 0.84375, learning_rate 0.00110228
2017-10-10T14:40:18.242938: step 390, loss 0.1457, acc 0.9375, learning_rate 0.00109818
2017-10-10T14:40:18.438358: step 391, loss 0.302037, acc 0.90625, learning_rate 0.00109411
2017-10-10T14:40:18.591240: step 392, loss 0.228326, acc 0.921569, learning_rate 0.00109005
2017-10-10T14:40:18.778103: step 393, loss 0.186369, acc 0.90625, learning_rate 0.00108601
2017-10-10T14:40:18.938194: step 394, loss 0.138962, acc 0.953125, learning_rate 0.00108199
2017-10-10T14:40:19.108854: step 395, loss 0.285269, acc 0.890625, learning_rate 0.00107798
2017-10-10T14:40:19.299402: step 396, loss 0.244686, acc 0.921875, learning_rate 0.00107399
2017-10-10T14:40:19.489956: step 397, loss 0.274915, acc 0.890625, learning_rate 0.00107001
2017-10-10T14:40:19.679769: step 398, loss 0.19959, acc 0.921875, learning_rate 0.00106605
2017-10-10T14:40:19.866978: step 399, loss 0.125197, acc 0.96875, learning_rate 0.00106211
2017-10-10T14:40:20.048623: step 400, loss 0.475037, acc 0.8125, learning_rate 0.00105818

Evaluation:
2017-10-10T14:40:20.416943: step 400, loss 0.239713, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-400

2017-10-10T14:40:21.426222: step 401, loss 0.186011, acc 0.90625, learning_rate 0.00105427
2017-10-10T14:40:21.620009: step 402, loss 0.336449, acc 0.859375, learning_rate 0.00105037
2017-10-10T14:40:21.797129: step 403, loss 0.256552, acc 0.9375, learning_rate 0.0010465
2017-10-10T14:40:21.976738: step 404, loss 0.251775, acc 0.90625, learning_rate 0.00104263
2017-10-10T14:40:22.165861: step 405, loss 0.42872, acc 0.9375, learning_rate 0.00103878
2017-10-10T14:40:22.357621: step 406, loss 0.225501, acc 0.890625, learning_rate 0.00103495
2017-10-10T14:40:22.535018: step 407, loss 0.313199, acc 0.890625, learning_rate 0.00103114
2017-10-10T14:40:22.716026: step 408, loss 0.157642, acc 0.921875, learning_rate 0.00102734
2017-10-10T14:40:22.908886: step 409, loss 0.418436, acc 0.890625, learning_rate 0.00102355
2017-10-10T14:40:23.087617: step 410, loss 0.273551, acc 0.9375, learning_rate 0.00101978
2017-10-10T14:40:23.290626: step 411, loss 0.182596, acc 0.953125, learning_rate 0.00101603
2017-10-10T14:40:23.510447: step 412, loss 0.171096, acc 0.96875, learning_rate 0.00101229
2017-10-10T14:40:23.714457: step 413, loss 0.183131, acc 0.90625, learning_rate 0.00100856
2017-10-10T14:40:23.890762: step 414, loss 0.175115, acc 0.9375, learning_rate 0.00100486
2017-10-10T14:40:24.094687: step 415, loss 0.119821, acc 0.953125, learning_rate 0.00100116
2017-10-10T14:40:24.315475: step 416, loss 0.160131, acc 0.953125, learning_rate 0.000997483
2017-10-10T14:40:24.574530: step 417, loss 0.237131, acc 0.90625, learning_rate 0.00099382
2017-10-10T14:40:24.697502: step 418, loss 0.288039, acc 0.875, learning_rate 0.000990172
2017-10-10T14:40:24.830371: step 419, loss 0.328314, acc 0.828125, learning_rate 0.000986538
2017-10-10T14:40:24.951239: step 420, loss 0.27863, acc 0.875, learning_rate 0.00098292
2017-10-10T14:40:25.081376: step 421, loss 0.182576, acc 0.9375, learning_rate 0.000979316
2017-10-10T14:40:25.213204: step 422, loss 0.214774, acc 0.90625, learning_rate 0.000975727
2017-10-10T14:40:25.355142: step 423, loss 0.151134, acc 0.9375, learning_rate 0.000972152
2017-10-10T14:40:25.536801: step 424, loss 0.117295, acc 0.96875, learning_rate 0.000968592
2017-10-10T14:40:25.732319: step 425, loss 0.223656, acc 0.9375, learning_rate 0.000965047
2017-10-10T14:40:25.929517: step 426, loss 0.137131, acc 0.9375, learning_rate 0.000961516
2017-10-10T14:40:26.125778: step 427, loss 0.142909, acc 0.953125, learning_rate 0.000958
2017-10-10T14:40:26.329433: step 428, loss 0.208382, acc 0.953125, learning_rate 0.000954497
2017-10-10T14:40:26.525908: step 429, loss 0.184224, acc 0.953125, learning_rate 0.00095101
2017-10-10T14:40:26.735329: step 430, loss 0.155684, acc 0.953125, learning_rate 0.000947536
2017-10-10T14:40:26.946868: step 431, loss 0.225392, acc 0.921875, learning_rate 0.000944076
2017-10-10T14:40:27.148243: step 432, loss 0.115702, acc 0.953125, learning_rate 0.000940631
2017-10-10T14:40:27.357211: step 433, loss 0.248276, acc 0.9375, learning_rate 0.0009372
2017-10-10T14:40:27.563898: step 434, loss 0.177346, acc 0.921875, learning_rate 0.000933783
2017-10-10T14:40:27.766159: step 435, loss 0.150487, acc 0.96875, learning_rate 0.000930379
2017-10-10T14:40:27.986681: step 436, loss 0.305409, acc 0.9375, learning_rate 0.00092699
2017-10-10T14:40:28.182512: step 437, loss 0.272876, acc 0.90625, learning_rate 0.000923614
2017-10-10T14:40:28.384383: step 438, loss 0.0824471, acc 0.96875, learning_rate 0.000920253
2017-10-10T14:40:28.570371: step 439, loss 0.219334, acc 0.890625, learning_rate 0.000916905
2017-10-10T14:40:28.748841: step 440, loss 0.169544, acc 0.9375, learning_rate 0.00091357

Evaluation:
2017-10-10T14:40:29.121571: step 440, loss 0.239843, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-440

2017-10-10T14:40:29.997415: step 441, loss 0.192127, acc 0.953125, learning_rate 0.000910249
2017-10-10T14:40:30.202021: step 442, loss 0.200784, acc 0.921875, learning_rate 0.000906942
2017-10-10T14:40:30.398426: step 443, loss 0.163713, acc 0.921875, learning_rate 0.000903648
2017-10-10T14:40:30.581876: step 444, loss 0.267279, acc 0.890625, learning_rate 0.000900368
2017-10-10T14:40:30.781912: step 445, loss 0.414829, acc 0.859375, learning_rate 0.000897101
2017-10-10T14:40:30.978857: step 446, loss 0.323452, acc 0.90625, learning_rate 0.000893848
2017-10-10T14:40:31.175915: step 447, loss 0.165669, acc 0.9375, learning_rate 0.000890607
2017-10-10T14:40:31.379751: step 448, loss 0.333463, acc 0.921875, learning_rate 0.00088738
2017-10-10T14:40:31.582301: step 449, loss 0.296521, acc 0.890625, learning_rate 0.000884166
2017-10-10T14:40:31.782739: step 450, loss 0.229038, acc 0.90625, learning_rate 0.000880966
2017-10-10T14:40:31.986287: step 451, loss 0.130347, acc 0.96875, learning_rate 0.000877778
2017-10-10T14:40:32.178866: step 452, loss 0.216756, acc 0.921875, learning_rate 0.000874603
2017-10-10T14:40:32.372492: step 453, loss 0.28233, acc 0.9375, learning_rate 0.000871441
2017-10-10T14:40:32.568328: step 454, loss 0.162546, acc 0.953125, learning_rate 0.000868293
2017-10-10T14:40:32.750042: step 455, loss 0.349479, acc 0.84375, learning_rate 0.000865157
2017-10-10T14:40:32.936948: step 456, loss 0.31909, acc 0.921875, learning_rate 0.000862033
2017-10-10T14:40:33.133007: step 457, loss 0.180826, acc 0.953125, learning_rate 0.000858923
2017-10-10T14:40:33.364844: step 458, loss 0.307606, acc 0.90625, learning_rate 0.000855825
2017-10-10T14:40:33.616528: step 459, loss 0.173638, acc 0.921875, learning_rate 0.00085274
2017-10-10T14:40:33.741005: step 460, loss 0.127862, acc 0.96875, learning_rate 0.000849668
2017-10-10T14:40:33.863716: step 461, loss 0.220627, acc 0.953125, learning_rate 0.000846608
2017-10-10T14:40:33.993951: step 462, loss 0.198888, acc 0.953125, learning_rate 0.00084356
2017-10-10T14:40:34.114052: step 463, loss 0.315078, acc 0.875, learning_rate 0.000840525
2017-10-10T14:40:34.233145: step 464, loss 0.166544, acc 0.9375, learning_rate 0.000837502
2017-10-10T14:40:34.352317: step 465, loss 0.268752, acc 0.921875, learning_rate 0.000834492
2017-10-10T14:40:34.559793: step 466, loss 0.288326, acc 0.921875, learning_rate 0.000831494
2017-10-10T14:40:34.759336: step 467, loss 0.166298, acc 0.953125, learning_rate 0.000828508
2017-10-10T14:40:34.972551: step 468, loss 0.327383, acc 0.875, learning_rate 0.000825535
2017-10-10T14:40:35.166440: step 469, loss 0.244329, acc 0.9375, learning_rate 0.000822573
2017-10-10T14:40:35.362871: step 470, loss 0.105707, acc 0.96875, learning_rate 0.000819624
2017-10-10T14:40:35.522217: step 471, loss 0.210568, acc 0.90625, learning_rate 0.000816687
2017-10-10T14:40:35.678936: step 472, loss 0.125499, acc 0.9375, learning_rate 0.000813761
2017-10-10T14:40:35.896840: step 473, loss 0.163839, acc 0.953125, learning_rate 0.000810848
2017-10-10T14:40:36.040659: step 474, loss 0.110671, acc 0.9375, learning_rate 0.000807946
2017-10-10T14:40:36.222144: step 475, loss 0.259902, acc 0.875, learning_rate 0.000805057
2017-10-10T14:40:36.412626: step 476, loss 0.190982, acc 0.953125, learning_rate 0.000802179
2017-10-10T14:40:36.604819: step 477, loss 0.157761, acc 0.9375, learning_rate 0.000799313
2017-10-10T14:40:36.794134: step 478, loss 0.186898, acc 0.921875, learning_rate 0.000796458
2017-10-10T14:40:36.989763: step 479, loss 0.225724, acc 0.890625, learning_rate 0.000793616
2017-10-10T14:40:37.190697: step 480, loss 0.219388, acc 0.921875, learning_rate 0.000790784

Evaluation:
2017-10-10T14:40:37.570037: step 480, loss 0.237763, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-480

2017-10-10T14:40:38.548667: step 481, loss 0.263797, acc 0.90625, learning_rate 0.000787965
2017-10-10T14:40:38.750630: step 482, loss 0.26264, acc 0.921875, learning_rate 0.000785157
2017-10-10T14:40:38.956735: step 483, loss 0.216956, acc 0.921875, learning_rate 0.00078236
2017-10-10T14:40:39.166312: step 484, loss 0.13332, acc 0.921875, learning_rate 0.000779575
2017-10-10T14:40:39.373368: step 485, loss 0.224584, acc 0.96875, learning_rate 0.000776801
2017-10-10T14:40:39.616105: step 486, loss 0.182473, acc 0.921875, learning_rate 0.000774038
2017-10-10T14:40:39.840142: step 487, loss 0.104584, acc 0.953125, learning_rate 0.000771287
2017-10-10T14:40:40.066265: step 488, loss 0.223795, acc 0.90625, learning_rate 0.000768547
2017-10-10T14:40:40.302428: step 489, loss 0.323944, acc 0.859375, learning_rate 0.000765818
2017-10-10T14:40:40.503187: step 490, loss 0.149801, acc 0.960784, learning_rate 0.000763101
2017-10-10T14:40:40.677071: step 491, loss 0.108544, acc 0.953125, learning_rate 0.000760394
2017-10-10T14:40:40.873781: step 492, loss 0.141865, acc 0.953125, learning_rate 0.000757698
2017-10-10T14:40:41.050558: step 493, loss 0.229327, acc 0.9375, learning_rate 0.000755014
2017-10-10T14:40:41.258905: step 494, loss 0.20131, acc 0.953125, learning_rate 0.00075234
2017-10-10T14:40:41.474858: step 495, loss 0.301426, acc 0.875, learning_rate 0.000749677
2017-10-10T14:40:41.687543: step 496, loss 0.247896, acc 0.9375, learning_rate 0.000747026
2017-10-10T14:40:41.903197: step 497, loss 0.335066, acc 0.859375, learning_rate 0.000744385
2017-10-10T14:40:42.110951: step 498, loss 0.245398, acc 0.90625, learning_rate 0.000741754
2017-10-10T14:40:42.300639: step 499, loss 0.046171, acc 0.984375, learning_rate 0.000739135
2017-10-10T14:40:42.480788: step 500, loss 0.116594, acc 0.953125, learning_rate 0.000736526
2017-10-10T14:40:42.652857: step 501, loss 0.0744558, acc 0.984375, learning_rate 0.000733928
2017-10-10T14:40:42.940953: step 502, loss 0.307551, acc 0.90625, learning_rate 0.00073134
2017-10-10T14:40:43.152884: step 503, loss 0.239476, acc 0.921875, learning_rate 0.000728763
2017-10-10T14:40:43.287711: step 504, loss 0.187084, acc 0.921875, learning_rate 0.000726197
2017-10-10T14:40:43.426282: step 505, loss 0.127108, acc 0.953125, learning_rate 0.000723641
2017-10-10T14:40:43.552359: step 506, loss 0.206596, acc 0.921875, learning_rate 0.000721095
2017-10-10T14:40:43.690218: step 507, loss 0.199075, acc 0.921875, learning_rate 0.00071856
2017-10-10T14:40:43.830373: step 508, loss 0.149667, acc 0.921875, learning_rate 0.000716036
2017-10-10T14:40:44.063076: step 509, loss 0.369409, acc 0.859375, learning_rate 0.000713521
2017-10-10T14:40:44.282707: step 510, loss 0.249586, acc 0.9375, learning_rate 0.000711017
2017-10-10T14:40:44.502358: step 511, loss 0.200381, acc 0.9375, learning_rate 0.000708523
2017-10-10T14:40:44.715900: step 512, loss 0.237381, acc 0.890625, learning_rate 0.000706039
2017-10-10T14:40:44.935627: step 513, loss 0.405576, acc 0.875, learning_rate 0.000703565
2017-10-10T14:40:45.142603: step 514, loss 0.0328966, acc 1, learning_rate 0.000701102
2017-10-10T14:40:45.356235: step 515, loss 0.212612, acc 0.953125, learning_rate 0.000698648
2017-10-10T14:40:45.575688: step 516, loss 0.179294, acc 0.96875, learning_rate 0.000696204
2017-10-10T14:40:45.805713: step 517, loss 0.179697, acc 0.9375, learning_rate 0.000693771
2017-10-10T14:40:46.012500: step 518, loss 0.250106, acc 0.890625, learning_rate 0.000691347
2017-10-10T14:40:46.240067: step 519, loss 0.206466, acc 0.90625, learning_rate 0.000688934
2017-10-10T14:40:46.463490: step 520, loss 0.234799, acc 0.953125, learning_rate 0.00068653

Evaluation:
2017-10-10T14:40:46.940790: step 520, loss 0.229104, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-520

2017-10-10T14:40:47.901414: step 521, loss 0.208693, acc 0.984375, learning_rate 0.000684136
2017-10-10T14:40:48.109946: step 522, loss 0.23391, acc 0.9375, learning_rate 0.000681751
2017-10-10T14:40:48.314794: step 523, loss 0.349471, acc 0.859375, learning_rate 0.000679377
2017-10-10T14:40:48.506588: step 524, loss 0.203199, acc 0.890625, learning_rate 0.000677012
2017-10-10T14:40:48.680244: step 525, loss 0.171913, acc 0.9375, learning_rate 0.000674657
2017-10-10T14:40:48.885885: step 526, loss 0.296829, acc 0.90625, learning_rate 0.000672311
2017-10-10T14:40:49.055626: step 527, loss 0.0954721, acc 0.96875, learning_rate 0.000669975
2017-10-10T14:40:49.274130: step 528, loss 0.310096, acc 0.859375, learning_rate 0.000667648
2017-10-10T14:40:49.472822: step 529, loss 0.154121, acc 0.9375, learning_rate 0.000665331
2017-10-10T14:40:49.684822: step 530, loss 0.12651, acc 0.96875, learning_rate 0.000663024
2017-10-10T14:40:49.886010: step 531, loss 0.202373, acc 0.953125, learning_rate 0.000660726
2017-10-10T14:40:50.096190: step 532, loss 0.427919, acc 0.875, learning_rate 0.000658437
2017-10-10T14:40:50.308370: step 533, loss 0.248185, acc 0.90625, learning_rate 0.000656158
2017-10-10T14:40:50.514055: step 534, loss 0.240561, acc 0.890625, learning_rate 0.000653888
2017-10-10T14:40:50.696044: step 535, loss 0.379085, acc 0.859375, learning_rate 0.000651627
2017-10-10T14:40:50.890164: step 536, loss 0.280113, acc 0.90625, learning_rate 0.000649375
2017-10-10T14:40:51.108462: step 537, loss 0.241531, acc 0.890625, learning_rate 0.000647133
2017-10-10T14:40:51.309592: step 538, loss 0.0775588, acc 0.96875, learning_rate 0.000644899
2017-10-10T14:40:51.512930: step 539, loss 0.275107, acc 0.890625, learning_rate 0.000642675
2017-10-10T14:40:51.713647: step 540, loss 0.17665, acc 0.953125, learning_rate 0.00064046
2017-10-10T14:40:51.884875: step 541, loss 0.175686, acc 0.953125, learning_rate 0.000638254
2017-10-10T14:40:52.088852: step 542, loss 0.321854, acc 0.90625, learning_rate 0.000636057
2017-10-10T14:40:52.310000: step 543, loss 0.0704195, acc 0.96875, learning_rate 0.000633869
2017-10-10T14:40:52.588862: step 544, loss 0.564748, acc 0.875, learning_rate 0.00063169
2017-10-10T14:40:52.842543: step 545, loss 0.162529, acc 0.96875, learning_rate 0.00062952
2017-10-10T14:40:52.968730: step 546, loss 0.148747, acc 0.953125, learning_rate 0.000627358
2017-10-10T14:40:53.107319: step 547, loss 0.181681, acc 0.9375, learning_rate 0.000625206
2017-10-10T14:40:53.244409: step 548, loss 0.212252, acc 0.921875, learning_rate 0.000623062
2017-10-10T14:40:53.374647: step 549, loss 0.204118, acc 0.953125, learning_rate 0.000620927
2017-10-10T14:40:53.501488: step 550, loss 0.124705, acc 0.96875, learning_rate 0.000618801
2017-10-10T14:40:53.690492: step 551, loss 0.0987409, acc 0.96875, learning_rate 0.000616683
2017-10-10T14:40:53.912225: step 552, loss 0.0816786, acc 0.984375, learning_rate 0.000614574
2017-10-10T14:40:54.112538: step 553, loss 0.209884, acc 0.921875, learning_rate 0.000612474
2017-10-10T14:40:54.308853: step 554, loss 0.179035, acc 0.9375, learning_rate 0.000610382
2017-10-10T14:40:54.499633: step 555, loss 0.0803218, acc 0.96875, learning_rate 0.000608299
2017-10-10T14:40:54.672867: step 556, loss 0.178837, acc 0.9375, learning_rate 0.000606224
2017-10-10T14:40:54.893217: step 557, loss 0.149354, acc 0.921875, learning_rate 0.000604158
2017-10-10T14:40:55.096141: step 558, loss 0.133287, acc 0.953125, learning_rate 0.0006021
2017-10-10T14:40:55.325342: step 559, loss 0.167738, acc 0.953125, learning_rate 0.00060005
2017-10-10T14:40:55.550540: step 560, loss 0.119492, acc 0.96875, learning_rate 0.000598009

Evaluation:
2017-10-10T14:40:55.971181: step 560, loss 0.231993, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-560

2017-10-10T14:40:56.919657: step 561, loss 0.184454, acc 0.9375, learning_rate 0.000595977
2017-10-10T14:40:57.120072: step 562, loss 0.346743, acc 0.90625, learning_rate 0.000593952
2017-10-10T14:40:57.332435: step 563, loss 0.205305, acc 0.921875, learning_rate 0.000591936
2017-10-10T14:40:57.497859: step 564, loss 0.218421, acc 0.953125, learning_rate 0.000589928
2017-10-10T14:40:57.715549: step 565, loss 0.162838, acc 0.90625, learning_rate 0.000587928
2017-10-10T14:40:57.928069: step 566, loss 0.169059, acc 0.921875, learning_rate 0.000585937
2017-10-10T14:40:58.129141: step 567, loss 0.249068, acc 0.90625, learning_rate 0.000583953
2017-10-10T14:40:58.348164: step 568, loss 0.209444, acc 0.890625, learning_rate 0.000581978
2017-10-10T14:40:58.549557: step 569, loss 0.21397, acc 0.921875, learning_rate 0.00058001
2017-10-10T14:40:58.729055: step 570, loss 0.349614, acc 0.859375, learning_rate 0.000578051
2017-10-10T14:40:58.956382: step 571, loss 0.0798098, acc 0.984375, learning_rate 0.0005761
2017-10-10T14:40:59.115892: step 572, loss 0.172015, acc 0.921875, learning_rate 0.000574157
2017-10-10T14:40:59.328872: step 573, loss 0.161407, acc 0.96875, learning_rate 0.000572221
2017-10-10T14:40:59.541063: step 574, loss 0.0915183, acc 0.96875, learning_rate 0.000570294
2017-10-10T14:40:59.767369: step 575, loss 0.176946, acc 0.953125, learning_rate 0.000568374
2017-10-10T14:40:59.980547: step 576, loss 0.18453, acc 0.921875, learning_rate 0.000566462
2017-10-10T14:41:00.184355: step 577, loss 0.260026, acc 0.875, learning_rate 0.000564558
2017-10-10T14:41:00.405588: step 578, loss 0.218005, acc 0.921875, learning_rate 0.000562662
2017-10-10T14:41:00.565212: step 579, loss 0.154505, acc 0.953125, learning_rate 0.000560774
2017-10-10T14:41:00.777052: step 580, loss 0.232955, acc 0.9375, learning_rate 0.000558893
2017-10-10T14:41:00.961120: step 581, loss 0.36566, acc 0.90625, learning_rate 0.00055702
2017-10-10T14:41:01.154571: step 582, loss 0.102828, acc 0.953125, learning_rate 0.000555154
2017-10-10T14:41:01.359806: step 583, loss 0.1569, acc 0.921875, learning_rate 0.000553296
2017-10-10T14:41:01.572492: step 584, loss 0.209507, acc 0.953125, learning_rate 0.000551446
2017-10-10T14:41:01.792096: step 585, loss 0.104799, acc 0.9375, learning_rate 0.000549604
2017-10-10T14:41:02.004494: step 586, loss 0.160384, acc 0.953125, learning_rate 0.000547768
2017-10-10T14:41:02.210393: step 587, loss 0.20064, acc 0.96875, learning_rate 0.000545941
2017-10-10T14:41:02.444821: step 588, loss 0.461994, acc 0.862745, learning_rate 0.00054412
2017-10-10T14:41:02.715604: step 589, loss 0.114337, acc 0.9375, learning_rate 0.000542308
2017-10-10T14:41:02.851278: step 590, loss 0.194558, acc 0.9375, learning_rate 0.000540502
2017-10-10T14:41:02.986917: step 591, loss 0.153185, acc 0.9375, learning_rate 0.000538704
2017-10-10T14:41:03.112579: step 592, loss 0.132809, acc 0.953125, learning_rate 0.000536914
2017-10-10T14:41:03.248555: step 593, loss 0.182886, acc 0.890625, learning_rate 0.00053513
2017-10-10T14:41:03.388017: step 594, loss 0.064228, acc 1, learning_rate 0.000533354
2017-10-10T14:41:03.521431: step 595, loss 0.167389, acc 0.984375, learning_rate 0.000531585
2017-10-10T14:41:03.710641: step 596, loss 0.248369, acc 0.890625, learning_rate 0.000529824
2017-10-10T14:41:03.935576: step 597, loss 0.2123, acc 0.90625, learning_rate 0.000528069
2017-10-10T14:41:04.154158: step 598, loss 0.149239, acc 0.96875, learning_rate 0.000526322
2017-10-10T14:41:04.365135: step 599, loss 0.128697, acc 0.9375, learning_rate 0.000524582
2017-10-10T14:41:04.594307: step 600, loss 0.21976, acc 0.90625, learning_rate 0.000522849

Evaluation:
2017-10-10T14:41:04.992862: step 600, loss 0.22719, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-600

2017-10-10T14:41:05.958083: step 601, loss 0.345213, acc 0.890625, learning_rate 0.000521123
2017-10-10T14:41:06.142466: step 602, loss 0.177737, acc 0.953125, learning_rate 0.000519404
2017-10-10T14:41:06.343045: step 603, loss 0.152706, acc 0.953125, learning_rate 0.000517692
2017-10-10T14:41:06.555238: step 604, loss 0.262687, acc 0.90625, learning_rate 0.000515987
2017-10-10T14:41:06.768832: step 605, loss 0.115926, acc 0.9375, learning_rate 0.000514289
2017-10-10T14:41:06.978879: step 606, loss 0.107544, acc 0.953125, learning_rate 0.000512598
2017-10-10T14:41:07.195920: step 607, loss 0.237408, acc 0.9375, learning_rate 0.000510914
2017-10-10T14:41:07.424374: step 608, loss 0.240693, acc 0.921875, learning_rate 0.000509237
2017-10-10T14:41:07.675151: step 609, loss 0.194366, acc 0.9375, learning_rate 0.000507566
2017-10-10T14:41:07.933036: step 610, loss 0.167009, acc 0.953125, learning_rate 0.000505903
2017-10-10T14:41:08.149656: step 611, loss 0.0928661, acc 0.953125, learning_rate 0.000504246
2017-10-10T14:41:08.373244: step 612, loss 0.279493, acc 0.9375, learning_rate 0.000502596
2017-10-10T14:41:08.676864: step 613, loss 0.202329, acc 0.90625, learning_rate 0.000500953
2017-10-10T14:41:08.925033: step 614, loss 0.199362, acc 0.953125, learning_rate 0.000499316
2017-10-10T14:41:09.180863: step 615, loss 0.164633, acc 0.9375, learning_rate 0.000497686
2017-10-10T14:41:09.505177: step 616, loss 0.162408, acc 0.9375, learning_rate 0.000496063
2017-10-10T14:41:09.771835: step 617, loss 0.256122, acc 0.9375, learning_rate 0.000494446
2017-10-10T14:41:10.018516: step 618, loss 0.189294, acc 0.921875, learning_rate 0.000492836
2017-10-10T14:41:10.274800: step 619, loss 0.165741, acc 0.953125, learning_rate 0.000491233
2017-10-10T14:41:10.523565: step 620, loss 0.122631, acc 0.96875, learning_rate 0.000489636
2017-10-10T14:41:10.800109: step 621, loss 0.0878663, acc 0.96875, learning_rate 0.000488045
2017-10-10T14:41:11.050329: step 622, loss 0.211267, acc 0.890625, learning_rate 0.000486461
2017-10-10T14:41:11.292822: step 623, loss 0.23036, acc 0.90625, learning_rate 0.000484884
2017-10-10T14:41:11.544072: step 624, loss 0.188132, acc 0.9375, learning_rate 0.000483313
2017-10-10T14:41:11.746497: step 625, loss 0.0690826, acc 0.984375, learning_rate 0.000481748
2017-10-10T14:41:11.969844: step 626, loss 0.190245, acc 0.9375, learning_rate 0.00048019
2017-10-10T14:41:12.249187: step 627, loss 0.153721, acc 0.96875, learning_rate 0.000478638
2017-10-10T14:41:12.531777: step 628, loss 0.271756, acc 0.9375, learning_rate 0.000477093
2017-10-10T14:41:12.768555: step 629, loss 0.0723875, acc 0.984375, learning_rate 0.000475554
2017-10-10T14:41:13.033918: step 630, loss 0.0816529, acc 0.984375, learning_rate 0.000474021
2017-10-10T14:41:13.324940: step 631, loss 0.160534, acc 0.96875, learning_rate 0.000472494
2017-10-10T14:41:13.637449: step 632, loss 0.181847, acc 0.921875, learning_rate 0.000470974
2017-10-10T14:41:13.804659: step 633, loss 0.202305, acc 0.9375, learning_rate 0.000469459
2017-10-10T14:41:13.977993: step 634, loss 0.148721, acc 0.90625, learning_rate 0.000467951
2017-10-10T14:41:14.156708: step 635, loss 0.229773, acc 0.953125, learning_rate 0.000466449
2017-10-10T14:41:14.390631: step 636, loss 0.174305, acc 0.890625, learning_rate 0.000464954
2017-10-10T14:41:14.576852: step 637, loss 0.30606, acc 0.90625, learning_rate 0.000463464
2017-10-10T14:41:14.796233: step 638, loss 0.0950393, acc 0.96875, learning_rate 0.00046198
2017-10-10T14:41:14.981152: step 639, loss 0.147843, acc 0.921875, learning_rate 0.000460503
2017-10-10T14:41:15.233050: step 640, loss 0.168463, acc 0.9375, learning_rate 0.000459031

Evaluation:
2017-10-10T14:41:15.726349: step 640, loss 0.229475, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-640

2017-10-10T14:41:16.885744: step 641, loss 0.162296, acc 0.9375, learning_rate 0.000457566
2017-10-10T14:41:17.140895: step 642, loss 0.138124, acc 0.953125, learning_rate 0.000456106
2017-10-10T14:41:17.381573: step 643, loss 0.235046, acc 0.90625, learning_rate 0.000454653
2017-10-10T14:41:17.604107: step 644, loss 0.280418, acc 0.890625, learning_rate 0.000453205
2017-10-10T14:41:17.827254: step 645, loss 0.271559, acc 0.890625, learning_rate 0.000451764
2017-10-10T14:41:18.082838: step 646, loss 0.276055, acc 0.859375, learning_rate 0.000450328
2017-10-10T14:41:18.339179: step 647, loss 0.0777209, acc 0.96875, learning_rate 0.000448898
2017-10-10T14:41:18.577753: step 648, loss 0.275372, acc 0.875, learning_rate 0.000447474
2017-10-10T14:41:18.803191: step 649, loss 0.263929, acc 0.90625, learning_rate 0.000446055
2017-10-10T14:41:19.048917: step 650, loss 0.125741, acc 0.96875, learning_rate 0.000444643
2017-10-10T14:41:19.245216: step 651, loss 0.275397, acc 0.9375, learning_rate 0.000443236
2017-10-10T14:41:19.456117: step 652, loss 0.128427, acc 0.96875, learning_rate 0.000441835
2017-10-10T14:41:19.708335: step 653, loss 0.315667, acc 0.875, learning_rate 0.00044044
2017-10-10T14:41:19.972687: step 654, loss 0.0970422, acc 0.96875, learning_rate 0.00043905
2017-10-10T14:41:20.221367: step 655, loss 0.170784, acc 0.9375, learning_rate 0.000437666
2017-10-10T14:41:20.467216: step 656, loss 0.181598, acc 0.953125, learning_rate 0.000436288
2017-10-10T14:41:20.739336: step 657, loss 0.287389, acc 0.9375, learning_rate 0.000434915
2017-10-10T14:41:21.000235: step 658, loss 0.265656, acc 0.921875, learning_rate 0.000433548
2017-10-10T14:41:21.212918: step 659, loss 0.131176, acc 0.953125, learning_rate 0.000432187
2017-10-10T14:41:21.472880: step 660, loss 0.299315, acc 0.875, learning_rate 0.000430831
2017-10-10T14:41:21.772857: step 661, loss 0.14458, acc 0.953125, learning_rate 0.000429481
2017-10-10T14:41:21.958884: step 662, loss 0.200613, acc 0.9375, learning_rate 0.000428136
2017-10-10T14:41:22.165150: step 663, loss 0.161036, acc 0.953125, learning_rate 0.000426796
2017-10-10T14:41:22.360921: step 664, loss 0.250397, acc 0.90625, learning_rate 0.000425463
2017-10-10T14:41:22.600937: step 665, loss 0.1905, acc 0.96875, learning_rate 0.000424134
2017-10-10T14:41:22.824534: step 666, loss 0.173999, acc 0.921875, learning_rate 0.000422811
2017-10-10T14:41:23.052866: step 667, loss 0.0970286, acc 0.984375, learning_rate 0.000421493
2017-10-10T14:41:23.292311: step 668, loss 0.226543, acc 0.9375, learning_rate 0.000420181
2017-10-10T14:41:23.550020: step 669, loss 0.168615, acc 0.953125, learning_rate 0.000418874
2017-10-10T14:41:23.778656: step 670, loss 0.135492, acc 0.953125, learning_rate 0.000417573
2017-10-10T14:41:23.998616: step 671, loss 0.16243, acc 0.921875, learning_rate 0.000416276
2017-10-10T14:41:24.226490: step 672, loss 0.171179, acc 0.921875, learning_rate 0.000414985
2017-10-10T14:41:24.499125: step 673, loss 0.259462, acc 0.90625, learning_rate 0.0004137
2017-10-10T14:41:24.725907: step 674, loss 0.190678, acc 0.9375, learning_rate 0.000412419
2017-10-10T14:41:25.120998: step 675, loss 0.09853, acc 0.953125, learning_rate 0.000411144
2017-10-10T14:41:25.302206: step 676, loss 0.156897, acc 0.9375, learning_rate 0.000409874
2017-10-10T14:41:25.462990: step 677, loss 0.134992, acc 0.96875, learning_rate 0.000408609
2017-10-10T14:41:25.652678: step 678, loss 0.233793, acc 0.90625, learning_rate 0.00040735
2017-10-10T14:41:25.818665: step 679, loss 0.224632, acc 0.90625, learning_rate 0.000406095
2017-10-10T14:41:25.999852: step 680, loss 0.210964, acc 0.9375, learning_rate 0.000404846

Evaluation:
2017-10-10T14:41:26.465135: step 680, loss 0.229199, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-680

2017-10-10T14:41:27.624146: step 681, loss 0.188713, acc 0.9375, learning_rate 0.000403601
2017-10-10T14:41:27.884922: step 682, loss 0.198524, acc 0.9375, learning_rate 0.000402362
2017-10-10T14:41:28.141371: step 683, loss 0.140446, acc 0.953125, learning_rate 0.000401128
2017-10-10T14:41:28.405399: step 684, loss 0.205507, acc 0.9375, learning_rate 0.000399899
2017-10-10T14:41:28.632828: step 685, loss 0.205345, acc 0.90625, learning_rate 0.000398675
2017-10-10T14:41:28.830192: step 686, loss 0.296879, acc 0.941176, learning_rate 0.000397456
2017-10-10T14:41:29.091649: step 687, loss 0.133643, acc 0.96875, learning_rate 0.000396241
2017-10-10T14:41:29.300499: step 688, loss 0.141178, acc 0.9375, learning_rate 0.000395032
2017-10-10T14:41:29.494483: step 689, loss 0.240323, acc 0.90625, learning_rate 0.000393828
2017-10-10T14:41:29.689513: step 690, loss 0.0758879, acc 0.984375, learning_rate 0.000392629
2017-10-10T14:41:29.937293: step 691, loss 0.329493, acc 0.921875, learning_rate 0.000391434
2017-10-10T14:41:30.206906: step 692, loss 0.277813, acc 0.921875, learning_rate 0.000390245
2017-10-10T14:41:30.450593: step 693, loss 0.236601, acc 0.9375, learning_rate 0.00038906
2017-10-10T14:41:30.716966: step 694, loss 0.118995, acc 0.96875, learning_rate 0.00038788
2017-10-10T14:41:30.986234: step 695, loss 0.169349, acc 0.921875, learning_rate 0.000386705
2017-10-10T14:41:31.229740: step 696, loss 0.268089, acc 0.921875, learning_rate 0.000385535
2017-10-10T14:41:31.476847: step 697, loss 0.0686165, acc 0.984375, learning_rate 0.000384369
2017-10-10T14:41:31.704232: step 698, loss 0.17801, acc 0.90625, learning_rate 0.000383209
2017-10-10T14:41:31.913624: step 699, loss 0.134921, acc 0.96875, learning_rate 0.000382053
2017-10-10T14:41:32.138040: step 700, loss 0.17766, acc 0.9375, learning_rate 0.000380901
2017-10-10T14:41:32.367238: step 701, loss 0.0966056, acc 0.953125, learning_rate 0.000379755
2017-10-10T14:41:32.620973: step 702, loss 0.121197, acc 0.96875, learning_rate 0.000378613
2017-10-10T14:41:32.904964: step 703, loss 0.161584, acc 0.921875, learning_rate 0.000377476
2017-10-10T14:41:33.166523: step 704, loss 0.0970999, acc 0.96875, learning_rate 0.000376343
2017-10-10T14:41:33.416717: step 705, loss 0.141654, acc 0.9375, learning_rate 0.000375215
2017-10-10T14:41:33.662018: step 706, loss 0.217178, acc 0.921875, learning_rate 0.000374092
2017-10-10T14:41:33.920306: step 707, loss 0.204616, acc 0.90625, learning_rate 0.000372973
2017-10-10T14:41:34.172411: step 708, loss 0.168442, acc 0.921875, learning_rate 0.000371859
2017-10-10T14:41:34.407378: step 709, loss 0.17929, acc 0.921875, learning_rate 0.000370749
2017-10-10T14:41:34.669202: step 710, loss 0.185648, acc 0.9375, learning_rate 0.000369644
2017-10-10T14:41:34.917002: step 711, loss 0.211111, acc 0.921875, learning_rate 0.000368543
2017-10-10T14:41:35.177165: step 712, loss 0.219734, acc 0.90625, learning_rate 0.000367447
2017-10-10T14:41:35.420284: step 713, loss 0.19277, acc 0.90625, learning_rate 0.000366356
2017-10-10T14:41:35.684438: step 714, loss 0.361637, acc 0.921875, learning_rate 0.000365268
2017-10-10T14:41:35.923760: step 715, loss 0.102242, acc 0.96875, learning_rate 0.000364186
2017-10-10T14:41:36.187746: step 716, loss 0.207784, acc 0.9375, learning_rate 0.000363107
2017-10-10T14:41:36.405322: step 717, loss 0.0776511, acc 1, learning_rate 0.000362033
2017-10-10T14:41:36.737169: step 718, loss 0.195581, acc 0.90625, learning_rate 0.000360964
2017-10-10T14:41:36.992433: step 719, loss 0.163717, acc 0.921875, learning_rate 0.000359899
2017-10-10T14:41:37.136787: step 720, loss 0.119741, acc 0.953125, learning_rate 0.000358838

Evaluation:
2017-10-10T14:41:37.449023: step 720, loss 0.227281, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-720

2017-10-10T14:41:38.276155: step 721, loss 0.214471, acc 0.90625, learning_rate 0.000357781
2017-10-10T14:41:38.544302: step 722, loss 0.210151, acc 0.9375, learning_rate 0.000356729
2017-10-10T14:41:38.801355: step 723, loss 0.223212, acc 0.90625, learning_rate 0.000355681
2017-10-10T14:41:39.065398: step 724, loss 0.126117, acc 0.921875, learning_rate 0.000354637
2017-10-10T14:41:39.323845: step 725, loss 0.236112, acc 0.921875, learning_rate 0.000353598
2017-10-10T14:41:39.593607: step 726, loss 0.222252, acc 0.9375, learning_rate 0.000352563
2017-10-10T14:41:39.861407: step 727, loss 0.125671, acc 0.984375, learning_rate 0.000351532
2017-10-10T14:41:40.102742: step 728, loss 0.309973, acc 0.9375, learning_rate 0.000350505
2017-10-10T14:41:40.344832: step 729, loss 0.110252, acc 0.953125, learning_rate 0.000349483
2017-10-10T14:41:40.604423: step 730, loss 0.133851, acc 0.96875, learning_rate 0.000348465
2017-10-10T14:41:40.861861: step 731, loss 0.252959, acc 0.90625, learning_rate 0.00034745
2017-10-10T14:41:41.093506: step 732, loss 0.129133, acc 0.953125, learning_rate 0.00034644
2017-10-10T14:41:41.347888: step 733, loss 0.162865, acc 0.90625, learning_rate 0.000345434
2017-10-10T14:41:41.611507: step 734, loss 0.237096, acc 0.921875, learning_rate 0.000344433
2017-10-10T14:41:41.890863: step 735, loss 0.166828, acc 0.90625, learning_rate 0.000343435
2017-10-10T14:41:42.148247: step 736, loss 0.170344, acc 0.9375, learning_rate 0.000342441
2017-10-10T14:41:42.328264: step 737, loss 0.138632, acc 0.984375, learning_rate 0.000341452
2017-10-10T14:41:42.590632: step 738, loss 0.101329, acc 0.984375, learning_rate 0.000340466
2017-10-10T14:41:42.860971: step 739, loss 0.160985, acc 0.9375, learning_rate 0.000339485
2017-10-10T14:41:43.064918: step 740, loss 0.085328, acc 0.96875, learning_rate 0.000338507
2017-10-10T14:41:43.298365: step 741, loss 0.203303, acc 0.921875, learning_rate 0.000337534
2017-10-10T14:41:43.527421: step 742, loss 0.252063, acc 0.921875, learning_rate 0.000336564
2017-10-10T14:41:43.785292: step 743, loss 0.26949, acc 0.90625, learning_rate 0.000335598
2017-10-10T14:41:44.105146: step 744, loss 0.112389, acc 0.953125, learning_rate 0.000334637
2017-10-10T14:41:44.356177: step 745, loss 0.156132, acc 0.9375, learning_rate 0.000333679
2017-10-10T14:41:44.569121: step 746, loss 0.117683, acc 0.953125, learning_rate 0.000332725
2017-10-10T14:41:44.813588: step 747, loss 0.195985, acc 0.921875, learning_rate 0.000331775
2017-10-10T14:41:45.060657: step 748, loss 0.191116, acc 0.921875, learning_rate 0.000330829
2017-10-10T14:41:45.324407: step 749, loss 0.155876, acc 0.9375, learning_rate 0.000329887
2017-10-10T14:41:45.558793: step 750, loss 0.062282, acc 0.984375, learning_rate 0.000328949
2017-10-10T14:41:45.836205: step 751, loss 0.101261, acc 0.984375, learning_rate 0.000328014
2017-10-10T14:41:46.076361: step 752, loss 0.193031, acc 0.9375, learning_rate 0.000327083
2017-10-10T14:41:46.330590: step 753, loss 0.207265, acc 0.9375, learning_rate 0.000326157
2017-10-10T14:41:46.600892: step 754, loss 0.361144, acc 0.921875, learning_rate 0.000325233
2017-10-10T14:41:46.849389: step 755, loss 0.136066, acc 0.953125, learning_rate 0.000324314
2017-10-10T14:41:47.093949: step 756, loss 0.0757112, acc 0.984375, learning_rate 0.000323399
2017-10-10T14:41:47.310817: step 757, loss 0.136776, acc 0.953125, learning_rate 0.000322487
2017-10-10T14:41:47.514012: step 758, loss 0.357963, acc 0.90625, learning_rate 0.000321579
2017-10-10T14:41:47.738840: step 759, loss 0.166889, acc 0.953125, learning_rate 0.000320674
2017-10-10T14:41:48.006447: step 760, loss 0.132993, acc 0.9375, learning_rate 0.000319773

Evaluation:
2017-10-10T14:41:48.544910: step 760, loss 0.224949, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-760

2017-10-10T14:41:49.488255: step 761, loss 0.147993, acc 0.9375, learning_rate 0.000318876
2017-10-10T14:41:49.760678: step 762, loss 0.142146, acc 0.9375, learning_rate 0.000317983
2017-10-10T14:41:50.021831: step 763, loss 0.118974, acc 0.953125, learning_rate 0.000317093
2017-10-10T14:41:50.294925: step 764, loss 0.0917111, acc 0.984375, learning_rate 0.000316207
2017-10-10T14:41:50.543161: step 765, loss 0.0876577, acc 0.984375, learning_rate 0.000315325
2017-10-10T14:41:50.789794: step 766, loss 0.220045, acc 0.921875, learning_rate 0.000314446
2017-10-10T14:41:51.044956: step 767, loss 0.127976, acc 0.96875, learning_rate 0.00031357
2017-10-10T14:41:51.344483: step 768, loss 0.10255, acc 0.96875, learning_rate 0.000312699
2017-10-10T14:41:51.555916: step 769, loss 0.308003, acc 0.859375, learning_rate 0.00031183
2017-10-10T14:41:51.771240: step 770, loss 0.276144, acc 0.890625, learning_rate 0.000310966
2017-10-10T14:41:51.982182: step 771, loss 0.18354, acc 0.9375, learning_rate 0.000310105
2017-10-10T14:41:52.182941: step 772, loss 0.172276, acc 0.953125, learning_rate 0.000309247
2017-10-10T14:41:52.394487: step 773, loss 0.414211, acc 0.875, learning_rate 0.000308393
2017-10-10T14:41:52.602829: step 774, loss 0.256725, acc 0.9375, learning_rate 0.000307542
2017-10-10T14:41:52.816773: step 775, loss 0.200504, acc 0.9375, learning_rate 0.000306695
2017-10-10T14:41:53.036977: step 776, loss 0.0738615, acc 0.96875, learning_rate 0.000305852
2017-10-10T14:41:53.243115: step 777, loss 0.166983, acc 0.9375, learning_rate 0.000305011
2017-10-10T14:41:53.449357: step 778, loss 0.270384, acc 0.90625, learning_rate 0.000304174
2017-10-10T14:41:53.667495: step 779, loss 0.124413, acc 0.953125, learning_rate 0.000303341
2017-10-10T14:41:53.900981: step 780, loss 0.178291, acc 0.90625, learning_rate 0.000302511
2017-10-10T14:41:54.111711: step 781, loss 0.193488, acc 0.921875, learning_rate 0.000301684
2017-10-10T14:41:54.356833: step 782, loss 0.214907, acc 0.9375, learning_rate 0.000300861
2017-10-10T14:41:54.605247: step 783, loss 0.127015, acc 0.921875, learning_rate 0.000300041
2017-10-10T14:41:54.816960: step 784, loss 0.171339, acc 0.941176, learning_rate 0.000299225
2017-10-10T14:41:55.074684: step 785, loss 0.164151, acc 0.9375, learning_rate 0.000298412
2017-10-10T14:41:55.324966: step 786, loss 0.148379, acc 0.96875, learning_rate 0.000297602
2017-10-10T14:41:55.572517: step 787, loss 0.134046, acc 0.953125, learning_rate 0.000296795
2017-10-10T14:41:55.817741: step 788, loss 0.177619, acc 0.90625, learning_rate 0.000295992
2017-10-10T14:41:56.075493: step 789, loss 0.165465, acc 0.921875, learning_rate 0.000295192
2017-10-10T14:41:56.324895: step 790, loss 0.231344, acc 0.890625, learning_rate 0.000294395
2017-10-10T14:41:56.563503: step 791, loss 0.112931, acc 0.96875, learning_rate 0.000293602
2017-10-10T14:41:56.804819: step 792, loss 0.109774, acc 0.984375, learning_rate 0.000292812
2017-10-10T14:41:57.057374: step 793, loss 0.166158, acc 0.9375, learning_rate 0.000292025
2017-10-10T14:41:57.310083: step 794, loss 0.135605, acc 0.9375, learning_rate 0.000291241
2017-10-10T14:41:57.547897: step 795, loss 0.144929, acc 0.9375, learning_rate 0.00029046
2017-10-10T14:41:57.783245: step 796, loss 0.168601, acc 0.96875, learning_rate 0.000289683
2017-10-10T14:41:57.995894: step 797, loss 0.214946, acc 0.921875, learning_rate 0.000288908
2017-10-10T14:41:58.232705: step 798, loss 0.183002, acc 0.953125, learning_rate 0.000288137
2017-10-10T14:41:58.488026: step 799, loss 0.121727, acc 0.96875, learning_rate 0.000287369
2017-10-10T14:41:58.726873: step 800, loss 0.181976, acc 0.921875, learning_rate 0.000286605

Evaluation:
2017-10-10T14:41:59.266627: step 800, loss 0.220454, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-800

2017-10-10T14:42:00.253051: step 801, loss 0.159107, acc 0.96875, learning_rate 0.000285843
2017-10-10T14:42:00.381818: step 802, loss 0.113448, acc 0.921875, learning_rate 0.000285084
2017-10-10T14:42:00.518839: step 803, loss 0.117257, acc 0.96875, learning_rate 0.000284329
2017-10-10T14:42:00.734130: step 804, loss 0.220819, acc 0.953125, learning_rate 0.000283577
2017-10-10T14:42:00.989117: step 805, loss 0.237945, acc 0.921875, learning_rate 0.000282827
2017-10-10T14:42:01.249457: step 806, loss 0.101276, acc 0.96875, learning_rate 0.000282081
2017-10-10T14:42:01.517727: step 807, loss 0.129814, acc 0.921875, learning_rate 0.000281338
2017-10-10T14:42:01.763929: step 808, loss 0.155994, acc 0.96875, learning_rate 0.000280598
2017-10-10T14:42:01.976652: step 809, loss 0.249934, acc 0.90625, learning_rate 0.00027986
2017-10-10T14:42:02.244671: step 810, loss 0.277506, acc 0.90625, learning_rate 0.000279126
2017-10-10T14:42:02.457296: step 811, loss 0.156281, acc 0.96875, learning_rate 0.000278395
2017-10-10T14:42:02.725102: step 812, loss 0.126141, acc 0.921875, learning_rate 0.000277667
2017-10-10T14:42:02.940897: step 813, loss 0.168915, acc 0.9375, learning_rate 0.000276942
2017-10-10T14:42:03.198427: step 814, loss 0.131862, acc 0.953125, learning_rate 0.00027622
2017-10-10T14:42:03.396815: step 815, loss 0.121317, acc 0.953125, learning_rate 0.0002755
2017-10-10T14:42:03.654042: step 816, loss 0.137469, acc 0.96875, learning_rate 0.000274784
2017-10-10T14:42:03.904195: step 817, loss 0.11945, acc 0.96875, learning_rate 0.000274071
2017-10-10T14:42:04.162670: step 818, loss 0.0686028, acc 0.984375, learning_rate 0.00027336
2017-10-10T14:42:04.414615: step 819, loss 0.220743, acc 0.921875, learning_rate 0.000272652
2017-10-10T14:42:04.664308: step 820, loss 0.0904201, acc 0.96875, learning_rate 0.000271948
2017-10-10T14:42:04.907464: step 821, loss 0.16744, acc 0.90625, learning_rate 0.000271246
2017-10-10T14:42:05.157418: step 822, loss 0.171535, acc 0.9375, learning_rate 0.000270547
2017-10-10T14:42:05.370588: step 823, loss 0.274388, acc 0.90625, learning_rate 0.000269851
2017-10-10T14:42:05.605055: step 824, loss 0.133328, acc 0.96875, learning_rate 0.000269157
2017-10-10T14:42:05.856285: step 825, loss 0.23354, acc 0.875, learning_rate 0.000268467
2017-10-10T14:42:06.093242: step 826, loss 0.267099, acc 0.890625, learning_rate 0.000267779
2017-10-10T14:42:06.377729: step 827, loss 0.321982, acc 0.890625, learning_rate 0.000267094
2017-10-10T14:42:06.605825: step 828, loss 0.162904, acc 0.953125, learning_rate 0.000266412
2017-10-10T14:42:06.888860: step 829, loss 0.102355, acc 0.96875, learning_rate 0.000265733
2017-10-10T14:42:07.095250: step 830, loss 0.197881, acc 0.921875, learning_rate 0.000265057
2017-10-10T14:42:07.357105: step 831, loss 0.209564, acc 0.9375, learning_rate 0.000264383
2017-10-10T14:42:07.609973: step 832, loss 0.148079, acc 0.9375, learning_rate 0.000263712
2017-10-10T14:42:07.939015: step 833, loss 0.233675, acc 0.9375, learning_rate 0.000263044
2017-10-10T14:42:08.164406: step 834, loss 0.0915099, acc 0.984375, learning_rate 0.000262378
2017-10-10T14:42:08.375113: step 835, loss 0.0691619, acc 0.984375, learning_rate 0.000261715
2017-10-10T14:42:08.564821: step 836, loss 0.231633, acc 0.921875, learning_rate 0.000261055
2017-10-10T14:42:08.810635: step 837, loss 0.24227, acc 0.90625, learning_rate 0.000260398
2017-10-10T14:42:09.086432: step 838, loss 0.252139, acc 0.890625, learning_rate 0.000259743
2017-10-10T14:42:09.326972: step 839, loss 0.10632, acc 0.953125, learning_rate 0.000259091
2017-10-10T14:42:09.592422: step 840, loss 0.138236, acc 0.953125, learning_rate 0.000258442

Evaluation:
2017-10-10T14:42:10.070876: step 840, loss 0.221061, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-840

2017-10-10T14:42:11.207346: step 841, loss 0.187268, acc 0.921875, learning_rate 0.000257795
2017-10-10T14:42:11.367621: step 842, loss 0.197176, acc 0.9375, learning_rate 0.000257151
2017-10-10T14:42:11.544198: step 843, loss 0.243795, acc 0.921875, learning_rate 0.00025651
2017-10-10T14:42:11.716997: step 844, loss 0.132547, acc 0.96875, learning_rate 0.000255871
2017-10-10T14:42:11.885692: step 845, loss 0.247527, acc 0.921875, learning_rate 0.000255235
2017-10-10T14:42:12.125809: step 846, loss 0.157783, acc 0.921875, learning_rate 0.000254601
2017-10-10T14:42:12.380861: step 847, loss 0.0900763, acc 0.984375, learning_rate 0.00025397
2017-10-10T14:42:12.632056: step 848, loss 0.248341, acc 0.90625, learning_rate 0.000253341
2017-10-10T14:42:12.871874: step 849, loss 0.128824, acc 0.96875, learning_rate 0.000252716
2017-10-10T14:42:13.122352: step 850, loss 0.121011, acc 0.9375, learning_rate 0.000252092
2017-10-10T14:42:13.373375: step 851, loss 0.110476, acc 0.96875, learning_rate 0.000251471
2017-10-10T14:42:13.589098: step 852, loss 0.209032, acc 0.9375, learning_rate 0.000250853
2017-10-10T14:42:13.833203: step 853, loss 0.166319, acc 0.953125, learning_rate 0.000250237
2017-10-10T14:42:14.044523: step 854, loss 0.111594, acc 0.953125, learning_rate 0.000249624
2017-10-10T14:42:14.292490: step 855, loss 0.0932653, acc 0.984375, learning_rate 0.000249013
2017-10-10T14:42:14.546529: step 856, loss 0.161974, acc 0.9375, learning_rate 0.000248405
2017-10-10T14:42:14.812770: step 857, loss 0.309393, acc 0.890625, learning_rate 0.000247799
2017-10-10T14:42:15.068888: step 858, loss 0.091642, acc 0.984375, learning_rate 0.000247196
2017-10-10T14:42:15.324835: step 859, loss 0.162284, acc 0.90625, learning_rate 0.000246595
2017-10-10T14:42:15.514400: step 860, loss 0.230214, acc 0.921875, learning_rate 0.000245997
2017-10-10T14:42:15.720847: step 861, loss 0.139128, acc 0.953125, learning_rate 0.000245401
2017-10-10T14:42:15.900877: step 862, loss 0.163837, acc 0.953125, learning_rate 0.000244808
2017-10-10T14:42:16.121358: step 863, loss 0.199367, acc 0.90625, learning_rate 0.000244216
2017-10-10T14:42:16.369128: step 864, loss 0.118181, acc 0.96875, learning_rate 0.000243628
2017-10-10T14:42:16.641558: step 865, loss 0.140554, acc 0.953125, learning_rate 0.000243042
2017-10-10T14:42:16.908010: step 866, loss 0.169676, acc 0.90625, learning_rate 0.000242458
2017-10-10T14:42:17.160917: step 867, loss 0.147806, acc 0.921875, learning_rate 0.000241876
2017-10-10T14:42:17.444082: step 868, loss 0.0832099, acc 0.96875, learning_rate 0.000241297
2017-10-10T14:42:17.703161: step 869, loss 0.0891539, acc 0.953125, learning_rate 0.00024072
2017-10-10T14:42:17.955726: step 870, loss 0.253065, acc 0.90625, learning_rate 0.000240146
2017-10-10T14:42:18.213289: step 871, loss 0.107324, acc 0.96875, learning_rate 0.000239574
2017-10-10T14:42:18.475043: step 872, loss 0.11381, acc 0.953125, learning_rate 0.000239004
2017-10-10T14:42:18.733984: step 873, loss 0.153424, acc 0.9375, learning_rate 0.000238437
2017-10-10T14:42:18.957291: step 874, loss 0.0594051, acc 0.984375, learning_rate 0.000237872
2017-10-10T14:42:19.169152: step 875, loss 0.232301, acc 0.9375, learning_rate 0.000237309
2017-10-10T14:42:19.399557: step 876, loss 0.129685, acc 0.9375, learning_rate 0.000236749
2017-10-10T14:42:19.658531: step 877, loss 0.177308, acc 0.921875, learning_rate 0.00023619
2017-10-10T14:42:19.913520: step 878, loss 0.154526, acc 0.953125, learning_rate 0.000235635
2017-10-10T14:42:20.171326: step 879, loss 0.190551, acc 0.921875, learning_rate 0.000235081
2017-10-10T14:42:20.416791: step 880, loss 0.181233, acc 0.9375, learning_rate 0.00023453

Evaluation:
2017-10-10T14:42:20.868883: step 880, loss 0.223689, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-880

2017-10-10T14:42:22.049397: step 881, loss 0.220693, acc 0.890625, learning_rate 0.00023398
2017-10-10T14:42:22.260844: step 882, loss 0.237167, acc 0.941176, learning_rate 0.000233434
2017-10-10T14:42:22.604876: step 883, loss 0.140386, acc 0.953125, learning_rate 0.000232889
2017-10-10T14:42:22.878612: step 884, loss 0.263292, acc 0.875, learning_rate 0.000232346
2017-10-10T14:42:23.024311: step 885, loss 0.0822755, acc 0.984375, learning_rate 0.000231806
2017-10-10T14:42:23.151202: step 886, loss 0.145187, acc 0.96875, learning_rate 0.000231268
2017-10-10T14:42:23.279921: step 887, loss 0.0588005, acc 0.984375, learning_rate 0.000230732
2017-10-10T14:42:23.414435: step 888, loss 0.227643, acc 0.90625, learning_rate 0.000230199
2017-10-10T14:42:23.600769: step 889, loss 0.166357, acc 0.921875, learning_rate 0.000229667
2017-10-10T14:42:23.864004: step 890, loss 0.16783, acc 0.90625, learning_rate 0.000229138
2017-10-10T14:42:24.120798: step 891, loss 0.195559, acc 0.9375, learning_rate 0.000228611
2017-10-10T14:42:24.358569: step 892, loss 0.339913, acc 0.890625, learning_rate 0.000228086
2017-10-10T14:42:24.603903: step 893, loss 0.132788, acc 0.96875, learning_rate 0.000227563
2017-10-10T14:42:24.866612: step 894, loss 0.131719, acc 0.953125, learning_rate 0.000227043
2017-10-10T14:42:25.128817: step 895, loss 0.182095, acc 0.921875, learning_rate 0.000226524
2017-10-10T14:42:25.344829: step 896, loss 0.0701606, acc 0.96875, learning_rate 0.000226008
2017-10-10T14:42:25.612892: step 897, loss 0.140081, acc 0.953125, learning_rate 0.000225493
2017-10-10T14:42:25.872585: step 898, loss 0.133402, acc 0.9375, learning_rate 0.000224981
2017-10-10T14:42:26.126624: step 899, loss 0.243983, acc 0.90625, learning_rate 0.000224471
2017-10-10T14:42:26.376842: step 900, loss 0.325851, acc 0.90625, learning_rate 0.000223963
2017-10-10T14:42:26.616495: step 901, loss 0.113321, acc 0.96875, learning_rate 0.000223457
2017-10-10T14:42:26.893084: step 902, loss 0.109089, acc 0.96875, learning_rate 0.000222953
2017-10-10T14:42:27.167079: step 903, loss 0.208393, acc 0.890625, learning_rate 0.000222451
2017-10-10T14:42:27.413080: step 904, loss 0.116861, acc 0.96875, learning_rate 0.000221951
2017-10-10T14:42:27.687221: step 905, loss 0.243463, acc 0.921875, learning_rate 0.000221453
2017-10-10T14:42:27.941161: step 906, loss 0.205168, acc 0.9375, learning_rate 0.000220958
2017-10-10T14:42:28.207460: step 907, loss 0.083921, acc 0.96875, learning_rate 0.000220464
2017-10-10T14:42:28.452950: step 908, loss 0.179718, acc 0.921875, learning_rate 0.000219972
2017-10-10T14:42:28.715817: step 909, loss 0.081364, acc 0.96875, learning_rate 0.000219483
2017-10-10T14:42:28.955371: step 910, loss 0.209219, acc 0.9375, learning_rate 0.000218995
2017-10-10T14:42:29.188863: step 911, loss 0.161225, acc 0.953125, learning_rate 0.000218509
2017-10-10T14:42:29.414898: step 912, loss 0.230702, acc 0.921875, learning_rate 0.000218025
2017-10-10T14:42:29.665014: step 913, loss 0.148284, acc 0.9375, learning_rate 0.000217544
2017-10-10T14:42:29.919592: step 914, loss 0.118888, acc 0.921875, learning_rate 0.000217064
2017-10-10T14:42:30.164833: step 915, loss 0.215691, acc 0.890625, learning_rate 0.000216586
2017-10-10T14:42:30.462925: step 916, loss 0.102083, acc 0.96875, learning_rate 0.00021611
2017-10-10T14:42:30.674473: step 917, loss 0.121503, acc 0.9375, learning_rate 0.000215636
2017-10-10T14:42:30.916805: step 918, loss 0.109793, acc 0.96875, learning_rate 0.000215164
2017-10-10T14:42:31.131140: step 919, loss 0.108784, acc 0.9375, learning_rate 0.000214694
2017-10-10T14:42:31.368614: step 920, loss 0.234935, acc 0.9375, learning_rate 0.000214226

Evaluation:
2017-10-10T14:42:31.792530: step 920, loss 0.219924, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-920

2017-10-10T14:42:33.652908: step 921, loss 0.162406, acc 0.953125, learning_rate 0.00021376
2017-10-10T14:42:33.933035: step 922, loss 0.220254, acc 0.921875, learning_rate 0.000213295
2017-10-10T14:42:34.133732: step 923, loss 0.127251, acc 0.96875, learning_rate 0.000212833
2017-10-10T14:42:34.307533: step 924, loss 0.191806, acc 0.921875, learning_rate 0.000212372
2017-10-10T14:42:34.503134: step 925, loss 0.0908597, acc 0.96875, learning_rate 0.000211914
2017-10-10T14:42:34.681203: step 926, loss 0.108636, acc 0.96875, learning_rate 0.000211457
2017-10-10T14:42:34.939606: step 927, loss 0.152384, acc 0.9375, learning_rate 0.000211002
2017-10-10T14:42:35.169174: step 928, loss 0.1904, acc 0.890625, learning_rate 0.000210549
2017-10-10T14:42:35.420822: step 929, loss 0.0536035, acc 0.984375, learning_rate 0.000210098
2017-10-10T14:42:35.664826: step 930, loss 0.152084, acc 0.9375, learning_rate 0.000209648
2017-10-10T14:42:35.904755: step 931, loss 0.227123, acc 0.921875, learning_rate 0.000209201
2017-10-10T14:42:36.103162: step 932, loss 0.040897, acc 0.984375, learning_rate 0.000208755
2017-10-10T14:42:36.327924: step 933, loss 0.0905628, acc 0.9375, learning_rate 0.000208311
2017-10-10T14:42:36.573504: step 934, loss 0.109108, acc 0.953125, learning_rate 0.000207869
2017-10-10T14:42:36.804862: step 935, loss 0.176871, acc 0.921875, learning_rate 0.000207429
2017-10-10T14:42:37.056908: step 936, loss 0.247681, acc 0.90625, learning_rate 0.00020699
2017-10-10T14:42:37.308003: step 937, loss 0.0561822, acc 0.984375, learning_rate 0.000206554
2017-10-10T14:42:37.572022: step 938, loss 0.0786435, acc 1, learning_rate 0.000206119
2017-10-10T14:42:37.771966: step 939, loss 0.203274, acc 0.90625, learning_rate 0.000205685
2017-10-10T14:42:37.972460: step 940, loss 0.14426, acc 0.953125, learning_rate 0.000205254
2017-10-10T14:42:38.153344: step 941, loss 0.332315, acc 0.890625, learning_rate 0.000204824
2017-10-10T14:42:38.356312: step 942, loss 0.386236, acc 0.890625, learning_rate 0.000204397
2017-10-10T14:42:38.624604: step 943, loss 0.220708, acc 0.9375, learning_rate 0.00020397
2017-10-10T14:42:38.897363: step 944, loss 0.091006, acc 0.96875, learning_rate 0.000203546
2017-10-10T14:42:39.150239: step 945, loss 0.162394, acc 0.953125, learning_rate 0.000203123
2017-10-10T14:42:39.418742: step 946, loss 0.345012, acc 0.90625, learning_rate 0.000202702
2017-10-10T14:42:39.699103: step 947, loss 0.241033, acc 0.90625, learning_rate 0.000202283
2017-10-10T14:42:39.943721: step 948, loss 0.0682743, acc 0.96875, learning_rate 0.000201866
2017-10-10T14:42:40.148072: step 949, loss 0.12141, acc 0.96875, learning_rate 0.00020145
2017-10-10T14:42:40.396258: step 950, loss 0.183636, acc 0.9375, learning_rate 0.000201036
2017-10-10T14:42:40.628872: step 951, loss 0.219674, acc 0.90625, learning_rate 0.000200623
2017-10-10T14:42:40.873741: step 952, loss 0.149226, acc 0.953125, learning_rate 0.000200213
2017-10-10T14:42:41.117232: step 953, loss 0.272086, acc 0.921875, learning_rate 0.000199804
2017-10-10T14:42:41.377794: step 954, loss 0.219727, acc 0.921875, learning_rate 0.000199396
2017-10-10T14:42:41.639668: step 955, loss 0.231768, acc 0.890625, learning_rate 0.000198991
2017-10-10T14:42:41.882233: step 956, loss 0.0795661, acc 0.984375, learning_rate 0.000198587
2017-10-10T14:42:42.122293: step 957, loss 0.1229, acc 0.96875, learning_rate 0.000198184
2017-10-10T14:42:42.418915: step 958, loss 0.0962077, acc 0.984375, learning_rate 0.000197783
2017-10-10T14:42:42.694243: step 959, loss 0.137451, acc 0.921875, learning_rate 0.000197384
2017-10-10T14:42:42.960847: step 960, loss 0.3679, acc 0.890625, learning_rate 0.000196987

Evaluation:
2017-10-10T14:42:43.464508: step 960, loss 0.223165, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-960

2017-10-10T14:42:44.603740: step 961, loss 0.226422, acc 0.890625, learning_rate 0.000196591
2017-10-10T14:42:44.856719: step 962, loss 0.17028, acc 0.9375, learning_rate 0.000196197
2017-10-10T14:42:45.054417: step 963, loss 0.126086, acc 0.953125, learning_rate 0.000195804
2017-10-10T14:42:45.320817: step 964, loss 0.148517, acc 0.96875, learning_rate 0.000195413
2017-10-10T14:42:45.587016: step 965, loss 0.182043, acc 0.9375, learning_rate 0.000195023
2017-10-10T14:42:45.809065: step 966, loss 0.188656, acc 0.921875, learning_rate 0.000194636
2017-10-10T14:42:46.009153: step 967, loss 0.19307, acc 0.9375, learning_rate 0.000194249
2017-10-10T14:42:46.203947: step 968, loss 0.333099, acc 0.875, learning_rate 0.000193865
2017-10-10T14:42:46.381646: step 969, loss 0.121395, acc 0.953125, learning_rate 0.000193482
2017-10-10T14:42:46.632679: step 970, loss 0.143579, acc 0.953125, learning_rate 0.0001931
2017-10-10T14:42:46.888481: step 971, loss 0.118227, acc 0.96875, learning_rate 0.00019272
2017-10-10T14:42:47.132827: step 972, loss 0.14575, acc 0.953125, learning_rate 0.000192341
2017-10-10T14:42:47.381270: step 973, loss 0.0846954, acc 0.96875, learning_rate 0.000191965
2017-10-10T14:42:47.653889: step 974, loss 0.244992, acc 0.921875, learning_rate 0.000191589
2017-10-10T14:42:47.928834: step 975, loss 0.103273, acc 0.96875, learning_rate 0.000191215
2017-10-10T14:42:48.135438: step 976, loss 0.0898796, acc 0.96875, learning_rate 0.000190843
2017-10-10T14:42:48.360187: step 977, loss 0.0988313, acc 0.984375, learning_rate 0.000190472
2017-10-10T14:42:48.620977: step 978, loss 0.274549, acc 0.890625, learning_rate 0.000190103
2017-10-10T14:42:48.844149: step 979, loss 0.107653, acc 0.96875, learning_rate 0.000189735
2017-10-10T14:42:49.054526: step 980, loss 0.24066, acc 0.921569, learning_rate 0.000189369
2017-10-10T14:42:49.320702: step 981, loss 0.275285, acc 0.921875, learning_rate 0.000189004
2017-10-10T14:42:49.549381: step 982, loss 0.213727, acc 0.90625, learning_rate 0.000188641
2017-10-10T14:42:49.764510: step 983, loss 0.135456, acc 0.953125, learning_rate 0.000188279
2017-10-10T14:42:50.001003: step 984, loss 0.126603, acc 0.96875, learning_rate 0.000187919
2017-10-10T14:42:50.252461: step 985, loss 0.0800447, acc 0.96875, learning_rate 0.00018756
2017-10-10T14:42:50.460824: step 986, loss 0.182535, acc 0.9375, learning_rate 0.000187202
2017-10-10T14:42:50.728398: step 987, loss 0.11797, acc 0.96875, learning_rate 0.000186846
2017-10-10T14:42:50.966013: step 988, loss 0.0923345, acc 0.953125, learning_rate 0.000186492
2017-10-10T14:42:51.203190: step 989, loss 0.126356, acc 0.96875, learning_rate 0.000186139
2017-10-10T14:42:51.460126: step 990, loss 0.148164, acc 0.953125, learning_rate 0.000185787
2017-10-10T14:42:51.675010: step 991, loss 0.151827, acc 0.9375, learning_rate 0.000185437
2017-10-10T14:42:51.916708: step 992, loss 0.166339, acc 0.9375, learning_rate 0.000185088
2017-10-10T14:42:52.163125: step 993, loss 0.123647, acc 0.984375, learning_rate 0.000184741
2017-10-10T14:42:52.408477: step 994, loss 0.118612, acc 0.9375, learning_rate 0.000184395
2017-10-10T14:42:52.643984: step 995, loss 0.268977, acc 0.921875, learning_rate 0.000184051
2017-10-10T14:42:52.878038: step 996, loss 0.177579, acc 0.9375, learning_rate 0.000183708
2017-10-10T14:42:53.115195: step 997, loss 0.141243, acc 0.953125, learning_rate 0.000183366
2017-10-10T14:42:53.328046: step 998, loss 0.288975, acc 0.921875, learning_rate 0.000183026
2017-10-10T14:42:53.538962: step 999, loss 0.176075, acc 0.921875, learning_rate 0.000182687
2017-10-10T14:42:53.793848: step 1000, loss 0.140968, acc 0.984375, learning_rate 0.000182349

Evaluation:
2017-10-10T14:42:54.274852: step 1000, loss 0.22099, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1000

2017-10-10T14:42:55.434824: step 1001, loss 0.173548, acc 0.921875, learning_rate 0.000182013
2017-10-10T14:42:55.687495: step 1002, loss 0.121631, acc 0.96875, learning_rate 0.000181678
2017-10-10T14:42:55.945594: step 1003, loss 0.102175, acc 0.953125, learning_rate 0.000181345
2017-10-10T14:42:56.204383: step 1004, loss 0.129983, acc 0.953125, learning_rate 0.000181013
2017-10-10T14:42:56.444908: step 1005, loss 0.097995, acc 0.96875, learning_rate 0.000180682
2017-10-10T14:42:56.672824: step 1006, loss 0.286744, acc 0.921875, learning_rate 0.000180353
2017-10-10T14:42:56.928934: step 1007, loss 0.200873, acc 0.921875, learning_rate 0.000180025
2017-10-10T14:42:57.248209: step 1008, loss 0.146306, acc 0.953125, learning_rate 0.000179698
2017-10-10T14:42:57.443712: step 1009, loss 0.122125, acc 0.9375, learning_rate 0.000179373
2017-10-10T14:42:57.616856: step 1010, loss 0.110947, acc 0.984375, learning_rate 0.000179049
2017-10-10T14:42:57.804115: step 1011, loss 0.137786, acc 0.96875, learning_rate 0.000178726
2017-10-10T14:42:57.979376: step 1012, loss 0.0900426, acc 0.96875, learning_rate 0.000178405
2017-10-10T14:42:58.155171: step 1013, loss 0.227543, acc 0.9375, learning_rate 0.000178085
2017-10-10T14:42:58.409315: step 1014, loss 0.155914, acc 0.984375, learning_rate 0.000177766
2017-10-10T14:42:58.625390: step 1015, loss 0.198062, acc 0.90625, learning_rate 0.000177449
2017-10-10T14:42:58.825149: step 1016, loss 0.115667, acc 0.96875, learning_rate 0.000177133
2017-10-10T14:42:59.084716: step 1017, loss 0.0647016, acc 0.96875, learning_rate 0.000176818
2017-10-10T14:42:59.346126: step 1018, loss 0.118459, acc 0.953125, learning_rate 0.000176504
2017-10-10T14:42:59.608106: step 1019, loss 0.108146, acc 0.953125, learning_rate 0.000176192
2017-10-10T14:42:59.898315: step 1020, loss 0.0504216, acc 1, learning_rate 0.000175881
2017-10-10T14:43:00.225892: step 1021, loss 0.139583, acc 0.953125, learning_rate 0.000175571
2017-10-10T14:43:00.428873: step 1022, loss 0.346119, acc 0.890625, learning_rate 0.000175263
2017-10-10T14:43:00.612821: step 1023, loss 0.12014, acc 0.96875, learning_rate 0.000174956
2017-10-10T14:43:00.849400: step 1024, loss 0.076793, acc 0.984375, learning_rate 0.00017465
2017-10-10T14:43:01.117532: step 1025, loss 0.180914, acc 0.9375, learning_rate 0.000174345
2017-10-10T14:43:01.393808: step 1026, loss 0.163918, acc 0.921875, learning_rate 0.000174042
2017-10-10T14:43:01.681461: step 1027, loss 0.221309, acc 0.921875, learning_rate 0.000173739
2017-10-10T14:43:01.958029: step 1028, loss 0.0849127, acc 0.96875, learning_rate 0.000173438
2017-10-10T14:43:02.228806: step 1029, loss 0.0685957, acc 0.984375, learning_rate 0.000173139
2017-10-10T14:43:02.480857: step 1030, loss 0.124499, acc 0.9375, learning_rate 0.00017284
2017-10-10T14:43:02.775226: step 1031, loss 0.226533, acc 0.953125, learning_rate 0.000172543
2017-10-10T14:43:03.046401: step 1032, loss 0.180369, acc 0.953125, learning_rate 0.000172247
2017-10-10T14:43:03.298253: step 1033, loss 0.281933, acc 0.9375, learning_rate 0.000171952
2017-10-10T14:43:03.572237: step 1034, loss 0.0842017, acc 0.953125, learning_rate 0.000171658
2017-10-10T14:43:03.801836: step 1035, loss 0.228244, acc 0.875, learning_rate 0.000171366
2017-10-10T14:43:04.098981: step 1036, loss 0.215196, acc 0.96875, learning_rate 0.000171074
2017-10-10T14:43:04.348089: step 1037, loss 0.167893, acc 0.9375, learning_rate 0.000170784
2017-10-10T14:43:04.613643: step 1038, loss 0.238972, acc 0.90625, learning_rate 0.000170495
2017-10-10T14:43:04.915854: step 1039, loss 0.201396, acc 0.921875, learning_rate 0.000170208
2017-10-10T14:43:05.193687: step 1040, loss 0.232086, acc 0.9375, learning_rate 0.000169921

Evaluation:
2017-10-10T14:43:05.748892: step 1040, loss 0.219447, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1040

2017-10-10T14:43:06.708382: step 1041, loss 0.0882947, acc 0.984375, learning_rate 0.000169636
2017-10-10T14:43:06.972860: step 1042, loss 0.0923254, acc 0.96875, learning_rate 0.000169351
2017-10-10T14:43:07.249211: step 1043, loss 0.145692, acc 0.9375, learning_rate 0.000169068
2017-10-10T14:43:07.551021: step 1044, loss 0.163699, acc 0.9375, learning_rate 0.000168786
2017-10-10T14:43:07.832999: step 1045, loss 0.13049, acc 0.953125, learning_rate 0.000168506
2017-10-10T14:43:08.125196: step 1046, loss 0.199737, acc 0.9375, learning_rate 0.000168226
2017-10-10T14:43:08.473089: step 1047, loss 0.218984, acc 0.921875, learning_rate 0.000167947
2017-10-10T14:43:08.724718: step 1048, loss 0.133756, acc 0.921875, learning_rate 0.00016767
2017-10-10T14:43:08.941411: step 1049, loss 0.148649, acc 0.921875, learning_rate 0.000167394
2017-10-10T14:43:09.164983: step 1050, loss 0.150002, acc 0.953125, learning_rate 0.000167119
2017-10-10T14:43:09.386341: step 1051, loss 0.0634375, acc 0.984375, learning_rate 0.000166845
2017-10-10T14:43:09.664349: step 1052, loss 0.114576, acc 0.953125, learning_rate 0.000166572
2017-10-10T14:43:09.961629: step 1053, loss 0.150567, acc 0.921875, learning_rate 0.0001663
2017-10-10T14:43:10.256799: step 1054, loss 0.171832, acc 0.953125, learning_rate 0.00016603
2017-10-10T14:43:10.563847: step 1055, loss 0.156324, acc 0.953125, learning_rate 0.00016576
2017-10-10T14:43:10.792868: step 1056, loss 0.188886, acc 0.9375, learning_rate 0.000165492
2017-10-10T14:43:11.015078: step 1057, loss 0.131577, acc 0.9375, learning_rate 0.000165224
2017-10-10T14:43:11.204912: step 1058, loss 0.307605, acc 0.890625, learning_rate 0.000164958
2017-10-10T14:43:11.395152: step 1059, loss 0.189355, acc 0.9375, learning_rate 0.000164693
2017-10-10T14:43:11.633791: step 1060, loss 0.246543, acc 0.890625, learning_rate 0.000164429
2017-10-10T14:43:11.893826: step 1061, loss 0.22097, acc 0.921875, learning_rate 0.000164166
2017-10-10T14:43:12.140630: step 1062, loss 0.113536, acc 0.96875, learning_rate 0.000163904
2017-10-10T14:43:12.374824: step 1063, loss 0.18584, acc 0.9375, learning_rate 0.000163643
2017-10-10T14:43:12.652592: step 1064, loss 0.118174, acc 0.953125, learning_rate 0.000163383
2017-10-10T14:43:12.924175: step 1065, loss 0.0842157, acc 0.96875, learning_rate 0.000163125
2017-10-10T14:43:13.168880: step 1066, loss 0.0872849, acc 0.984375, learning_rate 0.000162867
2017-10-10T14:43:13.433102: step 1067, loss 0.0907033, acc 0.96875, learning_rate 0.00016261
2017-10-10T14:43:13.717437: step 1068, loss 0.0623345, acc 0.984375, learning_rate 0.000162355
2017-10-10T14:43:14.001496: step 1069, loss 0.0790276, acc 0.984375, learning_rate 0.0001621
2017-10-10T14:43:14.298862: step 1070, loss 0.192836, acc 0.9375, learning_rate 0.000161847
2017-10-10T14:43:14.559943: step 1071, loss 0.241227, acc 0.921875, learning_rate 0.000161594
2017-10-10T14:43:14.836509: step 1072, loss 0.089867, acc 0.984375, learning_rate 0.000161343
2017-10-10T14:43:15.108624: step 1073, loss 0.132064, acc 0.9375, learning_rate 0.000161093
2017-10-10T14:43:15.454315: step 1074, loss 0.241657, acc 0.90625, learning_rate 0.000160843
2017-10-10T14:43:15.708332: step 1075, loss 0.307154, acc 0.859375, learning_rate 0.000160595
2017-10-10T14:43:16.008969: step 1076, loss 0.193164, acc 0.890625, learning_rate 0.000160348
2017-10-10T14:43:16.239039: step 1077, loss 0.0922015, acc 0.984375, learning_rate 0.000160101
2017-10-10T14:43:16.429083: step 1078, loss 0.128336, acc 0.921569, learning_rate 0.000159856
2017-10-10T14:43:16.736561: step 1079, loss 0.166629, acc 0.9375, learning_rate 0.000159612
2017-10-10T14:43:17.088021: step 1080, loss 0.0959205, acc 0.984375, learning_rate 0.000159368

Evaluation:
2017-10-10T14:43:17.611003: step 1080, loss 0.218142, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1080

2017-10-10T14:43:18.687951: step 1081, loss 0.186914, acc 0.9375, learning_rate 0.000159126
2017-10-10T14:43:19.019434: step 1082, loss 0.135713, acc 0.9375, learning_rate 0.000158885
2017-10-10T14:43:19.309156: step 1083, loss 0.209727, acc 0.890625, learning_rate 0.000158644
2017-10-10T14:43:19.560957: step 1084, loss 0.203172, acc 0.921875, learning_rate 0.000158405
2017-10-10T14:43:19.880360: step 1085, loss 0.170115, acc 0.953125, learning_rate 0.000158167
2017-10-10T14:43:20.153272: step 1086, loss 0.227662, acc 0.921875, learning_rate 0.000157929
2017-10-10T14:43:20.414514: step 1087, loss 0.203754, acc 0.96875, learning_rate 0.000157693
2017-10-10T14:43:20.685330: step 1088, loss 0.123787, acc 0.953125, learning_rate 0.000157457
2017-10-10T14:43:20.962795: step 1089, loss 0.138616, acc 0.921875, learning_rate 0.000157223
2017-10-10T14:43:21.240817: step 1090, loss 0.141677, acc 0.96875, learning_rate 0.000156989
2017-10-10T14:43:21.540055: step 1091, loss 0.271138, acc 0.921875, learning_rate 0.000156757
2017-10-10T14:43:21.877033: step 1092, loss 0.203967, acc 0.921875, learning_rate 0.000156525
2017-10-10T14:43:22.208528: step 1093, loss 0.0898777, acc 0.96875, learning_rate 0.000156294
2017-10-10T14:43:22.520421: step 1094, loss 0.162427, acc 0.9375, learning_rate 0.000156064
2017-10-10T14:43:22.804906: step 1095, loss 0.109766, acc 0.984375, learning_rate 0.000155836
2017-10-10T14:43:23.072873: step 1096, loss 0.323703, acc 0.90625, learning_rate 0.000155608
2017-10-10T14:43:23.352971: step 1097, loss 0.09546, acc 0.953125, learning_rate 0.000155381
2017-10-10T14:43:23.638023: step 1098, loss 0.201777, acc 0.9375, learning_rate 0.000155155
2017-10-10T14:43:23.966660: step 1099, loss 0.153494, acc 0.96875, learning_rate 0.000154929
2017-10-10T14:43:24.267217: step 1100, loss 0.252194, acc 0.828125, learning_rate 0.000154705
2017-10-10T14:43:24.612193: step 1101, loss 0.207502, acc 0.90625, learning_rate 0.000154482
2017-10-10T14:43:24.935053: step 1102, loss 0.200342, acc 0.9375, learning_rate 0.00015426
2017-10-10T14:43:25.228275: step 1103, loss 0.293435, acc 0.890625, learning_rate 0.000154038
2017-10-10T14:43:25.469686: step 1104, loss 0.12189, acc 0.9375, learning_rate 0.000153818
2017-10-10T14:43:25.646288: step 1105, loss 0.130512, acc 0.953125, learning_rate 0.000153598
2017-10-10T14:43:25.821255: step 1106, loss 0.219612, acc 0.921875, learning_rate 0.000153379
2017-10-10T14:43:26.094298: step 1107, loss 0.264116, acc 0.875, learning_rate 0.000153161
2017-10-10T14:43:26.395499: step 1108, loss 0.252566, acc 0.890625, learning_rate 0.000152944
2017-10-10T14:43:26.680802: step 1109, loss 0.159274, acc 0.9375, learning_rate 0.000152728
2017-10-10T14:43:26.960853: step 1110, loss 0.111769, acc 0.953125, learning_rate 0.000152513
2017-10-10T14:43:27.240869: step 1111, loss 0.11222, acc 0.96875, learning_rate 0.000152299
2017-10-10T14:43:27.594531: step 1112, loss 0.209595, acc 0.890625, learning_rate 0.000152085
2017-10-10T14:43:27.896297: step 1113, loss 0.138027, acc 0.9375, learning_rate 0.000151872
2017-10-10T14:43:28.214563: step 1114, loss 0.236464, acc 0.90625, learning_rate 0.000151661
2017-10-10T14:43:28.500832: step 1115, loss 0.127332, acc 0.953125, learning_rate 0.00015145
2017-10-10T14:43:28.864947: step 1116, loss 0.221542, acc 0.9375, learning_rate 0.00015124
2017-10-10T14:43:29.180977: step 1117, loss 0.118708, acc 0.96875, learning_rate 0.000151031
2017-10-10T14:43:29.469131: step 1118, loss 0.173728, acc 0.890625, learning_rate 0.000150822
2017-10-10T14:43:29.767690: step 1119, loss 0.240984, acc 0.90625, learning_rate 0.000150615
2017-10-10T14:43:30.116519: step 1120, loss 0.101697, acc 0.96875, learning_rate 0.000150408

Evaluation:
2017-10-10T14:43:30.844489: step 1120, loss 0.220583, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1120

2017-10-10T14:43:32.003180: step 1121, loss 0.143996, acc 0.953125, learning_rate 0.000150203
2017-10-10T14:43:32.331604: step 1122, loss 0.0553032, acc 0.984375, learning_rate 0.000149998
2017-10-10T14:43:32.690571: step 1123, loss 0.0770258, acc 1, learning_rate 0.000149794
2017-10-10T14:43:33.005048: step 1124, loss 0.183569, acc 0.9375, learning_rate 0.00014959
2017-10-10T14:43:33.288758: step 1125, loss 0.18127, acc 0.921875, learning_rate 0.000149388
2017-10-10T14:43:33.651820: step 1126, loss 0.146784, acc 0.9375, learning_rate 0.000149186
2017-10-10T14:43:33.988996: step 1127, loss 0.0908979, acc 0.96875, learning_rate 0.000148986
2017-10-10T14:43:34.320447: step 1128, loss 0.157352, acc 0.9375, learning_rate 0.000148786
2017-10-10T14:43:34.665036: step 1129, loss 0.22101, acc 0.921875, learning_rate 0.000148587
2017-10-10T14:43:35.002502: step 1130, loss 0.0859799, acc 0.984375, learning_rate 0.000148388
2017-10-10T14:43:35.342350: step 1131, loss 0.212296, acc 0.921875, learning_rate 0.000148191
2017-10-10T14:43:35.650174: step 1132, loss 0.117113, acc 0.96875, learning_rate 0.000147994
2017-10-10T14:43:36.035901: step 1133, loss 0.110425, acc 0.953125, learning_rate 0.000147798
2017-10-10T14:43:36.369349: step 1134, loss 0.15556, acc 0.953125, learning_rate 0.000147603
2017-10-10T14:43:36.708730: step 1135, loss 0.146478, acc 0.953125, learning_rate 0.000147409
2017-10-10T14:43:36.992362: step 1136, loss 0.164528, acc 0.9375, learning_rate 0.000147215
2017-10-10T14:43:37.285979: step 1137, loss 0.131953, acc 0.953125, learning_rate 0.000147022
2017-10-10T14:43:37.602226: step 1138, loss 0.0754927, acc 0.984375, learning_rate 0.000146831
2017-10-10T14:43:37.966452: step 1139, loss 0.216934, acc 0.921875, learning_rate 0.000146639
2017-10-10T14:43:38.318504: step 1140, loss 0.225045, acc 0.921875, learning_rate 0.000146449
2017-10-10T14:43:38.658844: step 1141, loss 0.190884, acc 0.953125, learning_rate 0.000146259
2017-10-10T14:43:39.026791: step 1142, loss 0.0649374, acc 0.984375, learning_rate 0.000146071
2017-10-10T14:43:39.375561: step 1143, loss 0.0820437, acc 0.96875, learning_rate 0.000145883
2017-10-10T14:43:39.761880: step 1144, loss 0.0788717, acc 0.953125, learning_rate 0.000145695
2017-10-10T14:43:40.097351: step 1145, loss 0.458708, acc 0.84375, learning_rate 0.000145509
2017-10-10T14:43:40.451468: step 1146, loss 0.100686, acc 0.9375, learning_rate 0.000145323
2017-10-10T14:43:40.812822: step 1147, loss 0.120208, acc 0.96875, learning_rate 0.000145138
2017-10-10T14:43:41.179475: step 1148, loss 0.165244, acc 0.9375, learning_rate 0.000144954
2017-10-10T14:43:41.435376: step 1149, loss 0.163168, acc 0.9375, learning_rate 0.00014477
2017-10-10T14:43:41.617767: step 1150, loss 0.143555, acc 0.953125, learning_rate 0.000144588
2017-10-10T14:43:41.868786: step 1151, loss 0.121013, acc 0.953125, learning_rate 0.000144406
2017-10-10T14:43:42.115594: step 1152, loss 0.131407, acc 0.953125, learning_rate 0.000144224
2017-10-10T14:43:42.409653: step 1153, loss 0.0730299, acc 0.953125, learning_rate 0.000144044
2017-10-10T14:43:42.807155: step 1154, loss 0.0887474, acc 0.96875, learning_rate 0.000143864
2017-10-10T14:43:43.151534: step 1155, loss 0.124586, acc 0.953125, learning_rate 0.000143685
2017-10-10T14:43:43.464221: step 1156, loss 0.122992, acc 0.96875, learning_rate 0.000143507
2017-10-10T14:43:43.789220: step 1157, loss 0.106376, acc 0.96875, learning_rate 0.000143329
2017-10-10T14:43:44.138676: step 1158, loss 0.102382, acc 0.9375, learning_rate 0.000143152
2017-10-10T14:43:44.564841: step 1159, loss 0.0663634, acc 0.96875, learning_rate 0.000142976
2017-10-10T14:43:44.864522: step 1160, loss 0.112721, acc 0.953125, learning_rate 0.000142801

Evaluation:
2017-10-10T14:43:45.483744: step 1160, loss 0.218636, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1160

2017-10-10T14:43:46.720840: step 1161, loss 0.139588, acc 0.953125, learning_rate 0.000142626
2017-10-10T14:43:47.048836: step 1162, loss 0.103151, acc 0.953125, learning_rate 0.000142452
2017-10-10T14:43:47.346695: step 1163, loss 0.207155, acc 0.90625, learning_rate 0.000142279
2017-10-10T14:43:47.680911: step 1164, loss 0.209421, acc 0.9375, learning_rate 0.000142106
2017-10-10T14:43:47.967184: step 1165, loss 0.0806074, acc 0.96875, learning_rate 0.000141934
2017-10-10T14:43:48.304838: step 1166, loss 0.0561335, acc 0.984375, learning_rate 0.000141763
2017-10-10T14:43:48.659758: step 1167, loss 0.130935, acc 0.96875, learning_rate 0.000141593
2017-10-10T14:43:49.072959: step 1168, loss 0.107131, acc 0.96875, learning_rate 0.000141423
2017-10-10T14:43:49.447669: step 1169, loss 0.0790359, acc 0.984375, learning_rate 0.000141254
2017-10-10T14:43:49.848904: step 1170, loss 0.178048, acc 0.9375, learning_rate 0.000141085
2017-10-10T14:43:50.257045: step 1171, loss 0.0966535, acc 0.96875, learning_rate 0.000140918
2017-10-10T14:43:50.682643: step 1172, loss 0.120024, acc 0.953125, learning_rate 0.000140751
2017-10-10T14:43:51.060245: step 1173, loss 0.0773539, acc 0.984375, learning_rate 0.000140584
2017-10-10T14:43:51.464743: step 1174, loss 0.179503, acc 0.9375, learning_rate 0.000140419
2017-10-10T14:43:51.821992: step 1175, loss 0.0955115, acc 0.984375, learning_rate 0.000140254
2017-10-10T14:43:52.155390: step 1176, loss 0.17846, acc 0.901961, learning_rate 0.000140089
2017-10-10T14:43:52.556058: step 1177, loss 0.089542, acc 0.96875, learning_rate 0.000139926
2017-10-10T14:43:52.953143: step 1178, loss 0.120025, acc 0.96875, learning_rate 0.000139763
2017-10-10T14:43:53.328981: step 1179, loss 0.0753435, acc 0.984375, learning_rate 0.0001396
2017-10-10T14:43:53.706639: step 1180, loss 0.138142, acc 0.96875, learning_rate 0.000139439
2017-10-10T14:43:54.037043: step 1181, loss 0.103855, acc 0.953125, learning_rate 0.000139278
2017-10-10T14:43:54.404906: step 1182, loss 0.179425, acc 0.921875, learning_rate 0.000139118
2017-10-10T14:43:54.781338: step 1183, loss 0.0655577, acc 0.984375, learning_rate 0.000138958
2017-10-10T14:43:55.147043: step 1184, loss 0.0681833, acc 0.984375, learning_rate 0.000138799
2017-10-10T14:43:55.584549: step 1185, loss 0.219431, acc 0.953125, learning_rate 0.00013864
2017-10-10T14:43:55.911640: step 1186, loss 0.0944517, acc 0.96875, learning_rate 0.000138483
2017-10-10T14:43:56.292870: step 1187, loss 0.217906, acc 0.953125, learning_rate 0.000138326
2017-10-10T14:43:56.635350: step 1188, loss 0.13283, acc 0.953125, learning_rate 0.000138169
2017-10-10T14:43:56.987248: step 1189, loss 0.264666, acc 0.90625, learning_rate 0.000138013
2017-10-10T14:43:57.328791: step 1190, loss 0.0937076, acc 0.96875, learning_rate 0.000137858
2017-10-10T14:43:57.719184: step 1191, loss 0.148216, acc 0.953125, learning_rate 0.000137704
2017-10-10T14:43:58.048716: step 1192, loss 0.348091, acc 0.890625, learning_rate 0.00013755
2017-10-10T14:43:58.429102: step 1193, loss 0.171057, acc 0.921875, learning_rate 0.000137397
2017-10-10T14:43:58.842707: step 1194, loss 0.240003, acc 0.921875, learning_rate 0.000137244
2017-10-10T14:43:59.287399: step 1195, loss 0.230256, acc 0.90625, learning_rate 0.000137092
2017-10-10T14:43:59.601448: step 1196, loss 0.103449, acc 0.96875, learning_rate 0.000136941
2017-10-10T14:43:59.933332: step 1197, loss 0.138187, acc 0.9375, learning_rate 0.00013679
2017-10-10T14:44:00.232351: step 1198, loss 0.256629, acc 0.953125, learning_rate 0.00013664
2017-10-10T14:44:00.574852: step 1199, loss 0.203495, acc 0.953125, learning_rate 0.00013649
2017-10-10T14:44:01.455675: step 1200, loss 0.21335, acc 0.921875, learning_rate 0.000136341

Evaluation:
2017-10-10T14:44:02.273840: step 1200, loss 0.219809, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1200

2017-10-10T14:44:03.587179: step 1201, loss 0.145837, acc 0.9375, learning_rate 0.000136193
2017-10-10T14:44:03.999062: step 1202, loss 0.143911, acc 0.953125, learning_rate 0.000136045
2017-10-10T14:44:04.405244: step 1203, loss 0.0459146, acc 1, learning_rate 0.000135898
2017-10-10T14:44:04.796871: step 1204, loss 0.112085, acc 0.9375, learning_rate 0.000135751
2017-10-10T14:44:05.212965: step 1205, loss 0.170878, acc 0.953125, learning_rate 0.000135605
2017-10-10T14:44:05.567864: step 1206, loss 0.0735202, acc 1, learning_rate 0.00013546
2017-10-10T14:44:05.855960: step 1207, loss 0.251337, acc 0.9375, learning_rate 0.000135315
2017-10-10T14:44:06.206641: step 1208, loss 0.224714, acc 0.90625, learning_rate 0.000135171
2017-10-10T14:44:06.620890: step 1209, loss 0.188403, acc 0.9375, learning_rate 0.000135028
2017-10-10T14:44:07.069118: step 1210, loss 0.0992452, acc 0.96875, learning_rate 0.000134885
2017-10-10T14:44:07.401879: step 1211, loss 0.210613, acc 0.90625, learning_rate 0.000134742
2017-10-10T14:44:07.736571: step 1212, loss 0.210489, acc 0.90625, learning_rate 0.0001346
2017-10-10T14:44:08.141773: step 1213, loss 0.14163, acc 0.9375, learning_rate 0.000134459
2017-10-10T14:44:08.492945: step 1214, loss 0.167115, acc 0.9375, learning_rate 0.000134319
2017-10-10T14:44:08.900130: step 1215, loss 0.208463, acc 0.953125, learning_rate 0.000134178
2017-10-10T14:44:09.299709: step 1216, loss 0.0774498, acc 1, learning_rate 0.000134039
2017-10-10T14:44:09.685732: step 1217, loss 0.129395, acc 0.9375, learning_rate 0.0001339
2017-10-10T14:44:10.087434: step 1218, loss 0.239575, acc 0.9375, learning_rate 0.000133762
2017-10-10T14:44:10.460905: step 1219, loss 0.20242, acc 0.953125, learning_rate 0.000133624
2017-10-10T14:44:10.806057: step 1220, loss 0.0804014, acc 0.984375, learning_rate 0.000133487
2017-10-10T14:44:11.181509: step 1221, loss 0.0777806, acc 0.96875, learning_rate 0.00013335
2017-10-10T14:44:11.538471: step 1222, loss 0.105846, acc 0.96875, learning_rate 0.000133214
2017-10-10T14:44:11.931401: step 1223, loss 0.166517, acc 0.9375, learning_rate 0.000133078
2017-10-10T14:44:12.255327: step 1224, loss 0.18694, acc 0.953125, learning_rate 0.000132943
2017-10-10T14:44:12.635792: step 1225, loss 0.100934, acc 0.96875, learning_rate 0.000132809
2017-10-10T14:44:13.036504: step 1226, loss 0.0914599, acc 0.96875, learning_rate 0.000132675
2017-10-10T14:44:13.470823: step 1227, loss 0.0925345, acc 0.953125, learning_rate 0.000132541
2017-10-10T14:44:13.879745: step 1228, loss 0.117255, acc 0.984375, learning_rate 0.000132409
2017-10-10T14:44:14.283697: step 1229, loss 0.107444, acc 0.984375, learning_rate 0.000132276
2017-10-10T14:44:14.525754: step 1230, loss 0.325705, acc 0.890625, learning_rate 0.000132145
2017-10-10T14:44:14.838575: step 1231, loss 0.0811613, acc 0.96875, learning_rate 0.000132013
2017-10-10T14:44:15.248976: step 1232, loss 0.0963827, acc 0.96875, learning_rate 0.000131883
2017-10-10T14:44:15.605597: step 1233, loss 0.289137, acc 0.90625, learning_rate 0.000131753
2017-10-10T14:44:16.032331: step 1234, loss 0.0558176, acc 0.984375, learning_rate 0.000131623
2017-10-10T14:44:16.429950: step 1235, loss 0.0995821, acc 0.96875, learning_rate 0.000131494
2017-10-10T14:44:16.871921: step 1236, loss 0.176345, acc 0.953125, learning_rate 0.000131365
2017-10-10T14:44:17.354948: step 1237, loss 0.044065, acc 0.984375, learning_rate 0.000131237
2017-10-10T14:44:17.708976: step 1238, loss 0.154005, acc 0.9375, learning_rate 0.00013111
2017-10-10T14:44:18.036870: step 1239, loss 0.229578, acc 0.90625, learning_rate 0.000130983
2017-10-10T14:44:18.301232: step 1240, loss 0.0704728, acc 0.984375, learning_rate 0.000130856

Evaluation:
2017-10-10T14:44:19.075375: step 1240, loss 0.217662, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1240

2017-10-10T14:44:20.484815: step 1241, loss 0.168699, acc 0.890625, learning_rate 0.00013073
2017-10-10T14:44:20.848867: step 1242, loss 0.330057, acc 0.890625, learning_rate 0.000130605
2017-10-10T14:44:21.156509: step 1243, loss 0.105638, acc 0.96875, learning_rate 0.00013048
2017-10-10T14:44:21.567063: step 1244, loss 0.112584, acc 0.984375, learning_rate 0.000130356
2017-10-10T14:44:21.851016: step 1245, loss 0.264978, acc 0.890625, learning_rate 0.000130232
2017-10-10T14:44:22.296925: step 1246, loss 0.152445, acc 0.921875, learning_rate 0.000130108
2017-10-10T14:44:22.662468: step 1247, loss 0.0753716, acc 0.96875, learning_rate 0.000129985
2017-10-10T14:44:23.026910: step 1248, loss 0.17819, acc 0.921875, learning_rate 0.000129863
2017-10-10T14:44:23.381307: step 1249, loss 0.0779622, acc 0.984375, learning_rate 0.000129741
2017-10-10T14:44:23.748897: step 1250, loss 0.0992536, acc 0.953125, learning_rate 0.00012962
2017-10-10T14:44:24.129076: step 1251, loss 0.0708852, acc 1, learning_rate 0.000129499
2017-10-10T14:44:24.518691: step 1252, loss 0.137397, acc 0.9375, learning_rate 0.000129378
2017-10-10T14:44:24.878636: step 1253, loss 0.0555832, acc 0.984375, learning_rate 0.000129259
2017-10-10T14:44:25.241436: step 1254, loss 0.0926524, acc 0.96875, learning_rate 0.000129139
2017-10-10T14:44:25.660872: step 1255, loss 0.0662546, acc 0.984375, learning_rate 0.00012902
2017-10-10T14:44:25.974800: step 1256, loss 0.252382, acc 0.921875, learning_rate 0.000128902
2017-10-10T14:44:26.284424: step 1257, loss 0.101272, acc 0.96875, learning_rate 0.000128784
2017-10-10T14:44:26.704516: step 1258, loss 0.127605, acc 0.96875, learning_rate 0.000128666
2017-10-10T14:44:27.116867: step 1259, loss 0.16874, acc 0.9375, learning_rate 0.000128549
2017-10-10T14:44:27.474507: step 1260, loss 0.219734, acc 0.90625, learning_rate 0.000128433
2017-10-10T14:44:27.836052: step 1261, loss 0.117014, acc 0.96875, learning_rate 0.000128317
2017-10-10T14:44:28.264364: step 1262, loss 0.144286, acc 0.953125, learning_rate 0.000128201
2017-10-10T14:44:28.700456: step 1263, loss 0.130047, acc 0.953125, learning_rate 0.000128086
2017-10-10T14:44:29.136854: step 1264, loss 0.129518, acc 0.984375, learning_rate 0.000127971
2017-10-10T14:44:29.484878: step 1265, loss 0.119469, acc 0.953125, learning_rate 0.000127857
2017-10-10T14:44:29.825096: step 1266, loss 0.294704, acc 0.921875, learning_rate 0.000127743
2017-10-10T14:44:30.232888: step 1267, loss 0.0797475, acc 0.96875, learning_rate 0.00012763
2017-10-10T14:44:30.540332: step 1268, loss 0.132588, acc 0.953125, learning_rate 0.000127517
2017-10-10T14:44:30.920855: step 1269, loss 0.167362, acc 0.921875, learning_rate 0.000127405
2017-10-10T14:44:31.264546: step 1270, loss 0.094918, acc 0.96875, learning_rate 0.000127293
2017-10-10T14:44:31.619626: step 1271, loss 0.217788, acc 0.90625, learning_rate 0.000127182
2017-10-10T14:44:32.021086: step 1272, loss 0.222809, acc 0.921875, learning_rate 0.000127071
2017-10-10T14:44:32.378145: step 1273, loss 0.110175, acc 0.953125, learning_rate 0.00012696
2017-10-10T14:44:32.716998: step 1274, loss 0.262148, acc 0.882353, learning_rate 0.00012685
2017-10-10T14:44:33.151145: step 1275, loss 0.0855505, acc 0.953125, learning_rate 0.000126741
2017-10-10T14:44:33.492019: step 1276, loss 0.0529227, acc 1, learning_rate 0.000126632
2017-10-10T14:44:33.884815: step 1277, loss 0.185088, acc 0.9375, learning_rate 0.000126523
2017-10-10T14:44:34.188579: step 1278, loss 0.0858282, acc 0.96875, learning_rate 0.000126415
2017-10-10T14:44:34.563597: step 1279, loss 0.238182, acc 0.921875, learning_rate 0.000126307
2017-10-10T14:44:34.898230: step 1280, loss 0.0842605, acc 0.984375, learning_rate 0.000126199

Evaluation:
2017-10-10T14:44:35.838279: step 1280, loss 0.221956, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1280

2017-10-10T14:44:37.404663: step 1281, loss 0.108387, acc 0.96875, learning_rate 0.000126093
2017-10-10T14:44:37.789094: step 1282, loss 0.139445, acc 0.9375, learning_rate 0.000125986
2017-10-10T14:44:38.206382: step 1283, loss 0.160623, acc 0.9375, learning_rate 0.00012588
2017-10-10T14:44:38.616152: step 1284, loss 0.152426, acc 0.9375, learning_rate 0.000125774
2017-10-10T14:44:39.046434: step 1285, loss 0.0304824, acc 1, learning_rate 0.000125669
2017-10-10T14:44:39.419498: step 1286, loss 0.104495, acc 0.953125, learning_rate 0.000125564
2017-10-10T14:44:39.807016: step 1287, loss 0.0612629, acc 0.984375, learning_rate 0.00012546
2017-10-10T14:44:40.250738: step 1288, loss 0.156433, acc 0.9375, learning_rate 0.000125356
2017-10-10T14:44:40.601107: step 1289, loss 0.201298, acc 0.921875, learning_rate 0.000125253
2017-10-10T14:44:40.926111: step 1290, loss 0.073461, acc 0.984375, learning_rate 0.00012515
2017-10-10T14:44:41.237088: step 1291, loss 0.177293, acc 0.9375, learning_rate 0.000125047
2017-10-10T14:44:41.577155: step 1292, loss 0.333679, acc 0.90625, learning_rate 0.000124945
2017-10-10T14:44:42.028978: step 1293, loss 0.232974, acc 0.890625, learning_rate 0.000124843
2017-10-10T14:44:42.328219: step 1294, loss 0.278705, acc 0.875, learning_rate 0.000124741
2017-10-10T14:44:42.647190: step 1295, loss 0.108745, acc 0.953125, learning_rate 0.00012464
2017-10-10T14:44:43.033626: step 1296, loss 0.243034, acc 0.90625, learning_rate 0.00012454
2017-10-10T14:44:43.457143: step 1297, loss 0.14231, acc 0.953125, learning_rate 0.00012444
2017-10-10T14:44:43.851981: step 1298, loss 0.166368, acc 0.9375, learning_rate 0.00012434
2017-10-10T14:44:44.262243: step 1299, loss 0.132309, acc 0.96875, learning_rate 0.000124241
2017-10-10T14:44:44.652683: step 1300, loss 0.0733855, acc 0.96875, learning_rate 0.000124142
2017-10-10T14:44:45.040016: step 1301, loss 0.0624155, acc 1, learning_rate 0.000124043
2017-10-10T14:44:45.409493: step 1302, loss 0.100045, acc 0.953125, learning_rate 0.000123945
2017-10-10T14:44:45.832746: step 1303, loss 0.162883, acc 0.9375, learning_rate 0.000123847
2017-10-10T14:44:46.217724: step 1304, loss 0.146553, acc 0.921875, learning_rate 0.00012375
2017-10-10T14:44:46.661200: step 1305, loss 0.113032, acc 0.984375, learning_rate 0.000123653
2017-10-10T14:44:47.025737: step 1306, loss 0.0660516, acc 0.984375, learning_rate 0.000123556
2017-10-10T14:44:47.408514: step 1307, loss 0.0650618, acc 1, learning_rate 0.00012346
2017-10-10T14:44:47.828832: step 1308, loss 0.101486, acc 0.953125, learning_rate 0.000123364
2017-10-10T14:44:48.198879: step 1309, loss 0.053012, acc 0.984375, learning_rate 0.000123269
2017-10-10T14:44:48.531698: step 1310, loss 0.10578, acc 0.96875, learning_rate 0.000123174
2017-10-10T14:44:48.921152: step 1311, loss 0.246827, acc 0.9375, learning_rate 0.00012308
2017-10-10T14:44:49.269099: step 1312, loss 0.180803, acc 0.9375, learning_rate 0.000122985
2017-10-10T14:44:49.665009: step 1313, loss 0.127949, acc 0.9375, learning_rate 0.000122892
2017-10-10T14:44:50.011348: step 1314, loss 0.144504, acc 0.953125, learning_rate 0.000122798
2017-10-10T14:44:50.377837: step 1315, loss 0.161, acc 0.953125, learning_rate 0.000122705
2017-10-10T14:44:50.763859: step 1316, loss 0.1371, acc 0.953125, learning_rate 0.000122612
2017-10-10T14:44:51.161110: step 1317, loss 0.19153, acc 0.9375, learning_rate 0.00012252
2017-10-10T14:44:51.594993: step 1318, loss 0.139786, acc 0.953125, learning_rate 0.000122428
2017-10-10T14:44:51.960068: step 1319, loss 0.284052, acc 0.875, learning_rate 0.000122337
2017-10-10T14:44:52.287319: step 1320, loss 0.120635, acc 0.953125, learning_rate 0.000122245

Evaluation:
2017-10-10T14:44:53.072840: step 1320, loss 0.214932, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1320

2017-10-10T14:44:54.366501: step 1321, loss 0.114484, acc 0.953125, learning_rate 0.000122155
2017-10-10T14:44:54.572324: step 1322, loss 0.21554, acc 0.90625, learning_rate 0.000122064
2017-10-10T14:44:54.888026: step 1323, loss 0.151037, acc 0.921875, learning_rate 0.000121974
2017-10-10T14:44:55.275315: step 1324, loss 0.270278, acc 0.890625, learning_rate 0.000121884
2017-10-10T14:44:55.633253: step 1325, loss 0.1277, acc 0.984375, learning_rate 0.000121795
2017-10-10T14:44:56.009109: step 1326, loss 0.119179, acc 0.96875, learning_rate 0.000121706
2017-10-10T14:44:56.375800: step 1327, loss 0.0977381, acc 0.96875, learning_rate 0.000121618
2017-10-10T14:44:56.724539: step 1328, loss 0.0989553, acc 0.953125, learning_rate 0.000121529
2017-10-10T14:44:57.143377: step 1329, loss 0.130677, acc 0.953125, learning_rate 0.000121441
2017-10-10T14:44:57.550918: step 1330, loss 0.0934897, acc 0.953125, learning_rate 0.000121354
2017-10-10T14:44:57.924872: step 1331, loss 0.111912, acc 0.96875, learning_rate 0.000121267
2017-10-10T14:44:58.290578: step 1332, loss 0.0938657, acc 0.984375, learning_rate 0.00012118
2017-10-10T14:44:58.665673: step 1333, loss 0.195193, acc 0.9375, learning_rate 0.000121093
2017-10-10T14:44:59.030906: step 1334, loss 0.126566, acc 0.9375, learning_rate 0.000121007
2017-10-10T14:44:59.343825: step 1335, loss 0.177538, acc 0.96875, learning_rate 0.000120922
2017-10-10T14:44:59.728370: step 1336, loss 0.163614, acc 0.953125, learning_rate 0.000120836
2017-10-10T14:45:00.126792: step 1337, loss 0.115288, acc 0.9375, learning_rate 0.000120751
2017-10-10T14:45:00.514902: step 1338, loss 0.15643, acc 0.953125, learning_rate 0.000120666
2017-10-10T14:45:00.914637: step 1339, loss 0.0841913, acc 0.984375, learning_rate 0.000120582
2017-10-10T14:45:01.277757: step 1340, loss 0.274068, acc 0.890625, learning_rate 0.000120498
2017-10-10T14:45:01.600386: step 1341, loss 0.0772646, acc 0.96875, learning_rate 0.000120414
2017-10-10T14:45:01.996898: step 1342, loss 0.0925837, acc 0.984375, learning_rate 0.000120331
2017-10-10T14:45:02.360552: step 1343, loss 0.09191, acc 0.984375, learning_rate 0.000120248
2017-10-10T14:45:02.764828: step 1344, loss 0.104915, acc 0.9375, learning_rate 0.000120165
2017-10-10T14:45:03.132813: step 1345, loss 0.0809971, acc 0.96875, learning_rate 0.000120083
2017-10-10T14:45:03.513002: step 1346, loss 0.158608, acc 0.9375, learning_rate 0.000120001
2017-10-10T14:45:03.886248: step 1347, loss 0.0824177, acc 0.96875, learning_rate 0.00011992
2017-10-10T14:45:04.201325: step 1348, loss 0.252015, acc 0.875, learning_rate 0.000119838
2017-10-10T14:45:04.570729: step 1349, loss 0.109693, acc 0.9375, learning_rate 0.000119757
2017-10-10T14:45:04.945003: step 1350, loss 0.233757, acc 0.921875, learning_rate 0.000119677
2017-10-10T14:45:05.319091: step 1351, loss 0.17864, acc 0.9375, learning_rate 0.000119596
2017-10-10T14:45:05.757543: step 1352, loss 0.158599, acc 0.9375, learning_rate 0.000119516
2017-10-10T14:45:06.177289: step 1353, loss 0.104758, acc 0.953125, learning_rate 0.000119437
2017-10-10T14:45:06.545575: step 1354, loss 0.174062, acc 0.9375, learning_rate 0.000119357
2017-10-10T14:45:06.896895: step 1355, loss 0.111276, acc 0.96875, learning_rate 0.000119278
2017-10-10T14:45:07.272320: step 1356, loss 0.121788, acc 0.953125, learning_rate 0.0001192
2017-10-10T14:45:07.628877: step 1357, loss 0.0848885, acc 0.96875, learning_rate 0.000119121
2017-10-10T14:45:07.980048: step 1358, loss 0.0797592, acc 0.984375, learning_rate 0.000119043
2017-10-10T14:45:08.310788: step 1359, loss 0.197302, acc 0.921875, learning_rate 0.000118965
2017-10-10T14:45:08.699203: step 1360, loss 0.104513, acc 0.953125, learning_rate 0.000118888

Evaluation:
2017-10-10T14:45:09.435389: step 1360, loss 0.214912, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1360

2017-10-10T14:45:10.648211: step 1361, loss 0.294164, acc 0.921875, learning_rate 0.000118811
2017-10-10T14:45:11.061223: step 1362, loss 0.0928939, acc 0.953125, learning_rate 0.000118734
2017-10-10T14:45:11.484871: step 1363, loss 0.196691, acc 0.953125, learning_rate 0.000118658
2017-10-10T14:45:11.981108: step 1364, loss 0.147007, acc 0.984375, learning_rate 0.000118582
2017-10-10T14:45:12.391329: step 1365, loss 0.232067, acc 0.90625, learning_rate 0.000118506
2017-10-10T14:45:12.736379: step 1366, loss 0.102696, acc 0.953125, learning_rate 0.00011843
2017-10-10T14:45:13.068975: step 1367, loss 0.148543, acc 0.9375, learning_rate 0.000118355
2017-10-10T14:45:13.407778: step 1368, loss 0.114856, acc 0.9375, learning_rate 0.00011828
2017-10-10T14:45:13.744876: step 1369, loss 0.255228, acc 0.921875, learning_rate 0.000118205
2017-10-10T14:45:14.211318: step 1370, loss 0.188596, acc 0.984375, learning_rate 0.000118131
2017-10-10T14:45:14.529084: step 1371, loss 0.175094, acc 0.953125, learning_rate 0.000118057
2017-10-10T14:45:14.848991: step 1372, loss 0.182534, acc 0.921569, learning_rate 0.000117983
2017-10-10T14:45:15.232811: step 1373, loss 0.13034, acc 0.953125, learning_rate 0.00011791
2017-10-10T14:45:15.633050: step 1374, loss 0.0814927, acc 0.984375, learning_rate 0.000117837
2017-10-10T14:45:15.977514: step 1375, loss 0.091502, acc 0.984375, learning_rate 0.000117764
2017-10-10T14:45:16.384889: step 1376, loss 0.110335, acc 0.984375, learning_rate 0.000117692
2017-10-10T14:45:16.775830: step 1377, loss 0.131554, acc 0.953125, learning_rate 0.000117619
2017-10-10T14:45:17.163882: step 1378, loss 0.135132, acc 0.953125, learning_rate 0.000117547
2017-10-10T14:45:17.528838: step 1379, loss 0.0881908, acc 0.96875, learning_rate 0.000117476
2017-10-10T14:45:17.915504: step 1380, loss 0.0940993, acc 0.96875, learning_rate 0.000117404
2017-10-10T14:45:18.345040: step 1381, loss 0.124047, acc 0.953125, learning_rate 0.000117333
2017-10-10T14:45:18.994155: step 1382, loss 0.113901, acc 0.96875, learning_rate 0.000117263
2017-10-10T14:45:19.362790: step 1383, loss 0.0471945, acc 1, learning_rate 0.000117192
2017-10-10T14:45:19.736896: step 1384, loss 0.112078, acc 0.96875, learning_rate 0.000117122
2017-10-10T14:45:20.196919: step 1385, loss 0.097809, acc 0.953125, learning_rate 0.000117052
2017-10-10T14:45:20.544985: step 1386, loss 0.165352, acc 0.921875, learning_rate 0.000116983
2017-10-10T14:45:20.910947: step 1387, loss 0.113187, acc 0.96875, learning_rate 0.000116913
2017-10-10T14:45:21.243804: step 1388, loss 0.112361, acc 0.984375, learning_rate 0.000116844
2017-10-10T14:45:21.572865: step 1389, loss 0.176395, acc 0.9375, learning_rate 0.000116775
2017-10-10T14:45:22.016931: step 1390, loss 0.253241, acc 0.953125, learning_rate 0.000116707
2017-10-10T14:45:22.405049: step 1391, loss 0.1432, acc 0.953125, learning_rate 0.000116639
2017-10-10T14:45:22.865017: step 1392, loss 0.117376, acc 0.953125, learning_rate 0.000116571
2017-10-10T14:45:23.264838: step 1393, loss 0.0995018, acc 0.984375, learning_rate 0.000116503
2017-10-10T14:45:23.609953: step 1394, loss 0.327089, acc 0.921875, learning_rate 0.000116436
2017-10-10T14:45:24.029121: step 1395, loss 0.212915, acc 0.890625, learning_rate 0.000116369
2017-10-10T14:45:24.318645: step 1396, loss 0.118683, acc 0.953125, learning_rate 0.000116302
2017-10-10T14:45:24.654009: step 1397, loss 0.0847555, acc 0.96875, learning_rate 0.000116235
2017-10-10T14:45:25.036206: step 1398, loss 0.212137, acc 0.90625, learning_rate 0.000116169
2017-10-10T14:45:25.448484: step 1399, loss 0.137602, acc 0.953125, learning_rate 0.000116103
2017-10-10T14:45:25.794921: step 1400, loss 0.222598, acc 0.90625, learning_rate 0.000116037

Evaluation:
2017-10-10T14:45:26.500882: step 1400, loss 0.215862, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1400

2017-10-10T14:45:27.819890: step 1401, loss 0.171946, acc 0.9375, learning_rate 0.000115972
2017-10-10T14:45:28.181187: step 1402, loss 0.169912, acc 0.921875, learning_rate 0.000115907
2017-10-10T14:45:28.512954: step 1403, loss 0.135202, acc 0.921875, learning_rate 0.000115842
2017-10-10T14:45:28.900967: step 1404, loss 0.12518, acc 0.953125, learning_rate 0.000115777
2017-10-10T14:45:29.303057: step 1405, loss 0.206592, acc 0.90625, learning_rate 0.000115713
2017-10-10T14:45:29.693224: step 1406, loss 0.0594717, acc 0.984375, learning_rate 0.000115649
2017-10-10T14:45:30.127000: step 1407, loss 0.124342, acc 0.953125, learning_rate 0.000115585
2017-10-10T14:45:30.564176: step 1408, loss 0.147805, acc 0.9375, learning_rate 0.000115521
2017-10-10T14:45:30.894180: step 1409, loss 0.171681, acc 0.953125, learning_rate 0.000115458
2017-10-10T14:45:31.163987: step 1410, loss 0.121615, acc 0.9375, learning_rate 0.000115395
2017-10-10T14:45:31.436931: step 1411, loss 0.195563, acc 0.9375, learning_rate 0.000115332
2017-10-10T14:45:31.769641: step 1412, loss 0.138945, acc 0.96875, learning_rate 0.000115269
2017-10-10T14:45:32.125148: step 1413, loss 0.0949434, acc 0.96875, learning_rate 0.000115207
2017-10-10T14:45:32.470075: step 1414, loss 0.115078, acc 0.9375, learning_rate 0.000115145
2017-10-10T14:45:32.829592: step 1415, loss 0.0853206, acc 0.96875, learning_rate 0.000115083
2017-10-10T14:45:33.220011: step 1416, loss 0.0507722, acc 0.984375, learning_rate 0.000115022
2017-10-10T14:45:33.589151: step 1417, loss 0.09536, acc 0.984375, learning_rate 0.00011496
2017-10-10T14:45:33.957912: step 1418, loss 0.0793977, acc 0.96875, learning_rate 0.000114899
2017-10-10T14:45:34.313306: step 1419, loss 0.250675, acc 0.890625, learning_rate 0.000114838
2017-10-10T14:45:34.636973: step 1420, loss 0.0655348, acc 0.984375, learning_rate 0.000114778
2017-10-10T14:45:35.096847: step 1421, loss 0.146098, acc 0.953125, learning_rate 0.000114717
2017-10-10T14:45:35.432839: step 1422, loss 0.0484189, acc 1, learning_rate 0.000114657
2017-10-10T14:45:35.765511: step 1423, loss 0.0150088, acc 1, learning_rate 0.000114598
2017-10-10T14:45:36.126952: step 1424, loss 0.0499968, acc 0.984375, learning_rate 0.000114538
2017-10-10T14:45:36.485552: step 1425, loss 0.314789, acc 0.875, learning_rate 0.000114479
2017-10-10T14:45:36.859480: step 1426, loss 0.148272, acc 0.953125, learning_rate 0.00011442
2017-10-10T14:45:37.204949: step 1427, loss 0.115911, acc 0.953125, learning_rate 0.000114361
2017-10-10T14:45:37.585146: step 1428, loss 0.129001, acc 0.9375, learning_rate 0.000114302
2017-10-10T14:45:37.960157: step 1429, loss 0.203369, acc 0.9375, learning_rate 0.000114244
2017-10-10T14:45:38.350639: step 1430, loss 0.121601, acc 0.96875, learning_rate 0.000114186
2017-10-10T14:45:38.708808: step 1431, loss 0.274488, acc 0.90625, learning_rate 0.000114128
2017-10-10T14:45:39.067587: step 1432, loss 0.106073, acc 0.953125, learning_rate 0.00011407
2017-10-10T14:45:39.456880: step 1433, loss 0.183127, acc 0.90625, learning_rate 0.000114013
2017-10-10T14:45:39.839539: step 1434, loss 0.200137, acc 0.921875, learning_rate 0.000113955
2017-10-10T14:45:40.200460: step 1435, loss 0.243959, acc 0.9375, learning_rate 0.000113898
2017-10-10T14:45:40.584842: step 1436, loss 0.135263, acc 0.953125, learning_rate 0.000113842
2017-10-10T14:45:40.953147: step 1437, loss 0.108193, acc 0.984375, learning_rate 0.000113785
2017-10-10T14:45:41.369983: step 1438, loss 0.120543, acc 0.9375, learning_rate 0.000113729
2017-10-10T14:45:41.721336: step 1439, loss 0.180535, acc 0.9375, learning_rate 0.000113673
2017-10-10T14:45:42.060091: step 1440, loss 0.171706, acc 0.953125, learning_rate 0.000113617

Evaluation:
2017-10-10T14:45:42.880735: step 1440, loss 0.214413, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1440

2017-10-10T14:45:44.486035: step 1441, loss 0.157484, acc 0.9375, learning_rate 0.000113561
2017-10-10T14:45:44.850212: step 1442, loss 0.0831104, acc 0.984375, learning_rate 0.000113506
2017-10-10T14:45:45.262454: step 1443, loss 0.191953, acc 0.9375, learning_rate 0.000113451
2017-10-10T14:45:45.672916: step 1444, loss 0.175341, acc 0.953125, learning_rate 0.000113396
2017-10-10T14:45:46.046782: step 1445, loss 0.109874, acc 0.96875, learning_rate 0.000113341
2017-10-10T14:45:46.386572: step 1446, loss 0.117907, acc 0.953125, learning_rate 0.000113287
2017-10-10T14:45:46.743836: step 1447, loss 0.202147, acc 0.9375, learning_rate 0.000113233
2017-10-10T14:45:47.073497: step 1448, loss 0.13835, acc 0.96875, learning_rate 0.000113179
2017-10-10T14:45:47.456898: step 1449, loss 0.0777711, acc 0.953125, learning_rate 0.000113125
2017-10-10T14:45:47.852841: step 1450, loss 0.158756, acc 0.9375, learning_rate 0.000113071
2017-10-10T14:45:48.221393: step 1451, loss 0.109483, acc 0.953125, learning_rate 0.000113018
2017-10-10T14:45:48.674706: step 1452, loss 0.196444, acc 0.9375, learning_rate 0.000112965
2017-10-10T14:45:49.108248: step 1453, loss 0.132292, acc 0.953125, learning_rate 0.000112912
2017-10-10T14:45:49.409585: step 1454, loss 0.195096, acc 0.921875, learning_rate 0.000112859
2017-10-10T14:45:49.727439: step 1455, loss 0.141597, acc 0.9375, learning_rate 0.000112807
2017-10-10T14:45:50.132942: step 1456, loss 0.109895, acc 0.953125, learning_rate 0.000112754
2017-10-10T14:45:50.501626: step 1457, loss 0.108077, acc 0.96875, learning_rate 0.000112702
2017-10-10T14:45:50.832929: step 1458, loss 0.229007, acc 0.9375, learning_rate 0.000112651
2017-10-10T14:45:51.188208: step 1459, loss 0.178132, acc 0.9375, learning_rate 0.000112599
2017-10-10T14:45:51.561547: step 1460, loss 0.224961, acc 0.90625, learning_rate 0.000112547
2017-10-10T14:45:51.944844: step 1461, loss 0.287058, acc 0.875, learning_rate 0.000112496
2017-10-10T14:45:52.323561: step 1462, loss 0.081072, acc 0.984375, learning_rate 0.000112445
2017-10-10T14:45:52.697106: step 1463, loss 0.157328, acc 0.984375, learning_rate 0.000112394
2017-10-10T14:45:53.083433: step 1464, loss 0.332574, acc 0.859375, learning_rate 0.000112344
2017-10-10T14:45:53.441829: step 1465, loss 0.174167, acc 0.9375, learning_rate 0.000112293
2017-10-10T14:45:53.867960: step 1466, loss 0.188984, acc 0.921875, learning_rate 0.000112243
2017-10-10T14:45:54.264990: step 1467, loss 0.205969, acc 0.90625, learning_rate 0.000112193
2017-10-10T14:45:54.615206: step 1468, loss 0.042054, acc 1, learning_rate 0.000112144
2017-10-10T14:45:55.011593: step 1469, loss 0.161009, acc 0.9375, learning_rate 0.000112094
2017-10-10T14:45:55.463077: step 1470, loss 0.109831, acc 0.960784, learning_rate 0.000112045
2017-10-10T14:45:55.811099: step 1471, loss 0.229164, acc 0.921875, learning_rate 0.000111995
2017-10-10T14:45:56.153483: step 1472, loss 0.108103, acc 0.96875, learning_rate 0.000111946
2017-10-10T14:45:56.532280: step 1473, loss 0.134555, acc 0.953125, learning_rate 0.000111898
2017-10-10T14:45:56.984564: step 1474, loss 0.293221, acc 0.9375, learning_rate 0.000111849
2017-10-10T14:45:57.352937: step 1475, loss 0.116647, acc 0.953125, learning_rate 0.000111801
2017-10-10T14:45:57.726439: step 1476, loss 0.148873, acc 0.921875, learning_rate 0.000111753
2017-10-10T14:45:58.128542: step 1477, loss 0.269646, acc 0.921875, learning_rate 0.000111705
2017-10-10T14:45:58.515439: step 1478, loss 0.106428, acc 0.953125, learning_rate 0.000111657
2017-10-10T14:45:58.916992: step 1479, loss 0.104013, acc 0.96875, learning_rate 0.000111609
2017-10-10T14:45:59.312317: step 1480, loss 0.170954, acc 0.953125, learning_rate 0.000111562

Evaluation:
2017-10-10T14:46:00.110242: step 1480, loss 0.212284, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1480

2017-10-10T14:46:01.472942: step 1481, loss 0.150792, acc 0.953125, learning_rate 0.000111515
2017-10-10T14:46:01.851446: step 1482, loss 0.144436, acc 0.9375, learning_rate 0.000111468
2017-10-10T14:46:02.315793: step 1483, loss 0.149496, acc 0.953125, learning_rate 0.000111421
2017-10-10T14:46:02.653845: step 1484, loss 0.0950336, acc 0.96875, learning_rate 0.000111374
2017-10-10T14:46:03.072858: step 1485, loss 0.201063, acc 0.953125, learning_rate 0.000111328
2017-10-10T14:46:03.396916: step 1486, loss 0.231342, acc 0.9375, learning_rate 0.000111282
2017-10-10T14:46:03.798545: step 1487, loss 0.0928031, acc 0.984375, learning_rate 0.000111236
2017-10-10T14:46:04.148017: step 1488, loss 0.0759863, acc 0.984375, learning_rate 0.00011119
2017-10-10T14:46:04.540405: step 1489, loss 0.0996167, acc 0.984375, learning_rate 0.000111144
2017-10-10T14:46:04.929061: step 1490, loss 0.21806, acc 0.90625, learning_rate 0.000111099
2017-10-10T14:46:05.348992: step 1491, loss 0.152891, acc 0.953125, learning_rate 0.000111053
2017-10-10T14:46:05.674345: step 1492, loss 0.0583025, acc 1, learning_rate 0.000111008
2017-10-10T14:46:06.026219: step 1493, loss 0.222258, acc 0.90625, learning_rate 0.000110963
2017-10-10T14:46:06.508130: step 1494, loss 0.171925, acc 0.953125, learning_rate 0.000110918
2017-10-10T14:46:06.894437: step 1495, loss 0.137754, acc 0.9375, learning_rate 0.000110874
2017-10-10T14:46:07.281021: step 1496, loss 0.147284, acc 0.953125, learning_rate 0.00011083
2017-10-10T14:46:07.616669: step 1497, loss 0.221184, acc 0.921875, learning_rate 0.000110785
2017-10-10T14:46:07.952228: step 1498, loss 0.14417, acc 0.9375, learning_rate 0.000110741
2017-10-10T14:46:08.272908: step 1499, loss 0.113127, acc 0.953125, learning_rate 0.000110697
2017-10-10T14:46:08.621219: step 1500, loss 0.0938152, acc 0.984375, learning_rate 0.000110654
2017-10-10T14:46:09.057740: step 1501, loss 0.127657, acc 0.953125, learning_rate 0.00011061
2017-10-10T14:46:09.390488: step 1502, loss 0.0934268, acc 0.96875, learning_rate 0.000110567
2017-10-10T14:46:09.780995: step 1503, loss 0.105101, acc 0.953125, learning_rate 0.000110524
2017-10-10T14:46:10.140825: step 1504, loss 0.190295, acc 0.9375, learning_rate 0.000110481
2017-10-10T14:46:10.557371: step 1505, loss 0.178893, acc 0.9375, learning_rate 0.000110438
2017-10-10T14:46:10.938746: step 1506, loss 0.128622, acc 0.953125, learning_rate 0.000110396
2017-10-10T14:46:11.332222: step 1507, loss 0.0496423, acc 0.984375, learning_rate 0.000110353
2017-10-10T14:46:11.738754: step 1508, loss 0.0595014, acc 0.984375, learning_rate 0.000110311
2017-10-10T14:46:12.123238: step 1509, loss 0.10876, acc 0.953125, learning_rate 0.000110269
2017-10-10T14:46:12.528970: step 1510, loss 0.11841, acc 0.953125, learning_rate 0.000110227
2017-10-10T14:46:12.914675: step 1511, loss 0.155887, acc 0.953125, learning_rate 0.000110185
2017-10-10T14:46:13.373656: step 1512, loss 0.10378, acc 0.96875, learning_rate 0.000110144
2017-10-10T14:46:13.716133: step 1513, loss 0.0853647, acc 0.96875, learning_rate 0.000110102
2017-10-10T14:46:14.079135: step 1514, loss 0.124238, acc 0.96875, learning_rate 0.000110061
2017-10-10T14:46:14.396959: step 1515, loss 0.213266, acc 0.890625, learning_rate 0.00011002
2017-10-10T14:46:14.801996: step 1516, loss 0.191114, acc 0.921875, learning_rate 0.000109979
2017-10-10T14:46:15.169390: step 1517, loss 0.133283, acc 0.953125, learning_rate 0.000109938
2017-10-10T14:46:15.591541: step 1518, loss 0.187318, acc 0.921875, learning_rate 0.000109898
2017-10-10T14:46:15.936297: step 1519, loss 0.154529, acc 0.9375, learning_rate 0.000109857
2017-10-10T14:46:16.290442: step 1520, loss 0.226769, acc 0.921875, learning_rate 0.000109817

Evaluation:
2017-10-10T14:46:17.095708: step 1520, loss 0.215741, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1520

2017-10-10T14:46:18.518617: step 1521, loss 0.134979, acc 0.96875, learning_rate 0.000109777
2017-10-10T14:46:18.923686: step 1522, loss 0.0976221, acc 0.96875, learning_rate 0.000109737
2017-10-10T14:46:19.241778: step 1523, loss 0.0609577, acc 0.984375, learning_rate 0.000109697
2017-10-10T14:46:19.708869: step 1524, loss 0.174592, acc 0.921875, learning_rate 0.000109658
2017-10-10T14:46:20.058816: step 1525, loss 0.227764, acc 0.921875, learning_rate 0.000109618
2017-10-10T14:46:20.405391: step 1526, loss 0.099205, acc 0.96875, learning_rate 0.000109579
2017-10-10T14:46:20.705835: step 1527, loss 0.137213, acc 0.984375, learning_rate 0.00010954
2017-10-10T14:46:21.094684: step 1528, loss 0.0495187, acc 0.984375, learning_rate 0.000109501
2017-10-10T14:46:21.534091: step 1529, loss 0.201338, acc 0.9375, learning_rate 0.000109462
2017-10-10T14:46:21.936952: step 1530, loss 0.285033, acc 0.90625, learning_rate 0.000109424
2017-10-10T14:46:22.329922: step 1531, loss 0.157399, acc 0.953125, learning_rate 0.000109385
2017-10-10T14:46:22.652914: step 1532, loss 0.206979, acc 0.921875, learning_rate 0.000109347
2017-10-10T14:46:23.040611: step 1533, loss 0.146737, acc 0.9375, learning_rate 0.000109309
2017-10-10T14:46:23.493093: step 1534, loss 0.170307, acc 0.953125, learning_rate 0.000109271
2017-10-10T14:46:23.842856: step 1535, loss 0.188733, acc 0.90625, learning_rate 0.000109233
2017-10-10T14:46:24.230494: step 1536, loss 0.289175, acc 0.921875, learning_rate 0.000109195
2017-10-10T14:46:24.685019: step 1537, loss 0.152479, acc 0.953125, learning_rate 0.000109158
2017-10-10T14:46:25.133968: step 1538, loss 0.113101, acc 0.96875, learning_rate 0.00010912
2017-10-10T14:46:25.434010: step 1539, loss 0.124154, acc 0.9375, learning_rate 0.000109083
2017-10-10T14:46:25.732817: step 1540, loss 0.0520455, acc 0.984375, learning_rate 0.000109046
2017-10-10T14:46:26.080227: step 1541, loss 0.297058, acc 0.875, learning_rate 0.000109009
2017-10-10T14:46:26.511227: step 1542, loss 0.0959901, acc 0.984375, learning_rate 0.000108972
2017-10-10T14:46:26.897392: step 1543, loss 0.200207, acc 0.921875, learning_rate 0.000108936
2017-10-10T14:46:27.232178: step 1544, loss 0.0621243, acc 1, learning_rate 0.000108899
2017-10-10T14:46:27.619102: step 1545, loss 0.127402, acc 0.96875, learning_rate 0.000108863
2017-10-10T14:46:28.008469: step 1546, loss 0.173061, acc 0.921875, learning_rate 0.000108827
2017-10-10T14:46:28.413183: step 1547, loss 0.158761, acc 0.953125, learning_rate 0.000108791
2017-10-10T14:46:28.804923: step 1548, loss 0.174919, acc 0.9375, learning_rate 0.000108755
2017-10-10T14:46:29.203899: step 1549, loss 0.0499829, acc 1, learning_rate 0.000108719
2017-10-10T14:46:29.637641: step 1550, loss 0.0641223, acc 0.984375, learning_rate 0.000108683
2017-10-10T14:46:30.003339: step 1551, loss 0.150254, acc 0.953125, learning_rate 0.000108648
2017-10-10T14:46:30.346661: step 1552, loss 0.0350234, acc 1, learning_rate 0.000108613
2017-10-10T14:46:30.680281: step 1553, loss 0.111497, acc 0.9375, learning_rate 0.000108577
2017-10-10T14:46:30.988420: step 1554, loss 0.0953259, acc 0.984375, learning_rate 0.000108542
2017-10-10T14:46:31.335603: step 1555, loss 0.165771, acc 0.953125, learning_rate 0.000108508
2017-10-10T14:46:31.778754: step 1556, loss 0.0568424, acc 0.984375, learning_rate 0.000108473
2017-10-10T14:46:32.140047: step 1557, loss 0.123822, acc 0.9375, learning_rate 0.000108438
2017-10-10T14:46:32.446196: step 1558, loss 0.130202, acc 0.96875, learning_rate 0.000108404
2017-10-10T14:46:32.816868: step 1559, loss 0.134869, acc 0.9375, learning_rate 0.00010837
2017-10-10T14:46:33.206467: step 1560, loss 0.0896208, acc 0.984375, learning_rate 0.000108335

Evaluation:
2017-10-10T14:46:34.034625: step 1560, loss 0.217343, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1560

2017-10-10T14:46:35.553798: step 1561, loss 0.068361, acc 0.96875, learning_rate 0.000108301
2017-10-10T14:46:35.995602: step 1562, loss 0.12497, acc 0.953125, learning_rate 0.000108267
2017-10-10T14:46:36.307425: step 1563, loss 0.203901, acc 0.9375, learning_rate 0.000108234
2017-10-10T14:46:36.667716: step 1564, loss 0.160506, acc 0.984375, learning_rate 0.0001082
2017-10-10T14:46:37.057967: step 1565, loss 0.116173, acc 0.9375, learning_rate 0.000108167
2017-10-10T14:46:37.437764: step 1566, loss 0.207914, acc 0.9375, learning_rate 0.000108133
2017-10-10T14:46:37.796897: step 1567, loss 0.0996199, acc 0.984375, learning_rate 0.0001081
2017-10-10T14:46:38.160853: step 1568, loss 0.0684003, acc 1, learning_rate 0.000108067
2017-10-10T14:46:38.529846: step 1569, loss 0.171807, acc 0.9375, learning_rate 0.000108034
2017-10-10T14:46:38.904632: step 1570, loss 0.216971, acc 0.921875, learning_rate 0.000108001
2017-10-10T14:46:39.245762: step 1571, loss 0.186715, acc 0.953125, learning_rate 0.000107969
2017-10-10T14:46:39.676296: step 1572, loss 0.0988453, acc 0.96875, learning_rate 0.000107936
2017-10-10T14:46:40.045070: step 1573, loss 0.181387, acc 0.953125, learning_rate 0.000107904
2017-10-10T14:46:40.399916: step 1574, loss 0.156582, acc 0.953125, learning_rate 0.000107871
2017-10-10T14:46:40.725208: step 1575, loss 0.104252, acc 0.953125, learning_rate 0.000107839
2017-10-10T14:46:41.096002: step 1576, loss 0.210761, acc 0.921875, learning_rate 0.000107807
2017-10-10T14:46:41.408951: step 1577, loss 0.186931, acc 0.90625, learning_rate 0.000107775
2017-10-10T14:46:41.830455: step 1578, loss 0.167677, acc 0.9375, learning_rate 0.000107744
2017-10-10T14:46:42.174701: step 1579, loss 0.133107, acc 0.953125, learning_rate 0.000107712
2017-10-10T14:46:42.528874: step 1580, loss 0.107214, acc 0.953125, learning_rate 0.000107681
2017-10-10T14:46:42.937212: step 1581, loss 0.101388, acc 0.953125, learning_rate 0.000107649
2017-10-10T14:46:43.318214: step 1582, loss 0.0548848, acc 1, learning_rate 0.000107618
2017-10-10T14:46:43.691544: step 1583, loss 0.27498, acc 0.90625, learning_rate 0.000107587
2017-10-10T14:46:43.987907: step 1584, loss 0.0694451, acc 0.984375, learning_rate 0.000107556
2017-10-10T14:46:44.277119: step 1585, loss 0.151212, acc 0.96875, learning_rate 0.000107525
2017-10-10T14:46:44.589288: step 1586, loss 0.141473, acc 0.921875, learning_rate 0.000107494
2017-10-10T14:46:44.960907: step 1587, loss 0.15019, acc 0.9375, learning_rate 0.000107464
2017-10-10T14:46:45.322453: step 1588, loss 0.137208, acc 0.96875, learning_rate 0.000107433
2017-10-10T14:46:45.680680: step 1589, loss 0.122646, acc 0.953125, learning_rate 0.000107403
2017-10-10T14:46:46.080881: step 1590, loss 0.0946427, acc 0.953125, learning_rate 0.000107373
2017-10-10T14:46:46.505774: step 1591, loss 0.164808, acc 0.9375, learning_rate 0.000107343
2017-10-10T14:46:46.910301: step 1592, loss 0.197664, acc 0.96875, learning_rate 0.000107313
2017-10-10T14:46:47.316874: step 1593, loss 0.0977699, acc 0.953125, learning_rate 0.000107283
2017-10-10T14:46:47.690297: step 1594, loss 0.212215, acc 0.921875, learning_rate 0.000107253
2017-10-10T14:46:48.085830: step 1595, loss 0.0909767, acc 0.984375, learning_rate 0.000107224
2017-10-10T14:46:48.414515: step 1596, loss 0.112257, acc 0.96875, learning_rate 0.000107194
2017-10-10T14:46:48.733082: step 1597, loss 0.112303, acc 0.953125, learning_rate 0.000107165
2017-10-10T14:46:49.133202: step 1598, loss 0.0895732, acc 0.953125, learning_rate 0.000107136
2017-10-10T14:46:49.474850: step 1599, loss 0.146428, acc 0.9375, learning_rate 0.000107106
2017-10-10T14:46:49.881819: step 1600, loss 0.105956, acc 0.96875, learning_rate 0.000107077

Evaluation:
2017-10-10T14:46:50.676927: step 1600, loss 0.214846, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1600

2017-10-10T14:46:51.951792: step 1601, loss 0.1918, acc 0.9375, learning_rate 0.000107048
2017-10-10T14:46:52.297823: step 1602, loss 0.0637807, acc 0.984375, learning_rate 0.00010702
2017-10-10T14:46:52.634988: step 1603, loss 0.106113, acc 0.96875, learning_rate 0.000106991
2017-10-10T14:46:53.044974: step 1604, loss 0.230339, acc 0.890625, learning_rate 0.000106963
2017-10-10T14:46:53.390661: step 1605, loss 0.0914075, acc 1, learning_rate 0.000106934
2017-10-10T14:46:53.732847: step 1606, loss 0.153868, acc 0.9375, learning_rate 0.000106906
2017-10-10T14:46:54.153381: step 1607, loss 0.25952, acc 0.9375, learning_rate 0.000106878
2017-10-10T14:46:54.528885: step 1608, loss 0.0914491, acc 0.96875, learning_rate 0.00010685
2017-10-10T14:46:54.962311: step 1609, loss 0.101002, acc 0.96875, learning_rate 0.000106822
2017-10-10T14:46:55.432776: step 1610, loss 0.117497, acc 0.9375, learning_rate 0.000106794
2017-10-10T14:46:55.805127: step 1611, loss 0.112839, acc 0.953125, learning_rate 0.000106766
2017-10-10T14:46:56.237286: step 1612, loss 0.0853658, acc 0.96875, learning_rate 0.000106738
2017-10-10T14:46:56.632882: step 1613, loss 0.147354, acc 0.953125, learning_rate 0.000106711
2017-10-10T14:46:56.912990: step 1614, loss 0.111848, acc 0.984375, learning_rate 0.000106684
2017-10-10T14:46:57.314540: step 1615, loss 0.124522, acc 0.9375, learning_rate 0.000106656
2017-10-10T14:46:57.740879: step 1616, loss 0.133418, acc 0.9375, learning_rate 0.000106629
2017-10-10T14:46:58.125037: step 1617, loss 0.0780832, acc 0.984375, learning_rate 0.000106602
2017-10-10T14:46:58.440849: step 1618, loss 0.23552, acc 0.921875, learning_rate 0.000106575
2017-10-10T14:46:58.835409: step 1619, loss 0.122307, acc 0.953125, learning_rate 0.000106548
2017-10-10T14:46:59.208628: step 1620, loss 0.146792, acc 0.9375, learning_rate 0.000106521
2017-10-10T14:46:59.588863: step 1621, loss 0.0679269, acc 1, learning_rate 0.000106495
2017-10-10T14:46:59.940989: step 1622, loss 0.285763, acc 0.9375, learning_rate 0.000106468
2017-10-10T14:47:00.376211: step 1623, loss 0.0826621, acc 0.953125, learning_rate 0.000106442
2017-10-10T14:47:00.760847: step 1624, loss 0.127381, acc 0.9375, learning_rate 0.000106416
2017-10-10T14:47:01.278206: step 1625, loss 0.284589, acc 0.875, learning_rate 0.000106389
2017-10-10T14:47:01.581889: step 1626, loss 0.0918823, acc 0.96875, learning_rate 0.000106363
2017-10-10T14:47:01.880922: step 1627, loss 0.195359, acc 0.953125, learning_rate 0.000106337
2017-10-10T14:47:02.193167: step 1628, loss 0.243034, acc 0.9375, learning_rate 0.000106312
2017-10-10T14:47:02.558027: step 1629, loss 0.188949, acc 0.921875, learning_rate 0.000106286
2017-10-10T14:47:02.919699: step 1630, loss 0.0401085, acc 0.984375, learning_rate 0.00010626
2017-10-10T14:47:03.240080: step 1631, loss 0.155303, acc 0.953125, learning_rate 0.000106235
2017-10-10T14:47:03.668890: step 1632, loss 0.185803, acc 0.953125, learning_rate 0.000106209
2017-10-10T14:47:04.077477: step 1633, loss 0.117914, acc 0.9375, learning_rate 0.000106184
2017-10-10T14:47:04.457010: step 1634, loss 0.211401, acc 0.90625, learning_rate 0.000106159
2017-10-10T14:47:04.867039: step 1635, loss 0.0609531, acc 0.984375, learning_rate 0.000106133
2017-10-10T14:47:05.180320: step 1636, loss 0.11944, acc 0.984375, learning_rate 0.000106108
2017-10-10T14:47:05.545055: step 1637, loss 0.125735, acc 0.9375, learning_rate 0.000106083
2017-10-10T14:47:05.950910: step 1638, loss 0.322296, acc 0.875, learning_rate 0.000106059
2017-10-10T14:47:06.324838: step 1639, loss 0.16308, acc 0.96875, learning_rate 0.000106034
2017-10-10T14:47:06.667924: step 1640, loss 0.172181, acc 0.9375, learning_rate 0.000106009

Evaluation:
2017-10-10T14:47:07.518441: step 1640, loss 0.214324, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1640

2017-10-10T14:47:08.612152: step 1641, loss 0.201751, acc 0.953125, learning_rate 0.000105985
2017-10-10T14:47:08.920392: step 1642, loss 0.152589, acc 0.921875, learning_rate 0.00010596
2017-10-10T14:47:09.226456: step 1643, loss 0.261983, acc 0.921875, learning_rate 0.000105936
2017-10-10T14:47:09.572922: step 1644, loss 0.158426, acc 0.953125, learning_rate 0.000105912
2017-10-10T14:47:09.979281: step 1645, loss 0.116464, acc 0.96875, learning_rate 0.000105888
2017-10-10T14:47:10.350862: step 1646, loss 0.173018, acc 0.953125, learning_rate 0.000105864
2017-10-10T14:47:10.694882: step 1647, loss 0.0496494, acc 1, learning_rate 0.00010584
2017-10-10T14:47:11.103895: step 1648, loss 0.148564, acc 0.9375, learning_rate 0.000105816
2017-10-10T14:47:11.511860: step 1649, loss 0.129137, acc 0.984375, learning_rate 0.000105792
2017-10-10T14:47:11.822883: step 1650, loss 0.153007, acc 0.921875, learning_rate 0.000105768
2017-10-10T14:47:12.236236: step 1651, loss 0.0526476, acc 0.984375, learning_rate 0.000105745
2017-10-10T14:47:12.548985: step 1652, loss 0.058962, acc 1, learning_rate 0.000105721
2017-10-10T14:47:12.940125: step 1653, loss 0.0798615, acc 0.96875, learning_rate 0.000105698
2017-10-10T14:47:13.355899: step 1654, loss 0.146362, acc 0.9375, learning_rate 0.000105675
2017-10-10T14:47:13.790482: step 1655, loss 0.164392, acc 0.953125, learning_rate 0.000105652
2017-10-10T14:47:14.157007: step 1656, loss 0.214202, acc 0.9375, learning_rate 0.000105629
2017-10-10T14:47:14.518952: step 1657, loss 0.120332, acc 0.953125, learning_rate 0.000105606
2017-10-10T14:47:14.893157: step 1658, loss 0.152859, acc 0.9375, learning_rate 0.000105583
2017-10-10T14:47:15.279823: step 1659, loss 0.177904, acc 0.921875, learning_rate 0.00010556
2017-10-10T14:47:15.636874: step 1660, loss 0.134324, acc 0.96875, learning_rate 0.000105537
2017-10-10T14:47:16.038656: step 1661, loss 0.0913425, acc 0.96875, learning_rate 0.000105515
2017-10-10T14:47:16.393024: step 1662, loss 0.187429, acc 0.96875, learning_rate 0.000105492
2017-10-10T14:47:16.793526: step 1663, loss 0.118084, acc 0.96875, learning_rate 0.00010547
2017-10-10T14:47:17.160893: step 1664, loss 0.246608, acc 0.9375, learning_rate 0.000105447
2017-10-10T14:47:17.569071: step 1665, loss 0.0646136, acc 0.984375, learning_rate 0.000105425
2017-10-10T14:47:17.833029: step 1666, loss 0.207866, acc 0.941176, learning_rate 0.000105403
2017-10-10T14:47:18.265018: step 1667, loss 0.0803028, acc 0.96875, learning_rate 0.000105381
2017-10-10T14:47:18.692490: step 1668, loss 0.208026, acc 0.921875, learning_rate 0.000105359
2017-10-10T14:47:19.071941: step 1669, loss 0.144879, acc 0.953125, learning_rate 0.000105337
2017-10-10T14:47:19.492865: step 1670, loss 0.105962, acc 0.953125, learning_rate 0.000105315
2017-10-10T14:47:19.976666: step 1671, loss 0.16066, acc 0.921875, learning_rate 0.000105294
2017-10-10T14:47:20.193800: step 1672, loss 0.155285, acc 0.921875, learning_rate 0.000105272
2017-10-10T14:47:20.485256: step 1673, loss 0.196462, acc 0.953125, learning_rate 0.000105251
2017-10-10T14:47:20.791001: step 1674, loss 0.128326, acc 0.953125, learning_rate 0.000105229
2017-10-10T14:47:21.021004: step 1675, loss 0.0786628, acc 0.984375, learning_rate 0.000105208
2017-10-10T14:47:21.372770: step 1676, loss 0.119859, acc 0.96875, learning_rate 0.000105186
2017-10-10T14:47:21.760208: step 1677, loss 0.113646, acc 0.96875, learning_rate 0.000105165
2017-10-10T14:47:22.139828: step 1678, loss 0.181542, acc 0.90625, learning_rate 0.000105144
2017-10-10T14:47:22.553077: step 1679, loss 0.146929, acc 0.9375, learning_rate 0.000105123
2017-10-10T14:47:22.873427: step 1680, loss 0.12667, acc 0.953125, learning_rate 0.000105102

Evaluation:
2017-10-10T14:47:23.676041: step 1680, loss 0.216904, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1680

2017-10-10T14:47:24.988857: step 1681, loss 0.285217, acc 0.90625, learning_rate 0.000105081
2017-10-10T14:47:25.310413: step 1682, loss 0.110351, acc 0.984375, learning_rate 0.000105061
2017-10-10T14:47:25.704062: step 1683, loss 0.0471714, acc 0.984375, learning_rate 0.00010504
2017-10-10T14:47:26.026353: step 1684, loss 0.283479, acc 0.90625, learning_rate 0.00010502
2017-10-10T14:47:26.417401: step 1685, loss 0.0859347, acc 0.984375, learning_rate 0.000104999
2017-10-10T14:47:26.805636: step 1686, loss 0.0905295, acc 0.96875, learning_rate 0.000104979
2017-10-10T14:47:27.189612: step 1687, loss 0.139507, acc 0.953125, learning_rate 0.000104958
2017-10-10T14:47:27.577602: step 1688, loss 0.0790246, acc 0.96875, learning_rate 0.000104938
2017-10-10T14:47:27.969050: step 1689, loss 0.160052, acc 0.9375, learning_rate 0.000104918
2017-10-10T14:47:28.364594: step 1690, loss 0.151413, acc 0.9375, learning_rate 0.000104898
2017-10-10T14:47:28.780530: step 1691, loss 0.0806145, acc 0.96875, learning_rate 0.000104878
2017-10-10T14:47:29.159452: step 1692, loss 0.203842, acc 0.921875, learning_rate 0.000104858
2017-10-10T14:47:29.541241: step 1693, loss 0.312532, acc 0.9375, learning_rate 0.000104838
2017-10-10T14:47:29.929013: step 1694, loss 0.102514, acc 0.96875, learning_rate 0.000104818
2017-10-10T14:47:30.302792: step 1695, loss 0.104915, acc 0.984375, learning_rate 0.000104799
2017-10-10T14:47:30.658826: step 1696, loss 0.141594, acc 0.96875, learning_rate 0.000104779
2017-10-10T14:47:31.096697: step 1697, loss 0.132212, acc 0.921875, learning_rate 0.00010476
2017-10-10T14:47:31.411520: step 1698, loss 0.108084, acc 0.953125, learning_rate 0.00010474
2017-10-10T14:47:31.733901: step 1699, loss 0.044468, acc 0.984375, learning_rate 0.000104721
2017-10-10T14:47:32.103030: step 1700, loss 0.0862144, acc 0.953125, learning_rate 0.000104702
2017-10-10T14:47:32.444871: step 1701, loss 0.218177, acc 0.953125, learning_rate 0.000104682
2017-10-10T14:47:32.902365: step 1702, loss 0.126337, acc 0.96875, learning_rate 0.000104663
2017-10-10T14:47:33.251167: step 1703, loss 0.0886919, acc 0.984375, learning_rate 0.000104644
2017-10-10T14:47:33.692787: step 1704, loss 0.0970267, acc 0.984375, learning_rate 0.000104625
2017-10-10T14:47:33.999431: step 1705, loss 0.18591, acc 0.9375, learning_rate 0.000104606
2017-10-10T14:47:34.349114: step 1706, loss 0.0807097, acc 0.953125, learning_rate 0.000104588
2017-10-10T14:47:34.745105: step 1707, loss 0.143481, acc 0.953125, learning_rate 0.000104569
2017-10-10T14:47:35.084519: step 1708, loss 0.0642284, acc 0.984375, learning_rate 0.00010455
2017-10-10T14:47:35.468886: step 1709, loss 0.152793, acc 0.953125, learning_rate 0.000104532
2017-10-10T14:47:35.880778: step 1710, loss 0.133927, acc 0.953125, learning_rate 0.000104513
2017-10-10T14:47:36.225498: step 1711, loss 0.131139, acc 0.96875, learning_rate 0.000104495
2017-10-10T14:47:36.562661: step 1712, loss 0.123127, acc 0.96875, learning_rate 0.000104476
2017-10-10T14:47:36.926024: step 1713, loss 0.104093, acc 0.96875, learning_rate 0.000104458
2017-10-10T14:47:37.311658: step 1714, loss 0.13364, acc 0.9375, learning_rate 0.00010444
2017-10-10T14:47:37.732999: step 1715, loss 0.148002, acc 0.953125, learning_rate 0.000104422
2017-10-10T14:47:38.180743: step 1716, loss 0.0660727, acc 0.96875, learning_rate 0.000104404
2017-10-10T14:47:38.527268: step 1717, loss 0.174351, acc 0.9375, learning_rate 0.000104386
2017-10-10T14:47:38.842129: step 1718, loss 0.142509, acc 0.953125, learning_rate 0.000104368
2017-10-10T14:47:39.135121: step 1719, loss 0.189951, acc 0.921875, learning_rate 0.00010435
2017-10-10T14:47:39.524509: step 1720, loss 0.0723102, acc 1, learning_rate 0.000104332

Evaluation:
2017-10-10T14:47:40.225018: step 1720, loss 0.218883, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1720

2017-10-10T14:47:41.520854: step 1721, loss 0.241, acc 0.921875, learning_rate 0.000104315
2017-10-10T14:47:41.864478: step 1722, loss 0.122461, acc 0.984375, learning_rate 0.000104297
2017-10-10T14:47:42.280834: step 1723, loss 0.197734, acc 0.953125, learning_rate 0.000104279
2017-10-10T14:47:42.629157: step 1724, loss 0.151673, acc 0.953125, learning_rate 0.000104262
2017-10-10T14:47:43.036528: step 1725, loss 0.241086, acc 0.90625, learning_rate 0.000104245
2017-10-10T14:47:43.417927: step 1726, loss 0.0602415, acc 0.96875, learning_rate 0.000104227
2017-10-10T14:47:43.806745: step 1727, loss 0.080389, acc 0.96875, learning_rate 0.00010421
2017-10-10T14:47:44.157334: step 1728, loss 0.10405, acc 0.96875, learning_rate 0.000104193
2017-10-10T14:47:44.549181: step 1729, loss 0.19098, acc 0.9375, learning_rate 0.000104176
2017-10-10T14:47:44.986538: step 1730, loss 0.169708, acc 0.953125, learning_rate 0.000104159
2017-10-10T14:47:45.384845: step 1731, loss 0.0510742, acc 1, learning_rate 0.000104142
2017-10-10T14:47:45.709138: step 1732, loss 0.150852, acc 0.953125, learning_rate 0.000104125
2017-10-10T14:47:46.094758: step 1733, loss 0.106025, acc 0.9375, learning_rate 0.000104108
2017-10-10T14:47:46.569570: step 1734, loss 0.180622, acc 0.953125, learning_rate 0.000104091
2017-10-10T14:47:47.016858: step 1735, loss 0.228058, acc 0.890625, learning_rate 0.000104074
2017-10-10T14:47:47.309424: step 1736, loss 0.140091, acc 0.96875, learning_rate 0.000104058
2017-10-10T14:47:47.623586: step 1737, loss 0.050805, acc 0.984375, learning_rate 0.000104041
2017-10-10T14:47:48.013005: step 1738, loss 0.221683, acc 0.953125, learning_rate 0.000104025
2017-10-10T14:47:48.412834: step 1739, loss 0.162578, acc 0.96875, learning_rate 0.000104008
2017-10-10T14:47:48.815319: step 1740, loss 0.17566, acc 0.953125, learning_rate 0.000103992
2017-10-10T14:47:49.205017: step 1741, loss 0.0487023, acc 1, learning_rate 0.000103976
2017-10-10T14:47:49.540985: step 1742, loss 0.110603, acc 0.96875, learning_rate 0.000103959
2017-10-10T14:47:49.964865: step 1743, loss 0.149711, acc 0.953125, learning_rate 0.000103943
2017-10-10T14:47:50.344971: step 1744, loss 0.138981, acc 0.953125, learning_rate 0.000103927
2017-10-10T14:47:50.722385: step 1745, loss 0.161973, acc 0.9375, learning_rate 0.000103911
2017-10-10T14:47:51.100311: step 1746, loss 0.172978, acc 0.953125, learning_rate 0.000103895
2017-10-10T14:47:51.493109: step 1747, loss 0.0936332, acc 0.984375, learning_rate 0.000103879
2017-10-10T14:47:51.875489: step 1748, loss 0.0959363, acc 0.953125, learning_rate 0.000103863
2017-10-10T14:47:52.270238: step 1749, loss 0.0461676, acc 1, learning_rate 0.000103848
2017-10-10T14:47:52.679924: step 1750, loss 0.072954, acc 1, learning_rate 0.000103832
2017-10-10T14:47:53.021056: step 1751, loss 0.111642, acc 0.96875, learning_rate 0.000103816
2017-10-10T14:47:53.403017: step 1752, loss 0.10249, acc 0.953125, learning_rate 0.000103801
2017-10-10T14:47:53.731960: step 1753, loss 0.209645, acc 0.9375, learning_rate 0.000103785
2017-10-10T14:47:54.127172: step 1754, loss 0.130253, acc 0.953125, learning_rate 0.00010377
2017-10-10T14:47:54.468546: step 1755, loss 0.141473, acc 0.96875, learning_rate 0.000103754
2017-10-10T14:47:54.839307: step 1756, loss 0.182916, acc 0.9375, learning_rate 0.000103739
2017-10-10T14:47:55.261646: step 1757, loss 0.216626, acc 0.921875, learning_rate 0.000103724
2017-10-10T14:47:55.634872: step 1758, loss 0.143505, acc 0.953125, learning_rate 0.000103709
2017-10-10T14:47:55.982158: step 1759, loss 0.165829, acc 0.9375, learning_rate 0.000103694
2017-10-10T14:47:56.432551: step 1760, loss 0.0857197, acc 0.96875, learning_rate 0.000103678

Evaluation:
2017-10-10T14:47:57.160410: step 1760, loss 0.213325, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1760

2017-10-10T14:47:58.461488: step 1761, loss 0.132889, acc 0.921875, learning_rate 0.000103663
2017-10-10T14:47:58.847387: step 1762, loss 0.0625004, acc 0.984375, learning_rate 0.000103648
2017-10-10T14:47:59.268846: step 1763, loss 0.198994, acc 0.890625, learning_rate 0.000103634
2017-10-10T14:47:59.639948: step 1764, loss 0.156124, acc 0.901961, learning_rate 0.000103619
2017-10-10T14:48:00.016870: step 1765, loss 0.155618, acc 0.9375, learning_rate 0.000103604
2017-10-10T14:48:00.358348: step 1766, loss 0.130257, acc 0.953125, learning_rate 0.000103589
2017-10-10T14:48:00.636355: step 1767, loss 0.10287, acc 0.984375, learning_rate 0.000103575
2017-10-10T14:48:01.023006: step 1768, loss 0.0616042, acc 0.984375, learning_rate 0.00010356
2017-10-10T14:48:01.426912: step 1769, loss 0.134396, acc 0.953125, learning_rate 0.000103545
2017-10-10T14:48:01.797627: step 1770, loss 0.0749824, acc 0.984375, learning_rate 0.000103531
2017-10-10T14:48:02.179914: step 1771, loss 0.176312, acc 0.953125, learning_rate 0.000103517
2017-10-10T14:48:02.509216: step 1772, loss 0.16983, acc 0.921875, learning_rate 0.000103502
2017-10-10T14:48:02.941493: step 1773, loss 0.0795468, acc 1, learning_rate 0.000103488
2017-10-10T14:48:03.284896: step 1774, loss 0.137239, acc 0.921875, learning_rate 0.000103474
2017-10-10T14:48:03.641036: step 1775, loss 0.0634859, acc 1, learning_rate 0.00010346
2017-10-10T14:48:04.003034: step 1776, loss 0.111847, acc 0.953125, learning_rate 0.000103445
2017-10-10T14:48:04.360293: step 1777, loss 0.047981, acc 1, learning_rate 0.000103431
2017-10-10T14:48:04.744938: step 1778, loss 0.291041, acc 0.9375, learning_rate 0.000103417
2017-10-10T14:48:05.184427: step 1779, loss 0.160715, acc 0.9375, learning_rate 0.000103403
2017-10-10T14:48:05.572549: step 1780, loss 0.0951079, acc 0.953125, learning_rate 0.00010339
2017-10-10T14:48:05.937175: step 1781, loss 0.0368879, acc 1, learning_rate 0.000103376
2017-10-10T14:48:06.309177: step 1782, loss 0.11929, acc 0.953125, learning_rate 0.000103362
2017-10-10T14:48:06.681141: step 1783, loss 0.196846, acc 0.921875, learning_rate 0.000103348
2017-10-10T14:48:07.021042: step 1784, loss 0.236478, acc 0.921875, learning_rate 0.000103335
2017-10-10T14:48:07.372951: step 1785, loss 0.0666965, acc 0.96875, learning_rate 0.000103321
2017-10-10T14:48:07.729378: step 1786, loss 0.137843, acc 0.953125, learning_rate 0.000103307
2017-10-10T14:48:08.094091: step 1787, loss 0.108447, acc 0.953125, learning_rate 0.000103294
2017-10-10T14:48:08.474610: step 1788, loss 0.0962494, acc 0.953125, learning_rate 0.00010328
2017-10-10T14:48:08.936234: step 1789, loss 0.0803644, acc 0.984375, learning_rate 0.000103267
2017-10-10T14:48:09.295665: step 1790, loss 0.089307, acc 0.953125, learning_rate 0.000103254
2017-10-10T14:48:09.696511: step 1791, loss 0.155559, acc 0.953125, learning_rate 0.00010324
2017-10-10T14:48:10.049418: step 1792, loss 0.137215, acc 0.953125, learning_rate 0.000103227
2017-10-10T14:48:10.377317: step 1793, loss 0.239629, acc 0.9375, learning_rate 0.000103214
2017-10-10T14:48:10.784187: step 1794, loss 0.179598, acc 0.921875, learning_rate 0.000103201
2017-10-10T14:48:11.135949: step 1795, loss 0.152329, acc 0.9375, learning_rate 0.000103188
2017-10-10T14:48:11.495152: step 1796, loss 0.0872049, acc 0.984375, learning_rate 0.000103175
2017-10-10T14:48:11.904597: step 1797, loss 0.19411, acc 0.90625, learning_rate 0.000103162
2017-10-10T14:48:12.275643: step 1798, loss 0.176956, acc 0.9375, learning_rate 0.000103149
2017-10-10T14:48:12.676443: step 1799, loss 0.193207, acc 0.921875, learning_rate 0.000103136
2017-10-10T14:48:13.105281: step 1800, loss 0.0675132, acc 0.96875, learning_rate 0.000103123

Evaluation:
2017-10-10T14:48:13.780137: step 1800, loss 0.215053, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1800

2017-10-10T14:48:14.970578: step 1801, loss 0.12332, acc 0.953125, learning_rate 0.000103111
2017-10-10T14:48:15.311670: step 1802, loss 0.227717, acc 0.96875, learning_rate 0.000103098
2017-10-10T14:48:15.642444: step 1803, loss 0.108033, acc 0.984375, learning_rate 0.000103085
2017-10-10T14:48:16.045250: step 1804, loss 0.0687401, acc 0.984375, learning_rate 0.000103073
2017-10-10T14:48:16.393922: step 1805, loss 0.124149, acc 0.96875, learning_rate 0.00010306
2017-10-10T14:48:16.734184: step 1806, loss 0.0977363, acc 0.96875, learning_rate 0.000103048
2017-10-10T14:48:17.067701: step 1807, loss 0.297632, acc 0.875, learning_rate 0.000103035
2017-10-10T14:48:17.429119: step 1808, loss 0.0438809, acc 0.984375, learning_rate 0.000103023
2017-10-10T14:48:17.788995: step 1809, loss 0.07979, acc 0.984375, learning_rate 0.00010301
2017-10-10T14:48:18.205101: step 1810, loss 0.151837, acc 0.96875, learning_rate 0.000102998
2017-10-10T14:48:18.597390: step 1811, loss 0.101087, acc 0.96875, learning_rate 0.000102986
2017-10-10T14:48:18.945156: step 1812, loss 0.137963, acc 0.953125, learning_rate 0.000102974
2017-10-10T14:48:19.373284: step 1813, loss 0.244861, acc 0.90625, learning_rate 0.000102962
2017-10-10T14:48:19.746127: step 1814, loss 0.0978639, acc 0.96875, learning_rate 0.000102949
2017-10-10T14:48:20.151909: step 1815, loss 0.0648345, acc 0.984375, learning_rate 0.000102937
2017-10-10T14:48:20.532233: step 1816, loss 0.12687, acc 0.953125, learning_rate 0.000102925
2017-10-10T14:48:20.889448: step 1817, loss 0.178233, acc 0.984375, learning_rate 0.000102913
2017-10-10T14:48:21.246887: step 1818, loss 0.134285, acc 0.96875, learning_rate 0.000102902
2017-10-10T14:48:21.612809: step 1819, loss 0.251019, acc 0.90625, learning_rate 0.00010289
2017-10-10T14:48:22.049059: step 1820, loss 0.107777, acc 0.96875, learning_rate 0.000102878
2017-10-10T14:48:22.388863: step 1821, loss 0.140659, acc 0.953125, learning_rate 0.000102866
2017-10-10T14:48:22.825395: step 1822, loss 0.0801409, acc 0.984375, learning_rate 0.000102855
2017-10-10T14:48:23.206374: step 1823, loss 0.139562, acc 0.953125, learning_rate 0.000102843
2017-10-10T14:48:23.578002: step 1824, loss 0.167559, acc 0.921875, learning_rate 0.000102831
2017-10-10T14:48:23.980991: step 1825, loss 0.134708, acc 0.953125, learning_rate 0.00010282
2017-10-10T14:48:24.340902: step 1826, loss 0.0975647, acc 0.984375, learning_rate 0.000102808
2017-10-10T14:48:24.642020: step 1827, loss 0.168693, acc 0.96875, learning_rate 0.000102797
2017-10-10T14:48:25.028932: step 1828, loss 0.118532, acc 0.953125, learning_rate 0.000102785
2017-10-10T14:48:25.426984: step 1829, loss 0.103305, acc 0.96875, learning_rate 0.000102774
2017-10-10T14:48:25.787410: step 1830, loss 0.127012, acc 0.9375, learning_rate 0.000102763
2017-10-10T14:48:26.186597: step 1831, loss 0.136089, acc 0.96875, learning_rate 0.000102751
2017-10-10T14:48:26.567393: step 1832, loss 0.0991715, acc 0.96875, learning_rate 0.00010274
2017-10-10T14:48:26.911053: step 1833, loss 0.142764, acc 0.953125, learning_rate 0.000102729
2017-10-10T14:48:27.282376: step 1834, loss 0.131958, acc 0.96875, learning_rate 0.000102718
2017-10-10T14:48:27.642917: step 1835, loss 0.129107, acc 0.953125, learning_rate 0.000102707
2017-10-10T14:48:28.041349: step 1836, loss 0.223195, acc 0.953125, learning_rate 0.000102696
2017-10-10T14:48:28.406719: step 1837, loss 0.22312, acc 0.90625, learning_rate 0.000102685
2017-10-10T14:48:28.807741: step 1838, loss 0.159776, acc 0.96875, learning_rate 0.000102674
2017-10-10T14:48:29.205114: step 1839, loss 0.143087, acc 0.9375, learning_rate 0.000102663
2017-10-10T14:48:29.614226: step 1840, loss 0.116277, acc 0.9375, learning_rate 0.000102652

Evaluation:
2017-10-10T14:48:30.336227: step 1840, loss 0.214758, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1840

2017-10-10T14:48:31.632997: step 1841, loss 0.223106, acc 0.9375, learning_rate 0.000102641
2017-10-10T14:48:31.990522: step 1842, loss 0.128219, acc 0.96875, learning_rate 0.00010263
2017-10-10T14:48:32.409793: step 1843, loss 0.1241, acc 0.953125, learning_rate 0.00010262
2017-10-10T14:48:32.850040: step 1844, loss 0.0909182, acc 0.96875, learning_rate 0.000102609
2017-10-10T14:48:33.163125: step 1845, loss 0.089919, acc 0.984375, learning_rate 0.000102598
2017-10-10T14:48:33.496933: step 1846, loss 0.238516, acc 0.875, learning_rate 0.000102588
2017-10-10T14:48:33.877099: step 1847, loss 0.104837, acc 0.953125, learning_rate 0.000102577
2017-10-10T14:48:34.254006: step 1848, loss 0.058577, acc 1, learning_rate 0.000102567
2017-10-10T14:48:34.654895: step 1849, loss 0.0965032, acc 0.96875, learning_rate 0.000102556
2017-10-10T14:48:34.986956: step 1850, loss 0.11711, acc 0.9375, learning_rate 0.000102546
2017-10-10T14:48:35.412963: step 1851, loss 0.142419, acc 0.96875, learning_rate 0.000102535
2017-10-10T14:48:35.820934: step 1852, loss 0.175285, acc 0.921875, learning_rate 0.000102525
2017-10-10T14:48:36.228916: step 1853, loss 0.160171, acc 0.9375, learning_rate 0.000102515
2017-10-10T14:48:36.629056: step 1854, loss 0.180653, acc 0.890625, learning_rate 0.000102504
2017-10-10T14:48:37.009001: step 1855, loss 0.101886, acc 0.96875, learning_rate 0.000102494
2017-10-10T14:48:37.360927: step 1856, loss 0.0745647, acc 0.984375, learning_rate 0.000102484
2017-10-10T14:48:37.705199: step 1857, loss 0.113843, acc 0.953125, learning_rate 0.000102474
2017-10-10T14:48:38.096907: step 1858, loss 0.0750296, acc 0.984375, learning_rate 0.000102464
2017-10-10T14:48:38.496233: step 1859, loss 0.120678, acc 0.953125, learning_rate 0.000102454
2017-10-10T14:48:38.888266: step 1860, loss 0.149486, acc 0.921875, learning_rate 0.000102444
2017-10-10T14:48:39.307305: step 1861, loss 0.0737405, acc 0.953125, learning_rate 0.000102434
2017-10-10T14:48:39.648816: step 1862, loss 0.167648, acc 0.941176, learning_rate 0.000102424
2017-10-10T14:48:39.968863: step 1863, loss 0.150729, acc 0.96875, learning_rate 0.000102414
2017-10-10T14:48:40.281078: step 1864, loss 0.29693, acc 0.890625, learning_rate 0.000102404
2017-10-10T14:48:40.633903: step 1865, loss 0.0492342, acc 1, learning_rate 0.000102394
2017-10-10T14:48:41.055930: step 1866, loss 0.0768609, acc 0.96875, learning_rate 0.000102384
2017-10-10T14:48:41.428553: step 1867, loss 0.108362, acc 0.96875, learning_rate 0.000102375
2017-10-10T14:48:41.870958: step 1868, loss 0.155851, acc 0.96875, learning_rate 0.000102365
2017-10-10T14:48:42.204938: step 1869, loss 0.210311, acc 0.890625, learning_rate 0.000102355
2017-10-10T14:48:42.543426: step 1870, loss 0.0485038, acc 0.984375, learning_rate 0.000102346
2017-10-10T14:48:42.884840: step 1871, loss 0.182923, acc 0.90625, learning_rate 0.000102336
2017-10-10T14:48:43.296594: step 1872, loss 0.0825139, acc 0.984375, learning_rate 0.000102327
2017-10-10T14:48:43.648933: step 1873, loss 0.0561565, acc 0.984375, learning_rate 0.000102317
2017-10-10T14:48:44.003848: step 1874, loss 0.29364, acc 0.890625, learning_rate 0.000102308
2017-10-10T14:48:44.361114: step 1875, loss 0.0651595, acc 0.984375, learning_rate 0.000102298
2017-10-10T14:48:44.697191: step 1876, loss 0.224298, acc 0.96875, learning_rate 0.000102289
2017-10-10T14:48:45.096823: step 1877, loss 0.068987, acc 0.96875, learning_rate 0.000102279
2017-10-10T14:48:45.479562: step 1878, loss 0.219404, acc 0.953125, learning_rate 0.00010227
2017-10-10T14:48:45.867452: step 1879, loss 0.114062, acc 0.953125, learning_rate 0.000102261
2017-10-10T14:48:46.229039: step 1880, loss 0.0955917, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-10-10T14:48:46.857635: step 1880, loss 0.214732, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1880

2017-10-10T14:48:48.404847: step 1881, loss 0.0807551, acc 0.984375, learning_rate 0.000102242
2017-10-10T14:48:48.742003: step 1882, loss 0.13276, acc 0.984375, learning_rate 0.000102233
2017-10-10T14:48:49.198127: step 1883, loss 0.1598, acc 0.953125, learning_rate 0.000102224
2017-10-10T14:48:49.497460: step 1884, loss 0.226168, acc 0.90625, learning_rate 0.000102215
2017-10-10T14:48:49.890038: step 1885, loss 0.115617, acc 0.953125, learning_rate 0.000102206
2017-10-10T14:48:50.280069: step 1886, loss 0.122931, acc 0.953125, learning_rate 0.000102197
2017-10-10T14:48:50.692839: step 1887, loss 0.167273, acc 0.921875, learning_rate 0.000102188
2017-10-10T14:48:51.150715: step 1888, loss 0.187336, acc 0.953125, learning_rate 0.000102179
2017-10-10T14:48:51.437726: step 1889, loss 0.115545, acc 0.953125, learning_rate 0.00010217
2017-10-10T14:48:51.758916: step 1890, loss 0.0949629, acc 0.96875, learning_rate 0.000102161
2017-10-10T14:48:52.076182: step 1891, loss 0.11134, acc 0.96875, learning_rate 0.000102153
2017-10-10T14:48:52.472690: step 1892, loss 0.198056, acc 0.953125, learning_rate 0.000102144
2017-10-10T14:48:52.964418: step 1893, loss 0.12213, acc 0.96875, learning_rate 0.000102135
2017-10-10T14:48:53.277637: step 1894, loss 0.139721, acc 0.953125, learning_rate 0.000102126
2017-10-10T14:48:53.580175: step 1895, loss 0.102386, acc 0.96875, learning_rate 0.000102118
2017-10-10T14:48:53.896653: step 1896, loss 0.170113, acc 0.953125, learning_rate 0.000102109
2017-10-10T14:48:54.252824: step 1897, loss 0.186469, acc 0.9375, learning_rate 0.0001021
2017-10-10T14:48:54.600945: step 1898, loss 0.156727, acc 0.9375, learning_rate 0.000102092
2017-10-10T14:48:54.916253: step 1899, loss 0.133985, acc 0.96875, learning_rate 0.000102083
2017-10-10T14:48:55.317331: step 1900, loss 0.168717, acc 0.953125, learning_rate 0.000102075
2017-10-10T14:48:55.640835: step 1901, loss 0.127123, acc 0.984375, learning_rate 0.000102066
2017-10-10T14:48:56.016869: step 1902, loss 0.125744, acc 0.953125, learning_rate 0.000102058
2017-10-10T14:48:56.440888: step 1903, loss 0.134211, acc 0.96875, learning_rate 0.00010205
2017-10-10T14:48:56.782194: step 1904, loss 0.131426, acc 0.9375, learning_rate 0.000102041
2017-10-10T14:48:57.216961: step 1905, loss 0.140508, acc 0.921875, learning_rate 0.000102033
2017-10-10T14:48:57.633105: step 1906, loss 0.0764129, acc 0.96875, learning_rate 0.000102025
2017-10-10T14:48:58.014188: step 1907, loss 0.0520389, acc 0.984375, learning_rate 0.000102016
2017-10-10T14:48:58.380936: step 1908, loss 0.105837, acc 0.953125, learning_rate 0.000102008
2017-10-10T14:48:58.726116: step 1909, loss 0.0939276, acc 0.96875, learning_rate 0.000102
2017-10-10T14:48:59.084249: step 1910, loss 0.090987, acc 0.96875, learning_rate 0.000101992
2017-10-10T14:48:59.412497: step 1911, loss 0.242197, acc 0.90625, learning_rate 0.000101984
2017-10-10T14:48:59.796975: step 1912, loss 0.182868, acc 0.9375, learning_rate 0.000101975
2017-10-10T14:49:00.181401: step 1913, loss 0.157566, acc 0.9375, learning_rate 0.000101967
2017-10-10T14:49:00.564943: step 1914, loss 0.0747042, acc 0.96875, learning_rate 0.000101959
2017-10-10T14:49:00.953956: step 1915, loss 0.170197, acc 0.953125, learning_rate 0.000101951
2017-10-10T14:49:01.252540: step 1916, loss 0.188738, acc 0.953125, learning_rate 0.000101943
2017-10-10T14:49:01.629199: step 1917, loss 0.0740082, acc 0.984375, learning_rate 0.000101935
2017-10-10T14:49:02.037036: step 1918, loss 0.111663, acc 0.96875, learning_rate 0.000101928
2017-10-10T14:49:02.387558: step 1919, loss 0.104799, acc 0.96875, learning_rate 0.00010192
2017-10-10T14:49:02.767769: step 1920, loss 0.308638, acc 0.875, learning_rate 0.000101912

Evaluation:
2017-10-10T14:49:03.468879: step 1920, loss 0.21342, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1920

2017-10-10T14:49:04.791153: step 1921, loss 0.0502363, acc 1, learning_rate 0.000101904
2017-10-10T14:49:05.139335: step 1922, loss 0.156893, acc 0.921875, learning_rate 0.000101896
2017-10-10T14:49:05.540873: step 1923, loss 0.265633, acc 0.90625, learning_rate 0.000101889
2017-10-10T14:49:05.867709: step 1924, loss 0.133154, acc 0.9375, learning_rate 0.000101881
2017-10-10T14:49:06.339782: step 1925, loss 0.136242, acc 0.96875, learning_rate 0.000101873
2017-10-10T14:49:06.660974: step 1926, loss 0.178256, acc 0.921875, learning_rate 0.000101865
2017-10-10T14:49:06.976983: step 1927, loss 0.167274, acc 0.90625, learning_rate 0.000101858
2017-10-10T14:49:07.345426: step 1928, loss 0.107912, acc 0.953125, learning_rate 0.00010185
2017-10-10T14:49:07.726483: step 1929, loss 0.145708, acc 0.953125, learning_rate 0.000101843
2017-10-10T14:49:08.064190: step 1930, loss 0.124791, acc 0.953125, learning_rate 0.000101835
2017-10-10T14:49:08.427223: step 1931, loss 0.0370562, acc 0.984375, learning_rate 0.000101828
2017-10-10T14:49:08.792857: step 1932, loss 0.0646392, acc 0.984375, learning_rate 0.00010182
2017-10-10T14:49:09.201162: step 1933, loss 0.214373, acc 0.90625, learning_rate 0.000101813
2017-10-10T14:49:09.600657: step 1934, loss 0.175379, acc 0.953125, learning_rate 0.000101805
2017-10-10T14:49:09.906728: step 1935, loss 0.246009, acc 0.890625, learning_rate 0.000101798
2017-10-10T14:49:10.183108: step 1936, loss 0.134026, acc 0.9375, learning_rate 0.000101791
2017-10-10T14:49:10.498337: step 1937, loss 0.131626, acc 0.921875, learning_rate 0.000101783
2017-10-10T14:49:10.870385: step 1938, loss 0.0726481, acc 0.96875, learning_rate 0.000101776
2017-10-10T14:49:11.252262: step 1939, loss 0.151942, acc 0.96875, learning_rate 0.000101769
2017-10-10T14:49:11.590970: step 1940, loss 0.176273, acc 0.953125, learning_rate 0.000101762
2017-10-10T14:49:11.961182: step 1941, loss 0.13538, acc 0.921875, learning_rate 0.000101754
2017-10-10T14:49:12.304985: step 1942, loss 0.106465, acc 0.96875, learning_rate 0.000101747
2017-10-10T14:49:12.664627: step 1943, loss 0.087642, acc 0.96875, learning_rate 0.00010174
2017-10-10T14:49:13.015616: step 1944, loss 0.0896643, acc 0.96875, learning_rate 0.000101733
2017-10-10T14:49:13.424862: step 1945, loss 0.0399331, acc 1, learning_rate 0.000101726
2017-10-10T14:49:13.798065: step 1946, loss 0.17964, acc 0.953125, learning_rate 0.000101719
2017-10-10T14:49:14.177034: step 1947, loss 0.154512, acc 0.9375, learning_rate 0.000101712
2017-10-10T14:49:14.528405: step 1948, loss 0.176196, acc 0.9375, learning_rate 0.000101705
2017-10-10T14:49:14.902243: step 1949, loss 0.0987065, acc 0.953125, learning_rate 0.000101698
2017-10-10T14:49:15.274472: step 1950, loss 0.139124, acc 0.9375, learning_rate 0.000101691
2017-10-10T14:49:15.677228: step 1951, loss 0.130906, acc 0.953125, learning_rate 0.000101684
2017-10-10T14:49:16.076386: step 1952, loss 0.253259, acc 0.9375, learning_rate 0.000101677
2017-10-10T14:49:16.405789: step 1953, loss 0.101667, acc 0.984375, learning_rate 0.00010167
2017-10-10T14:49:16.795373: step 1954, loss 0.140173, acc 0.953125, learning_rate 0.000101664
2017-10-10T14:49:17.204864: step 1955, loss 0.200186, acc 0.953125, learning_rate 0.000101657
2017-10-10T14:49:17.612415: step 1956, loss 0.0994391, acc 0.96875, learning_rate 0.00010165
2017-10-10T14:49:18.022448: step 1957, loss 0.112565, acc 0.953125, learning_rate 0.000101643
2017-10-10T14:49:18.376084: step 1958, loss 0.0633232, acc 1, learning_rate 0.000101637
2017-10-10T14:49:18.749005: step 1959, loss 0.202756, acc 0.90625, learning_rate 0.00010163
2017-10-10T14:49:19.044930: step 1960, loss 0.0905688, acc 0.960784, learning_rate 0.000101623

Evaluation:
2017-10-10T14:49:19.938546: step 1960, loss 0.211411, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-1960

2017-10-10T14:49:21.547409: step 1961, loss 0.0694118, acc 0.984375, learning_rate 0.000101617
2017-10-10T14:49:21.917105: step 1962, loss 0.105948, acc 0.96875, learning_rate 0.00010161
2017-10-10T14:49:22.262361: step 1963, loss 0.130289, acc 0.953125, learning_rate 0.000101604
2017-10-10T14:49:22.576956: step 1964, loss 0.136291, acc 0.9375, learning_rate 0.000101597
2017-10-10T14:49:22.971729: step 1965, loss 0.117738, acc 0.9375, learning_rate 0.00010159
2017-10-10T14:49:23.413151: step 1966, loss 0.0615909, acc 1, learning_rate 0.000101584
2017-10-10T14:49:23.736884: step 1967, loss 0.143342, acc 0.953125, learning_rate 0.000101577
2017-10-10T14:49:24.092283: step 1968, loss 0.194193, acc 0.953125, learning_rate 0.000101571
2017-10-10T14:49:24.484097: step 1969, loss 0.234406, acc 0.890625, learning_rate 0.000101565
2017-10-10T14:49:24.872916: step 1970, loss 0.0613203, acc 0.984375, learning_rate 0.000101558
2017-10-10T14:49:25.300939: step 1971, loss 0.129264, acc 0.96875, learning_rate 0.000101552
2017-10-10T14:49:25.696831: step 1972, loss 0.119015, acc 0.953125, learning_rate 0.000101546
2017-10-10T14:49:26.086925: step 1973, loss 0.239763, acc 0.953125, learning_rate 0.000101539
2017-10-10T14:49:26.472919: step 1974, loss 0.0567865, acc 0.984375, learning_rate 0.000101533
2017-10-10T14:49:26.929027: step 1975, loss 0.152566, acc 0.96875, learning_rate 0.000101527
2017-10-10T14:49:27.402778: step 1976, loss 0.205192, acc 0.9375, learning_rate 0.00010152
2017-10-10T14:49:27.674659: step 1977, loss 0.133886, acc 0.953125, learning_rate 0.000101514
2017-10-10T14:49:27.995559: step 1978, loss 0.0676576, acc 0.984375, learning_rate 0.000101508
2017-10-10T14:49:28.341888: step 1979, loss 0.10347, acc 0.953125, learning_rate 0.000101502
2017-10-10T14:49:28.689083: step 1980, loss 0.117361, acc 0.96875, learning_rate 0.000101496
2017-10-10T14:49:29.077500: step 1981, loss 0.0966208, acc 0.953125, learning_rate 0.00010149
2017-10-10T14:49:29.456622: step 1982, loss 0.151096, acc 0.9375, learning_rate 0.000101484
2017-10-10T14:49:29.852778: step 1983, loss 0.180825, acc 0.921875, learning_rate 0.000101478
2017-10-10T14:49:30.273320: step 1984, loss 0.191532, acc 0.921875, learning_rate 0.000101472
2017-10-10T14:49:30.648934: step 1985, loss 0.110033, acc 0.953125, learning_rate 0.000101466
2017-10-10T14:49:31.061322: step 1986, loss 0.100622, acc 0.9375, learning_rate 0.00010146
2017-10-10T14:49:31.436877: step 1987, loss 0.0989785, acc 0.984375, learning_rate 0.000101454
2017-10-10T14:49:31.787540: step 1988, loss 0.151253, acc 0.953125, learning_rate 0.000101448
2017-10-10T14:49:32.123253: step 1989, loss 0.193004, acc 0.9375, learning_rate 0.000101442
2017-10-10T14:49:32.471510: step 1990, loss 0.0996465, acc 0.96875, learning_rate 0.000101436
2017-10-10T14:49:32.691738: step 1991, loss 0.0686017, acc 1, learning_rate 0.00010143
2017-10-10T14:49:33.182990: step 1992, loss 0.0873908, acc 0.96875, learning_rate 0.000101424
2017-10-10T14:49:33.524125: step 1993, loss 0.104122, acc 0.96875, learning_rate 0.000101418
2017-10-10T14:49:33.886833: step 1994, loss 0.145419, acc 0.9375, learning_rate 0.000101413
2017-10-10T14:49:34.232064: step 1995, loss 0.0368836, acc 1, learning_rate 0.000101407
2017-10-10T14:49:34.588823: step 1996, loss 0.162509, acc 0.921875, learning_rate 0.000101401
2017-10-10T14:49:35.011205: step 1997, loss 0.139669, acc 0.9375, learning_rate 0.000101395
2017-10-10T14:49:35.429061: step 1998, loss 0.141179, acc 0.953125, learning_rate 0.00010139
2017-10-10T14:49:35.864826: step 1999, loss 0.0877742, acc 0.984375, learning_rate 0.000101384
2017-10-10T14:49:36.169065: step 2000, loss 0.115031, acc 0.96875, learning_rate 0.000101378

Evaluation:
2017-10-10T14:49:36.901721: step 2000, loss 0.21195, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2000

2017-10-10T14:49:38.444508: step 2001, loss 0.109312, acc 0.953125, learning_rate 0.000101373
2017-10-10T14:49:38.795654: step 2002, loss 0.148209, acc 0.9375, learning_rate 0.000101367
2017-10-10T14:49:39.140861: step 2003, loss 0.117288, acc 0.9375, learning_rate 0.000101362
2017-10-10T14:49:39.524887: step 2004, loss 0.149431, acc 0.921875, learning_rate 0.000101356
2017-10-10T14:49:39.988355: step 2005, loss 0.162075, acc 0.953125, learning_rate 0.00010135
2017-10-10T14:49:40.384887: step 2006, loss 0.117783, acc 0.96875, learning_rate 0.000101345
2017-10-10T14:49:40.835542: step 2007, loss 0.140159, acc 0.953125, learning_rate 0.000101339
2017-10-10T14:49:41.220688: step 2008, loss 0.178862, acc 0.921875, learning_rate 0.000101334
2017-10-10T14:49:41.615457: step 2009, loss 0.140114, acc 0.96875, learning_rate 0.000101328
2017-10-10T14:49:42.014333: step 2010, loss 0.218238, acc 0.90625, learning_rate 0.000101323
2017-10-10T14:49:42.320052: step 2011, loss 0.0982024, acc 0.953125, learning_rate 0.000101318
2017-10-10T14:49:42.692311: step 2012, loss 0.196764, acc 0.90625, learning_rate 0.000101312
2017-10-10T14:49:43.125012: step 2013, loss 0.18257, acc 0.9375, learning_rate 0.000101307
2017-10-10T14:49:43.460939: step 2014, loss 0.0493988, acc 1, learning_rate 0.000101302
2017-10-10T14:49:43.841471: step 2015, loss 0.170411, acc 0.921875, learning_rate 0.000101296
2017-10-10T14:49:44.218031: step 2016, loss 0.124704, acc 0.953125, learning_rate 0.000101291
2017-10-10T14:49:44.557217: step 2017, loss 0.166604, acc 0.9375, learning_rate 0.000101286
2017-10-10T14:49:44.959034: step 2018, loss 0.179823, acc 0.921875, learning_rate 0.00010128
2017-10-10T14:49:45.348846: step 2019, loss 0.0375513, acc 1, learning_rate 0.000101275
2017-10-10T14:49:45.836898: step 2020, loss 0.12901, acc 0.96875, learning_rate 0.00010127
2017-10-10T14:49:46.293603: step 2021, loss 0.236967, acc 0.921875, learning_rate 0.000101265
2017-10-10T14:49:46.576987: step 2022, loss 0.182225, acc 0.921875, learning_rate 0.00010126
2017-10-10T14:49:46.864865: step 2023, loss 0.136158, acc 0.921875, learning_rate 0.000101255
2017-10-10T14:49:47.247174: step 2024, loss 0.0767804, acc 0.984375, learning_rate 0.000101249
2017-10-10T14:49:47.634226: step 2025, loss 0.14056, acc 0.9375, learning_rate 0.000101244
2017-10-10T14:49:47.996954: step 2026, loss 0.0739779, acc 1, learning_rate 0.000101239
2017-10-10T14:49:48.356835: step 2027, loss 0.211937, acc 0.90625, learning_rate 0.000101234
2017-10-10T14:49:48.821818: step 2028, loss 0.205973, acc 0.921875, learning_rate 0.000101229
2017-10-10T14:49:49.199971: step 2029, loss 0.155197, acc 0.9375, learning_rate 0.000101224
2017-10-10T14:49:49.532992: step 2030, loss 0.131796, acc 0.9375, learning_rate 0.000101219
2017-10-10T14:49:49.957090: step 2031, loss 0.110707, acc 0.953125, learning_rate 0.000101214
2017-10-10T14:49:50.340845: step 2032, loss 0.085808, acc 0.96875, learning_rate 0.000101209
2017-10-10T14:49:50.666069: step 2033, loss 0.091148, acc 0.96875, learning_rate 0.000101204
2017-10-10T14:49:51.009229: step 2034, loss 0.102029, acc 0.984375, learning_rate 0.000101199
2017-10-10T14:49:51.366417: step 2035, loss 0.0728399, acc 0.96875, learning_rate 0.000101194
2017-10-10T14:49:51.721199: step 2036, loss 0.0663672, acc 1, learning_rate 0.00010119
2017-10-10T14:49:52.107301: step 2037, loss 0.108153, acc 0.9375, learning_rate 0.000101185
2017-10-10T14:49:52.485887: step 2038, loss 0.138277, acc 0.9375, learning_rate 0.00010118
2017-10-10T14:49:52.835420: step 2039, loss 0.282523, acc 0.890625, learning_rate 0.000101175
2017-10-10T14:49:53.205798: step 2040, loss 0.196529, acc 0.9375, learning_rate 0.00010117

Evaluation:
2017-10-10T14:49:53.915776: step 2040, loss 0.213933, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2040

2017-10-10T14:49:55.246574: step 2041, loss 0.102876, acc 0.96875, learning_rate 0.000101166
2017-10-10T14:49:55.616843: step 2042, loss 0.117028, acc 0.953125, learning_rate 0.000101161
2017-10-10T14:49:56.016947: step 2043, loss 0.130586, acc 0.921875, learning_rate 0.000101156
2017-10-10T14:49:56.337235: step 2044, loss 0.0922393, acc 0.984375, learning_rate 0.000101151
2017-10-10T14:49:56.720894: step 2045, loss 0.128277, acc 0.9375, learning_rate 0.000101147
2017-10-10T14:49:57.077215: step 2046, loss 0.0702784, acc 0.984375, learning_rate 0.000101142
2017-10-10T14:49:57.431795: step 2047, loss 0.344194, acc 0.890625, learning_rate 0.000101137
2017-10-10T14:49:57.813782: step 2048, loss 0.0760944, acc 0.984375, learning_rate 0.000101133
2017-10-10T14:49:58.147732: step 2049, loss 0.106716, acc 0.953125, learning_rate 0.000101128
2017-10-10T14:49:58.548328: step 2050, loss 0.12858, acc 0.953125, learning_rate 0.000101123
2017-10-10T14:49:58.980673: step 2051, loss 0.221609, acc 0.921875, learning_rate 0.000101119
2017-10-10T14:49:59.325028: step 2052, loss 0.126091, acc 0.953125, learning_rate 0.000101114
2017-10-10T14:49:59.774716: step 2053, loss 0.0349253, acc 1, learning_rate 0.00010111
2017-10-10T14:50:00.096824: step 2054, loss 0.0720404, acc 0.984375, learning_rate 0.000101105
2017-10-10T14:50:00.404835: step 2055, loss 0.232833, acc 0.890625, learning_rate 0.000101101
2017-10-10T14:50:00.707744: step 2056, loss 0.12747, acc 0.953125, learning_rate 0.000101096
2017-10-10T14:50:01.068903: step 2057, loss 0.136897, acc 0.953125, learning_rate 0.000101092
2017-10-10T14:50:01.346496: step 2058, loss 0.164532, acc 0.960784, learning_rate 0.000101087
2017-10-10T14:50:01.746061: step 2059, loss 0.190123, acc 0.953125, learning_rate 0.000101083
2017-10-10T14:50:02.117136: step 2060, loss 0.0351537, acc 1, learning_rate 0.000101078
2017-10-10T14:50:02.481770: step 2061, loss 0.253029, acc 0.90625, learning_rate 0.000101074
2017-10-10T14:50:02.848609: step 2062, loss 0.19818, acc 0.953125, learning_rate 0.00010107
2017-10-10T14:50:03.209052: step 2063, loss 0.179946, acc 0.921875, learning_rate 0.000101065
2017-10-10T14:50:03.651698: step 2064, loss 0.031881, acc 1, learning_rate 0.000101061
2017-10-10T14:50:04.108418: step 2065, loss 0.0947922, acc 0.96875, learning_rate 0.000101057
2017-10-10T14:50:04.446111: step 2066, loss 0.111409, acc 0.96875, learning_rate 0.000101052
2017-10-10T14:50:04.753376: step 2067, loss 0.0606639, acc 0.984375, learning_rate 0.000101048
2017-10-10T14:50:05.049661: step 2068, loss 0.0561776, acc 0.984375, learning_rate 0.000101044
2017-10-10T14:50:05.367859: step 2069, loss 0.0502945, acc 1, learning_rate 0.000101039
2017-10-10T14:50:05.664122: step 2070, loss 0.164394, acc 0.9375, learning_rate 0.000101035
2017-10-10T14:50:06.101199: step 2071, loss 0.273996, acc 0.890625, learning_rate 0.000101031
2017-10-10T14:50:06.441808: step 2072, loss 0.161684, acc 0.953125, learning_rate 0.000101027
2017-10-10T14:50:06.818271: step 2073, loss 0.0812226, acc 0.984375, learning_rate 0.000101023
2017-10-10T14:50:07.196877: step 2074, loss 0.219449, acc 0.90625, learning_rate 0.000101018
2017-10-10T14:50:07.568833: step 2075, loss 0.141875, acc 0.921875, learning_rate 0.000101014
2017-10-10T14:50:07.933697: step 2076, loss 0.168548, acc 0.921875, learning_rate 0.00010101
2017-10-10T14:50:08.315778: step 2077, loss 0.146104, acc 0.921875, learning_rate 0.000101006
2017-10-10T14:50:08.729589: step 2078, loss 0.250829, acc 0.90625, learning_rate 0.000101002
2017-10-10T14:50:09.082545: step 2079, loss 0.0884783, acc 0.953125, learning_rate 0.000100998
2017-10-10T14:50:09.434897: step 2080, loss 0.166061, acc 0.9375, learning_rate 0.000100994

Evaluation:
2017-10-10T14:50:10.201895: step 2080, loss 0.214358, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2080

2017-10-10T14:50:11.524861: step 2081, loss 0.123073, acc 0.953125, learning_rate 0.00010099
2017-10-10T14:50:11.875457: step 2082, loss 0.167038, acc 0.921875, learning_rate 0.000100986
2017-10-10T14:50:12.256454: step 2083, loss 0.237132, acc 0.9375, learning_rate 0.000100982
2017-10-10T14:50:12.633102: step 2084, loss 0.0820216, acc 0.984375, learning_rate 0.000100978
2017-10-10T14:50:12.952947: step 2085, loss 0.118993, acc 0.96875, learning_rate 0.000100974
2017-10-10T14:50:13.332804: step 2086, loss 0.087274, acc 0.984375, learning_rate 0.00010097
2017-10-10T14:50:13.636995: step 2087, loss 0.104426, acc 0.96875, learning_rate 0.000100966
2017-10-10T14:50:14.029004: step 2088, loss 0.191677, acc 0.953125, learning_rate 0.000100962
2017-10-10T14:50:14.409277: step 2089, loss 0.104228, acc 0.96875, learning_rate 0.000100958
2017-10-10T14:50:14.757670: step 2090, loss 0.114948, acc 0.96875, learning_rate 0.000100954
2017-10-10T14:50:15.148087: step 2091, loss 0.127134, acc 0.96875, learning_rate 0.00010095
2017-10-10T14:50:15.579149: step 2092, loss 0.091429, acc 0.96875, learning_rate 0.000100946
2017-10-10T14:50:15.964839: step 2093, loss 0.130196, acc 0.96875, learning_rate 0.000100942
2017-10-10T14:50:16.368865: step 2094, loss 0.105554, acc 1, learning_rate 0.000100938
2017-10-10T14:50:16.768923: step 2095, loss 0.14663, acc 0.953125, learning_rate 0.000100935
2017-10-10T14:50:17.149006: step 2096, loss 0.104141, acc 0.96875, learning_rate 0.000100931
2017-10-10T14:50:17.514906: step 2097, loss 0.0484371, acc 1, learning_rate 0.000100927
2017-10-10T14:50:17.860864: step 2098, loss 0.184695, acc 0.921875, learning_rate 0.000100923
2017-10-10T14:50:18.254592: step 2099, loss 0.111827, acc 0.96875, learning_rate 0.000100919
2017-10-10T14:50:18.640093: step 2100, loss 0.176053, acc 0.921875, learning_rate 0.000100916
2017-10-10T14:50:19.055912: step 2101, loss 0.0772353, acc 0.96875, learning_rate 0.000100912
2017-10-10T14:50:19.434191: step 2102, loss 0.0684307, acc 1, learning_rate 0.000100908
2017-10-10T14:50:19.792080: step 2103, loss 0.121726, acc 0.96875, learning_rate 0.000100904
2017-10-10T14:50:20.159899: step 2104, loss 0.151483, acc 0.953125, learning_rate 0.000100901
2017-10-10T14:50:20.495011: step 2105, loss 0.190753, acc 0.921875, learning_rate 0.000100897
2017-10-10T14:50:20.852917: step 2106, loss 0.104903, acc 0.96875, learning_rate 0.000100893
2017-10-10T14:50:21.256279: step 2107, loss 0.129944, acc 0.953125, learning_rate 0.00010089
2017-10-10T14:50:21.683900: step 2108, loss 0.159242, acc 0.96875, learning_rate 0.000100886
2017-10-10T14:50:22.108882: step 2109, loss 0.126664, acc 0.96875, learning_rate 0.000100883
2017-10-10T14:50:22.528634: step 2110, loss 0.105978, acc 0.96875, learning_rate 0.000100879
2017-10-10T14:50:22.827464: step 2111, loss 0.0957126, acc 0.984375, learning_rate 0.000100875
2017-10-10T14:50:23.186254: step 2112, loss 0.192988, acc 0.921875, learning_rate 0.000100872
2017-10-10T14:50:23.508553: step 2113, loss 0.0894509, acc 0.96875, learning_rate 0.000100868
2017-10-10T14:50:23.928916: step 2114, loss 0.145671, acc 0.9375, learning_rate 0.000100865
2017-10-10T14:50:24.274373: step 2115, loss 0.117318, acc 0.953125, learning_rate 0.000100861
2017-10-10T14:50:24.713195: step 2116, loss 0.0615796, acc 1, learning_rate 0.000100858
2017-10-10T14:50:25.104845: step 2117, loss 0.121412, acc 0.953125, learning_rate 0.000100854
2017-10-10T14:50:25.397134: step 2118, loss 0.146599, acc 0.9375, learning_rate 0.000100851
2017-10-10T14:50:25.757648: step 2119, loss 0.16713, acc 0.921875, learning_rate 0.000100847
2017-10-10T14:50:26.125150: step 2120, loss 0.0976537, acc 0.96875, learning_rate 0.000100844

Evaluation:
2017-10-10T14:50:26.823881: step 2120, loss 0.210858, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2120

2017-10-10T14:50:28.064786: step 2121, loss 0.0512815, acc 1, learning_rate 0.00010084
2017-10-10T14:50:28.398046: step 2122, loss 0.23704, acc 0.9375, learning_rate 0.000100837
2017-10-10T14:50:28.777881: step 2123, loss 0.0823615, acc 0.9375, learning_rate 0.000100833
2017-10-10T14:50:29.130202: step 2124, loss 0.0946864, acc 0.96875, learning_rate 0.00010083
2017-10-10T14:50:29.521115: step 2125, loss 0.0720286, acc 0.984375, learning_rate 0.000100827
2017-10-10T14:50:29.878062: step 2126, loss 0.187802, acc 0.921875, learning_rate 0.000100823
2017-10-10T14:50:30.256960: step 2127, loss 0.0752128, acc 0.984375, learning_rate 0.00010082
2017-10-10T14:50:30.608867: step 2128, loss 0.0572762, acc 0.96875, learning_rate 0.000100817
2017-10-10T14:50:30.999934: step 2129, loss 0.192983, acc 0.921875, learning_rate 0.000100813
2017-10-10T14:50:31.384910: step 2130, loss 0.136663, acc 0.9375, learning_rate 0.00010081
2017-10-10T14:50:31.744912: step 2131, loss 0.0809004, acc 0.984375, learning_rate 0.000100807
2017-10-10T14:50:32.115266: step 2132, loss 0.206367, acc 0.921875, learning_rate 0.000100803
2017-10-10T14:50:32.480937: step 2133, loss 0.0739062, acc 0.984375, learning_rate 0.0001008
2017-10-10T14:50:32.844999: step 2134, loss 0.0791809, acc 0.984375, learning_rate 0.000100797
2017-10-10T14:50:33.217026: step 2135, loss 0.13065, acc 0.984375, learning_rate 0.000100793
2017-10-10T14:50:33.560825: step 2136, loss 0.131279, acc 0.9375, learning_rate 0.00010079
2017-10-10T14:50:33.983805: step 2137, loss 0.119579, acc 0.953125, learning_rate 0.000100787
2017-10-10T14:50:34.345438: step 2138, loss 0.117755, acc 0.96875, learning_rate 0.000100784
2017-10-10T14:50:34.736849: step 2139, loss 0.105831, acc 0.953125, learning_rate 0.000100781
2017-10-10T14:50:35.105819: step 2140, loss 0.123687, acc 0.953125, learning_rate 0.000100777
2017-10-10T14:50:35.520939: step 2141, loss 0.0954177, acc 0.953125, learning_rate 0.000100774
2017-10-10T14:50:35.948873: step 2142, loss 0.0821824, acc 0.984375, learning_rate 0.000100771
2017-10-10T14:50:36.392920: step 2143, loss 0.0926551, acc 0.96875, learning_rate 0.000100768
2017-10-10T14:50:36.748510: step 2144, loss 0.107207, acc 0.953125, learning_rate 0.000100765
2017-10-10T14:50:37.222595: step 2145, loss 0.0829007, acc 0.96875, learning_rate 0.000100762
2017-10-10T14:50:37.581581: step 2146, loss 0.139359, acc 0.9375, learning_rate 0.000100759
2017-10-10T14:50:37.989238: step 2147, loss 0.0538604, acc 0.984375, learning_rate 0.000100755
2017-10-10T14:50:38.371697: step 2148, loss 0.0953989, acc 0.96875, learning_rate 0.000100752
2017-10-10T14:50:38.755537: step 2149, loss 0.227898, acc 0.90625, learning_rate 0.000100749
2017-10-10T14:50:39.155585: step 2150, loss 0.110401, acc 0.96875, learning_rate 0.000100746
2017-10-10T14:50:39.641408: step 2151, loss 0.141234, acc 0.921875, learning_rate 0.000100743
2017-10-10T14:50:39.994131: step 2152, loss 0.212506, acc 0.9375, learning_rate 0.00010074
2017-10-10T14:50:40.258464: step 2153, loss 0.0998315, acc 0.984375, learning_rate 0.000100737
2017-10-10T14:50:40.689084: step 2154, loss 0.166036, acc 0.953125, learning_rate 0.000100734
2017-10-10T14:50:41.116469: step 2155, loss 0.143672, acc 0.9375, learning_rate 0.000100731
2017-10-10T14:50:41.325785: step 2156, loss 0.113867, acc 0.960784, learning_rate 0.000100728
2017-10-10T14:50:41.656814: step 2157, loss 0.0462548, acc 1, learning_rate 0.000100725
2017-10-10T14:50:42.036887: step 2158, loss 0.0657435, acc 1, learning_rate 0.000100722
2017-10-10T14:50:42.390099: step 2159, loss 0.147722, acc 0.9375, learning_rate 0.000100719
2017-10-10T14:50:42.783816: step 2160, loss 0.160216, acc 0.90625, learning_rate 0.000100716

Evaluation:
2017-10-10T14:50:43.540310: step 2160, loss 0.208762, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2160

2017-10-10T14:50:44.859163: step 2161, loss 0.0789565, acc 0.96875, learning_rate 0.000100713
2017-10-10T14:50:45.282345: step 2162, loss 0.123978, acc 0.96875, learning_rate 0.000100711
2017-10-10T14:50:45.605057: step 2163, loss 0.0675839, acc 0.953125, learning_rate 0.000100708
2017-10-10T14:50:46.040858: step 2164, loss 0.245047, acc 0.90625, learning_rate 0.000100705
2017-10-10T14:50:46.360865: step 2165, loss 0.151685, acc 0.9375, learning_rate 0.000100702
2017-10-10T14:50:46.658842: step 2166, loss 0.140495, acc 0.9375, learning_rate 0.000100699
2017-10-10T14:50:47.025051: step 2167, loss 0.12061, acc 0.96875, learning_rate 0.000100696
2017-10-10T14:50:47.377173: step 2168, loss 0.160198, acc 0.96875, learning_rate 0.000100693
2017-10-10T14:50:47.751584: step 2169, loss 0.0963665, acc 0.953125, learning_rate 0.00010069
2017-10-10T14:50:48.175715: step 2170, loss 0.108171, acc 0.96875, learning_rate 0.000100688
2017-10-10T14:50:48.545263: step 2171, loss 0.10563, acc 0.953125, learning_rate 0.000100685
2017-10-10T14:50:48.927174: step 2172, loss 0.136061, acc 0.9375, learning_rate 0.000100682
2017-10-10T14:50:49.388217: step 2173, loss 0.158916, acc 0.96875, learning_rate 0.000100679
2017-10-10T14:50:49.781528: step 2174, loss 0.190425, acc 0.90625, learning_rate 0.000100677
2017-10-10T14:50:50.125150: step 2175, loss 0.120606, acc 0.96875, learning_rate 0.000100674
2017-10-10T14:50:50.515966: step 2176, loss 0.082094, acc 0.984375, learning_rate 0.000100671
2017-10-10T14:50:50.940983: step 2177, loss 0.0409636, acc 0.984375, learning_rate 0.000100668
2017-10-10T14:50:51.319246: step 2178, loss 0.13452, acc 0.96875, learning_rate 0.000100666
2017-10-10T14:50:51.711940: step 2179, loss 0.133297, acc 0.96875, learning_rate 0.000100663
2017-10-10T14:50:52.132856: step 2180, loss 0.140349, acc 0.953125, learning_rate 0.00010066
2017-10-10T14:50:52.500957: step 2181, loss 0.363279, acc 0.859375, learning_rate 0.000100657
2017-10-10T14:50:52.896034: step 2182, loss 0.180595, acc 0.9375, learning_rate 0.000100655
2017-10-10T14:50:53.287099: step 2183, loss 0.105121, acc 0.9375, learning_rate 0.000100652
2017-10-10T14:50:53.619678: step 2184, loss 0.191028, acc 0.9375, learning_rate 0.000100649
2017-10-10T14:50:54.017027: step 2185, loss 0.0341045, acc 1, learning_rate 0.000100647
2017-10-10T14:50:54.424451: step 2186, loss 0.138563, acc 0.953125, learning_rate 0.000100644
2017-10-10T14:50:54.813160: step 2187, loss 0.130524, acc 0.953125, learning_rate 0.000100641
2017-10-10T14:50:55.152846: step 2188, loss 0.217019, acc 0.875, learning_rate 0.000100639
2017-10-10T14:50:55.497084: step 2189, loss 0.114888, acc 0.953125, learning_rate 0.000100636
2017-10-10T14:50:55.916663: step 2190, loss 0.0360824, acc 1, learning_rate 0.000100634
2017-10-10T14:50:56.285981: step 2191, loss 0.0778248, acc 0.984375, learning_rate 0.000100631
2017-10-10T14:50:56.732933: step 2192, loss 0.150882, acc 0.9375, learning_rate 0.000100628
2017-10-10T14:50:57.105194: step 2193, loss 0.25217, acc 0.90625, learning_rate 0.000100626
2017-10-10T14:50:57.490579: step 2194, loss 0.083378, acc 0.984375, learning_rate 0.000100623
2017-10-10T14:50:57.834991: step 2195, loss 0.060603, acc 1, learning_rate 0.000100621
2017-10-10T14:50:58.198718: step 2196, loss 0.137434, acc 0.953125, learning_rate 0.000100618
2017-10-10T14:50:58.625131: step 2197, loss 0.0640354, acc 0.984375, learning_rate 0.000100616
2017-10-10T14:50:59.020851: step 2198, loss 0.0853236, acc 0.96875, learning_rate 0.000100613
2017-10-10T14:50:59.304836: step 2199, loss 0.0782099, acc 0.96875, learning_rate 0.000100611
2017-10-10T14:50:59.613720: step 2200, loss 0.189712, acc 0.921875, learning_rate 0.000100608

Evaluation:
2017-10-10T14:51:00.266484: step 2200, loss 0.210083, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2200

2017-10-10T14:51:01.617168: step 2201, loss 0.120113, acc 0.953125, learning_rate 0.000100606
2017-10-10T14:51:01.961390: step 2202, loss 0.197531, acc 0.9375, learning_rate 0.000100603
2017-10-10T14:51:02.299741: step 2203, loss 0.0746506, acc 0.984375, learning_rate 0.000100601
2017-10-10T14:51:02.626275: step 2204, loss 0.0840188, acc 0.984375, learning_rate 0.000100598
2017-10-10T14:51:02.980155: step 2205, loss 0.103372, acc 0.96875, learning_rate 0.000100596
2017-10-10T14:51:03.325053: step 2206, loss 0.191208, acc 0.953125, learning_rate 0.000100594
2017-10-10T14:51:03.764693: step 2207, loss 0.164861, acc 0.953125, learning_rate 0.000100591
2017-10-10T14:51:04.134631: step 2208, loss 0.0962113, acc 0.984375, learning_rate 0.000100589
2017-10-10T14:51:04.494339: step 2209, loss 0.120101, acc 0.953125, learning_rate 0.000100586
2017-10-10T14:51:04.900891: step 2210, loss 0.0804396, acc 0.96875, learning_rate 0.000100584
2017-10-10T14:51:05.281333: step 2211, loss 0.103521, acc 0.9375, learning_rate 0.000100581
2017-10-10T14:51:05.647690: step 2212, loss 0.0869034, acc 0.96875, learning_rate 0.000100579
2017-10-10T14:51:06.056336: step 2213, loss 0.117681, acc 0.953125, learning_rate 0.000100577
2017-10-10T14:51:06.378398: step 2214, loss 0.105623, acc 0.9375, learning_rate 0.000100574
2017-10-10T14:51:06.687325: step 2215, loss 0.109258, acc 0.984375, learning_rate 0.000100572
2017-10-10T14:51:07.160848: step 2216, loss 0.133457, acc 0.953125, learning_rate 0.00010057
2017-10-10T14:51:07.510696: step 2217, loss 0.0855653, acc 0.953125, learning_rate 0.000100567
2017-10-10T14:51:07.882165: step 2218, loss 0.15378, acc 0.9375, learning_rate 0.000100565
2017-10-10T14:51:08.212854: step 2219, loss 0.0910819, acc 0.984375, learning_rate 0.000100563
2017-10-10T14:51:08.613589: step 2220, loss 0.0594802, acc 1, learning_rate 0.00010056
2017-10-10T14:51:08.970627: step 2221, loss 0.113618, acc 0.953125, learning_rate 0.000100558
2017-10-10T14:51:09.339063: step 2222, loss 0.127899, acc 0.9375, learning_rate 0.000100556
2017-10-10T14:51:09.686684: step 2223, loss 0.143357, acc 0.953125, learning_rate 0.000100554
2017-10-10T14:51:10.021344: step 2224, loss 0.0857921, acc 0.984375, learning_rate 0.000100551
2017-10-10T14:51:10.360937: step 2225, loss 0.100106, acc 0.953125, learning_rate 0.000100549
2017-10-10T14:51:10.771176: step 2226, loss 0.104011, acc 0.96875, learning_rate 0.000100547
2017-10-10T14:51:11.181095: step 2227, loss 0.116284, acc 0.953125, learning_rate 0.000100545
2017-10-10T14:51:11.544833: step 2228, loss 0.0817324, acc 0.96875, learning_rate 0.000100542
2017-10-10T14:51:11.941476: step 2229, loss 0.105132, acc 0.953125, learning_rate 0.00010054
2017-10-10T14:51:12.253560: step 2230, loss 0.160054, acc 0.953125, learning_rate 0.000100538
2017-10-10T14:51:12.626773: step 2231, loss 0.121684, acc 0.953125, learning_rate 0.000100536
2017-10-10T14:51:13.000019: step 2232, loss 0.0938909, acc 0.953125, learning_rate 0.000100534
2017-10-10T14:51:13.344849: step 2233, loss 0.119784, acc 0.96875, learning_rate 0.000100531
2017-10-10T14:51:13.749316: step 2234, loss 0.296109, acc 0.921875, learning_rate 0.000100529
2017-10-10T14:51:14.208853: step 2235, loss 0.13221, acc 0.9375, learning_rate 0.000100527
2017-10-10T14:51:14.544916: step 2236, loss 0.168437, acc 0.921875, learning_rate 0.000100525
2017-10-10T14:51:14.871047: step 2237, loss 0.114507, acc 0.953125, learning_rate 0.000100523
2017-10-10T14:51:15.316712: step 2238, loss 0.188373, acc 0.921875, learning_rate 0.000100521
2017-10-10T14:51:15.636886: step 2239, loss 0.115303, acc 0.96875, learning_rate 0.000100519
2017-10-10T14:51:16.023454: step 2240, loss 0.119571, acc 0.953125, learning_rate 0.000100516

Evaluation:
2017-10-10T14:52:55.974313: step 2240, loss 0.210378, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2240

2017-10-10T14:57:29.208115: step 2241, loss 0.172494, acc 0.921875, learning_rate 0.000100514
2017-10-10T14:57:35.701062: step 2242, loss 0.0955189, acc 0.984375, learning_rate 0.000100512
2017-10-10T14:57:39.082254: step 2243, loss 0.200069, acc 0.9375, learning_rate 0.00010051
2017-10-10T14:57:41.613242: step 2244, loss 0.0901381, acc 0.96875, learning_rate 0.000100508
2017-10-10T14:57:43.433249: step 2245, loss 0.0931049, acc 0.96875, learning_rate 0.000100506
2017-10-10T14:57:44.257408: step 2246, loss 0.134961, acc 0.953125, learning_rate 0.000100504
2017-10-10T14:57:46.062712: step 2247, loss 0.0872484, acc 0.96875, learning_rate 0.000100502
2017-10-10T14:57:47.284102: step 2248, loss 0.131433, acc 0.96875, learning_rate 0.0001005
2017-10-10T14:57:48.727404: step 2249, loss 0.113022, acc 0.984375, learning_rate 0.000100498
2017-10-10T14:57:50.483572: step 2250, loss 0.124883, acc 0.96875, learning_rate 0.000100496
2017-10-10T14:57:51.993334: step 2251, loss 0.0993981, acc 0.984375, learning_rate 0.000100494
2017-10-10T14:57:53.843667: step 2252, loss 0.134318, acc 0.953125, learning_rate 0.000100492
2017-10-10T14:57:55.796911: step 2253, loss 0.0844383, acc 0.96875, learning_rate 0.00010049
2017-10-10T14:57:56.253281: step 2254, loss 0.147628, acc 0.980392, learning_rate 0.000100488
2017-10-10T14:57:57.117104: step 2255, loss 0.189143, acc 0.9375, learning_rate 0.000100486
2017-10-10T14:57:58.047403: step 2256, loss 0.162207, acc 0.953125, learning_rate 0.000100484
2017-10-10T14:57:58.779060: step 2257, loss 0.171645, acc 0.9375, learning_rate 0.000100482
2017-10-10T14:57:59.940850: step 2258, loss 0.121627, acc 0.9375, learning_rate 0.00010048
2017-10-10T14:58:00.843491: step 2259, loss 0.0828905, acc 0.984375, learning_rate 0.000100478
2017-10-10T14:58:01.805565: step 2260, loss 0.0392826, acc 1, learning_rate 0.000100476
2017-10-10T14:58:02.260526: step 2261, loss 0.0262933, acc 1, learning_rate 0.000100474
2017-10-10T14:58:02.856921: step 2262, loss 0.152666, acc 0.953125, learning_rate 0.000100472
2017-10-10T14:58:03.469575: step 2263, loss 0.0645985, acc 1, learning_rate 0.00010047
2017-10-10T14:58:03.869014: step 2264, loss 0.0673381, acc 0.984375, learning_rate 0.000100468
2017-10-10T14:58:04.897678: step 2265, loss 0.101052, acc 0.96875, learning_rate 0.000100466
2017-10-10T14:58:05.713007: step 2266, loss 0.12887, acc 0.953125, learning_rate 0.000100464
2017-10-10T14:58:06.321413: step 2267, loss 0.119683, acc 0.96875, learning_rate 0.000100462
2017-10-10T14:58:06.880704: step 2268, loss 0.117804, acc 0.9375, learning_rate 0.000100461
2017-10-10T14:58:07.579952: step 2269, loss 0.0887088, acc 0.984375, learning_rate 0.000100459
2017-10-10T14:58:08.037568: step 2270, loss 0.11162, acc 0.9375, learning_rate 0.000100457
2017-10-10T14:58:08.585617: step 2271, loss 0.117226, acc 0.953125, learning_rate 0.000100455
2017-10-10T14:58:09.123310: step 2272, loss 0.116561, acc 0.953125, learning_rate 0.000100453
2017-10-10T14:58:09.511668: step 2273, loss 0.138565, acc 0.953125, learning_rate 0.000100451
2017-10-10T14:58:10.119089: step 2274, loss 0.122033, acc 0.96875, learning_rate 0.000100449
2017-10-10T14:58:10.467223: step 2275, loss 0.0882451, acc 0.984375, learning_rate 0.000100448
2017-10-10T14:58:10.969535: step 2276, loss 0.130125, acc 0.953125, learning_rate 0.000100446
2017-10-10T14:58:11.407023: step 2277, loss 0.156623, acc 0.953125, learning_rate 0.000100444
2017-10-10T14:58:11.830692: step 2278, loss 0.146922, acc 0.9375, learning_rate 0.000100442
2017-10-10T14:58:12.741215: step 2279, loss 0.118691, acc 0.96875, learning_rate 0.00010044
2017-10-10T14:58:13.156233: step 2280, loss 0.153705, acc 0.9375, learning_rate 0.000100439

Evaluation:
2017-10-10T14:58:16.218965: step 2280, loss 0.21145, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2280

2017-10-10T14:58:17.523188: step 2281, loss 0.154707, acc 0.953125, learning_rate 0.000100437
2017-10-10T14:58:18.109664: step 2282, loss 0.101371, acc 0.96875, learning_rate 0.000100435
2017-10-10T14:58:18.532937: step 2283, loss 0.109867, acc 0.953125, learning_rate 0.000100433
2017-10-10T14:58:18.957109: step 2284, loss 0.0735998, acc 0.984375, learning_rate 0.000100431
2017-10-10T14:58:19.765332: step 2285, loss 0.157909, acc 0.9375, learning_rate 0.00010043
2017-10-10T14:58:20.363522: step 2286, loss 0.130477, acc 0.953125, learning_rate 0.000100428
2017-10-10T14:58:20.806276: step 2287, loss 0.124376, acc 0.953125, learning_rate 0.000100426
2017-10-10T14:58:21.180135: step 2288, loss 0.157831, acc 0.9375, learning_rate 0.000100424
2017-10-10T14:58:21.590489: step 2289, loss 0.172686, acc 0.921875, learning_rate 0.000100423
2017-10-10T14:58:22.048840: step 2290, loss 0.149128, acc 0.953125, learning_rate 0.000100421
2017-10-10T14:58:22.490501: step 2291, loss 0.124989, acc 0.96875, learning_rate 0.000100419
2017-10-10T14:58:22.917745: step 2292, loss 0.0458351, acc 1, learning_rate 0.000100418
2017-10-10T14:58:23.327699: step 2293, loss 0.207341, acc 0.921875, learning_rate 0.000100416
2017-10-10T14:58:23.740906: step 2294, loss 0.0568813, acc 0.984375, learning_rate 0.000100414
2017-10-10T14:58:24.089941: step 2295, loss 0.0620191, acc 0.984375, learning_rate 0.000100412
2017-10-10T14:58:24.438680: step 2296, loss 0.0646319, acc 0.984375, learning_rate 0.000100411
2017-10-10T14:58:24.992748: step 2297, loss 0.0756572, acc 0.984375, learning_rate 0.000100409
2017-10-10T14:58:25.424308: step 2298, loss 0.154665, acc 0.921875, learning_rate 0.000100407
2017-10-10T14:58:25.831661: step 2299, loss 0.187237, acc 0.921875, learning_rate 0.000100406
2017-10-10T14:58:26.286841: step 2300, loss 0.115796, acc 0.96875, learning_rate 0.000100404
2017-10-10T14:58:27.000790: step 2301, loss 0.182721, acc 0.96875, learning_rate 0.000100402
2017-10-10T14:58:28.027346: step 2302, loss 0.177594, acc 0.921875, learning_rate 0.000100401
2017-10-10T14:58:29.419170: step 2303, loss 0.0596157, acc 0.984375, learning_rate 0.000100399
2017-10-10T14:58:30.366199: step 2304, loss 0.118482, acc 0.96875, learning_rate 0.000100398
2017-10-10T14:58:31.157113: step 2305, loss 0.0803867, acc 0.96875, learning_rate 0.000100396
2017-10-10T14:58:31.453641: step 2306, loss 0.0970301, acc 0.96875, learning_rate 0.000100394
2017-10-10T14:58:32.197135: step 2307, loss 0.131975, acc 0.953125, learning_rate 0.000100393
2017-10-10T14:58:32.625443: step 2308, loss 0.113033, acc 0.96875, learning_rate 0.000100391
2017-10-10T14:58:33.012845: step 2309, loss 0.140171, acc 0.96875, learning_rate 0.000100389
2017-10-10T14:58:33.461001: step 2310, loss 0.189229, acc 0.921875, learning_rate 0.000100388
2017-10-10T14:58:33.908282: step 2311, loss 0.0313529, acc 0.984375, learning_rate 0.000100386
2017-10-10T14:58:34.397080: step 2312, loss 0.0234323, acc 1, learning_rate 0.000100385
2017-10-10T14:58:34.783241: step 2313, loss 0.0932021, acc 1, learning_rate 0.000100383
2017-10-10T14:58:35.504181: step 2314, loss 0.0973797, acc 0.984375, learning_rate 0.000100382
2017-10-10T14:58:35.912927: step 2315, loss 0.14604, acc 0.953125, learning_rate 0.00010038
2017-10-10T14:58:37.015835: step 2316, loss 0.168654, acc 0.921875, learning_rate 0.000100378
2017-10-10T14:58:37.339767: step 2317, loss 0.0626864, acc 0.984375, learning_rate 0.000100377
2017-10-10T14:58:37.680916: step 2318, loss 0.141797, acc 0.921875, learning_rate 0.000100375
2017-10-10T14:58:38.126560: step 2319, loss 0.151126, acc 0.953125, learning_rate 0.000100374
2017-10-10T14:58:38.496836: step 2320, loss 0.100098, acc 0.953125, learning_rate 0.000100372

Evaluation:
2017-10-10T14:58:43.684680: step 2320, loss 0.209745, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2320

2017-10-10T14:58:45.264685: step 2321, loss 0.0969025, acc 0.96875, learning_rate 0.000100371
2017-10-10T14:58:45.664918: step 2322, loss 0.109593, acc 0.953125, learning_rate 0.000100369
2017-10-10T14:58:46.210932: step 2323, loss 0.111535, acc 0.984375, learning_rate 0.000100368
2017-10-10T14:58:46.552829: step 2324, loss 0.102967, acc 0.96875, learning_rate 0.000100366
2017-10-10T14:58:46.862266: step 2325, loss 0.132699, acc 0.953125, learning_rate 0.000100365
2017-10-10T14:58:47.268843: step 2326, loss 0.0669671, acc 0.96875, learning_rate 0.000100363
2017-10-10T14:58:47.641187: step 2327, loss 0.123709, acc 0.96875, learning_rate 0.000100362
2017-10-10T14:58:48.068854: step 2328, loss 0.157549, acc 0.96875, learning_rate 0.00010036
2017-10-10T14:58:48.420808: step 2329, loss 0.129347, acc 0.96875, learning_rate 0.000100359
2017-10-10T14:58:48.736836: step 2330, loss 0.1289, acc 0.953125, learning_rate 0.000100357
2017-10-10T14:58:49.102183: step 2331, loss 0.0636948, acc 0.984375, learning_rate 0.000100356
2017-10-10T14:58:49.449154: step 2332, loss 0.0494812, acc 0.984375, learning_rate 0.000100354
2017-10-10T14:58:49.900702: step 2333, loss 0.138957, acc 0.9375, learning_rate 0.000100353
2017-10-10T14:58:50.369678: step 2334, loss 0.250828, acc 0.921875, learning_rate 0.000100352
2017-10-10T14:58:50.772881: step 2335, loss 0.128846, acc 0.9375, learning_rate 0.00010035
2017-10-10T14:58:51.132035: step 2336, loss 0.225389, acc 0.9375, learning_rate 0.000100349
2017-10-10T14:58:51.494378: step 2337, loss 0.204927, acc 0.953125, learning_rate 0.000100347
2017-10-10T14:58:51.853117: step 2338, loss 0.14475, acc 0.953125, learning_rate 0.000100346
2017-10-10T14:58:52.250881: step 2339, loss 0.168864, acc 0.9375, learning_rate 0.000100344
2017-10-10T14:58:52.632957: step 2340, loss 0.126476, acc 0.96875, learning_rate 0.000100343
2017-10-10T14:58:53.048904: step 2341, loss 0.0271884, acc 1, learning_rate 0.000100342
2017-10-10T14:58:53.425123: step 2342, loss 0.0739778, acc 0.984375, learning_rate 0.00010034
2017-10-10T14:58:53.832239: step 2343, loss 0.143889, acc 0.953125, learning_rate 0.000100339
2017-10-10T14:58:54.203052: step 2344, loss 0.151557, acc 0.953125, learning_rate 0.000100338
2017-10-10T14:58:54.583069: step 2345, loss 0.17265, acc 0.9375, learning_rate 0.000100336
2017-10-10T14:58:54.908095: step 2346, loss 0.183625, acc 0.921875, learning_rate 0.000100335
2017-10-10T14:58:55.348821: step 2347, loss 0.0925077, acc 1, learning_rate 0.000100333
2017-10-10T14:58:55.646667: step 2348, loss 0.0945115, acc 0.9375, learning_rate 0.000100332
2017-10-10T14:58:55.903049: step 2349, loss 0.23197, acc 0.9375, learning_rate 0.000100331
2017-10-10T14:58:56.277024: step 2350, loss 0.095823, acc 0.984375, learning_rate 0.000100329
2017-10-10T14:58:56.704844: step 2351, loss 0.14708, acc 0.953125, learning_rate 0.000100328
2017-10-10T14:58:57.017160: step 2352, loss 0.0557706, acc 0.980392, learning_rate 0.000100327
2017-10-10T14:58:57.403845: step 2353, loss 0.0612348, acc 0.96875, learning_rate 0.000100325
2017-10-10T14:58:57.729495: step 2354, loss 0.195307, acc 0.953125, learning_rate 0.000100324
2017-10-10T14:58:58.153033: step 2355, loss 0.143404, acc 0.953125, learning_rate 0.000100323
2017-10-10T14:58:58.592464: step 2356, loss 0.073108, acc 0.984375, learning_rate 0.000100321
2017-10-10T14:58:58.998561: step 2357, loss 0.0612353, acc 1, learning_rate 0.00010032
2017-10-10T14:58:59.348639: step 2358, loss 0.1303, acc 0.953125, learning_rate 0.000100319
2017-10-10T14:58:59.788902: step 2359, loss 0.159516, acc 0.953125, learning_rate 0.000100317
2017-10-10T14:59:00.170456: step 2360, loss 0.0431634, acc 0.96875, learning_rate 0.000100316

Evaluation:
2017-10-10T14:59:00.792602: step 2360, loss 0.209762, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2360

2017-10-10T14:59:02.068918: step 2361, loss 0.12452, acc 0.96875, learning_rate 0.000100315
2017-10-10T14:59:02.447171: step 2362, loss 0.205795, acc 0.9375, learning_rate 0.000100314
2017-10-10T14:59:02.829003: step 2363, loss 0.181884, acc 0.953125, learning_rate 0.000100312
2017-10-10T14:59:03.228172: step 2364, loss 0.179763, acc 0.890625, learning_rate 0.000100311
2017-10-10T14:59:03.556222: step 2365, loss 0.12534, acc 0.953125, learning_rate 0.00010031
2017-10-10T14:59:03.848702: step 2366, loss 0.0909448, acc 0.984375, learning_rate 0.000100308
2017-10-10T14:59:04.324937: step 2367, loss 0.116999, acc 0.96875, learning_rate 0.000100307
2017-10-10T14:59:04.644996: step 2368, loss 0.125303, acc 0.9375, learning_rate 0.000100306
2017-10-10T14:59:04.954760: step 2369, loss 0.0697539, acc 0.96875, learning_rate 0.000100305
2017-10-10T14:59:05.847279: step 2370, loss 0.0727739, acc 0.984375, learning_rate 0.000100303
2017-10-10T14:59:06.229409: step 2371, loss 0.104602, acc 0.96875, learning_rate 0.000100302
2017-10-10T14:59:06.576489: step 2372, loss 0.03779, acc 0.984375, learning_rate 0.000100301
2017-10-10T14:59:06.900955: step 2373, loss 0.118515, acc 0.953125, learning_rate 0.0001003
2017-10-10T14:59:07.259374: step 2374, loss 0.135565, acc 0.953125, learning_rate 0.000100299
2017-10-10T14:59:07.639904: step 2375, loss 0.152, acc 0.953125, learning_rate 0.000100297
2017-10-10T14:59:07.982625: step 2376, loss 0.0461251, acc 1, learning_rate 0.000100296
2017-10-10T14:59:08.396925: step 2377, loss 0.166461, acc 0.921875, learning_rate 0.000100295
2017-10-10T14:59:08.761302: step 2378, loss 0.154844, acc 0.9375, learning_rate 0.000100294
2017-10-10T14:59:09.149633: step 2379, loss 0.141248, acc 0.9375, learning_rate 0.000100292
2017-10-10T14:59:09.494025: step 2380, loss 0.0993876, acc 0.96875, learning_rate 0.000100291
2017-10-10T14:59:09.850684: step 2381, loss 0.0989124, acc 0.96875, learning_rate 0.00010029
2017-10-10T14:59:10.221193: step 2382, loss 0.0606735, acc 0.96875, learning_rate 0.000100289
2017-10-10T14:59:10.587551: step 2383, loss 0.158826, acc 0.9375, learning_rate 0.000100288
2017-10-10T14:59:10.980611: step 2384, loss 0.110758, acc 0.953125, learning_rate 0.000100287
2017-10-10T14:59:11.345039: step 2385, loss 0.122272, acc 0.953125, learning_rate 0.000100285
2017-10-10T14:59:11.729550: step 2386, loss 0.0886566, acc 0.96875, learning_rate 0.000100284
2017-10-10T14:59:12.136907: step 2387, loss 0.197417, acc 0.921875, learning_rate 0.000100283
2017-10-10T14:59:12.510200: step 2388, loss 0.0695193, acc 0.96875, learning_rate 0.000100282
2017-10-10T14:59:12.882979: step 2389, loss 0.158921, acc 0.953125, learning_rate 0.000100281
2017-10-10T14:59:13.215935: step 2390, loss 0.153823, acc 0.953125, learning_rate 0.00010028
2017-10-10T14:59:13.578538: step 2391, loss 0.0740115, acc 0.96875, learning_rate 0.000100278
2017-10-10T14:59:13.912836: step 2392, loss 0.0600399, acc 1, learning_rate 0.000100277
2017-10-10T14:59:14.289941: step 2393, loss 0.0905281, acc 0.96875, learning_rate 0.000100276
2017-10-10T14:59:14.628211: step 2394, loss 0.112013, acc 0.96875, learning_rate 0.000100275
2017-10-10T14:59:15.008939: step 2395, loss 0.100223, acc 0.984375, learning_rate 0.000100274
2017-10-10T14:59:15.354444: step 2396, loss 0.17879, acc 0.953125, learning_rate 0.000100273
2017-10-10T14:59:15.724187: step 2397, loss 0.127641, acc 0.96875, learning_rate 0.000100272
2017-10-10T14:59:16.096222: step 2398, loss 0.151916, acc 0.9375, learning_rate 0.000100271
2017-10-10T14:59:16.480468: step 2399, loss 0.0801951, acc 0.984375, learning_rate 0.00010027
2017-10-10T14:59:16.885673: step 2400, loss 0.0843439, acc 0.984375, learning_rate 0.000100268

Evaluation:
2017-10-10T14:59:17.633247: step 2400, loss 0.210109, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2400

2017-10-10T14:59:18.871773: step 2401, loss 0.124201, acc 0.953125, learning_rate 0.000100267
2017-10-10T14:59:19.281165: step 2402, loss 0.0873359, acc 0.96875, learning_rate 0.000100266
2017-10-10T14:59:19.627368: step 2403, loss 0.0620113, acc 0.984375, learning_rate 0.000100265
2017-10-10T14:59:19.996824: step 2404, loss 0.153557, acc 0.953125, learning_rate 0.000100264
2017-10-10T14:59:20.286789: step 2405, loss 0.065949, acc 0.96875, learning_rate 0.000100263
2017-10-10T14:59:20.697163: step 2406, loss 0.196822, acc 0.90625, learning_rate 0.000100262
2017-10-10T14:59:21.087244: step 2407, loss 0.104955, acc 0.9375, learning_rate 0.000100261
2017-10-10T14:59:21.476860: step 2408, loss 0.126049, acc 0.984375, learning_rate 0.00010026
2017-10-10T14:59:21.849675: step 2409, loss 0.0478276, acc 0.984375, learning_rate 0.000100259
2017-10-10T14:59:22.276132: step 2410, loss 0.143864, acc 0.9375, learning_rate 0.000100258
2017-10-10T14:59:22.681077: step 2411, loss 0.227282, acc 0.90625, learning_rate 0.000100257
2017-10-10T14:59:23.157101: step 2412, loss 0.164459, acc 0.953125, learning_rate 0.000100256
2017-10-10T14:59:23.506616: step 2413, loss 0.197348, acc 0.953125, learning_rate 0.000100255
2017-10-10T14:59:23.849313: step 2414, loss 0.0991943, acc 0.96875, learning_rate 0.000100253
2017-10-10T14:59:24.178440: step 2415, loss 0.08222, acc 0.96875, learning_rate 0.000100252
2017-10-10T14:59:24.558679: step 2416, loss 0.165647, acc 0.921875, learning_rate 0.000100251
2017-10-10T14:59:24.923413: step 2417, loss 0.141292, acc 0.953125, learning_rate 0.00010025
2017-10-10T14:59:25.319036: step 2418, loss 0.0502888, acc 0.984375, learning_rate 0.000100249
2017-10-10T14:59:25.688131: step 2419, loss 0.167343, acc 0.953125, learning_rate 0.000100248
2017-10-10T14:59:26.056239: step 2420, loss 0.130662, acc 0.921875, learning_rate 0.000100247
2017-10-10T14:59:26.431133: step 2421, loss 0.039632, acc 0.984375, learning_rate 0.000100246
2017-10-10T14:59:26.804868: step 2422, loss 0.0710355, acc 1, learning_rate 0.000100245
2017-10-10T14:59:27.128164: step 2423, loss 0.239173, acc 0.921875, learning_rate 0.000100244
2017-10-10T14:59:27.533057: step 2424, loss 0.0973435, acc 0.96875, learning_rate 0.000100243
2017-10-10T14:59:27.904749: step 2425, loss 0.162973, acc 0.9375, learning_rate 0.000100242
2017-10-10T14:59:28.326003: step 2426, loss 0.0439683, acc 1, learning_rate 0.000100241
2017-10-10T14:59:28.690513: step 2427, loss 0.179989, acc 0.9375, learning_rate 0.00010024
2017-10-10T14:59:29.097057: step 2428, loss 0.120485, acc 0.96875, learning_rate 0.000100239
2017-10-10T14:59:29.489798: step 2429, loss 0.150025, acc 0.96875, learning_rate 0.000100238
2017-10-10T14:59:29.847912: step 2430, loss 0.0672039, acc 0.984375, learning_rate 0.000100237
2017-10-10T14:59:30.232021: step 2431, loss 0.0851256, acc 0.984375, learning_rate 0.000100236
2017-10-10T14:59:30.590042: step 2432, loss 0.0774864, acc 0.984375, learning_rate 0.000100235
2017-10-10T14:59:31.010678: step 2433, loss 0.0319148, acc 0.984375, learning_rate 0.000100235
2017-10-10T14:59:31.420451: step 2434, loss 0.0503998, acc 1, learning_rate 0.000100234
2017-10-10T14:59:31.785172: step 2435, loss 0.0726831, acc 0.984375, learning_rate 0.000100233
2017-10-10T14:59:32.175480: step 2436, loss 0.157148, acc 0.953125, learning_rate 0.000100232
2017-10-10T14:59:32.528800: step 2437, loss 0.135425, acc 0.9375, learning_rate 0.000100231
2017-10-10T14:59:32.984352: step 2438, loss 0.110702, acc 0.953125, learning_rate 0.00010023
2017-10-10T14:59:33.352825: step 2439, loss 0.10841, acc 0.96875, learning_rate 0.000100229
2017-10-10T14:59:34.288764: step 2440, loss 0.166737, acc 0.953125, learning_rate 0.000100228

Evaluation:
2017-10-10T14:59:34.983457: step 2440, loss 0.209626, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2440

2017-10-10T14:59:36.312402: step 2441, loss 0.153582, acc 0.953125, learning_rate 0.000100227
2017-10-10T14:59:36.700391: step 2442, loss 0.169008, acc 0.9375, learning_rate 0.000100226
2017-10-10T14:59:37.068821: step 2443, loss 0.0401892, acc 1, learning_rate 0.000100225
2017-10-10T14:59:37.453005: step 2444, loss 0.178469, acc 0.96875, learning_rate 0.000100224
2017-10-10T14:59:37.783564: step 2445, loss 0.242985, acc 0.90625, learning_rate 0.000100223
2017-10-10T14:59:38.161037: step 2446, loss 0.0751382, acc 0.984375, learning_rate 0.000100222
2017-10-10T14:59:38.576188: step 2447, loss 0.0864705, acc 0.984375, learning_rate 0.000100221
2017-10-10T14:59:38.955785: step 2448, loss 0.118191, acc 0.9375, learning_rate 0.000100221
2017-10-10T14:59:39.356912: step 2449, loss 0.18765, acc 0.9375, learning_rate 0.00010022
2017-10-10T14:59:39.756872: step 2450, loss 0.204501, acc 0.921569, learning_rate 0.000100219
2017-10-10T14:59:40.216827: step 2451, loss 0.134388, acc 0.96875, learning_rate 0.000100218
2017-10-10T14:59:40.599490: step 2452, loss 0.147766, acc 0.953125, learning_rate 0.000100217
2017-10-10T14:59:41.116569: step 2453, loss 0.170044, acc 0.9375, learning_rate 0.000100216
2017-10-10T14:59:41.459675: step 2454, loss 0.126326, acc 0.984375, learning_rate 0.000100215
2017-10-10T14:59:41.758495: step 2455, loss 0.151961, acc 0.9375, learning_rate 0.000100214
2017-10-10T14:59:42.132916: step 2456, loss 0.0546605, acc 0.984375, learning_rate 0.000100213
2017-10-10T14:59:42.545393: step 2457, loss 0.138514, acc 0.953125, learning_rate 0.000100213
2017-10-10T14:59:42.994520: step 2458, loss 0.0803473, acc 0.96875, learning_rate 0.000100212
2017-10-10T14:59:43.343293: step 2459, loss 0.0789482, acc 0.96875, learning_rate 0.000100211
2017-10-10T14:59:43.704875: step 2460, loss 0.162906, acc 0.9375, learning_rate 0.00010021
2017-10-10T14:59:44.108998: step 2461, loss 0.0790464, acc 0.953125, learning_rate 0.000100209
2017-10-10T14:59:44.465899: step 2462, loss 0.102538, acc 0.9375, learning_rate 0.000100208
2017-10-10T14:59:44.833365: step 2463, loss 0.0594641, acc 0.984375, learning_rate 0.000100207
2017-10-10T14:59:45.225053: step 2464, loss 0.144059, acc 0.96875, learning_rate 0.000100207
2017-10-10T14:59:45.646012: step 2465, loss 0.28431, acc 0.90625, learning_rate 0.000100206
2017-10-10T14:59:46.039416: step 2466, loss 0.128172, acc 0.953125, learning_rate 0.000100205
2017-10-10T14:59:46.437330: step 2467, loss 0.0515088, acc 1, learning_rate 0.000100204
2017-10-10T14:59:46.763930: step 2468, loss 0.0946039, acc 0.953125, learning_rate 0.000100203
2017-10-10T14:59:47.161020: step 2469, loss 0.107439, acc 0.984375, learning_rate 0.000100202
2017-10-10T14:59:47.567694: step 2470, loss 0.23371, acc 0.90625, learning_rate 0.000100202
2017-10-10T14:59:47.936126: step 2471, loss 0.0827736, acc 0.953125, learning_rate 0.000100201
2017-10-10T14:59:48.284876: step 2472, loss 0.0788166, acc 0.96875, learning_rate 0.0001002
2017-10-10T14:59:48.722409: step 2473, loss 0.129187, acc 0.96875, learning_rate 0.000100199
2017-10-10T14:59:49.045907: step 2474, loss 0.143602, acc 0.953125, learning_rate 0.000100198
2017-10-10T14:59:49.505977: step 2475, loss 0.184716, acc 0.921875, learning_rate 0.000100198
2017-10-10T14:59:49.831323: step 2476, loss 0.0500803, acc 1, learning_rate 0.000100197
2017-10-10T14:59:50.144839: step 2477, loss 0.104257, acc 0.953125, learning_rate 0.000100196
2017-10-10T14:59:50.561356: step 2478, loss 0.165499, acc 0.9375, learning_rate 0.000100195
2017-10-10T14:59:51.000856: step 2479, loss 0.0817263, acc 1, learning_rate 0.000100194
2017-10-10T14:59:51.345071: step 2480, loss 0.0727609, acc 0.984375, learning_rate 0.000100194

Evaluation:
2017-10-10T14:59:51.974887: step 2480, loss 0.207641, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2480

2017-10-10T14:59:53.318217: step 2481, loss 0.133172, acc 0.953125, learning_rate 0.000100193
2017-10-10T14:59:54.006459: step 2482, loss 0.12573, acc 0.953125, learning_rate 0.000100192
2017-10-10T14:59:54.334162: step 2483, loss 0.122216, acc 0.9375, learning_rate 0.000100191
2017-10-10T14:59:54.695703: step 2484, loss 0.117044, acc 0.953125, learning_rate 0.00010019
2017-10-10T14:59:55.076874: step 2485, loss 0.106452, acc 0.96875, learning_rate 0.00010019
2017-10-10T14:59:55.440140: step 2486, loss 0.129131, acc 0.96875, learning_rate 0.000100189
2017-10-10T14:59:55.878809: step 2487, loss 0.0972184, acc 0.984375, learning_rate 0.000100188
2017-10-10T14:59:56.254356: step 2488, loss 0.144465, acc 0.9375, learning_rate 0.000100187
2017-10-10T14:59:56.624913: step 2489, loss 0.0774599, acc 0.984375, learning_rate 0.000100187
2017-10-10T14:59:57.021065: step 2490, loss 0.170917, acc 0.9375, learning_rate 0.000100186
2017-10-10T14:59:57.405917: step 2491, loss 0.116361, acc 0.96875, learning_rate 0.000100185
2017-10-10T14:59:57.782915: step 2492, loss 0.128711, acc 0.953125, learning_rate 0.000100184
2017-10-10T14:59:58.107990: step 2493, loss 0.105912, acc 0.96875, learning_rate 0.000100183
2017-10-10T14:59:58.472096: step 2494, loss 0.179106, acc 0.921875, learning_rate 0.000100183
2017-10-10T14:59:58.953114: step 2495, loss 0.111047, acc 0.953125, learning_rate 0.000100182
2017-10-10T14:59:59.364178: step 2496, loss 0.0924155, acc 0.984375, learning_rate 0.000100181
2017-10-10T14:59:59.646348: step 2497, loss 0.0964144, acc 0.96875, learning_rate 0.000100181
2017-10-10T14:59:59.912405: step 2498, loss 0.173405, acc 0.953125, learning_rate 0.00010018
2017-10-10T15:00:00.240903: step 2499, loss 0.1305, acc 0.9375, learning_rate 0.000100179
2017-10-10T15:00:00.624873: step 2500, loss 0.0773919, acc 1, learning_rate 0.000100178
2017-10-10T15:00:00.978498: step 2501, loss 0.147577, acc 0.9375, learning_rate 0.000100178
2017-10-10T15:00:01.353615: step 2502, loss 0.0847721, acc 0.96875, learning_rate 0.000100177
2017-10-10T15:00:01.736735: step 2503, loss 0.0685209, acc 1, learning_rate 0.000100176
2017-10-10T15:00:02.064006: step 2504, loss 0.142606, acc 0.9375, learning_rate 0.000100175
2017-10-10T15:00:02.416841: step 2505, loss 0.118362, acc 0.96875, learning_rate 0.000100175
2017-10-10T15:00:02.745081: step 2506, loss 0.084348, acc 0.984375, learning_rate 0.000100174
2017-10-10T15:00:03.074426: step 2507, loss 0.0884256, acc 0.984375, learning_rate 0.000100173
2017-10-10T15:00:03.450913: step 2508, loss 0.12092, acc 0.953125, learning_rate 0.000100173
2017-10-10T15:00:03.823939: step 2509, loss 0.229582, acc 0.90625, learning_rate 0.000100172
2017-10-10T15:00:04.194827: step 2510, loss 0.131692, acc 0.953125, learning_rate 0.000100171
2017-10-10T15:00:04.607700: step 2511, loss 0.135761, acc 0.9375, learning_rate 0.00010017
2017-10-10T15:00:05.029859: step 2512, loss 0.0994535, acc 0.96875, learning_rate 0.00010017
2017-10-10T15:00:05.404762: step 2513, loss 0.124313, acc 0.984375, learning_rate 0.000100169
2017-10-10T15:00:05.764997: step 2514, loss 0.0892844, acc 0.984375, learning_rate 0.000100168
2017-10-10T15:00:06.165063: step 2515, loss 0.08538, acc 0.984375, learning_rate 0.000100168
2017-10-10T15:00:06.528346: step 2516, loss 0.110055, acc 0.953125, learning_rate 0.000100167
2017-10-10T15:00:06.934209: step 2517, loss 0.185868, acc 0.921875, learning_rate 0.000100166
2017-10-10T15:00:07.300941: step 2518, loss 0.130406, acc 0.953125, learning_rate 0.000100166
2017-10-10T15:00:07.677757: step 2519, loss 0.146839, acc 0.921875, learning_rate 0.000100165
2017-10-10T15:00:08.064833: step 2520, loss 0.126473, acc 0.984375, learning_rate 0.000100164

Evaluation:
2017-10-10T15:00:08.782199: step 2520, loss 0.209908, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2520

2017-10-10T15:00:10.054770: step 2521, loss 0.184426, acc 0.9375, learning_rate 0.000100164
2017-10-10T15:00:10.436914: step 2522, loss 0.253284, acc 0.921875, learning_rate 0.000100163
2017-10-10T15:00:11.510979: step 2523, loss 0.0702919, acc 0.984375, learning_rate 0.000100162
2017-10-10T15:00:12.030619: step 2524, loss 0.04327, acc 1, learning_rate 0.000100162
2017-10-10T15:00:12.420913: step 2525, loss 0.195274, acc 0.9375, learning_rate 0.000100161
2017-10-10T15:00:12.708781: step 2526, loss 0.0795082, acc 0.96875, learning_rate 0.00010016
2017-10-10T15:00:13.115683: step 2527, loss 0.134989, acc 0.96875, learning_rate 0.00010016
2017-10-10T15:00:13.506601: step 2528, loss 0.0704208, acc 0.96875, learning_rate 0.000100159
2017-10-10T15:00:13.810992: step 2529, loss 0.160495, acc 0.96875, learning_rate 0.000100158
2017-10-10T15:00:14.093693: step 2530, loss 0.0986078, acc 0.953125, learning_rate 0.000100158
2017-10-10T15:00:14.476847: step 2531, loss 0.101485, acc 0.953125, learning_rate 0.000100157
2017-10-10T15:00:14.859826: step 2532, loss 0.0623473, acc 0.984375, learning_rate 0.000100156
2017-10-10T15:00:15.199938: step 2533, loss 0.107158, acc 0.96875, learning_rate 0.000100156
2017-10-10T15:00:15.506825: step 2534, loss 0.0578109, acc 0.984375, learning_rate 0.000100155
2017-10-10T15:00:15.898082: step 2535, loss 0.101505, acc 0.96875, learning_rate 0.000100155
2017-10-10T15:00:16.236820: step 2536, loss 0.0886506, acc 0.96875, learning_rate 0.000100154
2017-10-10T15:00:16.597052: step 2537, loss 0.146408, acc 0.953125, learning_rate 0.000100153
2017-10-10T15:00:16.981104: step 2538, loss 0.100573, acc 0.953125, learning_rate 0.000100153
2017-10-10T15:00:17.401171: step 2539, loss 0.0350745, acc 1, learning_rate 0.000100152
2017-10-10T15:00:17.731864: step 2540, loss 0.108187, acc 0.96875, learning_rate 0.000100151
2017-10-10T15:00:18.044765: step 2541, loss 0.130803, acc 0.9375, learning_rate 0.000100151
2017-10-10T15:00:18.364830: step 2542, loss 0.0677905, acc 0.984375, learning_rate 0.00010015
2017-10-10T15:00:18.688497: step 2543, loss 0.126668, acc 0.96875, learning_rate 0.00010015
2017-10-10T15:00:19.017368: step 2544, loss 0.080747, acc 0.96875, learning_rate 0.000100149
2017-10-10T15:00:19.356956: step 2545, loss 0.15841, acc 0.9375, learning_rate 0.000100148
2017-10-10T15:00:19.695348: step 2546, loss 0.204425, acc 0.921875, learning_rate 0.000100148
2017-10-10T15:00:20.044913: step 2547, loss 0.172365, acc 0.9375, learning_rate 0.000100147
2017-10-10T15:00:20.524968: step 2548, loss 0.0530072, acc 0.980392, learning_rate 0.000100147
2017-10-10T15:00:20.917283: step 2549, loss 0.159769, acc 0.953125, learning_rate 0.000100146
2017-10-10T15:00:21.313027: step 2550, loss 0.112916, acc 0.96875, learning_rate 0.000100145
2017-10-10T15:00:21.680973: step 2551, loss 0.17071, acc 0.953125, learning_rate 0.000100145
2017-10-10T15:00:22.034587: step 2552, loss 0.0903578, acc 0.984375, learning_rate 0.000100144
2017-10-10T15:00:22.374080: step 2553, loss 0.0885284, acc 0.96875, learning_rate 0.000100144
2017-10-10T15:00:22.772186: step 2554, loss 0.109137, acc 0.9375, learning_rate 0.000100143
2017-10-10T15:00:23.176534: step 2555, loss 0.0570549, acc 1, learning_rate 0.000100142
2017-10-10T15:00:23.547576: step 2556, loss 0.165327, acc 0.921875, learning_rate 0.000100142
2017-10-10T15:00:23.956112: step 2557, loss 0.152251, acc 0.953125, learning_rate 0.000100141
2017-10-10T15:00:24.336895: step 2558, loss 0.0788544, acc 0.96875, learning_rate 0.000100141
2017-10-10T15:00:24.731322: step 2559, loss 0.0700796, acc 0.984375, learning_rate 0.00010014
2017-10-10T15:00:25.142287: step 2560, loss 0.173674, acc 0.90625, learning_rate 0.00010014

Evaluation:
2017-10-10T15:00:25.830373: step 2560, loss 0.208203, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2560

2017-10-10T15:00:27.169507: step 2561, loss 0.153009, acc 0.9375, learning_rate 0.000100139
2017-10-10T15:00:27.424776: step 2562, loss 0.126991, acc 0.96875, learning_rate 0.000100138
2017-10-10T15:00:27.805053: step 2563, loss 0.148287, acc 0.9375, learning_rate 0.000100138
2017-10-10T15:00:28.151004: step 2564, loss 0.155126, acc 0.953125, learning_rate 0.000100137
2017-10-10T15:00:28.550128: step 2565, loss 0.0681652, acc 0.96875, learning_rate 0.000100137
2017-10-10T15:00:28.878145: step 2566, loss 0.204258, acc 0.9375, learning_rate 0.000100136
2017-10-10T15:00:29.251970: step 2567, loss 0.162011, acc 0.9375, learning_rate 0.000100136
2017-10-10T15:00:29.713016: step 2568, loss 0.130835, acc 0.953125, learning_rate 0.000100135
2017-10-10T15:00:30.110919: step 2569, loss 0.129722, acc 0.9375, learning_rate 0.000100134
2017-10-10T15:00:30.483748: step 2570, loss 0.0820076, acc 0.96875, learning_rate 0.000100134
2017-10-10T15:00:30.906623: step 2571, loss 0.0886291, acc 0.984375, learning_rate 0.000100133
2017-10-10T15:00:31.302989: step 2572, loss 0.135015, acc 0.96875, learning_rate 0.000100133
2017-10-10T15:00:31.650750: step 2573, loss 0.149388, acc 0.9375, learning_rate 0.000100132
2017-10-10T15:00:31.985605: step 2574, loss 0.0725456, acc 0.96875, learning_rate 0.000100132
2017-10-10T15:00:32.349150: step 2575, loss 0.097277, acc 1, learning_rate 0.000100131
2017-10-10T15:00:32.719391: step 2576, loss 0.069915, acc 1, learning_rate 0.000100131
2017-10-10T15:00:33.037486: step 2577, loss 0.176265, acc 0.9375, learning_rate 0.00010013
2017-10-10T15:00:33.428724: step 2578, loss 0.166104, acc 0.9375, learning_rate 0.00010013
2017-10-10T15:00:33.810734: step 2579, loss 0.165617, acc 0.953125, learning_rate 0.000100129
2017-10-10T15:00:34.218987: step 2580, loss 0.0475752, acc 1, learning_rate 0.000100129
2017-10-10T15:00:34.568423: step 2581, loss 0.17583, acc 0.9375, learning_rate 0.000100128
2017-10-10T15:00:34.912840: step 2582, loss 0.0901241, acc 0.953125, learning_rate 0.000100128
2017-10-10T15:00:35.405114: step 2583, loss 0.224069, acc 0.9375, learning_rate 0.000100127
2017-10-10T15:00:36.087272: step 2584, loss 0.184045, acc 0.9375, learning_rate 0.000100126
2017-10-10T15:00:36.397691: step 2585, loss 0.139496, acc 0.9375, learning_rate 0.000100126
2017-10-10T15:00:36.820549: step 2586, loss 0.104939, acc 0.953125, learning_rate 0.000100125
2017-10-10T15:00:37.179517: step 2587, loss 0.140673, acc 0.953125, learning_rate 0.000100125
2017-10-10T15:00:37.508868: step 2588, loss 0.0929123, acc 0.96875, learning_rate 0.000100124
2017-10-10T15:00:37.909030: step 2589, loss 0.110752, acc 0.9375, learning_rate 0.000100124
2017-10-10T15:00:38.248349: step 2590, loss 0.118956, acc 0.984375, learning_rate 0.000100123
2017-10-10T15:00:38.580908: step 2591, loss 0.120162, acc 0.953125, learning_rate 0.000100123
2017-10-10T15:00:39.023864: step 2592, loss 0.10499, acc 0.96875, learning_rate 0.000100122
2017-10-10T15:00:39.453040: step 2593, loss 0.15777, acc 0.9375, learning_rate 0.000100122
2017-10-10T15:00:39.812682: step 2594, loss 0.0842468, acc 0.96875, learning_rate 0.000100121
2017-10-10T15:00:40.166757: step 2595, loss 0.129525, acc 0.9375, learning_rate 0.000100121
2017-10-10T15:00:40.512982: step 2596, loss 0.116203, acc 0.953125, learning_rate 0.00010012
2017-10-10T15:00:40.891061: step 2597, loss 0.0620975, acc 0.984375, learning_rate 0.00010012
2017-10-10T15:00:41.257314: step 2598, loss 0.179458, acc 0.953125, learning_rate 0.000100119
2017-10-10T15:00:41.655329: step 2599, loss 0.0816404, acc 0.96875, learning_rate 0.000100119
2017-10-10T15:00:42.110643: step 2600, loss 0.0480823, acc 1, learning_rate 0.000100118

Evaluation:
2017-10-10T15:00:42.896886: step 2600, loss 0.209673, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2600

2017-10-10T15:00:44.253219: step 2601, loss 0.0872386, acc 0.96875, learning_rate 0.000100118
2017-10-10T15:00:44.617226: step 2602, loss 0.0728357, acc 0.984375, learning_rate 0.000100117
2017-10-10T15:00:45.043785: step 2603, loss 0.0732609, acc 0.984375, learning_rate 0.000100117
2017-10-10T15:00:45.398170: step 2604, loss 0.190874, acc 0.9375, learning_rate 0.000100117
2017-10-10T15:00:45.786690: step 2605, loss 0.0833603, acc 0.984375, learning_rate 0.000100116
2017-10-10T15:00:46.137056: step 2606, loss 0.114308, acc 0.953125, learning_rate 0.000100116
2017-10-10T15:00:46.523050: step 2607, loss 0.158362, acc 0.953125, learning_rate 0.000100115
2017-10-10T15:00:46.892879: step 2608, loss 0.157222, acc 0.9375, learning_rate 0.000100115
2017-10-10T15:00:47.375676: step 2609, loss 0.155092, acc 0.953125, learning_rate 0.000100114
2017-10-10T15:00:47.702254: step 2610, loss 0.058947, acc 0.984375, learning_rate 0.000100114
2017-10-10T15:00:48.039420: step 2611, loss 0.189663, acc 0.921875, learning_rate 0.000100113
2017-10-10T15:00:48.360918: step 2612, loss 0.0490689, acc 0.984375, learning_rate 0.000100113
2017-10-10T15:00:48.749906: step 2613, loss 0.187835, acc 0.921875, learning_rate 0.000100112
2017-10-10T15:00:49.070137: step 2614, loss 0.15346, acc 0.90625, learning_rate 0.000100112
2017-10-10T15:00:49.484844: step 2615, loss 0.158953, acc 0.953125, learning_rate 0.000100111
2017-10-10T15:00:49.907546: step 2616, loss 0.141147, acc 0.9375, learning_rate 0.000100111
2017-10-10T15:00:50.324804: step 2617, loss 0.0746647, acc 1, learning_rate 0.000100111
2017-10-10T15:00:50.710814: step 2618, loss 0.0831639, acc 0.96875, learning_rate 0.00010011
2017-10-10T15:00:51.046309: step 2619, loss 0.0744686, acc 0.984375, learning_rate 0.00010011
2017-10-10T15:00:51.478295: step 2620, loss 0.108565, acc 0.984375, learning_rate 0.000100109
2017-10-10T15:00:51.793706: step 2621, loss 0.134127, acc 0.953125, learning_rate 0.000100109
2017-10-10T15:00:52.159267: step 2622, loss 0.100807, acc 0.9375, learning_rate 0.000100108
2017-10-10T15:00:52.505075: step 2623, loss 0.0924641, acc 0.984375, learning_rate 0.000100108
2017-10-10T15:00:52.942354: step 2624, loss 0.164645, acc 0.953125, learning_rate 0.000100107
2017-10-10T15:00:53.248895: step 2625, loss 0.0504224, acc 0.984375, learning_rate 0.000100107
2017-10-10T15:00:53.717184: step 2626, loss 0.0867538, acc 0.984375, learning_rate 0.000100107
2017-10-10T15:00:54.060952: step 2627, loss 0.0817795, acc 0.984375, learning_rate 0.000100106
2017-10-10T15:00:54.361053: step 2628, loss 0.0589885, acc 0.984375, learning_rate 0.000100106
2017-10-10T15:00:54.702858: step 2629, loss 0.11309, acc 0.953125, learning_rate 0.000100105
2017-10-10T15:00:55.067226: step 2630, loss 0.200087, acc 0.96875, learning_rate 0.000100105
2017-10-10T15:00:55.469530: step 2631, loss 0.116486, acc 0.953125, learning_rate 0.000100104
2017-10-10T15:00:55.831838: step 2632, loss 0.128948, acc 0.9375, learning_rate 0.000100104
2017-10-10T15:00:56.157653: step 2633, loss 0.130974, acc 0.96875, learning_rate 0.000100104
2017-10-10T15:00:56.561177: step 2634, loss 0.144474, acc 0.953125, learning_rate 0.000100103
2017-10-10T15:00:56.955432: step 2635, loss 0.152533, acc 0.921875, learning_rate 0.000100103
2017-10-10T15:00:57.328855: step 2636, loss 0.217227, acc 0.90625, learning_rate 0.000100102
2017-10-10T15:00:57.759821: step 2637, loss 0.0748745, acc 0.96875, learning_rate 0.000100102
2017-10-10T15:00:58.143989: step 2638, loss 0.0974703, acc 0.96875, learning_rate 0.000100101
2017-10-10T15:00:58.565005: step 2639, loss 0.143771, acc 0.921875, learning_rate 0.000100101
2017-10-10T15:00:58.985433: step 2640, loss 0.0749247, acc 0.96875, learning_rate 0.000100101

Evaluation:
2017-10-10T15:00:59.672008: step 2640, loss 0.207103, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2640

2017-10-10T15:01:00.824033: step 2641, loss 0.169536, acc 0.953125, learning_rate 0.0001001
2017-10-10T15:01:01.207958: step 2642, loss 0.0757968, acc 0.96875, learning_rate 0.0001001
2017-10-10T15:01:01.560988: step 2643, loss 0.143242, acc 0.96875, learning_rate 0.000100099
2017-10-10T15:01:01.942273: step 2644, loss 0.152004, acc 0.953125, learning_rate 0.000100099
2017-10-10T15:01:02.293232: step 2645, loss 0.127437, acc 0.9375, learning_rate 0.000100099
2017-10-10T15:01:02.593777: step 2646, loss 0.0868464, acc 0.960784, learning_rate 0.000100098
2017-10-10T15:01:03.027628: step 2647, loss 0.158084, acc 0.9375, learning_rate 0.000100098
2017-10-10T15:01:03.381219: step 2648, loss 0.0606789, acc 0.96875, learning_rate 0.000100097
2017-10-10T15:01:03.742778: step 2649, loss 0.212477, acc 0.90625, learning_rate 0.000100097
2017-10-10T15:01:04.170331: step 2650, loss 0.0789225, acc 0.96875, learning_rate 0.000100097
2017-10-10T15:01:04.520510: step 2651, loss 0.14887, acc 0.9375, learning_rate 0.000100096
2017-10-10T15:01:04.788026: step 2652, loss 0.226545, acc 0.921875, learning_rate 0.000100096
2017-10-10T15:01:05.188124: step 2653, loss 0.142938, acc 0.9375, learning_rate 0.000100095
2017-10-10T15:01:05.622917: step 2654, loss 0.0474921, acc 1, learning_rate 0.000100095
2017-10-10T15:01:06.073492: step 2655, loss 0.13796, acc 0.96875, learning_rate 0.000100095
2017-10-10T15:01:06.429973: step 2656, loss 0.0844639, acc 0.96875, learning_rate 0.000100094
2017-10-10T15:01:06.805780: step 2657, loss 0.0957382, acc 0.96875, learning_rate 0.000100094
2017-10-10T15:01:07.236271: step 2658, loss 0.309128, acc 0.90625, learning_rate 0.000100093
2017-10-10T15:01:07.629424: step 2659, loss 0.0638335, acc 0.984375, learning_rate 0.000100093
2017-10-10T15:01:08.010706: step 2660, loss 0.0588631, acc 1, learning_rate 0.000100093
2017-10-10T15:01:08.420827: step 2661, loss 0.0739601, acc 0.984375, learning_rate 0.000100092
2017-10-10T15:01:08.839317: step 2662, loss 0.127111, acc 0.96875, learning_rate 0.000100092
2017-10-10T15:01:09.232819: step 2663, loss 0.070555, acc 0.984375, learning_rate 0.000100092
2017-10-10T15:01:09.668036: step 2664, loss 0.165003, acc 0.90625, learning_rate 0.000100091
2017-10-10T15:01:10.010642: step 2665, loss 0.0975848, acc 0.96875, learning_rate 0.000100091
2017-10-10T15:01:10.396907: step 2666, loss 0.167139, acc 0.921875, learning_rate 0.00010009
2017-10-10T15:01:10.755256: step 2667, loss 0.135254, acc 0.953125, learning_rate 0.00010009
2017-10-10T15:01:11.161045: step 2668, loss 0.117825, acc 0.96875, learning_rate 0.00010009
2017-10-10T15:01:11.544881: step 2669, loss 0.103027, acc 0.953125, learning_rate 0.000100089
2017-10-10T15:01:12.019569: step 2670, loss 0.0957812, acc 0.953125, learning_rate 0.000100089
2017-10-10T15:01:12.334178: step 2671, loss 0.171035, acc 0.9375, learning_rate 0.000100089
2017-10-10T15:01:13.661712: step 2672, loss 0.0710848, acc 0.96875, learning_rate 0.000100088
2017-10-10T15:01:14.032960: step 2673, loss 0.143361, acc 0.953125, learning_rate 0.000100088
2017-10-10T15:01:14.454904: step 2674, loss 0.114446, acc 0.953125, learning_rate 0.000100088
2017-10-10T15:01:14.895700: step 2675, loss 0.0850414, acc 0.953125, learning_rate 0.000100087
2017-10-10T15:01:15.296950: step 2676, loss 0.20027, acc 0.921875, learning_rate 0.000100087
2017-10-10T15:01:15.698115: step 2677, loss 0.15102, acc 0.9375, learning_rate 0.000100086
2017-10-10T15:01:16.053321: step 2678, loss 0.176157, acc 0.921875, learning_rate 0.000100086
2017-10-10T15:01:16.466702: step 2679, loss 0.168026, acc 0.9375, learning_rate 0.000100086
2017-10-10T15:01:16.836916: step 2680, loss 0.141608, acc 0.9375, learning_rate 0.000100085

Evaluation:
2017-10-10T15:01:17.600749: step 2680, loss 0.207706, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2680

2017-10-10T15:01:18.984893: step 2681, loss 0.0497787, acc 1, learning_rate 0.000100085
2017-10-10T15:01:19.372887: step 2682, loss 0.079487, acc 0.96875, learning_rate 0.000100085
2017-10-10T15:01:19.701759: step 2683, loss 0.111794, acc 0.96875, learning_rate 0.000100084
2017-10-10T15:01:20.236984: step 2684, loss 0.064767, acc 0.984375, learning_rate 0.000100084
2017-10-10T15:01:20.568461: step 2685, loss 0.12572, acc 0.96875, learning_rate 0.000100084
2017-10-10T15:01:20.846466: step 2686, loss 0.119839, acc 0.984375, learning_rate 0.000100083
2017-10-10T15:01:21.223088: step 2687, loss 0.146344, acc 0.96875, learning_rate 0.000100083
2017-10-10T15:01:21.644503: step 2688, loss 0.106305, acc 0.953125, learning_rate 0.000100083
2017-10-10T15:01:21.976871: step 2689, loss 0.0445884, acc 1, learning_rate 0.000100082
2017-10-10T15:01:22.310828: step 2690, loss 0.21934, acc 0.90625, learning_rate 0.000100082
2017-10-10T15:01:22.662983: step 2691, loss 0.0916325, acc 0.984375, learning_rate 0.000100082
2017-10-10T15:01:23.069706: step 2692, loss 0.0766958, acc 0.984375, learning_rate 0.000100081
2017-10-10T15:01:23.471801: step 2693, loss 0.221867, acc 0.9375, learning_rate 0.000100081
2017-10-10T15:01:23.867803: step 2694, loss 0.106255, acc 0.953125, learning_rate 0.000100081
2017-10-10T15:01:24.276058: step 2695, loss 0.194457, acc 0.9375, learning_rate 0.00010008
2017-10-10T15:01:24.664902: step 2696, loss 0.0632584, acc 0.96875, learning_rate 0.00010008
2017-10-10T15:01:25.056597: step 2697, loss 0.113824, acc 0.96875, learning_rate 0.00010008
2017-10-10T15:01:25.417841: step 2698, loss 0.211995, acc 0.921875, learning_rate 0.000100079
2017-10-10T15:01:25.765147: step 2699, loss 0.181032, acc 0.953125, learning_rate 0.000100079
2017-10-10T15:01:26.048021: step 2700, loss 0.0424912, acc 0.984375, learning_rate 0.000100079
2017-10-10T15:01:26.441877: step 2701, loss 0.0728208, acc 0.96875, learning_rate 0.000100078
2017-10-10T15:01:26.792878: step 2702, loss 0.24118, acc 0.921875, learning_rate 0.000100078
2017-10-10T15:01:27.197579: step 2703, loss 0.110558, acc 0.96875, learning_rate 0.000100078
2017-10-10T15:01:27.547150: step 2704, loss 0.109098, acc 0.953125, learning_rate 0.000100077
2017-10-10T15:01:27.921317: step 2705, loss 0.112808, acc 0.953125, learning_rate 0.000100077
2017-10-10T15:01:28.346074: step 2706, loss 0.129109, acc 0.96875, learning_rate 0.000100077
2017-10-10T15:01:28.693342: step 2707, loss 0.0722513, acc 0.984375, learning_rate 0.000100076
2017-10-10T15:01:29.112836: step 2708, loss 0.0796488, acc 0.96875, learning_rate 0.000100076
2017-10-10T15:01:29.484424: step 2709, loss 0.0645993, acc 0.984375, learning_rate 0.000100076
2017-10-10T15:01:29.844813: step 2710, loss 0.0215864, acc 1, learning_rate 0.000100076
2017-10-10T15:01:30.216078: step 2711, loss 0.103582, acc 0.953125, learning_rate 0.000100075
2017-10-10T15:01:30.614915: step 2712, loss 0.228971, acc 0.953125, learning_rate 0.000100075
2017-10-10T15:01:30.967838: step 2713, loss 0.185137, acc 0.953125, learning_rate 0.000100075
2017-10-10T15:01:31.259091: step 2714, loss 0.130254, acc 0.96875, learning_rate 0.000100074
2017-10-10T15:01:31.580846: step 2715, loss 0.0947476, acc 0.96875, learning_rate 0.000100074
2017-10-10T15:01:31.978235: step 2716, loss 0.169424, acc 0.921875, learning_rate 0.000100074
2017-10-10T15:01:32.384882: step 2717, loss 0.0524867, acc 1, learning_rate 0.000100073
2017-10-10T15:01:32.828523: step 2718, loss 0.0864264, acc 0.96875, learning_rate 0.000100073
2017-10-10T15:01:33.176906: step 2719, loss 0.232651, acc 0.90625, learning_rate 0.000100073
2017-10-10T15:01:33.482977: step 2720, loss 0.157117, acc 0.9375, learning_rate 0.000100073

Evaluation:
2017-10-10T15:01:34.156864: step 2720, loss 0.207581, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2720

2017-10-10T15:01:35.488115: step 2721, loss 0.083996, acc 0.96875, learning_rate 0.000100072
2017-10-10T15:01:35.860841: step 2722, loss 0.219265, acc 0.9375, learning_rate 0.000100072
2017-10-10T15:01:36.220947: step 2723, loss 0.034636, acc 1, learning_rate 0.000100072
2017-10-10T15:01:36.634269: step 2724, loss 0.0835722, acc 0.96875, learning_rate 0.000100071
2017-10-10T15:01:37.012850: step 2725, loss 0.0304498, acc 1, learning_rate 0.000100071
2017-10-10T15:01:37.376145: step 2726, loss 0.0659909, acc 1, learning_rate 0.000100071
2017-10-10T15:01:37.744701: step 2727, loss 0.0458064, acc 1, learning_rate 0.00010007
2017-10-10T15:01:38.124898: step 2728, loss 0.0622263, acc 0.96875, learning_rate 0.00010007
2017-10-10T15:01:38.481282: step 2729, loss 0.0708397, acc 0.984375, learning_rate 0.00010007
2017-10-10T15:01:38.864952: step 2730, loss 0.131917, acc 0.9375, learning_rate 0.00010007
2017-10-10T15:01:39.218782: step 2731, loss 0.0721556, acc 0.96875, learning_rate 0.000100069
2017-10-10T15:01:39.581316: step 2732, loss 0.166043, acc 0.9375, learning_rate 0.000100069
2017-10-10T15:01:39.956870: step 2733, loss 0.0632121, acc 0.984375, learning_rate 0.000100069
2017-10-10T15:01:40.316908: step 2734, loss 0.139753, acc 0.953125, learning_rate 0.000100068
2017-10-10T15:01:40.739259: step 2735, loss 0.167213, acc 0.953125, learning_rate 0.000100068
2017-10-10T15:01:41.128876: step 2736, loss 0.0670037, acc 0.984375, learning_rate 0.000100068
2017-10-10T15:01:41.497581: step 2737, loss 0.0676538, acc 0.984375, learning_rate 0.000100068
2017-10-10T15:01:41.816624: step 2738, loss 0.155749, acc 0.96875, learning_rate 0.000100067
2017-10-10T15:01:42.223723: step 2739, loss 0.178563, acc 0.90625, learning_rate 0.000100067
2017-10-10T15:01:42.552958: step 2740, loss 0.137877, acc 0.953125, learning_rate 0.000100067
2017-10-10T15:01:42.931766: step 2741, loss 0.109822, acc 0.953125, learning_rate 0.000100067
2017-10-10T15:01:43.332859: step 2742, loss 0.197875, acc 0.921875, learning_rate 0.000100066
2017-10-10T15:01:43.760991: step 2743, loss 0.11345, acc 0.953125, learning_rate 0.000100066
2017-10-10T15:01:44.031352: step 2744, loss 0.181348, acc 0.941176, learning_rate 0.000100066
2017-10-10T15:01:44.394846: step 2745, loss 0.120408, acc 0.9375, learning_rate 0.000100065
2017-10-10T15:01:44.757498: step 2746, loss 0.140468, acc 0.953125, learning_rate 0.000100065
2017-10-10T15:01:45.105724: step 2747, loss 0.150928, acc 0.9375, learning_rate 0.000100065
2017-10-10T15:01:45.500848: step 2748, loss 0.106117, acc 0.953125, learning_rate 0.000100065
2017-10-10T15:01:45.828521: step 2749, loss 0.0971864, acc 0.96875, learning_rate 0.000100064
2017-10-10T15:01:46.236965: step 2750, loss 0.17332, acc 0.953125, learning_rate 0.000100064
2017-10-10T15:01:46.652947: step 2751, loss 0.0885818, acc 0.96875, learning_rate 0.000100064
2017-10-10T15:01:47.012894: step 2752, loss 0.125925, acc 0.96875, learning_rate 0.000100064
2017-10-10T15:01:47.292828: step 2753, loss 0.0793134, acc 0.96875, learning_rate 0.000100063
2017-10-10T15:01:47.661737: step 2754, loss 0.151546, acc 0.953125, learning_rate 0.000100063
2017-10-10T15:01:48.037501: step 2755, loss 0.101728, acc 0.984375, learning_rate 0.000100063
2017-10-10T15:01:48.400211: step 2756, loss 0.225651, acc 0.921875, learning_rate 0.000100063
2017-10-10T15:01:48.861101: step 2757, loss 0.0825015, acc 0.953125, learning_rate 0.000100062
2017-10-10T15:01:49.344898: step 2758, loss 0.147103, acc 0.921875, learning_rate 0.000100062
2017-10-10T15:01:49.640926: step 2759, loss 0.169897, acc 0.953125, learning_rate 0.000100062
2017-10-10T15:01:49.962532: step 2760, loss 0.0795059, acc 0.953125, learning_rate 0.000100062

Evaluation:
2017-10-10T15:01:50.684217: step 2760, loss 0.206651, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2760

2017-10-10T15:01:51.949102: step 2761, loss 0.0441947, acc 1, learning_rate 0.000100061
2017-10-10T15:01:52.288965: step 2762, loss 0.0484346, acc 1, learning_rate 0.000100061
2017-10-10T15:01:52.680686: step 2763, loss 0.0923328, acc 0.96875, learning_rate 0.000100061
2017-10-10T15:01:53.076066: step 2764, loss 0.0939222, acc 0.96875, learning_rate 0.000100061
2017-10-10T15:01:53.458215: step 2765, loss 0.133403, acc 0.96875, learning_rate 0.00010006
2017-10-10T15:01:53.833746: step 2766, loss 0.14654, acc 0.953125, learning_rate 0.00010006
2017-10-10T15:01:54.288812: step 2767, loss 0.155283, acc 0.9375, learning_rate 0.00010006
2017-10-10T15:01:54.693144: step 2768, loss 0.142216, acc 0.9375, learning_rate 0.00010006
2017-10-10T15:01:54.976271: step 2769, loss 0.159301, acc 0.96875, learning_rate 0.000100059
2017-10-10T15:01:55.328096: step 2770, loss 0.0953553, acc 0.984375, learning_rate 0.000100059
2017-10-10T15:01:55.645023: step 2771, loss 0.166028, acc 0.9375, learning_rate 0.000100059
2017-10-10T15:01:56.016992: step 2772, loss 0.0717897, acc 0.984375, learning_rate 0.000100059
2017-10-10T15:01:56.413913: step 2773, loss 0.140848, acc 0.9375, learning_rate 0.000100058
2017-10-10T15:01:56.753718: step 2774, loss 0.098309, acc 0.96875, learning_rate 0.000100058
2017-10-10T15:01:57.184979: step 2775, loss 0.099064, acc 0.96875, learning_rate 0.000100058
2017-10-10T15:01:57.562295: step 2776, loss 0.106749, acc 0.953125, learning_rate 0.000100058
2017-10-10T15:01:57.913030: step 2777, loss 0.116029, acc 0.96875, learning_rate 0.000100057
2017-10-10T15:01:58.315797: step 2778, loss 0.0989372, acc 0.984375, learning_rate 0.000100057
2017-10-10T15:01:58.726878: step 2779, loss 0.11276, acc 0.953125, learning_rate 0.000100057
2017-10-10T15:01:59.060756: step 2780, loss 0.137682, acc 0.9375, learning_rate 0.000100057
2017-10-10T15:01:59.408950: step 2781, loss 0.16316, acc 0.921875, learning_rate 0.000100056
2017-10-10T15:01:59.809635: step 2782, loss 0.102348, acc 0.984375, learning_rate 0.000100056
2017-10-10T15:02:00.196996: step 2783, loss 0.113668, acc 0.96875, learning_rate 0.000100056
2017-10-10T15:02:00.532851: step 2784, loss 0.0759289, acc 0.96875, learning_rate 0.000100056
2017-10-10T15:02:00.883310: step 2785, loss 0.0762928, acc 0.984375, learning_rate 0.000100056
2017-10-10T15:02:01.282299: step 2786, loss 0.111256, acc 0.96875, learning_rate 0.000100055
2017-10-10T15:02:01.617138: step 2787, loss 0.0821146, acc 0.984375, learning_rate 0.000100055
2017-10-10T15:02:01.961311: step 2788, loss 0.163533, acc 0.96875, learning_rate 0.000100055
2017-10-10T15:02:02.296731: step 2789, loss 0.100355, acc 0.953125, learning_rate 0.000100055
2017-10-10T15:02:02.689893: step 2790, loss 0.214633, acc 0.9375, learning_rate 0.000100054
2017-10-10T15:02:03.106156: step 2791, loss 0.155631, acc 0.96875, learning_rate 0.000100054
2017-10-10T15:02:03.490376: step 2792, loss 0.0583638, acc 1, learning_rate 0.000100054
2017-10-10T15:02:03.858573: step 2793, loss 0.200478, acc 0.90625, learning_rate 0.000100054
2017-10-10T15:02:04.213023: step 2794, loss 0.101657, acc 0.984375, learning_rate 0.000100054
2017-10-10T15:02:04.643231: step 2795, loss 0.118461, acc 0.96875, learning_rate 0.000100053
2017-10-10T15:02:05.039351: step 2796, loss 0.160307, acc 0.953125, learning_rate 0.000100053
2017-10-10T15:02:05.464738: step 2797, loss 0.0216167, acc 1, learning_rate 0.000100053
2017-10-10T15:02:05.857450: step 2798, loss 0.122375, acc 0.96875, learning_rate 0.000100053
2017-10-10T15:02:06.224342: step 2799, loss 0.100701, acc 0.96875, learning_rate 0.000100052
2017-10-10T15:02:06.551701: step 2800, loss 0.0607467, acc 1, learning_rate 0.000100052

Evaluation:
2017-10-10T15:02:07.277477: step 2800, loss 0.208936, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2800

2017-10-10T15:02:08.543697: step 2801, loss 0.0771446, acc 0.96875, learning_rate 0.000100052
2017-10-10T15:02:08.900845: step 2802, loss 0.0719851, acc 1, learning_rate 0.000100052
2017-10-10T15:02:09.304410: step 2803, loss 0.0893929, acc 0.96875, learning_rate 0.000100052
2017-10-10T15:02:09.643722: step 2804, loss 0.116148, acc 0.953125, learning_rate 0.000100051
2017-10-10T15:02:10.069156: step 2805, loss 0.135396, acc 0.984375, learning_rate 0.000100051
2017-10-10T15:02:10.420842: step 2806, loss 0.146114, acc 0.953125, learning_rate 0.000100051
2017-10-10T15:02:10.808500: step 2807, loss 0.0724482, acc 0.96875, learning_rate 0.000100051
2017-10-10T15:02:11.153111: step 2808, loss 0.115197, acc 0.9375, learning_rate 0.000100051
2017-10-10T15:02:11.568941: step 2809, loss 0.164317, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:02:12.021220: step 2810, loss 0.176979, acc 0.9375, learning_rate 0.00010005
2017-10-10T15:02:12.389560: step 2811, loss 0.115665, acc 0.953125, learning_rate 0.00010005
2017-10-10T15:02:12.782254: step 2812, loss 0.0992029, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:02:13.246848: step 2813, loss 0.0806371, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:02:13.577010: step 2814, loss 0.164309, acc 0.9375, learning_rate 0.000100049
2017-10-10T15:02:13.892857: step 2815, loss 0.099772, acc 0.9375, learning_rate 0.000100049
2017-10-10T15:02:14.308381: step 2816, loss 0.12014, acc 0.9375, learning_rate 0.000100049
2017-10-10T15:02:14.710095: step 2817, loss 0.0855222, acc 0.953125, learning_rate 0.000100049
2017-10-10T15:02:15.028876: step 2818, loss 0.0836128, acc 0.953125, learning_rate 0.000100049
2017-10-10T15:02:15.379050: step 2819, loss 0.0958862, acc 0.96875, learning_rate 0.000100048
2017-10-10T15:02:15.756889: step 2820, loss 0.0470398, acc 0.984375, learning_rate 0.000100048
2017-10-10T15:02:16.135326: step 2821, loss 0.193729, acc 0.953125, learning_rate 0.000100048
2017-10-10T15:02:16.521312: step 2822, loss 0.14042, acc 0.96875, learning_rate 0.000100048
2017-10-10T15:02:16.803463: step 2823, loss 0.0453825, acc 0.984375, learning_rate 0.000100048
2017-10-10T15:02:17.104869: step 2824, loss 0.126071, acc 0.953125, learning_rate 0.000100047
2017-10-10T15:02:17.370841: step 2825, loss 0.10877, acc 0.96875, learning_rate 0.000100047
2017-10-10T15:02:17.720885: step 2826, loss 0.154378, acc 0.9375, learning_rate 0.000100047
2017-10-10T15:02:18.032885: step 2827, loss 0.0814814, acc 0.96875, learning_rate 0.000100047
2017-10-10T15:02:18.344441: step 2828, loss 0.0761706, acc 0.984375, learning_rate 0.000100047
2017-10-10T15:02:18.664804: step 2829, loss 0.0809772, acc 0.984375, learning_rate 0.000100046
2017-10-10T15:02:18.942170: step 2830, loss 0.127129, acc 0.953125, learning_rate 0.000100046
2017-10-10T15:02:19.234360: step 2831, loss 0.146033, acc 0.9375, learning_rate 0.000100046
2017-10-10T15:02:19.548234: step 2832, loss 0.0778908, acc 0.984375, learning_rate 0.000100046
2017-10-10T15:02:19.852828: step 2833, loss 0.065219, acc 0.984375, learning_rate 0.000100046
2017-10-10T15:02:20.156943: step 2834, loss 0.112687, acc 0.96875, learning_rate 0.000100045
2017-10-10T15:02:20.583076: step 2835, loss 0.0903824, acc 0.96875, learning_rate 0.000100045
2017-10-10T15:02:20.900868: step 2836, loss 0.191395, acc 0.921875, learning_rate 0.000100045
2017-10-10T15:02:21.273065: step 2837, loss 0.2412, acc 0.90625, learning_rate 0.000100045
2017-10-10T15:02:21.672966: step 2838, loss 0.190014, acc 0.9375, learning_rate 0.000100045
2017-10-10T15:02:22.076104: step 2839, loss 0.161473, acc 0.9375, learning_rate 0.000100045
2017-10-10T15:02:22.481850: step 2840, loss 0.0519626, acc 0.984375, learning_rate 0.000100044

Evaluation:
2017-10-10T15:02:23.182383: step 2840, loss 0.208798, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2840

2017-10-10T15:02:24.464202: step 2841, loss 0.056609, acc 1, learning_rate 0.000100044
2017-10-10T15:02:24.843795: step 2842, loss 0.111041, acc 0.941176, learning_rate 0.000100044
2017-10-10T15:02:25.181718: step 2843, loss 0.054034, acc 1, learning_rate 0.000100044
2017-10-10T15:02:25.572490: step 2844, loss 0.103106, acc 0.984375, learning_rate 0.000100044
2017-10-10T15:02:25.856751: step 2845, loss 0.193395, acc 0.9375, learning_rate 0.000100043
2017-10-10T15:02:26.185020: step 2846, loss 0.0813699, acc 0.96875, learning_rate 0.000100043
2017-10-10T15:02:26.541243: step 2847, loss 0.0991645, acc 0.953125, learning_rate 0.000100043
2017-10-10T15:02:26.922249: step 2848, loss 0.082605, acc 0.96875, learning_rate 0.000100043
2017-10-10T15:02:27.321240: step 2849, loss 0.233304, acc 0.890625, learning_rate 0.000100043
2017-10-10T15:02:27.624974: step 2850, loss 0.109836, acc 0.953125, learning_rate 0.000100043
2017-10-10T15:02:28.005086: step 2851, loss 0.0470268, acc 1, learning_rate 0.000100042
2017-10-10T15:02:28.386902: step 2852, loss 0.0863297, acc 0.96875, learning_rate 0.000100042
2017-10-10T15:02:28.734885: step 2853, loss 0.16502, acc 0.921875, learning_rate 0.000100042
2017-10-10T15:02:29.145481: step 2854, loss 0.0913752, acc 0.953125, learning_rate 0.000100042
2017-10-10T15:02:29.554496: step 2855, loss 0.131116, acc 0.96875, learning_rate 0.000100042
2017-10-10T15:02:29.948888: step 2856, loss 0.141192, acc 0.953125, learning_rate 0.000100042
2017-10-10T15:02:30.320479: step 2857, loss 0.153752, acc 0.9375, learning_rate 0.000100041
2017-10-10T15:02:30.684850: step 2858, loss 0.171121, acc 0.96875, learning_rate 0.000100041
2017-10-10T15:02:30.981484: step 2859, loss 0.097037, acc 0.96875, learning_rate 0.000100041
2017-10-10T15:02:31.374635: step 2860, loss 0.0887993, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:02:31.794372: step 2861, loss 0.111238, acc 0.953125, learning_rate 0.000100041
2017-10-10T15:02:32.159778: step 2862, loss 0.115744, acc 0.96875, learning_rate 0.000100041
2017-10-10T15:02:32.528987: step 2863, loss 0.077641, acc 1, learning_rate 0.00010004
2017-10-10T15:02:32.965522: step 2864, loss 0.181396, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:02:33.362034: step 2865, loss 0.0756083, acc 0.953125, learning_rate 0.00010004
2017-10-10T15:02:33.714116: step 2866, loss 0.115922, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:02:34.065076: step 2867, loss 0.0386442, acc 1, learning_rate 0.00010004
2017-10-10T15:02:34.448263: step 2868, loss 0.0656283, acc 0.984375, learning_rate 0.00010004
2017-10-10T15:02:34.885777: step 2869, loss 0.0575718, acc 0.984375, learning_rate 0.000100039
2017-10-10T15:02:35.232876: step 2870, loss 0.0439579, acc 0.984375, learning_rate 0.000100039
2017-10-10T15:02:35.667464: step 2871, loss 0.12124, acc 0.96875, learning_rate 0.000100039
2017-10-10T15:02:36.024072: step 2872, loss 0.0555378, acc 1, learning_rate 0.000100039
2017-10-10T15:02:36.421723: step 2873, loss 0.208421, acc 0.9375, learning_rate 0.000100039
2017-10-10T15:02:36.796363: step 2874, loss 0.0952349, acc 0.96875, learning_rate 0.000100039
2017-10-10T15:02:37.167953: step 2875, loss 0.150739, acc 0.953125, learning_rate 0.000100038
2017-10-10T15:02:37.532227: step 2876, loss 0.0438696, acc 1, learning_rate 0.000100038
2017-10-10T15:02:37.911955: step 2877, loss 0.120603, acc 0.953125, learning_rate 0.000100038
2017-10-10T15:02:38.278858: step 2878, loss 0.11908, acc 0.921875, learning_rate 0.000100038
2017-10-10T15:02:38.669795: step 2879, loss 0.140569, acc 0.953125, learning_rate 0.000100038
2017-10-10T15:02:39.102957: step 2880, loss 0.0552009, acc 1, learning_rate 0.000100038

Evaluation:
2017-10-10T15:02:39.805934: step 2880, loss 0.208134, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2880

2017-10-10T15:02:41.291831: step 2881, loss 0.161963, acc 0.96875, learning_rate 0.000100038
2017-10-10T15:02:41.701067: step 2882, loss 0.0567546, acc 0.984375, learning_rate 0.000100037
2017-10-10T15:02:42.142761: step 2883, loss 0.245872, acc 0.90625, learning_rate 0.000100037
2017-10-10T15:02:42.570601: step 2884, loss 0.082604, acc 0.96875, learning_rate 0.000100037
2017-10-10T15:02:42.928859: step 2885, loss 0.0759811, acc 1, learning_rate 0.000100037
2017-10-10T15:02:43.368792: step 2886, loss 0.13102, acc 0.9375, learning_rate 0.000100037
2017-10-10T15:02:43.704797: step 2887, loss 0.159152, acc 0.953125, learning_rate 0.000100037
2017-10-10T15:02:44.102186: step 2888, loss 0.106125, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:02:44.500688: step 2889, loss 0.11076, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:02:44.908843: step 2890, loss 0.0858299, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:02:45.221953: step 2891, loss 0.113862, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:02:45.688925: step 2892, loss 0.0804851, acc 0.984375, learning_rate 0.000100036
2017-10-10T15:02:46.042067: step 2893, loss 0.092822, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:02:46.421659: step 2894, loss 0.0858139, acc 0.984375, learning_rate 0.000100036
2017-10-10T15:02:46.759578: step 2895, loss 0.047, acc 1, learning_rate 0.000100035
2017-10-10T15:02:47.149025: step 2896, loss 0.13774, acc 0.953125, learning_rate 0.000100035
2017-10-10T15:02:47.570388: step 2897, loss 0.155581, acc 0.953125, learning_rate 0.000100035
2017-10-10T15:02:47.984884: step 2898, loss 0.109842, acc 0.953125, learning_rate 0.000100035
2017-10-10T15:02:48.373709: step 2899, loss 0.074022, acc 0.96875, learning_rate 0.000100035
2017-10-10T15:02:48.741052: step 2900, loss 0.188054, acc 0.9375, learning_rate 0.000100035
2017-10-10T15:02:49.119848: step 2901, loss 0.0962533, acc 0.953125, learning_rate 0.000100035
2017-10-10T15:02:49.549144: step 2902, loss 0.0585468, acc 1, learning_rate 0.000100034
2017-10-10T15:02:49.900914: step 2903, loss 0.122911, acc 0.953125, learning_rate 0.000100034
2017-10-10T15:02:50.319165: step 2904, loss 0.0658905, acc 0.984375, learning_rate 0.000100034
2017-10-10T15:02:50.721030: step 2905, loss 0.106582, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:02:51.079498: step 2906, loss 0.211466, acc 0.90625, learning_rate 0.000100034
2017-10-10T15:02:51.458764: step 2907, loss 0.134945, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:02:51.836860: step 2908, loss 0.0654066, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:02:52.276871: step 2909, loss 0.0904902, acc 0.984375, learning_rate 0.000100033
2017-10-10T15:02:52.628888: step 2910, loss 0.0633354, acc 0.984375, learning_rate 0.000100033
2017-10-10T15:02:52.979130: step 2911, loss 0.126542, acc 0.953125, learning_rate 0.000100033
2017-10-10T15:02:53.305102: step 2912, loss 0.109322, acc 0.953125, learning_rate 0.000100033
2017-10-10T15:02:53.681859: step 2913, loss 0.195802, acc 0.90625, learning_rate 0.000100033
2017-10-10T15:02:54.117170: step 2914, loss 0.122888, acc 0.953125, learning_rate 0.000100033
2017-10-10T15:02:54.438727: step 2915, loss 0.195774, acc 0.96875, learning_rate 0.000100033
2017-10-10T15:02:54.709512: step 2916, loss 0.0775133, acc 0.984375, learning_rate 0.000100033
2017-10-10T15:02:55.035321: step 2917, loss 0.108172, acc 0.96875, learning_rate 0.000100032
2017-10-10T15:02:55.441040: step 2918, loss 0.0862152, acc 0.984375, learning_rate 0.000100032
2017-10-10T15:02:55.817281: step 2919, loss 0.072938, acc 0.984375, learning_rate 0.000100032
2017-10-10T15:02:56.202066: step 2920, loss 0.129634, acc 0.96875, learning_rate 0.000100032

Evaluation:
2017-10-10T15:02:56.930842: step 2920, loss 0.206381, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2920

2017-10-10T15:02:58.247435: step 2921, loss 0.0509626, acc 1, learning_rate 0.000100032
2017-10-10T15:02:58.633667: step 2922, loss 0.108939, acc 0.96875, learning_rate 0.000100032
2017-10-10T15:02:59.014992: step 2923, loss 0.0789134, acc 0.984375, learning_rate 0.000100032
2017-10-10T15:02:59.426733: step 2924, loss 0.0433743, acc 0.984375, learning_rate 0.000100031
2017-10-10T15:02:59.768939: step 2925, loss 0.0869185, acc 0.953125, learning_rate 0.000100031
2017-10-10T15:03:00.192893: step 2926, loss 0.115825, acc 0.953125, learning_rate 0.000100031
2017-10-10T15:03:00.557947: step 2927, loss 0.101571, acc 0.984375, learning_rate 0.000100031
2017-10-10T15:03:00.984890: step 2928, loss 0.15514, acc 0.90625, learning_rate 0.000100031
2017-10-10T15:03:01.452921: step 2929, loss 0.0762658, acc 0.984375, learning_rate 0.000100031
2017-10-10T15:03:01.843994: step 2930, loss 0.244004, acc 0.9375, learning_rate 0.000100031
2017-10-10T15:03:02.166735: step 2931, loss 0.145975, acc 0.90625, learning_rate 0.000100031
2017-10-10T15:03:02.506504: step 2932, loss 0.218386, acc 0.921875, learning_rate 0.00010003
2017-10-10T15:03:02.910678: step 2933, loss 0.094287, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:03:03.259278: step 2934, loss 0.0908864, acc 0.984375, learning_rate 0.00010003
2017-10-10T15:03:03.628849: step 2935, loss 0.116496, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:03:04.073780: step 2936, loss 0.122609, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:03:04.390739: step 2937, loss 0.141267, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:03:04.757060: step 2938, loss 0.0358273, acc 1, learning_rate 0.00010003
2017-10-10T15:03:05.153955: step 2939, loss 0.181933, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:03:05.516798: step 2940, loss 0.0348351, acc 1, learning_rate 0.000100029
2017-10-10T15:03:05.856930: step 2941, loss 0.0600442, acc 0.984375, learning_rate 0.000100029
2017-10-10T15:03:06.237115: step 2942, loss 0.128296, acc 0.953125, learning_rate 0.000100029
2017-10-10T15:03:06.570504: step 2943, loss 0.123149, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:03:06.905865: step 2944, loss 0.106105, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:03:07.277055: step 2945, loss 0.103638, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:03:07.645001: step 2946, loss 0.124657, acc 0.953125, learning_rate 0.000100029
2017-10-10T15:03:08.016906: step 2947, loss 0.128607, acc 0.9375, learning_rate 0.000100029
2017-10-10T15:03:08.386607: step 2948, loss 0.126423, acc 0.953125, learning_rate 0.000100029
2017-10-10T15:03:08.738266: step 2949, loss 0.107538, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:03:09.122567: step 2950, loss 0.155631, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:03:09.437478: step 2951, loss 0.0713055, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:03:09.819657: step 2952, loss 0.169994, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:03:10.163326: step 2953, loss 0.106961, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:03:10.562537: step 2954, loss 0.0803178, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:03:10.989056: step 2955, loss 0.114068, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:03:11.385036: step 2956, loss 0.146331, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:03:11.776985: step 2957, loss 0.0933739, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:03:12.140824: step 2958, loss 0.102798, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:03:12.547364: step 2959, loss 0.087393, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:03:12.944839: step 2960, loss 0.106956, acc 0.953125, learning_rate 0.000100027

Evaluation:
2017-10-10T15:03:13.676927: step 2960, loss 0.206774, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-2960

2017-10-10T15:03:15.060891: step 2961, loss 0.115562, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:03:15.421313: step 2962, loss 0.157412, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:03:15.838890: step 2963, loss 0.0838354, acc 0.984375, learning_rate 0.000100027
2017-10-10T15:03:16.174411: step 2964, loss 0.0445572, acc 1, learning_rate 0.000100027
2017-10-10T15:03:16.500849: step 2965, loss 0.166928, acc 0.9375, learning_rate 0.000100027
2017-10-10T15:03:16.943844: step 2966, loss 0.211774, acc 0.9375, learning_rate 0.000100027
2017-10-10T15:03:17.287281: step 2967, loss 0.19131, acc 0.9375, learning_rate 0.000100026
2017-10-10T15:03:17.656162: step 2968, loss 0.0483924, acc 1, learning_rate 0.000100026
2017-10-10T15:03:18.040615: step 2969, loss 0.0910536, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:03:18.401230: step 2970, loss 0.104718, acc 0.953125, learning_rate 0.000100026
2017-10-10T15:03:18.861086: step 2971, loss 0.0712102, acc 0.96875, learning_rate 0.000100026
2017-10-10T15:03:19.151268: step 2972, loss 0.220329, acc 0.890625, learning_rate 0.000100026
2017-10-10T15:03:19.524958: step 2973, loss 0.0509507, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:03:19.958099: step 2974, loss 0.0259241, acc 1, learning_rate 0.000100026
2017-10-10T15:03:20.248836: step 2975, loss 0.124913, acc 0.953125, learning_rate 0.000100026
2017-10-10T15:03:20.550519: step 2976, loss 0.0238594, acc 1, learning_rate 0.000100025
2017-10-10T15:03:20.841127: step 2977, loss 0.214728, acc 0.9375, learning_rate 0.000100025
2017-10-10T15:03:21.269093: step 2978, loss 0.184192, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:03:21.668097: step 2979, loss 0.147399, acc 0.9375, learning_rate 0.000100025
2017-10-10T15:03:22.060951: step 2980, loss 0.0765378, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:03:22.426908: step 2981, loss 0.100345, acc 0.96875, learning_rate 0.000100025
2017-10-10T15:03:22.849577: step 2982, loss 0.10913, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:03:23.292443: step 2983, loss 0.222741, acc 0.921875, learning_rate 0.000100025
2017-10-10T15:03:23.685509: step 2984, loss 0.0579087, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:03:24.052929: step 2985, loss 0.178254, acc 0.90625, learning_rate 0.000100025
2017-10-10T15:03:24.423306: step 2986, loss 0.0891402, acc 0.96875, learning_rate 0.000100024
2017-10-10T15:03:24.808944: step 2987, loss 0.09744, acc 0.96875, learning_rate 0.000100024
2017-10-10T15:03:25.225421: step 2988, loss 0.180739, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:03:25.601015: step 2989, loss 0.263258, acc 0.890625, learning_rate 0.000100024
2017-10-10T15:03:26.013046: step 2990, loss 0.20294, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:03:26.392916: step 2991, loss 0.118473, acc 0.96875, learning_rate 0.000100024
2017-10-10T15:03:26.796858: step 2992, loss 0.155331, acc 0.9375, learning_rate 0.000100024
2017-10-10T15:03:27.192173: step 2993, loss 0.03338, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:03:27.558764: step 2994, loss 0.0654157, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:03:27.944964: step 2995, loss 0.0264424, acc 1, learning_rate 0.000100024
2017-10-10T15:03:28.287319: step 2996, loss 0.0855981, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:03:28.626962: step 2997, loss 0.0368773, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:03:29.013373: step 2998, loss 0.0819606, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:03:29.375582: step 2999, loss 0.110212, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:03:29.763503: step 3000, loss 0.0424234, acc 1, learning_rate 0.000100023

Evaluation:
2017-10-10T15:03:30.435856: step 3000, loss 0.207895, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3000

2017-10-10T15:03:31.632883: step 3001, loss 0.156396, acc 0.9375, learning_rate 0.000100023
2017-10-10T15:03:32.114437: step 3002, loss 0.148885, acc 0.9375, learning_rate 0.000100023
2017-10-10T15:03:32.431657: step 3003, loss 0.145516, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:03:32.729126: step 3004, loss 0.0504115, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:03:33.065717: step 3005, loss 0.109498, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:03:33.460839: step 3006, loss 0.123922, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:03:33.839363: step 3007, loss 0.158741, acc 0.90625, learning_rate 0.000100022
2017-10-10T15:03:34.185179: step 3008, loss 0.119016, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:03:34.555219: step 3009, loss 0.0420859, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:03:34.971287: step 3010, loss 0.180562, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:03:35.365025: step 3011, loss 0.0852837, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:03:35.741587: step 3012, loss 0.10816, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:03:36.041700: step 3013, loss 0.0812251, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:03:36.417864: step 3014, loss 0.106504, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:03:36.794096: step 3015, loss 0.146733, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:03:37.169046: step 3016, loss 0.113552, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:03:37.492348: step 3017, loss 0.0713321, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:03:38.005326: step 3018, loss 0.109932, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:03:38.355298: step 3019, loss 0.107667, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:03:38.617795: step 3020, loss 0.0549845, acc 1, learning_rate 0.000100021
2017-10-10T15:03:38.872387: step 3021, loss 0.101354, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:03:39.746304: step 3022, loss 0.128989, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:03:40.128847: step 3023, loss 0.163476, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:03:40.508947: step 3024, loss 0.0761977, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:03:40.841708: step 3025, loss 0.117433, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:03:41.224875: step 3026, loss 0.134479, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:03:41.635294: step 3027, loss 0.0893078, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:03:42.033036: step 3028, loss 0.0787927, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:03:42.405046: step 3029, loss 0.103647, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:03:42.827015: step 3030, loss 0.126435, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:03:43.140850: step 3031, loss 0.0864114, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:03:43.488844: step 3032, loss 0.153778, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:03:43.851997: step 3033, loss 0.0760209, acc 0.984375, learning_rate 0.00010002
2017-10-10T15:03:44.168304: step 3034, loss 0.211825, acc 0.9375, learning_rate 0.00010002
2017-10-10T15:03:44.547827: step 3035, loss 0.0812144, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:03:44.999320: step 3036, loss 0.205931, acc 0.9375, learning_rate 0.00010002
2017-10-10T15:03:45.355473: step 3037, loss 0.0568669, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:03:45.604227: step 3038, loss 0.146933, acc 0.960784, learning_rate 0.00010002
2017-10-10T15:03:45.929046: step 3039, loss 0.0906417, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:03:46.277927: step 3040, loss 0.121893, acc 0.953125, learning_rate 0.00010002

Evaluation:
2017-10-10T15:03:46.996113: step 3040, loss 0.2069, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3040

2017-10-10T15:03:48.260999: step 3041, loss 0.0712687, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:03:48.671929: step 3042, loss 0.083234, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:03:49.100895: step 3043, loss 0.150609, acc 0.9375, learning_rate 0.000100019
2017-10-10T15:03:49.395411: step 3044, loss 0.23858, acc 0.9375, learning_rate 0.000100019
2017-10-10T15:03:49.712617: step 3045, loss 0.0697322, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:03:50.034753: step 3046, loss 0.0735226, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:03:50.360530: step 3047, loss 0.121607, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:03:50.728924: step 3048, loss 0.0673276, acc 1, learning_rate 0.000100019
2017-10-10T15:03:51.072954: step 3049, loss 0.201331, acc 0.90625, learning_rate 0.000100019
2017-10-10T15:03:51.405800: step 3050, loss 0.1033, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:03:51.776580: step 3051, loss 0.0967465, acc 0.953125, learning_rate 0.000100019
2017-10-10T15:03:52.147564: step 3052, loss 0.0758265, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:03:52.553092: step 3053, loss 0.0525725, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:03:52.930009: step 3054, loss 0.237759, acc 0.890625, learning_rate 0.000100018
2017-10-10T15:03:53.296853: step 3055, loss 0.0369135, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:03:53.738168: step 3056, loss 0.0969034, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:03:54.073144: step 3057, loss 0.109628, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:03:54.445090: step 3058, loss 0.0747842, acc 1, learning_rate 0.000100018
2017-10-10T15:03:54.824858: step 3059, loss 0.0955817, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:03:55.210809: step 3060, loss 0.0998335, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:03:55.591211: step 3061, loss 0.073987, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:03:55.922334: step 3062, loss 0.185251, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:03:56.249166: step 3063, loss 0.0890017, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:03:56.684907: step 3064, loss 0.139096, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:03:57.049573: step 3065, loss 0.134032, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:03:57.348981: step 3066, loss 0.0985958, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:03:57.656883: step 3067, loss 0.11556, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:03:58.032945: step 3068, loss 0.0749699, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:03:58.398989: step 3069, loss 0.109117, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:03:58.753321: step 3070, loss 0.136525, acc 0.9375, learning_rate 0.000100017
2017-10-10T15:03:59.161037: step 3071, loss 0.153228, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:03:59.510585: step 3072, loss 0.0952675, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:03:59.892877: step 3073, loss 0.0430102, acc 1, learning_rate 0.000100017
2017-10-10T15:04:00.313607: step 3074, loss 0.101732, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:04:00.660117: step 3075, loss 0.137164, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:04:00.987538: step 3076, loss 0.099987, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:04:01.396136: step 3077, loss 0.279765, acc 0.921875, learning_rate 0.000100017
2017-10-10T15:04:01.781934: step 3078, loss 0.0947963, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:04:02.228901: step 3079, loss 0.145957, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:04:02.616041: step 3080, loss 0.0657135, acc 0.984375, learning_rate 0.000100017

Evaluation:
2017-10-10T15:04:03.314573: step 3080, loss 0.206725, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3080

2017-10-10T15:04:04.736877: step 3081, loss 0.129158, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:04:05.176850: step 3082, loss 0.0736926, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:04:05.520010: step 3083, loss 0.0945136, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:04:05.952838: step 3084, loss 0.0576717, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:04:06.303460: step 3085, loss 0.106415, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:04:06.757285: step 3086, loss 0.102961, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:04:07.128937: step 3087, loss 0.115063, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:04:07.514322: step 3088, loss 0.185269, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:04:07.816402: step 3089, loss 0.133048, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:04:08.173051: step 3090, loss 0.0973206, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:04:08.554392: step 3091, loss 0.192184, acc 0.90625, learning_rate 0.000100016
2017-10-10T15:04:08.913092: step 3092, loss 0.102055, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:04:09.276188: step 3093, loss 0.0687393, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:04:09.714909: step 3094, loss 0.176069, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:04:10.096151: step 3095, loss 0.220856, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:04:10.492993: step 3096, loss 0.134235, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:04:10.914830: step 3097, loss 0.0554389, acc 1, learning_rate 0.000100016
2017-10-10T15:04:11.366322: step 3098, loss 0.0865442, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:04:11.794736: step 3099, loss 0.111176, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:04:12.096835: step 3100, loss 0.152003, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:04:12.411273: step 3101, loss 0.113603, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:04:12.820265: step 3102, loss 0.0608061, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:04:13.214777: step 3103, loss 0.162117, acc 0.90625, learning_rate 0.000100015
2017-10-10T15:04:13.525048: step 3104, loss 0.0699254, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:04:13.868921: step 3105, loss 0.087047, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:04:14.324810: step 3106, loss 0.105986, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:04:14.755009: step 3107, loss 0.0963324, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:04:15.085502: step 3108, loss 0.0349715, acc 1, learning_rate 0.000100015
2017-10-10T15:04:15.418677: step 3109, loss 0.11916, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:04:15.754330: step 3110, loss 0.169728, acc 0.921875, learning_rate 0.000100015
2017-10-10T15:04:16.164789: step 3111, loss 0.0918743, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:04:16.547946: step 3112, loss 0.135119, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:04:16.928568: step 3113, loss 0.0929962, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:04:17.333380: step 3114, loss 0.0924263, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:04:17.703256: step 3115, loss 0.174697, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:04:18.058771: step 3116, loss 0.0966423, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:04:18.448464: step 3117, loss 0.199973, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:04:18.852832: step 3118, loss 0.0773364, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:04:19.201156: step 3119, loss 0.0827587, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:04:19.625922: step 3120, loss 0.209281, acc 0.953125, learning_rate 0.000100014

Evaluation:
2017-10-10T15:04:20.334028: step 3120, loss 0.206605, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3120

2017-10-10T15:04:21.569256: step 3121, loss 0.0688307, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:04:21.988913: step 3122, loss 0.125705, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:04:22.388921: step 3123, loss 0.152865, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:04:22.759297: step 3124, loss 0.121902, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:04:23.057380: step 3125, loss 0.177307, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:04:23.390807: step 3126, loss 0.111038, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:04:23.800902: step 3127, loss 0.0930386, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:04:24.135363: step 3128, loss 0.10952, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:04:24.524435: step 3129, loss 0.12373, acc 0.921875, learning_rate 0.000100014
2017-10-10T15:04:24.933030: step 3130, loss 0.0500692, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:04:25.339883: step 3131, loss 0.108366, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:04:25.680904: step 3132, loss 0.0732735, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:04:25.998860: step 3133, loss 0.105584, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:04:26.331876: step 3134, loss 0.146081, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:04:26.720825: step 3135, loss 0.0912334, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:04:27.014053: step 3136, loss 0.164017, acc 0.921569, learning_rate 0.000100013
2017-10-10T15:04:27.432843: step 3137, loss 0.179112, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:04:27.800827: step 3138, loss 0.0339772, acc 1, learning_rate 0.000100013
2017-10-10T15:04:28.204871: step 3139, loss 0.11153, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:04:28.601032: step 3140, loss 0.141262, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:04:28.976874: step 3141, loss 0.10425, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:04:29.358638: step 3142, loss 0.0729047, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:04:29.753606: step 3143, loss 0.122355, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:04:30.138236: step 3144, loss 0.0420927, acc 1, learning_rate 0.000100013
2017-10-10T15:04:30.500346: step 3145, loss 0.01223, acc 1, learning_rate 0.000100013
2017-10-10T15:04:30.885011: step 3146, loss 0.0847966, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:04:31.295896: step 3147, loss 0.101496, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:04:31.656885: step 3148, loss 0.123915, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:04:32.048936: step 3149, loss 0.0835041, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:04:32.439089: step 3150, loss 0.0698115, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:04:32.696869: step 3151, loss 0.121871, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:04:32.965056: step 3152, loss 0.102492, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:04:33.438160: step 3153, loss 0.046575, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:04:33.778106: step 3154, loss 0.13543, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:04:34.084863: step 3155, loss 0.0960061, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:04:34.426413: step 3156, loss 0.08058, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:04:34.784852: step 3157, loss 0.0749915, acc 1, learning_rate 0.000100012
2017-10-10T15:04:35.186928: step 3158, loss 0.122281, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:04:35.526666: step 3159, loss 0.141979, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:04:35.913724: step 3160, loss 0.144532, acc 0.96875, learning_rate 0.000100012

Evaluation:
2017-10-10T15:04:36.560463: step 3160, loss 0.207136, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3160

2017-10-10T15:04:37.857239: step 3161, loss 0.21409, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:04:38.358142: step 3162, loss 0.104972, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:04:38.740279: step 3163, loss 0.0841713, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:04:39.049145: step 3164, loss 0.11786, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:04:39.424774: step 3165, loss 0.144579, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:04:39.813293: step 3166, loss 0.147241, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:04:40.188379: step 3167, loss 0.206885, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:04:40.575027: step 3168, loss 0.173862, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:04:40.933271: step 3169, loss 0.168441, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:04:41.337284: step 3170, loss 0.0418135, acc 1, learning_rate 0.000100012
2017-10-10T15:04:41.682587: step 3171, loss 0.0903572, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:42.095250: step 3172, loss 0.0868408, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:42.436942: step 3173, loss 0.0913507, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:04:42.814649: step 3174, loss 0.134981, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:43.196863: step 3175, loss 0.141116, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:43.606423: step 3176, loss 0.129511, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:43.982242: step 3177, loss 0.0447772, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:04:44.355711: step 3178, loss 0.0958076, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:44.741647: step 3179, loss 0.169964, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:04:45.087066: step 3180, loss 0.140986, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:04:45.446925: step 3181, loss 0.0503522, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:04:45.827407: step 3182, loss 0.116356, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:04:46.214250: step 3183, loss 0.106482, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:46.588711: step 3184, loss 0.108465, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:04:46.956479: step 3185, loss 0.0960008, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:04:47.315721: step 3186, loss 0.10689, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:47.720848: step 3187, loss 0.0550997, acc 1, learning_rate 0.000100011
2017-10-10T15:04:48.136947: step 3188, loss 0.160992, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:04:48.478843: step 3189, loss 0.0859166, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:04:48.918810: step 3190, loss 0.169692, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:04:49.252967: step 3191, loss 0.165295, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:04:49.608918: step 3192, loss 0.0664329, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:04:50.052884: step 3193, loss 0.102459, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:04:50.440922: step 3194, loss 0.0961026, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:04:50.747963: step 3195, loss 0.0925413, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:04:51.058780: step 3196, loss 0.160277, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:04:51.405273: step 3197, loss 0.0551538, acc 1, learning_rate 0.00010001
2017-10-10T15:04:51.801002: step 3198, loss 0.121336, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:04:52.256920: step 3199, loss 0.0393882, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:04:52.581856: step 3200, loss 0.151036, acc 0.9375, learning_rate 0.00010001

Evaluation:
2017-10-10T15:04:53.280441: step 3200, loss 0.206911, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3200

2017-10-10T15:04:54.832607: step 3201, loss 0.12796, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:04:55.197271: step 3202, loss 0.0429736, acc 1, learning_rate 0.00010001
2017-10-10T15:04:55.605109: step 3203, loss 0.0608072, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:04:55.967237: step 3204, loss 0.12392, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:04:56.387996: step 3205, loss 0.0759393, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:04:56.718096: step 3206, loss 0.207271, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:04:57.049084: step 3207, loss 0.169248, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:04:57.393888: step 3208, loss 0.0507667, acc 1, learning_rate 0.00010001
2017-10-10T15:04:57.788939: step 3209, loss 0.128289, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:04:58.188561: step 3210, loss 0.136743, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:04:58.589707: step 3211, loss 0.146572, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:04:58.942875: step 3212, loss 0.0820114, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:04:59.344859: step 3213, loss 0.0712601, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:04:59.781105: step 3214, loss 0.131868, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:05:00.148158: step 3215, loss 0.110972, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:05:00.569001: step 3216, loss 0.0424619, acc 1, learning_rate 0.00010001
2017-10-10T15:05:00.946516: step 3217, loss 0.140865, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:01.336954: step 3218, loss 0.151772, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:05:01.663722: step 3219, loss 0.0624375, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:05:02.044891: step 3220, loss 0.0848723, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:02.407537: step 3221, loss 0.145264, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:02.815532: step 3222, loss 0.091887, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:03.198793: step 3223, loss 0.040824, acc 1, learning_rate 0.000100009
2017-10-10T15:05:03.528886: step 3224, loss 0.0249584, acc 1, learning_rate 0.000100009
2017-10-10T15:05:03.863991: step 3225, loss 0.125177, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:04.229013: step 3226, loss 0.0519561, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:05:04.590810: step 3227, loss 0.292055, acc 0.921875, learning_rate 0.000100009
2017-10-10T15:05:04.988829: step 3228, loss 0.0602608, acc 1, learning_rate 0.000100009
2017-10-10T15:05:05.385118: step 3229, loss 0.0815846, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:05.707416: step 3230, loss 0.151886, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:06.030835: step 3231, loss 0.0695285, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:06.436870: step 3232, loss 0.162367, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:05:06.864799: step 3233, loss 0.145636, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:07.156838: step 3234, loss 0.166131, acc 0.941176, learning_rate 0.000100009
2017-10-10T15:05:07.484354: step 3235, loss 0.0889793, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:05:07.849877: step 3236, loss 0.0996002, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:08.240077: step 3237, loss 0.0838239, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:05:08.668117: step 3238, loss 0.0486322, acc 1, learning_rate 0.000100009
2017-10-10T15:05:08.964137: step 3239, loss 0.062605, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:05:09.240938: step 3240, loss 0.129979, acc 0.953125, learning_rate 0.000100009

Evaluation:
2017-10-10T15:05:09.976909: step 3240, loss 0.204877, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3240

2017-10-10T15:05:11.292564: step 3241, loss 0.141554, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:11.626441: step 3242, loss 0.0880713, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:12.021015: step 3243, loss 0.0658171, acc 1, learning_rate 0.000100009
2017-10-10T15:05:12.338290: step 3244, loss 0.0989773, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:12.716873: step 3245, loss 0.137738, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:05:13.111146: step 3246, loss 0.0738718, acc 1, learning_rate 0.000100008
2017-10-10T15:05:13.482474: step 3247, loss 0.184824, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:05:13.868849: step 3248, loss 0.0982318, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:05:14.192223: step 3249, loss 0.0879683, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:05:14.569049: step 3250, loss 0.04254, acc 1, learning_rate 0.000100008
2017-10-10T15:05:14.977101: step 3251, loss 0.0971214, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:05:15.379477: step 3252, loss 0.217486, acc 0.90625, learning_rate 0.000100008
2017-10-10T15:05:15.747976: step 3253, loss 0.0961824, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:05:16.113073: step 3254, loss 0.249295, acc 0.90625, learning_rate 0.000100008
2017-10-10T15:05:16.478659: step 3255, loss 0.154453, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:05:16.837364: step 3256, loss 0.0760516, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:05:17.212804: step 3257, loss 0.124335, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:05:17.638066: step 3258, loss 0.0756606, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:05:18.046307: step 3259, loss 0.188733, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:05:18.437114: step 3260, loss 0.151719, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:05:18.822842: step 3261, loss 0.121039, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:05:19.106348: step 3262, loss 0.0705703, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:05:19.479785: step 3263, loss 0.0824492, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:05:19.814371: step 3264, loss 0.147045, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:05:20.213071: step 3265, loss 0.113269, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:05:20.673820: step 3266, loss 0.0500048, acc 1, learning_rate 0.000100008
2017-10-10T15:05:21.031775: step 3267, loss 0.170402, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:05:21.360898: step 3268, loss 0.058134, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:05:21.704957: step 3269, loss 0.0903987, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:05:22.120567: step 3270, loss 0.180769, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:05:22.488637: step 3271, loss 0.0590298, acc 1, learning_rate 0.000100008
2017-10-10T15:05:22.844869: step 3272, loss 0.0877526, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:05:23.245012: step 3273, loss 0.0305227, acc 1, learning_rate 0.000100008
2017-10-10T15:05:23.619571: step 3274, loss 0.0576492, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:05:23.920252: step 3275, loss 0.125078, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:05:24.309206: step 3276, loss 0.176812, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:05:24.704954: step 3277, loss 0.087015, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:25.113137: step 3278, loss 0.0915074, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:05:25.508445: step 3279, loss 0.185256, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:05:25.898777: step 3280, loss 0.0800555, acc 0.953125, learning_rate 0.000100007

Evaluation:
2017-10-10T15:05:26.800806: step 3280, loss 0.20748, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3280

2017-10-10T15:05:28.016944: step 3281, loss 0.142783, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:28.348847: step 3282, loss 0.0870785, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:28.774922: step 3283, loss 0.131583, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:05:29.163572: step 3284, loss 0.0771759, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:05:29.535568: step 3285, loss 0.135072, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:05:29.934755: step 3286, loss 0.172367, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:05:30.341557: step 3287, loss 0.0816449, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:05:30.719084: step 3288, loss 0.0678414, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:05:31.144083: step 3289, loss 0.141793, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:05:31.524863: step 3290, loss 0.0738056, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:31.940906: step 3291, loss 0.118415, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:05:32.303954: step 3292, loss 0.178603, acc 0.90625, learning_rate 0.000100007
2017-10-10T15:05:32.628758: step 3293, loss 0.0596048, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:05:33.000304: step 3294, loss 0.157075, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:05:33.398316: step 3295, loss 0.109293, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:05:33.736843: step 3296, loss 0.0882576, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:05:34.113874: step 3297, loss 0.0756262, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:05:34.470278: step 3298, loss 0.0832316, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:34.863933: step 3299, loss 0.116468, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:35.225899: step 3300, loss 0.0875609, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:35.645836: step 3301, loss 0.137861, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:05:36.025964: step 3302, loss 0.128605, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:36.425478: step 3303, loss 0.117039, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:05:36.822678: step 3304, loss 0.122395, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:05:37.201695: step 3305, loss 0.130451, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:05:37.615954: step 3306, loss 0.0759002, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:05:37.976844: step 3307, loss 0.0430788, acc 1, learning_rate 0.000100007
2017-10-10T15:05:38.396862: step 3308, loss 0.0792238, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:05:38.704590: step 3309, loss 0.180882, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:05:39.101155: step 3310, loss 0.156831, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:05:39.421074: step 3311, loss 0.0791919, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:39.872834: step 3312, loss 0.113185, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:05:40.221321: step 3313, loss 0.148042, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:05:40.503940: step 3314, loss 0.0684481, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:40.896981: step 3315, loss 0.143909, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:05:41.288970: step 3316, loss 0.0679218, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:41.733014: step 3317, loss 0.059521, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:42.036982: step 3318, loss 0.135417, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:05:42.416560: step 3319, loss 0.0491488, acc 1, learning_rate 0.000100006
2017-10-10T15:05:42.801121: step 3320, loss 0.0889787, acc 0.953125, learning_rate 0.000100006

Evaluation:
2017-10-10T15:05:43.529716: step 3320, loss 0.205654, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3320

2017-10-10T15:05:44.744973: step 3321, loss 0.0835196, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:45.058934: step 3322, loss 0.134339, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:05:45.557954: step 3323, loss 0.096795, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:45.764276: step 3324, loss 0.0810752, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:46.093050: step 3325, loss 0.0693683, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:46.424581: step 3326, loss 0.117973, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:05:46.854318: step 3327, loss 0.0944579, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:05:47.256810: step 3328, loss 0.0738288, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:47.633012: step 3329, loss 0.0680113, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:48.012429: step 3330, loss 0.193971, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:48.380418: step 3331, loss 0.237049, acc 0.90625, learning_rate 0.000100006
2017-10-10T15:05:48.716216: step 3332, loss 0.224589, acc 0.901961, learning_rate 0.000100006
2017-10-10T15:05:49.075893: step 3333, loss 0.0598965, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:49.464918: step 3334, loss 0.194403, acc 0.90625, learning_rate 0.000100006
2017-10-10T15:05:49.849047: step 3335, loss 0.0909143, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:05:50.236108: step 3336, loss 0.158059, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:05:50.664957: step 3337, loss 0.153362, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:05:51.012030: step 3338, loss 0.107995, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:05:51.354761: step 3339, loss 0.110407, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:51.740100: step 3340, loss 0.0754911, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:52.059192: step 3341, loss 0.0505937, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:52.483513: step 3342, loss 0.0550325, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:52.875533: step 3343, loss 0.117437, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:05:53.297172: step 3344, loss 0.0702018, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:53.739963: step 3345, loss 0.0672592, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:54.104364: step 3346, loss 0.155533, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:05:54.424913: step 3347, loss 0.0807214, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:05:54.763439: step 3348, loss 0.098242, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:55.156375: step 3349, loss 0.0650946, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:05:55.525012: step 3350, loss 0.0327295, acc 1, learning_rate 0.000100006
2017-10-10T15:05:55.897171: step 3351, loss 0.110632, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:05:56.257689: step 3352, loss 0.0884438, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:05:56.718329: step 3353, loss 0.0507481, acc 1, learning_rate 0.000100005
2017-10-10T15:05:57.097528: step 3354, loss 0.0972914, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:05:57.497011: step 3355, loss 0.0925949, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:05:57.975618: step 3356, loss 0.209533, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:05:58.362096: step 3357, loss 0.0859029, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:05:58.769041: step 3358, loss 0.119481, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:05:59.145636: step 3359, loss 0.065137, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:05:59.485057: step 3360, loss 0.147366, acc 0.9375, learning_rate 0.000100005

Evaluation:
2017-10-10T15:06:00.206237: step 3360, loss 0.20743, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3360

2017-10-10T15:06:01.527089: step 3361, loss 0.117259, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:01.973533: step 3362, loss 0.118449, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:02.292851: step 3363, loss 0.0777665, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:02.754509: step 3364, loss 0.17424, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:06:03.075518: step 3365, loss 0.0617028, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:03.412673: step 3366, loss 0.0839909, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:03.711642: step 3367, loss 0.0649756, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:04.102793: step 3368, loss 0.138729, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:04.463288: step 3369, loss 0.0491894, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:04.825209: step 3370, loss 0.0686632, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:05.141031: step 3371, loss 0.195982, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:06:05.525607: step 3372, loss 0.0516447, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:05.890620: step 3373, loss 0.138854, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:06.270169: step 3374, loss 0.124498, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:06.667682: step 3375, loss 0.111934, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:07.067746: step 3376, loss 0.0559471, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:07.461131: step 3377, loss 0.128103, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:07.793078: step 3378, loss 0.18541, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:08.155016: step 3379, loss 0.0703108, acc 1, learning_rate 0.000100005
2017-10-10T15:06:08.512843: step 3380, loss 0.115098, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:08.873284: step 3381, loss 0.103806, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:09.256661: step 3382, loss 0.102345, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:09.639197: step 3383, loss 0.0908781, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:10.009151: step 3384, loss 0.0805189, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:10.458394: step 3385, loss 0.116567, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:10.798800: step 3386, loss 0.215911, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:06:11.140636: step 3387, loss 0.0345804, acc 1, learning_rate 0.000100005
2017-10-10T15:06:11.496833: step 3388, loss 0.0498923, acc 1, learning_rate 0.000100005
2017-10-10T15:06:11.852177: step 3389, loss 0.14946, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:12.208419: step 3390, loss 0.16882, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:12.569085: step 3391, loss 0.083262, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:12.895001: step 3392, loss 0.204549, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:13.236018: step 3393, loss 0.214049, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:06:13.633098: step 3394, loss 0.134068, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:14.024196: step 3395, loss 0.110305, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:14.433810: step 3396, loss 0.119743, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:14.816910: step 3397, loss 0.135843, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:15.172854: step 3398, loss 0.133042, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:15.495725: step 3399, loss 0.0446551, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:15.899137: step 3400, loss 0.134455, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-10T15:06:16.631880: step 3400, loss 0.208007, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3400

2017-10-10T15:06:17.940852: step 3401, loss 0.0449288, acc 1, learning_rate 0.000100004
2017-10-10T15:06:18.330261: step 3402, loss 0.128805, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:18.720816: step 3403, loss 0.0736485, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:19.124677: step 3404, loss 0.0849953, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:19.532852: step 3405, loss 0.165311, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:19.873336: step 3406, loss 0.249119, acc 0.90625, learning_rate 0.000100004
2017-10-10T15:06:20.197635: step 3407, loss 0.119807, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:20.572666: step 3408, loss 0.0736467, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:20.976872: step 3409, loss 0.133186, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:06:21.415960: step 3410, loss 0.22153, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:21.752621: step 3411, loss 0.0961142, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:22.058249: step 3412, loss 0.0988602, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:22.425395: step 3413, loss 0.0345588, acc 1, learning_rate 0.000100004
2017-10-10T15:06:22.767437: step 3414, loss 0.0833901, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:23.172920: step 3415, loss 0.0368292, acc 1, learning_rate 0.000100004
2017-10-10T15:06:23.556950: step 3416, loss 0.0669668, acc 1, learning_rate 0.000100004
2017-10-10T15:06:23.856889: step 3417, loss 0.103692, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:24.284566: step 3418, loss 0.0989477, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:24.586850: step 3419, loss 0.156995, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:25.025233: step 3420, loss 0.152462, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:25.384621: step 3421, loss 0.173114, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:25.819254: step 3422, loss 0.0873799, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:26.163095: step 3423, loss 0.138059, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:26.532534: step 3424, loss 0.0641923, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:26.896269: step 3425, loss 0.13964, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:27.273401: step 3426, loss 0.257605, acc 0.890625, learning_rate 0.000100004
2017-10-10T15:06:27.664389: step 3427, loss 0.0957441, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:28.086483: step 3428, loss 0.055514, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:28.464803: step 3429, loss 0.127606, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:28.813740: step 3430, loss 0.257283, acc 0.921569, learning_rate 0.000100004
2017-10-10T15:06:29.203728: step 3431, loss 0.0518341, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:29.577078: step 3432, loss 0.0913542, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:30.010382: step 3433, loss 0.211547, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:06:30.398535: step 3434, loss 0.0723461, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:30.796808: step 3435, loss 0.149793, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:31.202237: step 3436, loss 0.0644547, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:31.537364: step 3437, loss 0.103841, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:31.909169: step 3438, loss 0.123774, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:32.272227: step 3439, loss 0.0490851, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:32.700883: step 3440, loss 0.13106, acc 0.9375, learning_rate 0.000100004

Evaluation:
2017-10-10T15:06:33.465021: step 3440, loss 0.206677, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3440

2017-10-10T15:06:34.714767: step 3441, loss 0.213298, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:35.042738: step 3442, loss 0.189696, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:35.375491: step 3443, loss 0.0847444, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:35.776997: step 3444, loss 0.203928, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:06:36.106356: step 3445, loss 0.0958978, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:36.483632: step 3446, loss 0.120204, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:36.839589: step 3447, loss 0.127928, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:06:37.214131: step 3448, loss 0.152275, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:37.556929: step 3449, loss 0.0980228, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:37.948869: step 3450, loss 0.0858554, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:38.336989: step 3451, loss 0.0366742, acc 1, learning_rate 0.000100004
2017-10-10T15:06:38.742109: step 3452, loss 0.125259, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:39.172966: step 3453, loss 0.0276481, acc 1, learning_rate 0.000100004
2017-10-10T15:06:39.484226: step 3454, loss 0.0999357, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:39.751334: step 3455, loss 0.0356237, acc 1, learning_rate 0.000100004
2017-10-10T15:06:40.036884: step 3456, loss 0.105368, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:40.444371: step 3457, loss 0.0623095, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:40.808938: step 3458, loss 0.120582, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:41.133037: step 3459, loss 0.0450701, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:41.449066: step 3460, loss 0.138513, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:41.820620: step 3461, loss 0.0673317, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:42.211651: step 3462, loss 0.0843168, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:42.607802: step 3463, loss 0.158189, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:06:42.975120: step 3464, loss 0.182423, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:06:43.340898: step 3465, loss 0.118997, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:06:43.684949: step 3466, loss 0.17186, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:06:44.039094: step 3467, loss 0.13245, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:06:44.388956: step 3468, loss 0.1368, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:06:44.760847: step 3469, loss 0.0445075, acc 1, learning_rate 0.000100003
2017-10-10T15:06:45.129053: step 3470, loss 0.10475, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:06:45.582559: step 3471, loss 0.0810104, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:45.912897: step 3472, loss 0.0799844, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:46.170727: step 3473, loss 0.0633701, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:46.522681: step 3474, loss 0.127705, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:06:46.975221: step 3475, loss 0.0731903, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:47.346759: step 3476, loss 0.0682563, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:47.689828: step 3477, loss 0.16237, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:06:48.026427: step 3478, loss 0.0831069, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:06:48.398956: step 3479, loss 0.140605, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:06:48.762469: step 3480, loss 0.201834, acc 0.9375, learning_rate 0.000100003

Evaluation:
2017-10-10T15:06:49.460981: step 3480, loss 0.204962, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3480

2017-10-10T15:06:50.780850: step 3481, loss 0.0718124, acc 1, learning_rate 0.000100003
2017-10-10T15:06:51.169985: step 3482, loss 0.151089, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:06:51.563495: step 3483, loss 0.0704026, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:51.890239: step 3484, loss 0.118431, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:06:52.251721: step 3485, loss 0.0780109, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:06:52.701908: step 3486, loss 0.0396528, acc 1, learning_rate 0.000100003
2017-10-10T15:06:53.085786: step 3487, loss 0.104539, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:06:53.415430: step 3488, loss 0.111596, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:06:53.801198: step 3489, loss 0.0316362, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:54.215520: step 3490, loss 0.11751, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:54.596890: step 3491, loss 0.152603, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:06:54.980987: step 3492, loss 0.0835911, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:06:55.429086: step 3493, loss 0.0735157, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:55.800910: step 3494, loss 0.164995, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:06:56.179569: step 3495, loss 0.104525, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:06:56.535547: step 3496, loss 0.0390153, acc 1, learning_rate 0.000100003
2017-10-10T15:06:56.888753: step 3497, loss 0.0595637, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:06:57.301024: step 3498, loss 0.207232, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:06:57.690445: step 3499, loss 0.0343734, acc 1, learning_rate 0.000100003
2017-10-10T15:06:58.040291: step 3500, loss 0.146332, acc 0.90625, learning_rate 0.000100003
2017-10-10T15:06:58.308946: step 3501, loss 0.185798, acc 0.90625, learning_rate 0.000100003
2017-10-10T15:06:58.674255: step 3502, loss 0.161246, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:06:59.021766: step 3503, loss 0.0844792, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:06:59.477479: step 3504, loss 0.175674, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:06:59.843658: step 3505, loss 0.175894, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:00.176241: step 3506, loss 0.111078, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:00.593264: step 3507, loss 0.0598918, acc 1, learning_rate 0.000100003
2017-10-10T15:07:00.965386: step 3508, loss 0.034332, acc 1, learning_rate 0.000100003
2017-10-10T15:07:01.329922: step 3509, loss 0.132014, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:01.736357: step 3510, loss 0.111493, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:02.092899: step 3511, loss 0.119732, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:02.468377: step 3512, loss 0.218255, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:02.857746: step 3513, loss 0.0845995, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:03.144095: step 3514, loss 0.236838, acc 0.890625, learning_rate 0.000100003
2017-10-10T15:07:03.558837: step 3515, loss 0.163791, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:03.921829: step 3516, loss 0.085371, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:04.262914: step 3517, loss 0.0380085, acc 1, learning_rate 0.000100003
2017-10-10T15:07:04.640970: step 3518, loss 0.118197, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:05.028157: step 3519, loss 0.120025, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:05.537076: step 3520, loss 0.07236, acc 0.984375, learning_rate 0.000100003

Evaluation:
2017-10-10T15:07:06.197810: step 3520, loss 0.206586, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3520

2017-10-10T15:07:07.485760: step 3521, loss 0.200417, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:07.857129: step 3522, loss 0.123478, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:08.223409: step 3523, loss 0.088476, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:08.620777: step 3524, loss 0.157458, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:09.024853: step 3525, loss 0.140976, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:09.386033: step 3526, loss 0.0631631, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:09.786472: step 3527, loss 0.116591, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:10.084832: step 3528, loss 0.0262027, acc 1, learning_rate 0.000100003
2017-10-10T15:07:10.444070: step 3529, loss 0.152452, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:10.777730: step 3530, loss 0.0600294, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:11.165389: step 3531, loss 0.188388, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:11.544856: step 3532, loss 0.0240536, acc 1, learning_rate 0.000100003
2017-10-10T15:07:11.900755: step 3533, loss 0.14762, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:12.624897: step 3534, loss 0.131743, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:12.957763: step 3535, loss 0.0590165, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:13.305082: step 3536, loss 0.0596652, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:13.635680: step 3537, loss 0.0817125, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:14.000851: step 3538, loss 0.111011, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:14.360842: step 3539, loss 0.137556, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:14.789118: step 3540, loss 0.127795, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:15.199372: step 3541, loss 0.187448, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:15.579565: step 3542, loss 0.12537, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:15.999536: step 3543, loss 0.0662464, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:16.444095: step 3544, loss 0.132735, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:16.792814: step 3545, loss 0.0711632, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:17.141183: step 3546, loss 0.143116, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:17.532665: step 3547, loss 0.0696683, acc 1, learning_rate 0.000100002
2017-10-10T15:07:17.946604: step 3548, loss 0.0609398, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:18.376897: step 3549, loss 0.0927064, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:18.764913: step 3550, loss 0.0582488, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:19.228362: step 3551, loss 0.103428, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:19.521428: step 3552, loss 0.09667, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:19.828857: step 3553, loss 0.14612, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:20.157509: step 3554, loss 0.0565487, acc 1, learning_rate 0.000100002
2017-10-10T15:07:20.547834: step 3555, loss 0.0800817, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:20.933039: step 3556, loss 0.0442898, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:21.365021: step 3557, loss 0.234822, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:07:21.737090: step 3558, loss 0.119041, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:22.108341: step 3559, loss 0.0875395, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:22.504825: step 3560, loss 0.0496762, acc 1, learning_rate 0.000100002

Evaluation:
2017-10-10T15:07:23.252881: step 3560, loss 0.208344, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3560

2017-10-10T15:07:24.527640: step 3561, loss 0.0493526, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:24.900948: step 3562, loss 0.155197, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:25.264337: step 3563, loss 0.13602, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:25.613037: step 3564, loss 0.0595651, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:25.985923: step 3565, loss 0.112016, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:26.306361: step 3566, loss 0.0870872, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:26.717057: step 3567, loss 0.0905623, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:27.076008: step 3568, loss 0.0841719, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:27.464863: step 3569, loss 0.171755, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:27.864910: step 3570, loss 0.0995956, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:28.277886: step 3571, loss 0.150132, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:28.704927: step 3572, loss 0.0828605, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:29.067672: step 3573, loss 0.154316, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:29.464568: step 3574, loss 0.192636, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:29.852932: step 3575, loss 0.112215, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:30.226727: step 3576, loss 0.0893437, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:30.528139: step 3577, loss 0.15072, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:30.912349: step 3578, loss 0.0884612, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:31.300945: step 3579, loss 0.0816813, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:31.709008: step 3580, loss 0.0584126, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:32.176981: step 3581, loss 0.11557, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:32.544925: step 3582, loss 0.0587111, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:32.868991: step 3583, loss 0.118309, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:33.203910: step 3584, loss 0.14363, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:33.556416: step 3585, loss 0.048364, acc 1, learning_rate 0.000100002
2017-10-10T15:07:33.872992: step 3586, loss 0.170499, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:34.284552: step 3587, loss 0.0536632, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:34.687959: step 3588, loss 0.0951513, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:35.000344: step 3589, loss 0.0568182, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:35.320798: step 3590, loss 0.0909364, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:35.623082: step 3591, loss 0.0628715, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:36.002302: step 3592, loss 0.103357, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:36.418050: step 3593, loss 0.124099, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:36.760856: step 3594, loss 0.053225, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:37.083442: step 3595, loss 0.0693976, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:37.311171: step 3596, loss 0.127155, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:37.780982: step 3597, loss 0.213021, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:38.160826: step 3598, loss 0.0841115, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:38.538974: step 3599, loss 0.0759615, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:38.853210: step 3600, loss 0.0921303, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T15:07:39.541567: step 3600, loss 0.204555, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3600

2017-10-10T15:07:40.880986: step 3601, loss 0.0798446, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:41.206390: step 3602, loss 0.100594, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:41.616273: step 3603, loss 0.104193, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:41.980950: step 3604, loss 0.0371704, acc 1, learning_rate 0.000100002
2017-10-10T15:07:42.392868: step 3605, loss 0.107079, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:42.815370: step 3606, loss 0.104863, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:43.192845: step 3607, loss 0.145282, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:43.573199: step 3608, loss 0.0845209, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:43.970908: step 3609, loss 0.10422, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:44.350522: step 3610, loss 0.067235, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:44.655294: step 3611, loss 0.113311, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:44.993328: step 3612, loss 0.0658075, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:45.346210: step 3613, loss 0.0636812, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:45.805378: step 3614, loss 0.0962798, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:46.181911: step 3615, loss 0.111274, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:46.532859: step 3616, loss 0.116237, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:46.864910: step 3617, loss 0.149186, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:47.234611: step 3618, loss 0.103849, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:47.542942: step 3619, loss 0.0583504, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:47.932918: step 3620, loss 0.12284, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:48.318278: step 3621, loss 0.0781675, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:48.709240: step 3622, loss 0.0612601, acc 1, learning_rate 0.000100002
2017-10-10T15:07:49.053847: step 3623, loss 0.138842, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:49.396039: step 3624, loss 0.141528, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:49.798922: step 3625, loss 0.103417, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:50.116387: step 3626, loss 0.258364, acc 0.901961, learning_rate 0.000100002
2017-10-10T15:07:50.496475: step 3627, loss 0.142615, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:50.860941: step 3628, loss 0.112129, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:51.310632: step 3629, loss 0.0537402, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:51.669240: step 3630, loss 0.157759, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:52.017087: step 3631, loss 0.098098, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:52.464762: step 3632, loss 0.113999, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:52.852070: step 3633, loss 0.0709794, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:53.274879: step 3634, loss 0.0940405, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:53.541944: step 3635, loss 0.0853327, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:53.881472: step 3636, loss 0.210011, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:54.187004: step 3637, loss 0.0297712, acc 1, learning_rate 0.000100002
2017-10-10T15:07:54.554866: step 3638, loss 0.131732, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:54.922652: step 3639, loss 0.0802058, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:55.269045: step 3640, loss 0.035371, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T15:07:55.989428: step 3640, loss 0.203768, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3640

2017-10-10T15:07:57.252205: step 3641, loss 0.15358, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:57.584870: step 3642, loss 0.176838, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:57.970965: step 3643, loss 0.122353, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:58.347966: step 3644, loss 0.110231, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:58.689349: step 3645, loss 0.0492695, acc 1, learning_rate 0.000100002
2017-10-10T15:07:59.061692: step 3646, loss 0.0783974, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:59.389160: step 3647, loss 0.0726135, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:59.725835: step 3648, loss 0.0817485, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:00.145027: step 3649, loss 0.1979, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:08:00.564830: step 3650, loss 0.0718998, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:00.997818: step 3651, loss 0.0698449, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:01.328817: step 3652, loss 0.116414, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:01.725676: step 3653, loss 0.0735849, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:02.084904: step 3654, loss 0.124863, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:02.433881: step 3655, loss 0.111738, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:02.869343: step 3656, loss 0.125583, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:03.301030: step 3657, loss 0.0952289, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:03.704924: step 3658, loss 0.0832268, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:04.065181: step 3659, loss 0.0717877, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:04.385181: step 3660, loss 0.0687741, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:04.711913: step 3661, loss 0.0801284, acc 1, learning_rate 0.000100002
2017-10-10T15:08:05.136181: step 3662, loss 0.146252, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:05.499406: step 3663, loss 0.0683632, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:05.880854: step 3664, loss 0.109188, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:06.210435: step 3665, loss 0.108272, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:06.577897: step 3666, loss 0.246852, acc 0.890625, learning_rate 0.000100002
2017-10-10T15:08:06.962839: step 3667, loss 0.0714724, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:07.381150: step 3668, loss 0.0750167, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:07.804390: step 3669, loss 0.0469885, acc 1, learning_rate 0.000100001
2017-10-10T15:08:08.220923: step 3670, loss 0.0592337, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:08.676851: step 3671, loss 0.0463313, acc 1, learning_rate 0.000100001
2017-10-10T15:08:09.046607: step 3672, loss 0.101706, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:09.484912: step 3673, loss 0.14737, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:09.861535: step 3674, loss 0.0831795, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:10.247799: step 3675, loss 0.067112, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:10.648874: step 3676, loss 0.0972857, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:11.056895: step 3677, loss 0.0851087, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:11.481012: step 3678, loss 0.102237, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:11.810462: step 3679, loss 0.0666274, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:12.128076: step 3680, loss 0.0745854, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:08:12.933037: step 3680, loss 0.205096, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3680

2017-10-10T15:08:14.020990: step 3681, loss 0.0444215, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:14.423934: step 3682, loss 0.100339, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:14.767967: step 3683, loss 0.143633, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:15.116820: step 3684, loss 0.12324, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:15.494664: step 3685, loss 0.102258, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:15.940550: step 3686, loss 0.105396, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:16.320525: step 3687, loss 0.157888, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:16.720774: step 3688, loss 0.122729, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:17.079384: step 3689, loss 0.168805, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:17.487177: step 3690, loss 0.105475, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:17.812915: step 3691, loss 0.162988, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:18.181787: step 3692, loss 0.0991477, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:18.548506: step 3693, loss 0.0838329, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:18.882111: step 3694, loss 0.177028, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:19.274985: step 3695, loss 0.0976426, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:19.710296: step 3696, loss 0.210926, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:20.104580: step 3697, loss 0.106389, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:20.488923: step 3698, loss 0.0854623, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:20.844991: step 3699, loss 0.145321, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:21.207921: step 3700, loss 0.0822097, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:21.571616: step 3701, loss 0.0926273, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:21.916894: step 3702, loss 0.144668, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:22.281924: step 3703, loss 0.0724868, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:22.660841: step 3704, loss 0.170052, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:23.040847: step 3705, loss 0.0708107, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:23.375197: step 3706, loss 0.111313, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:23.729043: step 3707, loss 0.0496042, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:24.072512: step 3708, loss 0.101456, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:24.486822: step 3709, loss 0.0491718, acc 1, learning_rate 0.000100001
2017-10-10T15:08:24.887956: step 3710, loss 0.0810683, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:25.288990: step 3711, loss 0.130286, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:25.748922: step 3712, loss 0.129292, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:26.104827: step 3713, loss 0.030172, acc 1, learning_rate 0.000100001
2017-10-10T15:08:26.361016: step 3714, loss 0.0637402, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:26.744624: step 3715, loss 0.110894, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:27.129012: step 3716, loss 0.123543, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:27.545069: step 3717, loss 0.0398042, acc 1, learning_rate 0.000100001
2017-10-10T15:08:27.912946: step 3718, loss 0.0923661, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:28.261306: step 3719, loss 0.0714019, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:28.605153: step 3720, loss 0.144938, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T15:08:29.327288: step 3720, loss 0.203106, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3720

2017-10-10T15:08:30.927396: step 3721, loss 0.0877178, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:31.270817: step 3722, loss 0.0920208, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:31.637694: step 3723, loss 0.0874247, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:31.933703: step 3724, loss 0.133593, acc 0.941176, learning_rate 0.000100001
2017-10-10T15:08:32.325249: step 3725, loss 0.102066, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:32.736047: step 3726, loss 0.105002, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:33.160919: step 3727, loss 0.136754, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:33.556990: step 3728, loss 0.102197, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:33.961743: step 3729, loss 0.143075, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:34.345969: step 3730, loss 0.136137, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:34.732634: step 3731, loss 0.139165, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:35.060293: step 3732, loss 0.170516, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:35.441469: step 3733, loss 0.174282, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:35.838262: step 3734, loss 0.0370791, acc 1, learning_rate 0.000100001
2017-10-10T15:08:36.278386: step 3735, loss 0.107708, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:36.694012: step 3736, loss 0.116318, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:37.024390: step 3737, loss 0.0516649, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:37.416861: step 3738, loss 0.070516, acc 1, learning_rate 0.000100001
2017-10-10T15:08:37.763454: step 3739, loss 0.0951393, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:38.101081: step 3740, loss 0.170752, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:38.468404: step 3741, loss 0.138118, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:38.872306: step 3742, loss 0.24027, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:08:39.220958: step 3743, loss 0.110509, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:39.530977: step 3744, loss 0.110697, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:39.856954: step 3745, loss 0.0914576, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:40.262711: step 3746, loss 0.102463, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:40.640938: step 3747, loss 0.0761565, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:41.057753: step 3748, loss 0.117033, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:41.403915: step 3749, loss 0.111653, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:41.774165: step 3750, loss 0.0847205, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:42.174493: step 3751, loss 0.138968, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:42.583603: step 3752, loss 0.124325, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:42.909631: step 3753, loss 0.110677, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:43.315205: step 3754, loss 0.137658, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:43.736909: step 3755, loss 0.0464635, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:44.098857: step 3756, loss 0.0754343, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:44.460920: step 3757, loss 0.0963799, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:44.893047: step 3758, loss 0.0682988, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:45.212464: step 3759, loss 0.19259, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:45.600529: step 3760, loss 0.0736184, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T15:08:46.353229: step 3760, loss 0.204911, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3760

2017-10-10T15:08:47.805899: step 3761, loss 0.113049, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:48.072839: step 3762, loss 0.0644565, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:48.388850: step 3763, loss 0.101937, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:48.732971: step 3764, loss 0.126583, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:49.135655: step 3765, loss 0.0708796, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:49.569480: step 3766, loss 0.106346, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:49.898791: step 3767, loss 0.0664119, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:50.237094: step 3768, loss 0.0875117, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:50.588436: step 3769, loss 0.0945862, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:50.938515: step 3770, loss 0.155064, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:51.312938: step 3771, loss 0.0697177, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:51.690031: step 3772, loss 0.184813, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:52.115650: step 3773, loss 0.0790313, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:52.469941: step 3774, loss 0.0934898, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:52.799557: step 3775, loss 0.0907681, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:53.184861: step 3776, loss 0.0847312, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:53.592732: step 3777, loss 0.0572567, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:54.006525: step 3778, loss 0.0700602, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:54.395160: step 3779, loss 0.12476, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:54.769266: step 3780, loss 0.0698524, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:55.161677: step 3781, loss 0.103866, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:55.510295: step 3782, loss 0.10467, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:55.928538: step 3783, loss 0.1583, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:56.320870: step 3784, loss 0.024974, acc 1, learning_rate 0.000100001
2017-10-10T15:08:56.730382: step 3785, loss 0.0515128, acc 1, learning_rate 0.000100001
2017-10-10T15:08:57.123300: step 3786, loss 0.0899624, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:57.516920: step 3787, loss 0.10118, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:57.897670: step 3788, loss 0.0784234, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:58.282707: step 3789, loss 0.109143, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:58.648966: step 3790, loss 0.0721708, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:58.985066: step 3791, loss 0.0701129, acc 1, learning_rate 0.000100001
2017-10-10T15:08:59.334998: step 3792, loss 0.143932, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:59.659003: step 3793, loss 0.186953, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:00.043369: step 3794, loss 0.143244, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:00.444343: step 3795, loss 0.0497639, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:00.813109: step 3796, loss 0.100246, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:01.193659: step 3797, loss 0.212448, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:01.576788: step 3798, loss 0.07172, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:01.969123: step 3799, loss 0.148007, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:02.305144: step 3800, loss 0.0566858, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T15:09:03.000890: step 3800, loss 0.20478, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3800

2017-10-10T15:09:04.508968: step 3801, loss 0.0416378, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:04.899069: step 3802, loss 0.141645, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:05.298170: step 3803, loss 0.0851126, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:05.666072: step 3804, loss 0.0734828, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:06.044916: step 3805, loss 0.0965383, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:06.429020: step 3806, loss 0.13042, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:06.836877: step 3807, loss 0.0925371, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:07.144830: step 3808, loss 0.0469746, acc 1, learning_rate 0.000100001
2017-10-10T15:09:07.397036: step 3809, loss 0.207471, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:07.725327: step 3810, loss 0.126715, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:08.070642: step 3811, loss 0.0695538, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:08.445458: step 3812, loss 0.157686, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:08.818912: step 3813, loss 0.118604, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:09.174958: step 3814, loss 0.145197, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:09.612500: step 3815, loss 0.0624477, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:10.082334: step 3816, loss 0.0600342, acc 1, learning_rate 0.000100001
2017-10-10T15:09:10.483508: step 3817, loss 0.07816, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:10.904226: step 3818, loss 0.0978676, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:11.262839: step 3819, loss 0.105292, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:11.576281: step 3820, loss 0.0381254, acc 1, learning_rate 0.000100001
2017-10-10T15:09:11.900934: step 3821, loss 0.145884, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:12.206502: step 3822, loss 0.121896, acc 0.960784, learning_rate 0.000100001
2017-10-10T15:09:12.583855: step 3823, loss 0.154957, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:12.964059: step 3824, loss 0.113339, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:13.334184: step 3825, loss 0.0473965, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:13.727669: step 3826, loss 0.0404339, acc 1, learning_rate 0.000100001
2017-10-10T15:09:14.132929: step 3827, loss 0.086413, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:14.544319: step 3828, loss 0.0624969, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:14.888901: step 3829, loss 0.0829113, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:15.200476: step 3830, loss 0.0322593, acc 1, learning_rate 0.000100001
2017-10-10T15:09:15.548899: step 3831, loss 0.0144549, acc 1, learning_rate 0.000100001
2017-10-10T15:09:15.949389: step 3832, loss 0.0382016, acc 1, learning_rate 0.000100001
2017-10-10T15:09:16.312953: step 3833, loss 0.0439498, acc 1, learning_rate 0.000100001
2017-10-10T15:09:16.702509: step 3834, loss 0.10735, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:17.113388: step 3835, loss 0.0875321, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:17.497160: step 3836, loss 0.187185, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:17.851355: step 3837, loss 0.165697, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:18.239125: step 3838, loss 0.0474036, acc 1, learning_rate 0.000100001
2017-10-10T15:09:18.640525: step 3839, loss 0.0500064, acc 1, learning_rate 0.000100001
2017-10-10T15:09:19.060947: step 3840, loss 0.0628953, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T15:09:19.726047: step 3840, loss 0.204656, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3840

2017-10-10T15:09:21.008932: step 3841, loss 0.0579258, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:21.334372: step 3842, loss 0.0951262, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:21.779180: step 3843, loss 0.102915, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:22.136778: step 3844, loss 0.104056, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:22.448461: step 3845, loss 0.133168, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:22.862819: step 3846, loss 0.084182, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:23.266304: step 3847, loss 0.151561, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:23.628993: step 3848, loss 0.0911232, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:24.008687: step 3849, loss 0.114652, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:24.440987: step 3850, loss 0.0537636, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:24.928519: step 3851, loss 0.0980038, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:25.276856: step 3852, loss 0.144887, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:25.565978: step 3853, loss 0.105825, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:25.904909: step 3854, loss 0.0940748, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:26.272796: step 3855, loss 0.0507067, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:26.689122: step 3856, loss 0.139784, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:27.056835: step 3857, loss 0.0882696, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:27.468911: step 3858, loss 0.134102, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:27.827406: step 3859, loss 0.0479111, acc 1, learning_rate 0.000100001
2017-10-10T15:09:28.208932: step 3860, loss 0.0242585, acc 1, learning_rate 0.000100001
2017-10-10T15:09:28.607284: step 3861, loss 0.193573, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:09:28.949049: step 3862, loss 0.191863, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:29.304906: step 3863, loss 0.109221, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:29.683457: step 3864, loss 0.194702, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:30.052920: step 3865, loss 0.0490226, acc 1, learning_rate 0.000100001
2017-10-10T15:09:30.425098: step 3866, loss 0.0906915, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:30.835012: step 3867, loss 0.148452, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:31.161878: step 3868, loss 0.141164, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:31.506284: step 3869, loss 0.0607076, acc 1, learning_rate 0.000100001
2017-10-10T15:09:31.843733: step 3870, loss 0.0745192, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:32.232852: step 3871, loss 0.073743, acc 1, learning_rate 0.000100001
2017-10-10T15:09:32.630673: step 3872, loss 0.131003, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:33.056639: step 3873, loss 0.0219421, acc 1, learning_rate 0.000100001
2017-10-10T15:09:33.332839: step 3874, loss 0.0891079, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:33.805468: step 3875, loss 0.119986, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:34.113627: step 3876, loss 0.0952334, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:34.480890: step 3877, loss 0.0944898, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:34.900125: step 3878, loss 0.0780952, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:35.304373: step 3879, loss 0.143852, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:35.676064: step 3880, loss 0.0606023, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:09:36.387091: step 3880, loss 0.205802, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3880

2017-10-10T15:09:37.673378: step 3881, loss 0.0427762, acc 1, learning_rate 0.000100001
2017-10-10T15:09:38.060933: step 3882, loss 0.10413, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:38.472933: step 3883, loss 0.0842272, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:38.949818: step 3884, loss 0.0927129, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:39.403916: step 3885, loss 0.107546, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:39.706981: step 3886, loss 0.058226, acc 1, learning_rate 0.000100001
2017-10-10T15:09:40.078053: step 3887, loss 0.0517541, acc 1, learning_rate 0.000100001
2017-10-10T15:09:40.472851: step 3888, loss 0.0565738, acc 1, learning_rate 0.000100001
2017-10-10T15:09:40.929651: step 3889, loss 0.0597993, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:41.320834: step 3890, loss 0.0802716, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:41.666525: step 3891, loss 0.0696027, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:42.045602: step 3892, loss 0.176378, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:42.471639: step 3893, loss 0.0423009, acc 1, learning_rate 0.000100001
2017-10-10T15:09:42.889274: step 3894, loss 0.0542621, acc 1, learning_rate 0.000100001
2017-10-10T15:09:43.251909: step 3895, loss 0.179096, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:43.575156: step 3896, loss 0.150269, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:43.913149: step 3897, loss 0.204562, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:44.278518: step 3898, loss 0.0844479, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:44.657602: step 3899, loss 0.0795774, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:45.048245: step 3900, loss 0.155206, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:45.416515: step 3901, loss 0.171168, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:45.843343: step 3902, loss 0.0715013, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:46.236853: step 3903, loss 0.118241, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:46.648838: step 3904, loss 0.145044, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:47.033898: step 3905, loss 0.04929, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:47.345545: step 3906, loss 0.148936, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:47.656829: step 3907, loss 0.198394, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:09:48.068512: step 3908, loss 0.162828, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:48.401078: step 3909, loss 0.0234478, acc 1, learning_rate 0.000100001
2017-10-10T15:09:48.761093: step 3910, loss 0.0902283, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:49.127841: step 3911, loss 0.153885, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:49.512834: step 3912, loss 0.0485341, acc 1, learning_rate 0.000100001
2017-10-10T15:09:49.923426: step 3913, loss 0.125974, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:50.361498: step 3914, loss 0.149307, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:50.719505: step 3915, loss 0.0788672, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:51.060855: step 3916, loss 0.0506026, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:51.488904: step 3917, loss 0.131411, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:51.865256: step 3918, loss 0.0742135, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:52.224878: step 3919, loss 0.0592327, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:52.537258: step 3920, loss 0.0739788, acc 0.980392, learning_rate 0.000100001

Evaluation:
2017-10-10T15:09:53.239038: step 3920, loss 0.204823, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3920

2017-10-10T15:09:54.541609: step 3921, loss 0.153773, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:54.980878: step 3922, loss 0.0842659, acc 1, learning_rate 0.000100001
2017-10-10T15:09:55.283142: step 3923, loss 0.0705567, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:55.624495: step 3924, loss 0.0877358, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:55.980965: step 3925, loss 0.0905036, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:56.341105: step 3926, loss 0.107916, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:56.707867: step 3927, loss 0.143786, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:57.094272: step 3928, loss 0.0590372, acc 1, learning_rate 0.000100001
2017-10-10T15:09:57.501606: step 3929, loss 0.110606, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:57.897018: step 3930, loss 0.164672, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:58.274733: step 3931, loss 0.150452, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:58.576970: step 3932, loss 0.0652037, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:58.944128: step 3933, loss 0.153782, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:59.314834: step 3934, loss 0.0919387, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:59.653828: step 3935, loss 0.112979, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:00.073053: step 3936, loss 0.145569, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:00.496888: step 3937, loss 0.0993832, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:00.938949: step 3938, loss 0.18684, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:01.189384: step 3939, loss 0.134098, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:01.466335: step 3940, loss 0.0587093, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:01.845106: step 3941, loss 0.08185, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:02.175889: step 3942, loss 0.128189, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:02.544560: step 3943, loss 0.0946639, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:02.919770: step 3944, loss 0.0817518, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:03.282370: step 3945, loss 0.0946866, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:03.620466: step 3946, loss 0.0894985, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:04.021013: step 3947, loss 0.125373, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:04.383986: step 3948, loss 0.061118, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:04.669063: step 3949, loss 0.108019, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:04.972898: step 3950, loss 0.173357, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:05.356510: step 3951, loss 0.0451862, acc 1, learning_rate 0.0001
2017-10-10T15:10:05.773041: step 3952, loss 0.0695947, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:06.161328: step 3953, loss 0.0391701, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:06.490142: step 3954, loss 0.0524887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:06.801290: step 3955, loss 0.162774, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:07.209776: step 3956, loss 0.0525803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:07.612170: step 3957, loss 0.11549, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:08.009263: step 3958, loss 0.0776515, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:08.400704: step 3959, loss 0.047932, acc 1, learning_rate 0.0001
2017-10-10T15:10:08.796416: step 3960, loss 0.16376, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:09.561009: step 3960, loss 0.202529, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-3960

2017-10-10T15:10:10.888096: step 3961, loss 0.194391, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:11.300850: step 3962, loss 0.0424809, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:11.716859: step 3963, loss 0.060562, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:12.155862: step 3964, loss 0.0883627, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:12.538303: step 3965, loss 0.157165, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:12.923826: step 3966, loss 0.106149, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:13.309482: step 3967, loss 0.0580788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:13.659653: step 3968, loss 0.0617392, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:14.088947: step 3969, loss 0.074367, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:14.410177: step 3970, loss 0.0611727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:14.800464: step 3971, loss 0.0700454, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:15.189009: step 3972, loss 0.057125, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:15.575589: step 3973, loss 0.0889565, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:15.993031: step 3974, loss 0.113194, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:16.388831: step 3975, loss 0.0699203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:16.795423: step 3976, loss 0.0640026, acc 1, learning_rate 0.0001
2017-10-10T15:10:17.101308: step 3977, loss 0.0783436, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:17.445009: step 3978, loss 0.112821, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:17.805761: step 3979, loss 0.0449924, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:18.186591: step 3980, loss 0.140698, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:18.636998: step 3981, loss 0.016337, acc 1, learning_rate 0.0001
2017-10-10T15:10:19.072841: step 3982, loss 0.0376554, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:19.428824: step 3983, loss 0.149792, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:19.738657: step 3984, loss 0.134662, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:20.125095: step 3985, loss 0.0709388, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:20.475979: step 3986, loss 0.109904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:20.825549: step 3987, loss 0.060059, acc 1, learning_rate 0.0001
2017-10-10T15:10:21.228933: step 3988, loss 0.126254, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:21.591492: step 3989, loss 0.164014, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:21.951834: step 3990, loss 0.189203, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:22.303710: step 3991, loss 0.09806, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:22.663371: step 3992, loss 0.0656741, acc 1, learning_rate 0.0001
2017-10-10T15:10:23.017057: step 3993, loss 0.113304, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:23.407980: step 3994, loss 0.186522, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:23.777149: step 3995, loss 0.140409, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:24.169018: step 3996, loss 0.143278, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:24.515115: step 3997, loss 0.0839098, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:24.845014: step 3998, loss 0.132888, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:25.250029: step 3999, loss 0.0758423, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:25.622853: step 4000, loss 0.119979, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:26.345332: step 4000, loss 0.203834, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4000

2017-10-10T15:10:27.422299: step 4001, loss 0.124431, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:27.974161: step 4002, loss 0.103988, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:28.288111: step 4003, loss 0.0695437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:28.617012: step 4004, loss 0.0520815, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:28.959932: step 4005, loss 0.0436729, acc 1, learning_rate 0.0001
2017-10-10T15:10:29.308000: step 4006, loss 0.100053, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:29.664158: step 4007, loss 0.235172, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:30.087163: step 4008, loss 0.15957, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:30.463732: step 4009, loss 0.153661, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:30.857198: step 4010, loss 0.113138, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:31.232938: step 4011, loss 0.125049, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:31.628756: step 4012, loss 0.151713, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:32.009896: step 4013, loss 0.17973, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:32.386353: step 4014, loss 0.0947535, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:32.721242: step 4015, loss 0.19459, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:33.060600: step 4016, loss 0.0382973, acc 1, learning_rate 0.0001
2017-10-10T15:10:33.425035: step 4017, loss 0.0552767, acc 1, learning_rate 0.0001
2017-10-10T15:10:33.744394: step 4018, loss 0.0645508, acc 1, learning_rate 0.0001
2017-10-10T15:10:34.154383: step 4019, loss 0.0377796, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:34.554456: step 4020, loss 0.0537836, acc 1, learning_rate 0.0001
2017-10-10T15:10:34.952564: step 4021, loss 0.155223, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:35.336819: step 4022, loss 0.233187, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:35.696155: step 4023, loss 0.0394487, acc 1, learning_rate 0.0001
2017-10-10T15:10:36.056466: step 4024, loss 0.0683314, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:36.424177: step 4025, loss 0.128117, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:36.792882: step 4026, loss 0.126986, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:37.216850: step 4027, loss 0.108312, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:37.682207: step 4028, loss 0.0736882, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:38.018110: step 4029, loss 0.0761927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:38.300846: step 4030, loss 0.131924, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:38.621069: step 4031, loss 0.153908, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:38.994345: step 4032, loss 0.109935, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:39.338559: step 4033, loss 0.0350335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:39.679707: step 4034, loss 0.0865157, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:40.080868: step 4035, loss 0.152447, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:40.415145: step 4036, loss 0.079927, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:40.921231: step 4037, loss 0.0602652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:41.242225: step 4038, loss 0.0666625, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:41.564830: step 4039, loss 0.07262, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:41.951206: step 4040, loss 0.141374, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:42.748848: step 4040, loss 0.203023, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4040

2017-10-10T15:10:44.038638: step 4041, loss 0.0930919, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:44.405256: step 4042, loss 0.0768927, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:44.799583: step 4043, loss 0.146204, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:45.188529: step 4044, loss 0.0824688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:45.551099: step 4045, loss 0.0882157, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:45.921121: step 4046, loss 0.186409, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:46.352931: step 4047, loss 0.116305, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:46.789170: step 4048, loss 0.0757699, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:47.157513: step 4049, loss 0.0556528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:47.585044: step 4050, loss 0.139199, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:48.024580: step 4051, loss 0.0704216, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:48.383188: step 4052, loss 0.136458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:48.770706: step 4053, loss 0.131936, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:49.177409: step 4054, loss 0.041008, acc 1, learning_rate 0.0001
2017-10-10T15:10:49.520844: step 4055, loss 0.177897, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:49.905921: step 4056, loss 0.0581709, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:50.320815: step 4057, loss 0.0682589, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:50.644824: step 4058, loss 0.0412558, acc 1, learning_rate 0.0001
2017-10-10T15:10:50.956836: step 4059, loss 0.0663395, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:51.363078: step 4060, loss 0.04003, acc 1, learning_rate 0.0001
2017-10-10T15:10:51.764491: step 4061, loss 0.140536, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:52.141037: step 4062, loss 0.0816036, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:52.508859: step 4063, loss 0.0730363, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:52.872992: step 4064, loss 0.16139, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:53.294115: step 4065, loss 0.0775512, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:53.585493: step 4066, loss 0.088421, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:53.882917: step 4067, loss 0.133926, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:54.295144: step 4068, loss 0.0374119, acc 1, learning_rate 0.0001
2017-10-10T15:10:54.587160: step 4069, loss 0.0515862, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:55.011757: step 4070, loss 0.143882, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:55.371862: step 4071, loss 0.245287, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:55.672984: step 4072, loss 0.0913466, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:56.017289: step 4073, loss 0.109187, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:56.395928: step 4074, loss 0.144096, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:56.728203: step 4075, loss 0.234342, acc 0.890625, learning_rate 0.0001
2017-10-10T15:10:57.115148: step 4076, loss 0.11964, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:57.539961: step 4077, loss 0.065372, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:57.842523: step 4078, loss 0.0883131, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:58.259649: step 4079, loss 0.137958, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:58.625289: step 4080, loss 0.114935, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:59.284565: step 4080, loss 0.203867, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4080

2017-10-10T15:11:00.646607: step 4081, loss 0.100996, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:01.089037: step 4082, loss 0.0677633, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:01.500808: step 4083, loss 0.0432299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:01.876876: step 4084, loss 0.0934879, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:02.263154: step 4085, loss 0.0971807, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:02.644845: step 4086, loss 0.0400126, acc 1, learning_rate 0.0001
2017-10-10T15:11:03.023925: step 4087, loss 0.0758085, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:03.390536: step 4088, loss 0.156284, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:03.849173: step 4089, loss 0.0512129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:04.196845: step 4090, loss 0.079769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:04.522557: step 4091, loss 0.112395, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:04.899239: step 4092, loss 0.0547288, acc 1, learning_rate 0.0001
2017-10-10T15:11:05.301126: step 4093, loss 0.226071, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:05.652100: step 4094, loss 0.0982722, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:06.084826: step 4095, loss 0.134594, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:06.427248: step 4096, loss 0.0934847, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:06.771334: step 4097, loss 0.0460086, acc 1, learning_rate 0.0001
2017-10-10T15:11:07.121164: step 4098, loss 0.123483, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:07.568950: step 4099, loss 0.0778416, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:07.960666: step 4100, loss 0.0612815, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:08.256834: step 4101, loss 0.160382, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:08.700294: step 4102, loss 0.108742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:09.057183: step 4103, loss 0.0565683, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:09.477205: step 4104, loss 0.065603, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:09.872940: step 4105, loss 0.136976, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:10.264180: step 4106, loss 0.117409, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:10.692874: step 4107, loss 0.0583184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:11.077976: step 4108, loss 0.112912, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:11.446420: step 4109, loss 0.121236, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:11.844860: step 4110, loss 0.123463, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:12.270338: step 4111, loss 0.074615, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:12.700834: step 4112, loss 0.0581438, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:13.085579: step 4113, loss 0.072061, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:13.440901: step 4114, loss 0.0528076, acc 1, learning_rate 0.0001
2017-10-10T15:11:13.767088: step 4115, loss 0.124182, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:14.025434: step 4116, loss 0.0604573, acc 1, learning_rate 0.0001
2017-10-10T15:11:14.422575: step 4117, loss 0.0762647, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:14.803187: step 4118, loss 0.0319029, acc 1, learning_rate 0.0001
2017-10-10T15:11:15.161605: step 4119, loss 0.0894293, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:15.517187: step 4120, loss 0.0975133, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:16.225605: step 4120, loss 0.200749, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4120

2017-10-10T15:11:17.504390: step 4121, loss 0.156239, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:17.899508: step 4122, loss 0.0534455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:18.356170: step 4123, loss 0.127003, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:18.659687: step 4124, loss 0.143811, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:18.976917: step 4125, loss 0.133181, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:19.381240: step 4126, loss 0.0845087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:19.768868: step 4127, loss 0.225412, acc 0.859375, learning_rate 0.0001
2017-10-10T15:11:20.149046: step 4128, loss 0.120928, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:20.595938: step 4129, loss 0.0499775, acc 1, learning_rate 0.0001
2017-10-10T15:11:20.945962: step 4130, loss 0.0566954, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:21.264848: step 4131, loss 0.0743412, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:21.680880: step 4132, loss 0.132283, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:22.090866: step 4133, loss 0.102881, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:22.504957: step 4134, loss 0.0906603, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:22.864049: step 4135, loss 0.0884754, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:23.167383: step 4136, loss 0.175651, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:23.488880: step 4137, loss 0.0915151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:23.853627: step 4138, loss 0.0914909, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:24.212926: step 4139, loss 0.160437, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:24.584988: step 4140, loss 0.148685, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:25.055351: step 4141, loss 0.0798261, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:25.444833: step 4142, loss 0.194628, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:25.812407: step 4143, loss 0.0779564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:26.153083: step 4144, loss 0.0241385, acc 1, learning_rate 0.0001
2017-10-10T15:11:26.531632: step 4145, loss 0.111603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:26.867945: step 4146, loss 0.0640875, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:27.250992: step 4147, loss 0.0442849, acc 1, learning_rate 0.0001
2017-10-10T15:11:27.661059: step 4148, loss 0.0670519, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:28.055081: step 4149, loss 0.12507, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:28.415133: step 4150, loss 0.118462, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:28.764434: step 4151, loss 0.056542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:29.174050: step 4152, loss 0.0424434, acc 1, learning_rate 0.0001
2017-10-10T15:11:29.589548: step 4153, loss 0.0821595, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:29.960294: step 4154, loss 0.106072, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:30.318808: step 4155, loss 0.0613214, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:30.681031: step 4156, loss 0.0209637, acc 1, learning_rate 0.0001
2017-10-10T15:11:31.163549: step 4157, loss 0.117014, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:31.471321: step 4158, loss 0.0829695, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:31.745984: step 4159, loss 0.157094, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:32.026977: step 4160, loss 0.192806, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:32.855056: step 4160, loss 0.203241, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4160

2017-10-10T15:11:34.160828: step 4161, loss 0.110624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:34.468957: step 4162, loss 0.11905, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:34.807699: step 4163, loss 0.0803322, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:35.105227: step 4164, loss 0.0594105, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:35.500317: step 4165, loss 0.0626114, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:35.877156: step 4166, loss 0.0672039, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:36.233041: step 4167, loss 0.0639107, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:36.602353: step 4168, loss 0.105209, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:36.954727: step 4169, loss 0.165434, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:37.387090: step 4170, loss 0.0237535, acc 1, learning_rate 0.0001
2017-10-10T15:11:37.763474: step 4171, loss 0.0585855, acc 1, learning_rate 0.0001
2017-10-10T15:11:38.196552: step 4172, loss 0.0954144, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:38.531736: step 4173, loss 0.121097, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:38.949767: step 4174, loss 0.103853, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:39.329342: step 4175, loss 0.0441781, acc 1, learning_rate 0.0001
2017-10-10T15:11:39.704786: step 4176, loss 0.0971818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:40.073966: step 4177, loss 0.0835503, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:40.476701: step 4178, loss 0.038911, acc 1, learning_rate 0.0001
2017-10-10T15:11:40.891680: step 4179, loss 0.117923, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:41.263697: step 4180, loss 0.108816, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:41.632948: step 4181, loss 0.125441, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:42.039153: step 4182, loss 0.183176, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:42.429168: step 4183, loss 0.153688, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:42.795846: step 4184, loss 0.0580878, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:43.186799: step 4185, loss 0.121314, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:43.545312: step 4186, loss 0.0717467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:43.882446: step 4187, loss 0.198038, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:44.204235: step 4188, loss 0.18248, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:44.573958: step 4189, loss 0.0830961, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:44.953165: step 4190, loss 0.13378, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:45.320964: step 4191, loss 0.105053, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:45.664421: step 4192, loss 0.179896, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:46.092348: step 4193, loss 0.095965, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:46.496857: step 4194, loss 0.124031, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:46.924978: step 4195, loss 0.115409, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:47.339595: step 4196, loss 0.0419199, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:47.668255: step 4197, loss 0.11174, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:47.970220: step 4198, loss 0.0924501, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:48.344860: step 4199, loss 0.169835, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:48.703659: step 4200, loss 0.073984, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:49.457601: step 4200, loss 0.200973, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4200

2017-10-10T15:11:50.736989: step 4201, loss 0.0678275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:51.128749: step 4202, loss 0.143808, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:51.507525: step 4203, loss 0.0689627, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:51.829886: step 4204, loss 0.0312276, acc 1, learning_rate 0.0001
2017-10-10T15:11:52.161256: step 4205, loss 0.281927, acc 0.890625, learning_rate 0.0001
2017-10-10T15:11:52.538926: step 4206, loss 0.104206, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:52.866630: step 4207, loss 0.0941541, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:53.236056: step 4208, loss 0.0724659, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:53.629197: step 4209, loss 0.130668, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:54.049034: step 4210, loss 0.0823359, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:54.458688: step 4211, loss 0.0548503, acc 1, learning_rate 0.0001
2017-10-10T15:11:54.840852: step 4212, loss 0.119104, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:55.212300: step 4213, loss 0.0322861, acc 1, learning_rate 0.0001
2017-10-10T15:11:55.563985: step 4214, loss 0.0801022, acc 0.980392, learning_rate 0.0001
2017-10-10T15:11:55.984831: step 4215, loss 0.107205, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:56.390372: step 4216, loss 0.050717, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:56.660711: step 4217, loss 0.0785111, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:57.055816: step 4218, loss 0.0971589, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:57.419274: step 4219, loss 0.135308, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:57.773657: step 4220, loss 0.0505676, acc 1, learning_rate 0.0001
2017-10-10T15:11:58.205453: step 4221, loss 0.0569967, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:58.537878: step 4222, loss 0.170484, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:58.903819: step 4223, loss 0.0320838, acc 1, learning_rate 0.0001
2017-10-10T15:11:59.272919: step 4224, loss 0.212721, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:59.640961: step 4225, loss 0.103658, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:00.065016: step 4226, loss 0.140046, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:00.475778: step 4227, loss 0.127658, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:00.877217: step 4228, loss 0.124657, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:01.196985: step 4229, loss 0.0681053, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:01.572435: step 4230, loss 0.179511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:01.965087: step 4231, loss 0.0670919, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:02.327425: step 4232, loss 0.103065, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:02.673078: step 4233, loss 0.097508, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:03.025423: step 4234, loss 0.0820151, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:03.411859: step 4235, loss 0.0474619, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:03.846758: step 4236, loss 0.100307, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:04.244930: step 4237, loss 0.066434, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:04.621615: step 4238, loss 0.146033, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:05.004860: step 4239, loss 0.0600268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:05.372635: step 4240, loss 0.087751, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:06.017588: step 4240, loss 0.203116, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4240

2017-10-10T15:12:07.477020: step 4241, loss 0.0533697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:07.812832: step 4242, loss 0.0754648, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:08.257899: step 4243, loss 0.0817052, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:08.542044: step 4244, loss 0.135183, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:08.895741: step 4245, loss 0.0718534, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:09.211271: step 4246, loss 0.143742, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:09.580791: step 4247, loss 0.164364, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:09.956872: step 4248, loss 0.0398462, acc 1, learning_rate 0.0001
2017-10-10T15:12:10.336941: step 4249, loss 0.152553, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:10.672232: step 4250, loss 0.114905, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:11.041023: step 4251, loss 0.106955, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:11.444923: step 4252, loss 0.0136497, acc 1, learning_rate 0.0001
2017-10-10T15:12:11.820828: step 4253, loss 0.0386063, acc 1, learning_rate 0.0001
2017-10-10T15:12:12.211215: step 4254, loss 0.210166, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:12.585484: step 4255, loss 0.105684, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:12.955041: step 4256, loss 0.146263, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:13.322006: step 4257, loss 0.0512241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:13.745220: step 4258, loss 0.125004, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:14.095157: step 4259, loss 0.0796145, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:14.407373: step 4260, loss 0.113101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:14.755029: step 4261, loss 0.0376939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:15.146760: step 4262, loss 0.217265, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:15.529223: step 4263, loss 0.0940246, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:15.884929: step 4264, loss 0.161953, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:16.288909: step 4265, loss 0.0457281, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:16.688207: step 4266, loss 0.0988278, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:17.048541: step 4267, loss 0.147187, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:17.461621: step 4268, loss 0.0815137, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:17.842773: step 4269, loss 0.108663, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:18.256173: step 4270, loss 0.0441895, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:18.572897: step 4271, loss 0.0874249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:18.932885: step 4272, loss 0.0963263, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:19.430121: step 4273, loss 0.132023, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:19.876809: step 4274, loss 0.0771448, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:20.189160: step 4275, loss 0.140202, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:20.517960: step 4276, loss 0.0770758, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:20.927769: step 4277, loss 0.136038, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:21.269341: step 4278, loss 0.0664523, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:21.663702: step 4279, loss 0.0916132, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:21.980266: step 4280, loss 0.156796, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:22.741191: step 4280, loss 0.20315, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4280

2017-10-10T15:12:24.015678: step 4281, loss 0.0847253, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:24.412282: step 4282, loss 0.0687456, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:24.789205: step 4283, loss 0.0387243, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:25.178767: step 4284, loss 0.0689182, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:25.580908: step 4285, loss 0.103833, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:26.027419: step 4286, loss 0.0894952, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:26.468580: step 4287, loss 0.165505, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:26.739181: step 4288, loss 0.0652712, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:27.125150: step 4289, loss 0.116163, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:27.404681: step 4290, loss 0.0900077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:27.738835: step 4291, loss 0.0987776, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.081546: step 4292, loss 0.0633707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.453276: step 4293, loss 0.094064, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.826330: step 4294, loss 0.0751704, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:29.209757: step 4295, loss 0.133092, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:29.551636: step 4296, loss 0.089277, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:29.893597: step 4297, loss 0.123525, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:30.282571: step 4298, loss 0.0501195, acc 1, learning_rate 0.0001
2017-10-10T15:12:30.689156: step 4299, loss 0.0807067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:31.083343: step 4300, loss 0.202279, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:31.403816: step 4301, loss 0.0747048, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:31.823401: step 4302, loss 0.0575489, acc 1, learning_rate 0.0001
2017-10-10T15:12:32.251992: step 4303, loss 0.033161, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:32.618912: step 4304, loss 0.128916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:32.947930: step 4305, loss 0.0617594, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:33.389696: step 4306, loss 0.0809152, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:33.775295: step 4307, loss 0.12861, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:34.134882: step 4308, loss 0.190273, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:34.504689: step 4309, loss 0.115925, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:34.887242: step 4310, loss 0.0717385, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:35.269693: step 4311, loss 0.127275, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:35.619772: step 4312, loss 0.107931, acc 0.941176, learning_rate 0.0001
2017-10-10T15:12:36.003378: step 4313, loss 0.03587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:36.449242: step 4314, loss 0.137157, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:36.801405: step 4315, loss 0.0813274, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:37.226595: step 4316, loss 0.0293873, acc 1, learning_rate 0.0001
2017-10-10T15:12:37.632211: step 4317, loss 0.0653299, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:37.996726: step 4318, loss 0.0874157, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:38.400966: step 4319, loss 0.0723768, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:38.762252: step 4320, loss 0.119776, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:39.423650: step 4320, loss 0.203283, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4320

2017-10-10T15:12:40.707405: step 4321, loss 0.111981, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:41.004195: step 4322, loss 0.0598029, acc 1, learning_rate 0.0001
2017-10-10T15:12:41.292812: step 4323, loss 0.0730295, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:41.683526: step 4324, loss 0.116644, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:42.054212: step 4325, loss 0.19672, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:42.384928: step 4326, loss 0.114807, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:42.741494: step 4327, loss 0.0507544, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:43.085353: step 4328, loss 0.0295894, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:43.489311: step 4329, loss 0.108648, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:43.904998: step 4330, loss 0.0426752, acc 1, learning_rate 0.0001
2017-10-10T15:12:44.464860: step 4331, loss 0.107269, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:44.809036: step 4332, loss 0.0847742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:45.096624: step 4333, loss 0.107327, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:45.414204: step 4334, loss 0.0342995, acc 1, learning_rate 0.0001
2017-10-10T15:12:45.769038: step 4335, loss 0.141901, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:46.169055: step 4336, loss 0.0911217, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:46.552398: step 4337, loss 0.0308533, acc 1, learning_rate 0.0001
2017-10-10T15:12:46.928967: step 4338, loss 0.121224, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:47.282433: step 4339, loss 0.0323066, acc 1, learning_rate 0.0001
2017-10-10T15:12:47.684441: step 4340, loss 0.0366844, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:48.100892: step 4341, loss 0.14564, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:48.509999: step 4342, loss 0.091602, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:48.953024: step 4343, loss 0.0357834, acc 1, learning_rate 0.0001
2017-10-10T15:12:49.372479: step 4344, loss 0.24246, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:49.771151: step 4345, loss 0.0444717, acc 1, learning_rate 0.0001
2017-10-10T15:12:50.131171: step 4346, loss 0.0847671, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:50.491143: step 4347, loss 0.0589643, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:50.892843: step 4348, loss 0.185256, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:51.268843: step 4349, loss 0.0863428, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:51.717776: step 4350, loss 0.0386896, acc 1, learning_rate 0.0001
2017-10-10T15:12:52.097021: step 4351, loss 0.114421, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:52.466739: step 4352, loss 0.0778575, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:52.816843: step 4353, loss 0.132531, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:53.200915: step 4354, loss 0.0813625, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:53.660932: step 4355, loss 0.144474, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:53.930859: step 4356, loss 0.125904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:54.268657: step 4357, loss 0.118801, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:54.635557: step 4358, loss 0.0508999, acc 1, learning_rate 0.0001
2017-10-10T15:12:55.000870: step 4359, loss 0.109384, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:55.395603: step 4360, loss 0.116805, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:56.048268: step 4360, loss 0.200056, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4360

2017-10-10T15:12:57.116772: step 4361, loss 0.157449, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:57.426687: step 4362, loss 0.131534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:57.807261: step 4363, loss 0.104975, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:58.168246: step 4364, loss 0.0456962, acc 1, learning_rate 0.0001
2017-10-10T15:12:58.546729: step 4365, loss 0.141177, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:58.927971: step 4366, loss 0.0828818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:59.247685: step 4367, loss 0.108523, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:59.667209: step 4368, loss 0.0678802, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:00.041300: step 4369, loss 0.0531059, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:00.445375: step 4370, loss 0.0967764, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:00.896262: step 4371, loss 0.0993056, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:01.289048: step 4372, loss 0.107001, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:01.677939: step 4373, loss 0.112543, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:02.091159: step 4374, loss 0.0687389, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:02.516975: step 4375, loss 0.0492943, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:02.985518: step 4376, loss 0.0450296, acc 1, learning_rate 0.0001
2017-10-10T15:13:03.227112: step 4377, loss 0.0757508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:03.504316: step 4378, loss 0.0518886, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:03.794177: step 4379, loss 0.0818428, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:04.111202: step 4380, loss 0.068483, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:04.509364: step 4381, loss 0.0645929, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:04.933275: step 4382, loss 0.0543489, acc 1, learning_rate 0.0001
2017-10-10T15:13:05.316773: step 4383, loss 0.0540725, acc 1, learning_rate 0.0001
2017-10-10T15:13:05.713681: step 4384, loss 0.0441167, acc 1, learning_rate 0.0001
2017-10-10T15:13:06.052995: step 4385, loss 0.0587765, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:06.367824: step 4386, loss 0.0330505, acc 1, learning_rate 0.0001
2017-10-10T15:13:06.867095: step 4387, loss 0.132162, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:07.228751: step 4388, loss 0.142943, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:07.620963: step 4389, loss 0.041898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:08.045440: step 4390, loss 0.105021, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:08.484634: step 4391, loss 0.13138, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:08.828991: step 4392, loss 0.11187, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:09.160956: step 4393, loss 0.150197, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:09.495655: step 4394, loss 0.097156, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:09.884609: step 4395, loss 0.0904852, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:10.240858: step 4396, loss 0.110864, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:10.649933: step 4397, loss 0.04864, acc 1, learning_rate 0.0001
2017-10-10T15:13:10.951019: step 4398, loss 0.088149, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:11.314164: step 4399, loss 0.0578405, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:11.692939: step 4400, loss 0.134045, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:12.417629: step 4400, loss 0.201645, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4400

2017-10-10T15:13:13.720971: step 4401, loss 0.0815363, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:14.137668: step 4402, loss 0.0599872, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:14.455059: step 4403, loss 0.0407748, acc 1, learning_rate 0.0001
2017-10-10T15:13:14.801853: step 4404, loss 0.0787983, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:15.172214: step 4405, loss 0.0356528, acc 1, learning_rate 0.0001
2017-10-10T15:13:15.589541: step 4406, loss 0.0579606, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:15.953286: step 4407, loss 0.100939, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:16.337388: step 4408, loss 0.0731712, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:16.696969: step 4409, loss 0.0672423, acc 1, learning_rate 0.0001
2017-10-10T15:13:17.068856: step 4410, loss 0.119535, acc 0.980392, learning_rate 0.0001
2017-10-10T15:13:17.476310: step 4411, loss 0.161806, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:17.828844: step 4412, loss 0.0632742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:18.180925: step 4413, loss 0.133458, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:18.539069: step 4414, loss 0.0863628, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:18.937040: step 4415, loss 0.0553723, acc 1, learning_rate 0.0001
2017-10-10T15:13:19.283986: step 4416, loss 0.110892, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:19.753006: step 4417, loss 0.028958, acc 1, learning_rate 0.0001
2017-10-10T15:13:20.166189: step 4418, loss 0.150942, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:20.519243: step 4419, loss 0.112093, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:20.899317: step 4420, loss 0.0482399, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:21.289105: step 4421, loss 0.108207, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:21.620638: step 4422, loss 0.0496574, acc 1, learning_rate 0.0001
2017-10-10T15:13:21.934717: step 4423, loss 0.21642, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:22.237840: step 4424, loss 0.0912388, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:22.623386: step 4425, loss 0.0434412, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:22.998119: step 4426, loss 0.0567841, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:23.417937: step 4427, loss 0.0625422, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:23.824791: step 4428, loss 0.084195, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:24.224826: step 4429, loss 0.0913732, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:24.599552: step 4430, loss 0.149942, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:25.011646: step 4431, loss 0.111443, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:25.469021: step 4432, loss 0.099774, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:25.822209: step 4433, loss 0.131379, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:26.124850: step 4434, loss 0.092612, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:26.464049: step 4435, loss 0.0805415, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:26.859717: step 4436, loss 0.0901188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:27.228860: step 4437, loss 0.0932766, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:27.636951: step 4438, loss 0.0425874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:27.950897: step 4439, loss 0.166229, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:28.361607: step 4440, loss 0.108715, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:29.104945: step 4440, loss 0.204383, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4440

2017-10-10T15:13:30.524928: step 4441, loss 0.0447626, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:30.863138: step 4442, loss 0.0628265, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:31.213228: step 4443, loss 0.039415, acc 1, learning_rate 0.0001
2017-10-10T15:13:31.628888: step 4444, loss 0.142883, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:32.012842: step 4445, loss 0.103214, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:32.406711: step 4446, loss 0.109462, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:32.837613: step 4447, loss 0.186752, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:33.245617: step 4448, loss 0.102086, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:33.576810: step 4449, loss 0.0862271, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:33.894506: step 4450, loss 0.0338457, acc 1, learning_rate 0.0001
2017-10-10T15:13:34.228834: step 4451, loss 0.0931235, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:34.604636: step 4452, loss 0.114215, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:34.981017: step 4453, loss 0.055016, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:35.296890: step 4454, loss 0.144348, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:35.691247: step 4455, loss 0.112948, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:36.080389: step 4456, loss 0.0655972, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:36.408888: step 4457, loss 0.195839, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:36.832985: step 4458, loss 0.0648928, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:37.185693: step 4459, loss 0.11978, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:37.562502: step 4460, loss 0.0702632, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:37.935805: step 4461, loss 0.127849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:38.315493: step 4462, loss 0.0472869, acc 1, learning_rate 0.0001
2017-10-10T15:13:38.672267: step 4463, loss 0.0455926, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:39.103222: step 4464, loss 0.0984484, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:39.414976: step 4465, loss 0.104083, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:39.754077: step 4466, loss 0.059639, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:40.074053: step 4467, loss 0.0637188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:40.479048: step 4468, loss 0.130696, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:40.870290: step 4469, loss 0.0515106, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:41.240893: step 4470, loss 0.0923417, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:41.622896: step 4471, loss 0.0816174, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:41.987132: step 4472, loss 0.0637368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:42.358281: step 4473, loss 0.111249, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:42.732392: step 4474, loss 0.112953, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:43.120955: step 4475, loss 0.0621126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:43.529028: step 4476, loss 0.0967175, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:43.893108: step 4477, loss 0.152929, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:44.271714: step 4478, loss 0.196206, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:44.572695: step 4479, loss 0.087846, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:44.940891: step 4480, loss 0.0829881, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:45.756897: step 4480, loss 0.20015, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4480

2017-10-10T15:13:47.069115: step 4481, loss 0.130507, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:47.460432: step 4482, loss 0.150245, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:47.892895: step 4483, loss 0.0757236, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:48.217321: step 4484, loss 0.0979497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:48.571799: step 4485, loss 0.0501175, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:48.976697: step 4486, loss 0.12102, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:49.344869: step 4487, loss 0.156177, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:49.752831: step 4488, loss 0.089735, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:50.131734: step 4489, loss 0.155971, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:50.511045: step 4490, loss 0.0631457, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:50.855880: step 4491, loss 0.1129, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:51.236843: step 4492, loss 0.123506, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:51.626524: step 4493, loss 0.0774632, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:51.997428: step 4494, loss 0.111016, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:52.369094: step 4495, loss 0.0749196, acc 1, learning_rate 0.0001
2017-10-10T15:13:52.743818: step 4496, loss 0.0344829, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:53.127854: step 4497, loss 0.116296, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:53.465206: step 4498, loss 0.0931491, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:53.766506: step 4499, loss 0.0358314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:54.163431: step 4500, loss 0.0357196, acc 1, learning_rate 0.0001
2017-10-10T15:13:54.530256: step 4501, loss 0.113336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:54.892071: step 4502, loss 0.102857, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:55.268277: step 4503, loss 0.148984, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:55.655262: step 4504, loss 0.103306, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:56.020454: step 4505, loss 0.0333662, acc 1, learning_rate 0.0001
2017-10-10T15:13:56.472856: step 4506, loss 0.143711, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:56.882665: step 4507, loss 0.132456, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:57.275247: step 4508, loss 0.0707222, acc 0.960784, learning_rate 0.0001
2017-10-10T15:13:57.662260: step 4509, loss 0.112971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:57.941293: step 4510, loss 0.0715451, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:58.291965: step 4511, loss 0.0506339, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:58.603012: step 4512, loss 0.0394493, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:58.937003: step 4513, loss 0.102083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:59.300511: step 4514, loss 0.0929084, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:59.726042: step 4515, loss 0.0861878, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:00.086376: step 4516, loss 0.0595857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:00.377946: step 4517, loss 0.121989, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:00.700848: step 4518, loss 0.104896, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:01.035521: step 4519, loss 0.124426, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:01.440166: step 4520, loss 0.0996426, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:02.266201: step 4520, loss 0.203064, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4520

2017-10-10T15:14:03.643773: step 4521, loss 0.114758, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:04.020859: step 4522, loss 0.132663, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:04.427961: step 4523, loss 0.184611, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:04.831747: step 4524, loss 0.0146522, acc 1, learning_rate 0.0001
2017-10-10T15:14:05.229722: step 4525, loss 0.0986611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:05.619197: step 4526, loss 0.10644, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:06.024789: step 4527, loss 0.121281, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:06.365162: step 4528, loss 0.146019, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:06.718023: step 4529, loss 0.0782447, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:07.060951: step 4530, loss 0.115205, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:07.432041: step 4531, loss 0.0824599, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:07.800165: step 4532, loss 0.0660798, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.198632: step 4533, loss 0.0800541, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.576240: step 4534, loss 0.0949151, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:09.026074: step 4535, loss 0.0927301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:09.404822: step 4536, loss 0.0818543, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:09.805628: step 4537, loss 0.0462723, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:10.240857: step 4538, loss 0.140666, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:10.624905: step 4539, loss 0.0784798, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:10.997120: step 4540, loss 0.128335, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:11.331218: step 4541, loss 0.0762301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:11.667014: step 4542, loss 0.0995438, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:12.013726: step 4543, loss 0.044741, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:12.392627: step 4544, loss 0.0531291, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:12.782695: step 4545, loss 0.0476701, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:13.153430: step 4546, loss 0.0370855, acc 1, learning_rate 0.0001
2017-10-10T15:14:13.520810: step 4547, loss 0.148771, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:13.858318: step 4548, loss 0.0772879, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:14.152982: step 4549, loss 0.11185, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:14.548922: step 4550, loss 0.0517971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:14.926466: step 4551, loss 0.0841837, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:15.452942: step 4552, loss 0.100609, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:15.864103: step 4553, loss 0.0471684, acc 1, learning_rate 0.0001
2017-10-10T15:14:16.163845: step 4554, loss 0.0406991, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:16.495536: step 4555, loss 0.150806, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:16.846298: step 4556, loss 0.105391, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:17.222219: step 4557, loss 0.160015, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:17.596991: step 4558, loss 0.0818054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:17.970745: step 4559, loss 0.0847977, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:18.308909: step 4560, loss 0.206509, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:19.068886: step 4560, loss 0.198772, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4560

2017-10-10T15:14:20.356995: step 4561, loss 0.103173, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:20.696884: step 4562, loss 0.064637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:21.080923: step 4563, loss 0.204336, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:21.478220: step 4564, loss 0.0515943, acc 1, learning_rate 0.0001
2017-10-10T15:14:21.917833: step 4565, loss 0.0977422, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:22.316267: step 4566, loss 0.0699851, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:22.604156: step 4567, loss 0.0874011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:22.932817: step 4568, loss 0.0782613, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:23.316448: step 4569, loss 0.123672, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:23.644864: step 4570, loss 0.0647864, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:23.996473: step 4571, loss 0.0816178, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:24.336637: step 4572, loss 0.0568643, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:24.724443: step 4573, loss 0.0743497, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:25.172896: step 4574, loss 0.0670019, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:25.536568: step 4575, loss 0.119537, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:25.946995: step 4576, loss 0.149247, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:26.351766: step 4577, loss 0.153429, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:26.785444: step 4578, loss 0.0811107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:27.132873: step 4579, loss 0.181776, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:27.472880: step 4580, loss 0.0432336, acc 1, learning_rate 0.0001
2017-10-10T15:14:27.802371: step 4581, loss 0.0632038, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:28.220972: step 4582, loss 0.138496, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:28.569042: step 4583, loss 0.12166, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:28.964835: step 4584, loss 0.0647085, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:29.354195: step 4585, loss 0.0849859, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:29.755778: step 4586, loss 0.0880437, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:30.119992: step 4587, loss 0.194937, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:30.512881: step 4588, loss 0.125074, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:30.917614: step 4589, loss 0.1009, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:31.332810: step 4590, loss 0.0383605, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:31.687770: step 4591, loss 0.0497589, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:32.107122: step 4592, loss 0.0468498, acc 1, learning_rate 0.0001
2017-10-10T15:14:32.461839: step 4593, loss 0.0475614, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:32.837066: step 4594, loss 0.0480793, acc 1, learning_rate 0.0001
2017-10-10T15:14:33.304749: step 4595, loss 0.100248, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:33.661799: step 4596, loss 0.19351, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:34.105369: step 4597, loss 0.115062, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:34.502590: step 4598, loss 0.061791, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:34.800936: step 4599, loss 0.132305, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:35.060070: step 4600, loss 0.122603, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:35.701812: step 4600, loss 0.20007, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4600

2017-10-10T15:14:37.077465: step 4601, loss 0.10541, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:37.436844: step 4602, loss 0.0525309, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:37.784908: step 4603, loss 0.101131, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:38.152505: step 4604, loss 0.051446, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:38.490809: step 4605, loss 0.0287068, acc 1, learning_rate 0.0001
2017-10-10T15:14:38.768867: step 4606, loss 0.0413539, acc 1, learning_rate 0.0001
2017-10-10T15:14:39.105140: step 4607, loss 0.0693145, acc 1, learning_rate 0.0001
2017-10-10T15:14:39.524984: step 4608, loss 0.0653319, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:39.981492: step 4609, loss 0.134188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:40.324791: step 4610, loss 0.14014, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:40.753889: step 4611, loss 0.129138, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:41.067563: step 4612, loss 0.0834226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.456880: step 4613, loss 0.042968, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:41.864916: step 4614, loss 0.155057, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:42.240863: step 4615, loss 0.0731829, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:42.615219: step 4616, loss 0.121911, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:43.008033: step 4617, loss 0.0944446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:43.401035: step 4618, loss 0.0954138, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:43.793189: step 4619, loss 0.170025, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:44.139818: step 4620, loss 0.132718, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:44.528951: step 4621, loss 0.100424, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:44.896855: step 4622, loss 0.106326, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:45.215330: step 4623, loss 0.0693142, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:45.488695: step 4624, loss 0.154357, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:45.880487: step 4625, loss 0.0833776, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:46.276938: step 4626, loss 0.170315, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:46.688830: step 4627, loss 0.0754421, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:47.064690: step 4628, loss 0.0568224, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:47.446201: step 4629, loss 0.0724327, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:47.789942: step 4630, loss 0.0672094, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:48.182693: step 4631, loss 0.0844864, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:48.550247: step 4632, loss 0.0918086, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:48.950058: step 4633, loss 0.106695, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:49.312795: step 4634, loss 0.16161, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:49.757996: step 4635, loss 0.0475731, acc 1, learning_rate 0.0001
2017-10-10T15:14:50.161085: step 4636, loss 0.0911618, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:50.531415: step 4637, loss 0.0492042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:50.867420: step 4638, loss 0.126932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:51.312848: step 4639, loss 0.0999049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:51.713117: step 4640, loss 0.0464323, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:52.576798: step 4640, loss 0.200174, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4640

2017-10-10T15:14:53.875040: step 4641, loss 0.106798, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:54.289370: step 4642, loss 0.062801, acc 1, learning_rate 0.0001
2017-10-10T15:14:54.648801: step 4643, loss 0.0988329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:55.072886: step 4644, loss 0.0431101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:55.436852: step 4645, loss 0.10425, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:55.833044: step 4646, loss 0.0702245, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:56.114115: step 4647, loss 0.0642357, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:56.475098: step 4648, loss 0.0904632, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:56.852157: step 4649, loss 0.0547656, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:57.164010: step 4650, loss 0.0808066, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:57.512835: step 4651, loss 0.161541, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:57.865166: step 4652, loss 0.089911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:58.230128: step 4653, loss 0.0464077, acc 1, learning_rate 0.0001
2017-10-10T15:14:58.626505: step 4654, loss 0.0571442, acc 1, learning_rate 0.0001
2017-10-10T15:14:59.019672: step 4655, loss 0.0479304, acc 1, learning_rate 0.0001
2017-10-10T15:14:59.348915: step 4656, loss 0.0669491, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:59.653180: step 4657, loss 0.118809, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:00.025056: step 4658, loss 0.0444129, acc 1, learning_rate 0.0001
2017-10-10T15:15:00.332840: step 4659, loss 0.15641, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:00.676834: step 4660, loss 0.0475904, acc 1, learning_rate 0.0001
2017-10-10T15:15:01.063562: step 4661, loss 0.065006, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:01.405132: step 4662, loss 0.0574716, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:01.789691: step 4663, loss 0.13896, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:02.201801: step 4664, loss 0.169542, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:02.552984: step 4665, loss 0.0518859, acc 1, learning_rate 0.0001
2017-10-10T15:15:03.023080: step 4666, loss 0.0920006, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:03.326311: step 4667, loss 0.092695, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:03.752996: step 4668, loss 0.139086, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:04.088887: step 4669, loss 0.138464, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:04.521083: step 4670, loss 0.0655766, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:04.921051: step 4671, loss 0.101515, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:05.320820: step 4672, loss 0.0429787, acc 1, learning_rate 0.0001
2017-10-10T15:15:05.653844: step 4673, loss 0.152897, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:06.041386: step 4674, loss 0.0695432, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:06.466320: step 4675, loss 0.127279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:06.888918: step 4676, loss 0.0435918, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:07.218494: step 4677, loss 0.0553474, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:07.614457: step 4678, loss 0.122895, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:07.954693: step 4679, loss 0.10273, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:08.367501: step 4680, loss 0.0753936, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:09.120826: step 4680, loss 0.202356, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4680

2017-10-10T15:15:10.468271: step 4681, loss 0.0475105, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:10.821180: step 4682, loss 0.113797, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:11.217035: step 4683, loss 0.0364344, acc 1, learning_rate 0.0001
2017-10-10T15:15:11.608322: step 4684, loss 0.0221319, acc 1, learning_rate 0.0001
2017-10-10T15:15:11.965353: step 4685, loss 0.0323426, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:12.355119: step 4686, loss 0.0685393, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:12.697915: step 4687, loss 0.137914, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:13.114693: step 4688, loss 0.0541722, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:13.516834: step 4689, loss 0.12323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:13.884102: step 4690, loss 0.0902785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.277739: step 4691, loss 0.0487453, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.596864: step 4692, loss 0.0395168, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.957140: step 4693, loss 0.0742909, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:15.342096: step 4694, loss 0.103428, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:15.729030: step 4695, loss 0.100849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:16.043164: step 4696, loss 0.102308, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:16.391128: step 4697, loss 0.108485, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:16.754564: step 4698, loss 0.0970186, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:17.155900: step 4699, loss 0.0475745, acc 1, learning_rate 0.0001
2017-10-10T15:15:17.592856: step 4700, loss 0.0578833, acc 1, learning_rate 0.0001
2017-10-10T15:15:18.025017: step 4701, loss 0.0516189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:18.394013: step 4702, loss 0.0462127, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:18.720819: step 4703, loss 0.0426063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:18.982141: step 4704, loss 0.097395, acc 0.960784, learning_rate 0.0001
2017-10-10T15:15:19.440252: step 4705, loss 0.127161, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:19.807716: step 4706, loss 0.0919751, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:20.222606: step 4707, loss 0.0960035, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:20.631857: step 4708, loss 0.0623656, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:21.055070: step 4709, loss 0.255621, acc 0.90625, learning_rate 0.0001
2017-10-10T15:15:21.411544: step 4710, loss 0.0449241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:21.766706: step 4711, loss 0.0472241, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:22.135692: step 4712, loss 0.0890405, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:22.504427: step 4713, loss 0.0544217, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:22.897543: step 4714, loss 0.129554, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:23.289301: step 4715, loss 0.1537, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:23.640842: step 4716, loss 0.0814041, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:24.073847: step 4717, loss 0.0728989, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:24.391332: step 4718, loss 0.0858218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:24.737370: step 4719, loss 0.207593, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:25.097038: step 4720, loss 0.0874807, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:25.889532: step 4720, loss 0.200441, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4720

2017-10-10T15:15:27.189502: step 4721, loss 0.0483411, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:27.731902: step 4722, loss 0.106156, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:28.121595: step 4723, loss 0.0764247, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:28.389163: step 4724, loss 0.0760555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:28.744879: step 4725, loss 0.0387706, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:29.154004: step 4726, loss 0.0303197, acc 1, learning_rate 0.0001
2017-10-10T15:15:29.497957: step 4727, loss 0.140981, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:29.853745: step 4728, loss 0.0349854, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:30.197956: step 4729, loss 0.086017, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:30.546817: step 4730, loss 0.101632, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:30.928964: step 4731, loss 0.0845229, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:31.279083: step 4732, loss 0.0448687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:31.634609: step 4733, loss 0.0660134, acc 1, learning_rate 0.0001
2017-10-10T15:15:31.963175: step 4734, loss 0.0669269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:32.335760: step 4735, loss 0.148425, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:32.780939: step 4736, loss 0.0662221, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:33.120374: step 4737, loss 0.134841, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:33.453079: step 4738, loss 0.0797984, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:33.792220: step 4739, loss 0.124612, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:34.229148: step 4740, loss 0.109805, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:34.621207: step 4741, loss 0.0279435, acc 1, learning_rate 0.0001
2017-10-10T15:15:34.977336: step 4742, loss 0.130121, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:35.364831: step 4743, loss 0.112323, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:35.747306: step 4744, loss 0.147775, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:36.169076: step 4745, loss 0.0464424, acc 1, learning_rate 0.0001
2017-10-10T15:15:36.512813: step 4746, loss 0.158065, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:36.840980: step 4747, loss 0.135791, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:37.213444: step 4748, loss 0.035058, acc 1, learning_rate 0.0001
2017-10-10T15:15:37.607024: step 4749, loss 0.180439, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:37.963880: step 4750, loss 0.0907813, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:38.356301: step 4751, loss 0.066267, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:38.749870: step 4752, loss 0.110446, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:39.128232: step 4753, loss 0.0789717, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:39.496980: step 4754, loss 0.144904, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:39.822498: step 4755, loss 0.0979279, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:40.248930: step 4756, loss 0.159023, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:40.595557: step 4757, loss 0.0625729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:40.954116: step 4758, loss 0.0612252, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:41.308913: step 4759, loss 0.0643415, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:41.700886: step 4760, loss 0.222475, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:42.416720: step 4760, loss 0.199986, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4760

2017-10-10T15:15:43.700979: step 4761, loss 0.0992395, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:44.070375: step 4762, loss 0.0524369, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:44.410147: step 4763, loss 0.0468289, acc 1, learning_rate 0.0001
2017-10-10T15:15:44.796942: step 4764, loss 0.0925156, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:45.157065: step 4765, loss 0.0743744, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:45.604854: step 4766, loss 0.0767269, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:46.133416: step 4767, loss 0.121963, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:46.429281: step 4768, loss 0.12969, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:46.688809: step 4769, loss 0.200142, acc 0.90625, learning_rate 0.0001
2017-10-10T15:15:47.020738: step 4770, loss 0.0788866, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:47.401646: step 4771, loss 0.102159, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:47.776007: step 4772, loss 0.0674429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:48.181792: step 4773, loss 0.0663609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:48.550532: step 4774, loss 0.0789073, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:48.875433: step 4775, loss 0.0802908, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:49.205163: step 4776, loss 0.101985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:49.578345: step 4777, loss 0.0539354, acc 1, learning_rate 0.0001
2017-10-10T15:15:49.969117: step 4778, loss 0.0514942, acc 1, learning_rate 0.0001
2017-10-10T15:15:50.369638: step 4779, loss 0.0714885, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:50.750981: step 4780, loss 0.0506203, acc 1, learning_rate 0.0001
2017-10-10T15:15:51.123190: step 4781, loss 0.0780407, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:51.499496: step 4782, loss 0.0723665, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:51.888420: step 4783, loss 0.0744914, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:52.260896: step 4784, loss 0.0932383, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:52.609212: step 4785, loss 0.0541275, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:52.969019: step 4786, loss 0.0373379, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:53.344289: step 4787, loss 0.0338252, acc 1, learning_rate 0.0001
2017-10-10T15:15:53.729954: step 4788, loss 0.110122, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:54.128837: step 4789, loss 0.123787, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:54.478489: step 4790, loss 0.0968308, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:54.834731: step 4791, loss 0.0486109, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:55.200674: step 4792, loss 0.0974586, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:55.552088: step 4793, loss 0.061593, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:55.954318: step 4794, loss 0.0549542, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:56.339010: step 4795, loss 0.16452, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:56.748885: step 4796, loss 0.0638242, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:57.164836: step 4797, loss 0.0842624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:57.530494: step 4798, loss 0.0454185, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:57.892030: step 4799, loss 0.0368703, acc 1, learning_rate 0.0001
2017-10-10T15:15:58.242515: step 4800, loss 0.0507441, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:58.914950: step 4800, loss 0.197238, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4800

2017-10-10T15:16:00.442000: step 4801, loss 0.076103, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:00.751029: step 4802, loss 0.0665645, acc 1, learning_rate 0.0001
2017-10-10T15:16:01.115868: step 4803, loss 0.060648, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:01.502080: step 4804, loss 0.101429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:01.942896: step 4805, loss 0.162975, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:02.374375: step 4806, loss 0.0788447, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:02.710985: step 4807, loss 0.0771489, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:03.050193: step 4808, loss 0.0484936, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:03.457797: step 4809, loss 0.194818, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:03.864129: step 4810, loss 0.113722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:04.251285: step 4811, loss 0.0595085, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:04.625106: step 4812, loss 0.148433, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:04.941930: step 4813, loss 0.105928, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:05.257661: step 4814, loss 0.105802, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:05.583284: step 4815, loss 0.129856, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:05.973126: step 4816, loss 0.120815, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:06.352295: step 4817, loss 0.0440192, acc 1, learning_rate 0.0001
2017-10-10T15:16:06.740938: step 4818, loss 0.0797616, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:07.135017: step 4819, loss 0.0903077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:07.527601: step 4820, loss 0.177169, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:07.893769: step 4821, loss 0.0504502, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:08.226917: step 4822, loss 0.0874144, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:08.576379: step 4823, loss 0.0624811, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:08.910967: step 4824, loss 0.118452, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:09.345649: step 4825, loss 0.0824649, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:09.776845: step 4826, loss 0.0355548, acc 1, learning_rate 0.0001
2017-10-10T15:16:10.141706: step 4827, loss 0.120665, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:10.545961: step 4828, loss 0.100124, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:10.915253: step 4829, loss 0.0538973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:11.296853: step 4830, loss 0.077198, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:11.628856: step 4831, loss 0.110396, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:12.030593: step 4832, loss 0.104695, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:12.481769: step 4833, loss 0.0948896, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:12.748407: step 4834, loss 0.103254, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:13.112852: step 4835, loss 0.0819231, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:13.483624: step 4836, loss 0.113028, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:13.810417: step 4837, loss 0.0849345, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:14.145645: step 4838, loss 0.0655285, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:14.545027: step 4839, loss 0.138426, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:14.905477: step 4840, loss 0.152508, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:15.678590: step 4840, loss 0.200043, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4840

2017-10-10T15:16:17.000171: step 4841, loss 0.107094, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:17.364204: step 4842, loss 0.0365423, acc 1, learning_rate 0.0001
2017-10-10T15:16:17.713348: step 4843, loss 0.090732, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:18.064309: step 4844, loss 0.0420852, acc 1, learning_rate 0.0001
2017-10-10T15:16:18.452329: step 4845, loss 0.15275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:18.905076: step 4846, loss 0.16484, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:19.253504: step 4847, loss 0.174281, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:19.672629: step 4848, loss 0.108358, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:20.072941: step 4849, loss 0.0963592, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:20.433048: step 4850, loss 0.0961485, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:20.793086: step 4851, loss 0.12412, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:21.156916: step 4852, loss 0.063514, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:21.579484: step 4853, loss 0.226371, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:21.923630: step 4854, loss 0.108251, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:22.289495: step 4855, loss 0.0479706, acc 1, learning_rate 0.0001
2017-10-10T15:16:22.710496: step 4856, loss 0.0838723, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:23.105080: step 4857, loss 0.102784, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:23.430858: step 4858, loss 0.111764, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:23.741282: step 4859, loss 0.0399435, acc 1, learning_rate 0.0001
2017-10-10T15:16:24.092079: step 4860, loss 0.18997, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:24.532878: step 4861, loss 0.077721, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:24.904908: step 4862, loss 0.0978644, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:25.260971: step 4863, loss 0.0359634, acc 1, learning_rate 0.0001
2017-10-10T15:16:25.660935: step 4864, loss 0.0777378, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:26.087618: step 4865, loss 0.105496, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:26.448895: step 4866, loss 0.169002, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:26.851447: step 4867, loss 0.11048, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:27.194066: step 4868, loss 0.0353701, acc 1, learning_rate 0.0001
2017-10-10T15:16:27.613222: step 4869, loss 0.0243742, acc 1, learning_rate 0.0001
2017-10-10T15:16:27.961047: step 4870, loss 0.117859, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:28.308661: step 4871, loss 0.0804925, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:28.684406: step 4872, loss 0.031056, acc 1, learning_rate 0.0001
2017-10-10T15:16:29.070649: step 4873, loss 0.0625254, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:29.464986: step 4874, loss 0.095711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:29.848903: step 4875, loss 0.158014, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:30.252879: step 4876, loss 0.118939, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:30.637044: step 4877, loss 0.0195112, acc 1, learning_rate 0.0001
2017-10-10T15:16:30.970657: step 4878, loss 0.0556862, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:31.367343: step 4879, loss 0.0646571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:31.709035: step 4880, loss 0.130319, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:32.506142: step 4880, loss 0.200333, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4880

2017-10-10T15:16:33.756984: step 4881, loss 0.032153, acc 1, learning_rate 0.0001
2017-10-10T15:16:34.114686: step 4882, loss 0.109271, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:34.492844: step 4883, loss 0.0816763, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:34.886122: step 4884, loss 0.124968, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:35.324217: step 4885, loss 0.127265, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:35.708686: step 4886, loss 0.0599265, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:35.987332: step 4887, loss 0.125561, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:36.343022: step 4888, loss 0.0588116, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:36.784904: step 4889, loss 0.105692, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:37.209394: step 4890, loss 0.0736218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:37.525361: step 4891, loss 0.0350766, acc 1, learning_rate 0.0001
2017-10-10T15:16:37.883411: step 4892, loss 0.0584211, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:38.304834: step 4893, loss 0.082569, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:38.704926: step 4894, loss 0.0737995, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:39.137292: step 4895, loss 0.123477, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:39.445437: step 4896, loss 0.0454563, acc 1, learning_rate 0.0001
2017-10-10T15:16:39.784597: step 4897, loss 0.0923264, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:40.124629: step 4898, loss 0.0425044, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:40.486286: step 4899, loss 0.147636, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:40.789134: step 4900, loss 0.054236, acc 0.980392, learning_rate 0.0001
2017-10-10T15:16:41.222889: step 4901, loss 0.0813156, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:41.635125: step 4902, loss 0.0752078, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:41.905055: step 4903, loss 0.0944287, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:42.052805: step 4904, loss 0.0246651, acc 1, learning_rate 0.0001
2017-10-10T15:16:42.417331: step 4905, loss 0.0530939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:42.808861: step 4906, loss 0.0509428, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:43.125305: step 4907, loss 0.119695, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:43.468941: step 4908, loss 0.0668394, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:43.814570: step 4909, loss 0.0784666, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:44.200933: step 4910, loss 0.0530482, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:44.551530: step 4911, loss 0.0953188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:44.936828: step 4912, loss 0.0635459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:45.264680: step 4913, loss 0.0828789, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:45.624248: step 4914, loss 0.0612867, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:46.006636: step 4915, loss 0.107409, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:46.444738: step 4916, loss 0.198778, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:46.805789: step 4917, loss 0.130659, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:47.180894: step 4918, loss 0.025628, acc 1, learning_rate 0.0001
2017-10-10T15:16:47.536937: step 4919, loss 0.0608418, acc 1, learning_rate 0.0001
2017-10-10T15:16:48.004558: step 4920, loss 0.106619, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:48.801783: step 4920, loss 0.198309, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4920

2017-10-10T15:16:50.115534: step 4921, loss 0.0801638, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:50.515869: step 4922, loss 0.140752, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:50.866504: step 4923, loss 0.130797, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:51.275646: step 4924, loss 0.142771, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:51.692981: step 4925, loss 0.0814229, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:52.058975: step 4926, loss 0.096169, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:52.476100: step 4927, loss 0.0807128, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:52.923108: step 4928, loss 0.047844, acc 1, learning_rate 0.0001
2017-10-10T15:16:53.223528: step 4929, loss 0.0830065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:53.587317: step 4930, loss 0.130479, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:53.993890: step 4931, loss 0.18004, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:54.409615: step 4932, loss 0.0902123, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:54.776383: step 4933, loss 0.0696377, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:55.142108: step 4934, loss 0.028513, acc 1, learning_rate 0.0001
2017-10-10T15:16:55.503325: step 4935, loss 0.128612, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:55.877986: step 4936, loss 0.0407778, acc 1, learning_rate 0.0001
2017-10-10T15:16:56.233000: step 4937, loss 0.145497, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:56.622369: step 4938, loss 0.0890955, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:57.033080: step 4939, loss 0.132883, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:57.405119: step 4940, loss 0.0584885, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:57.821395: step 4941, loss 0.164806, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:58.251778: step 4942, loss 0.159367, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:58.602749: step 4943, loss 0.0918493, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:58.980923: step 4944, loss 0.141942, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:59.400979: step 4945, loss 0.196834, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:59.825692: step 4946, loss 0.101613, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:00.272595: step 4947, loss 0.0736212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:00.693208: step 4948, loss 0.0687599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:01.006265: step 4949, loss 0.0250554, acc 1, learning_rate 0.0001
2017-10-10T15:17:01.397993: step 4950, loss 0.0888916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:01.719715: step 4951, loss 0.153501, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:02.024831: step 4952, loss 0.107184, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:02.401382: step 4953, loss 0.0818157, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:02.793231: step 4954, loss 0.131931, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:03.209243: step 4955, loss 0.0575836, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:03.605145: step 4956, loss 0.0807035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:04.018201: step 4957, loss 0.0284466, acc 1, learning_rate 0.0001
2017-10-10T15:17:04.352201: step 4958, loss 0.047801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:04.761305: step 4959, loss 0.275561, acc 0.890625, learning_rate 0.0001
2017-10-10T15:17:05.182991: step 4960, loss 0.0341404, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:05.948857: step 4960, loss 0.199616, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-4960

2017-10-10T15:17:07.503842: step 4961, loss 0.048317, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:07.853310: step 4962, loss 0.0961033, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:08.179264: step 4963, loss 0.142214, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:08.501396: step 4964, loss 0.0357084, acc 1, learning_rate 0.0001
2017-10-10T15:17:08.904290: step 4965, loss 0.141936, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:09.240941: step 4966, loss 0.06422, acc 1, learning_rate 0.0001
2017-10-10T15:17:09.599305: step 4967, loss 0.0835036, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:09.959682: step 4968, loss 0.0507435, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:10.386374: step 4969, loss 0.0691084, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:10.766355: step 4970, loss 0.154997, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:11.140926: step 4971, loss 0.0858594, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:11.463772: step 4972, loss 0.0244276, acc 1, learning_rate 0.0001
2017-10-10T15:17:11.815367: step 4973, loss 0.0852459, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:12.165886: step 4974, loss 0.1601, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:12.570119: step 4975, loss 0.1868, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:12.987044: step 4976, loss 0.0970577, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:13.428826: step 4977, loss 0.11832, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:13.816841: step 4978, loss 0.0469017, acc 1, learning_rate 0.0001
2017-10-10T15:17:14.141045: step 4979, loss 0.0987557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:14.542814: step 4980, loss 0.0590973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:14.929023: step 4981, loss 0.0467341, acc 1, learning_rate 0.0001
2017-10-10T15:17:15.283740: step 4982, loss 0.0903807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:15.672156: step 4983, loss 0.0580453, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:15.980808: step 4984, loss 0.0486433, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:16.324865: step 4985, loss 0.0601131, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:16.726894: step 4986, loss 0.0953792, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:17.097272: step 4987, loss 0.0261317, acc 1, learning_rate 0.0001
2017-10-10T15:17:17.467994: step 4988, loss 0.0419, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:17.840856: step 4989, loss 0.0643373, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:18.268864: step 4990, loss 0.0895753, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:18.675851: step 4991, loss 0.106136, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:19.041373: step 4992, loss 0.189779, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:19.405956: step 4993, loss 0.0846585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:19.672996: step 4994, loss 0.0952773, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:19.979118: step 4995, loss 0.0960631, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:20.309915: step 4996, loss 0.0736574, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:20.684568: step 4997, loss 0.0646214, acc 1, learning_rate 0.0001
2017-10-10T15:17:20.951730: step 4998, loss 0.105178, acc 0.980392, learning_rate 0.0001
2017-10-10T15:17:21.289802: step 4999, loss 0.0829791, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:21.696841: step 5000, loss 0.0750618, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:22.404906: step 5000, loss 0.200338, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5000

2017-10-10T15:17:23.740903: step 5001, loss 0.0852682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:24.104864: step 5002, loss 0.0528184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:24.434402: step 5003, loss 0.0502607, acc 1, learning_rate 0.0001
2017-10-10T15:17:24.777495: step 5004, loss 0.152107, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:25.100612: step 5005, loss 0.104731, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:25.512994: step 5006, loss 0.145173, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:25.874567: step 5007, loss 0.0863749, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:26.192346: step 5008, loss 0.0751635, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:26.563392: step 5009, loss 0.0696491, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:26.972965: step 5010, loss 0.0479092, acc 1, learning_rate 0.0001
2017-10-10T15:17:27.339607: step 5011, loss 0.0543045, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:27.653109: step 5012, loss 0.0632957, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:28.001039: step 5013, loss 0.0415742, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:28.385112: step 5014, loss 0.100077, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:28.812919: step 5015, loss 0.0747265, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:29.195162: step 5016, loss 0.0732786, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:29.574001: step 5017, loss 0.0633333, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:29.981170: step 5018, loss 0.0546151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:30.428830: step 5019, loss 0.176648, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:30.824496: step 5020, loss 0.105368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:31.208664: step 5021, loss 0.091577, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:31.505087: step 5022, loss 0.060888, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:31.956909: step 5023, loss 0.062572, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:32.316968: step 5024, loss 0.053012, acc 1, learning_rate 0.0001
2017-10-10T15:17:32.696933: step 5025, loss 0.118175, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:33.056894: step 5026, loss 0.0845319, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:33.388768: step 5027, loss 0.0826863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:33.739552: step 5028, loss 0.146027, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:34.100825: step 5029, loss 0.12264, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:34.489842: step 5030, loss 0.0590711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:34.823740: step 5031, loss 0.118114, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:35.169383: step 5032, loss 0.0409162, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:35.550506: step 5033, loss 0.118623, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:35.883697: step 5034, loss 0.0771848, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:36.285717: step 5035, loss 0.0839591, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:36.732431: step 5036, loss 0.0745083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:37.184721: step 5037, loss 0.0765866, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:37.477144: step 5038, loss 0.0517567, acc 1, learning_rate 0.0001
2017-10-10T15:17:37.852101: step 5039, loss 0.0489677, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:38.118846: step 5040, loss 0.0881414, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:39.036969: step 5040, loss 0.200306, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5040

2017-10-10T15:17:40.185026: step 5041, loss 0.0783296, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:40.580983: step 5042, loss 0.0710916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:40.996785: step 5043, loss 0.0905904, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:41.373161: step 5044, loss 0.084821, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:41.782711: step 5045, loss 0.052789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:42.178169: step 5046, loss 0.0287581, acc 1, learning_rate 0.0001
2017-10-10T15:17:42.563280: step 5047, loss 0.0703391, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:42.956512: step 5048, loss 0.0536661, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:43.384878: step 5049, loss 0.112865, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:43.701168: step 5050, loss 0.0810776, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:44.057054: step 5051, loss 0.0484177, acc 1, learning_rate 0.0001
2017-10-10T15:17:44.448846: step 5052, loss 0.0672483, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:44.812459: step 5053, loss 0.108585, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:45.209183: step 5054, loss 0.036078, acc 1, learning_rate 0.0001
2017-10-10T15:17:45.569449: step 5055, loss 0.0923424, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:46.009163: step 5056, loss 0.0569811, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:46.417328: step 5057, loss 0.0462052, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:46.796880: step 5058, loss 0.0531571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:47.188922: step 5059, loss 0.0388482, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:47.542329: step 5060, loss 0.107686, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:47.888958: step 5061, loss 0.0842691, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:48.213843: step 5062, loss 0.0376573, acc 1, learning_rate 0.0001
2017-10-10T15:17:48.560400: step 5063, loss 0.0780007, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:48.936816: step 5064, loss 0.0936662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:49.371744: step 5065, loss 0.0434923, acc 1, learning_rate 0.0001
2017-10-10T15:17:49.743267: step 5066, loss 0.140308, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:50.129830: step 5067, loss 0.11887, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:50.444511: step 5068, loss 0.0768835, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:50.764855: step 5069, loss 0.0419985, acc 1, learning_rate 0.0001
2017-10-10T15:17:51.122890: step 5070, loss 0.0939016, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:51.457131: step 5071, loss 0.10008, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:51.878005: step 5072, loss 0.0567027, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:52.261706: step 5073, loss 0.054641, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:52.672833: step 5074, loss 0.0791842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:53.068850: step 5075, loss 0.205326, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:53.505108: step 5076, loss 0.0412273, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:53.927422: step 5077, loss 0.102721, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:54.288933: step 5078, loss 0.0544734, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:54.626374: step 5079, loss 0.0709228, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:55.036503: step 5080, loss 0.0532195, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:55.948297: step 5080, loss 0.197941, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5080

2017-10-10T15:17:57.179583: step 5081, loss 0.0465383, acc 1, learning_rate 0.0001
2017-10-10T15:17:57.572827: step 5082, loss 0.0714852, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:57.981387: step 5083, loss 0.083637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:58.375403: step 5084, loss 0.0523397, acc 1, learning_rate 0.0001
2017-10-10T15:17:58.844917: step 5085, loss 0.121694, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:59.242381: step 5086, loss 0.0631776, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:59.583928: step 5087, loss 0.0849471, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:59.968559: step 5088, loss 0.0602552, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:00.353140: step 5089, loss 0.0265445, acc 1, learning_rate 0.0001
2017-10-10T15:18:00.709002: step 5090, loss 0.136745, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:01.093942: step 5091, loss 0.0499693, acc 1, learning_rate 0.0001
2017-10-10T15:18:01.504970: step 5092, loss 0.0584013, acc 1, learning_rate 0.0001
2017-10-10T15:18:01.861100: step 5093, loss 0.0580845, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:02.323826: step 5094, loss 0.0916308, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:02.595568: step 5095, loss 0.113348, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:02.904809: step 5096, loss 0.0975806, acc 0.941176, learning_rate 0.0001
2017-10-10T15:18:03.324836: step 5097, loss 0.0460146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:03.712707: step 5098, loss 0.0691861, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:04.099606: step 5099, loss 0.119142, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:04.493023: step 5100, loss 0.104687, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:04.885140: step 5101, loss 0.0338916, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:05.230445: step 5102, loss 0.110191, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:05.568801: step 5103, loss 0.121431, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:05.941207: step 5104, loss 0.147592, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:06.332462: step 5105, loss 0.115737, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:06.704333: step 5106, loss 0.0734655, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:07.105675: step 5107, loss 0.0890871, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:07.488296: step 5108, loss 0.1023, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:07.866348: step 5109, loss 0.0714505, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:08.216915: step 5110, loss 0.14355, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:08.573960: step 5111, loss 0.186494, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:08.980298: step 5112, loss 0.0330711, acc 1, learning_rate 0.0001
2017-10-10T15:18:09.393951: step 5113, loss 0.182075, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:09.796818: step 5114, loss 0.178136, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:10.226586: step 5115, loss 0.070028, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:10.630864: step 5116, loss 0.0459437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:10.999206: step 5117, loss 0.0463257, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:11.400887: step 5118, loss 0.123454, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:11.723595: step 5119, loss 0.139872, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:12.161139: step 5120, loss 0.092429, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:13.155041: step 5120, loss 0.195731, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5120

2017-10-10T15:18:14.428851: step 5121, loss 0.0616347, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:14.789317: step 5122, loss 0.0694489, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:15.153225: step 5123, loss 0.0590703, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:15.564836: step 5124, loss 0.0763357, acc 1, learning_rate 0.0001
2017-10-10T15:18:15.915295: step 5125, loss 0.0933953, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:16.264909: step 5126, loss 0.250654, acc 0.890625, learning_rate 0.0001
2017-10-10T15:18:16.589086: step 5127, loss 0.112497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:16.951180: step 5128, loss 0.102599, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:17.325113: step 5129, loss 0.0729629, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:17.717435: step 5130, loss 0.0767738, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:18.091193: step 5131, loss 0.0428948, acc 1, learning_rate 0.0001
2017-10-10T15:18:18.437102: step 5132, loss 0.210704, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:18.851280: step 5133, loss 0.0401693, acc 1, learning_rate 0.0001
2017-10-10T15:18:19.252951: step 5134, loss 0.0570449, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:19.648977: step 5135, loss 0.0834689, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:20.002962: step 5136, loss 0.0871624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:20.354562: step 5137, loss 0.0347694, acc 1, learning_rate 0.0001
2017-10-10T15:18:20.729465: step 5138, loss 0.119084, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:21.095903: step 5139, loss 0.06015, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:21.512967: step 5140, loss 0.0334295, acc 1, learning_rate 0.0001
2017-10-10T15:18:21.858350: step 5141, loss 0.0384993, acc 1, learning_rate 0.0001
2017-10-10T15:18:22.241403: step 5142, loss 0.0539317, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:22.677070: step 5143, loss 0.0795492, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:23.071212: step 5144, loss 0.105805, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:23.438206: step 5145, loss 0.037282, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:23.861648: step 5146, loss 0.0955347, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:24.203867: step 5147, loss 0.0908447, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:24.610224: step 5148, loss 0.0799862, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:25.000861: step 5149, loss 0.0485853, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:25.372061: step 5150, loss 0.0702489, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:25.728342: step 5151, loss 0.0906879, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:26.128355: step 5152, loss 0.079941, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:26.476928: step 5153, loss 0.0581487, acc 1, learning_rate 0.0001
2017-10-10T15:18:26.953435: step 5154, loss 0.161299, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:27.307221: step 5155, loss 0.0507684, acc 1, learning_rate 0.0001
2017-10-10T15:18:27.637827: step 5156, loss 0.0793476, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:27.966076: step 5157, loss 0.035001, acc 1, learning_rate 0.0001
2017-10-10T15:18:28.333194: step 5158, loss 0.0751205, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:28.746374: step 5159, loss 0.0677394, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:29.068449: step 5160, loss 0.0639294, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:29.829926: step 5160, loss 0.197947, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5160

2017-10-10T15:18:31.209411: step 5161, loss 0.0663379, acc 1, learning_rate 0.0001
2017-10-10T15:18:31.721874: step 5162, loss 0.0350004, acc 1, learning_rate 0.0001
2017-10-10T15:18:32.180953: step 5163, loss 0.033726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:32.526298: step 5164, loss 0.0779367, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:32.792730: step 5165, loss 0.0485436, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:33.108894: step 5166, loss 0.0397563, acc 1, learning_rate 0.0001
2017-10-10T15:18:33.525020: step 5167, loss 0.0436845, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:33.908810: step 5168, loss 0.112011, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:34.281790: step 5169, loss 0.0767137, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:34.688831: step 5170, loss 0.116477, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:35.114755: step 5171, loss 0.0631141, acc 1, learning_rate 0.0001
2017-10-10T15:18:35.482622: step 5172, loss 0.0580589, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:35.871017: step 5173, loss 0.187987, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:36.248175: step 5174, loss 0.108768, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:36.668365: step 5175, loss 0.151661, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:37.052475: step 5176, loss 0.0567348, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:37.478133: step 5177, loss 0.147409, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:37.829057: step 5178, loss 0.0501867, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:38.199076: step 5179, loss 0.130201, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:38.605880: step 5180, loss 0.0779588, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:39.044841: step 5181, loss 0.06373, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:39.364402: step 5182, loss 0.0792275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:39.660901: step 5183, loss 0.0696617, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:39.999332: step 5184, loss 0.0549673, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:40.368851: step 5185, loss 0.0890277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:40.608771: step 5186, loss 0.102153, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:40.960289: step 5187, loss 0.127839, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:41.310462: step 5188, loss 0.0939802, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:41.687800: step 5189, loss 0.0782299, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:42.072829: step 5190, loss 0.102582, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:42.431014: step 5191, loss 0.0538031, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:42.784826: step 5192, loss 0.131126, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:43.147928: step 5193, loss 0.067434, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:43.456870: step 5194, loss 0.0505012, acc 1, learning_rate 0.0001
2017-10-10T15:18:43.840150: step 5195, loss 0.0629343, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:44.194327: step 5196, loss 0.194623, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:44.622300: step 5197, loss 0.0840539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:45.023454: step 5198, loss 0.181528, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:45.416972: step 5199, loss 0.13303, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:46.288396: step 5200, loss 0.0560651, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:47.167642: step 5200, loss 0.19814, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5200

2017-10-10T15:18:48.555205: step 5201, loss 0.0508202, acc 1, learning_rate 0.0001
2017-10-10T15:18:48.879773: step 5202, loss 0.0794504, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:49.236294: step 5203, loss 0.0495051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:49.645054: step 5204, loss 0.0921717, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:50.033027: step 5205, loss 0.0713141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:50.432861: step 5206, loss 0.138954, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:50.872867: step 5207, loss 0.0348197, acc 1, learning_rate 0.0001
2017-10-10T15:18:51.200934: step 5208, loss 0.0732124, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:51.511121: step 5209, loss 0.169304, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:51.792896: step 5210, loss 0.148959, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:52.240859: step 5211, loss 0.0398316, acc 1, learning_rate 0.0001
2017-10-10T15:18:52.667278: step 5212, loss 0.0554646, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:53.004995: step 5213, loss 0.0535927, acc 1, learning_rate 0.0001
2017-10-10T15:18:53.313306: step 5214, loss 0.0988931, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:53.717998: step 5215, loss 0.115571, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:54.103188: step 5216, loss 0.103215, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:54.445708: step 5217, loss 0.0839729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:54.809230: step 5218, loss 0.0834146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:55.195250: step 5219, loss 0.0830808, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:55.565916: step 5220, loss 0.0527413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:55.955035: step 5221, loss 0.0678959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:56.279907: step 5222, loss 0.0870525, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:56.639653: step 5223, loss 0.0822814, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:57.032441: step 5224, loss 0.115487, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:57.412852: step 5225, loss 0.0994984, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:57.797438: step 5226, loss 0.153122, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:58.135605: step 5227, loss 0.136874, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:58.550475: step 5228, loss 0.050326, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:58.937450: step 5229, loss 0.0779844, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:59.297492: step 5230, loss 0.0647695, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:59.622096: step 5231, loss 0.0432599, acc 1, learning_rate 0.0001
2017-10-10T15:19:00.065692: step 5232, loss 0.0680882, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:00.468875: step 5233, loss 0.0547444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:00.784724: step 5234, loss 0.066506, acc 1, learning_rate 0.0001
2017-10-10T15:19:01.133935: step 5235, loss 0.118847, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:01.546982: step 5236, loss 0.082375, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:01.940897: step 5237, loss 0.0744294, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:02.328839: step 5238, loss 0.0779397, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:02.704510: step 5239, loss 0.153744, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:03.132342: step 5240, loss 0.0778669, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:03.949251: step 5240, loss 0.194577, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5240

2017-10-10T15:19:05.248973: step 5241, loss 0.0878092, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:05.619366: step 5242, loss 0.13596, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:06.031308: step 5243, loss 0.0945746, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:06.368828: step 5244, loss 0.0414073, acc 1, learning_rate 0.0001
2017-10-10T15:19:06.708346: step 5245, loss 0.136757, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:07.016379: step 5246, loss 0.072551, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:07.392955: step 5247, loss 0.0649396, acc 1, learning_rate 0.0001
2017-10-10T15:19:07.738972: step 5248, loss 0.130841, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:08.093275: step 5249, loss 0.0927355, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:08.499997: step 5250, loss 0.0319297, acc 1, learning_rate 0.0001
2017-10-10T15:19:08.921009: step 5251, loss 0.0466902, acc 1, learning_rate 0.0001
2017-10-10T15:19:09.285988: step 5252, loss 0.0432514, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:09.690371: step 5253, loss 0.070553, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:09.961635: step 5254, loss 0.0396494, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:10.313491: step 5255, loss 0.155818, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:10.620865: step 5256, loss 0.13329, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:10.986866: step 5257, loss 0.0797729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:11.421403: step 5258, loss 0.0292288, acc 1, learning_rate 0.0001
2017-10-10T15:19:11.692448: step 5259, loss 0.036699, acc 1, learning_rate 0.0001
2017-10-10T15:19:12.015020: step 5260, loss 0.0518497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:12.368206: step 5261, loss 0.0423792, acc 1, learning_rate 0.0001
2017-10-10T15:19:12.753883: step 5262, loss 0.0727942, acc 1, learning_rate 0.0001
2017-10-10T15:19:13.096841: step 5263, loss 0.0503036, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:13.473261: step 5264, loss 0.0921202, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:13.870666: step 5265, loss 0.0404893, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:14.252980: step 5266, loss 0.13868, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:14.637182: step 5267, loss 0.0430776, acc 1, learning_rate 0.0001
2017-10-10T15:19:15.043081: step 5268, loss 0.0813579, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:15.384853: step 5269, loss 0.10664, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:15.808968: step 5270, loss 0.16314, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:16.214488: step 5271, loss 0.119923, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:16.579902: step 5272, loss 0.123454, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:16.951321: step 5273, loss 0.10458, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:17.344869: step 5274, loss 0.0644126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:17.756902: step 5275, loss 0.202886, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:18.118949: step 5276, loss 0.095899, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:18.414695: step 5277, loss 0.120702, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:18.791326: step 5278, loss 0.062154, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:19.252945: step 5279, loss 0.0614032, acc 1, learning_rate 0.0001
2017-10-10T15:19:19.597067: step 5280, loss 0.08416, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:20.460931: step 5280, loss 0.19811, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5280

2017-10-10T15:19:21.788862: step 5281, loss 0.0614747, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:22.252972: step 5282, loss 0.103619, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:22.609165: step 5283, loss 0.0634508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:22.939067: step 5284, loss 0.0668286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:23.310415: step 5285, loss 0.08314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:23.707195: step 5286, loss 0.120103, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:24.141179: step 5287, loss 0.136381, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:24.507472: step 5288, loss 0.0855644, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:24.840871: step 5289, loss 0.0806735, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:25.226533: step 5290, loss 0.0577197, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:25.569121: step 5291, loss 0.0479575, acc 1, learning_rate 0.0001
2017-10-10T15:19:25.904840: step 5292, loss 0.0748501, acc 0.980392, learning_rate 0.0001
2017-10-10T15:19:26.340971: step 5293, loss 0.248653, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:26.751547: step 5294, loss 0.0943589, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:27.074943: step 5295, loss 0.0464452, acc 1, learning_rate 0.0001
2017-10-10T15:19:27.452277: step 5296, loss 0.0472468, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:27.908837: step 5297, loss 0.0768154, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:28.286474: step 5298, loss 0.0811145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:28.611736: step 5299, loss 0.0329707, acc 1, learning_rate 0.0001
2017-10-10T15:19:28.924538: step 5300, loss 0.0651045, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:29.260929: step 5301, loss 0.0695865, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:29.675586: step 5302, loss 0.128031, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:30.046009: step 5303, loss 0.0899909, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:30.406232: step 5304, loss 0.0824592, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:30.784876: step 5305, loss 0.0816956, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:31.194615: step 5306, loss 0.126436, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:31.540035: step 5307, loss 0.0957723, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:31.944985: step 5308, loss 0.0616045, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:32.345043: step 5309, loss 0.0597304, acc 1, learning_rate 0.0001
2017-10-10T15:19:32.666942: step 5310, loss 0.0537001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:32.990124: step 5311, loss 0.0800166, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:33.379727: step 5312, loss 0.0665014, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:33.764845: step 5313, loss 0.118667, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:34.372482: step 5314, loss 0.0998044, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:34.760862: step 5315, loss 0.0352467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:35.156973: step 5316, loss 0.0403565, acc 1, learning_rate 0.0001
2017-10-10T15:19:35.555470: step 5317, loss 0.19479, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:35.946851: step 5318, loss 0.0892667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:36.334316: step 5319, loss 0.0980839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:36.650446: step 5320, loss 0.0373299, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:37.474602: step 5320, loss 0.197489, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5320

2017-10-10T15:19:38.796838: step 5321, loss 0.126601, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:39.168805: step 5322, loss 0.11316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:39.583564: step 5323, loss 0.140316, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:39.936934: step 5324, loss 0.0542388, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:40.400894: step 5325, loss 0.034597, acc 1, learning_rate 0.0001
2017-10-10T15:19:40.763804: step 5326, loss 0.0158646, acc 1, learning_rate 0.0001
2017-10-10T15:19:41.124988: step 5327, loss 0.124541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:41.542125: step 5328, loss 0.0325152, acc 1, learning_rate 0.0001
2017-10-10T15:19:41.945202: step 5329, loss 0.0637109, acc 1, learning_rate 0.0001
2017-10-10T15:19:42.265064: step 5330, loss 0.056846, acc 1, learning_rate 0.0001
2017-10-10T15:19:42.579721: step 5331, loss 0.0405952, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:43.004860: step 5332, loss 0.0307801, acc 1, learning_rate 0.0001
2017-10-10T15:19:43.371397: step 5333, loss 0.0494017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:43.742239: step 5334, loss 0.0427945, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:44.118551: step 5335, loss 0.113937, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:44.481107: step 5336, loss 0.103026, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:44.868522: step 5337, loss 0.107852, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:45.269089: step 5338, loss 0.100867, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:45.626367: step 5339, loss 0.0405645, acc 1, learning_rate 0.0001
2017-10-10T15:19:45.999745: step 5340, loss 0.111248, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:46.469365: step 5341, loss 0.11998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:46.834431: step 5342, loss 0.0768616, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:47.133124: step 5343, loss 0.0916549, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:47.485074: step 5344, loss 0.0926505, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:47.824909: step 5345, loss 0.109164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:48.237331: step 5346, loss 0.134578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:48.547789: step 5347, loss 0.0822746, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:48.916735: step 5348, loss 0.0428115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:49.316253: step 5349, loss 0.0189347, acc 1, learning_rate 0.0001
2017-10-10T15:19:49.700834: step 5350, loss 0.0969411, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:50.035437: step 5351, loss 0.0780265, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:50.413926: step 5352, loss 0.143891, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:50.700861: step 5353, loss 0.0228815, acc 1, learning_rate 0.0001
2017-10-10T15:19:51.116872: step 5354, loss 0.0411594, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:51.508757: step 5355, loss 0.0417289, acc 1, learning_rate 0.0001
2017-10-10T15:19:51.920665: step 5356, loss 0.18156, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:52.322428: step 5357, loss 0.101571, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:52.675785: step 5358, loss 0.0647406, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:53.000312: step 5359, loss 0.0765302, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:53.404695: step 5360, loss 0.0851529, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:54.248857: step 5360, loss 0.196453, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5360

2017-10-10T15:19:55.389200: step 5361, loss 0.0808582, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:55.792006: step 5362, loss 0.0489288, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:56.134162: step 5363, loss 0.0868392, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:56.456912: step 5364, loss 0.0958871, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:56.893224: step 5365, loss 0.0790713, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:57.251799: step 5366, loss 0.0393936, acc 1, learning_rate 0.0001
2017-10-10T15:19:57.607052: step 5367, loss 0.108282, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:57.960852: step 5368, loss 0.194718, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:58.374637: step 5369, loss 0.0493315, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:58.756185: step 5370, loss 0.14718, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:58.991372: step 5371, loss 0.037363, acc 1, learning_rate 0.0001
2017-10-10T15:19:59.284941: step 5372, loss 0.078068, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:59.592847: step 5373, loss 0.128017, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:59.973029: step 5374, loss 0.116361, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:00.356895: step 5375, loss 0.118792, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:00.732546: step 5376, loss 0.145703, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:01.121411: step 5377, loss 0.0780805, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:01.508549: step 5378, loss 0.0622919, acc 1, learning_rate 0.0001
2017-10-10T15:20:01.885053: step 5379, loss 0.0554145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:02.228991: step 5380, loss 0.108886, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:02.587270: step 5381, loss 0.114189, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:02.965022: step 5382, loss 0.0531404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:03.345049: step 5383, loss 0.05909, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:03.684728: step 5384, loss 0.125599, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:04.023712: step 5385, loss 0.126243, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:04.409609: step 5386, loss 0.0586623, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:04.866489: step 5387, loss 0.0728116, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:05.260909: step 5388, loss 0.0624098, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:05.702784: step 5389, loss 0.0315233, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:05.976993: step 5390, loss 0.0585437, acc 1, learning_rate 0.0001
2017-10-10T15:20:06.341810: step 5391, loss 0.13823, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:06.712679: step 5392, loss 0.145191, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:07.007643: step 5393, loss 0.0427465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:07.321685: step 5394, loss 0.100787, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:07.711171: step 5395, loss 0.0953923, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:08.145080: step 5396, loss 0.0357849, acc 1, learning_rate 0.0001
2017-10-10T15:20:08.483109: step 5397, loss 0.109972, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:08.816824: step 5398, loss 0.0645849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:09.164964: step 5399, loss 0.0909637, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:09.529234: step 5400, loss 0.0709298, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:10.337006: step 5400, loss 0.197721, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5400

2017-10-10T15:20:11.677527: step 5401, loss 0.0277652, acc 1, learning_rate 0.0001
2017-10-10T15:20:12.100974: step 5402, loss 0.0627583, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:12.485464: step 5403, loss 0.0708768, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:12.824322: step 5404, loss 0.123385, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:13.159029: step 5405, loss 0.0624883, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:13.544629: step 5406, loss 0.0516874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:13.883333: step 5407, loss 0.13554, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:14.276951: step 5408, loss 0.0867528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:14.614366: step 5409, loss 0.0992623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:15.021156: step 5410, loss 0.104659, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:15.432852: step 5411, loss 0.250248, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:15.856919: step 5412, loss 0.0531816, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:16.306078: step 5413, loss 0.0853465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:16.692890: step 5414, loss 0.127317, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:17.031631: step 5415, loss 0.0443424, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:17.431200: step 5416, loss 0.0838821, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:17.868393: step 5417, loss 0.031425, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:18.226598: step 5418, loss 0.127099, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:18.571697: step 5419, loss 0.0540037, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:18.989137: step 5420, loss 0.0361554, acc 1, learning_rate 0.0001
2017-10-10T15:20:19.386032: step 5421, loss 0.0162763, acc 1, learning_rate 0.0001
2017-10-10T15:20:19.703904: step 5422, loss 0.108957, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:20.053118: step 5423, loss 0.126381, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:20.461447: step 5424, loss 0.0368566, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:20.825340: step 5425, loss 0.0610389, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:21.209088: step 5426, loss 0.033874, acc 1, learning_rate 0.0001
2017-10-10T15:20:21.585433: step 5427, loss 0.0631468, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:21.944409: step 5428, loss 0.233008, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:22.384857: step 5429, loss 0.101373, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:22.792165: step 5430, loss 0.0394426, acc 1, learning_rate 0.0001
2017-10-10T15:20:23.260886: step 5431, loss 0.0684767, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:23.712877: step 5432, loss 0.0652846, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.147632: step 5433, loss 0.124583, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.473818: step 5434, loss 0.0642058, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.796961: step 5435, loss 0.0997346, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:25.181209: step 5436, loss 0.125171, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:25.521692: step 5437, loss 0.0825633, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:25.853016: step 5438, loss 0.0258152, acc 1, learning_rate 0.0001
2017-10-10T15:20:26.194885: step 5439, loss 0.044292, acc 1, learning_rate 0.0001
2017-10-10T15:20:26.616273: step 5440, loss 0.0591046, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:27.496952: step 5440, loss 0.195128, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5440

2017-10-10T15:20:28.765072: step 5441, loss 0.0344913, acc 1, learning_rate 0.0001
2017-10-10T15:20:29.129815: step 5442, loss 0.0450682, acc 1, learning_rate 0.0001
2017-10-10T15:20:29.467462: step 5443, loss 0.12567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:29.880964: step 5444, loss 0.110685, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:30.256857: step 5445, loss 0.0760277, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:30.652964: step 5446, loss 0.13651, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:30.987746: step 5447, loss 0.107379, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:31.444929: step 5448, loss 0.109751, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:31.796324: step 5449, loss 0.042524, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:32.104787: step 5450, loss 0.0731148, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:32.456795: step 5451, loss 0.0878936, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:32.855697: step 5452, loss 0.169926, acc 0.90625, learning_rate 0.0001
2017-10-10T15:20:33.248913: step 5453, loss 0.0475316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:33.641041: step 5454, loss 0.0656228, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:34.112852: step 5455, loss 0.0777996, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:34.465414: step 5456, loss 0.0907289, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:34.876951: step 5457, loss 0.0923161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:35.253018: step 5458, loss 0.100177, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:35.648148: step 5459, loss 0.0618979, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:36.046337: step 5460, loss 0.101226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:36.447865: step 5461, loss 0.116563, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:36.816009: step 5462, loss 0.0959914, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:37.200108: step 5463, loss 0.0613414, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:37.554044: step 5464, loss 0.0332086, acc 1, learning_rate 0.0001
2017-10-10T15:20:37.933655: step 5465, loss 0.072652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:38.332898: step 5466, loss 0.0905058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:38.667842: step 5467, loss 0.0916978, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:39.001742: step 5468, loss 0.0458273, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:39.312893: step 5469, loss 0.0499078, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:39.716913: step 5470, loss 0.154553, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:40.143931: step 5471, loss 0.10636, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:40.481831: step 5472, loss 0.0814696, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:40.865380: step 5473, loss 0.0767266, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:41.240901: step 5474, loss 0.0475246, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:41.628376: step 5475, loss 0.106771, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:42.055411: step 5476, loss 0.067567, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:42.508894: step 5477, loss 0.0452872, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:42.881030: step 5478, loss 0.0507782, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:43.200470: step 5479, loss 0.0811138, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:43.576931: step 5480, loss 0.156437, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:44.394476: step 5480, loss 0.19554, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5480

2017-10-10T15:20:45.656046: step 5481, loss 0.0511205, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:46.051227: step 5482, loss 0.0642123, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:46.463875: step 5483, loss 0.0348814, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:46.824461: step 5484, loss 0.130062, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:47.186956: step 5485, loss 0.0394144, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:47.532934: step 5486, loss 0.0544159, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:47.832191: step 5487, loss 0.129709, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:48.160887: step 5488, loss 0.0953572, acc 0.980392, learning_rate 0.0001
2017-10-10T15:20:48.570567: step 5489, loss 0.114503, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:48.965153: step 5490, loss 0.148347, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:49.368085: step 5491, loss 0.157305, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:49.748524: step 5492, loss 0.0170934, acc 1, learning_rate 0.0001
2017-10-10T15:20:50.133224: step 5493, loss 0.146236, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:50.532098: step 5494, loss 0.0954509, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:50.985066: step 5495, loss 0.121537, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:51.368146: step 5496, loss 0.126025, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:51.763750: step 5497, loss 0.0387577, acc 1, learning_rate 0.0001
2017-10-10T15:20:52.069679: step 5498, loss 0.110297, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:52.405031: step 5499, loss 0.0490647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:52.781033: step 5500, loss 0.0437061, acc 1, learning_rate 0.0001
2017-10-10T15:20:53.162939: step 5501, loss 0.0868156, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:53.543158: step 5502, loss 0.0453204, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:53.918724: step 5503, loss 0.138562, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:54.332773: step 5504, loss 0.100735, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:54.704959: step 5505, loss 0.0811323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:55.041728: step 5506, loss 0.218669, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:55.415084: step 5507, loss 0.104721, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:55.744836: step 5508, loss 0.0450058, acc 1, learning_rate 0.0001
2017-10-10T15:20:56.144805: step 5509, loss 0.0570655, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:56.560106: step 5510, loss 0.094777, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:57.028156: step 5511, loss 0.101052, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:57.410733: step 5512, loss 0.0756427, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:57.851562: step 5513, loss 0.0284801, acc 1, learning_rate 0.0001
2017-10-10T15:20:58.261120: step 5514, loss 0.0529897, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:58.680928: step 5515, loss 0.071976, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:59.115292: step 5516, loss 0.0716259, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:59.539406: step 5517, loss 0.0480862, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:00.000769: step 5518, loss 0.0316336, acc 1, learning_rate 0.0001
2017-10-10T15:21:00.376866: step 5519, loss 0.0827533, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:00.785440: step 5520, loss 0.0398021, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:01.468893: step 5520, loss 0.196476, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5520

2017-10-10T15:21:02.676944: step 5521, loss 0.0722355, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:03.028119: step 5522, loss 0.101672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:03.380660: step 5523, loss 0.0558151, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:03.740319: step 5524, loss 0.049232, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:04.115879: step 5525, loss 0.0375604, acc 1, learning_rate 0.0001
2017-10-10T15:21:04.526851: step 5526, loss 0.0570061, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:04.920570: step 5527, loss 0.0651237, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:05.276838: step 5528, loss 0.0972062, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:05.598823: step 5529, loss 0.0659635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:05.985367: step 5530, loss 0.107556, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:06.320969: step 5531, loss 0.062283, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:06.684570: step 5532, loss 0.126274, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:07.057069: step 5533, loss 0.136914, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:07.541181: step 5534, loss 0.0555358, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:07.946725: step 5535, loss 0.114039, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:08.317853: step 5536, loss 0.0461383, acc 1, learning_rate 0.0001
2017-10-10T15:21:08.776685: step 5537, loss 0.0426207, acc 1, learning_rate 0.0001
2017-10-10T15:21:09.244827: step 5538, loss 0.0641424, acc 1, learning_rate 0.0001
2017-10-10T15:21:09.532049: step 5539, loss 0.030492, acc 1, learning_rate 0.0001
2017-10-10T15:21:09.861082: step 5540, loss 0.0377485, acc 1, learning_rate 0.0001
2017-10-10T15:21:10.220843: step 5541, loss 0.0323079, acc 1, learning_rate 0.0001
2017-10-10T15:21:10.732846: step 5542, loss 0.037538, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:11.096924: step 5543, loss 0.0346563, acc 1, learning_rate 0.0001
2017-10-10T15:21:11.506158: step 5544, loss 0.0912552, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:11.864862: step 5545, loss 0.0679132, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:12.205524: step 5546, loss 0.118571, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:12.622845: step 5547, loss 0.0618399, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:12.965213: step 5548, loss 0.0580337, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:13.300887: step 5549, loss 0.0757618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:13.721069: step 5550, loss 0.0618138, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:14.040827: step 5551, loss 0.141634, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:14.402352: step 5552, loss 0.146258, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:14.824882: step 5553, loss 0.0500926, acc 1, learning_rate 0.0001
2017-10-10T15:21:15.190574: step 5554, loss 0.0272576, acc 1, learning_rate 0.0001
2017-10-10T15:21:15.604639: step 5555, loss 0.0319511, acc 1, learning_rate 0.0001
2017-10-10T15:21:16.035123: step 5556, loss 0.0550228, acc 1, learning_rate 0.0001
2017-10-10T15:21:16.406351: step 5557, loss 0.0392178, acc 1, learning_rate 0.0001
2017-10-10T15:21:16.780242: step 5558, loss 0.0476539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:17.139672: step 5559, loss 0.137529, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:17.572010: step 5560, loss 0.0613052, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:18.872812: step 5560, loss 0.197849, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5560

2017-10-10T15:21:19.957866: step 5561, loss 0.0592818, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:20.332230: step 5562, loss 0.169864, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:20.800911: step 5563, loss 0.025703, acc 1, learning_rate 0.0001
2017-10-10T15:21:21.156800: step 5564, loss 0.0540252, acc 1, learning_rate 0.0001
2017-10-10T15:21:21.497087: step 5565, loss 0.0563261, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:21.900832: step 5566, loss 0.0528733, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:22.283153: step 5567, loss 0.179542, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:22.633479: step 5568, loss 0.0956392, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:23.008522: step 5569, loss 0.0697934, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:23.392440: step 5570, loss 0.0699075, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:23.829866: step 5571, loss 0.120046, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:24.128861: step 5572, loss 0.0532291, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:24.483326: step 5573, loss 0.0625996, acc 1, learning_rate 0.0001
2017-10-10T15:21:24.820476: step 5574, loss 0.0634301, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:25.232965: step 5575, loss 0.0288211, acc 1, learning_rate 0.0001
2017-10-10T15:21:25.696993: step 5576, loss 0.0766276, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:26.056706: step 5577, loss 0.0625818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:26.507782: step 5578, loss 0.177472, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:26.884592: step 5579, loss 0.0646901, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:27.251718: step 5580, loss 0.049149, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:27.600052: step 5581, loss 0.0808521, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:27.978433: step 5582, loss 0.0331106, acc 1, learning_rate 0.0001
2017-10-10T15:21:28.345027: step 5583, loss 0.0364781, acc 1, learning_rate 0.0001
2017-10-10T15:21:28.764801: step 5584, loss 0.0601765, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.195375: step 5585, loss 0.0871694, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.476997: step 5586, loss 0.10197, acc 0.960784, learning_rate 0.0001
2017-10-10T15:21:29.864934: step 5587, loss 0.0896914, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:30.192910: step 5588, loss 0.106044, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:30.594275: step 5589, loss 0.0815588, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:30.912968: step 5590, loss 0.0554026, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:31.304859: step 5591, loss 0.0532775, acc 1, learning_rate 0.0001
2017-10-10T15:21:31.669059: step 5592, loss 0.0259182, acc 1, learning_rate 0.0001
2017-10-10T15:21:32.116876: step 5593, loss 0.169818, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:32.486052: step 5594, loss 0.0579988, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:32.825831: step 5595, loss 0.0768427, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:33.273944: step 5596, loss 0.102323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:33.631470: step 5597, loss 0.0754204, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:33.985837: step 5598, loss 0.122417, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:34.344946: step 5599, loss 0.100915, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:34.753372: step 5600, loss 0.0742241, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:35.645899: step 5600, loss 0.196045, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5600

2017-10-10T15:21:36.903364: step 5601, loss 0.131794, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:37.279889: step 5602, loss 0.0827246, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:37.659906: step 5603, loss 0.0548122, acc 1, learning_rate 0.0001
2017-10-10T15:21:38.053771: step 5604, loss 0.0347269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:38.392722: step 5605, loss 0.111136, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:38.788617: step 5606, loss 0.147066, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:39.133001: step 5607, loss 0.0847734, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.555179: step 5608, loss 0.122129, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:39.946238: step 5609, loss 0.0508372, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:40.365039: step 5610, loss 0.0569239, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:40.731195: step 5611, loss 0.116522, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:41.161174: step 5612, loss 0.0735167, acc 1, learning_rate 0.0001
2017-10-10T15:21:41.527485: step 5613, loss 0.127043, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:41.889681: step 5614, loss 0.136696, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:42.287659: step 5615, loss 0.0267784, acc 1, learning_rate 0.0001
2017-10-10T15:21:42.620950: step 5616, loss 0.103062, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:43.021060: step 5617, loss 0.155759, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:43.397869: step 5618, loss 0.0623436, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:43.737013: step 5619, loss 0.0685809, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:44.133072: step 5620, loss 0.038809, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:44.512867: step 5621, loss 0.112727, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:44.877366: step 5622, loss 0.0617114, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:45.274583: step 5623, loss 0.0789832, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:45.651155: step 5624, loss 0.165292, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:45.954909: step 5625, loss 0.0350734, acc 1, learning_rate 0.0001
2017-10-10T15:21:46.289231: step 5626, loss 0.065701, acc 1, learning_rate 0.0001
2017-10-10T15:21:46.631581: step 5627, loss 0.0553404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:46.966979: step 5628, loss 0.0566669, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:47.368472: step 5629, loss 0.119747, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:47.750059: step 5630, loss 0.0487497, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:48.140913: step 5631, loss 0.156019, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:48.484994: step 5632, loss 0.0390398, acc 1, learning_rate 0.0001
2017-10-10T15:21:48.856069: step 5633, loss 0.0135593, acc 1, learning_rate 0.0001
2017-10-10T15:21:49.204124: step 5634, loss 0.0892042, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:49.573787: step 5635, loss 0.08556, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:49.970295: step 5636, loss 0.0766201, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:50.368836: step 5637, loss 0.0469498, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:50.796289: step 5638, loss 0.0579609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:51.158838: step 5639, loss 0.0524588, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:51.534242: step 5640, loss 0.131485, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:52.262464: step 5640, loss 0.194861, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5640

2017-10-10T15:21:53.640916: step 5641, loss 0.141536, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:53.995934: step 5642, loss 0.0391563, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:54.477158: step 5643, loss 0.0829015, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:54.912946: step 5644, loss 0.126585, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:55.232774: step 5645, loss 0.121368, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:55.537046: step 5646, loss 0.100263, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:55.901250: step 5647, loss 0.140907, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:56.212828: step 5648, loss 0.135786, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:56.581070: step 5649, loss 0.102975, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:56.961576: step 5650, loss 0.125471, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:57.432887: step 5651, loss 0.017053, acc 1, learning_rate 0.0001
2017-10-10T15:21:57.746500: step 5652, loss 0.0993087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:58.053156: step 5653, loss 0.0425451, acc 1, learning_rate 0.0001
2017-10-10T15:21:58.435359: step 5654, loss 0.0946268, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:58.885869: step 5655, loss 0.03349, acc 1, learning_rate 0.0001
2017-10-10T15:21:59.221574: step 5656, loss 0.0600109, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:59.536972: step 5657, loss 0.03825, acc 1, learning_rate 0.0001
2017-10-10T15:21:59.893559: step 5658, loss 0.135156, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:00.289951: step 5659, loss 0.0644387, acc 1, learning_rate 0.0001
2017-10-10T15:22:00.687251: step 5660, loss 0.0929899, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:01.053215: step 5661, loss 0.0349626, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:01.416228: step 5662, loss 0.047476, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:01.803599: step 5663, loss 0.0905782, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:02.199731: step 5664, loss 0.0891456, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:02.567841: step 5665, loss 0.0803267, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:02.934954: step 5666, loss 0.103562, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:03.308499: step 5667, loss 0.119377, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:03.664861: step 5668, loss 0.044896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:04.009179: step 5669, loss 0.0608359, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:04.381185: step 5670, loss 0.110882, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:04.768892: step 5671, loss 0.0734321, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:05.168864: step 5672, loss 0.0817595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:05.548320: step 5673, loss 0.053228, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:05.941862: step 5674, loss 0.0364231, acc 1, learning_rate 0.0001
2017-10-10T15:22:06.349906: step 5675, loss 0.0802192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:06.678313: step 5676, loss 0.0750955, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:07.022896: step 5677, loss 0.132778, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:07.366227: step 5678, loss 0.095777, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:07.726317: step 5679, loss 0.0547647, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:08.100975: step 5680, loss 0.0898895, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:08.877511: step 5680, loss 0.193758, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5680

2017-10-10T15:22:10.109281: step 5681, loss 0.0775837, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:10.426920: step 5682, loss 0.0339629, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:10.816480: step 5683, loss 0.0466981, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:11.148841: step 5684, loss 0.0307992, acc 1, learning_rate 0.0001
2017-10-10T15:22:11.513419: step 5685, loss 0.0524066, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:11.840710: step 5686, loss 0.0422057, acc 1, learning_rate 0.0001
2017-10-10T15:22:12.177571: step 5687, loss 0.142337, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:12.601118: step 5688, loss 0.0671315, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:13.064985: step 5689, loss 0.100088, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:13.374984: step 5690, loss 0.109074, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:13.619849: step 5691, loss 0.0923056, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:13.934591: step 5692, loss 0.050215, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:14.323419: step 5693, loss 0.0791771, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:14.692195: step 5694, loss 0.0654996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:15.075226: step 5695, loss 0.110524, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:15.456911: step 5696, loss 0.0510684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:15.855002: step 5697, loss 0.0765578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:16.261068: step 5698, loss 0.0929155, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:16.600942: step 5699, loss 0.0703622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:16.993044: step 5700, loss 0.0519873, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:17.391285: step 5701, loss 0.0856381, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:17.754657: step 5702, loss 0.0643665, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:18.059851: step 5703, loss 0.0881848, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:18.516350: step 5704, loss 0.1089, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:18.873041: step 5705, loss 0.106283, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:19.291985: step 5706, loss 0.0857603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:19.637999: step 5707, loss 0.05834, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:19.929043: step 5708, loss 0.0436565, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:20.275791: step 5709, loss 0.117895, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:20.614549: step 5710, loss 0.0858221, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:21.072852: step 5711, loss 0.100822, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:21.458291: step 5712, loss 0.102462, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:21.860894: step 5713, loss 0.13566, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:22.311377: step 5714, loss 0.0687606, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:22.624319: step 5715, loss 0.0379218, acc 1, learning_rate 0.0001
2017-10-10T15:22:22.964941: step 5716, loss 0.12323, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:23.373860: step 5717, loss 0.0559977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:23.679813: step 5718, loss 0.0720998, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:24.025728: step 5719, loss 0.11376, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:24.401963: step 5720, loss 0.0988414, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:25.092584: step 5720, loss 0.195737, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5720

2017-10-10T15:22:26.276901: step 5721, loss 0.034117, acc 1, learning_rate 0.0001
2017-10-10T15:22:26.672871: step 5722, loss 0.0261611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:26.975585: step 5723, loss 0.066914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:27.333156: step 5724, loss 0.0641219, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:27.744574: step 5725, loss 0.0930695, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:28.164849: step 5726, loss 0.0534718, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:28.558458: step 5727, loss 0.0549063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.941717: step 5728, loss 0.0498292, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:29.320619: step 5729, loss 0.0814835, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:29.740600: step 5730, loss 0.0611283, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:30.184878: step 5731, loss 0.127179, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:30.560503: step 5732, loss 0.0569672, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:30.941267: step 5733, loss 0.0475815, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:31.369014: step 5734, loss 0.0699829, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:31.668883: step 5735, loss 0.0516369, acc 1, learning_rate 0.0001
2017-10-10T15:22:32.009685: step 5736, loss 0.0412985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:32.306950: step 5737, loss 0.0216276, acc 1, learning_rate 0.0001
2017-10-10T15:22:32.729122: step 5738, loss 0.107308, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:33.116381: step 5739, loss 0.0661597, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:33.532863: step 5740, loss 0.118288, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:33.956891: step 5741, loss 0.0377822, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:34.389044: step 5742, loss 0.0590225, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:34.754753: step 5743, loss 0.0678587, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:35.132851: step 5744, loss 0.195056, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:35.495425: step 5745, loss 0.116291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:35.877088: step 5746, loss 0.0807257, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:36.264847: step 5747, loss 0.129383, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:36.684527: step 5748, loss 0.216212, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:37.085852: step 5749, loss 0.0753577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:37.463659: step 5750, loss 0.0371373, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:37.794452: step 5751, loss 0.101398, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:38.152986: step 5752, loss 0.132485, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:38.521016: step 5753, loss 0.0825959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:38.874667: step 5754, loss 0.0908971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:39.236920: step 5755, loss 0.096613, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:39.668844: step 5756, loss 0.138217, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:40.006935: step 5757, loss 0.08132, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:40.317499: step 5758, loss 0.0337966, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:40.695807: step 5759, loss 0.0893697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:41.019507: step 5760, loss 0.128278, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:41.765429: step 5760, loss 0.196049, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5760

2017-10-10T15:22:43.117647: step 5761, loss 0.0961306, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:43.487309: step 5762, loss 0.0502872, acc 1, learning_rate 0.0001
2017-10-10T15:22:43.839924: step 5763, loss 0.0670516, acc 1, learning_rate 0.0001
2017-10-10T15:22:44.211458: step 5764, loss 0.0690544, acc 1, learning_rate 0.0001
2017-10-10T15:22:44.560881: step 5765, loss 0.0786756, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:44.916999: step 5766, loss 0.0749713, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:45.260239: step 5767, loss 0.0572916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:45.563679: step 5768, loss 0.140246, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:45.918081: step 5769, loss 0.01145, acc 1, learning_rate 0.0001
2017-10-10T15:22:46.172843: step 5770, loss 0.113252, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:46.437044: step 5771, loss 0.0519202, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:46.705779: step 5772, loss 0.131087, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:46.997141: step 5773, loss 0.0753509, acc 1, learning_rate 0.0001
2017-10-10T15:22:47.313057: step 5774, loss 0.104777, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:47.610150: step 5775, loss 0.0488912, acc 1, learning_rate 0.0001
2017-10-10T15:22:47.950707: step 5776, loss 0.0532298, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:48.337851: step 5777, loss 0.0642435, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:48.552460: step 5778, loss 0.050644, acc 1, learning_rate 0.0001
2017-10-10T15:22:48.804321: step 5779, loss 0.0517791, acc 1, learning_rate 0.0001
2017-10-10T15:22:49.033140: step 5780, loss 0.03981, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:49.369078: step 5781, loss 0.0362289, acc 1, learning_rate 0.0001
2017-10-10T15:22:49.664857: step 5782, loss 0.0286651, acc 1, learning_rate 0.0001
2017-10-10T15:22:49.978198: step 5783, loss 0.0308031, acc 1, learning_rate 0.0001
2017-10-10T15:22:50.246530: step 5784, loss 0.088439, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:50.565315: step 5785, loss 0.0740197, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:50.845444: step 5786, loss 0.161964, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:51.193592: step 5787, loss 0.04522, acc 1, learning_rate 0.0001
2017-10-10T15:22:51.469786: step 5788, loss 0.0897316, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:51.795065: step 5789, loss 0.13742, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:52.105504: step 5790, loss 0.0335628, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:52.428475: step 5791, loss 0.0880131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:52.728809: step 5792, loss 0.0516388, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:53.060842: step 5793, loss 0.0479685, acc 1, learning_rate 0.0001
2017-10-10T15:22:53.361707: step 5794, loss 0.115486, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:53.660945: step 5795, loss 0.117006, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:53.974414: step 5796, loss 0.0519423, acc 1, learning_rate 0.0001
2017-10-10T15:22:54.263669: step 5797, loss 0.0402, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:54.526098: step 5798, loss 0.0564194, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:54.832596: step 5799, loss 0.0769278, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:55.177077: step 5800, loss 0.0597128, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:55.748710: step 5800, loss 0.197769, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5800

2017-10-10T15:22:56.884974: step 5801, loss 0.0473982, acc 1, learning_rate 0.0001
2017-10-10T15:22:57.152066: step 5802, loss 0.115956, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:57.437174: step 5803, loss 0.116205, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:57.751260: step 5804, loss 0.0571467, acc 1, learning_rate 0.0001
2017-10-10T15:22:58.049410: step 5805, loss 0.0690718, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:58.363599: step 5806, loss 0.0818294, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:58.640720: step 5807, loss 0.109583, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:58.950063: step 5808, loss 0.0750121, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:59.232968: step 5809, loss 0.1019, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:59.511104: step 5810, loss 0.170254, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:59.810687: step 5811, loss 0.120054, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:00.164909: step 5812, loss 0.0671815, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:00.489711: step 5813, loss 0.0504152, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:00.827243: step 5814, loss 0.0429585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:01.223092: step 5815, loss 0.0781018, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:01.475608: step 5816, loss 0.0643246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:01.798273: step 5817, loss 0.0864962, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:02.101591: step 5818, loss 0.153274, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:02.364692: step 5819, loss 0.0565707, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:02.616850: step 5820, loss 0.147944, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:02.976092: step 5821, loss 0.0436598, acc 1, learning_rate 0.0001
2017-10-10T15:23:03.275472: step 5822, loss 0.121455, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:03.511945: step 5823, loss 0.0593172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:03.745807: step 5824, loss 0.0743604, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:03.985267: step 5825, loss 0.137657, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:04.308925: step 5826, loss 0.0194054, acc 1, learning_rate 0.0001
2017-10-10T15:23:04.624321: step 5827, loss 0.0462647, acc 1, learning_rate 0.0001
2017-10-10T15:23:04.941801: step 5828, loss 0.0948385, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:05.241030: step 5829, loss 0.0830024, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:05.592495: step 5830, loss 0.10145, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:05.903401: step 5831, loss 0.0687466, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:06.205233: step 5832, loss 0.18875, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:06.493137: step 5833, loss 0.0981174, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:06.829022: step 5834, loss 0.105129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:07.133390: step 5835, loss 0.0659251, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:07.379747: step 5836, loss 0.201747, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:07.660364: step 5837, loss 0.0299236, acc 1, learning_rate 0.0001
2017-10-10T15:23:07.922105: step 5838, loss 0.0363605, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:08.278850: step 5839, loss 0.0281531, acc 1, learning_rate 0.0001
2017-10-10T15:23:08.609341: step 5840, loss 0.042107, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:09.275514: step 5840, loss 0.196439, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5840

2017-10-10T15:23:10.499432: step 5841, loss 0.0926404, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:10.804766: step 5842, loss 0.0429484, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:11.125437: step 5843, loss 0.038451, acc 1, learning_rate 0.0001
2017-10-10T15:23:11.459080: step 5844, loss 0.0732363, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:11.743342: step 5845, loss 0.0539212, acc 1, learning_rate 0.0001
2017-10-10T15:23:12.010193: step 5846, loss 0.0561128, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:12.293568: step 5847, loss 0.126102, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:12.570444: step 5848, loss 0.0658679, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:12.911951: step 5849, loss 0.0620028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:13.213185: step 5850, loss 0.0453144, acc 1, learning_rate 0.0001
2017-10-10T15:23:13.506487: step 5851, loss 0.120168, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:13.768833: step 5852, loss 0.133148, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:14.060444: step 5853, loss 0.0778995, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:14.329218: step 5854, loss 0.0574679, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:14.603997: step 5855, loss 0.0546383, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:14.904852: step 5856, loss 0.0283305, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:15.230995: step 5857, loss 0.0519316, acc 1, learning_rate 0.0001
2017-10-10T15:23:15.544831: step 5858, loss 0.0933632, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:15.869642: step 5859, loss 0.0316735, acc 1, learning_rate 0.0001
2017-10-10T15:23:16.165920: step 5860, loss 0.0593343, acc 1, learning_rate 0.0001
2017-10-10T15:23:16.484884: step 5861, loss 0.0958329, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:16.856980: step 5862, loss 0.0460554, acc 1, learning_rate 0.0001
2017-10-10T15:23:17.211920: step 5863, loss 0.0278279, acc 1, learning_rate 0.0001
2017-10-10T15:23:17.464932: step 5864, loss 0.0869573, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:17.849010: step 5865, loss 0.0365777, acc 1, learning_rate 0.0001
2017-10-10T15:23:18.126222: step 5866, loss 0.0759338, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:18.360350: step 5867, loss 0.0572314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:18.628600: step 5868, loss 0.126137, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:18.859095: step 5869, loss 0.058551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:19.176948: step 5870, loss 0.0678447, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:19.483919: step 5871, loss 0.0855537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:19.788427: step 5872, loss 0.0508163, acc 1, learning_rate 0.0001
2017-10-10T15:23:20.124846: step 5873, loss 0.160354, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:20.432801: step 5874, loss 0.0540637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:20.750093: step 5875, loss 0.13254, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:21.077731: step 5876, loss 0.111255, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:21.377029: step 5877, loss 0.0277638, acc 1, learning_rate 0.0001
2017-10-10T15:23:21.661015: step 5878, loss 0.0256565, acc 1, learning_rate 0.0001
2017-10-10T15:23:21.992868: step 5879, loss 0.124495, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:22.256698: step 5880, loss 0.126232, acc 0.921569, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:22.865692: step 5880, loss 0.193495, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5880

2017-10-10T15:23:24.090928: step 5881, loss 0.0595276, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:24.391909: step 5882, loss 0.0435594, acc 1, learning_rate 0.0001
2017-10-10T15:23:24.665605: step 5883, loss 0.0330103, acc 1, learning_rate 0.0001
2017-10-10T15:23:25.000885: step 5884, loss 0.0896139, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:25.292872: step 5885, loss 0.0469621, acc 1, learning_rate 0.0001
2017-10-10T15:23:25.566453: step 5886, loss 0.0989192, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:25.853329: step 5887, loss 0.0436131, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:26.136943: step 5888, loss 0.0511153, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:26.428854: step 5889, loss 0.0374128, acc 1, learning_rate 0.0001
2017-10-10T15:23:26.733148: step 5890, loss 0.045785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:27.068874: step 5891, loss 0.0203613, acc 1, learning_rate 0.0001
2017-10-10T15:23:27.414940: step 5892, loss 0.0716123, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:27.701375: step 5893, loss 0.0447766, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:27.968869: step 5894, loss 0.0507536, acc 1, learning_rate 0.0001
2017-10-10T15:23:28.260317: step 5895, loss 0.0225175, acc 1, learning_rate 0.0001
2017-10-10T15:23:28.541077: step 5896, loss 0.126437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:28.871091: step 5897, loss 0.114277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.175188: step 5898, loss 0.0463949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:29.500203: step 5899, loss 0.103162, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:29.818748: step 5900, loss 0.0570306, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.118925: step 5901, loss 0.067568, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:30.445499: step 5902, loss 0.115565, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:30.775723: step 5903, loss 0.103948, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:31.072885: step 5904, loss 0.0942083, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:31.367357: step 5905, loss 0.0584506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:31.672854: step 5906, loss 0.0328415, acc 1, learning_rate 0.0001
2017-10-10T15:23:32.020967: step 5907, loss 0.0279552, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:32.313513: step 5908, loss 0.122851, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:32.651350: step 5909, loss 0.018627, acc 1, learning_rate 0.0001
2017-10-10T15:23:32.942477: step 5910, loss 0.078879, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:33.187510: step 5911, loss 0.0584839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:33.462640: step 5912, loss 0.138948, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:33.694540: step 5913, loss 0.0469953, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:33.967080: step 5914, loss 0.0256464, acc 1, learning_rate 0.0001
2017-10-10T15:23:34.268942: step 5915, loss 0.0384309, acc 1, learning_rate 0.0001
2017-10-10T15:23:34.536004: step 5916, loss 0.0999697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:34.885104: step 5917, loss 0.0473883, acc 1, learning_rate 0.0001
2017-10-10T15:23:35.170044: step 5918, loss 0.050611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:35.461455: step 5919, loss 0.115236, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:35.750806: step 5920, loss 0.103724, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:36.300112: step 5920, loss 0.195307, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5920

2017-10-10T15:23:37.612862: step 5921, loss 0.0509496, acc 1, learning_rate 0.0001
2017-10-10T15:23:37.877685: step 5922, loss 0.0461192, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:38.160150: step 5923, loss 0.0824362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:38.380434: step 5924, loss 0.0484614, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:38.690864: step 5925, loss 0.117248, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:39.005250: step 5926, loss 0.0774227, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:39.300848: step 5927, loss 0.122926, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:39.615614: step 5928, loss 0.0761431, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:39.920859: step 5929, loss 0.0569412, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:40.244182: step 5930, loss 0.134124, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:40.556841: step 5931, loss 0.0226278, acc 1, learning_rate 0.0001
2017-10-10T15:23:40.850479: step 5932, loss 0.108101, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:41.168969: step 5933, loss 0.0900431, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:41.481192: step 5934, loss 0.0543214, acc 1, learning_rate 0.0001
2017-10-10T15:23:41.799853: step 5935, loss 0.158128, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.139271: step 5936, loss 0.0611155, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:42.489581: step 5937, loss 0.0542045, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.815977: step 5938, loss 0.166218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:43.126159: step 5939, loss 0.0447643, acc 1, learning_rate 0.0001
2017-10-10T15:23:43.458478: step 5940, loss 0.0732003, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:43.780526: step 5941, loss 0.0689924, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:44.083124: step 5942, loss 0.0751138, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:44.406715: step 5943, loss 0.149538, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:44.716855: step 5944, loss 0.033356, acc 1, learning_rate 0.0001
2017-10-10T15:23:45.024851: step 5945, loss 0.112822, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:45.381133: step 5946, loss 0.0441118, acc 1, learning_rate 0.0001
2017-10-10T15:23:45.680006: step 5947, loss 0.040829, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:46.121383: step 5948, loss 0.121342, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:46.410829: step 5949, loss 0.147505, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:46.670061: step 5950, loss 0.0622454, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:46.948999: step 5951, loss 0.0179798, acc 1, learning_rate 0.0001
2017-10-10T15:23:47.368951: step 5952, loss 0.0852499, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:47.648824: step 5953, loss 0.0771483, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:47.998852: step 5954, loss 0.0855148, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:48.209975: step 5955, loss 0.0988777, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:48.492814: step 5956, loss 0.0755321, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:48.784925: step 5957, loss 0.187822, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:49.103990: step 5958, loss 0.0690578, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:49.409136: step 5959, loss 0.126996, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:49.678186: step 5960, loss 0.0305793, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:50.284718: step 5960, loss 0.195401, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-5960

2017-10-10T15:23:51.476834: step 5961, loss 0.0894951, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:51.777814: step 5962, loss 0.0819365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:52.089080: step 5963, loss 0.0579438, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:52.419264: step 5964, loss 0.0863725, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:52.745206: step 5965, loss 0.166689, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:53.050328: step 5966, loss 0.0791619, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:53.395934: step 5967, loss 0.146765, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:53.739743: step 5968, loss 0.07694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:54.052923: step 5969, loss 0.0474141, acc 1, learning_rate 0.0001
2017-10-10T15:23:54.317050: step 5970, loss 0.0441476, acc 1, learning_rate 0.0001
2017-10-10T15:23:54.672912: step 5971, loss 0.0513917, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:54.949498: step 5972, loss 0.0644312, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:55.202778: step 5973, loss 0.0389663, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:55.511190: step 5974, loss 0.0939283, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:55.860296: step 5975, loss 0.070998, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:56.216828: step 5976, loss 0.0856902, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:56.561181: step 5977, loss 0.0655645, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:56.864766: step 5978, loss 0.0661095, acc 0.980392, learning_rate 0.0001
2017-10-10T15:23:57.204826: step 5979, loss 0.0544614, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:57.456835: step 5980, loss 0.0699043, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:57.728549: step 5981, loss 0.112157, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:58.060373: step 5982, loss 0.0372832, acc 1, learning_rate 0.0001
2017-10-10T15:23:58.312870: step 5983, loss 0.0810046, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:58.597109: step 5984, loss 0.0624271, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:58.840567: step 5985, loss 0.0763908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:59.148895: step 5986, loss 0.0909201, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:59.485319: step 5987, loss 0.104743, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:59.788811: step 5988, loss 0.078558, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:00.118991: step 5989, loss 0.103302, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:00.390406: step 5990, loss 0.0473819, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:00.677107: step 5991, loss 0.0842567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:00.987041: step 5992, loss 0.0360328, acc 1, learning_rate 0.0001
2017-10-10T15:24:01.273163: step 5993, loss 0.0523251, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:01.570428: step 5994, loss 0.0418511, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:01.948816: step 5995, loss 0.188571, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:02.285038: step 5996, loss 0.0887761, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:02.525013: step 5997, loss 0.0483165, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:02.768478: step 5998, loss 0.0392411, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:02.989917: step 5999, loss 0.0580629, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:03.240758: step 6000, loss 0.0509273, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:03.854802: step 6000, loss 0.197902, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6000

2017-10-10T15:24:05.043266: step 6001, loss 0.0588813, acc 1, learning_rate 0.0001
2017-10-10T15:24:05.341013: step 6002, loss 0.045348, acc 1, learning_rate 0.0001
2017-10-10T15:24:05.599198: step 6003, loss 0.0997608, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:05.892607: step 6004, loss 0.208447, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:06.218595: step 6005, loss 0.0782706, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:06.566081: step 6006, loss 0.0819985, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:06.889950: step 6007, loss 0.0440855, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:07.201924: step 6008, loss 0.072151, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:07.471963: step 6009, loss 0.0747081, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:07.812908: step 6010, loss 0.0673793, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:08.157930: step 6011, loss 0.0794933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:08.444210: step 6012, loss 0.0989736, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:08.653383: step 6013, loss 0.10983, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:08.940897: step 6014, loss 0.0704756, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:09.188781: step 6015, loss 0.0181718, acc 1, learning_rate 0.0001
2017-10-10T15:24:09.484128: step 6016, loss 0.0658041, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:09.817844: step 6017, loss 0.0809202, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:10.148881: step 6018, loss 0.0343664, acc 1, learning_rate 0.0001
2017-10-10T15:24:10.464052: step 6019, loss 0.106629, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:10.788622: step 6020, loss 0.0612934, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:11.083573: step 6021, loss 0.102663, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:11.368952: step 6022, loss 0.109627, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:11.679565: step 6023, loss 0.0575448, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:11.968510: step 6024, loss 0.0420521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:12.231914: step 6025, loss 0.0926403, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:12.501654: step 6026, loss 0.0384038, acc 1, learning_rate 0.0001
2017-10-10T15:24:12.822848: step 6027, loss 0.0402575, acc 1, learning_rate 0.0001
2017-10-10T15:24:13.152378: step 6028, loss 0.118965, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:13.441061: step 6029, loss 0.0801026, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:13.771263: step 6030, loss 0.0588888, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:14.084913: step 6031, loss 0.0475177, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:14.400086: step 6032, loss 0.0799756, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:14.660064: step 6033, loss 0.103608, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:14.918531: step 6034, loss 0.0607996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:15.241065: step 6035, loss 0.0417894, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:15.601012: step 6036, loss 0.121724, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:15.944101: step 6037, loss 0.0329978, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:16.236029: step 6038, loss 0.0796593, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:16.557226: step 6039, loss 0.0605819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:16.976842: step 6040, loss 0.0934894, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:17.578138: step 6040, loss 0.195201, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6040

2017-10-10T15:24:18.668974: step 6041, loss 0.0291619, acc 1, learning_rate 0.0001
2017-10-10T15:24:18.974577: step 6042, loss 0.0578132, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:19.235178: step 6043, loss 0.052097, acc 1, learning_rate 0.0001
2017-10-10T15:24:19.481223: step 6044, loss 0.109372, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:19.814177: step 6045, loss 0.156244, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:20.123479: step 6046, loss 0.0446927, acc 1, learning_rate 0.0001
2017-10-10T15:24:20.420881: step 6047, loss 0.0499677, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:20.764195: step 6048, loss 0.124074, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:21.004943: step 6049, loss 0.0496213, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:21.313446: step 6050, loss 0.142393, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:21.607641: step 6051, loss 0.0185252, acc 1, learning_rate 0.0001
2017-10-10T15:24:21.919486: step 6052, loss 0.0450369, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:22.228263: step 6053, loss 0.226387, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:22.537005: step 6054, loss 0.0630875, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:22.869896: step 6055, loss 0.0283393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:23.164828: step 6056, loss 0.111818, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:23.492599: step 6057, loss 0.0565749, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:23.791884: step 6058, loss 0.0826195, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:24.092560: step 6059, loss 0.0564045, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:24.409113: step 6060, loss 0.111552, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:24.735447: step 6061, loss 0.100799, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:25.012881: step 6062, loss 0.122456, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:25.352820: step 6063, loss 0.0763953, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:25.660832: step 6064, loss 0.123988, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:26.008194: step 6065, loss 0.0702345, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:26.369626: step 6066, loss 0.0502531, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:26.684488: step 6067, loss 0.052081, acc 1, learning_rate 0.0001
2017-10-10T15:24:26.993435: step 6068, loss 0.0758887, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:27.298920: step 6069, loss 0.0376923, acc 1, learning_rate 0.0001
2017-10-10T15:24:27.648836: step 6070, loss 0.0830964, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:27.973984: step 6071, loss 0.04076, acc 1, learning_rate 0.0001
2017-10-10T15:24:28.288130: step 6072, loss 0.0532784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:28.578615: step 6073, loss 0.0439262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:28.887582: step 6074, loss 0.052584, acc 1, learning_rate 0.0001
2017-10-10T15:24:29.208860: step 6075, loss 0.101638, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:29.471243: step 6076, loss 0.0463382, acc 1, learning_rate 0.0001
2017-10-10T15:24:29.728897: step 6077, loss 0.0528534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:30.098690: step 6078, loss 0.0488523, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:30.369091: step 6079, loss 0.0590699, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:30.593107: step 6080, loss 0.0109851, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:31.156727: step 6080, loss 0.194275, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6080

2017-10-10T15:24:32.401291: step 6081, loss 0.0581306, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:32.655943: step 6082, loss 0.0533449, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:32.906455: step 6083, loss 0.0550074, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:33.156894: step 6084, loss 0.0855985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:33.444904: step 6085, loss 0.097005, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:33.727581: step 6086, loss 0.0448978, acc 1, learning_rate 0.0001
2017-10-10T15:24:34.013915: step 6087, loss 0.097257, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:34.302717: step 6088, loss 0.0901549, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:34.603518: step 6089, loss 0.0653361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:34.895403: step 6090, loss 0.0479974, acc 1, learning_rate 0.0001
2017-10-10T15:24:35.241219: step 6091, loss 0.0374815, acc 1, learning_rate 0.0001
2017-10-10T15:24:35.549926: step 6092, loss 0.0891896, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:35.863064: step 6093, loss 0.0325304, acc 1, learning_rate 0.0001
2017-10-10T15:24:36.160853: step 6094, loss 0.0870103, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:36.460150: step 6095, loss 0.0973042, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:36.773523: step 6096, loss 0.0880541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:37.053100: step 6097, loss 0.091293, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:37.368862: step 6098, loss 0.170899, acc 0.90625, learning_rate 0.0001
2017-10-10T15:24:37.721172: step 6099, loss 0.132179, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:38.011733: step 6100, loss 0.0329156, acc 1, learning_rate 0.0001
2017-10-10T15:24:38.294629: step 6101, loss 0.0926195, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:38.616840: step 6102, loss 0.0887208, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:38.907247: step 6103, loss 0.0534415, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:39.302119: step 6104, loss 0.0235779, acc 1, learning_rate 0.0001
2017-10-10T15:24:39.587215: step 6105, loss 0.065921, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:39.889893: step 6106, loss 0.0335709, acc 1, learning_rate 0.0001
2017-10-10T15:24:40.180891: step 6107, loss 0.0767759, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:40.516766: step 6108, loss 0.0410099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:40.817213: step 6109, loss 0.0877932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:41.191634: step 6110, loss 0.0901874, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:41.457289: step 6111, loss 0.078805, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:41.683867: step 6112, loss 0.0704222, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:41.956432: step 6113, loss 0.0564527, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:42.283750: step 6114, loss 0.132173, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:42.616974: step 6115, loss 0.113594, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:42.917277: step 6116, loss 0.0660591, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:43.211759: step 6117, loss 0.0463556, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:43.473027: step 6118, loss 0.105256, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:43.798615: step 6119, loss 0.103621, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:44.098613: step 6120, loss 0.06159, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:44.735491: step 6120, loss 0.19387, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6120

2017-10-10T15:24:45.976940: step 6121, loss 0.0564431, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:46.334443: step 6122, loss 0.0654051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:46.646947: step 6123, loss 0.0626496, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:46.976932: step 6124, loss 0.0538531, acc 1, learning_rate 0.0001
2017-10-10T15:24:47.358521: step 6125, loss 0.106505, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:47.558571: step 6126, loss 0.0920571, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:47.811288: step 6127, loss 0.120423, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:48.069157: step 6128, loss 0.0483531, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:48.335341: step 6129, loss 0.0439906, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:48.677208: step 6130, loss 0.0846271, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:48.999750: step 6131, loss 0.074747, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:49.347049: step 6132, loss 0.119975, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:49.653651: step 6133, loss 0.0622668, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:49.935051: step 6134, loss 0.0889875, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:50.230821: step 6135, loss 0.0896686, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:50.520947: step 6136, loss 0.0630679, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:50.828931: step 6137, loss 0.182413, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:51.109146: step 6138, loss 0.0444969, acc 1, learning_rate 0.0001
2017-10-10T15:24:51.448828: step 6139, loss 0.11256, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:51.772862: step 6140, loss 0.0413917, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:52.142998: step 6141, loss 0.0505001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:52.412481: step 6142, loss 0.0891627, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:52.644390: step 6143, loss 0.037978, acc 1, learning_rate 0.0001
2017-10-10T15:24:52.934234: step 6144, loss 0.100616, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:53.251378: step 6145, loss 0.067065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:53.612598: step 6146, loss 0.0725702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:53.912822: step 6147, loss 0.112464, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:54.248182: step 6148, loss 0.0503068, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:54.500949: step 6149, loss 0.0984167, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:54.800679: step 6150, loss 0.0430972, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:55.117298: step 6151, loss 0.0559867, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:55.428867: step 6152, loss 0.0613583, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:55.738240: step 6153, loss 0.115369, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:56.014117: step 6154, loss 0.107334, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.327942: step 6155, loss 0.0828932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.649032: step 6156, loss 0.0552658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:56.906981: step 6157, loss 0.0198105, acc 1, learning_rate 0.0001
2017-10-10T15:24:57.204827: step 6158, loss 0.0715976, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:57.495953: step 6159, loss 0.067777, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:57.850954: step 6160, loss 0.0515863, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:58.452843: step 6160, loss 0.191147, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6160

2017-10-10T15:24:59.704139: step 6161, loss 0.0200307, acc 1, learning_rate 0.0001
2017-10-10T15:24:59.988075: step 6162, loss 0.0416638, acc 1, learning_rate 0.0001
2017-10-10T15:25:00.268876: step 6163, loss 0.0196485, acc 1, learning_rate 0.0001
2017-10-10T15:25:00.562966: step 6164, loss 0.0568243, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:00.832117: step 6165, loss 0.0973024, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:01.176905: step 6166, loss 0.110921, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:01.457090: step 6167, loss 0.0648189, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:01.722074: step 6168, loss 0.0405634, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:02.063011: step 6169, loss 0.0991818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:02.390275: step 6170, loss 0.13475, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:02.638111: step 6171, loss 0.17486, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:02.917933: step 6172, loss 0.0499288, acc 1, learning_rate 0.0001
2017-10-10T15:25:03.278105: step 6173, loss 0.0556663, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:03.471113: step 6174, loss 0.0535555, acc 0.980392, learning_rate 0.0001
2017-10-10T15:25:03.777590: step 6175, loss 0.0861698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:04.055500: step 6176, loss 0.0533801, acc 1, learning_rate 0.0001
2017-10-10T15:25:04.367380: step 6177, loss 0.023281, acc 1, learning_rate 0.0001
2017-10-10T15:25:04.698263: step 6178, loss 0.126927, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:05.022300: step 6179, loss 0.114035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:05.335232: step 6180, loss 0.0797881, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:05.644608: step 6181, loss 0.137188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:05.944780: step 6182, loss 0.0509187, acc 1, learning_rate 0.0001
2017-10-10T15:25:06.230870: step 6183, loss 0.0801475, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:06.517965: step 6184, loss 0.113634, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:06.866502: step 6185, loss 0.103414, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:07.139537: step 6186, loss 0.0220821, acc 1, learning_rate 0.0001
2017-10-10T15:25:07.474136: step 6187, loss 0.107774, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:07.817572: step 6188, loss 0.098331, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:08.109405: step 6189, loss 0.0642023, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:08.438521: step 6190, loss 0.118159, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:08.776846: step 6191, loss 0.0401407, acc 1, learning_rate 0.0001
2017-10-10T15:25:09.118180: step 6192, loss 0.0521372, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:09.468237: step 6193, loss 0.0991269, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:09.801531: step 6194, loss 0.0548056, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:10.123244: step 6195, loss 0.0493761, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:10.488902: step 6196, loss 0.0647511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:10.771227: step 6197, loss 0.0780626, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:11.017517: step 6198, loss 0.0930445, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:11.356236: step 6199, loss 0.0441822, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:11.617245: step 6200, loss 0.052535, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:12.220842: step 6200, loss 0.192526, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6200

2017-10-10T15:25:13.440994: step 6201, loss 0.127981, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:13.718566: step 6202, loss 0.0632, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:14.068884: step 6203, loss 0.0984613, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:14.444400: step 6204, loss 0.0834659, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:14.744895: step 6205, loss 0.0263647, acc 1, learning_rate 0.0001
2017-10-10T15:25:15.001910: step 6206, loss 0.0732133, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:15.309083: step 6207, loss 0.135157, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:15.552709: step 6208, loss 0.072616, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:15.837783: step 6209, loss 0.10518, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:16.157073: step 6210, loss 0.0687245, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:16.435786: step 6211, loss 0.0782139, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:16.849350: step 6212, loss 0.0351311, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:17.169013: step 6213, loss 0.0952763, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:17.410539: step 6214, loss 0.0652338, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:17.657718: step 6215, loss 0.0694274, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:17.860887: step 6216, loss 0.0392707, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:18.145403: step 6217, loss 0.077462, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:18.447156: step 6218, loss 0.0327428, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:18.728837: step 6219, loss 0.108983, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:19.037289: step 6220, loss 0.0663005, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:19.393502: step 6221, loss 0.105611, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:19.744869: step 6222, loss 0.0318935, acc 1, learning_rate 0.0001
2017-10-10T15:25:20.084877: step 6223, loss 0.0768029, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:20.432848: step 6224, loss 0.105867, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:20.799025: step 6225, loss 0.0620687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:21.062613: step 6226, loss 0.0249853, acc 1, learning_rate 0.0001
2017-10-10T15:25:21.288377: step 6227, loss 0.0804258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:21.556111: step 6228, loss 0.141717, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:21.859291: step 6229, loss 0.100766, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:22.171662: step 6230, loss 0.0571197, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:22.472857: step 6231, loss 0.0857685, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:22.767916: step 6232, loss 0.0384788, acc 1, learning_rate 0.0001
2017-10-10T15:25:23.103539: step 6233, loss 0.0903709, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:23.380849: step 6234, loss 0.10975, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:23.706443: step 6235, loss 0.0351334, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:24.002653: step 6236, loss 0.0741875, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:24.281129: step 6237, loss 0.0384608, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:24.558692: step 6238, loss 0.0598936, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:24.878759: step 6239, loss 0.0344674, acc 1, learning_rate 0.0001
2017-10-10T15:25:25.297973: step 6240, loss 0.103996, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:25.855541: step 6240, loss 0.193824, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6240

2017-10-10T15:25:27.021251: step 6241, loss 0.0606362, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:27.326707: step 6242, loss 0.176774, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:27.643926: step 6243, loss 0.0393729, acc 1, learning_rate 0.0001
2017-10-10T15:25:27.964760: step 6244, loss 0.0854023, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:28.264845: step 6245, loss 0.0577613, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:28.543516: step 6246, loss 0.0474833, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:28.898148: step 6247, loss 0.0475615, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:29.183171: step 6248, loss 0.117631, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:29.467270: step 6249, loss 0.047943, acc 1, learning_rate 0.0001
2017-10-10T15:25:29.753892: step 6250, loss 0.101389, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:30.040852: step 6251, loss 0.0903428, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:30.379076: step 6252, loss 0.114272, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:30.731775: step 6253, loss 0.0919443, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:31.081057: step 6254, loss 0.113602, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:31.314115: step 6255, loss 0.116062, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:31.537356: step 6256, loss 0.0808238, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:31.899357: step 6257, loss 0.0425548, acc 1, learning_rate 0.0001
2017-10-10T15:25:32.252966: step 6258, loss 0.0645642, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:32.503278: step 6259, loss 0.0274193, acc 1, learning_rate 0.0001
2017-10-10T15:25:32.757395: step 6260, loss 0.0313106, acc 1, learning_rate 0.0001
2017-10-10T15:25:32.983077: step 6261, loss 0.114608, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:33.239989: step 6262, loss 0.211251, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:33.548853: step 6263, loss 0.0694578, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:33.825120: step 6264, loss 0.10179, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:34.092467: step 6265, loss 0.122371, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:34.449700: step 6266, loss 0.07169, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:34.752158: step 6267, loss 0.0566328, acc 1, learning_rate 0.0001
2017-10-10T15:25:35.054984: step 6268, loss 0.208941, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:35.397055: step 6269, loss 0.0846036, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:35.758504: step 6270, loss 0.0484323, acc 1, learning_rate 0.0001
2017-10-10T15:25:36.144886: step 6271, loss 0.0440894, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:36.394315: step 6272, loss 0.0901876, acc 0.980392, learning_rate 0.0001
2017-10-10T15:25:36.702612: step 6273, loss 0.0437767, acc 1, learning_rate 0.0001
2017-10-10T15:25:36.981241: step 6274, loss 0.137852, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:37.309703: step 6275, loss 0.104224, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:37.600861: step 6276, loss 0.0673734, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:37.924957: step 6277, loss 0.0937547, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:38.255569: step 6278, loss 0.0972554, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:38.579850: step 6279, loss 0.0615938, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:38.902223: step 6280, loss 0.115282, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:39.492890: step 6280, loss 0.195551, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6280

2017-10-10T15:25:40.691478: step 6281, loss 0.113856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:40.973907: step 6282, loss 0.0656018, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:41.243704: step 6283, loss 0.0699265, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:41.628863: step 6284, loss 0.0496474, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:41.941938: step 6285, loss 0.101985, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:42.213238: step 6286, loss 0.054716, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:42.448883: step 6287, loss 0.0212141, acc 1, learning_rate 0.0001
2017-10-10T15:25:42.718263: step 6288, loss 0.0675998, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:43.027338: step 6289, loss 0.185099, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:43.327703: step 6290, loss 0.0567886, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:43.647035: step 6291, loss 0.0835158, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:43.955119: step 6292, loss 0.038245, acc 1, learning_rate 0.0001
2017-10-10T15:25:44.268907: step 6293, loss 0.0256116, acc 1, learning_rate 0.0001
2017-10-10T15:25:44.579954: step 6294, loss 0.156776, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:44.857179: step 6295, loss 0.0428691, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:45.232937: step 6296, loss 0.118807, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:45.529409: step 6297, loss 0.092009, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:45.814037: step 6298, loss 0.0617427, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:46.143093: step 6299, loss 0.0565476, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:46.516925: step 6300, loss 0.0424197, acc 1, learning_rate 0.0001
2017-10-10T15:25:46.892254: step 6301, loss 0.114086, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:47.230719: step 6302, loss 0.0691462, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:47.440889: step 6303, loss 0.0369661, acc 1, learning_rate 0.0001
2017-10-10T15:25:47.644396: step 6304, loss 0.0371929, acc 1, learning_rate 0.0001
2017-10-10T15:25:47.849456: step 6305, loss 0.0319243, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:48.177029: step 6306, loss 0.0951373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:48.489607: step 6307, loss 0.0298151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:48.817383: step 6308, loss 0.0680765, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:49.088960: step 6309, loss 0.064917, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:49.383631: step 6310, loss 0.0457646, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:49.713146: step 6311, loss 0.0852361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:50.000841: step 6312, loss 0.0610391, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:50.296849: step 6313, loss 0.0181035, acc 1, learning_rate 0.0001
2017-10-10T15:25:50.606906: step 6314, loss 0.0762897, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:50.899070: step 6315, loss 0.0885318, acc 1, learning_rate 0.0001
2017-10-10T15:25:51.212728: step 6316, loss 0.0941104, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:51.569523: step 6317, loss 0.059023, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:51.928858: step 6318, loss 0.0581147, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:52.231531: step 6319, loss 0.0448437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:52.517883: step 6320, loss 0.0744894, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:53.107450: step 6320, loss 0.193222, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6320

2017-10-10T15:25:54.328901: step 6321, loss 0.120773, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:54.690705: step 6322, loss 0.0695197, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:54.984823: step 6323, loss 0.0474662, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:55.245964: step 6324, loss 0.0812325, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:55.526089: step 6325, loss 0.0284847, acc 1, learning_rate 0.0001
2017-10-10T15:25:55.828072: step 6326, loss 0.048166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:56.148893: step 6327, loss 0.0609299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:56.441832: step 6328, loss 0.0724387, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:56.708432: step 6329, loss 0.0393533, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:56.991559: step 6330, loss 0.0184228, acc 1, learning_rate 0.0001
2017-10-10T15:25:57.308707: step 6331, loss 0.104867, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:57.618426: step 6332, loss 0.0414283, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:57.937652: step 6333, loss 0.0491515, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:58.250174: step 6334, loss 0.0869452, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:58.604252: step 6335, loss 0.0601407, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:58.896337: step 6336, loss 0.0312889, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:59.185969: step 6337, loss 0.0416687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:59.496820: step 6338, loss 0.0138626, acc 1, learning_rate 0.0001
2017-10-10T15:25:59.767292: step 6339, loss 0.0656184, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:00.100912: step 6340, loss 0.0234448, acc 1, learning_rate 0.0001
2017-10-10T15:26:00.424590: step 6341, loss 0.0487863, acc 1, learning_rate 0.0001
2017-10-10T15:26:00.724856: step 6342, loss 0.0556458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:01.024866: step 6343, loss 0.090923, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:01.357378: step 6344, loss 0.0915959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:01.696880: step 6345, loss 0.0567345, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:02.052967: step 6346, loss 0.0622489, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:02.268886: step 6347, loss 0.107579, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:02.484055: step 6348, loss 0.161987, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:02.674847: step 6349, loss 0.0722097, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:03.029758: step 6350, loss 0.0832907, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:03.349958: step 6351, loss 0.073297, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:03.650286: step 6352, loss 0.0607905, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:03.995535: step 6353, loss 0.0997845, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:04.296860: step 6354, loss 0.0771472, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:04.622106: step 6355, loss 0.0942149, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:04.887845: step 6356, loss 0.165111, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:05.162600: step 6357, loss 0.0278042, acc 1, learning_rate 0.0001
2017-10-10T15:26:05.456755: step 6358, loss 0.0742477, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:05.783171: step 6359, loss 0.0770864, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:06.127426: step 6360, loss 0.0681719, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:06.733285: step 6360, loss 0.193772, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6360

2017-10-10T15:26:07.955642: step 6361, loss 0.166486, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:08.257008: step 6362, loss 0.0927908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:08.583747: step 6363, loss 0.0633025, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:08.890639: step 6364, loss 0.101826, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:09.208922: step 6365, loss 0.0767679, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:09.588852: step 6366, loss 0.0328128, acc 1, learning_rate 0.0001
2017-10-10T15:26:09.861015: step 6367, loss 0.0613013, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:10.076975: step 6368, loss 0.0418398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:10.358978: step 6369, loss 0.0966789, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:10.631397: step 6370, loss 0.0328819, acc 1, learning_rate 0.0001
2017-10-10T15:26:10.996499: step 6371, loss 0.0508888, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:11.307766: step 6372, loss 0.0454509, acc 1, learning_rate 0.0001
2017-10-10T15:26:11.584841: step 6373, loss 0.0630307, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:11.855567: step 6374, loss 0.0759616, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:12.229305: step 6375, loss 0.0264499, acc 1, learning_rate 0.0001
2017-10-10T15:26:12.497658: step 6376, loss 0.133473, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:12.796506: step 6377, loss 0.121629, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:13.089634: step 6378, loss 0.0746205, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:13.404427: step 6379, loss 0.0813356, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:13.717284: step 6380, loss 0.15534, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:14.040721: step 6381, loss 0.159254, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:14.328837: step 6382, loss 0.0306169, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:14.676867: step 6383, loss 0.116878, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:14.994506: step 6384, loss 0.0373187, acc 1, learning_rate 0.0001
2017-10-10T15:26:15.325340: step 6385, loss 0.128578, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:15.628873: step 6386, loss 0.0408411, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:15.952081: step 6387, loss 0.0980895, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:16.254236: step 6388, loss 0.0519925, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:16.626817: step 6389, loss 0.0890941, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:16.926189: step 6390, loss 0.0960303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:17.177602: step 6391, loss 0.0664151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:17.438994: step 6392, loss 0.0890351, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:17.668599: step 6393, loss 0.0341886, acc 1, learning_rate 0.0001
2017-10-10T15:26:17.991136: step 6394, loss 0.0540669, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:18.340863: step 6395, loss 0.0276218, acc 1, learning_rate 0.0001
2017-10-10T15:26:18.664873: step 6396, loss 0.151855, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:18.979596: step 6397, loss 0.102253, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:19.343782: step 6398, loss 0.0504987, acc 1, learning_rate 0.0001
2017-10-10T15:26:19.663514: step 6399, loss 0.0346932, acc 1, learning_rate 0.0001
2017-10-10T15:26:20.048872: step 6400, loss 0.127963, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:20.646970: step 6400, loss 0.197225, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6400

2017-10-10T15:26:21.700356: step 6401, loss 0.0946917, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:21.985074: step 6402, loss 0.0676226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:22.276824: step 6403, loss 0.105884, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:22.684003: step 6404, loss 0.21544, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:22.947179: step 6405, loss 0.105087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:23.217992: step 6406, loss 0.0682682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:23.586231: step 6407, loss 0.140178, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:23.907043: step 6408, loss 0.0552169, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:24.214432: step 6409, loss 0.0425576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:24.512975: step 6410, loss 0.0914077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:24.797003: step 6411, loss 0.0636484, acc 1, learning_rate 0.0001
2017-10-10T15:26:25.163852: step 6412, loss 0.0230452, acc 1, learning_rate 0.0001
2017-10-10T15:26:25.471313: step 6413, loss 0.10373, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:25.780886: step 6414, loss 0.0298153, acc 1, learning_rate 0.0001
2017-10-10T15:26:26.127707: step 6415, loss 0.0569763, acc 1, learning_rate 0.0001
2017-10-10T15:26:26.424682: step 6416, loss 0.132043, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:26.761101: step 6417, loss 0.0484303, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:27.094880: step 6418, loss 0.0978647, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:27.392358: step 6419, loss 0.0795758, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:27.697323: step 6420, loss 0.135057, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:27.991099: step 6421, loss 0.0154003, acc 1, learning_rate 0.0001
2017-10-10T15:26:28.277209: step 6422, loss 0.0851151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:28.569602: step 6423, loss 0.12187, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:28.904464: step 6424, loss 0.0483694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:29.200256: step 6425, loss 0.132583, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:29.511932: step 6426, loss 0.0468152, acc 1, learning_rate 0.0001
2017-10-10T15:26:29.840882: step 6427, loss 0.0479819, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:30.163309: step 6428, loss 0.0137419, acc 1, learning_rate 0.0001
2017-10-10T15:26:30.543427: step 6429, loss 0.024049, acc 1, learning_rate 0.0001
2017-10-10T15:26:30.838832: step 6430, loss 0.0315761, acc 1, learning_rate 0.0001
2017-10-10T15:26:31.192896: step 6431, loss 0.102577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:31.618773: step 6432, loss 0.0319017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:31.812916: step 6433, loss 0.0363462, acc 1, learning_rate 0.0001
2017-10-10T15:26:32.046401: step 6434, loss 0.0502851, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:32.240729: step 6435, loss 0.0868408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:32.502054: step 6436, loss 0.0646618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:32.844942: step 6437, loss 0.109304, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:33.110444: step 6438, loss 0.0319042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:33.382197: step 6439, loss 0.0113934, acc 1, learning_rate 0.0001
2017-10-10T15:26:33.695701: step 6440, loss 0.0503812, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:34.291665: step 6440, loss 0.195383, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6440

2017-10-10T15:26:35.529123: step 6441, loss 0.0139804, acc 1, learning_rate 0.0001
2017-10-10T15:26:35.875328: step 6442, loss 0.0370506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:36.188942: step 6443, loss 0.0858446, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:36.484840: step 6444, loss 0.0858587, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:36.785587: step 6445, loss 0.072123, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:37.075504: step 6446, loss 0.0288308, acc 1, learning_rate 0.0001
2017-10-10T15:26:37.395402: step 6447, loss 0.0205613, acc 1, learning_rate 0.0001
2017-10-10T15:26:37.717997: step 6448, loss 0.0723706, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:37.979831: step 6449, loss 0.105733, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:38.316051: step 6450, loss 0.0750522, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:38.596736: step 6451, loss 0.109712, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:38.903799: step 6452, loss 0.0668861, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:39.244998: step 6453, loss 0.0599813, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:39.541696: step 6454, loss 0.0583979, acc 1, learning_rate 0.0001
2017-10-10T15:26:39.850280: step 6455, loss 0.0843216, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:40.143362: step 6456, loss 0.0266775, acc 1, learning_rate 0.0001
2017-10-10T15:26:40.432905: step 6457, loss 0.0819526, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:40.718464: step 6458, loss 0.023896, acc 1, learning_rate 0.0001
2017-10-10T15:26:40.999483: step 6459, loss 0.0385229, acc 1, learning_rate 0.0001
2017-10-10T15:26:41.312925: step 6460, loss 0.115728, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:41.639157: step 6461, loss 0.108246, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:41.964656: step 6462, loss 0.0581764, acc 1, learning_rate 0.0001
2017-10-10T15:26:42.233156: step 6463, loss 0.12265, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:42.607695: step 6464, loss 0.0418024, acc 1, learning_rate 0.0001
2017-10-10T15:26:42.872848: step 6465, loss 0.0714321, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:43.199694: step 6466, loss 0.0310971, acc 1, learning_rate 0.0001
2017-10-10T15:26:43.409379: step 6467, loss 0.0914508, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:43.652816: step 6468, loss 0.0425556, acc 0.980392, learning_rate 0.0001
2017-10-10T15:26:43.923651: step 6469, loss 0.0544254, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:44.254767: step 6470, loss 0.115625, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:44.577065: step 6471, loss 0.0231348, acc 1, learning_rate 0.0001
2017-10-10T15:26:44.890834: step 6472, loss 0.0741802, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:45.212839: step 6473, loss 0.0472268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:45.513020: step 6474, loss 0.0962252, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:45.820848: step 6475, loss 0.0974957, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:46.240265: step 6476, loss 0.130279, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:46.504256: step 6477, loss 0.146444, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:46.756843: step 6478, loss 0.11483, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:47.026754: step 6479, loss 0.0267082, acc 1, learning_rate 0.0001
2017-10-10T15:26:47.276828: step 6480, loss 0.161157, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:47.912909: step 6480, loss 0.198415, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6480

2017-10-10T15:26:49.046312: step 6481, loss 0.109514, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:49.336932: step 6482, loss 0.0873942, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:49.629496: step 6483, loss 0.117819, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:49.945146: step 6484, loss 0.0272755, acc 1, learning_rate 0.0001
2017-10-10T15:26:50.231781: step 6485, loss 0.0651518, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:50.501945: step 6486, loss 0.0691565, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:50.769066: step 6487, loss 0.037327, acc 1, learning_rate 0.0001
2017-10-10T15:26:51.125012: step 6488, loss 0.0955864, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:51.434177: step 6489, loss 0.111345, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:51.737003: step 6490, loss 0.0464985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:52.031190: step 6491, loss 0.0878025, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:52.368855: step 6492, loss 0.0781466, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:52.676749: step 6493, loss 0.0532447, acc 1, learning_rate 0.0001
2017-10-10T15:26:52.994664: step 6494, loss 0.0675184, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:53.380783: step 6495, loss 0.0613589, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:53.698802: step 6496, loss 0.099623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:53.929752: step 6497, loss 0.0769873, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:54.146834: step 6498, loss 0.0303409, acc 1, learning_rate 0.0001
2017-10-10T15:26:54.448003: step 6499, loss 0.0773031, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:54.760966: step 6500, loss 0.0311427, acc 1, learning_rate 0.0001
2017-10-10T15:26:55.063335: step 6501, loss 0.0371039, acc 1, learning_rate 0.0001
2017-10-10T15:26:55.340847: step 6502, loss 0.0702759, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:55.656748: step 6503, loss 0.0345381, acc 1, learning_rate 0.0001
2017-10-10T15:26:55.916545: step 6504, loss 0.039059, acc 1, learning_rate 0.0001
2017-10-10T15:26:56.188175: step 6505, loss 0.095965, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:56.490578: step 6506, loss 0.0667463, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:56.831113: step 6507, loss 0.0324466, acc 1, learning_rate 0.0001
2017-10-10T15:26:57.146812: step 6508, loss 0.0559085, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:57.466680: step 6509, loss 0.073153, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:57.785089: step 6510, loss 0.0367199, acc 1, learning_rate 0.0001
2017-10-10T15:26:58.113226: step 6511, loss 0.0421702, acc 1, learning_rate 0.0001
2017-10-10T15:26:58.390085: step 6512, loss 0.0440814, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:58.702018: step 6513, loss 0.10466, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:59.021569: step 6514, loss 0.079474, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:59.360876: step 6515, loss 0.0877708, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:59.706805: step 6516, loss 0.0362441, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:00.026198: step 6517, loss 0.13574, acc 0.921875, learning_rate 0.0001
2017-10-10T15:27:00.365344: step 6518, loss 0.0948184, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:00.677338: step 6519, loss 0.0341079, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:01.068956: step 6520, loss 0.0715574, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:01.679811: step 6520, loss 0.19466, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6520

2017-10-10T15:27:02.751333: step 6521, loss 0.0709087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:03.075466: step 6522, loss 0.0837812, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:03.392135: step 6523, loss 0.0143685, acc 1, learning_rate 0.0001
2017-10-10T15:27:03.684909: step 6524, loss 0.118356, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:03.968996: step 6525, loss 0.0653432, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:04.317596: step 6526, loss 0.106273, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:04.532994: step 6527, loss 0.122579, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:04.790080: step 6528, loss 0.0327523, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:05.070948: step 6529, loss 0.0664054, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:05.387294: step 6530, loss 0.0739279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:05.730793: step 6531, loss 0.0783059, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:06.076844: step 6532, loss 0.0987548, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:06.398991: step 6533, loss 0.0547667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:06.707981: step 6534, loss 0.0739649, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:07.010640: step 6535, loss 0.076593, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:07.299359: step 6536, loss 0.0840561, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:07.573559: step 6537, loss 0.0384027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:07.905217: step 6538, loss 0.0438211, acc 1, learning_rate 0.0001
2017-10-10T15:27:08.229551: step 6539, loss 0.0145664, acc 1, learning_rate 0.0001
2017-10-10T15:27:08.543401: step 6540, loss 0.0956301, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:08.816759: step 6541, loss 0.0230066, acc 1, learning_rate 0.0001
2017-10-10T15:27:09.126747: step 6542, loss 0.0283984, acc 1, learning_rate 0.0001
2017-10-10T15:27:09.423823: step 6543, loss 0.0345948, acc 1, learning_rate 0.0001
2017-10-10T15:27:09.710853: step 6544, loss 0.0897482, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:10.028623: step 6545, loss 0.114751, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:10.311142: step 6546, loss 0.0425448, acc 1, learning_rate 0.0001
2017-10-10T15:27:10.596878: step 6547, loss 0.0446429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:10.920808: step 6548, loss 0.0399117, acc 1, learning_rate 0.0001
2017-10-10T15:27:11.201156: step 6549, loss 0.0461488, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:11.540590: step 6550, loss 0.0665938, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:11.827540: step 6551, loss 0.0645173, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:12.120839: step 6552, loss 0.096213, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:12.437135: step 6553, loss 0.0886278, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:12.728051: step 6554, loss 0.0445668, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:13.067609: step 6555, loss 0.118231, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:13.389170: step 6556, loss 0.0215131, acc 1, learning_rate 0.0001
2017-10-10T15:27:13.672819: step 6557, loss 0.041116, acc 1, learning_rate 0.0001
2017-10-10T15:27:14.073337: step 6558, loss 0.0203333, acc 1, learning_rate 0.0001
2017-10-10T15:27:14.345830: step 6559, loss 0.0288317, acc 1, learning_rate 0.0001
2017-10-10T15:27:14.629587: step 6560, loss 0.0660871, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:15.288960: step 6560, loss 0.193775, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6560

2017-10-10T15:27:16.438444: step 6561, loss 0.0678869, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:16.684449: step 6562, loss 0.16957, acc 0.921875, learning_rate 0.0001
2017-10-10T15:27:16.946051: step 6563, loss 0.0373054, acc 1, learning_rate 0.0001
2017-10-10T15:27:17.268621: step 6564, loss 0.113053, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:17.596844: step 6565, loss 0.0401917, acc 1, learning_rate 0.0001
2017-10-10T15:27:17.885986: step 6566, loss 0.0445117, acc 1, learning_rate 0.0001
2017-10-10T15:27:18.172850: step 6567, loss 0.128193, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:18.473911: step 6568, loss 0.130306, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:18.824178: step 6569, loss 0.018028, acc 1, learning_rate 0.0001
2017-10-10T15:27:19.169088: step 6570, loss 0.0460212, acc 1, learning_rate 0.0001
2017-10-10T15:27:19.505792: step 6571, loss 0.0934247, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:19.820374: step 6572, loss 0.0348514, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:20.136833: step 6573, loss 0.149511, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:20.454528: step 6574, loss 0.0428683, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:20.762724: step 6575, loss 0.0162452, acc 1, learning_rate 0.0001
2017-10-10T15:27:21.069344: step 6576, loss 0.124528, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:21.437677: step 6577, loss 0.0939473, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:21.762800: step 6578, loss 0.0631501, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:22.033826: step 6579, loss 0.0279309, acc 1, learning_rate 0.0001
2017-10-10T15:27:22.322837: step 6580, loss 0.0975445, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:22.651182: step 6581, loss 0.0388895, acc 1, learning_rate 0.0001
2017-10-10T15:27:22.960948: step 6582, loss 0.0832188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:23.304836: step 6583, loss 0.0526536, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:23.660952: step 6584, loss 0.0635026, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:23.993002: step 6585, loss 0.0399766, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:24.343268: step 6586, loss 0.0281569, acc 1, learning_rate 0.0001
2017-10-10T15:27:24.659396: step 6587, loss 0.0526873, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:24.952501: step 6588, loss 0.08076, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:25.240362: step 6589, loss 0.0598949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:25.565531: step 6590, loss 0.115884, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:25.897150: step 6591, loss 0.0523888, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:26.270224: step 6592, loss 0.14093, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:26.520921: step 6593, loss 0.039826, acc 1, learning_rate 0.0001
2017-10-10T15:27:26.784625: step 6594, loss 0.0901853, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:27.041334: step 6595, loss 0.0818795, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:27.369034: step 6596, loss 0.111332, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:27.665130: step 6597, loss 0.0628871, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:28.009674: step 6598, loss 0.107857, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:28.324304: step 6599, loss 0.15681, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:28.642836: step 6600, loss 0.0334886, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:29.230787: step 6600, loss 0.192658, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6600

2017-10-10T15:27:30.420890: step 6601, loss 0.0441054, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:30.798745: step 6602, loss 0.0741355, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:31.121548: step 6603, loss 0.0246681, acc 1, learning_rate 0.0001
2017-10-10T15:27:31.376360: step 6604, loss 0.0655043, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:31.630384: step 6605, loss 0.0433905, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:31.850140: step 6606, loss 0.0864501, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:32.122503: step 6607, loss 0.0548843, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:32.450943: step 6608, loss 0.049915, acc 1, learning_rate 0.0001
2017-10-10T15:27:32.792594: step 6609, loss 0.0728359, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:33.116847: step 6610, loss 0.105246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:33.420168: step 6611, loss 0.0742293, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:33.696360: step 6612, loss 0.0536286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:33.969028: step 6613, loss 0.11387, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:34.356573: step 6614, loss 0.130906, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:34.620866: step 6615, loss 0.0709905, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:34.899404: step 6616, loss 0.125915, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:35.212700: step 6617, loss 0.135328, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:35.519122: step 6618, loss 0.0273085, acc 1, learning_rate 0.0001
2017-10-10T15:27:35.812831: step 6619, loss 0.147684, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:36.154752: step 6620, loss 0.0495983, acc 1, learning_rate 0.0001
2017-10-10T15:27:36.434414: step 6621, loss 0.0566555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:36.732897: step 6622, loss 0.0117052, acc 1, learning_rate 0.0001
2017-10-10T15:27:37.116567: step 6623, loss 0.0366498, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:37.411952: step 6624, loss 0.0891823, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:37.670008: step 6625, loss 0.0551583, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:37.916367: step 6626, loss 0.0694122, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:38.201215: step 6627, loss 0.0587336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:38.549934: step 6628, loss 0.0776457, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:38.851069: step 6629, loss 0.0420416, acc 1, learning_rate 0.0001
2017-10-10T15:27:39.144990: step 6630, loss 0.0656086, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:39.394481: step 6631, loss 0.0524221, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:39.696543: step 6632, loss 0.0938839, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:39.976973: step 6633, loss 0.0784062, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:40.279645: step 6634, loss 0.141907, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:40.591217: step 6635, loss 0.0716848, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:40.908895: step 6636, loss 0.119306, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:41.250939: step 6637, loss 0.075287, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:41.598938: step 6638, loss 0.115659, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:41.909849: step 6639, loss 0.0217662, acc 1, learning_rate 0.0001
2017-10-10T15:27:42.236119: step 6640, loss 0.0260855, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:42.825061: step 6640, loss 0.192283, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6640

2017-10-10T15:27:44.084804: step 6641, loss 0.0631541, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:44.395669: step 6642, loss 0.108248, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:44.709464: step 6643, loss 0.151328, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:44.970824: step 6644, loss 0.0662271, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:45.208955: step 6645, loss 0.0476063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:45.551670: step 6646, loss 0.113141, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:45.867829: step 6647, loss 0.17354, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:46.087597: step 6648, loss 0.0184442, acc 1, learning_rate 0.0001
2017-10-10T15:27:46.308951: step 6649, loss 0.119602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:46.542585: step 6650, loss 0.0597573, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:46.826004: step 6651, loss 0.104302, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:47.149149: step 6652, loss 0.0313282, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:47.447917: step 6653, loss 0.0692742, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:47.771258: step 6654, loss 0.0544503, acc 1, learning_rate 0.0001
2017-10-10T15:27:48.147702: step 6655, loss 0.045949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:48.482001: step 6656, loss 0.0776136, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:48.759751: step 6657, loss 0.0534557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:49.049312: step 6658, loss 0.0233231, acc 1, learning_rate 0.0001
2017-10-10T15:27:49.312437: step 6659, loss 0.0491097, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:49.555798: step 6660, loss 0.0361316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:49.872217: step 6661, loss 0.110726, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:50.185177: step 6662, loss 0.0269199, acc 1, learning_rate 0.0001
2017-10-10T15:27:50.533458: step 6663, loss 0.0759515, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:50.812859: step 6664, loss 0.0714388, acc 0.980392, learning_rate 0.0001
2017-10-10T15:27:51.149411: step 6665, loss 0.117544, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:51.444916: step 6666, loss 0.101175, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:51.740889: step 6667, loss 0.038368, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:52.044939: step 6668, loss 0.119565, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:52.308553: step 6669, loss 0.0842564, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:52.636584: step 6670, loss 0.0782038, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:52.924874: step 6671, loss 0.0623204, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:53.244909: step 6672, loss 0.0499712, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:53.557496: step 6673, loss 0.084389, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:53.876809: step 6674, loss 0.0624419, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:54.226628: step 6675, loss 0.0604054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:54.586354: step 6676, loss 0.0445947, acc 1, learning_rate 0.0001
2017-10-10T15:27:54.886591: step 6677, loss 0.0453547, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:55.153871: step 6678, loss 0.0452166, acc 1, learning_rate 0.0001
2017-10-10T15:27:55.400851: step 6679, loss 0.0642201, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:55.682540: step 6680, loss 0.0368471, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:56.267067: step 6680, loss 0.195621, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6680

2017-10-10T15:27:57.476889: step 6681, loss 0.0955614, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:57.786473: step 6682, loss 0.0852088, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:58.120867: step 6683, loss 0.100848, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:58.425937: step 6684, loss 0.180944, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:58.735530: step 6685, loss 0.0360467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:59.064526: step 6686, loss 0.0888756, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:59.428404: step 6687, loss 0.0866718, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:59.711614: step 6688, loss 0.0574119, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:59.960899: step 6689, loss 0.0309171, acc 1, learning_rate 0.0001
2017-10-10T15:28:00.232891: step 6690, loss 0.120445, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:00.608876: step 6691, loss 0.0212671, acc 1, learning_rate 0.0001
2017-10-10T15:28:00.930556: step 6692, loss 0.0869054, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:01.158882: step 6693, loss 0.0395244, acc 1, learning_rate 0.0001
2017-10-10T15:28:01.406481: step 6694, loss 0.040715, acc 1, learning_rate 0.0001
2017-10-10T15:28:01.648802: step 6695, loss 0.0583007, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:01.936852: step 6696, loss 0.113892, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:02.224987: step 6697, loss 0.0663634, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:02.490573: step 6698, loss 0.0561761, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:02.796135: step 6699, loss 0.0896568, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:03.123240: step 6700, loss 0.0837367, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:03.413832: step 6701, loss 0.0872872, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:03.731644: step 6702, loss 0.0424429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:04.050375: step 6703, loss 0.0619945, acc 1, learning_rate 0.0001
2017-10-10T15:28:04.343067: step 6704, loss 0.135502, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:04.586866: step 6705, loss 0.0655163, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:04.959864: step 6706, loss 0.0675579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:05.231155: step 6707, loss 0.017177, acc 1, learning_rate 0.0001
2017-10-10T15:28:05.526730: step 6708, loss 0.0651501, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:05.798283: step 6709, loss 0.119425, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:06.123553: step 6710, loss 0.0173562, acc 1, learning_rate 0.0001
2017-10-10T15:28:06.469912: step 6711, loss 0.0326637, acc 1, learning_rate 0.0001
2017-10-10T15:28:06.752831: step 6712, loss 0.0241555, acc 1, learning_rate 0.0001
2017-10-10T15:28:07.068971: step 6713, loss 0.0339635, acc 1, learning_rate 0.0001
2017-10-10T15:28:07.365382: step 6714, loss 0.063117, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:07.648830: step 6715, loss 0.133458, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:07.916949: step 6716, loss 0.0936908, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:08.264845: step 6717, loss 0.0410469, acc 1, learning_rate 0.0001
2017-10-10T15:28:08.587103: step 6718, loss 0.0370381, acc 1, learning_rate 0.0001
2017-10-10T15:28:08.908853: step 6719, loss 0.0109042, acc 1, learning_rate 0.0001
2017-10-10T15:28:09.236097: step 6720, loss 0.0946924, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:09.804937: step 6720, loss 0.194894, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6720

2017-10-10T15:28:10.817601: step 6721, loss 0.0190493, acc 1, learning_rate 0.0001
2017-10-10T15:28:11.093932: step 6722, loss 0.0301, acc 1, learning_rate 0.0001
2017-10-10T15:28:11.382179: step 6723, loss 0.0845443, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:11.662983: step 6724, loss 0.120789, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:11.976995: step 6725, loss 0.044903, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:12.324920: step 6726, loss 0.0570018, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:12.652856: step 6727, loss 0.0346508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:12.949118: step 6728, loss 0.0163377, acc 1, learning_rate 0.0001
2017-10-10T15:28:13.260909: step 6729, loss 0.0665325, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:13.564956: step 6730, loss 0.109122, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:13.879155: step 6731, loss 0.11711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:14.135493: step 6732, loss 0.0423554, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:14.444381: step 6733, loss 0.0742327, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:14.796854: step 6734, loss 0.0477439, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:15.098188: step 6735, loss 0.129646, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:15.808751: step 6736, loss 0.13172, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:15.997607: step 6737, loss 0.0853333, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:16.206780: step 6738, loss 0.0620988, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:16.434838: step 6739, loss 0.0862206, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:16.718969: step 6740, loss 0.0635383, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:16.989052: step 6741, loss 0.0332939, acc 1, learning_rate 0.0001
2017-10-10T15:28:17.308841: step 6742, loss 0.0404811, acc 1, learning_rate 0.0001
2017-10-10T15:28:17.625879: step 6743, loss 0.0763654, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:17.935702: step 6744, loss 0.0819971, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:18.237632: step 6745, loss 0.0116714, acc 1, learning_rate 0.0001
2017-10-10T15:28:18.520405: step 6746, loss 0.0308362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:18.818090: step 6747, loss 0.0918393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:19.104815: step 6748, loss 0.0514099, acc 1, learning_rate 0.0001
2017-10-10T15:28:19.387621: step 6749, loss 0.0935914, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:19.720854: step 6750, loss 0.0836133, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:20.048428: step 6751, loss 0.0466134, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:20.346857: step 6752, loss 0.0954274, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:20.622294: step 6753, loss 0.0489604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:20.901002: step 6754, loss 0.0913825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:21.176829: step 6755, loss 0.0216246, acc 1, learning_rate 0.0001
2017-10-10T15:28:21.576204: step 6756, loss 0.0777582, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:21.849418: step 6757, loss 0.0395782, acc 1, learning_rate 0.0001
2017-10-10T15:28:22.068870: step 6758, loss 0.0588923, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:22.344850: step 6759, loss 0.128999, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:22.675224: step 6760, loss 0.0944178, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:23.314375: step 6760, loss 0.19175, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6760

2017-10-10T15:28:24.501015: step 6761, loss 0.0610644, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:24.764902: step 6762, loss 0.0701891, acc 0.980392, learning_rate 0.0001
2017-10-10T15:28:25.080860: step 6763, loss 0.0483317, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:25.350593: step 6764, loss 0.116098, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:25.734172: step 6765, loss 0.0721393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:26.013136: step 6766, loss 0.0541713, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:26.308000: step 6767, loss 0.0518718, acc 1, learning_rate 0.0001
2017-10-10T15:28:26.624954: step 6768, loss 0.104484, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:26.893050: step 6769, loss 0.0969565, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:27.164039: step 6770, loss 0.0456175, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:27.432891: step 6771, loss 0.0687458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:27.710810: step 6772, loss 0.108044, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:27.975713: step 6773, loss 0.0424822, acc 1, learning_rate 0.0001
2017-10-10T15:28:28.234491: step 6774, loss 0.0629447, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:28.500528: step 6775, loss 0.0267434, acc 1, learning_rate 0.0001
2017-10-10T15:28:28.744588: step 6776, loss 0.0199774, acc 1, learning_rate 0.0001
2017-10-10T15:28:28.998065: step 6777, loss 0.0777778, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:29.224844: step 6778, loss 0.0257818, acc 1, learning_rate 0.0001
2017-10-10T15:28:29.476935: step 6779, loss 0.0294489, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:29.700892: step 6780, loss 0.146113, acc 0.921875, learning_rate 0.0001
2017-10-10T15:28:30.037378: step 6781, loss 0.0405267, acc 1, learning_rate 0.0001
2017-10-10T15:28:30.245880: step 6782, loss 0.0638294, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:30.428210: step 6783, loss 0.0939047, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:30.611727: step 6784, loss 0.0339497, acc 1, learning_rate 0.0001
2017-10-10T15:28:30.806070: step 6785, loss 0.0545866, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:31.060882: step 6786, loss 0.0756277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:31.292687: step 6787, loss 0.0195349, acc 1, learning_rate 0.0001
2017-10-10T15:28:31.520058: step 6788, loss 0.0343855, acc 1, learning_rate 0.0001
2017-10-10T15:28:31.735183: step 6789, loss 0.0711689, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:31.951335: step 6790, loss 0.0375749, acc 1, learning_rate 0.0001
2017-10-10T15:28:32.192379: step 6791, loss 0.0506161, acc 1, learning_rate 0.0001
2017-10-10T15:28:32.442218: step 6792, loss 0.10257, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:32.638184: step 6793, loss 0.0867372, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:32.888833: step 6794, loss 0.0710661, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:33.142633: step 6795, loss 0.10586, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:33.374806: step 6796, loss 0.0392705, acc 1, learning_rate 0.0001
2017-10-10T15:28:33.636977: step 6797, loss 0.0877589, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:33.891591: step 6798, loss 0.0406067, acc 1, learning_rate 0.0001
2017-10-10T15:28:34.159784: step 6799, loss 0.0593774, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:34.434911: step 6800, loss 0.035691, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:34.931097: step 6800, loss 0.19378, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6800

2017-10-10T15:28:35.976891: step 6801, loss 0.0861557, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:36.237717: step 6802, loss 0.101537, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:36.483227: step 6803, loss 0.0657082, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:36.748867: step 6804, loss 0.102133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:37.025878: step 6805, loss 0.0625357, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:37.288537: step 6806, loss 0.118537, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:37.546260: step 6807, loss 0.0448746, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:37.820589: step 6808, loss 0.052472, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:38.087222: step 6809, loss 0.0527122, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:38.327027: step 6810, loss 0.0341866, acc 1, learning_rate 0.0001
2017-10-10T15:28:38.592547: step 6811, loss 0.0349267, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:38.843466: step 6812, loss 0.0685921, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:39.096868: step 6813, loss 0.0400155, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:39.389351: step 6814, loss 0.0709763, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:39.638443: step 6815, loss 0.0571215, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:39.928856: step 6816, loss 0.101458, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:40.140891: step 6817, loss 0.059803, acc 1, learning_rate 0.0001
2017-10-10T15:28:40.359401: step 6818, loss 0.0290745, acc 1, learning_rate 0.0001
2017-10-10T15:28:40.518941: step 6819, loss 0.0571997, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:40.706670: step 6820, loss 0.0806447, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:40.952484: step 6821, loss 0.0579535, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:41.225106: step 6822, loss 0.102089, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:41.509475: step 6823, loss 0.0565522, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:41.820855: step 6824, loss 0.02921, acc 1, learning_rate 0.0001
2017-10-10T15:28:42.126934: step 6825, loss 0.0636311, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:42.312276: step 6826, loss 0.0493253, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:42.500850: step 6827, loss 0.0945304, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:42.684634: step 6828, loss 0.0151442, acc 1, learning_rate 0.0001
2017-10-10T15:28:42.866997: step 6829, loss 0.025375, acc 1, learning_rate 0.0001
2017-10-10T15:28:43.130839: step 6830, loss 0.0207973, acc 1, learning_rate 0.0001
2017-10-10T15:28:43.384882: step 6831, loss 0.0557001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:43.566040: step 6832, loss 0.0712036, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:43.795590: step 6833, loss 0.0482154, acc 1, learning_rate 0.0001
2017-10-10T15:28:44.060359: step 6834, loss 0.0795202, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:44.311626: step 6835, loss 0.0553474, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:44.564718: step 6836, loss 0.0890168, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:44.802482: step 6837, loss 0.11585, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:45.064837: step 6838, loss 0.0531564, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:45.286338: step 6839, loss 0.0511355, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:45.500088: step 6840, loss 0.0503139, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:46.008710: step 6840, loss 0.192047, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6840

2017-10-10T15:28:47.085489: step 6841, loss 0.0715533, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:47.281040: step 6842, loss 0.0375563, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:47.542262: step 6843, loss 0.0794476, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:47.816839: step 6844, loss 0.0726878, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:48.049440: step 6845, loss 0.201516, acc 0.921875, learning_rate 0.0001
2017-10-10T15:28:48.263198: step 6846, loss 0.0841058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:48.576827: step 6847, loss 0.0192451, acc 1, learning_rate 0.0001
2017-10-10T15:28:48.838370: step 6848, loss 0.0578487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:49.051159: step 6849, loss 0.0782163, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:49.275253: step 6850, loss 0.136143, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:49.506693: step 6851, loss 0.0652534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:49.757419: step 6852, loss 0.0197654, acc 1, learning_rate 0.0001
2017-10-10T15:28:49.994888: step 6853, loss 0.0503935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:50.232909: step 6854, loss 0.0488511, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:50.474285: step 6855, loss 0.153722, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:50.708518: step 6856, loss 0.0540071, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:50.920074: step 6857, loss 0.0963043, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:51.165007: step 6858, loss 0.090437, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:51.404760: step 6859, loss 0.104832, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:51.627357: step 6860, loss 0.0930442, acc 0.960784, learning_rate 0.0001
2017-10-10T15:28:51.904661: step 6861, loss 0.0786668, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:52.160941: step 6862, loss 0.126371, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:52.402629: step 6863, loss 0.0369233, acc 1, learning_rate 0.0001
2017-10-10T15:28:52.651953: step 6864, loss 0.0470184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:52.890116: step 6865, loss 0.0538793, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:53.112458: step 6866, loss 0.024248, acc 1, learning_rate 0.0001
2017-10-10T15:28:53.323882: step 6867, loss 0.0476891, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:53.578927: step 6868, loss 0.0480579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:53.924946: step 6869, loss 0.0479749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:54.187911: step 6870, loss 0.01828, acc 1, learning_rate 0.0001
2017-10-10T15:28:54.363696: step 6871, loss 0.0570727, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:54.551217: step 6872, loss 0.060887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:54.725261: step 6873, loss 0.0732969, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:54.890068: step 6874, loss 0.0663388, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:55.101082: step 6875, loss 0.045262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:55.316235: step 6876, loss 0.108219, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:55.573022: step 6877, loss 0.0360204, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:55.826733: step 6878, loss 0.110093, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:56.087961: step 6879, loss 0.0800493, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:56.323902: step 6880, loss 0.0628701, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:56.809029: step 6880, loss 0.193123, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6880

2017-10-10T15:28:57.893447: step 6881, loss 0.0599636, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:58.086384: step 6882, loss 0.0972106, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:58.308872: step 6883, loss 0.0592251, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:58.544666: step 6884, loss 0.068945, acc 1, learning_rate 0.0001
2017-10-10T15:28:58.813373: step 6885, loss 0.0820168, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:59.064839: step 6886, loss 0.124595, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:59.322670: step 6887, loss 0.110322, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:59.602197: step 6888, loss 0.140893, acc 0.921875, learning_rate 0.0001
2017-10-10T15:28:59.852316: step 6889, loss 0.162297, acc 0.921875, learning_rate 0.0001
2017-10-10T15:29:00.093569: step 6890, loss 0.159758, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:00.287402: step 6891, loss 0.0683555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:00.565693: step 6892, loss 0.0175382, acc 1, learning_rate 0.0001
2017-10-10T15:29:00.759821: step 6893, loss 0.0462253, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:01.009451: step 6894, loss 0.102214, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:01.261935: step 6895, loss 0.0262793, acc 1, learning_rate 0.0001
2017-10-10T15:29:01.523179: step 6896, loss 0.0202585, acc 1, learning_rate 0.0001
2017-10-10T15:29:01.808843: step 6897, loss 0.13588, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:02.062687: step 6898, loss 0.0440967, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:02.330802: step 6899, loss 0.0197143, acc 1, learning_rate 0.0001
2017-10-10T15:29:02.536562: step 6900, loss 0.0592421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:02.774948: step 6901, loss 0.0386717, acc 1, learning_rate 0.0001
2017-10-10T15:29:03.001422: step 6902, loss 0.10975, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:03.280890: step 6903, loss 0.137302, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:03.531545: step 6904, loss 0.0367156, acc 1, learning_rate 0.0001
2017-10-10T15:29:03.784939: step 6905, loss 0.0266138, acc 1, learning_rate 0.0001
2017-10-10T15:29:04.016456: step 6906, loss 0.106543, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:04.241711: step 6907, loss 0.0622557, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:04.488991: step 6908, loss 0.0580305, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:04.714888: step 6909, loss 0.0656769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:04.972982: step 6910, loss 0.0342703, acc 1, learning_rate 0.0001
2017-10-10T15:29:05.228847: step 6911, loss 0.0159213, acc 1, learning_rate 0.0001
2017-10-10T15:29:05.485734: step 6912, loss 0.0935547, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:05.772759: step 6913, loss 0.103495, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:06.109810: step 6914, loss 0.0245501, acc 1, learning_rate 0.0001
2017-10-10T15:29:06.272424: step 6915, loss 0.125358, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:06.516908: step 6916, loss 0.0384301, acc 1, learning_rate 0.0001
2017-10-10T15:29:06.689848: step 6917, loss 0.043335, acc 1, learning_rate 0.0001
2017-10-10T15:29:06.900822: step 6918, loss 0.0361862, acc 1, learning_rate 0.0001
2017-10-10T15:29:07.086245: step 6919, loss 0.0296421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:07.315780: step 6920, loss 0.102344, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:07.790263: step 6920, loss 0.194659, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6920

2017-10-10T15:29:08.856775: step 6921, loss 0.0355408, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:09.108853: step 6922, loss 0.0391742, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:09.360683: step 6923, loss 0.0770873, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:09.641606: step 6924, loss 0.0226564, acc 1, learning_rate 0.0001
2017-10-10T15:29:09.895403: step 6925, loss 0.125136, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:10.112839: step 6926, loss 0.0943701, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:10.324859: step 6927, loss 0.148675, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:10.557050: step 6928, loss 0.0542401, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:10.844867: step 6929, loss 0.0244698, acc 1, learning_rate 0.0001
2017-10-10T15:29:11.098979: step 6930, loss 0.0760137, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:11.339162: step 6931, loss 0.0605825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:11.622269: step 6932, loss 0.0905853, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:11.881227: step 6933, loss 0.0884208, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:12.146355: step 6934, loss 0.0679713, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:12.377386: step 6935, loss 0.10021, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:12.638833: step 6936, loss 0.0533092, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:12.888813: step 6937, loss 0.0566651, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:13.137967: step 6938, loss 0.0725016, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:13.411145: step 6939, loss 0.0689054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:13.672853: step 6940, loss 0.10065, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:13.912832: step 6941, loss 0.0466565, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:14.156930: step 6942, loss 0.136646, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:14.416942: step 6943, loss 0.0628892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:14.650321: step 6944, loss 0.03291, acc 1, learning_rate 0.0001
2017-10-10T15:29:14.896875: step 6945, loss 0.0905165, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:15.088337: step 6946, loss 0.0610516, acc 1, learning_rate 0.0001
2017-10-10T15:29:15.326430: step 6947, loss 0.0652281, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:15.620839: step 6948, loss 0.0396011, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:15.877269: step 6949, loss 0.0970862, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:16.103263: step 6950, loss 0.0343883, acc 1, learning_rate 0.0001
2017-10-10T15:29:16.316808: step 6951, loss 0.0669236, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:16.490377: step 6952, loss 0.0836673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:16.716913: step 6953, loss 0.0885783, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:16.932869: step 6954, loss 0.0490351, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:17.160450: step 6955, loss 0.0664236, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:17.464794: step 6956, loss 0.0543921, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:17.728869: step 6957, loss 0.0448066, acc 1, learning_rate 0.0001
2017-10-10T15:29:17.881012: step 6958, loss 0.0379251, acc 1, learning_rate 0.0001
2017-10-10T15:29:18.092248: step 6959, loss 0.0285101, acc 1, learning_rate 0.0001
2017-10-10T15:29:18.259040: step 6960, loss 0.0840041, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:18.701794: step 6960, loss 0.194123, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-6960

2017-10-10T15:29:19.763884: step 6961, loss 0.0466268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:19.972449: step 6962, loss 0.0522103, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:20.204283: step 6963, loss 0.0420156, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:20.461775: step 6964, loss 0.0285101, acc 1, learning_rate 0.0001
2017-10-10T15:29:20.704371: step 6965, loss 0.0866813, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:20.944298: step 6966, loss 0.114629, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:21.203260: step 6967, loss 0.0439147, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:21.490021: step 6968, loss 0.0237973, acc 1, learning_rate 0.0001
2017-10-10T15:29:21.756851: step 6969, loss 0.145217, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:22.043849: step 6970, loss 0.0677317, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:22.317717: step 6971, loss 0.0647495, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:22.586001: step 6972, loss 0.152289, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:22.846021: step 6973, loss 0.0693563, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:23.111655: step 6974, loss 0.0195979, acc 1, learning_rate 0.0001
2017-10-10T15:29:23.362702: step 6975, loss 0.0625472, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:23.608859: step 6976, loss 0.0217794, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:23.852948: step 6977, loss 0.110544, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:24.116854: step 6978, loss 0.029714, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:24.404927: step 6979, loss 0.0848574, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:24.633106: step 6980, loss 0.0934827, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:24.823080: step 6981, loss 0.0930353, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:25.036360: step 6982, loss 0.0442109, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:25.245340: step 6983, loss 0.0597238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:25.500848: step 6984, loss 0.0595034, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:25.756837: step 6985, loss 0.0609359, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:26.011274: step 6986, loss 0.0395837, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:26.260524: step 6987, loss 0.0187158, acc 1, learning_rate 0.0001
2017-10-10T15:29:26.501507: step 6988, loss 0.0537908, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:26.786825: step 6989, loss 0.112244, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:27.031093: step 6990, loss 0.0961465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:27.288403: step 6991, loss 0.0288713, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:27.573342: step 6992, loss 0.0628417, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:27.838864: step 6993, loss 0.0273773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:28.088897: step 6994, loss 0.0435653, acc 1, learning_rate 0.0001
2017-10-10T15:29:28.353568: step 6995, loss 0.0515607, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:28.633688: step 6996, loss 0.0413065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:28.891427: step 6997, loss 0.124323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:29.160853: step 6998, loss 0.0341914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:29.485012: step 6999, loss 0.0819647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:29.684049: step 7000, loss 0.0394929, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:30.081983: step 7000, loss 0.193667, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7000

2017-10-10T15:29:31.133769: step 7001, loss 0.0251258, acc 1, learning_rate 0.0001
2017-10-10T15:29:31.389230: step 7002, loss 0.0332193, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:31.634001: step 7003, loss 0.12526, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:31.894750: step 7004, loss 0.0922319, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:32.141967: step 7005, loss 0.0280127, acc 1, learning_rate 0.0001
2017-10-10T15:29:32.377043: step 7006, loss 0.0954962, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:32.616851: step 7007, loss 0.0256528, acc 1, learning_rate 0.0001
2017-10-10T15:29:32.869123: step 7008, loss 0.0311135, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:33.173034: step 7009, loss 0.117877, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:33.474326: step 7010, loss 0.0518212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:33.678436: step 7011, loss 0.0753654, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:33.864827: step 7012, loss 0.0859019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:34.100899: step 7013, loss 0.107609, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:34.348810: step 7014, loss 0.0423467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:34.569838: step 7015, loss 0.0357036, acc 1, learning_rate 0.0001
2017-10-10T15:29:34.824867: step 7016, loss 0.0904083, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:35.073468: step 7017, loss 0.0342384, acc 1, learning_rate 0.0001
2017-10-10T15:29:35.281050: step 7018, loss 0.0790271, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:35.524158: step 7019, loss 0.0657673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:35.790316: step 7020, loss 0.102418, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:36.028150: step 7021, loss 0.0669813, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:36.287521: step 7022, loss 0.0469722, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:36.536870: step 7023, loss 0.0525843, acc 1, learning_rate 0.0001
2017-10-10T15:29:36.772355: step 7024, loss 0.0776507, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:37.005569: step 7025, loss 0.0387021, acc 1, learning_rate 0.0001
2017-10-10T15:29:37.248166: step 7026, loss 0.0947251, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:37.478902: step 7027, loss 0.138897, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:37.701011: step 7028, loss 0.0807192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:37.971785: step 7029, loss 0.0705274, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:38.234427: step 7030, loss 0.0632731, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:38.497021: step 7031, loss 0.0721227, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:38.744781: step 7032, loss 0.0360002, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:39.004793: step 7033, loss 0.0700994, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:39.217753: step 7034, loss 0.103612, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:39.452214: step 7035, loss 0.137682, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:39.671669: step 7036, loss 0.0417245, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:39.927399: step 7037, loss 0.0291355, acc 1, learning_rate 0.0001
2017-10-10T15:29:40.192471: step 7038, loss 0.0665049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:40.454848: step 7039, loss 0.11266, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:40.710635: step 7040, loss 0.0347184, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:41.305749: step 7040, loss 0.19325, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7040

2017-10-10T15:29:42.178080: step 7041, loss 0.0995081, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:42.359604: step 7042, loss 0.073213, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:42.640478: step 7043, loss 0.0259289, acc 1, learning_rate 0.0001
2017-10-10T15:29:42.889339: step 7044, loss 0.0964295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:43.085717: step 7045, loss 0.0424759, acc 1, learning_rate 0.0001
2017-10-10T15:29:43.339876: step 7046, loss 0.0650604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:43.555632: step 7047, loss 0.0550388, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:43.828873: step 7048, loss 0.0890931, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:44.107340: step 7049, loss 0.104239, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:44.380890: step 7050, loss 0.0265323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:44.662601: step 7051, loss 0.0409269, acc 1, learning_rate 0.0001
2017-10-10T15:29:44.922709: step 7052, loss 0.0863078, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:45.179318: step 7053, loss 0.025339, acc 1, learning_rate 0.0001
2017-10-10T15:29:45.452804: step 7054, loss 0.103519, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:45.724451: step 7055, loss 0.144054, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:45.985347: step 7056, loss 0.0918395, acc 0.960784, learning_rate 0.0001
2017-10-10T15:29:46.251644: step 7057, loss 0.0177638, acc 1, learning_rate 0.0001
2017-10-10T15:29:46.502635: step 7058, loss 0.0589601, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:46.762661: step 7059, loss 0.0802487, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:47.019876: step 7060, loss 0.080554, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:47.289100: step 7061, loss 0.0483699, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:47.550776: step 7062, loss 0.024468, acc 1, learning_rate 0.0001
2017-10-10T15:29:47.810000: step 7063, loss 0.0246217, acc 1, learning_rate 0.0001
2017-10-10T15:29:48.042340: step 7064, loss 0.0524208, acc 1, learning_rate 0.0001
2017-10-10T15:29:48.288201: step 7065, loss 0.0512421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:48.510284: step 7066, loss 0.0652063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:48.781809: step 7067, loss 0.0538302, acc 1, learning_rate 0.0001
2017-10-10T15:29:49.053472: step 7068, loss 0.0400566, acc 1, learning_rate 0.0001
2017-10-10T15:29:49.298316: step 7069, loss 0.0608596, acc 1, learning_rate 0.0001
2017-10-10T15:29:49.501520: step 7070, loss 0.0796964, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:49.711366: step 7071, loss 0.0454488, acc 1, learning_rate 0.0001
2017-10-10T15:29:49.961158: step 7072, loss 0.0541633, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:50.218479: step 7073, loss 0.0224967, acc 1, learning_rate 0.0001
2017-10-10T15:29:50.429507: step 7074, loss 0.0486412, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:50.662577: step 7075, loss 0.0997228, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:50.979838: step 7076, loss 0.0239497, acc 1, learning_rate 0.0001
2017-10-10T15:29:51.199777: step 7077, loss 0.0315621, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:51.423434: step 7078, loss 0.0797668, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:51.637527: step 7079, loss 0.0679012, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:51.888293: step 7080, loss 0.0985503, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:52.413672: step 7080, loss 0.193755, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7080

2017-10-10T15:29:53.334490: step 7081, loss 0.0888579, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:53.505304: step 7082, loss 0.0520229, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:53.692902: step 7083, loss 0.0365383, acc 1, learning_rate 0.0001
2017-10-10T15:29:53.947123: step 7084, loss 0.0189586, acc 1, learning_rate 0.0001
2017-10-10T15:29:54.174564: step 7085, loss 0.0395901, acc 1, learning_rate 0.0001
2017-10-10T15:29:54.389931: step 7086, loss 0.0228672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:54.620877: step 7087, loss 0.0514718, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:54.889198: step 7088, loss 0.0368062, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:55.135263: step 7089, loss 0.0276896, acc 1, learning_rate 0.0001
2017-10-10T15:29:55.385454: step 7090, loss 0.0719707, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:55.636781: step 7091, loss 0.076489, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:55.890002: step 7092, loss 0.059615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:56.157763: step 7093, loss 0.0592165, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:56.409472: step 7094, loss 0.04055, acc 1, learning_rate 0.0001
2017-10-10T15:29:56.654716: step 7095, loss 0.0692715, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:56.900356: step 7096, loss 0.0439159, acc 1, learning_rate 0.0001
2017-10-10T15:29:57.119118: step 7097, loss 0.0589841, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:57.370605: step 7098, loss 0.103601, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:57.580763: step 7099, loss 0.0624638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:57.817350: step 7100, loss 0.0517175, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:58.066129: step 7101, loss 0.0531141, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:58.312157: step 7102, loss 0.0517962, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:58.563977: step 7103, loss 0.0890022, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:58.788082: step 7104, loss 0.0901499, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:58.993064: step 7105, loss 0.0331579, acc 1, learning_rate 0.0001
2017-10-10T15:29:59.252196: step 7106, loss 0.0761108, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:59.531958: step 7107, loss 0.0396713, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:59.852768: step 7108, loss 0.0460929, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:00.036998: step 7109, loss 0.0551668, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:00.210821: step 7110, loss 0.0643519, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:00.410092: step 7111, loss 0.0978909, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:00.665254: step 7112, loss 0.0599174, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:00.916967: step 7113, loss 0.130696, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:01.168624: step 7114, loss 0.0823279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:01.414040: step 7115, loss 0.0294307, acc 1, learning_rate 0.0001
2017-10-10T15:30:01.637259: step 7116, loss 0.0981559, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:01.840125: step 7117, loss 0.0268281, acc 1, learning_rate 0.0001
2017-10-10T15:30:02.087058: step 7118, loss 0.0252171, acc 1, learning_rate 0.0001
2017-10-10T15:30:02.328862: step 7119, loss 0.119751, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:02.567180: step 7120, loss 0.0425775, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:03.096404: step 7120, loss 0.191844, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7120

2017-10-10T15:30:04.200969: step 7121, loss 0.159998, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:04.431379: step 7122, loss 0.0644534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:04.741064: step 7123, loss 0.111348, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:04.957092: step 7124, loss 0.062596, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:05.145042: step 7125, loss 0.066957, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:05.325350: step 7126, loss 0.0577722, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:05.494525: step 7127, loss 0.0614717, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:05.690091: step 7128, loss 0.0387054, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:05.984202: step 7129, loss 0.079846, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:06.246576: step 7130, loss 0.0463702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:06.501232: step 7131, loss 0.0411871, acc 1, learning_rate 0.0001
2017-10-10T15:30:06.764666: step 7132, loss 0.0606142, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:07.014976: step 7133, loss 0.0540572, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:07.265050: step 7134, loss 0.0741836, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:07.546560: step 7135, loss 0.13851, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:07.799282: step 7136, loss 0.0736194, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:08.059634: step 7137, loss 0.0469478, acc 1, learning_rate 0.0001
2017-10-10T15:30:08.326853: step 7138, loss 0.0324895, acc 1, learning_rate 0.0001
2017-10-10T15:30:08.614291: step 7139, loss 0.0491081, acc 1, learning_rate 0.0001
2017-10-10T15:30:08.833169: step 7140, loss 0.0468318, acc 1, learning_rate 0.0001
2017-10-10T15:30:09.012813: step 7141, loss 0.048472, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:09.225142: step 7142, loss 0.080606, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:09.433182: step 7143, loss 0.0377553, acc 1, learning_rate 0.0001
2017-10-10T15:30:09.695733: step 7144, loss 0.0295113, acc 1, learning_rate 0.0001
2017-10-10T15:30:09.948584: step 7145, loss 0.0567108, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:10.190104: step 7146, loss 0.119084, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:10.432915: step 7147, loss 0.0609632, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:10.697959: step 7148, loss 0.048343, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:10.903894: step 7149, loss 0.11405, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:11.152458: step 7150, loss 0.0391187, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:11.387850: step 7151, loss 0.0662374, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:11.655471: step 7152, loss 0.145134, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:11.904757: step 7153, loss 0.027824, acc 1, learning_rate 0.0001
2017-10-10T15:30:12.088789: step 7154, loss 0.0846277, acc 0.980392, learning_rate 0.0001
2017-10-10T15:30:12.348868: step 7155, loss 0.0833077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:12.572568: step 7156, loss 0.113827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:12.811129: step 7157, loss 0.0275599, acc 1, learning_rate 0.0001
2017-10-10T15:30:13.062674: step 7158, loss 0.0182917, acc 1, learning_rate 0.0001
2017-10-10T15:30:13.315041: step 7159, loss 0.075141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:13.568569: step 7160, loss 0.0431645, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:13.998050: step 7160, loss 0.193869, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7160

2017-10-10T15:30:15.097494: step 7161, loss 0.069011, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:15.354736: step 7162, loss 0.0689657, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:15.619231: step 7163, loss 0.0468728, acc 1, learning_rate 0.0001
2017-10-10T15:30:15.868847: step 7164, loss 0.0435614, acc 1, learning_rate 0.0001
2017-10-10T15:30:16.119779: step 7165, loss 0.0442336, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:16.360854: step 7166, loss 0.0614774, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:16.694083: step 7167, loss 0.136082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:16.893536: step 7168, loss 0.169544, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:17.072489: step 7169, loss 0.0372027, acc 1, learning_rate 0.0001
2017-10-10T15:30:17.300813: step 7170, loss 0.050961, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:17.490817: step 7171, loss 0.0774104, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:17.622086: step 7172, loss 0.0193557, acc 1, learning_rate 0.0001
2017-10-10T15:30:17.839454: step 7173, loss 0.0442288, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:18.060019: step 7174, loss 0.0605342, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:18.327140: step 7175, loss 0.0458445, acc 1, learning_rate 0.0001
2017-10-10T15:30:18.595988: step 7176, loss 0.0256876, acc 1, learning_rate 0.0001
2017-10-10T15:30:18.871581: step 7177, loss 0.043129, acc 1, learning_rate 0.0001
2017-10-10T15:30:19.126426: step 7178, loss 0.0393375, acc 1, learning_rate 0.0001
2017-10-10T15:30:19.361746: step 7179, loss 0.0871066, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:19.620771: step 7180, loss 0.061356, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:19.856832: step 7181, loss 0.0683451, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:20.109145: step 7182, loss 0.0657743, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:20.339584: step 7183, loss 0.0727849, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:20.564187: step 7184, loss 0.0455791, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:20.857140: step 7185, loss 0.0840912, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:21.076966: step 7186, loss 0.0587398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:21.311297: step 7187, loss 0.0727309, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:21.548055: step 7188, loss 0.0647268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:21.806721: step 7189, loss 0.0235267, acc 1, learning_rate 0.0001
2017-10-10T15:30:22.071728: step 7190, loss 0.0596229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:22.325528: step 7191, loss 0.0896759, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:22.585787: step 7192, loss 0.0607821, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:22.845979: step 7193, loss 0.143452, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:23.108274: step 7194, loss 0.0185043, acc 1, learning_rate 0.0001
2017-10-10T15:30:23.372298: step 7195, loss 0.0350568, acc 1, learning_rate 0.0001
2017-10-10T15:30:23.643783: step 7196, loss 0.0437049, acc 1, learning_rate 0.0001
2017-10-10T15:30:23.896851: step 7197, loss 0.0348852, acc 1, learning_rate 0.0001
2017-10-10T15:30:24.160834: step 7198, loss 0.129801, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:24.428962: step 7199, loss 0.0396743, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:24.693427: step 7200, loss 0.139164, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:25.179267: step 7200, loss 0.191585, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7200

2017-10-10T15:30:26.313182: step 7201, loss 0.110672, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:26.499725: step 7202, loss 0.0768528, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:26.712334: step 7203, loss 0.0334238, acc 1, learning_rate 0.0001
2017-10-10T15:30:26.931147: step 7204, loss 0.115458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:27.121290: step 7205, loss 0.0353317, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:27.326939: step 7206, loss 0.11694, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:27.552785: step 7207, loss 0.0684288, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:27.802024: step 7208, loss 0.0834613, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:28.029446: step 7209, loss 0.0248794, acc 1, learning_rate 0.0001
2017-10-10T15:30:28.300988: step 7210, loss 0.185778, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:28.636690: step 7211, loss 0.0570632, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:28.825154: step 7212, loss 0.0605138, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:29.012936: step 7213, loss 0.0630164, acc 1, learning_rate 0.0001
2017-10-10T15:30:29.185037: step 7214, loss 0.0657231, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:29.356676: step 7215, loss 0.0387078, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:29.561431: step 7216, loss 0.0462809, acc 1, learning_rate 0.0001
2017-10-10T15:30:29.759750: step 7217, loss 0.151239, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:30.004996: step 7218, loss 0.0780187, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:30.220822: step 7219, loss 0.0776407, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:30.479906: step 7220, loss 0.0992554, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:30.670219: step 7221, loss 0.0694867, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:30.920986: step 7222, loss 0.111429, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:31.142291: step 7223, loss 0.0324362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:31.385327: step 7224, loss 0.0541017, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:31.637358: step 7225, loss 0.0181998, acc 1, learning_rate 0.0001
2017-10-10T15:30:31.904415: step 7226, loss 0.0376357, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:32.170821: step 7227, loss 0.0235962, acc 1, learning_rate 0.0001
2017-10-10T15:30:32.410949: step 7228, loss 0.138146, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:32.648778: step 7229, loss 0.0366938, acc 1, learning_rate 0.0001
2017-10-10T15:30:32.891273: step 7230, loss 0.0378087, acc 1, learning_rate 0.0001
2017-10-10T15:30:33.160884: step 7231, loss 0.04465, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:33.374810: step 7232, loss 0.0613741, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:33.605124: step 7233, loss 0.0563278, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:33.845899: step 7234, loss 0.0712765, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:34.100962: step 7235, loss 0.0737749, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:34.358001: step 7236, loss 0.287588, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:34.608876: step 7237, loss 0.104572, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:34.874980: step 7238, loss 0.0509598, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:35.169696: step 7239, loss 0.0855734, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:35.426889: step 7240, loss 0.0513952, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:35.819264: step 7240, loss 0.194205, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7240

2017-10-10T15:30:36.909223: step 7241, loss 0.0521985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:37.153002: step 7242, loss 0.0397784, acc 1, learning_rate 0.0001
2017-10-10T15:30:37.392712: step 7243, loss 0.0488121, acc 1, learning_rate 0.0001
2017-10-10T15:30:37.635383: step 7244, loss 0.0417185, acc 1, learning_rate 0.0001
2017-10-10T15:30:37.888465: step 7245, loss 0.0813119, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:38.127635: step 7246, loss 0.0230314, acc 1, learning_rate 0.0001
2017-10-10T15:30:38.370413: step 7247, loss 0.0208139, acc 1, learning_rate 0.0001
2017-10-10T15:30:38.565625: step 7248, loss 0.0471997, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:38.778830: step 7249, loss 0.0241886, acc 1, learning_rate 0.0001
2017-10-10T15:30:39.011463: step 7250, loss 0.0618105, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:39.252861: step 7251, loss 0.0641261, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:39.480896: step 7252, loss 0.0907318, acc 0.960784, learning_rate 0.0001
2017-10-10T15:30:39.751607: step 7253, loss 0.142329, acc 0.90625, learning_rate 0.0001
2017-10-10T15:30:39.996887: step 7254, loss 0.0697886, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:40.264825: step 7255, loss 0.0973808, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:40.570401: step 7256, loss 0.0854751, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:40.772341: step 7257, loss 0.0933173, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:40.938349: step 7258, loss 0.0248843, acc 1, learning_rate 0.0001
2017-10-10T15:30:41.130355: step 7259, loss 0.0517065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:41.313276: step 7260, loss 0.0644954, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:41.567906: step 7261, loss 0.0613024, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:41.820999: step 7262, loss 0.0472943, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:42.074809: step 7263, loss 0.0479902, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:42.311067: step 7264, loss 0.0209912, acc 1, learning_rate 0.0001
2017-10-10T15:30:42.560878: step 7265, loss 0.103609, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:42.798728: step 7266, loss 0.166942, acc 0.921875, learning_rate 0.0001
2017-10-10T15:30:43.043786: step 7267, loss 0.0162295, acc 1, learning_rate 0.0001
2017-10-10T15:30:43.244190: step 7268, loss 0.104402, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:43.472816: step 7269, loss 0.0353357, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:43.732862: step 7270, loss 0.063102, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:44.028944: step 7271, loss 0.0306044, acc 1, learning_rate 0.0001
2017-10-10T15:30:44.297371: step 7272, loss 0.0767039, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:44.520790: step 7273, loss 0.0423266, acc 1, learning_rate 0.0001
2017-10-10T15:30:44.723909: step 7274, loss 0.0492347, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:44.936824: step 7275, loss 0.10904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:45.187070: step 7276, loss 0.043394, acc 1, learning_rate 0.0001
2017-10-10T15:30:45.431206: step 7277, loss 0.0777877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:45.696849: step 7278, loss 0.0767464, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:45.948836: step 7279, loss 0.00988652, acc 1, learning_rate 0.0001
2017-10-10T15:30:46.192937: step 7280, loss 0.117348, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:46.697167: step 7280, loss 0.191267, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7280

2017-10-10T15:30:47.758898: step 7281, loss 0.035389, acc 1, learning_rate 0.0001
2017-10-10T15:30:47.977669: step 7282, loss 0.0652755, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:48.227351: step 7283, loss 0.0374181, acc 1, learning_rate 0.0001
2017-10-10T15:30:48.495066: step 7284, loss 0.0394129, acc 1, learning_rate 0.0001
2017-10-10T15:30:48.763197: step 7285, loss 0.0547402, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:49.025207: step 7286, loss 0.205311, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:49.275092: step 7287, loss 0.0519662, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:49.500256: step 7288, loss 0.059511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:49.698371: step 7289, loss 0.0344735, acc 1, learning_rate 0.0001
2017-10-10T15:30:49.941568: step 7290, loss 0.161427, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:50.188834: step 7291, loss 0.0649958, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:50.455151: step 7292, loss 0.0521459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:50.722659: step 7293, loss 0.145381, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:50.932842: step 7294, loss 0.0094124, acc 1, learning_rate 0.0001
2017-10-10T15:30:51.155334: step 7295, loss 0.170862, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:51.374007: step 7296, loss 0.041674, acc 1, learning_rate 0.0001
2017-10-10T15:30:51.612131: step 7297, loss 0.0468788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:51.885262: step 7298, loss 0.0422377, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:52.213049: step 7299, loss 0.0716299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:52.478394: step 7300, loss 0.123269, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:52.664893: step 7301, loss 0.131389, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:52.844821: step 7302, loss 0.0521364, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:53.092582: step 7303, loss 0.087706, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:53.226435: step 7304, loss 0.0562877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:53.452670: step 7305, loss 0.0556074, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:53.643303: step 7306, loss 0.0929238, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:53.877626: step 7307, loss 0.108976, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:54.156827: step 7308, loss 0.0509611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:54.403358: step 7309, loss 0.0249213, acc 1, learning_rate 0.0001
2017-10-10T15:30:54.657774: step 7310, loss 0.0366404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:54.888841: step 7311, loss 0.041054, acc 1, learning_rate 0.0001
2017-10-10T15:30:55.112915: step 7312, loss 0.0700627, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:55.364909: step 7313, loss 0.0618076, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:55.643149: step 7314, loss 0.0870438, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:55.890345: step 7315, loss 0.0647913, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:56.117034: step 7316, loss 0.0319335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:56.366596: step 7317, loss 0.0649592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:56.630505: step 7318, loss 0.0931622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:56.876828: step 7319, loss 0.0535245, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:57.097634: step 7320, loss 0.0357255, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:57.580672: step 7320, loss 0.193751, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7320

2017-10-10T15:30:58.512568: step 7321, loss 0.0451793, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:58.764410: step 7322, loss 0.0376918, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:59.044479: step 7323, loss 0.0304378, acc 1, learning_rate 0.0001
2017-10-10T15:30:59.302697: step 7324, loss 0.0663885, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:59.549277: step 7325, loss 0.0561477, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:59.786932: step 7326, loss 0.0285292, acc 1, learning_rate 0.0001
2017-10-10T15:31:00.026071: step 7327, loss 0.128409, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:00.280840: step 7328, loss 0.0306068, acc 1, learning_rate 0.0001
2017-10-10T15:31:00.534851: step 7329, loss 0.0499156, acc 1, learning_rate 0.0001
2017-10-10T15:31:00.783513: step 7330, loss 0.049771, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:01.033602: step 7331, loss 0.0674304, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:01.245499: step 7332, loss 0.0812365, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:01.468849: step 7333, loss 0.0213595, acc 1, learning_rate 0.0001
2017-10-10T15:31:01.722013: step 7334, loss 0.148387, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:01.988621: step 7335, loss 0.0220434, acc 1, learning_rate 0.0001
2017-10-10T15:31:02.323831: step 7336, loss 0.0284334, acc 1, learning_rate 0.0001
2017-10-10T15:31:02.536522: step 7337, loss 0.0567854, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:02.736832: step 7338, loss 0.0891663, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:02.916693: step 7339, loss 0.0835862, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:03.160409: step 7340, loss 0.0469777, acc 1, learning_rate 0.0001
2017-10-10T15:31:03.362719: step 7341, loss 0.0163715, acc 1, learning_rate 0.0001
2017-10-10T15:31:03.603191: step 7342, loss 0.131519, acc 0.921875, learning_rate 0.0001
2017-10-10T15:31:03.936886: step 7343, loss 0.087689, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:04.186616: step 7344, loss 0.0364697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:04.366546: step 7345, loss 0.0366453, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:04.546014: step 7346, loss 0.0527434, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:04.727007: step 7347, loss 0.183704, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:04.888763: step 7348, loss 0.0650158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:05.148160: step 7349, loss 0.0536078, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:05.379931: step 7350, loss 0.0264661, acc 1, learning_rate 0.0001
2017-10-10T15:31:05.630801: step 7351, loss 0.0769211, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:05.864816: step 7352, loss 0.146162, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:06.081972: step 7353, loss 0.0173381, acc 1, learning_rate 0.0001
2017-10-10T15:31:06.304143: step 7354, loss 0.188528, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:06.548835: step 7355, loss 0.035512, acc 1, learning_rate 0.0001
2017-10-10T15:31:06.792353: step 7356, loss 0.0402219, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:07.068079: step 7357, loss 0.0544863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:07.302758: step 7358, loss 0.0784487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:07.544338: step 7359, loss 0.0657274, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:07.744249: step 7360, loss 0.0375526, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:08.256875: step 7360, loss 0.193166, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7360

2017-10-10T15:31:09.228817: step 7361, loss 0.0949442, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:09.472519: step 7362, loss 0.0446307, acc 1, learning_rate 0.0001
2017-10-10T15:31:09.735558: step 7363, loss 0.050593, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:09.988728: step 7364, loss 0.125759, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:10.228897: step 7365, loss 0.0860857, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:10.506954: step 7366, loss 0.0862854, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:10.755578: step 7367, loss 0.042285, acc 1, learning_rate 0.0001
2017-10-10T15:31:10.942699: step 7368, loss 0.0459989, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:11.247006: step 7369, loss 0.0157903, acc 1, learning_rate 0.0001
2017-10-10T15:31:11.465029: step 7370, loss 0.0240837, acc 1, learning_rate 0.0001
2017-10-10T15:31:11.654725: step 7371, loss 0.0878886, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:11.860946: step 7372, loss 0.0930329, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:12.050029: step 7373, loss 0.146702, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:12.246996: step 7374, loss 0.118134, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:12.455748: step 7375, loss 0.0264358, acc 1, learning_rate 0.0001
2017-10-10T15:31:12.664682: step 7376, loss 0.0682199, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:12.875430: step 7377, loss 0.0722293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:13.088011: step 7378, loss 0.0582871, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:13.287981: step 7379, loss 0.0851079, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:13.475319: step 7380, loss 0.0279166, acc 1, learning_rate 0.0001
2017-10-10T15:31:13.659337: step 7381, loss 0.0663027, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:13.848062: step 7382, loss 0.0621911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:14.011601: step 7383, loss 0.0483903, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:14.168858: step 7384, loss 0.0569132, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:14.362332: step 7385, loss 0.0232784, acc 1, learning_rate 0.0001
2017-10-10T15:31:14.564852: step 7386, loss 0.0372728, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:14.783548: step 7387, loss 0.0607856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:15.064861: step 7388, loss 0.0178085, acc 1, learning_rate 0.0001
2017-10-10T15:31:15.267290: step 7389, loss 0.0190088, acc 1, learning_rate 0.0001
2017-10-10T15:31:15.390803: step 7390, loss 0.0471372, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:15.523905: step 7391, loss 0.0403937, acc 1, learning_rate 0.0001
2017-10-10T15:31:15.644931: step 7392, loss 0.160319, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:15.764952: step 7393, loss 0.0606459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:15.884520: step 7394, loss 0.0729258, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:16.048933: step 7395, loss 0.0683727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:16.240267: step 7396, loss 0.0906244, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:16.388547: step 7397, loss 0.0511502, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:16.585049: step 7398, loss 0.0859915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:16.752634: step 7399, loss 0.12602, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:16.940928: step 7400, loss 0.0443043, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:17.346872: step 7400, loss 0.193283, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7400

2017-10-10T15:31:18.201487: step 7401, loss 0.0276646, acc 1, learning_rate 0.0001
2017-10-10T15:31:18.365340: step 7402, loss 0.04898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:18.540615: step 7403, loss 0.0684066, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:18.714285: step 7404, loss 0.0594876, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:18.917884: step 7405, loss 0.0634486, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:19.120428: step 7406, loss 0.0844234, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:19.322963: step 7407, loss 0.0343373, acc 1, learning_rate 0.0001
2017-10-10T15:31:19.538853: step 7408, loss 0.0285645, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:19.727903: step 7409, loss 0.0429286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:19.926044: step 7410, loss 0.0338268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:20.124657: step 7411, loss 0.0508722, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:20.341435: step 7412, loss 0.0654004, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:20.544402: step 7413, loss 0.0662083, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:20.753494: step 7414, loss 0.135146, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:20.948706: step 7415, loss 0.0596917, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:21.148012: step 7416, loss 0.0500441, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:21.337496: step 7417, loss 0.126432, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:21.533272: step 7418, loss 0.0411781, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:21.736505: step 7419, loss 0.09011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:21.937827: step 7420, loss 0.0272805, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:22.129134: step 7421, loss 0.052235, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:22.322495: step 7422, loss 0.0801283, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:22.504343: step 7423, loss 0.0503693, acc 1, learning_rate 0.0001
2017-10-10T15:31:22.680844: step 7424, loss 0.11865, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:22.867125: step 7425, loss 0.0356739, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:23.013810: step 7426, loss 0.0577421, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:23.189608: step 7427, loss 0.169904, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:23.392227: step 7428, loss 0.0602417, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:23.607313: step 7429, loss 0.0527232, acc 1, learning_rate 0.0001
2017-10-10T15:31:23.806719: step 7430, loss 0.015574, acc 1, learning_rate 0.0001
2017-10-10T15:31:24.117311: step 7431, loss 0.0454949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:24.296194: step 7432, loss 0.0288203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:24.417825: step 7433, loss 0.0837273, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:24.534863: step 7434, loss 0.0667399, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:24.652493: step 7435, loss 0.0234545, acc 1, learning_rate 0.0001
2017-10-10T15:31:24.769590: step 7436, loss 0.0593711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:24.890160: step 7437, loss 0.0532769, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:25.056299: step 7438, loss 0.119889, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:25.245469: step 7439, loss 0.0941297, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:25.416911: step 7440, loss 0.108971, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:25.801779: step 7440, loss 0.193618, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7440

2017-10-10T15:31:26.728835: step 7441, loss 0.0103459, acc 1, learning_rate 0.0001
2017-10-10T15:31:26.915600: step 7442, loss 0.0256663, acc 1, learning_rate 0.0001
2017-10-10T15:31:27.120873: step 7443, loss 0.0380653, acc 1, learning_rate 0.0001
2017-10-10T15:31:27.309500: step 7444, loss 0.0612826, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:27.502934: step 7445, loss 0.0289467, acc 1, learning_rate 0.0001
2017-10-10T15:31:27.688823: step 7446, loss 0.0358609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:27.880818: step 7447, loss 0.0517878, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:28.015117: step 7448, loss 0.0426426, acc 1, learning_rate 0.0001
2017-10-10T15:31:28.188720: step 7449, loss 0.0313042, acc 1, learning_rate 0.0001
2017-10-10T15:31:28.362107: step 7450, loss 0.083609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:28.544367: step 7451, loss 0.0225239, acc 1, learning_rate 0.0001
2017-10-10T15:31:28.744568: step 7452, loss 0.0704051, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:28.950150: step 7453, loss 0.0349273, acc 1, learning_rate 0.0001
2017-10-10T15:31:29.156047: step 7454, loss 0.0603664, acc 1, learning_rate 0.0001
2017-10-10T15:31:29.361037: step 7455, loss 0.0871289, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:29.551044: step 7456, loss 0.103169, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:29.740816: step 7457, loss 0.072127, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:29.937599: step 7458, loss 0.074596, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:30.116098: step 7459, loss 0.082087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:30.287825: step 7460, loss 0.0120662, acc 1, learning_rate 0.0001
2017-10-10T15:31:30.499945: step 7461, loss 0.0467768, acc 1, learning_rate 0.0001
2017-10-10T15:31:30.688369: step 7462, loss 0.0978459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:30.882198: step 7463, loss 0.0826731, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:31.065769: step 7464, loss 0.0707761, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:31.248325: step 7465, loss 0.0392686, acc 1, learning_rate 0.0001
2017-10-10T15:31:31.460465: step 7466, loss 0.0623509, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:31.654377: step 7467, loss 0.0823218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:31.840152: step 7468, loss 0.060777, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:32.016057: step 7469, loss 0.043474, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:32.172165: step 7470, loss 0.022034, acc 1, learning_rate 0.0001
2017-10-10T15:31:32.363723: step 7471, loss 0.0302859, acc 1, learning_rate 0.0001
2017-10-10T15:31:32.556914: step 7472, loss 0.0270918, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:32.776841: step 7473, loss 0.0745484, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:33.062781: step 7474, loss 0.0606772, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:33.184846: step 7475, loss 0.0382119, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:33.307323: step 7476, loss 0.043929, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:33.426839: step 7477, loss 0.0593529, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:33.547981: step 7478, loss 0.0773235, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:33.666071: step 7479, loss 0.0181089, acc 1, learning_rate 0.0001
2017-10-10T15:31:33.785529: step 7480, loss 0.0719474, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:34.160400: step 7480, loss 0.195092, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7480

2017-10-10T15:31:35.133817: step 7481, loss 0.0417367, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:35.329777: step 7482, loss 0.03524, acc 1, learning_rate 0.0001
2017-10-10T15:31:35.523157: step 7483, loss 0.0165833, acc 1, learning_rate 0.0001
2017-10-10T15:31:35.713227: step 7484, loss 0.0545597, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:35.903865: step 7485, loss 0.0376313, acc 1, learning_rate 0.0001
2017-10-10T15:31:36.071996: step 7486, loss 0.0288387, acc 1, learning_rate 0.0001
2017-10-10T15:31:36.233185: step 7487, loss 0.062561, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:36.397711: step 7488, loss 0.0815659, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:36.577765: step 7489, loss 0.0331141, acc 1, learning_rate 0.0001
2017-10-10T15:31:36.773673: step 7490, loss 0.0498262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:36.969975: step 7491, loss 0.0525685, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:37.160628: step 7492, loss 0.106289, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:37.341404: step 7493, loss 0.0668106, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:37.520495: step 7494, loss 0.047352, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:37.687097: step 7495, loss 0.063753, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:37.875091: step 7496, loss 0.0498923, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:38.035101: step 7497, loss 0.0648781, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:38.237374: step 7498, loss 0.0255518, acc 1, learning_rate 0.0001
2017-10-10T15:31:38.404025: step 7499, loss 0.0399801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:38.576698: step 7500, loss 0.0535243, acc 1, learning_rate 0.0001
2017-10-10T15:31:38.774611: step 7501, loss 0.11559, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:38.972476: step 7502, loss 0.0864919, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:39.178343: step 7503, loss 0.10481, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:39.373023: step 7504, loss 0.0527844, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:39.589826: step 7505, loss 0.0358971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:39.792711: step 7506, loss 0.144755, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:40.001084: step 7507, loss 0.0735679, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:40.199683: step 7508, loss 0.0947811, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:40.414112: step 7509, loss 0.0509114, acc 1, learning_rate 0.0001
2017-10-10T15:31:40.612859: step 7510, loss 0.0815415, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:40.820205: step 7511, loss 0.10788, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:41.012520: step 7512, loss 0.0691563, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:41.202518: step 7513, loss 0.0521448, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:41.401199: step 7514, loss 0.052771, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:41.592886: step 7515, loss 0.0463731, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:41.836864: step 7516, loss 0.0936578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:42.077527: step 7517, loss 0.0466154, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:42.198760: step 7518, loss 0.0396248, acc 1, learning_rate 0.0001
2017-10-10T15:31:42.319995: step 7519, loss 0.00978586, acc 1, learning_rate 0.0001
2017-10-10T15:31:42.439727: step 7520, loss 0.0946852, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:42.731108: step 7520, loss 0.19519, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7520

2017-10-10T15:31:43.552347: step 7521, loss 0.0254895, acc 1, learning_rate 0.0001
2017-10-10T15:31:43.747477: step 7522, loss 0.0619493, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:43.955819: step 7523, loss 0.0721852, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:44.157397: step 7524, loss 0.0889964, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:44.355562: step 7525, loss 0.0781098, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:44.544455: step 7526, loss 0.0661207, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:44.742839: step 7527, loss 0.109981, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:44.939138: step 7528, loss 0.0203942, acc 1, learning_rate 0.0001
2017-10-10T15:31:45.132300: step 7529, loss 0.126403, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:45.329675: step 7530, loss 0.0706996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:45.526975: step 7531, loss 0.0803562, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:45.719055: step 7532, loss 0.0315221, acc 1, learning_rate 0.0001
2017-10-10T15:31:45.900833: step 7533, loss 0.0179577, acc 1, learning_rate 0.0001
2017-10-10T15:31:46.069972: step 7534, loss 0.0706774, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:46.263767: step 7535, loss 0.0997683, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:46.475651: step 7536, loss 0.0338487, acc 1, learning_rate 0.0001
2017-10-10T15:31:46.635809: step 7537, loss 0.0438149, acc 1, learning_rate 0.0001
2017-10-10T15:31:46.813079: step 7538, loss 0.115312, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:46.977864: step 7539, loss 0.0323178, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:47.167272: step 7540, loss 0.0828643, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:47.351673: step 7541, loss 0.0543198, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:47.549511: step 7542, loss 0.0477139, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:47.749460: step 7543, loss 0.0620839, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:47.956515: step 7544, loss 0.0479367, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:48.176035: step 7545, loss 0.0641515, acc 1, learning_rate 0.0001
2017-10-10T15:31:48.356517: step 7546, loss 0.12169, acc 0.941176, learning_rate 0.0001
2017-10-10T15:31:48.549806: step 7547, loss 0.102568, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:48.742988: step 7548, loss 0.0918159, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:48.932998: step 7549, loss 0.0452778, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:49.123650: step 7550, loss 0.0273508, acc 1, learning_rate 0.0001
2017-10-10T15:31:49.306654: step 7551, loss 0.0726391, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:49.497281: step 7552, loss 0.0549669, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:49.691813: step 7553, loss 0.0747389, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:49.889946: step 7554, loss 0.0205215, acc 1, learning_rate 0.0001
2017-10-10T15:31:50.089180: step 7555, loss 0.0487722, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:50.279841: step 7556, loss 0.0497395, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:50.460816: step 7557, loss 0.0708636, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:50.688864: step 7558, loss 0.0886793, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:50.948278: step 7559, loss 0.086354, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:51.069207: step 7560, loss 0.0650009, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:51.353714: step 7560, loss 0.192111, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7560

2017-10-10T15:31:52.127048: step 7561, loss 0.0312778, acc 1, learning_rate 0.0001
2017-10-10T15:31:52.308870: step 7562, loss 0.0305322, acc 1, learning_rate 0.0001
2017-10-10T15:31:52.484869: step 7563, loss 0.0225443, acc 1, learning_rate 0.0001
2017-10-10T15:31:52.642791: step 7564, loss 0.058758, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:52.811236: step 7565, loss 0.0460486, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:53.006256: step 7566, loss 0.160078, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:53.206661: step 7567, loss 0.0533814, acc 1, learning_rate 0.0001
2017-10-10T15:31:53.403121: step 7568, loss 0.0207985, acc 1, learning_rate 0.0001
2017-10-10T15:31:53.602907: step 7569, loss 0.0583225, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:53.797739: step 7570, loss 0.062036, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:54.018441: step 7571, loss 0.0219772, acc 1, learning_rate 0.0001
2017-10-10T15:31:54.224170: step 7572, loss 0.0506352, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:54.428420: step 7573, loss 0.0301645, acc 1, learning_rate 0.0001
2017-10-10T15:31:54.630362: step 7574, loss 0.0466495, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:54.818935: step 7575, loss 0.067639, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:55.000804: step 7576, loss 0.0557054, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:55.168869: step 7577, loss 0.0568828, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:55.344982: step 7578, loss 0.0386108, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:55.552849: step 7579, loss 0.0450456, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:55.760152: step 7580, loss 0.0709398, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:55.944362: step 7581, loss 0.0257151, acc 1, learning_rate 0.0001
2017-10-10T15:31:56.138337: step 7582, loss 0.00742021, acc 1, learning_rate 0.0001
2017-10-10T15:31:56.333867: step 7583, loss 0.0559716, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:56.524175: step 7584, loss 0.0491803, acc 1, learning_rate 0.0001
2017-10-10T15:31:56.717858: step 7585, loss 0.0932272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:56.896854: step 7586, loss 0.0785081, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:57.076832: step 7587, loss 0.0300123, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:57.250749: step 7588, loss 0.0407194, acc 1, learning_rate 0.0001
2017-10-10T15:31:57.424759: step 7589, loss 0.0208482, acc 1, learning_rate 0.0001
2017-10-10T15:31:57.591869: step 7590, loss 0.101486, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:57.759826: step 7591, loss 0.0320314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:57.954890: step 7592, loss 0.0689583, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:58.154758: step 7593, loss 0.0454257, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:58.344082: step 7594, loss 0.0400981, acc 1, learning_rate 0.0001
2017-10-10T15:31:58.515743: step 7595, loss 0.0961984, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:58.717777: step 7596, loss 0.0985753, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:58.910064: step 7597, loss 0.0634118, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:59.108536: step 7598, loss 0.0800876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:59.299548: step 7599, loss 0.0844958, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:59.512843: step 7600, loss 0.0110559, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:59.920540: step 7600, loss 0.19414, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7600

2017-10-10T15:32:00.557213: step 7601, loss 0.0573817, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:00.755204: step 7602, loss 0.131421, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:00.953329: step 7603, loss 0.037942, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:01.146603: step 7604, loss 0.0919584, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:01.328554: step 7605, loss 0.0295666, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:01.525206: step 7606, loss 0.0490659, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:01.699385: step 7607, loss 0.0393507, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:01.894467: step 7608, loss 0.0372276, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:02.095679: step 7609, loss 0.0567341, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:02.269502: step 7610, loss 0.0396845, acc 1, learning_rate 0.0001
2017-10-10T15:32:02.453810: step 7611, loss 0.0520209, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:02.616638: step 7612, loss 0.0189126, acc 1, learning_rate 0.0001
2017-10-10T15:32:02.813714: step 7613, loss 0.091892, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:03.043758: step 7614, loss 0.0253486, acc 1, learning_rate 0.0001
2017-10-10T15:32:03.240735: step 7615, loss 0.025065, acc 1, learning_rate 0.0001
2017-10-10T15:32:03.404836: step 7616, loss 0.0543944, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:03.581675: step 7617, loss 0.0407612, acc 1, learning_rate 0.0001
2017-10-10T15:32:03.775853: step 7618, loss 0.0878533, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:03.975636: step 7619, loss 0.0291748, acc 1, learning_rate 0.0001
2017-10-10T15:32:04.151795: step 7620, loss 0.0152012, acc 1, learning_rate 0.0001
2017-10-10T15:32:04.336478: step 7621, loss 0.0701035, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:04.517729: step 7622, loss 0.166923, acc 0.921875, learning_rate 0.0001
2017-10-10T15:32:04.710792: step 7623, loss 0.122502, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:04.884849: step 7624, loss 0.0206585, acc 1, learning_rate 0.0001
2017-10-10T15:32:05.040837: step 7625, loss 0.0261053, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:05.220879: step 7626, loss 0.0723542, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:05.403539: step 7627, loss 0.127749, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:05.601894: step 7628, loss 0.0948641, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:05.799759: step 7629, loss 0.0602139, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:05.996109: step 7630, loss 0.0304512, acc 1, learning_rate 0.0001
2017-10-10T15:32:06.197166: step 7631, loss 0.114134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:06.389073: step 7632, loss 0.0878433, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:06.579589: step 7633, loss 0.110836, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:06.775428: step 7634, loss 0.056566, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:06.988881: step 7635, loss 0.0989982, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:07.197191: step 7636, loss 0.0498514, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:07.388399: step 7637, loss 0.0953561, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:07.579875: step 7638, loss 0.0304793, acc 1, learning_rate 0.0001
2017-10-10T15:32:07.777041: step 7639, loss 0.0458331, acc 1, learning_rate 0.0001
2017-10-10T15:32:07.960833: step 7640, loss 0.0375804, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:08.334482: step 7640, loss 0.196118, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7640

2017-10-10T15:32:09.180082: step 7641, loss 0.0584011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:09.376842: step 7642, loss 0.0334613, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:09.539714: step 7643, loss 0.119598, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:09.668851: step 7644, loss 0.103144, acc 0.941176, learning_rate 0.0001
2017-10-10T15:32:09.868876: step 7645, loss 0.0424379, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:10.050354: step 7646, loss 0.102017, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:10.245766: step 7647, loss 0.116666, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:10.445076: step 7648, loss 0.0187402, acc 1, learning_rate 0.0001
2017-10-10T15:32:10.640130: step 7649, loss 0.13761, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:10.838404: step 7650, loss 0.111785, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:11.035189: step 7651, loss 0.0613667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:11.228981: step 7652, loss 0.0567321, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:11.396092: step 7653, loss 0.0197479, acc 1, learning_rate 0.0001
2017-10-10T15:32:11.544345: step 7654, loss 0.132239, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:11.708872: step 7655, loss 0.0426682, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:11.891332: step 7656, loss 0.0354927, acc 1, learning_rate 0.0001
2017-10-10T15:32:12.091292: step 7657, loss 0.0420986, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:12.288555: step 7658, loss 0.0367235, acc 1, learning_rate 0.0001
2017-10-10T15:32:12.494192: step 7659, loss 0.0351437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:12.693062: step 7660, loss 0.0132447, acc 1, learning_rate 0.0001
2017-10-10T15:32:12.891141: step 7661, loss 0.0869924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:13.073400: step 7662, loss 0.0369386, acc 1, learning_rate 0.0001
2017-10-10T15:32:13.278719: step 7663, loss 0.058431, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:13.474007: step 7664, loss 0.0268807, acc 1, learning_rate 0.0001
2017-10-10T15:32:13.687179: step 7665, loss 0.18999, acc 0.921875, learning_rate 0.0001
2017-10-10T15:32:13.884901: step 7666, loss 0.0710256, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:14.082767: step 7667, loss 0.0127271, acc 1, learning_rate 0.0001
2017-10-10T15:32:14.285065: step 7668, loss 0.0202864, acc 1, learning_rate 0.0001
2017-10-10T15:32:14.483478: step 7669, loss 0.0829113, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:14.694343: step 7670, loss 0.086172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:14.890777: step 7671, loss 0.0480314, acc 1, learning_rate 0.0001
2017-10-10T15:32:15.092035: step 7672, loss 0.121397, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:15.277822: step 7673, loss 0.0910156, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:15.449826: step 7674, loss 0.0763826, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:15.604849: step 7675, loss 0.0437119, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:15.779970: step 7676, loss 0.0509238, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:15.977062: step 7677, loss 0.0532446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:16.171078: step 7678, loss 0.101409, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:16.351825: step 7679, loss 0.14862, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:16.547612: step 7680, loss 0.0577767, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:16.907716: step 7680, loss 0.195364, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7680

2017-10-10T15:32:17.739777: step 7681, loss 0.0210472, acc 1, learning_rate 0.0001
2017-10-10T15:32:17.857824: step 7682, loss 0.097281, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:17.978431: step 7683, loss 0.0360456, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:18.096134: step 7684, loss 0.129667, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:18.294085: step 7685, loss 0.0256001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:18.495822: step 7686, loss 0.0342029, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:18.701110: step 7687, loss 0.0222514, acc 1, learning_rate 0.0001
2017-10-10T15:32:18.901764: step 7688, loss 0.0841134, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:19.108328: step 7689, loss 0.14511, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:19.277443: step 7690, loss 0.0619506, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:19.447201: step 7691, loss 0.0462014, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:19.601272: step 7692, loss 0.034246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:19.770823: step 7693, loss 0.0741295, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:19.965443: step 7694, loss 0.149031, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:20.164350: step 7695, loss 0.0581343, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:20.359755: step 7696, loss 0.059909, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:20.535470: step 7697, loss 0.0393299, acc 1, learning_rate 0.0001
2017-10-10T15:32:20.713948: step 7698, loss 0.065555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:20.883273: step 7699, loss 0.115823, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:21.075112: step 7700, loss 0.0911661, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:21.259114: step 7701, loss 0.0367261, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:21.419040: step 7702, loss 0.126421, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:21.591574: step 7703, loss 0.0532031, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:21.785591: step 7704, loss 0.0992844, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:21.990038: step 7705, loss 0.12623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:22.197050: step 7706, loss 0.0861869, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:22.400540: step 7707, loss 0.0742795, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:22.606948: step 7708, loss 0.0344942, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:22.808882: step 7709, loss 0.0875653, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:23.006572: step 7710, loss 0.0146514, acc 1, learning_rate 0.0001
2017-10-10T15:32:23.207369: step 7711, loss 0.064541, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:23.411801: step 7712, loss 0.133953, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:23.602139: step 7713, loss 0.0305545, acc 1, learning_rate 0.0001
2017-10-10T15:32:23.799832: step 7714, loss 0.0417906, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:23.998798: step 7715, loss 0.0377549, acc 1, learning_rate 0.0001
2017-10-10T15:32:24.206047: step 7716, loss 0.0217439, acc 1, learning_rate 0.0001
2017-10-10T15:32:24.418877: step 7717, loss 0.0593545, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:24.615063: step 7718, loss 0.0263912, acc 1, learning_rate 0.0001
2017-10-10T15:32:24.803891: step 7719, loss 0.0643152, acc 1, learning_rate 0.0001
2017-10-10T15:32:24.989055: step 7720, loss 0.0812194, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:25.367007: step 7720, loss 0.193888, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7720

2017-10-10T15:32:26.354026: step 7721, loss 0.0695283, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:26.497973: step 7722, loss 0.0243727, acc 1, learning_rate 0.0001
2017-10-10T15:32:26.617520: step 7723, loss 0.0258756, acc 1, learning_rate 0.0001
2017-10-10T15:32:26.737774: step 7724, loss 0.059088, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:26.853571: step 7725, loss 0.0789787, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:26.971951: step 7726, loss 0.0365082, acc 1, learning_rate 0.0001
2017-10-10T15:32:27.092900: step 7727, loss 0.0935295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:27.220943: step 7728, loss 0.0637767, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:27.406770: step 7729, loss 0.0398816, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:27.602098: step 7730, loss 0.011208, acc 1, learning_rate 0.0001
2017-10-10T15:32:27.800445: step 7731, loss 0.0435525, acc 1, learning_rate 0.0001
2017-10-10T15:32:28.006013: step 7732, loss 0.0387812, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:28.212006: step 7733, loss 0.0624158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:28.413708: step 7734, loss 0.107881, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:28.617545: step 7735, loss 0.0453955, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:28.815055: step 7736, loss 0.0702932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:29.004042: step 7737, loss 0.03881, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:29.171615: step 7738, loss 0.0229956, acc 1, learning_rate 0.0001
2017-10-10T15:32:29.341063: step 7739, loss 0.0247298, acc 1, learning_rate 0.0001
2017-10-10T15:32:29.540869: step 7740, loss 0.0697915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:29.722785: step 7741, loss 0.0543829, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:29.868690: step 7742, loss 0.0335197, acc 1, learning_rate 0.0001
2017-10-10T15:32:30.088930: step 7743, loss 0.0279897, acc 1, learning_rate 0.0001
2017-10-10T15:32:30.276849: step 7744, loss 0.036745, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:30.464852: step 7745, loss 0.0339915, acc 1, learning_rate 0.0001
2017-10-10T15:32:30.641893: step 7746, loss 0.0991187, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:30.798744: step 7747, loss 0.0850785, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:30.983717: step 7748, loss 0.0810137, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:31.183107: step 7749, loss 0.0748948, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:31.373717: step 7750, loss 0.0600455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:31.569967: step 7751, loss 0.0561176, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:31.759194: step 7752, loss 0.0555833, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:31.956711: step 7753, loss 0.037609, acc 1, learning_rate 0.0001
2017-10-10T15:32:32.148029: step 7754, loss 0.0439495, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:32.339405: step 7755, loss 0.0646502, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:32.534422: step 7756, loss 0.0623064, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:32.724501: step 7757, loss 0.152067, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:32.908761: step 7758, loss 0.141093, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:33.087472: step 7759, loss 0.0309676, acc 1, learning_rate 0.0001
2017-10-10T15:32:33.280662: step 7760, loss 0.0564346, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:33.671802: step 7760, loss 0.192939, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7760

2017-10-10T15:32:34.513349: step 7761, loss 0.0246868, acc 1, learning_rate 0.0001
2017-10-10T15:32:34.710376: step 7762, loss 0.021758, acc 1, learning_rate 0.0001
2017-10-10T15:32:34.922881: step 7763, loss 0.0272649, acc 1, learning_rate 0.0001
2017-10-10T15:32:35.184907: step 7764, loss 0.105591, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:35.405748: step 7765, loss 0.0385861, acc 1, learning_rate 0.0001
2017-10-10T15:32:35.525696: step 7766, loss 0.110255, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:35.647190: step 7767, loss 0.0902619, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:35.768139: step 7768, loss 0.0217197, acc 1, learning_rate 0.0001
2017-10-10T15:32:35.884605: step 7769, loss 0.0693964, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:36.004370: step 7770, loss 0.0577536, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:36.123760: step 7771, loss 0.0348341, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:36.320863: step 7772, loss 0.00919825, acc 1, learning_rate 0.0001
2017-10-10T15:32:36.522065: step 7773, loss 0.0679724, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:36.717953: step 7774, loss 0.122205, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:36.913284: step 7775, loss 0.0762413, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:37.106208: step 7776, loss 0.0765207, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:37.297473: step 7777, loss 0.0414096, acc 1, learning_rate 0.0001
2017-10-10T15:32:37.462827: step 7778, loss 0.0692314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:37.640077: step 7779, loss 0.0728168, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:37.829044: step 7780, loss 0.0871882, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:38.012318: step 7781, loss 0.0747652, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:38.190764: step 7782, loss 0.0616286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:38.392894: step 7783, loss 0.0631632, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:38.602704: step 7784, loss 0.053363, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:38.803427: step 7785, loss 0.0204547, acc 1, learning_rate 0.0001
2017-10-10T15:32:38.994363: step 7786, loss 0.0587201, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:39.172831: step 7787, loss 0.0226376, acc 1, learning_rate 0.0001
2017-10-10T15:32:39.356448: step 7788, loss 0.142263, acc 0.921875, learning_rate 0.0001
2017-10-10T15:32:39.527843: step 7789, loss 0.0129536, acc 1, learning_rate 0.0001
2017-10-10T15:32:39.703809: step 7790, loss 0.0431958, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:39.903419: step 7791, loss 0.100439, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:40.060878: step 7792, loss 0.0880066, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:40.243532: step 7793, loss 0.0504114, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:40.449124: step 7794, loss 0.190233, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:40.651913: step 7795, loss 0.0490284, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:40.852870: step 7796, loss 0.0896087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:41.057830: step 7797, loss 0.0477316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:41.257545: step 7798, loss 0.0714645, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:41.447901: step 7799, loss 0.117209, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:41.647225: step 7800, loss 0.0791377, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:41.998918: step 7800, loss 0.192049, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7800

2017-10-10T15:32:42.966762: step 7801, loss 0.0123844, acc 1, learning_rate 0.0001
2017-10-10T15:32:43.130056: step 7802, loss 0.049687, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:43.266623: step 7803, loss 0.0694035, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:43.460603: step 7804, loss 0.0542631, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:43.653461: step 7805, loss 0.0250054, acc 1, learning_rate 0.0001
2017-10-10T15:32:43.850032: step 7806, loss 0.0332466, acc 1, learning_rate 0.0001
2017-10-10T15:32:44.065026: step 7807, loss 0.0466796, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:44.321607: step 7808, loss 0.081974, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:44.441264: step 7809, loss 0.0234259, acc 1, learning_rate 0.0001
2017-10-10T15:32:44.564768: step 7810, loss 0.0451288, acc 1, learning_rate 0.0001
2017-10-10T15:32:44.699288: step 7811, loss 0.074631, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:44.816390: step 7812, loss 0.0583151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:44.935978: step 7813, loss 0.0305298, acc 1, learning_rate 0.0001
2017-10-10T15:32:45.053042: step 7814, loss 0.0179675, acc 1, learning_rate 0.0001
2017-10-10T15:32:45.265100: step 7815, loss 0.0379202, acc 1, learning_rate 0.0001
2017-10-10T15:32:45.467947: step 7816, loss 0.0741812, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:45.668222: step 7817, loss 0.072743, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:45.870176: step 7818, loss 0.0316187, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:46.046474: step 7819, loss 0.0323438, acc 1, learning_rate 0.0001
2017-10-10T15:32:46.220972: step 7820, loss 0.156551, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:46.363790: step 7821, loss 0.0954496, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:46.540933: step 7822, loss 0.0450362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:46.731837: step 7823, loss 0.0675536, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:46.930421: step 7824, loss 0.0732471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:47.122423: step 7825, loss 0.0114094, acc 1, learning_rate 0.0001
2017-10-10T15:32:47.314142: step 7826, loss 0.0453194, acc 1, learning_rate 0.0001
2017-10-10T15:32:47.492532: step 7827, loss 0.0636366, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:47.690466: step 7828, loss 0.0441994, acc 1, learning_rate 0.0001
2017-10-10T15:32:47.884102: step 7829, loss 0.0122909, acc 1, learning_rate 0.0001
2017-10-10T15:32:48.066562: step 7830, loss 0.0477051, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:48.226487: step 7831, loss 0.067288, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:48.408974: step 7832, loss 0.0232449, acc 1, learning_rate 0.0001
2017-10-10T15:32:48.564835: step 7833, loss 0.0443577, acc 1, learning_rate 0.0001
2017-10-10T15:32:48.752355: step 7834, loss 0.0460399, acc 1, learning_rate 0.0001
2017-10-10T15:32:48.946465: step 7835, loss 0.0770816, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:49.132991: step 7836, loss 0.131328, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:49.322821: step 7837, loss 0.0269301, acc 1, learning_rate 0.0001
2017-10-10T15:32:49.485201: step 7838, loss 0.0795456, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:49.644411: step 7839, loss 0.0434877, acc 1, learning_rate 0.0001
2017-10-10T15:32:49.796079: step 7840, loss 0.119515, acc 0.960784, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:50.204838: step 7840, loss 0.192568, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664326/checkpoints/model-7840

