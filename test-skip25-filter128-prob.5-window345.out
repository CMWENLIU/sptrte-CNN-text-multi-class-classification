
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507651957

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T11:12:41.369843: step 1, loss 8.95127, acc 0.203125, learning_rate 0.005
2017-10-10T11:12:41.540498: step 2, loss 6.41902, acc 0.25, learning_rate 0.00498
2017-10-10T11:12:41.706433: step 3, loss 6.73828, acc 0.359375, learning_rate 0.00496008
2017-10-10T11:12:41.869780: step 4, loss 6.7371, acc 0.34375, learning_rate 0.00494024
2017-10-10T11:12:42.034071: step 5, loss 6.94147, acc 0.328125, learning_rate 0.00492049
2017-10-10T11:12:42.193938: step 6, loss 4.44388, acc 0.484375, learning_rate 0.00490081
2017-10-10T11:12:42.358989: step 7, loss 3.58785, acc 0.375, learning_rate 0.00488121
2017-10-10T11:12:42.523157: step 8, loss 4.49157, acc 0.328125, learning_rate 0.0048617
2017-10-10T11:12:42.686191: step 9, loss 4.037, acc 0.40625, learning_rate 0.00484226
2017-10-10T11:12:42.855015: step 10, loss 3.04228, acc 0.46875, learning_rate 0.00482291
2017-10-10T11:12:43.017724: step 11, loss 2.15446, acc 0.609375, learning_rate 0.00480363
2017-10-10T11:12:43.183518: step 12, loss 2.28051, acc 0.59375, learning_rate 0.00478443
2017-10-10T11:12:43.349570: step 13, loss 3.31879, acc 0.5, learning_rate 0.00476531
2017-10-10T11:12:43.513907: step 14, loss 2.09992, acc 0.625, learning_rate 0.00474627
2017-10-10T11:12:43.675145: step 15, loss 4.42284, acc 0.453125, learning_rate 0.0047273
2017-10-10T11:12:43.844570: step 16, loss 2.85755, acc 0.609375, learning_rate 0.00470841
2017-10-10T11:12:44.012123: step 17, loss 3.96008, acc 0.5, learning_rate 0.0046896
2017-10-10T11:12:44.172902: step 18, loss 1.70129, acc 0.640625, learning_rate 0.00467087
2017-10-10T11:12:44.336804: step 19, loss 2.12658, acc 0.671875, learning_rate 0.00465221
2017-10-10T11:12:44.499215: step 20, loss 2.33946, acc 0.546875, learning_rate 0.00463363
2017-10-10T11:12:44.661640: step 21, loss 2.56386, acc 0.609375, learning_rate 0.00461513
2017-10-10T11:12:44.827595: step 22, loss 2.44621, acc 0.59375, learning_rate 0.0045967
2017-10-10T11:12:44.993842: step 23, loss 1.40309, acc 0.6875, learning_rate 0.00457834
2017-10-10T11:12:45.153412: step 24, loss 2.12867, acc 0.609375, learning_rate 0.00456006
2017-10-10T11:12:45.313901: step 25, loss 2.42262, acc 0.59375, learning_rate 0.00454186
2017-10-10T11:12:45.476821: step 26, loss 1.42908, acc 0.71875, learning_rate 0.00452373
2017-10-10T11:12:45.637392: step 27, loss 1.59595, acc 0.734375, learning_rate 0.00450567
2017-10-10T11:12:45.801279: step 28, loss 0.973984, acc 0.78125, learning_rate 0.00448769
2017-10-10T11:12:45.967890: step 29, loss 1.6162, acc 0.65625, learning_rate 0.00446978
2017-10-10T11:12:46.135577: step 30, loss 1.84562, acc 0.671875, learning_rate 0.00445194
2017-10-10T11:12:46.302634: step 31, loss 0.958199, acc 0.78125, learning_rate 0.00443418
2017-10-10T11:12:46.464839: step 32, loss 1.83538, acc 0.6875, learning_rate 0.00441649
2017-10-10T11:12:46.631542: step 33, loss 2.19266, acc 0.625, learning_rate 0.00439887
2017-10-10T11:12:46.801025: step 34, loss 1.66992, acc 0.65625, learning_rate 0.00438132
2017-10-10T11:12:46.965921: step 35, loss 1.07004, acc 0.8125, learning_rate 0.00436385
2017-10-10T11:12:47.132233: step 36, loss 1.60359, acc 0.75, learning_rate 0.00434644
2017-10-10T11:12:47.293688: step 37, loss 1.38446, acc 0.75, learning_rate 0.00432911
2017-10-10T11:12:47.457491: step 38, loss 1.6247, acc 0.71875, learning_rate 0.00431185
2017-10-10T11:12:47.619412: step 39, loss 1.37369, acc 0.78125, learning_rate 0.00429465
2017-10-10T11:12:47.784410: step 40, loss 1.23029, acc 0.75, learning_rate 0.00427753

Evaluation:
2017-10-10T11:12:48.290126: step 40, loss 0.614854, acc 0.848921

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-40

2017-10-10T11:12:48.990551: step 41, loss 2.0196, acc 0.671875, learning_rate 0.00426048
2017-10-10T11:12:49.152457: step 42, loss 1.45047, acc 0.78125, learning_rate 0.0042435
2017-10-10T11:12:49.313227: step 43, loss 1.72448, acc 0.703125, learning_rate 0.00422659
2017-10-10T11:12:49.475401: step 44, loss 1.00435, acc 0.734375, learning_rate 0.00420974
2017-10-10T11:12:49.637027: step 45, loss 1.26531, acc 0.765625, learning_rate 0.00419297
2017-10-10T11:12:49.798805: step 46, loss 1.79232, acc 0.734375, learning_rate 0.00417626
2017-10-10T11:12:49.961982: step 47, loss 1.59337, acc 0.65625, learning_rate 0.00415962
2017-10-10T11:12:50.126236: step 48, loss 1.06914, acc 0.765625, learning_rate 0.00414305
2017-10-10T11:12:50.289178: step 49, loss 0.657494, acc 0.8125, learning_rate 0.00412655
2017-10-10T11:12:50.451784: step 50, loss 0.635358, acc 0.84375, learning_rate 0.00411011
2017-10-10T11:12:50.609848: step 51, loss 1.29035, acc 0.84375, learning_rate 0.00409375
2017-10-10T11:12:50.772949: step 52, loss 1.15635, acc 0.75, learning_rate 0.00407744
2017-10-10T11:12:50.941684: step 53, loss 1.16948, acc 0.75, learning_rate 0.00406121
2017-10-10T11:12:51.103231: step 54, loss 0.856138, acc 0.765625, learning_rate 0.00404504
2017-10-10T11:12:51.265636: step 55, loss 0.928053, acc 0.78125, learning_rate 0.00402894
2017-10-10T11:12:51.431025: step 56, loss 1.31579, acc 0.765625, learning_rate 0.0040129
2017-10-10T11:12:51.593234: step 57, loss 1.01147, acc 0.765625, learning_rate 0.00399693
2017-10-10T11:12:51.754950: step 58, loss 0.953565, acc 0.8125, learning_rate 0.00398102
2017-10-10T11:12:51.918852: step 59, loss 0.791561, acc 0.84375, learning_rate 0.00396518
2017-10-10T11:12:52.079302: step 60, loss 1.24962, acc 0.734375, learning_rate 0.00394941
2017-10-10T11:12:52.241206: step 61, loss 1.52804, acc 0.734375, learning_rate 0.00393369
2017-10-10T11:12:52.403026: step 62, loss 0.856098, acc 0.828125, learning_rate 0.00391804
2017-10-10T11:12:52.567338: step 63, loss 0.587642, acc 0.84375, learning_rate 0.00390246
2017-10-10T11:12:52.728571: step 64, loss 1.72915, acc 0.765625, learning_rate 0.00388694
2017-10-10T11:12:52.892484: step 65, loss 0.85916, acc 0.8125, learning_rate 0.00387148
2017-10-10T11:12:53.052801: step 66, loss 1.03273, acc 0.75, learning_rate 0.00385609
2017-10-10T11:12:53.214892: step 67, loss 0.712862, acc 0.875, learning_rate 0.00384076
2017-10-10T11:12:53.377469: step 68, loss 0.828061, acc 0.828125, learning_rate 0.00382549
2017-10-10T11:12:53.539595: step 69, loss 1.24857, acc 0.765625, learning_rate 0.00381028
2017-10-10T11:12:53.701415: step 70, loss 1.23478, acc 0.765625, learning_rate 0.00379514
2017-10-10T11:12:53.863632: step 71, loss 0.533419, acc 0.828125, learning_rate 0.00378005
2017-10-10T11:12:54.027450: step 72, loss 0.321222, acc 0.875, learning_rate 0.00376503
2017-10-10T11:12:54.186815: step 73, loss 1.25134, acc 0.796875, learning_rate 0.00375007
2017-10-10T11:12:54.349317: step 74, loss 0.531878, acc 0.84375, learning_rate 0.00373517
2017-10-10T11:12:54.512473: step 75, loss 0.478756, acc 0.890625, learning_rate 0.00372034
2017-10-10T11:12:54.674362: step 76, loss 0.487482, acc 0.84375, learning_rate 0.00370556
2017-10-10T11:12:54.835624: step 77, loss 0.951408, acc 0.78125, learning_rate 0.00369084
2017-10-10T11:12:54.997147: step 78, loss 0.457551, acc 0.875, learning_rate 0.00367619
2017-10-10T11:12:55.160548: step 79, loss 0.796938, acc 0.875, learning_rate 0.00366159
2017-10-10T11:12:55.323983: step 80, loss 0.61813, acc 0.84375, learning_rate 0.00364705

Evaluation:
2017-10-10T11:12:55.793554: step 80, loss 0.407397, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-80

2017-10-10T11:12:56.441938: step 81, loss 1.05075, acc 0.765625, learning_rate 0.00363257
2017-10-10T11:12:56.600261: step 82, loss 1.05508, acc 0.765625, learning_rate 0.00361815
2017-10-10T11:12:56.762757: step 83, loss 0.608084, acc 0.828125, learning_rate 0.00360379
2017-10-10T11:12:56.927907: step 84, loss 1.18994, acc 0.796875, learning_rate 0.00358949
2017-10-10T11:12:57.090538: step 85, loss 0.620381, acc 0.8125, learning_rate 0.00357525
2017-10-10T11:12:57.251921: step 86, loss 0.626085, acc 0.828125, learning_rate 0.00356106
2017-10-10T11:12:57.415619: step 87, loss 0.782884, acc 0.78125, learning_rate 0.00354694
2017-10-10T11:12:57.576439: step 88, loss 0.812617, acc 0.78125, learning_rate 0.00353287
2017-10-10T11:12:57.740273: step 89, loss 0.491398, acc 0.875, learning_rate 0.00351885
2017-10-10T11:12:57.906036: step 90, loss 1.12235, acc 0.765625, learning_rate 0.0035049
2017-10-10T11:12:58.069806: step 91, loss 1.2335, acc 0.828125, learning_rate 0.003491
2017-10-10T11:12:58.230524: step 92, loss 0.484838, acc 0.890625, learning_rate 0.00347716
2017-10-10T11:12:58.393055: step 93, loss 0.838549, acc 0.765625, learning_rate 0.00346338
2017-10-10T11:12:58.557401: step 94, loss 0.786643, acc 0.796875, learning_rate 0.00344965
2017-10-10T11:12:58.721886: step 95, loss 0.677419, acc 0.8125, learning_rate 0.00343597
2017-10-10T11:12:58.885495: step 96, loss 0.49633, acc 0.875, learning_rate 0.00342236
2017-10-10T11:12:59.051340: step 97, loss 1.99343, acc 0.71875, learning_rate 0.0034088
2017-10-10T11:12:59.185645: step 98, loss 1.40863, acc 0.784314, learning_rate 0.00339529
2017-10-10T11:12:59.348522: step 99, loss 0.418987, acc 0.84375, learning_rate 0.00338184
2017-10-10T11:12:59.510524: step 100, loss 1.23334, acc 0.75, learning_rate 0.00336844
2017-10-10T11:12:59.673339: step 101, loss 0.886474, acc 0.828125, learning_rate 0.0033551
2017-10-10T11:12:59.836379: step 102, loss 0.514612, acc 0.84375, learning_rate 0.00334182
2017-10-10T11:13:00.000993: step 103, loss 0.655631, acc 0.84375, learning_rate 0.00332858
2017-10-10T11:13:00.165104: step 104, loss 0.36592, acc 0.9375, learning_rate 0.00331541
2017-10-10T11:13:00.326520: step 105, loss 0.712529, acc 0.828125, learning_rate 0.00330228
2017-10-10T11:13:00.489107: step 106, loss 0.601516, acc 0.796875, learning_rate 0.00328921
2017-10-10T11:13:00.649034: step 107, loss 0.370951, acc 0.90625, learning_rate 0.00327619
2017-10-10T11:13:00.813362: step 108, loss 0.738077, acc 0.78125, learning_rate 0.00326323
2017-10-10T11:13:00.981296: step 109, loss 0.771983, acc 0.8125, learning_rate 0.00325032
2017-10-10T11:13:01.143897: step 110, loss 0.541873, acc 0.828125, learning_rate 0.00323746
2017-10-10T11:13:01.308890: step 111, loss 0.757582, acc 0.859375, learning_rate 0.00322465
2017-10-10T11:13:01.469947: step 112, loss 0.522153, acc 0.875, learning_rate 0.0032119
2017-10-10T11:13:01.632825: step 113, loss 0.524604, acc 0.859375, learning_rate 0.0031992
2017-10-10T11:13:01.794978: step 114, loss 0.356994, acc 0.84375, learning_rate 0.00318655
2017-10-10T11:13:01.958820: step 115, loss 0.463843, acc 0.859375, learning_rate 0.00317395
2017-10-10T11:13:02.122667: step 116, loss 1.24969, acc 0.734375, learning_rate 0.0031614
2017-10-10T11:13:02.284782: step 117, loss 0.637808, acc 0.84375, learning_rate 0.0031489
2017-10-10T11:13:02.445350: step 118, loss 0.854511, acc 0.796875, learning_rate 0.00313646
2017-10-10T11:13:02.608093: step 119, loss 0.494604, acc 0.875, learning_rate 0.00312407
2017-10-10T11:13:02.769906: step 120, loss 0.759763, acc 0.796875, learning_rate 0.00311172

Evaluation:
2017-10-10T11:13:03.261460: step 120, loss 0.363501, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-120

2017-10-10T11:13:03.931713: step 121, loss 0.347159, acc 0.9375, learning_rate 0.00309943
2017-10-10T11:13:04.098783: step 122, loss 0.337377, acc 0.890625, learning_rate 0.00308719
2017-10-10T11:13:04.260505: step 123, loss 0.607482, acc 0.859375, learning_rate 0.00307499
2017-10-10T11:13:04.421188: step 124, loss 0.73652, acc 0.796875, learning_rate 0.00306285
2017-10-10T11:13:04.583007: step 125, loss 0.500039, acc 0.859375, learning_rate 0.00305076
2017-10-10T11:13:04.743874: step 126, loss 0.329432, acc 0.9375, learning_rate 0.00303871
2017-10-10T11:13:04.907515: step 127, loss 0.372212, acc 0.890625, learning_rate 0.00302672
2017-10-10T11:13:05.073552: step 128, loss 0.545108, acc 0.84375, learning_rate 0.00301477
2017-10-10T11:13:05.233348: step 129, loss 0.759386, acc 0.8125, learning_rate 0.00300287
2017-10-10T11:13:05.396677: step 130, loss 0.4699, acc 0.828125, learning_rate 0.00299102
2017-10-10T11:13:05.560759: step 131, loss 0.584742, acc 0.890625, learning_rate 0.00297922
2017-10-10T11:13:05.722568: step 132, loss 0.748025, acc 0.8125, learning_rate 0.00296747
2017-10-10T11:13:05.887964: step 133, loss 0.44435, acc 0.859375, learning_rate 0.00295577
2017-10-10T11:13:06.050958: step 134, loss 0.159512, acc 0.9375, learning_rate 0.00294411
2017-10-10T11:13:06.212226: step 135, loss 0.204775, acc 0.921875, learning_rate 0.0029325
2017-10-10T11:13:06.373581: step 136, loss 0.559821, acc 0.875, learning_rate 0.00292094
2017-10-10T11:13:06.534649: step 137, loss 0.471239, acc 0.859375, learning_rate 0.00290943
2017-10-10T11:13:06.695076: step 138, loss 0.983602, acc 0.8125, learning_rate 0.00289796
2017-10-10T11:13:06.861927: step 139, loss 0.63408, acc 0.859375, learning_rate 0.00288654
2017-10-10T11:13:07.024291: step 140, loss 0.559303, acc 0.875, learning_rate 0.00287516
2017-10-10T11:13:07.184286: step 141, loss 0.373694, acc 0.875, learning_rate 0.00286384
2017-10-10T11:13:07.346447: step 142, loss 0.577574, acc 0.84375, learning_rate 0.00285256
2017-10-10T11:13:07.507572: step 143, loss 0.865864, acc 0.765625, learning_rate 0.00284132
2017-10-10T11:13:07.667308: step 144, loss 0.564345, acc 0.890625, learning_rate 0.00283013
2017-10-10T11:13:07.829982: step 145, loss 0.332209, acc 0.859375, learning_rate 0.00281899
2017-10-10T11:13:07.994430: step 146, loss 0.651034, acc 0.921875, learning_rate 0.00280789
2017-10-10T11:13:08.156157: step 147, loss 0.535325, acc 0.859375, learning_rate 0.00279684
2017-10-10T11:13:08.315617: step 148, loss 0.342549, acc 0.875, learning_rate 0.00278583
2017-10-10T11:13:08.476294: step 149, loss 0.308745, acc 0.9375, learning_rate 0.00277486
2017-10-10T11:13:08.640726: step 150, loss 0.638469, acc 0.84375, learning_rate 0.00276395
2017-10-10T11:13:08.801584: step 151, loss 0.661164, acc 0.84375, learning_rate 0.00275307
2017-10-10T11:13:08.971133: step 152, loss 0.469849, acc 0.84375, learning_rate 0.00274224
2017-10-10T11:13:09.132522: step 153, loss 0.636747, acc 0.796875, learning_rate 0.00273146
2017-10-10T11:13:09.296357: step 154, loss 0.54242, acc 0.84375, learning_rate 0.00272072
2017-10-10T11:13:09.461787: step 155, loss 0.375223, acc 0.890625, learning_rate 0.00271002
2017-10-10T11:13:09.625687: step 156, loss 0.957886, acc 0.828125, learning_rate 0.00269937
2017-10-10T11:13:09.787616: step 157, loss 0.101173, acc 0.953125, learning_rate 0.00268876
2017-10-10T11:13:09.952911: step 158, loss 0.271025, acc 0.921875, learning_rate 0.00267819
2017-10-10T11:13:10.113973: step 159, loss 0.825803, acc 0.8125, learning_rate 0.00266767
2017-10-10T11:13:10.275894: step 160, loss 0.595271, acc 0.828125, learning_rate 0.00265719

Evaluation:
2017-10-10T11:13:10.729286: step 160, loss 0.325316, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-160

2017-10-10T11:13:11.374897: step 161, loss 0.134625, acc 0.921875, learning_rate 0.00264675
2017-10-10T11:13:11.537157: step 162, loss 0.591017, acc 0.828125, learning_rate 0.00263635
2017-10-10T11:13:11.698741: step 163, loss 0.65733, acc 0.84375, learning_rate 0.002626
2017-10-10T11:13:11.863645: step 164, loss 0.52641, acc 0.859375, learning_rate 0.00261569
2017-10-10T11:13:12.027744: step 165, loss 0.707932, acc 0.8125, learning_rate 0.00260542
2017-10-10T11:13:12.189257: step 166, loss 0.637943, acc 0.828125, learning_rate 0.0025952
2017-10-10T11:13:12.349295: step 167, loss 0.0235807, acc 1, learning_rate 0.00258501
2017-10-10T11:13:12.512570: step 168, loss 0.367982, acc 0.84375, learning_rate 0.00257487
2017-10-10T11:13:12.670014: step 169, loss 0.418673, acc 0.90625, learning_rate 0.00256477
2017-10-10T11:13:12.829785: step 170, loss 0.446592, acc 0.859375, learning_rate 0.0025547
2017-10-10T11:13:13.001951: step 171, loss 0.723336, acc 0.78125, learning_rate 0.00254469
2017-10-10T11:13:13.165116: step 172, loss 0.617043, acc 0.828125, learning_rate 0.00253471
2017-10-10T11:13:13.326659: step 173, loss 0.609688, acc 0.859375, learning_rate 0.00252477
2017-10-10T11:13:13.490107: step 174, loss 0.399923, acc 0.859375, learning_rate 0.00251487
2017-10-10T11:13:13.651054: step 175, loss 0.234078, acc 0.90625, learning_rate 0.00250501
2017-10-10T11:13:13.814460: step 176, loss 0.230597, acc 0.9375, learning_rate 0.0024952
2017-10-10T11:13:13.979934: step 177, loss 0.333448, acc 0.921875, learning_rate 0.00248542
2017-10-10T11:13:14.141528: step 178, loss 0.413421, acc 0.828125, learning_rate 0.00247568
2017-10-10T11:13:14.305234: step 179, loss 0.419598, acc 0.859375, learning_rate 0.00246599
2017-10-10T11:13:14.467704: step 180, loss 0.449651, acc 0.859375, learning_rate 0.00245633
2017-10-10T11:13:14.631019: step 181, loss 0.933589, acc 0.8125, learning_rate 0.00244671
2017-10-10T11:13:14.798560: step 182, loss 0.722852, acc 0.828125, learning_rate 0.00243713
2017-10-10T11:13:14.965126: step 183, loss 0.848331, acc 0.78125, learning_rate 0.00242759
2017-10-10T11:13:15.126768: step 184, loss 0.921468, acc 0.75, learning_rate 0.00241809
2017-10-10T11:13:15.289772: step 185, loss 0.485031, acc 0.90625, learning_rate 0.00240863
2017-10-10T11:13:15.452509: step 186, loss 0.417302, acc 0.90625, learning_rate 0.00239921
2017-10-10T11:13:15.615463: step 187, loss 0.404273, acc 0.875, learning_rate 0.00238982
2017-10-10T11:13:15.777149: step 188, loss 0.249891, acc 0.921875, learning_rate 0.00238048
2017-10-10T11:13:15.941781: step 189, loss 0.226954, acc 0.921875, learning_rate 0.00237117
2017-10-10T11:13:16.103756: step 190, loss 0.496139, acc 0.875, learning_rate 0.0023619
2017-10-10T11:13:16.266520: step 191, loss 0.797267, acc 0.84375, learning_rate 0.00235267
2017-10-10T11:13:16.430903: step 192, loss 0.607624, acc 0.875, learning_rate 0.00234347
2017-10-10T11:13:16.595059: step 193, loss 0.422968, acc 0.84375, learning_rate 0.00233431
2017-10-10T11:13:16.755984: step 194, loss 0.584506, acc 0.890625, learning_rate 0.00232519
2017-10-10T11:13:16.921063: step 195, loss 0.497372, acc 0.890625, learning_rate 0.00231611
2017-10-10T11:13:17.055535: step 196, loss 0.571951, acc 0.862745, learning_rate 0.00230707
2017-10-10T11:13:17.215648: step 197, loss 0.241341, acc 0.921875, learning_rate 0.00229806
2017-10-10T11:13:17.375343: step 198, loss 0.245777, acc 0.90625, learning_rate 0.00228908
2017-10-10T11:13:17.539361: step 199, loss 0.326334, acc 0.90625, learning_rate 0.00228015
2017-10-10T11:13:17.700461: step 200, loss 0.350309, acc 0.859375, learning_rate 0.00227125

Evaluation:
2017-10-10T11:13:18.131157: step 200, loss 0.363127, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-200

2017-10-10T11:13:18.773308: step 201, loss 0.435528, acc 0.84375, learning_rate 0.00226239
2017-10-10T11:13:18.937756: step 202, loss 0.580064, acc 0.828125, learning_rate 0.00225356
2017-10-10T11:13:19.102914: step 203, loss 0.314372, acc 0.859375, learning_rate 0.00224477
2017-10-10T11:13:19.271449: step 204, loss 0.436521, acc 0.859375, learning_rate 0.00223602
2017-10-10T11:13:19.430898: step 205, loss 0.493456, acc 0.890625, learning_rate 0.0022273
2017-10-10T11:13:19.594935: step 206, loss 0.253107, acc 0.90625, learning_rate 0.00221862
2017-10-10T11:13:19.756506: step 207, loss 0.223004, acc 0.921875, learning_rate 0.00220997
2017-10-10T11:13:19.929380: step 208, loss 0.151469, acc 0.9375, learning_rate 0.00220136
2017-10-10T11:13:20.092389: step 209, loss 0.415029, acc 0.875, learning_rate 0.00219278
2017-10-10T11:13:20.255495: step 210, loss 0.267537, acc 0.921875, learning_rate 0.00218424
2017-10-10T11:13:20.418695: step 211, loss 0.411785, acc 0.875, learning_rate 0.00217573
2017-10-10T11:13:20.579479: step 212, loss 0.242355, acc 0.90625, learning_rate 0.00216726
2017-10-10T11:13:20.743137: step 213, loss 0.174325, acc 0.921875, learning_rate 0.00215882
2017-10-10T11:13:20.908190: step 214, loss 0.341381, acc 0.875, learning_rate 0.00215041
2017-10-10T11:13:21.071394: step 215, loss 0.358366, acc 0.890625, learning_rate 0.00214204
2017-10-10T11:13:21.230218: step 216, loss 0.595757, acc 0.859375, learning_rate 0.00213371
2017-10-10T11:13:21.391785: step 217, loss 0.174373, acc 0.921875, learning_rate 0.00212541
2017-10-10T11:13:21.554409: step 218, loss 0.649317, acc 0.890625, learning_rate 0.00211714
2017-10-10T11:13:21.716833: step 219, loss 0.680086, acc 0.828125, learning_rate 0.00210891
2017-10-10T11:13:21.883511: step 220, loss 0.406891, acc 0.875, learning_rate 0.00210071
2017-10-10T11:13:22.044007: step 221, loss 0.379872, acc 0.875, learning_rate 0.00209254
2017-10-10T11:13:22.207418: step 222, loss 0.288204, acc 0.9375, learning_rate 0.00208441
2017-10-10T11:13:22.371140: step 223, loss 0.15719, acc 0.953125, learning_rate 0.00207631
2017-10-10T11:13:22.533098: step 224, loss 0.244469, acc 0.875, learning_rate 0.00206824
2017-10-10T11:13:22.695657: step 225, loss 0.191293, acc 0.953125, learning_rate 0.00206021
2017-10-10T11:13:22.858346: step 226, loss 0.279243, acc 0.90625, learning_rate 0.00205221
2017-10-10T11:13:23.022831: step 227, loss 0.354456, acc 0.921875, learning_rate 0.00204424
2017-10-10T11:13:23.182748: step 228, loss 0.689717, acc 0.78125, learning_rate 0.0020363
2017-10-10T11:13:23.345234: step 229, loss 0.351597, acc 0.875, learning_rate 0.0020284
2017-10-10T11:13:23.507578: step 230, loss 0.499793, acc 0.8125, learning_rate 0.00202053
2017-10-10T11:13:23.669512: step 231, loss 0.705755, acc 0.84375, learning_rate 0.00201269
2017-10-10T11:13:23.831756: step 232, loss 0.286503, acc 0.90625, learning_rate 0.00200488
2017-10-10T11:13:23.996317: step 233, loss 0.264087, acc 0.953125, learning_rate 0.00199711
2017-10-10T11:13:24.159494: step 234, loss 0.128085, acc 0.953125, learning_rate 0.00198936
2017-10-10T11:13:24.324072: step 235, loss 0.464365, acc 0.875, learning_rate 0.00198165
2017-10-10T11:13:24.487137: step 236, loss 0.247776, acc 0.90625, learning_rate 0.00197397
2017-10-10T11:13:24.649377: step 237, loss 0.401145, acc 0.875, learning_rate 0.00196632
2017-10-10T11:13:24.811163: step 238, loss 0.455117, acc 0.875, learning_rate 0.0019587
2017-10-10T11:13:24.973450: step 239, loss 0.415024, acc 0.90625, learning_rate 0.00195112
2017-10-10T11:13:25.135896: step 240, loss 0.283679, acc 0.921875, learning_rate 0.00194356

Evaluation:
2017-10-10T11:13:25.565275: step 240, loss 0.299939, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-240

2017-10-10T11:13:26.216873: step 241, loss 0.511891, acc 0.859375, learning_rate 0.00193604
2017-10-10T11:13:26.380346: step 242, loss 0.296875, acc 0.890625, learning_rate 0.00192854
2017-10-10T11:13:26.541187: step 243, loss 0.627733, acc 0.859375, learning_rate 0.00192108
2017-10-10T11:13:26.705699: step 244, loss 0.606117, acc 0.8125, learning_rate 0.00191364
2017-10-10T11:13:26.869722: step 245, loss 0.432449, acc 0.84375, learning_rate 0.00190624
2017-10-10T11:13:27.029460: step 246, loss 0.447428, acc 0.875, learning_rate 0.00189887
2017-10-10T11:13:27.193961: step 247, loss 0.644069, acc 0.875, learning_rate 0.00189153
2017-10-10T11:13:27.353789: step 248, loss 0.520487, acc 0.921875, learning_rate 0.00188421
2017-10-10T11:13:27.518014: step 249, loss 0.30726, acc 0.90625, learning_rate 0.00187693
2017-10-10T11:13:27.680387: step 250, loss 0.43435, acc 0.84375, learning_rate 0.00186968
2017-10-10T11:13:27.843017: step 251, loss 0.315882, acc 0.90625, learning_rate 0.00186245
2017-10-10T11:13:28.004900: step 252, loss 0.256563, acc 0.90625, learning_rate 0.00185526
2017-10-10T11:13:28.168276: step 253, loss 0.212724, acc 0.921875, learning_rate 0.0018481
2017-10-10T11:13:28.330707: step 254, loss 0.489199, acc 0.859375, learning_rate 0.00184096
2017-10-10T11:13:28.497356: step 255, loss 0.281699, acc 0.921875, learning_rate 0.00183385
2017-10-10T11:13:28.660879: step 256, loss 0.405405, acc 0.859375, learning_rate 0.00182678
2017-10-10T11:13:28.821574: step 257, loss 0.391218, acc 0.921875, learning_rate 0.00181973
2017-10-10T11:13:28.984498: step 258, loss 0.602008, acc 0.84375, learning_rate 0.00181271
2017-10-10T11:13:29.145923: step 259, loss 0.235112, acc 0.921875, learning_rate 0.00180572
2017-10-10T11:13:29.306721: step 260, loss 0.396457, acc 0.875, learning_rate 0.00179876
2017-10-10T11:13:29.468961: step 261, loss 0.569363, acc 0.859375, learning_rate 0.00179182
2017-10-10T11:13:29.629488: step 262, loss 0.5642, acc 0.828125, learning_rate 0.00178492
2017-10-10T11:13:29.791483: step 263, loss 0.381298, acc 0.875, learning_rate 0.00177804
2017-10-10T11:13:29.955369: step 264, loss 0.244158, acc 0.953125, learning_rate 0.00177119
2017-10-10T11:13:30.115379: step 265, loss 0.36892, acc 0.90625, learning_rate 0.00176437
2017-10-10T11:13:30.277906: step 266, loss 0.286156, acc 0.921875, learning_rate 0.00175758
2017-10-10T11:13:30.438634: step 267, loss 0.408109, acc 0.890625, learning_rate 0.00175081
2017-10-10T11:13:30.599416: step 268, loss 0.131617, acc 0.953125, learning_rate 0.00174407
2017-10-10T11:13:30.763251: step 269, loss 0.403708, acc 0.875, learning_rate 0.00173736
2017-10-10T11:13:30.932627: step 270, loss 0.144497, acc 0.9375, learning_rate 0.00173068
2017-10-10T11:13:31.098236: step 271, loss 0.175102, acc 0.921875, learning_rate 0.00172402
2017-10-10T11:13:31.264393: step 272, loss 0.200379, acc 0.90625, learning_rate 0.00171739
2017-10-10T11:13:31.429758: step 273, loss 0.0822044, acc 0.953125, learning_rate 0.00171079
2017-10-10T11:13:31.589484: step 274, loss 0.613481, acc 0.859375, learning_rate 0.00170422
2017-10-10T11:13:31.748946: step 275, loss 0.4759, acc 0.890625, learning_rate 0.00169767
2017-10-10T11:13:31.924146: step 276, loss 0.269807, acc 0.90625, learning_rate 0.00169115
2017-10-10T11:13:32.086285: step 277, loss 0.391197, acc 0.921875, learning_rate 0.00168465
2017-10-10T11:13:32.245689: step 278, loss 0.325366, acc 0.90625, learning_rate 0.00167818
2017-10-10T11:13:32.409649: step 279, loss 0.478564, acc 0.890625, learning_rate 0.00167174
2017-10-10T11:13:32.572974: step 280, loss 0.170345, acc 0.953125, learning_rate 0.00166533

Evaluation:
2017-10-10T11:13:33.006812: step 280, loss 0.253952, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-280

2017-10-10T11:13:33.655760: step 281, loss 0.285258, acc 0.9375, learning_rate 0.00165894
2017-10-10T11:13:33.814839: step 282, loss 0.851445, acc 0.765625, learning_rate 0.00165257
2017-10-10T11:13:33.979375: step 283, loss 0.318805, acc 0.921875, learning_rate 0.00164624
2017-10-10T11:13:34.141343: step 284, loss 0.747804, acc 0.78125, learning_rate 0.00163993
2017-10-10T11:13:34.302813: step 285, loss 0.314529, acc 0.90625, learning_rate 0.00163364
2017-10-10T11:13:34.466752: step 286, loss 0.236901, acc 0.90625, learning_rate 0.00162738
2017-10-10T11:13:34.626956: step 287, loss 0.255836, acc 0.9375, learning_rate 0.00162115
2017-10-10T11:13:34.791995: step 288, loss 0.181082, acc 0.921875, learning_rate 0.00161494
2017-10-10T11:13:34.959945: step 289, loss 0.653798, acc 0.8125, learning_rate 0.00160875
2017-10-10T11:13:35.124801: step 290, loss 0.383438, acc 0.875, learning_rate 0.00160259
2017-10-10T11:13:35.284377: step 291, loss 0.533155, acc 0.859375, learning_rate 0.00159646
2017-10-10T11:13:35.446406: step 292, loss 0.305778, acc 0.90625, learning_rate 0.00159035
2017-10-10T11:13:35.613888: step 293, loss 0.404271, acc 0.84375, learning_rate 0.00158427
2017-10-10T11:13:35.749860: step 294, loss 0.312009, acc 0.901961, learning_rate 0.00157821
2017-10-10T11:13:35.917127: step 295, loss 0.308389, acc 0.90625, learning_rate 0.00157218
2017-10-10T11:13:36.078300: step 296, loss 0.478049, acc 0.875, learning_rate 0.00156617
2017-10-10T11:13:36.240712: step 297, loss 0.274043, acc 0.921875, learning_rate 0.00156018
2017-10-10T11:13:36.402556: step 298, loss 0.181056, acc 0.90625, learning_rate 0.00155422
2017-10-10T11:13:36.559472: step 299, loss 0.405134, acc 0.890625, learning_rate 0.00154829
2017-10-10T11:13:36.722235: step 300, loss 0.188128, acc 0.953125, learning_rate 0.00154238
2017-10-10T11:13:36.888019: step 301, loss 0.418841, acc 0.875, learning_rate 0.00153649
2017-10-10T11:13:37.051627: step 302, loss 0.42616, acc 0.890625, learning_rate 0.00153063
2017-10-10T11:13:37.214999: step 303, loss 0.292364, acc 0.9375, learning_rate 0.00152479
2017-10-10T11:13:37.375658: step 304, loss 0.252976, acc 0.875, learning_rate 0.00151897
2017-10-10T11:13:37.535879: step 305, loss 0.229832, acc 0.921875, learning_rate 0.00151318
2017-10-10T11:13:37.700425: step 306, loss 0.666167, acc 0.828125, learning_rate 0.00150741
2017-10-10T11:13:37.865702: step 307, loss 0.201427, acc 0.921875, learning_rate 0.00150167
2017-10-10T11:13:38.029771: step 308, loss 0.233214, acc 0.90625, learning_rate 0.00149594
2017-10-10T11:13:38.189518: step 309, loss 0.244293, acc 0.953125, learning_rate 0.00149025
2017-10-10T11:13:38.349062: step 310, loss 0.229894, acc 0.890625, learning_rate 0.00148457
2017-10-10T11:13:38.510234: step 311, loss 0.354322, acc 0.890625, learning_rate 0.00147892
2017-10-10T11:13:38.673519: step 312, loss 0.304555, acc 0.953125, learning_rate 0.00147329
2017-10-10T11:13:38.834051: step 313, loss 0.173223, acc 0.90625, learning_rate 0.00146769
2017-10-10T11:13:38.995336: step 314, loss 0.375957, acc 0.875, learning_rate 0.0014621
2017-10-10T11:13:39.158164: step 315, loss 0.296829, acc 0.9375, learning_rate 0.00145654
2017-10-10T11:13:39.320827: step 316, loss 0.38686, acc 0.828125, learning_rate 0.00145101
2017-10-10T11:13:39.483300: step 317, loss 0.278834, acc 0.90625, learning_rate 0.00144549
2017-10-10T11:13:39.642805: step 318, loss 0.4061, acc 0.875, learning_rate 0.00144
2017-10-10T11:13:39.805987: step 319, loss 0.374051, acc 0.859375, learning_rate 0.00143453
2017-10-10T11:13:39.969141: step 320, loss 0.214183, acc 0.921875, learning_rate 0.00142908

Evaluation:
2017-10-10T11:13:40.385341: step 320, loss 0.272807, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-320

2017-10-10T11:13:41.041525: step 321, loss 0.638059, acc 0.828125, learning_rate 0.00142366
2017-10-10T11:13:41.205936: step 322, loss 0.11388, acc 0.984375, learning_rate 0.00141826
2017-10-10T11:13:41.366733: step 323, loss 0.208225, acc 0.921875, learning_rate 0.00141288
2017-10-10T11:13:41.529698: step 324, loss 0.545027, acc 0.8125, learning_rate 0.00140752
2017-10-10T11:13:41.694052: step 325, loss 0.393359, acc 0.828125, learning_rate 0.00140218
2017-10-10T11:13:41.870771: step 326, loss 0.272103, acc 0.90625, learning_rate 0.00139686
2017-10-10T11:13:42.037417: step 327, loss 0.145056, acc 0.953125, learning_rate 0.00139157
2017-10-10T11:13:42.199500: step 328, loss 0.386552, acc 0.875, learning_rate 0.0013863
2017-10-10T11:13:42.358300: step 329, loss 0.258987, acc 0.90625, learning_rate 0.00138105
2017-10-10T11:13:42.516559: step 330, loss 0.28789, acc 0.921875, learning_rate 0.00137582
2017-10-10T11:13:42.674416: step 331, loss 0.210005, acc 0.921875, learning_rate 0.00137061
2017-10-10T11:13:42.835998: step 332, loss 0.196343, acc 0.921875, learning_rate 0.00136543
2017-10-10T11:13:43.007104: step 333, loss 0.372797, acc 0.90625, learning_rate 0.00136026
2017-10-10T11:13:43.168828: step 334, loss 0.198007, acc 0.921875, learning_rate 0.00135512
2017-10-10T11:13:43.328881: step 335, loss 0.199747, acc 0.96875, learning_rate 0.00134999
2017-10-10T11:13:43.491444: step 336, loss 0.401174, acc 0.875, learning_rate 0.00134489
2017-10-10T11:13:43.651035: step 337, loss 0.2317, acc 0.90625, learning_rate 0.00133981
2017-10-10T11:13:43.810118: step 338, loss 0.345611, acc 0.921875, learning_rate 0.00133475
2017-10-10T11:13:43.972550: step 339, loss 0.227778, acc 0.921875, learning_rate 0.00132971
2017-10-10T11:13:44.134785: step 340, loss 0.304131, acc 0.90625, learning_rate 0.00132469
2017-10-10T11:13:44.298410: step 341, loss 0.227639, acc 0.875, learning_rate 0.00131969
2017-10-10T11:13:44.458571: step 342, loss 0.218848, acc 0.90625, learning_rate 0.00131471
2017-10-10T11:13:44.621925: step 343, loss 0.163636, acc 0.9375, learning_rate 0.00130975
2017-10-10T11:13:44.785635: step 344, loss 0.24658, acc 0.90625, learning_rate 0.00130482
2017-10-10T11:13:44.946520: step 345, loss 0.136557, acc 0.96875, learning_rate 0.0012999
2017-10-10T11:13:45.107789: step 346, loss 0.381221, acc 0.859375, learning_rate 0.001295
2017-10-10T11:13:45.270812: step 347, loss 0.236966, acc 0.921875, learning_rate 0.00129012
2017-10-10T11:13:45.433023: step 348, loss 0.226113, acc 0.921875, learning_rate 0.00128527
2017-10-10T11:13:45.598646: step 349, loss 0.247109, acc 0.90625, learning_rate 0.00128043
2017-10-10T11:13:45.764254: step 350, loss 0.298533, acc 0.921875, learning_rate 0.00127561
2017-10-10T11:13:45.941918: step 351, loss 0.107873, acc 0.953125, learning_rate 0.00127081
2017-10-10T11:13:46.112246: step 352, loss 0.212314, acc 0.9375, learning_rate 0.00126603
2017-10-10T11:13:46.282290: step 353, loss 0.684221, acc 0.890625, learning_rate 0.00126127
2017-10-10T11:13:46.448532: step 354, loss 0.54706, acc 0.84375, learning_rate 0.00125653
2017-10-10T11:13:46.611090: step 355, loss 0.203007, acc 0.9375, learning_rate 0.00125181
2017-10-10T11:13:46.781167: step 356, loss 0.115792, acc 0.953125, learning_rate 0.00124711
2017-10-10T11:13:46.945279: step 357, loss 0.247089, acc 0.9375, learning_rate 0.00124243
2017-10-10T11:13:47.106862: step 358, loss 0.27187, acc 0.90625, learning_rate 0.00123777
2017-10-10T11:13:47.269677: step 359, loss 0.202963, acc 0.90625, learning_rate 0.00123312
2017-10-10T11:13:47.429312: step 360, loss 0.295572, acc 0.90625, learning_rate 0.0012285

Evaluation:
2017-10-10T11:13:47.859470: step 360, loss 0.255508, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-360

2017-10-10T11:13:48.510859: step 361, loss 0.232142, acc 0.890625, learning_rate 0.00122389
2017-10-10T11:13:48.674158: step 362, loss 0.188844, acc 0.921875, learning_rate 0.0012193
2017-10-10T11:13:48.834960: step 363, loss 0.441216, acc 0.875, learning_rate 0.00121473
2017-10-10T11:13:48.999324: step 364, loss 0.228792, acc 0.921875, learning_rate 0.00121018
2017-10-10T11:13:49.160185: step 365, loss 0.333632, acc 0.90625, learning_rate 0.00120565
2017-10-10T11:13:49.322514: step 366, loss 0.319591, acc 0.890625, learning_rate 0.00120114
2017-10-10T11:13:49.482916: step 367, loss 0.339035, acc 0.875, learning_rate 0.00119664
2017-10-10T11:13:49.645573: step 368, loss 0.15319, acc 0.96875, learning_rate 0.00119217
2017-10-10T11:13:49.807700: step 369, loss 0.0984714, acc 0.96875, learning_rate 0.00118771
2017-10-10T11:13:49.972662: step 370, loss 0.364151, acc 0.890625, learning_rate 0.00118327
2017-10-10T11:13:50.132945: step 371, loss 0.100773, acc 0.953125, learning_rate 0.00117885
2017-10-10T11:13:50.292134: step 372, loss 0.139601, acc 0.9375, learning_rate 0.00117445
2017-10-10T11:13:50.448688: step 373, loss 0.216455, acc 0.9375, learning_rate 0.00117006
2017-10-10T11:13:50.611266: step 374, loss 0.373028, acc 0.90625, learning_rate 0.00116569
2017-10-10T11:13:50.768177: step 375, loss 0.107932, acc 0.9375, learning_rate 0.00116134
2017-10-10T11:13:50.933247: step 376, loss 0.184405, acc 0.9375, learning_rate 0.00115701
2017-10-10T11:13:51.094053: step 377, loss 0.208554, acc 0.921875, learning_rate 0.0011527
2017-10-10T11:13:51.254942: step 378, loss 0.198676, acc 0.9375, learning_rate 0.0011484
2017-10-10T11:13:51.417080: step 379, loss 0.172433, acc 0.9375, learning_rate 0.00114412
2017-10-10T11:13:51.576945: step 380, loss 0.29169, acc 0.890625, learning_rate 0.00113986
2017-10-10T11:13:51.738843: step 381, loss 0.359999, acc 0.890625, learning_rate 0.00113561
2017-10-10T11:13:51.902703: step 382, loss 0.140792, acc 0.9375, learning_rate 0.00113139
2017-10-10T11:13:52.064471: step 383, loss 0.289162, acc 0.90625, learning_rate 0.00112718
2017-10-10T11:13:52.224770: step 384, loss 0.23444, acc 0.90625, learning_rate 0.00112298
2017-10-10T11:13:52.387962: step 385, loss 0.278283, acc 0.90625, learning_rate 0.00111881
2017-10-10T11:13:52.549534: step 386, loss 0.0914018, acc 0.96875, learning_rate 0.00111465
2017-10-10T11:13:52.712346: step 387, loss 0.159316, acc 0.96875, learning_rate 0.00111051
2017-10-10T11:13:52.875862: step 388, loss 0.524654, acc 0.84375, learning_rate 0.00110638
2017-10-10T11:13:53.036233: step 389, loss 0.530127, acc 0.875, learning_rate 0.00110228
2017-10-10T11:13:53.199433: step 390, loss 0.207204, acc 0.96875, learning_rate 0.00109818
2017-10-10T11:13:53.362503: step 391, loss 0.417106, acc 0.921875, learning_rate 0.00109411
2017-10-10T11:13:53.496690: step 392, loss 0.335077, acc 0.921569, learning_rate 0.00109005
2017-10-10T11:13:53.660017: step 393, loss 0.218747, acc 0.90625, learning_rate 0.00108601
2017-10-10T11:13:53.820137: step 394, loss 0.207398, acc 0.9375, learning_rate 0.00108199
2017-10-10T11:13:53.989037: step 395, loss 0.346326, acc 0.890625, learning_rate 0.00107798
2017-10-10T11:13:54.150618: step 396, loss 0.0737944, acc 0.984375, learning_rate 0.00107399
2017-10-10T11:13:54.316018: step 397, loss 0.268728, acc 0.890625, learning_rate 0.00107001
2017-10-10T11:13:54.475259: step 398, loss 0.166146, acc 0.921875, learning_rate 0.00106605
2017-10-10T11:13:54.634928: step 399, loss 0.358316, acc 0.90625, learning_rate 0.00106211
2017-10-10T11:13:54.796074: step 400, loss 0.377958, acc 0.90625, learning_rate 0.00105818

Evaluation:
2017-10-10T11:13:55.869387: step 400, loss 0.240251, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-400

2017-10-10T11:13:56.781572: step 401, loss 0.244693, acc 0.90625, learning_rate 0.00105427
2017-10-10T11:13:56.945836: step 402, loss 0.423299, acc 0.890625, learning_rate 0.00105037
2017-10-10T11:13:57.108740: step 403, loss 0.0942474, acc 0.953125, learning_rate 0.0010465
2017-10-10T11:13:57.271188: step 404, loss 0.418363, acc 0.890625, learning_rate 0.00104263
2017-10-10T11:13:57.433456: step 405, loss 0.440668, acc 0.875, learning_rate 0.00103878
2017-10-10T11:13:57.598693: step 406, loss 0.235923, acc 0.890625, learning_rate 0.00103495
2017-10-10T11:13:57.758918: step 407, loss 0.546734, acc 0.875, learning_rate 0.00103114
2017-10-10T11:13:57.924147: step 408, loss 0.0986178, acc 0.96875, learning_rate 0.00102734
2017-10-10T11:13:58.084269: step 409, loss 0.293338, acc 0.90625, learning_rate 0.00102355
2017-10-10T11:13:58.245829: step 410, loss 0.134528, acc 0.96875, learning_rate 0.00101978
2017-10-10T11:13:58.404610: step 411, loss 0.314686, acc 0.890625, learning_rate 0.00101603
2017-10-10T11:13:58.564172: step 412, loss 0.195345, acc 0.9375, learning_rate 0.00101229
2017-10-10T11:13:58.725343: step 413, loss 0.228402, acc 0.90625, learning_rate 0.00100856
2017-10-10T11:13:58.895592: step 414, loss 0.181994, acc 0.9375, learning_rate 0.00100486
2017-10-10T11:13:59.057350: step 415, loss 0.187981, acc 0.921875, learning_rate 0.00100116
2017-10-10T11:13:59.220136: step 416, loss 0.230083, acc 0.9375, learning_rate 0.000997483
2017-10-10T11:13:59.382076: step 417, loss 0.104395, acc 0.96875, learning_rate 0.00099382
2017-10-10T11:13:59.544101: step 418, loss 0.298234, acc 0.84375, learning_rate 0.000990172
2017-10-10T11:13:59.705355: step 419, loss 0.274099, acc 0.953125, learning_rate 0.000986538
2017-10-10T11:13:59.869378: step 420, loss 0.171345, acc 0.953125, learning_rate 0.00098292
2017-10-10T11:14:00.033840: step 421, loss 0.118832, acc 0.953125, learning_rate 0.000979316
2017-10-10T11:14:00.197961: step 422, loss 0.2014, acc 0.90625, learning_rate 0.000975727
2017-10-10T11:14:00.359153: step 423, loss 0.281389, acc 0.859375, learning_rate 0.000972152
2017-10-10T11:14:00.523244: step 424, loss 0.125524, acc 0.953125, learning_rate 0.000968592
2017-10-10T11:14:00.687624: step 425, loss 0.297589, acc 0.90625, learning_rate 0.000965047
2017-10-10T11:14:00.850337: step 426, loss 0.192141, acc 0.921875, learning_rate 0.000961516
2017-10-10T11:14:01.018225: step 427, loss 0.146215, acc 0.953125, learning_rate 0.000958
2017-10-10T11:14:01.178146: step 428, loss 0.324254, acc 0.9375, learning_rate 0.000954497
2017-10-10T11:14:01.344552: step 429, loss 0.315059, acc 0.890625, learning_rate 0.00095101
2017-10-10T11:14:01.508226: step 430, loss 0.264142, acc 0.9375, learning_rate 0.000947536
2017-10-10T11:14:01.671033: step 431, loss 0.108988, acc 0.9375, learning_rate 0.000944076
2017-10-10T11:14:01.833363: step 432, loss 0.284916, acc 0.90625, learning_rate 0.000940631
2017-10-10T11:14:01.993808: step 433, loss 0.322181, acc 0.921875, learning_rate 0.0009372
2017-10-10T11:14:02.155500: step 434, loss 0.149741, acc 0.90625, learning_rate 0.000933783
2017-10-10T11:14:02.314836: step 435, loss 0.104201, acc 0.953125, learning_rate 0.000930379
2017-10-10T11:14:02.476100: step 436, loss 0.208974, acc 0.953125, learning_rate 0.00092699
2017-10-10T11:14:02.636490: step 437, loss 0.216555, acc 0.921875, learning_rate 0.000923614
2017-10-10T11:14:02.799954: step 438, loss 0.170002, acc 0.9375, learning_rate 0.000920253
2017-10-10T11:14:02.962443: step 439, loss 0.0827655, acc 0.96875, learning_rate 0.000916905
2017-10-10T11:14:03.123153: step 440, loss 0.338115, acc 0.875, learning_rate 0.00091357

Evaluation:
2017-10-10T11:14:03.554043: step 440, loss 0.270294, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-440

2017-10-10T11:14:04.196089: step 441, loss 0.382643, acc 0.921875, learning_rate 0.000910249
2017-10-10T11:14:04.356024: step 442, loss 0.106763, acc 0.96875, learning_rate 0.000906942
2017-10-10T11:14:04.517858: step 443, loss 0.149579, acc 0.921875, learning_rate 0.000903648
2017-10-10T11:14:04.679570: step 444, loss 0.363515, acc 0.890625, learning_rate 0.000900368
2017-10-10T11:14:04.841834: step 445, loss 0.4355, acc 0.890625, learning_rate 0.000897101
2017-10-10T11:14:05.006728: step 446, loss 0.37096, acc 0.890625, learning_rate 0.000893848
2017-10-10T11:14:05.170087: step 447, loss 0.111179, acc 0.9375, learning_rate 0.000890607
2017-10-10T11:14:05.330251: step 448, loss 0.166243, acc 0.921875, learning_rate 0.00088738
2017-10-10T11:14:05.488862: step 449, loss 0.388736, acc 0.859375, learning_rate 0.000884166
2017-10-10T11:14:05.647435: step 450, loss 0.213397, acc 0.921875, learning_rate 0.000880966
2017-10-10T11:14:05.811418: step 451, loss 0.106573, acc 0.953125, learning_rate 0.000877778
2017-10-10T11:14:05.976970: step 452, loss 0.244788, acc 0.90625, learning_rate 0.000874603
2017-10-10T11:14:06.142134: step 453, loss 0.243661, acc 0.9375, learning_rate 0.000871441
2017-10-10T11:14:06.304621: step 454, loss 0.110971, acc 0.96875, learning_rate 0.000868293
2017-10-10T11:14:06.464387: step 455, loss 0.294737, acc 0.890625, learning_rate 0.000865157
2017-10-10T11:14:06.623910: step 456, loss 0.209188, acc 0.921875, learning_rate 0.000862033
2017-10-10T11:14:06.787051: step 457, loss 0.270364, acc 0.921875, learning_rate 0.000858923
2017-10-10T11:14:06.952141: step 458, loss 0.169218, acc 0.921875, learning_rate 0.000855825
2017-10-10T11:14:07.118228: step 459, loss 0.179319, acc 0.953125, learning_rate 0.00085274
2017-10-10T11:14:07.278609: step 460, loss 0.136822, acc 0.96875, learning_rate 0.000849668
2017-10-10T11:14:07.442485: step 461, loss 0.0967745, acc 0.953125, learning_rate 0.000846608
2017-10-10T11:14:07.605568: step 462, loss 0.169974, acc 0.921875, learning_rate 0.00084356
2017-10-10T11:14:07.768543: step 463, loss 0.309055, acc 0.875, learning_rate 0.000840525
2017-10-10T11:14:07.932977: step 464, loss 0.17073, acc 0.9375, learning_rate 0.000837502
2017-10-10T11:14:08.095291: step 465, loss 0.492196, acc 0.84375, learning_rate 0.000834492
2017-10-10T11:14:08.255883: step 466, loss 0.228731, acc 0.9375, learning_rate 0.000831494
2017-10-10T11:14:08.418898: step 467, loss 0.180335, acc 0.953125, learning_rate 0.000828508
2017-10-10T11:14:08.583112: step 468, loss 0.264986, acc 0.921875, learning_rate 0.000825535
2017-10-10T11:14:08.745560: step 469, loss 0.189287, acc 0.921875, learning_rate 0.000822573
2017-10-10T11:14:08.910925: step 470, loss 0.0825239, acc 0.96875, learning_rate 0.000819624
2017-10-10T11:14:09.076285: step 471, loss 0.340105, acc 0.90625, learning_rate 0.000816687
2017-10-10T11:14:09.237271: step 472, loss 0.153497, acc 0.9375, learning_rate 0.000813761
2017-10-10T11:14:09.400876: step 473, loss 0.207492, acc 0.9375, learning_rate 0.000810848
2017-10-10T11:14:09.561704: step 474, loss 0.112633, acc 0.9375, learning_rate 0.000807946
2017-10-10T11:14:09.722974: step 475, loss 0.631456, acc 0.859375, learning_rate 0.000805057
2017-10-10T11:14:09.889656: step 476, loss 0.0605661, acc 1, learning_rate 0.000802179
2017-10-10T11:14:10.054069: step 477, loss 0.297883, acc 0.890625, learning_rate 0.000799313
2017-10-10T11:14:10.218039: step 478, loss 0.327427, acc 0.890625, learning_rate 0.000796458
2017-10-10T11:14:10.381720: step 479, loss 0.241478, acc 0.90625, learning_rate 0.000793616
2017-10-10T11:14:10.543438: step 480, loss 0.365451, acc 0.9375, learning_rate 0.000790784

Evaluation:
2017-10-10T11:14:10.992373: step 480, loss 0.241303, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-480

2017-10-10T11:14:11.708146: step 481, loss 0.483318, acc 0.875, learning_rate 0.000787965
2017-10-10T11:14:11.873813: step 482, loss 0.200917, acc 0.90625, learning_rate 0.000785157
2017-10-10T11:14:12.035874: step 483, loss 0.249237, acc 0.875, learning_rate 0.00078236
2017-10-10T11:14:12.196549: step 484, loss 0.130873, acc 0.96875, learning_rate 0.000779575
2017-10-10T11:14:12.357414: step 485, loss 0.0912937, acc 0.953125, learning_rate 0.000776801
2017-10-10T11:14:12.518361: step 486, loss 0.552753, acc 0.84375, learning_rate 0.000774038
2017-10-10T11:14:12.681488: step 487, loss 0.210672, acc 0.921875, learning_rate 0.000771287
2017-10-10T11:14:12.842139: step 488, loss 0.230351, acc 0.921875, learning_rate 0.000768547
2017-10-10T11:14:13.005839: step 489, loss 0.436911, acc 0.859375, learning_rate 0.000765818
2017-10-10T11:14:13.138642: step 490, loss 0.0720129, acc 0.980392, learning_rate 0.000763101
2017-10-10T11:14:13.302531: step 491, loss 0.132221, acc 0.9375, learning_rate 0.000760394
2017-10-10T11:14:13.462893: step 492, loss 0.160867, acc 0.9375, learning_rate 0.000757698
2017-10-10T11:14:13.626222: step 493, loss 0.0635214, acc 0.96875, learning_rate 0.000755014
2017-10-10T11:14:13.790808: step 494, loss 0.377768, acc 0.890625, learning_rate 0.00075234
2017-10-10T11:14:13.954274: step 495, loss 0.192769, acc 0.90625, learning_rate 0.000749677
2017-10-10T11:14:14.114976: step 496, loss 0.140614, acc 0.96875, learning_rate 0.000747026
2017-10-10T11:14:14.277237: step 497, loss 0.281848, acc 0.890625, learning_rate 0.000744385
2017-10-10T11:14:14.440097: step 498, loss 0.0739097, acc 0.96875, learning_rate 0.000741754
2017-10-10T11:14:14.600678: step 499, loss 0.0914361, acc 0.96875, learning_rate 0.000739135
2017-10-10T11:14:14.763297: step 500, loss 0.171478, acc 0.9375, learning_rate 0.000736526
2017-10-10T11:14:14.928796: step 501, loss 0.053576, acc 1, learning_rate 0.000733928
2017-10-10T11:14:15.090773: step 502, loss 0.309359, acc 0.875, learning_rate 0.00073134
2017-10-10T11:14:15.252329: step 503, loss 0.155618, acc 0.96875, learning_rate 0.000728763
2017-10-10T11:14:15.414970: step 504, loss 0.280719, acc 0.921875, learning_rate 0.000726197
2017-10-10T11:14:15.580590: step 505, loss 0.177288, acc 0.9375, learning_rate 0.000723641
2017-10-10T11:14:15.743894: step 506, loss 0.181956, acc 0.9375, learning_rate 0.000721095
2017-10-10T11:14:15.908434: step 507, loss 0.204888, acc 0.921875, learning_rate 0.00071856
2017-10-10T11:14:16.069653: step 508, loss 0.107235, acc 0.9375, learning_rate 0.000716036
2017-10-10T11:14:16.233209: step 509, loss 0.383116, acc 0.875, learning_rate 0.000713521
2017-10-10T11:14:16.396315: step 510, loss 0.164364, acc 0.9375, learning_rate 0.000711017
2017-10-10T11:14:16.555809: step 511, loss 0.212011, acc 0.90625, learning_rate 0.000708523
2017-10-10T11:14:16.721147: step 512, loss 0.172356, acc 0.921875, learning_rate 0.000706039
2017-10-10T11:14:16.890573: step 513, loss 0.4011, acc 0.875, learning_rate 0.000703565
2017-10-10T11:14:17.052083: step 514, loss 0.167054, acc 0.953125, learning_rate 0.000701102
2017-10-10T11:14:17.214459: step 515, loss 0.346946, acc 0.953125, learning_rate 0.000698648
2017-10-10T11:14:17.377572: step 516, loss 0.0986457, acc 0.9375, learning_rate 0.000696204
2017-10-10T11:14:17.539569: step 517, loss 0.117301, acc 0.96875, learning_rate 0.000693771
2017-10-10T11:14:17.702868: step 518, loss 0.295243, acc 0.875, learning_rate 0.000691347
2017-10-10T11:14:17.864564: step 519, loss 0.40024, acc 0.859375, learning_rate 0.000688934
2017-10-10T11:14:18.024676: step 520, loss 0.105326, acc 0.984375, learning_rate 0.00068653

Evaluation:
2017-10-10T11:14:18.450318: step 520, loss 0.235959, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-520

2017-10-10T11:14:19.025242: step 521, loss 0.225628, acc 0.890625, learning_rate 0.000684136
2017-10-10T11:14:19.187535: step 522, loss 0.209804, acc 0.953125, learning_rate 0.000681751
2017-10-10T11:14:19.349540: step 523, loss 0.163114, acc 0.9375, learning_rate 0.000679377
2017-10-10T11:14:19.517370: step 524, loss 0.152384, acc 0.9375, learning_rate 0.000677012
2017-10-10T11:14:19.682159: step 525, loss 0.177442, acc 0.9375, learning_rate 0.000674657
2017-10-10T11:14:19.846892: step 526, loss 0.350392, acc 0.875, learning_rate 0.000672311
2017-10-10T11:14:20.009000: step 527, loss 0.15669, acc 0.9375, learning_rate 0.000669975
2017-10-10T11:14:20.172202: step 528, loss 0.290787, acc 0.890625, learning_rate 0.000667648
2017-10-10T11:14:20.333460: step 529, loss 0.15048, acc 0.9375, learning_rate 0.000665331
2017-10-10T11:14:20.494990: step 530, loss 0.154295, acc 0.921875, learning_rate 0.000663024
2017-10-10T11:14:20.656569: step 531, loss 0.221538, acc 0.890625, learning_rate 0.000660726
2017-10-10T11:14:20.819517: step 532, loss 0.325767, acc 0.90625, learning_rate 0.000658437
2017-10-10T11:14:20.981954: step 533, loss 0.316729, acc 0.890625, learning_rate 0.000656158
2017-10-10T11:14:21.145219: step 534, loss 0.185148, acc 0.921875, learning_rate 0.000653888
2017-10-10T11:14:21.307895: step 535, loss 0.0840486, acc 0.96875, learning_rate 0.000651627
2017-10-10T11:14:21.468081: step 536, loss 0.222344, acc 0.9375, learning_rate 0.000649375
2017-10-10T11:14:21.628782: step 537, loss 0.242833, acc 0.90625, learning_rate 0.000647133
2017-10-10T11:14:21.791401: step 538, loss 0.212393, acc 0.9375, learning_rate 0.000644899
2017-10-10T11:14:21.959509: step 539, loss 0.339773, acc 0.890625, learning_rate 0.000642675
2017-10-10T11:14:22.121768: step 540, loss 0.101111, acc 0.96875, learning_rate 0.00064046
2017-10-10T11:14:22.283467: step 541, loss 0.163245, acc 0.90625, learning_rate 0.000638254
2017-10-10T11:14:22.445631: step 542, loss 0.147623, acc 0.9375, learning_rate 0.000636057
2017-10-10T11:14:22.604044: step 543, loss 0.0727258, acc 0.96875, learning_rate 0.000633869
2017-10-10T11:14:22.768280: step 544, loss 0.23542, acc 0.890625, learning_rate 0.00063169
2017-10-10T11:14:22.935277: step 545, loss 0.235384, acc 0.96875, learning_rate 0.00062952
2017-10-10T11:14:23.098218: step 546, loss 0.340378, acc 0.921875, learning_rate 0.000627358
2017-10-10T11:14:23.258336: step 547, loss 0.165085, acc 0.9375, learning_rate 0.000625206
2017-10-10T11:14:23.419948: step 548, loss 0.386833, acc 0.890625, learning_rate 0.000623062
2017-10-10T11:14:23.581676: step 549, loss 0.157834, acc 0.9375, learning_rate 0.000620927
2017-10-10T11:14:23.744051: step 550, loss 0.0403594, acc 0.984375, learning_rate 0.000618801
2017-10-10T11:14:23.907663: step 551, loss 0.20621, acc 0.96875, learning_rate 0.000616683
2017-10-10T11:14:24.071447: step 552, loss 0.15721, acc 0.90625, learning_rate 0.000614574
2017-10-10T11:14:24.233256: step 553, loss 0.187469, acc 0.9375, learning_rate 0.000612474
2017-10-10T11:14:24.397628: step 554, loss 0.208711, acc 0.96875, learning_rate 0.000610382
2017-10-10T11:14:24.557199: step 555, loss 0.0251137, acc 1, learning_rate 0.000608299
2017-10-10T11:14:24.719636: step 556, loss 0.182797, acc 0.921875, learning_rate 0.000606224
2017-10-10T11:14:24.883380: step 557, loss 0.196995, acc 0.921875, learning_rate 0.000604158
2017-10-10T11:14:25.046084: step 558, loss 0.134168, acc 0.9375, learning_rate 0.0006021
2017-10-10T11:14:25.204892: step 559, loss 0.145895, acc 0.953125, learning_rate 0.00060005
2017-10-10T11:14:25.367979: step 560, loss 0.143997, acc 0.96875, learning_rate 0.000598009

Evaluation:
2017-10-10T11:14:25.773217: step 560, loss 0.233132, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-560

2017-10-10T11:14:26.410564: step 561, loss 0.11112, acc 0.953125, learning_rate 0.000595977
2017-10-10T11:14:26.572450: step 562, loss 0.326072, acc 0.9375, learning_rate 0.000593952
2017-10-10T11:14:26.734953: step 563, loss 0.119302, acc 0.953125, learning_rate 0.000591936
2017-10-10T11:14:26.895477: step 564, loss 0.232754, acc 0.90625, learning_rate 0.000589928
2017-10-10T11:14:27.055573: step 565, loss 0.0989893, acc 0.984375, learning_rate 0.000587928
2017-10-10T11:14:27.215117: step 566, loss 0.288536, acc 0.84375, learning_rate 0.000585937
2017-10-10T11:14:27.376869: step 567, loss 0.150998, acc 0.96875, learning_rate 0.000583953
2017-10-10T11:14:27.540742: step 568, loss 0.248967, acc 0.9375, learning_rate 0.000581978
2017-10-10T11:14:27.704191: step 569, loss 0.182014, acc 0.921875, learning_rate 0.00058001
2017-10-10T11:14:27.870990: step 570, loss 0.27486, acc 0.90625, learning_rate 0.000578051
2017-10-10T11:14:28.033357: step 571, loss 0.0633766, acc 0.984375, learning_rate 0.0005761
2017-10-10T11:14:28.196734: step 572, loss 0.151701, acc 0.9375, learning_rate 0.000574157
2017-10-10T11:14:28.360776: step 573, loss 0.172056, acc 0.953125, learning_rate 0.000572221
2017-10-10T11:14:28.522319: step 574, loss 0.0559068, acc 0.984375, learning_rate 0.000570294
2017-10-10T11:14:28.682872: step 575, loss 0.16751, acc 0.96875, learning_rate 0.000568374
2017-10-10T11:14:28.844358: step 576, loss 0.143329, acc 0.9375, learning_rate 0.000566462
2017-10-10T11:14:29.003932: step 577, loss 0.278364, acc 0.875, learning_rate 0.000564558
2017-10-10T11:14:29.164965: step 578, loss 0.151619, acc 0.953125, learning_rate 0.000562662
2017-10-10T11:14:29.327338: step 579, loss 0.130127, acc 0.953125, learning_rate 0.000560774
2017-10-10T11:14:29.489237: step 580, loss 0.152832, acc 0.9375, learning_rate 0.000558893
2017-10-10T11:14:29.654882: step 581, loss 0.395329, acc 0.875, learning_rate 0.00055702
2017-10-10T11:14:29.816660: step 582, loss 0.0632087, acc 0.984375, learning_rate 0.000555154
2017-10-10T11:14:29.993141: step 583, loss 0.168807, acc 0.9375, learning_rate 0.000553296
2017-10-10T11:14:30.158968: step 584, loss 0.144683, acc 0.9375, learning_rate 0.000551446
2017-10-10T11:14:30.320453: step 585, loss 0.145845, acc 0.9375, learning_rate 0.000549604
2017-10-10T11:14:30.484973: step 586, loss 0.296507, acc 0.90625, learning_rate 0.000547768
2017-10-10T11:14:30.652615: step 587, loss 0.145947, acc 0.96875, learning_rate 0.000545941
2017-10-10T11:14:30.788817: step 588, loss 0.239956, acc 0.941176, learning_rate 0.00054412
2017-10-10T11:14:30.953905: step 589, loss 0.187823, acc 0.953125, learning_rate 0.000542308
2017-10-10T11:14:31.116332: step 590, loss 0.122044, acc 0.953125, learning_rate 0.000540502
2017-10-10T11:14:31.278698: step 591, loss 0.136046, acc 0.9375, learning_rate 0.000538704
2017-10-10T11:14:31.445189: step 592, loss 0.102932, acc 0.953125, learning_rate 0.000536914
2017-10-10T11:14:31.602396: step 593, loss 0.295304, acc 0.859375, learning_rate 0.00053513
2017-10-10T11:14:31.765200: step 594, loss 0.138334, acc 0.921875, learning_rate 0.000533354
2017-10-10T11:14:31.929284: step 595, loss 0.0982079, acc 0.953125, learning_rate 0.000531585
2017-10-10T11:14:32.093177: step 596, loss 0.153401, acc 0.953125, learning_rate 0.000529824
2017-10-10T11:14:32.255439: step 597, loss 0.224184, acc 0.9375, learning_rate 0.000528069
2017-10-10T11:14:32.420194: step 598, loss 0.107182, acc 0.9375, learning_rate 0.000526322
2017-10-10T11:14:32.574983: step 599, loss 0.251883, acc 0.875, learning_rate 0.000524582
2017-10-10T11:14:32.738019: step 600, loss 0.0946568, acc 0.96875, learning_rate 0.000522849

Evaluation:
2017-10-10T11:14:33.206879: step 600, loss 0.228572, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-600

2017-10-10T11:14:33.923979: step 601, loss 0.177894, acc 0.90625, learning_rate 0.000521123
2017-10-10T11:14:34.086849: step 602, loss 0.131146, acc 0.984375, learning_rate 0.000519404
2017-10-10T11:14:34.250182: step 603, loss 0.0876862, acc 0.96875, learning_rate 0.000517692
2017-10-10T11:14:34.413080: step 604, loss 0.182318, acc 0.9375, learning_rate 0.000515987
2017-10-10T11:14:34.573130: step 605, loss 0.237669, acc 0.953125, learning_rate 0.000514289
2017-10-10T11:14:34.733767: step 606, loss 0.295277, acc 0.90625, learning_rate 0.000512598
2017-10-10T11:14:34.896117: step 607, loss 0.0517743, acc 0.984375, learning_rate 0.000510914
2017-10-10T11:14:35.058024: step 608, loss 0.207214, acc 0.9375, learning_rate 0.000509237
2017-10-10T11:14:35.219724: step 609, loss 0.301288, acc 0.9375, learning_rate 0.000507566
2017-10-10T11:14:35.379745: step 610, loss 0.141358, acc 0.953125, learning_rate 0.000505903
2017-10-10T11:14:35.541758: step 611, loss 0.135459, acc 0.96875, learning_rate 0.000504246
2017-10-10T11:14:35.706503: step 612, loss 0.185486, acc 0.875, learning_rate 0.000502596
2017-10-10T11:14:35.871297: step 613, loss 0.191358, acc 0.921875, learning_rate 0.000500953
2017-10-10T11:14:36.036348: step 614, loss 0.267609, acc 0.90625, learning_rate 0.000499316
2017-10-10T11:14:36.200235: step 615, loss 0.180756, acc 0.953125, learning_rate 0.000497686
2017-10-10T11:14:36.363220: step 616, loss 0.162141, acc 0.96875, learning_rate 0.000496063
2017-10-10T11:14:36.526611: step 617, loss 0.324667, acc 0.90625, learning_rate 0.000494446
2017-10-10T11:14:36.689867: step 618, loss 0.141289, acc 0.953125, learning_rate 0.000492836
2017-10-10T11:14:36.853485: step 619, loss 0.271344, acc 0.90625, learning_rate 0.000491233
2017-10-10T11:14:37.013398: step 620, loss 0.255217, acc 0.890625, learning_rate 0.000489636
2017-10-10T11:14:37.171970: step 621, loss 0.0975974, acc 0.953125, learning_rate 0.000488045
2017-10-10T11:14:37.338102: step 622, loss 0.300886, acc 0.90625, learning_rate 0.000486461
2017-10-10T11:14:37.499376: step 623, loss 0.138585, acc 0.96875, learning_rate 0.000484884
2017-10-10T11:14:37.662671: step 624, loss 0.111495, acc 0.953125, learning_rate 0.000483313
2017-10-10T11:14:37.826930: step 625, loss 0.0765344, acc 0.96875, learning_rate 0.000481748
2017-10-10T11:14:37.989464: step 626, loss 0.275494, acc 0.921875, learning_rate 0.00048019
2017-10-10T11:14:38.152077: step 627, loss 0.153522, acc 0.921875, learning_rate 0.000478638
2017-10-10T11:14:38.315191: step 628, loss 0.178976, acc 0.921875, learning_rate 0.000477093
2017-10-10T11:14:38.478253: step 629, loss 0.127052, acc 0.953125, learning_rate 0.000475554
2017-10-10T11:14:38.638749: step 630, loss 0.0945154, acc 0.96875, learning_rate 0.000474021
2017-10-10T11:14:38.800386: step 631, loss 0.274673, acc 0.921875, learning_rate 0.000472494
2017-10-10T11:14:38.980555: step 632, loss 0.189063, acc 0.96875, learning_rate 0.000470974
2017-10-10T11:14:39.143822: step 633, loss 0.297789, acc 0.890625, learning_rate 0.000469459
2017-10-10T11:14:39.306134: step 634, loss 0.178726, acc 0.9375, learning_rate 0.000467951
2017-10-10T11:14:39.468933: step 635, loss 0.174437, acc 0.953125, learning_rate 0.000466449
2017-10-10T11:14:39.632181: step 636, loss 0.121283, acc 0.9375, learning_rate 0.000464954
2017-10-10T11:14:39.793476: step 637, loss 0.257908, acc 0.90625, learning_rate 0.000463464
2017-10-10T11:14:39.955853: step 638, loss 0.0566355, acc 0.96875, learning_rate 0.00046198
2017-10-10T11:14:40.119177: step 639, loss 0.222275, acc 0.90625, learning_rate 0.000460503
2017-10-10T11:14:40.281304: step 640, loss 0.237961, acc 0.9375, learning_rate 0.000459031

Evaluation:
2017-10-10T11:14:40.716810: step 640, loss 0.231864, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-640

2017-10-10T11:14:41.287729: step 641, loss 0.0405136, acc 1, learning_rate 0.000457566
2017-10-10T11:14:41.450197: step 642, loss 0.203866, acc 0.96875, learning_rate 0.000456106
2017-10-10T11:14:41.614955: step 643, loss 0.255661, acc 0.90625, learning_rate 0.000454653
2017-10-10T11:14:41.776877: step 644, loss 0.243385, acc 0.9375, learning_rate 0.000453205
2017-10-10T11:14:41.941835: step 645, loss 0.397222, acc 0.84375, learning_rate 0.000451764
2017-10-10T11:14:42.104356: step 646, loss 0.291443, acc 0.890625, learning_rate 0.000450328
2017-10-10T11:14:42.265747: step 647, loss 0.111758, acc 0.953125, learning_rate 0.000448898
2017-10-10T11:14:42.427149: step 648, loss 0.176426, acc 0.9375, learning_rate 0.000447474
2017-10-10T11:14:42.588167: step 649, loss 0.21107, acc 0.921875, learning_rate 0.000446055
2017-10-10T11:14:42.748363: step 650, loss 0.179959, acc 0.9375, learning_rate 0.000444643
2017-10-10T11:14:42.912171: step 651, loss 0.394589, acc 0.921875, learning_rate 0.000443236
2017-10-10T11:14:43.073035: step 652, loss 0.0931637, acc 0.953125, learning_rate 0.000441835
2017-10-10T11:14:43.233522: step 653, loss 0.228577, acc 0.9375, learning_rate 0.00044044
2017-10-10T11:14:43.395305: step 654, loss 0.170323, acc 0.90625, learning_rate 0.00043905
2017-10-10T11:14:43.554750: step 655, loss 0.187444, acc 0.921875, learning_rate 0.000437666
2017-10-10T11:14:43.714425: step 656, loss 0.317141, acc 0.890625, learning_rate 0.000436288
2017-10-10T11:14:43.891645: step 657, loss 0.151714, acc 0.953125, learning_rate 0.000434915
2017-10-10T11:14:44.055174: step 658, loss 0.0786683, acc 0.96875, learning_rate 0.000433548
2017-10-10T11:14:44.218861: step 659, loss 0.137106, acc 0.9375, learning_rate 0.000432187
2017-10-10T11:14:44.379884: step 660, loss 0.347632, acc 0.875, learning_rate 0.000430831
2017-10-10T11:14:44.543014: step 661, loss 0.159288, acc 0.9375, learning_rate 0.000429481
2017-10-10T11:14:44.704945: step 662, loss 0.17508, acc 0.9375, learning_rate 0.000428136
2017-10-10T11:14:44.867922: step 663, loss 0.217727, acc 0.921875, learning_rate 0.000426796
2017-10-10T11:14:45.031882: step 664, loss 0.191789, acc 0.9375, learning_rate 0.000425463
2017-10-10T11:14:45.196419: step 665, loss 0.147783, acc 0.953125, learning_rate 0.000424134
2017-10-10T11:14:45.359059: step 666, loss 0.273414, acc 0.875, learning_rate 0.000422811
2017-10-10T11:14:45.520494: step 667, loss 0.210919, acc 0.9375, learning_rate 0.000421493
2017-10-10T11:14:45.683139: step 668, loss 0.257141, acc 0.90625, learning_rate 0.000420181
2017-10-10T11:14:45.845825: step 669, loss 0.181453, acc 0.921875, learning_rate 0.000418874
2017-10-10T11:14:46.008619: step 670, loss 0.0671385, acc 0.984375, learning_rate 0.000417573
2017-10-10T11:14:46.168821: step 671, loss 0.210038, acc 0.921875, learning_rate 0.000416276
2017-10-10T11:14:46.329760: step 672, loss 0.0796683, acc 0.96875, learning_rate 0.000414985
2017-10-10T11:14:46.489629: step 673, loss 0.231481, acc 0.921875, learning_rate 0.0004137
2017-10-10T11:14:46.651577: step 674, loss 0.223091, acc 0.921875, learning_rate 0.000412419
2017-10-10T11:14:46.811316: step 675, loss 0.146468, acc 0.9375, learning_rate 0.000411144
2017-10-10T11:14:46.978360: step 676, loss 0.227138, acc 0.9375, learning_rate 0.000409874
2017-10-10T11:14:47.141504: step 677, loss 0.108813, acc 0.9375, learning_rate 0.000408609
2017-10-10T11:14:47.299979: step 678, loss 0.139059, acc 0.953125, learning_rate 0.00040735
2017-10-10T11:14:47.465294: step 679, loss 0.293242, acc 0.9375, learning_rate 0.000406095
2017-10-10T11:14:47.625034: step 680, loss 0.264523, acc 0.9375, learning_rate 0.000404846

Evaluation:
2017-10-10T11:14:48.065216: step 680, loss 0.235835, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-680

2017-10-10T11:14:48.700569: step 681, loss 0.139056, acc 0.953125, learning_rate 0.000403601
2017-10-10T11:14:48.863728: step 682, loss 0.184546, acc 0.90625, learning_rate 0.000402362
2017-10-10T11:14:49.028119: step 683, loss 0.151451, acc 0.921875, learning_rate 0.000401128
2017-10-10T11:14:49.185462: step 684, loss 0.114373, acc 0.9375, learning_rate 0.000399899
2017-10-10T11:14:49.347460: step 685, loss 0.152983, acc 0.9375, learning_rate 0.000398675
2017-10-10T11:14:49.481712: step 686, loss 0.238553, acc 0.921569, learning_rate 0.000397456
2017-10-10T11:14:49.645655: step 687, loss 0.126543, acc 0.96875, learning_rate 0.000396241
2017-10-10T11:14:49.808075: step 688, loss 0.158397, acc 0.953125, learning_rate 0.000395032
2017-10-10T11:14:49.978074: step 689, loss 0.143362, acc 0.953125, learning_rate 0.000393828
2017-10-10T11:14:50.136908: step 690, loss 0.185929, acc 0.953125, learning_rate 0.000392629
2017-10-10T11:14:50.297351: step 691, loss 0.166118, acc 0.953125, learning_rate 0.000391434
2017-10-10T11:14:50.458356: step 692, loss 0.191724, acc 0.921875, learning_rate 0.000390245
2017-10-10T11:14:50.621304: step 693, loss 0.16907, acc 0.953125, learning_rate 0.00038906
2017-10-10T11:14:50.786437: step 694, loss 0.301235, acc 0.890625, learning_rate 0.00038788
2017-10-10T11:14:50.952274: step 695, loss 0.168189, acc 0.90625, learning_rate 0.000386705
2017-10-10T11:14:51.113990: step 696, loss 0.18616, acc 0.953125, learning_rate 0.000385535
2017-10-10T11:14:51.279026: step 697, loss 0.0423567, acc 0.984375, learning_rate 0.000384369
2017-10-10T11:14:51.443365: step 698, loss 0.163881, acc 0.96875, learning_rate 0.000383209
2017-10-10T11:14:51.605174: step 699, loss 0.194936, acc 0.921875, learning_rate 0.000382053
2017-10-10T11:14:51.767978: step 700, loss 0.211105, acc 0.921875, learning_rate 0.000380901
2017-10-10T11:14:51.933496: step 701, loss 0.04729, acc 0.984375, learning_rate 0.000379755
2017-10-10T11:14:52.095683: step 702, loss 0.14041, acc 0.921875, learning_rate 0.000378613
2017-10-10T11:14:52.257975: step 703, loss 0.334464, acc 0.875, learning_rate 0.000377476
2017-10-10T11:14:52.419719: step 704, loss 0.18688, acc 0.9375, learning_rate 0.000376343
2017-10-10T11:14:52.581392: step 705, loss 0.333341, acc 0.921875, learning_rate 0.000375215
2017-10-10T11:14:52.743258: step 706, loss 0.158766, acc 0.96875, learning_rate 0.000374092
2017-10-10T11:14:52.908162: step 707, loss 0.138319, acc 0.953125, learning_rate 0.000372973
2017-10-10T11:14:53.070639: step 708, loss 0.109079, acc 0.96875, learning_rate 0.000371859
2017-10-10T11:14:53.229170: step 709, loss 0.0885661, acc 0.984375, learning_rate 0.000370749
2017-10-10T11:14:53.391577: step 710, loss 0.0931967, acc 0.953125, learning_rate 0.000369644
2017-10-10T11:14:53.553287: step 711, loss 0.124862, acc 0.96875, learning_rate 0.000368543
2017-10-10T11:14:53.717961: step 712, loss 0.305259, acc 0.90625, learning_rate 0.000367447
2017-10-10T11:14:53.880654: step 713, loss 0.188706, acc 0.921875, learning_rate 0.000366356
2017-10-10T11:14:54.044405: step 714, loss 0.293144, acc 0.890625, learning_rate 0.000365268
2017-10-10T11:14:54.208151: step 715, loss 0.202823, acc 0.90625, learning_rate 0.000364186
2017-10-10T11:14:54.368757: step 716, loss 0.203403, acc 0.90625, learning_rate 0.000363107
2017-10-10T11:14:54.530789: step 717, loss 0.257949, acc 0.9375, learning_rate 0.000362033
2017-10-10T11:14:54.691566: step 718, loss 0.143328, acc 0.953125, learning_rate 0.000360964
2017-10-10T11:14:54.856482: step 719, loss 0.194189, acc 0.921875, learning_rate 0.000359899
2017-10-10T11:14:55.018846: step 720, loss 0.184955, acc 0.921875, learning_rate 0.000358838

Evaluation:
2017-10-10T11:14:55.461859: step 720, loss 0.220888, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-720

2017-10-10T11:14:56.173464: step 721, loss 0.343625, acc 0.859375, learning_rate 0.000357781
2017-10-10T11:14:56.335473: step 722, loss 0.217389, acc 0.921875, learning_rate 0.000356729
2017-10-10T11:14:56.499358: step 723, loss 0.191895, acc 0.921875, learning_rate 0.000355681
2017-10-10T11:14:56.659902: step 724, loss 0.0798434, acc 0.953125, learning_rate 0.000354637
2017-10-10T11:14:56.824280: step 725, loss 0.344543, acc 0.890625, learning_rate 0.000353598
2017-10-10T11:14:56.995174: step 726, loss 0.261399, acc 0.921875, learning_rate 0.000352563
2017-10-10T11:14:57.157981: step 727, loss 0.170092, acc 0.921875, learning_rate 0.000351532
2017-10-10T11:14:57.318806: step 728, loss 0.357246, acc 0.90625, learning_rate 0.000350505
2017-10-10T11:14:57.483666: step 729, loss 0.0910494, acc 0.9375, learning_rate 0.000349483
2017-10-10T11:14:57.645486: step 730, loss 0.102665, acc 0.96875, learning_rate 0.000348465
2017-10-10T11:14:57.805251: step 731, loss 0.229892, acc 0.90625, learning_rate 0.00034745
2017-10-10T11:14:57.968789: step 732, loss 0.0871311, acc 0.96875, learning_rate 0.00034644
2017-10-10T11:14:58.128617: step 733, loss 0.149136, acc 0.9375, learning_rate 0.000345434
2017-10-10T11:14:58.290706: step 734, loss 0.292555, acc 0.9375, learning_rate 0.000344433
2017-10-10T11:14:58.450047: step 735, loss 0.101074, acc 0.96875, learning_rate 0.000343435
2017-10-10T11:14:58.612796: step 736, loss 0.134528, acc 0.9375, learning_rate 0.000342441
2017-10-10T11:14:58.770037: step 737, loss 0.181111, acc 0.953125, learning_rate 0.000341452
2017-10-10T11:14:58.942219: step 738, loss 0.150905, acc 0.96875, learning_rate 0.000340466
2017-10-10T11:14:59.104390: step 739, loss 0.120884, acc 0.921875, learning_rate 0.000339485
2017-10-10T11:14:59.268585: step 740, loss 0.0935299, acc 0.96875, learning_rate 0.000338507
2017-10-10T11:14:59.431625: step 741, loss 0.175179, acc 0.953125, learning_rate 0.000337534
2017-10-10T11:14:59.591158: step 742, loss 0.263714, acc 0.90625, learning_rate 0.000336564
2017-10-10T11:14:59.754745: step 743, loss 0.218628, acc 0.921875, learning_rate 0.000335598
2017-10-10T11:14:59.920308: step 744, loss 0.108843, acc 0.953125, learning_rate 0.000334637
2017-10-10T11:15:00.080981: step 745, loss 0.177561, acc 0.921875, learning_rate 0.000333679
2017-10-10T11:15:00.244636: step 746, loss 0.165644, acc 0.953125, learning_rate 0.000332725
2017-10-10T11:15:00.405113: step 747, loss 0.0712702, acc 0.96875, learning_rate 0.000331775
2017-10-10T11:15:00.568062: step 748, loss 0.112442, acc 0.953125, learning_rate 0.000330829
2017-10-10T11:15:00.731835: step 749, loss 0.0502005, acc 0.96875, learning_rate 0.000329887
2017-10-10T11:15:00.896613: step 750, loss 0.0824947, acc 0.953125, learning_rate 0.000328949
2017-10-10T11:15:01.058472: step 751, loss 0.231912, acc 0.921875, learning_rate 0.000328014
2017-10-10T11:15:01.221335: step 752, loss 0.0971014, acc 0.984375, learning_rate 0.000327083
2017-10-10T11:15:01.386332: step 753, loss 0.204567, acc 0.921875, learning_rate 0.000326157
2017-10-10T11:15:01.545917: step 754, loss 0.291811, acc 0.921875, learning_rate 0.000325233
2017-10-10T11:15:01.705570: step 755, loss 0.132446, acc 0.9375, learning_rate 0.000324314
2017-10-10T11:15:01.872531: step 756, loss 0.065698, acc 0.984375, learning_rate 0.000323399
2017-10-10T11:15:02.033983: step 757, loss 0.137486, acc 0.96875, learning_rate 0.000322487
2017-10-10T11:15:02.194700: step 758, loss 0.111773, acc 0.953125, learning_rate 0.000321579
2017-10-10T11:15:02.358848: step 759, loss 0.0819636, acc 0.953125, learning_rate 0.000320674
2017-10-10T11:15:02.518626: step 760, loss 0.187833, acc 0.9375, learning_rate 0.000319773

Evaluation:
2017-10-10T11:15:02.956010: step 760, loss 0.226205, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-760

2017-10-10T11:15:03.529389: step 761, loss 0.100144, acc 0.96875, learning_rate 0.000318876
2017-10-10T11:15:03.688237: step 762, loss 0.0958174, acc 0.953125, learning_rate 0.000317983
2017-10-10T11:15:03.855191: step 763, loss 0.118956, acc 0.96875, learning_rate 0.000317093
2017-10-10T11:15:04.016878: step 764, loss 0.0562573, acc 0.96875, learning_rate 0.000316207
2017-10-10T11:15:04.177882: step 765, loss 0.172329, acc 0.96875, learning_rate 0.000315325
2017-10-10T11:15:04.339146: step 766, loss 0.0938576, acc 0.953125, learning_rate 0.000314446
2017-10-10T11:15:04.501279: step 767, loss 0.0876283, acc 0.9375, learning_rate 0.00031357
2017-10-10T11:15:04.667264: step 768, loss 0.256011, acc 0.921875, learning_rate 0.000312699
2017-10-10T11:15:04.829319: step 769, loss 0.296316, acc 0.890625, learning_rate 0.00031183
2017-10-10T11:15:04.993047: step 770, loss 0.349638, acc 0.890625, learning_rate 0.000310966
2017-10-10T11:15:05.155094: step 771, loss 0.275518, acc 0.90625, learning_rate 0.000310105
2017-10-10T11:15:05.315179: step 772, loss 0.298008, acc 0.921875, learning_rate 0.000309247
2017-10-10T11:15:05.475406: step 773, loss 0.179382, acc 0.953125, learning_rate 0.000308393
2017-10-10T11:15:05.636363: step 774, loss 0.146796, acc 0.9375, learning_rate 0.000307542
2017-10-10T11:15:05.797232: step 775, loss 0.171124, acc 0.9375, learning_rate 0.000306695
2017-10-10T11:15:05.961238: step 776, loss 0.048868, acc 0.984375, learning_rate 0.000305852
2017-10-10T11:15:06.123539: step 777, loss 0.132596, acc 0.9375, learning_rate 0.000305011
2017-10-10T11:15:06.286366: step 778, loss 0.192074, acc 0.90625, learning_rate 0.000304174
2017-10-10T11:15:06.448527: step 779, loss 0.0706981, acc 0.984375, learning_rate 0.000303341
2017-10-10T11:15:06.608001: step 780, loss 0.134148, acc 0.96875, learning_rate 0.000302511
2017-10-10T11:15:06.772241: step 781, loss 0.163106, acc 0.9375, learning_rate 0.000301684
2017-10-10T11:15:06.940493: step 782, loss 0.121193, acc 0.96875, learning_rate 0.000300861
2017-10-10T11:15:07.105939: step 783, loss 0.0962629, acc 0.96875, learning_rate 0.000300041
2017-10-10T11:15:07.244960: step 784, loss 0.17664, acc 0.901961, learning_rate 0.000299225
2017-10-10T11:15:07.415144: step 785, loss 0.102422, acc 0.96875, learning_rate 0.000298412
2017-10-10T11:15:07.579631: step 786, loss 0.102689, acc 0.96875, learning_rate 0.000297602
2017-10-10T11:15:07.741574: step 787, loss 0.0646915, acc 0.953125, learning_rate 0.000296795
2017-10-10T11:15:07.916174: step 788, loss 0.201944, acc 0.90625, learning_rate 0.000295992
2017-10-10T11:15:08.081025: step 789, loss 0.183204, acc 0.921875, learning_rate 0.000295192
2017-10-10T11:15:08.250367: step 790, loss 0.110234, acc 0.96875, learning_rate 0.000294395
2017-10-10T11:15:08.416129: step 791, loss 0.075951, acc 0.984375, learning_rate 0.000293602
2017-10-10T11:15:08.575599: step 792, loss 0.0657749, acc 0.984375, learning_rate 0.000292812
2017-10-10T11:15:08.741866: step 793, loss 0.158951, acc 0.96875, learning_rate 0.000292025
2017-10-10T11:15:08.903591: step 794, loss 0.185981, acc 0.953125, learning_rate 0.000291241
2017-10-10T11:15:09.067785: step 795, loss 0.219738, acc 0.921875, learning_rate 0.00029046
2017-10-10T11:15:09.231663: step 796, loss 0.119931, acc 0.984375, learning_rate 0.000289683
2017-10-10T11:15:09.392728: step 797, loss 0.17924, acc 0.9375, learning_rate 0.000288908
2017-10-10T11:15:09.555712: step 798, loss 0.192247, acc 0.921875, learning_rate 0.000288137
2017-10-10T11:15:09.712917: step 799, loss 0.118648, acc 0.9375, learning_rate 0.000287369
2017-10-10T11:15:09.876503: step 800, loss 0.0953519, acc 0.96875, learning_rate 0.000286605

Evaluation:
2017-10-10T11:15:10.326722: step 800, loss 0.226708, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-800

2017-10-10T11:15:10.971240: step 801, loss 0.215908, acc 0.9375, learning_rate 0.000285843
2017-10-10T11:15:11.131026: step 802, loss 0.123831, acc 0.984375, learning_rate 0.000285084
2017-10-10T11:15:11.293238: step 803, loss 0.264363, acc 0.90625, learning_rate 0.000284329
2017-10-10T11:15:11.457581: step 804, loss 0.133641, acc 0.921875, learning_rate 0.000283577
2017-10-10T11:15:11.617978: step 805, loss 0.343236, acc 0.875, learning_rate 0.000282827
2017-10-10T11:15:11.776240: step 806, loss 0.130856, acc 0.953125, learning_rate 0.000282081
2017-10-10T11:15:11.940376: step 807, loss 0.13961, acc 0.9375, learning_rate 0.000281338
2017-10-10T11:15:12.102926: step 808, loss 0.14797, acc 0.9375, learning_rate 0.000280598
2017-10-10T11:15:12.266438: step 809, loss 0.229162, acc 0.921875, learning_rate 0.00027986
2017-10-10T11:15:12.428379: step 810, loss 0.207709, acc 0.90625, learning_rate 0.000279126
2017-10-10T11:15:12.585919: step 811, loss 0.138415, acc 0.90625, learning_rate 0.000278395
2017-10-10T11:15:12.748299: step 812, loss 0.130518, acc 0.9375, learning_rate 0.000277667
2017-10-10T11:15:12.916383: step 813, loss 0.226712, acc 0.9375, learning_rate 0.000276942
2017-10-10T11:15:13.081136: step 814, loss 0.169099, acc 0.953125, learning_rate 0.00027622
2017-10-10T11:15:13.242133: step 815, loss 0.0732061, acc 0.96875, learning_rate 0.0002755
2017-10-10T11:15:13.404947: step 816, loss 0.107646, acc 0.96875, learning_rate 0.000274784
2017-10-10T11:15:13.569424: step 817, loss 0.117278, acc 0.9375, learning_rate 0.000274071
2017-10-10T11:15:13.729654: step 818, loss 0.235372, acc 0.90625, learning_rate 0.00027336
2017-10-10T11:15:13.893464: step 819, loss 0.219344, acc 0.90625, learning_rate 0.000272652
2017-10-10T11:15:14.054937: step 820, loss 0.304297, acc 0.859375, learning_rate 0.000271948
2017-10-10T11:15:14.216717: step 821, loss 0.106141, acc 0.953125, learning_rate 0.000271246
2017-10-10T11:15:14.376710: step 822, loss 0.0874142, acc 0.96875, learning_rate 0.000270547
2017-10-10T11:15:14.537595: step 823, loss 0.166053, acc 0.921875, learning_rate 0.000269851
2017-10-10T11:15:14.698707: step 824, loss 0.16749, acc 0.96875, learning_rate 0.000269157
2017-10-10T11:15:14.863933: step 825, loss 0.139979, acc 0.9375, learning_rate 0.000268467
2017-10-10T11:15:15.026731: step 826, loss 0.195216, acc 0.921875, learning_rate 0.000267779
2017-10-10T11:15:15.188113: step 827, loss 0.208426, acc 0.90625, learning_rate 0.000267094
2017-10-10T11:15:15.349888: step 828, loss 0.100301, acc 0.96875, learning_rate 0.000266412
2017-10-10T11:15:15.512478: step 829, loss 0.0363555, acc 1, learning_rate 0.000265733
2017-10-10T11:15:15.675971: step 830, loss 0.211714, acc 0.90625, learning_rate 0.000265057
2017-10-10T11:15:15.838345: step 831, loss 0.388979, acc 0.890625, learning_rate 0.000264383
2017-10-10T11:15:16.001615: step 832, loss 0.0826355, acc 0.953125, learning_rate 0.000263712
2017-10-10T11:15:16.162673: step 833, loss 0.0480756, acc 0.96875, learning_rate 0.000263044
2017-10-10T11:15:16.328763: step 834, loss 0.123732, acc 0.9375, learning_rate 0.000262378
2017-10-10T11:15:16.489492: step 835, loss 0.0833135, acc 0.96875, learning_rate 0.000261715
2017-10-10T11:15:16.655284: step 836, loss 0.241667, acc 0.90625, learning_rate 0.000261055
2017-10-10T11:15:16.820880: step 837, loss 0.15051, acc 0.96875, learning_rate 0.000260398
2017-10-10T11:15:16.984288: step 838, loss 0.258676, acc 0.90625, learning_rate 0.000259743
2017-10-10T11:15:17.146056: step 839, loss 0.113852, acc 0.9375, learning_rate 0.000259091
2017-10-10T11:15:17.307251: step 840, loss 0.272599, acc 0.890625, learning_rate 0.000258442

Evaluation:
2017-10-10T11:15:17.745400: step 840, loss 0.2236, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-840

2017-10-10T11:15:18.456693: step 841, loss 0.195978, acc 0.90625, learning_rate 0.000257795
2017-10-10T11:15:18.616998: step 842, loss 0.217591, acc 0.96875, learning_rate 0.000257151
2017-10-10T11:15:18.780378: step 843, loss 0.307837, acc 0.90625, learning_rate 0.00025651
2017-10-10T11:15:18.944950: step 844, loss 0.138422, acc 0.953125, learning_rate 0.000255871
2017-10-10T11:15:19.107497: step 845, loss 0.120114, acc 0.953125, learning_rate 0.000255235
2017-10-10T11:15:19.268981: step 846, loss 0.131241, acc 0.953125, learning_rate 0.000254601
2017-10-10T11:15:19.431247: step 847, loss 0.0659863, acc 0.96875, learning_rate 0.00025397
2017-10-10T11:15:19.591745: step 848, loss 0.19511, acc 0.90625, learning_rate 0.000253341
2017-10-10T11:15:19.751731: step 849, loss 0.109401, acc 0.953125, learning_rate 0.000252716
2017-10-10T11:15:19.916160: step 850, loss 0.0976808, acc 0.96875, learning_rate 0.000252092
2017-10-10T11:15:20.077972: step 851, loss 0.179527, acc 0.953125, learning_rate 0.000251471
2017-10-10T11:15:20.238709: step 852, loss 0.253216, acc 0.9375, learning_rate 0.000250853
2017-10-10T11:15:20.400148: step 853, loss 0.206928, acc 0.96875, learning_rate 0.000250237
2017-10-10T11:15:20.559361: step 854, loss 0.0769398, acc 0.984375, learning_rate 0.000249624
2017-10-10T11:15:20.723132: step 855, loss 0.209225, acc 0.921875, learning_rate 0.000249013
2017-10-10T11:15:20.889441: step 856, loss 0.0759102, acc 0.96875, learning_rate 0.000248405
2017-10-10T11:15:21.051188: step 857, loss 0.184719, acc 0.953125, learning_rate 0.000247799
2017-10-10T11:15:21.211984: step 858, loss 0.126909, acc 0.953125, learning_rate 0.000247196
2017-10-10T11:15:21.373563: step 859, loss 0.0798656, acc 0.96875, learning_rate 0.000246595
2017-10-10T11:15:21.543144: step 860, loss 0.0538936, acc 0.984375, learning_rate 0.000245997
2017-10-10T11:15:21.706957: step 861, loss 0.077938, acc 0.96875, learning_rate 0.000245401
2017-10-10T11:15:21.871576: step 862, loss 0.410468, acc 0.875, learning_rate 0.000244808
2017-10-10T11:15:22.033061: step 863, loss 0.217296, acc 0.921875, learning_rate 0.000244216
2017-10-10T11:15:22.193886: step 864, loss 0.223341, acc 0.921875, learning_rate 0.000243628
2017-10-10T11:15:22.355947: step 865, loss 0.12114, acc 0.953125, learning_rate 0.000243042
2017-10-10T11:15:22.516745: step 866, loss 0.223953, acc 0.9375, learning_rate 0.000242458
2017-10-10T11:15:22.677457: step 867, loss 0.0998596, acc 0.984375, learning_rate 0.000241876
2017-10-10T11:15:22.836493: step 868, loss 0.0616194, acc 0.96875, learning_rate 0.000241297
2017-10-10T11:15:23.003107: step 869, loss 0.115932, acc 0.953125, learning_rate 0.00024072
2017-10-10T11:15:23.169618: step 870, loss 0.265702, acc 0.953125, learning_rate 0.000240146
2017-10-10T11:15:23.332243: step 871, loss 0.222225, acc 0.9375, learning_rate 0.000239574
2017-10-10T11:15:23.492998: step 872, loss 0.110618, acc 0.96875, learning_rate 0.000239004
2017-10-10T11:15:23.655053: step 873, loss 0.108946, acc 0.96875, learning_rate 0.000238437
2017-10-10T11:15:23.819218: step 874, loss 0.0526087, acc 0.984375, learning_rate 0.000237872
2017-10-10T11:15:23.982978: step 875, loss 0.106254, acc 0.96875, learning_rate 0.000237309
2017-10-10T11:15:24.145597: step 876, loss 0.084086, acc 0.96875, learning_rate 0.000236749
2017-10-10T11:15:24.307907: step 877, loss 0.130387, acc 0.953125, learning_rate 0.00023619
2017-10-10T11:15:24.466782: step 878, loss 0.120138, acc 0.953125, learning_rate 0.000235635
2017-10-10T11:15:24.627612: step 879, loss 0.193747, acc 0.921875, learning_rate 0.000235081
2017-10-10T11:15:24.791770: step 880, loss 0.184228, acc 0.96875, learning_rate 0.00023453

Evaluation:
2017-10-10T11:15:25.240563: step 880, loss 0.221026, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-880

2017-10-10T11:15:25.814743: step 881, loss 0.121858, acc 0.9375, learning_rate 0.00023398
2017-10-10T11:15:25.950733: step 882, loss 0.293159, acc 0.901961, learning_rate 0.000233434
2017-10-10T11:15:26.114463: step 883, loss 0.0968751, acc 0.953125, learning_rate 0.000232889
2017-10-10T11:15:26.276387: step 884, loss 0.279946, acc 0.90625, learning_rate 0.000232346
2017-10-10T11:15:26.437059: step 885, loss 0.116748, acc 0.953125, learning_rate 0.000231806
2017-10-10T11:15:26.599761: step 886, loss 0.0522131, acc 0.984375, learning_rate 0.000231268
2017-10-10T11:15:26.766611: step 887, loss 0.131993, acc 0.9375, learning_rate 0.000230732
2017-10-10T11:15:26.931758: step 888, loss 0.226297, acc 0.953125, learning_rate 0.000230199
2017-10-10T11:15:27.094975: step 889, loss 0.352718, acc 0.9375, learning_rate 0.000229667
2017-10-10T11:15:27.258399: step 890, loss 0.123515, acc 0.96875, learning_rate 0.000229138
2017-10-10T11:15:27.419763: step 891, loss 0.103444, acc 0.96875, learning_rate 0.000228611
2017-10-10T11:15:27.581648: step 892, loss 0.300775, acc 0.859375, learning_rate 0.000228086
2017-10-10T11:15:27.743134: step 893, loss 0.114337, acc 0.984375, learning_rate 0.000227563
2017-10-10T11:15:27.912765: step 894, loss 0.124736, acc 0.96875, learning_rate 0.000227043
2017-10-10T11:15:28.076911: step 895, loss 0.242519, acc 0.921875, learning_rate 0.000226524
2017-10-10T11:15:28.236006: step 896, loss 0.0321963, acc 0.984375, learning_rate 0.000226008
2017-10-10T11:15:28.396193: step 897, loss 0.0840573, acc 0.984375, learning_rate 0.000225493
2017-10-10T11:15:28.558173: step 898, loss 0.0620078, acc 0.984375, learning_rate 0.000224981
2017-10-10T11:15:28.723243: step 899, loss 0.154457, acc 0.953125, learning_rate 0.000224471
2017-10-10T11:15:28.886396: step 900, loss 0.204497, acc 0.90625, learning_rate 0.000223963
2017-10-10T11:15:29.045211: step 901, loss 0.0751042, acc 0.96875, learning_rate 0.000223457
2017-10-10T11:15:29.209031: step 902, loss 0.0985983, acc 0.96875, learning_rate 0.000222953
2017-10-10T11:15:29.372474: step 903, loss 0.119971, acc 0.953125, learning_rate 0.000222451
2017-10-10T11:15:29.534983: step 904, loss 0.061202, acc 1, learning_rate 0.000221951
2017-10-10T11:15:29.691786: step 905, loss 0.219485, acc 0.9375, learning_rate 0.000221453
2017-10-10T11:15:29.853117: step 906, loss 0.238285, acc 0.90625, learning_rate 0.000220958
2017-10-10T11:15:30.015052: step 907, loss 0.120505, acc 0.9375, learning_rate 0.000220464
2017-10-10T11:15:30.177797: step 908, loss 0.301163, acc 0.921875, learning_rate 0.000219972
2017-10-10T11:15:30.338351: step 909, loss 0.116429, acc 0.984375, learning_rate 0.000219483
2017-10-10T11:15:30.500404: step 910, loss 0.1886, acc 0.9375, learning_rate 0.000218995
2017-10-10T11:15:30.663663: step 911, loss 0.129165, acc 0.96875, learning_rate 0.000218509
2017-10-10T11:15:30.826966: step 912, loss 0.192341, acc 0.9375, learning_rate 0.000218025
2017-10-10T11:15:31.000073: step 913, loss 0.225841, acc 0.921875, learning_rate 0.000217544
2017-10-10T11:15:31.159272: step 914, loss 0.103756, acc 0.953125, learning_rate 0.000217064
2017-10-10T11:15:31.322305: step 915, loss 0.077414, acc 0.953125, learning_rate 0.000216586
2017-10-10T11:15:31.483371: step 916, loss 0.120938, acc 0.9375, learning_rate 0.00021611
2017-10-10T11:15:31.644297: step 917, loss 0.0442636, acc 0.984375, learning_rate 0.000215636
2017-10-10T11:15:31.806817: step 918, loss 0.0990151, acc 0.96875, learning_rate 0.000215164
2017-10-10T11:15:31.969057: step 919, loss 0.0473426, acc 0.984375, learning_rate 0.000214694
2017-10-10T11:15:32.130950: step 920, loss 0.179903, acc 0.90625, learning_rate 0.000214226

Evaluation:
2017-10-10T11:15:32.577008: step 920, loss 0.223464, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-920

2017-10-10T11:15:33.221050: step 921, loss 0.100745, acc 0.9375, learning_rate 0.00021376
2017-10-10T11:15:33.382150: step 922, loss 0.330447, acc 0.9375, learning_rate 0.000213295
2017-10-10T11:15:33.543995: step 923, loss 0.150957, acc 0.953125, learning_rate 0.000212833
2017-10-10T11:15:33.704651: step 924, loss 0.147309, acc 0.9375, learning_rate 0.000212372
2017-10-10T11:15:33.867509: step 925, loss 0.0343283, acc 1, learning_rate 0.000211914
2017-10-10T11:15:34.032231: step 926, loss 0.059203, acc 0.96875, learning_rate 0.000211457
2017-10-10T11:15:34.193630: step 927, loss 0.0728881, acc 0.953125, learning_rate 0.000211002
2017-10-10T11:15:34.357820: step 928, loss 0.151729, acc 0.96875, learning_rate 0.000210549
2017-10-10T11:15:34.519054: step 929, loss 0.10994, acc 0.96875, learning_rate 0.000210098
2017-10-10T11:15:34.678890: step 930, loss 0.120354, acc 0.9375, learning_rate 0.000209648
2017-10-10T11:15:34.841235: step 931, loss 0.225474, acc 0.90625, learning_rate 0.000209201
2017-10-10T11:15:35.003969: step 932, loss 0.0881029, acc 0.96875, learning_rate 0.000208755
2017-10-10T11:15:35.164252: step 933, loss 0.16424, acc 0.9375, learning_rate 0.000208311
2017-10-10T11:15:35.324168: step 934, loss 0.130042, acc 0.921875, learning_rate 0.000207869
2017-10-10T11:15:35.486216: step 935, loss 0.207311, acc 0.9375, learning_rate 0.000207429
2017-10-10T11:15:35.646689: step 936, loss 0.26297, acc 0.921875, learning_rate 0.00020699
2017-10-10T11:15:35.810541: step 937, loss 0.0309084, acc 0.984375, learning_rate 0.000206554
2017-10-10T11:15:35.972464: step 938, loss 0.113109, acc 0.96875, learning_rate 0.000206119
2017-10-10T11:15:36.134789: step 939, loss 0.254668, acc 0.90625, learning_rate 0.000205685
2017-10-10T11:15:36.297861: step 940, loss 0.0736748, acc 0.96875, learning_rate 0.000205254
2017-10-10T11:15:36.461266: step 941, loss 0.264978, acc 0.9375, learning_rate 0.000204824
2017-10-10T11:15:36.624925: step 942, loss 0.209192, acc 0.890625, learning_rate 0.000204397
2017-10-10T11:15:36.784829: step 943, loss 0.0824378, acc 0.984375, learning_rate 0.00020397
2017-10-10T11:15:36.966420: step 944, loss 0.13292, acc 0.96875, learning_rate 0.000203546
2017-10-10T11:15:37.122624: step 945, loss 0.19477, acc 0.9375, learning_rate 0.000203123
2017-10-10T11:15:37.284984: step 946, loss 0.298524, acc 0.90625, learning_rate 0.000202702
2017-10-10T11:15:37.443966: step 947, loss 0.225191, acc 0.890625, learning_rate 0.000202283
2017-10-10T11:15:37.603760: step 948, loss 0.0267234, acc 1, learning_rate 0.000201866
2017-10-10T11:15:37.765718: step 949, loss 0.0922838, acc 0.96875, learning_rate 0.00020145
2017-10-10T11:15:37.931232: step 950, loss 0.13248, acc 0.953125, learning_rate 0.000201036
2017-10-10T11:15:38.091890: step 951, loss 0.176644, acc 0.921875, learning_rate 0.000200623
2017-10-10T11:15:38.255376: step 952, loss 0.129555, acc 0.96875, learning_rate 0.000200213
2017-10-10T11:15:38.416374: step 953, loss 0.106639, acc 0.953125, learning_rate 0.000199804
2017-10-10T11:15:38.578108: step 954, loss 0.111673, acc 0.96875, learning_rate 0.000199396
2017-10-10T11:15:38.740751: step 955, loss 0.213334, acc 0.921875, learning_rate 0.000198991
2017-10-10T11:15:38.904112: step 956, loss 0.145875, acc 0.96875, learning_rate 0.000198587
2017-10-10T11:15:39.065649: step 957, loss 0.208364, acc 0.921875, learning_rate 0.000198184
2017-10-10T11:15:39.227235: step 958, loss 0.0918668, acc 0.953125, learning_rate 0.000197783
2017-10-10T11:15:39.387511: step 959, loss 0.169927, acc 0.921875, learning_rate 0.000197384
2017-10-10T11:15:39.546234: step 960, loss 0.296606, acc 0.921875, learning_rate 0.000196987

Evaluation:
2017-10-10T11:15:39.983009: step 960, loss 0.226572, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-960

2017-10-10T11:15:40.634938: step 961, loss 0.149476, acc 0.921875, learning_rate 0.000196591
2017-10-10T11:15:40.797982: step 962, loss 0.0687642, acc 0.96875, learning_rate 0.000196197
2017-10-10T11:15:40.960171: step 963, loss 0.123295, acc 0.953125, learning_rate 0.000195804
2017-10-10T11:15:41.123424: step 964, loss 0.195753, acc 0.921875, learning_rate 0.000195413
2017-10-10T11:15:41.285103: step 965, loss 0.0749761, acc 0.984375, learning_rate 0.000195023
2017-10-10T11:15:41.447918: step 966, loss 0.181531, acc 0.90625, learning_rate 0.000194636
2017-10-10T11:15:41.609126: step 967, loss 0.202888, acc 0.921875, learning_rate 0.000194249
2017-10-10T11:15:41.770553: step 968, loss 0.10107, acc 0.953125, learning_rate 0.000193865
2017-10-10T11:15:41.944045: step 969, loss 0.0704985, acc 0.953125, learning_rate 0.000193482
2017-10-10T11:15:42.106639: step 970, loss 0.177446, acc 0.953125, learning_rate 0.0001931
2017-10-10T11:15:42.268789: step 971, loss 0.0924042, acc 0.953125, learning_rate 0.00019272
2017-10-10T11:15:42.429783: step 972, loss 0.10101, acc 0.984375, learning_rate 0.000192341
2017-10-10T11:15:42.591424: step 973, loss 0.110987, acc 0.9375, learning_rate 0.000191965
2017-10-10T11:15:42.752493: step 974, loss 0.284417, acc 0.890625, learning_rate 0.000191589
2017-10-10T11:15:42.922795: step 975, loss 0.132776, acc 0.96875, learning_rate 0.000191215
2017-10-10T11:15:43.087355: step 976, loss 0.140426, acc 0.953125, learning_rate 0.000190843
2017-10-10T11:15:43.251750: step 977, loss 0.106572, acc 0.96875, learning_rate 0.000190472
2017-10-10T11:15:43.411448: step 978, loss 0.077591, acc 0.984375, learning_rate 0.000190103
2017-10-10T11:15:43.574166: step 979, loss 0.209046, acc 0.921875, learning_rate 0.000189735
2017-10-10T11:15:43.717882: step 980, loss 0.219954, acc 0.941176, learning_rate 0.000189369
2017-10-10T11:15:43.886162: step 981, loss 0.0715799, acc 0.96875, learning_rate 0.000189004
2017-10-10T11:15:44.050077: step 982, loss 0.224564, acc 0.9375, learning_rate 0.000188641
2017-10-10T11:15:44.213254: step 983, loss 0.0906181, acc 0.953125, learning_rate 0.000188279
2017-10-10T11:15:44.374272: step 984, loss 0.139966, acc 0.921875, learning_rate 0.000187919
2017-10-10T11:15:44.536186: step 985, loss 0.0723692, acc 1, learning_rate 0.00018756
2017-10-10T11:15:44.696235: step 986, loss 0.136705, acc 0.953125, learning_rate 0.000187202
2017-10-10T11:15:44.860906: step 987, loss 0.115234, acc 0.984375, learning_rate 0.000186846
2017-10-10T11:15:45.022594: step 988, loss 0.0576657, acc 0.984375, learning_rate 0.000186492
2017-10-10T11:15:45.186330: step 989, loss 0.171835, acc 0.953125, learning_rate 0.000186139
2017-10-10T11:15:45.346905: step 990, loss 0.069803, acc 0.953125, learning_rate 0.000185787
2017-10-10T11:15:45.509505: step 991, loss 0.217962, acc 0.90625, learning_rate 0.000185437
2017-10-10T11:15:45.671686: step 992, loss 0.163524, acc 0.953125, learning_rate 0.000185088
2017-10-10T11:15:45.831385: step 993, loss 0.0767891, acc 0.984375, learning_rate 0.000184741
2017-10-10T11:15:45.992163: step 994, loss 0.12501, acc 0.96875, learning_rate 0.000184395
2017-10-10T11:15:46.154407: step 995, loss 0.157378, acc 0.953125, learning_rate 0.000184051
2017-10-10T11:15:46.314696: step 996, loss 0.157145, acc 0.9375, learning_rate 0.000183708
2017-10-10T11:15:46.475913: step 997, loss 0.114807, acc 0.96875, learning_rate 0.000183366
2017-10-10T11:15:46.637490: step 998, loss 0.109402, acc 0.9375, learning_rate 0.000183026
2017-10-10T11:15:46.795997: step 999, loss 0.176849, acc 0.90625, learning_rate 0.000182687
2017-10-10T11:15:46.959272: step 1000, loss 0.102354, acc 0.96875, learning_rate 0.000182349

Evaluation:
2017-10-10T11:15:47.398574: step 1000, loss 0.220176, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1000

2017-10-10T11:15:48.110101: step 1001, loss 0.0757791, acc 0.96875, learning_rate 0.000182013
2017-10-10T11:15:48.273382: step 1002, loss 0.042614, acc 0.984375, learning_rate 0.000181678
2017-10-10T11:15:48.436531: step 1003, loss 0.151656, acc 0.9375, learning_rate 0.000181345
2017-10-10T11:15:48.597443: step 1004, loss 0.169326, acc 0.90625, learning_rate 0.000181013
2017-10-10T11:15:48.760391: step 1005, loss 0.143215, acc 0.9375, learning_rate 0.000180682
2017-10-10T11:15:48.928627: step 1006, loss 0.198872, acc 0.953125, learning_rate 0.000180353
2017-10-10T11:15:49.088377: step 1007, loss 0.176098, acc 0.9375, learning_rate 0.000180025
2017-10-10T11:15:49.251811: step 1008, loss 0.0730288, acc 0.96875, learning_rate 0.000179698
2017-10-10T11:15:49.414265: step 1009, loss 0.0994076, acc 0.9375, learning_rate 0.000179373
2017-10-10T11:15:49.578509: step 1010, loss 0.0831818, acc 0.96875, learning_rate 0.000179049
2017-10-10T11:15:49.744323: step 1011, loss 0.20496, acc 0.953125, learning_rate 0.000178726
2017-10-10T11:15:49.922609: step 1012, loss 0.0802045, acc 0.96875, learning_rate 0.000178405
2017-10-10T11:15:50.087943: step 1013, loss 0.200679, acc 0.9375, learning_rate 0.000178085
2017-10-10T11:15:50.249517: step 1014, loss 0.218387, acc 0.953125, learning_rate 0.000177766
2017-10-10T11:15:50.411705: step 1015, loss 0.199105, acc 0.9375, learning_rate 0.000177449
2017-10-10T11:15:50.576805: step 1016, loss 0.146437, acc 0.90625, learning_rate 0.000177133
2017-10-10T11:15:50.737437: step 1017, loss 0.14354, acc 0.921875, learning_rate 0.000176818
2017-10-10T11:15:50.902245: step 1018, loss 0.279215, acc 0.875, learning_rate 0.000176504
2017-10-10T11:15:51.065362: step 1019, loss 0.145439, acc 0.953125, learning_rate 0.000176192
2017-10-10T11:15:51.231088: step 1020, loss 0.0769848, acc 0.984375, learning_rate 0.000175881
2017-10-10T11:15:51.393151: step 1021, loss 0.157193, acc 0.921875, learning_rate 0.000175571
2017-10-10T11:15:51.556625: step 1022, loss 0.268357, acc 0.953125, learning_rate 0.000175263
2017-10-10T11:15:51.719339: step 1023, loss 0.142778, acc 0.9375, learning_rate 0.000174956
2017-10-10T11:15:51.881054: step 1024, loss 0.0837975, acc 0.96875, learning_rate 0.00017465
2017-10-10T11:15:52.047462: step 1025, loss 0.0932662, acc 0.96875, learning_rate 0.000174345
2017-10-10T11:15:52.208824: step 1026, loss 0.137226, acc 0.90625, learning_rate 0.000174042
2017-10-10T11:15:52.374787: step 1027, loss 0.151528, acc 0.9375, learning_rate 0.000173739
2017-10-10T11:15:52.535936: step 1028, loss 0.101745, acc 0.953125, learning_rate 0.000173438
2017-10-10T11:15:52.698415: step 1029, loss 0.111064, acc 0.9375, learning_rate 0.000173139
2017-10-10T11:15:52.861225: step 1030, loss 0.215947, acc 0.90625, learning_rate 0.00017284
2017-10-10T11:15:53.022959: step 1031, loss 0.214848, acc 0.90625, learning_rate 0.000172543
2017-10-10T11:15:53.185124: step 1032, loss 0.109185, acc 0.96875, learning_rate 0.000172247
2017-10-10T11:15:53.345476: step 1033, loss 0.16586, acc 0.9375, learning_rate 0.000171952
2017-10-10T11:15:53.505833: step 1034, loss 0.0933952, acc 0.953125, learning_rate 0.000171658
2017-10-10T11:15:53.669714: step 1035, loss 0.44619, acc 0.828125, learning_rate 0.000171366
2017-10-10T11:15:53.830570: step 1036, loss 0.215683, acc 0.96875, learning_rate 0.000171074
2017-10-10T11:15:53.997379: step 1037, loss 0.0882642, acc 0.984375, learning_rate 0.000170784
2017-10-10T11:15:54.157156: step 1038, loss 0.181058, acc 0.9375, learning_rate 0.000170495
2017-10-10T11:15:54.316385: step 1039, loss 0.23749, acc 0.890625, learning_rate 0.000170208
2017-10-10T11:15:54.477633: step 1040, loss 0.186115, acc 0.9375, learning_rate 0.000169921

Evaluation:
2017-10-10T11:15:54.921015: step 1040, loss 0.21917, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1040

2017-10-10T11:15:55.500914: step 1041, loss 0.0952925, acc 0.953125, learning_rate 0.000169636
2017-10-10T11:15:55.664412: step 1042, loss 0.142521, acc 0.953125, learning_rate 0.000169351
2017-10-10T11:15:55.828892: step 1043, loss 0.223461, acc 0.9375, learning_rate 0.000169068
2017-10-10T11:15:55.994817: step 1044, loss 0.268136, acc 0.90625, learning_rate 0.000168786
2017-10-10T11:15:56.158769: step 1045, loss 0.35407, acc 0.890625, learning_rate 0.000168506
2017-10-10T11:15:56.323341: step 1046, loss 0.166169, acc 0.9375, learning_rate 0.000168226
2017-10-10T11:15:56.488921: step 1047, loss 0.175744, acc 0.9375, learning_rate 0.000167947
2017-10-10T11:15:56.654388: step 1048, loss 0.105294, acc 0.96875, learning_rate 0.00016767
2017-10-10T11:15:56.813053: step 1049, loss 0.164806, acc 0.96875, learning_rate 0.000167394
2017-10-10T11:15:56.988248: step 1050, loss 0.142258, acc 0.96875, learning_rate 0.000167119
2017-10-10T11:15:57.149828: step 1051, loss 0.104842, acc 0.9375, learning_rate 0.000166845
2017-10-10T11:15:57.312187: step 1052, loss 0.227944, acc 0.9375, learning_rate 0.000166572
2017-10-10T11:15:57.474871: step 1053, loss 0.103402, acc 0.96875, learning_rate 0.0001663
2017-10-10T11:15:57.636726: step 1054, loss 0.078191, acc 0.96875, learning_rate 0.00016603
2017-10-10T11:15:57.799939: step 1055, loss 0.158883, acc 0.921875, learning_rate 0.00016576
2017-10-10T11:15:57.962257: step 1056, loss 0.0817567, acc 0.984375, learning_rate 0.000165492
2017-10-10T11:15:58.122653: step 1057, loss 0.105226, acc 0.953125, learning_rate 0.000165224
2017-10-10T11:15:58.284585: step 1058, loss 0.19263, acc 0.953125, learning_rate 0.000164958
2017-10-10T11:15:58.448469: step 1059, loss 0.118697, acc 0.953125, learning_rate 0.000164693
2017-10-10T11:15:58.609182: step 1060, loss 0.18007, acc 0.921875, learning_rate 0.000164429
2017-10-10T11:15:58.770385: step 1061, loss 0.113463, acc 0.9375, learning_rate 0.000164166
2017-10-10T11:15:58.931066: step 1062, loss 0.0720035, acc 0.984375, learning_rate 0.000163904
2017-10-10T11:15:59.095834: step 1063, loss 0.161284, acc 0.9375, learning_rate 0.000163643
2017-10-10T11:15:59.257477: step 1064, loss 0.0856782, acc 0.96875, learning_rate 0.000163383
2017-10-10T11:15:59.418079: step 1065, loss 0.136128, acc 0.984375, learning_rate 0.000163125
2017-10-10T11:15:59.578905: step 1066, loss 0.0931421, acc 0.96875, learning_rate 0.000162867
2017-10-10T11:15:59.740807: step 1067, loss 0.0382186, acc 1, learning_rate 0.00016261
2017-10-10T11:15:59.904495: step 1068, loss 0.0979367, acc 0.984375, learning_rate 0.000162355
2017-10-10T11:16:00.066541: step 1069, loss 0.0396506, acc 0.984375, learning_rate 0.0001621
2017-10-10T11:16:00.230257: step 1070, loss 0.118705, acc 0.953125, learning_rate 0.000161847
2017-10-10T11:16:00.391691: step 1071, loss 0.154093, acc 0.9375, learning_rate 0.000161594
2017-10-10T11:16:00.554405: step 1072, loss 0.254711, acc 0.859375, learning_rate 0.000161343
2017-10-10T11:16:00.719052: step 1073, loss 0.222858, acc 0.90625, learning_rate 0.000161093
2017-10-10T11:16:00.889083: step 1074, loss 0.144702, acc 0.953125, learning_rate 0.000160843
2017-10-10T11:16:01.052061: step 1075, loss 0.277161, acc 0.921875, learning_rate 0.000160595
2017-10-10T11:16:01.213148: step 1076, loss 0.0973004, acc 0.96875, learning_rate 0.000160348
2017-10-10T11:16:01.374236: step 1077, loss 0.115535, acc 0.96875, learning_rate 0.000160101
2017-10-10T11:16:01.510664: step 1078, loss 0.0877083, acc 0.960784, learning_rate 0.000159856
2017-10-10T11:16:01.674140: step 1079, loss 0.146546, acc 0.9375, learning_rate 0.000159612
2017-10-10T11:16:01.837345: step 1080, loss 0.116044, acc 0.9375, learning_rate 0.000159368

Evaluation:
2017-10-10T11:16:02.267864: step 1080, loss 0.219936, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1080

2017-10-10T11:16:02.906916: step 1081, loss 0.19424, acc 0.90625, learning_rate 0.000159126
2017-10-10T11:16:03.069690: step 1082, loss 0.158448, acc 0.96875, learning_rate 0.000158885
2017-10-10T11:16:03.231463: step 1083, loss 0.217039, acc 0.90625, learning_rate 0.000158644
2017-10-10T11:16:03.394524: step 1084, loss 0.118657, acc 0.96875, learning_rate 0.000158405
2017-10-10T11:16:03.555269: step 1085, loss 0.306609, acc 0.9375, learning_rate 0.000158167
2017-10-10T11:16:03.718005: step 1086, loss 0.0861881, acc 0.984375, learning_rate 0.000157929
2017-10-10T11:16:03.879311: step 1087, loss 0.145249, acc 0.90625, learning_rate 0.000157693
2017-10-10T11:16:04.039690: step 1088, loss 0.116731, acc 0.953125, learning_rate 0.000157457
2017-10-10T11:16:04.201697: step 1089, loss 0.066019, acc 0.96875, learning_rate 0.000157223
2017-10-10T11:16:04.361606: step 1090, loss 0.178613, acc 0.90625, learning_rate 0.000156989
2017-10-10T11:16:04.525512: step 1091, loss 0.0925448, acc 0.96875, learning_rate 0.000156757
2017-10-10T11:16:04.687919: step 1092, loss 0.146432, acc 0.953125, learning_rate 0.000156525
2017-10-10T11:16:04.851449: step 1093, loss 0.0828309, acc 0.9375, learning_rate 0.000156294
2017-10-10T11:16:05.014867: step 1094, loss 0.175954, acc 0.96875, learning_rate 0.000156064
2017-10-10T11:16:05.177148: step 1095, loss 0.075744, acc 0.96875, learning_rate 0.000155836
2017-10-10T11:16:05.337156: step 1096, loss 0.130806, acc 0.953125, learning_rate 0.000155608
2017-10-10T11:16:05.498739: step 1097, loss 0.150529, acc 0.953125, learning_rate 0.000155381
2017-10-10T11:16:05.661894: step 1098, loss 0.142855, acc 0.953125, learning_rate 0.000155155
2017-10-10T11:16:05.823911: step 1099, loss 0.132436, acc 0.953125, learning_rate 0.000154929
2017-10-10T11:16:05.997196: step 1100, loss 0.301663, acc 0.859375, learning_rate 0.000154705
2017-10-10T11:16:06.157709: step 1101, loss 0.129422, acc 0.953125, learning_rate 0.000154482
2017-10-10T11:16:06.320367: step 1102, loss 0.100616, acc 0.96875, learning_rate 0.00015426
2017-10-10T11:16:06.484872: step 1103, loss 0.388407, acc 0.859375, learning_rate 0.000154038
2017-10-10T11:16:06.647089: step 1104, loss 0.128497, acc 0.953125, learning_rate 0.000153818
2017-10-10T11:16:06.810562: step 1105, loss 0.207349, acc 0.953125, learning_rate 0.000153598
2017-10-10T11:16:06.982984: step 1106, loss 0.236315, acc 0.890625, learning_rate 0.000153379
2017-10-10T11:16:07.146386: step 1107, loss 0.172136, acc 0.90625, learning_rate 0.000153161
2017-10-10T11:16:07.307867: step 1108, loss 0.341611, acc 0.90625, learning_rate 0.000152944
2017-10-10T11:16:07.469727: step 1109, loss 0.0659379, acc 0.984375, learning_rate 0.000152728
2017-10-10T11:16:07.631438: step 1110, loss 0.068691, acc 0.96875, learning_rate 0.000152513
2017-10-10T11:16:07.795664: step 1111, loss 0.182404, acc 0.953125, learning_rate 0.000152299
2017-10-10T11:16:07.957621: step 1112, loss 0.227023, acc 0.859375, learning_rate 0.000152085
2017-10-10T11:16:08.123609: step 1113, loss 0.0699812, acc 0.984375, learning_rate 0.000151872
2017-10-10T11:16:08.284802: step 1114, loss 0.234737, acc 0.953125, learning_rate 0.000151661
2017-10-10T11:16:08.444554: step 1115, loss 0.123751, acc 0.96875, learning_rate 0.00015145
2017-10-10T11:16:08.605597: step 1116, loss 0.262113, acc 0.953125, learning_rate 0.00015124
2017-10-10T11:16:08.772310: step 1117, loss 0.191219, acc 0.953125, learning_rate 0.000151031
2017-10-10T11:16:08.934250: step 1118, loss 0.113726, acc 0.984375, learning_rate 0.000150822
2017-10-10T11:16:09.096636: step 1119, loss 0.265319, acc 0.90625, learning_rate 0.000150615
2017-10-10T11:16:09.259395: step 1120, loss 0.0639206, acc 0.96875, learning_rate 0.000150408

Evaluation:
2017-10-10T11:16:09.709624: step 1120, loss 0.221149, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1120

2017-10-10T11:16:10.426122: step 1121, loss 0.0949448, acc 0.953125, learning_rate 0.000150203
2017-10-10T11:16:10.591063: step 1122, loss 0.100975, acc 0.953125, learning_rate 0.000149998
2017-10-10T11:16:10.753077: step 1123, loss 0.049755, acc 1, learning_rate 0.000149794
2017-10-10T11:16:10.917089: step 1124, loss 0.23315, acc 0.953125, learning_rate 0.00014959
2017-10-10T11:16:11.078636: step 1125, loss 0.140682, acc 0.9375, learning_rate 0.000149388
2017-10-10T11:16:11.243682: step 1126, loss 0.176656, acc 0.96875, learning_rate 0.000149186
2017-10-10T11:16:11.403890: step 1127, loss 0.186964, acc 0.921875, learning_rate 0.000148986
2017-10-10T11:16:11.565388: step 1128, loss 0.0979984, acc 0.953125, learning_rate 0.000148786
2017-10-10T11:16:11.727397: step 1129, loss 0.193399, acc 0.9375, learning_rate 0.000148587
2017-10-10T11:16:11.891105: step 1130, loss 0.13134, acc 0.9375, learning_rate 0.000148388
2017-10-10T11:16:12.055281: step 1131, loss 0.236515, acc 0.90625, learning_rate 0.000148191
2017-10-10T11:16:12.215594: step 1132, loss 0.0983763, acc 0.953125, learning_rate 0.000147994
2017-10-10T11:16:12.376356: step 1133, loss 0.0654255, acc 0.96875, learning_rate 0.000147798
2017-10-10T11:16:12.536522: step 1134, loss 0.215825, acc 0.90625, learning_rate 0.000147603
2017-10-10T11:16:12.697023: step 1135, loss 0.0555607, acc 1, learning_rate 0.000147409
2017-10-10T11:16:12.861403: step 1136, loss 0.076791, acc 0.96875, learning_rate 0.000147215
2017-10-10T11:16:13.027055: step 1137, loss 0.143823, acc 0.9375, learning_rate 0.000147022
2017-10-10T11:16:13.189264: step 1138, loss 0.0891011, acc 0.96875, learning_rate 0.000146831
2017-10-10T11:16:13.352204: step 1139, loss 0.167727, acc 0.953125, learning_rate 0.000146639
2017-10-10T11:16:13.513289: step 1140, loss 0.128464, acc 0.9375, learning_rate 0.000146449
2017-10-10T11:16:13.673588: step 1141, loss 0.145628, acc 0.9375, learning_rate 0.000146259
2017-10-10T11:16:13.835110: step 1142, loss 0.106456, acc 0.96875, learning_rate 0.000146071
2017-10-10T11:16:13.999264: step 1143, loss 0.0481001, acc 0.984375, learning_rate 0.000145883
2017-10-10T11:16:14.162836: step 1144, loss 0.171667, acc 0.953125, learning_rate 0.000145695
2017-10-10T11:16:14.323983: step 1145, loss 0.449715, acc 0.890625, learning_rate 0.000145509
2017-10-10T11:16:14.487021: step 1146, loss 0.157599, acc 0.921875, learning_rate 0.000145323
2017-10-10T11:16:14.650038: step 1147, loss 0.15179, acc 0.953125, learning_rate 0.000145138
2017-10-10T11:16:14.811383: step 1148, loss 0.142055, acc 0.96875, learning_rate 0.000144954
2017-10-10T11:16:14.977115: step 1149, loss 0.14602, acc 0.953125, learning_rate 0.00014477
2017-10-10T11:16:15.140333: step 1150, loss 0.150859, acc 0.9375, learning_rate 0.000144588
2017-10-10T11:16:15.298823: step 1151, loss 0.146117, acc 0.96875, learning_rate 0.000144406
2017-10-10T11:16:15.464863: step 1152, loss 0.22165, acc 0.921875, learning_rate 0.000144224
2017-10-10T11:16:15.625662: step 1153, loss 0.101365, acc 0.96875, learning_rate 0.000144044
2017-10-10T11:16:15.787922: step 1154, loss 0.133198, acc 0.9375, learning_rate 0.000143864
2017-10-10T11:16:15.953736: step 1155, loss 0.0767361, acc 0.96875, learning_rate 0.000143685
2017-10-10T11:16:16.116127: step 1156, loss 0.216312, acc 0.9375, learning_rate 0.000143507
2017-10-10T11:16:16.279652: step 1157, loss 0.096369, acc 0.953125, learning_rate 0.000143329
2017-10-10T11:16:16.440166: step 1158, loss 0.0220834, acc 1, learning_rate 0.000143152
2017-10-10T11:16:16.599299: step 1159, loss 0.0372913, acc 1, learning_rate 0.000142976
2017-10-10T11:16:16.763453: step 1160, loss 0.10614, acc 0.953125, learning_rate 0.000142801

Evaluation:
2017-10-10T11:16:17.196627: step 1160, loss 0.219888, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1160

2017-10-10T11:16:17.769726: step 1161, loss 0.160029, acc 0.921875, learning_rate 0.000142626
2017-10-10T11:16:17.938193: step 1162, loss 0.0973113, acc 0.953125, learning_rate 0.000142452
2017-10-10T11:16:18.102176: step 1163, loss 0.110206, acc 0.9375, learning_rate 0.000142279
2017-10-10T11:16:18.262463: step 1164, loss 0.0595865, acc 0.984375, learning_rate 0.000142106
2017-10-10T11:16:18.426362: step 1165, loss 0.0722626, acc 0.96875, learning_rate 0.000141934
2017-10-10T11:16:18.588090: step 1166, loss 0.187961, acc 0.9375, learning_rate 0.000141763
2017-10-10T11:16:18.749671: step 1167, loss 0.197418, acc 0.9375, learning_rate 0.000141593
2017-10-10T11:16:18.913363: step 1168, loss 0.125087, acc 0.921875, learning_rate 0.000141423
2017-10-10T11:16:19.076354: step 1169, loss 0.0789378, acc 0.96875, learning_rate 0.000141254
2017-10-10T11:16:19.237662: step 1170, loss 0.111056, acc 0.9375, learning_rate 0.000141085
2017-10-10T11:16:19.405381: step 1171, loss 0.0189297, acc 1, learning_rate 0.000140918
2017-10-10T11:16:19.568672: step 1172, loss 0.225876, acc 0.90625, learning_rate 0.000140751
2017-10-10T11:16:19.731627: step 1173, loss 0.175535, acc 0.953125, learning_rate 0.000140584
2017-10-10T11:16:19.897119: step 1174, loss 0.272333, acc 0.90625, learning_rate 0.000140419
2017-10-10T11:16:20.056533: step 1175, loss 0.106028, acc 0.953125, learning_rate 0.000140254
2017-10-10T11:16:20.188996: step 1176, loss 0.13427, acc 0.960784, learning_rate 0.000140089
2017-10-10T11:16:20.347816: step 1177, loss 0.205143, acc 0.921875, learning_rate 0.000139926
2017-10-10T11:16:20.511690: step 1178, loss 0.0538343, acc 0.984375, learning_rate 0.000139763
2017-10-10T11:16:20.674364: step 1179, loss 0.0988735, acc 0.953125, learning_rate 0.0001396
2017-10-10T11:16:20.836343: step 1180, loss 0.198603, acc 0.9375, learning_rate 0.000139439
2017-10-10T11:16:21.000097: step 1181, loss 0.0618698, acc 0.984375, learning_rate 0.000139278
2017-10-10T11:16:21.164562: step 1182, loss 0.164254, acc 0.921875, learning_rate 0.000139118
2017-10-10T11:16:21.324844: step 1183, loss 0.0565773, acc 0.984375, learning_rate 0.000138958
2017-10-10T11:16:21.492300: step 1184, loss 0.113196, acc 0.9375, learning_rate 0.000138799
2017-10-10T11:16:21.653097: step 1185, loss 0.175807, acc 0.953125, learning_rate 0.00013864
2017-10-10T11:16:21.815047: step 1186, loss 0.0459175, acc 0.984375, learning_rate 0.000138483
2017-10-10T11:16:21.979058: step 1187, loss 0.179394, acc 0.921875, learning_rate 0.000138326
2017-10-10T11:16:22.142709: step 1188, loss 0.26461, acc 0.890625, learning_rate 0.000138169
2017-10-10T11:16:22.303294: step 1189, loss 0.317114, acc 0.890625, learning_rate 0.000138013
2017-10-10T11:16:22.463741: step 1190, loss 0.109148, acc 0.953125, learning_rate 0.000137858
2017-10-10T11:16:22.625603: step 1191, loss 0.0847944, acc 0.96875, learning_rate 0.000137704
2017-10-10T11:16:22.787602: step 1192, loss 0.136573, acc 0.953125, learning_rate 0.00013755
2017-10-10T11:16:22.944471: step 1193, loss 0.108841, acc 0.984375, learning_rate 0.000137397
2017-10-10T11:16:23.105109: step 1194, loss 0.27864, acc 0.921875, learning_rate 0.000137244
2017-10-10T11:16:23.266600: step 1195, loss 0.246308, acc 0.90625, learning_rate 0.000137092
2017-10-10T11:16:23.428149: step 1196, loss 0.194632, acc 0.9375, learning_rate 0.000136941
2017-10-10T11:16:23.591859: step 1197, loss 0.150468, acc 0.921875, learning_rate 0.00013679
2017-10-10T11:16:23.755629: step 1198, loss 0.133471, acc 0.953125, learning_rate 0.00013664
2017-10-10T11:16:23.922251: step 1199, loss 0.236725, acc 0.890625, learning_rate 0.00013649
2017-10-10T11:16:24.088189: step 1200, loss 0.185751, acc 0.953125, learning_rate 0.000136341

Evaluation:
2017-10-10T11:16:24.534339: step 1200, loss 0.220673, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1200

2017-10-10T11:16:25.174007: step 1201, loss 0.117482, acc 0.953125, learning_rate 0.000136193
2017-10-10T11:16:25.335882: step 1202, loss 0.171793, acc 0.9375, learning_rate 0.000136045
2017-10-10T11:16:25.497835: step 1203, loss 0.0841016, acc 0.984375, learning_rate 0.000135898
2017-10-10T11:16:25.655530: step 1204, loss 0.17874, acc 0.921875, learning_rate 0.000135751
2017-10-10T11:16:25.818444: step 1205, loss 0.209859, acc 0.890625, learning_rate 0.000135605
2017-10-10T11:16:25.999692: step 1206, loss 0.0581542, acc 0.984375, learning_rate 0.00013546
2017-10-10T11:16:26.165459: step 1207, loss 0.233609, acc 0.9375, learning_rate 0.000135315
2017-10-10T11:16:26.328412: step 1208, loss 0.0685485, acc 0.984375, learning_rate 0.000135171
2017-10-10T11:16:26.489046: step 1209, loss 0.142804, acc 0.9375, learning_rate 0.000135028
2017-10-10T11:16:26.651557: step 1210, loss 0.180065, acc 0.9375, learning_rate 0.000134885
2017-10-10T11:16:26.815590: step 1211, loss 0.184766, acc 0.9375, learning_rate 0.000134742
2017-10-10T11:16:26.977622: step 1212, loss 0.125276, acc 0.953125, learning_rate 0.0001346
2017-10-10T11:16:27.137767: step 1213, loss 0.0948311, acc 0.953125, learning_rate 0.000134459
2017-10-10T11:16:27.297781: step 1214, loss 0.210667, acc 0.890625, learning_rate 0.000134319
2017-10-10T11:16:27.458813: step 1215, loss 0.247588, acc 0.921875, learning_rate 0.000134178
2017-10-10T11:16:27.620914: step 1216, loss 0.0711435, acc 0.984375, learning_rate 0.000134039
2017-10-10T11:16:27.780089: step 1217, loss 0.0922439, acc 0.953125, learning_rate 0.0001339
2017-10-10T11:16:27.945860: step 1218, loss 0.207972, acc 0.921875, learning_rate 0.000133762
2017-10-10T11:16:28.107027: step 1219, loss 0.154114, acc 0.953125, learning_rate 0.000133624
2017-10-10T11:16:28.267211: step 1220, loss 0.11855, acc 0.96875, learning_rate 0.000133487
2017-10-10T11:16:28.430216: step 1221, loss 0.0990209, acc 0.96875, learning_rate 0.00013335
2017-10-10T11:16:28.590939: step 1222, loss 0.082907, acc 0.96875, learning_rate 0.000133214
2017-10-10T11:16:28.754184: step 1223, loss 0.0766207, acc 0.953125, learning_rate 0.000133078
2017-10-10T11:16:28.916246: step 1224, loss 0.0873558, acc 0.953125, learning_rate 0.000132943
2017-10-10T11:16:29.077719: step 1225, loss 0.123377, acc 0.953125, learning_rate 0.000132809
2017-10-10T11:16:29.240364: step 1226, loss 0.0400365, acc 0.984375, learning_rate 0.000132675
2017-10-10T11:16:29.401832: step 1227, loss 0.143565, acc 0.921875, learning_rate 0.000132541
2017-10-10T11:16:29.564678: step 1228, loss 0.0909852, acc 0.96875, learning_rate 0.000132409
2017-10-10T11:16:29.724357: step 1229, loss 0.0374793, acc 0.984375, learning_rate 0.000132276
2017-10-10T11:16:29.891578: step 1230, loss 0.161842, acc 0.90625, learning_rate 0.000132145
2017-10-10T11:16:30.054563: step 1231, loss 0.0867792, acc 0.953125, learning_rate 0.000132013
2017-10-10T11:16:30.214405: step 1232, loss 0.0680932, acc 0.984375, learning_rate 0.000131883
2017-10-10T11:16:30.374966: step 1233, loss 0.182674, acc 0.921875, learning_rate 0.000131753
2017-10-10T11:16:30.537320: step 1234, loss 0.0468902, acc 0.96875, learning_rate 0.000131623
2017-10-10T11:16:30.701635: step 1235, loss 0.12287, acc 0.96875, learning_rate 0.000131494
2017-10-10T11:16:30.864166: step 1236, loss 0.118905, acc 0.96875, learning_rate 0.000131365
2017-10-10T11:16:31.026451: step 1237, loss 0.192417, acc 0.921875, learning_rate 0.000131237
2017-10-10T11:16:31.184233: step 1238, loss 0.0667895, acc 0.984375, learning_rate 0.00013111
2017-10-10T11:16:31.346533: step 1239, loss 0.124587, acc 0.96875, learning_rate 0.000130983
2017-10-10T11:16:31.508200: step 1240, loss 0.0930399, acc 0.96875, learning_rate 0.000130856

Evaluation:
2017-10-10T11:16:31.952430: step 1240, loss 0.217305, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1240

2017-10-10T11:16:32.663641: step 1241, loss 0.107804, acc 0.953125, learning_rate 0.00013073
2017-10-10T11:16:32.829201: step 1242, loss 0.238059, acc 0.890625, learning_rate 0.000130605
2017-10-10T11:16:32.997608: step 1243, loss 0.0873617, acc 0.953125, learning_rate 0.00013048
2017-10-10T11:16:33.161545: step 1244, loss 0.145749, acc 0.96875, learning_rate 0.000130356
2017-10-10T11:16:33.325176: step 1245, loss 0.125538, acc 0.953125, learning_rate 0.000130232
2017-10-10T11:16:33.493674: step 1246, loss 0.129151, acc 0.921875, learning_rate 0.000130108
2017-10-10T11:16:33.651947: step 1247, loss 0.112808, acc 0.984375, learning_rate 0.000129985
2017-10-10T11:16:33.813756: step 1248, loss 0.221653, acc 0.921875, learning_rate 0.000129863
2017-10-10T11:16:33.978031: step 1249, loss 0.093875, acc 0.953125, learning_rate 0.000129741
2017-10-10T11:16:34.140565: step 1250, loss 0.0376123, acc 1, learning_rate 0.00012962
2017-10-10T11:16:34.305115: step 1251, loss 0.224023, acc 0.9375, learning_rate 0.000129499
2017-10-10T11:16:34.466972: step 1252, loss 0.172635, acc 0.921875, learning_rate 0.000129378
2017-10-10T11:16:34.626913: step 1253, loss 0.0477333, acc 0.984375, learning_rate 0.000129259
2017-10-10T11:16:34.786180: step 1254, loss 0.0299655, acc 1, learning_rate 0.000129139
2017-10-10T11:16:34.959298: step 1255, loss 0.0338101, acc 0.984375, learning_rate 0.00012902
2017-10-10T11:16:35.120941: step 1256, loss 0.20055, acc 0.921875, learning_rate 0.000128902
2017-10-10T11:16:35.284920: step 1257, loss 0.112731, acc 0.96875, learning_rate 0.000128784
2017-10-10T11:16:35.445575: step 1258, loss 0.0399472, acc 1, learning_rate 0.000128666
2017-10-10T11:16:35.607801: step 1259, loss 0.180539, acc 0.9375, learning_rate 0.000128549
2017-10-10T11:16:35.772854: step 1260, loss 0.156416, acc 0.96875, learning_rate 0.000128433
2017-10-10T11:16:35.942501: step 1261, loss 0.103434, acc 0.953125, learning_rate 0.000128317
2017-10-10T11:16:36.103047: step 1262, loss 0.0624221, acc 0.984375, learning_rate 0.000128201
2017-10-10T11:16:36.265808: step 1263, loss 0.140064, acc 0.9375, learning_rate 0.000128086
2017-10-10T11:16:36.425961: step 1264, loss 0.156593, acc 0.953125, learning_rate 0.000127971
2017-10-10T11:16:36.588017: step 1265, loss 0.0891114, acc 0.984375, learning_rate 0.000127857
2017-10-10T11:16:36.750573: step 1266, loss 0.297914, acc 0.90625, learning_rate 0.000127743
2017-10-10T11:16:36.914426: step 1267, loss 0.113074, acc 0.9375, learning_rate 0.00012763
2017-10-10T11:16:37.075445: step 1268, loss 0.146689, acc 0.9375, learning_rate 0.000127517
2017-10-10T11:16:37.237782: step 1269, loss 0.112749, acc 0.96875, learning_rate 0.000127405
2017-10-10T11:16:37.402030: step 1270, loss 0.190284, acc 0.921875, learning_rate 0.000127293
2017-10-10T11:16:37.564083: step 1271, loss 0.366183, acc 0.890625, learning_rate 0.000127182
2017-10-10T11:16:37.725090: step 1272, loss 0.0903283, acc 0.953125, learning_rate 0.000127071
2017-10-10T11:16:37.889018: step 1273, loss 0.0668536, acc 0.984375, learning_rate 0.00012696
2017-10-10T11:16:38.024862: step 1274, loss 0.185212, acc 0.941176, learning_rate 0.00012685
2017-10-10T11:16:38.187999: step 1275, loss 0.0670419, acc 0.96875, learning_rate 0.000126741
2017-10-10T11:16:38.351855: step 1276, loss 0.0685228, acc 0.96875, learning_rate 0.000126632
2017-10-10T11:16:38.511819: step 1277, loss 0.126388, acc 0.953125, learning_rate 0.000126523
2017-10-10T11:16:38.669081: step 1278, loss 0.115383, acc 0.953125, learning_rate 0.000126415
2017-10-10T11:16:38.831577: step 1279, loss 0.130696, acc 0.96875, learning_rate 0.000126307
2017-10-10T11:16:38.999927: step 1280, loss 0.0630501, acc 0.96875, learning_rate 0.000126199

Evaluation:
2017-10-10T11:16:39.443180: step 1280, loss 0.222181, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1280

2017-10-10T11:16:40.021282: step 1281, loss 0.132996, acc 0.953125, learning_rate 0.000126093
2017-10-10T11:16:40.186103: step 1282, loss 0.123256, acc 0.953125, learning_rate 0.000125986
2017-10-10T11:16:40.351080: step 1283, loss 0.185618, acc 0.953125, learning_rate 0.00012588
2017-10-10T11:16:40.512556: step 1284, loss 0.0495484, acc 0.984375, learning_rate 0.000125774
2017-10-10T11:16:40.677886: step 1285, loss 0.075163, acc 0.96875, learning_rate 0.000125669
2017-10-10T11:16:40.841338: step 1286, loss 0.157242, acc 0.9375, learning_rate 0.000125564
2017-10-10T11:16:41.008766: step 1287, loss 0.0610944, acc 0.96875, learning_rate 0.00012546
2017-10-10T11:16:41.169784: step 1288, loss 0.166025, acc 0.90625, learning_rate 0.000125356
2017-10-10T11:16:41.330629: step 1289, loss 0.162376, acc 0.953125, learning_rate 0.000125253
2017-10-10T11:16:41.494333: step 1290, loss 0.0895456, acc 0.984375, learning_rate 0.00012515
2017-10-10T11:16:41.657927: step 1291, loss 0.156768, acc 0.953125, learning_rate 0.000125047
2017-10-10T11:16:41.819327: step 1292, loss 0.209929, acc 0.9375, learning_rate 0.000124945
2017-10-10T11:16:41.986437: step 1293, loss 0.130047, acc 0.953125, learning_rate 0.000124843
2017-10-10T11:16:42.156106: step 1294, loss 0.171959, acc 0.953125, learning_rate 0.000124741
2017-10-10T11:16:42.316788: step 1295, loss 0.1705, acc 0.953125, learning_rate 0.00012464
2017-10-10T11:16:42.482084: step 1296, loss 0.171372, acc 0.9375, learning_rate 0.00012454
2017-10-10T11:16:42.645060: step 1297, loss 0.137157, acc 0.96875, learning_rate 0.00012444
2017-10-10T11:16:42.807480: step 1298, loss 0.175578, acc 0.953125, learning_rate 0.00012434
2017-10-10T11:16:42.972125: step 1299, loss 0.125653, acc 0.96875, learning_rate 0.000124241
2017-10-10T11:16:43.131533: step 1300, loss 0.0538265, acc 0.96875, learning_rate 0.000124142
2017-10-10T11:16:43.293378: step 1301, loss 0.0998238, acc 0.953125, learning_rate 0.000124043
2017-10-10T11:16:43.456972: step 1302, loss 0.0525929, acc 1, learning_rate 0.000123945
2017-10-10T11:16:43.616711: step 1303, loss 0.123482, acc 0.921875, learning_rate 0.000123847
2017-10-10T11:16:43.778436: step 1304, loss 0.176083, acc 0.9375, learning_rate 0.00012375
2017-10-10T11:16:43.941904: step 1305, loss 0.244309, acc 0.9375, learning_rate 0.000123653
2017-10-10T11:16:44.102213: step 1306, loss 0.146305, acc 0.9375, learning_rate 0.000123556
2017-10-10T11:16:44.262319: step 1307, loss 0.183727, acc 0.9375, learning_rate 0.00012346
2017-10-10T11:16:44.423510: step 1308, loss 0.0644783, acc 1, learning_rate 0.000123364
2017-10-10T11:16:44.586042: step 1309, loss 0.0348356, acc 0.984375, learning_rate 0.000123269
2017-10-10T11:16:44.747966: step 1310, loss 0.169505, acc 0.96875, learning_rate 0.000123174
2017-10-10T11:16:44.911004: step 1311, loss 0.197539, acc 0.9375, learning_rate 0.00012308
2017-10-10T11:16:45.072768: step 1312, loss 0.122519, acc 0.953125, learning_rate 0.000122985
2017-10-10T11:16:45.232384: step 1313, loss 0.194951, acc 0.921875, learning_rate 0.000122892
2017-10-10T11:16:45.394041: step 1314, loss 0.0737857, acc 0.953125, learning_rate 0.000122798
2017-10-10T11:16:45.556692: step 1315, loss 0.120535, acc 0.953125, learning_rate 0.000122705
2017-10-10T11:16:45.720585: step 1316, loss 0.117135, acc 0.96875, learning_rate 0.000122612
2017-10-10T11:16:45.884199: step 1317, loss 0.0802206, acc 0.953125, learning_rate 0.00012252
2017-10-10T11:16:46.046854: step 1318, loss 0.236026, acc 0.921875, learning_rate 0.000122428
2017-10-10T11:16:46.208872: step 1319, loss 0.283428, acc 0.90625, learning_rate 0.000122337
2017-10-10T11:16:46.371525: step 1320, loss 0.237362, acc 0.921875, learning_rate 0.000122245

Evaluation:
2017-10-10T11:16:46.810220: step 1320, loss 0.217216, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1320

2017-10-10T11:16:47.449289: step 1321, loss 0.0654247, acc 0.984375, learning_rate 0.000122155
2017-10-10T11:16:47.610061: step 1322, loss 0.262, acc 0.921875, learning_rate 0.000122064
2017-10-10T11:16:47.772460: step 1323, loss 0.0655466, acc 0.984375, learning_rate 0.000121974
2017-10-10T11:16:47.934985: step 1324, loss 0.32067, acc 0.875, learning_rate 0.000121884
2017-10-10T11:16:48.098099: step 1325, loss 0.157581, acc 0.921875, learning_rate 0.000121795
2017-10-10T11:16:48.257456: step 1326, loss 0.13809, acc 0.9375, learning_rate 0.000121706
2017-10-10T11:16:48.421409: step 1327, loss 0.0717274, acc 0.96875, learning_rate 0.000121618
2017-10-10T11:16:48.582986: step 1328, loss 0.163287, acc 0.96875, learning_rate 0.000121529
2017-10-10T11:16:48.746256: step 1329, loss 0.128885, acc 0.953125, learning_rate 0.000121441
2017-10-10T11:16:48.909971: step 1330, loss 0.0882931, acc 0.96875, learning_rate 0.000121354
2017-10-10T11:16:49.070855: step 1331, loss 0.118001, acc 0.96875, learning_rate 0.000121267
2017-10-10T11:16:49.235535: step 1332, loss 0.0586135, acc 0.984375, learning_rate 0.00012118
2017-10-10T11:16:49.398070: step 1333, loss 0.140417, acc 0.9375, learning_rate 0.000121093
2017-10-10T11:16:49.561441: step 1334, loss 0.0610548, acc 0.984375, learning_rate 0.000121007
2017-10-10T11:16:49.720276: step 1335, loss 0.200071, acc 0.90625, learning_rate 0.000120922
2017-10-10T11:16:49.884249: step 1336, loss 0.177173, acc 0.9375, learning_rate 0.000120836
2017-10-10T11:16:50.047108: step 1337, loss 0.0531847, acc 1, learning_rate 0.000120751
2017-10-10T11:16:50.206994: step 1338, loss 0.103134, acc 0.953125, learning_rate 0.000120666
2017-10-10T11:16:50.369920: step 1339, loss 0.09857, acc 0.96875, learning_rate 0.000120582
2017-10-10T11:16:50.531804: step 1340, loss 0.234555, acc 0.90625, learning_rate 0.000120498
2017-10-10T11:16:50.692791: step 1341, loss 0.0873504, acc 0.96875, learning_rate 0.000120414
2017-10-10T11:16:50.856320: step 1342, loss 0.105909, acc 0.953125, learning_rate 0.000120331
2017-10-10T11:16:51.018344: step 1343, loss 0.0458044, acc 0.984375, learning_rate 0.000120248
2017-10-10T11:16:51.180962: step 1344, loss 0.0523712, acc 0.984375, learning_rate 0.000120165
2017-10-10T11:16:51.341282: step 1345, loss 0.0978801, acc 0.96875, learning_rate 0.000120083
2017-10-10T11:16:51.501473: step 1346, loss 0.112908, acc 0.953125, learning_rate 0.000120001
2017-10-10T11:16:51.666789: step 1347, loss 0.106664, acc 0.96875, learning_rate 0.00011992
2017-10-10T11:16:51.827744: step 1348, loss 0.201754, acc 0.9375, learning_rate 0.000119838
2017-10-10T11:16:51.993696: step 1349, loss 0.225477, acc 0.9375, learning_rate 0.000119757
2017-10-10T11:16:52.155309: step 1350, loss 0.165413, acc 0.90625, learning_rate 0.000119677
2017-10-10T11:16:52.317647: step 1351, loss 0.220419, acc 0.90625, learning_rate 0.000119596
2017-10-10T11:16:52.478188: step 1352, loss 0.186913, acc 0.953125, learning_rate 0.000119516
2017-10-10T11:16:52.640126: step 1353, loss 0.109565, acc 0.96875, learning_rate 0.000119437
2017-10-10T11:16:52.801311: step 1354, loss 0.167316, acc 0.921875, learning_rate 0.000119357
2017-10-10T11:16:52.978001: step 1355, loss 0.143795, acc 0.9375, learning_rate 0.000119278
2017-10-10T11:16:53.139350: step 1356, loss 0.0836603, acc 0.984375, learning_rate 0.0001192
2017-10-10T11:16:53.305122: step 1357, loss 0.119533, acc 0.9375, learning_rate 0.000119121
2017-10-10T11:16:53.468382: step 1358, loss 0.103421, acc 0.96875, learning_rate 0.000119043
2017-10-10T11:16:53.629952: step 1359, loss 0.146286, acc 0.9375, learning_rate 0.000118965
2017-10-10T11:16:53.792689: step 1360, loss 0.1144, acc 0.9375, learning_rate 0.000118888

Evaluation:
2017-10-10T11:16:54.251057: step 1360, loss 0.219336, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1360

2017-10-10T11:16:54.890703: step 1361, loss 0.237442, acc 0.921875, learning_rate 0.000118811
2017-10-10T11:16:55.052687: step 1362, loss 0.147359, acc 0.96875, learning_rate 0.000118734
2017-10-10T11:16:55.215209: step 1363, loss 0.0756699, acc 0.984375, learning_rate 0.000118658
2017-10-10T11:16:55.377424: step 1364, loss 0.11193, acc 0.953125, learning_rate 0.000118582
2017-10-10T11:16:55.538857: step 1365, loss 0.101921, acc 0.984375, learning_rate 0.000118506
2017-10-10T11:16:55.701953: step 1366, loss 0.0855313, acc 0.984375, learning_rate 0.00011843
2017-10-10T11:16:55.867498: step 1367, loss 0.12638, acc 0.9375, learning_rate 0.000118355
2017-10-10T11:16:56.029730: step 1368, loss 0.151283, acc 0.9375, learning_rate 0.00011828
2017-10-10T11:16:56.190505: step 1369, loss 0.259658, acc 0.9375, learning_rate 0.000118205
2017-10-10T11:16:56.353283: step 1370, loss 0.134519, acc 0.953125, learning_rate 0.000118131
2017-10-10T11:16:56.518072: step 1371, loss 0.0750809, acc 0.96875, learning_rate 0.000118057
2017-10-10T11:16:56.650966: step 1372, loss 0.0617571, acc 0.980392, learning_rate 0.000117983
2017-10-10T11:16:56.815238: step 1373, loss 0.254462, acc 0.90625, learning_rate 0.00011791
2017-10-10T11:16:56.977149: step 1374, loss 0.0350039, acc 1, learning_rate 0.000117837
2017-10-10T11:16:57.140901: step 1375, loss 0.0733309, acc 0.984375, learning_rate 0.000117764
2017-10-10T11:16:57.306157: step 1376, loss 0.0695073, acc 0.984375, learning_rate 0.000117692
2017-10-10T11:16:57.466797: step 1377, loss 0.0640076, acc 0.984375, learning_rate 0.000117619
2017-10-10T11:16:57.627843: step 1378, loss 0.166785, acc 0.921875, learning_rate 0.000117547
2017-10-10T11:16:57.787270: step 1379, loss 0.1186, acc 0.953125, learning_rate 0.000117476
2017-10-10T11:16:57.961535: step 1380, loss 0.0681925, acc 0.984375, learning_rate 0.000117404
2017-10-10T11:16:58.124555: step 1381, loss 0.170255, acc 0.953125, learning_rate 0.000117333
2017-10-10T11:16:58.289560: step 1382, loss 0.20288, acc 0.953125, learning_rate 0.000117263
2017-10-10T11:16:58.453190: step 1383, loss 0.121246, acc 0.953125, learning_rate 0.000117192
2017-10-10T11:16:58.614790: step 1384, loss 0.0624036, acc 0.96875, learning_rate 0.000117122
2017-10-10T11:16:58.779060: step 1385, loss 0.11388, acc 0.953125, learning_rate 0.000117052
2017-10-10T11:16:58.941262: step 1386, loss 0.0647158, acc 0.96875, learning_rate 0.000116983
2017-10-10T11:16:59.098599: step 1387, loss 0.302068, acc 0.890625, learning_rate 0.000116913
2017-10-10T11:16:59.259325: step 1388, loss 0.0958464, acc 0.96875, learning_rate 0.000116844
2017-10-10T11:16:59.423879: step 1389, loss 0.0373425, acc 0.984375, learning_rate 0.000116775
2017-10-10T11:16:59.584436: step 1390, loss 0.271049, acc 0.875, learning_rate 0.000116707
2017-10-10T11:16:59.748948: step 1391, loss 0.129696, acc 0.953125, learning_rate 0.000116639
2017-10-10T11:16:59.912189: step 1392, loss 0.234656, acc 0.9375, learning_rate 0.000116571
2017-10-10T11:17:00.074871: step 1393, loss 0.102193, acc 0.96875, learning_rate 0.000116503
2017-10-10T11:17:00.242628: step 1394, loss 0.273409, acc 0.9375, learning_rate 0.000116436
2017-10-10T11:17:00.418978: step 1395, loss 0.0856748, acc 0.96875, learning_rate 0.000116369
2017-10-10T11:17:00.594542: step 1396, loss 0.0747462, acc 0.96875, learning_rate 0.000116302
2017-10-10T11:17:00.764201: step 1397, loss 0.0942534, acc 0.953125, learning_rate 0.000116235
2017-10-10T11:17:00.945491: step 1398, loss 0.296094, acc 0.921875, learning_rate 0.000116169
2017-10-10T11:17:01.125369: step 1399, loss 0.132476, acc 0.953125, learning_rate 0.000116103
2017-10-10T11:17:01.301356: step 1400, loss 0.139521, acc 0.9375, learning_rate 0.000116037

Evaluation:
2017-10-10T11:17:02.164704: step 1400, loss 0.218986, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1400

2017-10-10T11:17:02.952980: step 1401, loss 0.172227, acc 0.9375, learning_rate 0.000115972
2017-10-10T11:17:03.115905: step 1402, loss 0.0684336, acc 0.96875, learning_rate 0.000115907
2017-10-10T11:17:03.279012: step 1403, loss 0.119012, acc 0.953125, learning_rate 0.000115842
2017-10-10T11:17:03.441490: step 1404, loss 0.0996696, acc 0.96875, learning_rate 0.000115777
2017-10-10T11:17:03.603711: step 1405, loss 0.223859, acc 0.9375, learning_rate 0.000115713
2017-10-10T11:17:03.768488: step 1406, loss 0.107604, acc 0.96875, learning_rate 0.000115649
2017-10-10T11:17:03.946573: step 1407, loss 0.147265, acc 0.953125, learning_rate 0.000115585
2017-10-10T11:17:04.108062: step 1408, loss 0.164916, acc 0.96875, learning_rate 0.000115521
2017-10-10T11:17:04.271823: step 1409, loss 0.0722888, acc 0.96875, learning_rate 0.000115458
2017-10-10T11:17:04.431464: step 1410, loss 0.0754278, acc 0.96875, learning_rate 0.000115395
2017-10-10T11:17:04.594180: step 1411, loss 0.207847, acc 0.9375, learning_rate 0.000115332
2017-10-10T11:17:04.757734: step 1412, loss 0.135955, acc 0.9375, learning_rate 0.000115269
2017-10-10T11:17:04.919576: step 1413, loss 0.129969, acc 0.96875, learning_rate 0.000115207
2017-10-10T11:17:05.080730: step 1414, loss 0.126705, acc 0.96875, learning_rate 0.000115145
2017-10-10T11:17:05.241636: step 1415, loss 0.0373855, acc 1, learning_rate 0.000115083
2017-10-10T11:17:05.401867: step 1416, loss 0.10693, acc 0.9375, learning_rate 0.000115022
2017-10-10T11:17:05.564304: step 1417, loss 0.0906772, acc 0.96875, learning_rate 0.00011496
2017-10-10T11:17:05.726858: step 1418, loss 0.101738, acc 0.96875, learning_rate 0.000114899
2017-10-10T11:17:05.890791: step 1419, loss 0.204536, acc 0.921875, learning_rate 0.000114838
2017-10-10T11:17:06.053839: step 1420, loss 0.0869372, acc 0.953125, learning_rate 0.000114778
2017-10-10T11:17:06.219767: step 1421, loss 0.143381, acc 0.953125, learning_rate 0.000114717
2017-10-10T11:17:06.382102: step 1422, loss 0.114299, acc 0.953125, learning_rate 0.000114657
2017-10-10T11:17:06.544287: step 1423, loss 0.0482399, acc 0.984375, learning_rate 0.000114598
2017-10-10T11:17:06.709365: step 1424, loss 0.0516159, acc 1, learning_rate 0.000114538
2017-10-10T11:17:06.874829: step 1425, loss 0.208858, acc 0.921875, learning_rate 0.000114479
2017-10-10T11:17:07.037418: step 1426, loss 0.0737746, acc 0.96875, learning_rate 0.00011442
2017-10-10T11:17:07.199388: step 1427, loss 0.135434, acc 0.953125, learning_rate 0.000114361
2017-10-10T11:17:07.360789: step 1428, loss 0.0829987, acc 0.96875, learning_rate 0.000114302
2017-10-10T11:17:07.521190: step 1429, loss 0.095861, acc 0.953125, learning_rate 0.000114244
2017-10-10T11:17:07.684367: step 1430, loss 0.0991147, acc 0.984375, learning_rate 0.000114186
2017-10-10T11:17:07.848971: step 1431, loss 0.224907, acc 0.921875, learning_rate 0.000114128
2017-10-10T11:17:08.008159: step 1432, loss 0.0778096, acc 0.984375, learning_rate 0.00011407
2017-10-10T11:17:08.169345: step 1433, loss 0.213267, acc 0.875, learning_rate 0.000114013
2017-10-10T11:17:08.329278: step 1434, loss 0.154443, acc 0.953125, learning_rate 0.000113955
2017-10-10T11:17:08.492022: step 1435, loss 0.240972, acc 0.921875, learning_rate 0.000113898
2017-10-10T11:17:08.651313: step 1436, loss 0.0831185, acc 0.96875, learning_rate 0.000113842
2017-10-10T11:17:08.813428: step 1437, loss 0.202986, acc 0.9375, learning_rate 0.000113785
2017-10-10T11:17:08.978998: step 1438, loss 0.0918679, acc 0.96875, learning_rate 0.000113729
2017-10-10T11:17:09.138742: step 1439, loss 0.0976255, acc 0.96875, learning_rate 0.000113673
2017-10-10T11:17:09.298155: step 1440, loss 0.152815, acc 0.96875, learning_rate 0.000113617

Evaluation:
2017-10-10T11:17:09.744010: step 1440, loss 0.217374, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1440

2017-10-10T11:17:10.322143: step 1441, loss 0.164707, acc 0.953125, learning_rate 0.000113561
2017-10-10T11:17:10.480510: step 1442, loss 0.140795, acc 0.9375, learning_rate 0.000113506
2017-10-10T11:17:10.643536: step 1443, loss 0.130498, acc 0.9375, learning_rate 0.000113451
2017-10-10T11:17:10.804694: step 1444, loss 0.194726, acc 0.890625, learning_rate 0.000113396
2017-10-10T11:17:10.969521: step 1445, loss 0.0928198, acc 0.984375, learning_rate 0.000113341
2017-10-10T11:17:11.129450: step 1446, loss 0.103059, acc 0.953125, learning_rate 0.000113287
2017-10-10T11:17:11.291704: step 1447, loss 0.0887463, acc 0.953125, learning_rate 0.000113233
2017-10-10T11:17:11.453005: step 1448, loss 0.131855, acc 0.953125, learning_rate 0.000113179
2017-10-10T11:17:11.616284: step 1449, loss 0.170668, acc 0.953125, learning_rate 0.000113125
2017-10-10T11:17:11.779014: step 1450, loss 0.131195, acc 0.953125, learning_rate 0.000113071
2017-10-10T11:17:11.943268: step 1451, loss 0.0754053, acc 0.96875, learning_rate 0.000113018
2017-10-10T11:17:12.104519: step 1452, loss 0.122838, acc 0.953125, learning_rate 0.000112965
2017-10-10T11:17:12.266180: step 1453, loss 0.105003, acc 0.9375, learning_rate 0.000112912
2017-10-10T11:17:12.426089: step 1454, loss 0.089825, acc 0.96875, learning_rate 0.000112859
2017-10-10T11:17:12.586197: step 1455, loss 0.106969, acc 0.9375, learning_rate 0.000112807
2017-10-10T11:17:12.747037: step 1456, loss 0.095074, acc 0.96875, learning_rate 0.000112754
2017-10-10T11:17:12.909982: step 1457, loss 0.0870274, acc 0.96875, learning_rate 0.000112702
2017-10-10T11:17:13.068741: step 1458, loss 0.062252, acc 0.984375, learning_rate 0.000112651
2017-10-10T11:17:13.229749: step 1459, loss 0.282919, acc 0.921875, learning_rate 0.000112599
2017-10-10T11:17:13.389646: step 1460, loss 0.232314, acc 0.953125, learning_rate 0.000112547
2017-10-10T11:17:13.549222: step 1461, loss 0.23147, acc 0.921875, learning_rate 0.000112496
2017-10-10T11:17:13.714111: step 1462, loss 0.0504425, acc 1, learning_rate 0.000112445
2017-10-10T11:17:13.878812: step 1463, loss 0.189387, acc 0.921875, learning_rate 0.000112394
2017-10-10T11:17:14.041417: step 1464, loss 0.167116, acc 0.921875, learning_rate 0.000112344
2017-10-10T11:17:14.203968: step 1465, loss 0.138843, acc 0.953125, learning_rate 0.000112293
2017-10-10T11:17:14.367424: step 1466, loss 0.126817, acc 0.96875, learning_rate 0.000112243
2017-10-10T11:17:14.530284: step 1467, loss 0.111358, acc 0.984375, learning_rate 0.000112193
2017-10-10T11:17:14.693373: step 1468, loss 0.0165434, acc 1, learning_rate 0.000112144
2017-10-10T11:17:14.856827: step 1469, loss 0.12692, acc 0.953125, learning_rate 0.000112094
2017-10-10T11:17:14.990136: step 1470, loss 0.0609996, acc 0.980392, learning_rate 0.000112045
2017-10-10T11:17:15.155016: step 1471, loss 0.222032, acc 0.9375, learning_rate 0.000111995
2017-10-10T11:17:15.319107: step 1472, loss 0.131257, acc 0.9375, learning_rate 0.000111946
2017-10-10T11:17:15.482507: step 1473, loss 0.11817, acc 0.96875, learning_rate 0.000111898
2017-10-10T11:17:15.646107: step 1474, loss 0.147184, acc 0.953125, learning_rate 0.000111849
2017-10-10T11:17:15.814051: step 1475, loss 0.113018, acc 0.9375, learning_rate 0.000111801
2017-10-10T11:17:15.984655: step 1476, loss 0.12347, acc 0.96875, learning_rate 0.000111753
2017-10-10T11:17:16.146717: step 1477, loss 0.154931, acc 0.96875, learning_rate 0.000111705
2017-10-10T11:17:16.308158: step 1478, loss 0.10739, acc 0.953125, learning_rate 0.000111657
2017-10-10T11:17:16.468074: step 1479, loss 0.122776, acc 0.96875, learning_rate 0.000111609
2017-10-10T11:17:16.631066: step 1480, loss 0.159385, acc 0.921875, learning_rate 0.000111562

Evaluation:
2017-10-10T11:17:17.078975: step 1480, loss 0.215245, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1480

2017-10-10T11:17:17.718846: step 1481, loss 0.114326, acc 0.953125, learning_rate 0.000111515
2017-10-10T11:17:17.886076: step 1482, loss 0.133018, acc 0.9375, learning_rate 0.000111468
2017-10-10T11:17:18.045579: step 1483, loss 0.109008, acc 0.953125, learning_rate 0.000111421
2017-10-10T11:17:18.209147: step 1484, loss 0.123955, acc 0.953125, learning_rate 0.000111374
2017-10-10T11:17:18.372877: step 1485, loss 0.164839, acc 0.9375, learning_rate 0.000111328
2017-10-10T11:17:18.534928: step 1486, loss 0.121976, acc 0.984375, learning_rate 0.000111282
2017-10-10T11:17:18.696665: step 1487, loss 0.0933705, acc 0.953125, learning_rate 0.000111236
2017-10-10T11:17:18.862624: step 1488, loss 0.21804, acc 0.9375, learning_rate 0.00011119
2017-10-10T11:17:19.025366: step 1489, loss 0.133591, acc 0.921875, learning_rate 0.000111144
2017-10-10T11:17:19.185800: step 1490, loss 0.167625, acc 0.9375, learning_rate 0.000111099
2017-10-10T11:17:19.348375: step 1491, loss 0.159722, acc 0.9375, learning_rate 0.000111053
2017-10-10T11:17:19.511399: step 1492, loss 0.0498088, acc 1, learning_rate 0.000111008
2017-10-10T11:17:19.673711: step 1493, loss 0.135464, acc 0.953125, learning_rate 0.000110963
2017-10-10T11:17:19.834367: step 1494, loss 0.116523, acc 0.953125, learning_rate 0.000110918
2017-10-10T11:17:19.992429: step 1495, loss 0.151703, acc 0.9375, learning_rate 0.000110874
2017-10-10T11:17:20.154766: step 1496, loss 0.0950445, acc 0.96875, learning_rate 0.00011083
2017-10-10T11:17:20.315484: step 1497, loss 0.233218, acc 0.90625, learning_rate 0.000110785
2017-10-10T11:17:20.480065: step 1498, loss 0.119622, acc 0.953125, learning_rate 0.000110741
2017-10-10T11:17:20.643256: step 1499, loss 0.108654, acc 0.953125, learning_rate 0.000110697
2017-10-10T11:17:20.806526: step 1500, loss 0.245021, acc 0.921875, learning_rate 0.000110654
2017-10-10T11:17:20.968807: step 1501, loss 0.151185, acc 0.921875, learning_rate 0.00011061
2017-10-10T11:17:21.130989: step 1502, loss 0.13298, acc 0.953125, learning_rate 0.000110567
2017-10-10T11:17:21.295778: step 1503, loss 0.105527, acc 0.96875, learning_rate 0.000110524
2017-10-10T11:17:21.460683: step 1504, loss 0.193366, acc 0.90625, learning_rate 0.000110481
2017-10-10T11:17:21.622419: step 1505, loss 0.0702269, acc 0.96875, learning_rate 0.000110438
2017-10-10T11:17:21.784728: step 1506, loss 0.12141, acc 0.9375, learning_rate 0.000110396
2017-10-10T11:17:21.956149: step 1507, loss 0.0914297, acc 0.96875, learning_rate 0.000110353
2017-10-10T11:17:22.119943: step 1508, loss 0.0284401, acc 1, learning_rate 0.000110311
2017-10-10T11:17:22.282900: step 1509, loss 0.0798557, acc 0.984375, learning_rate 0.000110269
2017-10-10T11:17:22.443676: step 1510, loss 0.139624, acc 0.953125, learning_rate 0.000110227
2017-10-10T11:17:22.607178: step 1511, loss 0.139049, acc 0.96875, learning_rate 0.000110185
2017-10-10T11:17:22.769991: step 1512, loss 0.0882551, acc 0.96875, learning_rate 0.000110144
2017-10-10T11:17:22.935867: step 1513, loss 0.0820067, acc 0.953125, learning_rate 0.000110102
2017-10-10T11:17:23.097787: step 1514, loss 0.150087, acc 0.921875, learning_rate 0.000110061
2017-10-10T11:17:23.258737: step 1515, loss 0.185841, acc 0.953125, learning_rate 0.00011002
2017-10-10T11:17:23.421556: step 1516, loss 0.148107, acc 0.953125, learning_rate 0.000109979
2017-10-10T11:17:23.580193: step 1517, loss 0.204372, acc 0.953125, learning_rate 0.000109938
2017-10-10T11:17:23.741300: step 1518, loss 0.0494245, acc 0.984375, learning_rate 0.000109898
2017-10-10T11:17:23.915892: step 1519, loss 0.138274, acc 0.9375, learning_rate 0.000109857
2017-10-10T11:17:24.077205: step 1520, loss 0.146682, acc 0.921875, learning_rate 0.000109817

Evaluation:
2017-10-10T11:17:24.521990: step 1520, loss 0.217824, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1520

2017-10-10T11:17:25.233405: step 1521, loss 0.253256, acc 0.90625, learning_rate 0.000109777
2017-10-10T11:17:25.393858: step 1522, loss 0.080421, acc 0.953125, learning_rate 0.000109737
2017-10-10T11:17:25.556507: step 1523, loss 0.0492572, acc 1, learning_rate 0.000109697
2017-10-10T11:17:25.718389: step 1524, loss 0.213779, acc 0.953125, learning_rate 0.000109658
2017-10-10T11:17:25.882994: step 1525, loss 0.143365, acc 0.953125, learning_rate 0.000109618
2017-10-10T11:17:26.045344: step 1526, loss 0.0945259, acc 0.953125, learning_rate 0.000109579
2017-10-10T11:17:26.207444: step 1527, loss 0.271296, acc 0.9375, learning_rate 0.00010954
2017-10-10T11:17:26.371690: step 1528, loss 0.146731, acc 0.96875, learning_rate 0.000109501
2017-10-10T11:17:26.533629: step 1529, loss 0.161891, acc 0.921875, learning_rate 0.000109462
2017-10-10T11:17:26.693590: step 1530, loss 0.212606, acc 0.921875, learning_rate 0.000109424
2017-10-10T11:17:26.856783: step 1531, loss 0.0523324, acc 0.96875, learning_rate 0.000109385
2017-10-10T11:17:27.021286: step 1532, loss 0.207489, acc 0.9375, learning_rate 0.000109347
2017-10-10T11:17:27.183381: step 1533, loss 0.138699, acc 0.96875, learning_rate 0.000109309
2017-10-10T11:17:27.348614: step 1534, loss 0.269182, acc 0.921875, learning_rate 0.000109271
2017-10-10T11:17:27.510260: step 1535, loss 0.123883, acc 0.96875, learning_rate 0.000109233
2017-10-10T11:17:27.669027: step 1536, loss 0.245984, acc 0.9375, learning_rate 0.000109195
2017-10-10T11:17:27.830648: step 1537, loss 0.148652, acc 0.9375, learning_rate 0.000109158
2017-10-10T11:17:28.001142: step 1538, loss 0.0631556, acc 0.96875, learning_rate 0.00010912
2017-10-10T11:17:28.163393: step 1539, loss 0.171012, acc 0.921875, learning_rate 0.000109083
2017-10-10T11:17:28.323359: step 1540, loss 0.0896834, acc 0.96875, learning_rate 0.000109046
2017-10-10T11:17:28.485735: step 1541, loss 0.16104, acc 0.921875, learning_rate 0.000109009
2017-10-10T11:17:28.648714: step 1542, loss 0.108354, acc 0.96875, learning_rate 0.000108972
2017-10-10T11:17:28.809262: step 1543, loss 0.125242, acc 0.96875, learning_rate 0.000108936
2017-10-10T11:17:28.973669: step 1544, loss 0.0846713, acc 0.96875, learning_rate 0.000108899
2017-10-10T11:17:29.134752: step 1545, loss 0.181504, acc 0.96875, learning_rate 0.000108863
2017-10-10T11:17:29.297589: step 1546, loss 0.142574, acc 0.921875, learning_rate 0.000108827
2017-10-10T11:17:29.457759: step 1547, loss 0.215931, acc 0.9375, learning_rate 0.000108791
2017-10-10T11:17:29.618475: step 1548, loss 0.119359, acc 0.9375, learning_rate 0.000108755
2017-10-10T11:17:29.781913: step 1549, loss 0.100186, acc 0.96875, learning_rate 0.000108719
2017-10-10T11:17:29.950762: step 1550, loss 0.0466671, acc 1, learning_rate 0.000108683
2017-10-10T11:17:30.112092: step 1551, loss 0.0604901, acc 0.96875, learning_rate 0.000108648
2017-10-10T11:17:30.272714: step 1552, loss 0.0567493, acc 0.984375, learning_rate 0.000108613
2017-10-10T11:17:30.434718: step 1553, loss 0.163988, acc 0.921875, learning_rate 0.000108577
2017-10-10T11:17:30.596064: step 1554, loss 0.264317, acc 0.90625, learning_rate 0.000108542
2017-10-10T11:17:30.759718: step 1555, loss 0.0923975, acc 0.984375, learning_rate 0.000108508
2017-10-10T11:17:30.924616: step 1556, loss 0.0770548, acc 0.984375, learning_rate 0.000108473
2017-10-10T11:17:31.086880: step 1557, loss 0.0351544, acc 1, learning_rate 0.000108438
2017-10-10T11:17:31.249163: step 1558, loss 0.0769382, acc 0.96875, learning_rate 0.000108404
2017-10-10T11:17:31.410377: step 1559, loss 0.218717, acc 0.953125, learning_rate 0.00010837
2017-10-10T11:17:31.571937: step 1560, loss 0.0369707, acc 1, learning_rate 0.000108335

Evaluation:
2017-10-10T11:17:32.038612: step 1560, loss 0.216293, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1560

2017-10-10T11:17:32.617364: step 1561, loss 0.0390574, acc 0.984375, learning_rate 0.000108301
2017-10-10T11:17:32.780132: step 1562, loss 0.156618, acc 0.953125, learning_rate 0.000108267
2017-10-10T11:17:32.954512: step 1563, loss 0.126985, acc 0.9375, learning_rate 0.000108234
2017-10-10T11:17:33.117353: step 1564, loss 0.297817, acc 0.875, learning_rate 0.0001082
2017-10-10T11:17:33.281401: step 1565, loss 0.0903978, acc 0.9375, learning_rate 0.000108167
2017-10-10T11:17:33.445396: step 1566, loss 0.0945561, acc 0.953125, learning_rate 0.000108133
2017-10-10T11:17:33.607955: step 1567, loss 0.130891, acc 0.9375, learning_rate 0.0001081
2017-10-10T11:17:33.742982: step 1568, loss 0.078142, acc 0.960784, learning_rate 0.000108067
2017-10-10T11:17:33.913262: step 1569, loss 0.0434884, acc 1, learning_rate 0.000108034
2017-10-10T11:17:34.073605: step 1570, loss 0.134653, acc 0.9375, learning_rate 0.000108001
2017-10-10T11:17:34.238736: step 1571, loss 0.223071, acc 0.890625, learning_rate 0.000107969
2017-10-10T11:17:34.398547: step 1572, loss 0.103714, acc 0.96875, learning_rate 0.000107936
2017-10-10T11:17:34.564065: step 1573, loss 0.159754, acc 0.9375, learning_rate 0.000107904
2017-10-10T11:17:34.725501: step 1574, loss 0.0813488, acc 0.984375, learning_rate 0.000107871
2017-10-10T11:17:34.889743: step 1575, loss 0.138857, acc 0.96875, learning_rate 0.000107839
2017-10-10T11:17:35.053033: step 1576, loss 0.134565, acc 0.96875, learning_rate 0.000107807
2017-10-10T11:17:35.212408: step 1577, loss 0.125167, acc 0.953125, learning_rate 0.000107775
2017-10-10T11:17:35.373769: step 1578, loss 0.110663, acc 0.96875, learning_rate 0.000107744
2017-10-10T11:17:35.534818: step 1579, loss 0.106638, acc 0.96875, learning_rate 0.000107712
2017-10-10T11:17:35.698766: step 1580, loss 0.134976, acc 0.9375, learning_rate 0.000107681
2017-10-10T11:17:35.862419: step 1581, loss 0.0513912, acc 1, learning_rate 0.000107649
2017-10-10T11:17:36.022413: step 1582, loss 0.0856633, acc 0.96875, learning_rate 0.000107618
2017-10-10T11:17:36.182016: step 1583, loss 0.27353, acc 0.890625, learning_rate 0.000107587
2017-10-10T11:17:36.344739: step 1584, loss 0.103586, acc 0.96875, learning_rate 0.000107556
2017-10-10T11:17:36.504634: step 1585, loss 0.168325, acc 0.921875, learning_rate 0.000107525
2017-10-10T11:17:36.669058: step 1586, loss 0.0635921, acc 1, learning_rate 0.000107494
2017-10-10T11:17:36.831895: step 1587, loss 0.140346, acc 0.96875, learning_rate 0.000107464
2017-10-10T11:17:37.002790: step 1588, loss 0.135046, acc 0.953125, learning_rate 0.000107433
2017-10-10T11:17:37.166883: step 1589, loss 0.211549, acc 0.921875, learning_rate 0.000107403
2017-10-10T11:17:37.330651: step 1590, loss 0.0957335, acc 0.984375, learning_rate 0.000107373
2017-10-10T11:17:37.491242: step 1591, loss 0.159936, acc 0.9375, learning_rate 0.000107343
2017-10-10T11:17:37.653034: step 1592, loss 0.227372, acc 0.921875, learning_rate 0.000107313
2017-10-10T11:17:37.813999: step 1593, loss 0.0979176, acc 0.953125, learning_rate 0.000107283
2017-10-10T11:17:37.981474: step 1594, loss 0.0991799, acc 0.953125, learning_rate 0.000107253
2017-10-10T11:17:38.141419: step 1595, loss 0.0358021, acc 1, learning_rate 0.000107224
2017-10-10T11:17:38.305619: step 1596, loss 0.137909, acc 0.90625, learning_rate 0.000107194
2017-10-10T11:17:38.468839: step 1597, loss 0.0577995, acc 0.984375, learning_rate 0.000107165
2017-10-10T11:17:38.632514: step 1598, loss 0.267496, acc 0.9375, learning_rate 0.000107136
2017-10-10T11:17:38.796662: step 1599, loss 0.220175, acc 0.9375, learning_rate 0.000107106
2017-10-10T11:17:38.959019: step 1600, loss 0.229776, acc 0.9375, learning_rate 0.000107077

Evaluation:
2017-10-10T11:17:39.412988: step 1600, loss 0.214632, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1600

2017-10-10T11:17:40.051798: step 1601, loss 0.234087, acc 0.875, learning_rate 0.000107048
2017-10-10T11:17:40.213211: step 1602, loss 0.141903, acc 0.953125, learning_rate 0.00010702
2017-10-10T11:17:40.374383: step 1603, loss 0.121836, acc 0.953125, learning_rate 0.000106991
2017-10-10T11:17:40.537453: step 1604, loss 0.219379, acc 0.921875, learning_rate 0.000106963
2017-10-10T11:17:40.696877: step 1605, loss 0.0916723, acc 0.96875, learning_rate 0.000106934
2017-10-10T11:17:40.860613: step 1606, loss 0.0718479, acc 0.96875, learning_rate 0.000106906
2017-10-10T11:17:41.022063: step 1607, loss 0.275646, acc 0.90625, learning_rate 0.000106878
2017-10-10T11:17:41.184623: step 1608, loss 0.125839, acc 0.953125, learning_rate 0.00010685
2017-10-10T11:17:41.348400: step 1609, loss 0.14568, acc 0.96875, learning_rate 0.000106822
2017-10-10T11:17:41.510001: step 1610, loss 0.113295, acc 0.984375, learning_rate 0.000106794
2017-10-10T11:17:41.673405: step 1611, loss 0.119693, acc 0.984375, learning_rate 0.000106766
2017-10-10T11:17:41.831610: step 1612, loss 0.192772, acc 0.953125, learning_rate 0.000106738
2017-10-10T11:17:41.995473: step 1613, loss 0.100801, acc 0.96875, learning_rate 0.000106711
2017-10-10T11:17:42.158433: step 1614, loss 0.119059, acc 0.953125, learning_rate 0.000106684
2017-10-10T11:17:42.320833: step 1615, loss 0.141111, acc 0.96875, learning_rate 0.000106656
2017-10-10T11:17:42.480521: step 1616, loss 0.111018, acc 0.984375, learning_rate 0.000106629
2017-10-10T11:17:42.642171: step 1617, loss 0.092545, acc 0.96875, learning_rate 0.000106602
2017-10-10T11:17:42.804358: step 1618, loss 0.143746, acc 0.921875, learning_rate 0.000106575
2017-10-10T11:17:42.970273: step 1619, loss 0.0682343, acc 0.96875, learning_rate 0.000106548
2017-10-10T11:17:43.133709: step 1620, loss 0.233453, acc 0.9375, learning_rate 0.000106521
2017-10-10T11:17:43.297046: step 1621, loss 0.186093, acc 0.96875, learning_rate 0.000106495
2017-10-10T11:17:43.456920: step 1622, loss 0.313855, acc 0.921875, learning_rate 0.000106468
2017-10-10T11:17:43.617807: step 1623, loss 0.148743, acc 0.953125, learning_rate 0.000106442
2017-10-10T11:17:43.781857: step 1624, loss 0.0792964, acc 0.96875, learning_rate 0.000106416
2017-10-10T11:17:43.944976: step 1625, loss 0.115407, acc 0.953125, learning_rate 0.000106389
2017-10-10T11:17:44.106787: step 1626, loss 0.0806315, acc 0.96875, learning_rate 0.000106363
2017-10-10T11:17:44.270418: step 1627, loss 0.170494, acc 0.921875, learning_rate 0.000106337
2017-10-10T11:17:44.430440: step 1628, loss 0.230717, acc 0.921875, learning_rate 0.000106312
2017-10-10T11:17:44.595400: step 1629, loss 0.134684, acc 0.953125, learning_rate 0.000106286
2017-10-10T11:17:44.760747: step 1630, loss 0.125006, acc 0.96875, learning_rate 0.00010626
2017-10-10T11:17:44.932465: step 1631, loss 0.26134, acc 0.921875, learning_rate 0.000106235
2017-10-10T11:17:45.093313: step 1632, loss 0.161712, acc 0.921875, learning_rate 0.000106209
2017-10-10T11:17:45.251968: step 1633, loss 0.144336, acc 0.953125, learning_rate 0.000106184
2017-10-10T11:17:45.417782: step 1634, loss 0.068295, acc 0.96875, learning_rate 0.000106159
2017-10-10T11:17:45.579964: step 1635, loss 0.100053, acc 0.953125, learning_rate 0.000106133
2017-10-10T11:17:45.743501: step 1636, loss 0.163788, acc 0.953125, learning_rate 0.000106108
2017-10-10T11:17:45.908157: step 1637, loss 0.137863, acc 0.953125, learning_rate 0.000106083
2017-10-10T11:17:46.069803: step 1638, loss 0.228161, acc 0.90625, learning_rate 0.000106059
2017-10-10T11:17:46.231690: step 1639, loss 0.130636, acc 0.9375, learning_rate 0.000106034
2017-10-10T11:17:46.392519: step 1640, loss 0.0770487, acc 0.984375, learning_rate 0.000106009

Evaluation:
2017-10-10T11:17:46.842600: step 1640, loss 0.212293, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1640

2017-10-10T11:17:47.554106: step 1641, loss 0.142061, acc 0.9375, learning_rate 0.000105985
2017-10-10T11:17:47.712536: step 1642, loss 0.147648, acc 0.953125, learning_rate 0.00010596
2017-10-10T11:17:47.876939: step 1643, loss 0.197385, acc 0.921875, learning_rate 0.000105936
2017-10-10T11:17:48.040555: step 1644, loss 0.13735, acc 0.9375, learning_rate 0.000105912
2017-10-10T11:17:48.200056: step 1645, loss 0.155445, acc 0.9375, learning_rate 0.000105888
2017-10-10T11:17:48.359623: step 1646, loss 0.136768, acc 0.953125, learning_rate 0.000105864
2017-10-10T11:17:48.520256: step 1647, loss 0.0272576, acc 0.984375, learning_rate 0.00010584
2017-10-10T11:17:48.681287: step 1648, loss 0.203649, acc 0.921875, learning_rate 0.000105816
2017-10-10T11:17:48.843672: step 1649, loss 0.185859, acc 0.921875, learning_rate 0.000105792
2017-10-10T11:17:49.006895: step 1650, loss 0.134317, acc 0.953125, learning_rate 0.000105768
2017-10-10T11:17:49.169980: step 1651, loss 0.0451158, acc 0.984375, learning_rate 0.000105745
2017-10-10T11:17:49.332488: step 1652, loss 0.153828, acc 0.921875, learning_rate 0.000105721
2017-10-10T11:17:49.498121: step 1653, loss 0.0867662, acc 0.953125, learning_rate 0.000105698
2017-10-10T11:17:49.658790: step 1654, loss 0.128332, acc 0.96875, learning_rate 0.000105675
2017-10-10T11:17:49.819783: step 1655, loss 0.184182, acc 0.90625, learning_rate 0.000105652
2017-10-10T11:17:49.984820: step 1656, loss 0.0800752, acc 0.984375, learning_rate 0.000105629
2017-10-10T11:17:50.147564: step 1657, loss 0.0834535, acc 0.953125, learning_rate 0.000105606
2017-10-10T11:17:50.312191: step 1658, loss 0.081964, acc 0.984375, learning_rate 0.000105583
2017-10-10T11:17:50.474560: step 1659, loss 0.0790875, acc 0.953125, learning_rate 0.00010556
2017-10-10T11:17:50.638775: step 1660, loss 0.0937767, acc 0.953125, learning_rate 0.000105537
2017-10-10T11:17:50.804359: step 1661, loss 0.0669123, acc 0.96875, learning_rate 0.000105515
2017-10-10T11:17:50.981696: step 1662, loss 0.0480844, acc 0.96875, learning_rate 0.000105492
2017-10-10T11:17:51.147769: step 1663, loss 0.133877, acc 0.953125, learning_rate 0.00010547
2017-10-10T11:17:51.310584: step 1664, loss 0.1556, acc 0.96875, learning_rate 0.000105447
2017-10-10T11:17:51.472574: step 1665, loss 0.0424212, acc 0.984375, learning_rate 0.000105425
2017-10-10T11:17:51.609797: step 1666, loss 0.107968, acc 0.960784, learning_rate 0.000105403
2017-10-10T11:17:51.774746: step 1667, loss 0.134841, acc 0.953125, learning_rate 0.000105381
2017-10-10T11:17:51.939778: step 1668, loss 0.38395, acc 0.890625, learning_rate 0.000105359
2017-10-10T11:17:52.100867: step 1669, loss 0.113453, acc 0.984375, learning_rate 0.000105337
2017-10-10T11:17:52.262863: step 1670, loss 0.0771789, acc 0.96875, learning_rate 0.000105315
2017-10-10T11:17:52.423776: step 1671, loss 0.064886, acc 0.96875, learning_rate 0.000105294
2017-10-10T11:17:52.589289: step 1672, loss 0.128677, acc 0.96875, learning_rate 0.000105272
2017-10-10T11:17:52.750814: step 1673, loss 0.088556, acc 0.953125, learning_rate 0.000105251
2017-10-10T11:17:52.925118: step 1674, loss 0.231001, acc 0.921875, learning_rate 0.000105229
2017-10-10T11:17:53.087864: step 1675, loss 0.0821298, acc 0.96875, learning_rate 0.000105208
2017-10-10T11:17:53.248995: step 1676, loss 0.125618, acc 0.953125, learning_rate 0.000105186
2017-10-10T11:17:53.411865: step 1677, loss 0.091952, acc 0.984375, learning_rate 0.000105165
2017-10-10T11:17:53.575374: step 1678, loss 0.117828, acc 0.984375, learning_rate 0.000105144
2017-10-10T11:17:53.742177: step 1679, loss 0.0434792, acc 1, learning_rate 0.000105123
2017-10-10T11:17:53.904181: step 1680, loss 0.176177, acc 0.9375, learning_rate 0.000105102

Evaluation:
2017-10-10T11:17:54.356657: step 1680, loss 0.217024, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1680

2017-10-10T11:17:54.938417: step 1681, loss 0.210049, acc 0.9375, learning_rate 0.000105081
2017-10-10T11:17:55.099814: step 1682, loss 0.0645512, acc 0.984375, learning_rate 0.000105061
2017-10-10T11:17:55.259972: step 1683, loss 0.0917587, acc 0.953125, learning_rate 0.00010504
2017-10-10T11:17:55.418658: step 1684, loss 0.211576, acc 0.921875, learning_rate 0.00010502
2017-10-10T11:17:55.581021: step 1685, loss 0.167398, acc 0.9375, learning_rate 0.000104999
2017-10-10T11:17:55.743595: step 1686, loss 0.0819811, acc 0.984375, learning_rate 0.000104979
2017-10-10T11:17:55.907494: step 1687, loss 0.0985436, acc 0.984375, learning_rate 0.000104958
2017-10-10T11:17:56.067952: step 1688, loss 0.196459, acc 0.9375, learning_rate 0.000104938
2017-10-10T11:17:56.228908: step 1689, loss 0.158203, acc 0.953125, learning_rate 0.000104918
2017-10-10T11:17:56.391975: step 1690, loss 0.170544, acc 0.953125, learning_rate 0.000104898
2017-10-10T11:17:56.554117: step 1691, loss 0.0500027, acc 0.984375, learning_rate 0.000104878
2017-10-10T11:17:56.718935: step 1692, loss 0.106515, acc 0.953125, learning_rate 0.000104858
2017-10-10T11:17:56.887014: step 1693, loss 0.208097, acc 0.953125, learning_rate 0.000104838
2017-10-10T11:17:57.052250: step 1694, loss 0.0726895, acc 0.96875, learning_rate 0.000104818
2017-10-10T11:17:57.217316: step 1695, loss 0.173298, acc 0.921875, learning_rate 0.000104799
2017-10-10T11:17:57.380078: step 1696, loss 0.145657, acc 0.9375, learning_rate 0.000104779
2017-10-10T11:17:57.542678: step 1697, loss 0.0882984, acc 0.953125, learning_rate 0.00010476
2017-10-10T11:17:57.704475: step 1698, loss 0.122798, acc 0.953125, learning_rate 0.00010474
2017-10-10T11:17:57.867794: step 1699, loss 0.0147139, acc 1, learning_rate 0.000104721
2017-10-10T11:17:58.030546: step 1700, loss 0.101315, acc 0.96875, learning_rate 0.000104702
2017-10-10T11:17:58.195704: step 1701, loss 0.159225, acc 0.953125, learning_rate 0.000104682
2017-10-10T11:17:58.358101: step 1702, loss 0.121869, acc 0.96875, learning_rate 0.000104663
2017-10-10T11:17:58.519348: step 1703, loss 0.0385593, acc 1, learning_rate 0.000104644
2017-10-10T11:17:58.678951: step 1704, loss 0.121915, acc 0.953125, learning_rate 0.000104625
2017-10-10T11:17:58.839649: step 1705, loss 0.164931, acc 0.953125, learning_rate 0.000104606
2017-10-10T11:17:59.000974: step 1706, loss 0.0894009, acc 0.953125, learning_rate 0.000104588
2017-10-10T11:17:59.161664: step 1707, loss 0.141141, acc 0.9375, learning_rate 0.000104569
2017-10-10T11:17:59.321647: step 1708, loss 0.116369, acc 0.9375, learning_rate 0.00010455
2017-10-10T11:17:59.484826: step 1709, loss 0.0928646, acc 0.984375, learning_rate 0.000104532
2017-10-10T11:17:59.648753: step 1710, loss 0.112839, acc 0.953125, learning_rate 0.000104513
2017-10-10T11:17:59.811093: step 1711, loss 0.0860861, acc 0.984375, learning_rate 0.000104495
2017-10-10T11:17:59.972439: step 1712, loss 0.103566, acc 0.984375, learning_rate 0.000104476
2017-10-10T11:18:00.137274: step 1713, loss 0.128711, acc 0.9375, learning_rate 0.000104458
2017-10-10T11:18:00.301771: step 1714, loss 0.104726, acc 0.953125, learning_rate 0.00010444
2017-10-10T11:18:00.462671: step 1715, loss 0.133836, acc 0.953125, learning_rate 0.000104422
2017-10-10T11:18:00.624575: step 1716, loss 0.0470171, acc 0.96875, learning_rate 0.000104404
2017-10-10T11:18:00.784308: step 1717, loss 0.171998, acc 0.9375, learning_rate 0.000104386
2017-10-10T11:18:00.945612: step 1718, loss 0.185682, acc 0.9375, learning_rate 0.000104368
2017-10-10T11:18:01.108372: step 1719, loss 0.207211, acc 0.90625, learning_rate 0.00010435
2017-10-10T11:18:01.271483: step 1720, loss 0.0939145, acc 0.96875, learning_rate 0.000104332

Evaluation:
2017-10-10T11:18:01.706431: step 1720, loss 0.223612, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1720

2017-10-10T11:18:02.381956: step 1721, loss 0.232288, acc 0.921875, learning_rate 0.000104315
2017-10-10T11:18:02.542386: step 1722, loss 0.243016, acc 0.921875, learning_rate 0.000104297
2017-10-10T11:18:02.705415: step 1723, loss 0.19201, acc 0.921875, learning_rate 0.000104279
2017-10-10T11:18:02.871045: step 1724, loss 0.176367, acc 0.953125, learning_rate 0.000104262
2017-10-10T11:18:03.032186: step 1725, loss 0.155278, acc 0.953125, learning_rate 0.000104245
2017-10-10T11:18:03.194864: step 1726, loss 0.0355158, acc 0.984375, learning_rate 0.000104227
2017-10-10T11:18:03.359097: step 1727, loss 0.123369, acc 0.953125, learning_rate 0.00010421
2017-10-10T11:18:03.523062: step 1728, loss 0.125162, acc 0.9375, learning_rate 0.000104193
2017-10-10T11:18:03.686224: step 1729, loss 0.116739, acc 0.953125, learning_rate 0.000104176
2017-10-10T11:18:03.847477: step 1730, loss 0.214336, acc 0.953125, learning_rate 0.000104159
2017-10-10T11:18:04.012654: step 1731, loss 0.0352362, acc 1, learning_rate 0.000104142
2017-10-10T11:18:04.176249: step 1732, loss 0.115123, acc 0.953125, learning_rate 0.000104125
2017-10-10T11:18:04.338394: step 1733, loss 0.0872879, acc 0.9375, learning_rate 0.000104108
2017-10-10T11:18:04.502290: step 1734, loss 0.18472, acc 0.890625, learning_rate 0.000104091
2017-10-10T11:18:04.664229: step 1735, loss 0.247707, acc 0.890625, learning_rate 0.000104074
2017-10-10T11:18:04.827174: step 1736, loss 0.0945039, acc 0.984375, learning_rate 0.000104058
2017-10-10T11:18:04.989606: step 1737, loss 0.0555649, acc 0.984375, learning_rate 0.000104041
2017-10-10T11:18:05.153271: step 1738, loss 0.191632, acc 0.953125, learning_rate 0.000104025
2017-10-10T11:18:05.314624: step 1739, loss 0.189866, acc 0.953125, learning_rate 0.000104008
2017-10-10T11:18:05.479370: step 1740, loss 0.243926, acc 0.921875, learning_rate 0.000103992
2017-10-10T11:18:05.642362: step 1741, loss 0.0381554, acc 0.984375, learning_rate 0.000103976
2017-10-10T11:18:05.805537: step 1742, loss 0.0772612, acc 0.984375, learning_rate 0.000103959
2017-10-10T11:18:05.970587: step 1743, loss 0.164888, acc 0.921875, learning_rate 0.000103943
2017-10-10T11:18:06.133232: step 1744, loss 0.19801, acc 0.90625, learning_rate 0.000103927
2017-10-10T11:18:06.297661: step 1745, loss 0.179105, acc 0.90625, learning_rate 0.000103911
2017-10-10T11:18:06.458174: step 1746, loss 0.12675, acc 0.953125, learning_rate 0.000103895
2017-10-10T11:18:06.621333: step 1747, loss 0.149073, acc 0.9375, learning_rate 0.000103879
2017-10-10T11:18:06.780939: step 1748, loss 0.0706999, acc 0.953125, learning_rate 0.000103863
2017-10-10T11:18:06.943385: step 1749, loss 0.0354832, acc 0.984375, learning_rate 0.000103848
2017-10-10T11:18:07.107638: step 1750, loss 0.11328, acc 0.9375, learning_rate 0.000103832
2017-10-10T11:18:07.310357: step 1751, loss 0.194882, acc 0.9375, learning_rate 0.000103816
2017-10-10T11:18:07.473888: step 1752, loss 0.174443, acc 0.96875, learning_rate 0.000103801
2017-10-10T11:18:07.639492: step 1753, loss 0.173698, acc 0.953125, learning_rate 0.000103785
2017-10-10T11:18:07.799895: step 1754, loss 0.0838137, acc 0.96875, learning_rate 0.00010377
2017-10-10T11:18:07.968346: step 1755, loss 0.083177, acc 0.96875, learning_rate 0.000103754
2017-10-10T11:18:08.129828: step 1756, loss 0.10002, acc 0.953125, learning_rate 0.000103739
2017-10-10T11:18:08.291344: step 1757, loss 0.165604, acc 0.921875, learning_rate 0.000103724
2017-10-10T11:18:08.455038: step 1758, loss 0.121179, acc 0.921875, learning_rate 0.000103709
2017-10-10T11:18:08.619850: step 1759, loss 0.0910839, acc 0.984375, learning_rate 0.000103694
2017-10-10T11:18:08.781297: step 1760, loss 0.0624068, acc 0.96875, learning_rate 0.000103678

Evaluation:
2017-10-10T11:18:09.226217: step 1760, loss 0.211413, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1760

2017-10-10T11:18:09.942361: step 1761, loss 0.0866919, acc 0.984375, learning_rate 0.000103663
2017-10-10T11:18:10.106120: step 1762, loss 0.0523687, acc 0.984375, learning_rate 0.000103648
2017-10-10T11:18:10.268356: step 1763, loss 0.145211, acc 0.953125, learning_rate 0.000103634
2017-10-10T11:18:10.403653: step 1764, loss 0.109143, acc 0.960784, learning_rate 0.000103619
2017-10-10T11:18:10.567097: step 1765, loss 0.0658856, acc 0.984375, learning_rate 0.000103604
2017-10-10T11:18:10.734426: step 1766, loss 0.162546, acc 0.921875, learning_rate 0.000103589
2017-10-10T11:18:10.898628: step 1767, loss 0.199052, acc 0.9375, learning_rate 0.000103575
2017-10-10T11:18:11.062588: step 1768, loss 0.0990675, acc 0.96875, learning_rate 0.00010356
2017-10-10T11:18:11.223657: step 1769, loss 0.183955, acc 0.96875, learning_rate 0.000103545
2017-10-10T11:18:11.385051: step 1770, loss 0.0687578, acc 0.96875, learning_rate 0.000103531
2017-10-10T11:18:11.547455: step 1771, loss 0.195966, acc 0.9375, learning_rate 0.000103517
2017-10-10T11:18:11.709099: step 1772, loss 0.180646, acc 0.9375, learning_rate 0.000103502
2017-10-10T11:18:11.874117: step 1773, loss 0.0361354, acc 0.984375, learning_rate 0.000103488
2017-10-10T11:18:12.037326: step 1774, loss 0.0700364, acc 0.96875, learning_rate 0.000103474
2017-10-10T11:18:12.199486: step 1775, loss 0.0975839, acc 0.953125, learning_rate 0.00010346
2017-10-10T11:18:12.361126: step 1776, loss 0.179131, acc 0.921875, learning_rate 0.000103445
2017-10-10T11:18:12.522449: step 1777, loss 0.0425131, acc 0.984375, learning_rate 0.000103431
2017-10-10T11:18:12.684220: step 1778, loss 0.2519, acc 0.890625, learning_rate 0.000103417
2017-10-10T11:18:12.847453: step 1779, loss 0.182409, acc 0.9375, learning_rate 0.000103403
2017-10-10T11:18:13.010919: step 1780, loss 0.0952524, acc 0.96875, learning_rate 0.00010339
2017-10-10T11:18:13.173450: step 1781, loss 0.0844077, acc 0.953125, learning_rate 0.000103376
2017-10-10T11:18:13.337538: step 1782, loss 0.0941336, acc 0.96875, learning_rate 0.000103362
2017-10-10T11:18:13.499425: step 1783, loss 0.0937094, acc 0.96875, learning_rate 0.000103348
2017-10-10T11:18:13.660840: step 1784, loss 0.140771, acc 0.9375, learning_rate 0.000103335
2017-10-10T11:18:13.820899: step 1785, loss 0.117734, acc 0.953125, learning_rate 0.000103321
2017-10-10T11:18:13.990268: step 1786, loss 0.178827, acc 0.9375, learning_rate 0.000103307
2017-10-10T11:18:14.155587: step 1787, loss 0.0975331, acc 0.96875, learning_rate 0.000103294
2017-10-10T11:18:14.315945: step 1788, loss 0.134059, acc 0.96875, learning_rate 0.00010328
2017-10-10T11:18:14.478081: step 1789, loss 0.080215, acc 0.96875, learning_rate 0.000103267
2017-10-10T11:18:14.643530: step 1790, loss 0.0469187, acc 0.984375, learning_rate 0.000103254
2017-10-10T11:18:14.804408: step 1791, loss 0.0924983, acc 0.953125, learning_rate 0.00010324
2017-10-10T11:18:14.971618: step 1792, loss 0.189247, acc 0.890625, learning_rate 0.000103227
2017-10-10T11:18:15.136244: step 1793, loss 0.165085, acc 0.953125, learning_rate 0.000103214
2017-10-10T11:18:15.298090: step 1794, loss 0.197661, acc 0.921875, learning_rate 0.000103201
2017-10-10T11:18:15.461994: step 1795, loss 0.137765, acc 0.921875, learning_rate 0.000103188
2017-10-10T11:18:15.621908: step 1796, loss 0.144877, acc 0.9375, learning_rate 0.000103175
2017-10-10T11:18:15.782501: step 1797, loss 0.0606089, acc 0.984375, learning_rate 0.000103162
2017-10-10T11:18:15.942515: step 1798, loss 0.219923, acc 0.921875, learning_rate 0.000103149
2017-10-10T11:18:16.101351: step 1799, loss 0.152053, acc 0.9375, learning_rate 0.000103136
2017-10-10T11:18:16.263748: step 1800, loss 0.0464837, acc 0.984375, learning_rate 0.000103123

Evaluation:
2017-10-10T11:18:16.731014: step 1800, loss 0.214387, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1800

2017-10-10T11:18:17.334775: step 1801, loss 0.140922, acc 0.953125, learning_rate 0.000103111
2017-10-10T11:18:17.498615: step 1802, loss 0.401073, acc 0.921875, learning_rate 0.000103098
2017-10-10T11:18:17.660722: step 1803, loss 0.0613325, acc 0.984375, learning_rate 0.000103085
2017-10-10T11:18:17.824340: step 1804, loss 0.105892, acc 0.9375, learning_rate 0.000103073
2017-10-10T11:18:17.987409: step 1805, loss 0.125436, acc 0.9375, learning_rate 0.00010306
2017-10-10T11:18:18.150197: step 1806, loss 0.0370458, acc 1, learning_rate 0.000103048
2017-10-10T11:18:18.312403: step 1807, loss 0.138556, acc 0.9375, learning_rate 0.000103035
2017-10-10T11:18:18.472705: step 1808, loss 0.132235, acc 0.921875, learning_rate 0.000103023
2017-10-10T11:18:18.637194: step 1809, loss 0.0670486, acc 0.96875, learning_rate 0.00010301
2017-10-10T11:18:18.799862: step 1810, loss 0.205371, acc 0.953125, learning_rate 0.000102998
2017-10-10T11:18:18.963848: step 1811, loss 0.0827446, acc 0.953125, learning_rate 0.000102986
2017-10-10T11:18:19.127049: step 1812, loss 0.186539, acc 0.9375, learning_rate 0.000102974
2017-10-10T11:18:19.290990: step 1813, loss 0.230915, acc 0.90625, learning_rate 0.000102962
2017-10-10T11:18:19.450806: step 1814, loss 0.064268, acc 0.984375, learning_rate 0.000102949
2017-10-10T11:18:19.613616: step 1815, loss 0.238301, acc 0.921875, learning_rate 0.000102937
2017-10-10T11:18:19.778527: step 1816, loss 0.157615, acc 0.9375, learning_rate 0.000102925
2017-10-10T11:18:19.949080: step 1817, loss 0.0967714, acc 0.984375, learning_rate 0.000102913
2017-10-10T11:18:20.110932: step 1818, loss 0.1059, acc 0.96875, learning_rate 0.000102902
2017-10-10T11:18:20.274594: step 1819, loss 0.244302, acc 0.921875, learning_rate 0.00010289
2017-10-10T11:18:20.435925: step 1820, loss 0.0592933, acc 0.984375, learning_rate 0.000102878
2017-10-10T11:18:20.596564: step 1821, loss 0.0685067, acc 0.984375, learning_rate 0.000102866
2017-10-10T11:18:20.756143: step 1822, loss 0.0734831, acc 0.96875, learning_rate 0.000102855
2017-10-10T11:18:20.918634: step 1823, loss 0.0469747, acc 0.984375, learning_rate 0.000102843
2017-10-10T11:18:21.082321: step 1824, loss 0.2331, acc 0.90625, learning_rate 0.000102831
2017-10-10T11:18:21.244031: step 1825, loss 0.0810327, acc 1, learning_rate 0.00010282
2017-10-10T11:18:21.406506: step 1826, loss 0.270468, acc 0.890625, learning_rate 0.000102808
2017-10-10T11:18:21.567354: step 1827, loss 0.0754456, acc 0.96875, learning_rate 0.000102797
2017-10-10T11:18:21.729442: step 1828, loss 0.117653, acc 0.953125, learning_rate 0.000102785
2017-10-10T11:18:21.898949: step 1829, loss 0.0330104, acc 1, learning_rate 0.000102774
2017-10-10T11:18:22.063056: step 1830, loss 0.0553067, acc 0.96875, learning_rate 0.000102763
2017-10-10T11:18:22.228049: step 1831, loss 0.0535607, acc 1, learning_rate 0.000102751
2017-10-10T11:18:22.387414: step 1832, loss 0.0952458, acc 0.9375, learning_rate 0.00010274
2017-10-10T11:18:22.552285: step 1833, loss 0.105271, acc 0.96875, learning_rate 0.000102729
2017-10-10T11:18:22.715810: step 1834, loss 0.103849, acc 0.984375, learning_rate 0.000102718
2017-10-10T11:18:22.879067: step 1835, loss 0.0337837, acc 1, learning_rate 0.000102707
2017-10-10T11:18:23.040632: step 1836, loss 0.081374, acc 0.96875, learning_rate 0.000102696
2017-10-10T11:18:23.204485: step 1837, loss 0.216094, acc 0.921875, learning_rate 0.000102685
2017-10-10T11:18:23.367360: step 1838, loss 0.212968, acc 0.921875, learning_rate 0.000102674
2017-10-10T11:18:23.531993: step 1839, loss 0.118849, acc 0.953125, learning_rate 0.000102663
2017-10-10T11:18:23.694420: step 1840, loss 0.135165, acc 0.96875, learning_rate 0.000102652

Evaluation:
2017-10-10T11:18:24.151612: step 1840, loss 0.216939, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1840

2017-10-10T11:18:24.784907: step 1841, loss 0.11093, acc 0.953125, learning_rate 0.000102641
2017-10-10T11:18:24.951060: step 1842, loss 0.140688, acc 0.921875, learning_rate 0.00010263
2017-10-10T11:18:25.113929: step 1843, loss 0.0477433, acc 0.984375, learning_rate 0.00010262
2017-10-10T11:18:25.275027: step 1844, loss 0.0846859, acc 0.984375, learning_rate 0.000102609
2017-10-10T11:18:25.438527: step 1845, loss 0.091792, acc 0.984375, learning_rate 0.000102598
2017-10-10T11:18:25.603126: step 1846, loss 0.236936, acc 0.953125, learning_rate 0.000102588
2017-10-10T11:18:25.766296: step 1847, loss 0.109524, acc 0.953125, learning_rate 0.000102577
2017-10-10T11:18:25.935536: step 1848, loss 0.131323, acc 0.953125, learning_rate 0.000102567
2017-10-10T11:18:26.101446: step 1849, loss 0.0432226, acc 0.984375, learning_rate 0.000102556
2017-10-10T11:18:26.267930: step 1850, loss 0.0626614, acc 0.984375, learning_rate 0.000102546
2017-10-10T11:18:26.430355: step 1851, loss 0.107994, acc 0.953125, learning_rate 0.000102535
2017-10-10T11:18:26.591887: step 1852, loss 0.193985, acc 0.9375, learning_rate 0.000102525
2017-10-10T11:18:26.754259: step 1853, loss 0.265402, acc 0.890625, learning_rate 0.000102515
2017-10-10T11:18:26.916908: step 1854, loss 0.0749568, acc 0.984375, learning_rate 0.000102504
2017-10-10T11:18:27.077668: step 1855, loss 0.0708562, acc 0.984375, learning_rate 0.000102494
2017-10-10T11:18:27.239941: step 1856, loss 0.22627, acc 0.890625, learning_rate 0.000102484
2017-10-10T11:18:27.402197: step 1857, loss 0.152994, acc 0.9375, learning_rate 0.000102474
2017-10-10T11:18:27.565720: step 1858, loss 0.0751462, acc 0.984375, learning_rate 0.000102464
2017-10-10T11:18:27.726668: step 1859, loss 0.0640592, acc 1, learning_rate 0.000102454
2017-10-10T11:18:27.891822: step 1860, loss 0.231383, acc 0.9375, learning_rate 0.000102444
2017-10-10T11:18:28.054210: step 1861, loss 0.0451887, acc 0.984375, learning_rate 0.000102434
2017-10-10T11:18:28.190745: step 1862, loss 0.199037, acc 0.960784, learning_rate 0.000102424
2017-10-10T11:18:28.354654: step 1863, loss 0.0468807, acc 0.984375, learning_rate 0.000102414
2017-10-10T11:18:28.517158: step 1864, loss 0.160175, acc 0.9375, learning_rate 0.000102404
2017-10-10T11:18:28.681336: step 1865, loss 0.0642745, acc 0.984375, learning_rate 0.000102394
2017-10-10T11:18:28.846185: step 1866, loss 0.0623912, acc 0.984375, learning_rate 0.000102384
2017-10-10T11:18:29.004825: step 1867, loss 0.144567, acc 0.9375, learning_rate 0.000102375
2017-10-10T11:18:29.167015: step 1868, loss 0.116981, acc 0.953125, learning_rate 0.000102365
2017-10-10T11:18:29.328696: step 1869, loss 0.155079, acc 0.9375, learning_rate 0.000102355
2017-10-10T11:18:29.489024: step 1870, loss 0.0546429, acc 0.984375, learning_rate 0.000102346
2017-10-10T11:18:29.650869: step 1871, loss 0.170845, acc 0.9375, learning_rate 0.000102336
2017-10-10T11:18:29.811844: step 1872, loss 0.0997293, acc 0.984375, learning_rate 0.000102327
2017-10-10T11:18:29.989280: step 1873, loss 0.0540999, acc 0.984375, learning_rate 0.000102317
2017-10-10T11:18:30.150822: step 1874, loss 0.196569, acc 0.9375, learning_rate 0.000102308
2017-10-10T11:18:30.310387: step 1875, loss 0.051979, acc 1, learning_rate 0.000102298
2017-10-10T11:18:30.470120: step 1876, loss 0.26878, acc 0.953125, learning_rate 0.000102289
2017-10-10T11:18:30.634386: step 1877, loss 0.0318383, acc 0.984375, learning_rate 0.000102279
2017-10-10T11:18:30.794646: step 1878, loss 0.153698, acc 0.9375, learning_rate 0.00010227
2017-10-10T11:18:30.959241: step 1879, loss 0.121619, acc 0.96875, learning_rate 0.000102261
2017-10-10T11:18:31.120297: step 1880, loss 0.161531, acc 0.9375, learning_rate 0.000102252

Evaluation:
2017-10-10T11:18:31.563988: step 1880, loss 0.217782, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1880

2017-10-10T11:18:32.278739: step 1881, loss 0.104305, acc 0.9375, learning_rate 0.000102242
2017-10-10T11:18:32.437336: step 1882, loss 0.14188, acc 0.96875, learning_rate 0.000102233
2017-10-10T11:18:32.603799: step 1883, loss 0.0340053, acc 0.984375, learning_rate 0.000102224
2017-10-10T11:18:32.765803: step 1884, loss 0.1627, acc 0.9375, learning_rate 0.000102215
2017-10-10T11:18:32.930592: step 1885, loss 0.0931803, acc 0.96875, learning_rate 0.000102206
2017-10-10T11:18:33.092813: step 1886, loss 0.104652, acc 0.953125, learning_rate 0.000102197
2017-10-10T11:18:33.254932: step 1887, loss 0.0608649, acc 0.96875, learning_rate 0.000102188
2017-10-10T11:18:33.417408: step 1888, loss 0.201862, acc 0.921875, learning_rate 0.000102179
2017-10-10T11:18:33.580512: step 1889, loss 0.0722407, acc 0.96875, learning_rate 0.00010217
2017-10-10T11:18:33.742146: step 1890, loss 0.139573, acc 0.9375, learning_rate 0.000102161
2017-10-10T11:18:33.905803: step 1891, loss 0.170772, acc 0.953125, learning_rate 0.000102153
2017-10-10T11:18:34.066043: step 1892, loss 0.0295747, acc 1, learning_rate 0.000102144
2017-10-10T11:18:34.227042: step 1893, loss 0.0682778, acc 0.984375, learning_rate 0.000102135
2017-10-10T11:18:34.389987: step 1894, loss 0.124095, acc 0.953125, learning_rate 0.000102126
2017-10-10T11:18:34.553899: step 1895, loss 0.0606369, acc 0.984375, learning_rate 0.000102118
2017-10-10T11:18:34.713452: step 1896, loss 0.0511441, acc 1, learning_rate 0.000102109
2017-10-10T11:18:34.880272: step 1897, loss 0.197154, acc 0.90625, learning_rate 0.0001021
2017-10-10T11:18:35.041075: step 1898, loss 0.245721, acc 0.90625, learning_rate 0.000102092
2017-10-10T11:18:35.199084: step 1899, loss 0.165818, acc 0.953125, learning_rate 0.000102083
2017-10-10T11:18:35.361036: step 1900, loss 0.145917, acc 0.953125, learning_rate 0.000102075
2017-10-10T11:18:35.523260: step 1901, loss 0.125724, acc 0.9375, learning_rate 0.000102066
2017-10-10T11:18:35.681560: step 1902, loss 0.123134, acc 0.953125, learning_rate 0.000102058
2017-10-10T11:18:35.847639: step 1903, loss 0.149742, acc 0.9375, learning_rate 0.00010205
2017-10-10T11:18:36.005471: step 1904, loss 0.0990376, acc 0.953125, learning_rate 0.000102041
2017-10-10T11:18:36.169148: step 1905, loss 0.108269, acc 0.953125, learning_rate 0.000102033
2017-10-10T11:18:36.330107: step 1906, loss 0.0729577, acc 0.953125, learning_rate 0.000102025
2017-10-10T11:18:36.490450: step 1907, loss 0.0303247, acc 1, learning_rate 0.000102016
2017-10-10T11:18:36.653301: step 1908, loss 0.164, acc 0.984375, learning_rate 0.000102008
2017-10-10T11:18:36.815082: step 1909, loss 0.036202, acc 0.984375, learning_rate 0.000102
2017-10-10T11:18:36.978278: step 1910, loss 0.0509004, acc 0.984375, learning_rate 0.000101992
2017-10-10T11:18:37.139886: step 1911, loss 0.19754, acc 0.9375, learning_rate 0.000101984
2017-10-10T11:18:37.298862: step 1912, loss 0.107429, acc 0.953125, learning_rate 0.000101975
2017-10-10T11:18:37.463711: step 1913, loss 0.122601, acc 0.96875, learning_rate 0.000101967
2017-10-10T11:18:37.627709: step 1914, loss 0.0971189, acc 0.96875, learning_rate 0.000101959
2017-10-10T11:18:37.791936: step 1915, loss 0.143084, acc 0.9375, learning_rate 0.000101951
2017-10-10T11:18:37.961398: step 1916, loss 0.246286, acc 0.890625, learning_rate 0.000101943
2017-10-10T11:18:38.123900: step 1917, loss 0.0893184, acc 0.96875, learning_rate 0.000101935
2017-10-10T11:18:38.282742: step 1918, loss 0.130404, acc 0.953125, learning_rate 0.000101928
2017-10-10T11:18:38.445575: step 1919, loss 0.107106, acc 0.96875, learning_rate 0.00010192
2017-10-10T11:18:38.609043: step 1920, loss 0.152384, acc 0.953125, learning_rate 0.000101912

Evaluation:
2017-10-10T11:18:39.070596: step 1920, loss 0.215131, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1920

2017-10-10T11:18:39.640392: step 1921, loss 0.0905318, acc 0.96875, learning_rate 0.000101904
2017-10-10T11:18:39.802696: step 1922, loss 0.130816, acc 0.9375, learning_rate 0.000101896
2017-10-10T11:18:39.971422: step 1923, loss 0.249548, acc 0.890625, learning_rate 0.000101889
2017-10-10T11:18:40.136285: step 1924, loss 0.238796, acc 0.9375, learning_rate 0.000101881
2017-10-10T11:18:40.300063: step 1925, loss 0.0815144, acc 0.96875, learning_rate 0.000101873
2017-10-10T11:18:40.462138: step 1926, loss 0.125769, acc 0.96875, learning_rate 0.000101865
2017-10-10T11:18:40.626597: step 1927, loss 0.356641, acc 0.875, learning_rate 0.000101858
2017-10-10T11:18:40.785558: step 1928, loss 0.128171, acc 0.921875, learning_rate 0.00010185
2017-10-10T11:18:40.951369: step 1929, loss 0.094263, acc 0.96875, learning_rate 0.000101843
2017-10-10T11:18:41.111714: step 1930, loss 0.0859259, acc 0.96875, learning_rate 0.000101835
2017-10-10T11:18:41.274700: step 1931, loss 0.0645984, acc 0.984375, learning_rate 0.000101828
2017-10-10T11:18:41.436903: step 1932, loss 0.184371, acc 0.953125, learning_rate 0.00010182
2017-10-10T11:18:41.598562: step 1933, loss 0.202637, acc 0.9375, learning_rate 0.000101813
2017-10-10T11:18:41.759085: step 1934, loss 0.147718, acc 0.9375, learning_rate 0.000101805
2017-10-10T11:18:41.923548: step 1935, loss 0.147623, acc 0.953125, learning_rate 0.000101798
2017-10-10T11:18:42.084186: step 1936, loss 0.0496631, acc 1, learning_rate 0.000101791
2017-10-10T11:18:42.249586: step 1937, loss 0.139002, acc 0.953125, learning_rate 0.000101783
2017-10-10T11:18:42.410856: step 1938, loss 0.0620606, acc 0.96875, learning_rate 0.000101776
2017-10-10T11:18:42.574104: step 1939, loss 0.115507, acc 0.96875, learning_rate 0.000101769
2017-10-10T11:18:42.735510: step 1940, loss 0.182924, acc 0.953125, learning_rate 0.000101762
2017-10-10T11:18:42.895738: step 1941, loss 0.103397, acc 0.96875, learning_rate 0.000101754
2017-10-10T11:18:43.058926: step 1942, loss 0.105466, acc 0.96875, learning_rate 0.000101747
2017-10-10T11:18:43.221464: step 1943, loss 0.0341572, acc 0.984375, learning_rate 0.00010174
2017-10-10T11:18:43.385504: step 1944, loss 0.173803, acc 0.9375, learning_rate 0.000101733
2017-10-10T11:18:43.549207: step 1945, loss 0.131749, acc 0.96875, learning_rate 0.000101726
2017-10-10T11:18:43.710089: step 1946, loss 0.170622, acc 0.9375, learning_rate 0.000101719
2017-10-10T11:18:43.875727: step 1947, loss 0.327248, acc 0.921875, learning_rate 0.000101712
2017-10-10T11:18:44.034848: step 1948, loss 0.14896, acc 0.9375, learning_rate 0.000101705
2017-10-10T11:18:44.196501: step 1949, loss 0.162002, acc 0.96875, learning_rate 0.000101698
2017-10-10T11:18:44.362076: step 1950, loss 0.15975, acc 0.953125, learning_rate 0.000101691
2017-10-10T11:18:44.525410: step 1951, loss 0.0665105, acc 0.984375, learning_rate 0.000101684
2017-10-10T11:18:44.689000: step 1952, loss 0.146931, acc 0.9375, learning_rate 0.000101677
2017-10-10T11:18:44.853067: step 1953, loss 0.0936196, acc 0.953125, learning_rate 0.00010167
2017-10-10T11:18:45.018761: step 1954, loss 0.100008, acc 0.984375, learning_rate 0.000101664
2017-10-10T11:18:45.182427: step 1955, loss 0.157447, acc 0.9375, learning_rate 0.000101657
2017-10-10T11:18:45.342621: step 1956, loss 0.130468, acc 0.921875, learning_rate 0.00010165
2017-10-10T11:18:45.506941: step 1957, loss 0.0889041, acc 0.96875, learning_rate 0.000101643
2017-10-10T11:18:45.666539: step 1958, loss 0.14801, acc 0.9375, learning_rate 0.000101637
2017-10-10T11:18:45.830320: step 1959, loss 0.0906451, acc 0.953125, learning_rate 0.00010163
2017-10-10T11:18:45.965056: step 1960, loss 0.0590785, acc 0.980392, learning_rate 0.000101623

Evaluation:
2017-10-10T11:18:46.424677: step 1960, loss 0.212336, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-1960

2017-10-10T11:18:47.063979: step 1961, loss 0.0358826, acc 0.984375, learning_rate 0.000101617
2017-10-10T11:18:47.227939: step 1962, loss 0.109519, acc 0.984375, learning_rate 0.00010161
2017-10-10T11:18:47.389686: step 1963, loss 0.0917738, acc 0.96875, learning_rate 0.000101604
2017-10-10T11:18:47.550561: step 1964, loss 0.254777, acc 0.9375, learning_rate 0.000101597
2017-10-10T11:18:47.711690: step 1965, loss 0.163526, acc 0.921875, learning_rate 0.00010159
2017-10-10T11:18:47.874490: step 1966, loss 0.132181, acc 0.96875, learning_rate 0.000101584
2017-10-10T11:18:48.036197: step 1967, loss 0.0959595, acc 0.953125, learning_rate 0.000101577
2017-10-10T11:18:48.196914: step 1968, loss 0.0636127, acc 0.984375, learning_rate 0.000101571
2017-10-10T11:18:48.358525: step 1969, loss 0.238974, acc 0.921875, learning_rate 0.000101565
2017-10-10T11:18:48.520756: step 1970, loss 0.0830412, acc 0.984375, learning_rate 0.000101558
2017-10-10T11:18:48.691016: step 1971, loss 0.212149, acc 0.921875, learning_rate 0.000101552
2017-10-10T11:18:48.852043: step 1972, loss 0.108383, acc 0.96875, learning_rate 0.000101546
2017-10-10T11:18:49.016259: step 1973, loss 0.181501, acc 0.953125, learning_rate 0.000101539
2017-10-10T11:18:49.177922: step 1974, loss 0.0831504, acc 0.953125, learning_rate 0.000101533
2017-10-10T11:18:49.343687: step 1975, loss 0.158767, acc 0.9375, learning_rate 0.000101527
2017-10-10T11:18:49.508652: step 1976, loss 0.0849869, acc 0.96875, learning_rate 0.00010152
2017-10-10T11:18:49.673231: step 1977, loss 0.0745852, acc 0.984375, learning_rate 0.000101514
2017-10-10T11:18:49.837557: step 1978, loss 0.0750609, acc 0.9375, learning_rate 0.000101508
2017-10-10T11:18:50.004510: step 1979, loss 0.199113, acc 0.90625, learning_rate 0.000101502
2017-10-10T11:18:50.165145: step 1980, loss 0.0757903, acc 0.984375, learning_rate 0.000101496
2017-10-10T11:18:50.327335: step 1981, loss 0.133379, acc 0.9375, learning_rate 0.00010149
2017-10-10T11:18:50.488086: step 1982, loss 0.0977891, acc 0.984375, learning_rate 0.000101484
2017-10-10T11:18:50.651271: step 1983, loss 0.166199, acc 0.9375, learning_rate 0.000101478
2017-10-10T11:18:50.814255: step 1984, loss 0.128123, acc 0.953125, learning_rate 0.000101472
2017-10-10T11:18:50.987342: step 1985, loss 0.159728, acc 0.953125, learning_rate 0.000101466
2017-10-10T11:18:51.146616: step 1986, loss 0.10058, acc 0.96875, learning_rate 0.00010146
2017-10-10T11:18:51.308622: step 1987, loss 0.0886643, acc 0.96875, learning_rate 0.000101454
2017-10-10T11:18:51.472587: step 1988, loss 0.168552, acc 0.953125, learning_rate 0.000101448
2017-10-10T11:18:51.636774: step 1989, loss 0.0960826, acc 0.9375, learning_rate 0.000101442
2017-10-10T11:18:51.798637: step 1990, loss 0.173735, acc 0.9375, learning_rate 0.000101436
2017-10-10T11:18:51.962668: step 1991, loss 0.0185539, acc 1, learning_rate 0.00010143
2017-10-10T11:18:52.124570: step 1992, loss 0.0928838, acc 0.96875, learning_rate 0.000101424
2017-10-10T11:18:52.285936: step 1993, loss 0.11106, acc 0.9375, learning_rate 0.000101418
2017-10-10T11:18:52.446185: step 1994, loss 0.177694, acc 0.921875, learning_rate 0.000101413
2017-10-10T11:18:52.606116: step 1995, loss 0.0329804, acc 0.984375, learning_rate 0.000101407
2017-10-10T11:18:52.766872: step 1996, loss 0.0903541, acc 0.953125, learning_rate 0.000101401
2017-10-10T11:18:52.929682: step 1997, loss 0.10617, acc 0.953125, learning_rate 0.000101395
2017-10-10T11:18:53.092026: step 1998, loss 0.107663, acc 0.953125, learning_rate 0.00010139
2017-10-10T11:18:53.253992: step 1999, loss 0.0683079, acc 0.96875, learning_rate 0.000101384
2017-10-10T11:18:53.417036: step 2000, loss 0.128137, acc 0.953125, learning_rate 0.000101378

Evaluation:
2017-10-10T11:18:53.871405: step 2000, loss 0.216751, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2000

2017-10-10T11:18:54.510184: step 2001, loss 0.113617, acc 0.96875, learning_rate 0.000101373
2017-10-10T11:18:54.671706: step 2002, loss 0.190307, acc 0.921875, learning_rate 0.000101367
2017-10-10T11:18:54.833912: step 2003, loss 0.0678433, acc 0.96875, learning_rate 0.000101362
2017-10-10T11:18:54.993555: step 2004, loss 0.164768, acc 0.9375, learning_rate 0.000101356
2017-10-10T11:18:55.154733: step 2005, loss 0.322782, acc 0.9375, learning_rate 0.00010135
2017-10-10T11:18:55.318101: step 2006, loss 0.0564479, acc 0.96875, learning_rate 0.000101345
2017-10-10T11:18:55.478190: step 2007, loss 0.0468045, acc 0.984375, learning_rate 0.000101339
2017-10-10T11:18:55.642622: step 2008, loss 0.110884, acc 0.953125, learning_rate 0.000101334
2017-10-10T11:18:55.805383: step 2009, loss 0.178704, acc 0.953125, learning_rate 0.000101328
2017-10-10T11:18:55.972853: step 2010, loss 0.144001, acc 0.9375, learning_rate 0.000101323
2017-10-10T11:18:56.133014: step 2011, loss 0.166157, acc 0.953125, learning_rate 0.000101318
2017-10-10T11:18:56.297920: step 2012, loss 0.168399, acc 0.921875, learning_rate 0.000101312
2017-10-10T11:18:56.461556: step 2013, loss 0.286252, acc 0.921875, learning_rate 0.000101307
2017-10-10T11:18:56.623351: step 2014, loss 0.0737734, acc 0.96875, learning_rate 0.000101302
2017-10-10T11:18:56.783667: step 2015, loss 0.125991, acc 0.953125, learning_rate 0.000101296
2017-10-10T11:18:56.945712: step 2016, loss 0.114516, acc 0.953125, learning_rate 0.000101291
2017-10-10T11:18:57.117271: step 2017, loss 0.123603, acc 0.953125, learning_rate 0.000101286
2017-10-10T11:18:57.277103: step 2018, loss 0.0362521, acc 1, learning_rate 0.00010128
2017-10-10T11:18:57.540200: step 2019, loss 0.0322633, acc 1, learning_rate 0.000101275
2017-10-10T11:18:57.701885: step 2020, loss 0.127253, acc 0.96875, learning_rate 0.00010127
2017-10-10T11:18:57.868367: step 2021, loss 0.082373, acc 0.984375, learning_rate 0.000101265
2017-10-10T11:18:58.028443: step 2022, loss 0.149789, acc 0.953125, learning_rate 0.00010126
2017-10-10T11:18:58.189329: step 2023, loss 0.0972977, acc 0.96875, learning_rate 0.000101255
2017-10-10T11:18:58.349618: step 2024, loss 0.0672824, acc 0.984375, learning_rate 0.000101249
2017-10-10T11:18:58.513547: step 2025, loss 0.167732, acc 0.921875, learning_rate 0.000101244
2017-10-10T11:18:58.676857: step 2026, loss 0.0495673, acc 1, learning_rate 0.000101239
2017-10-10T11:18:58.840420: step 2027, loss 0.0904401, acc 0.96875, learning_rate 0.000101234
2017-10-10T11:18:59.013417: step 2028, loss 0.128791, acc 0.96875, learning_rate 0.000101229
2017-10-10T11:18:59.171999: step 2029, loss 0.140464, acc 0.921875, learning_rate 0.000101224
2017-10-10T11:18:59.332684: step 2030, loss 0.136403, acc 0.953125, learning_rate 0.000101219
2017-10-10T11:18:59.495133: step 2031, loss 0.120669, acc 0.953125, learning_rate 0.000101214
2017-10-10T11:18:59.659122: step 2032, loss 0.0357819, acc 1, learning_rate 0.000101209
2017-10-10T11:18:59.821331: step 2033, loss 0.0702217, acc 0.96875, learning_rate 0.000101204
2017-10-10T11:18:59.986701: step 2034, loss 0.0749245, acc 0.96875, learning_rate 0.000101199
2017-10-10T11:19:00.150212: step 2035, loss 0.0443817, acc 0.984375, learning_rate 0.000101194
2017-10-10T11:19:00.311719: step 2036, loss 0.0498125, acc 0.96875, learning_rate 0.00010119
2017-10-10T11:19:00.472239: step 2037, loss 0.18557, acc 0.96875, learning_rate 0.000101185
2017-10-10T11:19:00.634885: step 2038, loss 0.165022, acc 0.96875, learning_rate 0.00010118
2017-10-10T11:19:00.798482: step 2039, loss 0.0534195, acc 0.984375, learning_rate 0.000101175
2017-10-10T11:19:00.963189: step 2040, loss 0.208339, acc 0.9375, learning_rate 0.00010117

Evaluation:
2017-10-10T11:19:01.408204: step 2040, loss 0.21229, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2040

2017-10-10T11:19:02.117206: step 2041, loss 0.0606048, acc 0.984375, learning_rate 0.000101166
2017-10-10T11:19:02.281539: step 2042, loss 0.0907983, acc 0.96875, learning_rate 0.000101161
2017-10-10T11:19:02.446949: step 2043, loss 0.221359, acc 0.9375, learning_rate 0.000101156
2017-10-10T11:19:02.611847: step 2044, loss 0.171186, acc 0.921875, learning_rate 0.000101151
2017-10-10T11:19:02.775478: step 2045, loss 0.116062, acc 0.953125, learning_rate 0.000101147
2017-10-10T11:19:02.937868: step 2046, loss 0.0669521, acc 1, learning_rate 0.000101142
2017-10-10T11:19:03.097944: step 2047, loss 0.22722, acc 0.90625, learning_rate 0.000101137
2017-10-10T11:19:03.259473: step 2048, loss 0.0401302, acc 1, learning_rate 0.000101133
2017-10-10T11:19:03.420345: step 2049, loss 0.156665, acc 0.921875, learning_rate 0.000101128
2017-10-10T11:19:03.582286: step 2050, loss 0.0919611, acc 0.96875, learning_rate 0.000101123
2017-10-10T11:19:03.744571: step 2051, loss 0.142694, acc 0.953125, learning_rate 0.000101119
2017-10-10T11:19:03.908287: step 2052, loss 0.0749193, acc 0.953125, learning_rate 0.000101114
2017-10-10T11:19:04.070830: step 2053, loss 0.150575, acc 0.953125, learning_rate 0.00010111
2017-10-10T11:19:04.228510: step 2054, loss 0.14248, acc 0.953125, learning_rate 0.000101105
2017-10-10T11:19:04.389821: step 2055, loss 0.229824, acc 0.875, learning_rate 0.000101101
2017-10-10T11:19:04.551361: step 2056, loss 0.145561, acc 0.96875, learning_rate 0.000101096
2017-10-10T11:19:04.714144: step 2057, loss 0.10168, acc 0.953125, learning_rate 0.000101092
2017-10-10T11:19:04.854862: step 2058, loss 0.13896, acc 0.941176, learning_rate 0.000101087
2017-10-10T11:19:05.017105: step 2059, loss 0.0919685, acc 0.96875, learning_rate 0.000101083
2017-10-10T11:19:05.176794: step 2060, loss 0.0423118, acc 0.984375, learning_rate 0.000101078
2017-10-10T11:19:05.339187: step 2061, loss 0.257624, acc 0.921875, learning_rate 0.000101074
2017-10-10T11:19:05.502401: step 2062, loss 0.18433, acc 0.90625, learning_rate 0.00010107
2017-10-10T11:19:05.662359: step 2063, loss 0.0631589, acc 0.984375, learning_rate 0.000101065
2017-10-10T11:19:05.824113: step 2064, loss 0.0536898, acc 0.984375, learning_rate 0.000101061
2017-10-10T11:19:05.988627: step 2065, loss 0.138078, acc 0.953125, learning_rate 0.000101057
2017-10-10T11:19:06.149679: step 2066, loss 0.149021, acc 0.953125, learning_rate 0.000101052
2017-10-10T11:19:06.312288: step 2067, loss 0.099864, acc 0.96875, learning_rate 0.000101048
2017-10-10T11:19:06.473317: step 2068, loss 0.0546518, acc 1, learning_rate 0.000101044
2017-10-10T11:19:06.638741: step 2069, loss 0.0581657, acc 0.96875, learning_rate 0.000101039
2017-10-10T11:19:06.799524: step 2070, loss 0.0999435, acc 0.96875, learning_rate 0.000101035
2017-10-10T11:19:06.969719: step 2071, loss 0.10264, acc 0.96875, learning_rate 0.000101031
2017-10-10T11:19:07.133492: step 2072, loss 0.133324, acc 0.96875, learning_rate 0.000101027
2017-10-10T11:19:07.297992: step 2073, loss 0.233398, acc 0.90625, learning_rate 0.000101023
2017-10-10T11:19:07.457379: step 2074, loss 0.211371, acc 0.875, learning_rate 0.000101018
2017-10-10T11:19:07.622095: step 2075, loss 0.127967, acc 0.953125, learning_rate 0.000101014
2017-10-10T11:19:07.779399: step 2076, loss 0.104142, acc 0.96875, learning_rate 0.00010101
2017-10-10T11:19:07.941078: step 2077, loss 0.0980815, acc 0.96875, learning_rate 0.000101006
2017-10-10T11:19:08.101219: step 2078, loss 0.222223, acc 0.90625, learning_rate 0.000101002
2017-10-10T11:19:08.262636: step 2079, loss 0.196032, acc 0.96875, learning_rate 0.000100998
2017-10-10T11:19:08.423794: step 2080, loss 0.208365, acc 0.921875, learning_rate 0.000100994

Evaluation:
2017-10-10T11:19:08.890539: step 2080, loss 0.21407, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2080

2017-10-10T11:19:09.461814: step 2081, loss 0.0712402, acc 0.96875, learning_rate 0.00010099
2017-10-10T11:19:09.627955: step 2082, loss 0.175917, acc 0.9375, learning_rate 0.000100986
2017-10-10T11:19:09.792251: step 2083, loss 0.143017, acc 0.9375, learning_rate 0.000100982
2017-10-10T11:19:09.955322: step 2084, loss 0.139647, acc 0.96875, learning_rate 0.000100978
2017-10-10T11:19:10.119200: step 2085, loss 0.129018, acc 0.96875, learning_rate 0.000100974
2017-10-10T11:19:10.280336: step 2086, loss 0.139644, acc 0.953125, learning_rate 0.00010097
2017-10-10T11:19:10.444253: step 2087, loss 0.0914477, acc 0.953125, learning_rate 0.000100966
2017-10-10T11:19:10.606348: step 2088, loss 0.0837709, acc 0.96875, learning_rate 0.000100962
2017-10-10T11:19:10.769049: step 2089, loss 0.0760215, acc 0.984375, learning_rate 0.000100958
2017-10-10T11:19:10.931047: step 2090, loss 0.109511, acc 0.953125, learning_rate 0.000100954
2017-10-10T11:19:11.093292: step 2091, loss 0.106383, acc 0.953125, learning_rate 0.00010095
2017-10-10T11:19:11.254485: step 2092, loss 0.0963122, acc 0.953125, learning_rate 0.000100946
2017-10-10T11:19:11.416433: step 2093, loss 0.114363, acc 0.96875, learning_rate 0.000100942
2017-10-10T11:19:11.574467: step 2094, loss 0.158157, acc 0.921875, learning_rate 0.000100938
2017-10-10T11:19:11.732356: step 2095, loss 0.269582, acc 0.90625, learning_rate 0.000100935
2017-10-10T11:19:11.895931: step 2096, loss 0.0403047, acc 1, learning_rate 0.000100931
2017-10-10T11:19:12.058525: step 2097, loss 0.151566, acc 0.9375, learning_rate 0.000100927
2017-10-10T11:19:12.219377: step 2098, loss 0.140866, acc 0.9375, learning_rate 0.000100923
2017-10-10T11:19:12.382641: step 2099, loss 0.0567883, acc 0.984375, learning_rate 0.000100919
2017-10-10T11:19:12.546362: step 2100, loss 0.137127, acc 0.96875, learning_rate 0.000100916
2017-10-10T11:19:12.706795: step 2101, loss 0.0462911, acc 0.984375, learning_rate 0.000100912
2017-10-10T11:19:12.881337: step 2102, loss 0.0141837, acc 1, learning_rate 0.000100908
2017-10-10T11:19:13.042618: step 2103, loss 0.0681472, acc 0.96875, learning_rate 0.000100904
2017-10-10T11:19:13.203668: step 2104, loss 0.213346, acc 0.921875, learning_rate 0.000100901
2017-10-10T11:19:13.364021: step 2105, loss 0.153899, acc 0.953125, learning_rate 0.000100897
2017-10-10T11:19:13.528585: step 2106, loss 0.0834824, acc 0.984375, learning_rate 0.000100893
2017-10-10T11:19:13.689830: step 2107, loss 0.117863, acc 0.9375, learning_rate 0.00010089
2017-10-10T11:19:13.849309: step 2108, loss 0.1257, acc 0.96875, learning_rate 0.000100886
2017-10-10T11:19:14.009804: step 2109, loss 0.0901807, acc 0.96875, learning_rate 0.000100883
2017-10-10T11:19:14.168780: step 2110, loss 0.0614745, acc 0.984375, learning_rate 0.000100879
2017-10-10T11:19:14.332859: step 2111, loss 0.173285, acc 0.953125, learning_rate 0.000100875
2017-10-10T11:19:14.496523: step 2112, loss 0.205169, acc 0.96875, learning_rate 0.000100872
2017-10-10T11:19:14.660480: step 2113, loss 0.166785, acc 0.9375, learning_rate 0.000100868
2017-10-10T11:19:14.821775: step 2114, loss 0.210586, acc 0.890625, learning_rate 0.000100865
2017-10-10T11:19:14.984748: step 2115, loss 0.0680435, acc 0.96875, learning_rate 0.000100861
2017-10-10T11:19:15.147264: step 2116, loss 0.077126, acc 0.96875, learning_rate 0.000100858
2017-10-10T11:19:15.308215: step 2117, loss 0.0912601, acc 0.984375, learning_rate 0.000100854
2017-10-10T11:19:15.470436: step 2118, loss 0.0819607, acc 0.953125, learning_rate 0.000100851
2017-10-10T11:19:15.631692: step 2119, loss 0.164235, acc 0.953125, learning_rate 0.000100847
2017-10-10T11:19:15.793232: step 2120, loss 0.122925, acc 0.96875, learning_rate 0.000100844

Evaluation:
2017-10-10T11:19:16.244766: step 2120, loss 0.21497, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2120

2017-10-10T11:19:16.883337: step 2121, loss 0.0468064, acc 1, learning_rate 0.00010084
2017-10-10T11:19:17.043438: step 2122, loss 0.28153, acc 0.890625, learning_rate 0.000100837
2017-10-10T11:19:17.206092: step 2123, loss 0.0759807, acc 0.96875, learning_rate 0.000100833
2017-10-10T11:19:17.366769: step 2124, loss 0.0397594, acc 1, learning_rate 0.00010083
2017-10-10T11:19:17.531591: step 2125, loss 0.0749314, acc 0.96875, learning_rate 0.000100827
2017-10-10T11:19:17.695797: step 2126, loss 0.0824293, acc 0.96875, learning_rate 0.000100823
2017-10-10T11:19:17.858171: step 2127, loss 0.0744105, acc 0.984375, learning_rate 0.00010082
2017-10-10T11:19:18.020143: step 2128, loss 0.15879, acc 0.96875, learning_rate 0.000100817
2017-10-10T11:19:18.181774: step 2129, loss 0.0961742, acc 0.953125, learning_rate 0.000100813
2017-10-10T11:19:18.343764: step 2130, loss 0.134698, acc 0.96875, learning_rate 0.00010081
2017-10-10T11:19:18.507009: step 2131, loss 0.170616, acc 0.921875, learning_rate 0.000100807
2017-10-10T11:19:18.669730: step 2132, loss 0.198534, acc 0.9375, learning_rate 0.000100803
2017-10-10T11:19:18.830379: step 2133, loss 0.17727, acc 0.921875, learning_rate 0.0001008
2017-10-10T11:19:18.997533: step 2134, loss 0.173928, acc 0.921875, learning_rate 0.000100797
2017-10-10T11:19:19.160561: step 2135, loss 0.0316748, acc 1, learning_rate 0.000100793
2017-10-10T11:19:19.322898: step 2136, loss 0.14228, acc 0.953125, learning_rate 0.00010079
2017-10-10T11:19:19.482919: step 2137, loss 0.266374, acc 0.921875, learning_rate 0.000100787
2017-10-10T11:19:19.645662: step 2138, loss 0.0659071, acc 0.984375, learning_rate 0.000100784
2017-10-10T11:19:19.804787: step 2139, loss 0.100446, acc 0.953125, learning_rate 0.000100781
2017-10-10T11:19:19.981659: step 2140, loss 0.136422, acc 0.9375, learning_rate 0.000100777
2017-10-10T11:19:20.143050: step 2141, loss 0.222019, acc 0.90625, learning_rate 0.000100774
2017-10-10T11:19:20.309064: step 2142, loss 0.0574573, acc 1, learning_rate 0.000100771
2017-10-10T11:19:20.470046: step 2143, loss 0.104608, acc 0.96875, learning_rate 0.000100768
2017-10-10T11:19:20.630901: step 2144, loss 0.0420758, acc 0.984375, learning_rate 0.000100765
2017-10-10T11:19:20.789142: step 2145, loss 0.126913, acc 0.953125, learning_rate 0.000100762
2017-10-10T11:19:20.953026: step 2146, loss 0.206611, acc 0.921875, learning_rate 0.000100759
2017-10-10T11:19:21.115726: step 2147, loss 0.045704, acc 1, learning_rate 0.000100755
2017-10-10T11:19:21.277934: step 2148, loss 0.0936097, acc 0.96875, learning_rate 0.000100752
2017-10-10T11:19:21.441960: step 2149, loss 0.153729, acc 0.9375, learning_rate 0.000100749
2017-10-10T11:19:21.603935: step 2150, loss 0.108936, acc 0.9375, learning_rate 0.000100746
2017-10-10T11:19:21.767167: step 2151, loss 0.109511, acc 0.96875, learning_rate 0.000100743
2017-10-10T11:19:21.933789: step 2152, loss 0.194582, acc 0.921875, learning_rate 0.00010074
2017-10-10T11:19:22.096009: step 2153, loss 0.099474, acc 0.953125, learning_rate 0.000100737
2017-10-10T11:19:22.258402: step 2154, loss 0.15097, acc 0.90625, learning_rate 0.000100734
2017-10-10T11:19:22.419659: step 2155, loss 0.0814839, acc 0.96875, learning_rate 0.000100731
2017-10-10T11:19:22.554867: step 2156, loss 0.133979, acc 0.941176, learning_rate 0.000100728
2017-10-10T11:19:22.724589: step 2157, loss 0.141032, acc 0.9375, learning_rate 0.000100725
2017-10-10T11:19:22.888057: step 2158, loss 0.062571, acc 0.984375, learning_rate 0.000100722
2017-10-10T11:19:23.049002: step 2159, loss 0.0701873, acc 0.96875, learning_rate 0.000100719
2017-10-10T11:19:23.210391: step 2160, loss 0.115294, acc 0.953125, learning_rate 0.000100716

Evaluation:
2017-10-10T11:19:23.658479: step 2160, loss 0.212745, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2160

2017-10-10T11:19:24.369110: step 2161, loss 0.10472, acc 0.96875, learning_rate 0.000100713
2017-10-10T11:19:24.527040: step 2162, loss 0.102271, acc 0.96875, learning_rate 0.000100711
2017-10-10T11:19:24.690536: step 2163, loss 0.060481, acc 0.96875, learning_rate 0.000100708
2017-10-10T11:19:24.855401: step 2164, loss 0.212229, acc 0.921875, learning_rate 0.000100705
2017-10-10T11:19:25.020476: step 2165, loss 0.191299, acc 0.90625, learning_rate 0.000100702
2017-10-10T11:19:25.180331: step 2166, loss 0.1057, acc 0.96875, learning_rate 0.000100699
2017-10-10T11:19:25.341792: step 2167, loss 0.0713058, acc 0.984375, learning_rate 0.000100696
2017-10-10T11:19:25.502770: step 2168, loss 0.168378, acc 0.96875, learning_rate 0.000100693
2017-10-10T11:19:25.663665: step 2169, loss 0.146797, acc 0.9375, learning_rate 0.00010069
2017-10-10T11:19:25.826744: step 2170, loss 0.108967, acc 0.96875, learning_rate 0.000100688
2017-10-10T11:19:25.990462: step 2171, loss 0.139903, acc 0.9375, learning_rate 0.000100685
2017-10-10T11:19:26.151808: step 2172, loss 0.127026, acc 0.953125, learning_rate 0.000100682
2017-10-10T11:19:26.312138: step 2173, loss 0.130256, acc 0.96875, learning_rate 0.000100679
2017-10-10T11:19:26.472954: step 2174, loss 0.192557, acc 0.921875, learning_rate 0.000100677
2017-10-10T11:19:26.636054: step 2175, loss 0.139361, acc 0.953125, learning_rate 0.000100674
2017-10-10T11:19:26.798104: step 2176, loss 0.174349, acc 0.953125, learning_rate 0.000100671
2017-10-10T11:19:26.960014: step 2177, loss 0.0164101, acc 1, learning_rate 0.000100668
2017-10-10T11:19:27.121598: step 2178, loss 0.0698753, acc 0.984375, learning_rate 0.000100666
2017-10-10T11:19:27.280148: step 2179, loss 0.18281, acc 0.90625, learning_rate 0.000100663
2017-10-10T11:19:27.444180: step 2180, loss 0.147265, acc 0.96875, learning_rate 0.00010066
2017-10-10T11:19:27.605404: step 2181, loss 0.170599, acc 0.921875, learning_rate 0.000100657
2017-10-10T11:19:27.765467: step 2182, loss 0.0947263, acc 0.953125, learning_rate 0.000100655
2017-10-10T11:19:27.939089: step 2183, loss 0.111292, acc 0.953125, learning_rate 0.000100652
2017-10-10T11:19:28.101706: step 2184, loss 0.104197, acc 0.9375, learning_rate 0.000100649
2017-10-10T11:19:28.264933: step 2185, loss 0.0314364, acc 1, learning_rate 0.000100647
2017-10-10T11:19:28.427685: step 2186, loss 0.184739, acc 0.96875, learning_rate 0.000100644
2017-10-10T11:19:28.590419: step 2187, loss 0.0814432, acc 0.96875, learning_rate 0.000100641
2017-10-10T11:19:28.752306: step 2188, loss 0.251784, acc 0.90625, learning_rate 0.000100639
2017-10-10T11:19:28.916582: step 2189, loss 0.0632783, acc 0.984375, learning_rate 0.000100636
2017-10-10T11:19:29.074744: step 2190, loss 0.0483933, acc 0.984375, learning_rate 0.000100634
2017-10-10T11:19:29.238719: step 2191, loss 0.103392, acc 0.96875, learning_rate 0.000100631
2017-10-10T11:19:29.399267: step 2192, loss 0.129153, acc 0.96875, learning_rate 0.000100628
2017-10-10T11:19:29.559356: step 2193, loss 0.316109, acc 0.921875, learning_rate 0.000100626
2017-10-10T11:19:29.722522: step 2194, loss 0.0851781, acc 0.984375, learning_rate 0.000100623
2017-10-10T11:19:29.890008: step 2195, loss 0.071528, acc 0.96875, learning_rate 0.000100621
2017-10-10T11:19:30.050946: step 2196, loss 0.223872, acc 0.9375, learning_rate 0.000100618
2017-10-10T11:19:30.215447: step 2197, loss 0.0811166, acc 0.96875, learning_rate 0.000100616
2017-10-10T11:19:30.378575: step 2198, loss 0.103646, acc 0.96875, learning_rate 0.000100613
2017-10-10T11:19:30.541284: step 2199, loss 0.0657118, acc 0.96875, learning_rate 0.000100611
2017-10-10T11:19:30.704811: step 2200, loss 0.129019, acc 0.921875, learning_rate 0.000100608

Evaluation:
2017-10-10T11:19:31.151303: step 2200, loss 0.21225, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2200

2017-10-10T11:19:31.723126: step 2201, loss 0.0762878, acc 0.96875, learning_rate 0.000100606
2017-10-10T11:19:31.888229: step 2202, loss 0.120664, acc 0.96875, learning_rate 0.000100603
2017-10-10T11:19:32.051122: step 2203, loss 0.0830406, acc 0.953125, learning_rate 0.000100601
2017-10-10T11:19:32.213097: step 2204, loss 0.0650917, acc 0.984375, learning_rate 0.000100598
2017-10-10T11:19:32.377625: step 2205, loss 0.0910173, acc 0.96875, learning_rate 0.000100596
2017-10-10T11:19:32.539266: step 2206, loss 0.086676, acc 0.96875, learning_rate 0.000100594
2017-10-10T11:19:32.702445: step 2207, loss 0.0802113, acc 0.96875, learning_rate 0.000100591
2017-10-10T11:19:32.867989: step 2208, loss 0.0337155, acc 1, learning_rate 0.000100589
2017-10-10T11:19:33.030834: step 2209, loss 0.154683, acc 0.9375, learning_rate 0.000100586
2017-10-10T11:19:33.190104: step 2210, loss 0.0962864, acc 0.953125, learning_rate 0.000100584
2017-10-10T11:19:33.352926: step 2211, loss 0.0496294, acc 0.984375, learning_rate 0.000100581
2017-10-10T11:19:33.515411: step 2212, loss 0.0838259, acc 0.984375, learning_rate 0.000100579
2017-10-10T11:19:33.677797: step 2213, loss 0.147242, acc 0.96875, learning_rate 0.000100577
2017-10-10T11:19:33.839912: step 2214, loss 0.0636856, acc 0.96875, learning_rate 0.000100574
2017-10-10T11:19:34.000536: step 2215, loss 0.0844585, acc 0.984375, learning_rate 0.000100572
2017-10-10T11:19:34.163397: step 2216, loss 0.115071, acc 0.984375, learning_rate 0.00010057
2017-10-10T11:19:34.324809: step 2217, loss 0.0706643, acc 0.984375, learning_rate 0.000100567
2017-10-10T11:19:34.487771: step 2218, loss 0.169725, acc 0.953125, learning_rate 0.000100565
2017-10-10T11:19:34.650706: step 2219, loss 0.214946, acc 0.9375, learning_rate 0.000100563
2017-10-10T11:19:34.812865: step 2220, loss 0.0665409, acc 0.96875, learning_rate 0.00010056
2017-10-10T11:19:34.976971: step 2221, loss 0.121087, acc 0.9375, learning_rate 0.000100558
2017-10-10T11:19:35.138118: step 2222, loss 0.155021, acc 0.9375, learning_rate 0.000100556
2017-10-10T11:19:35.298800: step 2223, loss 0.143899, acc 0.96875, learning_rate 0.000100554
2017-10-10T11:19:35.460613: step 2224, loss 0.0831066, acc 0.96875, learning_rate 0.000100551
2017-10-10T11:19:35.621941: step 2225, loss 0.08698, acc 0.984375, learning_rate 0.000100549
2017-10-10T11:19:35.781285: step 2226, loss 0.07629, acc 0.984375, learning_rate 0.000100547
2017-10-10T11:19:35.953645: step 2227, loss 0.0407575, acc 0.984375, learning_rate 0.000100545
2017-10-10T11:19:36.117633: step 2228, loss 0.18477, acc 0.9375, learning_rate 0.000100542
2017-10-10T11:19:36.278663: step 2229, loss 0.0791593, acc 0.96875, learning_rate 0.00010054
2017-10-10T11:19:36.442693: step 2230, loss 0.0679206, acc 0.96875, learning_rate 0.000100538
2017-10-10T11:19:36.606063: step 2231, loss 0.0683906, acc 0.984375, learning_rate 0.000100536
2017-10-10T11:19:36.768968: step 2232, loss 0.043036, acc 1, learning_rate 0.000100534
2017-10-10T11:19:36.934953: step 2233, loss 0.195773, acc 0.921875, learning_rate 0.000100531
2017-10-10T11:19:37.096092: step 2234, loss 0.174811, acc 0.921875, learning_rate 0.000100529
2017-10-10T11:19:37.258322: step 2235, loss 0.184568, acc 0.9375, learning_rate 0.000100527
2017-10-10T11:19:37.417935: step 2236, loss 0.19858, acc 0.9375, learning_rate 0.000100525
2017-10-10T11:19:37.579292: step 2237, loss 0.102335, acc 0.96875, learning_rate 0.000100523
2017-10-10T11:19:37.741668: step 2238, loss 0.0919022, acc 0.953125, learning_rate 0.000100521
2017-10-10T11:19:37.912109: step 2239, loss 0.148756, acc 0.953125, learning_rate 0.000100519
2017-10-10T11:19:38.074347: step 2240, loss 0.0495602, acc 1, learning_rate 0.000100516

Evaluation:
2017-10-10T11:19:38.544555: step 2240, loss 0.214964, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2240

2017-10-10T11:19:39.199395: step 2241, loss 0.128042, acc 0.921875, learning_rate 0.000100514
2017-10-10T11:19:39.360944: step 2242, loss 0.0748154, acc 0.984375, learning_rate 0.000100512
2017-10-10T11:19:39.521956: step 2243, loss 0.102329, acc 0.9375, learning_rate 0.00010051
2017-10-10T11:19:39.685754: step 2244, loss 0.0741331, acc 0.953125, learning_rate 0.000100508
2017-10-10T11:19:39.847195: step 2245, loss 0.0993873, acc 0.96875, learning_rate 0.000100506
2017-10-10T11:19:40.009478: step 2246, loss 0.11579, acc 0.921875, learning_rate 0.000100504
2017-10-10T11:19:40.171820: step 2247, loss 0.133303, acc 0.984375, learning_rate 0.000100502
2017-10-10T11:19:40.335196: step 2248, loss 0.178893, acc 0.90625, learning_rate 0.0001005
2017-10-10T11:19:40.499120: step 2249, loss 0.109249, acc 0.96875, learning_rate 0.000100498
2017-10-10T11:19:40.662285: step 2250, loss 0.121597, acc 0.953125, learning_rate 0.000100496
2017-10-10T11:19:40.823510: step 2251, loss 0.352916, acc 0.890625, learning_rate 0.000100494
2017-10-10T11:19:40.992800: step 2252, loss 0.161506, acc 0.9375, learning_rate 0.000100492
2017-10-10T11:19:41.154871: step 2253, loss 0.112761, acc 0.953125, learning_rate 0.00010049
2017-10-10T11:19:41.291940: step 2254, loss 0.202814, acc 0.960784, learning_rate 0.000100488
2017-10-10T11:19:41.454564: step 2255, loss 0.151087, acc 0.96875, learning_rate 0.000100486
2017-10-10T11:19:41.617919: step 2256, loss 0.221497, acc 0.890625, learning_rate 0.000100484
2017-10-10T11:19:41.783926: step 2257, loss 0.0887826, acc 0.984375, learning_rate 0.000100482
2017-10-10T11:19:41.948847: step 2258, loss 0.0525656, acc 0.96875, learning_rate 0.00010048
2017-10-10T11:19:42.109572: step 2259, loss 0.101054, acc 0.96875, learning_rate 0.000100478
2017-10-10T11:19:42.270589: step 2260, loss 0.0257377, acc 1, learning_rate 0.000100476
2017-10-10T11:19:42.432552: step 2261, loss 0.0202066, acc 1, learning_rate 0.000100474
2017-10-10T11:19:42.595168: step 2262, loss 0.139934, acc 0.921875, learning_rate 0.000100472
2017-10-10T11:19:42.757959: step 2263, loss 0.0698119, acc 0.984375, learning_rate 0.00010047
2017-10-10T11:19:42.922812: step 2264, loss 0.088588, acc 0.953125, learning_rate 0.000100468
2017-10-10T11:19:43.086892: step 2265, loss 0.0982755, acc 0.984375, learning_rate 0.000100466
2017-10-10T11:19:43.248449: step 2266, loss 0.105702, acc 0.96875, learning_rate 0.000100464
2017-10-10T11:19:43.411282: step 2267, loss 0.0868132, acc 0.953125, learning_rate 0.000100462
2017-10-10T11:19:43.571537: step 2268, loss 0.0985191, acc 0.96875, learning_rate 0.000100461
2017-10-10T11:19:43.734886: step 2269, loss 0.0580684, acc 0.984375, learning_rate 0.000100459
2017-10-10T11:19:43.898085: step 2270, loss 0.0981758, acc 0.953125, learning_rate 0.000100457
2017-10-10T11:19:44.060181: step 2271, loss 0.154489, acc 0.921875, learning_rate 0.000100455
2017-10-10T11:19:44.219979: step 2272, loss 0.119734, acc 0.953125, learning_rate 0.000100453
2017-10-10T11:19:44.384249: step 2273, loss 0.146889, acc 0.921875, learning_rate 0.000100451
2017-10-10T11:19:44.543058: step 2274, loss 0.138823, acc 0.953125, learning_rate 0.000100449
2017-10-10T11:19:44.705550: step 2275, loss 0.104542, acc 0.96875, learning_rate 0.000100448
2017-10-10T11:19:44.866325: step 2276, loss 0.134544, acc 0.953125, learning_rate 0.000100446
2017-10-10T11:19:45.027839: step 2277, loss 0.15082, acc 0.96875, learning_rate 0.000100444
2017-10-10T11:19:45.192039: step 2278, loss 0.200959, acc 0.921875, learning_rate 0.000100442
2017-10-10T11:19:45.355334: step 2279, loss 0.0679272, acc 0.984375, learning_rate 0.00010044
2017-10-10T11:19:45.519828: step 2280, loss 0.174749, acc 0.9375, learning_rate 0.000100439

Evaluation:
2017-10-10T11:19:45.998689: step 2280, loss 0.213687, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2280

2017-10-10T11:19:46.713061: step 2281, loss 0.252605, acc 0.953125, learning_rate 0.000100437
2017-10-10T11:19:46.878673: step 2282, loss 0.163781, acc 0.921875, learning_rate 0.000100435
2017-10-10T11:19:47.041006: step 2283, loss 0.0749395, acc 0.96875, learning_rate 0.000100433
2017-10-10T11:19:47.200169: step 2284, loss 0.0275966, acc 1, learning_rate 0.000100431
2017-10-10T11:19:47.361380: step 2285, loss 0.110097, acc 0.953125, learning_rate 0.00010043
2017-10-10T11:19:47.523802: step 2286, loss 0.143094, acc 0.953125, learning_rate 0.000100428
2017-10-10T11:19:47.687142: step 2287, loss 0.0745703, acc 0.984375, learning_rate 0.000100426
2017-10-10T11:19:47.848145: step 2288, loss 0.0908673, acc 0.96875, learning_rate 0.000100424
2017-10-10T11:19:48.015501: step 2289, loss 0.234109, acc 0.90625, learning_rate 0.000100423
2017-10-10T11:19:48.176630: step 2290, loss 0.0904471, acc 0.96875, learning_rate 0.000100421
2017-10-10T11:19:48.338268: step 2291, loss 0.168587, acc 0.9375, learning_rate 0.000100419
2017-10-10T11:19:48.501342: step 2292, loss 0.0704778, acc 0.96875, learning_rate 0.000100418
2017-10-10T11:19:48.664912: step 2293, loss 0.178709, acc 0.9375, learning_rate 0.000100416
2017-10-10T11:19:48.826778: step 2294, loss 0.0870714, acc 0.96875, learning_rate 0.000100414
2017-10-10T11:19:49.002379: step 2295, loss 0.0368039, acc 1, learning_rate 0.000100412
2017-10-10T11:19:49.162423: step 2296, loss 0.0587957, acc 0.984375, learning_rate 0.000100411
2017-10-10T11:19:49.325381: step 2297, loss 0.126464, acc 0.96875, learning_rate 0.000100409
2017-10-10T11:19:49.486813: step 2298, loss 0.159467, acc 0.9375, learning_rate 0.000100407
2017-10-10T11:19:49.649242: step 2299, loss 0.149797, acc 0.953125, learning_rate 0.000100406
2017-10-10T11:19:49.810958: step 2300, loss 0.0720881, acc 0.984375, learning_rate 0.000100404
2017-10-10T11:19:49.977030: step 2301, loss 0.104717, acc 0.96875, learning_rate 0.000100402
2017-10-10T11:19:50.139657: step 2302, loss 0.103005, acc 0.96875, learning_rate 0.000100401
2017-10-10T11:19:50.301241: step 2303, loss 0.0550623, acc 0.984375, learning_rate 0.000100399
2017-10-10T11:19:50.464331: step 2304, loss 0.0588248, acc 0.984375, learning_rate 0.000100398
2017-10-10T11:19:50.627406: step 2305, loss 0.0865685, acc 0.96875, learning_rate 0.000100396
2017-10-10T11:19:50.789501: step 2306, loss 0.0773264, acc 0.96875, learning_rate 0.000100394
2017-10-10T11:19:50.953641: step 2307, loss 0.0599249, acc 0.984375, learning_rate 0.000100393
2017-10-10T11:19:51.117704: step 2308, loss 0.166967, acc 0.953125, learning_rate 0.000100391
2017-10-10T11:19:51.281203: step 2309, loss 0.102642, acc 0.984375, learning_rate 0.000100389
2017-10-10T11:19:51.443806: step 2310, loss 0.12498, acc 0.953125, learning_rate 0.000100388
2017-10-10T11:19:51.603401: step 2311, loss 0.047741, acc 0.984375, learning_rate 0.000100386
2017-10-10T11:19:51.767299: step 2312, loss 0.043962, acc 1, learning_rate 0.000100385
2017-10-10T11:19:51.941026: step 2313, loss 0.11, acc 0.953125, learning_rate 0.000100383
2017-10-10T11:19:52.106786: step 2314, loss 0.117487, acc 0.96875, learning_rate 0.000100382
2017-10-10T11:19:52.269033: step 2315, loss 0.212078, acc 0.921875, learning_rate 0.00010038
2017-10-10T11:19:52.433513: step 2316, loss 0.153323, acc 0.921875, learning_rate 0.000100378
2017-10-10T11:19:52.594316: step 2317, loss 0.0419895, acc 1, learning_rate 0.000100377
2017-10-10T11:19:52.756381: step 2318, loss 0.174982, acc 0.9375, learning_rate 0.000100375
2017-10-10T11:19:52.920707: step 2319, loss 0.046826, acc 1, learning_rate 0.000100374
2017-10-10T11:19:53.080769: step 2320, loss 0.06429, acc 0.984375, learning_rate 0.000100372

Evaluation:
2017-10-10T11:19:53.532981: step 2320, loss 0.212214, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2320

2017-10-10T11:19:54.108374: step 2321, loss 0.0716793, acc 0.96875, learning_rate 0.000100371
2017-10-10T11:19:54.270493: step 2322, loss 0.0962717, acc 0.96875, learning_rate 0.000100369
2017-10-10T11:19:54.434601: step 2323, loss 0.111135, acc 0.953125, learning_rate 0.000100368
2017-10-10T11:19:54.600945: step 2324, loss 0.0340311, acc 0.984375, learning_rate 0.000100366
2017-10-10T11:19:54.771569: step 2325, loss 0.170184, acc 0.921875, learning_rate 0.000100365
2017-10-10T11:19:54.947754: step 2326, loss 0.0755734, acc 0.96875, learning_rate 0.000100363
2017-10-10T11:19:55.111115: step 2327, loss 0.138018, acc 0.953125, learning_rate 0.000100362
2017-10-10T11:19:55.270036: step 2328, loss 0.147171, acc 0.9375, learning_rate 0.00010036
2017-10-10T11:19:55.428279: step 2329, loss 0.0485741, acc 1, learning_rate 0.000100359
2017-10-10T11:19:55.590717: step 2330, loss 0.135945, acc 0.921875, learning_rate 0.000100357
2017-10-10T11:19:55.753828: step 2331, loss 0.061461, acc 0.96875, learning_rate 0.000100356
2017-10-10T11:19:55.919000: step 2332, loss 0.0243036, acc 1, learning_rate 0.000100354
2017-10-10T11:19:56.083058: step 2333, loss 0.117061, acc 0.953125, learning_rate 0.000100353
2017-10-10T11:19:56.245870: step 2334, loss 0.175714, acc 0.90625, learning_rate 0.000100352
2017-10-10T11:19:56.410070: step 2335, loss 0.144962, acc 0.9375, learning_rate 0.00010035
2017-10-10T11:19:56.571891: step 2336, loss 0.168071, acc 0.9375, learning_rate 0.000100349
2017-10-10T11:19:56.733529: step 2337, loss 0.120152, acc 0.96875, learning_rate 0.000100347
2017-10-10T11:19:56.901647: step 2338, loss 0.0562056, acc 1, learning_rate 0.000100346
2017-10-10T11:19:57.063210: step 2339, loss 0.196827, acc 0.9375, learning_rate 0.000100344
2017-10-10T11:19:57.223460: step 2340, loss 0.0800218, acc 0.984375, learning_rate 0.000100343
2017-10-10T11:19:57.387148: step 2341, loss 0.056936, acc 1, learning_rate 0.000100342
2017-10-10T11:19:57.548090: step 2342, loss 0.0768131, acc 0.953125, learning_rate 0.00010034
2017-10-10T11:19:57.711549: step 2343, loss 0.0628957, acc 0.96875, learning_rate 0.000100339
2017-10-10T11:19:57.878793: step 2344, loss 0.139297, acc 0.9375, learning_rate 0.000100338
2017-10-10T11:19:58.038903: step 2345, loss 0.168935, acc 0.9375, learning_rate 0.000100336
2017-10-10T11:19:58.201111: step 2346, loss 0.243, acc 0.921875, learning_rate 0.000100335
2017-10-10T11:19:58.364321: step 2347, loss 0.0692332, acc 0.96875, learning_rate 0.000100333
2017-10-10T11:19:58.523175: step 2348, loss 0.0812653, acc 0.984375, learning_rate 0.000100332
2017-10-10T11:19:58.685342: step 2349, loss 0.13203, acc 0.953125, learning_rate 0.000100331
2017-10-10T11:19:58.844906: step 2350, loss 0.0995422, acc 0.953125, learning_rate 0.000100329
2017-10-10T11:19:59.006059: step 2351, loss 0.155327, acc 0.953125, learning_rate 0.000100328
2017-10-10T11:19:59.143097: step 2352, loss 0.0248718, acc 1, learning_rate 0.000100327
2017-10-10T11:19:59.308357: step 2353, loss 0.136968, acc 0.953125, learning_rate 0.000100325
2017-10-10T11:19:59.469554: step 2354, loss 0.0557689, acc 0.984375, learning_rate 0.000100324
2017-10-10T11:19:59.630138: step 2355, loss 0.0240369, acc 1, learning_rate 0.000100323
2017-10-10T11:19:59.789531: step 2356, loss 0.11647, acc 0.953125, learning_rate 0.000100321
2017-10-10T11:19:59.953882: step 2357, loss 0.0751086, acc 0.953125, learning_rate 0.00010032
2017-10-10T11:20:00.117493: step 2358, loss 0.131551, acc 0.9375, learning_rate 0.000100319
2017-10-10T11:20:00.281892: step 2359, loss 0.109447, acc 0.96875, learning_rate 0.000100317
2017-10-10T11:20:00.443899: step 2360, loss 0.121099, acc 0.953125, learning_rate 0.000100316

Evaluation:
2017-10-10T11:20:00.920423: step 2360, loss 0.211939, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2360

2017-10-10T11:20:01.559474: step 2361, loss 0.0565363, acc 0.984375, learning_rate 0.000100315
2017-10-10T11:20:01.722137: step 2362, loss 0.121327, acc 0.96875, learning_rate 0.000100314
2017-10-10T11:20:01.885618: step 2363, loss 0.146086, acc 0.9375, learning_rate 0.000100312
2017-10-10T11:20:02.047828: step 2364, loss 0.182709, acc 0.953125, learning_rate 0.000100311
2017-10-10T11:20:02.208983: step 2365, loss 0.1078, acc 0.953125, learning_rate 0.00010031
2017-10-10T11:20:02.372616: step 2366, loss 0.12294, acc 0.953125, learning_rate 0.000100308
2017-10-10T11:20:02.536662: step 2367, loss 0.115623, acc 0.96875, learning_rate 0.000100307
2017-10-10T11:20:02.700832: step 2368, loss 0.163996, acc 0.953125, learning_rate 0.000100306
2017-10-10T11:20:02.866899: step 2369, loss 0.0513875, acc 0.984375, learning_rate 0.000100305
2017-10-10T11:20:03.029081: step 2370, loss 0.0474367, acc 1, learning_rate 0.000100303
2017-10-10T11:20:03.190841: step 2371, loss 0.0849892, acc 0.984375, learning_rate 0.000100302
2017-10-10T11:20:03.350460: step 2372, loss 0.0120814, acc 1, learning_rate 0.000100301
2017-10-10T11:20:03.512943: step 2373, loss 0.122445, acc 0.921875, learning_rate 0.0001003
2017-10-10T11:20:03.674279: step 2374, loss 0.0318964, acc 1, learning_rate 0.000100299
2017-10-10T11:20:03.836232: step 2375, loss 0.207618, acc 0.921875, learning_rate 0.000100297
2017-10-10T11:20:03.997160: step 2376, loss 0.106349, acc 0.96875, learning_rate 0.000100296
2017-10-10T11:20:04.159801: step 2377, loss 0.0894224, acc 0.96875, learning_rate 0.000100295
2017-10-10T11:20:04.319810: step 2378, loss 0.107445, acc 0.96875, learning_rate 0.000100294
2017-10-10T11:20:04.478589: step 2379, loss 0.0361061, acc 1, learning_rate 0.000100292
2017-10-10T11:20:04.639017: step 2380, loss 0.169131, acc 0.9375, learning_rate 0.000100291
2017-10-10T11:20:04.802533: step 2381, loss 0.0996498, acc 0.96875, learning_rate 0.00010029
2017-10-10T11:20:04.965325: step 2382, loss 0.0727006, acc 0.96875, learning_rate 0.000100289
2017-10-10T11:20:05.128323: step 2383, loss 0.153392, acc 0.921875, learning_rate 0.000100288
2017-10-10T11:20:05.289064: step 2384, loss 0.0760347, acc 1, learning_rate 0.000100287
2017-10-10T11:20:05.450860: step 2385, loss 0.0226556, acc 0.984375, learning_rate 0.000100285
2017-10-10T11:20:05.613759: step 2386, loss 0.0876311, acc 0.953125, learning_rate 0.000100284
2017-10-10T11:20:05.775630: step 2387, loss 0.260319, acc 0.953125, learning_rate 0.000100283
2017-10-10T11:20:05.939148: step 2388, loss 0.0378659, acc 1, learning_rate 0.000100282
2017-10-10T11:20:06.100725: step 2389, loss 0.0696026, acc 0.984375, learning_rate 0.000100281
2017-10-10T11:20:06.262876: step 2390, loss 0.259528, acc 0.921875, learning_rate 0.00010028
2017-10-10T11:20:06.423008: step 2391, loss 0.0830676, acc 0.96875, learning_rate 0.000100278
2017-10-10T11:20:06.585853: step 2392, loss 0.0608259, acc 0.984375, learning_rate 0.000100277
2017-10-10T11:20:06.753395: step 2393, loss 0.0593924, acc 0.96875, learning_rate 0.000100276
2017-10-10T11:20:06.917371: step 2394, loss 0.129398, acc 0.953125, learning_rate 0.000100275
2017-10-10T11:20:07.081330: step 2395, loss 0.0662343, acc 0.96875, learning_rate 0.000100274
2017-10-10T11:20:07.242239: step 2396, loss 0.111682, acc 0.9375, learning_rate 0.000100273
2017-10-10T11:20:07.406237: step 2397, loss 0.0766525, acc 0.984375, learning_rate 0.000100272
2017-10-10T11:20:07.569013: step 2398, loss 0.0944367, acc 0.984375, learning_rate 0.000100271
2017-10-10T11:20:07.734875: step 2399, loss 0.179793, acc 0.9375, learning_rate 0.00010027
2017-10-10T11:20:07.899038: step 2400, loss 0.0778583, acc 0.96875, learning_rate 0.000100268

Evaluation:
2017-10-10T11:20:08.357453: step 2400, loss 0.210498, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2400

2017-10-10T11:20:09.002075: step 2401, loss 0.0398821, acc 1, learning_rate 0.000100267
2017-10-10T11:20:09.167059: step 2402, loss 0.094993, acc 0.984375, learning_rate 0.000100266
2017-10-10T11:20:09.332720: step 2403, loss 0.0977782, acc 0.953125, learning_rate 0.000100265
2017-10-10T11:20:09.496182: step 2404, loss 0.127622, acc 0.953125, learning_rate 0.000100264
2017-10-10T11:20:09.660797: step 2405, loss 0.131386, acc 0.953125, learning_rate 0.000100263
2017-10-10T11:20:09.819898: step 2406, loss 0.221427, acc 0.921875, learning_rate 0.000100262
2017-10-10T11:20:09.982422: step 2407, loss 0.0744475, acc 0.984375, learning_rate 0.000100261
2017-10-10T11:20:10.146453: step 2408, loss 0.0787603, acc 0.953125, learning_rate 0.00010026
2017-10-10T11:20:10.311669: step 2409, loss 0.139855, acc 0.96875, learning_rate 0.000100259
2017-10-10T11:20:10.471992: step 2410, loss 0.104981, acc 0.96875, learning_rate 0.000100258
2017-10-10T11:20:10.633497: step 2411, loss 0.225341, acc 0.921875, learning_rate 0.000100257
2017-10-10T11:20:10.795517: step 2412, loss 0.0860465, acc 0.96875, learning_rate 0.000100256
2017-10-10T11:20:10.962010: step 2413, loss 0.11565, acc 0.953125, learning_rate 0.000100255
2017-10-10T11:20:11.127129: step 2414, loss 0.085352, acc 0.984375, learning_rate 0.000100253
2017-10-10T11:20:11.289766: step 2415, loss 0.0475041, acc 0.984375, learning_rate 0.000100252
2017-10-10T11:20:11.449189: step 2416, loss 0.150919, acc 0.953125, learning_rate 0.000100251
2017-10-10T11:20:11.608894: step 2417, loss 0.0724853, acc 0.953125, learning_rate 0.00010025
2017-10-10T11:20:11.773573: step 2418, loss 0.107765, acc 0.953125, learning_rate 0.000100249
2017-10-10T11:20:11.938808: step 2419, loss 0.127461, acc 0.953125, learning_rate 0.000100248
2017-10-10T11:20:12.098817: step 2420, loss 0.204857, acc 0.9375, learning_rate 0.000100247
2017-10-10T11:20:12.259762: step 2421, loss 0.0521301, acc 0.984375, learning_rate 0.000100246
2017-10-10T11:20:12.421216: step 2422, loss 0.0687414, acc 0.984375, learning_rate 0.000100245
2017-10-10T11:20:12.580901: step 2423, loss 0.103572, acc 0.953125, learning_rate 0.000100244
2017-10-10T11:20:12.743381: step 2424, loss 0.0877064, acc 0.96875, learning_rate 0.000100243
2017-10-10T11:20:12.907879: step 2425, loss 0.211153, acc 0.90625, learning_rate 0.000100242
2017-10-10T11:20:13.069636: step 2426, loss 0.0465417, acc 0.984375, learning_rate 0.000100241
2017-10-10T11:20:13.231851: step 2427, loss 0.102576, acc 0.96875, learning_rate 0.00010024
2017-10-10T11:20:13.393937: step 2428, loss 0.0868332, acc 0.96875, learning_rate 0.000100239
2017-10-10T11:20:13.555185: step 2429, loss 0.119485, acc 0.953125, learning_rate 0.000100238
2017-10-10T11:20:13.717747: step 2430, loss 0.0546355, acc 0.984375, learning_rate 0.000100237
2017-10-10T11:20:13.881825: step 2431, loss 0.082476, acc 0.96875, learning_rate 0.000100236
2017-10-10T11:20:14.044368: step 2432, loss 0.0650251, acc 0.984375, learning_rate 0.000100235
2017-10-10T11:20:14.205729: step 2433, loss 0.0837904, acc 0.984375, learning_rate 0.000100235
2017-10-10T11:20:14.370716: step 2434, loss 0.054278, acc 0.96875, learning_rate 0.000100234
2017-10-10T11:20:14.536662: step 2435, loss 0.120496, acc 0.953125, learning_rate 0.000100233
2017-10-10T11:20:14.698111: step 2436, loss 0.0982514, acc 0.953125, learning_rate 0.000100232
2017-10-10T11:20:14.863874: step 2437, loss 0.118101, acc 0.953125, learning_rate 0.000100231
2017-10-10T11:20:15.027028: step 2438, loss 0.0889367, acc 0.96875, learning_rate 0.00010023
2017-10-10T11:20:15.190888: step 2439, loss 0.13201, acc 0.953125, learning_rate 0.000100229
2017-10-10T11:20:15.353583: step 2440, loss 0.204333, acc 0.921875, learning_rate 0.000100228

Evaluation:
2017-10-10T11:20:15.817087: step 2440, loss 0.212552, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2440

2017-10-10T11:20:16.539117: step 2441, loss 0.0586335, acc 0.984375, learning_rate 0.000100227
2017-10-10T11:20:16.702844: step 2442, loss 0.178726, acc 0.921875, learning_rate 0.000100226
2017-10-10T11:20:16.868308: step 2443, loss 0.0367872, acc 0.984375, learning_rate 0.000100225
2017-10-10T11:20:17.032230: step 2444, loss 0.181491, acc 0.953125, learning_rate 0.000100224
2017-10-10T11:20:17.193118: step 2445, loss 0.156303, acc 0.96875, learning_rate 0.000100223
2017-10-10T11:20:17.356606: step 2446, loss 0.130445, acc 0.953125, learning_rate 0.000100222
2017-10-10T11:20:17.517875: step 2447, loss 0.102007, acc 0.984375, learning_rate 0.000100221
2017-10-10T11:20:17.681987: step 2448, loss 0.0796799, acc 0.96875, learning_rate 0.000100221
2017-10-10T11:20:17.846853: step 2449, loss 0.256881, acc 0.875, learning_rate 0.00010022
2017-10-10T11:20:17.995351: step 2450, loss 0.118938, acc 0.960784, learning_rate 0.000100219
2017-10-10T11:20:18.160552: step 2451, loss 0.135738, acc 0.9375, learning_rate 0.000100218
2017-10-10T11:20:18.324288: step 2452, loss 0.119848, acc 0.96875, learning_rate 0.000100217
2017-10-10T11:20:18.487879: step 2453, loss 0.165311, acc 0.9375, learning_rate 0.000100216
2017-10-10T11:20:18.650100: step 2454, loss 0.0475114, acc 0.984375, learning_rate 0.000100215
2017-10-10T11:20:18.814119: step 2455, loss 0.198607, acc 0.96875, learning_rate 0.000100214
2017-10-10T11:20:18.979740: step 2456, loss 0.108346, acc 0.953125, learning_rate 0.000100213
2017-10-10T11:20:19.142734: step 2457, loss 0.131593, acc 0.984375, learning_rate 0.000100213
2017-10-10T11:20:19.304748: step 2458, loss 0.109985, acc 0.953125, learning_rate 0.000100212
2017-10-10T11:20:19.469362: step 2459, loss 0.0973768, acc 0.96875, learning_rate 0.000100211
2017-10-10T11:20:19.630047: step 2460, loss 0.113038, acc 0.953125, learning_rate 0.00010021
2017-10-10T11:20:19.794334: step 2461, loss 0.0812303, acc 0.96875, learning_rate 0.000100209
2017-10-10T11:20:19.970627: step 2462, loss 0.128602, acc 0.96875, learning_rate 0.000100208
2017-10-10T11:20:20.134928: step 2463, loss 0.11059, acc 0.96875, learning_rate 0.000100207
2017-10-10T11:20:20.295813: step 2464, loss 0.144999, acc 0.953125, learning_rate 0.000100207
2017-10-10T11:20:20.457917: step 2465, loss 0.206646, acc 0.9375, learning_rate 0.000100206
2017-10-10T11:20:20.618068: step 2466, loss 0.126851, acc 0.953125, learning_rate 0.000100205
2017-10-10T11:20:20.777288: step 2467, loss 0.118995, acc 0.953125, learning_rate 0.000100204
2017-10-10T11:20:20.942533: step 2468, loss 0.100038, acc 0.953125, learning_rate 0.000100203
2017-10-10T11:20:21.104420: step 2469, loss 0.0733922, acc 0.96875, learning_rate 0.000100202
2017-10-10T11:20:21.266306: step 2470, loss 0.186776, acc 0.921875, learning_rate 0.000100202
2017-10-10T11:20:21.425969: step 2471, loss 0.0905529, acc 0.953125, learning_rate 0.000100201
2017-10-10T11:20:21.589641: step 2472, loss 0.113089, acc 0.953125, learning_rate 0.0001002
2017-10-10T11:20:21.755464: step 2473, loss 0.117053, acc 0.953125, learning_rate 0.000100199
2017-10-10T11:20:21.918438: step 2474, loss 0.133882, acc 0.96875, learning_rate 0.000100198
2017-10-10T11:20:22.080592: step 2475, loss 0.104884, acc 0.96875, learning_rate 0.000100198
2017-10-10T11:20:22.242958: step 2476, loss 0.0309262, acc 1, learning_rate 0.000100197
2017-10-10T11:20:22.402356: step 2477, loss 0.164676, acc 0.90625, learning_rate 0.000100196
2017-10-10T11:20:22.566302: step 2478, loss 0.135281, acc 0.953125, learning_rate 0.000100195
2017-10-10T11:20:22.729307: step 2479, loss 0.0670791, acc 0.984375, learning_rate 0.000100194
2017-10-10T11:20:22.891397: step 2480, loss 0.109811, acc 0.953125, learning_rate 0.000100194

Evaluation:
2017-10-10T11:20:23.364403: step 2480, loss 0.212643, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2480

2017-10-10T11:20:23.937919: step 2481, loss 0.144309, acc 0.921875, learning_rate 0.000100193
2017-10-10T11:20:24.100150: step 2482, loss 0.0887674, acc 0.984375, learning_rate 0.000100192
2017-10-10T11:20:24.262944: step 2483, loss 0.115777, acc 0.9375, learning_rate 0.000100191
2017-10-10T11:20:24.424085: step 2484, loss 0.036884, acc 0.984375, learning_rate 0.00010019
2017-10-10T11:20:24.581984: step 2485, loss 0.152984, acc 0.96875, learning_rate 0.00010019
2017-10-10T11:20:24.745842: step 2486, loss 0.126207, acc 0.96875, learning_rate 0.000100189
2017-10-10T11:20:24.918327: step 2487, loss 0.0895149, acc 0.96875, learning_rate 0.000100188
2017-10-10T11:20:25.081798: step 2488, loss 0.189867, acc 0.9375, learning_rate 0.000100187
2017-10-10T11:20:25.246614: step 2489, loss 0.126603, acc 0.96875, learning_rate 0.000100187
2017-10-10T11:20:25.408626: step 2490, loss 0.14357, acc 0.9375, learning_rate 0.000100186
2017-10-10T11:20:25.570225: step 2491, loss 0.0977142, acc 0.984375, learning_rate 0.000100185
2017-10-10T11:20:25.730265: step 2492, loss 0.108375, acc 0.953125, learning_rate 0.000100184
2017-10-10T11:20:25.899836: step 2493, loss 0.0404799, acc 1, learning_rate 0.000100183
2017-10-10T11:20:26.061752: step 2494, loss 0.0600973, acc 0.984375, learning_rate 0.000100183
2017-10-10T11:20:26.224961: step 2495, loss 0.099623, acc 0.953125, learning_rate 0.000100182
2017-10-10T11:20:26.387606: step 2496, loss 0.0585783, acc 0.984375, learning_rate 0.000100181
2017-10-10T11:20:26.551654: step 2497, loss 0.0446185, acc 0.984375, learning_rate 0.000100181
2017-10-10T11:20:26.712064: step 2498, loss 0.20655, acc 0.921875, learning_rate 0.00010018
2017-10-10T11:20:26.874214: step 2499, loss 0.14031, acc 0.953125, learning_rate 0.000100179
2017-10-10T11:20:27.036993: step 2500, loss 0.0914972, acc 0.984375, learning_rate 0.000100178
2017-10-10T11:20:27.197503: step 2501, loss 0.142796, acc 0.953125, learning_rate 0.000100178
2017-10-10T11:20:27.362031: step 2502, loss 0.0875345, acc 0.96875, learning_rate 0.000100177
2017-10-10T11:20:27.524166: step 2503, loss 0.111183, acc 0.96875, learning_rate 0.000100176
2017-10-10T11:20:27.685687: step 2504, loss 0.0726532, acc 1, learning_rate 0.000100175
2017-10-10T11:20:27.851870: step 2505, loss 0.0207679, acc 1, learning_rate 0.000100175
2017-10-10T11:20:28.015412: step 2506, loss 0.0946901, acc 0.96875, learning_rate 0.000100174
2017-10-10T11:20:28.181933: step 2507, loss 0.103978, acc 0.953125, learning_rate 0.000100173
2017-10-10T11:20:28.341224: step 2508, loss 0.220648, acc 0.875, learning_rate 0.000100173
2017-10-10T11:20:28.503039: step 2509, loss 0.0887319, acc 0.984375, learning_rate 0.000100172
2017-10-10T11:20:28.663655: step 2510, loss 0.101421, acc 0.96875, learning_rate 0.000100171
2017-10-10T11:20:28.826317: step 2511, loss 0.0487929, acc 0.984375, learning_rate 0.00010017
2017-10-10T11:20:28.992265: step 2512, loss 0.0837916, acc 0.96875, learning_rate 0.00010017
2017-10-10T11:20:29.151682: step 2513, loss 0.185539, acc 0.953125, learning_rate 0.000100169
2017-10-10T11:20:29.314203: step 2514, loss 0.0842197, acc 0.96875, learning_rate 0.000100168
2017-10-10T11:20:29.474771: step 2515, loss 0.046608, acc 1, learning_rate 0.000100168
2017-10-10T11:20:29.634412: step 2516, loss 0.0961561, acc 0.96875, learning_rate 0.000100167
2017-10-10T11:20:29.795880: step 2517, loss 0.0642336, acc 1, learning_rate 0.000100166
2017-10-10T11:20:29.961808: step 2518, loss 0.0737381, acc 0.984375, learning_rate 0.000100166
2017-10-10T11:20:30.120457: step 2519, loss 0.139678, acc 0.9375, learning_rate 0.000100165
2017-10-10T11:20:30.282679: step 2520, loss 0.0718887, acc 0.984375, learning_rate 0.000100164

Evaluation:
2017-10-10T11:20:30.744317: step 2520, loss 0.213615, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2520

2017-10-10T11:20:31.389061: step 2521, loss 0.0592906, acc 0.96875, learning_rate 0.000100164
2017-10-10T11:20:31.552297: step 2522, loss 0.147178, acc 0.953125, learning_rate 0.000100163
2017-10-10T11:20:31.714975: step 2523, loss 0.0445011, acc 0.984375, learning_rate 0.000100162
2017-10-10T11:20:31.881232: step 2524, loss 0.116693, acc 0.953125, learning_rate 0.000100162
2017-10-10T11:20:32.045166: step 2525, loss 0.0510224, acc 0.96875, learning_rate 0.000100161
2017-10-10T11:20:32.205143: step 2526, loss 0.0979611, acc 0.984375, learning_rate 0.00010016
2017-10-10T11:20:32.368623: step 2527, loss 0.078531, acc 0.953125, learning_rate 0.00010016
2017-10-10T11:20:32.531764: step 2528, loss 0.0534526, acc 1, learning_rate 0.000100159
2017-10-10T11:20:32.692638: step 2529, loss 0.155465, acc 0.96875, learning_rate 0.000100158
2017-10-10T11:20:32.860629: step 2530, loss 0.111656, acc 0.953125, learning_rate 0.000100158
2017-10-10T11:20:33.022640: step 2531, loss 0.15187, acc 0.96875, learning_rate 0.000100157
2017-10-10T11:20:33.185500: step 2532, loss 0.0731819, acc 0.96875, learning_rate 0.000100156
2017-10-10T11:20:33.346698: step 2533, loss 0.0503496, acc 0.984375, learning_rate 0.000100156
2017-10-10T11:20:33.509955: step 2534, loss 0.0434324, acc 0.984375, learning_rate 0.000100155
2017-10-10T11:20:33.677830: step 2535, loss 0.121735, acc 0.953125, learning_rate 0.000100155
2017-10-10T11:20:33.839473: step 2536, loss 0.0821789, acc 0.96875, learning_rate 0.000100154
2017-10-10T11:20:34.000238: step 2537, loss 0.16755, acc 0.9375, learning_rate 0.000100153
2017-10-10T11:20:34.163728: step 2538, loss 0.0775786, acc 0.984375, learning_rate 0.000100153
2017-10-10T11:20:34.327190: step 2539, loss 0.0257157, acc 1, learning_rate 0.000100152
2017-10-10T11:20:34.489692: step 2540, loss 0.131688, acc 0.96875, learning_rate 0.000100151
2017-10-10T11:20:34.648827: step 2541, loss 0.101242, acc 0.96875, learning_rate 0.000100151
2017-10-10T11:20:34.810119: step 2542, loss 0.041035, acc 0.984375, learning_rate 0.00010015
2017-10-10T11:20:34.973411: step 2543, loss 0.202355, acc 0.921875, learning_rate 0.00010015
2017-10-10T11:20:35.133946: step 2544, loss 0.0540769, acc 0.984375, learning_rate 0.000100149
2017-10-10T11:20:35.299061: step 2545, loss 0.0686684, acc 0.984375, learning_rate 0.000100148
2017-10-10T11:20:35.462666: step 2546, loss 0.236324, acc 0.90625, learning_rate 0.000100148
2017-10-10T11:20:35.622781: step 2547, loss 0.0852083, acc 0.96875, learning_rate 0.000100147
2017-10-10T11:20:35.757650: step 2548, loss 0.12117, acc 0.960784, learning_rate 0.000100147
2017-10-10T11:20:35.921194: step 2549, loss 0.181931, acc 0.921875, learning_rate 0.000100146
2017-10-10T11:20:36.085367: step 2550, loss 0.241814, acc 0.953125, learning_rate 0.000100145
2017-10-10T11:20:36.248285: step 2551, loss 0.187466, acc 0.953125, learning_rate 0.000100145
2017-10-10T11:20:36.412065: step 2552, loss 0.0775799, acc 0.984375, learning_rate 0.000100144
2017-10-10T11:20:36.572893: step 2553, loss 0.104277, acc 0.96875, learning_rate 0.000100144
2017-10-10T11:20:36.732792: step 2554, loss 0.281378, acc 0.90625, learning_rate 0.000100143
2017-10-10T11:20:36.901237: step 2555, loss 0.205944, acc 0.921875, learning_rate 0.000100142
2017-10-10T11:20:37.063029: step 2556, loss 0.115534, acc 0.96875, learning_rate 0.000100142
2017-10-10T11:20:37.226845: step 2557, loss 0.149766, acc 0.9375, learning_rate 0.000100141
2017-10-10T11:20:37.390231: step 2558, loss 0.0716916, acc 0.96875, learning_rate 0.000100141
2017-10-10T11:20:37.552706: step 2559, loss 0.033692, acc 0.984375, learning_rate 0.00010014
2017-10-10T11:20:37.713980: step 2560, loss 0.167086, acc 0.9375, learning_rate 0.00010014

Evaluation:
2017-10-10T11:20:38.174406: step 2560, loss 0.211924, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2560

2017-10-10T11:20:38.895048: step 2561, loss 0.104556, acc 0.984375, learning_rate 0.000100139
2017-10-10T11:20:39.059596: step 2562, loss 0.0811085, acc 0.984375, learning_rate 0.000100138
2017-10-10T11:20:39.223438: step 2563, loss 0.0527218, acc 1, learning_rate 0.000100138
2017-10-10T11:20:39.384383: step 2564, loss 0.0736433, acc 0.953125, learning_rate 0.000100137
2017-10-10T11:20:39.547488: step 2565, loss 0.018666, acc 1, learning_rate 0.000100137
2017-10-10T11:20:39.710450: step 2566, loss 0.258934, acc 0.90625, learning_rate 0.000100136
2017-10-10T11:20:39.873820: step 2567, loss 0.0824832, acc 0.984375, learning_rate 0.000100136
2017-10-10T11:20:40.032943: step 2568, loss 0.172149, acc 0.921875, learning_rate 0.000100135
2017-10-10T11:20:40.192820: step 2569, loss 0.0970656, acc 0.96875, learning_rate 0.000100134
2017-10-10T11:20:40.354957: step 2570, loss 0.0730033, acc 0.984375, learning_rate 0.000100134
2017-10-10T11:20:40.515547: step 2571, loss 0.0657862, acc 0.953125, learning_rate 0.000100133
2017-10-10T11:20:40.678017: step 2572, loss 0.0457282, acc 1, learning_rate 0.000100133
2017-10-10T11:20:40.845214: step 2573, loss 0.148682, acc 0.921875, learning_rate 0.000100132
2017-10-10T11:20:41.009859: step 2574, loss 0.0902974, acc 0.953125, learning_rate 0.000100132
2017-10-10T11:20:41.176071: step 2575, loss 0.0673246, acc 0.984375, learning_rate 0.000100131
2017-10-10T11:20:41.338758: step 2576, loss 0.115134, acc 0.9375, learning_rate 0.000100131
2017-10-10T11:20:41.504736: step 2577, loss 0.0870239, acc 0.984375, learning_rate 0.00010013
2017-10-10T11:20:41.666364: step 2578, loss 0.115672, acc 0.96875, learning_rate 0.00010013
2017-10-10T11:20:41.825862: step 2579, loss 0.095658, acc 0.96875, learning_rate 0.000100129
2017-10-10T11:20:41.989785: step 2580, loss 0.0346875, acc 1, learning_rate 0.000100129
2017-10-10T11:20:42.150407: step 2581, loss 0.133793, acc 0.96875, learning_rate 0.000100128
2017-10-10T11:20:42.312604: step 2582, loss 0.0578619, acc 0.984375, learning_rate 0.000100128
2017-10-10T11:20:42.476342: step 2583, loss 0.284855, acc 0.921875, learning_rate 0.000100127
2017-10-10T11:20:42.638368: step 2584, loss 0.166582, acc 0.953125, learning_rate 0.000100126
2017-10-10T11:20:42.799506: step 2585, loss 0.0996415, acc 0.953125, learning_rate 0.000100126
2017-10-10T11:20:42.968990: step 2586, loss 0.0632994, acc 0.984375, learning_rate 0.000100125
2017-10-10T11:20:43.130606: step 2587, loss 0.0763708, acc 0.96875, learning_rate 0.000100125
2017-10-10T11:20:43.289773: step 2588, loss 0.0671737, acc 0.96875, learning_rate 0.000100124
2017-10-10T11:20:43.451928: step 2589, loss 0.171479, acc 0.90625, learning_rate 0.000100124
2017-10-10T11:20:43.613696: step 2590, loss 0.0732331, acc 0.96875, learning_rate 0.000100123
2017-10-10T11:20:43.774535: step 2591, loss 0.168119, acc 0.921875, learning_rate 0.000100123
2017-10-10T11:20:43.940466: step 2592, loss 0.079776, acc 0.96875, learning_rate 0.000100122
2017-10-10T11:20:44.103884: step 2593, loss 0.0822646, acc 0.96875, learning_rate 0.000100122
2017-10-10T11:20:44.267465: step 2594, loss 0.158015, acc 0.953125, learning_rate 0.000100121
2017-10-10T11:20:44.427680: step 2595, loss 0.0821133, acc 0.953125, learning_rate 0.000100121
2017-10-10T11:20:44.592568: step 2596, loss 0.113033, acc 0.9375, learning_rate 0.00010012
2017-10-10T11:20:44.756837: step 2597, loss 0.019842, acc 1, learning_rate 0.00010012
2017-10-10T11:20:44.923958: step 2598, loss 0.113983, acc 0.96875, learning_rate 0.000100119
2017-10-10T11:20:45.086509: step 2599, loss 0.150003, acc 0.9375, learning_rate 0.000100119
2017-10-10T11:20:45.248943: step 2600, loss 0.0741675, acc 0.953125, learning_rate 0.000100118

Evaluation:
2017-10-10T11:20:45.706880: step 2600, loss 0.215053, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2600

2017-10-10T11:20:46.280139: step 2601, loss 0.216231, acc 0.9375, learning_rate 0.000100118
2017-10-10T11:20:46.441557: step 2602, loss 0.0656969, acc 0.96875, learning_rate 0.000100117
2017-10-10T11:20:46.603030: step 2603, loss 0.083394, acc 0.96875, learning_rate 0.000100117
2017-10-10T11:20:46.766333: step 2604, loss 0.0453284, acc 0.984375, learning_rate 0.000100117
2017-10-10T11:20:46.939513: step 2605, loss 0.0832895, acc 0.953125, learning_rate 0.000100116
2017-10-10T11:20:47.101797: step 2606, loss 0.0859727, acc 0.96875, learning_rate 0.000100116
2017-10-10T11:20:47.266636: step 2607, loss 0.149, acc 0.96875, learning_rate 0.000100115
2017-10-10T11:20:47.428265: step 2608, loss 0.272078, acc 0.921875, learning_rate 0.000100115
2017-10-10T11:20:47.591393: step 2609, loss 0.244621, acc 0.90625, learning_rate 0.000100114
2017-10-10T11:20:47.752686: step 2610, loss 0.0437654, acc 1, learning_rate 0.000100114
2017-10-10T11:20:47.914337: step 2611, loss 0.140559, acc 0.953125, learning_rate 0.000100113
2017-10-10T11:20:48.080152: step 2612, loss 0.10605, acc 0.9375, learning_rate 0.000100113
2017-10-10T11:20:48.240616: step 2613, loss 0.126848, acc 0.953125, learning_rate 0.000100112
2017-10-10T11:20:48.403281: step 2614, loss 0.207232, acc 0.90625, learning_rate 0.000100112
2017-10-10T11:20:48.565656: step 2615, loss 0.177217, acc 0.9375, learning_rate 0.000100111
2017-10-10T11:20:48.727766: step 2616, loss 0.136652, acc 0.9375, learning_rate 0.000100111
2017-10-10T11:20:48.892567: step 2617, loss 0.123329, acc 0.9375, learning_rate 0.000100111
2017-10-10T11:20:49.057890: step 2618, loss 0.0547181, acc 0.984375, learning_rate 0.00010011
2017-10-10T11:20:49.221577: step 2619, loss 0.14426, acc 0.9375, learning_rate 0.00010011
2017-10-10T11:20:49.385624: step 2620, loss 0.0989302, acc 0.984375, learning_rate 0.000100109
2017-10-10T11:20:49.549677: step 2621, loss 0.124234, acc 0.9375, learning_rate 0.000100109
2017-10-10T11:20:49.714055: step 2622, loss 0.13593, acc 0.96875, learning_rate 0.000100108
2017-10-10T11:20:49.877269: step 2623, loss 0.0881006, acc 0.96875, learning_rate 0.000100108
2017-10-10T11:20:50.039924: step 2624, loss 0.130566, acc 0.953125, learning_rate 0.000100107
2017-10-10T11:20:50.200612: step 2625, loss 0.0418001, acc 0.96875, learning_rate 0.000100107
2017-10-10T11:20:50.364979: step 2626, loss 0.161065, acc 0.953125, learning_rate 0.000100107
2017-10-10T11:20:50.527875: step 2627, loss 0.0774554, acc 0.96875, learning_rate 0.000100106
2017-10-10T11:20:50.696113: step 2628, loss 0.0966696, acc 0.953125, learning_rate 0.000100106
2017-10-10T11:20:50.861744: step 2629, loss 0.144546, acc 0.96875, learning_rate 0.000100105
2017-10-10T11:20:51.024301: step 2630, loss 0.120277, acc 0.96875, learning_rate 0.000100105
2017-10-10T11:20:51.185512: step 2631, loss 0.167248, acc 0.9375, learning_rate 0.000100104
2017-10-10T11:20:51.347410: step 2632, loss 0.265193, acc 0.90625, learning_rate 0.000100104
2017-10-10T11:20:51.507310: step 2633, loss 0.0975023, acc 0.953125, learning_rate 0.000100104
2017-10-10T11:20:51.670646: step 2634, loss 0.139524, acc 0.96875, learning_rate 0.000100103
2017-10-10T11:20:51.830947: step 2635, loss 0.0504921, acc 0.984375, learning_rate 0.000100103
2017-10-10T11:20:51.992625: step 2636, loss 0.21314, acc 0.9375, learning_rate 0.000100102
2017-10-10T11:20:52.153601: step 2637, loss 0.0587946, acc 0.984375, learning_rate 0.000100102
2017-10-10T11:20:52.315856: step 2638, loss 0.0501069, acc 0.984375, learning_rate 0.000100101
2017-10-10T11:20:52.479693: step 2639, loss 0.139122, acc 0.9375, learning_rate 0.000100101
2017-10-10T11:20:52.639253: step 2640, loss 0.134099, acc 0.96875, learning_rate 0.000100101

Evaluation:
2017-10-10T11:20:53.107293: step 2640, loss 0.208084, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2640

2017-10-10T11:20:53.744600: step 2641, loss 0.160744, acc 0.9375, learning_rate 0.0001001
2017-10-10T11:20:53.914014: step 2642, loss 0.0382314, acc 0.984375, learning_rate 0.0001001
2017-10-10T11:20:54.077475: step 2643, loss 0.142609, acc 0.953125, learning_rate 0.000100099
2017-10-10T11:20:54.239413: step 2644, loss 0.067768, acc 0.984375, learning_rate 0.000100099
2017-10-10T11:20:54.405416: step 2645, loss 0.0526638, acc 1, learning_rate 0.000100099
2017-10-10T11:20:54.543522: step 2646, loss 0.152292, acc 0.921569, learning_rate 0.000100098
2017-10-10T11:20:54.706647: step 2647, loss 0.136804, acc 0.953125, learning_rate 0.000100098
2017-10-10T11:20:54.872961: step 2648, loss 0.036521, acc 1, learning_rate 0.000100097
2017-10-10T11:20:55.035210: step 2649, loss 0.12731, acc 0.921875, learning_rate 0.000100097
2017-10-10T11:20:55.199762: step 2650, loss 0.105116, acc 0.96875, learning_rate 0.000100097
2017-10-10T11:20:55.362971: step 2651, loss 0.0892575, acc 0.96875, learning_rate 0.000100096
2017-10-10T11:20:55.526640: step 2652, loss 0.154878, acc 0.96875, learning_rate 0.000100096
2017-10-10T11:20:55.687919: step 2653, loss 0.101258, acc 0.984375, learning_rate 0.000100095
2017-10-10T11:20:55.851412: step 2654, loss 0.0543653, acc 0.984375, learning_rate 0.000100095
2017-10-10T11:20:56.011152: step 2655, loss 0.095248, acc 0.984375, learning_rate 0.000100095
2017-10-10T11:20:56.173894: step 2656, loss 0.0884342, acc 0.96875, learning_rate 0.000100094
2017-10-10T11:20:56.335046: step 2657, loss 0.133999, acc 0.984375, learning_rate 0.000100094
2017-10-10T11:20:56.493665: step 2658, loss 0.392115, acc 0.875, learning_rate 0.000100093
2017-10-10T11:20:56.655763: step 2659, loss 0.0684192, acc 0.984375, learning_rate 0.000100093
2017-10-10T11:20:56.818591: step 2660, loss 0.0497295, acc 0.984375, learning_rate 0.000100093
2017-10-10T11:20:56.979684: step 2661, loss 0.0915791, acc 0.953125, learning_rate 0.000100092
2017-10-10T11:20:57.142448: step 2662, loss 0.120819, acc 0.953125, learning_rate 0.000100092
2017-10-10T11:20:57.307058: step 2663, loss 0.105397, acc 0.9375, learning_rate 0.000100092
2017-10-10T11:20:57.466910: step 2664, loss 0.165576, acc 0.921875, learning_rate 0.000100091
2017-10-10T11:20:57.627619: step 2665, loss 0.112569, acc 0.984375, learning_rate 0.000100091
2017-10-10T11:20:57.787838: step 2666, loss 0.111519, acc 0.96875, learning_rate 0.00010009
2017-10-10T11:20:57.952828: step 2667, loss 0.0697102, acc 0.953125, learning_rate 0.00010009
2017-10-10T11:20:58.115052: step 2668, loss 0.150743, acc 0.9375, learning_rate 0.00010009
2017-10-10T11:20:58.276922: step 2669, loss 0.0932096, acc 0.96875, learning_rate 0.000100089
2017-10-10T11:20:58.440918: step 2670, loss 0.105541, acc 0.96875, learning_rate 0.000100089
2017-10-10T11:20:58.602676: step 2671, loss 0.131056, acc 0.9375, learning_rate 0.000100089
2017-10-10T11:20:58.765680: step 2672, loss 0.0932397, acc 0.96875, learning_rate 0.000100088
2017-10-10T11:20:58.931050: step 2673, loss 0.0770523, acc 0.984375, learning_rate 0.000100088
2017-10-10T11:20:59.095508: step 2674, loss 0.0919471, acc 0.96875, learning_rate 0.000100088
2017-10-10T11:20:59.260791: step 2675, loss 0.0676543, acc 0.96875, learning_rate 0.000100087
2017-10-10T11:20:59.425096: step 2676, loss 0.204559, acc 0.9375, learning_rate 0.000100087
2017-10-10T11:20:59.586429: step 2677, loss 0.0657071, acc 0.984375, learning_rate 0.000100086
2017-10-10T11:20:59.747982: step 2678, loss 0.150989, acc 0.90625, learning_rate 0.000100086
2017-10-10T11:20:59.917532: step 2679, loss 0.0258829, acc 1, learning_rate 0.000100086
2017-10-10T11:21:00.078490: step 2680, loss 0.0881645, acc 0.96875, learning_rate 0.000100085

Evaluation:
2017-10-10T11:21:00.535174: step 2680, loss 0.208424, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2680

2017-10-10T11:21:01.257219: step 2681, loss 0.177284, acc 0.9375, learning_rate 0.000100085
2017-10-10T11:21:01.419320: step 2682, loss 0.0628668, acc 0.953125, learning_rate 0.000100085
2017-10-10T11:21:01.579457: step 2683, loss 0.0618131, acc 0.984375, learning_rate 0.000100084
2017-10-10T11:21:01.743674: step 2684, loss 0.0797376, acc 0.953125, learning_rate 0.000100084
2017-10-10T11:21:01.907001: step 2685, loss 0.118455, acc 0.953125, learning_rate 0.000100084
2017-10-10T11:21:02.069771: step 2686, loss 0.0394937, acc 1, learning_rate 0.000100083
2017-10-10T11:21:02.231836: step 2687, loss 0.0879407, acc 0.953125, learning_rate 0.000100083
2017-10-10T11:21:02.391465: step 2688, loss 0.0917566, acc 0.96875, learning_rate 0.000100083
2017-10-10T11:21:02.552219: step 2689, loss 0.0475891, acc 1, learning_rate 0.000100082
2017-10-10T11:21:02.710845: step 2690, loss 0.156854, acc 0.9375, learning_rate 0.000100082
2017-10-10T11:21:02.873218: step 2691, loss 0.0824943, acc 0.96875, learning_rate 0.000100082
2017-10-10T11:21:03.036842: step 2692, loss 0.0527393, acc 0.984375, learning_rate 0.000100081
2017-10-10T11:21:03.198402: step 2693, loss 0.199867, acc 0.953125, learning_rate 0.000100081
2017-10-10T11:21:03.361865: step 2694, loss 0.12869, acc 0.953125, learning_rate 0.000100081
2017-10-10T11:21:03.523331: step 2695, loss 0.0960575, acc 0.96875, learning_rate 0.00010008
2017-10-10T11:21:03.683964: step 2696, loss 0.0588395, acc 0.984375, learning_rate 0.00010008
2017-10-10T11:21:03.845424: step 2697, loss 0.195258, acc 0.921875, learning_rate 0.00010008
2017-10-10T11:21:04.008064: step 2698, loss 0.263269, acc 0.875, learning_rate 0.000100079
2017-10-10T11:21:04.172032: step 2699, loss 0.0824961, acc 0.96875, learning_rate 0.000100079
2017-10-10T11:21:04.331900: step 2700, loss 0.0610852, acc 0.984375, learning_rate 0.000100079
2017-10-10T11:21:04.493964: step 2701, loss 0.0604321, acc 0.96875, learning_rate 0.000100078
2017-10-10T11:21:04.656941: step 2702, loss 0.0752344, acc 0.953125, learning_rate 0.000100078
2017-10-10T11:21:04.818448: step 2703, loss 0.117899, acc 0.953125, learning_rate 0.000100078
2017-10-10T11:21:04.981779: step 2704, loss 0.0950494, acc 0.96875, learning_rate 0.000100077
2017-10-10T11:21:05.144906: step 2705, loss 0.190516, acc 0.921875, learning_rate 0.000100077
2017-10-10T11:21:05.307614: step 2706, loss 0.156307, acc 0.9375, learning_rate 0.000100077
2017-10-10T11:21:05.472977: step 2707, loss 0.0680589, acc 0.984375, learning_rate 0.000100076
2017-10-10T11:21:05.636831: step 2708, loss 0.077717, acc 0.984375, learning_rate 0.000100076
2017-10-10T11:21:05.800625: step 2709, loss 0.0411694, acc 1, learning_rate 0.000100076
2017-10-10T11:21:05.970245: step 2710, loss 0.0408117, acc 1, learning_rate 0.000100076
2017-10-10T11:21:06.133741: step 2711, loss 0.0430404, acc 0.984375, learning_rate 0.000100075
2017-10-10T11:21:06.294640: step 2712, loss 0.0730643, acc 0.96875, learning_rate 0.000100075
2017-10-10T11:21:06.459321: step 2713, loss 0.228659, acc 0.90625, learning_rate 0.000100075
2017-10-10T11:21:06.622676: step 2714, loss 0.258325, acc 0.90625, learning_rate 0.000100074
2017-10-10T11:21:06.787566: step 2715, loss 0.0582685, acc 0.984375, learning_rate 0.000100074
2017-10-10T11:21:06.949982: step 2716, loss 0.0849296, acc 0.953125, learning_rate 0.000100074
2017-10-10T11:21:07.113864: step 2717, loss 0.0398972, acc 1, learning_rate 0.000100073
2017-10-10T11:21:07.277460: step 2718, loss 0.0649617, acc 0.984375, learning_rate 0.000100073
2017-10-10T11:21:07.442833: step 2719, loss 0.241255, acc 0.9375, learning_rate 0.000100073
2017-10-10T11:21:07.604490: step 2720, loss 0.0940551, acc 0.96875, learning_rate 0.000100073

Evaluation:
2017-10-10T11:21:08.058290: step 2720, loss 0.21084, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2720

2017-10-10T11:21:08.633989: step 2721, loss 0.124451, acc 0.953125, learning_rate 0.000100072
2017-10-10T11:21:08.797348: step 2722, loss 0.103123, acc 0.96875, learning_rate 0.000100072
2017-10-10T11:21:08.958669: step 2723, loss 0.0498005, acc 0.984375, learning_rate 0.000100072
2017-10-10T11:21:09.120209: step 2724, loss 0.0850102, acc 0.96875, learning_rate 0.000100071
2017-10-10T11:21:09.281867: step 2725, loss 0.0611411, acc 0.984375, learning_rate 0.000100071
2017-10-10T11:21:09.446776: step 2726, loss 0.0716716, acc 0.96875, learning_rate 0.000100071
2017-10-10T11:21:09.608195: step 2727, loss 0.0444283, acc 0.96875, learning_rate 0.00010007
2017-10-10T11:21:09.771244: step 2728, loss 0.0553698, acc 0.984375, learning_rate 0.00010007
2017-10-10T11:21:09.935150: step 2729, loss 0.217593, acc 0.921875, learning_rate 0.00010007
2017-10-10T11:21:10.098946: step 2730, loss 0.217765, acc 0.921875, learning_rate 0.00010007
2017-10-10T11:21:10.261572: step 2731, loss 0.0561116, acc 0.96875, learning_rate 0.000100069
2017-10-10T11:21:10.422052: step 2732, loss 0.125713, acc 0.9375, learning_rate 0.000100069
2017-10-10T11:21:10.581975: step 2733, loss 0.049815, acc 0.984375, learning_rate 0.000100069
2017-10-10T11:21:10.748612: step 2734, loss 0.148578, acc 0.96875, learning_rate 0.000100068
2017-10-10T11:21:10.916238: step 2735, loss 0.14126, acc 0.953125, learning_rate 0.000100068
2017-10-10T11:21:11.077416: step 2736, loss 0.0709805, acc 0.984375, learning_rate 0.000100068
2017-10-10T11:21:11.243779: step 2737, loss 0.0586771, acc 0.984375, learning_rate 0.000100068
2017-10-10T11:21:11.406341: step 2738, loss 0.18414, acc 0.90625, learning_rate 0.000100067
2017-10-10T11:21:11.568996: step 2739, loss 0.23376, acc 0.921875, learning_rate 0.000100067
2017-10-10T11:21:11.729668: step 2740, loss 0.0912217, acc 0.96875, learning_rate 0.000100067
2017-10-10T11:21:11.894639: step 2741, loss 0.111606, acc 0.96875, learning_rate 0.000100067
2017-10-10T11:21:12.057581: step 2742, loss 0.143744, acc 0.96875, learning_rate 0.000100066
2017-10-10T11:21:12.222091: step 2743, loss 0.0700785, acc 0.984375, learning_rate 0.000100066
2017-10-10T11:21:12.360368: step 2744, loss 0.0843631, acc 0.960784, learning_rate 0.000100066
2017-10-10T11:21:12.523517: step 2745, loss 0.189615, acc 0.953125, learning_rate 0.000100065
2017-10-10T11:21:12.684183: step 2746, loss 0.159184, acc 0.90625, learning_rate 0.000100065
2017-10-10T11:21:12.849367: step 2747, loss 0.187771, acc 0.9375, learning_rate 0.000100065
2017-10-10T11:21:13.011495: step 2748, loss 0.124423, acc 0.9375, learning_rate 0.000100065
2017-10-10T11:21:13.173414: step 2749, loss 0.102484, acc 0.9375, learning_rate 0.000100064
2017-10-10T11:21:13.336368: step 2750, loss 0.0476537, acc 0.96875, learning_rate 0.000100064
2017-10-10T11:21:13.499302: step 2751, loss 0.0865377, acc 0.96875, learning_rate 0.000100064
2017-10-10T11:21:13.662751: step 2752, loss 0.269313, acc 0.90625, learning_rate 0.000100064
2017-10-10T11:21:13.823859: step 2753, loss 0.0550222, acc 0.984375, learning_rate 0.000100063
2017-10-10T11:21:13.987042: step 2754, loss 0.0684062, acc 0.984375, learning_rate 0.000100063
2017-10-10T11:21:14.146396: step 2755, loss 0.136336, acc 0.953125, learning_rate 0.000100063
2017-10-10T11:21:14.307834: step 2756, loss 0.361275, acc 0.90625, learning_rate 0.000100063
2017-10-10T11:21:14.468737: step 2757, loss 0.0721152, acc 1, learning_rate 0.000100062
2017-10-10T11:21:14.626632: step 2758, loss 0.0574241, acc 0.984375, learning_rate 0.000100062
2017-10-10T11:21:14.788139: step 2759, loss 0.124968, acc 0.96875, learning_rate 0.000100062
2017-10-10T11:21:14.955192: step 2760, loss 0.0734679, acc 0.96875, learning_rate 0.000100062

Evaluation:
2017-10-10T11:21:15.407675: step 2760, loss 0.209095, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2760

2017-10-10T11:21:16.045772: step 2761, loss 0.0732521, acc 0.96875, learning_rate 0.000100061
2017-10-10T11:21:16.203974: step 2762, loss 0.119324, acc 0.984375, learning_rate 0.000100061
2017-10-10T11:21:16.363968: step 2763, loss 0.0937071, acc 0.953125, learning_rate 0.000100061
2017-10-10T11:21:16.528288: step 2764, loss 0.0777239, acc 0.96875, learning_rate 0.000100061
2017-10-10T11:21:16.692154: step 2765, loss 0.140426, acc 0.953125, learning_rate 0.00010006
2017-10-10T11:21:16.859170: step 2766, loss 0.106676, acc 0.96875, learning_rate 0.00010006
2017-10-10T11:21:17.021089: step 2767, loss 0.048733, acc 1, learning_rate 0.00010006
2017-10-10T11:21:17.182935: step 2768, loss 0.0549236, acc 1, learning_rate 0.00010006
2017-10-10T11:21:17.344046: step 2769, loss 0.26935, acc 0.921875, learning_rate 0.000100059
2017-10-10T11:21:17.505630: step 2770, loss 0.127368, acc 0.9375, learning_rate 0.000100059
2017-10-10T11:21:17.666837: step 2771, loss 0.185524, acc 0.9375, learning_rate 0.000100059
2017-10-10T11:21:17.829766: step 2772, loss 0.0534797, acc 0.984375, learning_rate 0.000100059
2017-10-10T11:21:17.998677: step 2773, loss 0.133016, acc 0.96875, learning_rate 0.000100058
2017-10-10T11:21:18.163013: step 2774, loss 0.0875034, acc 0.984375, learning_rate 0.000100058
2017-10-10T11:21:18.324597: step 2775, loss 0.0411069, acc 1, learning_rate 0.000100058
2017-10-10T11:21:18.484397: step 2776, loss 0.0906281, acc 0.984375, learning_rate 0.000100058
2017-10-10T11:21:18.648183: step 2777, loss 0.0328409, acc 1, learning_rate 0.000100057
2017-10-10T11:21:18.812166: step 2778, loss 0.104291, acc 0.9375, learning_rate 0.000100057
2017-10-10T11:21:18.979141: step 2779, loss 0.118104, acc 0.953125, learning_rate 0.000100057
2017-10-10T11:21:19.141057: step 2780, loss 0.122225, acc 0.96875, learning_rate 0.000100057
2017-10-10T11:21:19.303796: step 2781, loss 0.126975, acc 0.953125, learning_rate 0.000100056
2017-10-10T11:21:19.464615: step 2782, loss 0.103117, acc 0.96875, learning_rate 0.000100056
2017-10-10T11:21:19.625726: step 2783, loss 0.105479, acc 0.984375, learning_rate 0.000100056
2017-10-10T11:21:19.787666: step 2784, loss 0.0317107, acc 1, learning_rate 0.000100056
2017-10-10T11:21:19.951613: step 2785, loss 0.0596707, acc 0.984375, learning_rate 0.000100056
2017-10-10T11:21:20.113637: step 2786, loss 0.10098, acc 0.96875, learning_rate 0.000100055
2017-10-10T11:21:20.275788: step 2787, loss 0.0665628, acc 0.96875, learning_rate 0.000100055
2017-10-10T11:21:20.438369: step 2788, loss 0.155998, acc 0.9375, learning_rate 0.000100055
2017-10-10T11:21:20.599324: step 2789, loss 0.186752, acc 0.9375, learning_rate 0.000100055
2017-10-10T11:21:20.763426: step 2790, loss 0.082571, acc 0.96875, learning_rate 0.000100054
2017-10-10T11:21:20.928286: step 2791, loss 0.0831406, acc 0.96875, learning_rate 0.000100054
2017-10-10T11:21:21.087141: step 2792, loss 0.0467561, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:21:21.248779: step 2793, loss 0.200637, acc 0.9375, learning_rate 0.000100054
2017-10-10T11:21:21.412098: step 2794, loss 0.058612, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:21:21.574954: step 2795, loss 0.12001, acc 0.953125, learning_rate 0.000100053
2017-10-10T11:21:21.738374: step 2796, loss 0.112552, acc 0.984375, learning_rate 0.000100053
2017-10-10T11:21:21.900141: step 2797, loss 0.0284642, acc 1, learning_rate 0.000100053
2017-10-10T11:21:22.063635: step 2798, loss 0.10132, acc 0.96875, learning_rate 0.000100053
2017-10-10T11:21:22.226280: step 2799, loss 0.166977, acc 0.9375, learning_rate 0.000100052
2017-10-10T11:21:22.385449: step 2800, loss 0.0814988, acc 0.984375, learning_rate 0.000100052

Evaluation:
2017-10-10T11:21:22.850691: step 2800, loss 0.210523, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2800

2017-10-10T11:21:24.051095: step 2801, loss 0.0476101, acc 0.984375, learning_rate 0.000100052
2017-10-10T11:21:24.217907: step 2802, loss 0.084477, acc 0.96875, learning_rate 0.000100052
2017-10-10T11:21:24.379663: step 2803, loss 0.0459915, acc 0.984375, learning_rate 0.000100052
2017-10-10T11:21:24.539466: step 2804, loss 0.0600913, acc 0.96875, learning_rate 0.000100051
2017-10-10T11:21:24.703056: step 2805, loss 0.086819, acc 0.96875, learning_rate 0.000100051
2017-10-10T11:21:24.867663: step 2806, loss 0.0952619, acc 0.96875, learning_rate 0.000100051
2017-10-10T11:21:25.030686: step 2807, loss 0.108472, acc 0.953125, learning_rate 0.000100051
2017-10-10T11:21:25.190465: step 2808, loss 0.115421, acc 0.953125, learning_rate 0.000100051
2017-10-10T11:21:25.355069: step 2809, loss 0.135373, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:21:25.516181: step 2810, loss 0.0581344, acc 0.984375, learning_rate 0.00010005
2017-10-10T11:21:25.677471: step 2811, loss 0.220054, acc 0.9375, learning_rate 0.00010005
2017-10-10T11:21:25.840286: step 2812, loss 0.121336, acc 0.953125, learning_rate 0.00010005
2017-10-10T11:21:25.999666: step 2813, loss 0.0545704, acc 0.984375, learning_rate 0.00010005
2017-10-10T11:21:26.161459: step 2814, loss 0.238727, acc 0.90625, learning_rate 0.000100049
2017-10-10T11:21:26.321826: step 2815, loss 0.047766, acc 0.984375, learning_rate 0.000100049
2017-10-10T11:21:26.479391: step 2816, loss 0.0929609, acc 0.96875, learning_rate 0.000100049
2017-10-10T11:21:26.641679: step 2817, loss 0.0514065, acc 1, learning_rate 0.000100049
2017-10-10T11:21:26.805287: step 2818, loss 0.124988, acc 0.96875, learning_rate 0.000100049
2017-10-10T11:21:26.965821: step 2819, loss 0.0893043, acc 0.96875, learning_rate 0.000100048
2017-10-10T11:21:27.130515: step 2820, loss 0.103651, acc 0.953125, learning_rate 0.000100048
2017-10-10T11:21:27.292593: step 2821, loss 0.218094, acc 0.9375, learning_rate 0.000100048
2017-10-10T11:21:27.452173: step 2822, loss 0.240761, acc 0.9375, learning_rate 0.000100048
2017-10-10T11:21:27.616056: step 2823, loss 0.0507253, acc 0.984375, learning_rate 0.000100048
2017-10-10T11:21:27.776804: step 2824, loss 0.172879, acc 0.96875, learning_rate 0.000100047
2017-10-10T11:21:27.955541: step 2825, loss 0.11629, acc 0.953125, learning_rate 0.000100047
2017-10-10T11:21:28.118832: step 2826, loss 0.0609964, acc 0.984375, learning_rate 0.000100047
2017-10-10T11:21:28.280516: step 2827, loss 0.122688, acc 0.953125, learning_rate 0.000100047
2017-10-10T11:21:28.444156: step 2828, loss 0.118852, acc 0.953125, learning_rate 0.000100047
2017-10-10T11:21:28.605642: step 2829, loss 0.172423, acc 0.96875, learning_rate 0.000100046
2017-10-10T11:21:28.765299: step 2830, loss 0.140741, acc 0.9375, learning_rate 0.000100046
2017-10-10T11:21:28.930942: step 2831, loss 0.150233, acc 0.90625, learning_rate 0.000100046
2017-10-10T11:21:29.097730: step 2832, loss 0.0882893, acc 0.953125, learning_rate 0.000100046
2017-10-10T11:21:29.261285: step 2833, loss 0.0628774, acc 0.984375, learning_rate 0.000100046
2017-10-10T11:21:29.421439: step 2834, loss 0.120298, acc 0.96875, learning_rate 0.000100045
2017-10-10T11:21:29.583698: step 2835, loss 0.0978736, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:21:29.744182: step 2836, loss 0.137079, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:21:29.906876: step 2837, loss 0.177684, acc 0.921875, learning_rate 0.000100045
2017-10-10T11:21:30.067685: step 2838, loss 0.084278, acc 0.96875, learning_rate 0.000100045
2017-10-10T11:21:30.229649: step 2839, loss 0.170567, acc 0.9375, learning_rate 0.000100045
2017-10-10T11:21:30.392524: step 2840, loss 0.0497271, acc 1, learning_rate 0.000100044

Evaluation:
2017-10-10T11:21:30.849716: step 2840, loss 0.207268, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2840

2017-10-10T11:21:31.419497: step 2841, loss 0.0855417, acc 0.96875, learning_rate 0.000100044
2017-10-10T11:21:31.554076: step 2842, loss 0.176614, acc 0.921569, learning_rate 0.000100044
2017-10-10T11:21:31.714025: step 2843, loss 0.0640271, acc 0.984375, learning_rate 0.000100044
2017-10-10T11:21:31.877172: step 2844, loss 0.0592312, acc 0.984375, learning_rate 0.000100044
2017-10-10T11:21:32.039437: step 2845, loss 0.135467, acc 0.9375, learning_rate 0.000100043
2017-10-10T11:21:32.202989: step 2846, loss 0.0527966, acc 0.96875, learning_rate 0.000100043
2017-10-10T11:21:32.366768: step 2847, loss 0.0715948, acc 1, learning_rate 0.000100043
2017-10-10T11:21:32.533155: step 2848, loss 0.138255, acc 0.953125, learning_rate 0.000100043
2017-10-10T11:21:32.694163: step 2849, loss 0.120301, acc 0.9375, learning_rate 0.000100043
2017-10-10T11:21:32.857851: step 2850, loss 0.15784, acc 0.96875, learning_rate 0.000100043
2017-10-10T11:21:33.018583: step 2851, loss 0.0206128, acc 1, learning_rate 0.000100042
2017-10-10T11:21:33.180608: step 2852, loss 0.142523, acc 0.921875, learning_rate 0.000100042
2017-10-10T11:21:33.341804: step 2853, loss 0.0856712, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:21:33.502808: step 2854, loss 0.182536, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:21:33.665551: step 2855, loss 0.0979501, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:21:33.828270: step 2856, loss 0.112536, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:21:33.995802: step 2857, loss 0.0886845, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:21:34.159342: step 2858, loss 0.126318, acc 0.953125, learning_rate 0.000100041
2017-10-10T11:21:34.322938: step 2859, loss 0.137548, acc 0.984375, learning_rate 0.000100041
2017-10-10T11:21:34.483276: step 2860, loss 0.121905, acc 0.9375, learning_rate 0.000100041
2017-10-10T11:21:34.646613: step 2861, loss 0.0623226, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:21:34.811327: step 2862, loss 0.0961556, acc 0.953125, learning_rate 0.000100041
2017-10-10T11:21:34.972303: step 2863, loss 0.127616, acc 0.953125, learning_rate 0.00010004
2017-10-10T11:21:35.136456: step 2864, loss 0.238174, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:21:35.298633: step 2865, loss 0.0499206, acc 1, learning_rate 0.00010004
2017-10-10T11:21:35.462198: step 2866, loss 0.121362, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:21:35.623682: step 2867, loss 0.072712, acc 0.984375, learning_rate 0.00010004
2017-10-10T11:21:35.788517: step 2868, loss 0.0443359, acc 0.984375, learning_rate 0.00010004
2017-10-10T11:21:35.954173: step 2869, loss 0.0564763, acc 0.984375, learning_rate 0.000100039
2017-10-10T11:21:36.116544: step 2870, loss 0.0648642, acc 0.96875, learning_rate 0.000100039
2017-10-10T11:21:36.279817: step 2871, loss 0.0688473, acc 0.96875, learning_rate 0.000100039
2017-10-10T11:21:36.439540: step 2872, loss 0.0568343, acc 0.984375, learning_rate 0.000100039
2017-10-10T11:21:36.602499: step 2873, loss 0.0901553, acc 0.96875, learning_rate 0.000100039
2017-10-10T11:21:36.763604: step 2874, loss 0.210091, acc 0.921875, learning_rate 0.000100039
2017-10-10T11:21:36.930403: step 2875, loss 0.11054, acc 0.984375, learning_rate 0.000100038
2017-10-10T11:21:37.092550: step 2876, loss 0.0383284, acc 0.984375, learning_rate 0.000100038
2017-10-10T11:21:37.255584: step 2877, loss 0.0398425, acc 0.984375, learning_rate 0.000100038
2017-10-10T11:21:37.417687: step 2878, loss 0.146551, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:21:37.579325: step 2879, loss 0.123221, acc 0.953125, learning_rate 0.000100038
2017-10-10T11:21:37.743171: step 2880, loss 0.0927648, acc 0.96875, learning_rate 0.000100038

Evaluation:
2017-10-10T11:21:38.222433: step 2880, loss 0.208313, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2880

2017-10-10T11:21:38.862639: step 2881, loss 0.12404, acc 0.953125, learning_rate 0.000100038
2017-10-10T11:21:39.023797: step 2882, loss 0.0436839, acc 1, learning_rate 0.000100037
2017-10-10T11:21:39.188403: step 2883, loss 0.183941, acc 0.9375, learning_rate 0.000100037
2017-10-10T11:21:39.350344: step 2884, loss 0.0814851, acc 0.96875, learning_rate 0.000100037
2017-10-10T11:21:39.509927: step 2885, loss 0.116633, acc 0.953125, learning_rate 0.000100037
2017-10-10T11:21:39.670010: step 2886, loss 0.137068, acc 0.953125, learning_rate 0.000100037
2017-10-10T11:21:39.830052: step 2887, loss 0.176315, acc 0.953125, learning_rate 0.000100037
2017-10-10T11:21:39.996605: step 2888, loss 0.196344, acc 0.890625, learning_rate 0.000100036
2017-10-10T11:21:40.157401: step 2889, loss 0.0976995, acc 0.96875, learning_rate 0.000100036
2017-10-10T11:21:40.317236: step 2890, loss 0.136701, acc 0.953125, learning_rate 0.000100036
2017-10-10T11:21:40.478874: step 2891, loss 0.166434, acc 0.9375, learning_rate 0.000100036
2017-10-10T11:21:40.644104: step 2892, loss 0.0456448, acc 1, learning_rate 0.000100036
2017-10-10T11:21:40.808013: step 2893, loss 0.0813009, acc 0.984375, learning_rate 0.000100036
2017-10-10T11:21:40.970838: step 2894, loss 0.184433, acc 0.9375, learning_rate 0.000100036
2017-10-10T11:21:41.136912: step 2895, loss 0.0668823, acc 0.96875, learning_rate 0.000100035
2017-10-10T11:21:41.301657: step 2896, loss 0.155218, acc 0.90625, learning_rate 0.000100035
2017-10-10T11:21:41.461103: step 2897, loss 0.114401, acc 0.96875, learning_rate 0.000100035
2017-10-10T11:21:41.626066: step 2898, loss 0.122737, acc 0.953125, learning_rate 0.000100035
2017-10-10T11:21:41.789902: step 2899, loss 0.0583602, acc 0.984375, learning_rate 0.000100035
2017-10-10T11:21:41.949272: step 2900, loss 0.112015, acc 0.96875, learning_rate 0.000100035
2017-10-10T11:21:42.112556: step 2901, loss 0.0442135, acc 0.984375, learning_rate 0.000100035
2017-10-10T11:21:42.274958: step 2902, loss 0.0367875, acc 0.984375, learning_rate 0.000100034
2017-10-10T11:21:42.438428: step 2903, loss 0.106585, acc 0.96875, learning_rate 0.000100034
2017-10-10T11:21:42.599132: step 2904, loss 0.104377, acc 0.96875, learning_rate 0.000100034
2017-10-10T11:21:42.759886: step 2905, loss 0.141803, acc 0.90625, learning_rate 0.000100034
2017-10-10T11:21:42.930065: step 2906, loss 0.18614, acc 0.921875, learning_rate 0.000100034
2017-10-10T11:21:43.092484: step 2907, loss 0.160885, acc 0.9375, learning_rate 0.000100034
2017-10-10T11:21:43.255748: step 2908, loss 0.10863, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:21:43.419014: step 2909, loss 0.0769642, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:21:43.582728: step 2910, loss 0.0533191, acc 1, learning_rate 0.000100033
2017-10-10T11:21:43.747829: step 2911, loss 0.0819538, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:21:43.919687: step 2912, loss 0.1226, acc 0.984375, learning_rate 0.000100033
2017-10-10T11:21:44.084414: step 2913, loss 0.15357, acc 0.90625, learning_rate 0.000100033
2017-10-10T11:21:44.246657: step 2914, loss 0.0605802, acc 0.984375, learning_rate 0.000100033
2017-10-10T11:21:44.409729: step 2915, loss 0.130888, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:21:44.569837: step 2916, loss 0.0703318, acc 1, learning_rate 0.000100033
2017-10-10T11:21:44.730721: step 2917, loss 0.0856597, acc 0.96875, learning_rate 0.000100032
2017-10-10T11:21:44.901996: step 2918, loss 0.131036, acc 0.953125, learning_rate 0.000100032
2017-10-10T11:21:45.064746: step 2919, loss 0.132164, acc 0.953125, learning_rate 0.000100032
2017-10-10T11:21:45.225068: step 2920, loss 0.180268, acc 0.953125, learning_rate 0.000100032

Evaluation:
2017-10-10T11:21:45.674294: step 2920, loss 0.212072, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2920

2017-10-10T11:21:46.382852: step 2921, loss 0.0482377, acc 0.984375, learning_rate 0.000100032
2017-10-10T11:21:46.544126: step 2922, loss 0.170391, acc 0.953125, learning_rate 0.000100032
2017-10-10T11:21:46.707604: step 2923, loss 0.0572521, acc 1, learning_rate 0.000100032
2017-10-10T11:21:46.868625: step 2924, loss 0.0307535, acc 0.984375, learning_rate 0.000100031
2017-10-10T11:21:47.029084: step 2925, loss 0.074728, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:21:47.191796: step 2926, loss 0.222512, acc 0.921875, learning_rate 0.000100031
2017-10-10T11:21:47.355235: step 2927, loss 0.189306, acc 0.9375, learning_rate 0.000100031
2017-10-10T11:21:47.521105: step 2928, loss 0.143646, acc 0.953125, learning_rate 0.000100031
2017-10-10T11:21:47.683386: step 2929, loss 0.0650548, acc 0.984375, learning_rate 0.000100031
2017-10-10T11:21:47.847397: step 2930, loss 0.0556828, acc 0.984375, learning_rate 0.000100031
2017-10-10T11:21:48.011763: step 2931, loss 0.141913, acc 0.9375, learning_rate 0.000100031
2017-10-10T11:21:48.179698: step 2932, loss 0.160508, acc 0.953125, learning_rate 0.00010003
2017-10-10T11:21:48.340482: step 2933, loss 0.0948712, acc 0.9375, learning_rate 0.00010003
2017-10-10T11:21:48.502124: step 2934, loss 0.0522098, acc 0.984375, learning_rate 0.00010003
2017-10-10T11:21:48.669626: step 2935, loss 0.0905456, acc 0.984375, learning_rate 0.00010003
2017-10-10T11:21:48.830335: step 2936, loss 0.0942364, acc 0.953125, learning_rate 0.00010003
2017-10-10T11:21:48.992921: step 2937, loss 0.13656, acc 0.9375, learning_rate 0.00010003
2017-10-10T11:21:49.157251: step 2938, loss 0.0785506, acc 0.96875, learning_rate 0.00010003
2017-10-10T11:21:49.321993: step 2939, loss 0.202522, acc 0.9375, learning_rate 0.00010003
2017-10-10T11:21:49.456579: step 2940, loss 0.0750133, acc 0.960784, learning_rate 0.000100029
2017-10-10T11:21:49.622829: step 2941, loss 0.057828, acc 0.96875, learning_rate 0.000100029
2017-10-10T11:21:49.785864: step 2942, loss 0.145355, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:21:49.949379: step 2943, loss 0.146124, acc 0.9375, learning_rate 0.000100029
2017-10-10T11:21:50.109437: step 2944, loss 0.0793902, acc 0.984375, learning_rate 0.000100029
2017-10-10T11:21:50.269229: step 2945, loss 0.0883722, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:21:50.432441: step 2946, loss 0.198109, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:21:50.592301: step 2947, loss 0.0328518, acc 0.984375, learning_rate 0.000100029
2017-10-10T11:21:50.754187: step 2948, loss 0.0786859, acc 0.984375, learning_rate 0.000100029
2017-10-10T11:21:50.928435: step 2949, loss 0.0827018, acc 0.984375, learning_rate 0.000100028
2017-10-10T11:21:51.090714: step 2950, loss 0.0900277, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:21:51.253443: step 2951, loss 0.126244, acc 0.953125, learning_rate 0.000100028
2017-10-10T11:21:51.412995: step 2952, loss 0.111862, acc 0.953125, learning_rate 0.000100028
2017-10-10T11:21:51.574011: step 2953, loss 0.111271, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:21:51.735714: step 2954, loss 0.0627371, acc 0.984375, learning_rate 0.000100028
2017-10-10T11:21:51.899281: step 2955, loss 0.108493, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:21:52.061977: step 2956, loss 0.0416353, acc 1, learning_rate 0.000100028
2017-10-10T11:21:52.222049: step 2957, loss 0.0643817, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:21:52.383924: step 2958, loss 0.11854, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:21:52.546271: step 2959, loss 0.126921, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:21:52.711224: step 2960, loss 0.0960633, acc 0.953125, learning_rate 0.000100027

Evaluation:
2017-10-10T11:21:53.175495: step 2960, loss 0.207078, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-2960

2017-10-10T11:21:53.748350: step 2961, loss 0.104523, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:21:53.915546: step 2962, loss 0.145691, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:21:54.078126: step 2963, loss 0.0301652, acc 1, learning_rate 0.000100027
2017-10-10T11:21:54.238390: step 2964, loss 0.0645376, acc 0.984375, learning_rate 0.000100027
2017-10-10T11:21:54.400959: step 2965, loss 0.0610556, acc 0.984375, learning_rate 0.000100027
2017-10-10T11:21:54.562072: step 2966, loss 0.0651216, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:21:54.722859: step 2967, loss 0.125143, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:21:54.886479: step 2968, loss 0.0481893, acc 1, learning_rate 0.000100026
2017-10-10T11:21:55.048899: step 2969, loss 0.0962472, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:21:55.210443: step 2970, loss 0.0527235, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:21:55.375382: step 2971, loss 0.132072, acc 0.96875, learning_rate 0.000100026
2017-10-10T11:21:55.534666: step 2972, loss 0.256962, acc 0.90625, learning_rate 0.000100026
2017-10-10T11:21:55.698656: step 2973, loss 0.0407779, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:21:55.859585: step 2974, loss 0.0411929, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:21:56.020108: step 2975, loss 0.149619, acc 0.953125, learning_rate 0.000100026
2017-10-10T11:21:56.181508: step 2976, loss 0.0328641, acc 1, learning_rate 0.000100025
2017-10-10T11:21:56.344522: step 2977, loss 0.0788188, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:21:56.506492: step 2978, loss 0.125047, acc 0.9375, learning_rate 0.000100025
2017-10-10T11:21:56.668038: step 2979, loss 0.0745236, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:21:56.829525: step 2980, loss 0.0497322, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:21:56.995283: step 2981, loss 0.105253, acc 0.953125, learning_rate 0.000100025
2017-10-10T11:21:57.159323: step 2982, loss 0.0832239, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:21:57.318025: step 2983, loss 0.218107, acc 0.90625, learning_rate 0.000100025
2017-10-10T11:21:57.480760: step 2984, loss 0.0404158, acc 1, learning_rate 0.000100025
2017-10-10T11:21:57.642443: step 2985, loss 0.077601, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:21:57.805098: step 2986, loss 0.0422577, acc 1, learning_rate 0.000100024
2017-10-10T11:21:57.970847: step 2987, loss 0.0770967, acc 0.96875, learning_rate 0.000100024
2017-10-10T11:21:58.128822: step 2988, loss 0.15898, acc 0.9375, learning_rate 0.000100024
2017-10-10T11:21:58.299491: step 2989, loss 0.232534, acc 0.890625, learning_rate 0.000100024
2017-10-10T11:21:58.458170: step 2990, loss 0.0582374, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:21:58.623306: step 2991, loss 0.100373, acc 0.953125, learning_rate 0.000100024
2017-10-10T11:21:58.787847: step 2992, loss 0.163999, acc 0.9375, learning_rate 0.000100024
2017-10-10T11:21:58.954055: step 2993, loss 0.0343707, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:21:59.116569: step 2994, loss 0.0594396, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:21:59.279774: step 2995, loss 0.0611874, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:21:59.445291: step 2996, loss 0.0869739, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:21:59.608551: step 2997, loss 0.0147658, acc 1, learning_rate 0.000100023
2017-10-10T11:21:59.768769: step 2998, loss 0.102787, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:21:59.934711: step 2999, loss 0.119282, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:22:00.100393: step 3000, loss 0.0582401, acc 0.96875, learning_rate 0.000100023

Evaluation:
2017-10-10T11:22:00.554516: step 3000, loss 0.206163, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3000

2017-10-10T11:22:01.189837: step 3001, loss 0.0874806, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:22:01.350853: step 3002, loss 0.206725, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:22:01.512687: step 3003, loss 0.148534, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:22:01.675671: step 3004, loss 0.0457061, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:22:01.838752: step 3005, loss 0.0645636, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:22:02.002247: step 3006, loss 0.0377196, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:22:02.162354: step 3007, loss 0.138349, acc 0.953125, learning_rate 0.000100022
2017-10-10T11:22:02.323438: step 3008, loss 0.216836, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:22:02.484722: step 3009, loss 0.094585, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:22:02.646511: step 3010, loss 0.140605, acc 0.953125, learning_rate 0.000100022
2017-10-10T11:22:02.811033: step 3011, loss 0.033844, acc 1, learning_rate 0.000100022
2017-10-10T11:22:02.972087: step 3012, loss 0.0915167, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:22:03.133870: step 3013, loss 0.0302549, acc 1, learning_rate 0.000100022
2017-10-10T11:22:03.295542: step 3014, loss 0.0254202, acc 1, learning_rate 0.000100022
2017-10-10T11:22:03.459365: step 3015, loss 0.136072, acc 0.9375, learning_rate 0.000100022
2017-10-10T11:22:03.618874: step 3016, loss 0.0985821, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:22:03.778136: step 3017, loss 0.100159, acc 0.984375, learning_rate 0.000100022
2017-10-10T11:22:03.942760: step 3018, loss 0.148694, acc 0.953125, learning_rate 0.000100021
2017-10-10T11:22:04.103441: step 3019, loss 0.14868, acc 0.953125, learning_rate 0.000100021
2017-10-10T11:22:04.268217: step 3020, loss 0.0950708, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:22:04.430622: step 3021, loss 0.0649045, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:22:04.589166: step 3022, loss 0.0769373, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:22:04.752610: step 3023, loss 0.0887099, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:22:04.918574: step 3024, loss 0.060495, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:22:05.079783: step 3025, loss 0.158079, acc 0.9375, learning_rate 0.000100021
2017-10-10T11:22:05.242414: step 3026, loss 0.198924, acc 0.921875, learning_rate 0.000100021
2017-10-10T11:22:05.403762: step 3027, loss 0.071794, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:22:05.562814: step 3028, loss 0.136579, acc 0.953125, learning_rate 0.000100021
2017-10-10T11:22:05.724174: step 3029, loss 0.0981818, acc 0.953125, learning_rate 0.00010002
2017-10-10T11:22:05.889535: step 3030, loss 0.0723311, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:22:06.052155: step 3031, loss 0.123217, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:22:06.214631: step 3032, loss 0.0683041, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:22:06.379090: step 3033, loss 0.0491795, acc 1, learning_rate 0.00010002
2017-10-10T11:22:06.542949: step 3034, loss 0.212956, acc 0.953125, learning_rate 0.00010002
2017-10-10T11:22:06.706432: step 3035, loss 0.032203, acc 1, learning_rate 0.00010002
2017-10-10T11:22:06.872785: step 3036, loss 0.158181, acc 0.9375, learning_rate 0.00010002
2017-10-10T11:22:07.035387: step 3037, loss 0.0137008, acc 1, learning_rate 0.00010002
2017-10-10T11:22:07.171536: step 3038, loss 0.0657134, acc 0.980392, learning_rate 0.00010002
2017-10-10T11:22:07.330364: step 3039, loss 0.104293, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:22:07.492114: step 3040, loss 0.145896, acc 0.953125, learning_rate 0.00010002

Evaluation:
2017-10-10T11:22:07.966146: step 3040, loss 0.205374, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3040

2017-10-10T11:22:08.604160: step 3041, loss 0.0882381, acc 0.953125, learning_rate 0.00010002
2017-10-10T11:22:08.766507: step 3042, loss 0.034975, acc 1, learning_rate 0.000100019
2017-10-10T11:22:08.932085: step 3043, loss 0.0565048, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:22:09.093447: step 3044, loss 0.178345, acc 0.953125, learning_rate 0.000100019
2017-10-10T11:22:09.258592: step 3045, loss 0.0372686, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:22:09.421125: step 3046, loss 0.0853584, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:22:09.582431: step 3047, loss 0.277785, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:22:09.743001: step 3048, loss 0.0373784, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:22:09.908028: step 3049, loss 0.101718, acc 0.953125, learning_rate 0.000100019
2017-10-10T11:22:10.071136: step 3050, loss 0.122455, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:22:10.236134: step 3051, loss 0.083531, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:22:10.396137: step 3052, loss 0.0471581, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:22:10.559264: step 3053, loss 0.0539181, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:22:10.720435: step 3054, loss 0.136029, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:22:10.883996: step 3055, loss 0.0473168, acc 1, learning_rate 0.000100018
2017-10-10T11:22:11.045157: step 3056, loss 0.136507, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:22:11.210627: step 3057, loss 0.0539093, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:22:11.371486: step 3058, loss 0.0871008, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:22:11.533194: step 3059, loss 0.0938307, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:22:11.695260: step 3060, loss 0.114421, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:22:11.858626: step 3061, loss 0.144762, acc 0.921875, learning_rate 0.000100018
2017-10-10T11:22:12.018955: step 3062, loss 0.220955, acc 0.921875, learning_rate 0.000100018
2017-10-10T11:22:12.179548: step 3063, loss 0.129206, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:22:12.341291: step 3064, loss 0.0863046, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:22:12.503542: step 3065, loss 0.0824506, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:22:12.665189: step 3066, loss 0.0830696, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:22:12.833075: step 3067, loss 0.142589, acc 0.9375, learning_rate 0.000100018
2017-10-10T11:22:13.012298: step 3068, loss 0.0549829, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:22:13.177378: step 3069, loss 0.12715, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:22:13.340348: step 3070, loss 0.121997, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:22:13.505433: step 3071, loss 0.0578059, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:22:13.670963: step 3072, loss 0.0922332, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:22:13.833622: step 3073, loss 0.0573969, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:22:14.005781: step 3074, loss 0.211278, acc 0.9375, learning_rate 0.000100017
2017-10-10T11:22:14.170301: step 3075, loss 0.0558241, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:22:14.333974: step 3076, loss 0.0783381, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:22:14.495393: step 3077, loss 0.100417, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:22:14.657998: step 3078, loss 0.106751, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:22:14.816246: step 3079, loss 0.0524764, acc 1, learning_rate 0.000100017
2017-10-10T11:22:14.984499: step 3080, loss 0.0682417, acc 0.984375, learning_rate 0.000100017

Evaluation:
2017-10-10T11:22:15.444112: step 3080, loss 0.206632, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3080

2017-10-10T11:22:16.158721: step 3081, loss 0.104728, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:22:16.318467: step 3082, loss 0.0813691, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:22:16.482285: step 3083, loss 0.0550061, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:22:16.643008: step 3084, loss 0.122065, acc 0.921875, learning_rate 0.000100016
2017-10-10T11:22:16.805154: step 3085, loss 0.0461013, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:22:16.967199: step 3086, loss 0.169532, acc 0.921875, learning_rate 0.000100016
2017-10-10T11:22:17.130407: step 3087, loss 0.169596, acc 0.9375, learning_rate 0.000100016
2017-10-10T11:22:17.292278: step 3088, loss 0.114062, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:22:17.453774: step 3089, loss 0.0296898, acc 1, learning_rate 0.000100016
2017-10-10T11:22:17.616606: step 3090, loss 0.118964, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:22:17.778379: step 3091, loss 0.0880269, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:22:17.945069: step 3092, loss 0.216326, acc 0.921875, learning_rate 0.000100016
2017-10-10T11:22:18.106653: step 3093, loss 0.0871067, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:22:18.268692: step 3094, loss 0.0710631, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:22:18.434838: step 3095, loss 0.0841588, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:22:18.598032: step 3096, loss 0.0652976, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:22:18.761452: step 3097, loss 0.08053, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:22:18.926274: step 3098, loss 0.147969, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:22:19.086099: step 3099, loss 0.248352, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:22:19.246202: step 3100, loss 0.150869, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:22:19.408944: step 3101, loss 0.0704413, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:22:19.573804: step 3102, loss 0.0302229, acc 1, learning_rate 0.000100015
2017-10-10T11:22:19.736229: step 3103, loss 0.0607832, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:22:19.908357: step 3104, loss 0.056262, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:22:20.070080: step 3105, loss 0.0788093, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:22:20.232774: step 3106, loss 0.165927, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:22:20.394331: step 3107, loss 0.0775576, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:22:20.556306: step 3108, loss 0.0373094, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:22:20.718213: step 3109, loss 0.107708, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:22:20.881095: step 3110, loss 0.0777178, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:22:21.040069: step 3111, loss 0.163838, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:22:21.200744: step 3112, loss 0.0911999, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:22:21.362659: step 3113, loss 0.0596051, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:22:21.523529: step 3114, loss 0.0951709, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:22:21.685787: step 3115, loss 0.129688, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:22:21.845608: step 3116, loss 0.101631, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:22:22.010536: step 3117, loss 0.31727, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:22:22.173376: step 3118, loss 0.0743243, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:22:22.332996: step 3119, loss 0.0320624, acc 1, learning_rate 0.000100014
2017-10-10T11:22:22.494111: step 3120, loss 0.168312, acc 0.921875, learning_rate 0.000100014

Evaluation:
2017-10-10T11:22:22.966826: step 3120, loss 0.208025, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3120

2017-10-10T11:22:23.540861: step 3121, loss 0.130637, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:22:23.701663: step 3122, loss 0.0548887, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:22:23.867830: step 3123, loss 0.114421, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:22:24.029802: step 3124, loss 0.0549881, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:22:24.191729: step 3125, loss 0.111893, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:22:24.352327: step 3126, loss 0.121915, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:22:24.515942: step 3127, loss 0.0254618, acc 1, learning_rate 0.000100014
2017-10-10T11:22:24.678152: step 3128, loss 0.0523086, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:22:24.840292: step 3129, loss 0.11939, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:22:25.003355: step 3130, loss 0.0733054, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:22:25.166393: step 3131, loss 0.033621, acc 1, learning_rate 0.000100014
2017-10-10T11:22:25.332995: step 3132, loss 0.0198611, acc 1, learning_rate 0.000100013
2017-10-10T11:22:25.496908: step 3133, loss 0.066601, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:22:25.657780: step 3134, loss 0.0794184, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:22:25.817942: step 3135, loss 0.165365, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:22:25.952516: step 3136, loss 0.0772432, acc 0.960784, learning_rate 0.000100013
2017-10-10T11:22:26.115773: step 3137, loss 0.141436, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:22:26.276404: step 3138, loss 0.0867151, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:22:26.438332: step 3139, loss 0.106186, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:22:26.597848: step 3140, loss 0.0658391, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:22:26.763668: step 3141, loss 0.164501, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:22:26.928625: step 3142, loss 0.0689508, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:22:27.088978: step 3143, loss 0.0519559, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:22:27.251916: step 3144, loss 0.0551383, acc 1, learning_rate 0.000100013
2017-10-10T11:22:27.413123: step 3145, loss 0.00910069, acc 1, learning_rate 0.000100013
2017-10-10T11:22:27.577580: step 3146, loss 0.101899, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:22:27.742311: step 3147, loss 0.0825295, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:22:27.903775: step 3148, loss 0.0571987, acc 1, learning_rate 0.000100013
2017-10-10T11:22:28.062940: step 3149, loss 0.172836, acc 0.9375, learning_rate 0.000100013
2017-10-10T11:22:28.223264: step 3150, loss 0.0853181, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:28.384660: step 3151, loss 0.0477066, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:22:28.546473: step 3152, loss 0.0959528, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:28.709246: step 3153, loss 0.0735597, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:22:28.871922: step 3154, loss 0.0642007, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:29.032212: step 3155, loss 0.0639179, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:22:29.193814: step 3156, loss 0.0770204, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:29.353456: step 3157, loss 0.079184, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:22:29.513614: step 3158, loss 0.107292, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:22:29.674087: step 3159, loss 0.0467516, acc 1, learning_rate 0.000100012
2017-10-10T11:22:29.839859: step 3160, loss 0.102252, acc 0.984375, learning_rate 0.000100012

Evaluation:
2017-10-10T11:22:30.306906: step 3160, loss 0.208662, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3160

2017-10-10T11:22:30.958743: step 3161, loss 0.0793033, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:31.120320: step 3162, loss 0.0826823, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:31.280109: step 3163, loss 0.0481426, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:22:31.438922: step 3164, loss 0.071174, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:31.600215: step 3165, loss 0.167905, acc 0.9375, learning_rate 0.000100012
2017-10-10T11:22:31.759386: step 3166, loss 0.177522, acc 0.921875, learning_rate 0.000100012
2017-10-10T11:22:31.924205: step 3167, loss 0.156153, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:22:32.085843: step 3168, loss 0.0894423, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:22:32.247050: step 3169, loss 0.132644, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:32.407263: step 3170, loss 0.0809032, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:22:32.565758: step 3171, loss 0.0554794, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:22:32.727754: step 3172, loss 0.0469133, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:22:32.894423: step 3173, loss 0.112065, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:33.055945: step 3174, loss 0.0713211, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:33.216001: step 3175, loss 0.101201, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:33.380463: step 3176, loss 0.0703054, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:33.544461: step 3177, loss 0.0957627, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:33.707547: step 3178, loss 0.138279, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:22:33.869978: step 3179, loss 0.0862633, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:34.028820: step 3180, loss 0.112927, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:22:34.188949: step 3181, loss 0.0292359, acc 1, learning_rate 0.000100011
2017-10-10T11:22:34.349917: step 3182, loss 0.12876, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:22:34.515139: step 3183, loss 0.0628385, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:22:34.674519: step 3184, loss 0.102329, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:34.838334: step 3185, loss 0.0926187, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:22:34.998231: step 3186, loss 0.105704, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:35.161351: step 3187, loss 0.0702406, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:35.326134: step 3188, loss 0.16258, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:35.487272: step 3189, loss 0.0845609, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:35.648503: step 3190, loss 0.0791447, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:22:35.808622: step 3191, loss 0.160935, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:35.979656: step 3192, loss 0.0579159, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:22:36.143898: step 3193, loss 0.0960541, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:36.303469: step 3194, loss 0.099815, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:22:36.467850: step 3195, loss 0.0905575, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:36.628883: step 3196, loss 0.132549, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:36.791267: step 3197, loss 0.0342277, acc 1, learning_rate 0.00010001
2017-10-10T11:22:36.955115: step 3198, loss 0.103769, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:22:37.116149: step 3199, loss 0.0422112, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:22:37.279232: step 3200, loss 0.138275, acc 0.953125, learning_rate 0.00010001

Evaluation:
2017-10-10T11:22:37.743284: step 3200, loss 0.206501, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3200

2017-10-10T11:22:38.454070: step 3201, loss 0.135573, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:22:38.617513: step 3202, loss 0.109838, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:38.780912: step 3203, loss 0.0515408, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:38.946397: step 3204, loss 0.059137, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:22:39.109093: step 3205, loss 0.104305, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:39.270271: step 3206, loss 0.154207, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:22:39.432404: step 3207, loss 0.119374, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:39.594338: step 3208, loss 0.149094, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:39.758015: step 3209, loss 0.175189, acc 0.90625, learning_rate 0.00010001
2017-10-10T11:22:39.921927: step 3210, loss 0.168989, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:22:40.084751: step 3211, loss 0.11398, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:40.247163: step 3212, loss 0.0748666, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:22:40.410842: step 3213, loss 0.0445168, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:22:40.573024: step 3214, loss 0.196438, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:22:40.733544: step 3215, loss 0.0649135, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:22:40.904324: step 3216, loss 0.0558053, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:22:41.066556: step 3217, loss 0.296956, acc 0.90625, learning_rate 0.000100009
2017-10-10T11:22:41.229517: step 3218, loss 0.155101, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:22:41.392744: step 3219, loss 0.0484679, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:22:41.552487: step 3220, loss 0.203563, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:22:41.714361: step 3221, loss 0.195213, acc 0.90625, learning_rate 0.000100009
2017-10-10T11:22:41.898345: step 3222, loss 0.0903547, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:22:42.059499: step 3223, loss 0.0406272, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:22:42.221157: step 3224, loss 0.0401253, acc 1, learning_rate 0.000100009
2017-10-10T11:22:42.386600: step 3225, loss 0.10882, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:22:42.547700: step 3226, loss 0.0474863, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:22:42.708751: step 3227, loss 0.271471, acc 0.921875, learning_rate 0.000100009
2017-10-10T11:22:42.873344: step 3228, loss 0.0796942, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:22:43.034224: step 3229, loss 0.0478685, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:22:43.197743: step 3230, loss 0.159766, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:22:43.358817: step 3231, loss 0.138372, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:22:43.520949: step 3232, loss 0.175595, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:22:43.684332: step 3233, loss 0.100672, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:22:43.822108: step 3234, loss 0.149888, acc 0.941176, learning_rate 0.000100009
2017-10-10T11:22:43.991655: step 3235, loss 0.0527391, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:22:44.153585: step 3236, loss 0.0469464, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:22:44.317694: step 3237, loss 0.0619519, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:22:44.480064: step 3238, loss 0.136705, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:22:44.642048: step 3239, loss 0.0484593, acc 1, learning_rate 0.000100009
2017-10-10T11:22:44.803413: step 3240, loss 0.114733, acc 0.96875, learning_rate 0.000100009

Evaluation:
2017-10-10T11:22:45.259265: step 3240, loss 0.202924, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3240

2017-10-10T11:22:45.830533: step 3241, loss 0.112408, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:22:45.992455: step 3242, loss 0.092501, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:22:46.155310: step 3243, loss 0.0921555, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:22:46.315854: step 3244, loss 0.124571, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:22:46.480258: step 3245, loss 0.11854, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:22:46.643359: step 3246, loss 0.0742593, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:46.804632: step 3247, loss 0.179092, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:22:46.974457: step 3248, loss 0.069362, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:22:47.139361: step 3249, loss 0.11476, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:47.304000: step 3250, loss 0.0328116, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:47.471040: step 3251, loss 0.0472508, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:47.632966: step 3252, loss 0.092851, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:47.796776: step 3253, loss 0.0916901, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:47.959399: step 3254, loss 0.145454, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:22:48.119276: step 3255, loss 0.108022, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:48.281421: step 3256, loss 0.0823271, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:48.444143: step 3257, loss 0.0913226, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:48.607322: step 3258, loss 0.12866, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:48.771376: step 3259, loss 0.138255, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:22:48.946743: step 3260, loss 0.0481537, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:49.108398: step 3261, loss 0.0577937, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:49.272199: step 3262, loss 0.207404, acc 0.90625, learning_rate 0.000100008
2017-10-10T11:22:49.435550: step 3263, loss 0.0865121, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:49.599371: step 3264, loss 0.0942139, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:49.760758: step 3265, loss 0.0632395, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:22:49.927065: step 3266, loss 0.060415, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:50.090486: step 3267, loss 0.166756, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:22:50.255458: step 3268, loss 0.0620948, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:50.417401: step 3269, loss 0.121024, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:22:50.580201: step 3270, loss 0.0636764, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:50.744831: step 3271, loss 0.112021, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:22:50.909065: step 3272, loss 0.0793062, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:22:51.073825: step 3273, loss 0.0342156, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:51.236925: step 3274, loss 0.0455178, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:22:51.398865: step 3275, loss 0.108591, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:22:51.561218: step 3276, loss 0.0350104, acc 1, learning_rate 0.000100007
2017-10-10T11:22:51.722792: step 3277, loss 0.207914, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:51.885146: step 3278, loss 0.0688453, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:22:52.046959: step 3279, loss 0.175056, acc 0.90625, learning_rate 0.000100007
2017-10-10T11:22:52.210525: step 3280, loss 0.0984669, acc 0.96875, learning_rate 0.000100007

Evaluation:
2017-10-10T11:22:52.668701: step 3280, loss 0.211683, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3280

2017-10-10T11:22:53.314410: step 3281, loss 0.0935788, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:53.476430: step 3282, loss 0.161944, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:22:53.638865: step 3283, loss 0.20398, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:22:53.802648: step 3284, loss 0.0817808, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:22:53.966700: step 3285, loss 0.159874, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:54.128897: step 3286, loss 0.103647, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:54.288465: step 3287, loss 0.0330342, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:22:54.452560: step 3288, loss 0.0618222, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:22:54.617335: step 3289, loss 0.0672142, acc 1, learning_rate 0.000100007
2017-10-10T11:22:54.780000: step 3290, loss 0.0717343, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:22:54.944684: step 3291, loss 0.109324, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:55.108236: step 3292, loss 0.148889, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:22:55.268728: step 3293, loss 0.10306, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:55.430380: step 3294, loss 0.0655678, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:22:55.592152: step 3295, loss 0.0607142, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:22:55.752527: step 3296, loss 0.0643376, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:22:55.918202: step 3297, loss 0.171847, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:22:56.082192: step 3298, loss 0.185747, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:56.248878: step 3299, loss 0.101086, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:22:56.411402: step 3300, loss 0.141722, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:22:56.576848: step 3301, loss 0.0663717, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:22:56.741488: step 3302, loss 0.15356, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:22:56.908433: step 3303, loss 0.093927, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:57.072271: step 3304, loss 0.0951372, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:22:57.233612: step 3305, loss 0.125577, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:22:57.396148: step 3306, loss 0.0488467, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:22:57.557141: step 3307, loss 0.031415, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:22:57.720142: step 3308, loss 0.118628, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:22:57.881494: step 3309, loss 0.249951, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:22:58.042227: step 3310, loss 0.136134, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:22:58.205301: step 3311, loss 0.189065, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:22:58.367799: step 3312, loss 0.0902825, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:22:58.528396: step 3313, loss 0.122797, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:22:58.690243: step 3314, loss 0.0499756, acc 1, learning_rate 0.000100006
2017-10-10T11:22:58.860520: step 3315, loss 0.0699361, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:22:59.022148: step 3316, loss 0.134145, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:22:59.183360: step 3317, loss 0.0704618, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:22:59.345632: step 3318, loss 0.0643692, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:22:59.507441: step 3319, loss 0.0422402, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:22:59.668207: step 3320, loss 0.103527, acc 0.96875, learning_rate 0.000100006

Evaluation:
2017-10-10T11:23:00.130994: step 3320, loss 0.206778, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3320

2017-10-10T11:23:00.841296: step 3321, loss 0.0562145, acc 1, learning_rate 0.000100006
2017-10-10T11:23:01.005843: step 3322, loss 0.1172, acc 0.921875, learning_rate 0.000100006
2017-10-10T11:23:01.173841: step 3323, loss 0.0742346, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:01.337833: step 3324, loss 0.0930853, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:23:01.502646: step 3325, loss 0.0486123, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:23:01.660340: step 3326, loss 0.0676894, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:01.819125: step 3327, loss 0.100802, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:23:01.984467: step 3328, loss 0.0252188, acc 1, learning_rate 0.000100006
2017-10-10T11:23:02.146395: step 3329, loss 0.172151, acc 0.90625, learning_rate 0.000100006
2017-10-10T11:23:02.307521: step 3330, loss 0.244506, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:23:02.471771: step 3331, loss 0.0902389, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:23:02.606602: step 3332, loss 0.242917, acc 0.921569, learning_rate 0.000100006
2017-10-10T11:23:02.769139: step 3333, loss 0.109368, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:23:02.932651: step 3334, loss 0.0828216, acc 1, learning_rate 0.000100006
2017-10-10T11:23:03.095119: step 3335, loss 0.0504574, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:23:03.254991: step 3336, loss 0.0615409, acc 1, learning_rate 0.000100006
2017-10-10T11:23:03.419322: step 3337, loss 0.170654, acc 0.890625, learning_rate 0.000100006
2017-10-10T11:23:03.580198: step 3338, loss 0.0955805, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:03.741053: step 3339, loss 0.0703843, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:03.908991: step 3340, loss 0.1066, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:04.068883: step 3341, loss 0.0231546, acc 1, learning_rate 0.000100006
2017-10-10T11:23:04.230856: step 3342, loss 0.053143, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:23:04.392523: step 3343, loss 0.0719764, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:04.553333: step 3344, loss 0.0255503, acc 1, learning_rate 0.000100006
2017-10-10T11:23:04.713948: step 3345, loss 0.101839, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:23:04.875613: step 3346, loss 0.0972873, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:05.036861: step 3347, loss 0.118527, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:05.197211: step 3348, loss 0.128168, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:05.360549: step 3349, loss 0.074114, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:23:05.523062: step 3350, loss 0.0596803, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:23:05.683826: step 3351, loss 0.201811, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:23:05.847170: step 3352, loss 0.163205, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:06.009895: step 3353, loss 0.0424889, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:06.171466: step 3354, loss 0.0295115, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:06.331432: step 3355, loss 0.0719494, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:06.492936: step 3356, loss 0.15481, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:23:06.653213: step 3357, loss 0.0520211, acc 1, learning_rate 0.000100005
2017-10-10T11:23:06.811249: step 3358, loss 0.164717, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:06.978960: step 3359, loss 0.0434771, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:07.141183: step 3360, loss 0.0940799, acc 0.984375, learning_rate 0.000100005

Evaluation:
2017-10-10T11:23:07.644093: step 3360, loss 0.205956, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3360

2017-10-10T11:23:08.222201: step 3361, loss 0.12998, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:08.385556: step 3362, loss 0.0769309, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:08.544881: step 3363, loss 0.055168, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:08.706753: step 3364, loss 0.104378, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:08.868108: step 3365, loss 0.0558634, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:09.031527: step 3366, loss 0.0932773, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:09.192319: step 3367, loss 0.144074, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:09.355873: step 3368, loss 0.185892, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:23:09.517745: step 3369, loss 0.0274923, acc 1, learning_rate 0.000100005
2017-10-10T11:23:09.678450: step 3370, loss 0.0770783, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:09.843808: step 3371, loss 0.118401, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:23:10.009364: step 3372, loss 0.113472, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:10.173884: step 3373, loss 0.0963402, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:10.333559: step 3374, loss 0.0782379, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:10.494528: step 3375, loss 0.0651893, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:10.659351: step 3376, loss 0.093026, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:10.821906: step 3377, loss 0.0418585, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:10.995124: step 3378, loss 0.160524, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:23:11.158523: step 3379, loss 0.0612028, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:11.322688: step 3380, loss 0.108016, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:11.483766: step 3381, loss 0.128366, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:23:11.642134: step 3382, loss 0.16575, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:11.805860: step 3383, loss 0.0598653, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:11.970610: step 3384, loss 0.121655, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:12.133421: step 3385, loss 0.0971927, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:12.297450: step 3386, loss 0.205648, acc 0.90625, learning_rate 0.000100005
2017-10-10T11:23:12.461086: step 3387, loss 0.0107271, acc 1, learning_rate 0.000100005
2017-10-10T11:23:12.628513: step 3388, loss 0.0538671, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:12.790325: step 3389, loss 0.106363, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:12.956860: step 3390, loss 0.0635378, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:13.119213: step 3391, loss 0.0576574, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:13.280813: step 3392, loss 0.0800799, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:13.440267: step 3393, loss 0.121548, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:13.601887: step 3394, loss 0.0788587, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:23:13.761241: step 3395, loss 0.122459, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:13.923583: step 3396, loss 0.0815998, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:14.085013: step 3397, loss 0.115289, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:23:14.249685: step 3398, loss 0.107045, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:23:14.412028: step 3399, loss 0.0742506, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:23:14.571731: step 3400, loss 0.0578977, acc 0.984375, learning_rate 0.000100004

Evaluation:
2017-10-10T11:23:15.042237: step 3400, loss 0.205997, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3400

2017-10-10T11:23:15.687372: step 3401, loss 0.038018, acc 1, learning_rate 0.000100004
2017-10-10T11:23:15.852097: step 3402, loss 0.0982351, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:16.014102: step 3403, loss 0.0871712, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:16.186270: step 3404, loss 0.110224, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:16.350301: step 3405, loss 0.139304, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:16.513266: step 3406, loss 0.244737, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:16.677290: step 3407, loss 0.145082, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:16.834593: step 3408, loss 0.0294263, acc 1, learning_rate 0.000100004
2017-10-10T11:23:17.000973: step 3409, loss 0.0709344, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:17.165351: step 3410, loss 0.0799983, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:17.327156: step 3411, loss 0.0886308, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:17.489674: step 3412, loss 0.156433, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:17.653216: step 3413, loss 0.0191873, acc 1, learning_rate 0.000100004
2017-10-10T11:23:17.814228: step 3414, loss 0.0781399, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:17.991739: step 3415, loss 0.041336, acc 1, learning_rate 0.000100004
2017-10-10T11:23:18.153823: step 3416, loss 0.0743845, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:18.319169: step 3417, loss 0.0686356, acc 1, learning_rate 0.000100004
2017-10-10T11:23:18.483152: step 3418, loss 0.121801, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:18.646779: step 3419, loss 0.116256, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:23:18.807529: step 3420, loss 0.0737019, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:18.971695: step 3421, loss 0.0732766, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:19.135498: step 3422, loss 0.166666, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:19.303494: step 3423, loss 0.116596, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:19.466604: step 3424, loss 0.132915, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:19.626682: step 3425, loss 0.097457, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:23:19.790962: step 3426, loss 0.106694, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:23:19.954777: step 3427, loss 0.131204, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:23:20.115557: step 3428, loss 0.0924415, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:23:20.274878: step 3429, loss 0.095659, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:20.407858: step 3430, loss 0.0916602, acc 0.960784, learning_rate 0.000100004
2017-10-10T11:23:20.572350: step 3431, loss 0.069038, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:20.734350: step 3432, loss 0.0793451, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:20.903098: step 3433, loss 0.0473448, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:21.065030: step 3434, loss 0.036765, acc 1, learning_rate 0.000100004
2017-10-10T11:23:21.223635: step 3435, loss 0.0625152, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:21.385930: step 3436, loss 0.0511315, acc 1, learning_rate 0.000100004
2017-10-10T11:23:21.550432: step 3437, loss 0.0725205, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:21.711717: step 3438, loss 0.070787, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:21.870319: step 3439, loss 0.0877396, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:22.034723: step 3440, loss 0.16823, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-10T11:23:22.503911: step 3440, loss 0.203635, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3440

2017-10-10T11:23:23.141538: step 3441, loss 0.157019, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:23:23.301913: step 3442, loss 0.0672226, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:23.465394: step 3443, loss 0.108616, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:23.630493: step 3444, loss 0.053188, acc 1, learning_rate 0.000100004
2017-10-10T11:23:23.793021: step 3445, loss 0.104619, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:23.957431: step 3446, loss 0.142511, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:24.122583: step 3447, loss 0.289565, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:23:24.285750: step 3448, loss 0.0531366, acc 1, learning_rate 0.000100004
2017-10-10T11:23:24.447207: step 3449, loss 0.0547746, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:24.610675: step 3450, loss 0.0747429, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:24.775038: step 3451, loss 0.0165448, acc 1, learning_rate 0.000100004
2017-10-10T11:23:24.938194: step 3452, loss 0.064496, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:25.102621: step 3453, loss 0.0279617, acc 1, learning_rate 0.000100004
2017-10-10T11:23:25.266082: step 3454, loss 0.0654543, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:25.431778: step 3455, loss 0.0568774, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:25.592168: step 3456, loss 0.0598921, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:25.753466: step 3457, loss 0.0583833, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:25.915590: step 3458, loss 0.0846884, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:23:26.078055: step 3459, loss 0.0265796, acc 1, learning_rate 0.000100004
2017-10-10T11:23:26.241880: step 3460, loss 0.165307, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:23:26.402631: step 3461, loss 0.0959372, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:23:26.565282: step 3462, loss 0.123706, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:23:26.728763: step 3463, loss 0.0521725, acc 1, learning_rate 0.000100003
2017-10-10T11:23:26.892142: step 3464, loss 0.119227, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:27.077320: step 3465, loss 0.0925554, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:27.236770: step 3466, loss 0.160483, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:27.398651: step 3467, loss 0.101767, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:27.559383: step 3468, loss 0.0993639, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:27.719223: step 3469, loss 0.0675571, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:27.884899: step 3470, loss 0.0995898, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:28.045891: step 3471, loss 0.0671008, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:28.209108: step 3472, loss 0.0430524, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:28.371959: step 3473, loss 0.0513422, acc 1, learning_rate 0.000100003
2017-10-10T11:23:28.532661: step 3474, loss 0.076232, acc 1, learning_rate 0.000100003
2017-10-10T11:23:28.695403: step 3475, loss 0.0347057, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:28.866891: step 3476, loss 0.0650581, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:29.030177: step 3477, loss 0.151234, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:23:29.190083: step 3478, loss 0.0647824, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:29.353119: step 3479, loss 0.0707249, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:29.513588: step 3480, loss 0.143494, acc 0.90625, learning_rate 0.000100003

Evaluation:
2017-10-10T11:23:29.992610: step 3480, loss 0.204976, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3480

2017-10-10T11:23:30.705253: step 3481, loss 0.103155, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:30.870259: step 3482, loss 0.0292222, acc 1, learning_rate 0.000100003
2017-10-10T11:23:31.034242: step 3483, loss 0.116147, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:31.196955: step 3484, loss 0.0820478, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:31.361651: step 3485, loss 0.183988, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:23:31.526400: step 3486, loss 0.0476563, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:31.689705: step 3487, loss 0.0239163, acc 1, learning_rate 0.000100003
2017-10-10T11:23:31.852904: step 3488, loss 0.0732867, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:32.018576: step 3489, loss 0.0256867, acc 1, learning_rate 0.000100003
2017-10-10T11:23:32.180024: step 3490, loss 0.0674167, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:32.343627: step 3491, loss 0.129151, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:23:32.505269: step 3492, loss 0.0552768, acc 1, learning_rate 0.000100003
2017-10-10T11:23:32.669769: step 3493, loss 0.0411063, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:32.832661: step 3494, loss 0.0382673, acc 1, learning_rate 0.000100003
2017-10-10T11:23:32.999391: step 3495, loss 0.0755483, acc 1, learning_rate 0.000100003
2017-10-10T11:23:33.161445: step 3496, loss 0.0329642, acc 1, learning_rate 0.000100003
2017-10-10T11:23:33.325353: step 3497, loss 0.0481545, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:33.489740: step 3498, loss 0.222844, acc 0.890625, learning_rate 0.000100003
2017-10-10T11:23:33.650180: step 3499, loss 0.0812304, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:33.811660: step 3500, loss 0.0786478, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:33.975516: step 3501, loss 0.141731, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:34.136245: step 3502, loss 0.185638, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:34.300973: step 3503, loss 0.165743, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:23:34.465099: step 3504, loss 0.0579416, acc 1, learning_rate 0.000100003
2017-10-10T11:23:34.627698: step 3505, loss 0.0976258, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:34.791896: step 3506, loss 0.150008, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:23:34.956883: step 3507, loss 0.136184, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:35.117708: step 3508, loss 0.0410181, acc 1, learning_rate 0.000100003
2017-10-10T11:23:35.282719: step 3509, loss 0.116674, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:35.446753: step 3510, loss 0.155744, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:35.610663: step 3511, loss 0.147396, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:35.773808: step 3512, loss 0.173032, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:23:35.936255: step 3513, loss 0.0767523, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:36.105399: step 3514, loss 0.149591, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:23:36.267056: step 3515, loss 0.139773, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:36.431830: step 3516, loss 0.0574784, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:36.593690: step 3517, loss 0.0593817, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:36.757609: step 3518, loss 0.0647072, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:36.925409: step 3519, loss 0.082421, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:37.088792: step 3520, loss 0.0618501, acc 0.984375, learning_rate 0.000100003

Evaluation:
2017-10-10T11:23:37.564397: step 3520, loss 0.206113, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3520

2017-10-10T11:23:38.138005: step 3521, loss 0.279689, acc 0.890625, learning_rate 0.000100003
2017-10-10T11:23:38.303127: step 3522, loss 0.0804474, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:38.465377: step 3523, loss 0.103198, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:38.628414: step 3524, loss 0.123238, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:38.788421: step 3525, loss 0.063315, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:38.954802: step 3526, loss 0.0553682, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:39.119224: step 3527, loss 0.0666688, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:39.251880: step 3528, loss 0.0925474, acc 0.960784, learning_rate 0.000100003
2017-10-10T11:23:39.416401: step 3529, loss 0.190368, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:23:39.577458: step 3530, loss 0.119881, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:23:39.742524: step 3531, loss 0.115462, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:23:39.923436: step 3532, loss 0.0863907, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:40.085154: step 3533, loss 0.156333, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:23:40.247443: step 3534, loss 0.130263, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:23:40.409300: step 3535, loss 0.156322, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:40.569946: step 3536, loss 0.0526359, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:23:40.732956: step 3537, loss 0.100461, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:40.898330: step 3538, loss 0.108746, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:41.060782: step 3539, loss 0.12074, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:41.222796: step 3540, loss 0.129586, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:23:41.386686: step 3541, loss 0.133287, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:23:41.549514: step 3542, loss 0.0818771, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:41.714035: step 3543, loss 0.0782678, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:23:41.875830: step 3544, loss 0.076166, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:42.036565: step 3545, loss 0.0704084, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:42.209913: step 3546, loss 0.118246, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:23:42.374980: step 3547, loss 0.104904, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:42.537813: step 3548, loss 0.0408944, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:42.701345: step 3549, loss 0.0575241, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:42.863812: step 3550, loss 0.0439055, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:43.027089: step 3551, loss 0.0892625, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:43.188612: step 3552, loss 0.080209, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:43.349705: step 3553, loss 0.109971, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:43.509284: step 3554, loss 0.0435983, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:43.670625: step 3555, loss 0.151801, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:43.833226: step 3556, loss 0.0443526, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:43.998469: step 3557, loss 0.15201, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:44.156062: step 3558, loss 0.1496, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:44.319396: step 3559, loss 0.0846011, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:44.482504: step 3560, loss 0.0659928, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T11:23:44.967578: step 3560, loss 0.205291, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3560

2017-10-10T11:23:45.597527: step 3561, loss 0.0655161, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:45.761082: step 3562, loss 0.0628773, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:45.920413: step 3563, loss 0.204481, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:46.083175: step 3564, loss 0.0703605, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:46.247411: step 3565, loss 0.0382755, acc 1, learning_rate 0.000100002
2017-10-10T11:23:46.407033: step 3566, loss 0.0461053, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:46.569340: step 3567, loss 0.0846486, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:46.733713: step 3568, loss 0.0980616, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:46.902123: step 3569, loss 0.184264, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:23:47.066832: step 3570, loss 0.0879799, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:47.229292: step 3571, loss 0.117537, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:47.394974: step 3572, loss 0.0929693, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:47.556404: step 3573, loss 0.0950664, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:47.719275: step 3574, loss 0.0724483, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:47.884832: step 3575, loss 0.0379424, acc 1, learning_rate 0.000100002
2017-10-10T11:23:48.046445: step 3576, loss 0.100852, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:48.206980: step 3577, loss 0.111244, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:48.371323: step 3578, loss 0.079745, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:48.535582: step 3579, loss 0.0580152, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:48.695803: step 3580, loss 0.0408825, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:48.855467: step 3581, loss 0.0927125, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:49.023391: step 3582, loss 0.0575449, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:49.185635: step 3583, loss 0.20298, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:23:49.351179: step 3584, loss 0.0574328, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:49.513958: step 3585, loss 0.0419935, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:49.675576: step 3586, loss 0.105916, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:49.841130: step 3587, loss 0.0230381, acc 1, learning_rate 0.000100002
2017-10-10T11:23:50.006369: step 3588, loss 0.0520047, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:50.170199: step 3589, loss 0.0687446, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:50.333590: step 3590, loss 0.0900409, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:50.498689: step 3591, loss 0.015465, acc 1, learning_rate 0.000100002
2017-10-10T11:23:50.657960: step 3592, loss 0.13471, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:50.818609: step 3593, loss 0.19665, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:50.981950: step 3594, loss 0.0656529, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:51.146615: step 3595, loss 0.0784813, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:51.309481: step 3596, loss 0.0792466, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:51.475422: step 3597, loss 0.139494, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:23:51.639023: step 3598, loss 0.0347695, acc 1, learning_rate 0.000100002
2017-10-10T11:23:51.804221: step 3599, loss 0.103516, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:51.969801: step 3600, loss 0.0517045, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T11:23:52.442049: step 3600, loss 0.204589, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3600

2017-10-10T11:23:53.162518: step 3601, loss 0.0734807, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:53.323347: step 3602, loss 0.0713332, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:53.485707: step 3603, loss 0.0872264, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:53.647542: step 3604, loss 0.0215269, acc 1, learning_rate 0.000100002
2017-10-10T11:23:53.812104: step 3605, loss 0.0305207, acc 1, learning_rate 0.000100002
2017-10-10T11:23:53.974315: step 3606, loss 0.0499435, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:54.136272: step 3607, loss 0.0896104, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:54.297843: step 3608, loss 0.0964412, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:54.458198: step 3609, loss 0.0619467, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:54.625638: step 3610, loss 0.0839936, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:54.788203: step 3611, loss 0.0734319, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:54.949523: step 3612, loss 0.0523967, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:23:55.110379: step 3613, loss 0.107338, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:55.276907: step 3614, loss 0.0704451, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:55.439577: step 3615, loss 0.173175, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:55.601513: step 3616, loss 0.178606, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:55.765452: step 3617, loss 0.111767, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:55.937201: step 3618, loss 0.080687, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:56.100218: step 3619, loss 0.0991571, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:23:56.263676: step 3620, loss 0.0983767, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:23:56.424171: step 3621, loss 0.0983969, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:56.584148: step 3622, loss 0.157499, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:23:56.743279: step 3623, loss 0.0838284, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:56.908331: step 3624, loss 0.105377, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:57.073273: step 3625, loss 0.154649, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:23:57.208996: step 3626, loss 0.232378, acc 0.901961, learning_rate 0.000100002
2017-10-10T11:23:57.371492: step 3627, loss 0.275366, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:23:57.533280: step 3628, loss 0.14981, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:57.697780: step 3629, loss 0.0221138, acc 1, learning_rate 0.000100002
2017-10-10T11:23:57.862810: step 3630, loss 0.17835, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:23:58.026399: step 3631, loss 0.0789569, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:23:58.191515: step 3632, loss 0.145604, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:58.365104: step 3633, loss 0.0249059, acc 1, learning_rate 0.000100002
2017-10-10T11:23:58.527071: step 3634, loss 0.0626134, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:58.689597: step 3635, loss 0.0436394, acc 1, learning_rate 0.000100002
2017-10-10T11:23:58.856154: step 3636, loss 0.0884829, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:59.016659: step 3637, loss 0.0528755, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:23:59.179458: step 3638, loss 0.0374651, acc 1, learning_rate 0.000100002
2017-10-10T11:23:59.341340: step 3639, loss 0.0171414, acc 1, learning_rate 0.000100002
2017-10-10T11:23:59.503708: step 3640, loss 0.052418, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T11:23:59.977107: step 3640, loss 0.204926, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3640

2017-10-10T11:24:00.552548: step 3641, loss 0.19187, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:24:00.715304: step 3642, loss 0.175907, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:24:00.883310: step 3643, loss 0.0643476, acc 1, learning_rate 0.000100002
2017-10-10T11:24:01.045496: step 3644, loss 0.0522491, acc 1, learning_rate 0.000100002
2017-10-10T11:24:01.211494: step 3645, loss 0.137855, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:24:01.376541: step 3646, loss 0.0350218, acc 1, learning_rate 0.000100002
2017-10-10T11:24:01.538977: step 3647, loss 0.0432891, acc 1, learning_rate 0.000100002
2017-10-10T11:24:01.702416: step 3648, loss 0.12943, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:24:01.865666: step 3649, loss 0.106134, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:24:02.026151: step 3650, loss 0.0392609, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:24:02.186433: step 3651, loss 0.0429415, acc 1, learning_rate 0.000100002
2017-10-10T11:24:02.344157: step 3652, loss 0.146253, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:24:02.506374: step 3653, loss 0.0315892, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:24:02.668961: step 3654, loss 0.0982109, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:24:02.833536: step 3655, loss 0.0778692, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:24:02.996017: step 3656, loss 0.0785891, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:24:03.158486: step 3657, loss 0.105634, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:24:03.321888: step 3658, loss 0.0583987, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:24:03.483507: step 3659, loss 0.0388957, acc 1, learning_rate 0.000100002
2017-10-10T11:24:03.643973: step 3660, loss 0.0949368, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:24:03.805383: step 3661, loss 0.0680032, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:24:03.972621: step 3662, loss 0.142898, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:24:04.135332: step 3663, loss 0.0667263, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:24:04.298292: step 3664, loss 0.0837012, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:24:04.459438: step 3665, loss 0.0949936, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:24:04.619788: step 3666, loss 0.187159, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:24:04.781700: step 3667, loss 0.0567683, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:24:04.948958: step 3668, loss 0.0784441, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:24:05.109505: step 3669, loss 0.0216366, acc 1, learning_rate 0.000100001
2017-10-10T11:24:05.272144: step 3670, loss 0.100455, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:05.433487: step 3671, loss 0.0744377, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:05.596444: step 3672, loss 0.0623809, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:05.758577: step 3673, loss 0.0980253, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:05.927433: step 3674, loss 0.127086, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:06.088860: step 3675, loss 0.0258215, acc 1, learning_rate 0.000100001
2017-10-10T11:24:06.251883: step 3676, loss 0.056938, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:06.414632: step 3677, loss 0.0363096, acc 1, learning_rate 0.000100001
2017-10-10T11:24:06.578572: step 3678, loss 0.119211, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:06.742356: step 3679, loss 0.0299396, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:06.907003: step 3680, loss 0.063765, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:24:07.372724: step 3680, loss 0.204342, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3680

2017-10-10T11:24:08.013666: step 3681, loss 0.0725649, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:08.174427: step 3682, loss 0.0921057, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:08.338568: step 3683, loss 0.092576, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:08.498228: step 3684, loss 0.0716362, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:08.660125: step 3685, loss 0.199439, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:08.820905: step 3686, loss 0.0798646, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:08.994472: step 3687, loss 0.309688, acc 0.859375, learning_rate 0.000100001
2017-10-10T11:24:09.158950: step 3688, loss 0.0836383, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:09.318974: step 3689, loss 0.108529, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:09.478688: step 3690, loss 0.074672, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:09.642226: step 3691, loss 0.146753, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:09.804432: step 3692, loss 0.185158, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:09.969642: step 3693, loss 0.0799601, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:10.132453: step 3694, loss 0.253469, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:24:10.288354: step 3695, loss 0.0267124, acc 1, learning_rate 0.000100001
2017-10-10T11:24:10.451638: step 3696, loss 0.114153, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:10.613289: step 3697, loss 0.0974527, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:10.775496: step 3698, loss 0.0557491, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:10.945214: step 3699, loss 0.184496, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:11.107147: step 3700, loss 0.088374, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:11.269907: step 3701, loss 0.15625, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:11.431092: step 3702, loss 0.119624, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:11.591333: step 3703, loss 0.104897, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:11.755818: step 3704, loss 0.0825061, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:11.921749: step 3705, loss 0.130888, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:12.082464: step 3706, loss 0.123155, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:12.244191: step 3707, loss 0.0741757, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:12.406136: step 3708, loss 0.0833458, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:12.567959: step 3709, loss 0.076167, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:12.731287: step 3710, loss 0.0820931, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:12.895803: step 3711, loss 0.162948, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:13.058200: step 3712, loss 0.107659, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:13.219399: step 3713, loss 0.0266959, acc 1, learning_rate 0.000100001
2017-10-10T11:24:13.381801: step 3714, loss 0.0965966, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:13.544660: step 3715, loss 0.126773, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:13.704598: step 3716, loss 0.0664924, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:13.871114: step 3717, loss 0.14737, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:14.035189: step 3718, loss 0.0434734, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:14.197870: step 3719, loss 0.0533257, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:14.358521: step 3720, loss 0.211423, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-10T11:24:14.828431: step 3720, loss 0.204165, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3720

2017-10-10T11:24:15.546929: step 3721, loss 0.0394704, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:15.710160: step 3722, loss 0.143993, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:15.877686: step 3723, loss 0.139862, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:16.012392: step 3724, loss 0.0989615, acc 0.980392, learning_rate 0.000100001
2017-10-10T11:24:16.173849: step 3725, loss 0.0243516, acc 1, learning_rate 0.000100001
2017-10-10T11:24:16.337960: step 3726, loss 0.0827027, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:16.500523: step 3727, loss 0.091008, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:16.663297: step 3728, loss 0.100889, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:16.823836: step 3729, loss 0.0418733, acc 1, learning_rate 0.000100001
2017-10-10T11:24:16.990115: step 3730, loss 0.0770011, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:17.152887: step 3731, loss 0.11898, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:17.309885: step 3732, loss 0.131718, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:17.471159: step 3733, loss 0.0658423, acc 1, learning_rate 0.000100001
2017-10-10T11:24:17.635578: step 3734, loss 0.0563324, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:17.795982: step 3735, loss 0.17129, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:17.968158: step 3736, loss 0.0400528, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:18.138789: step 3737, loss 0.0521195, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:18.300539: step 3738, loss 0.0880165, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:18.459991: step 3739, loss 0.0814318, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:18.624469: step 3740, loss 0.101278, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:18.787880: step 3741, loss 0.164511, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:18.949320: step 3742, loss 0.140661, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:19.111621: step 3743, loss 0.0509984, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:19.272841: step 3744, loss 0.0985643, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:19.432431: step 3745, loss 0.0375081, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:19.598955: step 3746, loss 0.0753197, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:19.762078: step 3747, loss 0.0571925, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:19.926499: step 3748, loss 0.109344, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:20.091208: step 3749, loss 0.0838827, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:20.256792: step 3750, loss 0.137258, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:20.418099: step 3751, loss 0.0895734, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:20.583583: step 3752, loss 0.0927506, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:20.747590: step 3753, loss 0.0451436, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:20.918740: step 3754, loss 0.168593, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:21.077437: step 3755, loss 0.0905841, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:21.239899: step 3756, loss 0.0854133, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:21.403989: step 3757, loss 0.0646149, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:21.565305: step 3758, loss 0.0917599, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:21.728287: step 3759, loss 0.109714, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:21.890104: step 3760, loss 0.1503, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T11:24:22.349547: step 3760, loss 0.210389, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3760

2017-10-10T11:24:22.922438: step 3761, loss 0.170158, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:23.085804: step 3762, loss 0.0481442, acc 1, learning_rate 0.000100001
2017-10-10T11:24:23.249609: step 3763, loss 0.0928004, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:23.415120: step 3764, loss 0.110001, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:23.578847: step 3765, loss 0.074494, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:23.740442: step 3766, loss 0.0761053, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:23.911817: step 3767, loss 0.0790845, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:24.074895: step 3768, loss 0.0259806, acc 1, learning_rate 0.000100001
2017-10-10T11:24:24.237549: step 3769, loss 0.0961879, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:24.401012: step 3770, loss 0.136987, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:24.566374: step 3771, loss 0.121737, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:24.728853: step 3772, loss 0.176632, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:24.892401: step 3773, loss 0.0693545, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:25.054585: step 3774, loss 0.115116, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:25.218170: step 3775, loss 0.0572635, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:25.379538: step 3776, loss 0.0410664, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:25.542259: step 3777, loss 0.0411026, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:25.706241: step 3778, loss 0.103124, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:25.871892: step 3779, loss 0.0911421, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:26.032697: step 3780, loss 0.218777, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:26.193014: step 3781, loss 0.0549358, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:26.358167: step 3782, loss 0.0533879, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:26.518160: step 3783, loss 0.0950334, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:26.680589: step 3784, loss 0.0346598, acc 1, learning_rate 0.000100001
2017-10-10T11:24:26.842538: step 3785, loss 0.0570528, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:27.006316: step 3786, loss 0.0700703, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:27.167784: step 3787, loss 0.0860105, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:27.330271: step 3788, loss 0.075166, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:27.489431: step 3789, loss 0.153321, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:27.649943: step 3790, loss 0.106543, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:27.814246: step 3791, loss 0.0549677, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:27.978443: step 3792, loss 0.170616, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:28.142962: step 3793, loss 0.122978, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:28.307795: step 3794, loss 0.0870168, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:28.470861: step 3795, loss 0.0386117, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:28.632394: step 3796, loss 0.113785, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:28.792822: step 3797, loss 0.192136, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:28.954602: step 3798, loss 0.0143839, acc 1, learning_rate 0.000100001
2017-10-10T11:24:29.115045: step 3799, loss 0.0461656, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:29.278111: step 3800, loss 0.0451884, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:24:29.751233: step 3800, loss 0.203688, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3800

2017-10-10T11:24:30.403303: step 3801, loss 0.0421503, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:30.565369: step 3802, loss 0.111796, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:30.725362: step 3803, loss 0.0407606, acc 1, learning_rate 0.000100001
2017-10-10T11:24:30.892279: step 3804, loss 0.069254, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:31.054658: step 3805, loss 0.154291, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:31.217717: step 3806, loss 0.0752112, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:31.381462: step 3807, loss 0.0803351, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:31.543207: step 3808, loss 0.12898, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:31.707734: step 3809, loss 0.17586, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:31.871933: step 3810, loss 0.0904877, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:32.037333: step 3811, loss 0.0561135, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:32.200729: step 3812, loss 0.104626, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:32.361331: step 3813, loss 0.107045, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:32.520427: step 3814, loss 0.143922, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:32.683563: step 3815, loss 0.0376427, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:32.846794: step 3816, loss 0.119192, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:33.007966: step 3817, loss 0.070686, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:33.167954: step 3818, loss 0.170089, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:33.326934: step 3819, loss 0.102727, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:33.487852: step 3820, loss 0.0611971, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:33.648628: step 3821, loss 0.105132, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:33.785649: step 3822, loss 0.0954702, acc 0.960784, learning_rate 0.000100001
2017-10-10T11:24:33.947089: step 3823, loss 0.12638, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:34.114029: step 3824, loss 0.111632, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:34.276692: step 3825, loss 0.101442, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:34.436751: step 3826, loss 0.0737156, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:34.598084: step 3827, loss 0.0790789, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:34.759599: step 3828, loss 0.0619963, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:34.926433: step 3829, loss 0.0324621, acc 1, learning_rate 0.000100001
2017-10-10T11:24:35.090899: step 3830, loss 0.0855264, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:35.251400: step 3831, loss 0.0113822, acc 1, learning_rate 0.000100001
2017-10-10T11:24:35.414747: step 3832, loss 0.085445, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:35.580030: step 3833, loss 0.0752459, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:35.743626: step 3834, loss 0.0294069, acc 1, learning_rate 0.000100001
2017-10-10T11:24:35.912316: step 3835, loss 0.0690177, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:36.073881: step 3836, loss 0.150549, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:36.233119: step 3837, loss 0.122139, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:36.393642: step 3838, loss 0.17113, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:36.552611: step 3839, loss 0.0468972, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:36.716128: step 3840, loss 0.0371833, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-10T11:24:37.191476: step 3840, loss 0.205175, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3840

2017-10-10T11:24:37.910502: step 3841, loss 0.063871, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:38.073259: step 3842, loss 0.119334, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:38.232461: step 3843, loss 0.0522946, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:38.393432: step 3844, loss 0.0651236, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:38.555947: step 3845, loss 0.110691, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:38.719662: step 3846, loss 0.126102, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:38.884001: step 3847, loss 0.118256, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:39.045730: step 3848, loss 0.0822383, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:39.209457: step 3849, loss 0.0620944, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:39.373826: step 3850, loss 0.0761977, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:39.534994: step 3851, loss 0.0865109, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:39.702201: step 3852, loss 0.20248, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:39.867160: step 3853, loss 0.117815, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:40.025664: step 3854, loss 0.0672506, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:40.188713: step 3855, loss 0.0866395, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:40.353401: step 3856, loss 0.100339, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:40.516882: step 3857, loss 0.127853, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:40.677326: step 3858, loss 0.073017, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:40.838694: step 3859, loss 0.0991224, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:41.000526: step 3860, loss 0.0415923, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:41.163770: step 3861, loss 0.0761198, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:41.324985: step 3862, loss 0.0446888, acc 1, learning_rate 0.000100001
2017-10-10T11:24:41.488050: step 3863, loss 0.0392489, acc 1, learning_rate 0.000100001
2017-10-10T11:24:41.652701: step 3864, loss 0.0621389, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:41.812285: step 3865, loss 0.0833243, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:41.976443: step 3866, loss 0.0269716, acc 1, learning_rate 0.000100001
2017-10-10T11:24:42.136865: step 3867, loss 0.0879575, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:42.301896: step 3868, loss 0.0430886, acc 1, learning_rate 0.000100001
2017-10-10T11:24:42.466580: step 3869, loss 0.0735239, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:42.629854: step 3870, loss 0.0872513, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:42.791779: step 3871, loss 0.0637878, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:42.953221: step 3872, loss 0.121541, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:43.115876: step 3873, loss 0.0677207, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:43.278914: step 3874, loss 0.101431, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:43.441379: step 3875, loss 0.158412, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:43.603863: step 3876, loss 0.112854, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:43.764121: step 3877, loss 0.058153, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:43.930058: step 3878, loss 0.153399, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:44.092659: step 3879, loss 0.111782, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:44.254635: step 3880, loss 0.124743, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T11:24:44.727003: step 3880, loss 0.206789, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3880

2017-10-10T11:24:45.316176: step 3881, loss 0.0506127, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:45.479441: step 3882, loss 0.138798, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:45.640673: step 3883, loss 0.0585924, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:45.801382: step 3884, loss 0.10407, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:45.963737: step 3885, loss 0.197274, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:46.125007: step 3886, loss 0.0347797, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:46.288595: step 3887, loss 0.0541686, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:46.449951: step 3888, loss 0.0726466, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:46.611820: step 3889, loss 0.060377, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:46.772527: step 3890, loss 0.107435, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:46.940449: step 3891, loss 0.135516, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:47.100060: step 3892, loss 0.136437, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:47.266577: step 3893, loss 0.0291648, acc 1, learning_rate 0.000100001
2017-10-10T11:24:47.432256: step 3894, loss 0.0625946, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:47.594157: step 3895, loss 0.147552, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:47.759447: step 3896, loss 0.115699, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:47.922033: step 3897, loss 0.131112, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:48.083098: step 3898, loss 0.0526323, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:48.245967: step 3899, loss 0.0736137, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:48.409557: step 3900, loss 0.115661, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:48.572473: step 3901, loss 0.076191, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:48.732103: step 3902, loss 0.0821532, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:48.893979: step 3903, loss 0.135032, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:49.055097: step 3904, loss 0.113345, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:49.215651: step 3905, loss 0.0896242, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:49.378726: step 3906, loss 0.0977641, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:49.541601: step 3907, loss 0.223225, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:49.704069: step 3908, loss 0.123827, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:49.868623: step 3909, loss 0.0205835, acc 1, learning_rate 0.000100001
2017-10-10T11:24:50.032671: step 3910, loss 0.0353584, acc 1, learning_rate 0.000100001
2017-10-10T11:24:50.194891: step 3911, loss 0.130491, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:50.358546: step 3912, loss 0.0531316, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:50.522555: step 3913, loss 0.107953, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:50.683320: step 3914, loss 0.0701385, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:50.844860: step 3915, loss 0.117048, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:51.006456: step 3916, loss 0.0508001, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:51.168722: step 3917, loss 0.135727, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:51.332597: step 3918, loss 0.0346522, acc 1, learning_rate 0.000100001
2017-10-10T11:24:51.494292: step 3919, loss 0.0588829, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:51.629574: step 3920, loss 0.0576233, acc 0.980392, learning_rate 0.000100001

Evaluation:
2017-10-10T11:24:52.105952: step 3920, loss 0.201621, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3920

2017-10-10T11:24:52.751395: step 3921, loss 0.0911623, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:52.921141: step 3922, loss 0.0780314, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:53.085351: step 3923, loss 0.0884412, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:53.244091: step 3924, loss 0.108725, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:24:53.405644: step 3925, loss 0.041997, acc 1, learning_rate 0.000100001
2017-10-10T11:24:53.567263: step 3926, loss 0.0930221, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:53.727305: step 3927, loss 0.0654146, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:53.890882: step 3928, loss 0.14285, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:54.052427: step 3929, loss 0.0819263, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:54.215535: step 3930, loss 0.125058, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:24:54.379016: step 3931, loss 0.0706718, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:54.539641: step 3932, loss 0.0869661, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:24:54.700846: step 3933, loss 0.0721012, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:54.868524: step 3934, loss 0.046172, acc 1, learning_rate 0.000100001
2017-10-10T11:24:55.029572: step 3935, loss 0.117416, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:24:55.192816: step 3936, loss 0.104359, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:24:55.355508: step 3937, loss 0.0918845, acc 0.953125, learning_rate 0.0001
2017-10-10T11:24:55.519948: step 3938, loss 0.111026, acc 0.9375, learning_rate 0.0001
2017-10-10T11:24:55.683192: step 3939, loss 0.0494822, acc 1, learning_rate 0.0001
2017-10-10T11:24:55.845703: step 3940, loss 0.0710948, acc 0.96875, learning_rate 0.0001
2017-10-10T11:24:56.005282: step 3941, loss 0.0710531, acc 0.96875, learning_rate 0.0001
2017-10-10T11:24:56.164197: step 3942, loss 0.107489, acc 0.953125, learning_rate 0.0001
2017-10-10T11:24:56.324369: step 3943, loss 0.181278, acc 0.9375, learning_rate 0.0001
2017-10-10T11:24:56.489153: step 3944, loss 0.0611354, acc 0.984375, learning_rate 0.0001
2017-10-10T11:24:56.649657: step 3945, loss 0.0580777, acc 0.984375, learning_rate 0.0001
2017-10-10T11:24:56.810193: step 3946, loss 0.0452044, acc 0.984375, learning_rate 0.0001
2017-10-10T11:24:56.972904: step 3947, loss 0.0867767, acc 0.953125, learning_rate 0.0001
2017-10-10T11:24:57.132181: step 3948, loss 0.137156, acc 0.96875, learning_rate 0.0001
2017-10-10T11:24:57.295800: step 3949, loss 0.0920769, acc 0.96875, learning_rate 0.0001
2017-10-10T11:24:57.459293: step 3950, loss 0.118777, acc 0.96875, learning_rate 0.0001
2017-10-10T11:24:57.625841: step 3951, loss 0.126221, acc 0.90625, learning_rate 0.0001
2017-10-10T11:24:57.788450: step 3952, loss 0.0607098, acc 0.984375, learning_rate 0.0001
2017-10-10T11:24:57.956277: step 3953, loss 0.0962519, acc 0.953125, learning_rate 0.0001
2017-10-10T11:24:58.118719: step 3954, loss 0.0580215, acc 0.984375, learning_rate 0.0001
2017-10-10T11:24:58.281479: step 3955, loss 0.173653, acc 0.9375, learning_rate 0.0001
2017-10-10T11:24:58.442232: step 3956, loss 0.0246832, acc 1, learning_rate 0.0001
2017-10-10T11:24:58.607026: step 3957, loss 0.0964026, acc 0.984375, learning_rate 0.0001
2017-10-10T11:24:58.767841: step 3958, loss 0.114966, acc 0.953125, learning_rate 0.0001
2017-10-10T11:24:58.932236: step 3959, loss 0.0913978, acc 0.984375, learning_rate 0.0001
2017-10-10T11:24:59.093308: step 3960, loss 0.0605495, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:24:59.581833: step 3960, loss 0.203112, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-3960

2017-10-10T11:25:00.295546: step 3961, loss 0.139493, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:00.455608: step 3962, loss 0.0405733, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:00.617107: step 3963, loss 0.0162524, acc 1, learning_rate 0.0001
2017-10-10T11:25:00.777317: step 3964, loss 0.156451, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:00.944343: step 3965, loss 0.109815, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:01.109917: step 3966, loss 0.134508, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:01.278539: step 3967, loss 0.0633475, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:01.443254: step 3968, loss 0.0541676, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:01.607480: step 3969, loss 0.0921104, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:01.770684: step 3970, loss 0.0532332, acc 1, learning_rate 0.0001
2017-10-10T11:25:01.935150: step 3971, loss 0.102521, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:02.097159: step 3972, loss 0.0584321, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:02.260951: step 3973, loss 0.076794, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:02.427132: step 3974, loss 0.105539, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:02.588395: step 3975, loss 0.0636177, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:02.752399: step 3976, loss 0.0615536, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:02.913541: step 3977, loss 0.0765999, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:03.078073: step 3978, loss 0.0431438, acc 1, learning_rate 0.0001
2017-10-10T11:25:03.239044: step 3979, loss 0.10132, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:03.399055: step 3980, loss 0.0814915, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:03.562012: step 3981, loss 0.0891776, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:03.725006: step 3982, loss 0.0490546, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:03.892095: step 3983, loss 0.0635074, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:04.053208: step 3984, loss 0.117907, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:04.214647: step 3985, loss 0.0645448, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:04.376708: step 3986, loss 0.0977591, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:04.538525: step 3987, loss 0.0663484, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:04.702808: step 3988, loss 0.0346472, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:04.866852: step 3989, loss 0.114721, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:05.028260: step 3990, loss 0.129548, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:05.191432: step 3991, loss 0.0828593, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:05.353091: step 3992, loss 0.0811342, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:05.513676: step 3993, loss 0.0578728, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:05.675905: step 3994, loss 0.270448, acc 0.875, learning_rate 0.0001
2017-10-10T11:25:05.838887: step 3995, loss 0.0721525, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:06.001717: step 3996, loss 0.1624, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:06.164121: step 3997, loss 0.170674, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:06.321819: step 3998, loss 0.0956016, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:06.483460: step 3999, loss 0.195127, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:06.643343: step 4000, loss 0.0151628, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:25:07.136855: step 4000, loss 0.212357, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4000

2017-10-10T11:25:07.709616: step 4001, loss 0.138942, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:07.874151: step 4002, loss 0.0842682, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:08.037166: step 4003, loss 0.0748688, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:08.203180: step 4004, loss 0.0332553, acc 1, learning_rate 0.0001
2017-10-10T11:25:08.366675: step 4005, loss 0.0577284, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:08.530852: step 4006, loss 0.106477, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:08.695084: step 4007, loss 0.127467, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:08.859104: step 4008, loss 0.145729, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:09.032712: step 4009, loss 0.0833292, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:09.205810: step 4010, loss 0.141678, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:09.394752: step 4011, loss 0.116958, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:09.556136: step 4012, loss 0.189986, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:09.718445: step 4013, loss 0.136659, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:09.890188: step 4014, loss 0.171493, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:10.052772: step 4015, loss 0.189478, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:10.213437: step 4016, loss 0.105562, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:10.376518: step 4017, loss 0.0464109, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:10.516698: step 4018, loss 0.0359984, acc 0.980392, learning_rate 0.0001
2017-10-10T11:25:10.680171: step 4019, loss 0.0525832, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:10.842883: step 4020, loss 0.0369683, acc 1, learning_rate 0.0001
2017-10-10T11:25:11.009135: step 4021, loss 0.113038, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:11.167999: step 4022, loss 0.184327, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:11.329143: step 4023, loss 0.0628301, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:11.490247: step 4024, loss 0.0479365, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:11.651605: step 4025, loss 0.147462, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:11.814579: step 4026, loss 0.15319, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:11.979467: step 4027, loss 0.0887059, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:12.141891: step 4028, loss 0.0803637, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:12.306073: step 4029, loss 0.0946815, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:12.468581: step 4030, loss 0.0511145, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:12.630608: step 4031, loss 0.111439, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:12.793768: step 4032, loss 0.0680662, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:12.955532: step 4033, loss 0.0536872, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:13.118534: step 4034, loss 0.0912839, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:13.279512: step 4035, loss 0.112529, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:13.442785: step 4036, loss 0.0985751, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:13.606779: step 4037, loss 0.0510911, acc 1, learning_rate 0.0001
2017-10-10T11:25:13.769207: step 4038, loss 0.0622444, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:13.942569: step 4039, loss 0.101701, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:14.106154: step 4040, loss 0.117659, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:25:14.576129: step 4040, loss 0.204858, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4040

2017-10-10T11:25:15.220579: step 4041, loss 0.0559512, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:15.382472: step 4042, loss 0.0934003, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:15.544108: step 4043, loss 0.0680548, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:15.702912: step 4044, loss 0.104969, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:15.866240: step 4045, loss 0.0402623, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:16.027647: step 4046, loss 0.177672, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:16.190656: step 4047, loss 0.136785, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:16.351266: step 4048, loss 0.141563, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:16.514463: step 4049, loss 0.0689832, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:16.673721: step 4050, loss 0.127014, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:16.836590: step 4051, loss 0.0632269, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:17.000054: step 4052, loss 0.091184, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:17.164456: step 4053, loss 0.0242142, acc 1, learning_rate 0.0001
2017-10-10T11:25:17.324506: step 4054, loss 0.0555174, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:17.484901: step 4055, loss 0.1358, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:17.650077: step 4056, loss 0.035833, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:17.815217: step 4057, loss 0.0751092, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:17.983342: step 4058, loss 0.0390942, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:18.146816: step 4059, loss 0.100157, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:18.308760: step 4060, loss 0.0338182, acc 1, learning_rate 0.0001
2017-10-10T11:25:18.468364: step 4061, loss 0.130162, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:18.627337: step 4062, loss 0.103068, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:18.788258: step 4063, loss 0.0396066, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:18.953090: step 4064, loss 0.0982467, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:19.116700: step 4065, loss 0.102162, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:19.280588: step 4066, loss 0.137488, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:19.443112: step 4067, loss 0.119599, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:19.604078: step 4068, loss 0.0600674, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:19.767385: step 4069, loss 0.0816729, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:19.931744: step 4070, loss 0.0586369, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:20.096272: step 4071, loss 0.213997, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:20.265299: step 4072, loss 0.0462334, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:20.427207: step 4073, loss 0.0801815, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:20.588025: step 4074, loss 0.136732, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:20.749572: step 4075, loss 0.116734, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:20.916608: step 4076, loss 0.0501404, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:21.080900: step 4077, loss 0.0790122, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:21.245938: step 4078, loss 0.108272, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:21.405938: step 4079, loss 0.144232, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:21.568931: step 4080, loss 0.103254, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:25:22.043619: step 4080, loss 0.20491, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4080

2017-10-10T11:25:22.684236: step 4081, loss 0.116448, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:22.846357: step 4082, loss 0.0580315, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:23.007671: step 4083, loss 0.0408985, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:23.165616: step 4084, loss 0.0919856, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:23.328079: step 4085, loss 0.103706, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:23.490063: step 4086, loss 0.0338471, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:23.653652: step 4087, loss 0.0442736, acc 1, learning_rate 0.0001
2017-10-10T11:25:23.815221: step 4088, loss 0.0708961, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:23.978947: step 4089, loss 0.0682296, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:24.137931: step 4090, loss 0.0706035, acc 1, learning_rate 0.0001
2017-10-10T11:25:24.301128: step 4091, loss 0.0523486, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:24.464564: step 4092, loss 0.0294476, acc 1, learning_rate 0.0001
2017-10-10T11:25:24.625571: step 4093, loss 0.0675022, acc 1, learning_rate 0.0001
2017-10-10T11:25:24.784865: step 4094, loss 0.0395879, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:24.949146: step 4095, loss 0.081144, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:25.113758: step 4096, loss 0.0787861, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:25.278039: step 4097, loss 0.104428, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:25.442768: step 4098, loss 0.0738376, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:25.607428: step 4099, loss 0.0487861, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:25.767518: step 4100, loss 0.0290719, acc 1, learning_rate 0.0001
2017-10-10T11:25:25.928395: step 4101, loss 0.104713, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:26.091963: step 4102, loss 0.05189, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:26.253507: step 4103, loss 0.0291908, acc 1, learning_rate 0.0001
2017-10-10T11:25:26.417330: step 4104, loss 0.110374, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:26.578937: step 4105, loss 0.0656964, acc 1, learning_rate 0.0001
2017-10-10T11:25:26.738903: step 4106, loss 0.038152, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:26.907060: step 4107, loss 0.0338574, acc 1, learning_rate 0.0001
2017-10-10T11:25:27.069425: step 4108, loss 0.10502, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:27.230799: step 4109, loss 0.0993518, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:27.392892: step 4110, loss 0.0351814, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:27.556505: step 4111, loss 0.0521393, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:27.721144: step 4112, loss 0.0778649, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:27.886624: step 4113, loss 0.0654636, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:28.048511: step 4114, loss 0.0385087, acc 1, learning_rate 0.0001
2017-10-10T11:25:28.208923: step 4115, loss 0.0663302, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:28.345876: step 4116, loss 0.103372, acc 0.960784, learning_rate 0.0001
2017-10-10T11:25:28.510738: step 4117, loss 0.046412, acc 1, learning_rate 0.0001
2017-10-10T11:25:28.673566: step 4118, loss 0.0462415, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:28.836601: step 4119, loss 0.0730711, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:29.000677: step 4120, loss 0.0867822, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:25:29.473940: step 4120, loss 0.200889, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4120

2017-10-10T11:25:30.186556: step 4121, loss 0.166509, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:30.346217: step 4122, loss 0.0613647, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:30.509908: step 4123, loss 0.070388, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:30.669759: step 4124, loss 0.1943, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:30.832405: step 4125, loss 0.191385, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:30.997595: step 4126, loss 0.129328, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:31.160243: step 4127, loss 0.0702528, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:31.324630: step 4128, loss 0.126938, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:31.487134: step 4129, loss 0.102079, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:31.651639: step 4130, loss 0.0685986, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:31.813319: step 4131, loss 0.0885873, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:31.976474: step 4132, loss 0.0570178, acc 1, learning_rate 0.0001
2017-10-10T11:25:32.139147: step 4133, loss 0.1547, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:32.302666: step 4134, loss 0.167608, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:32.464289: step 4135, loss 0.0266583, acc 1, learning_rate 0.0001
2017-10-10T11:25:32.629604: step 4136, loss 0.112571, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:32.790264: step 4137, loss 0.070068, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:32.958832: step 4138, loss 0.0952418, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:33.124755: step 4139, loss 0.0950312, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:33.288123: step 4140, loss 0.132061, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:33.450028: step 4141, loss 0.0799439, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:33.611775: step 4142, loss 0.0959341, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:33.775394: step 4143, loss 0.0281753, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:33.941023: step 4144, loss 0.0671447, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:34.103429: step 4145, loss 0.133415, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:34.265586: step 4146, loss 0.0812416, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:34.428011: step 4147, loss 0.0244901, acc 1, learning_rate 0.0001
2017-10-10T11:25:34.587428: step 4148, loss 0.121392, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:34.751224: step 4149, loss 0.0703918, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:34.914920: step 4150, loss 0.0359067, acc 1, learning_rate 0.0001
2017-10-10T11:25:35.078378: step 4151, loss 0.176382, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:35.242373: step 4152, loss 0.0769046, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:35.409705: step 4153, loss 0.0403688, acc 1, learning_rate 0.0001
2017-10-10T11:25:35.572017: step 4154, loss 0.0933981, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:35.732552: step 4155, loss 0.0746913, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:35.907248: step 4156, loss 0.0618624, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:36.072075: step 4157, loss 0.0870184, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:36.232580: step 4158, loss 0.135027, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:36.393793: step 4159, loss 0.238361, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:36.557187: step 4160, loss 0.0618659, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:25:37.032576: step 4160, loss 0.206068, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4160

2017-10-10T11:25:37.607760: step 4161, loss 0.0482888, acc 1, learning_rate 0.0001
2017-10-10T11:25:37.769441: step 4162, loss 0.0630962, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:37.939674: step 4163, loss 0.0397883, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:38.103173: step 4164, loss 0.0621139, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:38.266003: step 4165, loss 0.0307733, acc 1, learning_rate 0.0001
2017-10-10T11:25:38.428283: step 4166, loss 0.0580063, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:38.591007: step 4167, loss 0.118812, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:38.758094: step 4168, loss 0.0815279, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:38.923430: step 4169, loss 0.172715, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:39.087945: step 4170, loss 0.0569771, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:39.252772: step 4171, loss 0.077796, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:39.415353: step 4172, loss 0.0366655, acc 1, learning_rate 0.0001
2017-10-10T11:25:39.576890: step 4173, loss 0.0709832, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:39.739013: step 4174, loss 0.0887893, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:39.904796: step 4175, loss 0.0658768, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:40.068366: step 4176, loss 0.196172, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:40.233210: step 4177, loss 0.0446851, acc 1, learning_rate 0.0001
2017-10-10T11:25:40.394269: step 4178, loss 0.0783779, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:40.555970: step 4179, loss 0.073278, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:40.715948: step 4180, loss 0.223603, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:40.879607: step 4181, loss 0.144812, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:41.041750: step 4182, loss 0.163096, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:41.203668: step 4183, loss 0.027884, acc 1, learning_rate 0.0001
2017-10-10T11:25:41.364785: step 4184, loss 0.109575, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:41.527493: step 4185, loss 0.0767234, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:41.690198: step 4186, loss 0.0629192, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:41.854667: step 4187, loss 0.129865, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:42.014815: step 4188, loss 0.113416, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:42.176665: step 4189, loss 0.0225018, acc 1, learning_rate 0.0001
2017-10-10T11:25:42.341904: step 4190, loss 0.184188, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:42.504139: step 4191, loss 0.0957128, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:42.668746: step 4192, loss 0.132731, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:42.832349: step 4193, loss 0.078124, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:43.008474: step 4194, loss 0.096126, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:43.171598: step 4195, loss 0.0664344, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:43.334230: step 4196, loss 0.0441121, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:43.496782: step 4197, loss 0.106667, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:43.656224: step 4198, loss 0.0794792, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:43.819040: step 4199, loss 0.181277, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:43.987777: step 4200, loss 0.0890908, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:25:44.451741: step 4200, loss 0.19955, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4200

2017-10-10T11:25:45.098618: step 4201, loss 0.0981152, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:45.262477: step 4202, loss 0.246026, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:45.425698: step 4203, loss 0.0925558, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:45.587372: step 4204, loss 0.0941238, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:45.750622: step 4205, loss 0.196084, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:45.914117: step 4206, loss 0.0823296, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:46.076996: step 4207, loss 0.0760522, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:46.237847: step 4208, loss 0.0459511, acc 1, learning_rate 0.0001
2017-10-10T11:25:46.398941: step 4209, loss 0.0513266, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:46.560316: step 4210, loss 0.0655354, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:46.724521: step 4211, loss 0.123109, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:46.885860: step 4212, loss 0.0476438, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:47.054968: step 4213, loss 0.0583735, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:47.190843: step 4214, loss 0.106029, acc 0.980392, learning_rate 0.0001
2017-10-10T11:25:47.351829: step 4215, loss 0.176708, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:47.513963: step 4216, loss 0.0442695, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:47.677222: step 4217, loss 0.111057, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:47.841479: step 4218, loss 0.060031, acc 1, learning_rate 0.0001
2017-10-10T11:25:48.003917: step 4219, loss 0.105079, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:48.168139: step 4220, loss 0.0394042, acc 1, learning_rate 0.0001
2017-10-10T11:25:48.330261: step 4221, loss 0.0607911, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:48.493527: step 4222, loss 0.036926, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:48.657718: step 4223, loss 0.0319946, acc 1, learning_rate 0.0001
2017-10-10T11:25:48.817209: step 4224, loss 0.128057, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:48.980424: step 4225, loss 0.0712587, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:49.143529: step 4226, loss 0.0441972, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:49.305771: step 4227, loss 0.180974, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:49.465071: step 4228, loss 0.187832, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:49.626806: step 4229, loss 0.0554244, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:49.788372: step 4230, loss 0.0588973, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:49.955861: step 4231, loss 0.0468375, acc 1, learning_rate 0.0001
2017-10-10T11:25:50.121195: step 4232, loss 0.0468636, acc 1, learning_rate 0.0001
2017-10-10T11:25:50.280494: step 4233, loss 0.121802, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:50.445659: step 4234, loss 0.0232072, acc 1, learning_rate 0.0001
2017-10-10T11:25:50.606617: step 4235, loss 0.0262247, acc 1, learning_rate 0.0001
2017-10-10T11:25:50.768063: step 4236, loss 0.0733365, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:50.931080: step 4237, loss 0.0281752, acc 1, learning_rate 0.0001
2017-10-10T11:25:51.099245: step 4238, loss 0.139383, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:51.262885: step 4239, loss 0.0766294, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:51.422105: step 4240, loss 0.168331, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T11:25:51.905950: step 4240, loss 0.200731, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4240

2017-10-10T11:25:52.621792: step 4241, loss 0.0675405, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:52.785727: step 4242, loss 0.0751132, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:52.952309: step 4243, loss 0.0716056, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:53.124194: step 4244, loss 0.0897618, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:53.296358: step 4245, loss 0.0773556, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:53.456414: step 4246, loss 0.196249, acc 0.921875, learning_rate 0.0001
2017-10-10T11:25:53.619344: step 4247, loss 0.049494, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:53.780911: step 4248, loss 0.0374482, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:53.944962: step 4249, loss 0.0838366, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:54.111612: step 4250, loss 0.0874608, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:54.273749: step 4251, loss 0.0617064, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:54.438268: step 4252, loss 0.0268389, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:54.601931: step 4253, loss 0.113878, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:54.761161: step 4254, loss 0.191148, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:54.925540: step 4255, loss 0.0305377, acc 1, learning_rate 0.0001
2017-10-10T11:25:55.083630: step 4256, loss 0.0982285, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:55.245106: step 4257, loss 0.0537425, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:55.407983: step 4258, loss 0.0921949, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:55.570172: step 4259, loss 0.0518812, acc 1, learning_rate 0.0001
2017-10-10T11:25:55.738206: step 4260, loss 0.131827, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:55.907882: step 4261, loss 0.0537604, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:56.071045: step 4262, loss 0.142697, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:56.231264: step 4263, loss 0.107712, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:56.393991: step 4264, loss 0.102167, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:56.558794: step 4265, loss 0.016728, acc 1, learning_rate 0.0001
2017-10-10T11:25:56.719182: step 4266, loss 0.057334, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:56.881024: step 4267, loss 0.100574, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:57.044086: step 4268, loss 0.0548785, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:57.205523: step 4269, loss 0.120095, acc 0.9375, learning_rate 0.0001
2017-10-10T11:25:57.367616: step 4270, loss 0.0545141, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:57.529937: step 4271, loss 0.0664374, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:57.699453: step 4272, loss 0.0349426, acc 0.984375, learning_rate 0.0001
2017-10-10T11:25:57.865423: step 4273, loss 0.170439, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:58.026718: step 4274, loss 0.0744438, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:58.190605: step 4275, loss 0.0845038, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:58.355569: step 4276, loss 0.0877632, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:58.519874: step 4277, loss 0.102638, acc 0.953125, learning_rate 0.0001
2017-10-10T11:25:58.680730: step 4278, loss 0.120253, acc 0.96875, learning_rate 0.0001
2017-10-10T11:25:58.844579: step 4279, loss 0.0379101, acc 1, learning_rate 0.0001
2017-10-10T11:25:59.008400: step 4280, loss 0.176646, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:25:59.484965: step 4280, loss 0.205783, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4280

2017-10-10T11:26:00.058061: step 4281, loss 0.092699, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:00.223292: step 4282, loss 0.0716678, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:00.386386: step 4283, loss 0.0899082, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:00.546234: step 4284, loss 0.0834723, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:00.709026: step 4285, loss 0.058595, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:00.875061: step 4286, loss 0.0692111, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:01.037650: step 4287, loss 0.191262, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:01.202042: step 4288, loss 0.0802393, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:01.361421: step 4289, loss 0.104282, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:01.531077: step 4290, loss 0.0652176, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:01.690784: step 4291, loss 0.101014, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:01.852753: step 4292, loss 0.0174503, acc 1, learning_rate 0.0001
2017-10-10T11:26:02.013448: step 4293, loss 0.0670239, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:02.176820: step 4294, loss 0.0644182, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:02.337437: step 4295, loss 0.0963284, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:02.497959: step 4296, loss 0.0564258, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:02.660922: step 4297, loss 0.0493327, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:02.822549: step 4298, loss 0.0316436, acc 1, learning_rate 0.0001
2017-10-10T11:26:02.987162: step 4299, loss 0.137083, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:03.150199: step 4300, loss 0.112452, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:03.315414: step 4301, loss 0.137848, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:03.476347: step 4302, loss 0.103197, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:03.634635: step 4303, loss 0.0212153, acc 1, learning_rate 0.0001
2017-10-10T11:26:03.800172: step 4304, loss 0.0586319, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:03.965107: step 4305, loss 0.0239032, acc 1, learning_rate 0.0001
2017-10-10T11:26:04.128219: step 4306, loss 0.0762249, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:04.289971: step 4307, loss 0.118843, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:04.454069: step 4308, loss 0.115868, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:04.618393: step 4309, loss 0.134751, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:04.779459: step 4310, loss 0.0603521, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:04.951915: step 4311, loss 0.0798137, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:05.086427: step 4312, loss 0.0960729, acc 0.980392, learning_rate 0.0001
2017-10-10T11:26:05.246819: step 4313, loss 0.0317975, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:05.412637: step 4314, loss 0.247962, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:05.572096: step 4315, loss 0.148246, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:05.734355: step 4316, loss 0.0274176, acc 1, learning_rate 0.0001
2017-10-10T11:26:05.898973: step 4317, loss 0.0337606, acc 1, learning_rate 0.0001
2017-10-10T11:26:06.065241: step 4318, loss 0.140152, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:06.229350: step 4319, loss 0.096269, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:06.390758: step 4320, loss 0.0966223, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:26:06.864059: step 4320, loss 0.204776, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4320

2017-10-10T11:26:07.509571: step 4321, loss 0.134372, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:07.669983: step 4322, loss 0.0739058, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:07.834824: step 4323, loss 0.0482356, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:07.996188: step 4324, loss 0.109513, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:08.154820: step 4325, loss 0.104758, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:08.320340: step 4326, loss 0.0843982, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:08.482514: step 4327, loss 0.0234524, acc 1, learning_rate 0.0001
2017-10-10T11:26:08.641597: step 4328, loss 0.0597891, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:08.802462: step 4329, loss 0.0404776, acc 1, learning_rate 0.0001
2017-10-10T11:26:08.965283: step 4330, loss 0.0890113, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:09.127301: step 4331, loss 0.0895324, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:09.286662: step 4332, loss 0.0567979, acc 1, learning_rate 0.0001
2017-10-10T11:26:09.446849: step 4333, loss 0.0749033, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:09.608799: step 4334, loss 0.0513423, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:09.768966: step 4335, loss 0.138528, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:09.940594: step 4336, loss 0.106466, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:10.105251: step 4337, loss 0.0724773, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:10.268770: step 4338, loss 0.20937, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:10.429577: step 4339, loss 0.0521564, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:10.593882: step 4340, loss 0.03043, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:10.753999: step 4341, loss 0.18247, acc 0.90625, learning_rate 0.0001
2017-10-10T11:26:10.918188: step 4342, loss 0.0760371, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:11.079847: step 4343, loss 0.0458911, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:11.240143: step 4344, loss 0.0957683, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:11.401290: step 4345, loss 0.0262922, acc 1, learning_rate 0.0001
2017-10-10T11:26:11.563932: step 4346, loss 0.060803, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:11.728195: step 4347, loss 0.0822913, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:11.897011: step 4348, loss 0.178686, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:12.057865: step 4349, loss 0.0902731, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:12.219178: step 4350, loss 0.169196, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:12.376870: step 4351, loss 0.0482437, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:12.535134: step 4352, loss 0.0897115, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:12.693988: step 4353, loss 0.12561, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:12.868164: step 4354, loss 0.0609779, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:13.031765: step 4355, loss 0.130494, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:13.195712: step 4356, loss 0.0983983, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:13.357833: step 4357, loss 0.120234, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:13.520692: step 4358, loss 0.0594719, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:13.682431: step 4359, loss 0.129343, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:13.849462: step 4360, loss 0.139798, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T11:26:14.315315: step 4360, loss 0.203322, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4360

2017-10-10T11:26:15.041322: step 4361, loss 0.178514, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:15.204349: step 4362, loss 0.0386721, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:15.366962: step 4363, loss 0.08348, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:15.530586: step 4364, loss 0.0531422, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:15.692881: step 4365, loss 0.1042, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:15.859022: step 4366, loss 0.0482844, acc 1, learning_rate 0.0001
2017-10-10T11:26:16.020234: step 4367, loss 0.0838879, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:16.185158: step 4368, loss 0.094957, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:16.348533: step 4369, loss 0.017596, acc 1, learning_rate 0.0001
2017-10-10T11:26:16.510602: step 4370, loss 0.10277, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:16.672884: step 4371, loss 0.0668108, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:16.840324: step 4372, loss 0.0481353, acc 1, learning_rate 0.0001
2017-10-10T11:26:17.008666: step 4373, loss 0.068329, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:17.167188: step 4374, loss 0.0578097, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:17.332680: step 4375, loss 0.092035, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:17.493412: step 4376, loss 0.0629642, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:17.656907: step 4377, loss 0.0676525, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:17.819088: step 4378, loss 0.0501403, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:17.981328: step 4379, loss 0.0942486, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:18.140916: step 4380, loss 0.0650176, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:18.302216: step 4381, loss 0.103807, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:18.464339: step 4382, loss 0.136448, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:18.631653: step 4383, loss 0.0280354, acc 1, learning_rate 0.0001
2017-10-10T11:26:18.793225: step 4384, loss 0.027676, acc 1, learning_rate 0.0001
2017-10-10T11:26:18.960949: step 4385, loss 0.0710549, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:19.124138: step 4386, loss 0.0601143, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:19.283579: step 4387, loss 0.155937, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:19.442827: step 4388, loss 0.0725815, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:19.605009: step 4389, loss 0.0486003, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:19.764484: step 4390, loss 0.0450956, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:19.927066: step 4391, loss 0.148255, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:20.088109: step 4392, loss 0.178901, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:20.251738: step 4393, loss 0.111569, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:20.416066: step 4394, loss 0.174647, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:20.578172: step 4395, loss 0.122199, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:20.739975: step 4396, loss 0.169202, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:20.911053: step 4397, loss 0.0552933, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:21.071791: step 4398, loss 0.0423401, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:21.236877: step 4399, loss 0.0870879, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:21.399050: step 4400, loss 0.112406, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:26:21.884605: step 4400, loss 0.206577, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4400

2017-10-10T11:26:22.453824: step 4401, loss 0.0644557, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:22.614588: step 4402, loss 0.0469144, acc 1, learning_rate 0.0001
2017-10-10T11:26:22.776964: step 4403, loss 0.109262, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:22.938728: step 4404, loss 0.0445968, acc 1, learning_rate 0.0001
2017-10-10T11:26:23.100872: step 4405, loss 0.056043, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:23.264545: step 4406, loss 0.150738, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:23.426310: step 4407, loss 0.136911, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:23.590198: step 4408, loss 0.0453798, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:23.751884: step 4409, loss 0.051267, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:23.889095: step 4410, loss 0.183237, acc 0.960784, learning_rate 0.0001
2017-10-10T11:26:24.052033: step 4411, loss 0.101677, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:24.215411: step 4412, loss 0.0298166, acc 1, learning_rate 0.0001
2017-10-10T11:26:24.378795: step 4413, loss 0.0686964, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:24.542084: step 4414, loss 0.0997898, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:24.703859: step 4415, loss 0.0523817, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:24.869349: step 4416, loss 0.035328, acc 1, learning_rate 0.0001
2017-10-10T11:26:25.028867: step 4417, loss 0.0335491, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:25.188228: step 4418, loss 0.0530101, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:25.351342: step 4419, loss 0.162297, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:25.513546: step 4420, loss 0.0522604, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:25.673759: step 4421, loss 0.07543, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:25.839830: step 4422, loss 0.0520552, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:26.004546: step 4423, loss 0.0734281, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:26.170873: step 4424, loss 0.0759703, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:26.335330: step 4425, loss 0.0893979, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:26.496204: step 4426, loss 0.10993, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:26.657470: step 4427, loss 0.0913073, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:26.820196: step 4428, loss 0.0463904, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:26.984274: step 4429, loss 0.049735, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:27.145545: step 4430, loss 0.192425, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:27.310551: step 4431, loss 0.0741206, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:27.473583: step 4432, loss 0.136629, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:27.633762: step 4433, loss 0.026298, acc 1, learning_rate 0.0001
2017-10-10T11:26:27.798926: step 4434, loss 0.0271728, acc 1, learning_rate 0.0001
2017-10-10T11:26:27.960578: step 4435, loss 0.0511946, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:28.125551: step 4436, loss 0.0747709, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:28.289624: step 4437, loss 0.157626, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:28.456323: step 4438, loss 0.0471179, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:28.618093: step 4439, loss 0.159967, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:28.779093: step 4440, loss 0.0581098, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:26:29.251518: step 4440, loss 0.205188, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4440

2017-10-10T11:26:29.882840: step 4441, loss 0.0275631, acc 1, learning_rate 0.0001
2017-10-10T11:26:30.042710: step 4442, loss 0.046225, acc 1, learning_rate 0.0001
2017-10-10T11:26:30.203767: step 4443, loss 0.0457097, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:30.367359: step 4444, loss 0.199964, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:30.530941: step 4445, loss 0.123689, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:30.693788: step 4446, loss 0.158949, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:30.857559: step 4447, loss 0.130714, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:31.019924: step 4448, loss 0.0913931, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:31.180202: step 4449, loss 0.0706522, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:31.343734: step 4450, loss 0.0540403, acc 1, learning_rate 0.0001
2017-10-10T11:26:31.507464: step 4451, loss 0.0799804, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:31.670254: step 4452, loss 0.156001, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:31.831718: step 4453, loss 0.0425576, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:32.002927: step 4454, loss 0.173028, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:32.167206: step 4455, loss 0.0671853, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:32.329711: step 4456, loss 0.0506416, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:32.492206: step 4457, loss 0.122905, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:32.653157: step 4458, loss 0.0566594, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:32.815514: step 4459, loss 0.0859269, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:32.980436: step 4460, loss 0.0416763, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:33.142677: step 4461, loss 0.0684639, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:33.307565: step 4462, loss 0.0434298, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:33.469780: step 4463, loss 0.0432138, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:33.630892: step 4464, loss 0.0508288, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:33.794438: step 4465, loss 0.0516322, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:33.968212: step 4466, loss 0.108838, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:34.127697: step 4467, loss 0.0757588, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:34.293554: step 4468, loss 0.0668157, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:34.457400: step 4469, loss 0.110892, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:34.617045: step 4470, loss 0.0807735, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:34.781158: step 4471, loss 0.0737601, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:34.943100: step 4472, loss 0.0585336, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:35.101997: step 4473, loss 0.0208081, acc 1, learning_rate 0.0001
2017-10-10T11:26:35.262770: step 4474, loss 0.0518949, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:35.425043: step 4475, loss 0.0682035, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:35.586394: step 4476, loss 0.0419938, acc 1, learning_rate 0.0001
2017-10-10T11:26:35.750502: step 4477, loss 0.0991313, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:35.916246: step 4478, loss 0.12433, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:36.076665: step 4479, loss 0.0285576, acc 1, learning_rate 0.0001
2017-10-10T11:26:36.237534: step 4480, loss 0.0416133, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:26:36.711624: step 4480, loss 0.203743, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4480

2017-10-10T11:26:37.349961: step 4481, loss 0.14737, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:37.515180: step 4482, loss 0.129252, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:37.675515: step 4483, loss 0.0807519, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:37.840448: step 4484, loss 0.121179, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:38.000175: step 4485, loss 0.0570411, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:38.164099: step 4486, loss 0.0741606, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:38.326658: step 4487, loss 0.1339, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:38.488496: step 4488, loss 0.13888, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:38.654479: step 4489, loss 0.102295, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:38.815661: step 4490, loss 0.0331654, acc 1, learning_rate 0.0001
2017-10-10T11:26:38.979166: step 4491, loss 0.0956098, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:39.142222: step 4492, loss 0.0431406, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:39.305868: step 4493, loss 0.0635956, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:39.469056: step 4494, loss 0.0657083, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:39.632686: step 4495, loss 0.0777766, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:39.795875: step 4496, loss 0.034303, acc 1, learning_rate 0.0001
2017-10-10T11:26:39.961289: step 4497, loss 0.09017, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:40.122211: step 4498, loss 0.0646757, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:40.285469: step 4499, loss 0.0499476, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:40.449848: step 4500, loss 0.0741332, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:40.611542: step 4501, loss 0.0810731, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:40.774566: step 4502, loss 0.192337, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:40.948909: step 4503, loss 0.116163, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:41.109052: step 4504, loss 0.0575355, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:41.268425: step 4505, loss 0.030942, acc 1, learning_rate 0.0001
2017-10-10T11:26:41.430193: step 4506, loss 0.112923, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:41.595006: step 4507, loss 0.0667785, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:41.728459: step 4508, loss 0.0358598, acc 1, learning_rate 0.0001
2017-10-10T11:26:41.895449: step 4509, loss 0.077225, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:42.059795: step 4510, loss 0.0578989, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:42.222690: step 4511, loss 0.0565462, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:42.386049: step 4512, loss 0.105987, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:42.548572: step 4513, loss 0.0533488, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:42.708830: step 4514, loss 0.0809999, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:42.871198: step 4515, loss 0.0421413, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:43.034813: step 4516, loss 0.0443753, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:43.195218: step 4517, loss 0.1916, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:43.359095: step 4518, loss 0.0816026, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:43.523002: step 4519, loss 0.0592352, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:43.685511: step 4520, loss 0.0450464, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:26:44.165208: step 4520, loss 0.20321, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4520

2017-10-10T11:26:44.882336: step 4521, loss 0.0746029, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:45.043661: step 4522, loss 0.107515, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:45.205958: step 4523, loss 0.130789, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:45.368979: step 4524, loss 0.00758534, acc 1, learning_rate 0.0001
2017-10-10T11:26:45.532889: step 4525, loss 0.0278965, acc 1, learning_rate 0.0001
2017-10-10T11:26:45.694607: step 4526, loss 0.118305, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:45.863136: step 4527, loss 0.0941201, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:46.024313: step 4528, loss 0.0659747, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:46.183950: step 4529, loss 0.105216, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:46.345279: step 4530, loss 0.0483485, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:46.507837: step 4531, loss 0.0694793, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:46.670890: step 4532, loss 0.128983, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:46.835365: step 4533, loss 0.144624, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:46.998217: step 4534, loss 0.056409, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:47.160767: step 4535, loss 0.0658663, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:47.321924: step 4536, loss 0.0752375, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:47.484911: step 4537, loss 0.0328954, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:47.647785: step 4538, loss 0.10165, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:47.812337: step 4539, loss 0.0540469, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:47.981762: step 4540, loss 0.11176, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:48.140415: step 4541, loss 0.0264505, acc 1, learning_rate 0.0001
2017-10-10T11:26:48.300530: step 4542, loss 0.0506201, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:48.462767: step 4543, loss 0.0611442, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:48.623026: step 4544, loss 0.064468, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:48.785663: step 4545, loss 0.141125, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:48.948264: step 4546, loss 0.0341295, acc 1, learning_rate 0.0001
2017-10-10T11:26:49.113181: step 4547, loss 0.13792, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:49.274603: step 4548, loss 0.068863, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:49.438826: step 4549, loss 0.0327029, acc 1, learning_rate 0.0001
2017-10-10T11:26:49.600702: step 4550, loss 0.038801, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:49.762215: step 4551, loss 0.105091, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:49.927135: step 4552, loss 0.220353, acc 0.90625, learning_rate 0.0001
2017-10-10T11:26:50.088102: step 4553, loss 0.0332724, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:50.250217: step 4554, loss 0.0337754, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:50.413530: step 4555, loss 0.10783, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:50.573414: step 4556, loss 0.195167, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:50.736188: step 4557, loss 0.0861584, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:50.901786: step 4558, loss 0.0691404, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:51.067391: step 4559, loss 0.0715604, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:51.228203: step 4560, loss 0.158905, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T11:26:51.710675: step 4560, loss 0.201954, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4560

2017-10-10T11:26:52.286665: step 4561, loss 0.06939, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:52.451049: step 4562, loss 0.036706, acc 1, learning_rate 0.0001
2017-10-10T11:26:52.615682: step 4563, loss 0.173898, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:52.777475: step 4564, loss 0.0566254, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:52.939086: step 4565, loss 0.0595469, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:53.101906: step 4566, loss 0.0500348, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:53.262514: step 4567, loss 0.0250769, acc 1, learning_rate 0.0001
2017-10-10T11:26:53.424894: step 4568, loss 0.0206992, acc 1, learning_rate 0.0001
2017-10-10T11:26:53.583578: step 4569, loss 0.1219, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:53.745718: step 4570, loss 0.0320222, acc 1, learning_rate 0.0001
2017-10-10T11:26:53.913565: step 4571, loss 0.156463, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:54.077639: step 4572, loss 0.0615382, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:54.238538: step 4573, loss 0.114866, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:54.398490: step 4574, loss 0.0758993, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:54.559103: step 4575, loss 0.0869905, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:54.723502: step 4576, loss 0.158186, acc 0.921875, learning_rate 0.0001
2017-10-10T11:26:54.890245: step 4577, loss 0.205926, acc 0.953125, learning_rate 0.0001
2017-10-10T11:26:55.053819: step 4578, loss 0.0547947, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:55.214238: step 4579, loss 0.131674, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:55.376756: step 4580, loss 0.0282645, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:55.539624: step 4581, loss 0.0735234, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:55.703024: step 4582, loss 0.058935, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:55.866158: step 4583, loss 0.104584, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:56.035186: step 4584, loss 0.0956223, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:56.198637: step 4585, loss 0.0617424, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:56.361745: step 4586, loss 0.0433284, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:56.525638: step 4587, loss 0.0975803, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:56.689709: step 4588, loss 0.0690172, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:56.854607: step 4589, loss 0.0692461, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:57.016819: step 4590, loss 0.0693225, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:57.178817: step 4591, loss 0.098145, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:57.342409: step 4592, loss 0.050384, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:57.505300: step 4593, loss 0.0846677, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:57.668252: step 4594, loss 0.0513995, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:57.830824: step 4595, loss 0.130133, acc 0.90625, learning_rate 0.0001
2017-10-10T11:26:57.992265: step 4596, loss 0.103345, acc 0.96875, learning_rate 0.0001
2017-10-10T11:26:58.151072: step 4597, loss 0.0386874, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:58.312715: step 4598, loss 0.105467, acc 0.9375, learning_rate 0.0001
2017-10-10T11:26:58.472392: step 4599, loss 0.107917, acc 0.984375, learning_rate 0.0001
2017-10-10T11:26:58.635712: step 4600, loss 0.0791321, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:26:59.207528: step 4600, loss 0.201247, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4600

2017-10-10T11:27:00.063531: step 4601, loss 0.106231, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:00.230310: step 4602, loss 0.046675, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:00.393279: step 4603, loss 0.100079, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:00.556928: step 4604, loss 0.0423443, acc 1, learning_rate 0.0001
2017-10-10T11:27:00.717626: step 4605, loss 0.0163687, acc 1, learning_rate 0.0001
2017-10-10T11:27:00.856538: step 4606, loss 0.0307066, acc 0.980392, learning_rate 0.0001
2017-10-10T11:27:01.020462: step 4607, loss 0.0518638, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:01.181207: step 4608, loss 0.0333651, acc 1, learning_rate 0.0001
2017-10-10T11:27:01.343756: step 4609, loss 0.120085, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:01.508550: step 4610, loss 0.0959241, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:01.673270: step 4611, loss 0.196063, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:01.834774: step 4612, loss 0.0612716, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:01.998688: step 4613, loss 0.0256848, acc 1, learning_rate 0.0001
2017-10-10T11:27:02.160697: step 4614, loss 0.0765113, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:02.326965: step 4615, loss 0.0843408, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:02.488822: step 4616, loss 0.146457, acc 0.90625, learning_rate 0.0001
2017-10-10T11:27:02.653630: step 4617, loss 0.0792833, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:02.816453: step 4618, loss 0.139948, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:02.986828: step 4619, loss 0.02672, acc 1, learning_rate 0.0001
2017-10-10T11:27:03.149003: step 4620, loss 0.0855253, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:03.316872: step 4621, loss 0.159439, acc 0.921875, learning_rate 0.0001
2017-10-10T11:27:03.476561: step 4622, loss 0.13199, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:03.639624: step 4623, loss 0.046857, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:03.801067: step 4624, loss 0.0961558, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:03.965120: step 4625, loss 0.0796516, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:04.123990: step 4626, loss 0.127459, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:04.284474: step 4627, loss 0.0449134, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:04.446893: step 4628, loss 0.0923096, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:04.610631: step 4629, loss 0.0886175, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:04.773334: step 4630, loss 0.0857899, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:04.933474: step 4631, loss 0.0803466, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:05.097149: step 4632, loss 0.0703971, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:05.262967: step 4633, loss 0.0710332, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:05.428775: step 4634, loss 0.0696587, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:05.590985: step 4635, loss 0.0430067, acc 1, learning_rate 0.0001
2017-10-10T11:27:05.751875: step 4636, loss 0.0678527, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:05.924711: step 4637, loss 0.0473232, acc 1, learning_rate 0.0001
2017-10-10T11:27:06.084175: step 4638, loss 0.104774, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:06.245309: step 4639, loss 0.0674175, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:06.407959: step 4640, loss 0.0457571, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:27:06.870048: step 4640, loss 0.201656, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4640

2017-10-10T11:27:07.582496: step 4641, loss 0.114949, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:07.742728: step 4642, loss 0.0717278, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:07.907878: step 4643, loss 0.0548493, acc 1, learning_rate 0.0001
2017-10-10T11:27:08.071774: step 4644, loss 0.047859, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:08.232399: step 4645, loss 0.105412, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:08.393546: step 4646, loss 0.139518, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:08.555741: step 4647, loss 0.0657817, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:08.718570: step 4648, loss 0.0517776, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:08.879571: step 4649, loss 0.0501898, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:09.040213: step 4650, loss 0.0963624, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:09.203176: step 4651, loss 0.121415, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:09.365641: step 4652, loss 0.0828437, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:09.528426: step 4653, loss 0.0533834, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:09.690570: step 4654, loss 0.0572999, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:09.863820: step 4655, loss 0.0584364, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:10.030023: step 4656, loss 0.0442824, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:10.196104: step 4657, loss 0.192188, acc 0.921875, learning_rate 0.0001
2017-10-10T11:27:10.356256: step 4658, loss 0.040135, acc 1, learning_rate 0.0001
2017-10-10T11:27:10.519108: step 4659, loss 0.152191, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:10.682995: step 4660, loss 0.0434388, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:10.843428: step 4661, loss 0.0939137, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:11.004824: step 4662, loss 0.0503873, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:11.164611: step 4663, loss 0.0717248, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:11.323664: step 4664, loss 0.0992082, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:11.483195: step 4665, loss 0.0751213, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:11.647115: step 4666, loss 0.0490658, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:11.811484: step 4667, loss 0.0924513, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:11.981904: step 4668, loss 0.159311, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:12.146941: step 4669, loss 0.0773503, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:12.304677: step 4670, loss 0.0471905, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:12.469172: step 4671, loss 0.11399, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:12.629793: step 4672, loss 0.0517763, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:12.787686: step 4673, loss 0.120835, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:12.949356: step 4674, loss 0.0481354, acc 1, learning_rate 0.0001
2017-10-10T11:27:13.108335: step 4675, loss 0.0738609, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:13.269085: step 4676, loss 0.042998, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:13.431587: step 4677, loss 0.0577371, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:13.594397: step 4678, loss 0.0609946, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:13.754994: step 4679, loss 0.108284, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:13.923825: step 4680, loss 0.095008, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:27:14.408901: step 4680, loss 0.208402, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4680

2017-10-10T11:27:14.980365: step 4681, loss 0.0436642, acc 1, learning_rate 0.0001
2017-10-10T11:27:15.147745: step 4682, loss 0.0811182, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:15.309058: step 4683, loss 0.0899609, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:15.471136: step 4684, loss 0.0905609, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:15.634154: step 4685, loss 0.153175, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:15.796566: step 4686, loss 0.0260373, acc 1, learning_rate 0.0001
2017-10-10T11:27:15.966046: step 4687, loss 0.0879485, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:16.131054: step 4688, loss 0.0748267, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:16.292776: step 4689, loss 0.083685, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:16.455038: step 4690, loss 0.0299826, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:16.618859: step 4691, loss 0.0921892, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:16.787825: step 4692, loss 0.0412049, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:16.953394: step 4693, loss 0.0905891, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:17.116192: step 4694, loss 0.0359976, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:17.275753: step 4695, loss 0.113104, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:17.438905: step 4696, loss 0.0424738, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:17.600747: step 4697, loss 0.0707613, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:17.763322: step 4698, loss 0.0302716, acc 1, learning_rate 0.0001
2017-10-10T11:27:17.929549: step 4699, loss 0.0514918, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:18.091157: step 4700, loss 0.0614643, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:18.251788: step 4701, loss 0.0387871, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:18.415302: step 4702, loss 0.0362262, acc 1, learning_rate 0.0001
2017-10-10T11:27:18.576107: step 4703, loss 0.0593297, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:18.712533: step 4704, loss 0.0537173, acc 0.980392, learning_rate 0.0001
2017-10-10T11:27:18.876370: step 4705, loss 0.151703, acc 0.921875, learning_rate 0.0001
2017-10-10T11:27:19.039387: step 4706, loss 0.0748431, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:19.202490: step 4707, loss 0.0369825, acc 1, learning_rate 0.0001
2017-10-10T11:27:19.364183: step 4708, loss 0.0520362, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:19.526212: step 4709, loss 0.0793643, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:19.689403: step 4710, loss 0.0589994, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:19.855218: step 4711, loss 0.0662275, acc 1, learning_rate 0.0001
2017-10-10T11:27:20.016128: step 4712, loss 0.125819, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:20.180870: step 4713, loss 0.113315, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:20.343593: step 4714, loss 0.0842372, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:20.508569: step 4715, loss 0.0630935, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:20.671603: step 4716, loss 0.103313, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:20.834531: step 4717, loss 0.161889, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:21.000492: step 4718, loss 0.023618, acc 1, learning_rate 0.0001
2017-10-10T11:27:21.160313: step 4719, loss 0.138253, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:21.322717: step 4720, loss 0.0540597, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:27:21.800418: step 4720, loss 0.206274, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4720

2017-10-10T11:27:22.436215: step 4721, loss 0.0822304, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:22.598524: step 4722, loss 0.142245, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:22.758058: step 4723, loss 0.0305153, acc 1, learning_rate 0.0001
2017-10-10T11:27:22.920521: step 4724, loss 0.152128, acc 0.90625, learning_rate 0.0001
2017-10-10T11:27:23.083434: step 4725, loss 0.0607625, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:23.246406: step 4726, loss 0.0282123, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:23.410265: step 4727, loss 0.0248211, acc 1, learning_rate 0.0001
2017-10-10T11:27:23.572990: step 4728, loss 0.0107404, acc 1, learning_rate 0.0001
2017-10-10T11:27:23.734621: step 4729, loss 0.0379639, acc 1, learning_rate 0.0001
2017-10-10T11:27:23.902221: step 4730, loss 0.162395, acc 0.921875, learning_rate 0.0001
2017-10-10T11:27:24.066660: step 4731, loss 0.0536341, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:24.227871: step 4732, loss 0.0362291, acc 1, learning_rate 0.0001
2017-10-10T11:27:24.391259: step 4733, loss 0.0321475, acc 1, learning_rate 0.0001
2017-10-10T11:27:24.557578: step 4734, loss 0.0856969, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:24.722043: step 4735, loss 0.123071, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:24.884112: step 4736, loss 0.0988518, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:25.043270: step 4737, loss 0.053293, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:25.202223: step 4738, loss 0.0631562, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:25.365811: step 4739, loss 0.0673767, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:25.528998: step 4740, loss 0.0818057, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:25.695274: step 4741, loss 0.0233861, acc 1, learning_rate 0.0001
2017-10-10T11:27:25.854583: step 4742, loss 0.0844101, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:26.017901: step 4743, loss 0.0876439, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:26.181144: step 4744, loss 0.0460685, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:26.343528: step 4745, loss 0.0743122, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:26.509090: step 4746, loss 0.134326, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:26.669601: step 4747, loss 0.0716683, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:26.829886: step 4748, loss 0.0297594, acc 1, learning_rate 0.0001
2017-10-10T11:27:26.990798: step 4749, loss 0.0894006, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:27.152284: step 4750, loss 0.0874865, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:27.314567: step 4751, loss 0.12483, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:27.475843: step 4752, loss 0.0766601, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:27.640430: step 4753, loss 0.066448, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:27.802962: step 4754, loss 0.20927, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:27.964041: step 4755, loss 0.137442, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:28.124992: step 4756, loss 0.143979, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:28.287606: step 4757, loss 0.0402647, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:28.450195: step 4758, loss 0.0749357, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:28.615566: step 4759, loss 0.06016, acc 1, learning_rate 0.0001
2017-10-10T11:27:28.777411: step 4760, loss 0.0610951, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:27:29.252136: step 4760, loss 0.206701, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4760

2017-10-10T11:27:29.975659: step 4761, loss 0.0377792, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:30.136461: step 4762, loss 0.0451121, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:30.295618: step 4763, loss 0.159975, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:30.460504: step 4764, loss 0.0852471, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:30.622653: step 4765, loss 0.123317, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:30.787890: step 4766, loss 0.0415368, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:30.950592: step 4767, loss 0.0856358, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:31.111815: step 4768, loss 0.0558501, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:31.273915: step 4769, loss 0.151298, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:31.441101: step 4770, loss 0.0938341, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:31.604836: step 4771, loss 0.1663, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:31.764425: step 4772, loss 0.0764695, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:31.932808: step 4773, loss 0.0391602, acc 1, learning_rate 0.0001
2017-10-10T11:27:32.094131: step 4774, loss 0.0811664, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:32.258395: step 4775, loss 0.0394837, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:32.420197: step 4776, loss 0.075408, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:32.586937: step 4777, loss 0.0513715, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:32.748506: step 4778, loss 0.0731771, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:32.918178: step 4779, loss 0.0869939, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:33.084422: step 4780, loss 0.0816606, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:33.244928: step 4781, loss 0.0302828, acc 1, learning_rate 0.0001
2017-10-10T11:27:33.406858: step 4782, loss 0.167874, acc 0.921875, learning_rate 0.0001
2017-10-10T11:27:33.569760: step 4783, loss 0.0419447, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:33.730053: step 4784, loss 0.174525, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:33.895287: step 4785, loss 0.0478385, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:34.057929: step 4786, loss 0.0636493, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:34.221953: step 4787, loss 0.0234236, acc 1, learning_rate 0.0001
2017-10-10T11:27:34.380622: step 4788, loss 0.098235, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:34.543194: step 4789, loss 0.0633651, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:34.703556: step 4790, loss 0.0454248, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:34.869396: step 4791, loss 0.0174662, acc 1, learning_rate 0.0001
2017-10-10T11:27:35.030483: step 4792, loss 0.0565327, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:35.197267: step 4793, loss 0.0178305, acc 1, learning_rate 0.0001
2017-10-10T11:27:35.363233: step 4794, loss 0.0553616, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:35.527374: step 4795, loss 0.135797, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:35.689391: step 4796, loss 0.0216424, acc 1, learning_rate 0.0001
2017-10-10T11:27:35.851659: step 4797, loss 0.0662133, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:36.016683: step 4798, loss 0.059888, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:36.180064: step 4799, loss 0.0514174, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:36.340780: step 4800, loss 0.0599316, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:27:36.815013: step 4800, loss 0.200375, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4800

2017-10-10T11:27:37.410968: step 4801, loss 0.0592699, acc 1, learning_rate 0.0001
2017-10-10T11:27:37.546745: step 4802, loss 0.124428, acc 0.980392, learning_rate 0.0001
2017-10-10T11:27:37.709237: step 4803, loss 0.0511147, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:37.887114: step 4804, loss 0.0997441, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:38.050434: step 4805, loss 0.229538, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:38.209708: step 4806, loss 0.0380049, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:38.377794: step 4807, loss 0.0424288, acc 1, learning_rate 0.0001
2017-10-10T11:27:38.540170: step 4808, loss 0.100816, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:38.709682: step 4809, loss 0.0634423, acc 1, learning_rate 0.0001
2017-10-10T11:27:38.875091: step 4810, loss 0.11204, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:39.041687: step 4811, loss 0.0599549, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:39.205842: step 4812, loss 0.067625, acc 1, learning_rate 0.0001
2017-10-10T11:27:39.369100: step 4813, loss 0.114105, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:39.530087: step 4814, loss 0.0806343, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:39.694610: step 4815, loss 0.0790716, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:39.860119: step 4816, loss 0.0901044, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:40.019876: step 4817, loss 0.0753093, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:40.182991: step 4818, loss 0.0746897, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:40.342964: step 4819, loss 0.10857, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:40.506728: step 4820, loss 0.0467693, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:40.668493: step 4821, loss 0.0295294, acc 1, learning_rate 0.0001
2017-10-10T11:27:40.832047: step 4822, loss 0.0690522, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:40.998065: step 4823, loss 0.0965155, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:41.160186: step 4824, loss 0.0843896, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:41.323020: step 4825, loss 0.0478203, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:41.488867: step 4826, loss 0.032916, acc 1, learning_rate 0.0001
2017-10-10T11:27:41.649870: step 4827, loss 0.17934, acc 0.90625, learning_rate 0.0001
2017-10-10T11:27:41.812396: step 4828, loss 0.0762573, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:41.985945: step 4829, loss 0.0235793, acc 1, learning_rate 0.0001
2017-10-10T11:27:42.146056: step 4830, loss 0.0293964, acc 1, learning_rate 0.0001
2017-10-10T11:27:42.311707: step 4831, loss 0.076232, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:42.476780: step 4832, loss 0.0480297, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:42.638948: step 4833, loss 0.0619118, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:42.799568: step 4834, loss 0.0597508, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:42.968094: step 4835, loss 0.0763038, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:43.133584: step 4836, loss 0.108422, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:43.297311: step 4837, loss 0.0530091, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:43.461949: step 4838, loss 0.0333476, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:43.626015: step 4839, loss 0.101535, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:43.789427: step 4840, loss 0.127824, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:27:44.249749: step 4840, loss 0.206052, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4840

2017-10-10T11:27:44.884153: step 4841, loss 0.106098, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:45.046786: step 4842, loss 0.0533494, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:45.212991: step 4843, loss 0.136408, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:45.375335: step 4844, loss 0.0418112, acc 1, learning_rate 0.0001
2017-10-10T11:27:45.537364: step 4845, loss 0.0627503, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:45.700169: step 4846, loss 0.117208, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:45.863327: step 4847, loss 0.165915, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:46.025898: step 4848, loss 0.0842159, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:46.188654: step 4849, loss 0.0719359, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:46.352597: step 4850, loss 0.0685561, acc 1, learning_rate 0.0001
2017-10-10T11:27:46.516706: step 4851, loss 0.0854144, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:46.677410: step 4852, loss 0.0273546, acc 1, learning_rate 0.0001
2017-10-10T11:27:46.841127: step 4853, loss 0.090357, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:47.009934: step 4854, loss 0.0541144, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:47.171298: step 4855, loss 0.0326242, acc 1, learning_rate 0.0001
2017-10-10T11:27:47.336266: step 4856, loss 0.0529441, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:47.497406: step 4857, loss 0.0785995, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:47.658611: step 4858, loss 0.0751452, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:47.820136: step 4859, loss 0.0222362, acc 1, learning_rate 0.0001
2017-10-10T11:27:47.982499: step 4860, loss 0.101775, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:48.147869: step 4861, loss 0.0589784, acc 1, learning_rate 0.0001
2017-10-10T11:27:48.305551: step 4862, loss 0.0688674, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:48.467021: step 4863, loss 0.0304316, acc 1, learning_rate 0.0001
2017-10-10T11:27:48.626706: step 4864, loss 0.0870407, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:48.788917: step 4865, loss 0.0893982, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:48.951183: step 4866, loss 0.0587045, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:49.114638: step 4867, loss 0.114476, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:49.275598: step 4868, loss 0.0989932, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:49.435821: step 4869, loss 0.0632738, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:49.600006: step 4870, loss 0.0707927, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:49.763416: step 4871, loss 0.115939, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:49.927288: step 4872, loss 0.096888, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:50.088327: step 4873, loss 0.0538236, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:50.250904: step 4874, loss 0.0979951, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:50.412676: step 4875, loss 0.139239, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:50.571637: step 4876, loss 0.15201, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:50.733212: step 4877, loss 0.0429517, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:50.895339: step 4878, loss 0.041555, acc 1, learning_rate 0.0001
2017-10-10T11:27:51.057735: step 4879, loss 0.0319359, acc 1, learning_rate 0.0001
2017-10-10T11:27:51.219297: step 4880, loss 0.0644605, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:27:51.691297: step 4880, loss 0.208833, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4880

2017-10-10T11:27:52.413431: step 4881, loss 0.0250939, acc 1, learning_rate 0.0001
2017-10-10T11:27:52.574585: step 4882, loss 0.195648, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:52.737553: step 4883, loss 0.0495069, acc 1, learning_rate 0.0001
2017-10-10T11:27:52.907423: step 4884, loss 0.0731435, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:53.070679: step 4885, loss 0.0334832, acc 1, learning_rate 0.0001
2017-10-10T11:27:53.234388: step 4886, loss 0.0251241, acc 1, learning_rate 0.0001
2017-10-10T11:27:53.397702: step 4887, loss 0.0831979, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:53.561512: step 4888, loss 0.0901425, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:53.722565: step 4889, loss 0.123205, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:53.885861: step 4890, loss 0.0394411, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:54.049708: step 4891, loss 0.0757634, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:54.210094: step 4892, loss 0.0668511, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:54.373716: step 4893, loss 0.0779869, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:54.537893: step 4894, loss 0.0368507, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:54.701556: step 4895, loss 0.155577, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:54.863530: step 4896, loss 0.0333356, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:55.023319: step 4897, loss 0.0294322, acc 1, learning_rate 0.0001
2017-10-10T11:27:55.184560: step 4898, loss 0.0350688, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:55.343741: step 4899, loss 0.0905275, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:55.476999: step 4900, loss 0.058391, acc 0.980392, learning_rate 0.0001
2017-10-10T11:27:55.644588: step 4901, loss 0.0608688, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:55.808588: step 4902, loss 0.0690185, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:55.971282: step 4903, loss 0.0482162, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:56.131575: step 4904, loss 0.0177388, acc 1, learning_rate 0.0001
2017-10-10T11:27:56.293443: step 4905, loss 0.0921802, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:56.459939: step 4906, loss 0.026349, acc 1, learning_rate 0.0001
2017-10-10T11:27:56.631981: step 4907, loss 0.115766, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:56.804041: step 4908, loss 0.0540951, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:56.971828: step 4909, loss 0.0537501, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:57.132007: step 4910, loss 0.0396835, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:57.294324: step 4911, loss 0.0389244, acc 1, learning_rate 0.0001
2017-10-10T11:27:57.456507: step 4912, loss 0.0351282, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:57.619705: step 4913, loss 0.105736, acc 0.953125, learning_rate 0.0001
2017-10-10T11:27:57.780421: step 4914, loss 0.0241556, acc 1, learning_rate 0.0001
2017-10-10T11:27:57.950935: step 4915, loss 0.0832166, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:58.113377: step 4916, loss 0.110422, acc 0.9375, learning_rate 0.0001
2017-10-10T11:27:58.276542: step 4917, loss 0.0831971, acc 0.984375, learning_rate 0.0001
2017-10-10T11:27:58.437409: step 4918, loss 0.0128951, acc 1, learning_rate 0.0001
2017-10-10T11:27:58.602908: step 4919, loss 0.0592307, acc 0.96875, learning_rate 0.0001
2017-10-10T11:27:58.764277: step 4920, loss 0.0712698, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:27:59.246018: step 4920, loss 0.203649, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4920

2017-10-10T11:27:59.820189: step 4921, loss 0.0350893, acc 1, learning_rate 0.0001
2017-10-10T11:27:59.984601: step 4922, loss 0.0360657, acc 1, learning_rate 0.0001
2017-10-10T11:28:00.144593: step 4923, loss 0.133066, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:00.305206: step 4924, loss 0.109398, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:00.471435: step 4925, loss 0.0815627, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:00.633536: step 4926, loss 0.0683035, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:00.796214: step 4927, loss 0.0703908, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:00.968413: step 4928, loss 0.0300644, acc 1, learning_rate 0.0001
2017-10-10T11:28:01.131771: step 4929, loss 0.146532, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:01.294884: step 4930, loss 0.0645692, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:01.456184: step 4931, loss 0.192802, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:01.616044: step 4932, loss 0.0485483, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:01.777977: step 4933, loss 0.0172898, acc 1, learning_rate 0.0001
2017-10-10T11:28:01.944076: step 4934, loss 0.0800238, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:02.107970: step 4935, loss 0.137077, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:02.271225: step 4936, loss 0.0556649, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:02.434541: step 4937, loss 0.100207, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:02.599783: step 4938, loss 0.0679734, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:02.762088: step 4939, loss 0.0509585, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:02.928121: step 4940, loss 0.0346484, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:03.086716: step 4941, loss 0.104411, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:03.246854: step 4942, loss 0.0541699, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:03.406643: step 4943, loss 0.0531502, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:03.567946: step 4944, loss 0.169191, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:03.731533: step 4945, loss 0.225471, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:03.899257: step 4946, loss 0.0921247, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:04.059801: step 4947, loss 0.0933362, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:04.222629: step 4948, loss 0.0498923, acc 1, learning_rate 0.0001
2017-10-10T11:28:04.384343: step 4949, loss 0.0331008, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:04.545026: step 4950, loss 0.111954, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:04.709314: step 4951, loss 0.154316, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:04.872820: step 4952, loss 0.0419587, acc 1, learning_rate 0.0001
2017-10-10T11:28:05.035134: step 4953, loss 0.0321911, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:05.197211: step 4954, loss 0.0166928, acc 1, learning_rate 0.0001
2017-10-10T11:28:05.361604: step 4955, loss 0.0463, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:05.521808: step 4956, loss 0.0952235, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:05.682910: step 4957, loss 0.0297881, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:05.846624: step 4958, loss 0.0568163, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:06.009429: step 4959, loss 0.0764547, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:06.171362: step 4960, loss 0.033015, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:28:06.650320: step 4960, loss 0.201093, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-4960

2017-10-10T11:28:07.290077: step 4961, loss 0.0648664, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:07.452077: step 4962, loss 0.0339347, acc 1, learning_rate 0.0001
2017-10-10T11:28:07.614588: step 4963, loss 0.0560641, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:07.774816: step 4964, loss 0.0443439, acc 1, learning_rate 0.0001
2017-10-10T11:28:07.942409: step 4965, loss 0.223361, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:08.107415: step 4966, loss 0.0731017, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:08.270670: step 4967, loss 0.0751162, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:08.433463: step 4968, loss 0.0189882, acc 1, learning_rate 0.0001
2017-10-10T11:28:08.595439: step 4969, loss 0.0511076, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:08.756864: step 4970, loss 0.0459722, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:08.927864: step 4971, loss 0.0893915, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:09.092675: step 4972, loss 0.0351372, acc 1, learning_rate 0.0001
2017-10-10T11:28:09.253729: step 4973, loss 0.0587661, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:09.414248: step 4974, loss 0.0933141, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:09.576059: step 4975, loss 0.0589402, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:09.737946: step 4976, loss 0.0870556, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:09.902943: step 4977, loss 0.0727211, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:10.063639: step 4978, loss 0.0297984, acc 1, learning_rate 0.0001
2017-10-10T11:28:10.225534: step 4979, loss 0.103203, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:10.387853: step 4980, loss 0.043344, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:10.544289: step 4981, loss 0.0789716, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:10.704490: step 4982, loss 0.0413371, acc 1, learning_rate 0.0001
2017-10-10T11:28:10.869270: step 4983, loss 0.087058, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:11.031800: step 4984, loss 0.0542969, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:11.197426: step 4985, loss 0.0950469, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:11.357228: step 4986, loss 0.0444415, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:11.519641: step 4987, loss 0.0594677, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:11.679119: step 4988, loss 0.0160075, acc 1, learning_rate 0.0001
2017-10-10T11:28:11.842864: step 4989, loss 0.1033, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:12.007204: step 4990, loss 0.112044, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:12.170095: step 4991, loss 0.0488482, acc 1, learning_rate 0.0001
2017-10-10T11:28:12.333048: step 4992, loss 0.127109, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:12.493016: step 4993, loss 0.0625323, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:12.657129: step 4994, loss 0.145514, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:12.818510: step 4995, loss 0.0859386, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:12.982247: step 4996, loss 0.0619618, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:13.141408: step 4997, loss 0.0443629, acc 1, learning_rate 0.0001
2017-10-10T11:28:13.275928: step 4998, loss 0.119106, acc 0.960784, learning_rate 0.0001
2017-10-10T11:28:13.440381: step 4999, loss 0.107672, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:13.604896: step 5000, loss 0.0794379, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:28:14.098549: step 5000, loss 0.206908, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5000

2017-10-10T11:28:14.815401: step 5001, loss 0.106274, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:14.988928: step 5002, loss 0.0273625, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:15.152131: step 5003, loss 0.0830689, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:15.315567: step 5004, loss 0.0895964, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:15.480225: step 5005, loss 0.0327545, acc 1, learning_rate 0.0001
2017-10-10T11:28:15.641199: step 5006, loss 0.0986257, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:15.803503: step 5007, loss 0.0735665, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:15.965724: step 5008, loss 0.0611384, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:16.129368: step 5009, loss 0.0941439, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:16.290076: step 5010, loss 0.0807375, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:16.450474: step 5011, loss 0.0231652, acc 1, learning_rate 0.0001
2017-10-10T11:28:16.613764: step 5012, loss 0.0788708, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:16.774222: step 5013, loss 0.022571, acc 1, learning_rate 0.0001
2017-10-10T11:28:16.936366: step 5014, loss 0.125241, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:17.097314: step 5015, loss 0.0371055, acc 1, learning_rate 0.0001
2017-10-10T11:28:17.258601: step 5016, loss 0.0975989, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:17.421543: step 5017, loss 0.0818609, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:17.582802: step 5018, loss 0.0375524, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:17.747746: step 5019, loss 0.132349, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:17.907676: step 5020, loss 0.101343, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:18.067450: step 5021, loss 0.139869, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:18.230338: step 5022, loss 0.0831983, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:18.389246: step 5023, loss 0.0488135, acc 1, learning_rate 0.0001
2017-10-10T11:28:18.549302: step 5024, loss 0.058542, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:18.710967: step 5025, loss 0.0763763, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:18.877258: step 5026, loss 0.170418, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:19.039701: step 5027, loss 0.127564, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:19.201217: step 5028, loss 0.0465305, acc 1, learning_rate 0.0001
2017-10-10T11:28:19.362705: step 5029, loss 0.0588392, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:19.523803: step 5030, loss 0.0903145, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:19.687947: step 5031, loss 0.199301, acc 0.90625, learning_rate 0.0001
2017-10-10T11:28:19.854255: step 5032, loss 0.0193558, acc 1, learning_rate 0.0001
2017-10-10T11:28:20.023168: step 5033, loss 0.101431, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:20.183975: step 5034, loss 0.0802864, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:20.348112: step 5035, loss 0.0364212, acc 1, learning_rate 0.0001
2017-10-10T11:28:20.507171: step 5036, loss 0.0923737, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:20.671259: step 5037, loss 0.0612678, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:20.835774: step 5038, loss 0.0784319, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:21.001014: step 5039, loss 0.0736287, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:21.166258: step 5040, loss 0.0444173, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:28:21.634636: step 5040, loss 0.204189, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5040

2017-10-10T11:28:22.213588: step 5041, loss 0.105553, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:22.375015: step 5042, loss 0.0426468, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:22.534868: step 5043, loss 0.0760313, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:22.697501: step 5044, loss 0.070151, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:22.861511: step 5045, loss 0.0321314, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:23.022385: step 5046, loss 0.0175662, acc 1, learning_rate 0.0001
2017-10-10T11:28:23.186579: step 5047, loss 0.0216732, acc 1, learning_rate 0.0001
2017-10-10T11:28:23.352051: step 5048, loss 0.0216387, acc 1, learning_rate 0.0001
2017-10-10T11:28:23.512688: step 5049, loss 0.105402, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:23.678407: step 5050, loss 0.120881, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:23.844514: step 5051, loss 0.0485824, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:24.006236: step 5052, loss 0.112713, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:24.167366: step 5053, loss 0.0668403, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:24.329796: step 5054, loss 0.0803785, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:24.491246: step 5055, loss 0.104976, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:24.654657: step 5056, loss 0.0501227, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:24.818315: step 5057, loss 0.0246654, acc 1, learning_rate 0.0001
2017-10-10T11:28:24.977368: step 5058, loss 0.0792035, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:25.140995: step 5059, loss 0.036795, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:25.306066: step 5060, loss 0.0810443, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:25.468072: step 5061, loss 0.0337415, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:25.628739: step 5062, loss 0.0403699, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:25.791569: step 5063, loss 0.0689605, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:25.954920: step 5064, loss 0.107816, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:26.115818: step 5065, loss 0.132329, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:26.279634: step 5066, loss 0.124935, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:26.440167: step 5067, loss 0.0283066, acc 1, learning_rate 0.0001
2017-10-10T11:28:26.599456: step 5068, loss 0.0404325, acc 1, learning_rate 0.0001
2017-10-10T11:28:26.760919: step 5069, loss 0.0270121, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:26.930392: step 5070, loss 0.0842157, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:27.093630: step 5071, loss 0.18654, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:27.254156: step 5072, loss 0.0294517, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:27.417979: step 5073, loss 0.0497816, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:27.579929: step 5074, loss 0.0448827, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:27.741466: step 5075, loss 0.175295, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:27.910222: step 5076, loss 0.0737263, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:28.072406: step 5077, loss 0.0639125, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:28.230379: step 5078, loss 0.0259585, acc 1, learning_rate 0.0001
2017-10-10T11:28:28.392661: step 5079, loss 0.0574589, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:28.560684: step 5080, loss 0.0853965, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:28:29.030718: step 5080, loss 0.207717, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5080

2017-10-10T11:28:29.668308: step 5081, loss 0.0438209, acc 1, learning_rate 0.0001
2017-10-10T11:28:29.831398: step 5082, loss 0.0792613, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:30.003176: step 5083, loss 0.078743, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:30.163537: step 5084, loss 0.0605146, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:30.327107: step 5085, loss 0.113283, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:30.491068: step 5086, loss 0.0895771, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:30.655589: step 5087, loss 0.0419512, acc 1, learning_rate 0.0001
2017-10-10T11:28:30.816950: step 5088, loss 0.0537825, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:30.980880: step 5089, loss 0.0389695, acc 1, learning_rate 0.0001
2017-10-10T11:28:31.141125: step 5090, loss 0.122951, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:31.300579: step 5091, loss 0.0214043, acc 1, learning_rate 0.0001
2017-10-10T11:28:31.458163: step 5092, loss 0.0345633, acc 1, learning_rate 0.0001
2017-10-10T11:28:31.618879: step 5093, loss 0.0575133, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:31.782244: step 5094, loss 0.0940887, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:31.954031: step 5095, loss 0.0829069, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:32.090198: step 5096, loss 0.187572, acc 0.921569, learning_rate 0.0001
2017-10-10T11:28:32.250466: step 5097, loss 0.0968819, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:32.414369: step 5098, loss 0.0642308, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:32.575641: step 5099, loss 0.0326768, acc 1, learning_rate 0.0001
2017-10-10T11:28:32.737456: step 5100, loss 0.0452532, acc 1, learning_rate 0.0001
2017-10-10T11:28:32.901464: step 5101, loss 0.0330137, acc 1, learning_rate 0.0001
2017-10-10T11:28:33.060569: step 5102, loss 0.121896, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:33.224055: step 5103, loss 0.138039, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:33.387001: step 5104, loss 0.078, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:33.547574: step 5105, loss 0.0752662, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:33.711080: step 5106, loss 0.0641725, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:33.875138: step 5107, loss 0.144766, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:34.037629: step 5108, loss 0.0604213, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:34.201938: step 5109, loss 0.116327, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:34.367185: step 5110, loss 0.130826, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:34.527780: step 5111, loss 0.134224, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:34.689394: step 5112, loss 0.0144487, acc 1, learning_rate 0.0001
2017-10-10T11:28:34.851375: step 5113, loss 0.202103, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:35.012106: step 5114, loss 0.107722, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:35.175681: step 5115, loss 0.0250794, acc 1, learning_rate 0.0001
2017-10-10T11:28:35.337680: step 5116, loss 0.0478719, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:35.498209: step 5117, loss 0.0686846, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:35.657813: step 5118, loss 0.0675151, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:35.818806: step 5119, loss 0.131475, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:35.983913: step 5120, loss 0.0679221, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:28:36.452542: step 5120, loss 0.203345, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5120

2017-10-10T11:28:37.125380: step 5121, loss 0.0391981, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:37.286703: step 5122, loss 0.103115, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:37.448243: step 5123, loss 0.0612369, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:37.610192: step 5124, loss 0.0434805, acc 1, learning_rate 0.0001
2017-10-10T11:28:37.770488: step 5125, loss 0.0839722, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:37.940002: step 5126, loss 0.191756, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:38.100738: step 5127, loss 0.099183, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:38.264902: step 5128, loss 0.0990668, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:38.425679: step 5129, loss 0.0399039, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:38.589174: step 5130, loss 0.0294672, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:38.749554: step 5131, loss 0.0500861, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:38.915918: step 5132, loss 0.245239, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:39.076548: step 5133, loss 0.0576537, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:39.238634: step 5134, loss 0.0996389, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:39.404336: step 5135, loss 0.0780929, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:39.566870: step 5136, loss 0.0790843, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:39.831545: step 5137, loss 0.0201761, acc 1, learning_rate 0.0001
2017-10-10T11:28:39.994309: step 5138, loss 0.101495, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:40.156590: step 5139, loss 0.0747173, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:40.320309: step 5140, loss 0.0291528, acc 1, learning_rate 0.0001
2017-10-10T11:28:40.482263: step 5141, loss 0.0759111, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:40.645698: step 5142, loss 0.0374823, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:40.810459: step 5143, loss 0.0640598, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:40.974658: step 5144, loss 0.0375616, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:41.134832: step 5145, loss 0.0459715, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:41.299572: step 5146, loss 0.0384652, acc 1, learning_rate 0.0001
2017-10-10T11:28:41.461501: step 5147, loss 0.0720764, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:41.625694: step 5148, loss 0.0579756, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:41.786756: step 5149, loss 0.0282805, acc 1, learning_rate 0.0001
2017-10-10T11:28:41.950130: step 5150, loss 0.122637, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:42.110134: step 5151, loss 0.0604846, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:42.271766: step 5152, loss 0.0873949, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:42.433321: step 5153, loss 0.0721409, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:42.596669: step 5154, loss 0.0441064, acc 1, learning_rate 0.0001
2017-10-10T11:28:42.759266: step 5155, loss 0.0839617, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:42.921199: step 5156, loss 0.0518258, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:43.081478: step 5157, loss 0.0413513, acc 1, learning_rate 0.0001
2017-10-10T11:28:43.244586: step 5158, loss 0.0821107, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:43.407936: step 5159, loss 0.0590312, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:43.569988: step 5160, loss 0.0445492, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:28:44.061117: step 5160, loss 0.206566, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5160

2017-10-10T11:28:44.779498: step 5161, loss 0.0832643, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:44.940160: step 5162, loss 0.0759392, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:45.103778: step 5163, loss 0.0211112, acc 1, learning_rate 0.0001
2017-10-10T11:28:45.269912: step 5164, loss 0.0373095, acc 1, learning_rate 0.0001
2017-10-10T11:28:45.437870: step 5165, loss 0.0923437, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:45.601562: step 5166, loss 0.055335, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:45.762905: step 5167, loss 0.0269146, acc 1, learning_rate 0.0001
2017-10-10T11:28:45.927810: step 5168, loss 0.0838347, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:46.091925: step 5169, loss 0.0724393, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:46.254818: step 5170, loss 0.146966, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:46.423749: step 5171, loss 0.0719637, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:46.584715: step 5172, loss 0.0848494, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:46.747768: step 5173, loss 0.143293, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:46.913401: step 5174, loss 0.0871439, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:47.078562: step 5175, loss 0.198169, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:47.241290: step 5176, loss 0.0687712, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:47.405254: step 5177, loss 0.0885764, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:47.566559: step 5178, loss 0.0274687, acc 1, learning_rate 0.0001
2017-10-10T11:28:47.730871: step 5179, loss 0.0844847, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:47.896133: step 5180, loss 0.119517, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:48.060535: step 5181, loss 0.100008, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:48.226409: step 5182, loss 0.0792267, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:48.391018: step 5183, loss 0.100249, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:48.552700: step 5184, loss 0.104117, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:48.714202: step 5185, loss 0.0338224, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:48.880745: step 5186, loss 0.199915, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:49.041962: step 5187, loss 0.0743797, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:49.204563: step 5188, loss 0.0305503, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:49.368428: step 5189, loss 0.123848, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:49.528415: step 5190, loss 0.0895447, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:49.689218: step 5191, loss 0.0840039, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:49.852540: step 5192, loss 0.158482, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:50.016315: step 5193, loss 0.070511, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:50.151762: step 5194, loss 0.0579892, acc 0.960784, learning_rate 0.0001
2017-10-10T11:28:50.312789: step 5195, loss 0.0752409, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:50.477289: step 5196, loss 0.161161, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:50.640100: step 5197, loss 0.0670627, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:50.802921: step 5198, loss 0.0714, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:50.966265: step 5199, loss 0.0840968, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:51.127836: step 5200, loss 0.045579, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:28:51.606096: step 5200, loss 0.20539, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5200

2017-10-10T11:28:52.180558: step 5201, loss 0.0338657, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:52.343069: step 5202, loss 0.0599033, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:52.507528: step 5203, loss 0.0494228, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:52.669464: step 5204, loss 0.0894392, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:52.836028: step 5205, loss 0.0568005, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:52.998436: step 5206, loss 0.0537195, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:53.162369: step 5207, loss 0.0270989, acc 1, learning_rate 0.0001
2017-10-10T11:28:53.325919: step 5208, loss 0.0524497, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:53.489543: step 5209, loss 0.181158, acc 0.921875, learning_rate 0.0001
2017-10-10T11:28:53.648291: step 5210, loss 0.0242435, acc 1, learning_rate 0.0001
2017-10-10T11:28:53.812887: step 5211, loss 0.0307014, acc 1, learning_rate 0.0001
2017-10-10T11:28:53.974986: step 5212, loss 0.0301307, acc 1, learning_rate 0.0001
2017-10-10T11:28:54.136022: step 5213, loss 0.0846008, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:54.297429: step 5214, loss 0.101026, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:54.459582: step 5215, loss 0.0597973, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:54.622883: step 5216, loss 0.0451055, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:54.784112: step 5217, loss 0.0748003, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:55.038929: step 5218, loss 0.0608257, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:55.202853: step 5219, loss 0.0555844, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:55.364011: step 5220, loss 0.0978466, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:55.528086: step 5221, loss 0.0629283, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:55.689628: step 5222, loss 0.0396344, acc 1, learning_rate 0.0001
2017-10-10T11:28:55.850539: step 5223, loss 0.0414378, acc 1, learning_rate 0.0001
2017-10-10T11:28:56.010835: step 5224, loss 0.0484787, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:56.170144: step 5225, loss 0.048061, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:56.333925: step 5226, loss 0.100572, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:56.499347: step 5227, loss 0.166757, acc 0.9375, learning_rate 0.0001
2017-10-10T11:28:56.658956: step 5228, loss 0.0347869, acc 1, learning_rate 0.0001
2017-10-10T11:28:56.823364: step 5229, loss 0.0906671, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:56.987465: step 5230, loss 0.037615, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:57.148546: step 5231, loss 0.0257319, acc 1, learning_rate 0.0001
2017-10-10T11:28:57.311123: step 5232, loss 0.0464958, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:57.476426: step 5233, loss 0.0131431, acc 1, learning_rate 0.0001
2017-10-10T11:28:57.639640: step 5234, loss 0.0934023, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:57.802807: step 5235, loss 0.0531865, acc 0.984375, learning_rate 0.0001
2017-10-10T11:28:57.968843: step 5236, loss 0.095077, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:58.134355: step 5237, loss 0.0511781, acc 0.96875, learning_rate 0.0001
2017-10-10T11:28:58.295317: step 5238, loss 0.0889394, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:58.456396: step 5239, loss 0.0402924, acc 1, learning_rate 0.0001
2017-10-10T11:28:58.618734: step 5240, loss 0.0461404, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:28:59.115492: step 5240, loss 0.199023, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5240

2017-10-10T11:28:59.749291: step 5241, loss 0.0969628, acc 0.953125, learning_rate 0.0001
2017-10-10T11:28:59.920189: step 5242, loss 0.0655291, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:00.084857: step 5243, loss 0.0504913, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:00.244473: step 5244, loss 0.0350267, acc 1, learning_rate 0.0001
2017-10-10T11:29:00.405693: step 5245, loss 0.105903, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:00.572994: step 5246, loss 0.0547203, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:00.738931: step 5247, loss 0.106393, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:00.902603: step 5248, loss 0.054812, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:01.063882: step 5249, loss 0.118243, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:01.224630: step 5250, loss 0.0617127, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:01.388879: step 5251, loss 0.0427745, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:01.551535: step 5252, loss 0.119552, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:01.712400: step 5253, loss 0.0324086, acc 1, learning_rate 0.0001
2017-10-10T11:29:01.880716: step 5254, loss 0.0829328, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:02.045673: step 5255, loss 0.0728261, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:02.211314: step 5256, loss 0.226183, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:02.374091: step 5257, loss 0.064569, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:02.541897: step 5258, loss 0.0513054, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:02.704444: step 5259, loss 0.0218997, acc 1, learning_rate 0.0001
2017-10-10T11:29:02.866852: step 5260, loss 0.0399696, acc 1, learning_rate 0.0001
2017-10-10T11:29:03.029528: step 5261, loss 0.0359641, acc 1, learning_rate 0.0001
2017-10-10T11:29:03.191238: step 5262, loss 0.124335, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:03.352866: step 5263, loss 0.0939695, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:03.514964: step 5264, loss 0.104473, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:03.678618: step 5265, loss 0.129135, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:03.840542: step 5266, loss 0.0617561, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:04.001661: step 5267, loss 0.0359219, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:04.167562: step 5268, loss 0.0764127, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:04.328547: step 5269, loss 0.0651681, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:04.491481: step 5270, loss 0.220929, acc 0.921875, learning_rate 0.0001
2017-10-10T11:29:04.651934: step 5271, loss 0.140714, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:04.812509: step 5272, loss 0.0492582, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:04.974234: step 5273, loss 0.0837532, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:05.138472: step 5274, loss 0.0292095, acc 1, learning_rate 0.0001
2017-10-10T11:29:05.300126: step 5275, loss 0.133293, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:05.466164: step 5276, loss 0.0859702, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:05.627343: step 5277, loss 0.0866533, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:05.790447: step 5278, loss 0.099221, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:05.973403: step 5279, loss 0.0574002, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:06.134996: step 5280, loss 0.074255, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:29:06.608310: step 5280, loss 0.207378, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5280

2017-10-10T11:29:07.363147: step 5281, loss 0.0778154, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:07.526443: step 5282, loss 0.0466201, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:07.688697: step 5283, loss 0.100915, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:07.852752: step 5284, loss 0.0839284, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:08.012136: step 5285, loss 0.0227866, acc 1, learning_rate 0.0001
2017-10-10T11:29:08.175485: step 5286, loss 0.132226, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:08.337349: step 5287, loss 0.133479, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:08.500872: step 5288, loss 0.121658, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:08.661294: step 5289, loss 0.0804821, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:08.823436: step 5290, loss 0.0486073, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:08.990630: step 5291, loss 0.0179888, acc 1, learning_rate 0.0001
2017-10-10T11:29:09.123238: step 5292, loss 0.055351, acc 0.980392, learning_rate 0.0001
2017-10-10T11:29:09.284908: step 5293, loss 0.191633, acc 0.90625, learning_rate 0.0001
2017-10-10T11:29:09.447094: step 5294, loss 0.0823349, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:09.611802: step 5295, loss 0.0751103, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:09.774808: step 5296, loss 0.0927807, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:09.941762: step 5297, loss 0.0769865, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:10.103632: step 5298, loss 0.0782736, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:10.263486: step 5299, loss 0.145914, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:10.423207: step 5300, loss 0.0762587, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:10.586352: step 5301, loss 0.0436015, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:10.747658: step 5302, loss 0.0597673, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:10.912439: step 5303, loss 0.06155, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:11.072543: step 5304, loss 0.123546, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:11.235937: step 5305, loss 0.045521, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:11.395816: step 5306, loss 0.0283187, acc 1, learning_rate 0.0001
2017-10-10T11:29:11.556456: step 5307, loss 0.0574805, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:11.716994: step 5308, loss 0.0107444, acc 1, learning_rate 0.0001
2017-10-10T11:29:11.885874: step 5309, loss 0.0789586, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:12.046445: step 5310, loss 0.0488366, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:12.210264: step 5311, loss 0.0521789, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:12.373089: step 5312, loss 0.0565296, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:12.533804: step 5313, loss 0.0652163, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:12.693591: step 5314, loss 0.0409252, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:12.861648: step 5315, loss 0.0355041, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:13.021891: step 5316, loss 0.00951093, acc 1, learning_rate 0.0001
2017-10-10T11:29:13.185157: step 5317, loss 0.15575, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:13.346112: step 5318, loss 0.0797327, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:13.508694: step 5319, loss 0.034115, acc 1, learning_rate 0.0001
2017-10-10T11:29:13.668963: step 5320, loss 0.0193145, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:29:14.144124: step 5320, loss 0.205038, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5320

2017-10-10T11:29:14.726201: step 5321, loss 0.0551758, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:14.889464: step 5322, loss 0.10636, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:15.054295: step 5323, loss 0.0696568, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:15.217610: step 5324, loss 0.0504607, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:15.382408: step 5325, loss 0.0729188, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:15.547961: step 5326, loss 0.0486804, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:15.711138: step 5327, loss 0.0874211, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:15.874956: step 5328, loss 0.0344871, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:16.035829: step 5329, loss 0.130317, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:16.198241: step 5330, loss 0.0521512, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:16.361001: step 5331, loss 0.031021, acc 1, learning_rate 0.0001
2017-10-10T11:29:16.522912: step 5332, loss 0.0377101, acc 1, learning_rate 0.0001
2017-10-10T11:29:16.685781: step 5333, loss 0.0671307, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:16.845765: step 5334, loss 0.0105216, acc 1, learning_rate 0.0001
2017-10-10T11:29:17.017183: step 5335, loss 0.0784289, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:17.178856: step 5336, loss 0.0317478, acc 1, learning_rate 0.0001
2017-10-10T11:29:17.341342: step 5337, loss 0.076275, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:17.501855: step 5338, loss 0.135046, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:17.664496: step 5339, loss 0.0643052, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:17.826945: step 5340, loss 0.0893035, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:17.991333: step 5341, loss 0.122868, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:18.153314: step 5342, loss 0.0861541, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:18.318162: step 5343, loss 0.122209, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:18.478857: step 5344, loss 0.0581917, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:18.641097: step 5345, loss 0.139583, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:18.804490: step 5346, loss 0.0926999, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:18.971307: step 5347, loss 0.0469114, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:19.140398: step 5348, loss 0.0448333, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:19.300597: step 5349, loss 0.00781981, acc 1, learning_rate 0.0001
2017-10-10T11:29:19.459379: step 5350, loss 0.0387606, acc 1, learning_rate 0.0001
2017-10-10T11:29:19.621561: step 5351, loss 0.0469472, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:19.785219: step 5352, loss 0.0535053, acc 1, learning_rate 0.0001
2017-10-10T11:29:19.948986: step 5353, loss 0.0369475, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:20.111805: step 5354, loss 0.0626268, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:20.274792: step 5355, loss 0.0254391, acc 1, learning_rate 0.0001
2017-10-10T11:29:20.436914: step 5356, loss 0.0897792, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:20.599834: step 5357, loss 0.0387566, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:20.759318: step 5358, loss 0.0434351, acc 1, learning_rate 0.0001
2017-10-10T11:29:20.925079: step 5359, loss 0.043544, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:21.088902: step 5360, loss 0.105189, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:29:21.552466: step 5360, loss 0.202415, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5360

2017-10-10T11:29:22.189617: step 5361, loss 0.0706193, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:22.354003: step 5362, loss 0.0386148, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:22.514464: step 5363, loss 0.100894, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:22.676856: step 5364, loss 0.118127, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:22.841770: step 5365, loss 0.0493002, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:23.006384: step 5366, loss 0.020638, acc 1, learning_rate 0.0001
2017-10-10T11:29:23.168478: step 5367, loss 0.0482526, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:23.331791: step 5368, loss 0.0278221, acc 1, learning_rate 0.0001
2017-10-10T11:29:23.494264: step 5369, loss 0.0998457, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:23.653946: step 5370, loss 0.125947, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:23.815172: step 5371, loss 0.0257449, acc 1, learning_rate 0.0001
2017-10-10T11:29:23.977187: step 5372, loss 0.0511563, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:24.141600: step 5373, loss 0.0716311, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:24.302808: step 5374, loss 0.0885474, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:24.467766: step 5375, loss 0.0364941, acc 1, learning_rate 0.0001
2017-10-10T11:29:24.629564: step 5376, loss 0.0389534, acc 1, learning_rate 0.0001
2017-10-10T11:29:24.792187: step 5377, loss 0.0625383, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:24.956535: step 5378, loss 0.0646531, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:25.115292: step 5379, loss 0.0622364, acc 1, learning_rate 0.0001
2017-10-10T11:29:25.277246: step 5380, loss 0.0513917, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:25.437546: step 5381, loss 0.142714, acc 0.90625, learning_rate 0.0001
2017-10-10T11:29:25.599984: step 5382, loss 0.156613, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:25.761288: step 5383, loss 0.0418193, acc 1, learning_rate 0.0001
2017-10-10T11:29:25.932371: step 5384, loss 0.0880171, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:26.094813: step 5385, loss 0.0346527, acc 1, learning_rate 0.0001
2017-10-10T11:29:26.255842: step 5386, loss 0.0469532, acc 1, learning_rate 0.0001
2017-10-10T11:29:26.419887: step 5387, loss 0.102617, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:26.582404: step 5388, loss 0.0743972, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:26.745524: step 5389, loss 0.0313599, acc 1, learning_rate 0.0001
2017-10-10T11:29:26.883048: step 5390, loss 0.0660555, acc 0.980392, learning_rate 0.0001
2017-10-10T11:29:27.048694: step 5391, loss 0.0624601, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:27.212516: step 5392, loss 0.103585, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:27.374009: step 5393, loss 0.0441366, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:27.541061: step 5394, loss 0.103995, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:27.702083: step 5395, loss 0.0711064, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:27.867199: step 5396, loss 0.0285952, acc 1, learning_rate 0.0001
2017-10-10T11:29:28.035975: step 5397, loss 0.060601, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:28.197785: step 5398, loss 0.0582021, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:28.358116: step 5399, loss 0.062713, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:28.522369: step 5400, loss 0.05338, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:29:28.997969: step 5400, loss 0.201225, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5400

2017-10-10T11:29:29.711189: step 5401, loss 0.0563565, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:29.878857: step 5402, loss 0.0638833, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:30.040468: step 5403, loss 0.0430038, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:30.201034: step 5404, loss 0.0710633, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:30.361836: step 5405, loss 0.0781589, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:30.522068: step 5406, loss 0.0708037, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:30.685400: step 5407, loss 0.0428534, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:30.848915: step 5408, loss 0.0492763, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:31.011807: step 5409, loss 0.0554203, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:31.172267: step 5410, loss 0.0602895, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:31.335179: step 5411, loss 0.0891942, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:31.501852: step 5412, loss 0.0868326, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:31.662887: step 5413, loss 0.131766, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:31.825295: step 5414, loss 0.119585, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:31.987080: step 5415, loss 0.0455307, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:32.150150: step 5416, loss 0.0375255, acc 1, learning_rate 0.0001
2017-10-10T11:29:32.312368: step 5417, loss 0.04716, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:32.477301: step 5418, loss 0.0647644, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:32.639800: step 5419, loss 0.0516825, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:32.801821: step 5420, loss 0.0290194, acc 1, learning_rate 0.0001
2017-10-10T11:29:32.963151: step 5421, loss 0.0541269, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:33.123961: step 5422, loss 0.126081, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:33.286974: step 5423, loss 0.0515559, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:33.448135: step 5424, loss 0.019663, acc 1, learning_rate 0.0001
2017-10-10T11:29:33.612298: step 5425, loss 0.0935407, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:33.772861: step 5426, loss 0.0897554, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:33.935941: step 5427, loss 0.0764655, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:34.096218: step 5428, loss 0.153101, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:34.255246: step 5429, loss 0.0272157, acc 1, learning_rate 0.0001
2017-10-10T11:29:34.417568: step 5430, loss 0.0196528, acc 1, learning_rate 0.0001
2017-10-10T11:29:34.578356: step 5431, loss 0.102184, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:34.740309: step 5432, loss 0.0843063, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:34.916473: step 5433, loss 0.0552956, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:35.081456: step 5434, loss 0.0704957, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:35.242085: step 5435, loss 0.0338466, acc 1, learning_rate 0.0001
2017-10-10T11:29:35.404625: step 5436, loss 0.0662615, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:35.567106: step 5437, loss 0.0628506, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:35.731111: step 5438, loss 0.0327043, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:35.897295: step 5439, loss 0.137827, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:36.062281: step 5440, loss 0.0720634, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:29:36.531293: step 5440, loss 0.199869, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5440

2017-10-10T11:29:37.108526: step 5441, loss 0.0569507, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:37.271868: step 5442, loss 0.0287172, acc 1, learning_rate 0.0001
2017-10-10T11:29:37.432937: step 5443, loss 0.129828, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:37.597843: step 5444, loss 0.121952, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:37.761378: step 5445, loss 0.0520144, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:37.928241: step 5446, loss 0.0697153, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:38.092772: step 5447, loss 0.143361, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:38.258631: step 5448, loss 0.0378241, acc 1, learning_rate 0.0001
2017-10-10T11:29:38.421551: step 5449, loss 0.120649, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:38.583741: step 5450, loss 0.0614143, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:38.751572: step 5451, loss 0.0618321, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:38.914869: step 5452, loss 0.0744194, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:39.077054: step 5453, loss 0.0828427, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:39.241470: step 5454, loss 0.0405228, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:39.404246: step 5455, loss 0.0826851, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:39.566570: step 5456, loss 0.128659, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:39.731535: step 5457, loss 0.117074, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:39.898771: step 5458, loss 0.0837595, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:40.065484: step 5459, loss 0.045995, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:40.228095: step 5460, loss 0.207076, acc 0.921875, learning_rate 0.0001
2017-10-10T11:29:40.398199: step 5461, loss 0.116261, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:40.562368: step 5462, loss 0.0784098, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:40.726139: step 5463, loss 0.0765138, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:40.897372: step 5464, loss 0.0252654, acc 1, learning_rate 0.0001
2017-10-10T11:29:41.058797: step 5465, loss 0.0198467, acc 1, learning_rate 0.0001
2017-10-10T11:29:41.220253: step 5466, loss 0.063374, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:41.380720: step 5467, loss 0.0580051, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:41.540202: step 5468, loss 0.0655502, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:41.701330: step 5469, loss 0.0848559, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:41.865026: step 5470, loss 0.112384, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:42.027146: step 5471, loss 0.0967519, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:42.189723: step 5472, loss 0.0483253, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:42.352175: step 5473, loss 0.169874, acc 0.921875, learning_rate 0.0001
2017-10-10T11:29:42.516422: step 5474, loss 0.046499, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:42.678987: step 5475, loss 0.0580498, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:42.838875: step 5476, loss 0.134042, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:43.001965: step 5477, loss 0.101782, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:43.163063: step 5478, loss 0.054008, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:43.326541: step 5479, loss 0.0390271, acc 1, learning_rate 0.0001
2017-10-10T11:29:43.491365: step 5480, loss 0.0896623, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:29:43.955869: step 5480, loss 0.19792, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5480

2017-10-10T11:29:44.604217: step 5481, loss 0.0701553, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:44.766133: step 5482, loss 0.0771201, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:44.929628: step 5483, loss 0.0525922, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:45.092115: step 5484, loss 0.0578276, acc 1, learning_rate 0.0001
2017-10-10T11:29:45.253154: step 5485, loss 0.0537461, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:45.419574: step 5486, loss 0.0480887, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:45.582773: step 5487, loss 0.0708983, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:45.716406: step 5488, loss 0.0552409, acc 1, learning_rate 0.0001
2017-10-10T11:29:45.885077: step 5489, loss 0.0230009, acc 1, learning_rate 0.0001
2017-10-10T11:29:46.050381: step 5490, loss 0.108823, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:46.213717: step 5491, loss 0.0340907, acc 1, learning_rate 0.0001
2017-10-10T11:29:46.375894: step 5492, loss 0.0148912, acc 1, learning_rate 0.0001
2017-10-10T11:29:46.537148: step 5493, loss 0.122068, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:46.700349: step 5494, loss 0.134977, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:46.864460: step 5495, loss 0.122761, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:47.028225: step 5496, loss 0.0988639, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:47.188377: step 5497, loss 0.0515684, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:47.351987: step 5498, loss 0.117939, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:47.514523: step 5499, loss 0.0262338, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:47.676164: step 5500, loss 0.0450067, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:47.839638: step 5501, loss 0.084582, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:48.001723: step 5502, loss 0.0302639, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:48.167934: step 5503, loss 0.131614, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:48.327360: step 5504, loss 0.162144, acc 0.90625, learning_rate 0.0001
2017-10-10T11:29:48.492735: step 5505, loss 0.0498422, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:48.660851: step 5506, loss 0.128969, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:48.821336: step 5507, loss 0.04729, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:48.985113: step 5508, loss 0.0511857, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:49.147917: step 5509, loss 0.0212613, acc 1, learning_rate 0.0001
2017-10-10T11:29:49.310589: step 5510, loss 0.0946884, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:49.474366: step 5511, loss 0.0973945, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:49.638484: step 5512, loss 0.0571742, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:49.798678: step 5513, loss 0.0192771, acc 1, learning_rate 0.0001
2017-10-10T11:29:49.961276: step 5514, loss 0.0196204, acc 1, learning_rate 0.0001
2017-10-10T11:29:50.120559: step 5515, loss 0.0902018, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:50.284678: step 5516, loss 0.0989458, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:50.448638: step 5517, loss 0.0541639, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:50.612071: step 5518, loss 0.0206081, acc 1, learning_rate 0.0001
2017-10-10T11:29:50.776124: step 5519, loss 0.0648143, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:50.946391: step 5520, loss 0.0952301, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T11:29:51.416051: step 5520, loss 0.196867, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5520

2017-10-10T11:29:52.052338: step 5521, loss 0.0730402, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:52.217195: step 5522, loss 0.0810806, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:52.382967: step 5523, loss 0.0589097, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:52.543598: step 5524, loss 0.0566807, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:52.704940: step 5525, loss 0.0384798, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:52.867697: step 5526, loss 0.063072, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:53.028006: step 5527, loss 0.105561, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:53.191357: step 5528, loss 0.107703, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:53.350968: step 5529, loss 0.0700041, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:53.512394: step 5530, loss 0.119525, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:53.673432: step 5531, loss 0.0539007, acc 1, learning_rate 0.0001
2017-10-10T11:29:53.835003: step 5532, loss 0.0757649, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:54.003320: step 5533, loss 0.12001, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:54.167077: step 5534, loss 0.0200342, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:54.327950: step 5535, loss 0.181267, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:54.490814: step 5536, loss 0.0289639, acc 1, learning_rate 0.0001
2017-10-10T11:29:54.654445: step 5537, loss 0.0839026, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:54.818044: step 5538, loss 0.0437147, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:54.983229: step 5539, loss 0.0406533, acc 1, learning_rate 0.0001
2017-10-10T11:29:55.147614: step 5540, loss 0.0249468, acc 1, learning_rate 0.0001
2017-10-10T11:29:55.309992: step 5541, loss 0.0298912, acc 1, learning_rate 0.0001
2017-10-10T11:29:55.473635: step 5542, loss 0.0209454, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:55.636832: step 5543, loss 0.0439889, acc 1, learning_rate 0.0001
2017-10-10T11:29:55.799677: step 5544, loss 0.0447183, acc 1, learning_rate 0.0001
2017-10-10T11:29:55.963304: step 5545, loss 0.0286863, acc 1, learning_rate 0.0001
2017-10-10T11:29:56.123905: step 5546, loss 0.134085, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:56.286026: step 5547, loss 0.0167793, acc 1, learning_rate 0.0001
2017-10-10T11:29:56.448388: step 5548, loss 0.0314372, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:56.612147: step 5549, loss 0.0643323, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:56.774390: step 5550, loss 0.0407267, acc 1, learning_rate 0.0001
2017-10-10T11:29:56.960016: step 5551, loss 0.174051, acc 0.9375, learning_rate 0.0001
2017-10-10T11:29:57.122855: step 5552, loss 0.136739, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:57.285725: step 5553, loss 0.0842208, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:57.445821: step 5554, loss 0.034581, acc 1, learning_rate 0.0001
2017-10-10T11:29:57.608696: step 5555, loss 0.0457091, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:57.771475: step 5556, loss 0.0345654, acc 0.984375, learning_rate 0.0001
2017-10-10T11:29:57.935056: step 5557, loss 0.0327746, acc 1, learning_rate 0.0001
2017-10-10T11:29:58.098753: step 5558, loss 0.0361004, acc 1, learning_rate 0.0001
2017-10-10T11:29:58.261578: step 5559, loss 0.139094, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:58.423530: step 5560, loss 0.0449181, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:29:58.892539: step 5560, loss 0.199328, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5560

2017-10-10T11:29:59.603618: step 5561, loss 0.0763807, acc 0.96875, learning_rate 0.0001
2017-10-10T11:29:59.765328: step 5562, loss 0.114986, acc 0.953125, learning_rate 0.0001
2017-10-10T11:29:59.929730: step 5563, loss 0.069811, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:00.094060: step 5564, loss 0.0962219, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:00.254509: step 5565, loss 0.0699955, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:00.417749: step 5566, loss 0.0733374, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:00.582345: step 5567, loss 0.0682966, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:00.746598: step 5568, loss 0.051724, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:00.914299: step 5569, loss 0.0372321, acc 1, learning_rate 0.0001
2017-10-10T11:30:01.076877: step 5570, loss 0.0410907, acc 1, learning_rate 0.0001
2017-10-10T11:30:01.239187: step 5571, loss 0.0887296, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:01.400254: step 5572, loss 0.076605, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:01.561489: step 5573, loss 0.0429227, acc 1, learning_rate 0.0001
2017-10-10T11:30:01.723641: step 5574, loss 0.0327954, acc 1, learning_rate 0.0001
2017-10-10T11:30:01.887974: step 5575, loss 0.0253615, acc 1, learning_rate 0.0001
2017-10-10T11:30:02.049449: step 5576, loss 0.0702063, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:02.212764: step 5577, loss 0.0622475, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:02.375230: step 5578, loss 0.137466, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:02.535932: step 5579, loss 0.135808, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:02.698988: step 5580, loss 0.0221011, acc 1, learning_rate 0.0001
2017-10-10T11:30:02.862378: step 5581, loss 0.0820783, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:03.023866: step 5582, loss 0.0511633, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:03.185362: step 5583, loss 0.0688221, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:03.347968: step 5584, loss 0.0741067, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:03.511604: step 5585, loss 0.0955596, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:03.648364: step 5586, loss 0.0718143, acc 0.980392, learning_rate 0.0001
2017-10-10T11:30:03.810975: step 5587, loss 0.07959, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:03.982568: step 5588, loss 0.13793, acc 0.921875, learning_rate 0.0001
2017-10-10T11:30:04.143267: step 5589, loss 0.0482054, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:04.305930: step 5590, loss 0.0703411, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:04.468915: step 5591, loss 0.0475475, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:04.630217: step 5592, loss 0.0234177, acc 1, learning_rate 0.0001
2017-10-10T11:30:04.788767: step 5593, loss 0.116102, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:04.952513: step 5594, loss 0.0384681, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:05.115487: step 5595, loss 0.0494769, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:05.278092: step 5596, loss 0.0588059, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:05.440635: step 5597, loss 0.0499059, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:05.604366: step 5598, loss 0.100452, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:05.766439: step 5599, loss 0.0380501, acc 1, learning_rate 0.0001
2017-10-10T11:30:05.928281: step 5600, loss 0.161846, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:30:06.407564: step 5600, loss 0.200921, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5600

2017-10-10T11:30:06.986952: step 5601, loss 0.0912306, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:07.150757: step 5602, loss 0.0945675, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:07.312090: step 5603, loss 0.065759, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:07.478373: step 5604, loss 0.0305294, acc 1, learning_rate 0.0001
2017-10-10T11:30:07.641399: step 5605, loss 0.0562004, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:07.809177: step 5606, loss 0.172808, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:07.975705: step 5607, loss 0.0584015, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:08.139547: step 5608, loss 0.0739854, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:08.304375: step 5609, loss 0.0784631, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:08.466409: step 5610, loss 0.045137, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:08.629856: step 5611, loss 0.112216, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:08.792804: step 5612, loss 0.0500888, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:08.954336: step 5613, loss 0.124622, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:09.116183: step 5614, loss 0.078955, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:09.279880: step 5615, loss 0.0157803, acc 1, learning_rate 0.0001
2017-10-10T11:30:09.449366: step 5616, loss 0.0608258, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:09.613717: step 5617, loss 0.0904946, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:09.776588: step 5618, loss 0.0868953, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:09.949626: step 5619, loss 0.0351169, acc 1, learning_rate 0.0001
2017-10-10T11:30:10.115041: step 5620, loss 0.152581, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:10.278236: step 5621, loss 0.0345849, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:10.438722: step 5622, loss 0.0364037, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:10.602226: step 5623, loss 0.135146, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:10.764811: step 5624, loss 0.104938, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:10.929458: step 5625, loss 0.0769628, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:11.092597: step 5626, loss 0.0335586, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:11.252861: step 5627, loss 0.038382, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:11.418684: step 5628, loss 0.0543958, acc 1, learning_rate 0.0001
2017-10-10T11:30:11.583138: step 5629, loss 0.0955197, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:11.745203: step 5630, loss 0.0239381, acc 1, learning_rate 0.0001
2017-10-10T11:30:11.904401: step 5631, loss 0.0598045, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:12.067215: step 5632, loss 0.0188241, acc 1, learning_rate 0.0001
2017-10-10T11:30:12.226586: step 5633, loss 0.0152374, acc 1, learning_rate 0.0001
2017-10-10T11:30:12.386756: step 5634, loss 0.0675003, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:12.551420: step 5635, loss 0.0724879, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:12.713799: step 5636, loss 0.0892745, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:12.875681: step 5637, loss 0.0420838, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:13.039345: step 5638, loss 0.110622, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:13.202596: step 5639, loss 0.0156388, acc 1, learning_rate 0.0001
2017-10-10T11:30:13.365723: step 5640, loss 0.122615, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T11:30:13.831629: step 5640, loss 0.200495, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5640

2017-10-10T11:30:14.475709: step 5641, loss 0.114392, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:14.637856: step 5642, loss 0.0120959, acc 1, learning_rate 0.0001
2017-10-10T11:30:14.799050: step 5643, loss 0.0452131, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:14.966000: step 5644, loss 0.0956227, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:15.129245: step 5645, loss 0.0722072, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:15.297837: step 5646, loss 0.0327847, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:15.462936: step 5647, loss 0.10536, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:15.625193: step 5648, loss 0.0669528, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:15.786464: step 5649, loss 0.0434105, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:15.951362: step 5650, loss 0.104914, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:16.108467: step 5651, loss 0.0376935, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:16.275911: step 5652, loss 0.0644648, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:16.437764: step 5653, loss 0.0617583, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:16.600025: step 5654, loss 0.0709082, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:16.760941: step 5655, loss 0.0250806, acc 1, learning_rate 0.0001
2017-10-10T11:30:16.927815: step 5656, loss 0.0343195, acc 1, learning_rate 0.0001
2017-10-10T11:30:17.090929: step 5657, loss 0.0399768, acc 1, learning_rate 0.0001
2017-10-10T11:30:17.252320: step 5658, loss 0.151188, acc 0.921875, learning_rate 0.0001
2017-10-10T11:30:17.412346: step 5659, loss 0.0762222, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:17.574531: step 5660, loss 0.171371, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:17.738776: step 5661, loss 0.0153694, acc 1, learning_rate 0.0001
2017-10-10T11:30:17.902442: step 5662, loss 0.0142763, acc 1, learning_rate 0.0001
2017-10-10T11:30:18.064525: step 5663, loss 0.121783, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:18.228067: step 5664, loss 0.0573725, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:18.391719: step 5665, loss 0.0338316, acc 1, learning_rate 0.0001
2017-10-10T11:30:18.555212: step 5666, loss 0.0370324, acc 1, learning_rate 0.0001
2017-10-10T11:30:18.716847: step 5667, loss 0.134467, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:18.884750: step 5668, loss 0.0223106, acc 1, learning_rate 0.0001
2017-10-10T11:30:19.049391: step 5669, loss 0.0530618, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:19.211798: step 5670, loss 0.0756623, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:19.372988: step 5671, loss 0.0821531, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:19.533622: step 5672, loss 0.0704541, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:19.695298: step 5673, loss 0.0791992, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:19.859340: step 5674, loss 0.0484089, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:20.019649: step 5675, loss 0.0585573, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:20.185342: step 5676, loss 0.0558086, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:20.346801: step 5677, loss 0.0377415, acc 1, learning_rate 0.0001
2017-10-10T11:30:20.510527: step 5678, loss 0.0899139, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:20.671351: step 5679, loss 0.0407907, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:20.833619: step 5680, loss 0.089018, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:30:21.305485: step 5680, loss 0.198598, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5680

2017-10-10T11:30:22.018773: step 5681, loss 0.0266274, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:22.179873: step 5682, loss 0.0287812, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:22.351341: step 5683, loss 0.0171906, acc 1, learning_rate 0.0001
2017-10-10T11:30:22.487517: step 5684, loss 0.0187885, acc 1, learning_rate 0.0001
2017-10-10T11:30:22.648893: step 5685, loss 0.0676478, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:22.812371: step 5686, loss 0.144099, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:22.977222: step 5687, loss 0.0385052, acc 1, learning_rate 0.0001
2017-10-10T11:30:23.140099: step 5688, loss 0.0683042, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:23.302132: step 5689, loss 0.133997, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:23.464813: step 5690, loss 0.062106, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:23.626800: step 5691, loss 0.0277111, acc 1, learning_rate 0.0001
2017-10-10T11:30:23.790187: step 5692, loss 0.0870584, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:23.958490: step 5693, loss 0.0677624, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:24.120997: step 5694, loss 0.0573, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:24.283971: step 5695, loss 0.0153668, acc 1, learning_rate 0.0001
2017-10-10T11:30:24.445413: step 5696, loss 0.0776282, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:24.614583: step 5697, loss 0.0882235, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:24.777998: step 5698, loss 0.0694467, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:24.943979: step 5699, loss 0.088749, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:25.114591: step 5700, loss 0.111582, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:25.277921: step 5701, loss 0.0499514, acc 1, learning_rate 0.0001
2017-10-10T11:30:25.439215: step 5702, loss 0.0502858, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:25.599640: step 5703, loss 0.104739, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:25.760978: step 5704, loss 0.0223419, acc 1, learning_rate 0.0001
2017-10-10T11:30:25.941579: step 5705, loss 0.0754372, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:26.109511: step 5706, loss 0.0704943, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:26.272424: step 5707, loss 0.0756069, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:26.442797: step 5708, loss 0.0191814, acc 1, learning_rate 0.0001
2017-10-10T11:30:26.603091: step 5709, loss 0.147079, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:26.766390: step 5710, loss 0.0378666, acc 1, learning_rate 0.0001
2017-10-10T11:30:26.931305: step 5711, loss 0.066947, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:27.095404: step 5712, loss 0.115788, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:27.255669: step 5713, loss 0.0946887, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:27.419364: step 5714, loss 0.0729794, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:27.584444: step 5715, loss 0.065392, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:27.753835: step 5716, loss 0.107937, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:27.920209: step 5717, loss 0.0613652, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:28.082913: step 5718, loss 0.0350093, acc 1, learning_rate 0.0001
2017-10-10T11:30:28.241494: step 5719, loss 0.0783493, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:28.401696: step 5720, loss 0.060432, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:30:28.878648: step 5720, loss 0.199934, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5720

2017-10-10T11:30:29.450834: step 5721, loss 0.0561119, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:29.610224: step 5722, loss 0.0615654, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:29.773476: step 5723, loss 0.0700524, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:29.943808: step 5724, loss 0.0377199, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:30.107842: step 5725, loss 0.048946, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:30.268599: step 5726, loss 0.0762971, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:30.431275: step 5727, loss 0.0287015, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:30.592801: step 5728, loss 0.0806114, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:30.753813: step 5729, loss 0.0609728, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:30.918561: step 5730, loss 0.0996651, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:31.081155: step 5731, loss 0.0875115, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:31.244405: step 5732, loss 0.0564487, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:31.405810: step 5733, loss 0.0701178, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:31.568394: step 5734, loss 0.0301868, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:31.731508: step 5735, loss 0.0863348, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:31.895839: step 5736, loss 0.0537862, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:32.058428: step 5737, loss 0.0277106, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:32.221031: step 5738, loss 0.0369568, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:32.383504: step 5739, loss 0.0438603, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:32.545923: step 5740, loss 0.11631, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:32.706877: step 5741, loss 0.0185459, acc 1, learning_rate 0.0001
2017-10-10T11:30:32.874795: step 5742, loss 0.0688805, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:33.040857: step 5743, loss 0.0788872, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:33.204149: step 5744, loss 0.0916751, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:33.363311: step 5745, loss 0.0430227, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:33.526467: step 5746, loss 0.0662581, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:33.691781: step 5747, loss 0.0610295, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:33.856567: step 5748, loss 0.0705111, acc 1, learning_rate 0.0001
2017-10-10T11:30:34.016408: step 5749, loss 0.208792, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:34.174099: step 5750, loss 0.0199448, acc 1, learning_rate 0.0001
2017-10-10T11:30:34.334620: step 5751, loss 0.0967199, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:34.494662: step 5752, loss 0.0856291, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:34.663818: step 5753, loss 0.0362004, acc 1, learning_rate 0.0001
2017-10-10T11:30:34.824330: step 5754, loss 0.0301425, acc 1, learning_rate 0.0001
2017-10-10T11:30:34.990069: step 5755, loss 0.0485963, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:35.153752: step 5756, loss 0.0887667, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:35.315572: step 5757, loss 0.0928884, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:35.475674: step 5758, loss 0.0197486, acc 1, learning_rate 0.0001
2017-10-10T11:30:35.637360: step 5759, loss 0.118865, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:35.799124: step 5760, loss 0.0366039, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:30:36.280071: step 5760, loss 0.200108, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5760

2017-10-10T11:30:36.964523: step 5761, loss 0.0693458, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:37.127377: step 5762, loss 0.0130357, acc 1, learning_rate 0.0001
2017-10-10T11:30:37.288852: step 5763, loss 0.0482575, acc 1, learning_rate 0.0001
2017-10-10T11:30:37.454185: step 5764, loss 0.10707, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:37.615579: step 5765, loss 0.048612, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:37.781551: step 5766, loss 0.0683528, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:37.944667: step 5767, loss 0.0909071, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:38.105948: step 5768, loss 0.114233, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:38.268194: step 5769, loss 0.0192834, acc 1, learning_rate 0.0001
2017-10-10T11:30:38.430778: step 5770, loss 0.0522725, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:38.593700: step 5771, loss 0.0347519, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:38.758348: step 5772, loss 0.0842298, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:38.926903: step 5773, loss 0.0567855, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:39.091175: step 5774, loss 0.0647009, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:39.254456: step 5775, loss 0.0392142, acc 1, learning_rate 0.0001
2017-10-10T11:30:39.413716: step 5776, loss 0.0450949, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:39.576237: step 5777, loss 0.0383448, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:39.741531: step 5778, loss 0.0952018, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:39.909938: step 5779, loss 0.0424283, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:40.069751: step 5780, loss 0.0814723, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:40.232555: step 5781, loss 0.10953, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:40.368161: step 5782, loss 0.0330457, acc 1, learning_rate 0.0001
2017-10-10T11:30:40.528119: step 5783, loss 0.0435673, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:40.690371: step 5784, loss 0.0501179, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:40.854285: step 5785, loss 0.0680399, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:41.019277: step 5786, loss 0.140874, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:41.180597: step 5787, loss 0.0176808, acc 1, learning_rate 0.0001
2017-10-10T11:30:41.343539: step 5788, loss 0.0536514, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:41.506601: step 5789, loss 0.0734057, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:41.669178: step 5790, loss 0.0499772, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:41.829627: step 5791, loss 0.101302, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:42.001743: step 5792, loss 0.0675191, acc 1, learning_rate 0.0001
2017-10-10T11:30:42.161442: step 5793, loss 0.0258799, acc 1, learning_rate 0.0001
2017-10-10T11:30:42.324568: step 5794, loss 0.0729755, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:42.484480: step 5795, loss 0.032577, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:42.648007: step 5796, loss 0.0424828, acc 1, learning_rate 0.0001
2017-10-10T11:30:42.808068: step 5797, loss 0.0652025, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:42.971616: step 5798, loss 0.045138, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:43.137415: step 5799, loss 0.0930684, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:43.301948: step 5800, loss 0.0396789, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:30:43.774005: step 5800, loss 0.20271, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5800

2017-10-10T11:30:44.486977: step 5801, loss 0.0676926, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:44.651088: step 5802, loss 0.104041, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:44.814282: step 5803, loss 0.0788668, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:44.978710: step 5804, loss 0.061879, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:45.141519: step 5805, loss 0.0785341, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:45.303422: step 5806, loss 0.0787628, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:45.463339: step 5807, loss 0.0486529, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:45.623896: step 5808, loss 0.0947048, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:45.786574: step 5809, loss 0.0596067, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:45.947251: step 5810, loss 0.149254, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:46.110904: step 5811, loss 0.0860794, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:46.275736: step 5812, loss 0.0201605, acc 1, learning_rate 0.0001
2017-10-10T11:30:46.438692: step 5813, loss 0.0202998, acc 1, learning_rate 0.0001
2017-10-10T11:30:46.599908: step 5814, loss 0.0317218, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:46.761567: step 5815, loss 0.0299504, acc 1, learning_rate 0.0001
2017-10-10T11:30:46.924562: step 5816, loss 0.032284, acc 1, learning_rate 0.0001
2017-10-10T11:30:47.085247: step 5817, loss 0.0488595, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:47.244019: step 5818, loss 0.0846916, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:47.403864: step 5819, loss 0.101549, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:47.569307: step 5820, loss 0.137153, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:47.730817: step 5821, loss 0.0449755, acc 1, learning_rate 0.0001
2017-10-10T11:30:47.899281: step 5822, loss 0.131164, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:48.061563: step 5823, loss 0.0688246, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:48.221141: step 5824, loss 0.0853798, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:48.384593: step 5825, loss 0.0564863, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:48.546650: step 5826, loss 0.0127068, acc 1, learning_rate 0.0001
2017-10-10T11:30:48.708621: step 5827, loss 0.0306185, acc 1, learning_rate 0.0001
2017-10-10T11:30:48.873123: step 5828, loss 0.07495, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:49.035643: step 5829, loss 0.0650473, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:49.196731: step 5830, loss 0.0531397, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:49.361447: step 5831, loss 0.0875898, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:49.526553: step 5832, loss 0.0878008, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:49.689569: step 5833, loss 0.0600781, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:49.852614: step 5834, loss 0.0497608, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:50.017340: step 5835, loss 0.0901451, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:50.181838: step 5836, loss 0.115696, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:50.343283: step 5837, loss 0.0505792, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:50.504106: step 5838, loss 0.0334035, acc 1, learning_rate 0.0001
2017-10-10T11:30:50.665988: step 5839, loss 0.0266993, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:50.827287: step 5840, loss 0.151632, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:30:51.310968: step 5840, loss 0.19904, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5840

2017-10-10T11:30:51.884948: step 5841, loss 0.0821845, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:52.048444: step 5842, loss 0.0410439, acc 1, learning_rate 0.0001
2017-10-10T11:30:52.207157: step 5843, loss 0.0488367, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:52.367345: step 5844, loss 0.15365, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:52.527523: step 5845, loss 0.104498, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:52.690957: step 5846, loss 0.0275816, acc 1, learning_rate 0.0001
2017-10-10T11:30:52.858725: step 5847, loss 0.0434028, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:53.024210: step 5848, loss 0.0596913, acc 1, learning_rate 0.0001
2017-10-10T11:30:53.189234: step 5849, loss 0.00637318, acc 1, learning_rate 0.0001
2017-10-10T11:30:53.349523: step 5850, loss 0.0586152, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:53.510755: step 5851, loss 0.0826395, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:53.670516: step 5852, loss 0.0780687, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:53.833615: step 5853, loss 0.0432014, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:53.998730: step 5854, loss 0.0272079, acc 1, learning_rate 0.0001
2017-10-10T11:30:54.159698: step 5855, loss 0.0736317, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:54.323557: step 5856, loss 0.0211464, acc 1, learning_rate 0.0001
2017-10-10T11:30:54.483893: step 5857, loss 0.0563306, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:54.644190: step 5858, loss 0.0918821, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:54.805909: step 5859, loss 0.0378325, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:54.981238: step 5860, loss 0.169114, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:55.141235: step 5861, loss 0.0881285, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:55.303540: step 5862, loss 0.0628109, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:55.468627: step 5863, loss 0.108299, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:55.629642: step 5864, loss 0.0501049, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:55.791210: step 5865, loss 0.0757378, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:55.952813: step 5866, loss 0.12735, acc 0.9375, learning_rate 0.0001
2017-10-10T11:30:56.115813: step 5867, loss 0.0734035, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:56.277662: step 5868, loss 0.0257746, acc 1, learning_rate 0.0001
2017-10-10T11:30:56.445293: step 5869, loss 0.025778, acc 1, learning_rate 0.0001
2017-10-10T11:30:56.604795: step 5870, loss 0.0286418, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:56.769242: step 5871, loss 0.059081, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:56.936638: step 5872, loss 0.113283, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:57.097747: step 5873, loss 0.101238, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:57.260141: step 5874, loss 0.0456965, acc 1, learning_rate 0.0001
2017-10-10T11:30:57.422755: step 5875, loss 0.0605782, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:57.583595: step 5876, loss 0.119793, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:57.744869: step 5877, loss 0.0336304, acc 0.984375, learning_rate 0.0001
2017-10-10T11:30:57.909495: step 5878, loss 0.0824667, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:58.070095: step 5879, loss 0.200539, acc 0.953125, learning_rate 0.0001
2017-10-10T11:30:58.205143: step 5880, loss 0.0899397, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-10T11:30:58.689385: step 5880, loss 0.20269, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5880

2017-10-10T11:30:59.339609: step 5881, loss 0.0295057, acc 1, learning_rate 0.0001
2017-10-10T11:30:59.502204: step 5882, loss 0.0147865, acc 1, learning_rate 0.0001
2017-10-10T11:30:59.671462: step 5883, loss 0.0364778, acc 1, learning_rate 0.0001
2017-10-10T11:30:59.831900: step 5884, loss 0.0672075, acc 0.96875, learning_rate 0.0001
2017-10-10T11:30:59.996010: step 5885, loss 0.0259114, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:00.160438: step 5886, loss 0.0333055, acc 1, learning_rate 0.0001
2017-10-10T11:31:00.322284: step 5887, loss 0.0334192, acc 1, learning_rate 0.0001
2017-10-10T11:31:00.483478: step 5888, loss 0.0320874, acc 1, learning_rate 0.0001
2017-10-10T11:31:00.648505: step 5889, loss 0.0579698, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:00.810018: step 5890, loss 0.0800774, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:00.974273: step 5891, loss 0.0226408, acc 1, learning_rate 0.0001
2017-10-10T11:31:01.139533: step 5892, loss 0.0502222, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:01.307103: step 5893, loss 0.0486492, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:01.469136: step 5894, loss 0.0232439, acc 1, learning_rate 0.0001
2017-10-10T11:31:01.633912: step 5895, loss 0.0146035, acc 1, learning_rate 0.0001
2017-10-10T11:31:01.801550: step 5896, loss 0.0816735, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:01.973796: step 5897, loss 0.0438428, acc 1, learning_rate 0.0001
2017-10-10T11:31:02.133627: step 5898, loss 0.0352482, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:02.295219: step 5899, loss 0.0629501, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:02.455067: step 5900, loss 0.0473357, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:02.619654: step 5901, loss 0.0507982, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:02.782178: step 5902, loss 0.0495192, acc 1, learning_rate 0.0001
2017-10-10T11:31:02.945829: step 5903, loss 0.130894, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:03.106516: step 5904, loss 0.176595, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:03.271078: step 5905, loss 0.101528, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:03.431094: step 5906, loss 0.035372, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:03.596620: step 5907, loss 0.0385072, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:03.761109: step 5908, loss 0.137167, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:03.939137: step 5909, loss 0.0333096, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:04.106107: step 5910, loss 0.0691222, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:04.268348: step 5911, loss 0.0817638, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:04.428985: step 5912, loss 0.118934, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:04.593632: step 5913, loss 0.030423, acc 1, learning_rate 0.0001
2017-10-10T11:31:04.754710: step 5914, loss 0.05779, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:04.918166: step 5915, loss 0.0352624, acc 1, learning_rate 0.0001
2017-10-10T11:31:05.083860: step 5916, loss 0.0292215, acc 1, learning_rate 0.0001
2017-10-10T11:31:05.245038: step 5917, loss 0.0411494, acc 1, learning_rate 0.0001
2017-10-10T11:31:05.408001: step 5918, loss 0.0436324, acc 1, learning_rate 0.0001
2017-10-10T11:31:05.568686: step 5919, loss 0.0561849, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:05.735077: step 5920, loss 0.102867, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:31:06.221864: step 5920, loss 0.201681, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5920

2017-10-10T11:31:06.934514: step 5921, loss 0.0502762, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:07.096566: step 5922, loss 0.0779477, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:07.261059: step 5923, loss 0.0709843, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:07.422225: step 5924, loss 0.0169224, acc 1, learning_rate 0.0001
2017-10-10T11:31:07.585141: step 5925, loss 0.142928, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:07.748707: step 5926, loss 0.0962748, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:07.918643: step 5927, loss 0.0627125, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:08.080337: step 5928, loss 0.0515793, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:08.243688: step 5929, loss 0.0435585, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:08.406878: step 5930, loss 0.132567, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:08.568717: step 5931, loss 0.082417, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:08.732812: step 5932, loss 0.0882054, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:08.902595: step 5933, loss 0.0993784, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:09.063760: step 5934, loss 0.0439671, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:09.228155: step 5935, loss 0.140518, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:09.389961: step 5936, loss 0.0309241, acc 1, learning_rate 0.0001
2017-10-10T11:31:09.549545: step 5937, loss 0.0442846, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:09.712632: step 5938, loss 0.128138, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:09.879610: step 5939, loss 0.119155, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:10.044580: step 5940, loss 0.0574152, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:10.206936: step 5941, loss 0.0701039, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:10.369748: step 5942, loss 0.045247, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:10.533408: step 5943, loss 0.135453, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:10.694493: step 5944, loss 0.0147002, acc 1, learning_rate 0.0001
2017-10-10T11:31:10.856473: step 5945, loss 0.05636, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:11.019195: step 5946, loss 0.021081, acc 1, learning_rate 0.0001
2017-10-10T11:31:11.185245: step 5947, loss 0.0303413, acc 1, learning_rate 0.0001
2017-10-10T11:31:11.347299: step 5948, loss 0.0641022, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:11.510094: step 5949, loss 0.0707198, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:11.673134: step 5950, loss 0.0698032, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:11.836651: step 5951, loss 0.0272137, acc 1, learning_rate 0.0001
2017-10-10T11:31:12.001830: step 5952, loss 0.0768031, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:12.166733: step 5953, loss 0.0547302, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:12.328379: step 5954, loss 0.0980576, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:12.491921: step 5955, loss 0.0755484, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:12.651581: step 5956, loss 0.0906602, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:12.812797: step 5957, loss 0.111051, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:12.977483: step 5958, loss 0.0908516, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:13.139805: step 5959, loss 0.108266, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:13.300046: step 5960, loss 0.0384901, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:31:13.771776: step 5960, loss 0.201022, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-5960

2017-10-10T11:31:14.362018: step 5961, loss 0.103782, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:14.525312: step 5962, loss 0.0456674, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:14.686670: step 5963, loss 0.0497266, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:14.849423: step 5964, loss 0.121337, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:15.010566: step 5965, loss 0.0985885, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:15.175294: step 5966, loss 0.0293927, acc 1, learning_rate 0.0001
2017-10-10T11:31:15.338471: step 5967, loss 0.0546795, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:15.501931: step 5968, loss 0.0455802, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:15.666130: step 5969, loss 0.0541253, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:15.827913: step 5970, loss 0.0479106, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:15.991162: step 5971, loss 0.0188833, acc 1, learning_rate 0.0001
2017-10-10T11:31:16.151289: step 5972, loss 0.0726609, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:16.314375: step 5973, loss 0.0380504, acc 1, learning_rate 0.0001
2017-10-10T11:31:16.482582: step 5974, loss 0.0456065, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:16.645036: step 5975, loss 0.0769845, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:16.805184: step 5976, loss 0.100362, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:16.967914: step 5977, loss 0.0396066, acc 1, learning_rate 0.0001
2017-10-10T11:31:17.104861: step 5978, loss 0.0310847, acc 1, learning_rate 0.0001
2017-10-10T11:31:17.266896: step 5979, loss 0.0531254, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:17.430704: step 5980, loss 0.0408719, acc 1, learning_rate 0.0001
2017-10-10T11:31:17.591550: step 5981, loss 0.120616, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:17.751847: step 5982, loss 0.0422367, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:17.916853: step 5983, loss 0.0734113, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:18.084065: step 5984, loss 0.107091, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:18.246404: step 5985, loss 0.0396025, acc 1, learning_rate 0.0001
2017-10-10T11:31:18.409480: step 5986, loss 0.0974005, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:18.569223: step 5987, loss 0.131014, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:18.732086: step 5988, loss 0.0673555, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:18.895283: step 5989, loss 0.0288411, acc 1, learning_rate 0.0001
2017-10-10T11:31:19.057425: step 5990, loss 0.0533327, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:19.218108: step 5991, loss 0.0751614, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:19.384081: step 5992, loss 0.0287899, acc 1, learning_rate 0.0001
2017-10-10T11:31:19.545348: step 5993, loss 0.0389633, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:19.706342: step 5994, loss 0.0779566, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:19.869435: step 5995, loss 0.0839903, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:20.026745: step 5996, loss 0.0870275, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:20.185362: step 5997, loss 0.0221912, acc 1, learning_rate 0.0001
2017-10-10T11:31:20.345476: step 5998, loss 0.0281707, acc 1, learning_rate 0.0001
2017-10-10T11:31:20.507503: step 5999, loss 0.0562488, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:20.671715: step 6000, loss 0.0318897, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:31:21.153301: step 6000, loss 0.204246, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6000

2017-10-10T11:31:21.789581: step 6001, loss 0.0822014, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:21.959110: step 6002, loss 0.0552356, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:22.120013: step 6003, loss 0.0584721, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:22.282100: step 6004, loss 0.117371, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:22.444810: step 6005, loss 0.0225463, acc 1, learning_rate 0.0001
2017-10-10T11:31:22.608406: step 6006, loss 0.0274839, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:22.769024: step 6007, loss 0.0429167, acc 1, learning_rate 0.0001
2017-10-10T11:31:22.935339: step 6008, loss 0.0702532, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:23.098467: step 6009, loss 0.0485482, acc 1, learning_rate 0.0001
2017-10-10T11:31:23.259850: step 6010, loss 0.116536, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:23.423066: step 6011, loss 0.0770236, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:23.586389: step 6012, loss 0.0591258, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:23.747537: step 6013, loss 0.0784899, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:23.920955: step 6014, loss 0.0245459, acc 1, learning_rate 0.0001
2017-10-10T11:31:24.081172: step 6015, loss 0.0528333, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:24.245844: step 6016, loss 0.0976641, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:24.408265: step 6017, loss 0.08615, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:24.568996: step 6018, loss 0.0880031, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:24.731279: step 6019, loss 0.077002, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:24.894604: step 6020, loss 0.0447159, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:25.055708: step 6021, loss 0.044242, acc 1, learning_rate 0.0001
2017-10-10T11:31:25.220891: step 6022, loss 0.0564998, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:25.381131: step 6023, loss 0.0561286, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:25.540696: step 6024, loss 0.0280283, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:25.702613: step 6025, loss 0.0482868, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:25.868295: step 6026, loss 0.0426934, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:26.033078: step 6027, loss 0.0599463, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:26.196616: step 6028, loss 0.123186, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:26.361325: step 6029, loss 0.0655691, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:26.523634: step 6030, loss 0.0956548, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:26.686004: step 6031, loss 0.0337716, acc 1, learning_rate 0.0001
2017-10-10T11:31:26.852140: step 6032, loss 0.0756741, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:27.014186: step 6033, loss 0.049889, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:27.180781: step 6034, loss 0.0420291, acc 1, learning_rate 0.0001
2017-10-10T11:31:27.341202: step 6035, loss 0.0387018, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:27.503629: step 6036, loss 0.0830124, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:27.664792: step 6037, loss 0.0257721, acc 1, learning_rate 0.0001
2017-10-10T11:31:27.827031: step 6038, loss 0.0941161, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:27.990576: step 6039, loss 0.0784917, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:28.152477: step 6040, loss 0.0781893, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:31:28.581232: step 6040, loss 0.202062, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6040

2017-10-10T11:31:29.293047: step 6041, loss 0.0562154, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:29.457834: step 6042, loss 0.0319584, acc 1, learning_rate 0.0001
2017-10-10T11:31:29.620028: step 6043, loss 0.103205, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:29.780398: step 6044, loss 0.0381239, acc 1, learning_rate 0.0001
2017-10-10T11:31:29.945224: step 6045, loss 0.0984633, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:30.108122: step 6046, loss 0.034618, acc 1, learning_rate 0.0001
2017-10-10T11:31:30.271613: step 6047, loss 0.0313244, acc 1, learning_rate 0.0001
2017-10-10T11:31:30.433137: step 6048, loss 0.106883, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:30.594990: step 6049, loss 0.0664803, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:30.758506: step 6050, loss 0.114494, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:30.934383: step 6051, loss 0.0537069, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:31.096905: step 6052, loss 0.075357, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:31.262520: step 6053, loss 0.134482, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:31.426495: step 6054, loss 0.0346746, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:31.590965: step 6055, loss 0.0688752, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:31.748728: step 6056, loss 0.0296022, acc 1, learning_rate 0.0001
2017-10-10T11:31:31.919733: step 6057, loss 0.0345301, acc 1, learning_rate 0.0001
2017-10-10T11:31:32.079310: step 6058, loss 0.0512108, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:32.243781: step 6059, loss 0.0672339, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:32.406283: step 6060, loss 0.0773331, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:32.568537: step 6061, loss 0.101436, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:32.730163: step 6062, loss 0.117761, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:32.895312: step 6063, loss 0.0148505, acc 1, learning_rate 0.0001
2017-10-10T11:31:33.058601: step 6064, loss 0.122808, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:33.220585: step 6065, loss 0.0640994, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:33.385437: step 6066, loss 0.106737, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:33.549991: step 6067, loss 0.0470174, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:33.711502: step 6068, loss 0.044886, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:33.878397: step 6069, loss 0.0130882, acc 1, learning_rate 0.0001
2017-10-10T11:31:34.043478: step 6070, loss 0.102063, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:34.207438: step 6071, loss 0.0750975, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:34.368966: step 6072, loss 0.0956434, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:34.529200: step 6073, loss 0.0317339, acc 1, learning_rate 0.0001
2017-10-10T11:31:34.692399: step 6074, loss 0.0336233, acc 1, learning_rate 0.0001
2017-10-10T11:31:34.854562: step 6075, loss 0.0501969, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:34.988684: step 6076, loss 0.0804642, acc 0.980392, learning_rate 0.0001
2017-10-10T11:31:35.151753: step 6077, loss 0.0815273, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:35.313092: step 6078, loss 0.0252406, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:35.478539: step 6079, loss 0.0185176, acc 1, learning_rate 0.0001
2017-10-10T11:31:35.643578: step 6080, loss 0.0111351, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:31:36.069532: step 6080, loss 0.203555, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6080

2017-10-10T11:31:36.641658: step 6081, loss 0.0727066, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:36.808076: step 6082, loss 0.0315894, acc 1, learning_rate 0.0001
2017-10-10T11:31:36.972966: step 6083, loss 0.0798397, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:37.135266: step 6084, loss 0.0601106, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:37.298831: step 6085, loss 0.0493475, acc 1, learning_rate 0.0001
2017-10-10T11:31:37.461613: step 6086, loss 0.0140051, acc 1, learning_rate 0.0001
2017-10-10T11:31:37.627655: step 6087, loss 0.125494, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:37.790830: step 6088, loss 0.0532498, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:37.961097: step 6089, loss 0.0133285, acc 1, learning_rate 0.0001
2017-10-10T11:31:38.120534: step 6090, loss 0.0725002, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:38.284497: step 6091, loss 0.0296181, acc 1, learning_rate 0.0001
2017-10-10T11:31:38.448890: step 6092, loss 0.0854872, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:38.608051: step 6093, loss 0.0223819, acc 1, learning_rate 0.0001
2017-10-10T11:31:38.770674: step 6094, loss 0.0358642, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:38.934816: step 6095, loss 0.0748637, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:39.097151: step 6096, loss 0.0316858, acc 1, learning_rate 0.0001
2017-10-10T11:31:39.259126: step 6097, loss 0.122054, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:39.420104: step 6098, loss 0.128673, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:39.582065: step 6099, loss 0.0790355, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:39.741260: step 6100, loss 0.0426352, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:39.905032: step 6101, loss 0.0258128, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:40.072588: step 6102, loss 0.058662, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:40.237820: step 6103, loss 0.086543, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:40.400169: step 6104, loss 0.0148414, acc 1, learning_rate 0.0001
2017-10-10T11:31:40.564762: step 6105, loss 0.0671856, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:40.726068: step 6106, loss 0.0326073, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:40.896096: step 6107, loss 0.183911, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:41.057638: step 6108, loss 0.0171768, acc 1, learning_rate 0.0001
2017-10-10T11:31:41.218689: step 6109, loss 0.107796, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:41.381293: step 6110, loss 0.0543217, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:41.546455: step 6111, loss 0.133284, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:41.710515: step 6112, loss 0.0886638, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:41.874221: step 6113, loss 0.0229984, acc 1, learning_rate 0.0001
2017-10-10T11:31:42.033885: step 6114, loss 0.0409164, acc 1, learning_rate 0.0001
2017-10-10T11:31:42.196290: step 6115, loss 0.143982, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:42.358791: step 6116, loss 0.0872514, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:42.519305: step 6117, loss 0.0329435, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:42.679076: step 6118, loss 0.107125, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:42.843361: step 6119, loss 0.0530584, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:43.007110: step 6120, loss 0.0286213, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:31:43.407932: step 6120, loss 0.199871, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6120

2017-10-10T11:31:44.048362: step 6121, loss 0.078384, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:44.213257: step 6122, loss 0.0365655, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:44.376013: step 6123, loss 0.0408082, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:44.538340: step 6124, loss 0.0215134, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:44.702698: step 6125, loss 0.145834, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:44.868999: step 6126, loss 0.0481234, acc 1, learning_rate 0.0001
2017-10-10T11:31:45.028079: step 6127, loss 0.104512, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:45.192220: step 6128, loss 0.0190076, acc 1, learning_rate 0.0001
2017-10-10T11:31:45.352817: step 6129, loss 0.0244278, acc 1, learning_rate 0.0001
2017-10-10T11:31:45.516584: step 6130, loss 0.0918071, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:45.676773: step 6131, loss 0.0586879, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:45.840070: step 6132, loss 0.0716097, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:46.004174: step 6133, loss 0.0909, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:46.168466: step 6134, loss 0.0721948, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:46.330831: step 6135, loss 0.0336256, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:46.494850: step 6136, loss 0.0581927, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:46.656923: step 6137, loss 0.0736599, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:46.820558: step 6138, loss 0.0439282, acc 1, learning_rate 0.0001
2017-10-10T11:31:46.983606: step 6139, loss 0.0462157, acc 1, learning_rate 0.0001
2017-10-10T11:31:47.144792: step 6140, loss 0.0803242, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:47.309331: step 6141, loss 0.0768686, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:47.471234: step 6142, loss 0.0858714, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:47.630075: step 6143, loss 0.00918703, acc 1, learning_rate 0.0001
2017-10-10T11:31:47.791581: step 6144, loss 0.060976, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:47.955792: step 6145, loss 0.0586214, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:48.120476: step 6146, loss 0.136553, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:48.281122: step 6147, loss 0.0600481, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:48.444032: step 6148, loss 0.0703739, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:48.605072: step 6149, loss 0.0590985, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:48.766462: step 6150, loss 0.064927, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:48.938556: step 6151, loss 0.0141827, acc 1, learning_rate 0.0001
2017-10-10T11:31:49.100965: step 6152, loss 0.0835661, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:49.261542: step 6153, loss 0.0695045, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:49.426825: step 6154, loss 0.121633, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:49.592417: step 6155, loss 0.054018, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:49.752050: step 6156, loss 0.0598888, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:49.917354: step 6157, loss 0.0172929, acc 1, learning_rate 0.0001
2017-10-10T11:31:50.077800: step 6158, loss 0.0260421, acc 1, learning_rate 0.0001
2017-10-10T11:31:50.240475: step 6159, loss 0.0323593, acc 1, learning_rate 0.0001
2017-10-10T11:31:50.405525: step 6160, loss 0.0500794, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:31:50.835129: step 6160, loss 0.19761, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6160

2017-10-10T11:31:51.476802: step 6161, loss 0.0505401, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:51.639407: step 6162, loss 0.0142447, acc 1, learning_rate 0.0001
2017-10-10T11:31:51.800592: step 6163, loss 0.0241615, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:51.963449: step 6164, loss 0.0740438, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:52.127697: step 6165, loss 0.0854596, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:52.290739: step 6166, loss 0.0843304, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:52.454530: step 6167, loss 0.0415134, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:52.616923: step 6168, loss 0.0350193, acc 1, learning_rate 0.0001
2017-10-10T11:31:52.776607: step 6169, loss 0.103974, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:52.946777: step 6170, loss 0.117167, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:53.110020: step 6171, loss 0.0732222, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:53.274182: step 6172, loss 0.0467344, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:53.438103: step 6173, loss 0.0335126, acc 1, learning_rate 0.0001
2017-10-10T11:31:53.570667: step 6174, loss 0.019786, acc 1, learning_rate 0.0001
2017-10-10T11:31:53.733565: step 6175, loss 0.0736008, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:53.902679: step 6176, loss 0.067307, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:54.065196: step 6177, loss 0.0238671, acc 1, learning_rate 0.0001
2017-10-10T11:31:54.227194: step 6178, loss 0.0649252, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:54.390137: step 6179, loss 0.126246, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:54.549424: step 6180, loss 0.0660603, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:54.710485: step 6181, loss 0.128872, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:54.875296: step 6182, loss 0.123668, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:55.039510: step 6183, loss 0.0759187, acc 1, learning_rate 0.0001
2017-10-10T11:31:55.201562: step 6184, loss 0.120368, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:55.366809: step 6185, loss 0.138301, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:55.528593: step 6186, loss 0.0233169, acc 1, learning_rate 0.0001
2017-10-10T11:31:55.692062: step 6187, loss 0.078327, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:55.853568: step 6188, loss 0.0590181, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:56.015684: step 6189, loss 0.0115942, acc 1, learning_rate 0.0001
2017-10-10T11:31:56.176709: step 6190, loss 0.116783, acc 0.9375, learning_rate 0.0001
2017-10-10T11:31:56.339535: step 6191, loss 0.0446609, acc 1, learning_rate 0.0001
2017-10-10T11:31:56.498715: step 6192, loss 0.0199464, acc 1, learning_rate 0.0001
2017-10-10T11:31:56.660698: step 6193, loss 0.0713702, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:56.824183: step 6194, loss 0.0960344, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:56.990357: step 6195, loss 0.114275, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:57.152324: step 6196, loss 0.114848, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:57.315408: step 6197, loss 0.0456622, acc 1, learning_rate 0.0001
2017-10-10T11:31:57.475473: step 6198, loss 0.046753, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:57.637654: step 6199, loss 0.0708515, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:57.799912: step 6200, loss 0.070003, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:31:58.216818: step 6200, loss 0.198836, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6200

2017-10-10T11:31:58.951731: step 6201, loss 0.0663373, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:59.114214: step 6202, loss 0.0760603, acc 0.953125, learning_rate 0.0001
2017-10-10T11:31:59.278677: step 6203, loss 0.065319, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:59.441955: step 6204, loss 0.058458, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:59.607519: step 6205, loss 0.0225462, acc 0.984375, learning_rate 0.0001
2017-10-10T11:31:59.772154: step 6206, loss 0.108402, acc 0.96875, learning_rate 0.0001
2017-10-10T11:31:59.948530: step 6207, loss 0.125726, acc 0.9375, learning_rate 0.0001
2017-10-10T11:32:00.110804: step 6208, loss 0.0525479, acc 1, learning_rate 0.0001
2017-10-10T11:32:00.274646: step 6209, loss 0.0706475, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:00.440428: step 6210, loss 0.0452294, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:00.600151: step 6211, loss 0.0340915, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:00.761475: step 6212, loss 0.0421274, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:00.926225: step 6213, loss 0.0495286, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:01.086650: step 6214, loss 0.0518553, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:01.252287: step 6215, loss 0.0378431, acc 1, learning_rate 0.0001
2017-10-10T11:32:01.415329: step 6216, loss 0.0572656, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:01.576255: step 6217, loss 0.0479324, acc 1, learning_rate 0.0001
2017-10-10T11:32:01.734670: step 6218, loss 0.0127185, acc 1, learning_rate 0.0001
2017-10-10T11:32:01.906222: step 6219, loss 0.0545342, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:02.067500: step 6220, loss 0.0526546, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:02.232879: step 6221, loss 0.0641037, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:02.395193: step 6222, loss 0.0506961, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:02.555504: step 6223, loss 0.0502617, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:02.717400: step 6224, loss 0.0632395, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:02.883029: step 6225, loss 0.0471231, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:03.043004: step 6226, loss 0.0503317, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:03.208634: step 6227, loss 0.0575803, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:03.369679: step 6228, loss 0.0464608, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:03.531335: step 6229, loss 0.0958884, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:03.692524: step 6230, loss 0.027789, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:03.855289: step 6231, loss 0.0914579, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:04.018260: step 6232, loss 0.0458464, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:04.183538: step 6233, loss 0.0691408, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:04.347312: step 6234, loss 0.057451, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:04.510910: step 6235, loss 0.0164849, acc 1, learning_rate 0.0001
2017-10-10T11:32:04.668600: step 6236, loss 0.0766304, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:04.828535: step 6237, loss 0.0807819, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:04.995174: step 6238, loss 0.0619045, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:05.161696: step 6239, loss 0.00954146, acc 1, learning_rate 0.0001
2017-10-10T11:32:05.322698: step 6240, loss 0.044829, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:32:05.748637: step 6240, loss 0.19934, acc 0.936691

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6240

2017-10-10T11:32:06.322643: step 6241, loss 0.0158809, acc 1, learning_rate 0.0001
2017-10-10T11:32:06.487343: step 6242, loss 0.0730988, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:06.651747: step 6243, loss 0.0311936, acc 1, learning_rate 0.0001
2017-10-10T11:32:06.813226: step 6244, loss 0.0631494, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:06.976613: step 6245, loss 0.050438, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:07.139178: step 6246, loss 0.0205672, acc 1, learning_rate 0.0001
2017-10-10T11:32:07.302170: step 6247, loss 0.0646519, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:07.462987: step 6248, loss 0.0697006, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:07.625773: step 6249, loss 0.0772705, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:07.791316: step 6250, loss 0.0809024, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:07.954586: step 6251, loss 0.0720138, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:08.118413: step 6252, loss 0.0526568, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:08.282332: step 6253, loss 0.0977098, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:08.446620: step 6254, loss 0.075173, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:08.610470: step 6255, loss 0.0714505, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:08.772309: step 6256, loss 0.0963924, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:08.933498: step 6257, loss 0.0533281, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:09.096391: step 6258, loss 0.0198227, acc 1, learning_rate 0.0001
2017-10-10T11:32:09.259643: step 6259, loss 0.0117898, acc 1, learning_rate 0.0001
2017-10-10T11:32:09.420182: step 6260, loss 0.016019, acc 1, learning_rate 0.0001
2017-10-10T11:32:09.581514: step 6261, loss 0.0838981, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:09.744270: step 6262, loss 0.144318, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:09.914231: step 6263, loss 0.0258076, acc 1, learning_rate 0.0001
2017-10-10T11:32:10.075301: step 6264, loss 0.0585714, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:10.238790: step 6265, loss 0.0489447, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:10.400126: step 6266, loss 0.0683181, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:10.562721: step 6267, loss 0.114887, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:10.724768: step 6268, loss 0.037006, acc 1, learning_rate 0.0001
2017-10-10T11:32:10.886891: step 6269, loss 0.0836923, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:11.048016: step 6270, loss 0.0595128, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:11.211467: step 6271, loss 0.0318491, acc 1, learning_rate 0.0001
2017-10-10T11:32:11.347336: step 6272, loss 0.0572229, acc 1, learning_rate 0.0001
2017-10-10T11:32:11.511289: step 6273, loss 0.0637182, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:11.672846: step 6274, loss 0.0716207, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:11.831528: step 6275, loss 0.0955829, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:11.993965: step 6276, loss 0.136201, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:12.156673: step 6277, loss 0.05414, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:12.314717: step 6278, loss 0.0562637, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:12.480549: step 6279, loss 0.0313077, acc 1, learning_rate 0.0001
2017-10-10T11:32:12.642803: step 6280, loss 0.077591, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:32:13.052730: step 6280, loss 0.198881, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6280

2017-10-10T11:32:13.687227: step 6281, loss 0.104447, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:13.850847: step 6282, loss 0.0235753, acc 1, learning_rate 0.0001
2017-10-10T11:32:14.015628: step 6283, loss 0.107481, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:14.180594: step 6284, loss 0.0381042, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:14.342709: step 6285, loss 0.0983409, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:14.503762: step 6286, loss 0.0128327, acc 1, learning_rate 0.0001
2017-10-10T11:32:14.666165: step 6287, loss 0.0165328, acc 1, learning_rate 0.0001
2017-10-10T11:32:14.828277: step 6288, loss 0.0894045, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:14.989469: step 6289, loss 0.155621, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:15.151064: step 6290, loss 0.0543625, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:15.313970: step 6291, loss 0.0356628, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:15.474970: step 6292, loss 0.0419554, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:15.640283: step 6293, loss 0.0345693, acc 1, learning_rate 0.0001
2017-10-10T11:32:15.802512: step 6294, loss 0.0762503, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:15.968955: step 6295, loss 0.0253173, acc 1, learning_rate 0.0001
2017-10-10T11:32:16.130840: step 6296, loss 0.0270139, acc 1, learning_rate 0.0001
2017-10-10T11:32:16.294410: step 6297, loss 0.120501, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:16.457991: step 6298, loss 0.0377621, acc 1, learning_rate 0.0001
2017-10-10T11:32:16.620785: step 6299, loss 0.061588, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:16.782153: step 6300, loss 0.0274998, acc 1, learning_rate 0.0001
2017-10-10T11:32:16.946640: step 6301, loss 0.0656019, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:17.106349: step 6302, loss 0.0159714, acc 1, learning_rate 0.0001
2017-10-10T11:32:17.269374: step 6303, loss 0.0388322, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:17.437058: step 6304, loss 0.028594, acc 1, learning_rate 0.0001
2017-10-10T11:32:17.599587: step 6305, loss 0.0475748, acc 1, learning_rate 0.0001
2017-10-10T11:32:17.763504: step 6306, loss 0.103488, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:17.927251: step 6307, loss 0.0216986, acc 1, learning_rate 0.0001
2017-10-10T11:32:18.088487: step 6308, loss 0.0598672, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:18.248631: step 6309, loss 0.0376842, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:18.407494: step 6310, loss 0.0276479, acc 1, learning_rate 0.0001
2017-10-10T11:32:18.568607: step 6311, loss 0.073491, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:18.730309: step 6312, loss 0.0858584, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:18.893078: step 6313, loss 0.0330681, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:19.058584: step 6314, loss 0.0787268, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:19.220888: step 6315, loss 0.133914, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:19.389570: step 6316, loss 0.0549913, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:19.551343: step 6317, loss 0.0431718, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:19.715128: step 6318, loss 0.12904, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:19.884063: step 6319, loss 0.0174408, acc 1, learning_rate 0.0001
2017-10-10T11:32:20.048700: step 6320, loss 0.0502854, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:32:20.456116: step 6320, loss 0.19966, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6320

2017-10-10T11:32:21.166020: step 6321, loss 0.116354, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:21.330134: step 6322, loss 0.0410681, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:21.496441: step 6323, loss 0.0767104, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:21.657664: step 6324, loss 0.0800683, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:21.820922: step 6325, loss 0.0196429, acc 1, learning_rate 0.0001
2017-10-10T11:32:21.998556: step 6326, loss 0.0149693, acc 1, learning_rate 0.0001
2017-10-10T11:32:22.160552: step 6327, loss 0.144279, acc 0.9375, learning_rate 0.0001
2017-10-10T11:32:22.321791: step 6328, loss 0.0455723, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:22.484636: step 6329, loss 0.0208868, acc 1, learning_rate 0.0001
2017-10-10T11:32:22.655240: step 6330, loss 0.0639731, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:22.820257: step 6331, loss 0.0445425, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:22.986645: step 6332, loss 0.023502, acc 1, learning_rate 0.0001
2017-10-10T11:32:23.149613: step 6333, loss 0.0958567, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:23.310469: step 6334, loss 0.0899837, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:23.471311: step 6335, loss 0.0687332, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:23.633281: step 6336, loss 0.0269719, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:23.795237: step 6337, loss 0.0471189, acc 1, learning_rate 0.0001
2017-10-10T11:32:23.956093: step 6338, loss 0.00639517, acc 1, learning_rate 0.0001
2017-10-10T11:32:24.118401: step 6339, loss 0.0270829, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:24.280901: step 6340, loss 0.0438682, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:24.443314: step 6341, loss 0.0585901, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:24.608449: step 6342, loss 0.0395361, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:24.769572: step 6343, loss 0.102635, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:24.936808: step 6344, loss 0.0298619, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:25.100616: step 6345, loss 0.0322794, acc 1, learning_rate 0.0001
2017-10-10T11:32:25.264867: step 6346, loss 0.0440007, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:25.430101: step 6347, loss 0.0429009, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:25.593343: step 6348, loss 0.122653, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:25.754096: step 6349, loss 0.053095, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:25.918812: step 6350, loss 0.0235737, acc 1, learning_rate 0.0001
2017-10-10T11:32:26.082361: step 6351, loss 0.0765191, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:26.245662: step 6352, loss 0.129123, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:26.409942: step 6353, loss 0.110705, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:26.571966: step 6354, loss 0.118023, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:26.733651: step 6355, loss 0.0444134, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:26.894930: step 6356, loss 0.0951659, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:27.053740: step 6357, loss 0.0220966, acc 1, learning_rate 0.0001
2017-10-10T11:32:27.216941: step 6358, loss 0.0327954, acc 1, learning_rate 0.0001
2017-10-10T11:32:27.379891: step 6359, loss 0.0437809, acc 1, learning_rate 0.0001
2017-10-10T11:32:27.543583: step 6360, loss 0.187402, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:32:27.978967: step 6360, loss 0.201513, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6360

2017-10-10T11:32:28.554849: step 6361, loss 0.1569, acc 0.921875, learning_rate 0.0001
2017-10-10T11:32:28.718218: step 6362, loss 0.0212966, acc 1, learning_rate 0.0001
2017-10-10T11:32:28.899518: step 6363, loss 0.114438, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:29.062462: step 6364, loss 0.0898152, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:29.222487: step 6365, loss 0.0719499, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:29.386260: step 6366, loss 0.022924, acc 1, learning_rate 0.0001
2017-10-10T11:32:29.548046: step 6367, loss 0.0621179, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:29.711186: step 6368, loss 0.0329349, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:29.875136: step 6369, loss 0.116347, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:30.011298: step 6370, loss 0.0245395, acc 1, learning_rate 0.0001
2017-10-10T11:32:30.172683: step 6371, loss 0.101694, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:30.338262: step 6372, loss 0.0422677, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:30.503082: step 6373, loss 0.0511742, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:30.665182: step 6374, loss 0.0298981, acc 1, learning_rate 0.0001
2017-10-10T11:32:30.826800: step 6375, loss 0.0386021, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:30.991537: step 6376, loss 0.0624615, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:31.154917: step 6377, loss 0.0566757, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:31.317292: step 6378, loss 0.0661041, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:31.485651: step 6379, loss 0.108871, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:31.649919: step 6380, loss 0.0954961, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:31.815032: step 6381, loss 0.100622, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:31.978134: step 6382, loss 0.0424906, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:32.142521: step 6383, loss 0.0763161, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:32.307372: step 6384, loss 0.0299744, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:32.469465: step 6385, loss 0.124992, acc 0.9375, learning_rate 0.0001
2017-10-10T11:32:32.630556: step 6386, loss 0.0573341, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:32.796620: step 6387, loss 0.0296504, acc 1, learning_rate 0.0001
2017-10-10T11:32:32.966826: step 6388, loss 0.0594798, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:33.130038: step 6389, loss 0.0894039, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:33.291539: step 6390, loss 0.0706017, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:33.454540: step 6391, loss 0.0375105, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:33.618516: step 6392, loss 0.0972352, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:33.781820: step 6393, loss 0.0179505, acc 1, learning_rate 0.0001
2017-10-10T11:32:33.943369: step 6394, loss 0.0362286, acc 1, learning_rate 0.0001
2017-10-10T11:32:34.104082: step 6395, loss 0.0475868, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:34.267735: step 6396, loss 0.159052, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:34.432781: step 6397, loss 0.110827, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:34.596582: step 6398, loss 0.0307106, acc 1, learning_rate 0.0001
2017-10-10T11:32:34.758513: step 6399, loss 0.0326906, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:34.922439: step 6400, loss 0.0728938, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:32:35.348110: step 6400, loss 0.204899, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6400

2017-10-10T11:32:36.209337: step 6401, loss 0.11027, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:36.373679: step 6402, loss 0.033181, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:36.537364: step 6403, loss 0.146895, acc 0.9375, learning_rate 0.0001
2017-10-10T11:32:36.700528: step 6404, loss 0.164124, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:36.866902: step 6405, loss 0.0413874, acc 1, learning_rate 0.0001
2017-10-10T11:32:37.031453: step 6406, loss 0.0642145, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:37.195221: step 6407, loss 0.154832, acc 0.9375, learning_rate 0.0001
2017-10-10T11:32:37.358211: step 6408, loss 0.0424139, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:37.520821: step 6409, loss 0.0425314, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:37.682922: step 6410, loss 0.0573979, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:37.845382: step 6411, loss 0.0675422, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:38.006063: step 6412, loss 0.0245302, acc 1, learning_rate 0.0001
2017-10-10T11:32:38.166798: step 6413, loss 0.0325098, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:38.328709: step 6414, loss 0.00929083, acc 1, learning_rate 0.0001
2017-10-10T11:32:38.490900: step 6415, loss 0.0604301, acc 1, learning_rate 0.0001
2017-10-10T11:32:38.651668: step 6416, loss 0.0501215, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:38.814159: step 6417, loss 0.0338661, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:38.981261: step 6418, loss 0.0763902, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:39.145308: step 6419, loss 0.0531758, acc 1, learning_rate 0.0001
2017-10-10T11:32:39.309989: step 6420, loss 0.0772456, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:39.473380: step 6421, loss 0.0604318, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:39.637238: step 6422, loss 0.0511205, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:39.801237: step 6423, loss 0.0427998, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:39.966970: step 6424, loss 0.0719256, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:40.133510: step 6425, loss 0.105543, acc 0.9375, learning_rate 0.0001
2017-10-10T11:32:40.291998: step 6426, loss 0.0761223, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:40.454028: step 6427, loss 0.0186157, acc 1, learning_rate 0.0001
2017-10-10T11:32:40.622781: step 6428, loss 0.0126255, acc 1, learning_rate 0.0001
2017-10-10T11:32:40.783474: step 6429, loss 0.0341743, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:40.949399: step 6430, loss 0.0599402, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:41.111714: step 6431, loss 0.031674, acc 1, learning_rate 0.0001
2017-10-10T11:32:41.269766: step 6432, loss 0.0246338, acc 1, learning_rate 0.0001
2017-10-10T11:32:41.430787: step 6433, loss 0.0311171, acc 1, learning_rate 0.0001
2017-10-10T11:32:41.595277: step 6434, loss 0.0412572, acc 1, learning_rate 0.0001
2017-10-10T11:32:41.758199: step 6435, loss 0.109637, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:41.924731: step 6436, loss 0.035198, acc 1, learning_rate 0.0001
2017-10-10T11:32:42.085345: step 6437, loss 0.0696214, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:42.248941: step 6438, loss 0.0723216, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:42.411371: step 6439, loss 0.0368039, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:42.575386: step 6440, loss 0.0725825, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:32:42.985326: step 6440, loss 0.200506, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6440

2017-10-10T11:32:43.696534: step 6441, loss 0.0300714, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:43.857295: step 6442, loss 0.0176337, acc 1, learning_rate 0.0001
2017-10-10T11:32:44.021681: step 6443, loss 0.0868859, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:44.185085: step 6444, loss 0.0707016, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:44.349534: step 6445, loss 0.034846, acc 1, learning_rate 0.0001
2017-10-10T11:32:44.512377: step 6446, loss 0.0297696, acc 1, learning_rate 0.0001
2017-10-10T11:32:44.673945: step 6447, loss 0.0206904, acc 1, learning_rate 0.0001
2017-10-10T11:32:44.836448: step 6448, loss 0.0223341, acc 1, learning_rate 0.0001
2017-10-10T11:32:45.004344: step 6449, loss 0.0310059, acc 1, learning_rate 0.0001
2017-10-10T11:32:45.164740: step 6450, loss 0.0395719, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:45.328560: step 6451, loss 0.0909155, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:45.490936: step 6452, loss 0.0631325, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:45.652242: step 6453, loss 0.0670821, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:45.813936: step 6454, loss 0.0449335, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:45.979644: step 6455, loss 0.1299, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:46.139840: step 6456, loss 0.0734035, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:46.302376: step 6457, loss 0.0594121, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:46.466336: step 6458, loss 0.0337839, acc 1, learning_rate 0.0001
2017-10-10T11:32:46.631234: step 6459, loss 0.0967533, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:46.799304: step 6460, loss 0.0823811, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:46.962750: step 6461, loss 0.156415, acc 0.9375, learning_rate 0.0001
2017-10-10T11:32:47.129146: step 6462, loss 0.0964758, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:47.289748: step 6463, loss 0.0852142, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:47.458706: step 6464, loss 0.0896095, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:47.624197: step 6465, loss 0.0643143, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:47.788336: step 6466, loss 0.0359641, acc 1, learning_rate 0.0001
2017-10-10T11:32:47.950299: step 6467, loss 0.0832479, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:48.085991: step 6468, loss 0.0328366, acc 1, learning_rate 0.0001
2017-10-10T11:32:48.249434: step 6469, loss 0.0332283, acc 1, learning_rate 0.0001
2017-10-10T11:32:48.411744: step 6470, loss 0.0633387, acc 1, learning_rate 0.0001
2017-10-10T11:32:48.576744: step 6471, loss 0.0494698, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:48.740437: step 6472, loss 0.0953466, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:48.904480: step 6473, loss 0.0512476, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:49.067384: step 6474, loss 0.0607764, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:49.229885: step 6475, loss 0.0979593, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:49.393194: step 6476, loss 0.0345925, acc 1, learning_rate 0.0001
2017-10-10T11:32:49.554770: step 6477, loss 0.115139, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:49.715480: step 6478, loss 0.0774474, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:49.877371: step 6479, loss 0.191204, acc 0.9375, learning_rate 0.0001
2017-10-10T11:32:50.041837: step 6480, loss 0.0882269, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:32:50.441484: step 6480, loss 0.198996, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6480

2017-10-10T11:32:51.042366: step 6481, loss 0.088732, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:51.206059: step 6482, loss 0.0403012, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:51.368315: step 6483, loss 0.113561, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:51.531680: step 6484, loss 0.0289013, acc 1, learning_rate 0.0001
2017-10-10T11:32:51.693045: step 6485, loss 0.0654949, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:51.857935: step 6486, loss 0.05363, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:52.018687: step 6487, loss 0.0650819, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:52.183480: step 6488, loss 0.0663567, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:52.343750: step 6489, loss 0.11102, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:52.502749: step 6490, loss 0.165377, acc 0.921875, learning_rate 0.0001
2017-10-10T11:32:52.662257: step 6491, loss 0.0744203, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:52.823423: step 6492, loss 0.0589258, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:52.986843: step 6493, loss 0.0502017, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:53.147988: step 6494, loss 0.106538, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:53.312128: step 6495, loss 0.0283984, acc 1, learning_rate 0.0001
2017-10-10T11:32:53.471636: step 6496, loss 0.0904829, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:53.633861: step 6497, loss 0.04123, acc 1, learning_rate 0.0001
2017-10-10T11:32:53.794360: step 6498, loss 0.0117634, acc 1, learning_rate 0.0001
2017-10-10T11:32:53.957558: step 6499, loss 0.0437869, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:54.119060: step 6500, loss 0.0730066, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:54.281099: step 6501, loss 0.0232347, acc 1, learning_rate 0.0001
2017-10-10T11:32:54.439821: step 6502, loss 0.0253883, acc 1, learning_rate 0.0001
2017-10-10T11:32:54.603492: step 6503, loss 0.0206546, acc 1, learning_rate 0.0001
2017-10-10T11:32:54.767525: step 6504, loss 0.0550755, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:54.934525: step 6505, loss 0.120063, acc 0.921875, learning_rate 0.0001
2017-10-10T11:32:55.093783: step 6506, loss 0.0587875, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:55.257854: step 6507, loss 0.0299213, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:55.418762: step 6508, loss 0.0504845, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:55.582371: step 6509, loss 0.113748, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:55.748140: step 6510, loss 0.0720148, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:55.911855: step 6511, loss 0.138871, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:56.076504: step 6512, loss 0.0492055, acc 1, learning_rate 0.0001
2017-10-10T11:32:56.239622: step 6513, loss 0.107703, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:56.402271: step 6514, loss 0.0671335, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:56.567139: step 6515, loss 0.0659894, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:56.729508: step 6516, loss 0.0444085, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:56.899758: step 6517, loss 0.209661, acc 0.921875, learning_rate 0.0001
2017-10-10T11:32:57.062340: step 6518, loss 0.0981879, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:57.226099: step 6519, loss 0.0467811, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:57.389923: step 6520, loss 0.0481835, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:32:57.806673: step 6520, loss 0.199318, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6520

2017-10-10T11:32:58.457155: step 6521, loss 0.0896542, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:58.618691: step 6522, loss 0.0456857, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:58.781815: step 6523, loss 0.0249154, acc 1, learning_rate 0.0001
2017-10-10T11:32:58.943767: step 6524, loss 0.119919, acc 0.953125, learning_rate 0.0001
2017-10-10T11:32:59.106535: step 6525, loss 0.0524184, acc 1, learning_rate 0.0001
2017-10-10T11:32:59.268776: step 6526, loss 0.0859563, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:59.433636: step 6527, loss 0.0460711, acc 0.984375, learning_rate 0.0001
2017-10-10T11:32:59.594762: step 6528, loss 0.0344627, acc 1, learning_rate 0.0001
2017-10-10T11:32:59.758193: step 6529, loss 0.0734181, acc 0.96875, learning_rate 0.0001
2017-10-10T11:32:59.929273: step 6530, loss 0.0457819, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:00.090877: step 6531, loss 0.0825441, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:00.251499: step 6532, loss 0.0413285, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:00.415160: step 6533, loss 0.0391217, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:00.583720: step 6534, loss 0.026073, acc 1, learning_rate 0.0001
2017-10-10T11:33:00.748678: step 6535, loss 0.0471527, acc 1, learning_rate 0.0001
2017-10-10T11:33:00.910294: step 6536, loss 0.0685182, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:01.073908: step 6537, loss 0.109786, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:01.238633: step 6538, loss 0.0336237, acc 1, learning_rate 0.0001
2017-10-10T11:33:01.401302: step 6539, loss 0.0156092, acc 1, learning_rate 0.0001
2017-10-10T11:33:01.565295: step 6540, loss 0.0434461, acc 1, learning_rate 0.0001
2017-10-10T11:33:01.726450: step 6541, loss 0.0218299, acc 1, learning_rate 0.0001
2017-10-10T11:33:01.889541: step 6542, loss 0.0346819, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:02.054700: step 6543, loss 0.0406379, acc 1, learning_rate 0.0001
2017-10-10T11:33:02.215914: step 6544, loss 0.10663, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:02.380566: step 6545, loss 0.0531814, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:02.542797: step 6546, loss 0.0274139, acc 1, learning_rate 0.0001
2017-10-10T11:33:02.703341: step 6547, loss 0.0343239, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:02.878073: step 6548, loss 0.0655247, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:03.042317: step 6549, loss 0.0476069, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:03.204488: step 6550, loss 0.0490068, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:03.368435: step 6551, loss 0.06053, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:03.531406: step 6552, loss 0.062536, acc 1, learning_rate 0.0001
2017-10-10T11:33:03.696631: step 6553, loss 0.0551188, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:03.859311: step 6554, loss 0.0685656, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:04.022238: step 6555, loss 0.111578, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:04.186113: step 6556, loss 0.0196148, acc 1, learning_rate 0.0001
2017-10-10T11:33:04.346569: step 6557, loss 0.0581837, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:04.508442: step 6558, loss 0.0216541, acc 1, learning_rate 0.0001
2017-10-10T11:33:04.671535: step 6559, loss 0.0162777, acc 1, learning_rate 0.0001
2017-10-10T11:33:04.835407: step 6560, loss 0.0382574, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:33:05.273584: step 6560, loss 0.204883, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6560

2017-10-10T11:33:05.911518: step 6561, loss 0.0627143, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:06.072670: step 6562, loss 0.0909767, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:06.232513: step 6563, loss 0.0133469, acc 1, learning_rate 0.0001
2017-10-10T11:33:06.397676: step 6564, loss 0.147003, acc 0.9375, learning_rate 0.0001
2017-10-10T11:33:06.557991: step 6565, loss 0.0370329, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:06.695429: step 6566, loss 0.0223599, acc 1, learning_rate 0.0001
2017-10-10T11:33:06.861371: step 6567, loss 0.0470297, acc 1, learning_rate 0.0001
2017-10-10T11:33:07.021089: step 6568, loss 0.143352, acc 0.9375, learning_rate 0.0001
2017-10-10T11:33:07.181892: step 6569, loss 0.0471177, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:07.343923: step 6570, loss 0.0671881, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:07.507588: step 6571, loss 0.0617162, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:07.668182: step 6572, loss 0.043392, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:07.830917: step 6573, loss 0.0853642, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:07.999471: step 6574, loss 0.0173849, acc 1, learning_rate 0.0001
2017-10-10T11:33:08.163228: step 6575, loss 0.00987683, acc 1, learning_rate 0.0001
2017-10-10T11:33:08.324139: step 6576, loss 0.0452085, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:08.485348: step 6577, loss 0.132066, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:08.646875: step 6578, loss 0.081109, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:08.809778: step 6579, loss 0.0339204, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:08.975375: step 6580, loss 0.063698, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:09.136352: step 6581, loss 0.0801537, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:09.296399: step 6582, loss 0.0722335, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:09.460311: step 6583, loss 0.0400104, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:09.624911: step 6584, loss 0.0220669, acc 1, learning_rate 0.0001
2017-10-10T11:33:09.788839: step 6585, loss 0.0380786, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:09.950478: step 6586, loss 0.0283374, acc 1, learning_rate 0.0001
2017-10-10T11:33:10.115938: step 6587, loss 0.0502852, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:10.277940: step 6588, loss 0.0256412, acc 1, learning_rate 0.0001
2017-10-10T11:33:10.442254: step 6589, loss 0.0887303, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:10.604727: step 6590, loss 0.0476737, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:10.765713: step 6591, loss 0.0730209, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:10.928612: step 6592, loss 0.0926604, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:11.091144: step 6593, loss 0.0283738, acc 1, learning_rate 0.0001
2017-10-10T11:33:11.252885: step 6594, loss 0.126225, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:11.413750: step 6595, loss 0.0370939, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:11.575703: step 6596, loss 0.0745326, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:11.736789: step 6597, loss 0.0522333, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:11.900225: step 6598, loss 0.0994461, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:12.062136: step 6599, loss 0.0988582, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:12.224956: step 6600, loss 0.0513389, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:33:12.652301: step 6600, loss 0.198759, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6600

2017-10-10T11:33:13.361919: step 6601, loss 0.0284726, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:13.526697: step 6602, loss 0.0770038, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:13.688864: step 6603, loss 0.0375379, acc 1, learning_rate 0.0001
2017-10-10T11:33:13.852583: step 6604, loss 0.123446, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:14.014920: step 6605, loss 0.0389842, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:14.177526: step 6606, loss 0.0482568, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:14.337075: step 6607, loss 0.0303912, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:14.499974: step 6608, loss 0.0998905, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:14.663317: step 6609, loss 0.0801258, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:14.824385: step 6610, loss 0.0607165, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:14.987948: step 6611, loss 0.0916543, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:15.150710: step 6612, loss 0.0510242, acc 1, learning_rate 0.0001
2017-10-10T11:33:15.312667: step 6613, loss 0.102757, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:15.473240: step 6614, loss 0.0637921, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:15.630681: step 6615, loss 0.093026, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:15.794696: step 6616, loss 0.0421803, acc 1, learning_rate 0.0001
2017-10-10T11:33:15.955492: step 6617, loss 0.119313, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:16.115555: step 6618, loss 0.01793, acc 1, learning_rate 0.0001
2017-10-10T11:33:16.277464: step 6619, loss 0.0309445, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:16.440203: step 6620, loss 0.0218194, acc 1, learning_rate 0.0001
2017-10-10T11:33:16.600811: step 6621, loss 0.029243, acc 1, learning_rate 0.0001
2017-10-10T11:33:16.763371: step 6622, loss 0.0398274, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:16.931500: step 6623, loss 0.0813859, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:17.094365: step 6624, loss 0.13097, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:17.257786: step 6625, loss 0.10785, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:17.420736: step 6626, loss 0.0684942, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:17.584682: step 6627, loss 0.0268383, acc 1, learning_rate 0.0001
2017-10-10T11:33:17.747452: step 6628, loss 0.0436001, acc 1, learning_rate 0.0001
2017-10-10T11:33:17.916186: step 6629, loss 0.0123364, acc 1, learning_rate 0.0001
2017-10-10T11:33:18.081198: step 6630, loss 0.054604, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:18.245691: step 6631, loss 0.10691, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:18.405805: step 6632, loss 0.0380439, acc 1, learning_rate 0.0001
2017-10-10T11:33:18.570331: step 6633, loss 0.0601959, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:18.736510: step 6634, loss 0.217315, acc 0.9375, learning_rate 0.0001
2017-10-10T11:33:18.911773: step 6635, loss 0.107151, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:19.077750: step 6636, loss 0.1585, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:19.238981: step 6637, loss 0.0681363, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:19.402919: step 6638, loss 0.0651629, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:19.565060: step 6639, loss 0.0213328, acc 1, learning_rate 0.0001
2017-10-10T11:33:19.727523: step 6640, loss 0.0213738, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:33:20.160430: step 6640, loss 0.197353, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6640

2017-10-10T11:33:20.730180: step 6641, loss 0.0684894, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:20.897363: step 6642, loss 0.0921819, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:21.062734: step 6643, loss 0.092274, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:21.223695: step 6644, loss 0.035246, acc 1, learning_rate 0.0001
2017-10-10T11:33:21.383582: step 6645, loss 0.0420986, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:21.546462: step 6646, loss 0.0493193, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:21.710564: step 6647, loss 0.101889, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:21.874232: step 6648, loss 0.0268949, acc 1, learning_rate 0.0001
2017-10-10T11:33:22.036231: step 6649, loss 0.105877, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:22.199830: step 6650, loss 0.0494798, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:22.363595: step 6651, loss 0.0957276, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:22.526791: step 6652, loss 0.0451856, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:22.692040: step 6653, loss 0.082604, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:22.855040: step 6654, loss 0.0593407, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:23.016176: step 6655, loss 0.0271416, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:23.176791: step 6656, loss 0.0664403, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:23.340452: step 6657, loss 0.0546043, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:23.504003: step 6658, loss 0.0233401, acc 1, learning_rate 0.0001
2017-10-10T11:33:23.665917: step 6659, loss 0.0185624, acc 1, learning_rate 0.0001
2017-10-10T11:33:23.828574: step 6660, loss 0.0828474, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:23.994429: step 6661, loss 0.0659561, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:24.154950: step 6662, loss 0.00893019, acc 1, learning_rate 0.0001
2017-10-10T11:33:24.318046: step 6663, loss 0.0276743, acc 1, learning_rate 0.0001
2017-10-10T11:33:24.451891: step 6664, loss 0.0704192, acc 0.980392, learning_rate 0.0001
2017-10-10T11:33:24.618889: step 6665, loss 0.0560961, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:24.779105: step 6666, loss 0.0809775, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:24.948749: step 6667, loss 0.049954, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:25.112057: step 6668, loss 0.0303718, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:25.272484: step 6669, loss 0.0566784, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:25.437629: step 6670, loss 0.0696292, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:25.600608: step 6671, loss 0.0508706, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:25.764277: step 6672, loss 0.0248661, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:25.930636: step 6673, loss 0.0116784, acc 1, learning_rate 0.0001
2017-10-10T11:33:26.091348: step 6674, loss 0.048772, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:26.254379: step 6675, loss 0.0649344, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:26.416308: step 6676, loss 0.06742, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:26.579199: step 6677, loss 0.0539718, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:26.741479: step 6678, loss 0.046692, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:26.912554: step 6679, loss 0.0411953, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:27.074672: step 6680, loss 0.0469825, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:33:27.478627: step 6680, loss 0.202103, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6680

2017-10-10T11:33:28.118648: step 6681, loss 0.112295, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:28.280092: step 6682, loss 0.085152, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:28.444938: step 6683, loss 0.125222, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:28.608602: step 6684, loss 0.120008, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:28.771573: step 6685, loss 0.0794125, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:28.937258: step 6686, loss 0.127159, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:29.103672: step 6687, loss 0.109627, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:29.265691: step 6688, loss 0.0420171, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:29.429805: step 6689, loss 0.0197043, acc 1, learning_rate 0.0001
2017-10-10T11:33:29.594616: step 6690, loss 0.0498617, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:29.759617: step 6691, loss 0.0401716, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:29.927772: step 6692, loss 0.230593, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:30.092852: step 6693, loss 0.0200624, acc 1, learning_rate 0.0001
2017-10-10T11:33:30.256174: step 6694, loss 0.0191442, acc 1, learning_rate 0.0001
2017-10-10T11:33:30.419653: step 6695, loss 0.0260433, acc 1, learning_rate 0.0001
2017-10-10T11:33:30.582743: step 6696, loss 0.0536715, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:30.745506: step 6697, loss 0.0547539, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:30.909939: step 6698, loss 0.0238226, acc 1, learning_rate 0.0001
2017-10-10T11:33:31.073287: step 6699, loss 0.0475955, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:31.237458: step 6700, loss 0.0399152, acc 1, learning_rate 0.0001
2017-10-10T11:33:31.398809: step 6701, loss 0.0648102, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:31.562170: step 6702, loss 0.0550492, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:31.724844: step 6703, loss 0.0356488, acc 1, learning_rate 0.0001
2017-10-10T11:33:31.886788: step 6704, loss 0.152676, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:32.049184: step 6705, loss 0.0490798, acc 1, learning_rate 0.0001
2017-10-10T11:33:32.208942: step 6706, loss 0.0374228, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:32.371597: step 6707, loss 0.011171, acc 1, learning_rate 0.0001
2017-10-10T11:33:32.532813: step 6708, loss 0.0492465, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:32.696635: step 6709, loss 0.128642, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:32.859984: step 6710, loss 0.0170573, acc 1, learning_rate 0.0001
2017-10-10T11:33:33.018564: step 6711, loss 0.0290479, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:33.180957: step 6712, loss 0.0254182, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:33.339512: step 6713, loss 0.0198316, acc 1, learning_rate 0.0001
2017-10-10T11:33:33.504358: step 6714, loss 0.0887485, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:33.668325: step 6715, loss 0.0545965, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:33.829580: step 6716, loss 0.019203, acc 1, learning_rate 0.0001
2017-10-10T11:33:33.990602: step 6717, loss 0.112498, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:34.151695: step 6718, loss 0.0256112, acc 1, learning_rate 0.0001
2017-10-10T11:33:34.312692: step 6719, loss 0.0199393, acc 1, learning_rate 0.0001
2017-10-10T11:33:34.476474: step 6720, loss 0.0596269, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:33:34.906824: step 6720, loss 0.200245, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6720

2017-10-10T11:33:35.622770: step 6721, loss 0.0270107, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:35.785121: step 6722, loss 0.0364427, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:35.955675: step 6723, loss 0.0295349, acc 1, learning_rate 0.0001
2017-10-10T11:33:36.118886: step 6724, loss 0.0636395, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:36.281707: step 6725, loss 0.0470005, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:36.448008: step 6726, loss 0.0344177, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:36.611106: step 6727, loss 0.0341323, acc 1, learning_rate 0.0001
2017-10-10T11:33:36.773260: step 6728, loss 0.0697286, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:36.944393: step 6729, loss 0.0189117, acc 1, learning_rate 0.0001
2017-10-10T11:33:37.104825: step 6730, loss 0.0336062, acc 1, learning_rate 0.0001
2017-10-10T11:33:37.269547: step 6731, loss 0.0688632, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:37.431032: step 6732, loss 0.0708893, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:37.593128: step 6733, loss 0.0634574, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:37.759887: step 6734, loss 0.0434675, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:37.923672: step 6735, loss 0.108068, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:38.103724: step 6736, loss 0.0733055, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:38.264295: step 6737, loss 0.111681, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:38.430041: step 6738, loss 0.0328359, acc 1, learning_rate 0.0001
2017-10-10T11:33:38.593438: step 6739, loss 0.0420384, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:38.751191: step 6740, loss 0.0668305, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:38.914165: step 6741, loss 0.0243077, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:39.080024: step 6742, loss 0.0678957, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:39.242975: step 6743, loss 0.0612181, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:39.405396: step 6744, loss 0.0757829, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:39.567901: step 6745, loss 0.01994, acc 1, learning_rate 0.0001
2017-10-10T11:33:39.730078: step 6746, loss 0.0475371, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:39.892574: step 6747, loss 0.101486, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:40.056803: step 6748, loss 0.0517491, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:40.219365: step 6749, loss 0.0935842, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:40.379772: step 6750, loss 0.0629161, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:40.540502: step 6751, loss 0.0934438, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:40.702404: step 6752, loss 0.0319961, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:40.864903: step 6753, loss 0.0236567, acc 1, learning_rate 0.0001
2017-10-10T11:33:41.027169: step 6754, loss 0.0250811, acc 1, learning_rate 0.0001
2017-10-10T11:33:41.191681: step 6755, loss 0.010107, acc 1, learning_rate 0.0001
2017-10-10T11:33:41.352578: step 6756, loss 0.0482057, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:41.516558: step 6757, loss 0.0265149, acc 1, learning_rate 0.0001
2017-10-10T11:33:41.680114: step 6758, loss 0.0362197, acc 1, learning_rate 0.0001
2017-10-10T11:33:41.845181: step 6759, loss 0.165993, acc 0.921875, learning_rate 0.0001
2017-10-10T11:33:42.008814: step 6760, loss 0.0949259, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:33:42.441098: step 6760, loss 0.198211, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6760

2017-10-10T11:33:43.016357: step 6761, loss 0.0267941, acc 1, learning_rate 0.0001
2017-10-10T11:33:43.153346: step 6762, loss 0.0238448, acc 1, learning_rate 0.0001
2017-10-10T11:33:43.316852: step 6763, loss 0.039997, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:43.480752: step 6764, loss 0.0811355, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:43.642768: step 6765, loss 0.085982, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:43.804590: step 6766, loss 0.0692779, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:43.969249: step 6767, loss 0.0751986, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:44.131063: step 6768, loss 0.0886878, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:44.294535: step 6769, loss 0.0640711, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:44.453374: step 6770, loss 0.0408233, acc 1, learning_rate 0.0001
2017-10-10T11:33:44.615331: step 6771, loss 0.0252359, acc 1, learning_rate 0.0001
2017-10-10T11:33:44.778171: step 6772, loss 0.11567, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:44.942144: step 6773, loss 0.0479392, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:45.103733: step 6774, loss 0.0610164, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:45.264554: step 6775, loss 0.0251996, acc 1, learning_rate 0.0001
2017-10-10T11:33:45.423383: step 6776, loss 0.0504668, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:45.586268: step 6777, loss 0.057675, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:45.752027: step 6778, loss 0.0171653, acc 1, learning_rate 0.0001
2017-10-10T11:33:46.000148: step 6779, loss 0.0146385, acc 1, learning_rate 0.0001
2017-10-10T11:33:46.165542: step 6780, loss 0.110202, acc 0.9375, learning_rate 0.0001
2017-10-10T11:33:46.336041: step 6781, loss 0.0231528, acc 1, learning_rate 0.0001
2017-10-10T11:33:46.498543: step 6782, loss 0.0882349, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:46.657413: step 6783, loss 0.0708895, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:46.817744: step 6784, loss 0.0166948, acc 1, learning_rate 0.0001
2017-10-10T11:33:46.985432: step 6785, loss 0.0628722, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:47.147216: step 6786, loss 0.0343044, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:47.313116: step 6787, loss 0.0276575, acc 1, learning_rate 0.0001
2017-10-10T11:33:47.474453: step 6788, loss 0.0158019, acc 1, learning_rate 0.0001
2017-10-10T11:33:47.637597: step 6789, loss 0.0478416, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:47.798963: step 6790, loss 0.0446642, acc 1, learning_rate 0.0001
2017-10-10T11:33:47.972693: step 6791, loss 0.0344792, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:48.134278: step 6792, loss 0.135179, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:48.297814: step 6793, loss 0.041761, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:48.459764: step 6794, loss 0.0621376, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:48.621402: step 6795, loss 0.0887546, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:48.785589: step 6796, loss 0.0256946, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:48.948773: step 6797, loss 0.0405546, acc 1, learning_rate 0.0001
2017-10-10T11:33:49.112727: step 6798, loss 0.0516631, acc 1, learning_rate 0.0001
2017-10-10T11:33:49.274176: step 6799, loss 0.031533, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:49.438006: step 6800, loss 0.109315, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:33:49.831214: step 6800, loss 0.205861, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6800

2017-10-10T11:33:50.468618: step 6801, loss 0.0307855, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:50.633052: step 6802, loss 0.0792138, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:50.794872: step 6803, loss 0.0571223, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:50.955737: step 6804, loss 0.0794979, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:51.117419: step 6805, loss 0.0165321, acc 1, learning_rate 0.0001
2017-10-10T11:33:51.278702: step 6806, loss 0.033336, acc 1, learning_rate 0.0001
2017-10-10T11:33:51.439685: step 6807, loss 0.0297251, acc 1, learning_rate 0.0001
2017-10-10T11:33:51.603200: step 6808, loss 0.0890879, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:51.764265: step 6809, loss 0.0357437, acc 1, learning_rate 0.0001
2017-10-10T11:33:51.930388: step 6810, loss 0.0395147, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:52.093352: step 6811, loss 0.0261912, acc 1, learning_rate 0.0001
2017-10-10T11:33:52.257104: step 6812, loss 0.0509909, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:52.417316: step 6813, loss 0.0966554, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:52.581045: step 6814, loss 0.0554169, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:52.745571: step 6815, loss 0.0365256, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:52.915697: step 6816, loss 0.0672759, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:53.080177: step 6817, loss 0.0317863, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:53.248270: step 6818, loss 0.0706369, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:53.409424: step 6819, loss 0.0728889, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:53.575222: step 6820, loss 0.0172678, acc 1, learning_rate 0.0001
2017-10-10T11:33:53.738414: step 6821, loss 0.0503066, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:53.901484: step 6822, loss 0.0525604, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:54.063192: step 6823, loss 0.0416675, acc 1, learning_rate 0.0001
2017-10-10T11:33:54.226963: step 6824, loss 0.0381186, acc 1, learning_rate 0.0001
2017-10-10T11:33:54.387053: step 6825, loss 0.0678528, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:54.547884: step 6826, loss 0.0872721, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:54.711965: step 6827, loss 0.0882583, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:54.876813: step 6828, loss 0.0739751, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:55.038567: step 6829, loss 0.0302003, acc 1, learning_rate 0.0001
2017-10-10T11:33:55.201599: step 6830, loss 0.0166109, acc 1, learning_rate 0.0001
2017-10-10T11:33:55.364833: step 6831, loss 0.0225903, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:55.529141: step 6832, loss 0.115949, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:55.692920: step 6833, loss 0.0744999, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:55.856757: step 6834, loss 0.156131, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:56.027858: step 6835, loss 0.0708416, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:56.189700: step 6836, loss 0.0167773, acc 1, learning_rate 0.0001
2017-10-10T11:33:56.351281: step 6837, loss 0.0926418, acc 0.9375, learning_rate 0.0001
2017-10-10T11:33:56.515140: step 6838, loss 0.0575833, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:56.679254: step 6839, loss 0.0743967, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:56.838854: step 6840, loss 0.0572187, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:33:57.253017: step 6840, loss 0.199552, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6840

2017-10-10T11:33:57.974754: step 6841, loss 0.0912433, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:58.138967: step 6842, loss 0.0328439, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:58.301657: step 6843, loss 0.0310084, acc 1, learning_rate 0.0001
2017-10-10T11:33:58.461917: step 6844, loss 0.0364992, acc 1, learning_rate 0.0001
2017-10-10T11:33:58.624034: step 6845, loss 0.127916, acc 0.9375, learning_rate 0.0001
2017-10-10T11:33:58.786947: step 6846, loss 0.0739558, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:58.956165: step 6847, loss 0.0313624, acc 0.984375, learning_rate 0.0001
2017-10-10T11:33:59.119439: step 6848, loss 0.0661493, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:59.281264: step 6849, loss 0.0960311, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:59.444237: step 6850, loss 0.127644, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:59.606762: step 6851, loss 0.0918707, acc 0.953125, learning_rate 0.0001
2017-10-10T11:33:59.766806: step 6852, loss 0.0460037, acc 0.96875, learning_rate 0.0001
2017-10-10T11:33:59.937046: step 6853, loss 0.0792097, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:00.102817: step 6854, loss 0.102506, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:00.263062: step 6855, loss 0.0562054, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:00.426092: step 6856, loss 0.0778823, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:00.588356: step 6857, loss 0.0614463, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:00.753245: step 6858, loss 0.0802666, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:00.924132: step 6859, loss 0.0337638, acc 1, learning_rate 0.0001
2017-10-10T11:34:01.061645: step 6860, loss 0.0574617, acc 0.980392, learning_rate 0.0001
2017-10-10T11:34:01.223608: step 6861, loss 0.0743859, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:01.384675: step 6862, loss 0.122304, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:01.549912: step 6863, loss 0.0427212, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:01.713583: step 6864, loss 0.069872, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:01.878012: step 6865, loss 0.0449817, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:02.036805: step 6866, loss 0.0223156, acc 1, learning_rate 0.0001
2017-10-10T11:34:02.198426: step 6867, loss 0.0599114, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:02.360674: step 6868, loss 0.0595351, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:02.524579: step 6869, loss 0.0301215, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:02.686196: step 6870, loss 0.0287255, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:02.847241: step 6871, loss 0.0999823, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:03.012666: step 6872, loss 0.110978, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:03.176469: step 6873, loss 0.0567725, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:03.340774: step 6874, loss 0.0323062, acc 1, learning_rate 0.0001
2017-10-10T11:34:03.500778: step 6875, loss 0.0952471, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:03.666008: step 6876, loss 0.0670119, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:03.829815: step 6877, loss 0.0589676, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:03.997131: step 6878, loss 0.067597, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:04.160371: step 6879, loss 0.0651165, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:04.321291: step 6880, loss 0.113648, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:34:04.727884: step 6880, loss 0.199813, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6880

2017-10-10T11:34:05.300733: step 6881, loss 0.0730106, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:05.460167: step 6882, loss 0.0468768, acc 1, learning_rate 0.0001
2017-10-10T11:34:05.618772: step 6883, loss 0.0998632, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:05.778795: step 6884, loss 0.0873394, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:05.939406: step 6885, loss 0.123204, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:06.102823: step 6886, loss 0.0659172, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:06.262875: step 6887, loss 0.0847827, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:06.427486: step 6888, loss 0.144419, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:06.592058: step 6889, loss 0.0358131, acc 1, learning_rate 0.0001
2017-10-10T11:34:06.752265: step 6890, loss 0.0778778, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:06.917033: step 6891, loss 0.0302827, acc 1, learning_rate 0.0001
2017-10-10T11:34:07.080375: step 6892, loss 0.0135918, acc 1, learning_rate 0.0001
2017-10-10T11:34:07.239596: step 6893, loss 0.021226, acc 1, learning_rate 0.0001
2017-10-10T11:34:07.402230: step 6894, loss 0.0626826, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:07.566125: step 6895, loss 0.019052, acc 1, learning_rate 0.0001
2017-10-10T11:34:07.728685: step 6896, loss 0.0320379, acc 1, learning_rate 0.0001
2017-10-10T11:34:07.890588: step 6897, loss 0.100799, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:08.055274: step 6898, loss 0.0239856, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:08.218345: step 6899, loss 0.036908, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:08.378828: step 6900, loss 0.0534112, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:08.541731: step 6901, loss 0.0525791, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:08.707055: step 6902, loss 0.0523551, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:08.872821: step 6903, loss 0.0394872, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:09.034177: step 6904, loss 0.0306768, acc 1, learning_rate 0.0001
2017-10-10T11:34:09.195379: step 6905, loss 0.0530489, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:09.354261: step 6906, loss 0.0389909, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:09.517327: step 6907, loss 0.0562685, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:09.680513: step 6908, loss 0.0257134, acc 1, learning_rate 0.0001
2017-10-10T11:34:09.841736: step 6909, loss 0.0386233, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:10.006451: step 6910, loss 0.0560737, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:10.170723: step 6911, loss 0.0120739, acc 1, learning_rate 0.0001
2017-10-10T11:34:10.332312: step 6912, loss 0.0454429, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:10.495640: step 6913, loss 0.0575386, acc 1, learning_rate 0.0001
2017-10-10T11:34:10.663513: step 6914, loss 0.0294273, acc 1, learning_rate 0.0001
2017-10-10T11:34:10.828323: step 6915, loss 0.0451401, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:10.991562: step 6916, loss 0.0735912, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:11.150258: step 6917, loss 0.0454031, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:11.312863: step 6918, loss 0.0391041, acc 1, learning_rate 0.0001
2017-10-10T11:34:11.474962: step 6919, loss 0.0173686, acc 1, learning_rate 0.0001
2017-10-10T11:34:11.638281: step 6920, loss 0.0250271, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:34:12.060124: step 6920, loss 0.204875, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6920

2017-10-10T11:34:12.694059: step 6921, loss 0.0141846, acc 1, learning_rate 0.0001
2017-10-10T11:34:12.858344: step 6922, loss 0.0407266, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:13.020499: step 6923, loss 0.0801848, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:13.182487: step 6924, loss 0.044327, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:13.344907: step 6925, loss 0.151563, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:13.509234: step 6926, loss 0.0482016, acc 1, learning_rate 0.0001
2017-10-10T11:34:13.673544: step 6927, loss 0.0290027, acc 1, learning_rate 0.0001
2017-10-10T11:34:13.838061: step 6928, loss 0.0549987, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:14.000007: step 6929, loss 0.0129653, acc 1, learning_rate 0.0001
2017-10-10T11:34:14.164458: step 6930, loss 0.0459848, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:14.327407: step 6931, loss 0.0437693, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:14.488762: step 6932, loss 0.0609639, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:14.650944: step 6933, loss 0.0959168, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:14.816766: step 6934, loss 0.111671, acc 0.9375, learning_rate 0.0001
2017-10-10T11:34:14.977383: step 6935, loss 0.0375558, acc 1, learning_rate 0.0001
2017-10-10T11:34:15.143922: step 6936, loss 0.0723474, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:15.307130: step 6937, loss 0.0757214, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:15.467607: step 6938, loss 0.058906, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:15.635137: step 6939, loss 0.0295176, acc 1, learning_rate 0.0001
2017-10-10T11:34:15.796238: step 6940, loss 0.0602315, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:15.971291: step 6941, loss 0.046169, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:16.135379: step 6942, loss 0.0431147, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:16.300524: step 6943, loss 0.150811, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:16.461139: step 6944, loss 0.0411646, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:16.624020: step 6945, loss 0.0738422, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:16.785232: step 6946, loss 0.0272155, acc 1, learning_rate 0.0001
2017-10-10T11:34:16.956375: step 6947, loss 0.0871874, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:17.117521: step 6948, loss 0.0330949, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:17.282417: step 6949, loss 0.0656729, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:17.443440: step 6950, loss 0.0364425, acc 1, learning_rate 0.0001
2017-10-10T11:34:17.604814: step 6951, loss 0.0592692, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:17.773089: step 6952, loss 0.0350169, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:17.936617: step 6953, loss 0.082217, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:18.105087: step 6954, loss 0.0324152, acc 1, learning_rate 0.0001
2017-10-10T11:34:18.269283: step 6955, loss 0.081959, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:18.433291: step 6956, loss 0.0137363, acc 1, learning_rate 0.0001
2017-10-10T11:34:18.595065: step 6957, loss 0.0556437, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:18.727767: step 6958, loss 0.0295194, acc 1, learning_rate 0.0001
2017-10-10T11:34:18.891141: step 6959, loss 0.028409, acc 1, learning_rate 0.0001
2017-10-10T11:34:19.049286: step 6960, loss 0.164229, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T11:34:19.469398: step 6960, loss 0.204014, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-6960

2017-10-10T11:34:20.184001: step 6961, loss 0.0152395, acc 1, learning_rate 0.0001
2017-10-10T11:34:20.347291: step 6962, loss 0.0440725, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:20.509022: step 6963, loss 0.0429166, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:20.670747: step 6964, loss 0.127283, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:20.832575: step 6965, loss 0.0706375, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:21.000811: step 6966, loss 0.0737333, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:21.162114: step 6967, loss 0.0572312, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:21.323959: step 6968, loss 0.00811953, acc 1, learning_rate 0.0001
2017-10-10T11:34:21.485635: step 6969, loss 0.112045, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:21.651349: step 6970, loss 0.0651546, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:21.817299: step 6971, loss 0.0614429, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:21.983512: step 6972, loss 0.0746766, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:22.149226: step 6973, loss 0.0515748, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:22.308158: step 6974, loss 0.0127919, acc 1, learning_rate 0.0001
2017-10-10T11:34:22.471884: step 6975, loss 0.0483017, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:22.633465: step 6976, loss 0.0391159, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:22.793143: step 6977, loss 0.0614581, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:22.953826: step 6978, loss 0.028971, acc 1, learning_rate 0.0001
2017-10-10T11:34:23.115341: step 6979, loss 0.121463, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:23.276589: step 6980, loss 0.0823282, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:23.437998: step 6981, loss 0.056345, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:23.601580: step 6982, loss 0.0418954, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:23.762206: step 6983, loss 0.021602, acc 1, learning_rate 0.0001
2017-10-10T11:34:23.924926: step 6984, loss 0.0439809, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:24.085006: step 6985, loss 0.0907818, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:24.246413: step 6986, loss 0.0384592, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:24.410914: step 6987, loss 0.0195013, acc 1, learning_rate 0.0001
2017-10-10T11:34:24.575173: step 6988, loss 0.0230271, acc 1, learning_rate 0.0001
2017-10-10T11:34:24.737458: step 6989, loss 0.0945766, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:24.908391: step 6990, loss 0.0791101, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:25.077839: step 6991, loss 0.0195732, acc 1, learning_rate 0.0001
2017-10-10T11:34:25.242956: step 6992, loss 0.045839, acc 1, learning_rate 0.0001
2017-10-10T11:34:25.407890: step 6993, loss 0.0487508, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:25.572040: step 6994, loss 0.0634322, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:25.736924: step 6995, loss 0.0446675, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:25.902998: step 6996, loss 0.0449291, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:26.068130: step 6997, loss 0.072053, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:26.231715: step 6998, loss 0.0565944, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:26.397292: step 6999, loss 0.0664632, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:26.561505: step 7000, loss 0.117907, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:34:27.000949: step 7000, loss 0.206044, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7000

2017-10-10T11:34:27.583704: step 7001, loss 0.0262026, acc 1, learning_rate 0.0001
2017-10-10T11:34:27.746773: step 7002, loss 0.0103388, acc 1, learning_rate 0.0001
2017-10-10T11:34:27.920405: step 7003, loss 0.0759878, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:28.086211: step 7004, loss 0.112272, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:28.256743: step 7005, loss 0.0217326, acc 1, learning_rate 0.0001
2017-10-10T11:34:28.421155: step 7006, loss 0.0200339, acc 1, learning_rate 0.0001
2017-10-10T11:34:28.582041: step 7007, loss 0.0143832, acc 1, learning_rate 0.0001
2017-10-10T11:34:28.744214: step 7008, loss 0.0991524, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:28.915183: step 7009, loss 0.106283, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:29.082995: step 7010, loss 0.0435658, acc 1, learning_rate 0.0001
2017-10-10T11:34:29.248177: step 7011, loss 0.0472978, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:29.413706: step 7012, loss 0.0444753, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:29.578080: step 7013, loss 0.0394839, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:29.743374: step 7014, loss 0.13883, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:29.910613: step 7015, loss 0.061331, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:30.078584: step 7016, loss 0.0586272, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:30.240290: step 7017, loss 0.046774, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:30.399716: step 7018, loss 0.0892074, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:30.563161: step 7019, loss 0.0481622, acc 1, learning_rate 0.0001
2017-10-10T11:34:30.726009: step 7020, loss 0.0325419, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:30.889493: step 7021, loss 0.0490374, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:31.052553: step 7022, loss 0.024753, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:31.213105: step 7023, loss 0.0290132, acc 1, learning_rate 0.0001
2017-10-10T11:34:31.372934: step 7024, loss 0.0500644, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:31.534828: step 7025, loss 0.0293528, acc 1, learning_rate 0.0001
2017-10-10T11:34:31.699526: step 7026, loss 0.0281341, acc 1, learning_rate 0.0001
2017-10-10T11:34:31.861651: step 7027, loss 0.118993, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:32.022339: step 7028, loss 0.0974491, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:32.186119: step 7029, loss 0.0642297, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:32.347136: step 7030, loss 0.0700427, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:32.511826: step 7031, loss 0.0672324, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:32.675158: step 7032, loss 0.0140107, acc 1, learning_rate 0.0001
2017-10-10T11:34:32.838279: step 7033, loss 0.0642801, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:33.001571: step 7034, loss 0.0605793, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:33.163752: step 7035, loss 0.0829007, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:33.326676: step 7036, loss 0.0228447, acc 1, learning_rate 0.0001
2017-10-10T11:34:33.495848: step 7037, loss 0.020277, acc 1, learning_rate 0.0001
2017-10-10T11:34:33.658555: step 7038, loss 0.049699, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:33.822340: step 7039, loss 0.0686551, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:33.989481: step 7040, loss 0.039541, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:34:34.410560: step 7040, loss 0.202045, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7040

2017-10-10T11:34:35.052481: step 7041, loss 0.030128, acc 1, learning_rate 0.0001
2017-10-10T11:34:35.216988: step 7042, loss 0.0457618, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:35.387819: step 7043, loss 0.0801752, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:35.552872: step 7044, loss 0.104541, acc 0.9375, learning_rate 0.0001
2017-10-10T11:34:35.716926: step 7045, loss 0.0776909, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:35.886766: step 7046, loss 0.0741668, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:36.053821: step 7047, loss 0.0412629, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:36.213493: step 7048, loss 0.0507343, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:36.376310: step 7049, loss 0.020515, acc 1, learning_rate 0.0001
2017-10-10T11:34:36.534707: step 7050, loss 0.0232731, acc 1, learning_rate 0.0001
2017-10-10T11:34:36.699374: step 7051, loss 0.096728, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:36.864718: step 7052, loss 0.012094, acc 1, learning_rate 0.0001
2017-10-10T11:34:37.029161: step 7053, loss 0.0190778, acc 1, learning_rate 0.0001
2017-10-10T11:34:37.207464: step 7054, loss 0.15152, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:37.380148: step 7055, loss 0.154852, acc 0.9375, learning_rate 0.0001
2017-10-10T11:34:37.524202: step 7056, loss 0.11265, acc 0.941176, learning_rate 0.0001
2017-10-10T11:34:37.720408: step 7057, loss 0.00944772, acc 1, learning_rate 0.0001
2017-10-10T11:34:37.881604: step 7058, loss 0.0410288, acc 1, learning_rate 0.0001
2017-10-10T11:34:38.046903: step 7059, loss 0.100075, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:38.209209: step 7060, loss 0.0490076, acc 1, learning_rate 0.0001
2017-10-10T11:34:38.373202: step 7061, loss 0.0516519, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:38.536077: step 7062, loss 0.0430232, acc 1, learning_rate 0.0001
2017-10-10T11:34:38.698341: step 7063, loss 0.0344827, acc 1, learning_rate 0.0001
2017-10-10T11:34:38.856501: step 7064, loss 0.0246773, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:39.019294: step 7065, loss 0.028143, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:39.183987: step 7066, loss 0.0531892, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:39.350653: step 7067, loss 0.110303, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:39.515013: step 7068, loss 0.0475424, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:39.680422: step 7069, loss 0.0196451, acc 1, learning_rate 0.0001
2017-10-10T11:34:39.843400: step 7070, loss 0.0779529, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:40.006982: step 7071, loss 0.0463803, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:40.171167: step 7072, loss 0.0268161, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:40.335106: step 7073, loss 0.0233283, acc 1, learning_rate 0.0001
2017-10-10T11:34:40.497008: step 7074, loss 0.0352922, acc 1, learning_rate 0.0001
2017-10-10T11:34:40.663266: step 7075, loss 0.10929, acc 0.9375, learning_rate 0.0001
2017-10-10T11:34:40.831461: step 7076, loss 0.0340637, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:40.996032: step 7077, loss 0.0234308, acc 1, learning_rate 0.0001
2017-10-10T11:34:41.162075: step 7078, loss 0.131208, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:41.324284: step 7079, loss 0.0689926, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:41.486363: step 7080, loss 0.127315, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:34:41.906411: step 7080, loss 0.200874, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7080

2017-10-10T11:34:42.623796: step 7081, loss 0.0458112, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:42.790121: step 7082, loss 0.0282168, acc 1, learning_rate 0.0001
2017-10-10T11:34:42.950886: step 7083, loss 0.0265979, acc 1, learning_rate 0.0001
2017-10-10T11:34:43.113431: step 7084, loss 0.0477183, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:43.275735: step 7085, loss 0.0369008, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:43.439494: step 7086, loss 0.0217156, acc 1, learning_rate 0.0001
2017-10-10T11:34:43.605367: step 7087, loss 0.0572186, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:43.772178: step 7088, loss 0.0478317, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:43.937189: step 7089, loss 0.0760572, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:44.102240: step 7090, loss 0.0591357, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:44.263171: step 7091, loss 0.0753352, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:44.427471: step 7092, loss 0.0984023, acc 0.9375, learning_rate 0.0001
2017-10-10T11:34:44.595257: step 7093, loss 0.0418099, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:44.759110: step 7094, loss 0.0170771, acc 1, learning_rate 0.0001
2017-10-10T11:34:44.929874: step 7095, loss 0.0413612, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:45.091838: step 7096, loss 0.0609022, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:45.254488: step 7097, loss 0.0459164, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:45.418561: step 7098, loss 0.0428334, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:45.578283: step 7099, loss 0.0358798, acc 1, learning_rate 0.0001
2017-10-10T11:34:45.739275: step 7100, loss 0.0364798, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:45.910505: step 7101, loss 0.0605648, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:46.081051: step 7102, loss 0.0579535, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:46.246136: step 7103, loss 0.0937544, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:46.408500: step 7104, loss 0.0548669, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:46.575739: step 7105, loss 0.0750619, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:46.741641: step 7106, loss 0.060192, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:46.905275: step 7107, loss 0.0346005, acc 1, learning_rate 0.0001
2017-10-10T11:34:47.068407: step 7108, loss 0.0283516, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:47.228845: step 7109, loss 0.0798531, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:47.394283: step 7110, loss 0.030969, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:47.560253: step 7111, loss 0.0637686, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:47.723711: step 7112, loss 0.0354791, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:47.890268: step 7113, loss 0.074242, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:48.054443: step 7114, loss 0.17191, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:48.220801: step 7115, loss 0.0969397, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:48.382285: step 7116, loss 0.0130692, acc 1, learning_rate 0.0001
2017-10-10T11:34:48.548228: step 7117, loss 0.0113039, acc 1, learning_rate 0.0001
2017-10-10T11:34:48.709863: step 7118, loss 0.0338023, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:48.871294: step 7119, loss 0.0461823, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:49.033175: step 7120, loss 0.0915952, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:34:49.460010: step 7120, loss 0.199012, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7120

2017-10-10T11:34:50.046433: step 7121, loss 0.0522438, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:50.211266: step 7122, loss 0.0642436, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:50.377057: step 7123, loss 0.0953108, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:50.540532: step 7124, loss 0.014706, acc 1, learning_rate 0.0001
2017-10-10T11:34:50.704975: step 7125, loss 0.0398252, acc 1, learning_rate 0.0001
2017-10-10T11:34:50.875562: step 7126, loss 0.0535599, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:51.039805: step 7127, loss 0.0542446, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:51.203618: step 7128, loss 0.0482953, acc 1, learning_rate 0.0001
2017-10-10T11:34:51.370650: step 7129, loss 0.0300428, acc 1, learning_rate 0.0001
2017-10-10T11:34:51.533133: step 7130, loss 0.0823696, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:51.696862: step 7131, loss 0.0209513, acc 1, learning_rate 0.0001
2017-10-10T11:34:51.863281: step 7132, loss 0.0474872, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:52.026720: step 7133, loss 0.0142999, acc 1, learning_rate 0.0001
2017-10-10T11:34:52.188348: step 7134, loss 0.0347432, acc 1, learning_rate 0.0001
2017-10-10T11:34:52.349944: step 7135, loss 0.0702086, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:52.513618: step 7136, loss 0.0400594, acc 1, learning_rate 0.0001
2017-10-10T11:34:52.676909: step 7137, loss 0.0234993, acc 1, learning_rate 0.0001
2017-10-10T11:34:52.838342: step 7138, loss 0.0254312, acc 1, learning_rate 0.0001
2017-10-10T11:34:53.007783: step 7139, loss 0.101059, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:53.168832: step 7140, loss 0.0470032, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:53.330793: step 7141, loss 0.0398515, acc 1, learning_rate 0.0001
2017-10-10T11:34:53.493622: step 7142, loss 0.0535826, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:53.656202: step 7143, loss 0.067615, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:53.816815: step 7144, loss 0.0297969, acc 1, learning_rate 0.0001
2017-10-10T11:34:53.988892: step 7145, loss 0.0426223, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:54.152740: step 7146, loss 0.0904296, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:54.314634: step 7147, loss 0.104632, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:54.477485: step 7148, loss 0.0711814, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:54.639683: step 7149, loss 0.131575, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:54.798408: step 7150, loss 0.0412843, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:54.973439: step 7151, loss 0.0983014, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:55.135826: step 7152, loss 0.179074, acc 0.921875, learning_rate 0.0001
2017-10-10T11:34:55.301522: step 7153, loss 0.0307576, acc 1, learning_rate 0.0001
2017-10-10T11:34:55.436987: step 7154, loss 0.0995163, acc 0.980392, learning_rate 0.0001
2017-10-10T11:34:55.609131: step 7155, loss 0.0512682, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:55.768984: step 7156, loss 0.123088, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:55.938747: step 7157, loss 0.0255282, acc 1, learning_rate 0.0001
2017-10-10T11:34:56.101126: step 7158, loss 0.0655051, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:56.263012: step 7159, loss 0.0888868, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:56.426097: step 7160, loss 0.0902469, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:34:56.852381: step 7160, loss 0.201345, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7160

2017-10-10T11:34:57.496103: step 7161, loss 0.0162385, acc 1, learning_rate 0.0001
2017-10-10T11:34:57.657206: step 7162, loss 0.0339347, acc 1, learning_rate 0.0001
2017-10-10T11:34:57.816441: step 7163, loss 0.0312624, acc 1, learning_rate 0.0001
2017-10-10T11:34:57.985031: step 7164, loss 0.0537077, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:58.145317: step 7165, loss 0.0318607, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:58.310965: step 7166, loss 0.10715, acc 0.953125, learning_rate 0.0001
2017-10-10T11:34:58.474841: step 7167, loss 0.0666795, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:58.635311: step 7168, loss 0.0841638, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:58.799523: step 7169, loss 0.058414, acc 0.96875, learning_rate 0.0001
2017-10-10T11:34:58.966440: step 7170, loss 0.0261004, acc 1, learning_rate 0.0001
2017-10-10T11:34:59.126252: step 7171, loss 0.0503345, acc 0.984375, learning_rate 0.0001
2017-10-10T11:34:59.284670: step 7172, loss 0.0333126, acc 1, learning_rate 0.0001
2017-10-10T11:34:59.447990: step 7173, loss 0.029704, acc 1, learning_rate 0.0001
2017-10-10T11:34:59.609151: step 7174, loss 0.0393326, acc 1, learning_rate 0.0001
2017-10-10T11:34:59.768831: step 7175, loss 0.0168394, acc 1, learning_rate 0.0001
2017-10-10T11:34:59.934968: step 7176, loss 0.0329639, acc 1, learning_rate 0.0001
2017-10-10T11:35:00.100819: step 7177, loss 0.0138859, acc 1, learning_rate 0.0001
2017-10-10T11:35:00.263497: step 7178, loss 0.0179456, acc 1, learning_rate 0.0001
2017-10-10T11:35:00.426660: step 7179, loss 0.0658774, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:00.586707: step 7180, loss 0.102451, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:00.755430: step 7181, loss 0.0548229, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:00.919193: step 7182, loss 0.0232797, acc 1, learning_rate 0.0001
2017-10-10T11:35:01.083359: step 7183, loss 0.0573636, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:01.245618: step 7184, loss 0.0378456, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:01.410777: step 7185, loss 0.0604932, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:01.572308: step 7186, loss 0.0594479, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:01.735206: step 7187, loss 0.0350342, acc 1, learning_rate 0.0001
2017-10-10T11:35:01.905473: step 7188, loss 0.0802826, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:02.069276: step 7189, loss 0.015556, acc 1, learning_rate 0.0001
2017-10-10T11:35:02.232154: step 7190, loss 0.0425341, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:02.393363: step 7191, loss 0.0637339, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:02.560321: step 7192, loss 0.03675, acc 1, learning_rate 0.0001
2017-10-10T11:35:02.724922: step 7193, loss 0.0770423, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:02.887374: step 7194, loss 0.0101456, acc 1, learning_rate 0.0001
2017-10-10T11:35:03.047166: step 7195, loss 0.06019, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:03.211699: step 7196, loss 0.0931634, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:03.375357: step 7197, loss 0.0364737, acc 1, learning_rate 0.0001
2017-10-10T11:35:03.535372: step 7198, loss 0.056873, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:03.697027: step 7199, loss 0.0371285, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:03.863054: step 7200, loss 0.075077, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:35:04.271234: step 7200, loss 0.200607, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7200

2017-10-10T11:35:04.908190: step 7201, loss 0.036501, acc 1, learning_rate 0.0001
2017-10-10T11:35:05.067833: step 7202, loss 0.126038, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:05.229677: step 7203, loss 0.0392427, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:05.392278: step 7204, loss 0.0299339, acc 1, learning_rate 0.0001
2017-10-10T11:35:05.555422: step 7205, loss 0.0191551, acc 1, learning_rate 0.0001
2017-10-10T11:35:05.715729: step 7206, loss 0.0289184, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:05.882269: step 7207, loss 0.0317678, acc 1, learning_rate 0.0001
2017-10-10T11:35:06.045557: step 7208, loss 0.0361483, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:06.211684: step 7209, loss 0.0196367, acc 1, learning_rate 0.0001
2017-10-10T11:35:06.374488: step 7210, loss 0.098757, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:06.535142: step 7211, loss 0.0517014, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:06.702134: step 7212, loss 0.0382212, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:06.869685: step 7213, loss 0.090714, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:07.031685: step 7214, loss 0.111062, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:07.195044: step 7215, loss 0.107231, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:07.363050: step 7216, loss 0.0430323, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:07.525233: step 7217, loss 0.0911045, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:07.692307: step 7218, loss 0.104829, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:07.857743: step 7219, loss 0.035054, acc 1, learning_rate 0.0001
2017-10-10T11:35:08.021104: step 7220, loss 0.07071, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:08.187901: step 7221, loss 0.0500999, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:08.348111: step 7222, loss 0.0277066, acc 1, learning_rate 0.0001
2017-10-10T11:35:08.508313: step 7223, loss 0.0304541, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:08.671278: step 7224, loss 0.0551898, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:08.838549: step 7225, loss 0.021519, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:09.010019: step 7226, loss 0.0736979, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:09.175006: step 7227, loss 0.0336967, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:09.338489: step 7228, loss 0.068249, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:09.500213: step 7229, loss 0.0218675, acc 1, learning_rate 0.0001
2017-10-10T11:35:09.669219: step 7230, loss 0.0853624, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:09.829012: step 7231, loss 0.0258297, acc 1, learning_rate 0.0001
2017-10-10T11:35:09.991352: step 7232, loss 0.0319126, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:10.153071: step 7233, loss 0.0200237, acc 1, learning_rate 0.0001
2017-10-10T11:35:10.318550: step 7234, loss 0.0633275, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:10.483603: step 7235, loss 0.0676606, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:10.647039: step 7236, loss 0.0902952, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:10.810950: step 7237, loss 0.154608, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:10.974713: step 7238, loss 0.0137899, acc 1, learning_rate 0.0001
2017-10-10T11:35:11.132718: step 7239, loss 0.095962, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:11.292203: step 7240, loss 0.0358392, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:35:11.726273: step 7240, loss 0.203883, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7240

2017-10-10T11:35:12.446534: step 7241, loss 0.0420696, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:12.616965: step 7242, loss 0.0427519, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:12.782134: step 7243, loss 0.105416, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:12.947684: step 7244, loss 0.0756093, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:13.111531: step 7245, loss 0.0853001, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:13.272568: step 7246, loss 0.0449715, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:13.434730: step 7247, loss 0.0303542, acc 1, learning_rate 0.0001
2017-10-10T11:35:13.596537: step 7248, loss 0.0778021, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:13.760890: step 7249, loss 0.0534118, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:13.936393: step 7250, loss 0.091153, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:14.100449: step 7251, loss 0.0596431, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:14.241394: step 7252, loss 0.0996212, acc 0.960784, learning_rate 0.0001
2017-10-10T11:35:14.403895: step 7253, loss 0.0395349, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:14.566364: step 7254, loss 0.0419046, acc 1, learning_rate 0.0001
2017-10-10T11:35:14.729820: step 7255, loss 0.0642577, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:14.907009: step 7256, loss 0.0557263, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:15.072979: step 7257, loss 0.0929573, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:15.235837: step 7258, loss 0.0747267, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:15.399350: step 7259, loss 0.0610716, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:15.564017: step 7260, loss 0.141879, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:15.728004: step 7261, loss 0.0447537, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:15.892176: step 7262, loss 0.0268862, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:16.053739: step 7263, loss 0.0403403, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:16.217430: step 7264, loss 0.015574, acc 1, learning_rate 0.0001
2017-10-10T11:35:16.379911: step 7265, loss 0.0963493, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:16.539866: step 7266, loss 0.0323476, acc 1, learning_rate 0.0001
2017-10-10T11:35:16.701715: step 7267, loss 0.0206145, acc 1, learning_rate 0.0001
2017-10-10T11:35:16.868611: step 7268, loss 0.0587879, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:17.033799: step 7269, loss 0.0105844, acc 1, learning_rate 0.0001
2017-10-10T11:35:17.214349: step 7270, loss 0.052669, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:17.377246: step 7271, loss 0.0287961, acc 1, learning_rate 0.0001
2017-10-10T11:35:17.540037: step 7272, loss 0.104362, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:17.705874: step 7273, loss 0.0347646, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:17.873896: step 7274, loss 0.0807991, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:18.036733: step 7275, loss 0.0594688, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:18.199301: step 7276, loss 0.0522982, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:18.363983: step 7277, loss 0.0890134, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:18.531781: step 7278, loss 0.0333491, acc 1, learning_rate 0.0001
2017-10-10T11:35:18.694648: step 7279, loss 0.00719117, acc 1, learning_rate 0.0001
2017-10-10T11:35:18.866299: step 7280, loss 0.112973, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:35:19.307669: step 7280, loss 0.202605, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7280

2017-10-10T11:35:19.893278: step 7281, loss 0.0417119, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:20.054941: step 7282, loss 0.127671, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:20.219850: step 7283, loss 0.0306095, acc 1, learning_rate 0.0001
2017-10-10T11:35:20.382081: step 7284, loss 0.0355498, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:20.546788: step 7285, loss 0.0161822, acc 1, learning_rate 0.0001
2017-10-10T11:35:20.711092: step 7286, loss 0.170271, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:20.877201: step 7287, loss 0.0515519, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:21.042352: step 7288, loss 0.0356929, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:21.204264: step 7289, loss 0.0753511, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:21.363875: step 7290, loss 0.0881158, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:21.524600: step 7291, loss 0.0484862, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:21.685431: step 7292, loss 0.031529, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:21.849447: step 7293, loss 0.119188, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:22.011836: step 7294, loss 0.0196403, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:22.172580: step 7295, loss 0.0478233, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:22.333181: step 7296, loss 0.0498947, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:22.497166: step 7297, loss 0.0306036, acc 1, learning_rate 0.0001
2017-10-10T11:35:22.658174: step 7298, loss 0.0285755, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:22.821484: step 7299, loss 0.0175912, acc 1, learning_rate 0.0001
2017-10-10T11:35:23.006125: step 7300, loss 0.0175442, acc 1, learning_rate 0.0001
2017-10-10T11:35:23.172160: step 7301, loss 0.0610548, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:23.338010: step 7302, loss 0.0635921, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:23.502970: step 7303, loss 0.026045, acc 1, learning_rate 0.0001
2017-10-10T11:35:23.668543: step 7304, loss 0.0357816, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:23.831007: step 7305, loss 0.019822, acc 1, learning_rate 0.0001
2017-10-10T11:35:23.991731: step 7306, loss 0.0925259, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:24.153158: step 7307, loss 0.0408886, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:24.316149: step 7308, loss 0.0417485, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:24.479868: step 7309, loss 0.0178352, acc 1, learning_rate 0.0001
2017-10-10T11:35:24.642226: step 7310, loss 0.0555894, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:24.803596: step 7311, loss 0.0509408, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:24.970494: step 7312, loss 0.038367, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:25.132342: step 7313, loss 0.03334, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:25.292996: step 7314, loss 0.060382, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:25.457149: step 7315, loss 0.0712038, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:25.622099: step 7316, loss 0.0640076, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:25.786433: step 7317, loss 0.0906244, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:25.952050: step 7318, loss 0.0652734, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:26.121538: step 7319, loss 0.0319659, acc 1, learning_rate 0.0001
2017-10-10T11:35:26.295786: step 7320, loss 0.0550084, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:35:26.748242: step 7320, loss 0.203194, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7320

2017-10-10T11:35:27.409253: step 7321, loss 0.0259539, acc 1, learning_rate 0.0001
2017-10-10T11:35:27.576617: step 7322, loss 0.0273265, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:27.743704: step 7323, loss 0.0856357, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:27.916175: step 7324, loss 0.0427428, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:28.083186: step 7325, loss 0.0915366, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:28.251167: step 7326, loss 0.0246166, acc 1, learning_rate 0.0001
2017-10-10T11:35:28.418431: step 7327, loss 0.031453, acc 1, learning_rate 0.0001
2017-10-10T11:35:28.584264: step 7328, loss 0.0413164, acc 1, learning_rate 0.0001
2017-10-10T11:35:28.750194: step 7329, loss 0.016408, acc 1, learning_rate 0.0001
2017-10-10T11:35:28.919108: step 7330, loss 0.0514811, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:29.086508: step 7331, loss 0.0591216, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:29.247618: step 7332, loss 0.0666829, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:29.417058: step 7333, loss 0.0139303, acc 1, learning_rate 0.0001
2017-10-10T11:35:29.587448: step 7334, loss 0.0468987, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:29.755484: step 7335, loss 0.0524653, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:29.929013: step 7336, loss 0.0681651, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:30.097938: step 7337, loss 0.0586014, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:30.263326: step 7338, loss 0.123151, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:30.431472: step 7339, loss 0.0957714, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:30.602131: step 7340, loss 0.0233894, acc 1, learning_rate 0.0001
2017-10-10T11:35:30.784015: step 7341, loss 0.0233656, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:30.961444: step 7342, loss 0.197307, acc 0.90625, learning_rate 0.0001
2017-10-10T11:35:31.129011: step 7343, loss 0.0471243, acc 1, learning_rate 0.0001
2017-10-10T11:35:31.295607: step 7344, loss 0.0179841, acc 1, learning_rate 0.0001
2017-10-10T11:35:31.459667: step 7345, loss 0.0296131, acc 1, learning_rate 0.0001
2017-10-10T11:35:31.627356: step 7346, loss 0.0396761, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:31.806232: step 7347, loss 0.0485102, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:31.990993: step 7348, loss 0.0228457, acc 1, learning_rate 0.0001
2017-10-10T11:35:32.165415: step 7349, loss 0.0657557, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:32.302416: step 7350, loss 0.0555619, acc 0.980392, learning_rate 0.0001
2017-10-10T11:35:32.473988: step 7351, loss 0.052389, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:32.642577: step 7352, loss 0.0904469, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:32.813150: step 7353, loss 0.0361235, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:32.985544: step 7354, loss 0.0606991, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:33.151836: step 7355, loss 0.0750654, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:33.319066: step 7356, loss 0.0807385, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:33.481997: step 7357, loss 0.0682975, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:33.647607: step 7358, loss 0.075373, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:33.816159: step 7359, loss 0.0512236, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:33.982136: step 7360, loss 0.0137261, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:35:34.401043: step 7360, loss 0.204791, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7360

2017-10-10T11:35:35.132227: step 7361, loss 0.1095, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:35.298793: step 7362, loss 0.0208857, acc 1, learning_rate 0.0001
2017-10-10T11:35:35.463859: step 7363, loss 0.0763948, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:35.627082: step 7364, loss 0.103103, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:35.789857: step 7365, loss 0.0437496, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:35.960477: step 7366, loss 0.111016, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:36.133228: step 7367, loss 0.0985726, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:36.299882: step 7368, loss 0.078932, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:36.467282: step 7369, loss 0.0893961, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:36.633655: step 7370, loss 0.0333852, acc 1, learning_rate 0.0001
2017-10-10T11:35:36.798529: step 7371, loss 0.176896, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:36.965577: step 7372, loss 0.019291, acc 1, learning_rate 0.0001
2017-10-10T11:35:37.126888: step 7373, loss 0.0352471, acc 1, learning_rate 0.0001
2017-10-10T11:35:37.291473: step 7374, loss 0.0234198, acc 1, learning_rate 0.0001
2017-10-10T11:35:37.454599: step 7375, loss 0.0323206, acc 1, learning_rate 0.0001
2017-10-10T11:35:37.616103: step 7376, loss 0.048135, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:37.779815: step 7377, loss 0.0837653, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:37.941528: step 7378, loss 0.124511, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:38.101946: step 7379, loss 0.0442403, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:38.270588: step 7380, loss 0.029635, acc 1, learning_rate 0.0001
2017-10-10T11:35:38.436176: step 7381, loss 0.070185, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:38.602361: step 7382, loss 0.0220003, acc 1, learning_rate 0.0001
2017-10-10T11:35:38.769409: step 7383, loss 0.0268162, acc 1, learning_rate 0.0001
2017-10-10T11:35:38.938229: step 7384, loss 0.0188569, acc 1, learning_rate 0.0001
2017-10-10T11:35:39.105724: step 7385, loss 0.0575709, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:39.271008: step 7386, loss 0.107475, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:39.438561: step 7387, loss 0.0360465, acc 1, learning_rate 0.0001
2017-10-10T11:35:39.605483: step 7388, loss 0.0528434, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:39.764718: step 7389, loss 0.00733271, acc 1, learning_rate 0.0001
2017-10-10T11:35:39.929228: step 7390, loss 0.0137707, acc 1, learning_rate 0.0001
2017-10-10T11:35:40.092108: step 7391, loss 0.0266795, acc 1, learning_rate 0.0001
2017-10-10T11:35:40.251345: step 7392, loss 0.150241, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:40.416865: step 7393, loss 0.0246964, acc 1, learning_rate 0.0001
2017-10-10T11:35:40.577720: step 7394, loss 0.0625167, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:40.738436: step 7395, loss 0.0328709, acc 1, learning_rate 0.0001
2017-10-10T11:35:40.907216: step 7396, loss 0.0582522, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:41.074149: step 7397, loss 0.0773524, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:41.234955: step 7398, loss 0.105902, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:41.398202: step 7399, loss 0.0839572, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:41.561253: step 7400, loss 0.0225552, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:35:41.992328: step 7400, loss 0.204919, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7400

2017-10-10T11:35:42.578558: step 7401, loss 0.0241969, acc 1, learning_rate 0.0001
2017-10-10T11:35:42.746001: step 7402, loss 0.0172073, acc 1, learning_rate 0.0001
2017-10-10T11:35:42.907057: step 7403, loss 0.0667854, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:43.070556: step 7404, loss 0.0267212, acc 1, learning_rate 0.0001
2017-10-10T11:35:43.234136: step 7405, loss 0.0190761, acc 1, learning_rate 0.0001
2017-10-10T11:35:43.400293: step 7406, loss 0.0777956, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:43.563673: step 7407, loss 0.0620033, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:43.727285: step 7408, loss 0.00681227, acc 1, learning_rate 0.0001
2017-10-10T11:35:43.906372: step 7409, loss 0.0256532, acc 1, learning_rate 0.0001
2017-10-10T11:35:44.080426: step 7410, loss 0.018288, acc 1, learning_rate 0.0001
2017-10-10T11:35:44.247879: step 7411, loss 0.0438508, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:44.411335: step 7412, loss 0.0758419, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:44.578039: step 7413, loss 0.0264486, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:44.745284: step 7414, loss 0.0707302, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:44.919542: step 7415, loss 0.0416784, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:45.081519: step 7416, loss 0.0492506, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:45.244904: step 7417, loss 0.11667, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:45.415293: step 7418, loss 0.0204038, acc 1, learning_rate 0.0001
2017-10-10T11:35:45.594007: step 7419, loss 0.0300314, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:45.756557: step 7420, loss 0.0372515, acc 1, learning_rate 0.0001
2017-10-10T11:35:45.924599: step 7421, loss 0.0377411, acc 1, learning_rate 0.0001
2017-10-10T11:35:46.100503: step 7422, loss 0.0546223, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:46.268020: step 7423, loss 0.106891, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:46.432284: step 7424, loss 0.115489, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:46.604257: step 7425, loss 0.0518236, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:46.768769: step 7426, loss 0.0531638, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:46.932469: step 7427, loss 0.0517091, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:47.097188: step 7428, loss 0.0435662, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:47.258400: step 7429, loss 0.0677616, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:47.422453: step 7430, loss 0.0214553, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:47.597824: step 7431, loss 0.108867, acc 0.9375, learning_rate 0.0001
2017-10-10T11:35:47.763400: step 7432, loss 0.0257993, acc 1, learning_rate 0.0001
2017-10-10T11:35:47.934890: step 7433, loss 0.051055, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:48.101396: step 7434, loss 0.127656, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:48.268274: step 7435, loss 0.0554472, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:48.433630: step 7436, loss 0.0876455, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:48.601586: step 7437, loss 0.057648, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:48.771289: step 7438, loss 0.059782, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:48.937446: step 7439, loss 0.0613373, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:49.098943: step 7440, loss 0.068758, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:35:49.517529: step 7440, loss 0.203445, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7440

2017-10-10T11:35:50.161147: step 7441, loss 0.020311, acc 1, learning_rate 0.0001
2017-10-10T11:35:50.324409: step 7442, loss 0.00626906, acc 1, learning_rate 0.0001
2017-10-10T11:35:50.488126: step 7443, loss 0.0365256, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:50.654849: step 7444, loss 0.052616, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:50.817579: step 7445, loss 0.0208494, acc 1, learning_rate 0.0001
2017-10-10T11:35:50.982342: step 7446, loss 0.0134571, acc 1, learning_rate 0.0001
2017-10-10T11:35:51.150041: step 7447, loss 0.0779761, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:51.285211: step 7448, loss 0.0258365, acc 1, learning_rate 0.0001
2017-10-10T11:35:51.452528: step 7449, loss 0.042446, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:51.615949: step 7450, loss 0.0292489, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:51.778822: step 7451, loss 0.0303344, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:51.954078: step 7452, loss 0.0525402, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:52.122665: step 7453, loss 0.0401687, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:52.286847: step 7454, loss 0.0196599, acc 1, learning_rate 0.0001
2017-10-10T11:35:52.450254: step 7455, loss 0.0719283, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:52.613559: step 7456, loss 0.0395791, acc 1, learning_rate 0.0001
2017-10-10T11:35:52.774703: step 7457, loss 0.0582227, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:52.944990: step 7458, loss 0.0617868, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:53.109274: step 7459, loss 0.0156767, acc 1, learning_rate 0.0001
2017-10-10T11:35:53.275633: step 7460, loss 0.048086, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:53.440653: step 7461, loss 0.027949, acc 1, learning_rate 0.0001
2017-10-10T11:35:53.601160: step 7462, loss 0.0483351, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:53.764683: step 7463, loss 0.0655917, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:53.935918: step 7464, loss 0.023988, acc 1, learning_rate 0.0001
2017-10-10T11:35:54.099473: step 7465, loss 0.0223628, acc 1, learning_rate 0.0001
2017-10-10T11:35:54.263353: step 7466, loss 0.0285595, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:54.427620: step 7467, loss 0.0506511, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:54.590669: step 7468, loss 0.0519552, acc 0.953125, learning_rate 0.0001
2017-10-10T11:35:54.749766: step 7469, loss 0.0749015, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:54.919937: step 7470, loss 0.0263664, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:55.087477: step 7471, loss 0.0370495, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:55.257168: step 7472, loss 0.0288158, acc 1, learning_rate 0.0001
2017-10-10T11:35:55.421420: step 7473, loss 0.0196955, acc 1, learning_rate 0.0001
2017-10-10T11:35:55.586408: step 7474, loss 0.0483723, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:55.751440: step 7475, loss 0.0622021, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:55.915317: step 7476, loss 0.0237831, acc 1, learning_rate 0.0001
2017-10-10T11:35:56.080016: step 7477, loss 0.0387249, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:56.247405: step 7478, loss 0.087316, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:56.407793: step 7479, loss 0.0489753, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:56.572925: step 7480, loss 0.0576865, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:35:57.006030: step 7480, loss 0.208422, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7480

2017-10-10T11:35:57.721519: step 7481, loss 0.065608, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:57.884331: step 7482, loss 0.0554308, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:58.050361: step 7483, loss 0.0322264, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:58.214677: step 7484, loss 0.0445311, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:58.374851: step 7485, loss 0.0760139, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:58.536823: step 7486, loss 0.0374179, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:58.700365: step 7487, loss 0.0694872, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:58.866117: step 7488, loss 0.0876643, acc 0.96875, learning_rate 0.0001
2017-10-10T11:35:59.030717: step 7489, loss 0.0200446, acc 1, learning_rate 0.0001
2017-10-10T11:35:59.194735: step 7490, loss 0.0167926, acc 1, learning_rate 0.0001
2017-10-10T11:35:59.358009: step 7491, loss 0.0280198, acc 1, learning_rate 0.0001
2017-10-10T11:35:59.520683: step 7492, loss 0.0526994, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:59.683043: step 7493, loss 0.0365273, acc 0.984375, learning_rate 0.0001
2017-10-10T11:35:59.849540: step 7494, loss 0.0502647, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:00.012038: step 7495, loss 0.0211183, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:00.176281: step 7496, loss 0.0833886, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:00.336414: step 7497, loss 0.0325423, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:00.498669: step 7498, loss 0.0243203, acc 1, learning_rate 0.0001
2017-10-10T11:36:00.662335: step 7499, loss 0.0326443, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:00.822256: step 7500, loss 0.0496029, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:00.986020: step 7501, loss 0.168229, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:01.149029: step 7502, loss 0.0672778, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:01.314958: step 7503, loss 0.0602513, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:01.480062: step 7504, loss 0.0523624, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:01.647744: step 7505, loss 0.0144336, acc 1, learning_rate 0.0001
2017-10-10T11:36:01.810905: step 7506, loss 0.0654677, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:01.975824: step 7507, loss 0.084681, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:02.137786: step 7508, loss 0.0720258, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:02.303332: step 7509, loss 0.124724, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:02.468909: step 7510, loss 0.101544, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:02.633743: step 7511, loss 0.0408012, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:02.796637: step 7512, loss 0.028701, acc 1, learning_rate 0.0001
2017-10-10T11:36:02.964662: step 7513, loss 0.0217567, acc 1, learning_rate 0.0001
2017-10-10T11:36:03.126787: step 7514, loss 0.0452506, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:03.288118: step 7515, loss 0.018398, acc 1, learning_rate 0.0001
2017-10-10T11:36:03.452121: step 7516, loss 0.084265, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:03.614619: step 7517, loss 0.0876234, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:03.777581: step 7518, loss 0.0354275, acc 1, learning_rate 0.0001
2017-10-10T11:36:03.940351: step 7519, loss 0.029478, acc 1, learning_rate 0.0001
2017-10-10T11:36:04.101860: step 7520, loss 0.0578841, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:36:04.537913: step 7520, loss 0.203491, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7520

2017-10-10T11:36:05.111878: step 7521, loss 0.0113661, acc 1, learning_rate 0.0001
2017-10-10T11:36:05.273383: step 7522, loss 0.082577, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:05.434452: step 7523, loss 0.0626042, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:05.598485: step 7524, loss 0.0763549, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:05.759800: step 7525, loss 0.0240089, acc 1, learning_rate 0.0001
2017-10-10T11:36:05.934128: step 7526, loss 0.0348451, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:06.098763: step 7527, loss 0.0418655, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:06.264490: step 7528, loss 0.0107514, acc 1, learning_rate 0.0001
2017-10-10T11:36:06.428669: step 7529, loss 0.0937081, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:06.593930: step 7530, loss 0.0372994, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:06.756679: step 7531, loss 0.0875898, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:06.920430: step 7532, loss 0.0410666, acc 1, learning_rate 0.0001
2017-10-10T11:36:07.084789: step 7533, loss 0.016843, acc 1, learning_rate 0.0001
2017-10-10T11:36:07.249927: step 7534, loss 0.0878124, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:07.412573: step 7535, loss 0.095385, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:07.576197: step 7536, loss 0.0236787, acc 1, learning_rate 0.0001
2017-10-10T11:36:07.736472: step 7537, loss 0.0290949, acc 1, learning_rate 0.0001
2017-10-10T11:36:07.902296: step 7538, loss 0.0751154, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:08.065789: step 7539, loss 0.0171272, acc 1, learning_rate 0.0001
2017-10-10T11:36:08.226964: step 7540, loss 0.0481143, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:08.388401: step 7541, loss 0.0386673, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:08.552034: step 7542, loss 0.0485, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:08.714672: step 7543, loss 0.00991522, acc 1, learning_rate 0.0001
2017-10-10T11:36:08.880907: step 7544, loss 0.0396291, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:09.046312: step 7545, loss 0.0414181, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:09.188512: step 7546, loss 0.0851585, acc 0.960784, learning_rate 0.0001
2017-10-10T11:36:09.354662: step 7547, loss 0.0318808, acc 1, learning_rate 0.0001
2017-10-10T11:36:09.520047: step 7548, loss 0.0765023, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:09.686911: step 7549, loss 0.0332452, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:09.853516: step 7550, loss 0.0367979, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:10.015068: step 7551, loss 0.0306681, acc 1, learning_rate 0.0001
2017-10-10T11:36:10.181010: step 7552, loss 0.110215, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:10.344457: step 7553, loss 0.08559, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:10.508228: step 7554, loss 0.0148902, acc 1, learning_rate 0.0001
2017-10-10T11:36:10.671809: step 7555, loss 0.022155, acc 1, learning_rate 0.0001
2017-10-10T11:36:10.834654: step 7556, loss 0.0380427, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:11.000889: step 7557, loss 0.0726395, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:11.163738: step 7558, loss 0.0812086, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:11.323588: step 7559, loss 0.029618, acc 1, learning_rate 0.0001
2017-10-10T11:36:11.490931: step 7560, loss 0.0861409, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:36:11.953936: step 7560, loss 0.199435, acc 0.933813

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7560

2017-10-10T11:36:12.597063: step 7561, loss 0.0741325, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:12.759747: step 7562, loss 0.0436286, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:12.933315: step 7563, loss 0.0242991, acc 1, learning_rate 0.0001
2017-10-10T11:36:13.099618: step 7564, loss 0.0474603, acc 1, learning_rate 0.0001
2017-10-10T11:36:13.262225: step 7565, loss 0.0234535, acc 1, learning_rate 0.0001
2017-10-10T11:36:13.428058: step 7566, loss 0.0626764, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:13.590389: step 7567, loss 0.0201385, acc 1, learning_rate 0.0001
2017-10-10T11:36:13.755158: step 7568, loss 0.0121957, acc 1, learning_rate 0.0001
2017-10-10T11:36:13.922237: step 7569, loss 0.021201, acc 1, learning_rate 0.0001
2017-10-10T11:36:14.084319: step 7570, loss 0.0957759, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:14.248174: step 7571, loss 0.0171362, acc 1, learning_rate 0.0001
2017-10-10T11:36:14.411751: step 7572, loss 0.0319192, acc 1, learning_rate 0.0001
2017-10-10T11:36:14.577803: step 7573, loss 0.010111, acc 1, learning_rate 0.0001
2017-10-10T11:36:14.742107: step 7574, loss 0.0269545, acc 1, learning_rate 0.0001
2017-10-10T11:36:14.910749: step 7575, loss 0.0551202, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:15.075121: step 7576, loss 0.113787, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:15.240638: step 7577, loss 0.0544256, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:15.405961: step 7578, loss 0.0511026, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:15.569827: step 7579, loss 0.0380359, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:15.735432: step 7580, loss 0.0313233, acc 1, learning_rate 0.0001
2017-10-10T11:36:15.903104: step 7581, loss 0.037026, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:16.070941: step 7582, loss 0.00589395, acc 1, learning_rate 0.0001
2017-10-10T11:36:16.236389: step 7583, loss 0.0227199, acc 1, learning_rate 0.0001
2017-10-10T11:36:16.403955: step 7584, loss 0.041834, acc 1, learning_rate 0.0001
2017-10-10T11:36:16.573918: step 7585, loss 0.0550118, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:16.742244: step 7586, loss 0.110872, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:16.913702: step 7587, loss 0.0312982, acc 1, learning_rate 0.0001
2017-10-10T11:36:17.077014: step 7588, loss 0.0231199, acc 1, learning_rate 0.0001
2017-10-10T11:36:17.243541: step 7589, loss 0.0608263, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:17.410360: step 7590, loss 0.050944, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:17.574295: step 7591, loss 0.0342154, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:17.734908: step 7592, loss 0.0745761, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:17.899029: step 7593, loss 0.0257716, acc 1, learning_rate 0.0001
2017-10-10T11:36:18.064140: step 7594, loss 0.0249551, acc 1, learning_rate 0.0001
2017-10-10T11:36:18.224723: step 7595, loss 0.106529, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:18.388260: step 7596, loss 0.0464522, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:18.549703: step 7597, loss 0.0893415, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:18.714003: step 7598, loss 0.0403166, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:18.877778: step 7599, loss 0.0158394, acc 1, learning_rate 0.0001
2017-10-10T11:36:19.041608: step 7600, loss 0.0232264, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:36:19.460273: step 7600, loss 0.20159, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7600

2017-10-10T11:36:20.105523: step 7601, loss 0.0227058, acc 1, learning_rate 0.0001
2017-10-10T11:36:20.268020: step 7602, loss 0.0750635, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:20.429415: step 7603, loss 0.0638936, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:20.593025: step 7604, loss 0.0634022, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:20.753042: step 7605, loss 0.0931394, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:20.928572: step 7606, loss 0.0721485, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:21.092173: step 7607, loss 0.0229988, acc 1, learning_rate 0.0001
2017-10-10T11:36:21.259226: step 7608, loss 0.0225194, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:21.421466: step 7609, loss 0.0513776, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:21.589368: step 7610, loss 0.0630391, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:21.751243: step 7611, loss 0.0735579, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:21.925998: step 7612, loss 0.0403368, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:22.091779: step 7613, loss 0.072742, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:22.255065: step 7614, loss 0.0509068, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:22.420017: step 7615, loss 0.0129377, acc 1, learning_rate 0.0001
2017-10-10T11:36:22.583627: step 7616, loss 0.0854819, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:22.746359: step 7617, loss 0.020082, acc 1, learning_rate 0.0001
2017-10-10T11:36:22.912822: step 7618, loss 0.15023, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:23.078584: step 7619, loss 0.0211405, acc 1, learning_rate 0.0001
2017-10-10T11:36:23.243874: step 7620, loss 0.0300005, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:23.403412: step 7621, loss 0.052928, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:23.567988: step 7622, loss 0.0695386, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:23.730785: step 7623, loss 0.12478, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:23.892690: step 7624, loss 0.0371257, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:24.054180: step 7625, loss 0.0223632, acc 1, learning_rate 0.0001
2017-10-10T11:36:24.217995: step 7626, loss 0.142853, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:24.380974: step 7627, loss 0.0841333, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:24.545032: step 7628, loss 0.0766334, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:24.706516: step 7629, loss 0.0153367, acc 1, learning_rate 0.0001
2017-10-10T11:36:24.871996: step 7630, loss 0.015485, acc 1, learning_rate 0.0001
2017-10-10T11:36:25.035982: step 7631, loss 0.0701152, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:25.196983: step 7632, loss 0.0363887, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:25.359586: step 7633, loss 0.0780159, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:25.521922: step 7634, loss 0.0493099, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:25.685299: step 7635, loss 0.0275702, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:25.849159: step 7636, loss 0.0816475, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:26.012552: step 7637, loss 0.0850621, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:26.176662: step 7638, loss 0.0339968, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:26.339219: step 7639, loss 0.0335578, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:26.503865: step 7640, loss 0.0364011, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:36:26.934851: step 7640, loss 0.203627, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7640

2017-10-10T11:36:27.645512: step 7641, loss 0.046195, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:27.806860: step 7642, loss 0.0150365, acc 1, learning_rate 0.0001
2017-10-10T11:36:27.974997: step 7643, loss 0.0585838, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:28.105424: step 7644, loss 0.110741, acc 0.941176, learning_rate 0.0001
2017-10-10T11:36:28.270822: step 7645, loss 0.0533596, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:28.434865: step 7646, loss 0.051281, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:28.598142: step 7647, loss 0.0766266, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:28.759861: step 7648, loss 0.0224019, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:28.923084: step 7649, loss 0.0986258, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:29.087853: step 7650, loss 0.0441713, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:29.249938: step 7651, loss 0.0344289, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:29.416791: step 7652, loss 0.041416, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:29.581407: step 7653, loss 0.00886806, acc 1, learning_rate 0.0001
2017-10-10T11:36:29.742613: step 7654, loss 0.0947169, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:29.907841: step 7655, loss 0.0172985, acc 1, learning_rate 0.0001
2017-10-10T11:36:30.069755: step 7656, loss 0.0788414, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:30.234173: step 7657, loss 0.0733235, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:30.397796: step 7658, loss 0.03118, acc 1, learning_rate 0.0001
2017-10-10T11:36:30.561900: step 7659, loss 0.033229, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:30.723230: step 7660, loss 0.00728167, acc 1, learning_rate 0.0001
2017-10-10T11:36:30.887067: step 7661, loss 0.0544227, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:31.048406: step 7662, loss 0.0519192, acc 1, learning_rate 0.0001
2017-10-10T11:36:31.212019: step 7663, loss 0.0976405, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:31.459571: step 7664, loss 0.054102, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:31.624530: step 7665, loss 0.0993744, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:31.788034: step 7666, loss 0.0126536, acc 1, learning_rate 0.0001
2017-10-10T11:36:31.952200: step 7667, loss 0.0104709, acc 1, learning_rate 0.0001
2017-10-10T11:36:32.115676: step 7668, loss 0.0391401, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:32.278658: step 7669, loss 0.0454852, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:32.443495: step 7670, loss 0.058781, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:32.608568: step 7671, loss 0.0392318, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:32.770141: step 7672, loss 0.145865, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:32.940789: step 7673, loss 0.0359816, acc 1, learning_rate 0.0001
2017-10-10T11:36:33.102360: step 7674, loss 0.0169393, acc 1, learning_rate 0.0001
2017-10-10T11:36:33.266867: step 7675, loss 0.0629506, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:33.426548: step 7676, loss 0.0308149, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:33.587827: step 7677, loss 0.0349981, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:33.749689: step 7678, loss 0.114166, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:33.916011: step 7679, loss 0.137375, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:34.080997: step 7680, loss 0.0229193, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:36:34.515241: step 7680, loss 0.202686, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7680

2017-10-10T11:36:35.092585: step 7681, loss 0.0421154, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:35.254618: step 7682, loss 0.0431199, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:35.419033: step 7683, loss 0.0538486, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:35.580375: step 7684, loss 0.0832829, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:35.742607: step 7685, loss 0.0160753, acc 1, learning_rate 0.0001
2017-10-10T11:36:35.905224: step 7686, loss 0.0450626, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:36.066523: step 7687, loss 0.0167299, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:36.229256: step 7688, loss 0.0212746, acc 1, learning_rate 0.0001
2017-10-10T11:36:36.394918: step 7689, loss 0.0779309, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:36.563300: step 7690, loss 0.0917387, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:36.728775: step 7691, loss 0.0988991, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:36.892341: step 7692, loss 0.0147388, acc 1, learning_rate 0.0001
2017-10-10T11:36:37.053320: step 7693, loss 0.0428065, acc 1, learning_rate 0.0001
2017-10-10T11:36:37.216241: step 7694, loss 0.208649, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:37.374766: step 7695, loss 0.0925983, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:37.535464: step 7696, loss 0.0447792, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:37.697610: step 7697, loss 0.038274, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:37.864239: step 7698, loss 0.0615468, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:38.027416: step 7699, loss 0.0871279, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:38.187066: step 7700, loss 0.0850478, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:38.348663: step 7701, loss 0.0391285, acc 1, learning_rate 0.0001
2017-10-10T11:36:38.512390: step 7702, loss 0.0883786, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:38.676410: step 7703, loss 0.0752023, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:38.840565: step 7704, loss 0.134488, acc 0.921875, learning_rate 0.0001
2017-10-10T11:36:39.007718: step 7705, loss 0.159068, acc 0.9375, learning_rate 0.0001
2017-10-10T11:36:39.173098: step 7706, loss 0.0229003, acc 1, learning_rate 0.0001
2017-10-10T11:36:39.338972: step 7707, loss 0.0530497, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:39.503851: step 7708, loss 0.122024, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:39.668624: step 7709, loss 0.0265503, acc 1, learning_rate 0.0001
2017-10-10T11:36:39.835719: step 7710, loss 0.0592211, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:40.003064: step 7711, loss 0.0316152, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:40.166642: step 7712, loss 0.0559369, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:40.331411: step 7713, loss 0.0166939, acc 1, learning_rate 0.0001
2017-10-10T11:36:40.493571: step 7714, loss 0.0465312, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:40.658170: step 7715, loss 0.149015, acc 0.9375, learning_rate 0.0001
2017-10-10T11:36:40.826966: step 7716, loss 0.0671811, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:40.999089: step 7717, loss 0.0349552, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:41.175563: step 7718, loss 0.0609653, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:41.338381: step 7719, loss 0.0297136, acc 1, learning_rate 0.0001
2017-10-10T11:36:41.500051: step 7720, loss 0.0149616, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:36:41.946998: step 7720, loss 0.198555, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7720

2017-10-10T11:36:42.590649: step 7721, loss 0.112347, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:42.752693: step 7722, loss 0.0157961, acc 1, learning_rate 0.0001
2017-10-10T11:36:42.915145: step 7723, loss 0.0122189, acc 1, learning_rate 0.0001
2017-10-10T11:36:43.075234: step 7724, loss 0.0910345, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:43.234646: step 7725, loss 0.0357375, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:43.395211: step 7726, loss 0.019853, acc 1, learning_rate 0.0001
2017-10-10T11:36:43.558061: step 7727, loss 0.0609164, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:43.723491: step 7728, loss 0.0169118, acc 1, learning_rate 0.0001
2017-10-10T11:36:43.885769: step 7729, loss 0.0182414, acc 1, learning_rate 0.0001
2017-10-10T11:36:44.050210: step 7730, loss 0.0312736, acc 1, learning_rate 0.0001
2017-10-10T11:36:44.211595: step 7731, loss 0.0286835, acc 1, learning_rate 0.0001
2017-10-10T11:36:44.373230: step 7732, loss 0.0494928, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:44.532470: step 7733, loss 0.0326839, acc 1, learning_rate 0.0001
2017-10-10T11:36:44.694440: step 7734, loss 0.138479, acc 0.921875, learning_rate 0.0001
2017-10-10T11:36:44.860070: step 7735, loss 0.0349957, acc 1, learning_rate 0.0001
2017-10-10T11:36:45.022843: step 7736, loss 0.0588018, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:45.187442: step 7737, loss 0.0263884, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:45.347296: step 7738, loss 0.0285351, acc 1, learning_rate 0.0001
2017-10-10T11:36:45.508607: step 7739, loss 0.0066451, acc 1, learning_rate 0.0001
2017-10-10T11:36:45.669681: step 7740, loss 0.0500558, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:45.831406: step 7741, loss 0.0235769, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:45.967829: step 7742, loss 0.0437555, acc 1, learning_rate 0.0001
2017-10-10T11:36:46.130035: step 7743, loss 0.0458898, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:46.293046: step 7744, loss 0.0369444, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:46.459427: step 7745, loss 0.0531613, acc 1, learning_rate 0.0001
2017-10-10T11:36:46.620409: step 7746, loss 0.0913779, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:46.782316: step 7747, loss 0.0662791, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:46.953794: step 7748, loss 0.0606697, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:47.124482: step 7749, loss 0.083204, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:47.285716: step 7750, loss 0.0289994, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:47.450842: step 7751, loss 0.025481, acc 1, learning_rate 0.0001
2017-10-10T11:36:47.617301: step 7752, loss 0.0674846, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:47.778859: step 7753, loss 0.0690661, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:47.943760: step 7754, loss 0.0436585, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:48.105179: step 7755, loss 0.0326677, acc 1, learning_rate 0.0001
2017-10-10T11:36:48.267626: step 7756, loss 0.0827807, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:48.431540: step 7757, loss 0.0667431, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:48.595530: step 7758, loss 0.070426, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:48.763680: step 7759, loss 0.0943759, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:48.929014: step 7760, loss 0.0513626, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:36:49.357868: step 7760, loss 0.200235, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7760

2017-10-10T11:36:50.083935: step 7761, loss 0.00886615, acc 1, learning_rate 0.0001
2017-10-10T11:36:50.245341: step 7762, loss 0.0494938, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:50.412292: step 7763, loss 0.0435386, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:50.580529: step 7764, loss 0.0389092, acc 1, learning_rate 0.0001
2017-10-10T11:36:50.740480: step 7765, loss 0.0711459, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:50.909746: step 7766, loss 0.105559, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:51.073802: step 7767, loss 0.0888617, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:51.238274: step 7768, loss 0.0225784, acc 1, learning_rate 0.0001
2017-10-10T11:36:51.404671: step 7769, loss 0.0386296, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:51.566177: step 7770, loss 0.073737, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:51.728778: step 7771, loss 0.0315245, acc 1, learning_rate 0.0001
2017-10-10T11:36:51.890961: step 7772, loss 0.0188461, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:52.052720: step 7773, loss 0.0949744, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:52.218340: step 7774, loss 0.104409, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:52.381200: step 7775, loss 0.106025, acc 0.9375, learning_rate 0.0001
2017-10-10T11:36:52.548429: step 7776, loss 0.0936677, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:52.713155: step 7777, loss 0.0413271, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:52.876307: step 7778, loss 0.0926586, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:53.041480: step 7779, loss 0.0444238, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:53.206118: step 7780, loss 0.0980199, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:53.371967: step 7781, loss 0.0158639, acc 1, learning_rate 0.0001
2017-10-10T11:36:53.535201: step 7782, loss 0.0656833, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:53.699246: step 7783, loss 0.0290006, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:53.860400: step 7784, loss 0.0187841, acc 1, learning_rate 0.0001
2017-10-10T11:36:54.022816: step 7785, loss 0.00890861, acc 1, learning_rate 0.0001
2017-10-10T11:36:54.187681: step 7786, loss 0.0685279, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:54.354869: step 7787, loss 0.0236756, acc 1, learning_rate 0.0001
2017-10-10T11:36:54.516820: step 7788, loss 0.0987202, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:54.680328: step 7789, loss 0.0222109, acc 1, learning_rate 0.0001
2017-10-10T11:36:54.848218: step 7790, loss 0.0992127, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:55.022921: step 7791, loss 0.0968819, acc 0.953125, learning_rate 0.0001
2017-10-10T11:36:55.187571: step 7792, loss 0.057848, acc 0.96875, learning_rate 0.0001
2017-10-10T11:36:55.352880: step 7793, loss 0.0309949, acc 1, learning_rate 0.0001
2017-10-10T11:36:55.517522: step 7794, loss 0.0648417, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:55.678565: step 7795, loss 0.014777, acc 1, learning_rate 0.0001
2017-10-10T11:36:55.843028: step 7796, loss 0.0733163, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:56.017574: step 7797, loss 0.0157788, acc 1, learning_rate 0.0001
2017-10-10T11:36:56.182971: step 7798, loss 0.0584158, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:56.346737: step 7799, loss 0.0446463, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:56.510964: step 7800, loss 0.0689017, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:36:56.956062: step 7800, loss 0.199164, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7800

2017-10-10T11:36:57.531410: step 7801, loss 0.033524, acc 1, learning_rate 0.0001
2017-10-10T11:36:57.695601: step 7802, loss 0.0166996, acc 1, learning_rate 0.0001
2017-10-10T11:36:57.862396: step 7803, loss 0.0326443, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:58.026557: step 7804, loss 0.0405842, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:58.194985: step 7805, loss 0.0410786, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:58.361330: step 7806, loss 0.0308234, acc 1, learning_rate 0.0001
2017-10-10T11:36:58.523273: step 7807, loss 0.0100256, acc 1, learning_rate 0.0001
2017-10-10T11:36:58.684847: step 7808, loss 0.0361796, acc 1, learning_rate 0.0001
2017-10-10T11:36:58.847916: step 7809, loss 0.0569131, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:59.010940: step 7810, loss 0.0217564, acc 1, learning_rate 0.0001
2017-10-10T11:36:59.171926: step 7811, loss 0.151337, acc 0.9375, learning_rate 0.0001
2017-10-10T11:36:59.334319: step 7812, loss 0.040484, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:59.498897: step 7813, loss 0.0149471, acc 1, learning_rate 0.0001
2017-10-10T11:36:59.661626: step 7814, loss 0.0116567, acc 1, learning_rate 0.0001
2017-10-10T11:36:59.822645: step 7815, loss 0.0679038, acc 0.984375, learning_rate 0.0001
2017-10-10T11:36:59.987912: step 7816, loss 0.0826174, acc 0.96875, learning_rate 0.0001
2017-10-10T11:37:00.149254: step 7817, loss 0.0451823, acc 0.953125, learning_rate 0.0001
2017-10-10T11:37:00.311012: step 7818, loss 0.0430457, acc 0.984375, learning_rate 0.0001
2017-10-10T11:37:00.470976: step 7819, loss 0.0442475, acc 0.96875, learning_rate 0.0001
2017-10-10T11:37:00.636419: step 7820, loss 0.118289, acc 0.9375, learning_rate 0.0001
2017-10-10T11:37:00.795596: step 7821, loss 0.0615967, acc 0.984375, learning_rate 0.0001
2017-10-10T11:37:00.960047: step 7822, loss 0.0468584, acc 0.984375, learning_rate 0.0001
2017-10-10T11:37:01.122477: step 7823, loss 0.0266975, acc 1, learning_rate 0.0001
2017-10-10T11:37:01.289418: step 7824, loss 0.0500459, acc 1, learning_rate 0.0001
2017-10-10T11:37:01.455937: step 7825, loss 0.0133252, acc 1, learning_rate 0.0001
2017-10-10T11:37:01.615086: step 7826, loss 0.0645316, acc 0.984375, learning_rate 0.0001
2017-10-10T11:37:01.778028: step 7827, loss 0.0552319, acc 1, learning_rate 0.0001
2017-10-10T11:37:01.950540: step 7828, loss 0.0404642, acc 0.984375, learning_rate 0.0001
2017-10-10T11:37:02.114751: step 7829, loss 0.0117564, acc 1, learning_rate 0.0001
2017-10-10T11:37:02.276141: step 7830, loss 0.0327683, acc 1, learning_rate 0.0001
2017-10-10T11:37:02.440177: step 7831, loss 0.0172638, acc 1, learning_rate 0.0001
2017-10-10T11:37:02.602159: step 7832, loss 0.0202679, acc 1, learning_rate 0.0001
2017-10-10T11:37:02.765375: step 7833, loss 0.0369306, acc 0.96875, learning_rate 0.0001
2017-10-10T11:37:02.931841: step 7834, loss 0.0345745, acc 1, learning_rate 0.0001
2017-10-10T11:37:03.097869: step 7835, loss 0.11172, acc 0.96875, learning_rate 0.0001
2017-10-10T11:37:03.259797: step 7836, loss 0.0436381, acc 1, learning_rate 0.0001
2017-10-10T11:37:03.422369: step 7837, loss 0.063842, acc 0.984375, learning_rate 0.0001
2017-10-10T11:37:03.587798: step 7838, loss 0.0774054, acc 0.96875, learning_rate 0.0001
2017-10-10T11:37:03.749178: step 7839, loss 0.0471106, acc 0.984375, learning_rate 0.0001
2017-10-10T11:37:03.885751: step 7840, loss 0.0806642, acc 0.980392, learning_rate 0.0001

Evaluation:
2017-10-10T11:37:04.329367: step 7840, loss 0.199407, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507651957/checkpoints/model-7840

