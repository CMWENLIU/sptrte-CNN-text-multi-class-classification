
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=64

Loading data...
6954
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
Vocabulary Size: 46116
Train/Dev split: 6259/695
Writing to /home/sheep/bigdata/runs/1507579543

2017-10-09T15:05:45.664934: step 1, loss 3.72015, acc 0.140625, learning_rate 0.005
2017-10-09T15:05:45.779256: step 2, loss 3.02611, acc 0.15625, learning_rate 0.00498
2017-10-09T15:05:45.886821: step 3, loss 2.32264, acc 0.328125, learning_rate 0.00496008
2017-10-09T15:05:45.992975: step 4, loss 2.97177, acc 0.234375, learning_rate 0.00494024
2017-10-09T15:05:46.095452: step 5, loss 2.95849, acc 0.25, learning_rate 0.00492049
2017-10-09T15:05:46.200084: step 6, loss 2.55532, acc 0.296875, learning_rate 0.00490081
2017-10-09T15:05:46.303099: step 7, loss 2.84732, acc 0.265625, learning_rate 0.00488121
2017-10-09T15:05:46.410315: step 8, loss 2.05117, acc 0.34375, learning_rate 0.0048617
2017-10-09T15:05:46.512121: step 9, loss 2.62098, acc 0.265625, learning_rate 0.00484226
2017-10-09T15:05:46.615595: step 10, loss 2.47716, acc 0.234375, learning_rate 0.00482291
2017-10-09T15:05:46.720330: step 11, loss 2.28228, acc 0.28125, learning_rate 0.00480363
2017-10-09T15:05:46.826090: step 12, loss 2.40969, acc 0.234375, learning_rate 0.00478443
2017-10-09T15:05:46.925816: step 13, loss 2.25482, acc 0.328125, learning_rate 0.00476531
2017-10-09T15:05:47.031967: step 14, loss 2.65128, acc 0.25, learning_rate 0.00474627
2017-10-09T15:05:47.134707: step 15, loss 2.33659, acc 0.265625, learning_rate 0.0047273
2017-10-09T15:05:47.239662: step 16, loss 2.26774, acc 0.3125, learning_rate 0.00470841
2017-10-09T15:05:47.346569: step 17, loss 2.113, acc 0.28125, learning_rate 0.0046896
2017-10-09T15:05:47.449733: step 18, loss 2.36402, acc 0.234375, learning_rate 0.00467087
2017-10-09T15:05:47.551169: step 19, loss 2.3759, acc 0.3125, learning_rate 0.00465221
2017-10-09T15:05:47.655364: step 20, loss 2.18101, acc 0.28125, learning_rate 0.00463363
2017-10-09T15:05:47.760375: step 21, loss 2.28328, acc 0.265625, learning_rate 0.00461513
2017-10-09T15:05:47.863916: step 22, loss 2.02333, acc 0.359375, learning_rate 0.0045967
2017-10-09T15:05:47.968251: step 23, loss 2.25958, acc 0.359375, learning_rate 0.00457834
2017-10-09T15:05:48.070839: step 24, loss 2.40309, acc 0.25, learning_rate 0.00456006
2017-10-09T15:05:48.173956: step 25, loss 2.5632, acc 0.28125, learning_rate 0.00454186
2017-10-09T15:05:48.279664: step 26, loss 2.25956, acc 0.390625, learning_rate 0.00452373
2017-10-09T15:05:48.377837: step 27, loss 2.12852, acc 0.28125, learning_rate 0.00450567
2017-10-09T15:05:48.480062: step 28, loss 2.26127, acc 0.3125, learning_rate 0.00448769
2017-10-09T15:05:48.584716: step 29, loss 1.99366, acc 0.265625, learning_rate 0.00446978
2017-10-09T15:05:48.688657: step 30, loss 1.92105, acc 0.359375, learning_rate 0.00445194
2017-10-09T15:05:48.795034: step 31, loss 2.57781, acc 0.171875, learning_rate 0.00443418
2017-10-09T15:05:48.903520: step 32, loss 2.19924, acc 0.296875, learning_rate 0.00441649
2017-10-09T15:05:49.008388: step 33, loss 2.14879, acc 0.296875, learning_rate 0.00439887
2017-10-09T15:05:49.112389: step 34, loss 1.9265, acc 0.3125, learning_rate 0.00438132
2017-10-09T15:05:49.217327: step 35, loss 2.08533, acc 0.25, learning_rate 0.00436385
2017-10-09T15:05:49.322638: step 36, loss 1.9272, acc 0.40625, learning_rate 0.00434644
2017-10-09T15:05:49.425715: step 37, loss 1.92444, acc 0.328125, learning_rate 0.00432911
2017-10-09T15:05:49.531945: step 38, loss 2.20897, acc 0.265625, learning_rate 0.00431185
2017-10-09T15:05:49.635746: step 39, loss 1.91016, acc 0.3125, learning_rate 0.00429465
2017-10-09T15:05:49.740369: step 40, loss 1.88907, acc 0.328125, learning_rate 0.00427753

Evaluation:
2017-10-09T15:05:50.017701: step 40, loss 1.37557, acc 0.417266

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-40

2017-10-09T15:05:50.603123: step 41, loss 1.76481, acc 0.453125, learning_rate 0.00426048
2017-10-09T15:05:50.707737: step 42, loss 1.79393, acc 0.34375, learning_rate 0.0042435
2017-10-09T15:05:50.812388: step 43, loss 1.90071, acc 0.296875, learning_rate 0.00422659
2017-10-09T15:05:50.919963: step 44, loss 1.56113, acc 0.4375, learning_rate 0.00420974
2017-10-09T15:05:51.024325: step 45, loss 1.79251, acc 0.390625, learning_rate 0.00419297
2017-10-09T15:05:51.130835: step 46, loss 1.85376, acc 0.296875, learning_rate 0.00417626
2017-10-09T15:05:51.236021: step 47, loss 1.83922, acc 0.328125, learning_rate 0.00415962
2017-10-09T15:05:51.341738: step 48, loss 1.67516, acc 0.390625, learning_rate 0.00414305
2017-10-09T15:05:51.445050: step 49, loss 1.66771, acc 0.375, learning_rate 0.00412655
2017-10-09T15:05:51.552589: step 50, loss 1.62703, acc 0.34375, learning_rate 0.00411011
2017-10-09T15:05:51.659844: step 51, loss 1.81214, acc 0.34375, learning_rate 0.00409375
2017-10-09T15:05:51.764057: step 52, loss 1.79671, acc 0.359375, learning_rate 0.00407744
2017-10-09T15:05:51.871463: step 53, loss 1.66589, acc 0.421875, learning_rate 0.00406121
2017-10-09T15:05:51.975300: step 54, loss 1.65315, acc 0.390625, learning_rate 0.00404504
2017-10-09T15:05:52.079963: step 55, loss 1.51467, acc 0.484375, learning_rate 0.00402894
2017-10-09T15:05:52.186627: step 56, loss 1.78755, acc 0.359375, learning_rate 0.0040129
2017-10-09T15:05:52.287784: step 57, loss 1.5369, acc 0.421875, learning_rate 0.00399693
2017-10-09T15:05:52.392374: step 58, loss 1.57048, acc 0.390625, learning_rate 0.00398102
2017-10-09T15:05:52.495987: step 59, loss 1.46834, acc 0.5, learning_rate 0.00396518
2017-10-09T15:05:52.601093: step 60, loss 1.65228, acc 0.359375, learning_rate 0.00394941
2017-10-09T15:05:52.708710: step 61, loss 1.53506, acc 0.359375, learning_rate 0.00393369
2017-10-09T15:05:52.813071: step 62, loss 1.77925, acc 0.328125, learning_rate 0.00391804
2017-10-09T15:05:52.924127: step 63, loss 1.60114, acc 0.390625, learning_rate 0.00390246
2017-10-09T15:05:53.030235: step 64, loss 1.42169, acc 0.4375, learning_rate 0.00388694
2017-10-09T15:05:53.136568: step 65, loss 1.50976, acc 0.46875, learning_rate 0.00387148
2017-10-09T15:05:53.243667: step 66, loss 1.46018, acc 0.390625, learning_rate 0.00385609
2017-10-09T15:05:53.347267: step 67, loss 1.65004, acc 0.375, learning_rate 0.00384076
2017-10-09T15:05:53.451923: step 68, loss 1.37057, acc 0.5, learning_rate 0.00382549
2017-10-09T15:05:53.560798: step 69, loss 1.76156, acc 0.359375, learning_rate 0.00381028
2017-10-09T15:05:53.667514: step 70, loss 1.6015, acc 0.421875, learning_rate 0.00379514
2017-10-09T15:05:53.769430: step 71, loss 1.65394, acc 0.40625, learning_rate 0.00378005
2017-10-09T15:05:53.874799: step 72, loss 1.57521, acc 0.4375, learning_rate 0.00376503
2017-10-09T15:05:53.977296: step 73, loss 1.58513, acc 0.421875, learning_rate 0.00375007
2017-10-09T15:05:54.081443: step 74, loss 1.54453, acc 0.375, learning_rate 0.00373517
2017-10-09T15:05:54.184903: step 75, loss 1.43178, acc 0.40625, learning_rate 0.00372034
2017-10-09T15:05:54.290231: step 76, loss 1.46514, acc 0.421875, learning_rate 0.00370556
2017-10-09T15:05:54.391763: step 77, loss 1.3431, acc 0.453125, learning_rate 0.00369084
2017-10-09T15:05:54.497517: step 78, loss 1.25184, acc 0.53125, learning_rate 0.00367619
2017-10-09T15:05:54.605330: step 79, loss 1.36561, acc 0.453125, learning_rate 0.00366159
2017-10-09T15:05:54.710089: step 80, loss 1.19234, acc 0.65625, learning_rate 0.00364705

Evaluation:
2017-10-09T15:05:54.964192: step 80, loss 1.1158, acc 0.569784

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-80

2017-10-09T15:05:55.546878: step 81, loss 1.29918, acc 0.546875, learning_rate 0.00363257
2017-10-09T15:05:55.649250: step 82, loss 1.49263, acc 0.4375, learning_rate 0.00361815
2017-10-09T15:05:55.753655: step 83, loss 1.4429, acc 0.46875, learning_rate 0.00360379
2017-10-09T15:05:55.859531: step 84, loss 1.22462, acc 0.59375, learning_rate 0.00358949
2017-10-09T15:05:55.962237: step 85, loss 1.48404, acc 0.546875, learning_rate 0.00357525
2017-10-09T15:05:56.067080: step 86, loss 1.40673, acc 0.484375, learning_rate 0.00356106
2017-10-09T15:05:56.171346: step 87, loss 1.13971, acc 0.5625, learning_rate 0.00354694
2017-10-09T15:05:56.276954: step 88, loss 1.49308, acc 0.453125, learning_rate 0.00353287
2017-10-09T15:05:56.378775: step 89, loss 1.42542, acc 0.5, learning_rate 0.00351885
2017-10-09T15:05:56.483562: step 90, loss 1.37143, acc 0.515625, learning_rate 0.0035049
2017-10-09T15:05:56.588804: step 91, loss 1.08469, acc 0.625, learning_rate 0.003491
2017-10-09T15:05:56.694319: step 92, loss 1.3651, acc 0.453125, learning_rate 0.00347716
2017-10-09T15:05:56.800434: step 93, loss 1.24376, acc 0.46875, learning_rate 0.00346338
2017-10-09T15:05:56.904987: step 94, loss 1.38107, acc 0.484375, learning_rate 0.00344965
2017-10-09T15:05:57.009418: step 95, loss 1.30098, acc 0.484375, learning_rate 0.00343597
2017-10-09T15:05:57.113646: step 96, loss 1.19105, acc 0.515625, learning_rate 0.00342236
2017-10-09T15:05:57.219091: step 97, loss 1.41394, acc 0.484375, learning_rate 0.0034088
2017-10-09T15:05:57.309157: step 98, loss 1.30265, acc 0.392157, learning_rate 0.00339529
2017-10-09T15:05:57.411232: step 99, loss 1.29347, acc 0.46875, learning_rate 0.00338184
2017-10-09T15:05:57.517757: step 100, loss 1.05767, acc 0.578125, learning_rate 0.00336844
2017-10-09T15:05:57.621891: step 101, loss 1.26805, acc 0.609375, learning_rate 0.0033551
2017-10-09T15:05:57.727335: step 102, loss 1.22051, acc 0.53125, learning_rate 0.00334182
2017-10-09T15:05:57.831577: step 103, loss 1.09288, acc 0.546875, learning_rate 0.00332858
2017-10-09T15:05:57.939841: step 104, loss 1.15381, acc 0.59375, learning_rate 0.00331541
2017-10-09T15:05:58.044501: step 105, loss 1.06194, acc 0.578125, learning_rate 0.00330228
2017-10-09T15:05:58.148773: step 106, loss 1.26953, acc 0.515625, learning_rate 0.00328921
2017-10-09T15:05:58.253113: step 107, loss 1.28841, acc 0.5625, learning_rate 0.00327619
2017-10-09T15:05:58.358153: step 108, loss 1.174, acc 0.625, learning_rate 0.00326323
2017-10-09T15:05:58.462312: step 109, loss 1.20315, acc 0.515625, learning_rate 0.00325032
2017-10-09T15:05:58.569222: step 110, loss 0.825871, acc 0.734375, learning_rate 0.00323746
2017-10-09T15:05:58.674167: step 111, loss 1.09467, acc 0.59375, learning_rate 0.00322465
2017-10-09T15:05:58.782844: step 112, loss 1.36441, acc 0.546875, learning_rate 0.0032119
2017-10-09T15:05:58.889123: step 113, loss 1.12355, acc 0.546875, learning_rate 0.0031992
2017-10-09T15:05:58.996393: step 114, loss 0.977759, acc 0.640625, learning_rate 0.00318655
2017-10-09T15:05:59.099538: step 115, loss 1.18419, acc 0.5625, learning_rate 0.00317395
2017-10-09T15:05:59.199275: step 116, loss 1.12428, acc 0.515625, learning_rate 0.0031614
2017-10-09T15:05:59.303766: step 117, loss 1.37088, acc 0.484375, learning_rate 0.0031489
2017-10-09T15:05:59.411311: step 118, loss 0.989742, acc 0.6875, learning_rate 0.00313646
2017-10-09T15:05:59.518219: step 119, loss 1.50276, acc 0.375, learning_rate 0.00312407
2017-10-09T15:05:59.624106: step 120, loss 1.16809, acc 0.59375, learning_rate 0.00311172

Evaluation:
2017-10-09T15:05:59.887274: step 120, loss 0.855624, acc 0.719424

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-120

2017-10-09T15:06:00.469195: step 121, loss 1.32993, acc 0.4375, learning_rate 0.00309943
2017-10-09T15:06:00.574063: step 122, loss 1.03234, acc 0.578125, learning_rate 0.00308719
2017-10-09T15:06:00.676142: step 123, loss 1.05215, acc 0.625, learning_rate 0.00307499
2017-10-09T15:06:00.782338: step 124, loss 0.883655, acc 0.671875, learning_rate 0.00306285
2017-10-09T15:06:00.888937: step 125, loss 1.26969, acc 0.5, learning_rate 0.00305076
2017-10-09T15:06:00.997722: step 126, loss 1.06939, acc 0.59375, learning_rate 0.00303871
2017-10-09T15:06:01.102418: step 127, loss 1.10559, acc 0.703125, learning_rate 0.00302672
2017-10-09T15:06:01.209371: step 128, loss 0.993661, acc 0.59375, learning_rate 0.00301477
2017-10-09T15:06:01.319273: step 129, loss 1.06221, acc 0.609375, learning_rate 0.00300287
2017-10-09T15:06:01.426227: step 130, loss 1.48643, acc 0.421875, learning_rate 0.00299102
2017-10-09T15:06:01.533679: step 131, loss 1.10827, acc 0.53125, learning_rate 0.00297922
2017-10-09T15:06:01.639912: step 132, loss 0.943187, acc 0.65625, learning_rate 0.00296747
2017-10-09T15:06:01.746309: step 133, loss 1.17724, acc 0.59375, learning_rate 0.00295577
2017-10-09T15:06:01.853179: step 134, loss 1.11168, acc 0.515625, learning_rate 0.00294411
2017-10-09T15:06:01.958996: step 135, loss 1.14391, acc 0.59375, learning_rate 0.0029325
2017-10-09T15:06:02.061643: step 136, loss 1.46438, acc 0.453125, learning_rate 0.00292094
2017-10-09T15:06:02.163378: step 137, loss 1.00657, acc 0.609375, learning_rate 0.00290943
2017-10-09T15:06:02.267607: step 138, loss 1.10068, acc 0.609375, learning_rate 0.00289796
2017-10-09T15:06:02.373420: step 139, loss 0.848746, acc 0.6875, learning_rate 0.00288654
2017-10-09T15:06:02.474605: step 140, loss 1.0405, acc 0.578125, learning_rate 0.00287516
2017-10-09T15:06:02.582150: step 141, loss 1.19003, acc 0.484375, learning_rate 0.00286384
2017-10-09T15:06:02.686520: step 142, loss 1.05645, acc 0.578125, learning_rate 0.00285256
2017-10-09T15:06:02.787838: step 143, loss 0.888606, acc 0.6875, learning_rate 0.00284132
2017-10-09T15:06:02.895182: step 144, loss 1.17824, acc 0.5, learning_rate 0.00283013
2017-10-09T15:06:02.998678: step 145, loss 0.879133, acc 0.765625, learning_rate 0.00281899
2017-10-09T15:06:03.106821: step 146, loss 1.21113, acc 0.53125, learning_rate 0.00280789
2017-10-09T15:06:03.211368: step 147, loss 0.857425, acc 0.71875, learning_rate 0.00279684
2017-10-09T15:06:03.319667: step 148, loss 0.985599, acc 0.6875, learning_rate 0.00278583
2017-10-09T15:06:03.419535: step 149, loss 0.909078, acc 0.65625, learning_rate 0.00277486
2017-10-09T15:06:03.526013: step 150, loss 1.06386, acc 0.609375, learning_rate 0.00276395
2017-10-09T15:06:03.630535: step 151, loss 0.931241, acc 0.609375, learning_rate 0.00275307
2017-10-09T15:06:03.735810: step 152, loss 0.887965, acc 0.65625, learning_rate 0.00274224
2017-10-09T15:06:03.841014: step 153, loss 1.04308, acc 0.625, learning_rate 0.00273146
2017-10-09T15:06:03.948672: step 154, loss 0.982586, acc 0.671875, learning_rate 0.00272072
2017-10-09T15:06:04.054189: step 155, loss 0.907985, acc 0.65625, learning_rate 0.00271002
2017-10-09T15:06:04.153866: step 156, loss 1.05479, acc 0.59375, learning_rate 0.00269937
2017-10-09T15:06:04.259878: step 157, loss 0.99842, acc 0.640625, learning_rate 0.00268876
2017-10-09T15:06:04.360531: step 158, loss 0.891812, acc 0.625, learning_rate 0.00267819
2017-10-09T15:06:04.466161: step 159, loss 0.873389, acc 0.65625, learning_rate 0.00266767
2017-10-09T15:06:04.568233: step 160, loss 1.02862, acc 0.625, learning_rate 0.00265719

Evaluation:
2017-10-09T15:06:04.824751: step 160, loss 0.799906, acc 0.716547

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-160

2017-10-09T15:06:05.411581: step 161, loss 0.809239, acc 0.734375, learning_rate 0.00264675
2017-10-09T15:06:05.513785: step 162, loss 0.871012, acc 0.703125, learning_rate 0.00263635
2017-10-09T15:06:05.619742: step 163, loss 1.04787, acc 0.609375, learning_rate 0.002626
2017-10-09T15:06:05.726126: step 164, loss 0.998968, acc 0.609375, learning_rate 0.00261569
2017-10-09T15:06:05.832245: step 165, loss 0.952651, acc 0.671875, learning_rate 0.00260542
2017-10-09T15:06:05.939779: step 166, loss 0.985104, acc 0.53125, learning_rate 0.0025952
2017-10-09T15:06:06.046257: step 167, loss 1.11169, acc 0.5625, learning_rate 0.00258501
2017-10-09T15:06:06.149761: step 168, loss 1.04126, acc 0.65625, learning_rate 0.00257487
2017-10-09T15:06:06.256084: step 169, loss 0.742023, acc 0.78125, learning_rate 0.00256477
2017-10-09T15:06:06.360720: step 170, loss 0.929265, acc 0.640625, learning_rate 0.0025547
2017-10-09T15:06:06.466907: step 171, loss 1.00477, acc 0.59375, learning_rate 0.00254469
2017-10-09T15:06:06.571194: step 172, loss 0.733967, acc 0.75, learning_rate 0.00253471
2017-10-09T15:06:06.676238: step 173, loss 1.08343, acc 0.53125, learning_rate 0.00252477
2017-10-09T15:06:06.781500: step 174, loss 1.00048, acc 0.65625, learning_rate 0.00251487
2017-10-09T15:06:06.887732: step 175, loss 0.931377, acc 0.671875, learning_rate 0.00250501
2017-10-09T15:06:06.999820: step 176, loss 0.994864, acc 0.546875, learning_rate 0.0024952
2017-10-09T15:06:07.104882: step 177, loss 0.87803, acc 0.65625, learning_rate 0.00248542
2017-10-09T15:06:07.205499: step 178, loss 0.837133, acc 0.734375, learning_rate 0.00247568
2017-10-09T15:06:07.308534: step 179, loss 0.878511, acc 0.703125, learning_rate 0.00246599
2017-10-09T15:06:07.413161: step 180, loss 0.965717, acc 0.65625, learning_rate 0.00245633
2017-10-09T15:06:07.518503: step 181, loss 0.713283, acc 0.78125, learning_rate 0.00244671
2017-10-09T15:06:07.622359: step 182, loss 1.08855, acc 0.59375, learning_rate 0.00243713
2017-10-09T15:06:07.721773: step 183, loss 0.89973, acc 0.625, learning_rate 0.00242759
2017-10-09T15:06:07.826074: step 184, loss 0.861206, acc 0.625, learning_rate 0.00241809
2017-10-09T15:06:07.933304: step 185, loss 1.05553, acc 0.578125, learning_rate 0.00240863
2017-10-09T15:06:08.040969: step 186, loss 1.19441, acc 0.484375, learning_rate 0.00239921
2017-10-09T15:06:08.140467: step 187, loss 1.07122, acc 0.609375, learning_rate 0.00238982
2017-10-09T15:06:08.238987: step 188, loss 0.837819, acc 0.703125, learning_rate 0.00238048
2017-10-09T15:06:08.340677: step 189, loss 0.854592, acc 0.6875, learning_rate 0.00237117
2017-10-09T15:06:08.442447: step 190, loss 1.15623, acc 0.53125, learning_rate 0.0023619
2017-10-09T15:06:08.544305: step 191, loss 0.780477, acc 0.671875, learning_rate 0.00235267
2017-10-09T15:06:08.650388: step 192, loss 0.882693, acc 0.6875, learning_rate 0.00234347
2017-10-09T15:06:08.755687: step 193, loss 1.06736, acc 0.6875, learning_rate 0.00233431
2017-10-09T15:06:08.863194: step 194, loss 0.802576, acc 0.71875, learning_rate 0.00232519
2017-10-09T15:06:08.968287: step 195, loss 0.896524, acc 0.625, learning_rate 0.00231611
2017-10-09T15:06:09.055290: step 196, loss 0.988998, acc 0.666667, learning_rate 0.00230707
2017-10-09T15:06:09.160376: step 197, loss 0.855913, acc 0.6875, learning_rate 0.00229806
2017-10-09T15:06:09.264421: step 198, loss 0.979115, acc 0.65625, learning_rate 0.00228908
2017-10-09T15:06:09.370320: step 199, loss 0.782387, acc 0.71875, learning_rate 0.00228015
2017-10-09T15:06:09.476715: step 200, loss 0.864185, acc 0.65625, learning_rate 0.00227125

Evaluation:
2017-10-09T15:06:09.740571: step 200, loss 0.673324, acc 0.779856

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-200

2017-10-09T15:06:10.325145: step 201, loss 0.813587, acc 0.671875, learning_rate 0.00226239
2017-10-09T15:06:10.431820: step 202, loss 0.798064, acc 0.75, learning_rate 0.00225356
2017-10-09T15:06:10.536491: step 203, loss 0.731898, acc 0.734375, learning_rate 0.00224477
2017-10-09T15:06:10.642384: step 204, loss 0.768404, acc 0.765625, learning_rate 0.00223602
2017-10-09T15:06:10.744667: step 205, loss 0.772271, acc 0.71875, learning_rate 0.0022273
2017-10-09T15:06:10.848398: step 206, loss 0.835517, acc 0.6875, learning_rate 0.00221862
2017-10-09T15:06:10.951457: step 207, loss 0.68327, acc 0.75, learning_rate 0.00220997
2017-10-09T15:06:11.058509: step 208, loss 0.702164, acc 0.765625, learning_rate 0.00220136
2017-10-09T15:06:11.162596: step 209, loss 0.866135, acc 0.65625, learning_rate 0.00219278
2017-10-09T15:06:11.268556: step 210, loss 0.752785, acc 0.671875, learning_rate 0.00218424
2017-10-09T15:06:11.374711: step 211, loss 0.828237, acc 0.71875, learning_rate 0.00217573
2017-10-09T15:06:11.480201: step 212, loss 0.736814, acc 0.765625, learning_rate 0.00216726
2017-10-09T15:06:11.581697: step 213, loss 1.11783, acc 0.546875, learning_rate 0.00215882
2017-10-09T15:06:11.687012: step 214, loss 0.909757, acc 0.671875, learning_rate 0.00215041
2017-10-09T15:06:11.794620: step 215, loss 0.7887, acc 0.703125, learning_rate 0.00214204
2017-10-09T15:06:11.896573: step 216, loss 0.718893, acc 0.78125, learning_rate 0.00213371
2017-10-09T15:06:12.000374: step 217, loss 0.825945, acc 0.734375, learning_rate 0.00212541
2017-10-09T15:06:12.105534: step 218, loss 0.792363, acc 0.71875, learning_rate 0.00211714
2017-10-09T15:06:12.209897: step 219, loss 0.863056, acc 0.625, learning_rate 0.00210891
2017-10-09T15:06:12.315721: step 220, loss 0.874292, acc 0.6875, learning_rate 0.00210071
2017-10-09T15:06:12.418189: step 221, loss 0.870032, acc 0.71875, learning_rate 0.00209254
2017-10-09T15:06:12.523392: step 222, loss 0.627709, acc 0.75, learning_rate 0.00208441
2017-10-09T15:06:12.627579: step 223, loss 0.814064, acc 0.65625, learning_rate 0.00207631
2017-10-09T15:06:12.732158: step 224, loss 0.736789, acc 0.765625, learning_rate 0.00206824
2017-10-09T15:06:12.835586: step 225, loss 0.702626, acc 0.75, learning_rate 0.00206021
2017-10-09T15:06:12.940457: step 226, loss 0.720594, acc 0.734375, learning_rate 0.00205221
2017-10-09T15:06:13.045881: step 227, loss 0.571167, acc 0.828125, learning_rate 0.00204424
2017-10-09T15:06:13.150884: step 228, loss 0.684995, acc 0.78125, learning_rate 0.0020363
2017-10-09T15:06:13.255258: step 229, loss 0.712534, acc 0.71875, learning_rate 0.0020284
2017-10-09T15:06:13.362933: step 230, loss 0.737324, acc 0.75, learning_rate 0.00202053
2017-10-09T15:06:13.464607: step 231, loss 0.823302, acc 0.734375, learning_rate 0.00201269
2017-10-09T15:06:13.573104: step 232, loss 0.766295, acc 0.671875, learning_rate 0.00200488
2017-10-09T15:06:13.678713: step 233, loss 0.819746, acc 0.6875, learning_rate 0.00199711
2017-10-09T15:06:13.784581: step 234, loss 0.665912, acc 0.78125, learning_rate 0.00198936
2017-10-09T15:06:13.888785: step 235, loss 0.539203, acc 0.828125, learning_rate 0.00198165
2017-10-09T15:06:13.995294: step 236, loss 0.764057, acc 0.71875, learning_rate 0.00197397
2017-10-09T15:06:14.103132: step 237, loss 0.900853, acc 0.640625, learning_rate 0.00196632
2017-10-09T15:06:14.209398: step 238, loss 0.76117, acc 0.71875, learning_rate 0.0019587
2017-10-09T15:06:14.312958: step 239, loss 0.75004, acc 0.71875, learning_rate 0.00195112
2017-10-09T15:06:14.419720: step 240, loss 0.720538, acc 0.71875, learning_rate 0.00194356

Evaluation:
2017-10-09T15:06:14.674462: step 240, loss 0.633853, acc 0.779856

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-240

2017-10-09T15:06:15.271189: step 241, loss 0.699774, acc 0.734375, learning_rate 0.00193604
2017-10-09T15:06:15.376454: step 242, loss 0.733628, acc 0.734375, learning_rate 0.00192854
2017-10-09T15:06:15.482885: step 243, loss 0.856714, acc 0.734375, learning_rate 0.00192108
2017-10-09T15:06:15.588947: step 244, loss 0.80999, acc 0.71875, learning_rate 0.00191364
2017-10-09T15:06:15.698296: step 245, loss 0.791801, acc 0.71875, learning_rate 0.00190624
2017-10-09T15:06:15.801605: step 246, loss 0.753482, acc 0.765625, learning_rate 0.00189887
2017-10-09T15:06:15.904359: step 247, loss 0.853102, acc 0.640625, learning_rate 0.00189153
2017-10-09T15:06:16.010877: step 248, loss 0.801977, acc 0.625, learning_rate 0.00188421
2017-10-09T15:06:16.117107: step 249, loss 0.746221, acc 0.75, learning_rate 0.00187693
2017-10-09T15:06:16.220264: step 250, loss 0.748918, acc 0.640625, learning_rate 0.00186968
2017-10-09T15:06:16.326793: step 251, loss 0.871432, acc 0.671875, learning_rate 0.00186245
2017-10-09T15:06:16.434732: step 252, loss 1.02518, acc 0.640625, learning_rate 0.00185526
2017-10-09T15:06:16.540020: step 253, loss 0.689493, acc 0.765625, learning_rate 0.0018481
2017-10-09T15:06:16.643775: step 254, loss 0.709049, acc 0.78125, learning_rate 0.00184096
2017-10-09T15:06:16.749440: step 255, loss 0.653181, acc 0.796875, learning_rate 0.00183385
2017-10-09T15:06:16.857920: step 256, loss 0.917922, acc 0.640625, learning_rate 0.00182678
2017-10-09T15:06:16.963101: step 257, loss 0.705294, acc 0.71875, learning_rate 0.00181973
2017-10-09T15:06:17.069386: step 258, loss 0.639545, acc 0.734375, learning_rate 0.00181271
2017-10-09T15:06:17.174413: step 259, loss 0.608067, acc 0.78125, learning_rate 0.00180572
2017-10-09T15:06:17.277923: step 260, loss 0.782642, acc 0.703125, learning_rate 0.00179876
2017-10-09T15:06:17.382961: step 261, loss 0.873954, acc 0.65625, learning_rate 0.00179182
2017-10-09T15:06:17.488204: step 262, loss 0.856388, acc 0.671875, learning_rate 0.00178492
2017-10-09T15:06:17.587765: step 263, loss 0.743511, acc 0.75, learning_rate 0.00177804
2017-10-09T15:06:17.689008: step 264, loss 0.842852, acc 0.71875, learning_rate 0.00177119
2017-10-09T15:06:17.794052: step 265, loss 0.63334, acc 0.765625, learning_rate 0.00176437
2017-10-09T15:06:17.902142: step 266, loss 0.717813, acc 0.71875, learning_rate 0.00175758
2017-10-09T15:06:18.003685: step 267, loss 0.852023, acc 0.703125, learning_rate 0.00175081
2017-10-09T15:06:18.112663: step 268, loss 0.731965, acc 0.703125, learning_rate 0.00174407
2017-10-09T15:06:18.220465: step 269, loss 0.606913, acc 0.734375, learning_rate 0.00173736
2017-10-09T15:06:18.325567: step 270, loss 0.597429, acc 0.796875, learning_rate 0.00173068
2017-10-09T15:06:18.430135: step 271, loss 0.649531, acc 0.75, learning_rate 0.00172402
2017-10-09T15:06:18.535341: step 272, loss 0.813322, acc 0.71875, learning_rate 0.00171739
2017-10-09T15:06:18.635980: step 273, loss 0.699541, acc 0.75, learning_rate 0.00171079
2017-10-09T15:06:18.741904: step 274, loss 0.71586, acc 0.765625, learning_rate 0.00170422
2017-10-09T15:06:18.848768: step 275, loss 0.757407, acc 0.734375, learning_rate 0.00169767
2017-10-09T15:06:18.950801: step 276, loss 0.657994, acc 0.765625, learning_rate 0.00169115
2017-10-09T15:06:19.054006: step 277, loss 0.883304, acc 0.671875, learning_rate 0.00168465
2017-10-09T15:06:19.157579: step 278, loss 0.715916, acc 0.734375, learning_rate 0.00167818
2017-10-09T15:06:19.262836: step 279, loss 0.541235, acc 0.828125, learning_rate 0.00167174
2017-10-09T15:06:19.370708: step 280, loss 0.803899, acc 0.6875, learning_rate 0.00166533

Evaluation:
2017-10-09T15:06:19.616529: step 280, loss 0.576324, acc 0.81295

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-280

2017-10-09T15:06:20.261563: step 281, loss 0.830427, acc 0.703125, learning_rate 0.00165894
2017-10-09T15:06:20.363531: step 282, loss 0.862907, acc 0.6875, learning_rate 0.00165257
2017-10-09T15:06:20.468673: step 283, loss 0.685195, acc 0.828125, learning_rate 0.00164624
2017-10-09T15:06:20.574254: step 284, loss 0.504971, acc 0.828125, learning_rate 0.00163993
2017-10-09T15:06:20.678053: step 285, loss 0.536149, acc 0.796875, learning_rate 0.00163364
2017-10-09T15:06:20.785212: step 286, loss 0.609116, acc 0.734375, learning_rate 0.00162738
2017-10-09T15:06:20.891908: step 287, loss 0.596494, acc 0.828125, learning_rate 0.00162115
2017-10-09T15:06:20.996664: step 288, loss 0.715739, acc 0.734375, learning_rate 0.00161494
2017-10-09T15:06:21.101033: step 289, loss 0.766961, acc 0.703125, learning_rate 0.00160875
2017-10-09T15:06:21.203586: step 290, loss 0.611368, acc 0.78125, learning_rate 0.00160259
2017-10-09T15:06:21.311335: step 291, loss 0.511164, acc 0.8125, learning_rate 0.00159646
2017-10-09T15:06:21.416349: step 292, loss 0.663277, acc 0.734375, learning_rate 0.00159035
2017-10-09T15:06:21.520181: step 293, loss 0.661534, acc 0.75, learning_rate 0.00158427
2017-10-09T15:06:21.608972: step 294, loss 0.717178, acc 0.686275, learning_rate 0.00157821
2017-10-09T15:06:21.714615: step 295, loss 0.541745, acc 0.796875, learning_rate 0.00157218
2017-10-09T15:06:21.820791: step 296, loss 0.632091, acc 0.734375, learning_rate 0.00156617
2017-10-09T15:06:21.927227: step 297, loss 0.730251, acc 0.703125, learning_rate 0.00156018
2017-10-09T15:06:22.032691: step 298, loss 0.756906, acc 0.703125, learning_rate 0.00155422
2017-10-09T15:06:22.136616: step 299, loss 0.439137, acc 0.828125, learning_rate 0.00154829
2017-10-09T15:06:22.247170: step 300, loss 0.613827, acc 0.765625, learning_rate 0.00154238
2017-10-09T15:06:22.352271: step 301, loss 0.786743, acc 0.734375, learning_rate 0.00153649
2017-10-09T15:06:22.457695: step 302, loss 0.443029, acc 0.84375, learning_rate 0.00153063
2017-10-09T15:06:22.565004: step 303, loss 0.749596, acc 0.703125, learning_rate 0.00152479
2017-10-09T15:06:22.672142: step 304, loss 0.73612, acc 0.75, learning_rate 0.00151897
2017-10-09T15:06:22.781283: step 305, loss 0.62651, acc 0.828125, learning_rate 0.00151318
2017-10-09T15:06:22.884971: step 306, loss 0.645264, acc 0.75, learning_rate 0.00150741
2017-10-09T15:06:22.992725: step 307, loss 0.604731, acc 0.75, learning_rate 0.00150167
2017-10-09T15:06:23.097810: step 308, loss 0.731619, acc 0.703125, learning_rate 0.00149594
2017-10-09T15:06:23.204033: step 309, loss 0.666128, acc 0.8125, learning_rate 0.00149025
2017-10-09T15:06:23.309134: step 310, loss 0.509433, acc 0.859375, learning_rate 0.00148457
2017-10-09T15:06:23.412825: step 311, loss 0.627259, acc 0.8125, learning_rate 0.00147892
2017-10-09T15:06:23.517513: step 312, loss 0.688323, acc 0.734375, learning_rate 0.00147329
2017-10-09T15:06:23.623820: step 313, loss 0.528931, acc 0.8125, learning_rate 0.00146769
2017-10-09T15:06:23.730382: step 314, loss 0.659035, acc 0.734375, learning_rate 0.0014621
2017-10-09T15:06:23.829088: step 315, loss 0.582938, acc 0.8125, learning_rate 0.00145654
2017-10-09T15:06:23.936479: step 316, loss 0.401209, acc 0.890625, learning_rate 0.00145101
2017-10-09T15:06:24.039363: step 317, loss 0.600149, acc 0.75, learning_rate 0.00144549
2017-10-09T15:06:24.145820: step 318, loss 0.860938, acc 0.71875, learning_rate 0.00144
2017-10-09T15:06:24.248369: step 319, loss 0.596505, acc 0.796875, learning_rate 0.00143453
2017-10-09T15:06:24.356752: step 320, loss 0.632291, acc 0.78125, learning_rate 0.00142908

Evaluation:
2017-10-09T15:06:24.606703: step 320, loss 0.536312, acc 0.833094

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-320

2017-10-09T15:06:25.124741: step 321, loss 0.656448, acc 0.765625, learning_rate 0.00142366
2017-10-09T15:06:25.228459: step 322, loss 0.526087, acc 0.78125, learning_rate 0.00141826
2017-10-09T15:06:25.336589: step 323, loss 0.635781, acc 0.734375, learning_rate 0.00141288
2017-10-09T15:06:25.441897: step 324, loss 0.560955, acc 0.796875, learning_rate 0.00140752
2017-10-09T15:06:25.548539: step 325, loss 0.742967, acc 0.78125, learning_rate 0.00140218
2017-10-09T15:06:25.652064: step 326, loss 0.577501, acc 0.71875, learning_rate 0.00139686
2017-10-09T15:06:25.757989: step 327, loss 0.561509, acc 0.734375, learning_rate 0.00139157
2017-10-09T15:06:25.864681: step 328, loss 0.55082, acc 0.796875, learning_rate 0.0013863
2017-10-09T15:06:25.969264: step 329, loss 0.883996, acc 0.6875, learning_rate 0.00138105
2017-10-09T15:06:26.077737: step 330, loss 0.500142, acc 0.796875, learning_rate 0.00137582
2017-10-09T15:06:26.179611: step 331, loss 0.668723, acc 0.71875, learning_rate 0.00137061
2017-10-09T15:06:26.284182: step 332, loss 0.61134, acc 0.75, learning_rate 0.00136543
2017-10-09T15:06:26.389952: step 333, loss 0.65863, acc 0.734375, learning_rate 0.00136026
2017-10-09T15:06:26.499498: step 334, loss 0.426018, acc 0.890625, learning_rate 0.00135512
2017-10-09T15:06:26.605159: step 335, loss 0.688255, acc 0.734375, learning_rate 0.00134999
2017-10-09T15:06:26.706167: step 336, loss 0.51798, acc 0.796875, learning_rate 0.00134489
2017-10-09T15:06:26.809774: step 337, loss 0.671492, acc 0.734375, learning_rate 0.00133981
2017-10-09T15:06:26.918693: step 338, loss 0.600446, acc 0.828125, learning_rate 0.00133475
2017-10-09T15:06:27.024419: step 339, loss 0.541457, acc 0.796875, learning_rate 0.00132971
2017-10-09T15:06:27.127257: step 340, loss 0.614881, acc 0.78125, learning_rate 0.00132469
2017-10-09T15:06:27.233104: step 341, loss 0.572809, acc 0.84375, learning_rate 0.00131969
2017-10-09T15:06:27.337965: step 342, loss 0.554584, acc 0.78125, learning_rate 0.00131471
2017-10-09T15:06:27.442428: step 343, loss 0.831977, acc 0.640625, learning_rate 0.00130975
2017-10-09T15:06:27.548725: step 344, loss 0.455467, acc 0.84375, learning_rate 0.00130482
2017-10-09T15:06:27.652407: step 345, loss 0.623183, acc 0.75, learning_rate 0.0012999
2017-10-09T15:06:27.757286: step 346, loss 0.811644, acc 0.609375, learning_rate 0.001295
2017-10-09T15:06:27.864819: step 347, loss 0.724321, acc 0.6875, learning_rate 0.00129012
2017-10-09T15:06:27.969405: step 348, loss 0.578873, acc 0.8125, learning_rate 0.00128527
2017-10-09T15:06:28.075183: step 349, loss 0.541982, acc 0.859375, learning_rate 0.00128043
2017-10-09T15:06:28.182808: step 350, loss 0.747276, acc 0.734375, learning_rate 0.00127561
2017-10-09T15:06:28.287806: step 351, loss 0.545052, acc 0.828125, learning_rate 0.00127081
2017-10-09T15:06:28.392009: step 352, loss 0.700315, acc 0.71875, learning_rate 0.00126603
2017-10-09T15:06:28.500376: step 353, loss 0.583665, acc 0.765625, learning_rate 0.00126127
2017-10-09T15:06:28.605046: step 354, loss 0.562399, acc 0.8125, learning_rate 0.00125653
2017-10-09T15:06:28.709534: step 355, loss 0.458287, acc 0.828125, learning_rate 0.00125181
2017-10-09T15:06:28.817728: step 356, loss 0.606247, acc 0.84375, learning_rate 0.00124711
2017-10-09T15:06:28.918790: step 357, loss 0.585195, acc 0.8125, learning_rate 0.00124243
2017-10-09T15:06:29.028414: step 358, loss 0.593559, acc 0.75, learning_rate 0.00123777
2017-10-09T15:06:29.133344: step 359, loss 0.519109, acc 0.828125, learning_rate 0.00123312
2017-10-09T15:06:29.241493: step 360, loss 0.645065, acc 0.765625, learning_rate 0.0012285

Evaluation:
2017-10-09T15:06:29.489184: step 360, loss 0.521461, acc 0.815827

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-360

2017-10-09T15:06:30.077300: step 361, loss 0.555881, acc 0.796875, learning_rate 0.00122389
2017-10-09T15:06:30.180433: step 362, loss 0.36248, acc 0.859375, learning_rate 0.0012193
2017-10-09T15:06:30.285614: step 363, loss 0.967661, acc 0.625, learning_rate 0.00121473
2017-10-09T15:06:30.387791: step 364, loss 0.493655, acc 0.8125, learning_rate 0.00121018
2017-10-09T15:06:30.493912: step 365, loss 0.576041, acc 0.78125, learning_rate 0.00120565
2017-10-09T15:06:30.600121: step 366, loss 0.49841, acc 0.78125, learning_rate 0.00120114
2017-10-09T15:06:30.705544: step 367, loss 0.708792, acc 0.78125, learning_rate 0.00119664
2017-10-09T15:06:30.810081: step 368, loss 0.781856, acc 0.75, learning_rate 0.00119217
2017-10-09T15:06:30.918772: step 369, loss 0.426255, acc 0.859375, learning_rate 0.00118771
2017-10-09T15:06:31.027469: step 370, loss 0.699624, acc 0.703125, learning_rate 0.00118327
2017-10-09T15:06:31.135899: step 371, loss 0.903126, acc 0.625, learning_rate 0.00117885
2017-10-09T15:06:31.239951: step 372, loss 0.492824, acc 0.765625, learning_rate 0.00117445
2017-10-09T15:06:31.347627: step 373, loss 0.753507, acc 0.703125, learning_rate 0.00117006
2017-10-09T15:06:31.449830: step 374, loss 0.727241, acc 0.75, learning_rate 0.00116569
2017-10-09T15:06:31.554792: step 375, loss 0.62901, acc 0.78125, learning_rate 0.00116134
2017-10-09T15:06:31.661867: step 376, loss 0.533061, acc 0.828125, learning_rate 0.00115701
2017-10-09T15:06:31.765489: step 377, loss 0.665876, acc 0.703125, learning_rate 0.0011527
2017-10-09T15:06:31.870810: step 378, loss 0.53921, acc 0.828125, learning_rate 0.0011484
2017-10-09T15:06:31.977270: step 379, loss 0.564272, acc 0.8125, learning_rate 0.00114412
2017-10-09T15:06:32.082265: step 380, loss 0.680779, acc 0.703125, learning_rate 0.00113986
2017-10-09T15:06:32.187727: step 381, loss 0.712094, acc 0.796875, learning_rate 0.00113561
2017-10-09T15:06:32.293840: step 382, loss 0.552056, acc 0.78125, learning_rate 0.00113139
2017-10-09T15:06:32.394071: step 383, loss 0.831898, acc 0.65625, learning_rate 0.00112718
2017-10-09T15:06:32.501386: step 384, loss 0.819198, acc 0.75, learning_rate 0.00112298
2017-10-09T15:06:32.604195: step 385, loss 0.623076, acc 0.78125, learning_rate 0.00111881
2017-10-09T15:06:32.705674: step 386, loss 0.7283, acc 0.71875, learning_rate 0.00111465
2017-10-09T15:06:32.815556: step 387, loss 0.649818, acc 0.75, learning_rate 0.00111051
2017-10-09T15:06:32.918999: step 388, loss 0.700771, acc 0.78125, learning_rate 0.00110638
2017-10-09T15:06:33.021158: step 389, loss 0.681061, acc 0.78125, learning_rate 0.00110228
2017-10-09T15:06:33.121994: step 390, loss 0.628515, acc 0.78125, learning_rate 0.00109818
2017-10-09T15:06:33.230466: step 391, loss 0.696731, acc 0.734375, learning_rate 0.00109411
2017-10-09T15:06:33.318251: step 392, loss 0.624026, acc 0.803922, learning_rate 0.00109005
2017-10-09T15:06:33.419141: step 393, loss 0.614666, acc 0.765625, learning_rate 0.00108601
2017-10-09T15:06:33.523504: step 394, loss 0.663266, acc 0.78125, learning_rate 0.00108199
2017-10-09T15:06:33.628020: step 395, loss 0.627625, acc 0.75, learning_rate 0.00107798
2017-10-09T15:06:33.733493: step 396, loss 0.502214, acc 0.796875, learning_rate 0.00107399
2017-10-09T15:06:33.839942: step 397, loss 0.497196, acc 0.875, learning_rate 0.00107001
2017-10-09T15:06:33.945014: step 398, loss 0.521944, acc 0.84375, learning_rate 0.00106605
2017-10-09T15:06:34.050708: step 399, loss 0.599196, acc 0.78125, learning_rate 0.00106211
2017-10-09T15:06:34.158663: step 400, loss 0.553199, acc 0.78125, learning_rate 0.00105818

Evaluation:
2017-10-09T15:06:34.409127: step 400, loss 0.49842, acc 0.831655

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-400

2017-10-09T15:06:35.066944: step 401, loss 0.489922, acc 0.8125, learning_rate 0.00105427
2017-10-09T15:06:35.172419: step 402, loss 0.487756, acc 0.828125, learning_rate 0.00105037
2017-10-09T15:06:35.277557: step 403, loss 0.56833, acc 0.796875, learning_rate 0.0010465
2017-10-09T15:06:35.383307: step 404, loss 0.494481, acc 0.84375, learning_rate 0.00104263
2017-10-09T15:06:35.488663: step 405, loss 0.467474, acc 0.84375, learning_rate 0.00103878
2017-10-09T15:06:35.591143: step 406, loss 0.634421, acc 0.71875, learning_rate 0.00103495
2017-10-09T15:06:35.699189: step 407, loss 0.643281, acc 0.796875, learning_rate 0.00103114
2017-10-09T15:06:35.803865: step 408, loss 0.444548, acc 0.859375, learning_rate 0.00102734
2017-10-09T15:06:35.903614: step 409, loss 0.51121, acc 0.859375, learning_rate 0.00102355
2017-10-09T15:06:36.008092: step 410, loss 0.507848, acc 0.8125, learning_rate 0.00101978
2017-10-09T15:06:36.115585: step 411, loss 0.656807, acc 0.796875, learning_rate 0.00101603
2017-10-09T15:06:36.215315: step 412, loss 0.614655, acc 0.75, learning_rate 0.00101229
2017-10-09T15:06:36.318550: step 413, loss 0.633239, acc 0.78125, learning_rate 0.00100856
2017-10-09T15:06:36.425773: step 414, loss 0.583113, acc 0.765625, learning_rate 0.00100486
2017-10-09T15:06:36.530905: step 415, loss 0.583888, acc 0.765625, learning_rate 0.00100116
2017-10-09T15:06:36.636803: step 416, loss 0.543459, acc 0.796875, learning_rate 0.000997483
2017-10-09T15:06:36.740462: step 417, loss 0.49333, acc 0.8125, learning_rate 0.00099382
2017-10-09T15:06:36.846948: step 418, loss 0.379008, acc 0.859375, learning_rate 0.000990172
2017-10-09T15:06:36.957578: step 419, loss 0.407335, acc 0.859375, learning_rate 0.000986538
2017-10-09T15:06:37.063902: step 420, loss 0.514209, acc 0.8125, learning_rate 0.00098292
2017-10-09T15:06:37.166591: step 421, loss 0.672707, acc 0.75, learning_rate 0.000979316
2017-10-09T15:06:37.275796: step 422, loss 0.488055, acc 0.8125, learning_rate 0.000975727
2017-10-09T15:06:37.381881: step 423, loss 0.527356, acc 0.8125, learning_rate 0.000972152
2017-10-09T15:06:37.487771: step 424, loss 0.636769, acc 0.8125, learning_rate 0.000968592
2017-10-09T15:06:37.596717: step 425, loss 0.474625, acc 0.8125, learning_rate 0.000965047
2017-10-09T15:06:37.698277: step 426, loss 0.635558, acc 0.734375, learning_rate 0.000961516
2017-10-09T15:06:37.803284: step 427, loss 0.515767, acc 0.765625, learning_rate 0.000958
2017-10-09T15:06:37.903655: step 428, loss 0.516512, acc 0.8125, learning_rate 0.000954497
2017-10-09T15:06:38.011123: step 429, loss 0.779754, acc 0.734375, learning_rate 0.00095101
2017-10-09T15:06:38.115589: step 430, loss 0.609672, acc 0.828125, learning_rate 0.000947536
2017-10-09T15:06:38.220586: step 431, loss 0.587036, acc 0.734375, learning_rate 0.000944076
2017-10-09T15:06:38.326435: step 432, loss 0.419455, acc 0.890625, learning_rate 0.000940631
2017-10-09T15:06:38.430991: step 433, loss 0.678379, acc 0.765625, learning_rate 0.0009372
2017-10-09T15:06:38.534736: step 434, loss 0.504506, acc 0.765625, learning_rate 0.000933783
2017-10-09T15:06:38.641490: step 435, loss 0.515496, acc 0.8125, learning_rate 0.000930379
2017-10-09T15:06:38.752749: step 436, loss 0.549056, acc 0.84375, learning_rate 0.00092699
2017-10-09T15:06:38.862132: step 437, loss 0.80552, acc 0.71875, learning_rate 0.000923614
2017-10-09T15:06:38.967149: step 438, loss 0.630282, acc 0.75, learning_rate 0.000920253
2017-10-09T15:06:39.069080: step 439, loss 0.53132, acc 0.78125, learning_rate 0.000916905
2017-10-09T15:06:39.175854: step 440, loss 0.548508, acc 0.84375, learning_rate 0.00091357

Evaluation:
2017-10-09T15:06:39.428181: step 440, loss 0.4866, acc 0.854676

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-440

2017-10-09T15:06:39.946021: step 441, loss 0.591091, acc 0.75, learning_rate 0.000910249
2017-10-09T15:06:40.051488: step 442, loss 0.573912, acc 0.75, learning_rate 0.000906942
2017-10-09T15:06:40.161947: step 443, loss 0.518058, acc 0.828125, learning_rate 0.000903648
2017-10-09T15:06:40.272666: step 444, loss 0.723728, acc 0.765625, learning_rate 0.000900368
2017-10-09T15:06:40.379086: step 445, loss 0.539746, acc 0.8125, learning_rate 0.000897101
2017-10-09T15:06:40.491229: step 446, loss 0.586651, acc 0.765625, learning_rate 0.000893848
2017-10-09T15:06:40.596237: step 447, loss 0.758504, acc 0.734375, learning_rate 0.000890607
2017-10-09T15:06:40.706800: step 448, loss 0.508021, acc 0.796875, learning_rate 0.00088738
2017-10-09T15:06:40.810638: step 449, loss 0.622337, acc 0.78125, learning_rate 0.000884166
2017-10-09T15:06:40.920089: step 450, loss 0.665167, acc 0.75, learning_rate 0.000880966
2017-10-09T15:06:41.031882: step 451, loss 0.451531, acc 0.8125, learning_rate 0.000877778
2017-10-09T15:06:41.140419: step 452, loss 0.5396, acc 0.78125, learning_rate 0.000874603
2017-10-09T15:06:41.243234: step 453, loss 0.515786, acc 0.828125, learning_rate 0.000871441
2017-10-09T15:06:41.354334: step 454, loss 0.573494, acc 0.78125, learning_rate 0.000868293
2017-10-09T15:06:41.465983: step 455, loss 0.627697, acc 0.703125, learning_rate 0.000865157
2017-10-09T15:06:41.569550: step 456, loss 0.507765, acc 0.796875, learning_rate 0.000862033
2017-10-09T15:06:41.681069: step 457, loss 0.541894, acc 0.796875, learning_rate 0.000858923
2017-10-09T15:06:41.796203: step 458, loss 0.512093, acc 0.78125, learning_rate 0.000855825
2017-10-09T15:06:41.908160: step 459, loss 0.424805, acc 0.828125, learning_rate 0.00085274
2017-10-09T15:06:42.018014: step 460, loss 0.483881, acc 0.828125, learning_rate 0.000849668
2017-10-09T15:06:42.127856: step 461, loss 0.814728, acc 0.703125, learning_rate 0.000846608
2017-10-09T15:06:42.240281: step 462, loss 0.696768, acc 0.78125, learning_rate 0.00084356
2017-10-09T15:06:42.353320: step 463, loss 0.537676, acc 0.8125, learning_rate 0.000840525
2017-10-09T15:06:42.466261: step 464, loss 0.665533, acc 0.65625, learning_rate 0.000837502
2017-10-09T15:06:42.574330: step 465, loss 0.526101, acc 0.84375, learning_rate 0.000834492
2017-10-09T15:06:42.693040: step 466, loss 0.347065, acc 0.859375, learning_rate 0.000831494
2017-10-09T15:06:42.804283: step 467, loss 0.640718, acc 0.765625, learning_rate 0.000828508
2017-10-09T15:06:42.923531: step 468, loss 0.722148, acc 0.75, learning_rate 0.000825535
2017-10-09T15:06:43.035161: step 469, loss 0.460985, acc 0.796875, learning_rate 0.000822573
2017-10-09T15:06:43.144475: step 470, loss 0.487223, acc 0.859375, learning_rate 0.000819624
2017-10-09T15:06:43.259955: step 471, loss 0.63261, acc 0.796875, learning_rate 0.000816687
2017-10-09T15:06:43.373879: step 472, loss 0.507486, acc 0.859375, learning_rate 0.000813761
2017-10-09T15:06:43.493680: step 473, loss 0.574629, acc 0.796875, learning_rate 0.000810848
2017-10-09T15:06:43.605847: step 474, loss 0.531011, acc 0.78125, learning_rate 0.000807946
2017-10-09T15:06:43.713226: step 475, loss 0.459904, acc 0.875, learning_rate 0.000805057
2017-10-09T15:06:43.818831: step 476, loss 0.515024, acc 0.828125, learning_rate 0.000802179
2017-10-09T15:06:43.927686: step 477, loss 0.431865, acc 0.796875, learning_rate 0.000799313
2017-10-09T15:06:44.033219: step 478, loss 0.385581, acc 0.890625, learning_rate 0.000796458
2017-10-09T15:06:44.139980: step 479, loss 0.542402, acc 0.8125, learning_rate 0.000793616
2017-10-09T15:06:44.242541: step 480, loss 0.607317, acc 0.75, learning_rate 0.000790784

Evaluation:
2017-10-09T15:06:44.494705: step 480, loss 0.466658, acc 0.853237

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-480

2017-10-09T15:06:45.075652: step 481, loss 0.688886, acc 0.71875, learning_rate 0.000787965
2017-10-09T15:06:45.181821: step 482, loss 0.672875, acc 0.734375, learning_rate 0.000785157
2017-10-09T15:06:45.285337: step 483, loss 0.371113, acc 0.859375, learning_rate 0.00078236
2017-10-09T15:06:45.390196: step 484, loss 0.570455, acc 0.734375, learning_rate 0.000779575
2017-10-09T15:06:45.494399: step 485, loss 0.428583, acc 0.84375, learning_rate 0.000776801
2017-10-09T15:06:45.595644: step 486, loss 0.445884, acc 0.828125, learning_rate 0.000774038
2017-10-09T15:06:45.700810: step 487, loss 0.586886, acc 0.78125, learning_rate 0.000771287
2017-10-09T15:06:45.805082: step 488, loss 0.480902, acc 0.8125, learning_rate 0.000768547
2017-10-09T15:06:45.910674: step 489, loss 0.464888, acc 0.828125, learning_rate 0.000765818
2017-10-09T15:06:45.999238: step 490, loss 0.385576, acc 0.882353, learning_rate 0.000763101
2017-10-09T15:06:46.104486: step 491, loss 0.527697, acc 0.78125, learning_rate 0.000760394
2017-10-09T15:06:46.209229: step 492, loss 0.307332, acc 0.90625, learning_rate 0.000757698
2017-10-09T15:06:46.313682: step 493, loss 0.669607, acc 0.78125, learning_rate 0.000755014
2017-10-09T15:06:46.418703: step 494, loss 0.394111, acc 0.875, learning_rate 0.00075234
2017-10-09T15:06:46.523421: step 495, loss 0.458708, acc 0.859375, learning_rate 0.000749677
2017-10-09T15:06:46.632900: step 496, loss 0.596388, acc 0.78125, learning_rate 0.000747026
2017-10-09T15:06:46.742157: step 497, loss 0.521331, acc 0.78125, learning_rate 0.000744385
2017-10-09T15:06:46.851049: step 498, loss 0.382029, acc 0.859375, learning_rate 0.000741754
2017-10-09T15:06:46.957217: step 499, loss 0.626418, acc 0.734375, learning_rate 0.000739135
2017-10-09T15:06:47.068169: step 500, loss 0.47565, acc 0.84375, learning_rate 0.000736526
2017-10-09T15:06:47.179180: step 501, loss 0.554213, acc 0.828125, learning_rate 0.000733928
2017-10-09T15:06:47.279086: step 502, loss 0.719784, acc 0.75, learning_rate 0.00073134
2017-10-09T15:06:47.380158: step 503, loss 0.40602, acc 0.828125, learning_rate 0.000728763
2017-10-09T15:06:47.486274: step 504, loss 0.453677, acc 0.8125, learning_rate 0.000726197
2017-10-09T15:06:47.588057: step 505, loss 0.716114, acc 0.71875, learning_rate 0.000723641
2017-10-09T15:06:47.693199: step 506, loss 0.690039, acc 0.671875, learning_rate 0.000721095
2017-10-09T15:06:47.797478: step 507, loss 0.520468, acc 0.796875, learning_rate 0.00071856
2017-10-09T15:06:47.904704: step 508, loss 0.507858, acc 0.828125, learning_rate 0.000716036
2017-10-09T15:06:48.008244: step 509, loss 0.608145, acc 0.8125, learning_rate 0.000713521
2017-10-09T15:06:48.115851: step 510, loss 0.67806, acc 0.71875, learning_rate 0.000711017
2017-10-09T15:06:48.221755: step 511, loss 0.601897, acc 0.796875, learning_rate 0.000708523
2017-10-09T15:06:48.329674: step 512, loss 0.621018, acc 0.8125, learning_rate 0.000706039
2017-10-09T15:06:48.437836: step 513, loss 0.54101, acc 0.765625, learning_rate 0.000703565
2017-10-09T15:06:48.542651: step 514, loss 0.474266, acc 0.859375, learning_rate 0.000701102
2017-10-09T15:06:48.647867: step 515, loss 0.559559, acc 0.8125, learning_rate 0.000698648
2017-10-09T15:06:48.753137: step 516, loss 0.548737, acc 0.8125, learning_rate 0.000696204
2017-10-09T15:06:48.865689: step 517, loss 0.642769, acc 0.796875, learning_rate 0.000693771
2017-10-09T15:06:48.973773: step 518, loss 0.535875, acc 0.796875, learning_rate 0.000691347
2017-10-09T15:06:49.079432: step 519, loss 0.482364, acc 0.84375, learning_rate 0.000688934
2017-10-09T15:06:49.191973: step 520, loss 0.535493, acc 0.796875, learning_rate 0.00068653

Evaluation:
2017-10-09T15:06:49.453119: step 520, loss 0.455204, acc 0.857554

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-520

2017-10-09T15:06:50.111087: step 521, loss 0.485196, acc 0.84375, learning_rate 0.000684136
2017-10-09T15:06:50.214149: step 522, loss 0.445817, acc 0.859375, learning_rate 0.000681751
2017-10-09T15:06:50.317633: step 523, loss 0.617603, acc 0.75, learning_rate 0.000679377
2017-10-09T15:06:50.425177: step 524, loss 0.434894, acc 0.8125, learning_rate 0.000677012
2017-10-09T15:06:50.532273: step 525, loss 0.469155, acc 0.78125, learning_rate 0.000674657
2017-10-09T15:06:50.638669: step 526, loss 0.477091, acc 0.84375, learning_rate 0.000672311
2017-10-09T15:06:50.746909: step 527, loss 0.610906, acc 0.8125, learning_rate 0.000669975
2017-10-09T15:06:50.852187: step 528, loss 0.403277, acc 0.828125, learning_rate 0.000667648
2017-10-09T15:06:50.958780: step 529, loss 0.447449, acc 0.828125, learning_rate 0.000665331
2017-10-09T15:06:51.066685: step 530, loss 0.484142, acc 0.875, learning_rate 0.000663024
2017-10-09T15:06:51.174029: step 531, loss 0.458193, acc 0.859375, learning_rate 0.000660726
2017-10-09T15:06:51.282449: step 532, loss 0.56998, acc 0.75, learning_rate 0.000658437
2017-10-09T15:06:51.388568: step 533, loss 0.429773, acc 0.84375, learning_rate 0.000656158
2017-10-09T15:06:51.499958: step 534, loss 0.617526, acc 0.828125, learning_rate 0.000653888
2017-10-09T15:06:51.605200: step 535, loss 0.509051, acc 0.8125, learning_rate 0.000651627
2017-10-09T15:06:51.708664: step 536, loss 0.367435, acc 0.890625, learning_rate 0.000649375
2017-10-09T15:06:51.818320: step 537, loss 0.512688, acc 0.84375, learning_rate 0.000647133
2017-10-09T15:06:51.926879: step 538, loss 0.667869, acc 0.765625, learning_rate 0.000644899
2017-10-09T15:06:52.033232: step 539, loss 0.523588, acc 0.8125, learning_rate 0.000642675
2017-10-09T15:06:52.140113: step 540, loss 0.457085, acc 0.84375, learning_rate 0.00064046
2017-10-09T15:06:52.245707: step 541, loss 0.467087, acc 0.859375, learning_rate 0.000638254
2017-10-09T15:06:52.350348: step 542, loss 0.556313, acc 0.78125, learning_rate 0.000636057
2017-10-09T15:06:52.455374: step 543, loss 0.501667, acc 0.8125, learning_rate 0.000633869
2017-10-09T15:06:52.559624: step 544, loss 0.560569, acc 0.765625, learning_rate 0.00063169
2017-10-09T15:06:52.666634: step 545, loss 0.317992, acc 0.890625, learning_rate 0.00062952
2017-10-09T15:06:52.774979: step 546, loss 0.754545, acc 0.78125, learning_rate 0.000627358
2017-10-09T15:06:52.887902: step 547, loss 0.519036, acc 0.78125, learning_rate 0.000625206
2017-10-09T15:06:52.999241: step 548, loss 0.568644, acc 0.796875, learning_rate 0.000623062
2017-10-09T15:06:53.116230: step 549, loss 0.53709, acc 0.796875, learning_rate 0.000620927
2017-10-09T15:06:53.221875: step 550, loss 0.434327, acc 0.828125, learning_rate 0.000618801
2017-10-09T15:06:53.329264: step 551, loss 0.577681, acc 0.8125, learning_rate 0.000616683
2017-10-09T15:06:53.433904: step 552, loss 0.555446, acc 0.78125, learning_rate 0.000614574
2017-10-09T15:06:53.537637: step 553, loss 0.343308, acc 0.875, learning_rate 0.000612474
2017-10-09T15:06:53.642241: step 554, loss 0.46295, acc 0.828125, learning_rate 0.000610382
2017-10-09T15:06:53.744687: step 555, loss 0.496034, acc 0.796875, learning_rate 0.000608299
2017-10-09T15:06:53.851084: step 556, loss 0.424372, acc 0.84375, learning_rate 0.000606224
2017-10-09T15:06:53.952525: step 557, loss 0.414243, acc 0.859375, learning_rate 0.000604158
2017-10-09T15:06:54.061044: step 558, loss 0.638134, acc 0.734375, learning_rate 0.0006021
2017-10-09T15:06:54.167958: step 559, loss 0.45947, acc 0.875, learning_rate 0.00060005
2017-10-09T15:06:54.272071: step 560, loss 0.487614, acc 0.78125, learning_rate 0.000598009

Evaluation:
2017-10-09T15:06:54.526062: step 560, loss 0.442504, acc 0.863309

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-560

2017-10-09T15:06:55.058389: step 561, loss 0.335635, acc 0.90625, learning_rate 0.000595977
2017-10-09T15:06:55.165872: step 562, loss 0.544247, acc 0.796875, learning_rate 0.000593952
2017-10-09T15:06:55.271605: step 563, loss 0.491754, acc 0.828125, learning_rate 0.000591936
2017-10-09T15:06:55.377626: step 564, loss 0.364467, acc 0.859375, learning_rate 0.000589928
2017-10-09T15:06:55.492590: step 565, loss 0.538416, acc 0.78125, learning_rate 0.000587928
2017-10-09T15:06:55.598601: step 566, loss 0.456749, acc 0.8125, learning_rate 0.000585937
2017-10-09T15:06:55.717007: step 567, loss 0.368132, acc 0.90625, learning_rate 0.000583953
2017-10-09T15:06:55.841402: step 568, loss 0.360604, acc 0.890625, learning_rate 0.000581978
2017-10-09T15:06:55.945350: step 569, loss 0.520746, acc 0.828125, learning_rate 0.00058001
2017-10-09T15:06:56.049618: step 570, loss 0.47366, acc 0.828125, learning_rate 0.000578051
2017-10-09T15:06:56.157546: step 571, loss 0.638404, acc 0.78125, learning_rate 0.0005761
2017-10-09T15:06:56.265639: step 572, loss 0.495109, acc 0.796875, learning_rate 0.000574157
2017-10-09T15:06:56.391695: step 573, loss 0.664198, acc 0.734375, learning_rate 0.000572221
2017-10-09T15:06:56.511221: step 574, loss 0.3782, acc 0.890625, learning_rate 0.000570294
2017-10-09T15:06:56.630127: step 575, loss 0.366814, acc 0.859375, learning_rate 0.000568374
2017-10-09T15:06:56.748547: step 576, loss 0.407924, acc 0.84375, learning_rate 0.000566462
2017-10-09T15:06:56.863031: step 577, loss 0.473985, acc 0.828125, learning_rate 0.000564558
2017-10-09T15:06:56.978218: step 578, loss 0.6145, acc 0.828125, learning_rate 0.000562662
2017-10-09T15:06:57.083623: step 579, loss 0.546439, acc 0.8125, learning_rate 0.000560774
2017-10-09T15:06:57.187863: step 580, loss 0.492672, acc 0.8125, learning_rate 0.000558893
2017-10-09T15:06:57.294583: step 581, loss 0.636452, acc 0.703125, learning_rate 0.00055702
2017-10-09T15:06:57.401713: step 582, loss 0.66481, acc 0.71875, learning_rate 0.000555154
2017-10-09T15:06:57.504763: step 583, loss 0.507074, acc 0.859375, learning_rate 0.000553296
2017-10-09T15:06:57.610616: step 584, loss 0.339813, acc 0.921875, learning_rate 0.000551446
2017-10-09T15:06:57.715821: step 585, loss 0.602208, acc 0.75, learning_rate 0.000549604
2017-10-09T15:06:57.820720: step 586, loss 0.545134, acc 0.828125, learning_rate 0.000547768
2017-10-09T15:06:57.926795: step 587, loss 0.516967, acc 0.765625, learning_rate 0.000545941
2017-10-09T15:06:58.017794: step 588, loss 0.388058, acc 0.901961, learning_rate 0.00054412
2017-10-09T15:06:58.127427: step 589, loss 0.598163, acc 0.84375, learning_rate 0.000542308
2017-10-09T15:06:58.236755: step 590, loss 0.628163, acc 0.703125, learning_rate 0.000540502
2017-10-09T15:06:58.340649: step 591, loss 0.360728, acc 0.90625, learning_rate 0.000538704
2017-10-09T15:06:58.443141: step 592, loss 0.491488, acc 0.84375, learning_rate 0.000536914
2017-10-09T15:06:58.549643: step 593, loss 0.442847, acc 0.90625, learning_rate 0.00053513
2017-10-09T15:06:58.655035: step 594, loss 0.385896, acc 0.859375, learning_rate 0.000533354
2017-10-09T15:06:58.767488: step 595, loss 0.393746, acc 0.890625, learning_rate 0.000531585
2017-10-09T15:06:58.879122: step 596, loss 0.375994, acc 0.84375, learning_rate 0.000529824
2017-10-09T15:06:58.989434: step 597, loss 0.553738, acc 0.796875, learning_rate 0.000528069
2017-10-09T15:06:59.097148: step 598, loss 0.34453, acc 0.875, learning_rate 0.000526322
2017-10-09T15:06:59.201581: step 599, loss 0.422896, acc 0.890625, learning_rate 0.000524582
2017-10-09T15:06:59.307868: step 600, loss 0.607842, acc 0.78125, learning_rate 0.000522849

Evaluation:
2017-10-09T15:06:59.604048: step 600, loss 0.444001, acc 0.851799

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-600

2017-10-09T15:07:00.250945: step 601, loss 0.321924, acc 0.890625, learning_rate 0.000521123
2017-10-09T15:07:00.373167: step 602, loss 0.571343, acc 0.8125, learning_rate 0.000519404
2017-10-09T15:07:00.497245: step 603, loss 0.380924, acc 0.84375, learning_rate 0.000517692
2017-10-09T15:07:00.621638: step 604, loss 0.342963, acc 0.90625, learning_rate 0.000515987
2017-10-09T15:07:00.740305: step 605, loss 0.380784, acc 0.875, learning_rate 0.000514289
2017-10-09T15:07:00.855831: step 606, loss 0.584577, acc 0.78125, learning_rate 0.000512598
2017-10-09T15:07:00.967089: step 607, loss 0.489103, acc 0.8125, learning_rate 0.000510914
2017-10-09T15:07:01.079810: step 608, loss 0.416601, acc 0.875, learning_rate 0.000509237
2017-10-09T15:07:01.191345: step 609, loss 0.355739, acc 0.828125, learning_rate 0.000507566
2017-10-09T15:07:01.303118: step 610, loss 0.54071, acc 0.796875, learning_rate 0.000505903
2017-10-09T15:07:01.414804: step 611, loss 0.46237, acc 0.859375, learning_rate 0.000504246
2017-10-09T15:07:01.524865: step 612, loss 0.378742, acc 0.890625, learning_rate 0.000502596
2017-10-09T15:07:01.637854: step 613, loss 0.582824, acc 0.78125, learning_rate 0.000500953
2017-10-09T15:07:01.743946: step 614, loss 0.413395, acc 0.875, learning_rate 0.000499316
2017-10-09T15:07:01.850768: step 615, loss 0.427921, acc 0.8125, learning_rate 0.000497686
2017-10-09T15:07:01.973522: step 616, loss 0.545127, acc 0.8125, learning_rate 0.000496063
2017-10-09T15:07:02.099968: step 617, loss 0.413687, acc 0.828125, learning_rate 0.000494446
2017-10-09T15:07:02.214353: step 618, loss 0.625536, acc 0.796875, learning_rate 0.000492836
2017-10-09T15:07:02.318074: step 619, loss 0.444173, acc 0.84375, learning_rate 0.000491233
2017-10-09T15:07:02.426869: step 620, loss 0.385485, acc 0.84375, learning_rate 0.000489636
2017-10-09T15:07:02.533067: step 621, loss 0.642271, acc 0.75, learning_rate 0.000488045
2017-10-09T15:07:02.633806: step 622, loss 0.474792, acc 0.8125, learning_rate 0.000486461
2017-10-09T15:07:02.743558: step 623, loss 0.330413, acc 0.875, learning_rate 0.000484884
2017-10-09T15:07:02.851775: step 624, loss 0.483469, acc 0.828125, learning_rate 0.000483313
2017-10-09T15:07:02.954101: step 625, loss 0.418508, acc 0.859375, learning_rate 0.000481748
2017-10-09T15:07:03.063773: step 626, loss 0.825046, acc 0.703125, learning_rate 0.00048019
2017-10-09T15:07:03.169577: step 627, loss 0.374535, acc 0.890625, learning_rate 0.000478638
2017-10-09T15:07:03.274175: step 628, loss 0.307345, acc 0.90625, learning_rate 0.000477093
2017-10-09T15:07:03.379491: step 629, loss 0.375469, acc 0.90625, learning_rate 0.000475554
2017-10-09T15:07:03.486713: step 630, loss 0.381372, acc 0.859375, learning_rate 0.000474021
2017-10-09T15:07:03.592370: step 631, loss 0.373953, acc 0.875, learning_rate 0.000472494
2017-10-09T15:07:03.694857: step 632, loss 0.489259, acc 0.78125, learning_rate 0.000470974
2017-10-09T15:07:03.808933: step 633, loss 0.40637, acc 0.875, learning_rate 0.000469459
2017-10-09T15:07:03.927322: step 634, loss 0.54802, acc 0.78125, learning_rate 0.000467951
2017-10-09T15:07:04.042161: step 635, loss 0.722587, acc 0.71875, learning_rate 0.000466449
2017-10-09T15:07:04.147655: step 636, loss 0.484687, acc 0.78125, learning_rate 0.000464954
2017-10-09T15:07:04.253311: step 637, loss 0.410859, acc 0.859375, learning_rate 0.000463464
2017-10-09T15:07:04.358334: step 638, loss 0.536557, acc 0.828125, learning_rate 0.00046198
2017-10-09T15:07:04.464265: step 639, loss 0.381151, acc 0.859375, learning_rate 0.000460503
2017-10-09T15:07:04.573676: step 640, loss 0.443145, acc 0.828125, learning_rate 0.000459031

Evaluation:
2017-10-09T15:07:04.840292: step 640, loss 0.435215, acc 0.860432

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-640

2017-10-09T15:07:05.430659: step 641, loss 0.469479, acc 0.828125, learning_rate 0.000457566
2017-10-09T15:07:05.538559: step 642, loss 0.312564, acc 0.921875, learning_rate 0.000456106
2017-10-09T15:07:05.641932: step 643, loss 0.378659, acc 0.875, learning_rate 0.000454653
2017-10-09T15:07:05.744417: step 644, loss 0.601942, acc 0.84375, learning_rate 0.000453205
2017-10-09T15:07:05.848411: step 645, loss 0.450706, acc 0.828125, learning_rate 0.000451764
2017-10-09T15:07:05.954314: step 646, loss 0.433986, acc 0.859375, learning_rate 0.000450328
2017-10-09T15:07:06.059995: step 647, loss 0.465983, acc 0.84375, learning_rate 0.000448898
2017-10-09T15:07:06.167399: step 648, loss 0.495865, acc 0.734375, learning_rate 0.000447474
2017-10-09T15:07:06.275325: step 649, loss 0.350853, acc 0.921875, learning_rate 0.000446055
2017-10-09T15:07:06.376112: step 650, loss 0.412429, acc 0.890625, learning_rate 0.000444643
2017-10-09T15:07:06.483435: step 651, loss 0.582369, acc 0.8125, learning_rate 0.000443236
2017-10-09T15:07:06.590650: step 652, loss 0.74945, acc 0.671875, learning_rate 0.000441835
2017-10-09T15:07:06.696193: step 653, loss 0.4779, acc 0.84375, learning_rate 0.00044044
2017-10-09T15:07:06.816279: step 654, loss 0.459899, acc 0.84375, learning_rate 0.00043905
2017-10-09T15:07:06.943481: step 655, loss 0.628743, acc 0.6875, learning_rate 0.000437666
2017-10-09T15:07:07.070892: step 656, loss 0.468048, acc 0.84375, learning_rate 0.000436288
2017-10-09T15:07:07.177924: step 657, loss 0.314044, acc 0.875, learning_rate 0.000434915
2017-10-09T15:07:07.287080: step 658, loss 0.399111, acc 0.84375, learning_rate 0.000433548
2017-10-09T15:07:07.399152: step 659, loss 0.405409, acc 0.859375, learning_rate 0.000432187
2017-10-09T15:07:07.521135: step 660, loss 0.633925, acc 0.78125, learning_rate 0.000430831
2017-10-09T15:07:07.644265: step 661, loss 0.562243, acc 0.765625, learning_rate 0.000429481
2017-10-09T15:07:07.747722: step 662, loss 0.390618, acc 0.84375, learning_rate 0.000428136
2017-10-09T15:07:07.854371: step 663, loss 0.725384, acc 0.703125, learning_rate 0.000426796
2017-10-09T15:07:07.970661: step 664, loss 0.670893, acc 0.78125, learning_rate 0.000425463
2017-10-09T15:07:08.087134: step 665, loss 0.607672, acc 0.8125, learning_rate 0.000424134
2017-10-09T15:07:08.195362: step 666, loss 0.471576, acc 0.828125, learning_rate 0.000422811
2017-10-09T15:07:08.304081: step 667, loss 0.434328, acc 0.84375, learning_rate 0.000421493
2017-10-09T15:07:08.411338: step 668, loss 0.408906, acc 0.921875, learning_rate 0.000420181
2017-10-09T15:07:08.513485: step 669, loss 0.446149, acc 0.828125, learning_rate 0.000418874
2017-10-09T15:07:08.618640: step 670, loss 0.475619, acc 0.84375, learning_rate 0.000417573
2017-10-09T15:07:08.722933: step 671, loss 0.418211, acc 0.859375, learning_rate 0.000416276
2017-10-09T15:07:08.826005: step 672, loss 0.482222, acc 0.828125, learning_rate 0.000414985
2017-10-09T15:07:08.929310: step 673, loss 0.582405, acc 0.8125, learning_rate 0.0004137
2017-10-09T15:07:09.038316: step 674, loss 0.421379, acc 0.84375, learning_rate 0.000412419
2017-10-09T15:07:09.140221: step 675, loss 0.318484, acc 0.890625, learning_rate 0.000411144
2017-10-09T15:07:09.246093: step 676, loss 0.432856, acc 0.796875, learning_rate 0.000409874
2017-10-09T15:07:09.355751: step 677, loss 0.426615, acc 0.859375, learning_rate 0.000408609
2017-10-09T15:07:09.462979: step 678, loss 0.4258, acc 0.84375, learning_rate 0.00040735
2017-10-09T15:07:09.577536: step 679, loss 0.48001, acc 0.875, learning_rate 0.000406095
2017-10-09T15:07:09.693071: step 680, loss 0.496043, acc 0.859375, learning_rate 0.000404846

Evaluation:
2017-10-09T15:07:09.982170: step 680, loss 0.426899, acc 0.87482

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-680

2017-10-09T15:07:10.641996: step 681, loss 0.350404, acc 0.890625, learning_rate 0.000403601
2017-10-09T15:07:10.747686: step 682, loss 0.416909, acc 0.828125, learning_rate 0.000402362
2017-10-09T15:07:10.856600: step 683, loss 0.360744, acc 0.859375, learning_rate 0.000401128
2017-10-09T15:07:10.983776: step 684, loss 0.410672, acc 0.90625, learning_rate 0.000399899
2017-10-09T15:07:11.100952: step 685, loss 0.422667, acc 0.84375, learning_rate 0.000398675
2017-10-09T15:07:11.196095: step 686, loss 0.394829, acc 0.882353, learning_rate 0.000397456
2017-10-09T15:07:11.301957: step 687, loss 0.452633, acc 0.828125, learning_rate 0.000396241
2017-10-09T15:07:11.403704: step 688, loss 0.342462, acc 0.875, learning_rate 0.000395032
2017-10-09T15:07:11.507015: step 689, loss 0.438931, acc 0.8125, learning_rate 0.000393828
2017-10-09T15:07:11.621607: step 690, loss 0.433429, acc 0.875, learning_rate 0.000392629
2017-10-09T15:07:11.742941: step 691, loss 0.505011, acc 0.8125, learning_rate 0.000391434
2017-10-09T15:07:11.865734: step 692, loss 0.567689, acc 0.828125, learning_rate 0.000390245
2017-10-09T15:07:11.981704: step 693, loss 0.311562, acc 0.875, learning_rate 0.00038906
2017-10-09T15:07:12.088941: step 694, loss 0.424163, acc 0.796875, learning_rate 0.00038788
2017-10-09T15:07:12.195266: step 695, loss 0.495476, acc 0.8125, learning_rate 0.000386705
2017-10-09T15:07:12.299785: step 696, loss 0.545812, acc 0.796875, learning_rate 0.000385535
2017-10-09T15:07:12.411349: step 697, loss 0.509043, acc 0.796875, learning_rate 0.000384369
2017-10-09T15:07:12.534756: step 698, loss 0.463191, acc 0.78125, learning_rate 0.000383209
2017-10-09T15:07:12.640611: step 699, loss 0.42197, acc 0.859375, learning_rate 0.000382053
2017-10-09T15:07:12.753348: step 700, loss 0.487047, acc 0.84375, learning_rate 0.000380901
2017-10-09T15:07:12.873916: step 701, loss 0.55079, acc 0.84375, learning_rate 0.000379755
2017-10-09T15:07:12.998984: step 702, loss 0.477982, acc 0.8125, learning_rate 0.000378613
2017-10-09T15:07:13.126646: step 703, loss 0.393768, acc 0.859375, learning_rate 0.000377476
2017-10-09T15:07:13.230930: step 704, loss 0.511131, acc 0.859375, learning_rate 0.000376343
2017-10-09T15:07:13.335077: step 705, loss 0.442763, acc 0.875, learning_rate 0.000375215
2017-10-09T15:07:13.439670: step 706, loss 0.454309, acc 0.875, learning_rate 0.000374092
2017-10-09T15:07:13.544607: step 707, loss 0.609705, acc 0.765625, learning_rate 0.000372973
2017-10-09T15:07:13.650155: step 708, loss 0.441673, acc 0.78125, learning_rate 0.000371859
2017-10-09T15:07:13.758031: step 709, loss 0.489076, acc 0.796875, learning_rate 0.000370749
2017-10-09T15:07:13.865653: step 710, loss 0.466824, acc 0.796875, learning_rate 0.000369644
2017-10-09T15:07:13.984958: step 711, loss 0.668354, acc 0.75, learning_rate 0.000368543
2017-10-09T15:07:14.100810: step 712, loss 0.29933, acc 0.890625, learning_rate 0.000367447
2017-10-09T15:07:14.208386: step 713, loss 0.4263, acc 0.859375, learning_rate 0.000366356
2017-10-09T15:07:14.310558: step 714, loss 0.40759, acc 0.890625, learning_rate 0.000365268
2017-10-09T15:07:14.416929: step 715, loss 0.546504, acc 0.796875, learning_rate 0.000364186
2017-10-09T15:07:14.520572: step 716, loss 0.556671, acc 0.765625, learning_rate 0.000363107
2017-10-09T15:07:14.625868: step 717, loss 0.38817, acc 0.859375, learning_rate 0.000362033
2017-10-09T15:07:14.729096: step 718, loss 0.551089, acc 0.796875, learning_rate 0.000360964
2017-10-09T15:07:14.832096: step 719, loss 0.34043, acc 0.90625, learning_rate 0.000359899
2017-10-09T15:07:14.939702: step 720, loss 0.498915, acc 0.78125, learning_rate 0.000358838

Evaluation:
2017-10-09T15:07:15.189138: step 720, loss 0.424214, acc 0.866187

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-720

2017-10-09T15:07:15.743719: step 721, loss 0.505657, acc 0.828125, learning_rate 0.000357781
2017-10-09T15:07:15.851791: step 722, loss 0.438175, acc 0.828125, learning_rate 0.000356729
2017-10-09T15:07:15.959479: step 723, loss 0.370546, acc 0.84375, learning_rate 0.000355681
2017-10-09T15:07:16.065942: step 724, loss 0.464079, acc 0.84375, learning_rate 0.000354637
2017-10-09T15:07:16.171145: step 725, loss 0.557218, acc 0.78125, learning_rate 0.000353598
2017-10-09T15:07:16.275195: step 726, loss 0.47117, acc 0.828125, learning_rate 0.000352563
2017-10-09T15:07:16.383228: step 727, loss 0.400163, acc 0.859375, learning_rate 0.000351532
2017-10-09T15:07:16.489984: step 728, loss 0.237104, acc 0.953125, learning_rate 0.000350505
2017-10-09T15:07:16.592778: step 729, loss 0.421471, acc 0.859375, learning_rate 0.000349483
2017-10-09T15:07:16.699955: step 730, loss 0.458148, acc 0.828125, learning_rate 0.000348465
2017-10-09T15:07:16.805575: step 731, loss 0.368735, acc 0.859375, learning_rate 0.00034745
2017-10-09T15:07:16.908001: step 732, loss 0.361023, acc 0.859375, learning_rate 0.00034644
2017-10-09T15:07:17.013436: step 733, loss 0.359635, acc 0.875, learning_rate 0.000345434
2017-10-09T15:07:17.117644: step 734, loss 0.410619, acc 0.828125, learning_rate 0.000344433
2017-10-09T15:07:17.225218: step 735, loss 0.424685, acc 0.875, learning_rate 0.000343435
2017-10-09T15:07:17.335679: step 736, loss 0.539771, acc 0.75, learning_rate 0.000342441
2017-10-09T15:07:17.440057: step 737, loss 0.46392, acc 0.796875, learning_rate 0.000341452
2017-10-09T15:07:17.544749: step 738, loss 0.428117, acc 0.84375, learning_rate 0.000340466
2017-10-09T15:07:17.652316: step 739, loss 0.557977, acc 0.78125, learning_rate 0.000339485
2017-10-09T15:07:17.758725: step 740, loss 0.532906, acc 0.8125, learning_rate 0.000338507
2017-10-09T15:07:17.861770: step 741, loss 0.292808, acc 0.890625, learning_rate 0.000337534
2017-10-09T15:07:17.967881: step 742, loss 0.438391, acc 0.8125, learning_rate 0.000336564
2017-10-09T15:07:18.076485: step 743, loss 0.454213, acc 0.8125, learning_rate 0.000335598
2017-10-09T15:07:18.183372: step 744, loss 0.525986, acc 0.78125, learning_rate 0.000334637
2017-10-09T15:07:18.287093: step 745, loss 0.359846, acc 0.890625, learning_rate 0.000333679
2017-10-09T15:07:18.391781: step 746, loss 0.646769, acc 0.796875, learning_rate 0.000332725
2017-10-09T15:07:18.495479: step 747, loss 0.50906, acc 0.84375, learning_rate 0.000331775
2017-10-09T15:07:18.602738: step 748, loss 0.310004, acc 0.890625, learning_rate 0.000330829
2017-10-09T15:07:18.704178: step 749, loss 0.474066, acc 0.8125, learning_rate 0.000329887
2017-10-09T15:07:18.808426: step 750, loss 0.392093, acc 0.84375, learning_rate 0.000328949
2017-10-09T15:07:18.915713: step 751, loss 0.335193, acc 0.84375, learning_rate 0.000328014
2017-10-09T15:07:19.021607: step 752, loss 0.488426, acc 0.8125, learning_rate 0.000327083
2017-10-09T15:07:19.129712: step 753, loss 0.525153, acc 0.734375, learning_rate 0.000326157
2017-10-09T15:07:19.235332: step 754, loss 0.474982, acc 0.84375, learning_rate 0.000325233
2017-10-09T15:07:19.340326: step 755, loss 0.655534, acc 0.78125, learning_rate 0.000324314
2017-10-09T15:07:19.446281: step 756, loss 0.420892, acc 0.8125, learning_rate 0.000323399
2017-10-09T15:07:19.550175: step 757, loss 0.314471, acc 0.875, learning_rate 0.000322487
2017-10-09T15:07:19.660958: step 758, loss 0.423403, acc 0.84375, learning_rate 0.000321579
2017-10-09T15:07:19.767137: step 759, loss 0.5858, acc 0.765625, learning_rate 0.000320674
2017-10-09T15:07:19.874073: step 760, loss 0.497412, acc 0.78125, learning_rate 0.000319773

Evaluation:
2017-10-09T15:07:20.136404: step 760, loss 0.418112, acc 0.870504

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-760

2017-10-09T15:07:20.728465: step 761, loss 0.526933, acc 0.75, learning_rate 0.000318876
2017-10-09T15:07:20.843619: step 762, loss 0.498333, acc 0.796875, learning_rate 0.000317983
2017-10-09T15:07:20.958964: step 763, loss 0.300664, acc 0.890625, learning_rate 0.000317093
2017-10-09T15:07:21.068868: step 764, loss 0.580033, acc 0.734375, learning_rate 0.000316207
2017-10-09T15:07:21.177338: step 765, loss 0.382256, acc 0.890625, learning_rate 0.000315325
2017-10-09T15:07:21.280920: step 766, loss 0.570892, acc 0.84375, learning_rate 0.000314446
2017-10-09T15:07:21.392082: step 767, loss 0.437783, acc 0.875, learning_rate 0.00031357
2017-10-09T15:07:21.498194: step 768, loss 0.33382, acc 0.859375, learning_rate 0.000312699
2017-10-09T15:07:21.604733: step 769, loss 0.498227, acc 0.8125, learning_rate 0.00031183
2017-10-09T15:07:21.711246: step 770, loss 0.302761, acc 0.90625, learning_rate 0.000310966
2017-10-09T15:07:21.818643: step 771, loss 0.438169, acc 0.84375, learning_rate 0.000310105
2017-10-09T15:07:21.927041: step 772, loss 0.492184, acc 0.828125, learning_rate 0.000309247
2017-10-09T15:07:22.034079: step 773, loss 0.378786, acc 0.875, learning_rate 0.000308393
2017-10-09T15:07:22.140580: step 774, loss 0.402164, acc 0.84375, learning_rate 0.000307542
2017-10-09T15:07:22.250688: step 775, loss 0.339678, acc 0.890625, learning_rate 0.000306695
2017-10-09T15:07:22.352482: step 776, loss 0.453926, acc 0.890625, learning_rate 0.000305852
2017-10-09T15:07:22.463009: step 777, loss 0.393156, acc 0.8125, learning_rate 0.000305011
2017-10-09T15:07:22.571520: step 778, loss 0.466469, acc 0.875, learning_rate 0.000304174
2017-10-09T15:07:22.683939: step 779, loss 0.6544, acc 0.734375, learning_rate 0.000303341
2017-10-09T15:07:22.793941: step 780, loss 0.392221, acc 0.84375, learning_rate 0.000302511
2017-10-09T15:07:22.906256: step 781, loss 0.502012, acc 0.84375, learning_rate 0.000301684
2017-10-09T15:07:23.010961: step 782, loss 0.461806, acc 0.828125, learning_rate 0.000300861
2017-10-09T15:07:23.116361: step 783, loss 0.348726, acc 0.859375, learning_rate 0.000300041
2017-10-09T15:07:23.216691: step 784, loss 0.307759, acc 0.901961, learning_rate 0.000299225
2017-10-09T15:07:23.343378: step 785, loss 0.474312, acc 0.796875, learning_rate 0.000298412
2017-10-09T15:07:23.458719: step 786, loss 0.399324, acc 0.921875, learning_rate 0.000297602
2017-10-09T15:07:23.561243: step 787, loss 0.595379, acc 0.703125, learning_rate 0.000296795
2017-10-09T15:07:23.668143: step 788, loss 0.335332, acc 0.859375, learning_rate 0.000295992
2017-10-09T15:07:23.788693: step 789, loss 0.421359, acc 0.859375, learning_rate 0.000295192
2017-10-09T15:07:23.917991: step 790, loss 0.412869, acc 0.84375, learning_rate 0.000294395
2017-10-09T15:07:24.055188: step 791, loss 0.535587, acc 0.84375, learning_rate 0.000293602
2017-10-09T15:07:24.164312: step 792, loss 0.469464, acc 0.8125, learning_rate 0.000292812
2017-10-09T15:07:24.270334: step 793, loss 0.411291, acc 0.828125, learning_rate 0.000292025
2017-10-09T15:07:24.374780: step 794, loss 0.500932, acc 0.796875, learning_rate 0.000291241
2017-10-09T15:07:24.497973: step 795, loss 0.7154, acc 0.71875, learning_rate 0.00029046
2017-10-09T15:07:24.623424: step 796, loss 0.503812, acc 0.78125, learning_rate 0.000289683
2017-10-09T15:07:24.733116: step 797, loss 0.394077, acc 0.9375, learning_rate 0.000288908
2017-10-09T15:07:24.841505: step 798, loss 0.367284, acc 0.828125, learning_rate 0.000288137
2017-10-09T15:07:24.944889: step 799, loss 0.445862, acc 0.8125, learning_rate 0.000287369
2017-10-09T15:07:25.047034: step 800, loss 0.384761, acc 0.875, learning_rate 0.000286605

Evaluation:
2017-10-09T15:07:25.301460: step 800, loss 0.410036, acc 0.873381

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-800

2017-10-09T15:07:25.883230: step 801, loss 0.505351, acc 0.8125, learning_rate 0.000285843
2017-10-09T15:07:25.984469: step 802, loss 0.408703, acc 0.796875, learning_rate 0.000285084
2017-10-09T15:07:26.090344: step 803, loss 0.347251, acc 0.84375, learning_rate 0.000284329
2017-10-09T15:07:26.194975: step 804, loss 0.688802, acc 0.734375, learning_rate 0.000283577
2017-10-09T15:07:26.300081: step 805, loss 0.301435, acc 0.859375, learning_rate 0.000282827
2017-10-09T15:07:26.405838: step 806, loss 0.465776, acc 0.84375, learning_rate 0.000282081
2017-10-09T15:07:26.512782: step 807, loss 0.396065, acc 0.84375, learning_rate 0.000281338
2017-10-09T15:07:26.616269: step 808, loss 0.344027, acc 0.875, learning_rate 0.000280598
2017-10-09T15:07:26.721093: step 809, loss 0.409797, acc 0.875, learning_rate 0.00027986
2017-10-09T15:07:26.824936: step 810, loss 0.418931, acc 0.84375, learning_rate 0.000279126
2017-10-09T15:07:26.927906: step 811, loss 0.541048, acc 0.765625, learning_rate 0.000278395
2017-10-09T15:07:27.033217: step 812, loss 0.552956, acc 0.828125, learning_rate 0.000277667
2017-10-09T15:07:27.142135: step 813, loss 0.32151, acc 0.90625, learning_rate 0.000276942
2017-10-09T15:07:27.250267: step 814, loss 0.323348, acc 0.90625, learning_rate 0.00027622
2017-10-09T15:07:27.355576: step 815, loss 0.332125, acc 0.84375, learning_rate 0.0002755
2017-10-09T15:07:27.466126: step 816, loss 0.429804, acc 0.828125, learning_rate 0.000274784
2017-10-09T15:07:27.586578: step 817, loss 0.392025, acc 0.875, learning_rate 0.000274071
2017-10-09T15:07:27.706468: step 818, loss 0.338974, acc 0.875, learning_rate 0.00027336
2017-10-09T15:07:27.811026: step 819, loss 0.391604, acc 0.90625, learning_rate 0.000272652
2017-10-09T15:07:27.936111: step 820, loss 0.353404, acc 0.875, learning_rate 0.000271948
2017-10-09T15:07:28.059779: step 821, loss 0.419246, acc 0.84375, learning_rate 0.000271246
2017-10-09T15:07:28.180789: step 822, loss 0.610852, acc 0.75, learning_rate 0.000270547
2017-10-09T15:07:28.285993: step 823, loss 0.411255, acc 0.84375, learning_rate 0.000269851
2017-10-09T15:07:28.400077: step 824, loss 0.526512, acc 0.828125, learning_rate 0.000269157
2017-10-09T15:07:28.516766: step 825, loss 0.524554, acc 0.828125, learning_rate 0.000268467
2017-10-09T15:07:28.631396: step 826, loss 0.411754, acc 0.8125, learning_rate 0.000267779
2017-10-09T15:07:28.738904: step 827, loss 0.458863, acc 0.875, learning_rate 0.000267094
2017-10-09T15:07:28.864326: step 828, loss 0.36911, acc 0.890625, learning_rate 0.000266412
2017-10-09T15:07:28.984513: step 829, loss 0.402123, acc 0.875, learning_rate 0.000265733
2017-10-09T15:07:29.091327: step 830, loss 0.671084, acc 0.75, learning_rate 0.000265057
2017-10-09T15:07:29.198281: step 831, loss 0.267176, acc 0.90625, learning_rate 0.000264383
2017-10-09T15:07:29.321519: step 832, loss 0.312957, acc 0.90625, learning_rate 0.000263712
2017-10-09T15:07:29.445541: step 833, loss 0.335646, acc 0.859375, learning_rate 0.000263044
2017-10-09T15:07:29.550872: step 834, loss 0.41546, acc 0.84375, learning_rate 0.000262378
2017-10-09T15:07:29.658260: step 835, loss 0.407203, acc 0.84375, learning_rate 0.000261715
2017-10-09T15:07:29.766698: step 836, loss 0.424332, acc 0.84375, learning_rate 0.000261055
2017-10-09T15:07:29.880897: step 837, loss 0.390333, acc 0.875, learning_rate 0.000260398
2017-10-09T15:07:29.993115: step 838, loss 0.431008, acc 0.859375, learning_rate 0.000259743
2017-10-09T15:07:30.111081: step 839, loss 0.427943, acc 0.84375, learning_rate 0.000259091
2017-10-09T15:07:30.220224: step 840, loss 0.449183, acc 0.90625, learning_rate 0.000258442

Evaluation:
2017-10-09T15:07:30.479275: step 840, loss 0.409656, acc 0.870504

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-840

2017-10-09T15:07:31.147096: step 841, loss 0.360109, acc 0.84375, learning_rate 0.000257795
2017-10-09T15:07:31.255359: step 842, loss 0.412253, acc 0.875, learning_rate 0.000257151
2017-10-09T15:07:31.367273: step 843, loss 0.479698, acc 0.84375, learning_rate 0.00025651
2017-10-09T15:07:31.471213: step 844, loss 0.378515, acc 0.890625, learning_rate 0.000255871
2017-10-09T15:07:31.575671: step 845, loss 0.442166, acc 0.84375, learning_rate 0.000255235
2017-10-09T15:07:31.682311: step 846, loss 0.604478, acc 0.75, learning_rate 0.000254601
2017-10-09T15:07:31.788384: step 847, loss 0.488401, acc 0.84375, learning_rate 0.00025397
2017-10-09T15:07:31.897807: step 848, loss 0.416599, acc 0.828125, learning_rate 0.000253341
2017-10-09T15:07:32.007275: step 849, loss 0.310026, acc 0.921875, learning_rate 0.000252716
2017-10-09T15:07:32.116934: step 850, loss 0.430835, acc 0.828125, learning_rate 0.000252092
2017-10-09T15:07:32.226980: step 851, loss 0.493344, acc 0.859375, learning_rate 0.000251471
2017-10-09T15:07:32.332560: step 852, loss 0.472249, acc 0.828125, learning_rate 0.000250853
2017-10-09T15:07:32.434958: step 853, loss 0.475367, acc 0.84375, learning_rate 0.000250237
2017-10-09T15:07:32.540358: step 854, loss 0.49052, acc 0.828125, learning_rate 0.000249624
2017-10-09T15:07:32.645179: step 855, loss 0.458873, acc 0.84375, learning_rate 0.000249013
2017-10-09T15:07:32.754491: step 856, loss 0.393075, acc 0.828125, learning_rate 0.000248405
2017-10-09T15:07:32.856802: step 857, loss 0.416397, acc 0.84375, learning_rate 0.000247799
2017-10-09T15:07:32.959453: step 858, loss 0.316944, acc 0.859375, learning_rate 0.000247196
2017-10-09T15:07:33.065435: step 859, loss 0.335865, acc 0.859375, learning_rate 0.000246595
2017-10-09T15:07:33.170463: step 860, loss 0.536197, acc 0.796875, learning_rate 0.000245997
2017-10-09T15:07:33.276865: step 861, loss 0.333006, acc 0.890625, learning_rate 0.000245401
2017-10-09T15:07:33.382440: step 862, loss 0.381834, acc 0.875, learning_rate 0.000244808
2017-10-09T15:07:33.491251: step 863, loss 0.428407, acc 0.84375, learning_rate 0.000244216
2017-10-09T15:07:33.590860: step 864, loss 0.323732, acc 0.90625, learning_rate 0.000243628
2017-10-09T15:07:33.697051: step 865, loss 0.325794, acc 0.921875, learning_rate 0.000243042
2017-10-09T15:07:33.802322: step 866, loss 0.494331, acc 0.84375, learning_rate 0.000242458
2017-10-09T15:07:33.909203: step 867, loss 0.363731, acc 0.875, learning_rate 0.000241876
2017-10-09T15:07:34.014382: step 868, loss 0.370187, acc 0.875, learning_rate 0.000241297
2017-10-09T15:07:34.119665: step 869, loss 0.479798, acc 0.828125, learning_rate 0.00024072
2017-10-09T15:07:34.224932: step 870, loss 0.531244, acc 0.796875, learning_rate 0.000240146
2017-10-09T15:07:34.328719: step 871, loss 0.584249, acc 0.8125, learning_rate 0.000239574
2017-10-09T15:07:34.435605: step 872, loss 0.284883, acc 0.90625, learning_rate 0.000239004
2017-10-09T15:07:34.539505: step 873, loss 0.49583, acc 0.859375, learning_rate 0.000238437
2017-10-09T15:07:34.646537: step 874, loss 0.551104, acc 0.78125, learning_rate 0.000237872
2017-10-09T15:07:34.746894: step 875, loss 0.414554, acc 0.875, learning_rate 0.000237309
2017-10-09T15:07:34.855083: step 876, loss 0.468995, acc 0.875, learning_rate 0.000236749
2017-10-09T15:07:34.964305: step 877, loss 0.362194, acc 0.859375, learning_rate 0.00023619
2017-10-09T15:07:35.062414: step 878, loss 0.396324, acc 0.875, learning_rate 0.000235635
2017-10-09T15:07:35.168783: step 879, loss 0.560129, acc 0.828125, learning_rate 0.000235081
2017-10-09T15:07:35.268964: step 880, loss 0.508901, acc 0.78125, learning_rate 0.00023453

Evaluation:
2017-10-09T15:07:35.523901: step 880, loss 0.412024, acc 0.869065

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-880

2017-10-09T15:07:36.043988: step 881, loss 0.361519, acc 0.90625, learning_rate 0.00023398
2017-10-09T15:07:36.131564: step 882, loss 0.381526, acc 0.862745, learning_rate 0.000233434
2017-10-09T15:07:36.235414: step 883, loss 0.453487, acc 0.84375, learning_rate 0.000232889
2017-10-09T15:07:36.340222: step 884, loss 0.417883, acc 0.859375, learning_rate 0.000232346
2017-10-09T15:07:36.443117: step 885, loss 0.588016, acc 0.765625, learning_rate 0.000231806
2017-10-09T15:07:36.551116: step 886, loss 0.515529, acc 0.8125, learning_rate 0.000231268
2017-10-09T15:07:36.656769: step 887, loss 0.417979, acc 0.828125, learning_rate 0.000230732
2017-10-09T15:07:36.759028: step 888, loss 0.513367, acc 0.796875, learning_rate 0.000230199
2017-10-09T15:07:36.864809: step 889, loss 0.40016, acc 0.828125, learning_rate 0.000229667
2017-10-09T15:07:36.972608: step 890, loss 0.443611, acc 0.859375, learning_rate 0.000229138
2017-10-09T15:07:37.079993: step 891, loss 0.520773, acc 0.8125, learning_rate 0.000228611
2017-10-09T15:07:37.186395: step 892, loss 0.481566, acc 0.859375, learning_rate 0.000228086
2017-10-09T15:07:37.292698: step 893, loss 0.312504, acc 0.875, learning_rate 0.000227563
2017-10-09T15:07:37.391923: step 894, loss 0.559797, acc 0.84375, learning_rate 0.000227043
2017-10-09T15:07:37.494726: step 895, loss 0.340859, acc 0.890625, learning_rate 0.000226524
2017-10-09T15:07:37.600258: step 896, loss 0.334064, acc 0.90625, learning_rate 0.000226008
2017-10-09T15:07:37.705510: step 897, loss 0.505696, acc 0.828125, learning_rate 0.000225493
2017-10-09T15:07:37.811839: step 898, loss 0.325892, acc 0.90625, learning_rate 0.000224981
2017-10-09T15:07:37.917578: step 899, loss 0.525683, acc 0.84375, learning_rate 0.000224471
2017-10-09T15:07:38.022740: step 900, loss 0.513161, acc 0.78125, learning_rate 0.000223963
2017-10-09T15:07:38.133758: step 901, loss 0.34419, acc 0.921875, learning_rate 0.000223457
2017-10-09T15:07:38.240986: step 902, loss 0.396505, acc 0.859375, learning_rate 0.000222953
2017-10-09T15:07:38.343977: step 903, loss 0.407916, acc 0.890625, learning_rate 0.000222451
2017-10-09T15:07:38.448575: step 904, loss 0.450959, acc 0.84375, learning_rate 0.000221951
2017-10-09T15:07:38.554428: step 905, loss 0.522918, acc 0.796875, learning_rate 0.000221453
2017-10-09T15:07:38.660603: step 906, loss 0.412294, acc 0.890625, learning_rate 0.000220958
2017-10-09T15:07:38.770830: step 907, loss 0.434288, acc 0.828125, learning_rate 0.000220464
2017-10-09T15:07:38.877803: step 908, loss 0.291678, acc 0.90625, learning_rate 0.000219972
2017-10-09T15:07:38.982810: step 909, loss 0.482409, acc 0.859375, learning_rate 0.000219483
2017-10-09T15:07:39.089073: step 910, loss 0.353658, acc 0.875, learning_rate 0.000218995
2017-10-09T15:07:39.195664: step 911, loss 0.428563, acc 0.875, learning_rate 0.000218509
2017-10-09T15:07:39.300894: step 912, loss 0.434264, acc 0.84375, learning_rate 0.000218025
2017-10-09T15:07:39.404237: step 913, loss 0.380238, acc 0.84375, learning_rate 0.000217544
2017-10-09T15:07:39.507859: step 914, loss 0.527148, acc 0.828125, learning_rate 0.000217064
2017-10-09T15:07:39.611597: step 915, loss 0.370849, acc 0.859375, learning_rate 0.000216586
2017-10-09T15:07:39.714665: step 916, loss 0.393975, acc 0.84375, learning_rate 0.00021611
2017-10-09T15:07:39.821202: step 917, loss 0.404871, acc 0.84375, learning_rate 0.000215636
2017-10-09T15:07:39.930173: step 918, loss 0.387453, acc 0.90625, learning_rate 0.000215164
2017-10-09T15:07:40.033298: step 919, loss 0.442774, acc 0.84375, learning_rate 0.000214694
2017-10-09T15:07:40.139438: step 920, loss 0.488132, acc 0.828125, learning_rate 0.000214226

Evaluation:
2017-10-09T15:07:40.392669: step 920, loss 0.407993, acc 0.870504

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-920

2017-10-09T15:07:40.981217: step 921, loss 0.614565, acc 0.78125, learning_rate 0.00021376
2017-10-09T15:07:41.087655: step 922, loss 0.392394, acc 0.875, learning_rate 0.000213295
2017-10-09T15:07:41.193974: step 923, loss 0.442675, acc 0.8125, learning_rate 0.000212833
2017-10-09T15:07:41.304354: step 924, loss 0.429358, acc 0.859375, learning_rate 0.000212372
2017-10-09T15:07:41.429437: step 925, loss 0.41386, acc 0.890625, learning_rate 0.000211914
2017-10-09T15:07:41.550384: step 926, loss 0.400142, acc 0.875, learning_rate 0.000211457
2017-10-09T15:07:41.673681: step 927, loss 0.397463, acc 0.875, learning_rate 0.000211002
2017-10-09T15:07:41.781319: step 928, loss 0.406719, acc 0.875, learning_rate 0.000210549
2017-10-09T15:07:41.887692: step 929, loss 0.50062, acc 0.78125, learning_rate 0.000210098
2017-10-09T15:07:41.990800: step 930, loss 0.427911, acc 0.859375, learning_rate 0.000209648
2017-10-09T15:07:42.096745: step 931, loss 0.373603, acc 0.859375, learning_rate 0.000209201
2017-10-09T15:07:42.203142: step 932, loss 0.365889, acc 0.890625, learning_rate 0.000208755
2017-10-09T15:07:42.306312: step 933, loss 0.527671, acc 0.875, learning_rate 0.000208311
2017-10-09T15:07:42.411209: step 934, loss 0.536405, acc 0.8125, learning_rate 0.000207869
2017-10-09T15:07:42.515976: step 935, loss 0.335443, acc 0.859375, learning_rate 0.000207429
2017-10-09T15:07:42.626014: step 936, loss 0.501549, acc 0.828125, learning_rate 0.00020699
2017-10-09T15:07:42.729310: step 937, loss 0.363914, acc 0.890625, learning_rate 0.000206554
2017-10-09T15:07:42.835450: step 938, loss 0.338252, acc 0.875, learning_rate 0.000206119
2017-10-09T15:07:42.945009: step 939, loss 0.457676, acc 0.84375, learning_rate 0.000205685
2017-10-09T15:07:43.048517: step 940, loss 0.446475, acc 0.828125, learning_rate 0.000205254
2017-10-09T15:07:43.152696: step 941, loss 0.40101, acc 0.828125, learning_rate 0.000204824
2017-10-09T15:07:43.250438: step 942, loss 0.4144, acc 0.84375, learning_rate 0.000204397
2017-10-09T15:07:43.356975: step 943, loss 0.542639, acc 0.828125, learning_rate 0.00020397
2017-10-09T15:07:43.458697: step 944, loss 0.264921, acc 0.921875, learning_rate 0.000203546
2017-10-09T15:07:43.564390: step 945, loss 0.393035, acc 0.84375, learning_rate 0.000203123
2017-10-09T15:07:43.671444: step 946, loss 0.404095, acc 0.84375, learning_rate 0.000202702
2017-10-09T15:07:43.778270: step 947, loss 0.221238, acc 0.96875, learning_rate 0.000202283
2017-10-09T15:07:43.893175: step 948, loss 0.33177, acc 0.890625, learning_rate 0.000201866
2017-10-09T15:07:44.013843: step 949, loss 0.433272, acc 0.890625, learning_rate 0.00020145
2017-10-09T15:07:44.134425: step 950, loss 0.361447, acc 0.84375, learning_rate 0.000201036
2017-10-09T15:07:44.253033: step 951, loss 0.458329, acc 0.84375, learning_rate 0.000200623
2017-10-09T15:07:44.361285: step 952, loss 0.404107, acc 0.859375, learning_rate 0.000200213
2017-10-09T15:07:44.468094: step 953, loss 0.491774, acc 0.78125, learning_rate 0.000199804
2017-10-09T15:07:44.572424: step 954, loss 0.473951, acc 0.84375, learning_rate 0.000199396
2017-10-09T15:07:44.677019: step 955, loss 0.504861, acc 0.8125, learning_rate 0.000198991
2017-10-09T15:07:44.783544: step 956, loss 0.430947, acc 0.796875, learning_rate 0.000198587
2017-10-09T15:07:44.891665: step 957, loss 0.366398, acc 0.890625, learning_rate 0.000198184
2017-10-09T15:07:45.002759: step 958, loss 0.327525, acc 0.90625, learning_rate 0.000197783
2017-10-09T15:07:45.129345: step 959, loss 0.373718, acc 0.890625, learning_rate 0.000197384
2017-10-09T15:07:45.246347: step 960, loss 0.485963, acc 0.84375, learning_rate 0.000196987

Evaluation:
2017-10-09T15:07:45.496405: step 960, loss 0.402151, acc 0.869065

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-960

2017-10-09T15:07:46.158812: step 961, loss 0.338188, acc 0.9375, learning_rate 0.000196591
2017-10-09T15:07:46.264151: step 962, loss 0.413948, acc 0.84375, learning_rate 0.000196197
2017-10-09T15:07:46.387174: step 963, loss 0.477182, acc 0.828125, learning_rate 0.000195804
2017-10-09T15:07:46.509954: step 964, loss 0.691101, acc 0.75, learning_rate 0.000195413
2017-10-09T15:07:46.625568: step 965, loss 0.257672, acc 0.890625, learning_rate 0.000195023
2017-10-09T15:07:46.732995: step 966, loss 0.491504, acc 0.796875, learning_rate 0.000194636
2017-10-09T15:07:46.839865: step 967, loss 0.465622, acc 0.8125, learning_rate 0.000194249
2017-10-09T15:07:46.947747: step 968, loss 0.568897, acc 0.84375, learning_rate 0.000193865
2017-10-09T15:07:47.047804: step 969, loss 0.388069, acc 0.84375, learning_rate 0.000193482
2017-10-09T15:07:47.152289: step 970, loss 0.433729, acc 0.859375, learning_rate 0.0001931
2017-10-09T15:07:47.257485: step 971, loss 0.425747, acc 0.796875, learning_rate 0.00019272
2017-10-09T15:07:47.364401: step 972, loss 0.439889, acc 0.84375, learning_rate 0.000192341
2017-10-09T15:07:47.469661: step 973, loss 0.389887, acc 0.890625, learning_rate 0.000191965
2017-10-09T15:07:47.579723: step 974, loss 0.300445, acc 0.9375, learning_rate 0.000191589
2017-10-09T15:07:47.684562: step 975, loss 0.517403, acc 0.828125, learning_rate 0.000191215
2017-10-09T15:07:47.793217: step 976, loss 0.374716, acc 0.859375, learning_rate 0.000190843
2017-10-09T15:07:47.902862: step 977, loss 0.247282, acc 0.90625, learning_rate 0.000190472
2017-10-09T15:07:48.004240: step 978, loss 0.352198, acc 0.859375, learning_rate 0.000190103
2017-10-09T15:07:48.109790: step 979, loss 0.461194, acc 0.859375, learning_rate 0.000189735
2017-10-09T15:07:48.196498: step 980, loss 0.42265, acc 0.862745, learning_rate 0.000189369
2017-10-09T15:07:48.303283: step 981, loss 0.42892, acc 0.859375, learning_rate 0.000189004
2017-10-09T15:07:48.408152: step 982, loss 0.248659, acc 0.9375, learning_rate 0.000188641
2017-10-09T15:07:48.514234: step 983, loss 0.37622, acc 0.890625, learning_rate 0.000188279
2017-10-09T15:07:48.618351: step 984, loss 0.38733, acc 0.828125, learning_rate 0.000187919
2017-10-09T15:07:48.719113: step 985, loss 0.443246, acc 0.875, learning_rate 0.00018756
2017-10-09T15:07:48.824462: step 986, loss 0.42671, acc 0.84375, learning_rate 0.000187202
2017-10-09T15:07:48.935422: step 987, loss 0.481235, acc 0.796875, learning_rate 0.000186846
2017-10-09T15:07:49.058752: step 988, loss 0.506568, acc 0.78125, learning_rate 0.000186492
2017-10-09T15:07:49.178583: step 989, loss 0.443663, acc 0.796875, learning_rate 0.000186139
2017-10-09T15:07:49.294525: step 990, loss 0.512405, acc 0.796875, learning_rate 0.000185787
2017-10-09T15:07:49.396200: step 991, loss 0.529047, acc 0.796875, learning_rate 0.000185437
2017-10-09T15:07:49.501755: step 992, loss 0.381832, acc 0.875, learning_rate 0.000185088
2017-10-09T15:07:49.607330: step 993, loss 0.492243, acc 0.84375, learning_rate 0.000184741
2017-10-09T15:07:49.714106: step 994, loss 0.402456, acc 0.84375, learning_rate 0.000184395
2017-10-09T15:07:49.813745: step 995, loss 0.366878, acc 0.859375, learning_rate 0.000184051
2017-10-09T15:07:49.921413: step 996, loss 0.43358, acc 0.875, learning_rate 0.000183708
2017-10-09T15:07:50.027047: step 997, loss 0.386242, acc 0.859375, learning_rate 0.000183366
2017-10-09T15:07:50.134416: step 998, loss 0.379912, acc 0.859375, learning_rate 0.000183026
2017-10-09T15:07:50.239815: step 999, loss 0.446271, acc 0.890625, learning_rate 0.000182687
2017-10-09T15:07:50.346152: step 1000, loss 0.385451, acc 0.875, learning_rate 0.000182349

Evaluation:
2017-10-09T15:07:50.606219: step 1000, loss 0.399043, acc 0.870504

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1000

2017-10-09T15:07:51.277455: step 1001, loss 0.450744, acc 0.859375, learning_rate 0.000182013
2017-10-09T15:07:51.382538: step 1002, loss 0.570256, acc 0.796875, learning_rate 0.000181678
2017-10-09T15:07:51.489027: step 1003, loss 0.460085, acc 0.84375, learning_rate 0.000181345
2017-10-09T15:07:51.590372: step 1004, loss 0.421071, acc 0.84375, learning_rate 0.000181013
2017-10-09T15:07:51.705355: step 1005, loss 0.422586, acc 0.84375, learning_rate 0.000180682
2017-10-09T15:07:51.821684: step 1006, loss 0.480029, acc 0.78125, learning_rate 0.000180353
2017-10-09T15:07:51.932527: step 1007, loss 0.335629, acc 0.875, learning_rate 0.000180025
2017-10-09T15:07:52.039561: step 1008, loss 0.592405, acc 0.8125, learning_rate 0.000179698
2017-10-09T15:07:52.146637: step 1009, loss 0.430454, acc 0.859375, learning_rate 0.000179373
2017-10-09T15:07:52.253400: step 1010, loss 0.404652, acc 0.875, learning_rate 0.000179049
2017-10-09T15:07:52.355277: step 1011, loss 0.346065, acc 0.890625, learning_rate 0.000178726
2017-10-09T15:07:52.464961: step 1012, loss 0.471436, acc 0.84375, learning_rate 0.000178405
2017-10-09T15:07:52.572000: step 1013, loss 0.488333, acc 0.828125, learning_rate 0.000178085
2017-10-09T15:07:52.679007: step 1014, loss 0.442977, acc 0.84375, learning_rate 0.000177766
2017-10-09T15:07:52.786065: step 1015, loss 0.371744, acc 0.828125, learning_rate 0.000177449
2017-10-09T15:07:52.898478: step 1016, loss 0.364277, acc 0.875, learning_rate 0.000177133
2017-10-09T15:07:53.008466: step 1017, loss 0.284263, acc 0.90625, learning_rate 0.000176818
2017-10-09T15:07:53.114445: step 1018, loss 0.503877, acc 0.859375, learning_rate 0.000176504
2017-10-09T15:07:53.222581: step 1019, loss 0.232253, acc 0.921875, learning_rate 0.000176192
2017-10-09T15:07:53.326894: step 1020, loss 0.477373, acc 0.859375, learning_rate 0.000175881
2017-10-09T15:07:53.434167: step 1021, loss 0.453913, acc 0.859375, learning_rate 0.000175571
2017-10-09T15:07:53.542245: step 1022, loss 0.478073, acc 0.78125, learning_rate 0.000175263
2017-10-09T15:07:53.653614: step 1023, loss 0.252484, acc 0.890625, learning_rate 0.000174956
2017-10-09T15:07:53.762619: step 1024, loss 0.433224, acc 0.859375, learning_rate 0.00017465
2017-10-09T15:07:53.873195: step 1025, loss 0.364148, acc 0.84375, learning_rate 0.000174345
2017-10-09T15:07:53.980500: step 1026, loss 0.451057, acc 0.84375, learning_rate 0.000174042
2017-10-09T15:07:54.085972: step 1027, loss 0.470134, acc 0.859375, learning_rate 0.000173739
2017-10-09T15:07:54.192534: step 1028, loss 0.358244, acc 0.875, learning_rate 0.000173438
2017-10-09T15:07:54.301317: step 1029, loss 0.467818, acc 0.78125, learning_rate 0.000173139
2017-10-09T15:07:54.416574: step 1030, loss 0.42484, acc 0.84375, learning_rate 0.00017284
2017-10-09T15:07:54.521426: step 1031, loss 0.403621, acc 0.859375, learning_rate 0.000172543
2017-10-09T15:07:54.629150: step 1032, loss 0.247819, acc 0.953125, learning_rate 0.000172247
2017-10-09T15:07:54.733104: step 1033, loss 0.403896, acc 0.875, learning_rate 0.000171952
2017-10-09T15:07:54.839195: step 1034, loss 0.361753, acc 0.828125, learning_rate 0.000171658
2017-10-09T15:07:54.945080: step 1035, loss 0.445955, acc 0.84375, learning_rate 0.000171366
2017-10-09T15:07:55.053137: step 1036, loss 0.372701, acc 0.859375, learning_rate 0.000171074
2017-10-09T15:07:55.159436: step 1037, loss 0.505418, acc 0.828125, learning_rate 0.000170784
2017-10-09T15:07:55.264923: step 1038, loss 0.325452, acc 0.828125, learning_rate 0.000170495
2017-10-09T15:07:55.379191: step 1039, loss 0.393368, acc 0.875, learning_rate 0.000170208
2017-10-09T15:07:55.490224: step 1040, loss 0.455948, acc 0.8125, learning_rate 0.000169921

Evaluation:
2017-10-09T15:07:55.744933: step 1040, loss 0.397333, acc 0.869065

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1040

2017-10-09T15:07:56.279082: step 1041, loss 0.384604, acc 0.859375, learning_rate 0.000169636
2017-10-09T15:07:56.383237: step 1042, loss 0.343751, acc 0.890625, learning_rate 0.000169351
2017-10-09T15:07:56.491564: step 1043, loss 0.491012, acc 0.796875, learning_rate 0.000169068
2017-10-09T15:07:56.595181: step 1044, loss 0.322646, acc 0.875, learning_rate 0.000168786
2017-10-09T15:07:56.701337: step 1045, loss 0.399046, acc 0.8125, learning_rate 0.000168506
2017-10-09T15:07:56.807411: step 1046, loss 0.369946, acc 0.84375, learning_rate 0.000168226
2017-10-09T15:07:56.918452: step 1047, loss 0.285694, acc 0.890625, learning_rate 0.000167947
2017-10-09T15:07:57.023498: step 1048, loss 0.420988, acc 0.90625, learning_rate 0.00016767
2017-10-09T15:07:57.140356: step 1049, loss 0.508095, acc 0.875, learning_rate 0.000167394
2017-10-09T15:07:57.253210: step 1050, loss 0.311244, acc 0.90625, learning_rate 0.000167119
2017-10-09T15:07:57.361896: step 1051, loss 0.439966, acc 0.859375, learning_rate 0.000166845
2017-10-09T15:07:57.468971: step 1052, loss 0.453857, acc 0.890625, learning_rate 0.000166572
2017-10-09T15:07:57.575120: step 1053, loss 0.416283, acc 0.84375, learning_rate 0.0001663
2017-10-09T15:07:57.678677: step 1054, loss 0.563113, acc 0.828125, learning_rate 0.00016603
2017-10-09T15:07:57.788337: step 1055, loss 0.483742, acc 0.828125, learning_rate 0.00016576
2017-10-09T15:07:57.899675: step 1056, loss 0.444275, acc 0.78125, learning_rate 0.000165492
2017-10-09T15:07:58.006566: step 1057, loss 0.363239, acc 0.875, learning_rate 0.000165224
2017-10-09T15:07:58.113127: step 1058, loss 0.360572, acc 0.875, learning_rate 0.000164958
2017-10-09T15:07:58.230323: step 1059, loss 0.393305, acc 0.875, learning_rate 0.000164693
2017-10-09T15:07:58.335508: step 1060, loss 0.448914, acc 0.828125, learning_rate 0.000164429
2017-10-09T15:07:58.442554: step 1061, loss 0.412753, acc 0.859375, learning_rate 0.000164166
2017-10-09T15:07:58.549479: step 1062, loss 0.381715, acc 0.875, learning_rate 0.000163904
2017-10-09T15:07:58.658262: step 1063, loss 0.335168, acc 0.890625, learning_rate 0.000163643
2017-10-09T15:07:58.759107: step 1064, loss 0.418798, acc 0.828125, learning_rate 0.000163383
2017-10-09T15:07:58.869807: step 1065, loss 0.482778, acc 0.828125, learning_rate 0.000163125
2017-10-09T15:07:58.975890: step 1066, loss 0.432715, acc 0.828125, learning_rate 0.000162867
2017-10-09T15:07:59.087587: step 1067, loss 0.419822, acc 0.90625, learning_rate 0.00016261
2017-10-09T15:07:59.196683: step 1068, loss 0.407978, acc 0.859375, learning_rate 0.000162355
2017-10-09T15:07:59.305402: step 1069, loss 0.286648, acc 0.9375, learning_rate 0.0001621
2017-10-09T15:07:59.412494: step 1070, loss 0.277287, acc 0.890625, learning_rate 0.000161847
2017-10-09T15:07:59.519984: step 1071, loss 0.351349, acc 0.890625, learning_rate 0.000161594
2017-10-09T15:07:59.628286: step 1072, loss 0.548175, acc 0.84375, learning_rate 0.000161343
2017-10-09T15:07:59.736119: step 1073, loss 0.493295, acc 0.8125, learning_rate 0.000161093
2017-10-09T15:07:59.850321: step 1074, loss 0.520503, acc 0.8125, learning_rate 0.000160843
2017-10-09T15:07:59.955657: step 1075, loss 0.446112, acc 0.8125, learning_rate 0.000160595
2017-10-09T15:08:00.066334: step 1076, loss 0.333374, acc 0.890625, learning_rate 0.000160348
2017-10-09T15:08:00.173081: step 1077, loss 0.351797, acc 0.84375, learning_rate 0.000160101
2017-10-09T15:08:00.262971: step 1078, loss 0.414651, acc 0.784314, learning_rate 0.000159856
2017-10-09T15:08:00.369727: step 1079, loss 0.367793, acc 0.921875, learning_rate 0.000159612
2017-10-09T15:08:00.472242: step 1080, loss 0.389126, acc 0.84375, learning_rate 0.000159368

Evaluation:
2017-10-09T15:08:00.729066: step 1080, loss 0.397606, acc 0.87482

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1080

2017-10-09T15:08:01.334003: step 1081, loss 0.444408, acc 0.84375, learning_rate 0.000159126
2017-10-09T15:08:01.440804: step 1082, loss 0.34325, acc 0.875, learning_rate 0.000158885
2017-10-09T15:08:01.547994: step 1083, loss 0.397258, acc 0.84375, learning_rate 0.000158644
2017-10-09T15:08:01.661078: step 1084, loss 0.423265, acc 0.828125, learning_rate 0.000158405
2017-10-09T15:08:01.771678: step 1085, loss 0.474554, acc 0.859375, learning_rate 0.000158167
2017-10-09T15:08:01.878119: step 1086, loss 0.368982, acc 0.828125, learning_rate 0.000157929
2017-10-09T15:08:01.985475: step 1087, loss 0.398874, acc 0.84375, learning_rate 0.000157693
2017-10-09T15:08:02.092266: step 1088, loss 0.414467, acc 0.859375, learning_rate 0.000157457
2017-10-09T15:08:02.192972: step 1089, loss 0.584169, acc 0.8125, learning_rate 0.000157223
2017-10-09T15:08:02.298383: step 1090, loss 0.494713, acc 0.8125, learning_rate 0.000156989
2017-10-09T15:08:02.405238: step 1091, loss 0.491132, acc 0.8125, learning_rate 0.000156757
2017-10-09T15:08:02.519240: step 1092, loss 0.317223, acc 0.90625, learning_rate 0.000156525
2017-10-09T15:08:02.623702: step 1093, loss 0.626649, acc 0.75, learning_rate 0.000156294
2017-10-09T15:08:02.736880: step 1094, loss 0.484457, acc 0.84375, learning_rate 0.000156064
2017-10-09T15:08:02.849232: step 1095, loss 0.405929, acc 0.8125, learning_rate 0.000155836
2017-10-09T15:08:02.957777: step 1096, loss 0.363488, acc 0.890625, learning_rate 0.000155608
2017-10-09T15:08:03.067586: step 1097, loss 0.562414, acc 0.796875, learning_rate 0.000155381
2017-10-09T15:08:03.179426: step 1098, loss 0.450236, acc 0.890625, learning_rate 0.000155155
2017-10-09T15:08:03.292868: step 1099, loss 0.348611, acc 0.90625, learning_rate 0.000154929
2017-10-09T15:08:03.404130: step 1100, loss 0.407488, acc 0.875, learning_rate 0.000154705
2017-10-09T15:08:03.513436: step 1101, loss 0.397309, acc 0.828125, learning_rate 0.000154482
2017-10-09T15:08:03.621413: step 1102, loss 0.278967, acc 0.90625, learning_rate 0.00015426
2017-10-09T15:08:03.722475: step 1103, loss 0.465893, acc 0.828125, learning_rate 0.000154038
2017-10-09T15:08:03.830068: step 1104, loss 0.466645, acc 0.828125, learning_rate 0.000153818
2017-10-09T15:08:03.933004: step 1105, loss 0.373032, acc 0.84375, learning_rate 0.000153598
2017-10-09T15:08:04.039651: step 1106, loss 0.30154, acc 0.859375, learning_rate 0.000153379
2017-10-09T15:08:04.147079: step 1107, loss 0.52652, acc 0.78125, learning_rate 0.000153161
2017-10-09T15:08:04.251357: step 1108, loss 0.370394, acc 0.890625, learning_rate 0.000152944
2017-10-09T15:08:04.357933: step 1109, loss 0.348437, acc 0.890625, learning_rate 0.000152728
2017-10-09T15:08:04.463684: step 1110, loss 0.373169, acc 0.890625, learning_rate 0.000152513
2017-10-09T15:08:04.570071: step 1111, loss 0.389506, acc 0.84375, learning_rate 0.000152299
2017-10-09T15:08:04.671920: step 1112, loss 0.436339, acc 0.796875, learning_rate 0.000152085
2017-10-09T15:08:04.777152: step 1113, loss 0.450894, acc 0.828125, learning_rate 0.000151872
2017-10-09T15:08:04.881632: step 1114, loss 0.384011, acc 0.84375, learning_rate 0.000151661
2017-10-09T15:08:04.990304: step 1115, loss 0.4249, acc 0.890625, learning_rate 0.00015145
2017-10-09T15:08:05.094834: step 1116, loss 0.357125, acc 0.90625, learning_rate 0.00015124
2017-10-09T15:08:05.196637: step 1117, loss 0.47174, acc 0.828125, learning_rate 0.000151031
2017-10-09T15:08:05.302233: step 1118, loss 0.314958, acc 0.875, learning_rate 0.000150822
2017-10-09T15:08:05.424813: step 1119, loss 0.319093, acc 0.828125, learning_rate 0.000150615
2017-10-09T15:08:05.552488: step 1120, loss 0.361033, acc 0.890625, learning_rate 0.000150408

Evaluation:
2017-10-09T15:08:05.805394: step 1120, loss 0.392549, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1120

2017-10-09T15:08:06.425595: step 1121, loss 0.471118, acc 0.828125, learning_rate 0.000150203
2017-10-09T15:08:06.538075: step 1122, loss 0.361912, acc 0.890625, learning_rate 0.000149998
2017-10-09T15:08:06.648703: step 1123, loss 0.452829, acc 0.78125, learning_rate 0.000149794
2017-10-09T15:08:06.759046: step 1124, loss 0.3788, acc 0.890625, learning_rate 0.00014959
2017-10-09T15:08:06.871677: step 1125, loss 0.297375, acc 0.921875, learning_rate 0.000149388
2017-10-09T15:08:06.979667: step 1126, loss 0.42657, acc 0.828125, learning_rate 0.000149186
2017-10-09T15:08:07.091468: step 1127, loss 0.353305, acc 0.859375, learning_rate 0.000148986
2017-10-09T15:08:07.201461: step 1128, loss 0.521749, acc 0.78125, learning_rate 0.000148786
2017-10-09T15:08:07.312317: step 1129, loss 0.513352, acc 0.796875, learning_rate 0.000148587
2017-10-09T15:08:07.424097: step 1130, loss 0.403363, acc 0.875, learning_rate 0.000148388
2017-10-09T15:08:07.536912: step 1131, loss 0.384066, acc 0.828125, learning_rate 0.000148191
2017-10-09T15:08:07.644503: step 1132, loss 0.369154, acc 0.828125, learning_rate 0.000147994
2017-10-09T15:08:07.758692: step 1133, loss 0.571329, acc 0.78125, learning_rate 0.000147798
2017-10-09T15:08:07.882011: step 1134, loss 0.401392, acc 0.84375, learning_rate 0.000147603
2017-10-09T15:08:07.998199: step 1135, loss 0.336599, acc 0.921875, learning_rate 0.000147409
2017-10-09T15:08:08.102540: step 1136, loss 0.464251, acc 0.84375, learning_rate 0.000147215
2017-10-09T15:08:08.212742: step 1137, loss 0.350164, acc 0.859375, learning_rate 0.000147022
2017-10-09T15:08:08.332428: step 1138, loss 0.328959, acc 0.90625, learning_rate 0.000146831
2017-10-09T15:08:08.437031: step 1139, loss 0.345116, acc 0.890625, learning_rate 0.000146639
2017-10-09T15:08:08.545745: step 1140, loss 0.382904, acc 0.828125, learning_rate 0.000146449
2017-10-09T15:08:08.658281: step 1141, loss 0.4211, acc 0.828125, learning_rate 0.000146259
2017-10-09T15:08:08.769658: step 1142, loss 0.387259, acc 0.859375, learning_rate 0.000146071
2017-10-09T15:08:08.882906: step 1143, loss 0.362516, acc 0.890625, learning_rate 0.000145883
2017-10-09T15:08:09.014902: step 1144, loss 0.263527, acc 0.90625, learning_rate 0.000145695
2017-10-09T15:08:09.133780: step 1145, loss 0.366739, acc 0.890625, learning_rate 0.000145509
2017-10-09T15:08:09.245984: step 1146, loss 0.439565, acc 0.828125, learning_rate 0.000145323
2017-10-09T15:08:09.356366: step 1147, loss 0.286928, acc 0.90625, learning_rate 0.000145138
2017-10-09T15:08:09.467028: step 1148, loss 0.465447, acc 0.875, learning_rate 0.000144954
2017-10-09T15:08:09.576663: step 1149, loss 0.527686, acc 0.84375, learning_rate 0.00014477
2017-10-09T15:08:09.682953: step 1150, loss 0.359476, acc 0.859375, learning_rate 0.000144588
2017-10-09T15:08:09.802249: step 1151, loss 0.383381, acc 0.828125, learning_rate 0.000144406
2017-10-09T15:08:09.921352: step 1152, loss 0.533521, acc 0.8125, learning_rate 0.000144224
2017-10-09T15:08:10.037825: step 1153, loss 0.42437, acc 0.859375, learning_rate 0.000144044
2017-10-09T15:08:10.154055: step 1154, loss 0.397287, acc 0.828125, learning_rate 0.000143864
2017-10-09T15:08:10.268469: step 1155, loss 0.34534, acc 0.84375, learning_rate 0.000143685
2017-10-09T15:08:10.377291: step 1156, loss 0.344177, acc 0.875, learning_rate 0.000143507
2017-10-09T15:08:10.487706: step 1157, loss 0.377228, acc 0.875, learning_rate 0.000143329
2017-10-09T15:08:10.595478: step 1158, loss 0.390632, acc 0.84375, learning_rate 0.000143152
2017-10-09T15:08:10.710763: step 1159, loss 0.512852, acc 0.84375, learning_rate 0.000142976
2017-10-09T15:08:10.826723: step 1160, loss 0.42355, acc 0.84375, learning_rate 0.000142801

Evaluation:
2017-10-09T15:08:11.096147: step 1160, loss 0.395302, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1160

2017-10-09T15:08:11.779962: step 1161, loss 0.467439, acc 0.8125, learning_rate 0.000142626
2017-10-09T15:08:11.891121: step 1162, loss 0.295338, acc 0.921875, learning_rate 0.000142452
2017-10-09T15:08:11.998033: step 1163, loss 0.332926, acc 0.90625, learning_rate 0.000142279
2017-10-09T15:08:12.104007: step 1164, loss 0.286733, acc 0.90625, learning_rate 0.000142106
2017-10-09T15:08:12.210789: step 1165, loss 0.35055, acc 0.859375, learning_rate 0.000141934
2017-10-09T15:08:12.316383: step 1166, loss 0.373806, acc 0.875, learning_rate 0.000141763
2017-10-09T15:08:12.421590: step 1167, loss 0.52078, acc 0.796875, learning_rate 0.000141593
2017-10-09T15:08:12.524898: step 1168, loss 0.384279, acc 0.875, learning_rate 0.000141423
2017-10-09T15:08:12.633442: step 1169, loss 0.370897, acc 0.84375, learning_rate 0.000141254
2017-10-09T15:08:12.741138: step 1170, loss 0.440204, acc 0.828125, learning_rate 0.000141085
2017-10-09T15:08:12.847599: step 1171, loss 0.477349, acc 0.859375, learning_rate 0.000140918
2017-10-09T15:08:12.954414: step 1172, loss 0.491267, acc 0.859375, learning_rate 0.000140751
2017-10-09T15:08:13.060883: step 1173, loss 0.43512, acc 0.84375, learning_rate 0.000140584
2017-10-09T15:08:13.169499: step 1174, loss 0.4235, acc 0.84375, learning_rate 0.000140419
2017-10-09T15:08:13.274894: step 1175, loss 0.361705, acc 0.875, learning_rate 0.000140254
2017-10-09T15:08:13.362303: step 1176, loss 0.28539, acc 0.901961, learning_rate 0.000140089
2017-10-09T15:08:13.467540: step 1177, loss 0.313504, acc 0.921875, learning_rate 0.000139926
2017-10-09T15:08:13.576160: step 1178, loss 0.31761, acc 0.90625, learning_rate 0.000139763
2017-10-09T15:08:13.679484: step 1179, loss 0.536068, acc 0.875, learning_rate 0.0001396
2017-10-09T15:08:13.787686: step 1180, loss 0.240752, acc 0.9375, learning_rate 0.000139439
2017-10-09T15:08:13.902505: step 1181, loss 0.355018, acc 0.890625, learning_rate 0.000139278
2017-10-09T15:08:14.009581: step 1182, loss 0.425296, acc 0.84375, learning_rate 0.000139118
2017-10-09T15:08:14.116362: step 1183, loss 0.431572, acc 0.84375, learning_rate 0.000138958
2017-10-09T15:08:14.224210: step 1184, loss 0.504847, acc 0.796875, learning_rate 0.000138799
2017-10-09T15:08:14.331859: step 1185, loss 0.425121, acc 0.859375, learning_rate 0.00013864
2017-10-09T15:08:14.433882: step 1186, loss 0.515846, acc 0.828125, learning_rate 0.000138483
2017-10-09T15:08:14.543404: step 1187, loss 0.443212, acc 0.828125, learning_rate 0.000138326
2017-10-09T15:08:14.649075: step 1188, loss 0.619027, acc 0.75, learning_rate 0.000138169
2017-10-09T15:08:14.754819: step 1189, loss 0.544556, acc 0.796875, learning_rate 0.000138013
2017-10-09T15:08:14.863996: step 1190, loss 0.281325, acc 0.96875, learning_rate 0.000137858
2017-10-09T15:08:14.973326: step 1191, loss 0.398093, acc 0.796875, learning_rate 0.000137704
2017-10-09T15:08:15.087606: step 1192, loss 0.242638, acc 0.921875, learning_rate 0.00013755
2017-10-09T15:08:15.195958: step 1193, loss 0.426089, acc 0.859375, learning_rate 0.000137397
2017-10-09T15:08:15.305497: step 1194, loss 0.454702, acc 0.875, learning_rate 0.000137244
2017-10-09T15:08:15.412351: step 1195, loss 0.3713, acc 0.890625, learning_rate 0.000137092
2017-10-09T15:08:15.522376: step 1196, loss 0.301398, acc 0.875, learning_rate 0.000136941
2017-10-09T15:08:15.633078: step 1197, loss 0.398948, acc 0.875, learning_rate 0.00013679
2017-10-09T15:08:15.753142: step 1198, loss 0.410386, acc 0.8125, learning_rate 0.00013664
2017-10-09T15:08:15.866569: step 1199, loss 0.270867, acc 0.875, learning_rate 0.00013649
2017-10-09T15:08:15.987168: step 1200, loss 0.331317, acc 0.875, learning_rate 0.000136341

Evaluation:
2017-10-09T15:08:16.261000: step 1200, loss 0.39095, acc 0.873381

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1200

2017-10-09T15:08:16.784278: step 1201, loss 0.394056, acc 0.84375, learning_rate 0.000136193
2017-10-09T15:08:16.891101: step 1202, loss 0.439522, acc 0.859375, learning_rate 0.000136045
2017-10-09T15:08:16.994578: step 1203, loss 0.42723, acc 0.8125, learning_rate 0.000135898
2017-10-09T15:08:17.098897: step 1204, loss 0.540369, acc 0.796875, learning_rate 0.000135751
2017-10-09T15:08:17.201902: step 1205, loss 0.351064, acc 0.875, learning_rate 0.000135605
2017-10-09T15:08:17.308198: step 1206, loss 0.344468, acc 0.859375, learning_rate 0.00013546
2017-10-09T15:08:17.408205: step 1207, loss 0.366313, acc 0.890625, learning_rate 0.000135315
2017-10-09T15:08:17.511059: step 1208, loss 0.323599, acc 0.953125, learning_rate 0.000135171
2017-10-09T15:08:17.619610: step 1209, loss 0.519294, acc 0.78125, learning_rate 0.000135028
2017-10-09T15:08:17.729298: step 1210, loss 0.51875, acc 0.765625, learning_rate 0.000134885
2017-10-09T15:08:17.836571: step 1211, loss 0.336714, acc 0.890625, learning_rate 0.000134742
2017-10-09T15:08:17.943392: step 1212, loss 0.373537, acc 0.859375, learning_rate 0.0001346
2017-10-09T15:08:18.046676: step 1213, loss 0.461394, acc 0.796875, learning_rate 0.000134459
2017-10-09T15:08:18.155829: step 1214, loss 0.494981, acc 0.78125, learning_rate 0.000134319
2017-10-09T15:08:18.262001: step 1215, loss 0.344374, acc 0.875, learning_rate 0.000134178
2017-10-09T15:08:18.368550: step 1216, loss 0.449822, acc 0.828125, learning_rate 0.000134039
2017-10-09T15:08:18.474938: step 1217, loss 0.427803, acc 0.828125, learning_rate 0.0001339
2017-10-09T15:08:18.582676: step 1218, loss 0.284628, acc 0.921875, learning_rate 0.000133762
2017-10-09T15:08:18.688361: step 1219, loss 0.474874, acc 0.84375, learning_rate 0.000133624
2017-10-09T15:08:18.791896: step 1220, loss 0.36417, acc 0.859375, learning_rate 0.000133487
2017-10-09T15:08:18.899821: step 1221, loss 0.427819, acc 0.828125, learning_rate 0.00013335
2017-10-09T15:08:19.008838: step 1222, loss 0.492107, acc 0.828125, learning_rate 0.000133214
2017-10-09T15:08:19.115325: step 1223, loss 0.551215, acc 0.8125, learning_rate 0.000133078
2017-10-09T15:08:19.216747: step 1224, loss 0.341446, acc 0.890625, learning_rate 0.000132943
2017-10-09T15:08:19.327418: step 1225, loss 0.465095, acc 0.8125, learning_rate 0.000132809
2017-10-09T15:08:19.433384: step 1226, loss 0.461418, acc 0.8125, learning_rate 0.000132675
2017-10-09T15:08:19.536460: step 1227, loss 0.270795, acc 0.890625, learning_rate 0.000132541
2017-10-09T15:08:19.646215: step 1228, loss 0.475466, acc 0.796875, learning_rate 0.000132409
2017-10-09T15:08:19.756748: step 1229, loss 0.314613, acc 0.875, learning_rate 0.000132276
2017-10-09T15:08:19.865778: step 1230, loss 0.474392, acc 0.8125, learning_rate 0.000132145
2017-10-09T15:08:19.980361: step 1231, loss 0.42781, acc 0.84375, learning_rate 0.000132013
2017-10-09T15:08:20.087786: step 1232, loss 0.23704, acc 0.921875, learning_rate 0.000131883
2017-10-09T15:08:20.194291: step 1233, loss 0.489395, acc 0.8125, learning_rate 0.000131753
2017-10-09T15:08:20.310851: step 1234, loss 0.425416, acc 0.828125, learning_rate 0.000131623
2017-10-09T15:08:20.432736: step 1235, loss 0.255433, acc 0.921875, learning_rate 0.000131494
2017-10-09T15:08:20.541140: step 1236, loss 0.451843, acc 0.859375, learning_rate 0.000131365
2017-10-09T15:08:20.646989: step 1237, loss 0.402016, acc 0.890625, learning_rate 0.000131237
2017-10-09T15:08:20.755590: step 1238, loss 0.670984, acc 0.765625, learning_rate 0.00013111
2017-10-09T15:08:20.874031: step 1239, loss 0.442833, acc 0.875, learning_rate 0.000130983
2017-10-09T15:08:20.985006: step 1240, loss 0.385066, acc 0.84375, learning_rate 0.000130856

Evaluation:
2017-10-09T15:08:21.256651: step 1240, loss 0.390831, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1240

2017-10-09T15:08:21.887587: step 1241, loss 0.393027, acc 0.875, learning_rate 0.00013073
2017-10-09T15:08:21.998355: step 1242, loss 0.341652, acc 0.90625, learning_rate 0.000130605
2017-10-09T15:08:22.110571: step 1243, loss 0.43564, acc 0.796875, learning_rate 0.00013048
2017-10-09T15:08:22.237016: step 1244, loss 0.527706, acc 0.8125, learning_rate 0.000130356
2017-10-09T15:08:22.351766: step 1245, loss 0.54266, acc 0.8125, learning_rate 0.000130232
2017-10-09T15:08:22.458562: step 1246, loss 0.550727, acc 0.8125, learning_rate 0.000130108
2017-10-09T15:08:22.574963: step 1247, loss 0.445588, acc 0.875, learning_rate 0.000129985
2017-10-09T15:08:22.695536: step 1248, loss 0.253243, acc 0.9375, learning_rate 0.000129863
2017-10-09T15:08:22.815235: step 1249, loss 0.378301, acc 0.859375, learning_rate 0.000129741
2017-10-09T15:08:22.928567: step 1250, loss 0.304935, acc 0.921875, learning_rate 0.00012962
2017-10-09T15:08:23.035203: step 1251, loss 0.507899, acc 0.84375, learning_rate 0.000129499
2017-10-09T15:08:23.143660: step 1252, loss 0.392333, acc 0.84375, learning_rate 0.000129378
2017-10-09T15:08:23.251303: step 1253, loss 0.342459, acc 0.859375, learning_rate 0.000129259
2017-10-09T15:08:23.356298: step 1254, loss 0.517517, acc 0.765625, learning_rate 0.000129139
2017-10-09T15:08:23.465038: step 1255, loss 0.319617, acc 0.875, learning_rate 0.00012902
2017-10-09T15:08:23.574190: step 1256, loss 0.380625, acc 0.84375, learning_rate 0.000128902
2017-10-09T15:08:23.680904: step 1257, loss 0.37339, acc 0.859375, learning_rate 0.000128784
2017-10-09T15:08:23.790521: step 1258, loss 0.282089, acc 0.890625, learning_rate 0.000128666
2017-10-09T15:08:23.898431: step 1259, loss 0.254658, acc 0.875, learning_rate 0.000128549
2017-10-09T15:08:24.004296: step 1260, loss 0.515477, acc 0.796875, learning_rate 0.000128433
2017-10-09T15:08:24.109542: step 1261, loss 0.240662, acc 0.90625, learning_rate 0.000128317
2017-10-09T15:08:24.224318: step 1262, loss 0.389559, acc 0.8125, learning_rate 0.000128201
2017-10-09T15:08:24.338241: step 1263, loss 0.420338, acc 0.859375, learning_rate 0.000128086
2017-10-09T15:08:24.456064: step 1264, loss 0.334745, acc 0.890625, learning_rate 0.000127971
2017-10-09T15:08:24.556811: step 1265, loss 0.419432, acc 0.875, learning_rate 0.000127857
2017-10-09T15:08:24.660309: step 1266, loss 0.358478, acc 0.875, learning_rate 0.000127743
2017-10-09T15:08:24.762563: step 1267, loss 0.419244, acc 0.84375, learning_rate 0.00012763
2017-10-09T15:08:24.869752: step 1268, loss 0.287698, acc 0.921875, learning_rate 0.000127517
2017-10-09T15:08:24.977093: step 1269, loss 0.493501, acc 0.8125, learning_rate 0.000127405
2017-10-09T15:08:25.094114: step 1270, loss 0.356672, acc 0.875, learning_rate 0.000127293
2017-10-09T15:08:25.216801: step 1271, loss 0.341919, acc 0.875, learning_rate 0.000127182
2017-10-09T15:08:25.319002: step 1272, loss 0.588919, acc 0.765625, learning_rate 0.000127071
2017-10-09T15:08:25.425783: step 1273, loss 0.450881, acc 0.84375, learning_rate 0.00012696
2017-10-09T15:08:25.514110: step 1274, loss 0.439243, acc 0.843137, learning_rate 0.00012685
2017-10-09T15:08:25.622129: step 1275, loss 0.354213, acc 0.875, learning_rate 0.000126741
2017-10-09T15:08:25.726649: step 1276, loss 0.466142, acc 0.875, learning_rate 0.000126632
2017-10-09T15:08:25.836495: step 1277, loss 0.334879, acc 0.921875, learning_rate 0.000126523
2017-10-09T15:08:25.945155: step 1278, loss 0.424733, acc 0.828125, learning_rate 0.000126415
2017-10-09T15:08:26.052325: step 1279, loss 0.298283, acc 0.890625, learning_rate 0.000126307
2017-10-09T15:08:26.157839: step 1280, loss 0.272736, acc 0.9375, learning_rate 0.000126199

Evaluation:
2017-10-09T15:08:26.412540: step 1280, loss 0.388205, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1280

2017-10-09T15:08:27.006266: step 1281, loss 0.292943, acc 0.859375, learning_rate 0.000126093
2017-10-09T15:08:27.114116: step 1282, loss 0.352992, acc 0.84375, learning_rate 0.000125986
2017-10-09T15:08:27.219708: step 1283, loss 0.450445, acc 0.84375, learning_rate 0.00012588
2017-10-09T15:08:27.326003: step 1284, loss 0.256052, acc 0.890625, learning_rate 0.000125774
2017-10-09T15:08:27.438276: step 1285, loss 0.575087, acc 0.734375, learning_rate 0.000125669
2017-10-09T15:08:27.558921: step 1286, loss 0.35595, acc 0.84375, learning_rate 0.000125564
2017-10-09T15:08:27.670815: step 1287, loss 0.489706, acc 0.8125, learning_rate 0.00012546
2017-10-09T15:08:27.788149: step 1288, loss 0.514152, acc 0.84375, learning_rate 0.000125356
2017-10-09T15:08:27.895577: step 1289, loss 0.49781, acc 0.796875, learning_rate 0.000125253
2017-10-09T15:08:28.000477: step 1290, loss 0.447952, acc 0.890625, learning_rate 0.00012515
2017-10-09T15:08:28.111862: step 1291, loss 0.326051, acc 0.90625, learning_rate 0.000125047
2017-10-09T15:08:28.220312: step 1292, loss 0.409744, acc 0.859375, learning_rate 0.000124945
2017-10-09T15:08:28.323320: step 1293, loss 0.273207, acc 0.921875, learning_rate 0.000124843
2017-10-09T15:08:28.431157: step 1294, loss 0.673888, acc 0.765625, learning_rate 0.000124741
2017-10-09T15:08:28.538944: step 1295, loss 0.45701, acc 0.859375, learning_rate 0.00012464
2017-10-09T15:08:28.644891: step 1296, loss 0.479982, acc 0.765625, learning_rate 0.00012454
2017-10-09T15:08:28.752298: step 1297, loss 0.356525, acc 0.875, learning_rate 0.00012444
2017-10-09T15:08:28.863177: step 1298, loss 0.312545, acc 0.890625, learning_rate 0.00012434
2017-10-09T15:08:28.968339: step 1299, loss 0.523206, acc 0.796875, learning_rate 0.000124241
2017-10-09T15:08:29.070632: step 1300, loss 0.451196, acc 0.84375, learning_rate 0.000124142
2017-10-09T15:08:29.169399: step 1301, loss 0.379431, acc 0.859375, learning_rate 0.000124043
2017-10-09T15:08:29.273671: step 1302, loss 0.401089, acc 0.8125, learning_rate 0.000123945
2017-10-09T15:08:29.382183: step 1303, loss 0.520238, acc 0.796875, learning_rate 0.000123847
2017-10-09T15:08:29.488332: step 1304, loss 0.399754, acc 0.84375, learning_rate 0.00012375
2017-10-09T15:08:29.596434: step 1305, loss 0.49981, acc 0.78125, learning_rate 0.000123653
2017-10-09T15:08:29.701935: step 1306, loss 0.409786, acc 0.828125, learning_rate 0.000123556
2017-10-09T15:08:29.808590: step 1307, loss 0.400945, acc 0.84375, learning_rate 0.00012346
2017-10-09T15:08:29.913990: step 1308, loss 0.37558, acc 0.84375, learning_rate 0.000123364
2017-10-09T15:08:30.020026: step 1309, loss 0.43391, acc 0.796875, learning_rate 0.000123269
2017-10-09T15:08:30.122969: step 1310, loss 0.329506, acc 0.875, learning_rate 0.000123174
2017-10-09T15:08:30.226264: step 1311, loss 0.305995, acc 0.90625, learning_rate 0.00012308
2017-10-09T15:08:30.332271: step 1312, loss 0.290326, acc 0.90625, learning_rate 0.000122985
2017-10-09T15:08:30.438315: step 1313, loss 0.361738, acc 0.875, learning_rate 0.000122892
2017-10-09T15:08:30.541666: step 1314, loss 0.370674, acc 0.8125, learning_rate 0.000122798
2017-10-09T15:08:30.648270: step 1315, loss 0.559502, acc 0.828125, learning_rate 0.000122705
2017-10-09T15:08:30.753550: step 1316, loss 0.297781, acc 0.90625, learning_rate 0.000122612
2017-10-09T15:08:30.861545: step 1317, loss 0.405499, acc 0.828125, learning_rate 0.00012252
2017-10-09T15:08:30.968762: step 1318, loss 0.269258, acc 0.90625, learning_rate 0.000122428
2017-10-09T15:08:31.075720: step 1319, loss 0.36818, acc 0.875, learning_rate 0.000122337
2017-10-09T15:08:31.181789: step 1320, loss 0.408982, acc 0.84375, learning_rate 0.000122245

Evaluation:
2017-10-09T15:08:31.431463: step 1320, loss 0.388821, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1320

2017-10-09T15:08:32.088873: step 1321, loss 0.342544, acc 0.875, learning_rate 0.000122155
2017-10-09T15:08:32.195542: step 1322, loss 0.354544, acc 0.875, learning_rate 0.000122064
2017-10-09T15:08:32.300467: step 1323, loss 0.353904, acc 0.828125, learning_rate 0.000121974
2017-10-09T15:08:32.406767: step 1324, loss 0.489886, acc 0.828125, learning_rate 0.000121884
2017-10-09T15:08:32.512330: step 1325, loss 0.357813, acc 0.90625, learning_rate 0.000121795
2017-10-09T15:08:32.615698: step 1326, loss 0.43074, acc 0.859375, learning_rate 0.000121706
2017-10-09T15:08:32.718137: step 1327, loss 0.427356, acc 0.828125, learning_rate 0.000121618
2017-10-09T15:08:32.821939: step 1328, loss 0.309449, acc 0.921875, learning_rate 0.000121529
2017-10-09T15:08:32.935540: step 1329, loss 0.470319, acc 0.84375, learning_rate 0.000121441
2017-10-09T15:08:33.041438: step 1330, loss 0.478101, acc 0.890625, learning_rate 0.000121354
2017-10-09T15:08:33.145715: step 1331, loss 0.248449, acc 0.9375, learning_rate 0.000121267
2017-10-09T15:08:33.250468: step 1332, loss 0.485764, acc 0.84375, learning_rate 0.00012118
2017-10-09T15:08:33.353760: step 1333, loss 0.286673, acc 0.890625, learning_rate 0.000121093
2017-10-09T15:08:33.459547: step 1334, loss 0.275668, acc 0.890625, learning_rate 0.000121007
2017-10-09T15:08:33.565557: step 1335, loss 0.354378, acc 0.875, learning_rate 0.000120922
2017-10-09T15:08:33.669417: step 1336, loss 0.399452, acc 0.890625, learning_rate 0.000120836
2017-10-09T15:08:33.772181: step 1337, loss 0.420949, acc 0.828125, learning_rate 0.000120751
2017-10-09T15:08:33.876952: step 1338, loss 0.562271, acc 0.765625, learning_rate 0.000120666
2017-10-09T15:08:33.981383: step 1339, loss 0.241472, acc 0.953125, learning_rate 0.000120582
2017-10-09T15:08:34.090853: step 1340, loss 0.337056, acc 0.890625, learning_rate 0.000120498
2017-10-09T15:08:34.198512: step 1341, loss 0.4881, acc 0.8125, learning_rate 0.000120414
2017-10-09T15:08:34.303516: step 1342, loss 0.417174, acc 0.90625, learning_rate 0.000120331
2017-10-09T15:08:34.404200: step 1343, loss 0.373094, acc 0.875, learning_rate 0.000120248
2017-10-09T15:08:34.511738: step 1344, loss 0.314747, acc 0.875, learning_rate 0.000120165
2017-10-09T15:08:34.619105: step 1345, loss 0.243461, acc 0.90625, learning_rate 0.000120083
2017-10-09T15:08:34.726466: step 1346, loss 0.433586, acc 0.859375, learning_rate 0.000120001
2017-10-09T15:08:34.831241: step 1347, loss 0.289419, acc 0.890625, learning_rate 0.00011992
2017-10-09T15:08:34.936596: step 1348, loss 0.473737, acc 0.84375, learning_rate 0.000119838
2017-10-09T15:08:35.041804: step 1349, loss 0.283589, acc 0.890625, learning_rate 0.000119757
2017-10-09T15:08:35.147811: step 1350, loss 0.361577, acc 0.890625, learning_rate 0.000119677
2017-10-09T15:08:35.254411: step 1351, loss 0.284709, acc 0.90625, learning_rate 0.000119596
2017-10-09T15:08:35.360407: step 1352, loss 0.254225, acc 0.90625, learning_rate 0.000119516
2017-10-09T15:08:35.463406: step 1353, loss 0.277817, acc 0.953125, learning_rate 0.000119437
2017-10-09T15:08:35.570315: step 1354, loss 0.35188, acc 0.90625, learning_rate 0.000119357
2017-10-09T15:08:35.675778: step 1355, loss 0.406727, acc 0.875, learning_rate 0.000119278
2017-10-09T15:08:35.780363: step 1356, loss 0.403132, acc 0.859375, learning_rate 0.0001192
2017-10-09T15:08:35.891230: step 1357, loss 0.361258, acc 0.90625, learning_rate 0.000119121
2017-10-09T15:08:36.000349: step 1358, loss 0.300357, acc 0.9375, learning_rate 0.000119043
2017-10-09T15:08:36.101281: step 1359, loss 0.39528, acc 0.859375, learning_rate 0.000118965
2017-10-09T15:08:36.207643: step 1360, loss 0.505256, acc 0.8125, learning_rate 0.000118888

Evaluation:
2017-10-09T15:08:36.456809: step 1360, loss 0.388773, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1360

2017-10-09T15:08:36.987566: step 1361, loss 0.380475, acc 0.84375, learning_rate 0.000118811
2017-10-09T15:08:37.091052: step 1362, loss 0.456768, acc 0.796875, learning_rate 0.000118734
2017-10-09T15:08:37.195721: step 1363, loss 0.332649, acc 0.875, learning_rate 0.000118658
2017-10-09T15:08:37.301815: step 1364, loss 0.442363, acc 0.84375, learning_rate 0.000118582
2017-10-09T15:08:37.404248: step 1365, loss 0.44797, acc 0.84375, learning_rate 0.000118506
2017-10-09T15:08:37.507732: step 1366, loss 0.463663, acc 0.8125, learning_rate 0.00011843
2017-10-09T15:08:37.615739: step 1367, loss 0.375309, acc 0.828125, learning_rate 0.000118355
2017-10-09T15:08:37.722362: step 1368, loss 0.402575, acc 0.890625, learning_rate 0.00011828
2017-10-09T15:08:37.829106: step 1369, loss 0.608155, acc 0.78125, learning_rate 0.000118205
2017-10-09T15:08:37.929268: step 1370, loss 0.23299, acc 0.921875, learning_rate 0.000118131
2017-10-09T15:08:38.030877: step 1371, loss 0.289715, acc 0.875, learning_rate 0.000118057
2017-10-09T15:08:38.115948: step 1372, loss 0.433142, acc 0.862745, learning_rate 0.000117983
2017-10-09T15:08:38.222014: step 1373, loss 0.50572, acc 0.828125, learning_rate 0.00011791
2017-10-09T15:08:38.325220: step 1374, loss 0.304661, acc 0.90625, learning_rate 0.000117837
2017-10-09T15:08:38.431318: step 1375, loss 0.496969, acc 0.796875, learning_rate 0.000117764
2017-10-09T15:08:38.536211: step 1376, loss 0.427977, acc 0.828125, learning_rate 0.000117692
2017-10-09T15:08:38.643044: step 1377, loss 0.406589, acc 0.875, learning_rate 0.000117619
2017-10-09T15:08:38.749121: step 1378, loss 0.484962, acc 0.828125, learning_rate 0.000117547
2017-10-09T15:08:38.855278: step 1379, loss 0.42004, acc 0.828125, learning_rate 0.000117476
2017-10-09T15:08:38.963549: step 1380, loss 0.289454, acc 0.90625, learning_rate 0.000117404
2017-10-09T15:08:39.070549: step 1381, loss 0.391453, acc 0.8125, learning_rate 0.000117333
2017-10-09T15:08:39.178091: step 1382, loss 0.236901, acc 0.90625, learning_rate 0.000117263
2017-10-09T15:08:39.285117: step 1383, loss 0.338243, acc 0.890625, learning_rate 0.000117192
2017-10-09T15:08:39.396122: step 1384, loss 0.347363, acc 0.875, learning_rate 0.000117122
2017-10-09T15:08:39.503519: step 1385, loss 0.406041, acc 0.875, learning_rate 0.000117052
2017-10-09T15:08:39.607091: step 1386, loss 0.392921, acc 0.890625, learning_rate 0.000116983
2017-10-09T15:08:39.716028: step 1387, loss 0.478092, acc 0.84375, learning_rate 0.000116913
2017-10-09T15:08:39.822221: step 1388, loss 0.406424, acc 0.84375, learning_rate 0.000116844
2017-10-09T15:08:39.922968: step 1389, loss 0.427008, acc 0.84375, learning_rate 0.000116775
2017-10-09T15:08:40.030525: step 1390, loss 0.465262, acc 0.828125, learning_rate 0.000116707
2017-10-09T15:08:40.144583: step 1391, loss 0.458667, acc 0.84375, learning_rate 0.000116639
2017-10-09T15:08:40.263621: step 1392, loss 0.318423, acc 0.890625, learning_rate 0.000116571
2017-10-09T15:08:40.383518: step 1393, loss 0.398463, acc 0.84375, learning_rate 0.000116503
2017-10-09T15:08:40.499981: step 1394, loss 0.415258, acc 0.859375, learning_rate 0.000116436
2017-10-09T15:08:40.604779: step 1395, loss 0.310517, acc 0.859375, learning_rate 0.000116369
2017-10-09T15:08:40.710844: step 1396, loss 0.371245, acc 0.84375, learning_rate 0.000116302
2017-10-09T15:08:40.815664: step 1397, loss 0.359731, acc 0.859375, learning_rate 0.000116235
2017-10-09T15:08:40.926462: step 1398, loss 0.37793, acc 0.90625, learning_rate 0.000116169
2017-10-09T15:08:41.033032: step 1399, loss 0.407964, acc 0.875, learning_rate 0.000116103
2017-10-09T15:08:41.134692: step 1400, loss 0.434992, acc 0.84375, learning_rate 0.000116037

Evaluation:
2017-10-09T15:08:41.403601: step 1400, loss 0.384676, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1400

2017-10-09T15:08:42.001556: step 1401, loss 0.437113, acc 0.859375, learning_rate 0.000115972
2017-10-09T15:08:42.107833: step 1402, loss 0.366043, acc 0.84375, learning_rate 0.000115907
2017-10-09T15:08:42.213993: step 1403, loss 0.252934, acc 0.953125, learning_rate 0.000115842
2017-10-09T15:08:42.317835: step 1404, loss 0.340471, acc 0.890625, learning_rate 0.000115777
2017-10-09T15:08:42.424763: step 1405, loss 0.453965, acc 0.84375, learning_rate 0.000115713
2017-10-09T15:08:42.527730: step 1406, loss 0.519705, acc 0.796875, learning_rate 0.000115649
2017-10-09T15:08:42.636363: step 1407, loss 0.490756, acc 0.828125, learning_rate 0.000115585
2017-10-09T15:08:42.741177: step 1408, loss 0.387997, acc 0.859375, learning_rate 0.000115521
2017-10-09T15:08:42.849677: step 1409, loss 0.497236, acc 0.78125, learning_rate 0.000115458
2017-10-09T15:08:42.959165: step 1410, loss 0.292983, acc 0.90625, learning_rate 0.000115395
2017-10-09T15:08:43.062677: step 1411, loss 0.469104, acc 0.84375, learning_rate 0.000115332
2017-10-09T15:08:43.172038: step 1412, loss 0.45035, acc 0.828125, learning_rate 0.000115269
2017-10-09T15:08:43.279144: step 1413, loss 0.474753, acc 0.84375, learning_rate 0.000115207
2017-10-09T15:08:43.383602: step 1414, loss 0.343202, acc 0.921875, learning_rate 0.000115145
2017-10-09T15:08:43.490291: step 1415, loss 0.368541, acc 0.859375, learning_rate 0.000115083
2017-10-09T15:08:43.595550: step 1416, loss 0.359561, acc 0.875, learning_rate 0.000115022
2017-10-09T15:08:43.700167: step 1417, loss 0.452416, acc 0.828125, learning_rate 0.00011496
2017-10-09T15:08:43.807035: step 1418, loss 0.431092, acc 0.875, learning_rate 0.000114899
2017-10-09T15:08:43.912621: step 1419, loss 0.501128, acc 0.8125, learning_rate 0.000114838
2017-10-09T15:08:44.017976: step 1420, loss 0.324304, acc 0.890625, learning_rate 0.000114778
2017-10-09T15:08:44.131108: step 1421, loss 0.391604, acc 0.859375, learning_rate 0.000114717
2017-10-09T15:08:44.236149: step 1422, loss 0.338238, acc 0.890625, learning_rate 0.000114657
2017-10-09T15:08:44.342526: step 1423, loss 0.307574, acc 0.9375, learning_rate 0.000114598
2017-10-09T15:08:44.448081: step 1424, loss 0.328284, acc 0.9375, learning_rate 0.000114538
2017-10-09T15:08:44.555172: step 1425, loss 0.605137, acc 0.8125, learning_rate 0.000114479
2017-10-09T15:08:44.659851: step 1426, loss 0.387779, acc 0.890625, learning_rate 0.00011442
2017-10-09T15:08:44.765418: step 1427, loss 0.382919, acc 0.84375, learning_rate 0.000114361
2017-10-09T15:08:44.869458: step 1428, loss 0.283708, acc 0.921875, learning_rate 0.000114302
2017-10-09T15:08:44.976729: step 1429, loss 0.389191, acc 0.875, learning_rate 0.000114244
2017-10-09T15:08:45.085257: step 1430, loss 0.379374, acc 0.859375, learning_rate 0.000114186
2017-10-09T15:08:45.191961: step 1431, loss 0.368743, acc 0.875, learning_rate 0.000114128
2017-10-09T15:08:45.295126: step 1432, loss 0.457762, acc 0.828125, learning_rate 0.00011407
2017-10-09T15:08:45.398221: step 1433, loss 0.403954, acc 0.84375, learning_rate 0.000114013
2017-10-09T15:08:45.503598: step 1434, loss 0.38511, acc 0.859375, learning_rate 0.000113955
2017-10-09T15:08:45.608775: step 1435, loss 0.457952, acc 0.8125, learning_rate 0.000113898
2017-10-09T15:08:45.717285: step 1436, loss 0.431412, acc 0.8125, learning_rate 0.000113842
2017-10-09T15:08:45.819801: step 1437, loss 0.351383, acc 0.890625, learning_rate 0.000113785
2017-10-09T15:08:45.931422: step 1438, loss 0.35837, acc 0.90625, learning_rate 0.000113729
2017-10-09T15:08:46.039570: step 1439, loss 0.393946, acc 0.859375, learning_rate 0.000113673
2017-10-09T15:08:46.149216: step 1440, loss 0.383562, acc 0.828125, learning_rate 0.000113617

Evaluation:
2017-10-09T15:08:46.407386: step 1440, loss 0.381982, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1440

2017-10-09T15:08:47.001283: step 1441, loss 0.48897, acc 0.828125, learning_rate 0.000113561
2017-10-09T15:08:47.107780: step 1442, loss 0.401582, acc 0.828125, learning_rate 0.000113506
2017-10-09T15:08:47.210448: step 1443, loss 0.323301, acc 0.9375, learning_rate 0.000113451
2017-10-09T15:08:47.324278: step 1444, loss 0.286067, acc 0.859375, learning_rate 0.000113396
2017-10-09T15:08:47.426790: step 1445, loss 0.282466, acc 0.875, learning_rate 0.000113341
2017-10-09T15:08:47.536010: step 1446, loss 0.411898, acc 0.859375, learning_rate 0.000113287
2017-10-09T15:08:47.644606: step 1447, loss 0.333124, acc 0.90625, learning_rate 0.000113233
2017-10-09T15:08:47.752054: step 1448, loss 0.357017, acc 0.859375, learning_rate 0.000113179
2017-10-09T15:08:47.859622: step 1449, loss 0.503101, acc 0.828125, learning_rate 0.000113125
2017-10-09T15:08:47.968107: step 1450, loss 0.529059, acc 0.765625, learning_rate 0.000113071
2017-10-09T15:08:48.073752: step 1451, loss 0.363267, acc 0.875, learning_rate 0.000113018
2017-10-09T15:08:48.178926: step 1452, loss 0.260486, acc 0.890625, learning_rate 0.000112965
2017-10-09T15:08:48.285825: step 1453, loss 0.393103, acc 0.828125, learning_rate 0.000112912
2017-10-09T15:08:48.393921: step 1454, loss 0.361438, acc 0.84375, learning_rate 0.000112859
2017-10-09T15:08:48.503443: step 1455, loss 0.40535, acc 0.859375, learning_rate 0.000112807
2017-10-09T15:08:48.609243: step 1456, loss 0.334176, acc 0.890625, learning_rate 0.000112754
2017-10-09T15:08:48.722462: step 1457, loss 0.335014, acc 0.875, learning_rate 0.000112702
2017-10-09T15:08:48.824045: step 1458, loss 0.34956, acc 0.875, learning_rate 0.000112651
2017-10-09T15:08:48.929623: step 1459, loss 0.316936, acc 0.859375, learning_rate 0.000112599
2017-10-09T15:08:49.029668: step 1460, loss 0.252135, acc 0.890625, learning_rate 0.000112547
2017-10-09T15:08:49.139230: step 1461, loss 0.280353, acc 0.90625, learning_rate 0.000112496
2017-10-09T15:08:49.247890: step 1462, loss 0.325516, acc 0.890625, learning_rate 0.000112445
2017-10-09T15:08:49.356098: step 1463, loss 0.627129, acc 0.78125, learning_rate 0.000112394
2017-10-09T15:08:49.458732: step 1464, loss 0.180186, acc 0.96875, learning_rate 0.000112344
2017-10-09T15:08:49.565164: step 1465, loss 0.427896, acc 0.828125, learning_rate 0.000112293
2017-10-09T15:08:49.667728: step 1466, loss 0.305236, acc 0.890625, learning_rate 0.000112243
2017-10-09T15:08:49.772576: step 1467, loss 0.330199, acc 0.90625, learning_rate 0.000112193
2017-10-09T15:08:49.880381: step 1468, loss 0.285022, acc 0.90625, learning_rate 0.000112144
2017-10-09T15:08:49.987489: step 1469, loss 0.314531, acc 0.90625, learning_rate 0.000112094
2017-10-09T15:08:50.075080: step 1470, loss 0.35256, acc 0.921569, learning_rate 0.000112045
2017-10-09T15:08:50.183142: step 1471, loss 0.471122, acc 0.8125, learning_rate 0.000111995
2017-10-09T15:08:50.293459: step 1472, loss 0.347695, acc 0.875, learning_rate 0.000111946
2017-10-09T15:08:50.398101: step 1473, loss 0.358055, acc 0.828125, learning_rate 0.000111898
2017-10-09T15:08:50.503256: step 1474, loss 0.287864, acc 0.90625, learning_rate 0.000111849
2017-10-09T15:08:50.614568: step 1475, loss 0.294639, acc 0.890625, learning_rate 0.000111801
2017-10-09T15:08:50.721399: step 1476, loss 0.38276, acc 0.890625, learning_rate 0.000111753
2017-10-09T15:08:50.832309: step 1477, loss 0.350149, acc 0.890625, learning_rate 0.000111705
2017-10-09T15:08:50.941863: step 1478, loss 0.374851, acc 0.90625, learning_rate 0.000111657
2017-10-09T15:08:51.044849: step 1479, loss 0.344877, acc 0.890625, learning_rate 0.000111609
2017-10-09T15:08:51.146860: step 1480, loss 0.540087, acc 0.8125, learning_rate 0.000111562

Evaluation:
2017-10-09T15:08:51.409080: step 1480, loss 0.383859, acc 0.87482

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1480

2017-10-09T15:08:52.066485: step 1481, loss 0.345016, acc 0.90625, learning_rate 0.000111515
2017-10-09T15:08:52.172512: step 1482, loss 0.33305, acc 0.890625, learning_rate 0.000111468
2017-10-09T15:08:52.283961: step 1483, loss 0.256086, acc 0.9375, learning_rate 0.000111421
2017-10-09T15:08:52.391579: step 1484, loss 0.286282, acc 0.90625, learning_rate 0.000111374
2017-10-09T15:08:52.493050: step 1485, loss 0.300658, acc 0.9375, learning_rate 0.000111328
2017-10-09T15:08:52.600293: step 1486, loss 0.273763, acc 0.90625, learning_rate 0.000111282
2017-10-09T15:08:52.706985: step 1487, loss 0.467279, acc 0.78125, learning_rate 0.000111236
2017-10-09T15:08:52.805996: step 1488, loss 0.365367, acc 0.84375, learning_rate 0.00011119
2017-10-09T15:08:52.914906: step 1489, loss 0.401307, acc 0.84375, learning_rate 0.000111144
2017-10-09T15:08:53.021067: step 1490, loss 0.341546, acc 0.90625, learning_rate 0.000111099
2017-10-09T15:08:53.123709: step 1491, loss 0.266116, acc 0.890625, learning_rate 0.000111053
2017-10-09T15:08:53.230831: step 1492, loss 0.224438, acc 0.9375, learning_rate 0.000111008
2017-10-09T15:08:53.337575: step 1493, loss 0.436857, acc 0.8125, learning_rate 0.000110963
2017-10-09T15:08:53.446472: step 1494, loss 0.279152, acc 0.859375, learning_rate 0.000110918
2017-10-09T15:08:53.554114: step 1495, loss 0.405256, acc 0.875, learning_rate 0.000110874
2017-10-09T15:08:53.658719: step 1496, loss 0.461324, acc 0.84375, learning_rate 0.00011083
2017-10-09T15:08:53.763180: step 1497, loss 0.34478, acc 0.84375, learning_rate 0.000110785
2017-10-09T15:08:53.868941: step 1498, loss 0.327516, acc 0.84375, learning_rate 0.000110741
2017-10-09T15:08:53.976277: step 1499, loss 0.320241, acc 0.859375, learning_rate 0.000110697
2017-10-09T15:08:54.078915: step 1500, loss 0.274566, acc 0.90625, learning_rate 0.000110654
2017-10-09T15:08:54.181626: step 1501, loss 0.379365, acc 0.84375, learning_rate 0.00011061
2017-10-09T15:08:54.284785: step 1502, loss 0.363847, acc 0.875, learning_rate 0.000110567
2017-10-09T15:08:54.395861: step 1503, loss 0.257959, acc 0.9375, learning_rate 0.000110524
2017-10-09T15:08:54.504991: step 1504, loss 0.312544, acc 0.90625, learning_rate 0.000110481
2017-10-09T15:08:54.613327: step 1505, loss 0.349145, acc 0.84375, learning_rate 0.000110438
2017-10-09T15:08:54.721093: step 1506, loss 0.567452, acc 0.8125, learning_rate 0.000110396
2017-10-09T15:08:54.828990: step 1507, loss 0.267856, acc 0.953125, learning_rate 0.000110353
2017-10-09T15:08:54.932125: step 1508, loss 0.452863, acc 0.84375, learning_rate 0.000110311
2017-10-09T15:08:55.043116: step 1509, loss 0.422678, acc 0.84375, learning_rate 0.000110269
2017-10-09T15:08:55.150480: step 1510, loss 0.300571, acc 0.90625, learning_rate 0.000110227
2017-10-09T15:08:55.255250: step 1511, loss 0.536647, acc 0.796875, learning_rate 0.000110185
2017-10-09T15:08:55.356815: step 1512, loss 0.34908, acc 0.890625, learning_rate 0.000110144
2017-10-09T15:08:55.467187: step 1513, loss 0.465674, acc 0.84375, learning_rate 0.000110102
2017-10-09T15:08:55.573778: step 1514, loss 0.358192, acc 0.859375, learning_rate 0.000110061
2017-10-09T15:08:55.679664: step 1515, loss 0.320612, acc 0.875, learning_rate 0.00011002
2017-10-09T15:08:55.787715: step 1516, loss 0.373248, acc 0.84375, learning_rate 0.000109979
2017-10-09T15:08:55.898411: step 1517, loss 0.402267, acc 0.890625, learning_rate 0.000109938
2017-10-09T15:08:56.000806: step 1518, loss 0.271278, acc 0.921875, learning_rate 0.000109898
2017-10-09T15:08:56.104467: step 1519, loss 0.2709, acc 0.90625, learning_rate 0.000109857
2017-10-09T15:08:56.210981: step 1520, loss 0.566564, acc 0.828125, learning_rate 0.000109817

Evaluation:
2017-10-09T15:08:56.476016: step 1520, loss 0.380058, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1520

2017-10-09T15:08:57.002428: step 1521, loss 0.485395, acc 0.8125, learning_rate 0.000109777
2017-10-09T15:08:57.107438: step 1522, loss 0.502362, acc 0.8125, learning_rate 0.000109737
2017-10-09T15:08:57.209332: step 1523, loss 0.41688, acc 0.84375, learning_rate 0.000109697
2017-10-09T15:08:57.318734: step 1524, loss 0.25611, acc 0.921875, learning_rate 0.000109658
2017-10-09T15:08:57.423717: step 1525, loss 0.47791, acc 0.890625, learning_rate 0.000109618
2017-10-09T15:08:57.529821: step 1526, loss 0.452729, acc 0.84375, learning_rate 0.000109579
2017-10-09T15:08:57.632768: step 1527, loss 0.380173, acc 0.890625, learning_rate 0.00010954
2017-10-09T15:08:57.736258: step 1528, loss 0.483723, acc 0.84375, learning_rate 0.000109501
2017-10-09T15:08:57.846441: step 1529, loss 0.515915, acc 0.8125, learning_rate 0.000109462
2017-10-09T15:08:57.953425: step 1530, loss 0.411794, acc 0.875, learning_rate 0.000109424
2017-10-09T15:08:58.061188: step 1531, loss 0.355705, acc 0.875, learning_rate 0.000109385
2017-10-09T15:08:58.163616: step 1532, loss 0.516579, acc 0.8125, learning_rate 0.000109347
2017-10-09T15:08:58.270798: step 1533, loss 0.441525, acc 0.859375, learning_rate 0.000109309
2017-10-09T15:08:58.378507: step 1534, loss 0.355376, acc 0.90625, learning_rate 0.000109271
2017-10-09T15:08:58.486857: step 1535, loss 0.347702, acc 0.859375, learning_rate 0.000109233
2017-10-09T15:08:58.591634: step 1536, loss 0.438902, acc 0.859375, learning_rate 0.000109195
2017-10-09T15:08:58.697957: step 1537, loss 0.319811, acc 0.90625, learning_rate 0.000109158
2017-10-09T15:08:58.802043: step 1538, loss 0.462482, acc 0.828125, learning_rate 0.00010912
2017-10-09T15:08:58.910710: step 1539, loss 0.369759, acc 0.875, learning_rate 0.000109083
2017-10-09T15:08:59.018824: step 1540, loss 0.51001, acc 0.859375, learning_rate 0.000109046
2017-10-09T15:08:59.120892: step 1541, loss 0.441842, acc 0.8125, learning_rate 0.000109009
2017-10-09T15:08:59.226225: step 1542, loss 0.379352, acc 0.875, learning_rate 0.000108972
2017-10-09T15:08:59.332494: step 1543, loss 0.332415, acc 0.859375, learning_rate 0.000108936
2017-10-09T15:08:59.438943: step 1544, loss 0.39654, acc 0.90625, learning_rate 0.000108899
2017-10-09T15:08:59.545012: step 1545, loss 0.417793, acc 0.875, learning_rate 0.000108863
2017-10-09T15:08:59.649512: step 1546, loss 0.327448, acc 0.890625, learning_rate 0.000108827
2017-10-09T15:08:59.754830: step 1547, loss 0.475787, acc 0.84375, learning_rate 0.000108791
2017-10-09T15:08:59.860911: step 1548, loss 0.247058, acc 0.9375, learning_rate 0.000108755
2017-10-09T15:08:59.963290: step 1549, loss 0.386026, acc 0.921875, learning_rate 0.000108719
2017-10-09T15:09:00.069118: step 1550, loss 0.324269, acc 0.921875, learning_rate 0.000108683
2017-10-09T15:09:00.176223: step 1551, loss 0.336786, acc 0.859375, learning_rate 0.000108648
2017-10-09T15:09:00.281162: step 1552, loss 0.339358, acc 0.90625, learning_rate 0.000108613
2017-10-09T15:09:00.393614: step 1553, loss 0.474408, acc 0.765625, learning_rate 0.000108577
2017-10-09T15:09:00.497988: step 1554, loss 0.507435, acc 0.78125, learning_rate 0.000108542
2017-10-09T15:09:00.606151: step 1555, loss 0.378105, acc 0.8125, learning_rate 0.000108508
2017-10-09T15:09:00.715651: step 1556, loss 0.265601, acc 0.9375, learning_rate 0.000108473
2017-10-09T15:09:00.815351: step 1557, loss 0.378237, acc 0.890625, learning_rate 0.000108438
2017-10-09T15:09:00.923523: step 1558, loss 0.396723, acc 0.890625, learning_rate 0.000108404
2017-10-09T15:09:01.029864: step 1559, loss 0.575223, acc 0.734375, learning_rate 0.00010837
2017-10-09T15:09:01.137644: step 1560, loss 0.305369, acc 0.859375, learning_rate 0.000108335

Evaluation:
2017-10-09T15:09:01.391288: step 1560, loss 0.382224, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1560

2017-10-09T15:09:01.975505: step 1561, loss 0.381502, acc 0.84375, learning_rate 0.000108301
2017-10-09T15:09:02.074781: step 1562, loss 0.256039, acc 0.890625, learning_rate 0.000108267
2017-10-09T15:09:02.179332: step 1563, loss 0.395764, acc 0.84375, learning_rate 0.000108234
2017-10-09T15:09:02.286718: step 1564, loss 0.359258, acc 0.875, learning_rate 0.0001082
2017-10-09T15:09:02.392802: step 1565, loss 0.210672, acc 0.921875, learning_rate 0.000108167
2017-10-09T15:09:02.499689: step 1566, loss 0.410903, acc 0.84375, learning_rate 0.000108133
2017-10-09T15:09:02.602515: step 1567, loss 0.299601, acc 0.890625, learning_rate 0.0001081
2017-10-09T15:09:02.692523: step 1568, loss 0.338023, acc 0.862745, learning_rate 0.000108067
2017-10-09T15:09:02.797178: step 1569, loss 0.498521, acc 0.765625, learning_rate 0.000108034
2017-10-09T15:09:02.910515: step 1570, loss 0.278426, acc 0.921875, learning_rate 0.000108001
2017-10-09T15:09:03.022544: step 1571, loss 0.355415, acc 0.875, learning_rate 0.000107969
2017-10-09T15:09:03.131042: step 1572, loss 0.453734, acc 0.84375, learning_rate 0.000107936
2017-10-09T15:09:03.237136: step 1573, loss 0.25599, acc 0.9375, learning_rate 0.000107904
2017-10-09T15:09:03.344369: step 1574, loss 0.385057, acc 0.84375, learning_rate 0.000107871
2017-10-09T15:09:03.446659: step 1575, loss 0.505127, acc 0.78125, learning_rate 0.000107839
2017-10-09T15:09:03.552086: step 1576, loss 0.331801, acc 0.890625, learning_rate 0.000107807
2017-10-09T15:09:03.660221: step 1577, loss 0.371326, acc 0.828125, learning_rate 0.000107775
2017-10-09T15:09:03.764944: step 1578, loss 0.385844, acc 0.859375, learning_rate 0.000107744
2017-10-09T15:09:03.873927: step 1579, loss 0.36655, acc 0.84375, learning_rate 0.000107712
2017-10-09T15:09:03.978302: step 1580, loss 0.605484, acc 0.859375, learning_rate 0.000107681
2017-10-09T15:09:04.083142: step 1581, loss 0.391656, acc 0.84375, learning_rate 0.000107649
2017-10-09T15:09:04.188318: step 1582, loss 0.289529, acc 0.90625, learning_rate 0.000107618
2017-10-09T15:09:04.294339: step 1583, loss 0.506755, acc 0.796875, learning_rate 0.000107587
2017-10-09T15:09:04.398588: step 1584, loss 0.357967, acc 0.84375, learning_rate 0.000107556
2017-10-09T15:09:04.504518: step 1585, loss 0.349345, acc 0.8125, learning_rate 0.000107525
2017-10-09T15:09:04.613272: step 1586, loss 0.258254, acc 0.90625, learning_rate 0.000107494
2017-10-09T15:09:04.716120: step 1587, loss 0.355916, acc 0.90625, learning_rate 0.000107464
2017-10-09T15:09:04.822028: step 1588, loss 0.422679, acc 0.84375, learning_rate 0.000107433
2017-10-09T15:09:04.928686: step 1589, loss 0.217686, acc 0.953125, learning_rate 0.000107403
2017-10-09T15:09:05.036941: step 1590, loss 0.370105, acc 0.859375, learning_rate 0.000107373
2017-10-09T15:09:05.143457: step 1591, loss 0.353118, acc 0.84375, learning_rate 0.000107343
2017-10-09T15:09:05.249934: step 1592, loss 0.341544, acc 0.859375, learning_rate 0.000107313
2017-10-09T15:09:05.351903: step 1593, loss 0.310917, acc 0.875, learning_rate 0.000107283
2017-10-09T15:09:05.458626: step 1594, loss 0.266518, acc 0.921875, learning_rate 0.000107253
2017-10-09T15:09:05.563388: step 1595, loss 0.217913, acc 0.96875, learning_rate 0.000107224
2017-10-09T15:09:05.669968: step 1596, loss 0.289801, acc 0.859375, learning_rate 0.000107194
2017-10-09T15:09:05.773763: step 1597, loss 0.514578, acc 0.84375, learning_rate 0.000107165
2017-10-09T15:09:05.877969: step 1598, loss 0.37401, acc 0.90625, learning_rate 0.000107136
2017-10-09T15:09:05.987345: step 1599, loss 0.253123, acc 0.921875, learning_rate 0.000107106
2017-10-09T15:09:06.092593: step 1600, loss 0.325338, acc 0.84375, learning_rate 0.000107077

Evaluation:
2017-10-09T15:09:06.351303: step 1600, loss 0.381091, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1600

2017-10-09T15:09:06.940490: step 1601, loss 0.46675, acc 0.859375, learning_rate 0.000107048
2017-10-09T15:09:07.049848: step 1602, loss 0.277917, acc 0.9375, learning_rate 0.00010702
2017-10-09T15:09:07.155039: step 1603, loss 0.3429, acc 0.890625, learning_rate 0.000106991
2017-10-09T15:09:07.261584: step 1604, loss 0.440152, acc 0.828125, learning_rate 0.000106963
2017-10-09T15:09:07.365264: step 1605, loss 0.452761, acc 0.875, learning_rate 0.000106934
2017-10-09T15:09:07.473576: step 1606, loss 0.534423, acc 0.8125, learning_rate 0.000106906
2017-10-09T15:09:07.576470: step 1607, loss 0.360643, acc 0.890625, learning_rate 0.000106878
2017-10-09T15:09:07.687541: step 1608, loss 0.435827, acc 0.859375, learning_rate 0.00010685
2017-10-09T15:09:07.795341: step 1609, loss 0.342568, acc 0.84375, learning_rate 0.000106822
2017-10-09T15:09:07.904535: step 1610, loss 0.399813, acc 0.890625, learning_rate 0.000106794
2017-10-09T15:09:08.011153: step 1611, loss 0.579456, acc 0.796875, learning_rate 0.000106766
2017-10-09T15:09:08.117959: step 1612, loss 0.296369, acc 0.890625, learning_rate 0.000106738
2017-10-09T15:09:08.227966: step 1613, loss 0.342745, acc 0.90625, learning_rate 0.000106711
2017-10-09T15:09:08.335112: step 1614, loss 0.447589, acc 0.828125, learning_rate 0.000106684
2017-10-09T15:09:08.444405: step 1615, loss 0.42077, acc 0.828125, learning_rate 0.000106656
2017-10-09T15:09:08.549048: step 1616, loss 0.312315, acc 0.859375, learning_rate 0.000106629
2017-10-09T15:09:08.656665: step 1617, loss 0.384533, acc 0.84375, learning_rate 0.000106602
2017-10-09T15:09:08.759705: step 1618, loss 0.403741, acc 0.859375, learning_rate 0.000106575
2017-10-09T15:09:08.868886: step 1619, loss 0.427258, acc 0.84375, learning_rate 0.000106548
2017-10-09T15:09:08.973348: step 1620, loss 0.378664, acc 0.859375, learning_rate 0.000106521
2017-10-09T15:09:09.080044: step 1621, loss 0.371398, acc 0.84375, learning_rate 0.000106495
2017-10-09T15:09:09.187862: step 1622, loss 0.485304, acc 0.828125, learning_rate 0.000106468
2017-10-09T15:09:09.300064: step 1623, loss 0.4107, acc 0.84375, learning_rate 0.000106442
2017-10-09T15:09:09.406451: step 1624, loss 0.40967, acc 0.828125, learning_rate 0.000106416
2017-10-09T15:09:09.506705: step 1625, loss 0.339737, acc 0.875, learning_rate 0.000106389
2017-10-09T15:09:09.612613: step 1626, loss 0.441957, acc 0.859375, learning_rate 0.000106363
2017-10-09T15:09:09.723950: step 1627, loss 0.360424, acc 0.859375, learning_rate 0.000106337
2017-10-09T15:09:09.826518: step 1628, loss 0.312384, acc 0.828125, learning_rate 0.000106312
2017-10-09T15:09:09.935078: step 1629, loss 0.300798, acc 0.875, learning_rate 0.000106286
2017-10-09T15:09:10.044147: step 1630, loss 0.260825, acc 0.90625, learning_rate 0.00010626
2017-10-09T15:09:10.150750: step 1631, loss 0.535254, acc 0.765625, learning_rate 0.000106235
2017-10-09T15:09:10.255756: step 1632, loss 0.320477, acc 0.859375, learning_rate 0.000106209
2017-10-09T15:09:10.365538: step 1633, loss 0.509157, acc 0.859375, learning_rate 0.000106184
2017-10-09T15:09:10.473028: step 1634, loss 0.311123, acc 0.890625, learning_rate 0.000106159
2017-10-09T15:09:10.580391: step 1635, loss 0.447091, acc 0.875, learning_rate 0.000106133
2017-10-09T15:09:10.687495: step 1636, loss 0.486036, acc 0.828125, learning_rate 0.000106108
2017-10-09T15:09:10.796610: step 1637, loss 0.418463, acc 0.859375, learning_rate 0.000106083
2017-10-09T15:09:10.905568: step 1638, loss 0.430292, acc 0.875, learning_rate 0.000106059
2017-10-09T15:09:11.011245: step 1639, loss 0.291749, acc 0.875, learning_rate 0.000106034
2017-10-09T15:09:11.115057: step 1640, loss 0.357516, acc 0.890625, learning_rate 0.000106009

Evaluation:
2017-10-09T15:09:11.378573: step 1640, loss 0.376428, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1640

2017-10-09T15:09:12.036973: step 1641, loss 0.429677, acc 0.84375, learning_rate 0.000105985
2017-10-09T15:09:12.144120: step 1642, loss 0.43358, acc 0.859375, learning_rate 0.00010596
2017-10-09T15:09:12.252568: step 1643, loss 0.354266, acc 0.8125, learning_rate 0.000105936
2017-10-09T15:09:12.358967: step 1644, loss 0.437002, acc 0.84375, learning_rate 0.000105912
2017-10-09T15:09:12.466595: step 1645, loss 0.306774, acc 0.875, learning_rate 0.000105888
2017-10-09T15:09:12.572250: step 1646, loss 0.522639, acc 0.84375, learning_rate 0.000105864
2017-10-09T15:09:12.681089: step 1647, loss 0.406383, acc 0.890625, learning_rate 0.00010584
2017-10-09T15:09:12.788714: step 1648, loss 0.330073, acc 0.875, learning_rate 0.000105816
2017-10-09T15:09:12.897347: step 1649, loss 0.563894, acc 0.78125, learning_rate 0.000105792
2017-10-09T15:09:13.004508: step 1650, loss 0.284145, acc 0.90625, learning_rate 0.000105768
2017-10-09T15:09:13.109616: step 1651, loss 0.32494, acc 0.890625, learning_rate 0.000105745
2017-10-09T15:09:13.214135: step 1652, loss 0.296995, acc 0.859375, learning_rate 0.000105721
2017-10-09T15:09:13.323573: step 1653, loss 0.572197, acc 0.8125, learning_rate 0.000105698
2017-10-09T15:09:13.428717: step 1654, loss 0.328563, acc 0.84375, learning_rate 0.000105675
2017-10-09T15:09:13.532324: step 1655, loss 0.232374, acc 0.953125, learning_rate 0.000105652
2017-10-09T15:09:13.635382: step 1656, loss 0.327141, acc 0.90625, learning_rate 0.000105629
2017-10-09T15:09:13.740270: step 1657, loss 0.270199, acc 0.90625, learning_rate 0.000105606
2017-10-09T15:09:13.848585: step 1658, loss 0.409941, acc 0.828125, learning_rate 0.000105583
2017-10-09T15:09:13.957344: step 1659, loss 0.278781, acc 0.890625, learning_rate 0.00010556
2017-10-09T15:09:14.067377: step 1660, loss 0.485799, acc 0.828125, learning_rate 0.000105537
2017-10-09T15:09:14.172512: step 1661, loss 0.340984, acc 0.90625, learning_rate 0.000105515
2017-10-09T15:09:14.275919: step 1662, loss 0.307957, acc 0.890625, learning_rate 0.000105492
2017-10-09T15:09:14.381110: step 1663, loss 0.389529, acc 0.84375, learning_rate 0.00010547
2017-10-09T15:09:14.488034: step 1664, loss 0.324945, acc 0.90625, learning_rate 0.000105447
2017-10-09T15:09:14.596325: step 1665, loss 0.325973, acc 0.90625, learning_rate 0.000105425
2017-10-09T15:09:14.685720: step 1666, loss 0.409702, acc 0.823529, learning_rate 0.000105403
2017-10-09T15:09:14.790131: step 1667, loss 0.382512, acc 0.90625, learning_rate 0.000105381
2017-10-09T15:09:14.897038: step 1668, loss 0.379101, acc 0.875, learning_rate 0.000105359
2017-10-09T15:09:15.002421: step 1669, loss 0.409415, acc 0.8125, learning_rate 0.000105337
2017-10-09T15:09:15.108510: step 1670, loss 0.3672, acc 0.890625, learning_rate 0.000105315
2017-10-09T15:09:15.216048: step 1671, loss 0.350058, acc 0.890625, learning_rate 0.000105294
2017-10-09T15:09:15.320167: step 1672, loss 0.456669, acc 0.828125, learning_rate 0.000105272
2017-10-09T15:09:15.424052: step 1673, loss 0.34904, acc 0.84375, learning_rate 0.000105251
2017-10-09T15:09:15.526490: step 1674, loss 0.399743, acc 0.828125, learning_rate 0.000105229
2017-10-09T15:09:15.633310: step 1675, loss 0.441737, acc 0.828125, learning_rate 0.000105208
2017-10-09T15:09:15.737970: step 1676, loss 0.364673, acc 0.890625, learning_rate 0.000105186
2017-10-09T15:09:15.841728: step 1677, loss 0.318312, acc 0.890625, learning_rate 0.000105165
2017-10-09T15:09:15.948783: step 1678, loss 0.470387, acc 0.859375, learning_rate 0.000105144
2017-10-09T15:09:16.056931: step 1679, loss 0.356069, acc 0.875, learning_rate 0.000105123
2017-10-09T15:09:16.156844: step 1680, loss 0.364061, acc 0.859375, learning_rate 0.000105102

Evaluation:
2017-10-09T15:09:16.413540: step 1680, loss 0.376659, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1680

2017-10-09T15:09:16.944105: step 1681, loss 0.552055, acc 0.703125, learning_rate 0.000105081
2017-10-09T15:09:17.052045: step 1682, loss 0.293005, acc 0.875, learning_rate 0.000105061
2017-10-09T15:09:17.162373: step 1683, loss 0.308102, acc 0.921875, learning_rate 0.00010504
2017-10-09T15:09:17.269208: step 1684, loss 0.353783, acc 0.90625, learning_rate 0.00010502
2017-10-09T15:09:17.376379: step 1685, loss 0.338079, acc 0.859375, learning_rate 0.000104999
2017-10-09T15:09:17.483145: step 1686, loss 0.329152, acc 0.875, learning_rate 0.000104979
2017-10-09T15:09:17.589638: step 1687, loss 0.404051, acc 0.875, learning_rate 0.000104958
2017-10-09T15:09:17.697264: step 1688, loss 0.483968, acc 0.78125, learning_rate 0.000104938
2017-10-09T15:09:17.801745: step 1689, loss 0.538258, acc 0.765625, learning_rate 0.000104918
2017-10-09T15:09:17.904447: step 1690, loss 0.447147, acc 0.84375, learning_rate 0.000104898
2017-10-09T15:09:18.012325: step 1691, loss 0.374288, acc 0.859375, learning_rate 0.000104878
2017-10-09T15:09:18.118819: step 1692, loss 0.257597, acc 0.921875, learning_rate 0.000104858
2017-10-09T15:09:18.228150: step 1693, loss 0.391701, acc 0.875, learning_rate 0.000104838
2017-10-09T15:09:18.339753: step 1694, loss 0.413656, acc 0.859375, learning_rate 0.000104818
2017-10-09T15:09:18.442318: step 1695, loss 0.256556, acc 0.921875, learning_rate 0.000104799
2017-10-09T15:09:18.549149: step 1696, loss 0.288036, acc 0.90625, learning_rate 0.000104779
2017-10-09T15:09:18.658486: step 1697, loss 0.28945, acc 0.90625, learning_rate 0.00010476
2017-10-09T15:09:18.764742: step 1698, loss 0.298061, acc 0.890625, learning_rate 0.00010474
2017-10-09T15:09:18.877309: step 1699, loss 0.612991, acc 0.78125, learning_rate 0.000104721
2017-10-09T15:09:18.983037: step 1700, loss 0.34946, acc 0.90625, learning_rate 0.000104702
2017-10-09T15:09:19.094214: step 1701, loss 0.378218, acc 0.890625, learning_rate 0.000104682
2017-10-09T15:09:19.203875: step 1702, loss 0.335558, acc 0.859375, learning_rate 0.000104663
2017-10-09T15:09:19.310983: step 1703, loss 0.330169, acc 0.84375, learning_rate 0.000104644
2017-10-09T15:09:19.418576: step 1704, loss 0.280799, acc 0.90625, learning_rate 0.000104625
2017-10-09T15:09:19.524029: step 1705, loss 0.34775, acc 0.890625, learning_rate 0.000104606
2017-10-09T15:09:19.632519: step 1706, loss 0.385703, acc 0.921875, learning_rate 0.000104588
2017-10-09T15:09:19.740314: step 1707, loss 0.485935, acc 0.78125, learning_rate 0.000104569
2017-10-09T15:09:19.848194: step 1708, loss 0.374647, acc 0.84375, learning_rate 0.00010455
2017-10-09T15:09:19.952806: step 1709, loss 0.509454, acc 0.8125, learning_rate 0.000104532
2017-10-09T15:09:20.059919: step 1710, loss 0.323016, acc 0.90625, learning_rate 0.000104513
2017-10-09T15:09:20.161513: step 1711, loss 0.232101, acc 0.921875, learning_rate 0.000104495
2017-10-09T15:09:20.267529: step 1712, loss 0.244299, acc 0.90625, learning_rate 0.000104476
2017-10-09T15:09:20.375846: step 1713, loss 0.381244, acc 0.84375, learning_rate 0.000104458
2017-10-09T15:09:20.484581: step 1714, loss 0.335177, acc 0.890625, learning_rate 0.00010444
2017-10-09T15:09:20.594067: step 1715, loss 0.405811, acc 0.828125, learning_rate 0.000104422
2017-10-09T15:09:20.700826: step 1716, loss 0.184091, acc 0.953125, learning_rate 0.000104404
2017-10-09T15:09:20.810176: step 1717, loss 0.41888, acc 0.859375, learning_rate 0.000104386
2017-10-09T15:09:20.924238: step 1718, loss 0.333093, acc 0.890625, learning_rate 0.000104368
2017-10-09T15:09:21.031381: step 1719, loss 0.505167, acc 0.84375, learning_rate 0.00010435
2017-10-09T15:09:21.139967: step 1720, loss 0.195641, acc 0.9375, learning_rate 0.000104332

Evaluation:
2017-10-09T15:09:21.393999: step 1720, loss 0.374044, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1720

2017-10-09T15:09:21.982862: step 1721, loss 0.351775, acc 0.875, learning_rate 0.000104315
2017-10-09T15:09:22.088164: step 1722, loss 0.36882, acc 0.859375, learning_rate 0.000104297
2017-10-09T15:09:22.194451: step 1723, loss 0.355585, acc 0.890625, learning_rate 0.000104279
2017-10-09T15:09:22.301442: step 1724, loss 0.433986, acc 0.828125, learning_rate 0.000104262
2017-10-09T15:09:22.405937: step 1725, loss 0.493629, acc 0.78125, learning_rate 0.000104245
2017-10-09T15:09:22.510584: step 1726, loss 0.255198, acc 0.890625, learning_rate 0.000104227
2017-10-09T15:09:22.616127: step 1727, loss 0.407173, acc 0.828125, learning_rate 0.00010421
2017-10-09T15:09:22.726351: step 1728, loss 0.506558, acc 0.84375, learning_rate 0.000104193
2017-10-09T15:09:22.834442: step 1729, loss 0.388709, acc 0.84375, learning_rate 0.000104176
2017-10-09T15:09:22.945100: step 1730, loss 0.299423, acc 0.875, learning_rate 0.000104159
2017-10-09T15:09:23.050546: step 1731, loss 0.517547, acc 0.859375, learning_rate 0.000104142
2017-10-09T15:09:23.153660: step 1732, loss 0.363211, acc 0.84375, learning_rate 0.000104125
2017-10-09T15:09:23.263287: step 1733, loss 0.443609, acc 0.8125, learning_rate 0.000104108
2017-10-09T15:09:23.369588: step 1734, loss 0.351439, acc 0.875, learning_rate 0.000104091
2017-10-09T15:09:23.473725: step 1735, loss 0.358851, acc 0.875, learning_rate 0.000104074
2017-10-09T15:09:23.582643: step 1736, loss 0.440496, acc 0.8125, learning_rate 0.000104058
2017-10-09T15:09:23.688840: step 1737, loss 0.397267, acc 0.84375, learning_rate 0.000104041
2017-10-09T15:09:23.793129: step 1738, loss 0.477577, acc 0.796875, learning_rate 0.000104025
2017-10-09T15:09:23.894994: step 1739, loss 0.286005, acc 0.921875, learning_rate 0.000104008
2017-10-09T15:09:24.008264: step 1740, loss 0.442031, acc 0.828125, learning_rate 0.000103992
2017-10-09T15:09:24.116266: step 1741, loss 0.523717, acc 0.8125, learning_rate 0.000103976
2017-10-09T15:09:24.222267: step 1742, loss 0.318349, acc 0.921875, learning_rate 0.000103959
2017-10-09T15:09:24.335527: step 1743, loss 0.390004, acc 0.84375, learning_rate 0.000103943
2017-10-09T15:09:24.445334: step 1744, loss 0.315949, acc 0.90625, learning_rate 0.000103927
2017-10-09T15:09:24.551734: step 1745, loss 0.285789, acc 0.90625, learning_rate 0.000103911
2017-10-09T15:09:24.656077: step 1746, loss 0.286504, acc 0.90625, learning_rate 0.000103895
2017-10-09T15:09:24.762043: step 1747, loss 0.408508, acc 0.859375, learning_rate 0.000103879
2017-10-09T15:09:24.873741: step 1748, loss 0.529293, acc 0.796875, learning_rate 0.000103863
2017-10-09T15:09:24.978704: step 1749, loss 0.484449, acc 0.828125, learning_rate 0.000103848
2017-10-09T15:09:25.083942: step 1750, loss 0.332808, acc 0.84375, learning_rate 0.000103832
2017-10-09T15:09:25.190973: step 1751, loss 0.292858, acc 0.90625, learning_rate 0.000103816
2017-10-09T15:09:25.294986: step 1752, loss 0.371045, acc 0.84375, learning_rate 0.000103801
2017-10-09T15:09:25.406887: step 1753, loss 0.338706, acc 0.890625, learning_rate 0.000103785
2017-10-09T15:09:25.517953: step 1754, loss 0.460177, acc 0.78125, learning_rate 0.00010377
2017-10-09T15:09:25.622510: step 1755, loss 0.245294, acc 0.9375, learning_rate 0.000103754
2017-10-09T15:09:25.731932: step 1756, loss 0.302913, acc 0.921875, learning_rate 0.000103739
2017-10-09T15:09:25.836610: step 1757, loss 0.56446, acc 0.78125, learning_rate 0.000103724
2017-10-09T15:09:25.942963: step 1758, loss 0.273243, acc 0.953125, learning_rate 0.000103709
2017-10-09T15:09:26.052474: step 1759, loss 0.35757, acc 0.84375, learning_rate 0.000103694
2017-10-09T15:09:26.158422: step 1760, loss 0.305809, acc 0.90625, learning_rate 0.000103678

Evaluation:
2017-10-09T15:09:26.411046: step 1760, loss 0.376075, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1760

2017-10-09T15:09:27.002435: step 1761, loss 0.475513, acc 0.8125, learning_rate 0.000103663
2017-10-09T15:09:27.110892: step 1762, loss 0.502676, acc 0.796875, learning_rate 0.000103648
2017-10-09T15:09:27.216002: step 1763, loss 0.398005, acc 0.875, learning_rate 0.000103634
2017-10-09T15:09:27.305493: step 1764, loss 0.313509, acc 0.862745, learning_rate 0.000103619
2017-10-09T15:09:27.413165: step 1765, loss 0.459345, acc 0.84375, learning_rate 0.000103604
2017-10-09T15:09:27.520336: step 1766, loss 0.498218, acc 0.8125, learning_rate 0.000103589
2017-10-09T15:09:27.630416: step 1767, loss 0.28771, acc 0.90625, learning_rate 0.000103575
2017-10-09T15:09:27.740692: step 1768, loss 0.462161, acc 0.828125, learning_rate 0.00010356
2017-10-09T15:09:27.848755: step 1769, loss 0.396048, acc 0.84375, learning_rate 0.000103545
2017-10-09T15:09:27.958067: step 1770, loss 0.356727, acc 0.875, learning_rate 0.000103531
2017-10-09T15:09:28.059315: step 1771, loss 0.348423, acc 0.875, learning_rate 0.000103517
2017-10-09T15:09:28.168941: step 1772, loss 0.297619, acc 0.84375, learning_rate 0.000103502
2017-10-09T15:09:28.271944: step 1773, loss 0.340317, acc 0.90625, learning_rate 0.000103488
2017-10-09T15:09:28.376187: step 1774, loss 0.382273, acc 0.8125, learning_rate 0.000103474
2017-10-09T15:09:28.485660: step 1775, loss 0.398212, acc 0.859375, learning_rate 0.00010346
2017-10-09T15:09:28.594931: step 1776, loss 0.356876, acc 0.875, learning_rate 0.000103445
2017-10-09T15:09:28.700265: step 1777, loss 0.639024, acc 0.71875, learning_rate 0.000103431
2017-10-09T15:09:28.809974: step 1778, loss 0.584278, acc 0.84375, learning_rate 0.000103417
2017-10-09T15:09:28.914305: step 1779, loss 0.596992, acc 0.78125, learning_rate 0.000103403
2017-10-09T15:09:29.020047: step 1780, loss 0.230487, acc 0.921875, learning_rate 0.00010339
2017-10-09T15:09:29.123184: step 1781, loss 0.424085, acc 0.796875, learning_rate 0.000103376
2017-10-09T15:09:29.220332: step 1782, loss 0.335731, acc 0.84375, learning_rate 0.000103362
2017-10-09T15:09:29.326195: step 1783, loss 0.410103, acc 0.828125, learning_rate 0.000103348
2017-10-09T15:09:29.432879: step 1784, loss 0.253923, acc 0.9375, learning_rate 0.000103335
2017-10-09T15:09:29.536013: step 1785, loss 0.327185, acc 0.890625, learning_rate 0.000103321
2017-10-09T15:09:29.642446: step 1786, loss 0.327249, acc 0.890625, learning_rate 0.000103307
2017-10-09T15:09:29.744982: step 1787, loss 0.482379, acc 0.796875, learning_rate 0.000103294
2017-10-09T15:09:29.852759: step 1788, loss 0.335667, acc 0.859375, learning_rate 0.00010328
2017-10-09T15:09:29.956270: step 1789, loss 0.301433, acc 0.921875, learning_rate 0.000103267
2017-10-09T15:09:30.065375: step 1790, loss 0.397422, acc 0.8125, learning_rate 0.000103254
2017-10-09T15:09:30.168587: step 1791, loss 0.531509, acc 0.828125, learning_rate 0.00010324
2017-10-09T15:09:30.277521: step 1792, loss 0.29142, acc 0.921875, learning_rate 0.000103227
2017-10-09T15:09:30.384473: step 1793, loss 0.547894, acc 0.78125, learning_rate 0.000103214
2017-10-09T15:09:30.488510: step 1794, loss 0.42559, acc 0.84375, learning_rate 0.000103201
2017-10-09T15:09:30.594169: step 1795, loss 0.424891, acc 0.84375, learning_rate 0.000103188
2017-10-09T15:09:30.699039: step 1796, loss 0.367848, acc 0.875, learning_rate 0.000103175
2017-10-09T15:09:30.804102: step 1797, loss 0.441915, acc 0.84375, learning_rate 0.000103162
2017-10-09T15:09:30.912781: step 1798, loss 0.459198, acc 0.828125, learning_rate 0.000103149
2017-10-09T15:09:31.017094: step 1799, loss 0.28475, acc 0.90625, learning_rate 0.000103136
2017-10-09T15:09:31.121674: step 1800, loss 0.33613, acc 0.875, learning_rate 0.000103123

Evaluation:
2017-10-09T15:09:31.381779: step 1800, loss 0.37252, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1800

2017-10-09T15:09:32.055367: step 1801, loss 0.414726, acc 0.84375, learning_rate 0.000103111
2017-10-09T15:09:32.172597: step 1802, loss 0.360616, acc 0.859375, learning_rate 0.000103098
2017-10-09T15:09:32.276138: step 1803, loss 0.29514, acc 0.90625, learning_rate 0.000103085
2017-10-09T15:09:32.379737: step 1804, loss 0.262356, acc 0.90625, learning_rate 0.000103073
2017-10-09T15:09:32.493557: step 1805, loss 0.347837, acc 0.875, learning_rate 0.00010306
2017-10-09T15:09:32.598772: step 1806, loss 0.344697, acc 0.875, learning_rate 0.000103048
2017-10-09T15:09:32.704625: step 1807, loss 0.486803, acc 0.828125, learning_rate 0.000103035
2017-10-09T15:09:32.812339: step 1808, loss 0.225359, acc 0.9375, learning_rate 0.000103023
2017-10-09T15:09:32.923391: step 1809, loss 0.29715, acc 0.9375, learning_rate 0.00010301
2017-10-09T15:09:33.032273: step 1810, loss 0.300787, acc 0.921875, learning_rate 0.000102998
2017-10-09T15:09:33.140349: step 1811, loss 0.364429, acc 0.859375, learning_rate 0.000102986
2017-10-09T15:09:33.249195: step 1812, loss 0.546739, acc 0.8125, learning_rate 0.000102974
2017-10-09T15:09:33.361323: step 1813, loss 0.351887, acc 0.890625, learning_rate 0.000102962
2017-10-09T15:09:33.468882: step 1814, loss 0.490545, acc 0.8125, learning_rate 0.000102949
2017-10-09T15:09:33.573748: step 1815, loss 0.448247, acc 0.84375, learning_rate 0.000102937
2017-10-09T15:09:33.678038: step 1816, loss 0.346871, acc 0.875, learning_rate 0.000102925
2017-10-09T15:09:33.787058: step 1817, loss 0.513901, acc 0.8125, learning_rate 0.000102913
2017-10-09T15:09:33.892817: step 1818, loss 0.463202, acc 0.765625, learning_rate 0.000102902
2017-10-09T15:09:33.996406: step 1819, loss 0.294649, acc 0.921875, learning_rate 0.00010289
2017-10-09T15:09:34.100388: step 1820, loss 0.202618, acc 0.9375, learning_rate 0.000102878
2017-10-09T15:09:34.208003: step 1821, loss 0.262113, acc 0.921875, learning_rate 0.000102866
2017-10-09T15:09:34.316621: step 1822, loss 0.334167, acc 0.875, learning_rate 0.000102855
2017-10-09T15:09:34.423334: step 1823, loss 0.574694, acc 0.828125, learning_rate 0.000102843
2017-10-09T15:09:34.529647: step 1824, loss 0.358559, acc 0.875, learning_rate 0.000102831
2017-10-09T15:09:34.636969: step 1825, loss 0.302774, acc 0.890625, learning_rate 0.00010282
2017-10-09T15:09:34.742021: step 1826, loss 0.378454, acc 0.84375, learning_rate 0.000102808
2017-10-09T15:09:34.844338: step 1827, loss 0.239158, acc 0.9375, learning_rate 0.000102797
2017-10-09T15:09:34.952306: step 1828, loss 0.394283, acc 0.84375, learning_rate 0.000102785
2017-10-09T15:09:35.057583: step 1829, loss 0.603834, acc 0.796875, learning_rate 0.000102774
2017-10-09T15:09:35.164796: step 1830, loss 0.417927, acc 0.828125, learning_rate 0.000102763
2017-10-09T15:09:35.274886: step 1831, loss 0.361935, acc 0.859375, learning_rate 0.000102751
2017-10-09T15:09:35.379761: step 1832, loss 0.451117, acc 0.859375, learning_rate 0.00010274
2017-10-09T15:09:35.487793: step 1833, loss 0.487827, acc 0.890625, learning_rate 0.000102729
2017-10-09T15:09:35.595393: step 1834, loss 0.229769, acc 0.96875, learning_rate 0.000102718
2017-10-09T15:09:35.703882: step 1835, loss 0.310964, acc 0.875, learning_rate 0.000102707
2017-10-09T15:09:35.810752: step 1836, loss 0.296315, acc 0.90625, learning_rate 0.000102696
2017-10-09T15:09:35.918104: step 1837, loss 0.361919, acc 0.875, learning_rate 0.000102685
2017-10-09T15:09:36.029953: step 1838, loss 0.395946, acc 0.859375, learning_rate 0.000102674
2017-10-09T15:09:36.135089: step 1839, loss 0.313693, acc 0.90625, learning_rate 0.000102663
2017-10-09T15:09:36.246216: step 1840, loss 0.184937, acc 0.953125, learning_rate 0.000102652

Evaluation:
2017-10-09T15:09:36.502827: step 1840, loss 0.373307, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1840

2017-10-09T15:09:37.031442: step 1841, loss 0.437106, acc 0.875, learning_rate 0.000102641
2017-10-09T15:09:37.139454: step 1842, loss 0.68023, acc 0.8125, learning_rate 0.00010263
2017-10-09T15:09:37.246075: step 1843, loss 0.362204, acc 0.875, learning_rate 0.00010262
2017-10-09T15:09:37.354780: step 1844, loss 0.173876, acc 0.953125, learning_rate 0.000102609
2017-10-09T15:09:37.461118: step 1845, loss 0.401986, acc 0.875, learning_rate 0.000102598
2017-10-09T15:09:37.569168: step 1846, loss 0.446191, acc 0.828125, learning_rate 0.000102588
2017-10-09T15:09:37.676397: step 1847, loss 0.336126, acc 0.90625, learning_rate 0.000102577
2017-10-09T15:09:37.784798: step 1848, loss 0.314189, acc 0.90625, learning_rate 0.000102567
2017-10-09T15:09:37.895800: step 1849, loss 0.30932, acc 0.890625, learning_rate 0.000102556
2017-10-09T15:09:38.002065: step 1850, loss 0.288724, acc 0.953125, learning_rate 0.000102546
2017-10-09T15:09:38.115009: step 1851, loss 0.361304, acc 0.859375, learning_rate 0.000102535
2017-10-09T15:09:38.228748: step 1852, loss 0.285219, acc 0.90625, learning_rate 0.000102525
2017-10-09T15:09:38.337054: step 1853, loss 0.268506, acc 0.953125, learning_rate 0.000102515
2017-10-09T15:09:38.440097: step 1854, loss 0.279249, acc 0.859375, learning_rate 0.000102504
2017-10-09T15:09:38.547534: step 1855, loss 0.314437, acc 0.890625, learning_rate 0.000102494
2017-10-09T15:09:38.666063: step 1856, loss 0.408867, acc 0.875, learning_rate 0.000102484
2017-10-09T15:09:38.774794: step 1857, loss 0.472234, acc 0.796875, learning_rate 0.000102474
2017-10-09T15:09:38.881971: step 1858, loss 0.407317, acc 0.796875, learning_rate 0.000102464
2017-10-09T15:09:38.986320: step 1859, loss 0.235669, acc 0.921875, learning_rate 0.000102454
2017-10-09T15:09:39.093395: step 1860, loss 0.370705, acc 0.921875, learning_rate 0.000102444
2017-10-09T15:09:39.203949: step 1861, loss 0.337759, acc 0.921875, learning_rate 0.000102434
2017-10-09T15:09:39.307874: step 1862, loss 0.382089, acc 0.882353, learning_rate 0.000102424
2017-10-09T15:09:39.424030: step 1863, loss 0.367529, acc 0.875, learning_rate 0.000102414
2017-10-09T15:09:39.538950: step 1864, loss 0.371261, acc 0.84375, learning_rate 0.000102404
2017-10-09T15:09:39.647494: step 1865, loss 0.3482, acc 0.859375, learning_rate 0.000102394
2017-10-09T15:09:39.756785: step 1866, loss 0.421299, acc 0.796875, learning_rate 0.000102384
2017-10-09T15:09:39.871136: step 1867, loss 0.395822, acc 0.84375, learning_rate 0.000102375
2017-10-09T15:09:39.979327: step 1868, loss 0.365875, acc 0.890625, learning_rate 0.000102365
2017-10-09T15:09:40.101515: step 1869, loss 0.349315, acc 0.890625, learning_rate 0.000102355
2017-10-09T15:09:40.215442: step 1870, loss 0.291905, acc 0.875, learning_rate 0.000102346
2017-10-09T15:09:40.330323: step 1871, loss 0.282069, acc 0.921875, learning_rate 0.000102336
2017-10-09T15:09:40.438694: step 1872, loss 0.32424, acc 0.90625, learning_rate 0.000102327
2017-10-09T15:09:40.548801: step 1873, loss 0.365647, acc 0.875, learning_rate 0.000102317
2017-10-09T15:09:40.661418: step 1874, loss 0.440546, acc 0.859375, learning_rate 0.000102308
2017-10-09T15:09:40.768197: step 1875, loss 0.52033, acc 0.78125, learning_rate 0.000102298
2017-10-09T15:09:40.883178: step 1876, loss 0.36019, acc 0.90625, learning_rate 0.000102289
2017-10-09T15:09:40.988596: step 1877, loss 0.343904, acc 0.875, learning_rate 0.000102279
2017-10-09T15:09:41.091858: step 1878, loss 0.360022, acc 0.828125, learning_rate 0.00010227
2017-10-09T15:09:41.199758: step 1879, loss 0.37715, acc 0.859375, learning_rate 0.000102261
2017-10-09T15:09:41.312749: step 1880, loss 0.311233, acc 0.875, learning_rate 0.000102252

Evaluation:
2017-10-09T15:09:41.584022: step 1880, loss 0.374573, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1880

2017-10-09T15:09:42.207757: step 1881, loss 0.230793, acc 0.921875, learning_rate 0.000102242
2017-10-09T15:09:42.312342: step 1882, loss 0.43095, acc 0.78125, learning_rate 0.000102233
2017-10-09T15:09:42.417665: step 1883, loss 0.402616, acc 0.828125, learning_rate 0.000102224
2017-10-09T15:09:42.526512: step 1884, loss 0.576893, acc 0.734375, learning_rate 0.000102215
2017-10-09T15:09:42.637172: step 1885, loss 0.339742, acc 0.890625, learning_rate 0.000102206
2017-10-09T15:09:42.744154: step 1886, loss 0.315107, acc 0.890625, learning_rate 0.000102197
2017-10-09T15:09:42.856991: step 1887, loss 0.351869, acc 0.921875, learning_rate 0.000102188
2017-10-09T15:09:42.979335: step 1888, loss 0.39099, acc 0.828125, learning_rate 0.000102179
2017-10-09T15:09:43.089132: step 1889, loss 0.259232, acc 0.90625, learning_rate 0.00010217
2017-10-09T15:09:43.200285: step 1890, loss 0.304496, acc 0.875, learning_rate 0.000102161
2017-10-09T15:09:43.305314: step 1891, loss 0.415273, acc 0.90625, learning_rate 0.000102153
2017-10-09T15:09:43.417518: step 1892, loss 0.466979, acc 0.828125, learning_rate 0.000102144
2017-10-09T15:09:43.528278: step 1893, loss 0.356915, acc 0.90625, learning_rate 0.000102135
2017-10-09T15:09:43.631919: step 1894, loss 0.273248, acc 0.875, learning_rate 0.000102126
2017-10-09T15:09:43.734463: step 1895, loss 0.351403, acc 0.84375, learning_rate 0.000102118
2017-10-09T15:09:43.842252: step 1896, loss 0.563079, acc 0.8125, learning_rate 0.000102109
2017-10-09T15:09:43.961066: step 1897, loss 0.517801, acc 0.84375, learning_rate 0.0001021
2017-10-09T15:09:44.087125: step 1898, loss 0.254841, acc 0.90625, learning_rate 0.000102092
2017-10-09T15:09:44.198619: step 1899, loss 0.336585, acc 0.90625, learning_rate 0.000102083
2017-10-09T15:09:44.310620: step 1900, loss 0.315197, acc 0.875, learning_rate 0.000102075
2017-10-09T15:09:44.420696: step 1901, loss 0.294063, acc 0.890625, learning_rate 0.000102066
2017-10-09T15:09:44.530413: step 1902, loss 0.492413, acc 0.8125, learning_rate 0.000102058
2017-10-09T15:09:44.632671: step 1903, loss 0.442206, acc 0.765625, learning_rate 0.00010205
2017-10-09T15:09:44.738395: step 1904, loss 0.339442, acc 0.859375, learning_rate 0.000102041
2017-10-09T15:09:44.843717: step 1905, loss 0.3504, acc 0.90625, learning_rate 0.000102033
2017-10-09T15:09:44.953148: step 1906, loss 0.384629, acc 0.890625, learning_rate 0.000102025
2017-10-09T15:09:45.058550: step 1907, loss 0.3055, acc 0.90625, learning_rate 0.000102016
2017-10-09T15:09:45.167693: step 1908, loss 0.395797, acc 0.828125, learning_rate 0.000102008
2017-10-09T15:09:45.275233: step 1909, loss 0.306558, acc 0.890625, learning_rate 0.000102
2017-10-09T15:09:45.385213: step 1910, loss 0.268872, acc 0.921875, learning_rate 0.000101992
2017-10-09T15:09:45.493638: step 1911, loss 0.445899, acc 0.828125, learning_rate 0.000101984
2017-10-09T15:09:45.606729: step 1912, loss 0.425474, acc 0.859375, learning_rate 0.000101975
2017-10-09T15:09:45.715575: step 1913, loss 0.356902, acc 0.875, learning_rate 0.000101967
2017-10-09T15:09:45.826050: step 1914, loss 0.263896, acc 0.90625, learning_rate 0.000101959
2017-10-09T15:09:45.933129: step 1915, loss 0.291747, acc 0.921875, learning_rate 0.000101951
2017-10-09T15:09:46.046733: step 1916, loss 0.407575, acc 0.890625, learning_rate 0.000101943
2017-10-09T15:09:46.156844: step 1917, loss 0.182912, acc 0.953125, learning_rate 0.000101935
2017-10-09T15:09:46.269034: step 1918, loss 0.400804, acc 0.828125, learning_rate 0.000101928
2017-10-09T15:09:46.380394: step 1919, loss 0.372854, acc 0.84375, learning_rate 0.00010192
2017-10-09T15:09:46.488962: step 1920, loss 0.364801, acc 0.875, learning_rate 0.000101912

Evaluation:
2017-10-09T15:09:46.767701: step 1920, loss 0.372623, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1920

2017-10-09T15:09:47.369476: step 1921, loss 0.307732, acc 0.890625, learning_rate 0.000101904
2017-10-09T15:09:47.475507: step 1922, loss 0.294086, acc 0.875, learning_rate 0.000101896
2017-10-09T15:09:47.584443: step 1923, loss 0.410291, acc 0.828125, learning_rate 0.000101889
2017-10-09T15:09:47.687243: step 1924, loss 0.376446, acc 0.875, learning_rate 0.000101881
2017-10-09T15:09:47.794140: step 1925, loss 0.439906, acc 0.828125, learning_rate 0.000101873
2017-10-09T15:09:47.898920: step 1926, loss 0.365759, acc 0.84375, learning_rate 0.000101865
2017-10-09T15:09:48.004429: step 1927, loss 0.374645, acc 0.875, learning_rate 0.000101858
2017-10-09T15:09:48.110546: step 1928, loss 0.457831, acc 0.828125, learning_rate 0.00010185
2017-10-09T15:09:48.214271: step 1929, loss 0.284856, acc 0.90625, learning_rate 0.000101843
2017-10-09T15:09:48.323490: step 1930, loss 0.410404, acc 0.859375, learning_rate 0.000101835
2017-10-09T15:09:48.430234: step 1931, loss 0.265473, acc 0.953125, learning_rate 0.000101828
2017-10-09T15:09:48.528588: step 1932, loss 0.490374, acc 0.796875, learning_rate 0.00010182
2017-10-09T15:09:48.636059: step 1933, loss 0.29059, acc 0.90625, learning_rate 0.000101813
2017-10-09T15:09:48.755846: step 1934, loss 0.515414, acc 0.8125, learning_rate 0.000101805
2017-10-09T15:09:48.865433: step 1935, loss 0.285813, acc 0.890625, learning_rate 0.000101798
2017-10-09T15:09:48.969744: step 1936, loss 0.258929, acc 0.90625, learning_rate 0.000101791
2017-10-09T15:09:49.076745: step 1937, loss 0.54473, acc 0.8125, learning_rate 0.000101783
2017-10-09T15:09:49.183050: step 1938, loss 0.319706, acc 0.84375, learning_rate 0.000101776
2017-10-09T15:09:49.288283: step 1939, loss 0.316959, acc 0.890625, learning_rate 0.000101769
2017-10-09T15:09:49.391112: step 1940, loss 0.261278, acc 0.921875, learning_rate 0.000101762
2017-10-09T15:09:49.496112: step 1941, loss 0.417493, acc 0.828125, learning_rate 0.000101754
2017-10-09T15:09:49.595863: step 1942, loss 0.219295, acc 0.953125, learning_rate 0.000101747
2017-10-09T15:09:49.700996: step 1943, loss 0.31585, acc 0.875, learning_rate 0.00010174
2017-10-09T15:09:49.805993: step 1944, loss 0.346154, acc 0.890625, learning_rate 0.000101733
2017-10-09T15:09:49.912634: step 1945, loss 0.343604, acc 0.859375, learning_rate 0.000101726
2017-10-09T15:09:50.019043: step 1946, loss 0.358904, acc 0.859375, learning_rate 0.000101719
2017-10-09T15:09:50.125381: step 1947, loss 0.42925, acc 0.828125, learning_rate 0.000101712
2017-10-09T15:09:50.230732: step 1948, loss 0.34812, acc 0.828125, learning_rate 0.000101705
2017-10-09T15:09:50.337268: step 1949, loss 0.47599, acc 0.828125, learning_rate 0.000101698
2017-10-09T15:09:50.439850: step 1950, loss 0.396623, acc 0.875, learning_rate 0.000101691
2017-10-09T15:09:50.539230: step 1951, loss 0.316769, acc 0.875, learning_rate 0.000101684
2017-10-09T15:09:50.643730: step 1952, loss 0.232906, acc 0.90625, learning_rate 0.000101677
2017-10-09T15:09:50.749866: step 1953, loss 0.456529, acc 0.796875, learning_rate 0.00010167
2017-10-09T15:09:50.859465: step 1954, loss 0.518288, acc 0.859375, learning_rate 0.000101664
2017-10-09T15:09:50.963687: step 1955, loss 0.464079, acc 0.8125, learning_rate 0.000101657
2017-10-09T15:09:51.072318: step 1956, loss 0.398048, acc 0.859375, learning_rate 0.00010165
2017-10-09T15:09:51.179064: step 1957, loss 0.35042, acc 0.84375, learning_rate 0.000101643
2017-10-09T15:09:51.284498: step 1958, loss 0.266658, acc 0.9375, learning_rate 0.000101637
2017-10-09T15:09:51.390149: step 1959, loss 0.511387, acc 0.875, learning_rate 0.00010163
2017-10-09T15:09:51.476693: step 1960, loss 0.43974, acc 0.862745, learning_rate 0.000101623

Evaluation:
2017-10-09T15:09:51.735770: step 1960, loss 0.372869, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-1960

2017-10-09T15:09:52.394219: step 1961, loss 0.480436, acc 0.828125, learning_rate 0.000101617
2017-10-09T15:09:52.500383: step 1962, loss 0.36021, acc 0.828125, learning_rate 0.00010161
2017-10-09T15:09:52.607861: step 1963, loss 0.335454, acc 0.90625, learning_rate 0.000101604
2017-10-09T15:09:52.713874: step 1964, loss 0.286157, acc 0.921875, learning_rate 0.000101597
2017-10-09T15:09:52.815268: step 1965, loss 0.347513, acc 0.859375, learning_rate 0.00010159
2017-10-09T15:09:52.924916: step 1966, loss 0.291794, acc 0.875, learning_rate 0.000101584
2017-10-09T15:09:53.028321: step 1967, loss 0.322653, acc 0.859375, learning_rate 0.000101577
2017-10-09T15:09:53.137259: step 1968, loss 0.300912, acc 0.84375, learning_rate 0.000101571
2017-10-09T15:09:53.237805: step 1969, loss 0.263955, acc 0.90625, learning_rate 0.000101565
2017-10-09T15:09:53.345272: step 1970, loss 0.464554, acc 0.828125, learning_rate 0.000101558
2017-10-09T15:09:53.452214: step 1971, loss 0.446723, acc 0.859375, learning_rate 0.000101552
2017-10-09T15:09:53.558961: step 1972, loss 0.477253, acc 0.8125, learning_rate 0.000101546
2017-10-09T15:09:53.662294: step 1973, loss 0.524711, acc 0.828125, learning_rate 0.000101539
2017-10-09T15:09:53.769389: step 1974, loss 0.307924, acc 0.875, learning_rate 0.000101533
2017-10-09T15:09:53.877447: step 1975, loss 0.332949, acc 0.875, learning_rate 0.000101527
2017-10-09T15:09:53.985810: step 1976, loss 0.346711, acc 0.90625, learning_rate 0.00010152
2017-10-09T15:09:54.093270: step 1977, loss 0.317384, acc 0.875, learning_rate 0.000101514
2017-10-09T15:09:54.198712: step 1978, loss 0.359605, acc 0.875, learning_rate 0.000101508
2017-10-09T15:09:54.304920: step 1979, loss 0.355063, acc 0.890625, learning_rate 0.000101502
2017-10-09T15:09:54.405917: step 1980, loss 0.424882, acc 0.875, learning_rate 0.000101496
2017-10-09T15:09:54.509993: step 1981, loss 0.297332, acc 0.90625, learning_rate 0.00010149
2017-10-09T15:09:54.615621: step 1982, loss 0.295848, acc 0.890625, learning_rate 0.000101484
2017-10-09T15:09:54.721457: step 1983, loss 0.193514, acc 0.9375, learning_rate 0.000101478
2017-10-09T15:09:54.820981: step 1984, loss 0.434888, acc 0.84375, learning_rate 0.000101472
2017-10-09T15:09:54.926074: step 1985, loss 0.309445, acc 0.90625, learning_rate 0.000101466
2017-10-09T15:09:55.031359: step 1986, loss 0.382038, acc 0.828125, learning_rate 0.00010146
2017-10-09T15:09:55.135550: step 1987, loss 0.450275, acc 0.84375, learning_rate 0.000101454
2017-10-09T15:09:55.241649: step 1988, loss 0.307655, acc 0.921875, learning_rate 0.000101448
2017-10-09T15:09:55.342793: step 1989, loss 0.352169, acc 0.890625, learning_rate 0.000101442
2017-10-09T15:09:55.452616: step 1990, loss 0.377284, acc 0.84375, learning_rate 0.000101436
2017-10-09T15:09:55.559189: step 1991, loss 0.379993, acc 0.78125, learning_rate 0.00010143
2017-10-09T15:09:55.665812: step 1992, loss 0.353401, acc 0.890625, learning_rate 0.000101424
2017-10-09T15:09:55.769393: step 1993, loss 0.531688, acc 0.765625, learning_rate 0.000101418
2017-10-09T15:09:55.876919: step 1994, loss 0.315547, acc 0.875, learning_rate 0.000101413
2017-10-09T15:09:55.983140: step 1995, loss 0.433273, acc 0.84375, learning_rate 0.000101407
2017-10-09T15:09:56.088455: step 1996, loss 0.372517, acc 0.84375, learning_rate 0.000101401
2017-10-09T15:09:56.193956: step 1997, loss 0.332, acc 0.890625, learning_rate 0.000101395
2017-10-09T15:09:56.300531: step 1998, loss 0.423731, acc 0.859375, learning_rate 0.00010139
2017-10-09T15:09:56.403488: step 1999, loss 0.237165, acc 0.921875, learning_rate 0.000101384
2017-10-09T15:09:56.510967: step 2000, loss 0.366616, acc 0.890625, learning_rate 0.000101378

Evaluation:
2017-10-09T15:09:56.766472: step 2000, loss 0.368479, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2000

2017-10-09T15:09:57.291244: step 2001, loss 0.258621, acc 0.90625, learning_rate 0.000101373
2017-10-09T15:09:57.398956: step 2002, loss 0.293689, acc 0.90625, learning_rate 0.000101367
2017-10-09T15:09:57.502418: step 2003, loss 0.424703, acc 0.84375, learning_rate 0.000101362
2017-10-09T15:09:57.605942: step 2004, loss 0.51903, acc 0.828125, learning_rate 0.000101356
2017-10-09T15:09:57.707591: step 2005, loss 0.3425, acc 0.890625, learning_rate 0.00010135
2017-10-09T15:09:57.813929: step 2006, loss 0.218006, acc 0.953125, learning_rate 0.000101345
2017-10-09T15:09:57.919684: step 2007, loss 0.268837, acc 0.921875, learning_rate 0.000101339
2017-10-09T15:09:58.026233: step 2008, loss 0.326894, acc 0.890625, learning_rate 0.000101334
2017-10-09T15:09:58.132638: step 2009, loss 0.357308, acc 0.859375, learning_rate 0.000101328
2017-10-09T15:09:58.233432: step 2010, loss 0.442907, acc 0.859375, learning_rate 0.000101323
2017-10-09T15:09:58.341154: step 2011, loss 0.269029, acc 0.953125, learning_rate 0.000101318
2017-10-09T15:09:58.446565: step 2012, loss 0.236724, acc 0.890625, learning_rate 0.000101312
2017-10-09T15:09:58.554625: step 2013, loss 0.46404, acc 0.84375, learning_rate 0.000101307
2017-10-09T15:09:58.661434: step 2014, loss 0.451799, acc 0.875, learning_rate 0.000101302
2017-10-09T15:09:58.766931: step 2015, loss 0.427393, acc 0.828125, learning_rate 0.000101296
2017-10-09T15:09:58.873533: step 2016, loss 0.26894, acc 0.890625, learning_rate 0.000101291
2017-10-09T15:09:58.977141: step 2017, loss 0.257625, acc 0.90625, learning_rate 0.000101286
2017-10-09T15:09:59.080138: step 2018, loss 0.529133, acc 0.8125, learning_rate 0.00010128
2017-10-09T15:09:59.188485: step 2019, loss 0.455966, acc 0.828125, learning_rate 0.000101275
2017-10-09T15:09:59.291864: step 2020, loss 0.249231, acc 0.953125, learning_rate 0.00010127
2017-10-09T15:09:59.399255: step 2021, loss 0.338228, acc 0.859375, learning_rate 0.000101265
2017-10-09T15:09:59.503648: step 2022, loss 0.418042, acc 0.859375, learning_rate 0.00010126
2017-10-09T15:09:59.619358: step 2023, loss 0.593194, acc 0.78125, learning_rate 0.000101255
2017-10-09T15:09:59.726104: step 2024, loss 0.487537, acc 0.796875, learning_rate 0.000101249
2017-10-09T15:09:59.832400: step 2025, loss 0.282516, acc 0.90625, learning_rate 0.000101244
2017-10-09T15:09:59.938154: step 2026, loss 0.473265, acc 0.859375, learning_rate 0.000101239
2017-10-09T15:10:00.042996: step 2027, loss 0.293543, acc 0.921875, learning_rate 0.000101234
2017-10-09T15:10:00.149921: step 2028, loss 0.34392, acc 0.859375, learning_rate 0.000101229
2017-10-09T15:10:00.256532: step 2029, loss 0.323591, acc 0.875, learning_rate 0.000101224
2017-10-09T15:10:00.362217: step 2030, loss 0.252096, acc 0.90625, learning_rate 0.000101219
2017-10-09T15:10:00.467593: step 2031, loss 0.395876, acc 0.859375, learning_rate 0.000101214
2017-10-09T15:10:00.571068: step 2032, loss 0.380154, acc 0.859375, learning_rate 0.000101209
2017-10-09T15:10:00.675317: step 2033, loss 0.335401, acc 0.890625, learning_rate 0.000101204
2017-10-09T15:10:00.778036: step 2034, loss 0.255778, acc 0.90625, learning_rate 0.000101199
2017-10-09T15:10:00.885614: step 2035, loss 0.268474, acc 0.90625, learning_rate 0.000101194
2017-10-09T15:10:00.991927: step 2036, loss 0.338373, acc 0.875, learning_rate 0.00010119
2017-10-09T15:10:01.095010: step 2037, loss 0.515607, acc 0.796875, learning_rate 0.000101185
2017-10-09T15:10:01.201796: step 2038, loss 0.294699, acc 0.90625, learning_rate 0.00010118
2017-10-09T15:10:01.307778: step 2039, loss 0.292453, acc 0.90625, learning_rate 0.000101175
2017-10-09T15:10:01.412664: step 2040, loss 0.499354, acc 0.8125, learning_rate 0.00010117

Evaluation:
2017-10-09T15:10:01.667256: step 2040, loss 0.369591, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2040

2017-10-09T15:10:02.260209: step 2041, loss 0.295945, acc 0.875, learning_rate 0.000101166
2017-10-09T15:10:02.366233: step 2042, loss 0.251459, acc 0.9375, learning_rate 0.000101161
2017-10-09T15:10:02.473723: step 2043, loss 0.354147, acc 0.859375, learning_rate 0.000101156
2017-10-09T15:10:02.580423: step 2044, loss 0.252216, acc 0.90625, learning_rate 0.000101151
2017-10-09T15:10:02.686166: step 2045, loss 0.41116, acc 0.859375, learning_rate 0.000101147
2017-10-09T15:10:02.795010: step 2046, loss 0.381254, acc 0.8125, learning_rate 0.000101142
2017-10-09T15:10:02.906898: step 2047, loss 0.47261, acc 0.78125, learning_rate 0.000101137
2017-10-09T15:10:03.011878: step 2048, loss 0.331055, acc 0.9375, learning_rate 0.000101133
2017-10-09T15:10:03.117622: step 2049, loss 0.381672, acc 0.875, learning_rate 0.000101128
2017-10-09T15:10:03.224465: step 2050, loss 0.195203, acc 0.953125, learning_rate 0.000101123
2017-10-09T15:10:03.330953: step 2051, loss 0.323443, acc 0.875, learning_rate 0.000101119
2017-10-09T15:10:03.436711: step 2052, loss 0.241707, acc 0.90625, learning_rate 0.000101114
2017-10-09T15:10:03.543737: step 2053, loss 0.34462, acc 0.859375, learning_rate 0.00010111
2017-10-09T15:10:03.649540: step 2054, loss 0.423437, acc 0.84375, learning_rate 0.000101105
2017-10-09T15:10:03.756114: step 2055, loss 0.382533, acc 0.859375, learning_rate 0.000101101
2017-10-09T15:10:03.865691: step 2056, loss 0.48593, acc 0.8125, learning_rate 0.000101096
2017-10-09T15:10:03.968354: step 2057, loss 0.480871, acc 0.8125, learning_rate 0.000101092
2017-10-09T15:10:04.055921: step 2058, loss 0.191842, acc 0.941176, learning_rate 0.000101087
2017-10-09T15:10:04.161863: step 2059, loss 0.311121, acc 0.921875, learning_rate 0.000101083
2017-10-09T15:10:04.268492: step 2060, loss 0.395068, acc 0.875, learning_rate 0.000101078
2017-10-09T15:10:04.375455: step 2061, loss 0.344993, acc 0.875, learning_rate 0.000101074
2017-10-09T15:10:04.479067: step 2062, loss 0.30097, acc 0.90625, learning_rate 0.00010107
2017-10-09T15:10:04.586210: step 2063, loss 0.230458, acc 0.953125, learning_rate 0.000101065
2017-10-09T15:10:04.692552: step 2064, loss 0.415481, acc 0.828125, learning_rate 0.000101061
2017-10-09T15:10:04.797280: step 2065, loss 0.278351, acc 0.90625, learning_rate 0.000101057
2017-10-09T15:10:04.903020: step 2066, loss 0.218119, acc 0.921875, learning_rate 0.000101052
2017-10-09T15:10:05.008052: step 2067, loss 0.315887, acc 0.875, learning_rate 0.000101048
2017-10-09T15:10:05.113235: step 2068, loss 0.319234, acc 0.890625, learning_rate 0.000101044
2017-10-09T15:10:05.221798: step 2069, loss 0.297829, acc 0.890625, learning_rate 0.000101039
2017-10-09T15:10:05.324905: step 2070, loss 0.360456, acc 0.90625, learning_rate 0.000101035
2017-10-09T15:10:05.431121: step 2071, loss 0.255641, acc 0.921875, learning_rate 0.000101031
2017-10-09T15:10:05.536074: step 2072, loss 0.420807, acc 0.828125, learning_rate 0.000101027
2017-10-09T15:10:05.638186: step 2073, loss 0.370973, acc 0.875, learning_rate 0.000101023
2017-10-09T15:10:05.740314: step 2074, loss 0.479555, acc 0.84375, learning_rate 0.000101018
2017-10-09T15:10:05.849064: step 2075, loss 0.346747, acc 0.90625, learning_rate 0.000101014
2017-10-09T15:10:05.955658: step 2076, loss 0.29565, acc 0.890625, learning_rate 0.00010101
2017-10-09T15:10:06.060962: step 2077, loss 0.339583, acc 0.890625, learning_rate 0.000101006
2017-10-09T15:10:06.169540: step 2078, loss 0.239665, acc 0.890625, learning_rate 0.000101002
2017-10-09T15:10:06.275134: step 2079, loss 0.378478, acc 0.859375, learning_rate 0.000100998
2017-10-09T15:10:06.380064: step 2080, loss 0.379951, acc 0.84375, learning_rate 0.000100994

Evaluation:
2017-10-09T15:10:06.637942: step 2080, loss 0.367161, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2080

2017-10-09T15:10:07.225924: step 2081, loss 0.38082, acc 0.859375, learning_rate 0.00010099
2017-10-09T15:10:07.331412: step 2082, loss 0.350571, acc 0.859375, learning_rate 0.000100986
2017-10-09T15:10:07.438742: step 2083, loss 0.312122, acc 0.890625, learning_rate 0.000100982
2017-10-09T15:10:07.544597: step 2084, loss 0.449251, acc 0.796875, learning_rate 0.000100978
2017-10-09T15:10:07.652047: step 2085, loss 0.353047, acc 0.875, learning_rate 0.000100974
2017-10-09T15:10:07.753800: step 2086, loss 0.279571, acc 0.921875, learning_rate 0.00010097
2017-10-09T15:10:07.860981: step 2087, loss 0.420457, acc 0.875, learning_rate 0.000100966
2017-10-09T15:10:07.964683: step 2088, loss 0.413031, acc 0.828125, learning_rate 0.000100962
2017-10-09T15:10:08.070746: step 2089, loss 0.506422, acc 0.8125, learning_rate 0.000100958
2017-10-09T15:10:08.172611: step 2090, loss 0.417161, acc 0.828125, learning_rate 0.000100954
2017-10-09T15:10:08.279086: step 2091, loss 0.43958, acc 0.84375, learning_rate 0.00010095
2017-10-09T15:10:08.385798: step 2092, loss 0.414681, acc 0.90625, learning_rate 0.000100946
2017-10-09T15:10:08.490886: step 2093, loss 0.396448, acc 0.859375, learning_rate 0.000100942
2017-10-09T15:10:08.595927: step 2094, loss 0.166404, acc 0.953125, learning_rate 0.000100938
2017-10-09T15:10:08.705253: step 2095, loss 0.287351, acc 0.90625, learning_rate 0.000100935
2017-10-09T15:10:08.808966: step 2096, loss 0.310996, acc 0.921875, learning_rate 0.000100931
2017-10-09T15:10:08.918805: step 2097, loss 0.292282, acc 0.921875, learning_rate 0.000100927
2017-10-09T15:10:09.023973: step 2098, loss 0.23569, acc 0.9375, learning_rate 0.000100923
2017-10-09T15:10:09.129784: step 2099, loss 0.372165, acc 0.875, learning_rate 0.000100919
2017-10-09T15:10:09.236894: step 2100, loss 0.211171, acc 0.9375, learning_rate 0.000100916
2017-10-09T15:10:09.344379: step 2101, loss 0.469745, acc 0.796875, learning_rate 0.000100912
2017-10-09T15:10:09.445679: step 2102, loss 0.373736, acc 0.875, learning_rate 0.000100908
2017-10-09T15:10:09.547212: step 2103, loss 0.392793, acc 0.84375, learning_rate 0.000100904
2017-10-09T15:10:09.653916: step 2104, loss 0.430334, acc 0.828125, learning_rate 0.000100901
2017-10-09T15:10:09.762848: step 2105, loss 0.305115, acc 0.875, learning_rate 0.000100897
2017-10-09T15:10:09.870701: step 2106, loss 0.490751, acc 0.828125, learning_rate 0.000100893
2017-10-09T15:10:09.974806: step 2107, loss 0.334639, acc 0.875, learning_rate 0.00010089
2017-10-09T15:10:10.080717: step 2108, loss 0.396837, acc 0.875, learning_rate 0.000100886
2017-10-09T15:10:10.187811: step 2109, loss 0.317996, acc 0.890625, learning_rate 0.000100883
2017-10-09T15:10:10.291015: step 2110, loss 0.356738, acc 0.90625, learning_rate 0.000100879
2017-10-09T15:10:10.397237: step 2111, loss 0.266431, acc 0.90625, learning_rate 0.000100875
2017-10-09T15:10:10.504360: step 2112, loss 0.267114, acc 0.90625, learning_rate 0.000100872
2017-10-09T15:10:10.611804: step 2113, loss 0.374975, acc 0.84375, learning_rate 0.000100868
2017-10-09T15:10:10.716423: step 2114, loss 0.222035, acc 0.953125, learning_rate 0.000100865
2017-10-09T15:10:10.820578: step 2115, loss 0.387121, acc 0.890625, learning_rate 0.000100861
2017-10-09T15:10:10.929107: step 2116, loss 0.387388, acc 0.875, learning_rate 0.000100858
2017-10-09T15:10:11.039895: step 2117, loss 0.200674, acc 0.953125, learning_rate 0.000100854
2017-10-09T15:10:11.143941: step 2118, loss 0.238758, acc 0.90625, learning_rate 0.000100851
2017-10-09T15:10:11.252894: step 2119, loss 0.42684, acc 0.859375, learning_rate 0.000100847
2017-10-09T15:10:11.358176: step 2120, loss 0.350298, acc 0.890625, learning_rate 0.000100844

Evaluation:
2017-10-09T15:10:11.620429: step 2120, loss 0.367602, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2120

2017-10-09T15:10:12.284939: step 2121, loss 0.228459, acc 0.890625, learning_rate 0.00010084
2017-10-09T15:10:12.391260: step 2122, loss 0.285336, acc 0.875, learning_rate 0.000100837
2017-10-09T15:10:12.496621: step 2123, loss 0.318128, acc 0.875, learning_rate 0.000100833
2017-10-09T15:10:12.600573: step 2124, loss 0.248547, acc 0.921875, learning_rate 0.00010083
2017-10-09T15:10:12.705580: step 2125, loss 0.468905, acc 0.859375, learning_rate 0.000100827
2017-10-09T15:10:12.812166: step 2126, loss 0.390617, acc 0.859375, learning_rate 0.000100823
2017-10-09T15:10:12.922224: step 2127, loss 0.438556, acc 0.859375, learning_rate 0.00010082
2017-10-09T15:10:13.023784: step 2128, loss 0.266756, acc 0.921875, learning_rate 0.000100817
2017-10-09T15:10:13.128602: step 2129, loss 0.384407, acc 0.875, learning_rate 0.000100813
2017-10-09T15:10:13.234856: step 2130, loss 0.249702, acc 0.921875, learning_rate 0.00010081
2017-10-09T15:10:13.340445: step 2131, loss 0.409268, acc 0.828125, learning_rate 0.000100807
2017-10-09T15:10:13.442956: step 2132, loss 0.467647, acc 0.84375, learning_rate 0.000100803
2017-10-09T15:10:13.551313: step 2133, loss 0.38041, acc 0.84375, learning_rate 0.0001008
2017-10-09T15:10:13.658143: step 2134, loss 0.256879, acc 0.921875, learning_rate 0.000100797
2017-10-09T15:10:13.760092: step 2135, loss 0.322447, acc 0.875, learning_rate 0.000100793
2017-10-09T15:10:13.866901: step 2136, loss 0.490876, acc 0.859375, learning_rate 0.00010079
2017-10-09T15:10:13.971323: step 2137, loss 0.369692, acc 0.890625, learning_rate 0.000100787
2017-10-09T15:10:14.077280: step 2138, loss 0.305774, acc 0.859375, learning_rate 0.000100784
2017-10-09T15:10:14.182443: step 2139, loss 0.431786, acc 0.8125, learning_rate 0.000100781
2017-10-09T15:10:14.280003: step 2140, loss 0.458956, acc 0.828125, learning_rate 0.000100777
2017-10-09T15:10:14.386545: step 2141, loss 0.437578, acc 0.828125, learning_rate 0.000100774
2017-10-09T15:10:14.492569: step 2142, loss 0.515649, acc 0.734375, learning_rate 0.000100771
2017-10-09T15:10:14.596531: step 2143, loss 0.356977, acc 0.875, learning_rate 0.000100768
2017-10-09T15:10:14.706076: step 2144, loss 0.433639, acc 0.84375, learning_rate 0.000100765
2017-10-09T15:10:14.814265: step 2145, loss 0.460113, acc 0.828125, learning_rate 0.000100762
2017-10-09T15:10:14.924187: step 2146, loss 0.420622, acc 0.875, learning_rate 0.000100759
2017-10-09T15:10:15.030774: step 2147, loss 0.341728, acc 0.875, learning_rate 0.000100755
2017-10-09T15:10:15.139624: step 2148, loss 0.430129, acc 0.8125, learning_rate 0.000100752
2017-10-09T15:10:15.247940: step 2149, loss 0.334971, acc 0.859375, learning_rate 0.000100749
2017-10-09T15:10:15.355246: step 2150, loss 0.386786, acc 0.90625, learning_rate 0.000100746
2017-10-09T15:10:15.461930: step 2151, loss 0.245032, acc 0.921875, learning_rate 0.000100743
2017-10-09T15:10:15.572440: step 2152, loss 0.205638, acc 0.921875, learning_rate 0.00010074
2017-10-09T15:10:15.681593: step 2153, loss 0.487483, acc 0.859375, learning_rate 0.000100737
2017-10-09T15:10:15.788400: step 2154, loss 0.342125, acc 0.84375, learning_rate 0.000100734
2017-10-09T15:10:15.898791: step 2155, loss 0.474277, acc 0.859375, learning_rate 0.000100731
2017-10-09T15:10:15.991861: step 2156, loss 0.206711, acc 0.941176, learning_rate 0.000100728
2017-10-09T15:10:16.101131: step 2157, loss 0.323763, acc 0.875, learning_rate 0.000100725
2017-10-09T15:10:16.205856: step 2158, loss 0.310727, acc 0.90625, learning_rate 0.000100722
2017-10-09T15:10:16.308373: step 2159, loss 0.442955, acc 0.875, learning_rate 0.000100719
2017-10-09T15:10:16.414227: step 2160, loss 0.417087, acc 0.875, learning_rate 0.000100716

Evaluation:
2017-10-09T15:10:16.667617: step 2160, loss 0.365943, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2160

2017-10-09T15:10:17.185671: step 2161, loss 0.438877, acc 0.875, learning_rate 0.000100713
2017-10-09T15:10:17.291054: step 2162, loss 0.465401, acc 0.84375, learning_rate 0.000100711
2017-10-09T15:10:17.395225: step 2163, loss 0.335372, acc 0.859375, learning_rate 0.000100708
2017-10-09T15:10:17.502675: step 2164, loss 0.361551, acc 0.890625, learning_rate 0.000100705
2017-10-09T15:10:17.606203: step 2165, loss 0.271343, acc 0.859375, learning_rate 0.000100702
2017-10-09T15:10:17.708158: step 2166, loss 0.502604, acc 0.8125, learning_rate 0.000100699
2017-10-09T15:10:17.814659: step 2167, loss 0.418771, acc 0.84375, learning_rate 0.000100696
2017-10-09T15:10:17.921144: step 2168, loss 0.453287, acc 0.84375, learning_rate 0.000100693
2017-10-09T15:10:18.027544: step 2169, loss 0.211565, acc 0.9375, learning_rate 0.00010069
2017-10-09T15:10:18.133424: step 2170, loss 0.388872, acc 0.890625, learning_rate 0.000100688
2017-10-09T15:10:18.240440: step 2171, loss 0.394319, acc 0.84375, learning_rate 0.000100685
2017-10-09T15:10:18.344362: step 2172, loss 0.390783, acc 0.84375, learning_rate 0.000100682
2017-10-09T15:10:18.448152: step 2173, loss 0.365453, acc 0.859375, learning_rate 0.000100679
2017-10-09T15:10:18.551739: step 2174, loss 0.58412, acc 0.78125, learning_rate 0.000100677
2017-10-09T15:10:18.655598: step 2175, loss 0.45379, acc 0.828125, learning_rate 0.000100674
2017-10-09T15:10:18.762414: step 2176, loss 0.262801, acc 0.890625, learning_rate 0.000100671
2017-10-09T15:10:18.868660: step 2177, loss 0.350951, acc 0.859375, learning_rate 0.000100668
2017-10-09T15:10:18.971912: step 2178, loss 0.459741, acc 0.828125, learning_rate 0.000100666
2017-10-09T15:10:19.076202: step 2179, loss 0.374014, acc 0.875, learning_rate 0.000100663
2017-10-09T15:10:19.183682: step 2180, loss 0.342511, acc 0.890625, learning_rate 0.00010066
2017-10-09T15:10:19.293153: step 2181, loss 0.368761, acc 0.90625, learning_rate 0.000100657
2017-10-09T15:10:19.400823: step 2182, loss 0.378938, acc 0.890625, learning_rate 0.000100655
2017-10-09T15:10:19.506945: step 2183, loss 0.277266, acc 0.890625, learning_rate 0.000100652
2017-10-09T15:10:19.611376: step 2184, loss 0.255326, acc 0.9375, learning_rate 0.000100649
2017-10-09T15:10:19.717716: step 2185, loss 0.367014, acc 0.84375, learning_rate 0.000100647
2017-10-09T15:10:19.825177: step 2186, loss 0.271849, acc 0.921875, learning_rate 0.000100644
2017-10-09T15:10:19.928212: step 2187, loss 0.30424, acc 0.875, learning_rate 0.000100641
2017-10-09T15:10:20.034653: step 2188, loss 0.296508, acc 0.890625, learning_rate 0.000100639
2017-10-09T15:10:20.139035: step 2189, loss 0.55596, acc 0.8125, learning_rate 0.000100636
2017-10-09T15:10:20.246079: step 2190, loss 0.433233, acc 0.796875, learning_rate 0.000100634
2017-10-09T15:10:20.351736: step 2191, loss 0.301585, acc 0.9375, learning_rate 0.000100631
2017-10-09T15:10:20.457067: step 2192, loss 0.328693, acc 0.890625, learning_rate 0.000100628
2017-10-09T15:10:20.560199: step 2193, loss 0.235261, acc 0.9375, learning_rate 0.000100626
2017-10-09T15:10:20.667069: step 2194, loss 0.307802, acc 0.90625, learning_rate 0.000100623
2017-10-09T15:10:20.772257: step 2195, loss 0.299062, acc 0.890625, learning_rate 0.000100621
2017-10-09T15:10:20.880652: step 2196, loss 0.41907, acc 0.828125, learning_rate 0.000100618
2017-10-09T15:10:20.983613: step 2197, loss 0.285936, acc 0.859375, learning_rate 0.000100616
2017-10-09T15:10:21.088657: step 2198, loss 0.230822, acc 0.921875, learning_rate 0.000100613
2017-10-09T15:10:21.193694: step 2199, loss 0.341928, acc 0.875, learning_rate 0.000100611
2017-10-09T15:10:21.300752: step 2200, loss 0.199519, acc 0.96875, learning_rate 0.000100608

Evaluation:
2017-10-09T15:10:21.552016: step 2200, loss 0.364095, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2200

2017-10-09T15:10:22.135408: step 2201, loss 0.374412, acc 0.828125, learning_rate 0.000100606
2017-10-09T15:10:22.235670: step 2202, loss 0.48089, acc 0.84375, learning_rate 0.000100603
2017-10-09T15:10:22.339677: step 2203, loss 0.461018, acc 0.78125, learning_rate 0.000100601
2017-10-09T15:10:22.442709: step 2204, loss 0.225079, acc 0.921875, learning_rate 0.000100598
2017-10-09T15:10:22.550479: step 2205, loss 0.290711, acc 0.890625, learning_rate 0.000100596
2017-10-09T15:10:22.656454: step 2206, loss 0.190067, acc 0.953125, learning_rate 0.000100594
2017-10-09T15:10:22.761439: step 2207, loss 0.321778, acc 0.90625, learning_rate 0.000100591
2017-10-09T15:10:22.864650: step 2208, loss 0.403329, acc 0.828125, learning_rate 0.000100589
2017-10-09T15:10:22.971066: step 2209, loss 0.222534, acc 0.90625, learning_rate 0.000100586
2017-10-09T15:10:23.078450: step 2210, loss 0.368536, acc 0.890625, learning_rate 0.000100584
2017-10-09T15:10:23.182638: step 2211, loss 0.33121, acc 0.875, learning_rate 0.000100581
2017-10-09T15:10:23.290738: step 2212, loss 0.362254, acc 0.875, learning_rate 0.000100579
2017-10-09T15:10:23.396177: step 2213, loss 0.267208, acc 0.953125, learning_rate 0.000100577
2017-10-09T15:10:23.498600: step 2214, loss 0.321348, acc 0.890625, learning_rate 0.000100574
2017-10-09T15:10:23.604720: step 2215, loss 0.244955, acc 0.9375, learning_rate 0.000100572
2017-10-09T15:10:23.708167: step 2216, loss 0.29041, acc 0.890625, learning_rate 0.00010057
2017-10-09T15:10:23.809167: step 2217, loss 0.252527, acc 0.9375, learning_rate 0.000100567
2017-10-09T15:10:23.915629: step 2218, loss 0.451638, acc 0.859375, learning_rate 0.000100565
2017-10-09T15:10:24.018611: step 2219, loss 0.375009, acc 0.84375, learning_rate 0.000100563
2017-10-09T15:10:24.123664: step 2220, loss 0.600864, acc 0.796875, learning_rate 0.00010056
2017-10-09T15:10:24.227418: step 2221, loss 0.287232, acc 0.90625, learning_rate 0.000100558
2017-10-09T15:10:24.329389: step 2222, loss 0.365531, acc 0.875, learning_rate 0.000100556
2017-10-09T15:10:24.437563: step 2223, loss 0.395866, acc 0.859375, learning_rate 0.000100554
2017-10-09T15:10:24.545383: step 2224, loss 0.278684, acc 0.9375, learning_rate 0.000100551
2017-10-09T15:10:24.652032: step 2225, loss 0.32942, acc 0.875, learning_rate 0.000100549
2017-10-09T15:10:24.758157: step 2226, loss 0.312146, acc 0.90625, learning_rate 0.000100547
2017-10-09T15:10:24.867228: step 2227, loss 0.303463, acc 0.859375, learning_rate 0.000100545
2017-10-09T15:10:24.970982: step 2228, loss 0.326046, acc 0.875, learning_rate 0.000100542
2017-10-09T15:10:25.080770: step 2229, loss 0.243665, acc 0.921875, learning_rate 0.00010054
2017-10-09T15:10:25.186493: step 2230, loss 0.402508, acc 0.921875, learning_rate 0.000100538
2017-10-09T15:10:25.294597: step 2231, loss 0.417611, acc 0.84375, learning_rate 0.000100536
2017-10-09T15:10:25.396899: step 2232, loss 0.479577, acc 0.78125, learning_rate 0.000100534
2017-10-09T15:10:25.497232: step 2233, loss 0.379964, acc 0.90625, learning_rate 0.000100531
2017-10-09T15:10:25.602768: step 2234, loss 0.553979, acc 0.796875, learning_rate 0.000100529
2017-10-09T15:10:25.709873: step 2235, loss 0.375199, acc 0.828125, learning_rate 0.000100527
2017-10-09T15:10:25.818533: step 2236, loss 0.304364, acc 0.890625, learning_rate 0.000100525
2017-10-09T15:10:25.923365: step 2237, loss 0.275178, acc 0.890625, learning_rate 0.000100523
2017-10-09T15:10:26.031245: step 2238, loss 0.510793, acc 0.828125, learning_rate 0.000100521
2017-10-09T15:10:26.136295: step 2239, loss 0.377274, acc 0.828125, learning_rate 0.000100519
2017-10-09T15:10:26.241611: step 2240, loss 0.373843, acc 0.875, learning_rate 0.000100516

Evaluation:
2017-10-09T15:10:26.497112: step 2240, loss 0.364252, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2240

2017-10-09T15:10:27.083228: step 2241, loss 0.320798, acc 0.875, learning_rate 0.000100514
2017-10-09T15:10:27.186930: step 2242, loss 0.543363, acc 0.796875, learning_rate 0.000100512
2017-10-09T15:10:27.296559: step 2243, loss 0.338132, acc 0.875, learning_rate 0.00010051
2017-10-09T15:10:27.399394: step 2244, loss 0.530961, acc 0.859375, learning_rate 0.000100508
2017-10-09T15:10:27.505092: step 2245, loss 0.140023, acc 0.96875, learning_rate 0.000100506
2017-10-09T15:10:27.611986: step 2246, loss 0.251318, acc 0.921875, learning_rate 0.000100504
2017-10-09T15:10:27.719697: step 2247, loss 0.344589, acc 0.875, learning_rate 0.000100502
2017-10-09T15:10:27.826101: step 2248, loss 0.231125, acc 0.921875, learning_rate 0.0001005
2017-10-09T15:10:27.933635: step 2249, loss 0.252607, acc 0.953125, learning_rate 0.000100498
2017-10-09T15:10:28.039498: step 2250, loss 0.244959, acc 0.921875, learning_rate 0.000100496
2017-10-09T15:10:28.142652: step 2251, loss 0.188713, acc 0.953125, learning_rate 0.000100494
2017-10-09T15:10:28.249014: step 2252, loss 0.291921, acc 0.90625, learning_rate 0.000100492
2017-10-09T15:10:28.357101: step 2253, loss 0.33621, acc 0.875, learning_rate 0.00010049
2017-10-09T15:10:28.443729: step 2254, loss 0.368746, acc 0.823529, learning_rate 0.000100488
2017-10-09T15:10:28.551418: step 2255, loss 0.318412, acc 0.875, learning_rate 0.000100486
2017-10-09T15:10:28.658830: step 2256, loss 0.486786, acc 0.796875, learning_rate 0.000100484
2017-10-09T15:10:28.765594: step 2257, loss 0.424844, acc 0.8125, learning_rate 0.000100482
2017-10-09T15:10:28.872771: step 2258, loss 0.161879, acc 0.953125, learning_rate 0.00010048
2017-10-09T15:10:28.977480: step 2259, loss 0.254064, acc 0.90625, learning_rate 0.000100478
2017-10-09T15:10:29.080891: step 2260, loss 0.340649, acc 0.890625, learning_rate 0.000100476
2017-10-09T15:10:29.187372: step 2261, loss 0.373701, acc 0.90625, learning_rate 0.000100474
2017-10-09T15:10:29.290878: step 2262, loss 0.218784, acc 0.921875, learning_rate 0.000100472
2017-10-09T15:10:29.396736: step 2263, loss 0.290193, acc 0.90625, learning_rate 0.00010047
2017-10-09T15:10:29.501841: step 2264, loss 0.596476, acc 0.796875, learning_rate 0.000100468
2017-10-09T15:10:29.608206: step 2265, loss 0.301765, acc 0.90625, learning_rate 0.000100466
2017-10-09T15:10:29.713748: step 2266, loss 0.322125, acc 0.859375, learning_rate 0.000100464
2017-10-09T15:10:29.820445: step 2267, loss 0.338419, acc 0.90625, learning_rate 0.000100462
2017-10-09T15:10:29.927248: step 2268, loss 0.343906, acc 0.890625, learning_rate 0.000100461
2017-10-09T15:10:30.032758: step 2269, loss 0.287856, acc 0.90625, learning_rate 0.000100459
2017-10-09T15:10:30.133990: step 2270, loss 0.33691, acc 0.90625, learning_rate 0.000100457
2017-10-09T15:10:30.241871: step 2271, loss 0.365528, acc 0.859375, learning_rate 0.000100455
2017-10-09T15:10:30.349143: step 2272, loss 0.387851, acc 0.828125, learning_rate 0.000100453
2017-10-09T15:10:30.456022: step 2273, loss 0.428689, acc 0.84375, learning_rate 0.000100451
2017-10-09T15:10:30.563030: step 2274, loss 0.283318, acc 0.875, learning_rate 0.000100449
2017-10-09T15:10:30.668226: step 2275, loss 0.285449, acc 0.875, learning_rate 0.000100448
2017-10-09T15:10:30.770721: step 2276, loss 0.132927, acc 1, learning_rate 0.000100446
2017-10-09T15:10:30.875929: step 2277, loss 0.540944, acc 0.796875, learning_rate 0.000100444
2017-10-09T15:10:30.981760: step 2278, loss 0.252968, acc 0.890625, learning_rate 0.000100442
2017-10-09T15:10:31.089140: step 2279, loss 0.371302, acc 0.84375, learning_rate 0.00010044
2017-10-09T15:10:31.196399: step 2280, loss 0.452248, acc 0.859375, learning_rate 0.000100439

Evaluation:
2017-10-09T15:10:31.457267: step 2280, loss 0.362517, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2280

2017-10-09T15:10:32.123004: step 2281, loss 0.316235, acc 0.90625, learning_rate 0.000100437
2017-10-09T15:10:32.233649: step 2282, loss 0.318994, acc 0.90625, learning_rate 0.000100435
2017-10-09T15:10:32.340246: step 2283, loss 0.429485, acc 0.859375, learning_rate 0.000100433
2017-10-09T15:10:32.442354: step 2284, loss 0.320517, acc 0.890625, learning_rate 0.000100431
2017-10-09T15:10:32.544378: step 2285, loss 0.291224, acc 0.890625, learning_rate 0.00010043
2017-10-09T15:10:32.650451: step 2286, loss 0.281697, acc 0.9375, learning_rate 0.000100428
2017-10-09T15:10:32.754599: step 2287, loss 0.330805, acc 0.859375, learning_rate 0.000100426
2017-10-09T15:10:32.874613: step 2288, loss 0.305061, acc 0.890625, learning_rate 0.000100424
2017-10-09T15:10:32.979799: step 2289, loss 0.483543, acc 0.78125, learning_rate 0.000100423
2017-10-09T15:10:33.083127: step 2290, loss 0.324368, acc 0.890625, learning_rate 0.000100421
2017-10-09T15:10:33.186270: step 2291, loss 0.354201, acc 0.921875, learning_rate 0.000100419
2017-10-09T15:10:33.292687: step 2292, loss 0.351612, acc 0.890625, learning_rate 0.000100418
2017-10-09T15:10:33.396196: step 2293, loss 0.332467, acc 0.921875, learning_rate 0.000100416
2017-10-09T15:10:33.502955: step 2294, loss 0.380334, acc 0.84375, learning_rate 0.000100414
2017-10-09T15:10:33.609388: step 2295, loss 0.354478, acc 0.859375, learning_rate 0.000100412
2017-10-09T15:10:33.715236: step 2296, loss 0.378881, acc 0.90625, learning_rate 0.000100411
2017-10-09T15:10:33.821844: step 2297, loss 0.302386, acc 0.875, learning_rate 0.000100409
2017-10-09T15:10:33.926077: step 2298, loss 0.344822, acc 0.875, learning_rate 0.000100407
2017-10-09T15:10:34.036723: step 2299, loss 0.661629, acc 0.828125, learning_rate 0.000100406
2017-10-09T15:10:34.143218: step 2300, loss 0.350498, acc 0.890625, learning_rate 0.000100404
2017-10-09T15:10:34.250768: step 2301, loss 0.360334, acc 0.859375, learning_rate 0.000100402
2017-10-09T15:10:34.357408: step 2302, loss 0.348089, acc 0.890625, learning_rate 0.000100401
2017-10-09T15:10:34.462263: step 2303, loss 0.384406, acc 0.828125, learning_rate 0.000100399
2017-10-09T15:10:34.568648: step 2304, loss 0.348039, acc 0.890625, learning_rate 0.000100398
2017-10-09T15:10:34.675736: step 2305, loss 0.441005, acc 0.859375, learning_rate 0.000100396
2017-10-09T15:10:34.782406: step 2306, loss 0.257741, acc 0.9375, learning_rate 0.000100394
2017-10-09T15:10:34.889171: step 2307, loss 0.453787, acc 0.796875, learning_rate 0.000100393
2017-10-09T15:10:34.996599: step 2308, loss 0.539135, acc 0.84375, learning_rate 0.000100391
2017-10-09T15:10:35.100353: step 2309, loss 0.647736, acc 0.75, learning_rate 0.000100389
2017-10-09T15:10:35.206663: step 2310, loss 0.377733, acc 0.8125, learning_rate 0.000100388
2017-10-09T15:10:35.315795: step 2311, loss 0.359625, acc 0.828125, learning_rate 0.000100386
2017-10-09T15:10:35.420536: step 2312, loss 0.469836, acc 0.765625, learning_rate 0.000100385
2017-10-09T15:10:35.521239: step 2313, loss 0.333681, acc 0.859375, learning_rate 0.000100383
2017-10-09T15:10:35.627176: step 2314, loss 0.348378, acc 0.859375, learning_rate 0.000100382
2017-10-09T15:10:35.734574: step 2315, loss 0.184423, acc 0.984375, learning_rate 0.00010038
2017-10-09T15:10:35.841424: step 2316, loss 0.306421, acc 0.90625, learning_rate 0.000100378
2017-10-09T15:10:35.947210: step 2317, loss 0.258423, acc 0.90625, learning_rate 0.000100377
2017-10-09T15:10:36.053459: step 2318, loss 0.264406, acc 0.90625, learning_rate 0.000100375
2017-10-09T15:10:36.161847: step 2319, loss 0.376409, acc 0.84375, learning_rate 0.000100374
2017-10-09T15:10:36.268299: step 2320, loss 0.55262, acc 0.796875, learning_rate 0.000100372

Evaluation:
2017-10-09T15:10:36.530735: step 2320, loss 0.363284, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2320

2017-10-09T15:10:37.055587: step 2321, loss 0.345376, acc 0.90625, learning_rate 0.000100371
2017-10-09T15:10:37.166872: step 2322, loss 0.267736, acc 0.921875, learning_rate 0.000100369
2017-10-09T15:10:37.275745: step 2323, loss 0.280974, acc 0.90625, learning_rate 0.000100368
2017-10-09T15:10:37.381973: step 2324, loss 0.483729, acc 0.84375, learning_rate 0.000100366
2017-10-09T15:10:37.488637: step 2325, loss 0.322664, acc 0.890625, learning_rate 0.000100365
2017-10-09T15:10:37.595727: step 2326, loss 0.256067, acc 0.90625, learning_rate 0.000100363
2017-10-09T15:10:37.703787: step 2327, loss 0.311657, acc 0.90625, learning_rate 0.000100362
2017-10-09T15:10:37.811278: step 2328, loss 0.290437, acc 0.875, learning_rate 0.00010036
2017-10-09T15:10:37.920044: step 2329, loss 0.28677, acc 0.90625, learning_rate 0.000100359
2017-10-09T15:10:38.024328: step 2330, loss 0.540929, acc 0.796875, learning_rate 0.000100357
2017-10-09T15:10:38.125037: step 2331, loss 0.395119, acc 0.875, learning_rate 0.000100356
2017-10-09T15:10:38.230519: step 2332, loss 0.31057, acc 0.875, learning_rate 0.000100354
2017-10-09T15:10:38.329856: step 2333, loss 0.359218, acc 0.90625, learning_rate 0.000100353
2017-10-09T15:10:38.435219: step 2334, loss 0.474426, acc 0.828125, learning_rate 0.000100352
2017-10-09T15:10:38.539626: step 2335, loss 0.386742, acc 0.921875, learning_rate 0.00010035
2017-10-09T15:10:38.644211: step 2336, loss 0.392445, acc 0.859375, learning_rate 0.000100349
2017-10-09T15:10:38.752547: step 2337, loss 0.351431, acc 0.875, learning_rate 0.000100347
2017-10-09T15:10:38.863403: step 2338, loss 0.284683, acc 0.921875, learning_rate 0.000100346
2017-10-09T15:10:38.969128: step 2339, loss 0.209975, acc 0.9375, learning_rate 0.000100344
2017-10-09T15:10:39.077529: step 2340, loss 0.361301, acc 0.875, learning_rate 0.000100343
2017-10-09T15:10:39.183004: step 2341, loss 0.308524, acc 0.828125, learning_rate 0.000100342
2017-10-09T15:10:39.288950: step 2342, loss 0.285117, acc 0.90625, learning_rate 0.00010034
2017-10-09T15:10:39.396986: step 2343, loss 0.327226, acc 0.90625, learning_rate 0.000100339
2017-10-09T15:10:39.503562: step 2344, loss 0.579684, acc 0.8125, learning_rate 0.000100338
2017-10-09T15:10:39.607036: step 2345, loss 0.298497, acc 0.890625, learning_rate 0.000100336
2017-10-09T15:10:39.713781: step 2346, loss 0.512231, acc 0.78125, learning_rate 0.000100335
2017-10-09T15:10:39.815665: step 2347, loss 0.486887, acc 0.796875, learning_rate 0.000100333
2017-10-09T15:10:39.924514: step 2348, loss 0.396252, acc 0.84375, learning_rate 0.000100332
2017-10-09T15:10:40.031364: step 2349, loss 0.223415, acc 0.921875, learning_rate 0.000100331
2017-10-09T15:10:40.136122: step 2350, loss 0.300266, acc 0.90625, learning_rate 0.000100329
2017-10-09T15:10:40.241789: step 2351, loss 0.321277, acc 0.90625, learning_rate 0.000100328
2017-10-09T15:10:40.329218: step 2352, loss 0.33066, acc 0.921569, learning_rate 0.000100327
2017-10-09T15:10:40.433943: step 2353, loss 0.327726, acc 0.890625, learning_rate 0.000100325
2017-10-09T15:10:40.539143: step 2354, loss 0.386416, acc 0.890625, learning_rate 0.000100324
2017-10-09T15:10:40.646303: step 2355, loss 0.343038, acc 0.890625, learning_rate 0.000100323
2017-10-09T15:10:40.752306: step 2356, loss 0.342012, acc 0.859375, learning_rate 0.000100321
2017-10-09T15:10:40.858483: step 2357, loss 0.222499, acc 0.90625, learning_rate 0.00010032
2017-10-09T15:10:40.961948: step 2358, loss 0.456636, acc 0.8125, learning_rate 0.000100319
2017-10-09T15:10:41.064284: step 2359, loss 0.357851, acc 0.859375, learning_rate 0.000100317
2017-10-09T15:10:41.170376: step 2360, loss 0.468152, acc 0.828125, learning_rate 0.000100316

Evaluation:
2017-10-09T15:10:41.421055: step 2360, loss 0.361301, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2360

2017-10-09T15:10:42.005989: step 2361, loss 0.252352, acc 0.9375, learning_rate 0.000100315
2017-10-09T15:10:42.114129: step 2362, loss 0.428072, acc 0.921875, learning_rate 0.000100314
2017-10-09T15:10:42.219633: step 2363, loss 0.273385, acc 0.921875, learning_rate 0.000100312
2017-10-09T15:10:42.323582: step 2364, loss 0.351587, acc 0.84375, learning_rate 0.000100311
2017-10-09T15:10:42.428442: step 2365, loss 0.329864, acc 0.84375, learning_rate 0.00010031
2017-10-09T15:10:42.537905: step 2366, loss 0.263679, acc 0.90625, learning_rate 0.000100308
2017-10-09T15:10:42.644081: step 2367, loss 0.371027, acc 0.828125, learning_rate 0.000100307
2017-10-09T15:10:42.747724: step 2368, loss 0.38095, acc 0.84375, learning_rate 0.000100306
2017-10-09T15:10:42.855467: step 2369, loss 0.298797, acc 0.921875, learning_rate 0.000100305
2017-10-09T15:10:42.962382: step 2370, loss 0.50648, acc 0.828125, learning_rate 0.000100303
2017-10-09T15:10:43.069913: step 2371, loss 0.438258, acc 0.90625, learning_rate 0.000100302
2017-10-09T15:10:43.173365: step 2372, loss 0.277477, acc 0.90625, learning_rate 0.000100301
2017-10-09T15:10:43.279995: step 2373, loss 0.278003, acc 0.90625, learning_rate 0.0001003
2017-10-09T15:10:43.386515: step 2374, loss 0.289826, acc 0.875, learning_rate 0.000100299
2017-10-09T15:10:43.492509: step 2375, loss 0.268777, acc 0.921875, learning_rate 0.000100297
2017-10-09T15:10:43.600195: step 2376, loss 0.300517, acc 0.921875, learning_rate 0.000100296
2017-10-09T15:10:43.708075: step 2377, loss 0.365799, acc 0.859375, learning_rate 0.000100295
2017-10-09T15:10:43.813452: step 2378, loss 0.387575, acc 0.828125, learning_rate 0.000100294
2017-10-09T15:10:43.915490: step 2379, loss 0.219306, acc 0.921875, learning_rate 0.000100292
2017-10-09T15:10:44.023532: step 2380, loss 0.28279, acc 0.90625, learning_rate 0.000100291
2017-10-09T15:10:44.129307: step 2381, loss 0.23882, acc 0.921875, learning_rate 0.00010029
2017-10-09T15:10:44.235938: step 2382, loss 0.32424, acc 0.890625, learning_rate 0.000100289
2017-10-09T15:10:44.340155: step 2383, loss 0.325506, acc 0.890625, learning_rate 0.000100288
2017-10-09T15:10:44.445920: step 2384, loss 0.35384, acc 0.890625, learning_rate 0.000100287
2017-10-09T15:10:44.556967: step 2385, loss 0.37495, acc 0.84375, learning_rate 0.000100285
2017-10-09T15:10:44.662520: step 2386, loss 0.547389, acc 0.875, learning_rate 0.000100284
2017-10-09T15:10:44.768466: step 2387, loss 0.259202, acc 0.90625, learning_rate 0.000100283
2017-10-09T15:10:44.875525: step 2388, loss 0.316008, acc 0.859375, learning_rate 0.000100282
2017-10-09T15:10:44.982918: step 2389, loss 0.5274, acc 0.84375, learning_rate 0.000100281
2017-10-09T15:10:45.086942: step 2390, loss 0.332396, acc 0.921875, learning_rate 0.00010028
2017-10-09T15:10:45.192981: step 2391, loss 0.35909, acc 0.84375, learning_rate 0.000100278
2017-10-09T15:10:45.298264: step 2392, loss 0.368072, acc 0.859375, learning_rate 0.000100277
2017-10-09T15:10:45.404794: step 2393, loss 0.335857, acc 0.90625, learning_rate 0.000100276
2017-10-09T15:10:45.511549: step 2394, loss 0.295926, acc 0.890625, learning_rate 0.000100275
2017-10-09T15:10:45.616387: step 2395, loss 0.298172, acc 0.90625, learning_rate 0.000100274
2017-10-09T15:10:45.723024: step 2396, loss 0.325583, acc 0.890625, learning_rate 0.000100273
2017-10-09T15:10:45.830109: step 2397, loss 0.422619, acc 0.859375, learning_rate 0.000100272
2017-10-09T15:10:45.937304: step 2398, loss 0.597436, acc 0.8125, learning_rate 0.000100271
2017-10-09T15:10:46.044412: step 2399, loss 0.360017, acc 0.859375, learning_rate 0.00010027
2017-10-09T15:10:46.150947: step 2400, loss 0.286685, acc 0.875, learning_rate 0.000100268

Evaluation:
2017-10-09T15:10:46.404103: step 2400, loss 0.359524, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2400

2017-10-09T15:10:46.988841: step 2401, loss 0.397005, acc 0.828125, learning_rate 0.000100267
2017-10-09T15:10:47.093854: step 2402, loss 0.370425, acc 0.890625, learning_rate 0.000100266
2017-10-09T15:10:47.200625: step 2403, loss 0.332769, acc 0.875, learning_rate 0.000100265
2017-10-09T15:10:47.307560: step 2404, loss 0.39024, acc 0.859375, learning_rate 0.000100264
2017-10-09T15:10:47.415315: step 2405, loss 0.290825, acc 0.875, learning_rate 0.000100263
2017-10-09T15:10:47.520726: step 2406, loss 0.325158, acc 0.90625, learning_rate 0.000100262
2017-10-09T15:10:47.628593: step 2407, loss 0.232432, acc 0.890625, learning_rate 0.000100261
2017-10-09T15:10:47.737506: step 2408, loss 0.420103, acc 0.859375, learning_rate 0.00010026
2017-10-09T15:10:47.846762: step 2409, loss 0.360659, acc 0.859375, learning_rate 0.000100259
2017-10-09T15:10:47.951441: step 2410, loss 0.375987, acc 0.859375, learning_rate 0.000100258
2017-10-09T15:10:48.056801: step 2411, loss 0.37563, acc 0.859375, learning_rate 0.000100257
2017-10-09T15:10:48.159997: step 2412, loss 0.330619, acc 0.875, learning_rate 0.000100256
2017-10-09T15:10:48.267316: step 2413, loss 0.2855, acc 0.875, learning_rate 0.000100255
2017-10-09T15:10:48.372794: step 2414, loss 0.342857, acc 0.890625, learning_rate 0.000100253
2017-10-09T15:10:48.477222: step 2415, loss 0.430482, acc 0.828125, learning_rate 0.000100252
2017-10-09T15:10:48.579498: step 2416, loss 0.428628, acc 0.875, learning_rate 0.000100251
2017-10-09T15:10:48.686265: step 2417, loss 0.43707, acc 0.8125, learning_rate 0.00010025
2017-10-09T15:10:48.791363: step 2418, loss 0.362109, acc 0.875, learning_rate 0.000100249
2017-10-09T15:10:48.898045: step 2419, loss 0.330293, acc 0.875, learning_rate 0.000100248
2017-10-09T15:10:49.004683: step 2420, loss 0.523439, acc 0.8125, learning_rate 0.000100247
2017-10-09T15:10:49.108058: step 2421, loss 0.347711, acc 0.796875, learning_rate 0.000100246
2017-10-09T15:10:49.212867: step 2422, loss 0.293099, acc 0.90625, learning_rate 0.000100245
2017-10-09T15:10:49.318093: step 2423, loss 0.347079, acc 0.859375, learning_rate 0.000100244
2017-10-09T15:10:49.419922: step 2424, loss 0.222819, acc 0.921875, learning_rate 0.000100243
2017-10-09T15:10:49.524405: step 2425, loss 0.396996, acc 0.859375, learning_rate 0.000100242
2017-10-09T15:10:49.629968: step 2426, loss 0.329183, acc 0.875, learning_rate 0.000100241
2017-10-09T15:10:49.735040: step 2427, loss 0.37076, acc 0.890625, learning_rate 0.00010024
2017-10-09T15:10:49.841351: step 2428, loss 0.248766, acc 0.890625, learning_rate 0.000100239
2017-10-09T15:10:49.943223: step 2429, loss 0.242418, acc 0.90625, learning_rate 0.000100238
2017-10-09T15:10:50.047499: step 2430, loss 0.265006, acc 0.9375, learning_rate 0.000100237
2017-10-09T15:10:50.151358: step 2431, loss 0.409661, acc 0.875, learning_rate 0.000100236
2017-10-09T15:10:50.253691: step 2432, loss 0.359945, acc 0.90625, learning_rate 0.000100235
2017-10-09T15:10:50.357175: step 2433, loss 0.319174, acc 0.90625, learning_rate 0.000100235
2017-10-09T15:10:50.462213: step 2434, loss 0.398735, acc 0.859375, learning_rate 0.000100234
2017-10-09T15:10:50.565634: step 2435, loss 0.423789, acc 0.859375, learning_rate 0.000100233
2017-10-09T15:10:50.671236: step 2436, loss 0.42524, acc 0.84375, learning_rate 0.000100232
2017-10-09T15:10:50.777734: step 2437, loss 0.46102, acc 0.8125, learning_rate 0.000100231
2017-10-09T15:10:50.884022: step 2438, loss 0.227483, acc 0.953125, learning_rate 0.00010023
2017-10-09T15:10:50.989427: step 2439, loss 0.389001, acc 0.875, learning_rate 0.000100229
2017-10-09T15:10:51.096034: step 2440, loss 0.448588, acc 0.828125, learning_rate 0.000100228

Evaluation:
2017-10-09T15:10:51.350068: step 2440, loss 0.361472, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2440

2017-10-09T15:10:52.008600: step 2441, loss 0.465505, acc 0.828125, learning_rate 0.000100227
2017-10-09T15:10:52.117662: step 2442, loss 0.195987, acc 0.953125, learning_rate 0.000100226
2017-10-09T15:10:52.223087: step 2443, loss 0.352473, acc 0.90625, learning_rate 0.000100225
2017-10-09T15:10:52.329778: step 2444, loss 0.321041, acc 0.921875, learning_rate 0.000100224
2017-10-09T15:10:52.436320: step 2445, loss 0.434674, acc 0.84375, learning_rate 0.000100223
2017-10-09T15:10:52.544774: step 2446, loss 0.296646, acc 0.890625, learning_rate 0.000100222
2017-10-09T15:10:52.650164: step 2447, loss 0.300792, acc 0.875, learning_rate 0.000100221
2017-10-09T15:10:52.751779: step 2448, loss 0.289312, acc 0.90625, learning_rate 0.000100221
2017-10-09T15:10:52.858751: step 2449, loss 0.330154, acc 0.921875, learning_rate 0.00010022
2017-10-09T15:10:52.946059: step 2450, loss 0.415674, acc 0.862745, learning_rate 0.000100219
2017-10-09T15:10:53.054526: step 2451, loss 0.322759, acc 0.921875, learning_rate 0.000100218
2017-10-09T15:10:53.166264: step 2452, loss 0.340154, acc 0.90625, learning_rate 0.000100217
2017-10-09T15:10:53.273901: step 2453, loss 0.291345, acc 0.875, learning_rate 0.000100216
2017-10-09T15:10:53.385696: step 2454, loss 0.261364, acc 0.90625, learning_rate 0.000100215
2017-10-09T15:10:53.499054: step 2455, loss 0.287227, acc 0.90625, learning_rate 0.000100214
2017-10-09T15:10:53.607688: step 2456, loss 0.27776, acc 0.921875, learning_rate 0.000100213
2017-10-09T15:10:53.712339: step 2457, loss 0.343075, acc 0.90625, learning_rate 0.000100213
2017-10-09T15:10:53.819081: step 2458, loss 0.39407, acc 0.859375, learning_rate 0.000100212
2017-10-09T15:10:53.928467: step 2459, loss 0.378787, acc 0.890625, learning_rate 0.000100211
2017-10-09T15:10:54.036041: step 2460, loss 0.226281, acc 0.9375, learning_rate 0.00010021
2017-10-09T15:10:54.143926: step 2461, loss 0.346113, acc 0.859375, learning_rate 0.000100209
2017-10-09T15:10:54.249287: step 2462, loss 0.528521, acc 0.796875, learning_rate 0.000100208
2017-10-09T15:10:54.352739: step 2463, loss 0.265921, acc 0.921875, learning_rate 0.000100207
2017-10-09T15:10:54.456681: step 2464, loss 0.381858, acc 0.859375, learning_rate 0.000100207
2017-10-09T15:10:54.562301: step 2465, loss 0.382212, acc 0.890625, learning_rate 0.000100206
2017-10-09T15:10:54.667670: step 2466, loss 0.1996, acc 0.921875, learning_rate 0.000100205
2017-10-09T15:10:54.774381: step 2467, loss 0.341832, acc 0.875, learning_rate 0.000100204
2017-10-09T15:10:54.878766: step 2468, loss 0.237, acc 0.90625, learning_rate 0.000100203
2017-10-09T15:10:54.986801: step 2469, loss 0.317577, acc 0.90625, learning_rate 0.000100202
2017-10-09T15:10:55.091670: step 2470, loss 0.332353, acc 0.859375, learning_rate 0.000100202
2017-10-09T15:10:55.195362: step 2471, loss 0.537936, acc 0.796875, learning_rate 0.000100201
2017-10-09T15:10:55.298077: step 2472, loss 0.31952, acc 0.90625, learning_rate 0.0001002
2017-10-09T15:10:55.404896: step 2473, loss 0.207831, acc 0.9375, learning_rate 0.000100199
2017-10-09T15:10:55.512753: step 2474, loss 0.428408, acc 0.84375, learning_rate 0.000100198
2017-10-09T15:10:55.618183: step 2475, loss 0.411104, acc 0.859375, learning_rate 0.000100198
2017-10-09T15:10:55.730277: step 2476, loss 0.309826, acc 0.875, learning_rate 0.000100197
2017-10-09T15:10:55.835925: step 2477, loss 0.312192, acc 0.890625, learning_rate 0.000100196
2017-10-09T15:10:55.947384: step 2478, loss 0.31957, acc 0.859375, learning_rate 0.000100195
2017-10-09T15:10:56.054915: step 2479, loss 0.436672, acc 0.875, learning_rate 0.000100194
2017-10-09T15:10:56.161746: step 2480, loss 0.257933, acc 0.90625, learning_rate 0.000100194

Evaluation:
2017-10-09T15:10:56.424368: step 2480, loss 0.357405, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2480

2017-10-09T15:10:56.957810: step 2481, loss 0.253306, acc 0.921875, learning_rate 0.000100193
2017-10-09T15:10:57.063102: step 2482, loss 0.305218, acc 0.90625, learning_rate 0.000100192
2017-10-09T15:10:57.168854: step 2483, loss 0.230313, acc 0.9375, learning_rate 0.000100191
2017-10-09T15:10:57.272861: step 2484, loss 0.261893, acc 0.921875, learning_rate 0.00010019
2017-10-09T15:10:57.377261: step 2485, loss 0.315632, acc 0.875, learning_rate 0.00010019
2017-10-09T15:10:57.484172: step 2486, loss 0.331284, acc 0.90625, learning_rate 0.000100189
2017-10-09T15:10:57.590653: step 2487, loss 0.441969, acc 0.828125, learning_rate 0.000100188
2017-10-09T15:10:57.694674: step 2488, loss 0.270297, acc 0.90625, learning_rate 0.000100187
2017-10-09T15:10:57.799800: step 2489, loss 0.211373, acc 0.9375, learning_rate 0.000100187
2017-10-09T15:10:57.903504: step 2490, loss 0.238992, acc 0.90625, learning_rate 0.000100186
2017-10-09T15:10:58.006581: step 2491, loss 0.472591, acc 0.875, learning_rate 0.000100185
2017-10-09T15:10:58.110108: step 2492, loss 0.3389, acc 0.90625, learning_rate 0.000100184
2017-10-09T15:10:58.212546: step 2493, loss 0.255675, acc 0.90625, learning_rate 0.000100183
2017-10-09T15:10:58.319107: step 2494, loss 0.3919, acc 0.859375, learning_rate 0.000100183
2017-10-09T15:10:58.421456: step 2495, loss 0.409705, acc 0.890625, learning_rate 0.000100182
2017-10-09T15:10:58.527396: step 2496, loss 0.287898, acc 0.875, learning_rate 0.000100181
2017-10-09T15:10:58.627626: step 2497, loss 0.363855, acc 0.828125, learning_rate 0.000100181
2017-10-09T15:10:58.733259: step 2498, loss 0.208422, acc 0.953125, learning_rate 0.00010018
2017-10-09T15:10:58.837678: step 2499, loss 0.383448, acc 0.859375, learning_rate 0.000100179
2017-10-09T15:10:58.943243: step 2500, loss 0.256512, acc 0.90625, learning_rate 0.000100178
2017-10-09T15:10:59.046473: step 2501, loss 0.418366, acc 0.828125, learning_rate 0.000100178
2017-10-09T15:10:59.157764: step 2502, loss 0.444629, acc 0.828125, learning_rate 0.000100177
2017-10-09T15:10:59.262512: step 2503, loss 0.299268, acc 0.90625, learning_rate 0.000100176
2017-10-09T15:10:59.369338: step 2504, loss 0.423714, acc 0.8125, learning_rate 0.000100175
2017-10-09T15:10:59.473171: step 2505, loss 0.242071, acc 0.953125, learning_rate 0.000100175
2017-10-09T15:10:59.575050: step 2506, loss 0.495734, acc 0.828125, learning_rate 0.000100174
2017-10-09T15:10:59.675069: step 2507, loss 0.327022, acc 0.9375, learning_rate 0.000100173
2017-10-09T15:10:59.778101: step 2508, loss 0.518289, acc 0.8125, learning_rate 0.000100173
2017-10-09T15:10:59.885255: step 2509, loss 0.335803, acc 0.859375, learning_rate 0.000100172
2017-10-09T15:10:59.992692: step 2510, loss 0.327417, acc 0.84375, learning_rate 0.000100171
2017-10-09T15:11:00.097109: step 2511, loss 0.435559, acc 0.84375, learning_rate 0.00010017
2017-10-09T15:11:00.205008: step 2512, loss 0.303073, acc 0.890625, learning_rate 0.00010017
2017-10-09T15:11:00.307688: step 2513, loss 0.425753, acc 0.84375, learning_rate 0.000100169
2017-10-09T15:11:00.415406: step 2514, loss 0.331064, acc 0.90625, learning_rate 0.000100168
2017-10-09T15:11:00.523433: step 2515, loss 0.360141, acc 0.875, learning_rate 0.000100168
2017-10-09T15:11:00.626849: step 2516, loss 0.426544, acc 0.8125, learning_rate 0.000100167
2017-10-09T15:11:00.732480: step 2517, loss 0.455918, acc 0.859375, learning_rate 0.000100166
2017-10-09T15:11:00.839006: step 2518, loss 0.354028, acc 0.875, learning_rate 0.000100166
2017-10-09T15:11:00.943431: step 2519, loss 0.288284, acc 0.890625, learning_rate 0.000100165
2017-10-09T15:11:01.049520: step 2520, loss 0.349853, acc 0.875, learning_rate 0.000100164

Evaluation:
2017-10-09T15:11:01.305352: step 2520, loss 0.362065, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2520

2017-10-09T15:11:01.889726: step 2521, loss 0.403117, acc 0.875, learning_rate 0.000100164
2017-10-09T15:11:01.992982: step 2522, loss 0.261039, acc 0.9375, learning_rate 0.000100163
2017-10-09T15:11:02.096881: step 2523, loss 0.166361, acc 0.921875, learning_rate 0.000100162
2017-10-09T15:11:02.201523: step 2524, loss 0.258478, acc 0.9375, learning_rate 0.000100162
2017-10-09T15:11:02.308555: step 2525, loss 0.376395, acc 0.859375, learning_rate 0.000100161
2017-10-09T15:11:02.410656: step 2526, loss 0.461353, acc 0.8125, learning_rate 0.00010016
2017-10-09T15:11:02.515662: step 2527, loss 0.291376, acc 0.921875, learning_rate 0.00010016
2017-10-09T15:11:02.620546: step 2528, loss 0.222791, acc 0.9375, learning_rate 0.000100159
2017-10-09T15:11:02.722816: step 2529, loss 0.386299, acc 0.859375, learning_rate 0.000100158
2017-10-09T15:11:02.826604: step 2530, loss 0.331133, acc 0.890625, learning_rate 0.000100158
2017-10-09T15:11:02.936619: step 2531, loss 0.48232, acc 0.78125, learning_rate 0.000100157
2017-10-09T15:11:03.039841: step 2532, loss 0.266361, acc 0.921875, learning_rate 0.000100156
2017-10-09T15:11:03.143168: step 2533, loss 0.329198, acc 0.875, learning_rate 0.000100156
2017-10-09T15:11:03.248160: step 2534, loss 0.203255, acc 0.953125, learning_rate 0.000100155
2017-10-09T15:11:03.353000: step 2535, loss 0.291046, acc 0.921875, learning_rate 0.000100155
2017-10-09T15:11:03.459870: step 2536, loss 0.272799, acc 0.90625, learning_rate 0.000100154
2017-10-09T15:11:03.565342: step 2537, loss 0.311746, acc 0.859375, learning_rate 0.000100153
2017-10-09T15:11:03.670591: step 2538, loss 0.378969, acc 0.84375, learning_rate 0.000100153
2017-10-09T15:11:03.775946: step 2539, loss 0.316361, acc 0.90625, learning_rate 0.000100152
2017-10-09T15:11:03.881023: step 2540, loss 0.289917, acc 0.9375, learning_rate 0.000100151
2017-10-09T15:11:03.986804: step 2541, loss 0.396886, acc 0.859375, learning_rate 0.000100151
2017-10-09T15:11:04.094074: step 2542, loss 0.271573, acc 0.921875, learning_rate 0.00010015
2017-10-09T15:11:04.201134: step 2543, loss 0.20411, acc 0.9375, learning_rate 0.00010015
2017-10-09T15:11:04.308696: step 2544, loss 0.499712, acc 0.8125, learning_rate 0.000100149
2017-10-09T15:11:04.414453: step 2545, loss 0.368716, acc 0.828125, learning_rate 0.000100148
2017-10-09T15:11:04.520393: step 2546, loss 0.479504, acc 0.828125, learning_rate 0.000100148
2017-10-09T15:11:04.627530: step 2547, loss 0.330958, acc 0.875, learning_rate 0.000100147
2017-10-09T15:11:04.713649: step 2548, loss 0.43872, acc 0.764706, learning_rate 0.000100147
2017-10-09T15:11:04.816106: step 2549, loss 0.295351, acc 0.890625, learning_rate 0.000100146
2017-10-09T15:11:04.924335: step 2550, loss 0.292291, acc 0.890625, learning_rate 0.000100145
2017-10-09T15:11:05.031074: step 2551, loss 0.307237, acc 0.875, learning_rate 0.000100145
2017-10-09T15:11:05.137438: step 2552, loss 0.399366, acc 0.90625, learning_rate 0.000100144
2017-10-09T15:11:05.245138: step 2553, loss 0.291867, acc 0.90625, learning_rate 0.000100144
2017-10-09T15:11:05.352897: step 2554, loss 0.323737, acc 0.890625, learning_rate 0.000100143
2017-10-09T15:11:05.459002: step 2555, loss 0.352346, acc 0.875, learning_rate 0.000100142
2017-10-09T15:11:05.566342: step 2556, loss 0.191129, acc 0.953125, learning_rate 0.000100142
2017-10-09T15:11:05.673900: step 2557, loss 0.351052, acc 0.890625, learning_rate 0.000100141
2017-10-09T15:11:05.781879: step 2558, loss 0.363225, acc 0.875, learning_rate 0.000100141
2017-10-09T15:11:05.893776: step 2559, loss 0.509151, acc 0.875, learning_rate 0.00010014
2017-10-09T15:11:06.004637: step 2560, loss 0.306029, acc 0.9375, learning_rate 0.00010014

Evaluation:
2017-10-09T15:11:06.266265: step 2560, loss 0.357379, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2560

2017-10-09T15:11:06.868320: step 2561, loss 0.330871, acc 0.84375, learning_rate 0.000100139
2017-10-09T15:11:06.973190: step 2562, loss 0.353456, acc 0.859375, learning_rate 0.000100138
2017-10-09T15:11:07.077423: step 2563, loss 0.342172, acc 0.859375, learning_rate 0.000100138
2017-10-09T15:11:07.183485: step 2564, loss 0.232378, acc 0.9375, learning_rate 0.000100137
2017-10-09T15:11:07.285420: step 2565, loss 0.267826, acc 0.921875, learning_rate 0.000100137
2017-10-09T15:11:07.389829: step 2566, loss 0.303275, acc 0.890625, learning_rate 0.000100136
2017-10-09T15:11:07.493930: step 2567, loss 0.376918, acc 0.90625, learning_rate 0.000100136
2017-10-09T15:11:07.601327: step 2568, loss 0.318323, acc 0.921875, learning_rate 0.000100135
2017-10-09T15:11:07.709748: step 2569, loss 0.492033, acc 0.84375, learning_rate 0.000100134
2017-10-09T15:11:07.813865: step 2570, loss 0.38205, acc 0.875, learning_rate 0.000100134
2017-10-09T15:11:07.924493: step 2571, loss 0.274782, acc 0.875, learning_rate 0.000100133
2017-10-09T15:11:08.032588: step 2572, loss 0.267305, acc 0.921875, learning_rate 0.000100133
2017-10-09T15:11:08.139908: step 2573, loss 0.316184, acc 0.875, learning_rate 0.000100132
2017-10-09T15:11:08.244369: step 2574, loss 0.358673, acc 0.875, learning_rate 0.000100132
2017-10-09T15:11:08.352789: step 2575, loss 0.361975, acc 0.8125, learning_rate 0.000100131
2017-10-09T15:11:08.458569: step 2576, loss 0.358979, acc 0.859375, learning_rate 0.000100131
2017-10-09T15:11:08.563702: step 2577, loss 0.290913, acc 0.90625, learning_rate 0.00010013
2017-10-09T15:11:08.671341: step 2578, loss 0.334945, acc 0.875, learning_rate 0.00010013
2017-10-09T15:11:08.775429: step 2579, loss 0.415027, acc 0.859375, learning_rate 0.000100129
2017-10-09T15:11:08.879330: step 2580, loss 0.565602, acc 0.765625, learning_rate 0.000100129
2017-10-09T15:11:08.986489: step 2581, loss 0.356481, acc 0.875, learning_rate 0.000100128
2017-10-09T15:11:09.094140: step 2582, loss 0.359059, acc 0.875, learning_rate 0.000100128
2017-10-09T15:11:09.198785: step 2583, loss 0.317728, acc 0.890625, learning_rate 0.000100127
2017-10-09T15:11:09.304620: step 2584, loss 0.429292, acc 0.859375, learning_rate 0.000100126
2017-10-09T15:11:09.411562: step 2585, loss 0.265459, acc 0.921875, learning_rate 0.000100126
2017-10-09T15:11:09.522510: step 2586, loss 0.239837, acc 0.90625, learning_rate 0.000100125
2017-10-09T15:11:09.630382: step 2587, loss 0.469042, acc 0.828125, learning_rate 0.000100125
2017-10-09T15:11:09.736256: step 2588, loss 0.335455, acc 0.921875, learning_rate 0.000100124
2017-10-09T15:11:09.840910: step 2589, loss 0.402624, acc 0.8125, learning_rate 0.000100124
2017-10-09T15:11:09.945897: step 2590, loss 0.208263, acc 0.953125, learning_rate 0.000100123
2017-10-09T15:11:10.050693: step 2591, loss 0.306874, acc 0.90625, learning_rate 0.000100123
2017-10-09T15:11:10.163125: step 2592, loss 0.28584, acc 0.890625, learning_rate 0.000100122
2017-10-09T15:11:10.270046: step 2593, loss 0.330109, acc 0.875, learning_rate 0.000100122
2017-10-09T15:11:10.375964: step 2594, loss 0.486846, acc 0.84375, learning_rate 0.000100121
2017-10-09T15:11:10.483984: step 2595, loss 0.300519, acc 0.890625, learning_rate 0.000100121
2017-10-09T15:11:10.591877: step 2596, loss 0.323575, acc 0.90625, learning_rate 0.00010012
2017-10-09T15:11:10.703317: step 2597, loss 0.34006, acc 0.90625, learning_rate 0.00010012
2017-10-09T15:11:10.811796: step 2598, loss 0.441956, acc 0.875, learning_rate 0.000100119
2017-10-09T15:11:10.916350: step 2599, loss 0.373827, acc 0.875, learning_rate 0.000100119
2017-10-09T15:11:11.025467: step 2600, loss 0.308502, acc 0.859375, learning_rate 0.000100118

Evaluation:
2017-10-09T15:11:11.283318: step 2600, loss 0.357107, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2600

2017-10-09T15:11:11.955925: step 2601, loss 0.336076, acc 0.859375, learning_rate 0.000100118
2017-10-09T15:11:12.066440: step 2602, loss 0.44249, acc 0.8125, learning_rate 0.000100117
2017-10-09T15:11:12.172653: step 2603, loss 0.29882, acc 0.90625, learning_rate 0.000100117
2017-10-09T15:11:12.277874: step 2604, loss 0.210206, acc 0.921875, learning_rate 0.000100117
2017-10-09T15:11:12.385347: step 2605, loss 0.243207, acc 0.921875, learning_rate 0.000100116
2017-10-09T15:11:12.488871: step 2606, loss 0.380402, acc 0.828125, learning_rate 0.000100116
2017-10-09T15:11:12.595188: step 2607, loss 0.329071, acc 0.890625, learning_rate 0.000100115
2017-10-09T15:11:12.700072: step 2608, loss 0.225075, acc 0.96875, learning_rate 0.000100115
2017-10-09T15:11:12.805910: step 2609, loss 0.240587, acc 0.90625, learning_rate 0.000100114
2017-10-09T15:11:12.914432: step 2610, loss 0.248941, acc 0.9375, learning_rate 0.000100114
2017-10-09T15:11:13.020216: step 2611, loss 0.424425, acc 0.859375, learning_rate 0.000100113
2017-10-09T15:11:13.126523: step 2612, loss 0.325513, acc 0.890625, learning_rate 0.000100113
2017-10-09T15:11:13.226251: step 2613, loss 0.351793, acc 0.875, learning_rate 0.000100112
2017-10-09T15:11:13.327961: step 2614, loss 0.458023, acc 0.828125, learning_rate 0.000100112
2017-10-09T15:11:13.433071: step 2615, loss 0.498662, acc 0.765625, learning_rate 0.000100111
2017-10-09T15:11:13.536146: step 2616, loss 0.340815, acc 0.875, learning_rate 0.000100111
2017-10-09T15:11:13.639824: step 2617, loss 0.313823, acc 0.890625, learning_rate 0.000100111
2017-10-09T15:11:13.745720: step 2618, loss 0.274347, acc 0.921875, learning_rate 0.00010011
2017-10-09T15:11:13.850651: step 2619, loss 0.367208, acc 0.859375, learning_rate 0.00010011
2017-10-09T15:11:13.958692: step 2620, loss 0.358863, acc 0.890625, learning_rate 0.000100109
2017-10-09T15:11:14.063979: step 2621, loss 0.234921, acc 0.9375, learning_rate 0.000100109
2017-10-09T15:11:14.168804: step 2622, loss 0.205379, acc 0.9375, learning_rate 0.000100108
2017-10-09T15:11:14.273470: step 2623, loss 0.314189, acc 0.890625, learning_rate 0.000100108
2017-10-09T15:11:14.381813: step 2624, loss 0.483064, acc 0.828125, learning_rate 0.000100107
2017-10-09T15:11:14.489357: step 2625, loss 0.412839, acc 0.890625, learning_rate 0.000100107
2017-10-09T15:11:14.593504: step 2626, loss 0.270234, acc 0.90625, learning_rate 0.000100107
2017-10-09T15:11:14.700297: step 2627, loss 0.231264, acc 0.953125, learning_rate 0.000100106
2017-10-09T15:11:14.803653: step 2628, loss 0.477667, acc 0.828125, learning_rate 0.000100106
2017-10-09T15:11:14.909851: step 2629, loss 0.367643, acc 0.890625, learning_rate 0.000100105
2017-10-09T15:11:15.017732: step 2630, loss 0.368165, acc 0.875, learning_rate 0.000100105
2017-10-09T15:11:15.124239: step 2631, loss 0.359333, acc 0.859375, learning_rate 0.000100104
2017-10-09T15:11:15.238212: step 2632, loss 0.252294, acc 0.921875, learning_rate 0.000100104
2017-10-09T15:11:15.346362: step 2633, loss 0.255685, acc 0.921875, learning_rate 0.000100104
2017-10-09T15:11:15.450708: step 2634, loss 0.36032, acc 0.890625, learning_rate 0.000100103
2017-10-09T15:11:15.553679: step 2635, loss 0.298086, acc 0.859375, learning_rate 0.000100103
2017-10-09T15:11:15.661066: step 2636, loss 0.359363, acc 0.9375, learning_rate 0.000100102
2017-10-09T15:11:15.767008: step 2637, loss 0.432366, acc 0.859375, learning_rate 0.000100102
2017-10-09T15:11:15.871587: step 2638, loss 0.430858, acc 0.875, learning_rate 0.000100101
2017-10-09T15:11:15.977739: step 2639, loss 0.234231, acc 0.90625, learning_rate 0.000100101
2017-10-09T15:11:16.079068: step 2640, loss 0.394078, acc 0.828125, learning_rate 0.000100101

Evaluation:
2017-10-09T15:11:16.329601: step 2640, loss 0.357033, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2640

2017-10-09T15:11:16.845293: step 2641, loss 0.278882, acc 0.921875, learning_rate 0.0001001
2017-10-09T15:11:16.949984: step 2642, loss 0.499492, acc 0.875, learning_rate 0.0001001
2017-10-09T15:11:17.052324: step 2643, loss 0.44877, acc 0.84375, learning_rate 0.000100099
2017-10-09T15:11:17.156944: step 2644, loss 0.440145, acc 0.84375, learning_rate 0.000100099
2017-10-09T15:11:17.261504: step 2645, loss 0.291359, acc 0.90625, learning_rate 0.000100099
2017-10-09T15:11:17.351528: step 2646, loss 0.287581, acc 0.882353, learning_rate 0.000100098
2017-10-09T15:11:17.456135: step 2647, loss 0.410198, acc 0.859375, learning_rate 0.000100098
2017-10-09T15:11:17.560531: step 2648, loss 0.424355, acc 0.875, learning_rate 0.000100097
2017-10-09T15:11:17.664703: step 2649, loss 0.485253, acc 0.875, learning_rate 0.000100097
2017-10-09T15:11:17.772603: step 2650, loss 0.249887, acc 0.921875, learning_rate 0.000100097
2017-10-09T15:11:17.877813: step 2651, loss 0.35824, acc 0.859375, learning_rate 0.000100096
2017-10-09T15:11:17.982764: step 2652, loss 0.282016, acc 0.921875, learning_rate 0.000100096
2017-10-09T15:11:18.085225: step 2653, loss 0.361939, acc 0.890625, learning_rate 0.000100095
2017-10-09T15:11:18.191498: step 2654, loss 0.245513, acc 0.921875, learning_rate 0.000100095
2017-10-09T15:11:18.298190: step 2655, loss 0.234582, acc 0.90625, learning_rate 0.000100095
2017-10-09T15:11:18.405753: step 2656, loss 0.398331, acc 0.84375, learning_rate 0.000100094
2017-10-09T15:11:18.516298: step 2657, loss 0.296729, acc 0.859375, learning_rate 0.000100094
2017-10-09T15:11:18.622917: step 2658, loss 0.137598, acc 0.96875, learning_rate 0.000100093
2017-10-09T15:11:18.727790: step 2659, loss 0.335831, acc 0.921875, learning_rate 0.000100093
2017-10-09T15:11:18.832437: step 2660, loss 0.286862, acc 0.90625, learning_rate 0.000100093
2017-10-09T15:11:18.941255: step 2661, loss 0.266348, acc 0.921875, learning_rate 0.000100092
2017-10-09T15:11:19.047122: step 2662, loss 0.337467, acc 0.890625, learning_rate 0.000100092
2017-10-09T15:11:19.152119: step 2663, loss 0.437192, acc 0.828125, learning_rate 0.000100092
2017-10-09T15:11:19.258276: step 2664, loss 0.259352, acc 0.90625, learning_rate 0.000100091
2017-10-09T15:11:19.357793: step 2665, loss 0.4279, acc 0.875, learning_rate 0.000100091
2017-10-09T15:11:19.461067: step 2666, loss 0.439674, acc 0.84375, learning_rate 0.00010009
2017-10-09T15:11:19.568105: step 2667, loss 0.356654, acc 0.890625, learning_rate 0.00010009
2017-10-09T15:11:19.673514: step 2668, loss 0.242254, acc 0.90625, learning_rate 0.00010009
2017-10-09T15:11:19.777156: step 2669, loss 0.30592, acc 0.9375, learning_rate 0.000100089
2017-10-09T15:11:19.885258: step 2670, loss 0.332514, acc 0.859375, learning_rate 0.000100089
2017-10-09T15:11:19.992086: step 2671, loss 0.429386, acc 0.859375, learning_rate 0.000100089
2017-10-09T15:11:20.097856: step 2672, loss 0.318686, acc 0.90625, learning_rate 0.000100088
2017-10-09T15:11:20.208806: step 2673, loss 0.296697, acc 0.875, learning_rate 0.000100088
2017-10-09T15:11:20.313108: step 2674, loss 0.321766, acc 0.875, learning_rate 0.000100088
2017-10-09T15:11:20.417621: step 2675, loss 0.341505, acc 0.90625, learning_rate 0.000100087
2017-10-09T15:11:20.524795: step 2676, loss 0.311315, acc 0.890625, learning_rate 0.000100087
2017-10-09T15:11:20.631342: step 2677, loss 0.430463, acc 0.8125, learning_rate 0.000100086
2017-10-09T15:11:20.738788: step 2678, loss 0.274549, acc 0.875, learning_rate 0.000100086
2017-10-09T15:11:20.845292: step 2679, loss 0.289293, acc 0.921875, learning_rate 0.000100086
2017-10-09T15:11:20.950006: step 2680, loss 0.461213, acc 0.8125, learning_rate 0.000100085

Evaluation:
2017-10-09T15:11:21.211100: step 2680, loss 0.355066, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2680

2017-10-09T15:11:21.791924: step 2681, loss 0.277407, acc 0.921875, learning_rate 0.000100085
2017-10-09T15:11:21.899355: step 2682, loss 0.266109, acc 0.875, learning_rate 0.000100085
2017-10-09T15:11:22.004976: step 2683, loss 0.319093, acc 0.921875, learning_rate 0.000100084
2017-10-09T15:11:22.110033: step 2684, loss 0.279003, acc 0.890625, learning_rate 0.000100084
2017-10-09T15:11:22.216590: step 2685, loss 0.29879, acc 0.90625, learning_rate 0.000100084
2017-10-09T15:11:22.322617: step 2686, loss 0.472602, acc 0.859375, learning_rate 0.000100083
2017-10-09T15:11:22.426973: step 2687, loss 0.196474, acc 0.9375, learning_rate 0.000100083
2017-10-09T15:11:22.530834: step 2688, loss 0.183213, acc 0.921875, learning_rate 0.000100083
2017-10-09T15:11:22.635761: step 2689, loss 0.313531, acc 0.90625, learning_rate 0.000100082
2017-10-09T15:11:22.740236: step 2690, loss 0.325873, acc 0.890625, learning_rate 0.000100082
2017-10-09T15:11:22.846949: step 2691, loss 0.366367, acc 0.890625, learning_rate 0.000100082
2017-10-09T15:11:22.949814: step 2692, loss 0.300784, acc 0.875, learning_rate 0.000100081
2017-10-09T15:11:23.054514: step 2693, loss 0.274546, acc 0.875, learning_rate 0.000100081
2017-10-09T15:11:23.164019: step 2694, loss 0.348539, acc 0.921875, learning_rate 0.000100081
2017-10-09T15:11:23.268039: step 2695, loss 0.408042, acc 0.859375, learning_rate 0.00010008
2017-10-09T15:11:23.372821: step 2696, loss 0.241065, acc 0.9375, learning_rate 0.00010008
2017-10-09T15:11:23.472467: step 2697, loss 0.227808, acc 0.9375, learning_rate 0.00010008
2017-10-09T15:11:23.578654: step 2698, loss 0.701396, acc 0.8125, learning_rate 0.000100079
2017-10-09T15:11:23.685226: step 2699, loss 0.339567, acc 0.859375, learning_rate 0.000100079
2017-10-09T15:11:23.789760: step 2700, loss 0.297643, acc 0.890625, learning_rate 0.000100079
2017-10-09T15:11:23.900210: step 2701, loss 0.41044, acc 0.859375, learning_rate 0.000100078
2017-10-09T15:11:24.006813: step 2702, loss 0.394083, acc 0.875, learning_rate 0.000100078
2017-10-09T15:11:24.109677: step 2703, loss 0.204891, acc 0.9375, learning_rate 0.000100078
2017-10-09T15:11:24.215472: step 2704, loss 0.274406, acc 0.890625, learning_rate 0.000100077
2017-10-09T15:11:24.320090: step 2705, loss 0.372923, acc 0.890625, learning_rate 0.000100077
2017-10-09T15:11:24.427402: step 2706, loss 0.456802, acc 0.78125, learning_rate 0.000100077
2017-10-09T15:11:24.532886: step 2707, loss 0.517598, acc 0.828125, learning_rate 0.000100076
2017-10-09T15:11:24.639806: step 2708, loss 0.423708, acc 0.890625, learning_rate 0.000100076
2017-10-09T15:11:24.743600: step 2709, loss 0.355838, acc 0.890625, learning_rate 0.000100076
2017-10-09T15:11:24.844151: step 2710, loss 0.350971, acc 0.859375, learning_rate 0.000100076
2017-10-09T15:11:24.946184: step 2711, loss 0.409689, acc 0.875, learning_rate 0.000100075
2017-10-09T15:11:25.051467: step 2712, loss 0.509225, acc 0.828125, learning_rate 0.000100075
2017-10-09T15:11:25.154723: step 2713, loss 0.494896, acc 0.78125, learning_rate 0.000100075
2017-10-09T15:11:25.261094: step 2714, loss 0.370042, acc 0.875, learning_rate 0.000100074
2017-10-09T15:11:25.366278: step 2715, loss 0.355321, acc 0.859375, learning_rate 0.000100074
2017-10-09T15:11:25.467547: step 2716, loss 0.26824, acc 0.921875, learning_rate 0.000100074
2017-10-09T15:11:25.573961: step 2717, loss 0.449546, acc 0.875, learning_rate 0.000100073
2017-10-09T15:11:25.680771: step 2718, loss 0.277772, acc 0.890625, learning_rate 0.000100073
2017-10-09T15:11:25.788433: step 2719, loss 0.198063, acc 0.9375, learning_rate 0.000100073
2017-10-09T15:11:25.895683: step 2720, loss 0.340373, acc 0.859375, learning_rate 0.000100073

Evaluation:
2017-10-09T15:11:26.150520: step 2720, loss 0.352645, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2720

2017-10-09T15:11:26.735563: step 2721, loss 0.473482, acc 0.765625, learning_rate 0.000100072
2017-10-09T15:11:26.833937: step 2722, loss 0.375654, acc 0.875, learning_rate 0.000100072
2017-10-09T15:11:26.938608: step 2723, loss 0.233876, acc 0.9375, learning_rate 0.000100072
2017-10-09T15:11:27.048767: step 2724, loss 0.334681, acc 0.921875, learning_rate 0.000100071
2017-10-09T15:11:27.155238: step 2725, loss 0.297165, acc 0.921875, learning_rate 0.000100071
2017-10-09T15:11:27.259676: step 2726, loss 0.356071, acc 0.875, learning_rate 0.000100071
2017-10-09T15:11:27.366651: step 2727, loss 0.388347, acc 0.828125, learning_rate 0.00010007
2017-10-09T15:11:27.477333: step 2728, loss 0.481413, acc 0.796875, learning_rate 0.00010007
2017-10-09T15:11:27.585344: step 2729, loss 0.359278, acc 0.875, learning_rate 0.00010007
2017-10-09T15:11:27.691127: step 2730, loss 0.366154, acc 0.890625, learning_rate 0.00010007
2017-10-09T15:11:27.794636: step 2731, loss 0.328261, acc 0.859375, learning_rate 0.000100069
2017-10-09T15:11:27.902352: step 2732, loss 0.300695, acc 0.859375, learning_rate 0.000100069
2017-10-09T15:11:28.009927: step 2733, loss 0.368728, acc 0.890625, learning_rate 0.000100069
2017-10-09T15:11:28.115647: step 2734, loss 0.231144, acc 0.890625, learning_rate 0.000100068
2017-10-09T15:11:28.220457: step 2735, loss 0.362651, acc 0.859375, learning_rate 0.000100068
2017-10-09T15:11:28.323321: step 2736, loss 0.354328, acc 0.859375, learning_rate 0.000100068
2017-10-09T15:11:28.428783: step 2737, loss 0.476373, acc 0.8125, learning_rate 0.000100068
2017-10-09T15:11:28.535097: step 2738, loss 0.247238, acc 0.921875, learning_rate 0.000100067
2017-10-09T15:11:28.639416: step 2739, loss 0.286121, acc 0.875, learning_rate 0.000100067
2017-10-09T15:11:28.745649: step 2740, loss 0.274171, acc 0.890625, learning_rate 0.000100067
2017-10-09T15:11:28.849119: step 2741, loss 0.358733, acc 0.84375, learning_rate 0.000100067
2017-10-09T15:11:28.954286: step 2742, loss 0.35306, acc 0.875, learning_rate 0.000100066
2017-10-09T15:11:29.058672: step 2743, loss 0.241661, acc 0.921875, learning_rate 0.000100066
2017-10-09T15:11:29.148870: step 2744, loss 0.359905, acc 0.901961, learning_rate 0.000100066
2017-10-09T15:11:29.259025: step 2745, loss 0.303241, acc 0.859375, learning_rate 0.000100065
2017-10-09T15:11:29.364033: step 2746, loss 0.231513, acc 0.953125, learning_rate 0.000100065
2017-10-09T15:11:29.467190: step 2747, loss 0.246686, acc 0.921875, learning_rate 0.000100065
2017-10-09T15:11:29.573892: step 2748, loss 0.295228, acc 0.859375, learning_rate 0.000100065
2017-10-09T15:11:29.679372: step 2749, loss 0.310761, acc 0.859375, learning_rate 0.000100064
2017-10-09T15:11:29.783423: step 2750, loss 0.306626, acc 0.921875, learning_rate 0.000100064
2017-10-09T15:11:29.889945: step 2751, loss 0.254913, acc 0.921875, learning_rate 0.000100064
2017-10-09T15:11:29.995079: step 2752, loss 0.524874, acc 0.8125, learning_rate 0.000100064
2017-10-09T15:11:30.102551: step 2753, loss 0.241844, acc 0.921875, learning_rate 0.000100063
2017-10-09T15:11:30.205857: step 2754, loss 0.236339, acc 0.921875, learning_rate 0.000100063
2017-10-09T15:11:30.308440: step 2755, loss 0.352015, acc 0.921875, learning_rate 0.000100063
2017-10-09T15:11:30.412728: step 2756, loss 0.409765, acc 0.828125, learning_rate 0.000100063
2017-10-09T15:11:30.519019: step 2757, loss 0.295086, acc 0.921875, learning_rate 0.000100062
2017-10-09T15:11:30.624371: step 2758, loss 0.182413, acc 0.953125, learning_rate 0.000100062
2017-10-09T15:11:30.729219: step 2759, loss 0.477272, acc 0.828125, learning_rate 0.000100062
2017-10-09T15:11:30.836341: step 2760, loss 0.362433, acc 0.875, learning_rate 0.000100062

Evaluation:
2017-10-09T15:11:31.093015: step 2760, loss 0.3562, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2760

2017-10-09T15:11:31.749486: step 2761, loss 0.283155, acc 0.90625, learning_rate 0.000100061
2017-10-09T15:11:31.856370: step 2762, loss 0.31851, acc 0.875, learning_rate 0.000100061
2017-10-09T15:11:31.961303: step 2763, loss 0.460865, acc 0.796875, learning_rate 0.000100061
2017-10-09T15:11:32.067708: step 2764, loss 0.300096, acc 0.859375, learning_rate 0.000100061
2017-10-09T15:11:32.172232: step 2765, loss 0.561203, acc 0.8125, learning_rate 0.00010006
2017-10-09T15:11:32.278161: step 2766, loss 0.362287, acc 0.890625, learning_rate 0.00010006
2017-10-09T15:11:32.378846: step 2767, loss 0.329336, acc 0.921875, learning_rate 0.00010006
2017-10-09T15:11:32.487007: step 2768, loss 0.194777, acc 0.96875, learning_rate 0.00010006
2017-10-09T15:11:32.595097: step 2769, loss 0.363843, acc 0.828125, learning_rate 0.000100059
2017-10-09T15:11:32.703300: step 2770, loss 0.326542, acc 0.953125, learning_rate 0.000100059
2017-10-09T15:11:32.808152: step 2771, loss 0.33855, acc 0.890625, learning_rate 0.000100059
2017-10-09T15:11:32.917700: step 2772, loss 0.331041, acc 0.875, learning_rate 0.000100059
2017-10-09T15:11:33.022123: step 2773, loss 0.460975, acc 0.828125, learning_rate 0.000100058
2017-10-09T15:11:33.121583: step 2774, loss 0.291607, acc 0.90625, learning_rate 0.000100058
2017-10-09T15:11:33.227412: step 2775, loss 0.382854, acc 0.84375, learning_rate 0.000100058
2017-10-09T15:11:33.333844: step 2776, loss 0.321233, acc 0.859375, learning_rate 0.000100058
2017-10-09T15:11:33.438827: step 2777, loss 0.267681, acc 0.90625, learning_rate 0.000100057
2017-10-09T15:11:33.542219: step 2778, loss 0.387311, acc 0.875, learning_rate 0.000100057
2017-10-09T15:11:33.648828: step 2779, loss 0.213115, acc 0.921875, learning_rate 0.000100057
2017-10-09T15:11:33.756638: step 2780, loss 0.268662, acc 0.90625, learning_rate 0.000100057
2017-10-09T15:11:33.859961: step 2781, loss 0.250833, acc 0.9375, learning_rate 0.000100056
2017-10-09T15:11:33.965859: step 2782, loss 0.258846, acc 0.875, learning_rate 0.000100056
2017-10-09T15:11:34.072571: step 2783, loss 0.286899, acc 0.90625, learning_rate 0.000100056
2017-10-09T15:11:34.178218: step 2784, loss 0.300236, acc 0.9375, learning_rate 0.000100056
2017-10-09T15:11:34.286221: step 2785, loss 0.451701, acc 0.859375, learning_rate 0.000100056
2017-10-09T15:11:34.392968: step 2786, loss 0.192795, acc 0.953125, learning_rate 0.000100055
2017-10-09T15:11:34.499110: step 2787, loss 0.392365, acc 0.875, learning_rate 0.000100055
2017-10-09T15:11:34.606811: step 2788, loss 0.208821, acc 0.890625, learning_rate 0.000100055
2017-10-09T15:11:34.710959: step 2789, loss 0.259493, acc 0.921875, learning_rate 0.000100055
2017-10-09T15:11:34.816261: step 2790, loss 0.358243, acc 0.859375, learning_rate 0.000100054
2017-10-09T15:11:34.922758: step 2791, loss 0.236633, acc 0.90625, learning_rate 0.000100054
2017-10-09T15:11:35.030861: step 2792, loss 0.386446, acc 0.828125, learning_rate 0.000100054
2017-10-09T15:11:35.136757: step 2793, loss 0.410164, acc 0.859375, learning_rate 0.000100054
2017-10-09T15:11:35.243129: step 2794, loss 0.247496, acc 0.921875, learning_rate 0.000100054
2017-10-09T15:11:35.345200: step 2795, loss 0.315139, acc 0.90625, learning_rate 0.000100053
2017-10-09T15:11:35.452502: step 2796, loss 0.494923, acc 0.8125, learning_rate 0.000100053
2017-10-09T15:11:35.559893: step 2797, loss 0.350718, acc 0.859375, learning_rate 0.000100053
2017-10-09T15:11:35.666723: step 2798, loss 0.285169, acc 0.890625, learning_rate 0.000100053
2017-10-09T15:11:35.771078: step 2799, loss 0.274088, acc 0.921875, learning_rate 0.000100052
2017-10-09T15:11:35.876505: step 2800, loss 0.347884, acc 0.875, learning_rate 0.000100052

Evaluation:
2017-10-09T15:11:36.131323: step 2800, loss 0.353461, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2800

2017-10-09T15:11:36.650610: step 2801, loss 0.334457, acc 0.875, learning_rate 0.000100052
2017-10-09T15:11:36.757099: step 2802, loss 0.246914, acc 0.890625, learning_rate 0.000100052
2017-10-09T15:11:36.866562: step 2803, loss 0.288804, acc 0.921875, learning_rate 0.000100052
2017-10-09T15:11:36.972674: step 2804, loss 0.268611, acc 0.875, learning_rate 0.000100051
2017-10-09T15:11:37.077227: step 2805, loss 0.478909, acc 0.78125, learning_rate 0.000100051
2017-10-09T15:11:37.185032: step 2806, loss 0.227257, acc 0.921875, learning_rate 0.000100051
2017-10-09T15:11:37.291105: step 2807, loss 0.399618, acc 0.828125, learning_rate 0.000100051
2017-10-09T15:11:37.397304: step 2808, loss 0.351948, acc 0.84375, learning_rate 0.000100051
2017-10-09T15:11:37.501329: step 2809, loss 0.238292, acc 0.921875, learning_rate 0.00010005
2017-10-09T15:11:37.609601: step 2810, loss 0.294335, acc 0.90625, learning_rate 0.00010005
2017-10-09T15:11:37.715427: step 2811, loss 0.228569, acc 0.953125, learning_rate 0.00010005
2017-10-09T15:11:37.821350: step 2812, loss 0.297987, acc 0.90625, learning_rate 0.00010005
2017-10-09T15:11:37.928734: step 2813, loss 0.276542, acc 0.953125, learning_rate 0.00010005
2017-10-09T15:11:38.031279: step 2814, loss 0.350483, acc 0.875, learning_rate 0.000100049
2017-10-09T15:11:38.139571: step 2815, loss 0.375509, acc 0.890625, learning_rate 0.000100049
2017-10-09T15:11:38.248325: step 2816, loss 0.240531, acc 0.9375, learning_rate 0.000100049
2017-10-09T15:11:38.357546: step 2817, loss 0.182524, acc 0.953125, learning_rate 0.000100049
2017-10-09T15:11:38.462972: step 2818, loss 0.169259, acc 0.953125, learning_rate 0.000100049
2017-10-09T15:11:38.570542: step 2819, loss 0.256844, acc 0.9375, learning_rate 0.000100048
2017-10-09T15:11:38.677565: step 2820, loss 0.285117, acc 0.921875, learning_rate 0.000100048
2017-10-09T15:11:38.782756: step 2821, loss 0.260784, acc 0.953125, learning_rate 0.000100048
2017-10-09T15:11:38.891052: step 2822, loss 0.346569, acc 0.859375, learning_rate 0.000100048
2017-10-09T15:11:38.999370: step 2823, loss 0.343893, acc 0.921875, learning_rate 0.000100048
2017-10-09T15:11:39.103079: step 2824, loss 0.492264, acc 0.8125, learning_rate 0.000100047
2017-10-09T15:11:39.203187: step 2825, loss 0.422322, acc 0.828125, learning_rate 0.000100047
2017-10-09T15:11:39.308835: step 2826, loss 0.406568, acc 0.828125, learning_rate 0.000100047
2017-10-09T15:11:39.413471: step 2827, loss 0.2702, acc 0.90625, learning_rate 0.000100047
2017-10-09T15:11:39.520224: step 2828, loss 0.282646, acc 0.90625, learning_rate 0.000100047
2017-10-09T15:11:39.625491: step 2829, loss 0.369532, acc 0.875, learning_rate 0.000100046
2017-10-09T15:11:39.732952: step 2830, loss 0.323393, acc 0.921875, learning_rate 0.000100046
2017-10-09T15:11:39.842379: step 2831, loss 0.397627, acc 0.8125, learning_rate 0.000100046
2017-10-09T15:11:39.947464: step 2832, loss 0.452951, acc 0.90625, learning_rate 0.000100046
2017-10-09T15:11:40.049256: step 2833, loss 0.446093, acc 0.84375, learning_rate 0.000100046
2017-10-09T15:11:40.152417: step 2834, loss 0.387513, acc 0.84375, learning_rate 0.000100045
2017-10-09T15:11:40.258853: step 2835, loss 0.282913, acc 0.921875, learning_rate 0.000100045
2017-10-09T15:11:40.364554: step 2836, loss 0.281881, acc 0.890625, learning_rate 0.000100045
2017-10-09T15:11:40.471118: step 2837, loss 0.359846, acc 0.84375, learning_rate 0.000100045
2017-10-09T15:11:40.579306: step 2838, loss 0.359494, acc 0.859375, learning_rate 0.000100045
2017-10-09T15:11:40.687131: step 2839, loss 0.356806, acc 0.90625, learning_rate 0.000100045
2017-10-09T15:11:40.792781: step 2840, loss 0.30155, acc 0.890625, learning_rate 0.000100044

Evaluation:
2017-10-09T15:11:41.045622: step 2840, loss 0.353559, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2840

2017-10-09T15:11:41.629320: step 2841, loss 0.308297, acc 0.90625, learning_rate 0.000100044
2017-10-09T15:11:41.720539: step 2842, loss 0.182556, acc 0.960784, learning_rate 0.000100044
2017-10-09T15:11:41.825778: step 2843, loss 0.338574, acc 0.90625, learning_rate 0.000100044
2017-10-09T15:11:41.934072: step 2844, loss 0.485664, acc 0.859375, learning_rate 0.000100044
2017-10-09T15:11:42.037989: step 2845, loss 0.341912, acc 0.875, learning_rate 0.000100043
2017-10-09T15:11:42.143770: step 2846, loss 0.331359, acc 0.90625, learning_rate 0.000100043
2017-10-09T15:11:42.248324: step 2847, loss 0.312748, acc 0.890625, learning_rate 0.000100043
2017-10-09T15:11:42.354693: step 2848, loss 0.272415, acc 0.890625, learning_rate 0.000100043
2017-10-09T15:11:42.461541: step 2849, loss 0.243473, acc 0.9375, learning_rate 0.000100043
2017-10-09T15:11:42.569352: step 2850, loss 0.318087, acc 0.90625, learning_rate 0.000100043
2017-10-09T15:11:42.676832: step 2851, loss 0.155568, acc 0.953125, learning_rate 0.000100042
2017-10-09T15:11:42.783311: step 2852, loss 0.245696, acc 0.9375, learning_rate 0.000100042
2017-10-09T15:11:42.891616: step 2853, loss 0.229217, acc 0.953125, learning_rate 0.000100042
2017-10-09T15:11:42.996707: step 2854, loss 0.360932, acc 0.859375, learning_rate 0.000100042
2017-10-09T15:11:43.105502: step 2855, loss 0.23076, acc 0.921875, learning_rate 0.000100042
2017-10-09T15:11:43.212042: step 2856, loss 0.333091, acc 0.890625, learning_rate 0.000100042
2017-10-09T15:11:43.316886: step 2857, loss 0.217118, acc 0.90625, learning_rate 0.000100041
2017-10-09T15:11:43.421893: step 2858, loss 0.308583, acc 0.890625, learning_rate 0.000100041
2017-10-09T15:11:43.529731: step 2859, loss 0.281626, acc 0.875, learning_rate 0.000100041
2017-10-09T15:11:43.636011: step 2860, loss 0.355331, acc 0.859375, learning_rate 0.000100041
2017-10-09T15:11:43.744023: step 2861, loss 0.210755, acc 0.9375, learning_rate 0.000100041
2017-10-09T15:11:43.856059: step 2862, loss 0.43534, acc 0.84375, learning_rate 0.000100041
2017-10-09T15:11:43.964610: step 2863, loss 0.282302, acc 0.921875, learning_rate 0.00010004
2017-10-09T15:11:44.066827: step 2864, loss 0.397634, acc 0.8125, learning_rate 0.00010004
2017-10-09T15:11:44.173368: step 2865, loss 0.305924, acc 0.90625, learning_rate 0.00010004
2017-10-09T15:11:44.278144: step 2866, loss 0.449092, acc 0.890625, learning_rate 0.00010004
2017-10-09T15:11:44.381673: step 2867, loss 0.347543, acc 0.90625, learning_rate 0.00010004
2017-10-09T15:11:44.485979: step 2868, loss 0.383581, acc 0.875, learning_rate 0.00010004
2017-10-09T15:11:44.595232: step 2869, loss 0.223796, acc 0.921875, learning_rate 0.000100039
2017-10-09T15:11:44.700902: step 2870, loss 0.362224, acc 0.84375, learning_rate 0.000100039
2017-10-09T15:11:44.806378: step 2871, loss 0.267968, acc 0.890625, learning_rate 0.000100039
2017-10-09T15:11:44.913065: step 2872, loss 0.173947, acc 0.9375, learning_rate 0.000100039
2017-10-09T15:11:45.016819: step 2873, loss 0.377253, acc 0.890625, learning_rate 0.000100039
2017-10-09T15:11:45.120167: step 2874, loss 0.324788, acc 0.90625, learning_rate 0.000100039
2017-10-09T15:11:45.225006: step 2875, loss 0.376222, acc 0.859375, learning_rate 0.000100038
2017-10-09T15:11:45.327884: step 2876, loss 0.333353, acc 0.890625, learning_rate 0.000100038
2017-10-09T15:11:45.431823: step 2877, loss 0.466571, acc 0.859375, learning_rate 0.000100038
2017-10-09T15:11:45.535590: step 2878, loss 0.264676, acc 0.890625, learning_rate 0.000100038
2017-10-09T15:11:45.640894: step 2879, loss 0.280246, acc 0.890625, learning_rate 0.000100038
2017-10-09T15:11:45.745030: step 2880, loss 0.27553, acc 0.953125, learning_rate 0.000100038

Evaluation:
2017-10-09T15:11:46.004971: step 2880, loss 0.35192, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2880

2017-10-09T15:11:46.583970: step 2881, loss 0.45, acc 0.84375, learning_rate 0.000100038
2017-10-09T15:11:46.693565: step 2882, loss 0.356397, acc 0.90625, learning_rate 0.000100037
2017-10-09T15:11:46.801449: step 2883, loss 0.253278, acc 0.9375, learning_rate 0.000100037
2017-10-09T15:11:46.905885: step 2884, loss 0.339821, acc 0.84375, learning_rate 0.000100037
2017-10-09T15:11:47.011732: step 2885, loss 0.266789, acc 0.921875, learning_rate 0.000100037
2017-10-09T15:11:47.117327: step 2886, loss 0.269895, acc 0.921875, learning_rate 0.000100037
2017-10-09T15:11:47.221871: step 2887, loss 0.394059, acc 0.859375, learning_rate 0.000100037
2017-10-09T15:11:47.328918: step 2888, loss 0.307635, acc 0.90625, learning_rate 0.000100036
2017-10-09T15:11:47.434106: step 2889, loss 0.455638, acc 0.828125, learning_rate 0.000100036
2017-10-09T15:11:47.540669: step 2890, loss 0.308626, acc 0.890625, learning_rate 0.000100036
2017-10-09T15:11:47.649396: step 2891, loss 0.387479, acc 0.890625, learning_rate 0.000100036
2017-10-09T15:11:47.756653: step 2892, loss 0.28685, acc 0.921875, learning_rate 0.000100036
2017-10-09T15:11:47.863393: step 2893, loss 0.462759, acc 0.84375, learning_rate 0.000100036
2017-10-09T15:11:47.967775: step 2894, loss 0.452157, acc 0.796875, learning_rate 0.000100036
2017-10-09T15:11:48.072522: step 2895, loss 0.431045, acc 0.84375, learning_rate 0.000100035
2017-10-09T15:11:48.180889: step 2896, loss 0.307389, acc 0.890625, learning_rate 0.000100035
2017-10-09T15:11:48.285268: step 2897, loss 0.361772, acc 0.90625, learning_rate 0.000100035
2017-10-09T15:11:48.391428: step 2898, loss 0.325888, acc 0.875, learning_rate 0.000100035
2017-10-09T15:11:48.496522: step 2899, loss 0.393977, acc 0.84375, learning_rate 0.000100035
2017-10-09T15:11:48.604449: step 2900, loss 0.205452, acc 0.96875, learning_rate 0.000100035
2017-10-09T15:11:48.711539: step 2901, loss 0.28146, acc 0.890625, learning_rate 0.000100035
2017-10-09T15:11:48.815124: step 2902, loss 0.340449, acc 0.875, learning_rate 0.000100034
2017-10-09T15:11:48.922476: step 2903, loss 0.340325, acc 0.890625, learning_rate 0.000100034
2017-10-09T15:11:49.025149: step 2904, loss 0.360014, acc 0.859375, learning_rate 0.000100034
2017-10-09T15:11:49.133249: step 2905, loss 0.418995, acc 0.8125, learning_rate 0.000100034
2017-10-09T15:11:49.238861: step 2906, loss 0.23824, acc 0.921875, learning_rate 0.000100034
2017-10-09T15:11:49.344273: step 2907, loss 0.390603, acc 0.84375, learning_rate 0.000100034
2017-10-09T15:11:49.454227: step 2908, loss 0.432621, acc 0.84375, learning_rate 0.000100034
2017-10-09T15:11:49.556914: step 2909, loss 0.219591, acc 0.890625, learning_rate 0.000100033
2017-10-09T15:11:49.662151: step 2910, loss 0.177504, acc 0.953125, learning_rate 0.000100033
2017-10-09T15:11:49.765062: step 2911, loss 0.363391, acc 0.890625, learning_rate 0.000100033
2017-10-09T15:11:49.877139: step 2912, loss 0.335086, acc 0.921875, learning_rate 0.000100033
2017-10-09T15:11:49.987266: step 2913, loss 0.353162, acc 0.890625, learning_rate 0.000100033
2017-10-09T15:11:50.093201: step 2914, loss 0.267549, acc 0.90625, learning_rate 0.000100033
2017-10-09T15:11:50.199314: step 2915, loss 0.242803, acc 0.921875, learning_rate 0.000100033
2017-10-09T15:11:50.304545: step 2916, loss 0.394376, acc 0.84375, learning_rate 0.000100033
2017-10-09T15:11:50.407897: step 2917, loss 0.223073, acc 0.9375, learning_rate 0.000100032
2017-10-09T15:11:50.515190: step 2918, loss 0.262471, acc 0.921875, learning_rate 0.000100032
2017-10-09T15:11:50.619391: step 2919, loss 0.194377, acc 0.96875, learning_rate 0.000100032
2017-10-09T15:11:50.724518: step 2920, loss 0.396416, acc 0.875, learning_rate 0.000100032

Evaluation:
2017-10-09T15:11:50.981038: step 2920, loss 0.352614, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2920

2017-10-09T15:11:51.644768: step 2921, loss 0.316528, acc 0.890625, learning_rate 0.000100032
2017-10-09T15:11:51.745320: step 2922, loss 0.411812, acc 0.859375, learning_rate 0.000100032
2017-10-09T15:11:51.851373: step 2923, loss 0.284518, acc 0.921875, learning_rate 0.000100032
2017-10-09T15:11:51.959312: step 2924, loss 0.25967, acc 0.921875, learning_rate 0.000100031
2017-10-09T15:11:52.065715: step 2925, loss 0.283086, acc 0.875, learning_rate 0.000100031
2017-10-09T15:11:52.176097: step 2926, loss 0.334798, acc 0.890625, learning_rate 0.000100031
2017-10-09T15:11:52.281931: step 2927, loss 0.292725, acc 0.921875, learning_rate 0.000100031
2017-10-09T15:11:52.387575: step 2928, loss 0.265519, acc 0.953125, learning_rate 0.000100031
2017-10-09T15:11:52.494217: step 2929, loss 0.180295, acc 0.953125, learning_rate 0.000100031
2017-10-09T15:11:52.599782: step 2930, loss 0.382661, acc 0.875, learning_rate 0.000100031
2017-10-09T15:11:52.703114: step 2931, loss 0.280061, acc 0.890625, learning_rate 0.000100031
2017-10-09T15:11:52.810585: step 2932, loss 0.25979, acc 0.953125, learning_rate 0.00010003
2017-10-09T15:11:52.911873: step 2933, loss 0.277306, acc 0.921875, learning_rate 0.00010003
2017-10-09T15:11:53.021156: step 2934, loss 0.400343, acc 0.828125, learning_rate 0.00010003
2017-10-09T15:11:53.127093: step 2935, loss 0.173725, acc 0.96875, learning_rate 0.00010003
2017-10-09T15:11:53.234871: step 2936, loss 0.283638, acc 0.890625, learning_rate 0.00010003
2017-10-09T15:11:53.338304: step 2937, loss 0.23472, acc 0.890625, learning_rate 0.00010003
2017-10-09T15:11:53.445735: step 2938, loss 0.232839, acc 0.90625, learning_rate 0.00010003
2017-10-09T15:11:53.550144: step 2939, loss 0.281651, acc 0.90625, learning_rate 0.00010003
2017-10-09T15:11:53.637745: step 2940, loss 0.212141, acc 0.960784, learning_rate 0.000100029
2017-10-09T15:11:53.742955: step 2941, loss 0.182409, acc 0.953125, learning_rate 0.000100029
2017-10-09T15:11:53.850819: step 2942, loss 0.183137, acc 0.953125, learning_rate 0.000100029
2017-10-09T15:11:53.957756: step 2943, loss 0.43899, acc 0.84375, learning_rate 0.000100029
2017-10-09T15:11:54.064740: step 2944, loss 0.238094, acc 0.921875, learning_rate 0.000100029
2017-10-09T15:11:54.170481: step 2945, loss 0.349606, acc 0.921875, learning_rate 0.000100029
2017-10-09T15:11:54.276954: step 2946, loss 0.448591, acc 0.859375, learning_rate 0.000100029
2017-10-09T15:11:54.384665: step 2947, loss 0.336849, acc 0.8125, learning_rate 0.000100029
2017-10-09T15:11:54.491418: step 2948, loss 0.310917, acc 0.890625, learning_rate 0.000100029
2017-10-09T15:11:54.599320: step 2949, loss 0.322875, acc 0.921875, learning_rate 0.000100028
2017-10-09T15:11:54.704991: step 2950, loss 0.391555, acc 0.828125, learning_rate 0.000100028
2017-10-09T15:11:54.812258: step 2951, loss 0.411194, acc 0.8125, learning_rate 0.000100028
2017-10-09T15:11:54.919065: step 2952, loss 0.292645, acc 0.90625, learning_rate 0.000100028
2017-10-09T15:11:55.021650: step 2953, loss 0.422632, acc 0.859375, learning_rate 0.000100028
2017-10-09T15:11:55.125817: step 2954, loss 0.362646, acc 0.890625, learning_rate 0.000100028
2017-10-09T15:11:55.230575: step 2955, loss 0.380002, acc 0.90625, learning_rate 0.000100028
2017-10-09T15:11:55.337196: step 2956, loss 0.306503, acc 0.90625, learning_rate 0.000100028
2017-10-09T15:11:55.441708: step 2957, loss 0.345636, acc 0.875, learning_rate 0.000100028
2017-10-09T15:11:55.548298: step 2958, loss 0.355121, acc 0.84375, learning_rate 0.000100027
2017-10-09T15:11:55.653428: step 2959, loss 0.332457, acc 0.875, learning_rate 0.000100027
2017-10-09T15:11:55.756763: step 2960, loss 0.303218, acc 0.90625, learning_rate 0.000100027

Evaluation:
2017-10-09T15:11:56.010661: step 2960, loss 0.346621, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-2960

2017-10-09T15:11:56.531582: step 2961, loss 0.333291, acc 0.9375, learning_rate 0.000100027
2017-10-09T15:11:56.637810: step 2962, loss 0.241483, acc 0.921875, learning_rate 0.000100027
2017-10-09T15:11:56.743701: step 2963, loss 0.500634, acc 0.8125, learning_rate 0.000100027
2017-10-09T15:11:56.846476: step 2964, loss 0.289163, acc 0.890625, learning_rate 0.000100027
2017-10-09T15:11:56.954201: step 2965, loss 0.333834, acc 0.890625, learning_rate 0.000100027
2017-10-09T15:11:57.057789: step 2966, loss 0.441306, acc 0.828125, learning_rate 0.000100027
2017-10-09T15:11:57.166503: step 2967, loss 0.404845, acc 0.859375, learning_rate 0.000100026
2017-10-09T15:11:57.270888: step 2968, loss 0.284728, acc 0.921875, learning_rate 0.000100026
2017-10-09T15:11:57.374463: step 2969, loss 0.333389, acc 0.84375, learning_rate 0.000100026
2017-10-09T15:11:57.481255: step 2970, loss 0.339576, acc 0.875, learning_rate 0.000100026
2017-10-09T15:11:57.586549: step 2971, loss 0.301573, acc 0.890625, learning_rate 0.000100026
2017-10-09T15:11:57.686503: step 2972, loss 0.443564, acc 0.890625, learning_rate 0.000100026
2017-10-09T15:11:57.791025: step 2973, loss 0.333516, acc 0.859375, learning_rate 0.000100026
2017-10-09T15:11:57.895450: step 2974, loss 0.271792, acc 0.921875, learning_rate 0.000100026
2017-10-09T15:11:58.001763: step 2975, loss 0.211532, acc 0.9375, learning_rate 0.000100026
2017-10-09T15:11:58.110006: step 2976, loss 0.294429, acc 0.890625, learning_rate 0.000100025
2017-10-09T15:11:58.216875: step 2977, loss 0.402985, acc 0.859375, learning_rate 0.000100025
2017-10-09T15:11:58.324415: step 2978, loss 0.407711, acc 0.890625, learning_rate 0.000100025
2017-10-09T15:11:58.433005: step 2979, loss 0.307941, acc 0.90625, learning_rate 0.000100025
2017-10-09T15:11:58.540557: step 2980, loss 0.303903, acc 0.90625, learning_rate 0.000100025
2017-10-09T15:11:58.641570: step 2981, loss 0.318058, acc 0.890625, learning_rate 0.000100025
2017-10-09T15:11:58.750567: step 2982, loss 0.47803, acc 0.828125, learning_rate 0.000100025
2017-10-09T15:11:58.858362: step 2983, loss 0.23936, acc 0.921875, learning_rate 0.000100025
2017-10-09T15:11:58.965312: step 2984, loss 0.221989, acc 0.953125, learning_rate 0.000100025
2017-10-09T15:11:59.071335: step 2985, loss 0.276141, acc 0.921875, learning_rate 0.000100025
2017-10-09T15:11:59.176507: step 2986, loss 0.239282, acc 0.921875, learning_rate 0.000100024
2017-10-09T15:11:59.280673: step 2987, loss 0.347897, acc 0.875, learning_rate 0.000100024
2017-10-09T15:11:59.388506: step 2988, loss 0.2288, acc 0.953125, learning_rate 0.000100024
2017-10-09T15:11:59.496376: step 2989, loss 0.228657, acc 0.921875, learning_rate 0.000100024
2017-10-09T15:11:59.599484: step 2990, loss 0.396448, acc 0.890625, learning_rate 0.000100024
2017-10-09T15:11:59.704577: step 2991, loss 0.318047, acc 0.90625, learning_rate 0.000100024
2017-10-09T15:11:59.806322: step 2992, loss 0.31556, acc 0.921875, learning_rate 0.000100024
2017-10-09T15:11:59.912282: step 2993, loss 0.359434, acc 0.875, learning_rate 0.000100024
2017-10-09T15:12:00.020423: step 2994, loss 0.241697, acc 0.9375, learning_rate 0.000100024
2017-10-09T15:12:00.124344: step 2995, loss 0.242281, acc 0.953125, learning_rate 0.000100024
2017-10-09T15:12:00.225410: step 2996, loss 0.27084, acc 0.90625, learning_rate 0.000100023
2017-10-09T15:12:00.331877: step 2997, loss 0.322161, acc 0.90625, learning_rate 0.000100023
2017-10-09T15:12:00.437880: step 2998, loss 0.1932, acc 0.9375, learning_rate 0.000100023
2017-10-09T15:12:00.545959: step 2999, loss 0.513108, acc 0.84375, learning_rate 0.000100023
2017-10-09T15:12:00.650789: step 3000, loss 0.306362, acc 0.9375, learning_rate 0.000100023

Evaluation:
2017-10-09T15:12:00.903921: step 3000, loss 0.348458, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3000

2017-10-09T15:12:01.487380: step 3001, loss 0.365621, acc 0.828125, learning_rate 0.000100023
2017-10-09T15:12:01.592592: step 3002, loss 0.353396, acc 0.859375, learning_rate 0.000100023
2017-10-09T15:12:01.698090: step 3003, loss 0.273959, acc 0.921875, learning_rate 0.000100023
2017-10-09T15:12:01.799387: step 3004, loss 0.28396, acc 0.90625, learning_rate 0.000100023
2017-10-09T15:12:01.904598: step 3005, loss 0.318359, acc 0.890625, learning_rate 0.000100023
2017-10-09T15:12:02.009176: step 3006, loss 0.283405, acc 0.890625, learning_rate 0.000100023
2017-10-09T15:12:02.112730: step 3007, loss 0.328184, acc 0.875, learning_rate 0.000100022
2017-10-09T15:12:02.215410: step 3008, loss 0.476764, acc 0.859375, learning_rate 0.000100022
2017-10-09T15:12:02.320779: step 3009, loss 0.293222, acc 0.875, learning_rate 0.000100022
2017-10-09T15:12:02.426745: step 3010, loss 0.173183, acc 0.96875, learning_rate 0.000100022
2017-10-09T15:12:02.534720: step 3011, loss 0.268358, acc 0.921875, learning_rate 0.000100022
2017-10-09T15:12:02.637618: step 3012, loss 0.230377, acc 0.890625, learning_rate 0.000100022
2017-10-09T15:12:02.743032: step 3013, loss 0.364787, acc 0.84375, learning_rate 0.000100022
2017-10-09T15:12:02.850631: step 3014, loss 0.364075, acc 0.875, learning_rate 0.000100022
2017-10-09T15:12:02.954733: step 3015, loss 0.533163, acc 0.78125, learning_rate 0.000100022
2017-10-09T15:12:03.058466: step 3016, loss 0.335963, acc 0.890625, learning_rate 0.000100022
2017-10-09T15:12:03.165117: step 3017, loss 0.37169, acc 0.859375, learning_rate 0.000100022
2017-10-09T15:12:03.274459: step 3018, loss 0.279276, acc 0.921875, learning_rate 0.000100021
2017-10-09T15:12:03.378877: step 3019, loss 0.321379, acc 0.90625, learning_rate 0.000100021
2017-10-09T15:12:03.485099: step 3020, loss 0.371751, acc 0.859375, learning_rate 0.000100021
2017-10-09T15:12:03.588332: step 3021, loss 0.19982, acc 0.953125, learning_rate 0.000100021
2017-10-09T15:12:03.692233: step 3022, loss 0.246989, acc 0.9375, learning_rate 0.000100021
2017-10-09T15:12:03.798628: step 3023, loss 0.350081, acc 0.875, learning_rate 0.000100021
2017-10-09T15:12:03.907994: step 3024, loss 0.35706, acc 0.890625, learning_rate 0.000100021
2017-10-09T15:12:04.007921: step 3025, loss 0.259368, acc 0.890625, learning_rate 0.000100021
2017-10-09T15:12:04.111250: step 3026, loss 0.341636, acc 0.890625, learning_rate 0.000100021
2017-10-09T15:12:04.216479: step 3027, loss 0.346972, acc 0.859375, learning_rate 0.000100021
2017-10-09T15:12:04.320844: step 3028, loss 0.320743, acc 0.890625, learning_rate 0.000100021
2017-10-09T15:12:04.430710: step 3029, loss 0.390638, acc 0.875, learning_rate 0.00010002
2017-10-09T15:12:04.535919: step 3030, loss 0.330777, acc 0.90625, learning_rate 0.00010002
2017-10-09T15:12:04.641647: step 3031, loss 0.476421, acc 0.828125, learning_rate 0.00010002
2017-10-09T15:12:04.747242: step 3032, loss 0.184854, acc 0.96875, learning_rate 0.00010002
2017-10-09T15:12:04.852671: step 3033, loss 0.330163, acc 0.90625, learning_rate 0.00010002
2017-10-09T15:12:04.960243: step 3034, loss 0.41807, acc 0.828125, learning_rate 0.00010002
2017-10-09T15:12:05.068064: step 3035, loss 0.510487, acc 0.828125, learning_rate 0.00010002
2017-10-09T15:12:05.172304: step 3036, loss 0.395932, acc 0.875, learning_rate 0.00010002
2017-10-09T15:12:05.272991: step 3037, loss 0.19363, acc 0.9375, learning_rate 0.00010002
2017-10-09T15:12:05.363644: step 3038, loss 0.385033, acc 0.862745, learning_rate 0.00010002
2017-10-09T15:12:05.471468: step 3039, loss 0.22663, acc 0.953125, learning_rate 0.00010002
2017-10-09T15:12:05.577018: step 3040, loss 0.332409, acc 0.875, learning_rate 0.00010002

Evaluation:
2017-10-09T15:12:05.831697: step 3040, loss 0.348422, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3040

2017-10-09T15:12:06.421455: step 3041, loss 0.221201, acc 0.90625, learning_rate 0.00010002
2017-10-09T15:12:06.521174: step 3042, loss 0.508649, acc 0.796875, learning_rate 0.000100019
2017-10-09T15:12:06.622105: step 3043, loss 0.340497, acc 0.859375, learning_rate 0.000100019
2017-10-09T15:12:06.729663: step 3044, loss 0.254558, acc 0.890625, learning_rate 0.000100019
2017-10-09T15:12:06.833915: step 3045, loss 0.314526, acc 0.90625, learning_rate 0.000100019
2017-10-09T15:12:06.940987: step 3046, loss 0.306733, acc 0.90625, learning_rate 0.000100019
2017-10-09T15:12:07.045140: step 3047, loss 0.297477, acc 0.890625, learning_rate 0.000100019
2017-10-09T15:12:07.152373: step 3048, loss 0.219063, acc 0.921875, learning_rate 0.000100019
2017-10-09T15:12:07.260276: step 3049, loss 0.39893, acc 0.875, learning_rate 0.000100019
2017-10-09T15:12:07.364878: step 3050, loss 0.473322, acc 0.796875, learning_rate 0.000100019
2017-10-09T15:12:07.468937: step 3051, loss 0.177223, acc 0.9375, learning_rate 0.000100019
2017-10-09T15:12:07.574057: step 3052, loss 0.306567, acc 0.875, learning_rate 0.000100019
2017-10-09T15:12:07.682384: step 3053, loss 0.282628, acc 0.90625, learning_rate 0.000100019
2017-10-09T15:12:07.788147: step 3054, loss 0.305898, acc 0.90625, learning_rate 0.000100018
2017-10-09T15:12:07.895909: step 3055, loss 0.41685, acc 0.84375, learning_rate 0.000100018
2017-10-09T15:12:08.003488: step 3056, loss 0.257383, acc 0.90625, learning_rate 0.000100018
2017-10-09T15:12:08.109026: step 3057, loss 0.316631, acc 0.90625, learning_rate 0.000100018
2017-10-09T15:12:08.215409: step 3058, loss 0.260146, acc 0.921875, learning_rate 0.000100018
2017-10-09T15:12:08.325528: step 3059, loss 0.172291, acc 0.9375, learning_rate 0.000100018
2017-10-09T15:12:08.435807: step 3060, loss 0.368888, acc 0.859375, learning_rate 0.000100018
2017-10-09T15:12:08.544750: step 3061, loss 0.235173, acc 0.921875, learning_rate 0.000100018
2017-10-09T15:12:08.652175: step 3062, loss 0.429041, acc 0.859375, learning_rate 0.000100018
2017-10-09T15:12:08.756789: step 3063, loss 0.222568, acc 0.96875, learning_rate 0.000100018
2017-10-09T15:12:08.864927: step 3064, loss 0.302129, acc 0.875, learning_rate 0.000100018
2017-10-09T15:12:08.971169: step 3065, loss 0.517868, acc 0.828125, learning_rate 0.000100018
2017-10-09T15:12:09.076717: step 3066, loss 0.408403, acc 0.8125, learning_rate 0.000100018
2017-10-09T15:12:09.181762: step 3067, loss 0.389516, acc 0.84375, learning_rate 0.000100018
2017-10-09T15:12:09.287979: step 3068, loss 0.350828, acc 0.890625, learning_rate 0.000100017
2017-10-09T15:12:09.393395: step 3069, loss 0.365914, acc 0.875, learning_rate 0.000100017
2017-10-09T15:12:09.499633: step 3070, loss 0.300313, acc 0.890625, learning_rate 0.000100017
2017-10-09T15:12:09.599967: step 3071, loss 0.196619, acc 0.96875, learning_rate 0.000100017
2017-10-09T15:12:09.708906: step 3072, loss 0.322905, acc 0.90625, learning_rate 0.000100017
2017-10-09T15:12:09.813281: step 3073, loss 0.281081, acc 0.90625, learning_rate 0.000100017
2017-10-09T15:12:09.918890: step 3074, loss 0.36802, acc 0.828125, learning_rate 0.000100017
2017-10-09T15:12:10.027414: step 3075, loss 0.364102, acc 0.875, learning_rate 0.000100017
2017-10-09T15:12:10.132545: step 3076, loss 0.294509, acc 0.90625, learning_rate 0.000100017
2017-10-09T15:12:10.239782: step 3077, loss 0.24867, acc 0.90625, learning_rate 0.000100017
2017-10-09T15:12:10.344290: step 3078, loss 0.383989, acc 0.890625, learning_rate 0.000100017
2017-10-09T15:12:10.451790: step 3079, loss 0.294414, acc 0.859375, learning_rate 0.000100017
2017-10-09T15:12:10.557601: step 3080, loss 0.297235, acc 0.90625, learning_rate 0.000100017

Evaluation:
2017-10-09T15:12:10.811620: step 3080, loss 0.348385, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3080

2017-10-09T15:12:11.473674: step 3081, loss 0.192609, acc 0.921875, learning_rate 0.000100017
2017-10-09T15:12:11.576768: step 3082, loss 0.330779, acc 0.890625, learning_rate 0.000100016
2017-10-09T15:12:11.680333: step 3083, loss 0.46174, acc 0.859375, learning_rate 0.000100016
2017-10-09T15:12:11.787582: step 3084, loss 0.301916, acc 0.921875, learning_rate 0.000100016
2017-10-09T15:12:11.896409: step 3085, loss 0.360876, acc 0.875, learning_rate 0.000100016
2017-10-09T15:12:12.001980: step 3086, loss 0.425582, acc 0.828125, learning_rate 0.000100016
2017-10-09T15:12:12.106949: step 3087, loss 0.249763, acc 0.921875, learning_rate 0.000100016
2017-10-09T15:12:12.214351: step 3088, loss 0.302647, acc 0.890625, learning_rate 0.000100016
2017-10-09T15:12:12.317800: step 3089, loss 0.231025, acc 0.9375, learning_rate 0.000100016
2017-10-09T15:12:12.425082: step 3090, loss 0.241861, acc 0.90625, learning_rate 0.000100016
2017-10-09T15:12:12.531949: step 3091, loss 0.306093, acc 0.890625, learning_rate 0.000100016
2017-10-09T15:12:12.640172: step 3092, loss 0.309351, acc 0.875, learning_rate 0.000100016
2017-10-09T15:12:12.742560: step 3093, loss 0.423466, acc 0.8125, learning_rate 0.000100016
2017-10-09T15:12:12.848662: step 3094, loss 0.289743, acc 0.90625, learning_rate 0.000100016
2017-10-09T15:12:12.951291: step 3095, loss 0.31421, acc 0.875, learning_rate 0.000100016
2017-10-09T15:12:13.052251: step 3096, loss 0.29931, acc 0.90625, learning_rate 0.000100016
2017-10-09T15:12:13.160729: step 3097, loss 0.325268, acc 0.90625, learning_rate 0.000100016
2017-10-09T15:12:13.265960: step 3098, loss 0.299694, acc 0.875, learning_rate 0.000100015
2017-10-09T15:12:13.367112: step 3099, loss 0.305456, acc 0.921875, learning_rate 0.000100015
2017-10-09T15:12:13.473749: step 3100, loss 0.231838, acc 0.9375, learning_rate 0.000100015
2017-10-09T15:12:13.578255: step 3101, loss 0.430798, acc 0.859375, learning_rate 0.000100015
2017-10-09T15:12:13.681681: step 3102, loss 0.676892, acc 0.734375, learning_rate 0.000100015
2017-10-09T15:12:13.788188: step 3103, loss 0.577182, acc 0.78125, learning_rate 0.000100015
2017-10-09T15:12:13.896260: step 3104, loss 0.405505, acc 0.859375, learning_rate 0.000100015
2017-10-09T15:12:14.004012: step 3105, loss 0.293757, acc 0.90625, learning_rate 0.000100015
2017-10-09T15:12:14.111032: step 3106, loss 0.287055, acc 0.890625, learning_rate 0.000100015
2017-10-09T15:12:14.219757: step 3107, loss 0.334346, acc 0.890625, learning_rate 0.000100015
2017-10-09T15:12:14.327130: step 3108, loss 0.537251, acc 0.796875, learning_rate 0.000100015
2017-10-09T15:12:14.430770: step 3109, loss 0.272895, acc 0.890625, learning_rate 0.000100015
2017-10-09T15:12:14.536944: step 3110, loss 0.317404, acc 0.9375, learning_rate 0.000100015
2017-10-09T15:12:14.641634: step 3111, loss 0.262828, acc 0.921875, learning_rate 0.000100015
2017-10-09T15:12:14.744925: step 3112, loss 0.36792, acc 0.859375, learning_rate 0.000100015
2017-10-09T15:12:14.853750: step 3113, loss 0.339714, acc 0.84375, learning_rate 0.000100015
2017-10-09T15:12:14.956238: step 3114, loss 0.303056, acc 0.890625, learning_rate 0.000100014
2017-10-09T15:12:15.060894: step 3115, loss 0.331858, acc 0.890625, learning_rate 0.000100014
2017-10-09T15:12:15.164814: step 3116, loss 0.196605, acc 0.9375, learning_rate 0.000100014
2017-10-09T15:12:15.263322: step 3117, loss 0.293539, acc 0.875, learning_rate 0.000100014
2017-10-09T15:12:15.369885: step 3118, loss 0.373252, acc 0.875, learning_rate 0.000100014
2017-10-09T15:12:15.471640: step 3119, loss 0.384644, acc 0.875, learning_rate 0.000100014
2017-10-09T15:12:15.576895: step 3120, loss 0.229108, acc 0.921875, learning_rate 0.000100014

Evaluation:
2017-10-09T15:12:15.833818: step 3120, loss 0.344862, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3120

2017-10-09T15:12:16.352159: step 3121, loss 0.233781, acc 0.90625, learning_rate 0.000100014
2017-10-09T15:12:16.459867: step 3122, loss 0.342046, acc 0.90625, learning_rate 0.000100014
2017-10-09T15:12:16.565030: step 3123, loss 0.468054, acc 0.828125, learning_rate 0.000100014
2017-10-09T15:12:16.675748: step 3124, loss 0.27099, acc 0.921875, learning_rate 0.000100014
2017-10-09T15:12:16.783677: step 3125, loss 0.203839, acc 0.9375, learning_rate 0.000100014
2017-10-09T15:12:16.889526: step 3126, loss 0.374464, acc 0.859375, learning_rate 0.000100014
2017-10-09T15:12:16.992505: step 3127, loss 0.380005, acc 0.875, learning_rate 0.000100014
2017-10-09T15:12:17.098507: step 3128, loss 0.283156, acc 0.875, learning_rate 0.000100014
2017-10-09T15:12:17.203898: step 3129, loss 0.270894, acc 0.890625, learning_rate 0.000100014
2017-10-09T15:12:17.304660: step 3130, loss 0.260495, acc 0.90625, learning_rate 0.000100014
2017-10-09T15:12:17.409990: step 3131, loss 0.520675, acc 0.84375, learning_rate 0.000100014
2017-10-09T15:12:17.515298: step 3132, loss 0.257113, acc 0.875, learning_rate 0.000100013
2017-10-09T15:12:17.621967: step 3133, loss 0.216255, acc 0.921875, learning_rate 0.000100013
2017-10-09T15:12:17.728794: step 3134, loss 0.292769, acc 0.875, learning_rate 0.000100013
2017-10-09T15:12:17.839499: step 3135, loss 0.232865, acc 0.90625, learning_rate 0.000100013
2017-10-09T15:12:17.927566: step 3136, loss 0.241086, acc 0.882353, learning_rate 0.000100013
2017-10-09T15:12:18.029560: step 3137, loss 0.355179, acc 0.890625, learning_rate 0.000100013
2017-10-09T15:12:18.136159: step 3138, loss 0.500465, acc 0.875, learning_rate 0.000100013
2017-10-09T15:12:18.243449: step 3139, loss 0.297373, acc 0.90625, learning_rate 0.000100013
2017-10-09T15:12:18.350316: step 3140, loss 0.329334, acc 0.890625, learning_rate 0.000100013
2017-10-09T15:12:18.458905: step 3141, loss 0.393272, acc 0.859375, learning_rate 0.000100013
2017-10-09T15:12:18.567613: step 3142, loss 0.36778, acc 0.84375, learning_rate 0.000100013
2017-10-09T15:12:18.673223: step 3143, loss 0.287274, acc 0.921875, learning_rate 0.000100013
2017-10-09T15:12:18.776899: step 3144, loss 0.318336, acc 0.921875, learning_rate 0.000100013
2017-10-09T15:12:18.882447: step 3145, loss 0.344836, acc 0.875, learning_rate 0.000100013
2017-10-09T15:12:18.987880: step 3146, loss 0.243368, acc 0.875, learning_rate 0.000100013
2017-10-09T15:12:19.097677: step 3147, loss 0.282949, acc 0.875, learning_rate 0.000100013
2017-10-09T15:12:19.199931: step 3148, loss 0.4109, acc 0.875, learning_rate 0.000100013
2017-10-09T15:12:19.305676: step 3149, loss 0.332538, acc 0.875, learning_rate 0.000100013
2017-10-09T15:12:19.414733: step 3150, loss 0.272132, acc 0.921875, learning_rate 0.000100012
2017-10-09T15:12:19.524501: step 3151, loss 0.233808, acc 0.96875, learning_rate 0.000100012
2017-10-09T15:12:19.632056: step 3152, loss 0.20734, acc 0.921875, learning_rate 0.000100012
2017-10-09T15:12:19.738516: step 3153, loss 0.239968, acc 0.90625, learning_rate 0.000100012
2017-10-09T15:12:19.845618: step 3154, loss 0.393212, acc 0.828125, learning_rate 0.000100012
2017-10-09T15:12:19.951215: step 3155, loss 0.217039, acc 0.9375, learning_rate 0.000100012
2017-10-09T15:12:20.056942: step 3156, loss 0.308868, acc 0.890625, learning_rate 0.000100012
2017-10-09T15:12:20.161585: step 3157, loss 0.288062, acc 0.9375, learning_rate 0.000100012
2017-10-09T15:12:20.263733: step 3158, loss 0.298858, acc 0.90625, learning_rate 0.000100012
2017-10-09T15:12:20.367328: step 3159, loss 0.23436, acc 0.921875, learning_rate 0.000100012
2017-10-09T15:12:20.474268: step 3160, loss 0.259363, acc 0.921875, learning_rate 0.000100012

Evaluation:
2017-10-09T15:12:20.730025: step 3160, loss 0.345506, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3160

2017-10-09T15:12:21.317343: step 3161, loss 0.231021, acc 0.9375, learning_rate 0.000100012
2017-10-09T15:12:21.422696: step 3162, loss 0.351096, acc 0.875, learning_rate 0.000100012
2017-10-09T15:12:21.528938: step 3163, loss 0.318756, acc 0.90625, learning_rate 0.000100012
2017-10-09T15:12:21.632035: step 3164, loss 0.242596, acc 0.9375, learning_rate 0.000100012
2017-10-09T15:12:21.738447: step 3165, loss 0.297286, acc 0.859375, learning_rate 0.000100012
2017-10-09T15:12:21.847135: step 3166, loss 0.306787, acc 0.90625, learning_rate 0.000100012
2017-10-09T15:12:21.951337: step 3167, loss 0.368446, acc 0.859375, learning_rate 0.000100012
2017-10-09T15:12:22.057642: step 3168, loss 0.222856, acc 0.9375, learning_rate 0.000100012
2017-10-09T15:12:22.159709: step 3169, loss 0.358185, acc 0.84375, learning_rate 0.000100012
2017-10-09T15:12:22.262178: step 3170, loss 0.351702, acc 0.890625, learning_rate 0.000100012
2017-10-09T15:12:22.367126: step 3171, loss 0.248395, acc 0.890625, learning_rate 0.000100011
2017-10-09T15:12:22.472852: step 3172, loss 0.325739, acc 0.875, learning_rate 0.000100011
2017-10-09T15:12:22.580957: step 3173, loss 0.255747, acc 0.96875, learning_rate 0.000100011
2017-10-09T15:12:22.683478: step 3174, loss 0.260682, acc 0.953125, learning_rate 0.000100011
2017-10-09T15:12:22.787558: step 3175, loss 0.312578, acc 0.859375, learning_rate 0.000100011
2017-10-09T15:12:22.895720: step 3176, loss 0.355747, acc 0.875, learning_rate 0.000100011
2017-10-09T15:12:23.003394: step 3177, loss 0.392599, acc 0.875, learning_rate 0.000100011
2017-10-09T15:12:23.108843: step 3178, loss 0.334207, acc 0.875, learning_rate 0.000100011
2017-10-09T15:12:23.210918: step 3179, loss 0.394018, acc 0.859375, learning_rate 0.000100011
2017-10-09T15:12:23.313015: step 3180, loss 0.441957, acc 0.8125, learning_rate 0.000100011
2017-10-09T15:12:23.422896: step 3181, loss 0.298694, acc 0.890625, learning_rate 0.000100011
2017-10-09T15:12:23.528865: step 3182, loss 0.220802, acc 0.953125, learning_rate 0.000100011
2017-10-09T15:12:23.633098: step 3183, loss 0.392703, acc 0.875, learning_rate 0.000100011
2017-10-09T15:12:23.737082: step 3184, loss 0.369314, acc 0.859375, learning_rate 0.000100011
2017-10-09T15:12:23.844816: step 3185, loss 0.334269, acc 0.90625, learning_rate 0.000100011
2017-10-09T15:12:23.957555: step 3186, loss 0.263684, acc 0.90625, learning_rate 0.000100011
2017-10-09T15:12:24.062472: step 3187, loss 0.280137, acc 0.890625, learning_rate 0.000100011
2017-10-09T15:12:24.167169: step 3188, loss 0.156009, acc 0.953125, learning_rate 0.000100011
2017-10-09T15:12:24.267622: step 3189, loss 0.297666, acc 0.875, learning_rate 0.000100011
2017-10-09T15:12:24.374275: step 3190, loss 0.482827, acc 0.828125, learning_rate 0.000100011
2017-10-09T15:12:24.478844: step 3191, loss 0.324493, acc 0.890625, learning_rate 0.000100011
2017-10-09T15:12:24.584414: step 3192, loss 0.31937, acc 0.90625, learning_rate 0.000100011
2017-10-09T15:12:24.687384: step 3193, loss 0.252317, acc 0.921875, learning_rate 0.00010001
2017-10-09T15:12:24.794116: step 3194, loss 0.230137, acc 0.921875, learning_rate 0.00010001
2017-10-09T15:12:24.901301: step 3195, loss 0.27484, acc 0.875, learning_rate 0.00010001
2017-10-09T15:12:25.004931: step 3196, loss 0.231548, acc 0.921875, learning_rate 0.00010001
2017-10-09T15:12:25.110183: step 3197, loss 0.267518, acc 0.953125, learning_rate 0.00010001
2017-10-09T15:12:25.219274: step 3198, loss 0.287604, acc 0.921875, learning_rate 0.00010001
2017-10-09T15:12:25.323128: step 3199, loss 0.272262, acc 0.9375, learning_rate 0.00010001
2017-10-09T15:12:25.430871: step 3200, loss 0.270905, acc 0.875, learning_rate 0.00010001

Evaluation:
2017-10-09T15:12:25.682853: step 3200, loss 0.347427, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3200

2017-10-09T15:12:26.268719: step 3201, loss 0.307055, acc 0.890625, learning_rate 0.00010001
2017-10-09T15:12:26.372639: step 3202, loss 0.242463, acc 0.9375, learning_rate 0.00010001
2017-10-09T15:12:26.475343: step 3203, loss 0.310596, acc 0.859375, learning_rate 0.00010001
2017-10-09T15:12:26.581117: step 3204, loss 0.320237, acc 0.875, learning_rate 0.00010001
2017-10-09T15:12:26.686975: step 3205, loss 0.300946, acc 0.890625, learning_rate 0.00010001
2017-10-09T15:12:26.794910: step 3206, loss 0.263009, acc 0.90625, learning_rate 0.00010001
2017-10-09T15:12:26.898359: step 3207, loss 0.284012, acc 0.859375, learning_rate 0.00010001
2017-10-09T15:12:27.001403: step 3208, loss 0.526492, acc 0.828125, learning_rate 0.00010001
2017-10-09T15:12:27.105252: step 3209, loss 0.291584, acc 0.953125, learning_rate 0.00010001
2017-10-09T15:12:27.206766: step 3210, loss 0.367632, acc 0.90625, learning_rate 0.00010001
2017-10-09T15:12:27.313436: step 3211, loss 0.374325, acc 0.875, learning_rate 0.00010001
2017-10-09T15:12:27.419786: step 3212, loss 0.233641, acc 0.953125, learning_rate 0.00010001
2017-10-09T15:12:27.524018: step 3213, loss 0.402917, acc 0.796875, learning_rate 0.00010001
2017-10-09T15:12:27.627838: step 3214, loss 0.626077, acc 0.75, learning_rate 0.00010001
2017-10-09T15:12:27.732378: step 3215, loss 0.389617, acc 0.828125, learning_rate 0.00010001
2017-10-09T15:12:27.837434: step 3216, loss 0.290481, acc 0.90625, learning_rate 0.00010001
2017-10-09T15:12:27.943798: step 3217, loss 0.301816, acc 0.921875, learning_rate 0.000100009
2017-10-09T15:12:28.051303: step 3218, loss 0.251454, acc 0.875, learning_rate 0.000100009
2017-10-09T15:12:28.150980: step 3219, loss 0.429202, acc 0.875, learning_rate 0.000100009
2017-10-09T15:12:28.256945: step 3220, loss 0.238991, acc 0.953125, learning_rate 0.000100009
2017-10-09T15:12:28.363089: step 3221, loss 0.345231, acc 0.890625, learning_rate 0.000100009
2017-10-09T15:12:28.465777: step 3222, loss 0.240996, acc 0.921875, learning_rate 0.000100009
2017-10-09T15:12:28.572372: step 3223, loss 0.243301, acc 0.9375, learning_rate 0.000100009
2017-10-09T15:12:28.678943: step 3224, loss 0.175455, acc 0.96875, learning_rate 0.000100009
2017-10-09T15:12:28.780982: step 3225, loss 0.304332, acc 0.828125, learning_rate 0.000100009
2017-10-09T15:12:28.881210: step 3226, loss 0.370183, acc 0.859375, learning_rate 0.000100009
2017-10-09T15:12:28.988966: step 3227, loss 0.380257, acc 0.8125, learning_rate 0.000100009
2017-10-09T15:12:29.095107: step 3228, loss 0.252807, acc 0.890625, learning_rate 0.000100009
2017-10-09T15:12:29.201839: step 3229, loss 0.313612, acc 0.875, learning_rate 0.000100009
2017-10-09T15:12:29.307460: step 3230, loss 0.301707, acc 0.875, learning_rate 0.000100009
2017-10-09T15:12:29.413989: step 3231, loss 0.459267, acc 0.828125, learning_rate 0.000100009
2017-10-09T15:12:29.517531: step 3232, loss 0.329287, acc 0.859375, learning_rate 0.000100009
2017-10-09T15:12:29.623791: step 3233, loss 0.394755, acc 0.84375, learning_rate 0.000100009
2017-10-09T15:12:29.713000: step 3234, loss 0.241887, acc 0.941176, learning_rate 0.000100009
2017-10-09T15:12:29.819055: step 3235, loss 0.483246, acc 0.8125, learning_rate 0.000100009
2017-10-09T15:12:29.925621: step 3236, loss 0.310465, acc 0.890625, learning_rate 0.000100009
2017-10-09T15:12:30.030412: step 3237, loss 0.314684, acc 0.890625, learning_rate 0.000100009
2017-10-09T15:12:30.131555: step 3238, loss 0.262128, acc 0.90625, learning_rate 0.000100009
2017-10-09T15:12:30.235440: step 3239, loss 0.245657, acc 0.90625, learning_rate 0.000100009
2017-10-09T15:12:30.341206: step 3240, loss 0.200678, acc 0.921875, learning_rate 0.000100009

Evaluation:
2017-10-09T15:12:30.596423: step 3240, loss 0.343609, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3240

2017-10-09T15:12:31.297806: step 3241, loss 0.244101, acc 0.90625, learning_rate 0.000100009
2017-10-09T15:12:31.401570: step 3242, loss 0.329892, acc 0.875, learning_rate 0.000100009
2017-10-09T15:12:31.510052: step 3243, loss 0.338623, acc 0.890625, learning_rate 0.000100009
2017-10-09T15:12:31.616970: step 3244, loss 0.21577, acc 0.9375, learning_rate 0.000100009
2017-10-09T15:12:31.720904: step 3245, loss 0.29445, acc 0.890625, learning_rate 0.000100008
2017-10-09T15:12:31.828728: step 3246, loss 0.293118, acc 0.90625, learning_rate 0.000100008
2017-10-09T15:12:31.937273: step 3247, loss 0.367168, acc 0.90625, learning_rate 0.000100008
2017-10-09T15:12:32.046469: step 3248, loss 0.266551, acc 0.921875, learning_rate 0.000100008
2017-10-09T15:12:32.150996: step 3249, loss 0.220996, acc 0.953125, learning_rate 0.000100008
2017-10-09T15:12:32.255847: step 3250, loss 0.304756, acc 0.921875, learning_rate 0.000100008
2017-10-09T15:12:32.357244: step 3251, loss 0.338911, acc 0.90625, learning_rate 0.000100008
2017-10-09T15:12:32.460458: step 3252, loss 0.218887, acc 0.921875, learning_rate 0.000100008
2017-10-09T15:12:32.560547: step 3253, loss 0.424844, acc 0.828125, learning_rate 0.000100008
2017-10-09T15:12:32.666017: step 3254, loss 0.282385, acc 0.890625, learning_rate 0.000100008
2017-10-09T15:12:32.770242: step 3255, loss 0.261373, acc 0.890625, learning_rate 0.000100008
2017-10-09T15:12:32.883219: step 3256, loss 0.360632, acc 0.890625, learning_rate 0.000100008
2017-10-09T15:12:32.990053: step 3257, loss 0.261545, acc 0.90625, learning_rate 0.000100008
2017-10-09T15:12:33.098248: step 3258, loss 0.459171, acc 0.859375, learning_rate 0.000100008
2017-10-09T15:12:33.204239: step 3259, loss 0.390022, acc 0.828125, learning_rate 0.000100008
2017-10-09T15:12:33.309069: step 3260, loss 0.311889, acc 0.859375, learning_rate 0.000100008
2017-10-09T15:12:33.416579: step 3261, loss 0.283329, acc 0.921875, learning_rate 0.000100008
2017-10-09T15:12:33.523534: step 3262, loss 0.25344, acc 0.921875, learning_rate 0.000100008
2017-10-09T15:12:33.628275: step 3263, loss 0.335202, acc 0.859375, learning_rate 0.000100008
2017-10-09T15:12:33.736599: step 3264, loss 0.319619, acc 0.890625, learning_rate 0.000100008
2017-10-09T15:12:33.842170: step 3265, loss 0.29353, acc 0.890625, learning_rate 0.000100008
2017-10-09T15:12:33.948444: step 3266, loss 0.205057, acc 0.90625, learning_rate 0.000100008
2017-10-09T15:12:34.053520: step 3267, loss 0.213104, acc 0.953125, learning_rate 0.000100008
2017-10-09T15:12:34.158696: step 3268, loss 0.262555, acc 0.921875, learning_rate 0.000100008
2017-10-09T15:12:34.267466: step 3269, loss 0.244486, acc 0.890625, learning_rate 0.000100008
2017-10-09T15:12:34.372151: step 3270, loss 0.285477, acc 0.9375, learning_rate 0.000100008
2017-10-09T15:12:34.472632: step 3271, loss 0.379127, acc 0.84375, learning_rate 0.000100008
2017-10-09T15:12:34.576423: step 3272, loss 0.30259, acc 0.90625, learning_rate 0.000100008
2017-10-09T15:12:34.684672: step 3273, loss 0.285955, acc 0.90625, learning_rate 0.000100008
2017-10-09T15:12:34.790858: step 3274, loss 0.368837, acc 0.859375, learning_rate 0.000100008
2017-10-09T15:12:34.899568: step 3275, loss 0.275415, acc 0.921875, learning_rate 0.000100007
2017-10-09T15:12:35.006968: step 3276, loss 0.267683, acc 0.859375, learning_rate 0.000100007
2017-10-09T15:12:35.112906: step 3277, loss 0.340942, acc 0.84375, learning_rate 0.000100007
2017-10-09T15:12:35.218995: step 3278, loss 0.329302, acc 0.875, learning_rate 0.000100007
2017-10-09T15:12:35.331592: step 3279, loss 0.389397, acc 0.890625, learning_rate 0.000100007
2017-10-09T15:12:35.435498: step 3280, loss 0.349098, acc 0.875, learning_rate 0.000100007

Evaluation:
2017-10-09T15:12:35.692576: step 3280, loss 0.344896, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3280

2017-10-09T15:12:36.219790: step 3281, loss 0.354331, acc 0.890625, learning_rate 0.000100007
2017-10-09T15:12:36.324052: step 3282, loss 0.3965, acc 0.828125, learning_rate 0.000100007
2017-10-09T15:12:36.428924: step 3283, loss 0.29286, acc 0.890625, learning_rate 0.000100007
2017-10-09T15:12:36.535682: step 3284, loss 0.23299, acc 0.890625, learning_rate 0.000100007
2017-10-09T15:12:36.642858: step 3285, loss 0.342233, acc 0.921875, learning_rate 0.000100007
2017-10-09T15:12:36.748905: step 3286, loss 0.258275, acc 0.90625, learning_rate 0.000100007
2017-10-09T15:12:36.856797: step 3287, loss 0.347863, acc 0.875, learning_rate 0.000100007
2017-10-09T15:12:36.956908: step 3288, loss 0.304047, acc 0.890625, learning_rate 0.000100007
2017-10-09T15:12:37.063246: step 3289, loss 0.243863, acc 0.9375, learning_rate 0.000100007
2017-10-09T15:12:37.172797: step 3290, loss 0.174224, acc 0.953125, learning_rate 0.000100007
2017-10-09T15:12:37.277132: step 3291, loss 0.299603, acc 0.890625, learning_rate 0.000100007
2017-10-09T15:12:37.382794: step 3292, loss 0.298596, acc 0.90625, learning_rate 0.000100007
2017-10-09T15:12:37.494515: step 3293, loss 0.295092, acc 0.9375, learning_rate 0.000100007
2017-10-09T15:12:37.601714: step 3294, loss 0.408163, acc 0.84375, learning_rate 0.000100007
2017-10-09T15:12:37.706801: step 3295, loss 0.320365, acc 0.921875, learning_rate 0.000100007
2017-10-09T15:12:37.817777: step 3296, loss 0.306928, acc 0.890625, learning_rate 0.000100007
2017-10-09T15:12:37.928275: step 3297, loss 0.19907, acc 0.9375, learning_rate 0.000100007
2017-10-09T15:12:38.035295: step 3298, loss 0.456857, acc 0.859375, learning_rate 0.000100007
2017-10-09T15:12:38.142807: step 3299, loss 0.314424, acc 0.859375, learning_rate 0.000100007
2017-10-09T15:12:38.247073: step 3300, loss 0.418847, acc 0.828125, learning_rate 0.000100007
2017-10-09T15:12:38.349834: step 3301, loss 0.292548, acc 0.890625, learning_rate 0.000100007
2017-10-09T15:12:38.458046: step 3302, loss 0.237533, acc 0.96875, learning_rate 0.000100007
2017-10-09T15:12:38.564544: step 3303, loss 0.363087, acc 0.859375, learning_rate 0.000100007
2017-10-09T15:12:38.672287: step 3304, loss 0.276147, acc 0.90625, learning_rate 0.000100007
2017-10-09T15:12:38.777125: step 3305, loss 0.297243, acc 0.90625, learning_rate 0.000100007
2017-10-09T15:12:38.883433: step 3306, loss 0.353675, acc 0.90625, learning_rate 0.000100007
2017-10-09T15:12:38.989634: step 3307, loss 0.17552, acc 0.96875, learning_rate 0.000100007
2017-10-09T15:12:39.095637: step 3308, loss 0.326637, acc 0.859375, learning_rate 0.000100007
2017-10-09T15:12:39.202169: step 3309, loss 0.325694, acc 0.859375, learning_rate 0.000100007
2017-10-09T15:12:39.308048: step 3310, loss 0.340023, acc 0.84375, learning_rate 0.000100006
2017-10-09T15:12:39.416688: step 3311, loss 0.33585, acc 0.921875, learning_rate 0.000100006
2017-10-09T15:12:39.519330: step 3312, loss 0.366039, acc 0.890625, learning_rate 0.000100006
2017-10-09T15:12:39.626567: step 3313, loss 0.283021, acc 0.875, learning_rate 0.000100006
2017-10-09T15:12:39.727781: step 3314, loss 0.242381, acc 0.921875, learning_rate 0.000100006
2017-10-09T15:12:39.832582: step 3315, loss 0.302863, acc 0.859375, learning_rate 0.000100006
2017-10-09T15:12:39.942833: step 3316, loss 0.401709, acc 0.875, learning_rate 0.000100006
2017-10-09T15:12:40.049382: step 3317, loss 0.168015, acc 0.953125, learning_rate 0.000100006
2017-10-09T15:12:40.154672: step 3318, loss 0.461981, acc 0.84375, learning_rate 0.000100006
2017-10-09T15:12:40.259084: step 3319, loss 0.423549, acc 0.84375, learning_rate 0.000100006
2017-10-09T15:12:40.367821: step 3320, loss 0.342339, acc 0.84375, learning_rate 0.000100006

Evaluation:
2017-10-09T15:12:40.621448: step 3320, loss 0.342431, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3320

2017-10-09T15:12:41.209106: step 3321, loss 0.311503, acc 0.890625, learning_rate 0.000100006
2017-10-09T15:12:41.314714: step 3322, loss 0.287594, acc 0.921875, learning_rate 0.000100006
2017-10-09T15:12:41.421633: step 3323, loss 0.42364, acc 0.859375, learning_rate 0.000100006
2017-10-09T15:12:41.528930: step 3324, loss 0.297256, acc 0.890625, learning_rate 0.000100006
2017-10-09T15:12:41.633377: step 3325, loss 0.399605, acc 0.828125, learning_rate 0.000100006
2017-10-09T15:12:41.737439: step 3326, loss 0.239178, acc 0.921875, learning_rate 0.000100006
2017-10-09T15:12:41.844833: step 3327, loss 0.39797, acc 0.875, learning_rate 0.000100006
2017-10-09T15:12:41.945387: step 3328, loss 0.300213, acc 0.90625, learning_rate 0.000100006
2017-10-09T15:12:42.050318: step 3329, loss 0.367423, acc 0.859375, learning_rate 0.000100006
2017-10-09T15:12:42.151924: step 3330, loss 0.293165, acc 0.90625, learning_rate 0.000100006
2017-10-09T15:12:42.256667: step 3331, loss 0.302478, acc 0.90625, learning_rate 0.000100006
2017-10-09T15:12:42.343414: step 3332, loss 0.312646, acc 0.901961, learning_rate 0.000100006
2017-10-09T15:12:42.450191: step 3333, loss 0.185227, acc 0.96875, learning_rate 0.000100006
2017-10-09T15:12:42.552060: step 3334, loss 0.273279, acc 0.90625, learning_rate 0.000100006
2017-10-09T15:12:42.657386: step 3335, loss 0.209543, acc 0.9375, learning_rate 0.000100006
2017-10-09T15:12:42.764259: step 3336, loss 0.299956, acc 0.9375, learning_rate 0.000100006
2017-10-09T15:12:42.868615: step 3337, loss 0.298022, acc 0.875, learning_rate 0.000100006
2017-10-09T15:12:42.973781: step 3338, loss 0.284442, acc 0.875, learning_rate 0.000100006
2017-10-09T15:12:43.078842: step 3339, loss 0.382438, acc 0.859375, learning_rate 0.000100006
2017-10-09T15:12:43.183104: step 3340, loss 0.319264, acc 0.859375, learning_rate 0.000100006
2017-10-09T15:12:43.290013: step 3341, loss 0.349778, acc 0.875, learning_rate 0.000100006
2017-10-09T15:12:43.396523: step 3342, loss 0.395462, acc 0.859375, learning_rate 0.000100006
2017-10-09T15:12:43.501168: step 3343, loss 0.258033, acc 0.9375, learning_rate 0.000100006
2017-10-09T15:12:43.608396: step 3344, loss 0.352971, acc 0.859375, learning_rate 0.000100006
2017-10-09T15:12:43.715224: step 3345, loss 0.311731, acc 0.875, learning_rate 0.000100006
2017-10-09T15:12:43.822148: step 3346, loss 0.192329, acc 0.9375, learning_rate 0.000100006
2017-10-09T15:12:43.928098: step 3347, loss 0.298705, acc 0.890625, learning_rate 0.000100006
2017-10-09T15:12:44.034186: step 3348, loss 0.222267, acc 0.9375, learning_rate 0.000100006
2017-10-09T15:12:44.140611: step 3349, loss 0.365194, acc 0.90625, learning_rate 0.000100006
2017-10-09T15:12:44.248179: step 3350, loss 0.421511, acc 0.828125, learning_rate 0.000100006
2017-10-09T15:12:44.354294: step 3351, loss 0.218693, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:44.461519: step 3352, loss 0.421309, acc 0.828125, learning_rate 0.000100005
2017-10-09T15:12:44.565952: step 3353, loss 0.261637, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:44.671240: step 3354, loss 0.437295, acc 0.84375, learning_rate 0.000100005
2017-10-09T15:12:44.777346: step 3355, loss 0.287846, acc 0.90625, learning_rate 0.000100005
2017-10-09T15:12:44.882590: step 3356, loss 0.370731, acc 0.859375, learning_rate 0.000100005
2017-10-09T15:12:44.988097: step 3357, loss 0.180462, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:45.091972: step 3358, loss 0.438023, acc 0.828125, learning_rate 0.000100005
2017-10-09T15:12:45.198756: step 3359, loss 0.270767, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:45.300801: step 3360, loss 0.271283, acc 0.921875, learning_rate 0.000100005

Evaluation:
2017-10-09T15:12:45.556714: step 3360, loss 0.340014, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3360

2017-10-09T15:12:46.144427: step 3361, loss 0.246363, acc 0.921875, learning_rate 0.000100005
2017-10-09T15:12:46.249882: step 3362, loss 0.370124, acc 0.859375, learning_rate 0.000100005
2017-10-09T15:12:46.353160: step 3363, loss 0.237528, acc 0.890625, learning_rate 0.000100005
2017-10-09T15:12:46.460603: step 3364, loss 0.368172, acc 0.875, learning_rate 0.000100005
2017-10-09T15:12:46.566225: step 3365, loss 0.251465, acc 0.921875, learning_rate 0.000100005
2017-10-09T15:12:46.671753: step 3366, loss 0.416636, acc 0.875, learning_rate 0.000100005
2017-10-09T15:12:46.775216: step 3367, loss 0.308445, acc 0.875, learning_rate 0.000100005
2017-10-09T15:12:46.883636: step 3368, loss 0.269419, acc 0.921875, learning_rate 0.000100005
2017-10-09T15:12:46.993967: step 3369, loss 0.240576, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:47.098333: step 3370, loss 0.343359, acc 0.859375, learning_rate 0.000100005
2017-10-09T15:12:47.203580: step 3371, loss 0.215306, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:47.323451: step 3372, loss 0.395335, acc 0.84375, learning_rate 0.000100005
2017-10-09T15:12:47.426438: step 3373, loss 0.226182, acc 0.953125, learning_rate 0.000100005
2017-10-09T15:12:47.531735: step 3374, loss 0.184875, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:47.643043: step 3375, loss 0.284942, acc 0.890625, learning_rate 0.000100005
2017-10-09T15:12:47.756186: step 3376, loss 0.29421, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:47.870241: step 3377, loss 0.171428, acc 0.96875, learning_rate 0.000100005
2017-10-09T15:12:47.979707: step 3378, loss 0.201001, acc 0.953125, learning_rate 0.000100005
2017-10-09T15:12:48.092082: step 3379, loss 0.411175, acc 0.875, learning_rate 0.000100005
2017-10-09T15:12:48.202134: step 3380, loss 0.447957, acc 0.828125, learning_rate 0.000100005
2017-10-09T15:12:48.311061: step 3381, loss 0.218434, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:48.426257: step 3382, loss 0.30203, acc 0.890625, learning_rate 0.000100005
2017-10-09T15:12:48.534217: step 3383, loss 0.445396, acc 0.8125, learning_rate 0.000100005
2017-10-09T15:12:48.649215: step 3384, loss 0.362705, acc 0.875, learning_rate 0.000100005
2017-10-09T15:12:48.755596: step 3385, loss 0.366336, acc 0.875, learning_rate 0.000100005
2017-10-09T15:12:48.878679: step 3386, loss 0.217239, acc 0.921875, learning_rate 0.000100005
2017-10-09T15:12:48.988281: step 3387, loss 0.361313, acc 0.84375, learning_rate 0.000100005
2017-10-09T15:12:49.096555: step 3388, loss 0.36826, acc 0.875, learning_rate 0.000100005
2017-10-09T15:12:49.207849: step 3389, loss 0.207762, acc 0.953125, learning_rate 0.000100005
2017-10-09T15:12:49.319334: step 3390, loss 0.418128, acc 0.828125, learning_rate 0.000100005
2017-10-09T15:12:49.427761: step 3391, loss 0.239999, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:49.541114: step 3392, loss 0.467691, acc 0.859375, learning_rate 0.000100005
2017-10-09T15:12:49.657368: step 3393, loss 0.393593, acc 0.859375, learning_rate 0.000100005
2017-10-09T15:12:49.772817: step 3394, loss 0.606032, acc 0.796875, learning_rate 0.000100005
2017-10-09T15:12:49.883651: step 3395, loss 0.341096, acc 0.875, learning_rate 0.000100005
2017-10-09T15:12:49.993211: step 3396, loss 0.242875, acc 0.9375, learning_rate 0.000100005
2017-10-09T15:12:50.099996: step 3397, loss 0.293156, acc 0.921875, learning_rate 0.000100005
2017-10-09T15:12:50.213658: step 3398, loss 0.408066, acc 0.921875, learning_rate 0.000100005
2017-10-09T15:12:50.320541: step 3399, loss 0.340282, acc 0.890625, learning_rate 0.000100005
2017-10-09T15:12:50.427955: step 3400, loss 0.273377, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-09T15:12:50.683638: step 3400, loss 0.340311, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3400

2017-10-09T15:12:51.351234: step 3401, loss 0.295475, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:51.455746: step 3402, loss 0.212156, acc 0.953125, learning_rate 0.000100004
2017-10-09T15:12:51.563078: step 3403, loss 0.260391, acc 0.90625, learning_rate 0.000100004
2017-10-09T15:12:51.666193: step 3404, loss 0.232467, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:51.769741: step 3405, loss 0.270243, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:51.874395: step 3406, loss 0.446102, acc 0.84375, learning_rate 0.000100004
2017-10-09T15:12:51.980219: step 3407, loss 0.265517, acc 0.9375, learning_rate 0.000100004
2017-10-09T15:12:52.084136: step 3408, loss 0.165156, acc 0.953125, learning_rate 0.000100004
2017-10-09T15:12:52.201188: step 3409, loss 0.340817, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:52.309619: step 3410, loss 0.344927, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:52.413823: step 3411, loss 0.309439, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:52.522202: step 3412, loss 0.301022, acc 0.953125, learning_rate 0.000100004
2017-10-09T15:12:52.637260: step 3413, loss 0.465844, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:52.750864: step 3414, loss 0.266812, acc 0.90625, learning_rate 0.000100004
2017-10-09T15:12:52.864051: step 3415, loss 0.220979, acc 0.9375, learning_rate 0.000100004
2017-10-09T15:12:52.970074: step 3416, loss 0.317567, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:53.079812: step 3417, loss 0.330626, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:53.191254: step 3418, loss 0.228525, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:53.299783: step 3419, loss 0.306695, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:53.402567: step 3420, loss 0.17298, acc 0.953125, learning_rate 0.000100004
2017-10-09T15:12:53.506914: step 3421, loss 0.270854, acc 0.890625, learning_rate 0.000100004
2017-10-09T15:12:53.611217: step 3422, loss 0.402962, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:53.718864: step 3423, loss 0.360692, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:53.825359: step 3424, loss 0.315783, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:53.936238: step 3425, loss 0.278865, acc 0.890625, learning_rate 0.000100004
2017-10-09T15:12:54.041919: step 3426, loss 0.193249, acc 0.90625, learning_rate 0.000100004
2017-10-09T15:12:54.152943: step 3427, loss 0.342357, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:54.265627: step 3428, loss 0.250199, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:54.370981: step 3429, loss 0.54183, acc 0.796875, learning_rate 0.000100004
2017-10-09T15:12:54.464243: step 3430, loss 0.34154, acc 0.862745, learning_rate 0.000100004
2017-10-09T15:12:54.570163: step 3431, loss 0.470733, acc 0.8125, learning_rate 0.000100004
2017-10-09T15:12:54.683140: step 3432, loss 0.213974, acc 0.9375, learning_rate 0.000100004
2017-10-09T15:12:54.788409: step 3433, loss 0.27111, acc 0.9375, learning_rate 0.000100004
2017-10-09T15:12:54.907745: step 3434, loss 0.346005, acc 0.890625, learning_rate 0.000100004
2017-10-09T15:12:55.016566: step 3435, loss 0.349547, acc 0.890625, learning_rate 0.000100004
2017-10-09T15:12:55.126381: step 3436, loss 0.294745, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:55.236745: step 3437, loss 0.287577, acc 0.90625, learning_rate 0.000100004
2017-10-09T15:12:55.342256: step 3438, loss 0.332578, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:55.448789: step 3439, loss 0.456955, acc 0.84375, learning_rate 0.000100004
2017-10-09T15:12:55.554255: step 3440, loss 0.300541, acc 0.875, learning_rate 0.000100004

Evaluation:
2017-10-09T15:12:55.807447: step 3440, loss 0.339505, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3440

2017-10-09T15:12:56.328762: step 3441, loss 0.213406, acc 0.90625, learning_rate 0.000100004
2017-10-09T15:12:56.434713: step 3442, loss 0.300855, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:56.539719: step 3443, loss 0.254802, acc 0.90625, learning_rate 0.000100004
2017-10-09T15:12:56.640654: step 3444, loss 0.424286, acc 0.890625, learning_rate 0.000100004
2017-10-09T15:12:56.746966: step 3445, loss 0.334453, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:56.850995: step 3446, loss 0.232104, acc 0.890625, learning_rate 0.000100004
2017-10-09T15:12:56.955065: step 3447, loss 0.244127, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:57.061227: step 3448, loss 0.320272, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:57.164660: step 3449, loss 0.313406, acc 0.84375, learning_rate 0.000100004
2017-10-09T15:12:57.270220: step 3450, loss 0.266574, acc 0.921875, learning_rate 0.000100004
2017-10-09T15:12:57.377504: step 3451, loss 0.372079, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:57.484885: step 3452, loss 0.374063, acc 0.84375, learning_rate 0.000100004
2017-10-09T15:12:57.592565: step 3453, loss 0.376099, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:57.697526: step 3454, loss 0.417265, acc 0.890625, learning_rate 0.000100004
2017-10-09T15:12:57.806493: step 3455, loss 0.360976, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:57.913975: step 3456, loss 0.516939, acc 0.828125, learning_rate 0.000100004
2017-10-09T15:12:58.022597: step 3457, loss 0.279322, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:58.140422: step 3458, loss 0.196212, acc 0.953125, learning_rate 0.000100004
2017-10-09T15:12:58.246859: step 3459, loss 0.331358, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:58.360939: step 3460, loss 0.358087, acc 0.875, learning_rate 0.000100004
2017-10-09T15:12:58.473611: step 3461, loss 0.333774, acc 0.859375, learning_rate 0.000100004
2017-10-09T15:12:58.580336: step 3462, loss 0.257896, acc 0.921875, learning_rate 0.000100003
2017-10-09T15:12:58.685777: step 3463, loss 0.393779, acc 0.875, learning_rate 0.000100003
2017-10-09T15:12:58.792790: step 3464, loss 0.295851, acc 0.84375, learning_rate 0.000100003
2017-10-09T15:12:58.917141: step 3465, loss 0.329217, acc 0.875, learning_rate 0.000100003
2017-10-09T15:12:59.052487: step 3466, loss 0.341209, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:12:59.170769: step 3467, loss 0.287421, acc 0.9375, learning_rate 0.000100003
2017-10-09T15:12:59.287963: step 3468, loss 0.275427, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:12:59.410613: step 3469, loss 0.380974, acc 0.84375, learning_rate 0.000100003
2017-10-09T15:12:59.522410: step 3470, loss 0.337756, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:12:59.634280: step 3471, loss 0.318361, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:12:59.735478: step 3472, loss 0.381629, acc 0.875, learning_rate 0.000100003
2017-10-09T15:12:59.844994: step 3473, loss 0.300935, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:12:59.954940: step 3474, loss 0.273047, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:00.066504: step 3475, loss 0.374214, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:00.179871: step 3476, loss 0.30546, acc 0.9375, learning_rate 0.000100003
2017-10-09T15:13:00.287331: step 3477, loss 0.176404, acc 0.9375, learning_rate 0.000100003
2017-10-09T15:13:00.393375: step 3478, loss 0.314788, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:00.497087: step 3479, loss 0.323513, acc 0.859375, learning_rate 0.000100003
2017-10-09T15:13:00.600947: step 3480, loss 0.199138, acc 0.9375, learning_rate 0.000100003

Evaluation:
2017-10-09T15:13:00.859106: step 3480, loss 0.3399, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3480

2017-10-09T15:13:01.462343: step 3481, loss 0.158999, acc 0.984375, learning_rate 0.000100003
2017-10-09T15:13:01.566620: step 3482, loss 0.290364, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:13:01.692632: step 3483, loss 0.274987, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:13:01.814518: step 3484, loss 0.243813, acc 0.921875, learning_rate 0.000100003
2017-10-09T15:13:01.926382: step 3485, loss 0.2564, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:13:02.045818: step 3486, loss 0.396253, acc 0.84375, learning_rate 0.000100003
2017-10-09T15:13:02.165493: step 3487, loss 0.235852, acc 0.9375, learning_rate 0.000100003
2017-10-09T15:13:02.283185: step 3488, loss 0.392174, acc 0.84375, learning_rate 0.000100003
2017-10-09T15:13:02.404017: step 3489, loss 0.113617, acc 1, learning_rate 0.000100003
2017-10-09T15:13:02.526027: step 3490, loss 0.313675, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:13:02.649254: step 3491, loss 0.291931, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:02.781885: step 3492, loss 0.337851, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:02.906916: step 3493, loss 0.216236, acc 0.9375, learning_rate 0.000100003
2017-10-09T15:13:03.041158: step 3494, loss 0.318391, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:03.150391: step 3495, loss 0.321529, acc 0.921875, learning_rate 0.000100003
2017-10-09T15:13:03.264405: step 3496, loss 0.301218, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:03.376007: step 3497, loss 0.272323, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:03.484728: step 3498, loss 0.194059, acc 0.96875, learning_rate 0.000100003
2017-10-09T15:13:03.591487: step 3499, loss 0.36137, acc 0.84375, learning_rate 0.000100003
2017-10-09T15:13:03.696916: step 3500, loss 0.30348, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:03.800867: step 3501, loss 0.226832, acc 0.9375, learning_rate 0.000100003
2017-10-09T15:13:03.903303: step 3502, loss 0.21407, acc 0.953125, learning_rate 0.000100003
2017-10-09T15:13:04.008790: step 3503, loss 0.353246, acc 0.84375, learning_rate 0.000100003
2017-10-09T15:13:04.114355: step 3504, loss 0.211851, acc 0.953125, learning_rate 0.000100003
2017-10-09T15:13:04.220655: step 3505, loss 0.236548, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:13:04.327717: step 3506, loss 0.163772, acc 0.953125, learning_rate 0.000100003
2017-10-09T15:13:04.434687: step 3507, loss 0.331326, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:04.545211: step 3508, loss 0.27882, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:13:04.646164: step 3509, loss 0.330094, acc 0.859375, learning_rate 0.000100003
2017-10-09T15:13:04.755783: step 3510, loss 0.278913, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:04.866063: step 3511, loss 0.206446, acc 0.9375, learning_rate 0.000100003
2017-10-09T15:13:04.994393: step 3512, loss 0.326765, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:05.113885: step 3513, loss 0.32554, acc 0.84375, learning_rate 0.000100003
2017-10-09T15:13:05.231779: step 3514, loss 0.326294, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:05.338708: step 3515, loss 0.345728, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:05.449160: step 3516, loss 0.219001, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:05.557574: step 3517, loss 0.335781, acc 0.828125, learning_rate 0.000100003
2017-10-09T15:13:05.662925: step 3518, loss 0.26204, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:05.768792: step 3519, loss 0.227573, acc 0.953125, learning_rate 0.000100003
2017-10-09T15:13:05.877424: step 3520, loss 0.246331, acc 0.890625, learning_rate 0.000100003

Evaluation:
2017-10-09T15:13:06.142141: step 3520, loss 0.339914, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3520

2017-10-09T15:13:06.736823: step 3521, loss 0.313786, acc 0.921875, learning_rate 0.000100003
2017-10-09T15:13:06.846020: step 3522, loss 0.265684, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:06.953495: step 3523, loss 0.326677, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:07.063136: step 3524, loss 0.387396, acc 0.859375, learning_rate 0.000100003
2017-10-09T15:13:07.169685: step 3525, loss 0.414716, acc 0.828125, learning_rate 0.000100003
2017-10-09T15:13:07.272437: step 3526, loss 0.333223, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:07.381371: step 3527, loss 0.185397, acc 0.953125, learning_rate 0.000100003
2017-10-09T15:13:07.468261: step 3528, loss 0.152641, acc 0.960784, learning_rate 0.000100003
2017-10-09T15:13:07.575056: step 3529, loss 0.335619, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:07.674826: step 3530, loss 0.227649, acc 0.953125, learning_rate 0.000100003
2017-10-09T15:13:07.779660: step 3531, loss 0.447625, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:07.887369: step 3532, loss 0.372369, acc 0.890625, learning_rate 0.000100003
2017-10-09T15:13:07.989508: step 3533, loss 0.345313, acc 0.859375, learning_rate 0.000100003
2017-10-09T15:13:08.097649: step 3534, loss 0.391119, acc 0.859375, learning_rate 0.000100003
2017-10-09T15:13:08.204567: step 3535, loss 0.294226, acc 0.859375, learning_rate 0.000100003
2017-10-09T15:13:08.312327: step 3536, loss 0.310939, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:13:08.419364: step 3537, loss 0.447173, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:08.528541: step 3538, loss 0.376929, acc 0.84375, learning_rate 0.000100003
2017-10-09T15:13:08.638152: step 3539, loss 0.247402, acc 0.90625, learning_rate 0.000100003
2017-10-09T15:13:08.741681: step 3540, loss 0.209006, acc 0.921875, learning_rate 0.000100003
2017-10-09T15:13:08.847342: step 3541, loss 0.227715, acc 0.921875, learning_rate 0.000100003
2017-10-09T15:13:08.958112: step 3542, loss 0.345117, acc 0.859375, learning_rate 0.000100003
2017-10-09T15:13:09.061761: step 3543, loss 0.313887, acc 0.875, learning_rate 0.000100003
2017-10-09T15:13:09.164713: step 3544, loss 0.323523, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:09.270641: step 3545, loss 0.268848, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:09.375608: step 3546, loss 0.394066, acc 0.84375, learning_rate 0.000100002
2017-10-09T15:13:09.482184: step 3547, loss 0.284858, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:09.587120: step 3548, loss 0.22243, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:09.694780: step 3549, loss 0.470688, acc 0.828125, learning_rate 0.000100002
2017-10-09T15:13:09.798207: step 3550, loss 0.329484, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:09.907469: step 3551, loss 0.316204, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:10.011673: step 3552, loss 0.219133, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:10.120129: step 3553, loss 0.317525, acc 0.84375, learning_rate 0.000100002
2017-10-09T15:13:10.225033: step 3554, loss 0.36346, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:10.331132: step 3555, loss 0.385297, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:10.438244: step 3556, loss 0.38354, acc 0.828125, learning_rate 0.000100002
2017-10-09T15:13:10.541566: step 3557, loss 0.180307, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:10.647842: step 3558, loss 0.266318, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:10.754825: step 3559, loss 0.31734, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:10.861429: step 3560, loss 0.304306, acc 0.875, learning_rate 0.000100002

Evaluation:
2017-10-09T15:13:11.116746: step 3560, loss 0.336995, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3560

2017-10-09T15:13:11.790342: step 3561, loss 0.442655, acc 0.828125, learning_rate 0.000100002
2017-10-09T15:13:11.893102: step 3562, loss 0.44625, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:11.999054: step 3563, loss 0.208849, acc 0.96875, learning_rate 0.000100002
2017-10-09T15:13:12.103196: step 3564, loss 0.162358, acc 0.953125, learning_rate 0.000100002
2017-10-09T15:13:12.206568: step 3565, loss 0.297204, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:12.313004: step 3566, loss 0.313089, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:12.417384: step 3567, loss 0.409852, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:12.527158: step 3568, loss 0.21784, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:12.630925: step 3569, loss 0.207346, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:12.734909: step 3570, loss 0.316936, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:12.840624: step 3571, loss 0.290395, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:12.945097: step 3572, loss 0.341393, acc 0.84375, learning_rate 0.000100002
2017-10-09T15:13:13.050892: step 3573, loss 0.291177, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:13.157450: step 3574, loss 0.347219, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:13.263505: step 3575, loss 0.459015, acc 0.828125, learning_rate 0.000100002
2017-10-09T15:13:13.368857: step 3576, loss 0.281399, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:13.472802: step 3577, loss 0.257558, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:13.578579: step 3578, loss 0.330513, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:13.685542: step 3579, loss 0.355947, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:13.795921: step 3580, loss 0.211921, acc 0.96875, learning_rate 0.000100002
2017-10-09T15:13:13.901610: step 3581, loss 0.128361, acc 0.96875, learning_rate 0.000100002
2017-10-09T15:13:14.010954: step 3582, loss 0.318124, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:14.117077: step 3583, loss 0.37612, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:14.225876: step 3584, loss 0.258023, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:14.327808: step 3585, loss 0.194359, acc 0.953125, learning_rate 0.000100002
2017-10-09T15:13:14.435092: step 3586, loss 0.359387, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:14.537600: step 3587, loss 0.430838, acc 0.765625, learning_rate 0.000100002
2017-10-09T15:13:14.643630: step 3588, loss 0.317221, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:14.752127: step 3589, loss 0.251215, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:14.872835: step 3590, loss 0.439963, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:14.997629: step 3591, loss 0.298866, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:15.122059: step 3592, loss 0.333627, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:15.228929: step 3593, loss 0.148632, acc 0.984375, learning_rate 0.000100002
2017-10-09T15:13:15.337499: step 3594, loss 0.313701, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:15.442912: step 3595, loss 0.224789, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:15.547381: step 3596, loss 0.295878, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:15.652943: step 3597, loss 0.402928, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:15.756098: step 3598, loss 0.372564, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:15.866251: step 3599, loss 0.272003, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:15.969159: step 3600, loss 0.185975, acc 0.90625, learning_rate 0.000100002

Evaluation:
2017-10-09T15:13:16.229111: step 3600, loss 0.337264, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3600

2017-10-09T15:13:16.766416: step 3601, loss 0.349972, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:16.873218: step 3602, loss 0.379359, acc 0.84375, learning_rate 0.000100002
2017-10-09T15:13:16.976894: step 3603, loss 0.265998, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:17.083494: step 3604, loss 0.271695, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:17.187266: step 3605, loss 0.477807, acc 0.8125, learning_rate 0.000100002
2017-10-09T15:13:17.287381: step 3606, loss 0.277506, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:17.392483: step 3607, loss 0.289641, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:17.494598: step 3608, loss 0.278233, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:17.599512: step 3609, loss 0.425007, acc 0.8125, learning_rate 0.000100002
2017-10-09T15:13:17.707858: step 3610, loss 0.266548, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:17.810552: step 3611, loss 0.275408, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:17.915000: step 3612, loss 0.352423, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:18.018905: step 3613, loss 0.285121, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:18.130743: step 3614, loss 0.239088, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:18.237045: step 3615, loss 0.341469, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:18.342710: step 3616, loss 0.284158, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:18.450308: step 3617, loss 0.351204, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:18.556723: step 3618, loss 0.230891, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:18.658468: step 3619, loss 0.173262, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:18.760804: step 3620, loss 0.277451, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:18.870690: step 3621, loss 0.2035, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:18.991929: step 3622, loss 0.301303, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:19.114231: step 3623, loss 0.232985, acc 0.953125, learning_rate 0.000100002
2017-10-09T15:13:19.223038: step 3624, loss 0.427683, acc 0.828125, learning_rate 0.000100002
2017-10-09T15:13:19.324941: step 3625, loss 0.179907, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:19.413423: step 3626, loss 0.323641, acc 0.921569, learning_rate 0.000100002
2017-10-09T15:13:19.521796: step 3627, loss 0.246457, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:19.630415: step 3628, loss 0.324055, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:19.738759: step 3629, loss 0.293826, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:19.846606: step 3630, loss 0.250883, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:19.956044: step 3631, loss 0.340182, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:20.059727: step 3632, loss 0.166705, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:20.164362: step 3633, loss 0.209568, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:20.274781: step 3634, loss 0.442881, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:20.381174: step 3635, loss 0.215707, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:20.489005: step 3636, loss 0.33692, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:20.594195: step 3637, loss 0.230702, acc 0.953125, learning_rate 0.000100002
2017-10-09T15:13:20.696619: step 3638, loss 0.308224, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:20.803953: step 3639, loss 0.389213, acc 0.828125, learning_rate 0.000100002
2017-10-09T15:13:20.913364: step 3640, loss 0.284705, acc 0.875, learning_rate 0.000100002

Evaluation:
2017-10-09T15:13:21.209991: step 3640, loss 0.337934, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3640

2017-10-09T15:13:21.829425: step 3641, loss 0.47306, acc 0.859375, learning_rate 0.000100002
2017-10-09T15:13:21.937662: step 3642, loss 0.296462, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:22.043760: step 3643, loss 0.369455, acc 0.84375, learning_rate 0.000100002
2017-10-09T15:13:22.148580: step 3644, loss 0.158092, acc 0.96875, learning_rate 0.000100002
2017-10-09T15:13:22.250839: step 3645, loss 0.241571, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:22.353379: step 3646, loss 0.319171, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:22.457947: step 3647, loss 0.2365, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:22.565913: step 3648, loss 0.25169, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:22.671070: step 3649, loss 0.258613, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:22.777365: step 3650, loss 0.358617, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:22.885086: step 3651, loss 0.378199, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:22.990986: step 3652, loss 0.424216, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:23.095290: step 3653, loss 0.347431, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:23.214496: step 3654, loss 0.275477, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:23.337885: step 3655, loss 0.258532, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:23.459868: step 3656, loss 0.239391, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:23.565496: step 3657, loss 0.208881, acc 0.953125, learning_rate 0.000100002
2017-10-09T15:13:23.673789: step 3658, loss 0.326859, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:23.780057: step 3659, loss 0.420445, acc 0.8125, learning_rate 0.000100002
2017-10-09T15:13:23.887676: step 3660, loss 0.256393, acc 0.921875, learning_rate 0.000100002
2017-10-09T15:13:23.994584: step 3661, loss 0.2723, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:24.105129: step 3662, loss 0.166718, acc 0.9375, learning_rate 0.000100002
2017-10-09T15:13:24.213531: step 3663, loss 0.349064, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:24.318864: step 3664, loss 0.203597, acc 0.953125, learning_rate 0.000100002
2017-10-09T15:13:24.426854: step 3665, loss 0.341913, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:24.535157: step 3666, loss 0.329109, acc 0.875, learning_rate 0.000100002
2017-10-09T15:13:24.642923: step 3667, loss 0.241483, acc 0.90625, learning_rate 0.000100002
2017-10-09T15:13:24.747940: step 3668, loss 0.242719, acc 0.890625, learning_rate 0.000100002
2017-10-09T15:13:24.857787: step 3669, loss 0.26764, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:24.974623: step 3670, loss 0.325783, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:25.080547: step 3671, loss 0.392262, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:25.186828: step 3672, loss 0.349716, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:25.299473: step 3673, loss 0.390892, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:25.410229: step 3674, loss 0.196648, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:25.522026: step 3675, loss 0.375708, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:25.633585: step 3676, loss 0.298973, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:25.741905: step 3677, loss 0.214028, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:25.849851: step 3678, loss 0.341309, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:25.956359: step 3679, loss 0.368429, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:26.064150: step 3680, loss 0.269595, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-09T15:13:26.313035: step 3680, loss 0.336837, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3680

2017-10-09T15:13:26.907366: step 3681, loss 0.292904, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:27.028149: step 3682, loss 0.224812, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:27.157477: step 3683, loss 0.204943, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:27.277675: step 3684, loss 0.295816, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:27.394046: step 3685, loss 0.172475, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:27.510461: step 3686, loss 0.231018, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:27.617185: step 3687, loss 0.426826, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:27.722306: step 3688, loss 0.342529, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:27.831240: step 3689, loss 0.250228, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:27.935077: step 3690, loss 0.360185, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:28.039759: step 3691, loss 0.301665, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:28.143775: step 3692, loss 0.225173, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:28.252617: step 3693, loss 0.495023, acc 0.8125, learning_rate 0.000100001
2017-10-09T15:13:28.361552: step 3694, loss 0.251148, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:28.468147: step 3695, loss 0.393476, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:28.575758: step 3696, loss 0.279563, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:28.682441: step 3697, loss 0.171558, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:28.786013: step 3698, loss 0.258817, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:28.908982: step 3699, loss 0.19056, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:29.031586: step 3700, loss 0.188024, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:29.135903: step 3701, loss 0.313389, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:29.241135: step 3702, loss 0.443684, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:29.361881: step 3703, loss 0.356407, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:29.487679: step 3704, loss 0.276269, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:29.608568: step 3705, loss 0.279567, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:29.730322: step 3706, loss 0.29797, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:29.834854: step 3707, loss 0.153693, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:29.937159: step 3708, loss 0.383664, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:30.044301: step 3709, loss 0.295661, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:30.150757: step 3710, loss 0.327242, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:30.253315: step 3711, loss 0.407017, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:30.367046: step 3712, loss 0.335491, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:30.474191: step 3713, loss 0.263553, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:30.580092: step 3714, loss 0.340321, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:30.685572: step 3715, loss 0.236136, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:30.792486: step 3716, loss 0.157194, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:30.916772: step 3717, loss 0.252947, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:31.036960: step 3718, loss 0.34386, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:31.150906: step 3719, loss 0.276631, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:31.255335: step 3720, loss 0.353832, acc 0.890625, learning_rate 0.000100001

Evaluation:
2017-10-09T15:13:31.543563: step 3720, loss 0.336388, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3720

2017-10-09T15:13:32.221225: step 3721, loss 0.206973, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:32.325973: step 3722, loss 0.208006, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:32.448801: step 3723, loss 0.355972, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:32.549971: step 3724, loss 0.292208, acc 0.941176, learning_rate 0.000100001
2017-10-09T15:13:32.653099: step 3725, loss 0.28968, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:32.760027: step 3726, loss 0.236377, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:32.872226: step 3727, loss 0.399371, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:32.980802: step 3728, loss 0.270339, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:33.083703: step 3729, loss 0.2603, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:33.191142: step 3730, loss 0.191591, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:33.296828: step 3731, loss 0.147311, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:33.404089: step 3732, loss 0.35745, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:33.509752: step 3733, loss 0.212334, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:33.614386: step 3734, loss 0.222736, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:33.715784: step 3735, loss 0.150769, acc 0.984375, learning_rate 0.000100001
2017-10-09T15:13:33.825347: step 3736, loss 0.238991, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:33.930020: step 3737, loss 0.423248, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:34.034335: step 3738, loss 0.253731, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:34.138945: step 3739, loss 0.375435, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:34.243056: step 3740, loss 0.201062, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:34.347405: step 3741, loss 0.422389, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:34.455587: step 3742, loss 0.272383, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:34.561682: step 3743, loss 0.25527, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:34.669265: step 3744, loss 0.301355, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:34.790660: step 3745, loss 0.209011, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:34.913381: step 3746, loss 0.236374, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:35.023356: step 3747, loss 0.21021, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:35.130473: step 3748, loss 0.239298, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:35.235856: step 3749, loss 0.388885, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:35.355052: step 3750, loss 0.234739, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:35.480438: step 3751, loss 0.517031, acc 0.796875, learning_rate 0.000100001
2017-10-09T15:13:35.589115: step 3752, loss 0.367752, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:35.693571: step 3753, loss 0.361251, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:35.798420: step 3754, loss 0.178929, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:35.905384: step 3755, loss 0.279793, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:36.012333: step 3756, loss 0.285161, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:36.120541: step 3757, loss 0.196045, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:36.223080: step 3758, loss 0.322228, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:36.331643: step 3759, loss 0.391759, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:36.438399: step 3760, loss 0.340134, acc 0.890625, learning_rate 0.000100001

Evaluation:
2017-10-09T15:13:36.699026: step 3760, loss 0.335902, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3760

2017-10-09T15:13:37.220182: step 3761, loss 0.338422, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:37.332109: step 3762, loss 0.421601, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:37.439308: step 3763, loss 0.253726, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:37.569876: step 3764, loss 0.336729, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:37.689701: step 3765, loss 0.290827, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:37.803513: step 3766, loss 0.435293, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:37.921113: step 3767, loss 0.406566, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:38.041270: step 3768, loss 0.254223, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:38.147553: step 3769, loss 0.293072, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:38.254178: step 3770, loss 0.276245, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:38.359467: step 3771, loss 0.37625, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:38.465208: step 3772, loss 0.333715, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:38.572255: step 3773, loss 0.185325, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:38.679636: step 3774, loss 0.270695, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:38.780603: step 3775, loss 0.16199, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:38.886810: step 3776, loss 0.265387, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:38.989452: step 3777, loss 0.369606, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:39.118591: step 3778, loss 0.284482, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:39.240252: step 3779, loss 0.557219, acc 0.78125, learning_rate 0.000100001
2017-10-09T15:13:39.354376: step 3780, loss 0.4176, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:39.471124: step 3781, loss 0.305943, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:39.596582: step 3782, loss 0.316301, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:39.701704: step 3783, loss 0.37578, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:39.808872: step 3784, loss 0.259768, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:39.930450: step 3785, loss 0.283869, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:40.050861: step 3786, loss 0.29466, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:40.172627: step 3787, loss 0.320597, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:40.277759: step 3788, loss 0.263496, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:40.383867: step 3789, loss 0.312809, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:40.491181: step 3790, loss 0.218247, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:40.598160: step 3791, loss 0.35162, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:40.705277: step 3792, loss 0.310842, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:40.808584: step 3793, loss 0.362624, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:40.913875: step 3794, loss 0.337463, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:41.017639: step 3795, loss 0.243314, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:41.122841: step 3796, loss 0.51707, acc 0.78125, learning_rate 0.000100001
2017-10-09T15:13:41.233264: step 3797, loss 0.373675, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:41.342029: step 3798, loss 0.374696, acc 0.8125, learning_rate 0.000100001
2017-10-09T15:13:41.448742: step 3799, loss 0.312941, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:41.555867: step 3800, loss 0.427546, acc 0.78125, learning_rate 0.000100001

Evaluation:
2017-10-09T15:13:41.812109: step 3800, loss 0.333747, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3800

2017-10-09T15:13:42.407934: step 3801, loss 0.469033, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:42.513032: step 3802, loss 0.395536, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:42.617821: step 3803, loss 0.17309, acc 0.984375, learning_rate 0.000100001
2017-10-09T15:13:42.721533: step 3804, loss 0.481945, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:42.829148: step 3805, loss 0.325404, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:42.937557: step 3806, loss 0.293351, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:43.044752: step 3807, loss 0.239112, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:43.152803: step 3808, loss 0.309881, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:43.262797: step 3809, loss 0.316755, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:43.369594: step 3810, loss 0.127449, acc 0.984375, learning_rate 0.000100001
2017-10-09T15:13:43.477620: step 3811, loss 0.24643, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:43.582753: step 3812, loss 0.378632, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:43.691243: step 3813, loss 0.348522, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:43.798839: step 3814, loss 0.229254, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:43.905772: step 3815, loss 0.20139, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:44.013491: step 3816, loss 0.361582, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:44.120793: step 3817, loss 0.297232, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:44.225835: step 3818, loss 0.187939, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:44.332606: step 3819, loss 0.218687, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:44.441742: step 3820, loss 0.246729, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:44.550080: step 3821, loss 0.324859, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:44.641189: step 3822, loss 0.307996, acc 0.921569, learning_rate 0.000100001
2017-10-09T15:13:44.746528: step 3823, loss 0.25341, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:44.852579: step 3824, loss 0.434828, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:44.964837: step 3825, loss 0.312066, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:45.072680: step 3826, loss 0.219125, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:45.178540: step 3827, loss 0.35953, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:45.284375: step 3828, loss 0.295032, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:45.389134: step 3829, loss 0.290818, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:45.493717: step 3830, loss 0.290193, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:45.601518: step 3831, loss 0.415089, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:45.700749: step 3832, loss 0.279376, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:45.807625: step 3833, loss 0.37714, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:45.914507: step 3834, loss 0.334555, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:46.016076: step 3835, loss 0.329833, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:46.124518: step 3836, loss 0.207291, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:46.229674: step 3837, loss 0.273721, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:46.337030: step 3838, loss 0.316502, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:46.443951: step 3839, loss 0.250208, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:46.553286: step 3840, loss 0.234952, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-09T15:13:46.813822: step 3840, loss 0.333134, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3840

2017-10-09T15:13:47.423255: step 3841, loss 0.334311, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:47.530433: step 3842, loss 0.332055, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:47.639793: step 3843, loss 0.335413, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:47.748288: step 3844, loss 0.255664, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:47.859626: step 3845, loss 0.260308, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:47.965455: step 3846, loss 0.254443, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:48.068792: step 3847, loss 0.291133, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:48.173296: step 3848, loss 0.294051, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:48.277971: step 3849, loss 0.24849, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:48.382009: step 3850, loss 0.311392, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:48.485794: step 3851, loss 0.272044, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:48.590433: step 3852, loss 0.263825, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:48.693642: step 3853, loss 0.445686, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:48.799625: step 3854, loss 0.238783, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:48.899286: step 3855, loss 0.246776, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:49.003170: step 3856, loss 0.249026, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:49.111604: step 3857, loss 0.314483, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:49.216797: step 3858, loss 0.302139, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:49.325738: step 3859, loss 0.333597, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:49.428406: step 3860, loss 0.263883, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:49.530494: step 3861, loss 0.161125, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:49.635897: step 3862, loss 0.27247, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:49.742679: step 3863, loss 0.379865, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:49.847485: step 3864, loss 0.541583, acc 0.78125, learning_rate 0.000100001
2017-10-09T15:13:49.952311: step 3865, loss 0.230058, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:50.060455: step 3866, loss 0.168274, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:50.168246: step 3867, loss 0.215307, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:50.275535: step 3868, loss 0.323905, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:50.377095: step 3869, loss 0.192109, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:50.482889: step 3870, loss 0.205699, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:50.585578: step 3871, loss 0.303518, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:50.695558: step 3872, loss 0.242841, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:50.800806: step 3873, loss 0.229685, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:50.909113: step 3874, loss 0.289095, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:51.013819: step 3875, loss 0.31289, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:51.119585: step 3876, loss 0.261286, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:51.224964: step 3877, loss 0.28703, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:51.330155: step 3878, loss 0.251593, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:51.436716: step 3879, loss 0.45831, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:51.544087: step 3880, loss 0.29751, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-09T15:13:51.792813: step 3880, loss 0.335251, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3880

2017-10-09T15:13:52.450579: step 3881, loss 0.18571, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:52.552268: step 3882, loss 0.40169, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:52.659636: step 3883, loss 0.202709, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:52.768771: step 3884, loss 0.20252, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:52.872181: step 3885, loss 0.25116, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:52.974803: step 3886, loss 0.406775, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:53.081893: step 3887, loss 0.155586, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:53.186945: step 3888, loss 0.324823, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:53.294382: step 3889, loss 0.214995, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:53.399347: step 3890, loss 0.12263, acc 0.984375, learning_rate 0.000100001
2017-10-09T15:13:53.505225: step 3891, loss 0.167501, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:53.610045: step 3892, loss 0.248132, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:53.718572: step 3893, loss 0.301058, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:53.825164: step 3894, loss 0.412984, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:53.930831: step 3895, loss 0.253731, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:54.030458: step 3896, loss 0.315669, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:54.137663: step 3897, loss 0.455249, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:54.242860: step 3898, loss 0.235111, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:54.346352: step 3899, loss 0.277063, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:54.453655: step 3900, loss 0.281968, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:54.560600: step 3901, loss 0.445431, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:54.667778: step 3902, loss 0.347143, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:54.770695: step 3903, loss 0.265277, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:54.876457: step 3904, loss 0.272723, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:54.978379: step 3905, loss 0.376134, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:55.083815: step 3906, loss 0.373591, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:55.190274: step 3907, loss 0.313471, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:55.296541: step 3908, loss 0.241669, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:55.402810: step 3909, loss 0.300022, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:55.507970: step 3910, loss 0.329863, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:55.617081: step 3911, loss 0.442204, acc 0.8125, learning_rate 0.000100001
2017-10-09T15:13:55.721074: step 3912, loss 0.444765, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:55.822694: step 3913, loss 0.297397, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:55.928892: step 3914, loss 0.252788, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:56.035036: step 3915, loss 0.28396, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:56.142506: step 3916, loss 0.279936, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:56.247059: step 3917, loss 0.209241, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:56.356503: step 3918, loss 0.348978, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:56.465894: step 3919, loss 0.355472, acc 0.890625, learning_rate 0.000100001
2017-10-09T15:13:56.552636: step 3920, loss 0.376384, acc 0.843137, learning_rate 0.000100001

Evaluation:
2017-10-09T15:13:56.804077: step 3920, loss 0.331161, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3920

2017-10-09T15:13:57.320617: step 3921, loss 0.346663, acc 0.859375, learning_rate 0.000100001
2017-10-09T15:13:57.429626: step 3922, loss 0.310382, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:57.536236: step 3923, loss 0.270139, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:57.642451: step 3924, loss 0.394175, acc 0.828125, learning_rate 0.000100001
2017-10-09T15:13:57.746793: step 3925, loss 0.295287, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:57.859143: step 3926, loss 0.19479, acc 0.9375, learning_rate 0.000100001
2017-10-09T15:13:57.967707: step 3927, loss 0.188567, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:58.078431: step 3928, loss 0.259147, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:58.182239: step 3929, loss 0.205487, acc 0.921875, learning_rate 0.000100001
2017-10-09T15:13:58.287818: step 3930, loss 0.281749, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:58.393620: step 3931, loss 0.48593, acc 0.796875, learning_rate 0.000100001
2017-10-09T15:13:58.496580: step 3932, loss 0.187415, acc 0.96875, learning_rate 0.000100001
2017-10-09T15:13:58.603791: step 3933, loss 0.261463, acc 0.953125, learning_rate 0.000100001
2017-10-09T15:13:58.708598: step 3934, loss 0.344785, acc 0.84375, learning_rate 0.000100001
2017-10-09T15:13:58.815481: step 3935, loss 0.371198, acc 0.875, learning_rate 0.000100001
2017-10-09T15:13:58.926439: step 3936, loss 0.266672, acc 0.90625, learning_rate 0.000100001
2017-10-09T15:13:59.031344: step 3937, loss 0.214054, acc 0.9375, learning_rate 0.0001
2017-10-09T15:13:59.136050: step 3938, loss 0.246435, acc 0.921875, learning_rate 0.0001
2017-10-09T15:13:59.241185: step 3939, loss 0.259553, acc 0.890625, learning_rate 0.0001
2017-10-09T15:13:59.348684: step 3940, loss 0.26123, acc 0.921875, learning_rate 0.0001
2017-10-09T15:13:59.453966: step 3941, loss 0.253468, acc 0.921875, learning_rate 0.0001
2017-10-09T15:13:59.555867: step 3942, loss 0.284861, acc 0.90625, learning_rate 0.0001
2017-10-09T15:13:59.657615: step 3943, loss 0.44768, acc 0.875, learning_rate 0.0001
2017-10-09T15:13:59.763961: step 3944, loss 0.190413, acc 0.9375, learning_rate 0.0001
2017-10-09T15:13:59.869573: step 3945, loss 0.208425, acc 0.9375, learning_rate 0.0001
2017-10-09T15:13:59.974175: step 3946, loss 0.336034, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:00.079904: step 3947, loss 0.385171, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:00.185867: step 3948, loss 0.326, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:00.293562: step 3949, loss 0.243985, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:00.398735: step 3950, loss 0.280638, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:00.503913: step 3951, loss 0.397117, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:00.609962: step 3952, loss 0.261801, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:00.717062: step 3953, loss 0.264361, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:00.821341: step 3954, loss 0.425709, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:00.926706: step 3955, loss 0.479989, acc 0.8125, learning_rate 0.0001
2017-10-09T15:14:01.030237: step 3956, loss 0.320548, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:01.139902: step 3957, loss 0.234798, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:01.244539: step 3958, loss 0.258123, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:01.350203: step 3959, loss 0.313219, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:01.457112: step 3960, loss 0.280593, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:01.706719: step 3960, loss 0.330747, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-3960

2017-10-09T15:14:02.291351: step 3961, loss 0.221965, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:02.398411: step 3962, loss 0.328691, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:02.503872: step 3963, loss 0.324271, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:02.607890: step 3964, loss 0.262724, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:02.715520: step 3965, loss 0.313907, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:02.824241: step 3966, loss 0.251708, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:02.927008: step 3967, loss 0.181707, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:03.034580: step 3968, loss 0.289186, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:03.140106: step 3969, loss 0.237077, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:03.247282: step 3970, loss 0.215192, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:03.352628: step 3971, loss 0.179746, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:03.461501: step 3972, loss 0.394807, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:03.568693: step 3973, loss 0.268019, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:03.676490: step 3974, loss 0.292417, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:03.780333: step 3975, loss 0.331093, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:03.886191: step 3976, loss 0.303899, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:03.993634: step 3977, loss 0.257089, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:04.097101: step 3978, loss 0.427884, acc 0.796875, learning_rate 0.0001
2017-10-09T15:14:04.202407: step 3979, loss 0.303638, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:04.306765: step 3980, loss 0.255192, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:04.410221: step 3981, loss 0.266329, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:04.515964: step 3982, loss 0.436108, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:04.621408: step 3983, loss 0.373634, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:04.725871: step 3984, loss 0.219669, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:04.829205: step 3985, loss 0.227591, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:04.936314: step 3986, loss 0.228971, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:05.042263: step 3987, loss 0.405571, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:05.146111: step 3988, loss 0.203273, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:05.254096: step 3989, loss 0.157837, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:05.361435: step 3990, loss 0.347965, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:05.467561: step 3991, loss 0.409463, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:05.575448: step 3992, loss 0.347652, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:05.681349: step 3993, loss 0.242911, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:05.788961: step 3994, loss 0.188537, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:05.895503: step 3995, loss 0.317361, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:06.005846: step 3996, loss 0.235177, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:06.111643: step 3997, loss 0.26871, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:06.216256: step 3998, loss 0.343454, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:06.321967: step 3999, loss 0.221852, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:06.428543: step 4000, loss 0.181952, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:06.694559: step 4000, loss 0.331454, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4000

2017-10-09T15:14:07.315335: step 4001, loss 0.26482, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:07.419864: step 4002, loss 0.226017, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:07.526389: step 4003, loss 0.184973, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:07.634189: step 4004, loss 0.19236, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:07.744580: step 4005, loss 0.236833, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:07.849466: step 4006, loss 0.318104, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:07.964955: step 4007, loss 0.223338, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:08.087128: step 4008, loss 0.316739, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:08.213747: step 4009, loss 0.244333, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:08.331904: step 4010, loss 0.282901, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:08.452847: step 4011, loss 0.469481, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:08.570884: step 4012, loss 0.350035, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:08.679100: step 4013, loss 0.197458, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:08.784698: step 4014, loss 0.152435, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:08.892164: step 4015, loss 0.199968, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:08.996294: step 4016, loss 0.390028, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:09.102687: step 4017, loss 0.354044, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:09.190388: step 4018, loss 0.278866, acc 0.843137, learning_rate 0.0001
2017-10-09T15:14:09.296871: step 4019, loss 0.356744, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:09.403916: step 4020, loss 0.303725, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:09.514402: step 4021, loss 0.218741, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:09.622664: step 4022, loss 0.228568, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:09.728811: step 4023, loss 0.372347, acc 0.796875, learning_rate 0.0001
2017-10-09T15:14:09.833809: step 4024, loss 0.182982, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:09.937166: step 4025, loss 0.174484, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:10.042281: step 4026, loss 0.383795, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:10.149819: step 4027, loss 0.161216, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:10.253855: step 4028, loss 0.190691, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:10.359574: step 4029, loss 0.321499, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:10.469885: step 4030, loss 0.327186, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:10.576852: step 4031, loss 0.293086, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:10.684077: step 4032, loss 0.158552, acc 0.984375, learning_rate 0.0001
2017-10-09T15:14:10.792378: step 4033, loss 0.263977, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:10.901941: step 4034, loss 0.28549, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:11.007011: step 4035, loss 0.391125, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:11.113168: step 4036, loss 0.220398, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:11.220143: step 4037, loss 0.195701, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:11.328114: step 4038, loss 0.36709, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:11.434980: step 4039, loss 0.267682, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:11.538836: step 4040, loss 0.29642, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:11.789872: step 4040, loss 0.332614, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4040

2017-10-09T15:14:12.467480: step 4041, loss 0.239246, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:12.575785: step 4042, loss 0.466318, acc 0.8125, learning_rate 0.0001
2017-10-09T15:14:12.680283: step 4043, loss 0.25775, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:12.791286: step 4044, loss 0.318025, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:12.905098: step 4045, loss 0.376398, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:13.016536: step 4046, loss 0.232473, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:13.131889: step 4047, loss 0.26759, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:13.236685: step 4048, loss 0.187635, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:13.342918: step 4049, loss 0.261732, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:13.456346: step 4050, loss 0.248529, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:13.562479: step 4051, loss 0.383372, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:13.664268: step 4052, loss 0.318278, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:13.771320: step 4053, loss 0.268892, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:13.882727: step 4054, loss 0.24211, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:13.989958: step 4055, loss 0.255722, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:14.095716: step 4056, loss 0.39637, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:14.204399: step 4057, loss 0.210657, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:14.310761: step 4058, loss 0.400431, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:14.414721: step 4059, loss 0.224902, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:14.515681: step 4060, loss 0.200319, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:14.632839: step 4061, loss 0.298787, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:14.747573: step 4062, loss 0.277372, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:14.852893: step 4063, loss 0.30365, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:14.964414: step 4064, loss 0.266092, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:15.076368: step 4065, loss 0.24859, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:15.195286: step 4066, loss 0.379208, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:15.315975: step 4067, loss 0.267028, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:15.439919: step 4068, loss 0.235067, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:15.561936: step 4069, loss 0.141746, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:15.663732: step 4070, loss 0.434404, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:15.769278: step 4071, loss 0.425987, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:15.883663: step 4072, loss 0.294594, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:16.010182: step 4073, loss 0.491987, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:16.130116: step 4074, loss 0.295651, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:16.248336: step 4075, loss 0.249087, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:16.354120: step 4076, loss 0.150748, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:16.458422: step 4077, loss 0.368928, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:16.565432: step 4078, loss 0.324853, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:16.671787: step 4079, loss 0.338986, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:16.790662: step 4080, loss 0.309819, acc 0.84375, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:17.067751: step 4080, loss 0.331063, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4080

2017-10-09T15:14:17.593957: step 4081, loss 0.151405, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:17.706433: step 4082, loss 0.29266, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:17.817784: step 4083, loss 0.299786, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:17.926160: step 4084, loss 0.268888, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:18.036999: step 4085, loss 0.26194, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:18.147582: step 4086, loss 0.236164, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:18.258086: step 4087, loss 0.260371, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:18.366031: step 4088, loss 0.277378, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:18.469325: step 4089, loss 0.220232, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:18.577353: step 4090, loss 0.399522, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:18.686048: step 4091, loss 0.359203, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:18.792769: step 4092, loss 0.269143, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:18.904558: step 4093, loss 0.296449, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:19.012381: step 4094, loss 0.259281, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:19.117774: step 4095, loss 0.266475, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:19.222913: step 4096, loss 0.269952, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:19.329228: step 4097, loss 0.329426, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:19.437796: step 4098, loss 0.340258, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:19.546891: step 4099, loss 0.365456, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:19.650621: step 4100, loss 0.371205, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:19.754073: step 4101, loss 0.432619, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:19.861760: step 4102, loss 0.303159, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:19.969611: step 4103, loss 0.12489, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:20.075416: step 4104, loss 0.167363, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:20.188197: step 4105, loss 0.273323, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:20.316467: step 4106, loss 0.363113, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:20.440232: step 4107, loss 0.290374, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:20.543986: step 4108, loss 0.273698, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:20.647449: step 4109, loss 0.508033, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:20.753269: step 4110, loss 0.203305, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:20.867728: step 4111, loss 0.275078, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:20.976448: step 4112, loss 0.187667, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:21.081835: step 4113, loss 0.263053, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:21.188562: step 4114, loss 0.321624, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:21.293222: step 4115, loss 0.311041, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:21.382841: step 4116, loss 0.164036, acc 0.941176, learning_rate 0.0001
2017-10-09T15:14:21.488602: step 4117, loss 0.329375, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:21.596524: step 4118, loss 0.318302, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:21.703649: step 4119, loss 0.293645, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:21.808124: step 4120, loss 0.254634, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:22.058886: step 4120, loss 0.326823, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4120

2017-10-09T15:14:22.658817: step 4121, loss 0.336875, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:22.781944: step 4122, loss 0.258652, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:22.908337: step 4123, loss 0.213241, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:23.026068: step 4124, loss 0.286964, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:23.132824: step 4125, loss 0.219427, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:23.239535: step 4126, loss 0.346432, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:23.345539: step 4127, loss 0.253155, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:23.454526: step 4128, loss 0.274535, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:23.562137: step 4129, loss 0.19801, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:23.668785: step 4130, loss 0.256496, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:23.779043: step 4131, loss 0.21337, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:23.887530: step 4132, loss 0.352439, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:23.999318: step 4133, loss 0.249931, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:24.104196: step 4134, loss 0.296617, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:24.210237: step 4135, loss 0.341173, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:24.318514: step 4136, loss 0.290274, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:24.427859: step 4137, loss 0.29695, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:24.547093: step 4138, loss 0.21362, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:24.663704: step 4139, loss 0.60037, acc 0.8125, learning_rate 0.0001
2017-10-09T15:14:24.790116: step 4140, loss 0.301717, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:24.915280: step 4141, loss 0.394436, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:25.035323: step 4142, loss 0.286867, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:25.142104: step 4143, loss 0.176886, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:25.243096: step 4144, loss 0.226462, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:25.347467: step 4145, loss 0.320739, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:25.453784: step 4146, loss 0.219318, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:25.557716: step 4147, loss 0.270834, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:25.666703: step 4148, loss 0.287174, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:25.770912: step 4149, loss 0.175701, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:25.873918: step 4150, loss 0.258193, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:25.981295: step 4151, loss 0.16852, acc 0.984375, learning_rate 0.0001
2017-10-09T15:14:26.087272: step 4152, loss 0.286069, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:26.193085: step 4153, loss 0.433429, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:26.301422: step 4154, loss 0.679284, acc 0.78125, learning_rate 0.0001
2017-10-09T15:14:26.404613: step 4155, loss 0.270748, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:26.511175: step 4156, loss 0.209804, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:26.616991: step 4157, loss 0.257206, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:26.722621: step 4158, loss 0.235126, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:26.831676: step 4159, loss 0.235423, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:26.939104: step 4160, loss 0.281589, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:27.199704: step 4160, loss 0.325639, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4160

2017-10-09T15:14:27.827068: step 4161, loss 0.233708, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:27.949445: step 4162, loss 0.248868, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:28.051216: step 4163, loss 0.241043, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:28.174563: step 4164, loss 0.34515, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:28.295160: step 4165, loss 0.349582, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:28.412413: step 4166, loss 0.283653, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:28.534456: step 4167, loss 0.366612, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:28.636967: step 4168, loss 0.337612, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:28.755885: step 4169, loss 0.353511, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:28.880417: step 4170, loss 0.347367, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:28.986504: step 4171, loss 0.35208, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:29.093408: step 4172, loss 0.252573, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:29.198939: step 4173, loss 0.403089, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:29.306085: step 4174, loss 0.255169, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:29.427702: step 4175, loss 0.266016, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:29.542349: step 4176, loss 0.220149, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:29.645917: step 4177, loss 0.350398, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:29.750637: step 4178, loss 0.284408, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:29.860934: step 4179, loss 0.242688, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:29.963630: step 4180, loss 0.242635, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:30.068293: step 4181, loss 0.235, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:30.182868: step 4182, loss 0.167939, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:30.299425: step 4183, loss 0.305975, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:30.423403: step 4184, loss 0.188868, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:30.532482: step 4185, loss 0.328561, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:30.642752: step 4186, loss 0.381447, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:30.748823: step 4187, loss 0.318134, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:30.857155: step 4188, loss 0.304208, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:30.961122: step 4189, loss 0.331088, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:31.070465: step 4190, loss 0.431767, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:31.175008: step 4191, loss 0.391251, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:31.281164: step 4192, loss 0.185247, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:31.379682: step 4193, loss 0.422482, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:31.487219: step 4194, loss 0.201446, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:31.591124: step 4195, loss 0.347917, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:31.698591: step 4196, loss 0.183044, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:31.802265: step 4197, loss 0.265008, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:31.905445: step 4198, loss 0.298293, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:32.010923: step 4199, loss 0.124057, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:32.115121: step 4200, loss 0.352232, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:32.386856: step 4200, loss 0.330761, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4200

2017-10-09T15:14:33.051076: step 4201, loss 0.379796, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:33.155895: step 4202, loss 0.248929, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:33.259727: step 4203, loss 0.24341, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:33.360395: step 4204, loss 0.22561, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:33.463368: step 4205, loss 0.201173, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:33.567461: step 4206, loss 0.262675, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:33.669110: step 4207, loss 0.209397, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:33.775073: step 4208, loss 0.272979, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:33.883808: step 4209, loss 0.246561, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:33.988931: step 4210, loss 0.390141, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:34.090731: step 4211, loss 0.447655, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:34.197935: step 4212, loss 0.187233, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:34.305847: step 4213, loss 0.277715, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:34.399677: step 4214, loss 0.468341, acc 0.843137, learning_rate 0.0001
2017-10-09T15:14:34.507927: step 4215, loss 0.266473, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:34.615883: step 4216, loss 0.358536, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:34.724294: step 4217, loss 0.301682, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:34.831497: step 4218, loss 0.187128, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:34.941730: step 4219, loss 0.182381, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:35.050202: step 4220, loss 0.189595, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:35.158273: step 4221, loss 0.238108, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:35.267151: step 4222, loss 0.186555, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:35.370012: step 4223, loss 0.251927, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:35.478671: step 4224, loss 0.338518, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:35.579936: step 4225, loss 0.247187, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:35.686870: step 4226, loss 0.332877, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:35.793440: step 4227, loss 0.352941, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:35.900586: step 4228, loss 0.217924, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:36.010995: step 4229, loss 0.244413, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:36.121213: step 4230, loss 0.364335, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:36.229229: step 4231, loss 0.44198, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:36.332626: step 4232, loss 0.189444, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:36.439043: step 4233, loss 0.239467, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:36.543514: step 4234, loss 0.210727, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:36.651029: step 4235, loss 0.146048, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:36.756130: step 4236, loss 0.222292, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:36.867845: step 4237, loss 0.172531, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:36.982011: step 4238, loss 0.226617, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:37.092251: step 4239, loss 0.278386, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:37.197226: step 4240, loss 0.27134, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:37.494171: step 4240, loss 0.32439, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4240

2017-10-09T15:14:38.056783: step 4241, loss 0.304728, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:38.166471: step 4242, loss 0.290865, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:38.269571: step 4243, loss 0.379024, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:38.379658: step 4244, loss 0.285608, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:38.484140: step 4245, loss 0.244155, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:38.588373: step 4246, loss 0.355573, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:38.695599: step 4247, loss 0.278988, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:38.820487: step 4248, loss 0.198404, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:38.941004: step 4249, loss 0.280662, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:39.067322: step 4250, loss 0.190204, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:39.184758: step 4251, loss 0.271189, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:39.289546: step 4252, loss 0.225084, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:39.395249: step 4253, loss 0.25785, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:39.503007: step 4254, loss 0.238084, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:39.627524: step 4255, loss 0.349881, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:39.752225: step 4256, loss 0.220904, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:39.862926: step 4257, loss 0.345051, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:39.966237: step 4258, loss 0.321788, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:40.073941: step 4259, loss 0.268727, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:40.178930: step 4260, loss 0.176005, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:40.299654: step 4261, loss 0.158323, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:40.420169: step 4262, loss 0.246125, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:40.538457: step 4263, loss 0.30256, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:40.645172: step 4264, loss 0.223185, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:40.765113: step 4265, loss 0.21436, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:40.889504: step 4266, loss 0.172456, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:41.000755: step 4267, loss 0.3306, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:41.126944: step 4268, loss 0.323566, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:41.248865: step 4269, loss 0.172679, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:41.359911: step 4270, loss 0.314296, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:41.480268: step 4271, loss 0.35947, acc 0.8125, learning_rate 0.0001
2017-10-09T15:14:41.597894: step 4272, loss 0.244043, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:41.700512: step 4273, loss 0.246978, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:41.804391: step 4274, loss 0.306494, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:41.911108: step 4275, loss 0.328192, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:42.017135: step 4276, loss 0.307071, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:42.125262: step 4277, loss 0.335114, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:42.232276: step 4278, loss 0.258869, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:42.337791: step 4279, loss 0.238955, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:42.445573: step 4280, loss 0.294717, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:42.717959: step 4280, loss 0.327518, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4280

2017-10-09T15:14:43.343627: step 4281, loss 0.373902, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:43.448722: step 4282, loss 0.29664, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:43.556794: step 4283, loss 0.306635, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:43.664424: step 4284, loss 0.146615, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:43.773121: step 4285, loss 0.272499, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:43.876045: step 4286, loss 0.361721, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:43.981627: step 4287, loss 0.314912, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:44.088495: step 4288, loss 0.34573, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:44.193619: step 4289, loss 0.363894, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:44.300285: step 4290, loss 0.486911, acc 0.8125, learning_rate 0.0001
2017-10-09T15:14:44.406587: step 4291, loss 0.200082, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:44.511121: step 4292, loss 0.358926, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:44.618055: step 4293, loss 0.246182, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:44.725394: step 4294, loss 0.363598, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:44.832899: step 4295, loss 0.233975, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:44.938461: step 4296, loss 0.306964, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:45.043787: step 4297, loss 0.219574, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:45.147975: step 4298, loss 0.178281, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:45.253208: step 4299, loss 0.433745, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:45.356767: step 4300, loss 0.284689, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:45.466545: step 4301, loss 0.320123, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:45.571547: step 4302, loss 0.266951, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:45.678339: step 4303, loss 0.316409, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:45.788983: step 4304, loss 0.244693, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:45.891882: step 4305, loss 0.238447, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:45.997585: step 4306, loss 0.15835, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:46.101531: step 4307, loss 0.24839, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:46.209455: step 4308, loss 0.283062, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:46.309993: step 4309, loss 0.267107, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:46.428272: step 4310, loss 0.435765, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:46.552669: step 4311, loss 0.24393, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:46.644691: step 4312, loss 0.220272, acc 0.941176, learning_rate 0.0001
2017-10-09T15:14:46.750437: step 4313, loss 0.165839, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:46.858039: step 4314, loss 0.381895, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:46.961589: step 4315, loss 0.252727, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:47.069495: step 4316, loss 0.308304, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:47.172029: step 4317, loss 0.219824, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:47.278384: step 4318, loss 0.365228, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:47.380771: step 4319, loss 0.263359, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:47.487512: step 4320, loss 0.215077, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:47.755814: step 4320, loss 0.323895, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4320

2017-10-09T15:14:48.365436: step 4321, loss 0.334923, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:48.473387: step 4322, loss 0.288672, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:48.578756: step 4323, loss 0.210061, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:48.683944: step 4324, loss 0.292333, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:48.791305: step 4325, loss 0.277444, acc 0.96875, learning_rate 0.0001
2017-10-09T15:14:48.896020: step 4326, loss 0.307754, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:49.001869: step 4327, loss 0.236496, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:49.106550: step 4328, loss 0.229857, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:49.213347: step 4329, loss 0.180726, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:49.318416: step 4330, loss 0.211001, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:49.429601: step 4331, loss 0.206613, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:49.549013: step 4332, loss 0.227981, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:49.675755: step 4333, loss 0.232723, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:49.794246: step 4334, loss 0.391439, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:49.906714: step 4335, loss 0.137545, acc 1, learning_rate 0.0001
2017-10-09T15:14:50.010255: step 4336, loss 0.224705, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:50.113869: step 4337, loss 0.299772, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:50.220341: step 4338, loss 0.196977, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:50.330155: step 4339, loss 0.558146, acc 0.78125, learning_rate 0.0001
2017-10-09T15:14:50.434945: step 4340, loss 0.18539, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:50.544298: step 4341, loss 0.27942, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:50.650864: step 4342, loss 0.357529, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:50.755904: step 4343, loss 0.248983, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:50.861761: step 4344, loss 0.321465, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:50.966123: step 4345, loss 0.262233, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:51.075322: step 4346, loss 0.291597, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:51.183687: step 4347, loss 0.0993852, acc 0.984375, learning_rate 0.0001
2017-10-09T15:14:51.303768: step 4348, loss 0.203106, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:51.424992: step 4349, loss 0.276136, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:51.545869: step 4350, loss 0.325683, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:51.654161: step 4351, loss 0.26409, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:51.758058: step 4352, loss 0.308981, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:51.869352: step 4353, loss 0.284722, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:51.975113: step 4354, loss 0.325753, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:52.084979: step 4355, loss 0.318868, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:52.197132: step 4356, loss 0.256864, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:52.302711: step 4357, loss 0.233519, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:52.407206: step 4358, loss 0.362619, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:52.519795: step 4359, loss 0.137897, acc 0.984375, learning_rate 0.0001
2017-10-09T15:14:52.638264: step 4360, loss 0.303756, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:52.919697: step 4360, loss 0.324508, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4360

2017-10-09T15:14:53.588311: step 4361, loss 0.265167, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:53.701875: step 4362, loss 0.386196, acc 0.828125, learning_rate 0.0001
2017-10-09T15:14:53.811524: step 4363, loss 0.253396, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:53.923549: step 4364, loss 0.366757, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:54.026779: step 4365, loss 0.237376, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:54.135128: step 4366, loss 0.289096, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:54.241755: step 4367, loss 0.199978, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:54.350242: step 4368, loss 0.39916, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:54.459451: step 4369, loss 0.323426, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:54.563359: step 4370, loss 0.223211, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:54.670629: step 4371, loss 0.249811, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:54.774894: step 4372, loss 0.199074, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:54.880193: step 4373, loss 0.265957, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:54.991190: step 4374, loss 0.215076, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:55.100560: step 4375, loss 0.190158, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:55.209658: step 4376, loss 0.349545, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:55.313507: step 4377, loss 0.251875, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:55.417516: step 4378, loss 0.184309, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:55.533758: step 4379, loss 0.323221, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:55.642436: step 4380, loss 0.354077, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:55.749206: step 4381, loss 0.259495, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:55.862927: step 4382, loss 0.216074, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:55.968788: step 4383, loss 0.312772, acc 0.84375, learning_rate 0.0001
2017-10-09T15:14:56.074571: step 4384, loss 0.39802, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:56.180533: step 4385, loss 0.321451, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:56.284919: step 4386, loss 0.39072, acc 0.859375, learning_rate 0.0001
2017-10-09T15:14:56.391173: step 4387, loss 0.479506, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:56.497547: step 4388, loss 0.236149, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:56.602191: step 4389, loss 0.260245, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:56.712039: step 4390, loss 0.363979, acc 0.8125, learning_rate 0.0001
2017-10-09T15:14:56.817799: step 4391, loss 0.218086, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:56.957364: step 4392, loss 0.257583, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:57.086770: step 4393, loss 0.230975, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:57.213099: step 4394, loss 0.168413, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:57.350542: step 4395, loss 0.240635, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:57.455198: step 4396, loss 0.285948, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:57.561700: step 4397, loss 0.225193, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:57.662659: step 4398, loss 0.256315, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:57.769207: step 4399, loss 0.265006, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:57.878976: step 4400, loss 0.272172, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:14:58.149548: step 4400, loss 0.325183, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4400

2017-10-09T15:14:58.688274: step 4401, loss 0.262727, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:58.797474: step 4402, loss 0.223584, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:58.907862: step 4403, loss 0.344191, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:59.016903: step 4404, loss 0.223059, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:59.121820: step 4405, loss 0.172842, acc 0.921875, learning_rate 0.0001
2017-10-09T15:14:59.231938: step 4406, loss 0.16604, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:59.341042: step 4407, loss 0.292054, acc 0.90625, learning_rate 0.0001
2017-10-09T15:14:59.444168: step 4408, loss 0.205809, acc 0.953125, learning_rate 0.0001
2017-10-09T15:14:59.551934: step 4409, loss 0.235706, acc 0.9375, learning_rate 0.0001
2017-10-09T15:14:59.644769: step 4410, loss 0.273491, acc 0.862745, learning_rate 0.0001
2017-10-09T15:14:59.752634: step 4411, loss 0.27899, acc 0.875, learning_rate 0.0001
2017-10-09T15:14:59.861529: step 4412, loss 0.233241, acc 0.890625, learning_rate 0.0001
2017-10-09T15:14:59.976782: step 4413, loss 0.34707, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:00.095709: step 4414, loss 0.240576, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:00.214391: step 4415, loss 0.309861, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:00.333431: step 4416, loss 0.299258, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:00.458512: step 4417, loss 0.215697, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:00.574885: step 4418, loss 0.442993, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:00.696598: step 4419, loss 0.123972, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:00.823804: step 4420, loss 0.106001, acc 0.984375, learning_rate 0.0001
2017-10-09T15:15:00.953106: step 4421, loss 0.215398, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:01.069871: step 4422, loss 0.143871, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:01.175277: step 4423, loss 0.299549, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:01.292765: step 4424, loss 0.171323, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:01.412861: step 4425, loss 0.35303, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:01.521971: step 4426, loss 0.298473, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:01.644021: step 4427, loss 0.296934, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:01.763139: step 4428, loss 0.233927, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:01.880576: step 4429, loss 0.174843, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:01.987647: step 4430, loss 0.280307, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:02.092557: step 4431, loss 0.328355, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:02.196399: step 4432, loss 0.357722, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:02.303286: step 4433, loss 0.116238, acc 0.984375, learning_rate 0.0001
2017-10-09T15:15:02.406482: step 4434, loss 0.196981, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:02.514962: step 4435, loss 0.236985, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:02.617321: step 4436, loss 0.198958, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:02.726455: step 4437, loss 0.318089, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:02.833443: step 4438, loss 0.322866, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:02.940250: step 4439, loss 0.298323, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:03.049427: step 4440, loss 0.266104, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:03.312867: step 4440, loss 0.327358, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4440

2017-10-09T15:15:03.917274: step 4441, loss 0.324547, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:04.029189: step 4442, loss 0.232836, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:04.137039: step 4443, loss 0.313024, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:04.242945: step 4444, loss 0.178822, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:04.348271: step 4445, loss 0.371706, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:04.459734: step 4446, loss 0.242285, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:04.566687: step 4447, loss 0.39235, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:04.674426: step 4448, loss 0.286957, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:04.784184: step 4449, loss 0.427586, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:04.889294: step 4450, loss 0.257087, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:04.996915: step 4451, loss 0.0677319, acc 0.984375, learning_rate 0.0001
2017-10-09T15:15:05.104773: step 4452, loss 0.275241, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:05.247642: step 4453, loss 0.229871, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:05.374035: step 4454, loss 0.325042, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:05.499225: step 4455, loss 0.277517, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:05.613743: step 4456, loss 0.277357, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:05.721547: step 4457, loss 0.251588, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:05.829366: step 4458, loss 0.250754, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:05.941967: step 4459, loss 0.363902, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:06.047111: step 4460, loss 0.382102, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:06.154275: step 4461, loss 0.296719, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:06.259012: step 4462, loss 0.337913, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:06.360660: step 4463, loss 0.284267, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:06.470328: step 4464, loss 0.191798, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:06.574404: step 4465, loss 0.304482, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:06.677712: step 4466, loss 0.230221, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:06.780269: step 4467, loss 0.355639, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:06.889126: step 4468, loss 0.186741, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:06.995710: step 4469, loss 0.299929, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:07.099816: step 4470, loss 0.215741, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:07.205506: step 4471, loss 0.341936, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:07.312232: step 4472, loss 0.211873, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:07.413235: step 4473, loss 0.20628, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:07.518453: step 4474, loss 0.197387, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:07.622962: step 4475, loss 0.238517, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:07.723839: step 4476, loss 0.229305, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:07.824657: step 4477, loss 0.306414, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:07.935663: step 4478, loss 0.435442, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:08.042784: step 4479, loss 0.194389, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:08.149922: step 4480, loss 0.400113, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:08.413306: step 4480, loss 0.322318, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4480

2017-10-09T15:15:09.007870: step 4481, loss 0.178902, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:09.110617: step 4482, loss 0.238519, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:09.216810: step 4483, loss 0.198462, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:09.325898: step 4484, loss 0.341119, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:09.431033: step 4485, loss 0.3043, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:09.537617: step 4486, loss 0.279784, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:09.640868: step 4487, loss 0.412034, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:09.751816: step 4488, loss 0.289444, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:09.867985: step 4489, loss 0.177742, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:09.983403: step 4490, loss 0.252165, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:10.092953: step 4491, loss 0.229273, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:10.201591: step 4492, loss 0.518808, acc 0.796875, learning_rate 0.0001
2017-10-09T15:15:10.309196: step 4493, loss 0.326301, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:10.416596: step 4494, loss 0.28828, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:10.527588: step 4495, loss 0.314878, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:10.632097: step 4496, loss 0.27004, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:10.741997: step 4497, loss 0.208584, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:10.851465: step 4498, loss 0.193157, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:10.959779: step 4499, loss 0.282756, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:11.070556: step 4500, loss 0.320275, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:11.175124: step 4501, loss 0.148137, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:11.280858: step 4502, loss 0.267859, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:11.388571: step 4503, loss 0.295555, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:11.492355: step 4504, loss 0.216937, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:11.595698: step 4505, loss 0.337262, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:11.700755: step 4506, loss 0.247122, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:11.811337: step 4507, loss 0.33973, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:11.906797: step 4508, loss 0.439537, acc 0.823529, learning_rate 0.0001
2017-10-09T15:15:12.020694: step 4509, loss 0.28266, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:12.129378: step 4510, loss 0.273991, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:12.242751: step 4511, loss 0.308891, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:12.352709: step 4512, loss 0.351318, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:12.459014: step 4513, loss 0.292739, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:12.566557: step 4514, loss 0.296339, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:12.675548: step 4515, loss 0.296063, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:12.781046: step 4516, loss 0.274199, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:12.890275: step 4517, loss 0.328486, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:12.997874: step 4518, loss 0.288131, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:13.103408: step 4519, loss 0.280621, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:13.211854: step 4520, loss 0.433995, acc 0.828125, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:13.508370: step 4520, loss 0.320006, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4520

2017-10-09T15:15:14.207258: step 4521, loss 0.270507, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:14.326381: step 4522, loss 0.165462, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:14.432715: step 4523, loss 0.266309, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:14.540295: step 4524, loss 0.233689, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:14.646032: step 4525, loss 0.289324, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:14.767241: step 4526, loss 0.201774, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:14.889556: step 4527, loss 0.246152, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:15.007796: step 4528, loss 0.30625, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:15.115704: step 4529, loss 0.430157, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:15.220549: step 4530, loss 0.267477, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:15.326551: step 4531, loss 0.371941, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:15.430681: step 4532, loss 0.272295, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:15.538097: step 4533, loss 0.340203, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:15.642391: step 4534, loss 0.305864, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:15.748061: step 4535, loss 0.266499, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:15.853133: step 4536, loss 0.209032, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:15.954734: step 4537, loss 0.252574, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:16.060045: step 4538, loss 0.320791, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:16.166877: step 4539, loss 0.24299, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:16.273183: step 4540, loss 0.366432, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:16.379818: step 4541, loss 0.318002, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:16.484763: step 4542, loss 0.197905, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:16.588442: step 4543, loss 0.340476, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:16.698553: step 4544, loss 0.145729, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:16.800375: step 4545, loss 0.17699, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:16.919624: step 4546, loss 0.264583, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:17.039619: step 4547, loss 0.234596, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:17.154561: step 4548, loss 0.124313, acc 0.984375, learning_rate 0.0001
2017-10-09T15:15:17.259678: step 4549, loss 0.222256, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:17.366692: step 4550, loss 0.277855, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:17.476079: step 4551, loss 0.300127, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:17.581123: step 4552, loss 0.318529, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:17.687498: step 4553, loss 0.18108, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:17.794189: step 4554, loss 0.200263, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:17.903660: step 4555, loss 0.252869, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:18.013206: step 4556, loss 0.175919, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:18.117200: step 4557, loss 0.170892, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:18.217832: step 4558, loss 0.400318, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:18.324116: step 4559, loss 0.324451, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:18.432490: step 4560, loss 0.142604, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:18.687202: step 4560, loss 0.321058, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4560

2017-10-09T15:15:19.215480: step 4561, loss 0.1791, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:19.336175: step 4562, loss 0.287318, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:19.460974: step 4563, loss 0.269914, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:19.580019: step 4564, loss 0.259857, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:19.680747: step 4565, loss 0.485193, acc 0.8125, learning_rate 0.0001
2017-10-09T15:15:19.787043: step 4566, loss 0.216946, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:19.889871: step 4567, loss 0.24882, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:20.000987: step 4568, loss 0.354199, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:20.109086: step 4569, loss 0.178593, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:20.215004: step 4570, loss 0.282904, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:20.336696: step 4571, loss 0.262434, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:20.462065: step 4572, loss 0.245144, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:20.582981: step 4573, loss 0.251502, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:20.691204: step 4574, loss 0.277986, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:20.798872: step 4575, loss 0.432567, acc 0.796875, learning_rate 0.0001
2017-10-09T15:15:20.905772: step 4576, loss 0.304441, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:21.014177: step 4577, loss 0.16369, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:21.122839: step 4578, loss 0.197229, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:21.228299: step 4579, loss 0.267511, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:21.336913: step 4580, loss 0.304905, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:21.443037: step 4581, loss 0.284783, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:21.549736: step 4582, loss 0.202245, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:21.658762: step 4583, loss 0.283849, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:21.771177: step 4584, loss 0.259137, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:21.879366: step 4585, loss 0.184475, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:21.990081: step 4586, loss 0.304191, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:22.100456: step 4587, loss 0.169174, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:22.212654: step 4588, loss 0.358286, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:22.335949: step 4589, loss 0.284502, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:22.459045: step 4590, loss 0.348254, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:22.578637: step 4591, loss 0.30357, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:22.682857: step 4592, loss 0.394314, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:22.788773: step 4593, loss 0.420972, acc 0.8125, learning_rate 0.0001
2017-10-09T15:15:22.897729: step 4594, loss 0.208147, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:23.005425: step 4595, loss 0.326939, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:23.114216: step 4596, loss 0.171369, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:23.223692: step 4597, loss 0.3568, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:23.328516: step 4598, loss 0.43054, acc 0.828125, learning_rate 0.0001
2017-10-09T15:15:23.435111: step 4599, loss 0.241446, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:23.543714: step 4600, loss 0.335853, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:23.802143: step 4600, loss 0.319889, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4600

2017-10-09T15:15:24.407303: step 4601, loss 0.375602, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:24.515395: step 4602, loss 0.265917, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:24.624499: step 4603, loss 0.448822, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:24.741315: step 4604, loss 0.325218, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:24.852675: step 4605, loss 0.154636, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:24.942507: step 4606, loss 0.155396, acc 0.941176, learning_rate 0.0001
2017-10-09T15:15:25.050573: step 4607, loss 0.242037, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:25.157488: step 4608, loss 0.368797, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:25.259757: step 4609, loss 0.269265, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:25.366355: step 4610, loss 0.48671, acc 0.828125, learning_rate 0.0001
2017-10-09T15:15:25.479272: step 4611, loss 0.343114, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:25.587301: step 4612, loss 0.294115, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:25.695292: step 4613, loss 0.274478, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:25.803640: step 4614, loss 0.322077, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:25.909503: step 4615, loss 0.181391, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:26.016734: step 4616, loss 0.213769, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:26.118657: step 4617, loss 0.217681, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:26.225673: step 4618, loss 0.276913, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:26.330877: step 4619, loss 0.253592, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:26.435682: step 4620, loss 0.3105, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:26.542618: step 4621, loss 0.285401, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:26.654067: step 4622, loss 0.285058, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:26.761270: step 4623, loss 0.333002, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:26.868970: step 4624, loss 0.277806, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:26.977729: step 4625, loss 0.285132, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:27.087083: step 4626, loss 0.293215, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:27.193849: step 4627, loss 0.340792, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:27.302309: step 4628, loss 0.26253, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:27.408282: step 4629, loss 0.24003, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:27.516391: step 4630, loss 0.33667, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:27.619857: step 4631, loss 0.286377, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:27.726551: step 4632, loss 0.338135, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:27.831989: step 4633, loss 0.244722, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:27.939912: step 4634, loss 0.252743, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:28.047124: step 4635, loss 0.290881, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:28.152103: step 4636, loss 0.294105, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:28.255845: step 4637, loss 0.195115, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:28.360849: step 4638, loss 0.241049, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:28.465691: step 4639, loss 0.338166, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:28.572351: step 4640, loss 0.158067, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:28.823322: step 4640, loss 0.320169, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4640

2017-10-09T15:15:29.415385: step 4641, loss 0.224569, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:29.524110: step 4642, loss 0.201464, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:29.624261: step 4643, loss 0.242789, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:29.729796: step 4644, loss 0.379589, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:29.833177: step 4645, loss 0.283362, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:29.941524: step 4646, loss 0.228809, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:30.047685: step 4647, loss 0.281902, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:30.151514: step 4648, loss 0.19367, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:30.257550: step 4649, loss 0.20627, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:30.359704: step 4650, loss 0.336935, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:30.464282: step 4651, loss 0.30018, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:30.572932: step 4652, loss 0.25559, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:30.675094: step 4653, loss 0.20641, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:30.778776: step 4654, loss 0.246853, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:30.885293: step 4655, loss 0.41927, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:30.991636: step 4656, loss 0.238185, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:31.099984: step 4657, loss 0.359874, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:31.205414: step 4658, loss 0.272713, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:31.310200: step 4659, loss 0.363713, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:31.414626: step 4660, loss 0.232797, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:31.520275: step 4661, loss 0.355434, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:31.630640: step 4662, loss 0.226728, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:31.740836: step 4663, loss 0.207037, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:31.845563: step 4664, loss 0.286392, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:31.953946: step 4665, loss 0.258625, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:32.062095: step 4666, loss 0.334513, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:32.168945: step 4667, loss 0.34443, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:32.278609: step 4668, loss 0.288372, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:32.382360: step 4669, loss 0.252323, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:32.491492: step 4670, loss 0.227641, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:32.599440: step 4671, loss 0.298856, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:32.709444: step 4672, loss 0.249605, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:32.813191: step 4673, loss 0.260673, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:32.925041: step 4674, loss 0.411237, acc 0.828125, learning_rate 0.0001
2017-10-09T15:15:33.030316: step 4675, loss 0.367864, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:33.133029: step 4676, loss 0.181337, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:33.241269: step 4677, loss 0.307121, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:33.345242: step 4678, loss 0.154654, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:33.455143: step 4679, loss 0.333153, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:33.555580: step 4680, loss 0.34751, acc 0.828125, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:33.809646: step 4680, loss 0.317574, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4680

2017-10-09T15:15:34.487736: step 4681, loss 0.212245, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:34.603092: step 4682, loss 0.169466, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:34.713089: step 4683, loss 0.289822, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:34.821192: step 4684, loss 0.233686, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:34.930899: step 4685, loss 0.314144, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:35.044324: step 4686, loss 0.258803, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:35.155071: step 4687, loss 0.253967, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:35.264926: step 4688, loss 0.326908, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:35.375600: step 4689, loss 0.268166, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:35.478268: step 4690, loss 0.158217, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:35.583889: step 4691, loss 0.140948, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:35.688714: step 4692, loss 0.334025, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:35.796092: step 4693, loss 0.242889, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:35.905351: step 4694, loss 0.256369, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:36.007342: step 4695, loss 0.119275, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:36.113239: step 4696, loss 0.351531, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:36.219291: step 4697, loss 0.387323, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:36.325053: step 4698, loss 0.262376, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:36.432065: step 4699, loss 0.299608, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:36.541202: step 4700, loss 0.175261, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:36.647126: step 4701, loss 0.231713, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:36.752330: step 4702, loss 0.28771, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:36.859888: step 4703, loss 0.294896, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:36.951077: step 4704, loss 0.169665, acc 0.941176, learning_rate 0.0001
2017-10-09T15:15:37.054757: step 4705, loss 0.224157, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:37.160920: step 4706, loss 0.312987, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:37.270485: step 4707, loss 0.187933, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:37.380155: step 4708, loss 0.288836, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:37.488501: step 4709, loss 0.406803, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:37.596007: step 4710, loss 0.277526, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:37.699497: step 4711, loss 0.182437, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:37.806367: step 4712, loss 0.317356, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:37.909395: step 4713, loss 0.27479, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:38.016451: step 4714, loss 0.258006, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:38.129619: step 4715, loss 0.321433, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:38.253088: step 4716, loss 0.284367, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:38.371159: step 4717, loss 0.192401, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:38.475692: step 4718, loss 0.199808, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:38.585824: step 4719, loss 0.216753, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:38.694238: step 4720, loss 0.207103, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:38.952568: step 4720, loss 0.31698, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4720

2017-10-09T15:15:39.489165: step 4721, loss 0.261505, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:39.595807: step 4722, loss 0.21644, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:39.707426: step 4723, loss 0.155234, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:39.822628: step 4724, loss 0.192663, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:39.931498: step 4725, loss 0.135532, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:40.037722: step 4726, loss 0.164388, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:40.150829: step 4727, loss 0.334979, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:40.262501: step 4728, loss 0.210942, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:40.373694: step 4729, loss 0.258627, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:40.487328: step 4730, loss 0.320635, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:40.593504: step 4731, loss 0.269381, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:40.695746: step 4732, loss 0.353529, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:40.804896: step 4733, loss 0.0988821, acc 0.984375, learning_rate 0.0001
2017-10-09T15:15:40.914221: step 4734, loss 0.454268, acc 0.796875, learning_rate 0.0001
2017-10-09T15:15:41.025934: step 4735, loss 0.54183, acc 0.828125, learning_rate 0.0001
2017-10-09T15:15:41.138342: step 4736, loss 0.212682, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:41.249572: step 4737, loss 0.28844, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:41.357850: step 4738, loss 0.212018, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:41.468003: step 4739, loss 0.263425, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:41.576016: step 4740, loss 0.259281, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:41.683984: step 4741, loss 0.4129, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:41.790424: step 4742, loss 0.320471, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:41.900677: step 4743, loss 0.249867, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:42.005489: step 4744, loss 0.129713, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:42.110033: step 4745, loss 0.170391, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:42.214712: step 4746, loss 0.276443, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:42.321370: step 4747, loss 0.236689, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:42.430413: step 4748, loss 0.202441, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:42.536812: step 4749, loss 0.191228, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:42.639652: step 4750, loss 0.267793, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:42.745760: step 4751, loss 0.319327, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:42.854399: step 4752, loss 0.320765, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:42.959765: step 4753, loss 0.177625, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:43.066588: step 4754, loss 0.120238, acc 0.984375, learning_rate 0.0001
2017-10-09T15:15:43.172500: step 4755, loss 0.292672, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:43.280587: step 4756, loss 0.320947, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:43.385498: step 4757, loss 0.267217, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:43.488095: step 4758, loss 0.338558, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:43.587204: step 4759, loss 0.300241, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:43.711728: step 4760, loss 0.252554, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:43.995154: step 4760, loss 0.315849, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4760

2017-10-09T15:15:44.606069: step 4761, loss 0.351914, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:44.711314: step 4762, loss 0.209376, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:44.814866: step 4763, loss 0.217384, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:44.923273: step 4764, loss 0.263599, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:45.027727: step 4765, loss 0.198621, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:45.136108: step 4766, loss 0.330784, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:45.239936: step 4767, loss 0.351834, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:45.347637: step 4768, loss 0.214342, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:45.454414: step 4769, loss 0.216683, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:45.558440: step 4770, loss 0.157989, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:45.664566: step 4771, loss 0.222492, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:45.772024: step 4772, loss 0.319941, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:45.877909: step 4773, loss 0.194951, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:45.986160: step 4774, loss 0.292222, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:46.093239: step 4775, loss 0.362825, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:46.198805: step 4776, loss 0.147214, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:46.303806: step 4777, loss 0.228843, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:46.408071: step 4778, loss 0.314851, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:46.519313: step 4779, loss 0.522544, acc 0.828125, learning_rate 0.0001
2017-10-09T15:15:46.623742: step 4780, loss 0.161676, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:46.731964: step 4781, loss 0.165217, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:46.839736: step 4782, loss 0.448004, acc 0.828125, learning_rate 0.0001
2017-10-09T15:15:46.947042: step 4783, loss 0.229172, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:47.058545: step 4784, loss 0.259118, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:47.170840: step 4785, loss 0.367665, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:47.276888: step 4786, loss 0.357901, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:47.384672: step 4787, loss 0.394567, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:47.494553: step 4788, loss 0.235697, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:47.602390: step 4789, loss 0.274204, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:47.709623: step 4790, loss 0.17131, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:47.813155: step 4791, loss 0.324587, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:47.923190: step 4792, loss 0.27844, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:48.026557: step 4793, loss 0.389959, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:48.132601: step 4794, loss 0.253918, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:48.237946: step 4795, loss 0.224698, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:48.354185: step 4796, loss 0.232349, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:48.478392: step 4797, loss 0.253341, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:48.598640: step 4798, loss 0.286671, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:48.717543: step 4799, loss 0.290312, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:48.826055: step 4800, loss 0.290885, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:49.088212: step 4800, loss 0.316467, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4800

2017-10-09T15:15:49.715929: step 4801, loss 0.267198, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:49.804499: step 4802, loss 0.133422, acc 0.980392, learning_rate 0.0001
2017-10-09T15:15:49.920082: step 4803, loss 0.267278, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:50.023505: step 4804, loss 0.133455, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:50.129636: step 4805, loss 0.342626, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:50.240297: step 4806, loss 0.157074, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:50.347743: step 4807, loss 0.234878, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:50.454517: step 4808, loss 0.183707, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:50.562888: step 4809, loss 0.291594, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:50.675119: step 4810, loss 0.192986, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:50.776473: step 4811, loss 0.232999, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:50.884860: step 4812, loss 0.317321, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:50.994387: step 4813, loss 0.218909, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:51.104050: step 4814, loss 0.268371, acc 0.859375, learning_rate 0.0001
2017-10-09T15:15:51.210300: step 4815, loss 0.257618, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:51.314002: step 4816, loss 0.182631, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:51.420187: step 4817, loss 0.354798, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:51.532453: step 4818, loss 0.375922, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:51.638166: step 4819, loss 0.248759, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:51.745653: step 4820, loss 0.365234, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:51.854894: step 4821, loss 0.352712, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:51.966412: step 4822, loss 0.215824, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:52.072075: step 4823, loss 0.332987, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:52.175902: step 4824, loss 0.173645, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:52.280421: step 4825, loss 0.161396, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:52.387685: step 4826, loss 0.272207, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:52.497536: step 4827, loss 0.251068, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:52.601821: step 4828, loss 0.540175, acc 0.8125, learning_rate 0.0001
2017-10-09T15:15:52.712629: step 4829, loss 0.254421, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:52.816308: step 4830, loss 0.222147, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:52.928139: step 4831, loss 0.421071, acc 0.796875, learning_rate 0.0001
2017-10-09T15:15:53.030712: step 4832, loss 0.250739, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:53.133464: step 4833, loss 0.311293, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:53.240472: step 4834, loss 0.153786, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:53.350265: step 4835, loss 0.232201, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:53.457598: step 4836, loss 0.338289, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:53.563668: step 4837, loss 0.23206, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:53.671003: step 4838, loss 0.183252, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:53.778536: step 4839, loss 0.309636, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:53.886155: step 4840, loss 0.456949, acc 0.796875, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:54.152406: step 4840, loss 0.317434, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4840

2017-10-09T15:15:54.856190: step 4841, loss 0.435177, acc 0.84375, learning_rate 0.0001
2017-10-09T15:15:54.980977: step 4842, loss 0.253875, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:55.099219: step 4843, loss 0.191713, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:55.218781: step 4844, loss 0.285048, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:55.336065: step 4845, loss 0.318043, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:55.442097: step 4846, loss 0.126551, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:55.552659: step 4847, loss 0.193383, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:55.658028: step 4848, loss 0.306847, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:55.765787: step 4849, loss 0.361629, acc 0.828125, learning_rate 0.0001
2017-10-09T15:15:55.867485: step 4850, loss 0.316894, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:55.974019: step 4851, loss 0.374995, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:56.079966: step 4852, loss 0.337737, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:56.191514: step 4853, loss 0.197369, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:56.297024: step 4854, loss 0.271652, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:56.404856: step 4855, loss 0.21072, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:56.510474: step 4856, loss 0.231535, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:56.613448: step 4857, loss 0.240296, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:56.718387: step 4858, loss 0.247045, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:56.823810: step 4859, loss 0.238938, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:56.930483: step 4860, loss 0.240626, acc 0.953125, learning_rate 0.0001
2017-10-09T15:15:57.036831: step 4861, loss 0.250494, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:57.142888: step 4862, loss 0.111725, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:57.250059: step 4863, loss 0.213803, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:57.352165: step 4864, loss 0.250533, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:57.455284: step 4865, loss 0.469893, acc 0.796875, learning_rate 0.0001
2017-10-09T15:15:57.559414: step 4866, loss 0.284752, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:57.663662: step 4867, loss 0.321732, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:57.772105: step 4868, loss 0.297028, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:57.880265: step 4869, loss 0.300958, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:57.984917: step 4870, loss 0.229044, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:58.089886: step 4871, loss 0.198362, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:58.196768: step 4872, loss 0.328101, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:58.306326: step 4873, loss 0.173963, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:58.410351: step 4874, loss 0.257416, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:58.515376: step 4875, loss 0.255895, acc 0.90625, learning_rate 0.0001
2017-10-09T15:15:58.622828: step 4876, loss 0.250892, acc 0.9375, learning_rate 0.0001
2017-10-09T15:15:58.731909: step 4877, loss 0.18293, acc 0.921875, learning_rate 0.0001
2017-10-09T15:15:58.841223: step 4878, loss 0.345318, acc 0.875, learning_rate 0.0001
2017-10-09T15:15:58.947763: step 4879, loss 0.16483, acc 0.96875, learning_rate 0.0001
2017-10-09T15:15:59.054384: step 4880, loss 0.215351, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-09T15:15:59.313463: step 4880, loss 0.317951, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4880

2017-10-09T15:15:59.853113: step 4881, loss 0.245114, acc 0.890625, learning_rate 0.0001
2017-10-09T15:15:59.957529: step 4882, loss 0.324412, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:00.066829: step 4883, loss 0.171385, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:00.173878: step 4884, loss 0.231702, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:00.276573: step 4885, loss 0.195345, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:00.383642: step 4886, loss 0.254832, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:00.488274: step 4887, loss 0.205229, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:00.595527: step 4888, loss 0.19721, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:00.704911: step 4889, loss 0.286231, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:00.809678: step 4890, loss 0.304869, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:00.917053: step 4891, loss 0.205295, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:01.025172: step 4892, loss 0.256658, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:01.130239: step 4893, loss 0.330643, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:01.232889: step 4894, loss 0.380486, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:01.339021: step 4895, loss 0.262562, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:01.442950: step 4896, loss 0.311177, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:01.548754: step 4897, loss 0.246165, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:01.651262: step 4898, loss 0.163881, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:01.753223: step 4899, loss 0.163302, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:01.840468: step 4900, loss 0.366289, acc 0.882353, learning_rate 0.0001
2017-10-09T15:16:01.946012: step 4901, loss 0.256978, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:02.053289: step 4902, loss 0.321214, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:02.159648: step 4903, loss 0.278987, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:02.264144: step 4904, loss 0.203646, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:02.370102: step 4905, loss 0.38506, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:02.474989: step 4906, loss 0.154838, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:02.583548: step 4907, loss 0.287413, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:02.690148: step 4908, loss 0.396249, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:02.796273: step 4909, loss 0.240964, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:02.903795: step 4910, loss 0.235411, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:03.007110: step 4911, loss 0.261323, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:03.113318: step 4912, loss 0.289241, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:03.217134: step 4913, loss 0.15201, acc 0.984375, learning_rate 0.0001
2017-10-09T15:16:03.320266: step 4914, loss 0.250518, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:03.426803: step 4915, loss 0.47336, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:03.533388: step 4916, loss 0.16023, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:03.635743: step 4917, loss 0.376213, acc 0.828125, learning_rate 0.0001
2017-10-09T15:16:03.742171: step 4918, loss 0.277163, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:03.850841: step 4919, loss 0.238756, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:03.960464: step 4920, loss 0.34589, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:04.216227: step 4920, loss 0.313078, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4920

2017-10-09T15:16:04.797631: step 4921, loss 0.253601, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:04.907270: step 4922, loss 0.321768, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:05.012333: step 4923, loss 0.161767, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:05.119032: step 4924, loss 0.249492, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:05.223900: step 4925, loss 0.230931, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:05.337852: step 4926, loss 0.206579, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:05.442348: step 4927, loss 0.209643, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:05.547349: step 4928, loss 0.290079, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:05.653385: step 4929, loss 0.254595, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:05.757432: step 4930, loss 0.188612, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:05.864364: step 4931, loss 0.330498, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:05.967526: step 4932, loss 0.290016, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:06.075228: step 4933, loss 0.219667, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:06.182859: step 4934, loss 0.319595, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:06.287720: step 4935, loss 0.261961, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:06.391786: step 4936, loss 0.278129, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:06.500237: step 4937, loss 0.237806, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:06.606655: step 4938, loss 0.181102, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:06.715581: step 4939, loss 0.253551, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:06.823815: step 4940, loss 0.294834, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:06.930755: step 4941, loss 0.351599, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:07.039881: step 4942, loss 0.25998, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:07.143158: step 4943, loss 0.286405, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:07.251848: step 4944, loss 0.292437, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:07.358348: step 4945, loss 0.290478, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:07.465886: step 4946, loss 0.358646, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:07.574111: step 4947, loss 0.333085, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:07.680521: step 4948, loss 0.233656, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:07.784270: step 4949, loss 0.295781, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:07.891315: step 4950, loss 0.249151, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:07.995283: step 4951, loss 0.290227, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:08.099787: step 4952, loss 0.324694, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:08.206400: step 4953, loss 0.243182, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:08.313029: step 4954, loss 0.290374, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:08.419631: step 4955, loss 0.234955, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:08.525603: step 4956, loss 0.319638, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:08.630564: step 4957, loss 0.192703, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:08.732789: step 4958, loss 0.303629, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:08.838388: step 4959, loss 0.204792, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:08.943298: step 4960, loss 0.260845, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:09.233514: step 4960, loss 0.314232, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-4960

2017-10-09T15:16:09.827283: step 4961, loss 0.291626, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:09.930259: step 4962, loss 0.37053, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:10.039636: step 4963, loss 0.191956, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:10.145724: step 4964, loss 0.25986, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:10.253828: step 4965, loss 0.359638, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:10.358931: step 4966, loss 0.225148, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:10.461890: step 4967, loss 0.352705, acc 0.828125, learning_rate 0.0001
2017-10-09T15:16:10.569604: step 4968, loss 0.237567, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:10.676918: step 4969, loss 0.375008, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:10.784160: step 4970, loss 0.16732, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:10.890117: step 4971, loss 0.290138, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:10.998442: step 4972, loss 0.146294, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:11.106361: step 4973, loss 0.190188, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:11.213255: step 4974, loss 0.347961, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:11.319364: step 4975, loss 0.207457, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:11.423854: step 4976, loss 0.305606, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:11.527079: step 4977, loss 0.188078, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:11.631877: step 4978, loss 0.205196, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:11.735582: step 4979, loss 0.541657, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:11.841834: step 4980, loss 0.244307, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:11.949638: step 4981, loss 0.258644, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:12.059033: step 4982, loss 0.17731, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:12.159759: step 4983, loss 0.181803, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:12.263979: step 4984, loss 0.294108, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:12.378894: step 4985, loss 0.250323, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:12.480410: step 4986, loss 0.379618, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:12.585276: step 4987, loss 0.311835, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:12.692211: step 4988, loss 0.344612, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:12.798939: step 4989, loss 0.365873, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:12.910518: step 4990, loss 0.231028, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:13.020933: step 4991, loss 0.257805, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:13.130068: step 4992, loss 0.291681, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:13.235959: step 4993, loss 0.286468, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:13.346596: step 4994, loss 0.259391, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:13.463774: step 4995, loss 0.255481, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:13.573831: step 4996, loss 0.22686, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:13.687546: step 4997, loss 0.295278, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:13.787924: step 4998, loss 0.355214, acc 0.882353, learning_rate 0.0001
2017-10-09T15:16:13.908592: step 4999, loss 0.183815, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:14.037164: step 5000, loss 0.174963, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:14.295583: step 5000, loss 0.315943, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5000

2017-10-09T15:16:15.035119: step 5001, loss 0.185862, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:15.157425: step 5002, loss 0.171863, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:15.262876: step 5003, loss 0.266296, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:15.366712: step 5004, loss 0.431364, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:15.475570: step 5005, loss 0.281107, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:15.591922: step 5006, loss 0.246479, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:15.702862: step 5007, loss 0.360923, acc 0.828125, learning_rate 0.0001
2017-10-09T15:16:15.809665: step 5008, loss 0.458508, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:15.927599: step 5009, loss 0.247411, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:16.037831: step 5010, loss 0.187162, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:16.149062: step 5011, loss 0.332448, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:16.262432: step 5012, loss 0.222103, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:16.373675: step 5013, loss 0.429482, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:16.479968: step 5014, loss 0.233317, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:16.588439: step 5015, loss 0.268573, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:16.697712: step 5016, loss 0.289792, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:16.810729: step 5017, loss 0.323288, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:16.921931: step 5018, loss 0.153861, acc 0.984375, learning_rate 0.0001
2017-10-09T15:16:17.030674: step 5019, loss 0.310072, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:17.138101: step 5020, loss 0.359862, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:17.259487: step 5021, loss 0.341011, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:17.366906: step 5022, loss 0.161517, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:17.471928: step 5023, loss 0.327331, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:17.579646: step 5024, loss 0.194075, acc 0.984375, learning_rate 0.0001
2017-10-09T15:16:17.684291: step 5025, loss 0.181222, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:17.790240: step 5026, loss 0.300925, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:17.894782: step 5027, loss 0.263213, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:18.001747: step 5028, loss 0.321411, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:18.112154: step 5029, loss 0.278738, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:18.223794: step 5030, loss 0.176554, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:18.339309: step 5031, loss 0.349713, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:18.449056: step 5032, loss 0.279819, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:18.569557: step 5033, loss 0.229914, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:18.680788: step 5034, loss 0.23287, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:18.782961: step 5035, loss 0.193444, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:18.890526: step 5036, loss 0.362732, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:18.996926: step 5037, loss 0.315584, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:19.102042: step 5038, loss 0.175027, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:19.206621: step 5039, loss 0.397188, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:19.314330: step 5040, loss 0.152934, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:19.572059: step 5040, loss 0.313362, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5040

2017-10-09T15:16:20.108463: step 5041, loss 0.290648, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:20.216702: step 5042, loss 0.320307, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:20.327236: step 5043, loss 0.190655, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:20.438128: step 5044, loss 0.254858, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:20.543650: step 5045, loss 0.146524, acc 0.984375, learning_rate 0.0001
2017-10-09T15:16:20.655755: step 5046, loss 0.195121, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:20.758177: step 5047, loss 0.285001, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:20.867429: step 5048, loss 0.247369, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:20.972672: step 5049, loss 0.283606, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:21.080244: step 5050, loss 0.238788, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:21.187660: step 5051, loss 0.167382, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:21.292682: step 5052, loss 0.336995, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:21.392076: step 5053, loss 0.341246, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:21.497511: step 5054, loss 0.272773, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:21.604972: step 5055, loss 0.247239, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:21.707685: step 5056, loss 0.293432, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:21.809760: step 5057, loss 0.24645, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:21.919068: step 5058, loss 0.318137, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:22.023271: step 5059, loss 0.219852, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:22.131034: step 5060, loss 0.291779, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:22.238840: step 5061, loss 0.306723, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:22.353054: step 5062, loss 0.204575, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:22.463484: step 5063, loss 0.231397, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:22.571692: step 5064, loss 0.221201, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:22.681536: step 5065, loss 0.311743, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:22.791044: step 5066, loss 0.246167, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:22.899739: step 5067, loss 0.199243, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:23.005379: step 5068, loss 0.209923, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:23.115663: step 5069, loss 0.245306, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:23.225284: step 5070, loss 0.194857, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:23.337866: step 5071, loss 0.214004, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:23.447268: step 5072, loss 0.369512, acc 0.8125, learning_rate 0.0001
2017-10-09T15:16:23.555576: step 5073, loss 0.249756, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:23.659153: step 5074, loss 0.185513, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:23.764466: step 5075, loss 0.149178, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:23.873620: step 5076, loss 0.214918, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:23.981349: step 5077, loss 0.212889, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:24.089305: step 5078, loss 0.357425, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:24.195797: step 5079, loss 0.240784, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:24.303467: step 5080, loss 0.382768, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:24.563671: step 5080, loss 0.315672, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5080

2017-10-09T15:16:25.167029: step 5081, loss 0.267281, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:25.267840: step 5082, loss 0.23374, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:25.376937: step 5083, loss 0.254514, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:25.479330: step 5084, loss 0.20164, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:25.584361: step 5085, loss 0.254747, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:25.691110: step 5086, loss 0.219859, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:25.790563: step 5087, loss 0.168489, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:25.897262: step 5088, loss 0.228514, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:26.002538: step 5089, loss 0.226556, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:26.115168: step 5090, loss 0.232799, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:26.230247: step 5091, loss 0.386304, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:26.335562: step 5092, loss 0.23237, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:26.439487: step 5093, loss 0.238245, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:26.548503: step 5094, loss 0.128244, acc 0.984375, learning_rate 0.0001
2017-10-09T15:16:26.654346: step 5095, loss 0.17907, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:26.744681: step 5096, loss 0.386446, acc 0.823529, learning_rate 0.0001
2017-10-09T15:16:26.852782: step 5097, loss 0.346403, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:26.957512: step 5098, loss 0.208498, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:27.065446: step 5099, loss 0.14046, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:27.177303: step 5100, loss 0.16799, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:27.281027: step 5101, loss 0.294966, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:27.391670: step 5102, loss 0.196654, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:27.501257: step 5103, loss 0.406987, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:27.609807: step 5104, loss 0.143598, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:27.713218: step 5105, loss 0.141419, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:27.816769: step 5106, loss 0.363826, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:27.925391: step 5107, loss 0.177398, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:28.035322: step 5108, loss 0.224096, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:28.142325: step 5109, loss 0.222242, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:28.248761: step 5110, loss 0.416515, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:28.353769: step 5111, loss 0.511975, acc 0.828125, learning_rate 0.0001
2017-10-09T15:16:28.459138: step 5112, loss 0.238214, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:28.564513: step 5113, loss 0.2715, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:28.668934: step 5114, loss 0.258895, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:28.772830: step 5115, loss 0.226683, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:28.880194: step 5116, loss 0.287241, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:28.985658: step 5117, loss 0.241381, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:29.095081: step 5118, loss 0.20579, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:29.200789: step 5119, loss 0.388118, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:29.310486: step 5120, loss 0.377697, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:29.569036: step 5120, loss 0.312618, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5120

2017-10-09T15:16:30.150038: step 5121, loss 0.272816, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:30.254433: step 5122, loss 0.26129, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:30.359143: step 5123, loss 0.342284, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:30.466187: step 5124, loss 0.261637, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:30.576952: step 5125, loss 0.162436, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:30.686495: step 5126, loss 0.130289, acc 0.984375, learning_rate 0.0001
2017-10-09T15:16:30.794300: step 5127, loss 0.255693, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:30.905055: step 5128, loss 0.319567, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:31.011066: step 5129, loss 0.227462, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:31.112849: step 5130, loss 0.232288, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:31.219612: step 5131, loss 0.304084, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:31.324480: step 5132, loss 0.211904, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:31.428392: step 5133, loss 0.338932, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:31.530683: step 5134, loss 0.196077, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:31.636491: step 5135, loss 0.374883, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:31.742550: step 5136, loss 0.149553, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:31.849548: step 5137, loss 0.341316, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:31.957800: step 5138, loss 0.273925, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:32.061718: step 5139, loss 0.21546, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:32.168871: step 5140, loss 0.267003, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:32.270983: step 5141, loss 0.162534, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:32.380272: step 5142, loss 0.169049, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:32.490904: step 5143, loss 0.323415, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:32.590379: step 5144, loss 0.158565, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:32.692686: step 5145, loss 0.327903, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:32.797878: step 5146, loss 0.312351, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:32.904672: step 5147, loss 0.329639, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:33.011295: step 5148, loss 0.279195, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:33.118759: step 5149, loss 0.189832, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:33.227067: step 5150, loss 0.307633, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:33.331035: step 5151, loss 0.248473, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:33.434923: step 5152, loss 0.270527, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:33.542019: step 5153, loss 0.268967, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:33.645559: step 5154, loss 0.241277, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:33.750255: step 5155, loss 0.221314, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:33.859476: step 5156, loss 0.126592, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:33.974208: step 5157, loss 0.316926, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:34.080112: step 5158, loss 0.151486, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:34.217641: step 5159, loss 0.303248, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:34.323056: step 5160, loss 0.209113, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:34.578910: step 5160, loss 0.312201, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5160

2017-10-09T15:16:35.305081: step 5161, loss 0.245338, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:35.418797: step 5162, loss 0.184142, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:35.531973: step 5163, loss 0.376939, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:35.645574: step 5164, loss 0.30747, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:35.754793: step 5165, loss 0.299727, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:35.859805: step 5166, loss 0.225948, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:35.968535: step 5167, loss 0.267149, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:36.070764: step 5168, loss 0.27383, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:36.175646: step 5169, loss 0.460414, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:36.286292: step 5170, loss 0.222077, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:36.393432: step 5171, loss 0.314827, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:36.501827: step 5172, loss 0.247266, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:36.604825: step 5173, loss 0.310727, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:36.708914: step 5174, loss 0.289453, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:36.815225: step 5175, loss 0.266736, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:36.921095: step 5176, loss 0.124672, acc 1, learning_rate 0.0001
2017-10-09T15:16:37.023890: step 5177, loss 0.268274, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:37.131368: step 5178, loss 0.359438, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:37.240272: step 5179, loss 0.273062, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:37.347656: step 5180, loss 0.389876, acc 0.828125, learning_rate 0.0001
2017-10-09T15:16:37.452631: step 5181, loss 0.170957, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:37.554527: step 5182, loss 0.266133, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:37.655147: step 5183, loss 0.154798, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:37.760524: step 5184, loss 0.298996, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:37.869254: step 5185, loss 0.36859, acc 0.828125, learning_rate 0.0001
2017-10-09T15:16:37.975691: step 5186, loss 0.274077, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:38.077902: step 5187, loss 0.297837, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:38.184955: step 5188, loss 0.290802, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:38.291854: step 5189, loss 0.260902, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:38.398681: step 5190, loss 0.205795, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:38.504859: step 5191, loss 0.289026, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:38.616506: step 5192, loss 0.344905, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:38.745181: step 5193, loss 0.26064, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:38.838196: step 5194, loss 0.178309, acc 0.960784, learning_rate 0.0001
2017-10-09T15:16:38.958853: step 5195, loss 0.28724, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:39.084113: step 5196, loss 0.363387, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:39.204962: step 5197, loss 0.121577, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:39.322402: step 5198, loss 0.246778, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:39.442999: step 5199, loss 0.284147, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:39.566220: step 5200, loss 0.338944, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:39.831823: step 5200, loss 0.311821, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5200

2017-10-09T15:16:40.391219: step 5201, loss 0.389842, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:40.505835: step 5202, loss 0.33934, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:40.613422: step 5203, loss 0.150056, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:40.721405: step 5204, loss 0.138464, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:40.836691: step 5205, loss 0.325018, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:40.954582: step 5206, loss 0.159599, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:41.062876: step 5207, loss 0.37615, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:41.184446: step 5208, loss 0.286537, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:41.294817: step 5209, loss 0.202982, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:41.402453: step 5210, loss 0.169616, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:41.511707: step 5211, loss 0.167141, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:41.621928: step 5212, loss 0.2644, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:41.725526: step 5213, loss 0.284648, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:41.832740: step 5214, loss 0.342488, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:41.940518: step 5215, loss 0.18888, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:42.047017: step 5216, loss 0.291144, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:42.152596: step 5217, loss 0.296945, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:42.263104: step 5218, loss 0.241022, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:42.367935: step 5219, loss 0.141689, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:42.479146: step 5220, loss 0.158025, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:42.584938: step 5221, loss 0.18043, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:42.694631: step 5222, loss 0.180928, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:42.802343: step 5223, loss 0.352284, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:42.916080: step 5224, loss 0.283585, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:43.043269: step 5225, loss 0.204398, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:43.172953: step 5226, loss 0.224755, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:43.299179: step 5227, loss 0.15864, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:43.411660: step 5228, loss 0.182633, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:43.537660: step 5229, loss 0.341271, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:43.660103: step 5230, loss 0.281721, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:43.775817: step 5231, loss 0.401137, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:43.894589: step 5232, loss 0.20687, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:44.012835: step 5233, loss 0.26735, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:44.139880: step 5234, loss 0.297858, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:44.244797: step 5235, loss 0.302512, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:44.349916: step 5236, loss 0.288806, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:44.456325: step 5237, loss 0.279022, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:44.565275: step 5238, loss 0.290035, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:44.671009: step 5239, loss 0.332848, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:44.776825: step 5240, loss 0.195318, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:45.038458: step 5240, loss 0.311793, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5240

2017-10-09T15:16:45.652019: step 5241, loss 0.329944, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:45.766144: step 5242, loss 0.2604, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:45.877814: step 5243, loss 0.358376, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:45.987472: step 5244, loss 0.162383, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:46.098113: step 5245, loss 0.189208, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:46.207923: step 5246, loss 0.200496, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:46.311760: step 5247, loss 0.309142, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:46.417451: step 5248, loss 0.184804, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:46.522404: step 5249, loss 0.2298, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:46.631081: step 5250, loss 0.505179, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:46.737917: step 5251, loss 0.310596, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:46.840261: step 5252, loss 0.134676, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:46.967149: step 5253, loss 0.202301, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:47.072263: step 5254, loss 0.229538, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:47.189594: step 5255, loss 0.246471, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:47.302362: step 5256, loss 0.400678, acc 0.828125, learning_rate 0.0001
2017-10-09T15:16:47.413373: step 5257, loss 0.238591, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:47.522630: step 5258, loss 0.139013, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:47.630200: step 5259, loss 0.339281, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:47.736708: step 5260, loss 0.296744, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:47.846273: step 5261, loss 0.25642, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:47.951538: step 5262, loss 0.236198, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:48.058541: step 5263, loss 0.308396, acc 0.828125, learning_rate 0.0001
2017-10-09T15:16:48.166885: step 5264, loss 0.274516, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:48.270912: step 5265, loss 0.235039, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:48.376772: step 5266, loss 0.304967, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:48.483851: step 5267, loss 0.136173, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:48.591057: step 5268, loss 0.25653, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:48.699829: step 5269, loss 0.171142, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:48.808131: step 5270, loss 0.289028, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:48.917966: step 5271, loss 0.20353, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:49.024875: step 5272, loss 0.236108, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:49.128485: step 5273, loss 0.338861, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:49.236489: step 5274, loss 0.301557, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:49.344892: step 5275, loss 0.339786, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:49.450532: step 5276, loss 0.181045, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:49.558648: step 5277, loss 0.194327, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:49.666687: step 5278, loss 0.18407, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:49.774387: step 5279, loss 0.13995, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:49.881425: step 5280, loss 0.284449, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:50.135393: step 5280, loss 0.311859, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5280

2017-10-09T15:16:50.719308: step 5281, loss 0.310802, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:50.824782: step 5282, loss 0.207245, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:50.932404: step 5283, loss 0.204344, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:51.038729: step 5284, loss 0.200507, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:51.139269: step 5285, loss 0.313284, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:51.242096: step 5286, loss 0.232249, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:51.346913: step 5287, loss 0.304859, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:51.454491: step 5288, loss 0.236526, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:51.559062: step 5289, loss 0.26126, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:51.673902: step 5290, loss 0.235971, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:51.781358: step 5291, loss 0.157044, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:51.875682: step 5292, loss 0.137375, acc 0.941176, learning_rate 0.0001
2017-10-09T15:16:51.984492: step 5293, loss 0.1477, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:52.090812: step 5294, loss 0.299268, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:52.195071: step 5295, loss 0.236682, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:52.298949: step 5296, loss 0.242709, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:52.405116: step 5297, loss 0.361101, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:52.509415: step 5298, loss 0.185904, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:52.617396: step 5299, loss 0.230213, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:52.732629: step 5300, loss 0.148183, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:52.850883: step 5301, loss 0.264992, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:52.956627: step 5302, loss 0.249061, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:53.064625: step 5303, loss 0.419704, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:53.170177: step 5304, loss 0.262222, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:53.274095: step 5305, loss 0.210297, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:53.379585: step 5306, loss 0.255646, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:53.488403: step 5307, loss 0.405627, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:53.599959: step 5308, loss 0.341926, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:53.707599: step 5309, loss 0.399225, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:53.815654: step 5310, loss 0.231906, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:53.926683: step 5311, loss 0.197712, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:54.034406: step 5312, loss 0.20806, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:54.139920: step 5313, loss 0.226254, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:54.241875: step 5314, loss 0.138762, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:54.347169: step 5315, loss 0.361875, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:54.458814: step 5316, loss 0.179435, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:54.563957: step 5317, loss 0.304306, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:54.669708: step 5318, loss 0.318162, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:54.772503: step 5319, loss 0.260538, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:54.880868: step 5320, loss 0.242032, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:16:55.139185: step 5320, loss 0.309334, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5320

2017-10-09T15:16:55.802814: step 5321, loss 0.164723, acc 0.96875, learning_rate 0.0001
2017-10-09T15:16:55.904416: step 5322, loss 0.219495, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:56.009850: step 5323, loss 0.147376, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:56.116284: step 5324, loss 0.326781, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:56.224715: step 5325, loss 0.285553, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:56.328859: step 5326, loss 0.304559, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:56.436074: step 5327, loss 0.252967, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:56.543203: step 5328, loss 0.431673, acc 0.84375, learning_rate 0.0001
2017-10-09T15:16:56.647596: step 5329, loss 0.130679, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:56.752955: step 5330, loss 0.280355, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:56.862423: step 5331, loss 0.233255, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:56.969202: step 5332, loss 0.107777, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:57.072016: step 5333, loss 0.256363, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:57.174218: step 5334, loss 0.254635, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:57.276605: step 5335, loss 0.234301, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:57.385643: step 5336, loss 0.271162, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:57.511044: step 5337, loss 0.25289, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:57.640985: step 5338, loss 0.252785, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:57.759858: step 5339, loss 0.146049, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:57.883262: step 5340, loss 0.218306, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:57.991203: step 5341, loss 0.266648, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:58.096229: step 5342, loss 0.307299, acc 0.875, learning_rate 0.0001
2017-10-09T15:16:58.203589: step 5343, loss 0.308868, acc 0.859375, learning_rate 0.0001
2017-10-09T15:16:58.310975: step 5344, loss 0.231963, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:58.420431: step 5345, loss 0.189199, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:58.526993: step 5346, loss 0.15819, acc 0.953125, learning_rate 0.0001
2017-10-09T15:16:58.628673: step 5347, loss 0.272575, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:58.729283: step 5348, loss 0.298062, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:58.835789: step 5349, loss 0.236748, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:58.944165: step 5350, loss 0.239579, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:59.050148: step 5351, loss 0.251833, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:59.158109: step 5352, loss 0.285403, acc 0.890625, learning_rate 0.0001
2017-10-09T15:16:59.283991: step 5353, loss 0.241578, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:59.407189: step 5354, loss 0.252785, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:59.525368: step 5355, loss 0.206497, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:59.644831: step 5356, loss 0.234799, acc 0.90625, learning_rate 0.0001
2017-10-09T15:16:59.752002: step 5357, loss 0.261094, acc 0.921875, learning_rate 0.0001
2017-10-09T15:16:59.858007: step 5358, loss 0.180057, acc 0.9375, learning_rate 0.0001
2017-10-09T15:16:59.964145: step 5359, loss 0.363901, acc 0.84375, learning_rate 0.0001
2017-10-09T15:17:00.067356: step 5360, loss 0.202363, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:00.319118: step 5360, loss 0.309758, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5360

2017-10-09T15:17:00.842808: step 5361, loss 0.185235, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:00.946255: step 5362, loss 0.209021, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:01.051509: step 5363, loss 0.254681, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:01.163136: step 5364, loss 0.231747, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:01.271254: step 5365, loss 0.311278, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:01.376368: step 5366, loss 0.409041, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:01.483213: step 5367, loss 0.205223, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:01.590666: step 5368, loss 0.262118, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:01.695880: step 5369, loss 0.179108, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:01.803038: step 5370, loss 0.196814, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:01.904775: step 5371, loss 0.177786, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:02.011618: step 5372, loss 0.317393, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:02.112511: step 5373, loss 0.25033, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:02.215855: step 5374, loss 0.250149, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:02.316691: step 5375, loss 0.365802, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:02.422636: step 5376, loss 0.361378, acc 0.84375, learning_rate 0.0001
2017-10-09T15:17:02.529317: step 5377, loss 0.24083, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:02.633594: step 5378, loss 0.166031, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:02.737741: step 5379, loss 0.313152, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:02.843316: step 5380, loss 0.313167, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:02.946739: step 5381, loss 0.187368, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:03.053566: step 5382, loss 0.331888, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:03.157987: step 5383, loss 0.172823, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:03.265926: step 5384, loss 0.337362, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:03.372009: step 5385, loss 0.328897, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:03.473090: step 5386, loss 0.184099, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:03.577504: step 5387, loss 0.262837, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:03.685288: step 5388, loss 0.290729, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:03.792142: step 5389, loss 0.171917, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:03.878599: step 5390, loss 0.301177, acc 0.882353, learning_rate 0.0001
2017-10-09T15:17:03.984781: step 5391, loss 0.20315, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:04.090183: step 5392, loss 0.341795, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:04.194544: step 5393, loss 0.268423, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:04.302078: step 5394, loss 0.176016, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:04.407433: step 5395, loss 0.224739, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:04.512593: step 5396, loss 0.141512, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:04.622514: step 5397, loss 0.23705, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:04.735362: step 5398, loss 0.223198, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:04.847807: step 5399, loss 0.187817, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:04.953139: step 5400, loss 0.139219, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:05.205401: step 5400, loss 0.309318, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5400

2017-10-09T15:17:05.809958: step 5401, loss 0.441979, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:05.931953: step 5402, loss 0.282219, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:06.053480: step 5403, loss 0.194922, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:06.159631: step 5404, loss 0.256227, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:06.263727: step 5405, loss 0.19676, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:06.369232: step 5406, loss 0.263113, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:06.473312: step 5407, loss 0.169081, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:06.580590: step 5408, loss 0.308189, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:06.687980: step 5409, loss 0.234922, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:06.794384: step 5410, loss 0.255962, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:06.913305: step 5411, loss 0.281492, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:07.034091: step 5412, loss 0.225422, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:07.151199: step 5413, loss 0.365046, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:07.254796: step 5414, loss 0.198827, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:07.361423: step 5415, loss 0.290436, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:07.467622: step 5416, loss 0.237439, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:07.574153: step 5417, loss 0.298236, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:07.680958: step 5418, loss 0.263781, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:07.786655: step 5419, loss 0.175823, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:07.893145: step 5420, loss 0.195081, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:07.999561: step 5421, loss 0.207501, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:08.107050: step 5422, loss 0.351353, acc 0.84375, learning_rate 0.0001
2017-10-09T15:17:08.210716: step 5423, loss 0.21708, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:08.316485: step 5424, loss 0.192931, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:08.422512: step 5425, loss 0.290047, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:08.525977: step 5426, loss 0.342671, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:08.630963: step 5427, loss 0.169607, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:08.736309: step 5428, loss 0.284801, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:08.838249: step 5429, loss 0.297637, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:08.943855: step 5430, loss 0.390783, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:09.051533: step 5431, loss 0.14895, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:09.153889: step 5432, loss 0.148978, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:09.259854: step 5433, loss 0.230237, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:09.365590: step 5434, loss 0.312525, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:09.468241: step 5435, loss 0.263312, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:09.574259: step 5436, loss 0.254955, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:09.682084: step 5437, loss 0.267365, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:09.788717: step 5438, loss 0.158528, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:09.896806: step 5439, loss 0.219776, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:10.002655: step 5440, loss 0.286236, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:10.252352: step 5440, loss 0.306787, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5440

2017-10-09T15:17:10.835508: step 5441, loss 0.239731, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:10.941738: step 5442, loss 0.358982, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:11.047276: step 5443, loss 0.273846, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:11.154856: step 5444, loss 0.280531, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:11.256177: step 5445, loss 0.217825, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:11.363080: step 5446, loss 0.198176, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:11.469529: step 5447, loss 0.251, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:11.578785: step 5448, loss 0.210248, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:11.683820: step 5449, loss 0.242413, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:11.789590: step 5450, loss 0.204369, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:11.897873: step 5451, loss 0.229393, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:12.003937: step 5452, loss 0.193677, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:12.111928: step 5453, loss 0.138118, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:12.222073: step 5454, loss 0.200828, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:12.325637: step 5455, loss 0.165721, acc 0.984375, learning_rate 0.0001
2017-10-09T15:17:12.432224: step 5456, loss 0.158723, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:12.539463: step 5457, loss 0.205028, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:12.648356: step 5458, loss 0.22604, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:12.755082: step 5459, loss 0.206686, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:12.864178: step 5460, loss 0.215273, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:12.968341: step 5461, loss 0.138251, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:13.082756: step 5462, loss 0.239627, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:13.194838: step 5463, loss 0.225785, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:13.301746: step 5464, loss 0.303497, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:13.406984: step 5465, loss 0.300714, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:13.511164: step 5466, loss 0.221598, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:13.609073: step 5467, loss 0.192089, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:13.715512: step 5468, loss 0.289102, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:13.824964: step 5469, loss 0.275153, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:13.930199: step 5470, loss 0.277419, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:14.036789: step 5471, loss 0.214939, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:14.144608: step 5472, loss 0.206529, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:14.249025: step 5473, loss 0.247792, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:14.352433: step 5474, loss 0.212787, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:14.456678: step 5475, loss 0.281689, acc 0.84375, learning_rate 0.0001
2017-10-09T15:17:14.558822: step 5476, loss 0.287968, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:14.663816: step 5477, loss 0.216897, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:14.771404: step 5478, loss 0.371632, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:14.877336: step 5479, loss 0.244358, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:14.984292: step 5480, loss 0.098144, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:15.242242: step 5480, loss 0.308924, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5480

2017-10-09T15:17:15.915748: step 5481, loss 0.251315, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:16.019163: step 5482, loss 0.25134, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:16.123419: step 5483, loss 0.18367, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:16.230053: step 5484, loss 0.161039, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:16.336738: step 5485, loss 0.192961, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:16.446751: step 5486, loss 0.186933, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:16.555707: step 5487, loss 0.224583, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:16.644264: step 5488, loss 0.296113, acc 0.941176, learning_rate 0.0001
2017-10-09T15:17:16.748043: step 5489, loss 0.346753, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:16.855005: step 5490, loss 0.211441, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:16.959153: step 5491, loss 0.297523, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:17.067563: step 5492, loss 0.143027, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:17.171576: step 5493, loss 0.223641, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:17.277715: step 5494, loss 0.221007, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:17.379822: step 5495, loss 0.166904, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:17.484119: step 5496, loss 0.164495, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:17.590649: step 5497, loss 0.184664, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:17.695399: step 5498, loss 0.282593, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:17.795434: step 5499, loss 0.222106, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:17.903709: step 5500, loss 0.161801, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:18.013767: step 5501, loss 0.348058, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:18.134321: step 5502, loss 0.206444, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:18.254761: step 5503, loss 0.206722, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:18.378681: step 5504, loss 0.255839, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:18.502640: step 5505, loss 0.16513, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:18.613396: step 5506, loss 0.133967, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:18.719162: step 5507, loss 0.214334, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:18.823172: step 5508, loss 0.235365, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:18.934305: step 5509, loss 0.284566, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:19.045814: step 5510, loss 0.26233, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:19.150051: step 5511, loss 0.229056, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:19.258815: step 5512, loss 0.219721, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:19.371367: step 5513, loss 0.279102, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:19.483015: step 5514, loss 0.457876, acc 0.828125, learning_rate 0.0001
2017-10-09T15:17:19.588765: step 5515, loss 0.417545, acc 0.84375, learning_rate 0.0001
2017-10-09T15:17:19.695162: step 5516, loss 0.173842, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:19.801747: step 5517, loss 0.169578, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:19.910729: step 5518, loss 0.208515, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:20.017485: step 5519, loss 0.200724, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:20.123036: step 5520, loss 0.168573, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:20.393004: step 5520, loss 0.3049, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5520

2017-10-09T15:17:20.928429: step 5521, loss 0.20359, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:21.039144: step 5522, loss 0.266157, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:21.149739: step 5523, loss 0.172091, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:21.259300: step 5524, loss 0.274491, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:21.366766: step 5525, loss 0.271457, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:21.480316: step 5526, loss 0.161735, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:21.592767: step 5527, loss 0.22877, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:21.693171: step 5528, loss 0.255142, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:21.799873: step 5529, loss 0.149948, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:21.900436: step 5530, loss 0.303025, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:22.006847: step 5531, loss 0.246164, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:22.112128: step 5532, loss 0.26169, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:22.219725: step 5533, loss 0.175739, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:22.321329: step 5534, loss 0.197703, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:22.427800: step 5535, loss 0.324458, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:22.532739: step 5536, loss 0.221637, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:22.641765: step 5537, loss 0.350941, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:22.746052: step 5538, loss 0.126871, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:22.856244: step 5539, loss 0.291183, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:22.964954: step 5540, loss 0.255572, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:23.074081: step 5541, loss 0.211526, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:23.180967: step 5542, loss 0.28212, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:23.287345: step 5543, loss 0.206249, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:23.395523: step 5544, loss 0.215325, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:23.501469: step 5545, loss 0.274974, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:23.604633: step 5546, loss 0.266823, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:23.712564: step 5547, loss 0.1899, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:23.816121: step 5548, loss 0.429686, acc 0.84375, learning_rate 0.0001
2017-10-09T15:17:23.923485: step 5549, loss 0.337097, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:24.029987: step 5550, loss 0.164634, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:24.143058: step 5551, loss 0.207412, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:24.256704: step 5552, loss 0.207, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:24.372880: step 5553, loss 0.261568, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:24.488279: step 5554, loss 0.357122, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:24.602924: step 5555, loss 0.247595, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:24.703542: step 5556, loss 0.259317, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:24.806461: step 5557, loss 0.257406, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:24.915382: step 5558, loss 0.143192, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:25.021614: step 5559, loss 0.21857, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:25.125361: step 5560, loss 0.348356, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:25.379744: step 5560, loss 0.309849, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5560

2017-10-09T15:17:26.026183: step 5561, loss 0.238254, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:26.133116: step 5562, loss 0.190139, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:26.240293: step 5563, loss 0.311172, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:26.364393: step 5564, loss 0.190156, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:26.468366: step 5565, loss 0.176385, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:26.573923: step 5566, loss 0.246471, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:26.685330: step 5567, loss 0.308714, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:26.796669: step 5568, loss 0.160404, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:26.902329: step 5569, loss 0.22346, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:27.008862: step 5570, loss 0.18346, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:27.118173: step 5571, loss 0.189091, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:27.220792: step 5572, loss 0.180937, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:27.328441: step 5573, loss 0.379974, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:27.437094: step 5574, loss 0.258969, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:27.560460: step 5575, loss 0.179084, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:27.663607: step 5576, loss 0.23455, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:27.771129: step 5577, loss 0.283051, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:27.879516: step 5578, loss 0.214125, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:27.986109: step 5579, loss 0.21056, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:28.091981: step 5580, loss 0.217177, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:28.193673: step 5581, loss 0.127629, acc 0.984375, learning_rate 0.0001
2017-10-09T15:17:28.301198: step 5582, loss 0.240453, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:28.406795: step 5583, loss 0.217236, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:28.512928: step 5584, loss 0.214757, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:28.617246: step 5585, loss 0.137898, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:28.705610: step 5586, loss 0.279693, acc 0.901961, learning_rate 0.0001
2017-10-09T15:17:28.809137: step 5587, loss 0.167502, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:28.916958: step 5588, loss 0.178137, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:29.024405: step 5589, loss 0.304511, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:29.131233: step 5590, loss 0.276381, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:29.236499: step 5591, loss 0.259234, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:29.340417: step 5592, loss 0.185956, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:29.448669: step 5593, loss 0.228428, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:29.553261: step 5594, loss 0.161643, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:29.661771: step 5595, loss 0.164015, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:29.769619: step 5596, loss 0.365981, acc 0.828125, learning_rate 0.0001
2017-10-09T15:17:29.880805: step 5597, loss 0.187279, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:29.987501: step 5598, loss 0.311787, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:30.094700: step 5599, loss 0.200235, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:30.203041: step 5600, loss 0.309661, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:30.459248: step 5600, loss 0.306042, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5600

2017-10-09T15:17:31.039338: step 5601, loss 0.230421, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:31.144386: step 5602, loss 0.257594, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:31.254756: step 5603, loss 0.365817, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:31.360429: step 5604, loss 0.233927, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:31.465454: step 5605, loss 0.191817, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:31.568761: step 5606, loss 0.230806, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:31.672924: step 5607, loss 0.265297, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:31.782131: step 5608, loss 0.305709, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:31.890868: step 5609, loss 0.229543, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:32.001057: step 5610, loss 0.331784, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:32.107553: step 5611, loss 0.199868, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:32.222715: step 5612, loss 0.370248, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:32.344941: step 5613, loss 0.315507, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:32.470070: step 5614, loss 0.228419, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:32.594946: step 5615, loss 0.212277, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:32.700361: step 5616, loss 0.244272, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:32.807043: step 5617, loss 0.311716, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:32.913580: step 5618, loss 0.288448, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:33.021473: step 5619, loss 0.245834, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:33.125458: step 5620, loss 0.214661, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:33.225683: step 5621, loss 0.180196, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:33.329334: step 5622, loss 0.317629, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:33.435343: step 5623, loss 0.168778, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:33.542978: step 5624, loss 0.174343, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:33.649974: step 5625, loss 0.308509, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:33.751223: step 5626, loss 0.281819, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:33.857408: step 5627, loss 0.167629, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:33.961066: step 5628, loss 0.125678, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:34.064679: step 5629, loss 0.407577, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:34.188856: step 5630, loss 0.210014, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:34.315172: step 5631, loss 0.252189, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:34.434311: step 5632, loss 0.360869, acc 0.828125, learning_rate 0.0001
2017-10-09T15:17:34.540748: step 5633, loss 0.206445, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:34.646208: step 5634, loss 0.22447, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:34.749992: step 5635, loss 0.22816, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:34.855694: step 5636, loss 0.183315, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:34.963724: step 5637, loss 0.183585, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:35.071696: step 5638, loss 0.148612, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:35.177147: step 5639, loss 0.151846, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:35.280895: step 5640, loss 0.345718, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:35.536678: step 5640, loss 0.305221, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5640

2017-10-09T15:17:36.200468: step 5641, loss 0.203368, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:36.307882: step 5642, loss 0.216302, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:36.412364: step 5643, loss 0.255365, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:36.541635: step 5644, loss 0.253708, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:36.660089: step 5645, loss 0.214768, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:36.770657: step 5646, loss 0.230357, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:36.878450: step 5647, loss 0.186844, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:36.985041: step 5648, loss 0.143843, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:37.093429: step 5649, loss 0.235156, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:37.202138: step 5650, loss 0.201487, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:37.305833: step 5651, loss 0.143363, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:37.418839: step 5652, loss 0.212953, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:37.525687: step 5653, loss 0.166946, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:37.635824: step 5654, loss 0.36093, acc 0.84375, learning_rate 0.0001
2017-10-09T15:17:37.737152: step 5655, loss 0.246352, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:37.842646: step 5656, loss 0.136044, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:37.956781: step 5657, loss 0.0933573, acc 0.984375, learning_rate 0.0001
2017-10-09T15:17:38.062934: step 5658, loss 0.220352, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:38.165024: step 5659, loss 0.250721, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:38.279438: step 5660, loss 0.172286, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:38.392122: step 5661, loss 0.19669, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:38.498849: step 5662, loss 0.146331, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:38.602460: step 5663, loss 0.196626, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:38.706798: step 5664, loss 0.188179, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:38.817046: step 5665, loss 0.314, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:38.920738: step 5666, loss 0.179769, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:39.030222: step 5667, loss 0.155066, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:39.137269: step 5668, loss 0.274146, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:39.239589: step 5669, loss 0.162502, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:39.347268: step 5670, loss 0.269843, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:39.457501: step 5671, loss 0.213962, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:39.563574: step 5672, loss 0.254549, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:39.671263: step 5673, loss 0.238735, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:39.795767: step 5674, loss 0.096744, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:39.928584: step 5675, loss 0.239241, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:40.043685: step 5676, loss 0.225769, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:40.164420: step 5677, loss 0.194009, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:40.276896: step 5678, loss 0.305918, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:40.384451: step 5679, loss 0.184661, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:40.485682: step 5680, loss 0.389822, acc 0.8125, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:40.741539: step 5680, loss 0.305992, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5680

2017-10-09T15:17:41.260022: step 5681, loss 0.165125, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:41.367725: step 5682, loss 0.325868, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:41.475565: step 5683, loss 0.353078, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:41.564381: step 5684, loss 0.258865, acc 0.921569, learning_rate 0.0001
2017-10-09T15:17:41.670728: step 5685, loss 0.229257, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:41.779280: step 5686, loss 0.199198, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:41.882233: step 5687, loss 0.202625, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:41.990458: step 5688, loss 0.350858, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:42.094521: step 5689, loss 0.260125, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:42.198188: step 5690, loss 0.195706, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:42.304213: step 5691, loss 0.410278, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:42.412866: step 5692, loss 0.191696, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:42.521862: step 5693, loss 0.260805, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:42.636479: step 5694, loss 0.345516, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:42.759850: step 5695, loss 0.193617, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:42.879896: step 5696, loss 0.108521, acc 0.984375, learning_rate 0.0001
2017-10-09T15:17:42.986084: step 5697, loss 0.228529, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:43.094513: step 5698, loss 0.150935, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:43.199097: step 5699, loss 0.313877, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:43.315336: step 5700, loss 0.214087, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:43.439120: step 5701, loss 0.184502, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:43.552164: step 5702, loss 0.226828, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:43.659862: step 5703, loss 0.298429, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:43.767614: step 5704, loss 0.116438, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:43.890146: step 5705, loss 0.169414, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:44.011307: step 5706, loss 0.347167, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:44.121939: step 5707, loss 0.285786, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:44.245144: step 5708, loss 0.207633, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:44.365208: step 5709, loss 0.25935, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:44.470739: step 5710, loss 0.179856, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:44.577006: step 5711, loss 0.231661, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:44.683914: step 5712, loss 0.287489, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:44.796196: step 5713, loss 0.189764, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:44.915013: step 5714, loss 0.237112, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:45.035134: step 5715, loss 0.177451, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:45.140854: step 5716, loss 0.363571, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:45.249429: step 5717, loss 0.355123, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:45.352832: step 5718, loss 0.313034, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:45.472558: step 5719, loss 0.185627, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:45.590491: step 5720, loss 0.256055, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:45.852694: step 5720, loss 0.304845, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5720

2017-10-09T15:17:46.458243: step 5721, loss 0.210245, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:46.562796: step 5722, loss 0.142515, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:46.668716: step 5723, loss 0.252158, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:46.775127: step 5724, loss 0.230728, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:46.888688: step 5725, loss 0.229539, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:47.003277: step 5726, loss 0.47311, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:47.113384: step 5727, loss 0.180016, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:47.222274: step 5728, loss 0.239148, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:47.327459: step 5729, loss 0.178563, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:47.439505: step 5730, loss 0.218334, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:47.547437: step 5731, loss 0.198553, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:47.654471: step 5732, loss 0.137448, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:47.762221: step 5733, loss 0.262383, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:47.875603: step 5734, loss 0.182257, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:47.982845: step 5735, loss 0.295909, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:48.090435: step 5736, loss 0.164488, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:48.200153: step 5737, loss 0.338477, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:48.313597: step 5738, loss 0.2611, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:48.423974: step 5739, loss 0.145926, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:48.535761: step 5740, loss 0.167945, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:48.647277: step 5741, loss 0.250679, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:48.758593: step 5742, loss 0.47342, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:48.890421: step 5743, loss 0.0952546, acc 0.984375, learning_rate 0.0001
2017-10-09T15:17:49.029391: step 5744, loss 0.388928, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:49.167116: step 5745, loss 0.358384, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:49.284356: step 5746, loss 0.357155, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:49.396302: step 5747, loss 0.156183, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:49.504194: step 5748, loss 0.291606, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:49.619212: step 5749, loss 0.551484, acc 0.765625, learning_rate 0.0001
2017-10-09T15:17:49.730615: step 5750, loss 0.109823, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:49.847937: step 5751, loss 0.208463, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:49.961304: step 5752, loss 0.158136, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:50.079652: step 5753, loss 0.266123, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:50.198035: step 5754, loss 0.237137, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:50.305905: step 5755, loss 0.201074, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:50.413195: step 5756, loss 0.19252, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:50.526598: step 5757, loss 0.224175, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:50.636063: step 5758, loss 0.15972, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:50.744185: step 5759, loss 0.280972, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:50.848885: step 5760, loss 0.290048, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:51.120240: step 5760, loss 0.306795, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5760

2017-10-09T15:17:51.723632: step 5761, loss 0.355995, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:51.828007: step 5762, loss 0.236686, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:51.938791: step 5763, loss 0.121607, acc 0.984375, learning_rate 0.0001
2017-10-09T15:17:52.040805: step 5764, loss 0.212323, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:52.147712: step 5765, loss 0.190595, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:52.252950: step 5766, loss 0.1939, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:52.358394: step 5767, loss 0.254278, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:52.461949: step 5768, loss 0.210805, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:52.570784: step 5769, loss 0.221467, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:52.670918: step 5770, loss 0.306808, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:52.778573: step 5771, loss 0.267271, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:52.888479: step 5772, loss 0.203353, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:52.994694: step 5773, loss 0.308279, acc 0.875, learning_rate 0.0001
2017-10-09T15:17:53.102546: step 5774, loss 0.41176, acc 0.84375, learning_rate 0.0001
2017-10-09T15:17:53.209191: step 5775, loss 0.243547, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:53.317319: step 5776, loss 0.29178, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:53.423806: step 5777, loss 0.195343, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:53.531473: step 5778, loss 0.264311, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:53.637488: step 5779, loss 0.201268, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:53.740903: step 5780, loss 0.134087, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:53.847365: step 5781, loss 0.188469, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:53.939694: step 5782, loss 0.346691, acc 0.921569, learning_rate 0.0001
2017-10-09T15:17:54.057929: step 5783, loss 0.104158, acc 1, learning_rate 0.0001
2017-10-09T15:17:54.167003: step 5784, loss 0.238972, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:54.276310: step 5785, loss 0.226333, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:54.386061: step 5786, loss 0.158346, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:54.493670: step 5787, loss 0.409857, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:54.606794: step 5788, loss 0.170637, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:54.721526: step 5789, loss 0.206658, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:54.829169: step 5790, loss 0.276896, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:54.934071: step 5791, loss 0.223001, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:55.042703: step 5792, loss 0.296518, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:55.177483: step 5793, loss 0.176516, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:55.294701: step 5794, loss 0.238083, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:55.405838: step 5795, loss 0.12793, acc 0.96875, learning_rate 0.0001
2017-10-09T15:17:55.519050: step 5796, loss 0.198706, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:55.625804: step 5797, loss 0.181523, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:55.744714: step 5798, loss 0.259019, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:55.861667: step 5799, loss 0.271827, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:55.975149: step 5800, loss 0.2798, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-09T15:17:56.251803: step 5800, loss 0.304548, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5800

2017-10-09T15:17:56.942678: step 5801, loss 0.192354, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:57.045574: step 5802, loss 0.385452, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:57.152717: step 5803, loss 0.363191, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:57.258679: step 5804, loss 0.166627, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:57.365121: step 5805, loss 0.198762, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:57.472173: step 5806, loss 0.347109, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:57.577765: step 5807, loss 0.1754, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:57.684351: step 5808, loss 0.218322, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:57.789503: step 5809, loss 0.188838, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:57.897414: step 5810, loss 0.21557, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:57.998863: step 5811, loss 0.244792, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:58.108020: step 5812, loss 0.162305, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:58.216208: step 5813, loss 0.220196, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:58.318383: step 5814, loss 0.211407, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:58.435023: step 5815, loss 0.146371, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:58.547186: step 5816, loss 0.252482, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:58.674561: step 5817, loss 0.223519, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:58.779080: step 5818, loss 0.219523, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:58.892702: step 5819, loss 0.180359, acc 0.953125, learning_rate 0.0001
2017-10-09T15:17:58.998728: step 5820, loss 0.303777, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:59.099834: step 5821, loss 0.216817, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:59.204112: step 5822, loss 0.317976, acc 0.890625, learning_rate 0.0001
2017-10-09T15:17:59.322098: step 5823, loss 0.242662, acc 0.9375, learning_rate 0.0001
2017-10-09T15:17:59.434688: step 5824, loss 0.270006, acc 0.921875, learning_rate 0.0001
2017-10-09T15:17:59.549740: step 5825, loss 0.184266, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:59.667041: step 5826, loss 0.280907, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:59.779396: step 5827, loss 0.27119, acc 0.859375, learning_rate 0.0001
2017-10-09T15:17:59.891173: step 5828, loss 0.240188, acc 0.90625, learning_rate 0.0001
2017-10-09T15:17:59.997435: step 5829, loss 0.186231, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:00.101923: step 5830, loss 0.202784, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:00.209142: step 5831, loss 0.426929, acc 0.84375, learning_rate 0.0001
2017-10-09T15:18:00.317575: step 5832, loss 0.174082, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:00.427275: step 5833, loss 0.254835, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:00.538906: step 5834, loss 0.210161, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:00.645961: step 5835, loss 0.284684, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:00.755158: step 5836, loss 0.146017, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:00.862138: step 5837, loss 0.203843, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:00.965993: step 5838, loss 0.202539, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:01.072230: step 5839, loss 0.133874, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:01.181928: step 5840, loss 0.189838, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:01.442597: step 5840, loss 0.30186, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5840

2017-10-09T15:18:01.979734: step 5841, loss 0.315678, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:02.086190: step 5842, loss 0.271766, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:02.196608: step 5843, loss 0.31155, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:02.307553: step 5844, loss 0.166555, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:02.414285: step 5845, loss 0.271685, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:02.520552: step 5846, loss 0.320517, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:02.629027: step 5847, loss 0.175081, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:02.734524: step 5848, loss 0.318939, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:02.843636: step 5849, loss 0.105865, acc 1, learning_rate 0.0001
2017-10-09T15:18:02.950726: step 5850, loss 0.131509, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:03.059038: step 5851, loss 0.157568, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:03.161961: step 5852, loss 0.220861, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:03.266722: step 5853, loss 0.133768, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:03.371948: step 5854, loss 0.277362, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:03.483156: step 5855, loss 0.209812, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:03.588786: step 5856, loss 0.136892, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:03.692342: step 5857, loss 0.223871, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:03.798692: step 5858, loss 0.248829, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:03.905305: step 5859, loss 0.332671, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:04.015412: step 5860, loss 0.202289, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:04.116621: step 5861, loss 0.226262, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:04.219427: step 5862, loss 0.204052, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:04.324533: step 5863, loss 0.257248, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:04.431588: step 5864, loss 0.195841, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:04.533129: step 5865, loss 0.292057, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:04.663432: step 5866, loss 0.359214, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:04.781122: step 5867, loss 0.181049, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:04.903157: step 5868, loss 0.274318, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:05.017857: step 5869, loss 0.153998, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:05.124047: step 5870, loss 0.299786, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:05.237278: step 5871, loss 0.17894, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:05.344224: step 5872, loss 0.228142, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:05.453040: step 5873, loss 0.269254, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:05.556660: step 5874, loss 0.312235, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:05.682951: step 5875, loss 0.236615, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:05.815041: step 5876, loss 0.296623, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:05.931312: step 5877, loss 0.262747, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:06.046644: step 5878, loss 0.29927, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:06.161686: step 5879, loss 0.277445, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:06.263218: step 5880, loss 0.266404, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:06.552488: step 5880, loss 0.304037, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5880

2017-10-09T15:18:07.165277: step 5881, loss 0.332671, acc 0.84375, learning_rate 0.0001
2017-10-09T15:18:07.271540: step 5882, loss 0.179357, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:07.378169: step 5883, loss 0.216242, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:07.484803: step 5884, loss 0.296017, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:07.590821: step 5885, loss 0.0993115, acc 1, learning_rate 0.0001
2017-10-09T15:18:07.699433: step 5886, loss 0.268122, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:07.806989: step 5887, loss 0.360185, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:07.916812: step 5888, loss 0.195823, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:08.020432: step 5889, loss 0.218775, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:08.127532: step 5890, loss 0.333948, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:08.236047: step 5891, loss 0.227375, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:08.342554: step 5892, loss 0.135576, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:08.448549: step 5893, loss 0.296848, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:08.555523: step 5894, loss 0.207168, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:08.668321: step 5895, loss 0.258501, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:08.769863: step 5896, loss 0.294782, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:08.881898: step 5897, loss 0.202574, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:09.007245: step 5898, loss 0.224796, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:09.112423: step 5899, loss 0.347469, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:09.217907: step 5900, loss 0.220024, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:09.326340: step 5901, loss 0.380537, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:09.434696: step 5902, loss 0.328428, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:09.539665: step 5903, loss 0.146582, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:09.648369: step 5904, loss 0.199177, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:09.757580: step 5905, loss 0.162167, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:09.869644: step 5906, loss 0.159529, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:09.976259: step 5907, loss 0.203502, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:10.083926: step 5908, loss 0.163495, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:10.188348: step 5909, loss 0.266349, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:10.292551: step 5910, loss 0.13684, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:10.398858: step 5911, loss 0.270141, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:10.504595: step 5912, loss 0.223086, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:10.608553: step 5913, loss 0.289113, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:10.715730: step 5914, loss 0.229986, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:10.827790: step 5915, loss 0.216012, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:10.937321: step 5916, loss 0.213646, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:11.042908: step 5917, loss 0.20542, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:11.159571: step 5918, loss 0.293708, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:11.280086: step 5919, loss 0.207854, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:11.396995: step 5920, loss 0.180921, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:11.656616: step 5920, loss 0.301538, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5920

2017-10-09T15:18:12.239987: step 5921, loss 0.30017, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:12.346622: step 5922, loss 0.392773, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:12.451304: step 5923, loss 0.145772, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:12.557516: step 5924, loss 0.134074, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:12.664819: step 5925, loss 0.20076, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:12.770103: step 5926, loss 0.129528, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:12.878200: step 5927, loss 0.239795, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:12.985468: step 5928, loss 0.217278, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:13.094691: step 5929, loss 0.190196, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:13.202018: step 5930, loss 0.281945, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:13.313074: step 5931, loss 0.260997, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:13.423041: step 5932, loss 0.283597, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:13.530856: step 5933, loss 0.252486, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:13.635973: step 5934, loss 0.191121, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:13.744435: step 5935, loss 0.12707, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:13.848940: step 5936, loss 0.213034, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:13.957679: step 5937, loss 0.293839, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:14.075262: step 5938, loss 0.139898, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:14.199553: step 5939, loss 0.143182, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:14.314601: step 5940, loss 0.358705, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:14.422619: step 5941, loss 0.264758, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:14.530887: step 5942, loss 0.166568, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:14.637588: step 5943, loss 0.226457, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:14.745122: step 5944, loss 0.227423, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:14.852162: step 5945, loss 0.292523, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:14.968603: step 5946, loss 0.19386, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:15.090022: step 5947, loss 0.201982, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:15.215696: step 5948, loss 0.287601, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:15.335256: step 5949, loss 0.26596, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:15.442522: step 5950, loss 0.285111, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:15.556658: step 5951, loss 0.281713, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:15.662888: step 5952, loss 0.336201, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:15.766647: step 5953, loss 0.197886, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:15.876580: step 5954, loss 0.261546, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:15.983491: step 5955, loss 0.290889, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:16.090723: step 5956, loss 0.173708, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:16.198260: step 5957, loss 0.195119, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:16.302676: step 5958, loss 0.193218, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:16.406823: step 5959, loss 0.280136, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:16.520834: step 5960, loss 0.234714, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:16.799687: step 5960, loss 0.303681, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-5960

2017-10-09T15:18:17.475354: step 5961, loss 0.406387, acc 0.8125, learning_rate 0.0001
2017-10-09T15:18:17.583376: step 5962, loss 0.298685, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:17.690303: step 5963, loss 0.281584, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:17.794365: step 5964, loss 0.314042, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:17.903181: step 5965, loss 0.14576, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:18.007710: step 5966, loss 0.481736, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:18.114986: step 5967, loss 0.276426, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:18.216157: step 5968, loss 0.329663, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:18.323268: step 5969, loss 0.245246, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:18.425060: step 5970, loss 0.277692, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:18.534104: step 5971, loss 0.202894, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:18.636166: step 5972, loss 0.223333, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:18.744396: step 5973, loss 0.132507, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:18.852128: step 5974, loss 0.255678, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:18.960310: step 5975, loss 0.256571, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:19.067921: step 5976, loss 0.169883, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:19.171906: step 5977, loss 0.214192, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:19.262972: step 5978, loss 0.31806, acc 0.843137, learning_rate 0.0001
2017-10-09T15:18:19.365747: step 5979, loss 0.244865, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:19.470970: step 5980, loss 0.176966, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:19.579868: step 5981, loss 0.198787, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:19.685819: step 5982, loss 0.173169, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:19.792553: step 5983, loss 0.314785, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:19.900570: step 5984, loss 0.248387, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:20.003755: step 5985, loss 0.20092, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:20.106832: step 5986, loss 0.23371, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:20.217640: step 5987, loss 0.298551, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:20.336395: step 5988, loss 0.167734, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:20.448179: step 5989, loss 0.202171, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:20.569881: step 5990, loss 0.275286, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:20.692833: step 5991, loss 0.240067, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:20.817911: step 5992, loss 0.139798, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:20.947944: step 5993, loss 0.208369, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:21.059878: step 5994, loss 0.27102, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:21.164443: step 5995, loss 0.223117, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:21.267352: step 5996, loss 0.253972, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:21.370311: step 5997, loss 0.276038, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:21.478312: step 5998, loss 0.406746, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:21.582836: step 5999, loss 0.220267, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:21.690539: step 6000, loss 0.274786, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:21.951659: step 6000, loss 0.305435, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6000

2017-10-09T15:18:22.482912: step 6001, loss 0.24777, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:22.591031: step 6002, loss 0.182411, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:22.699030: step 6003, loss 0.201184, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:22.820558: step 6004, loss 0.187044, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:22.946393: step 6005, loss 0.260617, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:23.064166: step 6006, loss 0.252582, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:23.173744: step 6007, loss 0.346657, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:23.291769: step 6008, loss 0.187106, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:23.408204: step 6009, loss 0.16166, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:23.528477: step 6010, loss 0.161951, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:23.636628: step 6011, loss 0.183659, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:23.749730: step 6012, loss 0.226855, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:23.867881: step 6013, loss 0.310804, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:23.980101: step 6014, loss 0.308993, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:24.093539: step 6015, loss 0.247406, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:24.212392: step 6016, loss 0.16707, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:24.327294: step 6017, loss 0.281924, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:24.442469: step 6018, loss 0.288881, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:24.551806: step 6019, loss 0.171907, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:24.660820: step 6020, loss 0.194712, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:24.768730: step 6021, loss 0.210836, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:24.881691: step 6022, loss 0.325033, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:24.995025: step 6023, loss 0.211363, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:25.099989: step 6024, loss 0.18934, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:25.214803: step 6025, loss 0.17973, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:25.336215: step 6026, loss 0.205148, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:25.449427: step 6027, loss 0.141557, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:25.559516: step 6028, loss 0.237628, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:25.669922: step 6029, loss 0.290641, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:25.782376: step 6030, loss 0.170256, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:25.894614: step 6031, loss 0.46545, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:26.013931: step 6032, loss 0.154701, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:26.121765: step 6033, loss 0.213916, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:26.236307: step 6034, loss 0.266878, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:26.350176: step 6035, loss 0.156182, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:26.464696: step 6036, loss 0.254743, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:26.579299: step 6037, loss 0.405977, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:26.697967: step 6038, loss 0.176678, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:26.809467: step 6039, loss 0.349158, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:26.921017: step 6040, loss 0.256415, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:27.175109: step 6040, loss 0.303819, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6040

2017-10-09T15:18:27.772332: step 6041, loss 0.194574, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:27.880917: step 6042, loss 0.218815, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:27.985820: step 6043, loss 0.264128, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:28.093896: step 6044, loss 0.323166, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:28.199764: step 6045, loss 0.222865, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:28.308503: step 6046, loss 0.270068, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:28.416177: step 6047, loss 0.196854, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:28.525671: step 6048, loss 0.148844, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:28.631937: step 6049, loss 0.421993, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:28.736420: step 6050, loss 0.287348, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:28.843224: step 6051, loss 0.129904, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:28.954194: step 6052, loss 0.310333, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:29.059287: step 6053, loss 0.25776, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:29.172059: step 6054, loss 0.171151, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:29.279272: step 6055, loss 0.274404, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:29.384071: step 6056, loss 0.216427, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:29.490663: step 6057, loss 0.282849, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:29.597216: step 6058, loss 0.217943, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:29.706765: step 6059, loss 0.227064, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:29.816438: step 6060, loss 0.191506, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:29.925370: step 6061, loss 0.170346, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:30.030327: step 6062, loss 0.241322, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:30.137971: step 6063, loss 0.247723, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:30.241743: step 6064, loss 0.171495, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:30.347042: step 6065, loss 0.176551, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:30.457370: step 6066, loss 0.16735, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:30.562230: step 6067, loss 0.159767, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:30.666941: step 6068, loss 0.267516, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:30.771765: step 6069, loss 0.238065, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:30.879142: step 6070, loss 0.318588, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:30.987634: step 6071, loss 0.277215, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:31.088937: step 6072, loss 0.197787, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:31.195041: step 6073, loss 0.179416, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:31.298377: step 6074, loss 0.245696, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:31.403319: step 6075, loss 0.248995, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:31.497587: step 6076, loss 0.168289, acc 0.941176, learning_rate 0.0001
2017-10-09T15:18:31.606837: step 6077, loss 0.287701, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:31.712752: step 6078, loss 0.141892, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:31.820033: step 6079, loss 0.239507, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:31.928668: step 6080, loss 0.166875, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:32.189539: step 6080, loss 0.300652, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6080

2017-10-09T15:18:32.775323: step 6081, loss 0.262126, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:32.888950: step 6082, loss 0.338204, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:32.992915: step 6083, loss 0.195392, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:33.098383: step 6084, loss 0.127153, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:33.199219: step 6085, loss 0.287711, acc 0.84375, learning_rate 0.0001
2017-10-09T15:18:33.306900: step 6086, loss 0.175809, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:33.413934: step 6087, loss 0.140884, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:33.518384: step 6088, loss 0.227256, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:33.623771: step 6089, loss 0.141779, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:33.730276: step 6090, loss 0.211355, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:33.836499: step 6091, loss 0.128172, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:33.946414: step 6092, loss 0.157509, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:34.056135: step 6093, loss 0.333794, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:34.164289: step 6094, loss 0.132997, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:34.267966: step 6095, loss 0.153532, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:34.377256: step 6096, loss 0.263572, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:34.482370: step 6097, loss 0.216927, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:34.587262: step 6098, loss 0.258869, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:34.693159: step 6099, loss 0.329894, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:34.799442: step 6100, loss 0.192027, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:34.905700: step 6101, loss 0.420959, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:35.009161: step 6102, loss 0.315895, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:35.116089: step 6103, loss 0.195452, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:35.220286: step 6104, loss 0.1548, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:35.326556: step 6105, loss 0.422474, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:35.432486: step 6106, loss 0.187409, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:35.539683: step 6107, loss 0.3486, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:35.645950: step 6108, loss 0.162104, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:35.749634: step 6109, loss 0.272094, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:35.868723: step 6110, loss 0.216995, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:35.973479: step 6111, loss 0.1237, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:36.084560: step 6112, loss 0.162814, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:36.187786: step 6113, loss 0.15635, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:36.294204: step 6114, loss 0.318848, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:36.402202: step 6115, loss 0.182799, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:36.505434: step 6116, loss 0.304944, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:36.615533: step 6117, loss 0.124804, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:36.720634: step 6118, loss 0.270325, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:36.827307: step 6119, loss 0.275923, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:36.929689: step 6120, loss 0.248463, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:37.186120: step 6120, loss 0.300877, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6120

2017-10-09T15:18:37.908484: step 6121, loss 0.145333, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:38.038143: step 6122, loss 0.186228, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:38.159006: step 6123, loss 0.217189, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:38.280407: step 6124, loss 0.224335, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:38.410458: step 6125, loss 0.122955, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:38.517573: step 6126, loss 0.203298, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:38.626462: step 6127, loss 0.243211, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:38.745263: step 6128, loss 0.312816, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:38.861312: step 6129, loss 0.191595, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:38.971253: step 6130, loss 0.161009, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:39.082909: step 6131, loss 0.239868, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:39.187497: step 6132, loss 0.212563, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:39.288119: step 6133, loss 0.224028, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:39.396670: step 6134, loss 0.23088, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:39.507069: step 6135, loss 0.3668, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:39.620300: step 6136, loss 0.22433, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:39.722004: step 6137, loss 0.232964, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:39.828248: step 6138, loss 0.265013, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:39.935628: step 6139, loss 0.131511, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:40.042857: step 6140, loss 0.28396, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:40.150220: step 6141, loss 0.250288, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:40.270480: step 6142, loss 0.206641, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:40.400589: step 6143, loss 0.235924, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:40.526632: step 6144, loss 0.220207, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:40.649362: step 6145, loss 0.339621, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:40.769617: step 6146, loss 0.278286, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:40.884657: step 6147, loss 0.16753, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:40.997077: step 6148, loss 0.157235, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:41.114303: step 6149, loss 0.197753, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:41.220916: step 6150, loss 0.257548, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:41.327848: step 6151, loss 0.34956, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:41.438506: step 6152, loss 0.36223, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:41.546353: step 6153, loss 0.168555, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:41.656353: step 6154, loss 0.160031, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:41.766283: step 6155, loss 0.140825, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:41.878090: step 6156, loss 0.212637, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:42.002027: step 6157, loss 0.266597, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:42.125163: step 6158, loss 0.189993, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:42.231871: step 6159, loss 0.122093, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:42.339091: step 6160, loss 0.180623, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:42.585785: step 6160, loss 0.297514, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6160

2017-10-09T15:18:43.107510: step 6161, loss 0.341031, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:43.223326: step 6162, loss 0.162514, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:43.334963: step 6163, loss 0.238304, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:43.455511: step 6164, loss 0.255813, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:43.573058: step 6165, loss 0.206647, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:43.678743: step 6166, loss 0.220929, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:43.782568: step 6167, loss 0.311258, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:43.901645: step 6168, loss 0.274078, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:44.010398: step 6169, loss 0.151285, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:44.114109: step 6170, loss 0.158997, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:44.220337: step 6171, loss 0.18758, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:44.327126: step 6172, loss 0.218772, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:44.433357: step 6173, loss 0.199198, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:44.524275: step 6174, loss 0.177008, acc 0.960784, learning_rate 0.0001
2017-10-09T15:18:44.631019: step 6175, loss 0.311235, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:44.738591: step 6176, loss 0.193591, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:44.854218: step 6177, loss 0.201462, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:44.978956: step 6178, loss 0.26859, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:45.110480: step 6179, loss 0.151879, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:45.224068: step 6180, loss 0.285012, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:45.341870: step 6181, loss 0.168015, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:45.467657: step 6182, loss 0.141028, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:45.591302: step 6183, loss 0.170576, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:45.709508: step 6184, loss 0.235575, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:45.831620: step 6185, loss 0.165844, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:45.949547: step 6186, loss 0.393242, acc 0.84375, learning_rate 0.0001
2017-10-09T15:18:46.082158: step 6187, loss 0.252951, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:46.205068: step 6188, loss 0.205137, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:46.320126: step 6189, loss 0.258351, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:46.442623: step 6190, loss 0.162411, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:46.556028: step 6191, loss 0.201078, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:46.669905: step 6192, loss 0.301342, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:46.799709: step 6193, loss 0.123318, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:46.918946: step 6194, loss 0.281666, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:47.041994: step 6195, loss 0.302466, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:47.169869: step 6196, loss 0.2716, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:47.298078: step 6197, loss 0.157281, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:47.425964: step 6198, loss 0.269459, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:47.557538: step 6199, loss 0.255993, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:47.680913: step 6200, loss 0.296903, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:47.979452: step 6200, loss 0.298241, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6200

2017-10-09T15:18:48.638191: step 6201, loss 0.208514, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:48.755327: step 6202, loss 0.130678, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:48.886041: step 6203, loss 0.179932, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:49.011116: step 6204, loss 0.158959, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:49.143324: step 6205, loss 0.209846, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:49.265367: step 6206, loss 0.152346, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:49.400046: step 6207, loss 0.210906, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:49.543970: step 6208, loss 0.169824, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:49.667182: step 6209, loss 0.106736, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:49.794636: step 6210, loss 0.18928, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:49.922639: step 6211, loss 0.299752, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:50.050326: step 6212, loss 0.171947, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:50.189840: step 6213, loss 0.263053, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:50.315432: step 6214, loss 0.23944, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:50.439236: step 6215, loss 0.220161, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:50.561169: step 6216, loss 0.274524, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:50.678134: step 6217, loss 0.121051, acc 0.984375, learning_rate 0.0001
2017-10-09T15:18:50.795419: step 6218, loss 0.2051, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:50.921095: step 6219, loss 0.15675, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:51.040416: step 6220, loss 0.129989, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:51.157363: step 6221, loss 0.333859, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:51.274733: step 6222, loss 0.192058, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:51.384811: step 6223, loss 0.321911, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:51.497854: step 6224, loss 0.38773, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:51.616790: step 6225, loss 0.259957, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:51.731241: step 6226, loss 0.313874, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:51.847265: step 6227, loss 0.188735, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:51.964922: step 6228, loss 0.177165, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:52.089443: step 6229, loss 0.252998, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:52.206418: step 6230, loss 0.292221, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:52.319959: step 6231, loss 0.186619, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:52.432079: step 6232, loss 0.392806, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:52.544108: step 6233, loss 0.295677, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:52.656126: step 6234, loss 0.213085, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:52.767558: step 6235, loss 0.342529, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:52.884509: step 6236, loss 0.284929, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:53.001245: step 6237, loss 0.200796, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:53.116463: step 6238, loss 0.137611, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:53.235304: step 6239, loss 0.202622, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:53.347178: step 6240, loss 0.143421, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:53.637995: step 6240, loss 0.299739, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6240

2017-10-09T15:18:54.273505: step 6241, loss 0.322226, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:54.380081: step 6242, loss 0.196012, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:54.493758: step 6243, loss 0.158007, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:54.607830: step 6244, loss 0.287707, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:54.717204: step 6245, loss 0.316027, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:54.839036: step 6246, loss 0.359159, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:54.963513: step 6247, loss 0.181261, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:55.083265: step 6248, loss 0.230741, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:55.200031: step 6249, loss 0.212404, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:55.317814: step 6250, loss 0.241934, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:55.436098: step 6251, loss 0.148199, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:55.555216: step 6252, loss 0.193248, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:55.673385: step 6253, loss 0.161999, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:55.810729: step 6254, loss 0.281265, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:55.968754: step 6255, loss 0.21204, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:56.109513: step 6256, loss 0.269476, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:56.237196: step 6257, loss 0.215912, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:56.352689: step 6258, loss 0.214082, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:56.469985: step 6259, loss 0.15079, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:56.575164: step 6260, loss 0.193545, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:56.685396: step 6261, loss 0.236176, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:56.794199: step 6262, loss 0.20022, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:56.918383: step 6263, loss 0.140847, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:57.037180: step 6264, loss 0.178019, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:57.148810: step 6265, loss 0.389908, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:57.260378: step 6266, loss 0.336192, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:57.368032: step 6267, loss 0.266197, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:57.480715: step 6268, loss 0.219046, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:57.587280: step 6269, loss 0.209138, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:57.694182: step 6270, loss 0.252635, acc 0.90625, learning_rate 0.0001
2017-10-09T15:18:57.804233: step 6271, loss 0.146003, acc 0.96875, learning_rate 0.0001
2017-10-09T15:18:57.901089: step 6272, loss 0.203328, acc 0.960784, learning_rate 0.0001
2017-10-09T15:18:58.021973: step 6273, loss 0.179865, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:58.127597: step 6274, loss 0.237909, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:58.238593: step 6275, loss 0.205681, acc 0.9375, learning_rate 0.0001
2017-10-09T15:18:58.348618: step 6276, loss 0.235429, acc 0.890625, learning_rate 0.0001
2017-10-09T15:18:58.457296: step 6277, loss 0.32517, acc 0.859375, learning_rate 0.0001
2017-10-09T15:18:58.561715: step 6278, loss 0.223204, acc 0.921875, learning_rate 0.0001
2017-10-09T15:18:58.674539: step 6279, loss 0.210822, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:58.788515: step 6280, loss 0.281556, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:18:59.049190: step 6280, loss 0.29695, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6280

2017-10-09T15:18:59.746590: step 6281, loss 0.165058, acc 0.953125, learning_rate 0.0001
2017-10-09T15:18:59.859242: step 6282, loss 0.31915, acc 0.875, learning_rate 0.0001
2017-10-09T15:18:59.978270: step 6283, loss 0.250052, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:00.095488: step 6284, loss 0.218948, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:00.203253: step 6285, loss 0.0971347, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:00.314555: step 6286, loss 0.156151, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:00.425203: step 6287, loss 0.20191, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:00.536705: step 6288, loss 0.125714, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:00.641369: step 6289, loss 0.27952, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:00.754379: step 6290, loss 0.313093, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:00.868070: step 6291, loss 0.198444, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:00.994008: step 6292, loss 0.203064, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:01.103010: step 6293, loss 0.152815, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:01.208086: step 6294, loss 0.32091, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:01.319221: step 6295, loss 0.0699646, acc 1, learning_rate 0.0001
2017-10-09T15:19:01.425056: step 6296, loss 0.126266, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:01.537536: step 6297, loss 0.43757, acc 0.796875, learning_rate 0.0001
2017-10-09T15:19:01.647464: step 6298, loss 0.297469, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:01.760098: step 6299, loss 0.245234, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:01.869220: step 6300, loss 0.203497, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:01.985379: step 6301, loss 0.221992, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:02.097147: step 6302, loss 0.135041, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:02.204553: step 6303, loss 0.148113, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:02.311749: step 6304, loss 0.283789, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:02.423199: step 6305, loss 0.301535, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:02.533214: step 6306, loss 0.177083, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:02.645312: step 6307, loss 0.169317, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:02.751261: step 6308, loss 0.188494, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:02.863278: step 6309, loss 0.155049, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:02.968729: step 6310, loss 0.229684, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:03.079444: step 6311, loss 0.230618, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:03.191532: step 6312, loss 0.118621, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:03.333917: step 6313, loss 0.172675, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:03.458890: step 6314, loss 0.202468, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:03.594380: step 6315, loss 0.143992, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:03.721080: step 6316, loss 0.138268, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:03.831999: step 6317, loss 0.278062, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:03.940875: step 6318, loss 0.371209, acc 0.84375, learning_rate 0.0001
2017-10-09T15:19:04.046991: step 6319, loss 0.224901, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:04.151092: step 6320, loss 0.202891, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:04.434827: step 6320, loss 0.297653, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6320

2017-10-09T15:19:04.988700: step 6321, loss 0.201946, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:05.095666: step 6322, loss 0.181034, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:05.204325: step 6323, loss 0.259057, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:05.313542: step 6324, loss 0.183214, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:05.454635: step 6325, loss 0.170737, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:05.585163: step 6326, loss 0.129523, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:05.703498: step 6327, loss 0.228731, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:05.807865: step 6328, loss 0.222863, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:05.915861: step 6329, loss 0.169504, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:06.042936: step 6330, loss 0.317823, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:06.168466: step 6331, loss 0.332595, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:06.281117: step 6332, loss 0.152841, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:06.404333: step 6333, loss 0.164835, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:06.531335: step 6334, loss 0.226575, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:06.641179: step 6335, loss 0.189317, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:06.742407: step 6336, loss 0.262766, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:06.867738: step 6337, loss 0.170166, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:06.991004: step 6338, loss 0.204331, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:07.104349: step 6339, loss 0.176527, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:07.213011: step 6340, loss 0.222994, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:07.324983: step 6341, loss 0.193101, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:07.451893: step 6342, loss 0.143318, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:07.576151: step 6343, loss 0.378096, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:07.683976: step 6344, loss 0.331781, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:07.793648: step 6345, loss 0.193431, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:07.900996: step 6346, loss 0.161877, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:08.012313: step 6347, loss 0.277208, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:08.123970: step 6348, loss 0.284252, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:08.239598: step 6349, loss 0.160973, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:08.353307: step 6350, loss 0.235798, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:08.466543: step 6351, loss 0.323343, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:08.581258: step 6352, loss 0.216269, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:08.691252: step 6353, loss 0.367949, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:08.803147: step 6354, loss 0.278269, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:08.954074: step 6355, loss 0.173403, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:09.107749: step 6356, loss 0.243895, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:09.274559: step 6357, loss 0.269688, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:09.402729: step 6358, loss 0.148257, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:09.542764: step 6359, loss 0.272425, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:09.696427: step 6360, loss 0.175952, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:10.010608: step 6360, loss 0.297779, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6360

2017-10-09T15:19:10.653330: step 6361, loss 0.244666, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:10.764635: step 6362, loss 0.249282, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:10.878561: step 6363, loss 0.25403, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:10.989637: step 6364, loss 0.19568, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:11.110197: step 6365, loss 0.155595, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:11.223469: step 6366, loss 0.20107, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:11.335020: step 6367, loss 0.348255, acc 0.84375, learning_rate 0.0001
2017-10-09T15:19:11.452884: step 6368, loss 0.36622, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:11.571754: step 6369, loss 0.360424, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:11.669445: step 6370, loss 0.241797, acc 0.901961, learning_rate 0.0001
2017-10-09T15:19:11.786286: step 6371, loss 0.169612, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:11.907611: step 6372, loss 0.164138, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:12.039722: step 6373, loss 0.222825, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:12.157007: step 6374, loss 0.312029, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:12.277027: step 6375, loss 0.269423, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:12.406907: step 6376, loss 0.200461, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:12.533695: step 6377, loss 0.135349, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:12.653624: step 6378, loss 0.171824, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:12.785430: step 6379, loss 0.171865, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:12.912212: step 6380, loss 0.268674, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:13.042487: step 6381, loss 0.203559, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:13.155687: step 6382, loss 0.141084, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:13.266781: step 6383, loss 0.104132, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:13.380361: step 6384, loss 0.186108, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:13.499721: step 6385, loss 0.254604, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:13.611444: step 6386, loss 0.252556, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:13.724015: step 6387, loss 0.17766, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:13.836654: step 6388, loss 0.129811, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:13.964339: step 6389, loss 0.0983745, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:14.088748: step 6390, loss 0.309384, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:14.220906: step 6391, loss 0.441089, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:14.354519: step 6392, loss 0.147425, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:14.469179: step 6393, loss 0.199372, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:14.580038: step 6394, loss 0.388717, acc 0.84375, learning_rate 0.0001
2017-10-09T15:19:14.692591: step 6395, loss 0.146465, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:14.802475: step 6396, loss 0.19699, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:14.910741: step 6397, loss 0.189525, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:15.020058: step 6398, loss 0.204489, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:15.130542: step 6399, loss 0.250538, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:15.243870: step 6400, loss 0.290983, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:15.557461: step 6400, loss 0.297926, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6400

2017-10-09T15:19:16.195408: step 6401, loss 0.245585, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:16.309827: step 6402, loss 0.211611, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:16.428274: step 6403, loss 0.250395, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:16.542558: step 6404, loss 0.196923, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:16.666078: step 6405, loss 0.102744, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:16.781806: step 6406, loss 0.0780524, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:16.902753: step 6407, loss 0.113021, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:17.013457: step 6408, loss 0.217007, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:17.121838: step 6409, loss 0.230515, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:17.246600: step 6410, loss 0.166798, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:17.355712: step 6411, loss 0.30843, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:17.472530: step 6412, loss 0.16968, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:17.578763: step 6413, loss 0.15405, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:17.686083: step 6414, loss 0.18863, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:17.813456: step 6415, loss 0.261996, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:17.939370: step 6416, loss 0.203954, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:18.056513: step 6417, loss 0.147915, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:18.172317: step 6418, loss 0.259095, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:18.284323: step 6419, loss 0.41283, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:18.400686: step 6420, loss 0.206042, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:18.519891: step 6421, loss 0.252082, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:18.643627: step 6422, loss 0.201648, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:18.764843: step 6423, loss 0.258831, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:18.889364: step 6424, loss 0.267979, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:19.003238: step 6425, loss 0.288117, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:19.108974: step 6426, loss 0.185126, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:19.230321: step 6427, loss 0.202584, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:19.345849: step 6428, loss 0.322591, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:19.465463: step 6429, loss 0.281933, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:19.583882: step 6430, loss 0.272109, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:19.704439: step 6431, loss 0.251468, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:19.819087: step 6432, loss 0.225909, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:19.931682: step 6433, loss 0.163601, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:20.044685: step 6434, loss 0.282921, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:20.165780: step 6435, loss 0.142034, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:20.280273: step 6436, loss 0.264217, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:20.399021: step 6437, loss 0.150877, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:20.529448: step 6438, loss 0.261906, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:20.651744: step 6439, loss 0.279174, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:20.772403: step 6440, loss 0.203396, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:21.052316: step 6440, loss 0.298619, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6440

2017-10-09T15:19:21.794108: step 6441, loss 0.312785, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:21.919146: step 6442, loss 0.150259, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:22.038720: step 6443, loss 0.20669, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:22.155935: step 6444, loss 0.129458, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:22.275233: step 6445, loss 0.228844, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:22.394006: step 6446, loss 0.19016, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:22.510725: step 6447, loss 0.294084, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:22.629408: step 6448, loss 0.156576, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:22.741252: step 6449, loss 0.125218, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:22.859145: step 6450, loss 0.216686, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:22.971623: step 6451, loss 0.30898, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:23.095821: step 6452, loss 0.195491, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:23.231676: step 6453, loss 0.286295, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:23.359286: step 6454, loss 0.200207, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:23.481942: step 6455, loss 0.177798, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:23.596742: step 6456, loss 0.349889, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:23.713782: step 6457, loss 0.159357, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:23.830662: step 6458, loss 0.259068, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:23.946714: step 6459, loss 0.274846, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:24.063081: step 6460, loss 0.204312, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:24.200398: step 6461, loss 0.110912, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:24.317916: step 6462, loss 0.255529, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:24.433573: step 6463, loss 0.276979, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:24.550261: step 6464, loss 0.242874, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:24.666346: step 6465, loss 0.201336, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:24.785124: step 6466, loss 0.205201, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:24.901325: step 6467, loss 0.362755, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:24.997082: step 6468, loss 0.264951, acc 0.901961, learning_rate 0.0001
2017-10-09T15:19:25.118082: step 6469, loss 0.220068, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:25.235217: step 6470, loss 0.0897411, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:25.349370: step 6471, loss 0.223833, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:25.464797: step 6472, loss 0.410804, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:25.587324: step 6473, loss 0.1752, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:25.703366: step 6474, loss 0.233432, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:25.814356: step 6475, loss 0.320165, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:25.926464: step 6476, loss 0.201205, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:26.040496: step 6477, loss 0.199737, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:26.151204: step 6478, loss 0.207145, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:26.264780: step 6479, loss 0.331357, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:26.379638: step 6480, loss 0.265173, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:26.666199: step 6480, loss 0.294594, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6480

2017-10-09T15:19:27.216604: step 6481, loss 0.33058, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:27.329370: step 6482, loss 0.216366, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:27.440277: step 6483, loss 0.191168, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:27.561330: step 6484, loss 0.205997, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:27.677724: step 6485, loss 0.336022, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:27.793769: step 6486, loss 0.251088, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:27.920288: step 6487, loss 0.146999, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:28.041515: step 6488, loss 0.24519, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:28.158441: step 6489, loss 0.245074, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:28.275580: step 6490, loss 0.297016, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:28.385485: step 6491, loss 0.189316, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:28.502082: step 6492, loss 0.173401, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:28.616866: step 6493, loss 0.188073, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:28.740424: step 6494, loss 0.326268, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:28.858930: step 6495, loss 0.179233, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:28.970104: step 6496, loss 0.263036, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:29.078075: step 6497, loss 0.210917, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:29.191365: step 6498, loss 0.254413, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:29.303726: step 6499, loss 0.147832, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:29.417497: step 6500, loss 0.365301, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:29.534647: step 6501, loss 0.320321, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:29.646286: step 6502, loss 0.203093, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:29.765197: step 6503, loss 0.219816, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:29.878866: step 6504, loss 0.321178, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:29.996236: step 6505, loss 0.22415, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:30.108944: step 6506, loss 0.212725, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:30.224962: step 6507, loss 0.137465, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:30.335848: step 6508, loss 0.2459, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:30.445765: step 6509, loss 0.158481, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:30.563800: step 6510, loss 0.221928, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:30.676259: step 6511, loss 0.143432, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:30.791378: step 6512, loss 0.124735, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:30.905820: step 6513, loss 0.225419, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:31.011753: step 6514, loss 0.224305, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:31.130628: step 6515, loss 0.234031, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:31.247177: step 6516, loss 0.21873, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:31.362018: step 6517, loss 0.319529, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:31.467229: step 6518, loss 0.416834, acc 0.84375, learning_rate 0.0001
2017-10-09T15:19:31.582737: step 6519, loss 0.200252, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:31.699624: step 6520, loss 0.20036, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:31.984537: step 6520, loss 0.295417, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6520

2017-10-09T15:19:32.597347: step 6521, loss 0.288042, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:32.709670: step 6522, loss 0.224545, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:32.828765: step 6523, loss 0.345076, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:32.947733: step 6524, loss 0.272101, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:33.063800: step 6525, loss 0.237892, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:33.173005: step 6526, loss 0.215372, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:33.286626: step 6527, loss 0.0582418, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:33.402406: step 6528, loss 0.197907, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:33.516900: step 6529, loss 0.253927, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:33.629244: step 6530, loss 0.139819, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:33.753089: step 6531, loss 0.152619, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:33.873011: step 6532, loss 0.336324, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:33.985402: step 6533, loss 0.249128, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:34.096437: step 6534, loss 0.247177, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:34.208419: step 6535, loss 0.319594, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:34.322841: step 6536, loss 0.237945, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:34.437058: step 6537, loss 0.163278, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:34.546076: step 6538, loss 0.196083, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:34.662560: step 6539, loss 0.121725, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:34.779708: step 6540, loss 0.192742, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:34.895809: step 6541, loss 0.119702, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:35.014267: step 6542, loss 0.213809, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:35.129006: step 6543, loss 0.281841, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:35.243365: step 6544, loss 0.230696, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:35.364594: step 6545, loss 0.203911, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:35.479440: step 6546, loss 0.320718, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:35.589910: step 6547, loss 0.105359, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:35.706594: step 6548, loss 0.362074, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:35.823482: step 6549, loss 0.379757, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:35.942479: step 6550, loss 0.162621, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:36.060439: step 6551, loss 0.213038, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:36.169411: step 6552, loss 0.284482, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:36.277561: step 6553, loss 0.167874, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:36.394138: step 6554, loss 0.244465, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:36.510708: step 6555, loss 0.196566, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:36.628164: step 6556, loss 0.224318, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:36.744823: step 6557, loss 0.170855, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:36.864930: step 6558, loss 0.139086, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:36.974361: step 6559, loss 0.174646, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:37.089051: step 6560, loss 0.175491, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:37.379661: step 6560, loss 0.295469, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6560

2017-10-09T15:19:38.016248: step 6561, loss 0.367014, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:38.126355: step 6562, loss 0.268256, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:38.237837: step 6563, loss 0.0943717, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:38.355027: step 6564, loss 0.163304, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:38.470839: step 6565, loss 0.228071, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:38.571764: step 6566, loss 0.188522, acc 0.921569, learning_rate 0.0001
2017-10-09T15:19:38.686493: step 6567, loss 0.211693, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:38.800784: step 6568, loss 0.299147, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:38.918687: step 6569, loss 0.126221, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:39.039694: step 6570, loss 0.0989187, acc 1, learning_rate 0.0001
2017-10-09T15:19:39.157183: step 6571, loss 0.165774, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:39.272639: step 6572, loss 0.231476, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:39.391056: step 6573, loss 0.211375, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:39.502692: step 6574, loss 0.229205, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:39.617837: step 6575, loss 0.181395, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:39.731018: step 6576, loss 0.177239, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:39.842060: step 6577, loss 0.264074, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:39.961094: step 6578, loss 0.186756, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:40.070263: step 6579, loss 0.203781, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:40.181575: step 6580, loss 0.237261, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:40.296358: step 6581, loss 0.159923, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:40.409515: step 6582, loss 0.245039, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:40.526616: step 6583, loss 0.176197, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:40.642451: step 6584, loss 0.282931, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:40.756864: step 6585, loss 0.298245, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:40.871029: step 6586, loss 0.210805, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:40.988768: step 6587, loss 0.329172, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:41.106129: step 6588, loss 0.247596, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:41.226323: step 6589, loss 0.447741, acc 0.8125, learning_rate 0.0001
2017-10-09T15:19:41.339550: step 6590, loss 0.240819, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:41.455531: step 6591, loss 0.213635, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:41.570127: step 6592, loss 0.299351, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:41.684427: step 6593, loss 0.179213, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:41.802964: step 6594, loss 0.235845, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:41.922931: step 6595, loss 0.217429, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:42.042663: step 6596, loss 0.144636, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:42.161259: step 6597, loss 0.223525, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:42.274647: step 6598, loss 0.255838, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:42.390961: step 6599, loss 0.196684, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:42.511787: step 6600, loss 0.178547, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:42.788248: step 6600, loss 0.293236, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6600

2017-10-09T15:19:43.491634: step 6601, loss 0.268143, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:43.612906: step 6602, loss 0.293601, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:43.731545: step 6603, loss 0.161452, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:43.842304: step 6604, loss 0.141104, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:43.953328: step 6605, loss 0.130882, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:44.073329: step 6606, loss 0.264255, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:44.186990: step 6607, loss 0.168278, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:44.302528: step 6608, loss 0.174762, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:44.414933: step 6609, loss 0.194503, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:44.525184: step 6610, loss 0.166252, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:44.642771: step 6611, loss 0.171782, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:44.751850: step 6612, loss 0.186024, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:44.870392: step 6613, loss 0.35429, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:44.983761: step 6614, loss 0.199455, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:45.096005: step 6615, loss 0.174112, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:45.212337: step 6616, loss 0.130269, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:45.326476: step 6617, loss 0.221875, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:45.445025: step 6618, loss 0.325276, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:45.556189: step 6619, loss 0.150343, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:45.674771: step 6620, loss 0.353307, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:45.789482: step 6621, loss 0.189429, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:45.907961: step 6622, loss 0.139864, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:46.019093: step 6623, loss 0.308371, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:46.136806: step 6624, loss 0.252523, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:46.251266: step 6625, loss 0.187493, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:46.364986: step 6626, loss 0.303031, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:46.474155: step 6627, loss 0.229252, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:46.586463: step 6628, loss 0.190297, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:46.700089: step 6629, loss 0.133517, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:46.817296: step 6630, loss 0.13665, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:46.931146: step 6631, loss 0.330595, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:47.041370: step 6632, loss 0.24098, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:47.152674: step 6633, loss 0.197396, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:47.266336: step 6634, loss 0.231499, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:47.380240: step 6635, loss 0.281559, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:47.495041: step 6636, loss 0.220261, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:47.607979: step 6637, loss 0.223401, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:47.721450: step 6638, loss 0.168997, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:47.834249: step 6639, loss 0.148387, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:47.951219: step 6640, loss 0.256678, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:48.227678: step 6640, loss 0.295746, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6640

2017-10-09T15:19:48.768752: step 6641, loss 0.215248, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:48.883683: step 6642, loss 0.215084, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:48.996932: step 6643, loss 0.25314, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:49.110667: step 6644, loss 0.188613, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:49.223560: step 6645, loss 0.239323, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:49.335139: step 6646, loss 0.17809, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:49.447405: step 6647, loss 0.17447, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:49.560065: step 6648, loss 0.160157, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:49.674119: step 6649, loss 0.21954, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:49.789188: step 6650, loss 0.29044, acc 0.84375, learning_rate 0.0001
2017-10-09T15:19:49.898992: step 6651, loss 0.178408, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:50.011671: step 6652, loss 0.320563, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:50.128795: step 6653, loss 0.165898, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:50.246019: step 6654, loss 0.108597, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:50.356739: step 6655, loss 0.203071, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:50.468448: step 6656, loss 0.214088, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:50.584617: step 6657, loss 0.289164, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:50.695945: step 6658, loss 0.293401, acc 0.875, learning_rate 0.0001
2017-10-09T15:19:50.808116: step 6659, loss 0.403335, acc 0.8125, learning_rate 0.0001
2017-10-09T15:19:50.921778: step 6660, loss 0.162912, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:51.035476: step 6661, loss 0.174547, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:51.149145: step 6662, loss 0.202742, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:51.263368: step 6663, loss 0.151066, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:51.355849: step 6664, loss 0.139483, acc 0.941176, learning_rate 0.0001
2017-10-09T15:19:51.466959: step 6665, loss 0.236466, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:51.579627: step 6666, loss 0.29871, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:51.687433: step 6667, loss 0.140651, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:51.799005: step 6668, loss 0.251594, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:51.911979: step 6669, loss 0.27065, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:52.029623: step 6670, loss 0.195588, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:52.140310: step 6671, loss 0.211809, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:52.245741: step 6672, loss 0.285807, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:52.359997: step 6673, loss 0.305986, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:52.477303: step 6674, loss 0.306972, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:52.594982: step 6675, loss 0.145429, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:52.707195: step 6676, loss 0.161585, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:52.812891: step 6677, loss 0.298922, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:52.923124: step 6678, loss 0.113433, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:53.031850: step 6679, loss 0.130084, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:53.143991: step 6680, loss 0.276688, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:53.408860: step 6680, loss 0.299228, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6680

2017-10-09T15:19:54.017997: step 6681, loss 0.232684, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:54.133444: step 6682, loss 0.199643, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:54.243352: step 6683, loss 0.203387, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:54.350663: step 6684, loss 0.169412, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:54.465942: step 6685, loss 0.108195, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:54.576513: step 6686, loss 0.161295, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:54.688245: step 6687, loss 0.307803, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:54.796175: step 6688, loss 0.157712, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:54.902517: step 6689, loss 0.229304, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:55.013274: step 6690, loss 0.26035, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:55.121705: step 6691, loss 0.262506, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:55.228189: step 6692, loss 0.193287, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:55.336703: step 6693, loss 0.287135, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:55.441749: step 6694, loss 0.271888, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:55.552864: step 6695, loss 0.157724, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:55.663643: step 6696, loss 0.124555, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:55.773293: step 6697, loss 0.192697, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:55.885164: step 6698, loss 0.262915, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:55.996680: step 6699, loss 0.179522, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:56.107147: step 6700, loss 0.198071, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:56.216683: step 6701, loss 0.315894, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:56.328884: step 6702, loss 0.290985, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:56.439019: step 6703, loss 0.216204, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:56.549438: step 6704, loss 0.210824, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:56.659196: step 6705, loss 0.233787, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:56.772387: step 6706, loss 0.337791, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:56.878463: step 6707, loss 0.127198, acc 0.984375, learning_rate 0.0001
2017-10-09T15:19:56.985780: step 6708, loss 0.281611, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:57.093780: step 6709, loss 0.150485, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:57.204894: step 6710, loss 0.303064, acc 0.84375, learning_rate 0.0001
2017-10-09T15:19:57.308768: step 6711, loss 0.16455, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:57.417504: step 6712, loss 0.350438, acc 0.84375, learning_rate 0.0001
2017-10-09T15:19:57.528996: step 6713, loss 0.160088, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:57.638642: step 6714, loss 0.190184, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:57.747835: step 6715, loss 0.135418, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:57.855576: step 6716, loss 0.126578, acc 0.96875, learning_rate 0.0001
2017-10-09T15:19:57.966721: step 6717, loss 0.264646, acc 0.890625, learning_rate 0.0001
2017-10-09T15:19:58.079351: step 6718, loss 0.168092, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:58.183322: step 6719, loss 0.388549, acc 0.859375, learning_rate 0.0001
2017-10-09T15:19:58.291518: step 6720, loss 0.107178, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-09T15:19:58.555518: step 6720, loss 0.294861, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6720

2017-10-09T15:19:59.150263: step 6721, loss 0.15819, acc 0.9375, learning_rate 0.0001
2017-10-09T15:19:59.259056: step 6722, loss 0.246482, acc 0.90625, learning_rate 0.0001
2017-10-09T15:19:59.361528: step 6723, loss 0.224708, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:59.471239: step 6724, loss 0.141264, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:59.575853: step 6725, loss 0.184026, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:59.683568: step 6726, loss 0.23596, acc 0.921875, learning_rate 0.0001
2017-10-09T15:19:59.795895: step 6727, loss 0.171427, acc 0.953125, learning_rate 0.0001
2017-10-09T15:19:59.910727: step 6728, loss 0.353391, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:00.021644: step 6729, loss 0.179149, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:00.135805: step 6730, loss 0.14585, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:00.250782: step 6731, loss 0.248329, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:00.363590: step 6732, loss 0.189733, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:00.475664: step 6733, loss 0.281477, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:00.585669: step 6734, loss 0.221281, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:00.692087: step 6735, loss 0.173494, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:00.804100: step 6736, loss 0.272603, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:00.917363: step 6737, loss 0.219118, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:01.034538: step 6738, loss 0.163173, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:01.151617: step 6739, loss 0.170676, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:01.261468: step 6740, loss 0.182064, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:01.386900: step 6741, loss 0.175806, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:01.502029: step 6742, loss 0.134411, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:01.616003: step 6743, loss 0.23953, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:01.730693: step 6744, loss 0.193092, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:01.848156: step 6745, loss 0.192015, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:01.968924: step 6746, loss 0.138257, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:02.087772: step 6747, loss 0.223279, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:02.200797: step 6748, loss 0.137403, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:02.311997: step 6749, loss 0.232805, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:02.427319: step 6750, loss 0.275026, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:02.551198: step 6751, loss 0.194437, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:02.670575: step 6752, loss 0.227107, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:02.788659: step 6753, loss 0.326163, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:02.905854: step 6754, loss 0.317067, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:03.017142: step 6755, loss 0.249161, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:03.133626: step 6756, loss 0.317629, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:03.256943: step 6757, loss 0.165303, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:03.366412: step 6758, loss 0.304686, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:03.481255: step 6759, loss 0.260575, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:03.609074: step 6760, loss 0.231238, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:03.894588: step 6760, loss 0.292416, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6760

2017-10-09T15:20:04.600576: step 6761, loss 0.24694, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:04.699254: step 6762, loss 0.149135, acc 0.941176, learning_rate 0.0001
2017-10-09T15:20:04.812147: step 6763, loss 0.13661, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:04.928335: step 6764, loss 0.164123, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:05.037770: step 6765, loss 0.266316, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:05.153796: step 6766, loss 0.210456, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:05.266189: step 6767, loss 0.13584, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:05.380967: step 6768, loss 0.139859, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:05.490277: step 6769, loss 0.226225, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:05.612181: step 6770, loss 0.311872, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:05.746027: step 6771, loss 0.210452, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:05.872334: step 6772, loss 0.15435, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:06.000300: step 6773, loss 0.287643, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:06.116986: step 6774, loss 0.178772, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:06.226491: step 6775, loss 0.136256, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:06.342840: step 6776, loss 0.377323, acc 0.859375, learning_rate 0.0001
2017-10-09T15:20:06.453591: step 6777, loss 0.283723, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:06.566161: step 6778, loss 0.182117, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:06.683091: step 6779, loss 0.200382, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:06.799652: step 6780, loss 0.166518, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:06.909197: step 6781, loss 0.228597, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:07.018952: step 6782, loss 0.20937, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:07.134603: step 6783, loss 0.216306, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:07.256731: step 6784, loss 0.220623, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:07.380586: step 6785, loss 0.316496, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:07.503431: step 6786, loss 0.0542581, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:07.611447: step 6787, loss 0.159815, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:07.726501: step 6788, loss 0.171603, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:07.840486: step 6789, loss 0.266662, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:07.956316: step 6790, loss 0.179796, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:08.071585: step 6791, loss 0.184363, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:08.183072: step 6792, loss 0.163053, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:08.293944: step 6793, loss 0.446225, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:08.406425: step 6794, loss 0.173859, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:08.517532: step 6795, loss 0.215088, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:08.629039: step 6796, loss 0.214928, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:08.744403: step 6797, loss 0.224434, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:08.872814: step 6798, loss 0.195725, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:08.994736: step 6799, loss 0.102338, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:09.103248: step 6800, loss 0.217399, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:09.390838: step 6800, loss 0.292415, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6800

2017-10-09T15:20:09.931497: step 6801, loss 0.254758, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:10.041498: step 6802, loss 0.26108, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:10.153744: step 6803, loss 0.252236, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:10.271346: step 6804, loss 0.170945, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:10.387858: step 6805, loss 0.239674, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:10.500295: step 6806, loss 0.178361, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:10.611615: step 6807, loss 0.156207, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:10.725585: step 6808, loss 0.168957, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:10.842386: step 6809, loss 0.315277, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:10.954438: step 6810, loss 0.0915111, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:11.075602: step 6811, loss 0.169629, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:11.193004: step 6812, loss 0.219461, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:11.310006: step 6813, loss 0.283257, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:11.425120: step 6814, loss 0.210567, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:11.547755: step 6815, loss 0.198069, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:11.664938: step 6816, loss 0.144744, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:11.781873: step 6817, loss 0.284739, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:11.904442: step 6818, loss 0.179643, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:12.038804: step 6819, loss 0.133836, acc 1, learning_rate 0.0001
2017-10-09T15:20:12.145615: step 6820, loss 0.351743, acc 0.859375, learning_rate 0.0001
2017-10-09T15:20:12.255418: step 6821, loss 0.146182, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:12.364255: step 6822, loss 0.193707, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:12.473713: step 6823, loss 0.23713, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:12.588894: step 6824, loss 0.169221, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:12.704427: step 6825, loss 0.183741, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:12.827122: step 6826, loss 0.272908, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:12.943592: step 6827, loss 0.13023, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:13.056409: step 6828, loss 0.329091, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:13.171899: step 6829, loss 0.294809, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:13.281570: step 6830, loss 0.184219, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:13.392873: step 6831, loss 0.274217, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:13.505285: step 6832, loss 0.253799, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:13.615643: step 6833, loss 0.246011, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:13.727727: step 6834, loss 0.167054, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:13.838094: step 6835, loss 0.192877, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:13.950341: step 6836, loss 0.162502, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:14.056566: step 6837, loss 0.254966, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:14.162419: step 6838, loss 0.119719, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:14.281076: step 6839, loss 0.245608, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:14.401537: step 6840, loss 0.217372, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:14.668862: step 6840, loss 0.292833, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6840

2017-10-09T15:20:15.280568: step 6841, loss 0.248378, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:15.387071: step 6842, loss 0.127392, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:15.497476: step 6843, loss 0.211872, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:15.614102: step 6844, loss 0.245359, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:15.727664: step 6845, loss 0.147187, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:15.842104: step 6846, loss 0.218118, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:15.957086: step 6847, loss 0.133572, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:16.069781: step 6848, loss 0.248475, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:16.179826: step 6849, loss 0.217722, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:16.292212: step 6850, loss 0.151825, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:16.400922: step 6851, loss 0.241554, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:16.514849: step 6852, loss 0.262669, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:16.629092: step 6853, loss 0.235193, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:16.744382: step 6854, loss 0.20703, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:16.858786: step 6855, loss 0.173812, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:16.981189: step 6856, loss 0.207684, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:17.104318: step 6857, loss 0.35336, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:17.235672: step 6858, loss 0.201589, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:17.353506: step 6859, loss 0.166794, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:17.449751: step 6860, loss 0.148218, acc 0.960784, learning_rate 0.0001
2017-10-09T15:20:17.571784: step 6861, loss 0.175366, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:17.684826: step 6862, loss 0.105851, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:17.802170: step 6863, loss 0.217579, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:17.923818: step 6864, loss 0.265277, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:18.036443: step 6865, loss 0.260637, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:18.142873: step 6866, loss 0.155226, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:18.252680: step 6867, loss 0.190674, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:18.364410: step 6868, loss 0.145817, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:18.483187: step 6869, loss 0.0967584, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:18.586786: step 6870, loss 0.252413, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:18.697071: step 6871, loss 0.242738, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:18.807011: step 6872, loss 0.227573, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:18.914810: step 6873, loss 0.215416, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:19.025411: step 6874, loss 0.253094, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:19.134664: step 6875, loss 0.142806, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:19.251644: step 6876, loss 0.350465, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:19.368112: step 6877, loss 0.261144, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:19.479001: step 6878, loss 0.0950128, acc 1, learning_rate 0.0001
2017-10-09T15:20:19.587783: step 6879, loss 0.180086, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:19.700795: step 6880, loss 0.155938, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:19.960205: step 6880, loss 0.290172, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6880

2017-10-09T15:20:20.567984: step 6881, loss 0.144292, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:20.679271: step 6882, loss 0.129351, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:20.784671: step 6883, loss 0.158236, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:20.895090: step 6884, loss 0.189592, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:21.000263: step 6885, loss 0.164903, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:21.104146: step 6886, loss 0.211486, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:21.209693: step 6887, loss 0.162722, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:21.318095: step 6888, loss 0.354507, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:21.424563: step 6889, loss 0.335426, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:21.535114: step 6890, loss 0.237932, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:21.644169: step 6891, loss 0.190786, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:21.754224: step 6892, loss 0.298596, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:21.859594: step 6893, loss 0.217684, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:21.972075: step 6894, loss 0.184645, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:22.076070: step 6895, loss 0.31213, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:22.184352: step 6896, loss 0.187719, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:22.295656: step 6897, loss 0.194421, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:22.404691: step 6898, loss 0.157429, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:22.511903: step 6899, loss 0.152216, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:22.624531: step 6900, loss 0.112116, acc 1, learning_rate 0.0001
2017-10-09T15:20:22.730537: step 6901, loss 0.243449, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:22.838723: step 6902, loss 0.195873, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:22.945745: step 6903, loss 0.116178, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:23.051317: step 6904, loss 0.119072, acc 1, learning_rate 0.0001
2017-10-09T15:20:23.164005: step 6905, loss 0.14324, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:23.269857: step 6906, loss 0.27282, acc 0.84375, learning_rate 0.0001
2017-10-09T15:20:23.376970: step 6907, loss 0.165884, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:23.483802: step 6908, loss 0.196992, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:23.592090: step 6909, loss 0.279547, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:23.705953: step 6910, loss 0.282949, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:23.823070: step 6911, loss 0.137358, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:23.928178: step 6912, loss 0.187762, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:24.037047: step 6913, loss 0.120432, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:24.147697: step 6914, loss 0.192349, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:24.255526: step 6915, loss 0.206472, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:24.360509: step 6916, loss 0.163327, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:24.471382: step 6917, loss 0.184469, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:24.587605: step 6918, loss 0.15845, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:24.700043: step 6919, loss 0.177953, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:24.814236: step 6920, loss 0.242758, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:25.074237: step 6920, loss 0.292714, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6920

2017-10-09T15:20:25.771868: step 6921, loss 0.173792, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:25.880615: step 6922, loss 0.160614, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:25.986597: step 6923, loss 0.238507, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:26.094190: step 6924, loss 0.270727, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:26.202853: step 6925, loss 0.150536, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:26.313398: step 6926, loss 0.17143, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:26.419290: step 6927, loss 0.35113, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:26.528755: step 6928, loss 0.449459, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:26.636520: step 6929, loss 0.205361, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:26.745888: step 6930, loss 0.367541, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:26.861682: step 6931, loss 0.352823, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:26.971192: step 6932, loss 0.0597617, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:27.077918: step 6933, loss 0.282316, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:27.188123: step 6934, loss 0.264502, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:27.297736: step 6935, loss 0.167471, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:27.410684: step 6936, loss 0.253091, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:27.525984: step 6937, loss 0.170703, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:27.632904: step 6938, loss 0.114026, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:27.747607: step 6939, loss 0.273744, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:27.863095: step 6940, loss 0.205475, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:27.975894: step 6941, loss 0.13515, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:28.082957: step 6942, loss 0.246418, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:28.196273: step 6943, loss 0.142074, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:28.302953: step 6944, loss 0.166827, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:28.416233: step 6945, loss 0.141627, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:28.528390: step 6946, loss 0.10201, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:28.639295: step 6947, loss 0.291948, acc 0.84375, learning_rate 0.0001
2017-10-09T15:20:28.743694: step 6948, loss 0.288829, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:28.856175: step 6949, loss 0.203383, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:28.964500: step 6950, loss 0.252144, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:29.075943: step 6951, loss 0.272512, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:29.181906: step 6952, loss 0.288286, acc 0.859375, learning_rate 0.0001
2017-10-09T15:20:29.292092: step 6953, loss 0.244324, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:29.406240: step 6954, loss 0.189979, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:29.514689: step 6955, loss 0.174146, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:29.627724: step 6956, loss 0.283252, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:29.733756: step 6957, loss 0.109005, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:29.826238: step 6958, loss 0.234747, acc 0.941176, learning_rate 0.0001
2017-10-09T15:20:29.937677: step 6959, loss 0.0928807, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:30.043945: step 6960, loss 0.225258, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:30.317169: step 6960, loss 0.289485, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-6960

2017-10-09T15:20:30.848122: step 6961, loss 0.258473, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:30.960096: step 6962, loss 0.166237, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:31.066559: step 6963, loss 0.158991, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:31.179259: step 6964, loss 0.167013, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:31.284427: step 6965, loss 0.146357, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:31.391589: step 6966, loss 0.146844, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:31.500208: step 6967, loss 0.188094, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:31.610444: step 6968, loss 0.245711, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:31.723480: step 6969, loss 0.339399, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:31.835491: step 6970, loss 0.243417, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:31.948469: step 6971, loss 0.155545, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:32.057335: step 6972, loss 0.181769, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:32.165796: step 6973, loss 0.179237, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:32.272900: step 6974, loss 0.233425, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:32.378007: step 6975, loss 0.194449, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:32.483926: step 6976, loss 0.166625, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:32.593338: step 6977, loss 0.270878, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:32.699019: step 6978, loss 0.268797, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:32.811723: step 6979, loss 0.0705192, acc 1, learning_rate 0.0001
2017-10-09T15:20:32.922564: step 6980, loss 0.240764, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:33.034569: step 6981, loss 0.225389, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:33.143929: step 6982, loss 0.263452, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:33.253167: step 6983, loss 0.175352, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:33.362289: step 6984, loss 0.349786, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:33.472982: step 6985, loss 0.262727, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:33.583184: step 6986, loss 0.218346, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:33.692315: step 6987, loss 0.216302, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:33.805648: step 6988, loss 0.128221, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:33.918809: step 6989, loss 0.130739, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:34.023810: step 6990, loss 0.129705, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:34.129017: step 6991, loss 0.186769, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:34.241207: step 6992, loss 0.155331, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:34.348607: step 6993, loss 0.25298, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:34.456772: step 6994, loss 0.199145, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:34.568065: step 6995, loss 0.351203, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:34.677247: step 6996, loss 0.158497, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:34.785210: step 6997, loss 0.243043, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:34.899477: step 6998, loss 0.136102, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:35.011527: step 6999, loss 0.235402, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:35.120278: step 7000, loss 0.056585, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:35.382650: step 7000, loss 0.288547, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7000

2017-10-09T15:20:35.979205: step 7001, loss 0.163253, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:36.086596: step 7002, loss 0.157158, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:36.196982: step 7003, loss 0.137523, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:36.299232: step 7004, loss 0.294517, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:36.410122: step 7005, loss 0.242843, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:36.520016: step 7006, loss 0.258873, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:36.631448: step 7007, loss 0.199917, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:36.740684: step 7008, loss 0.199545, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:36.848074: step 7009, loss 0.216196, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:36.958115: step 7010, loss 0.141903, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:37.070321: step 7011, loss 0.207001, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:37.179294: step 7012, loss 0.250344, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:37.285149: step 7013, loss 0.127838, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:37.394040: step 7014, loss 0.222986, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:37.507398: step 7015, loss 0.162389, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:37.619963: step 7016, loss 0.168144, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:37.734355: step 7017, loss 0.196141, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:37.848357: step 7018, loss 0.172199, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:37.963018: step 7019, loss 0.299084, acc 0.859375, learning_rate 0.0001
2017-10-09T15:20:38.084971: step 7020, loss 0.199144, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:38.208427: step 7021, loss 0.170347, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:38.328631: step 7022, loss 0.270636, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:38.436118: step 7023, loss 0.240551, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:38.546616: step 7024, loss 0.258565, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:38.661466: step 7025, loss 0.219657, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:38.770286: step 7026, loss 0.266195, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:38.878447: step 7027, loss 0.42426, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:38.993898: step 7028, loss 0.231008, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:39.104071: step 7029, loss 0.170498, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:39.212024: step 7030, loss 0.267244, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:39.324945: step 7031, loss 0.217211, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:39.434860: step 7032, loss 0.213192, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:39.548238: step 7033, loss 0.241749, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:39.657585: step 7034, loss 0.164224, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:39.769978: step 7035, loss 0.188323, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:39.883450: step 7036, loss 0.193777, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:39.994035: step 7037, loss 0.168771, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:40.105656: step 7038, loss 0.233094, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:40.217712: step 7039, loss 0.27852, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:40.326405: step 7040, loss 0.140261, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:40.610940: step 7040, loss 0.290826, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7040

2017-10-09T15:20:41.223184: step 7041, loss 0.179454, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:41.332904: step 7042, loss 0.112158, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:41.441453: step 7043, loss 0.188396, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:41.557257: step 7044, loss 0.144178, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:41.676573: step 7045, loss 0.101783, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:41.786565: step 7046, loss 0.209527, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:41.906868: step 7047, loss 0.114534, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:42.027507: step 7048, loss 0.291283, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:42.149210: step 7049, loss 0.17678, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:42.263918: step 7050, loss 0.12131, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:42.373608: step 7051, loss 0.273246, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:42.491064: step 7052, loss 0.337661, acc 0.859375, learning_rate 0.0001
2017-10-09T15:20:42.597420: step 7053, loss 0.124069, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:42.728460: step 7054, loss 0.174938, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:42.856800: step 7055, loss 0.198157, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:42.949723: step 7056, loss 0.230352, acc 0.921569, learning_rate 0.0001
2017-10-09T15:20:43.065987: step 7057, loss 0.180613, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:43.176421: step 7058, loss 0.219127, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:43.292920: step 7059, loss 0.130973, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:43.409481: step 7060, loss 0.139631, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:43.516869: step 7061, loss 0.199633, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:43.629141: step 7062, loss 0.220227, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:43.735237: step 7063, loss 0.192843, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:43.853597: step 7064, loss 0.128027, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:43.966003: step 7065, loss 0.30034, acc 0.859375, learning_rate 0.0001
2017-10-09T15:20:44.077925: step 7066, loss 0.223191, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:44.197597: step 7067, loss 0.301156, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:44.311701: step 7068, loss 0.171463, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:44.429840: step 7069, loss 0.140864, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:44.535350: step 7070, loss 0.22882, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:44.650088: step 7071, loss 0.28916, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:44.759993: step 7072, loss 0.149667, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:44.885104: step 7073, loss 0.312867, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:44.997526: step 7074, loss 0.236306, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:45.108386: step 7075, loss 0.280221, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:45.215884: step 7076, loss 0.115613, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:45.323073: step 7077, loss 0.233142, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:45.431443: step 7078, loss 0.244745, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:45.543552: step 7079, loss 0.165819, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:45.655993: step 7080, loss 0.147205, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:45.931574: step 7080, loss 0.289162, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7080

2017-10-09T15:20:46.619960: step 7081, loss 0.138657, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:46.729799: step 7082, loss 0.236399, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:46.845444: step 7083, loss 0.2711, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:46.963506: step 7084, loss 0.243653, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:47.081556: step 7085, loss 0.1904, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:47.192763: step 7086, loss 0.175715, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:47.298890: step 7087, loss 0.252266, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:47.413783: step 7088, loss 0.212327, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:47.526882: step 7089, loss 0.176137, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:47.632159: step 7090, loss 0.181809, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:47.745592: step 7091, loss 0.17612, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:47.855248: step 7092, loss 0.22477, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:47.968441: step 7093, loss 0.235508, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:48.076868: step 7094, loss 0.267945, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:48.186332: step 7095, loss 0.168604, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:48.300168: step 7096, loss 0.166719, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:48.410561: step 7097, loss 0.250732, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:48.517270: step 7098, loss 0.205214, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:48.623968: step 7099, loss 0.174853, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:48.740438: step 7100, loss 0.151214, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:48.851152: step 7101, loss 0.189483, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:48.963444: step 7102, loss 0.131235, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:49.071744: step 7103, loss 0.174236, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:49.185107: step 7104, loss 0.279822, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:49.295737: step 7105, loss 0.142593, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:49.405044: step 7106, loss 0.142655, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:49.518590: step 7107, loss 0.293277, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:49.625687: step 7108, loss 0.150723, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:49.741183: step 7109, loss 0.162551, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:49.846871: step 7110, loss 0.16574, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:49.953345: step 7111, loss 0.301292, acc 0.859375, learning_rate 0.0001
2017-10-09T15:20:50.064199: step 7112, loss 0.237053, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:50.174039: step 7113, loss 0.202363, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:50.280810: step 7114, loss 0.12764, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:50.391072: step 7115, loss 0.139721, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:50.503029: step 7116, loss 0.157939, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:50.608955: step 7117, loss 0.247112, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:50.719671: step 7118, loss 0.163951, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:50.831293: step 7119, loss 0.238428, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:50.943894: step 7120, loss 0.139524, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:51.210857: step 7120, loss 0.287839, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7120

2017-10-09T15:20:51.764636: step 7121, loss 0.159566, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:51.878907: step 7122, loss 0.245539, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:51.984979: step 7123, loss 0.249341, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:52.091347: step 7124, loss 0.205491, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:52.197245: step 7125, loss 0.286686, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:52.305563: step 7126, loss 0.146463, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:52.416427: step 7127, loss 0.180918, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:52.525655: step 7128, loss 0.190832, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:52.629858: step 7129, loss 0.243489, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:52.737401: step 7130, loss 0.175282, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:52.848211: step 7131, loss 0.258405, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:52.959427: step 7132, loss 0.312414, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:53.064853: step 7133, loss 0.227448, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:53.172998: step 7134, loss 0.329104, acc 0.84375, learning_rate 0.0001
2017-10-09T15:20:53.283286: step 7135, loss 0.221158, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:53.395267: step 7136, loss 0.150135, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:53.506742: step 7137, loss 0.191091, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:53.618544: step 7138, loss 0.187722, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:53.728047: step 7139, loss 0.255146, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:53.835734: step 7140, loss 0.193011, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:53.945804: step 7141, loss 0.288567, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:54.057570: step 7142, loss 0.243194, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:54.162375: step 7143, loss 0.163057, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:54.275101: step 7144, loss 0.22144, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:54.382578: step 7145, loss 0.123611, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:54.488272: step 7146, loss 0.156619, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:54.599970: step 7147, loss 0.136311, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:54.709739: step 7148, loss 0.234148, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:54.819707: step 7149, loss 0.184953, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:54.931317: step 7150, loss 0.16962, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:55.037864: step 7151, loss 0.112942, acc 0.984375, learning_rate 0.0001
2017-10-09T15:20:55.146727: step 7152, loss 0.169106, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:55.259642: step 7153, loss 0.161224, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:55.352444: step 7154, loss 0.223991, acc 0.901961, learning_rate 0.0001
2017-10-09T15:20:55.463943: step 7155, loss 0.208037, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:55.576431: step 7156, loss 0.24395, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:55.686994: step 7157, loss 0.253322, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:55.791702: step 7158, loss 0.229746, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:55.902701: step 7159, loss 0.317074, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:56.011874: step 7160, loss 0.167352, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:20:56.271086: step 7160, loss 0.289524, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7160

2017-10-09T15:20:56.875188: step 7161, loss 0.157332, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:56.983459: step 7162, loss 0.185873, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:57.090371: step 7163, loss 0.110191, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:57.201640: step 7164, loss 0.137751, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:57.314704: step 7165, loss 0.190587, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:57.424165: step 7166, loss 0.0972801, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:57.535441: step 7167, loss 0.376978, acc 0.828125, learning_rate 0.0001
2017-10-09T15:20:57.645049: step 7168, loss 0.12242, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:57.751504: step 7169, loss 0.224014, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:57.862584: step 7170, loss 0.173897, acc 0.953125, learning_rate 0.0001
2017-10-09T15:20:57.969428: step 7171, loss 0.0950514, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:58.078056: step 7172, loss 0.246448, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:58.183138: step 7173, loss 0.223448, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:58.286825: step 7174, loss 0.333769, acc 0.875, learning_rate 0.0001
2017-10-09T15:20:58.392085: step 7175, loss 0.262112, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:58.506311: step 7176, loss 0.203708, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:58.611712: step 7177, loss 0.208514, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:58.719204: step 7178, loss 0.233737, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:58.827726: step 7179, loss 0.157591, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:58.939459: step 7180, loss 0.14798, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:59.055849: step 7181, loss 0.198839, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:59.163504: step 7182, loss 0.317333, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:59.284534: step 7183, loss 0.0990247, acc 1, learning_rate 0.0001
2017-10-09T15:20:59.396017: step 7184, loss 0.126045, acc 0.96875, learning_rate 0.0001
2017-10-09T15:20:59.510137: step 7185, loss 0.355828, acc 0.890625, learning_rate 0.0001
2017-10-09T15:20:59.623563: step 7186, loss 0.239226, acc 0.90625, learning_rate 0.0001
2017-10-09T15:20:59.732346: step 7187, loss 0.204046, acc 0.921875, learning_rate 0.0001
2017-10-09T15:20:59.844997: step 7188, loss 0.182899, acc 0.9375, learning_rate 0.0001
2017-10-09T15:20:59.955678: step 7189, loss 0.235582, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:00.067699: step 7190, loss 0.113247, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:00.186990: step 7191, loss 0.130512, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:00.295483: step 7192, loss 0.222962, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:00.406811: step 7193, loss 0.211518, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:00.514633: step 7194, loss 0.211275, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:00.630427: step 7195, loss 0.212749, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:00.740514: step 7196, loss 0.156793, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:00.865500: step 7197, loss 0.15153, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:00.992267: step 7198, loss 0.185173, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:01.112892: step 7199, loss 0.125081, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:01.221560: step 7200, loss 0.11151, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:01.497943: step 7200, loss 0.287513, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7200

2017-10-09T15:21:02.122869: step 7201, loss 0.337844, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:02.243277: step 7202, loss 0.130154, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:02.357740: step 7203, loss 0.27496, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:02.475306: step 7204, loss 0.14358, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:02.583948: step 7205, loss 0.150094, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:02.700451: step 7206, loss 0.215356, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:02.813505: step 7207, loss 0.216168, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:02.935293: step 7208, loss 0.163207, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:03.047125: step 7209, loss 0.246861, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:03.164729: step 7210, loss 0.343666, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:03.281429: step 7211, loss 0.207641, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:03.394601: step 7212, loss 0.257757, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:03.507283: step 7213, loss 0.200359, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:03.618511: step 7214, loss 0.227896, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:03.740136: step 7215, loss 0.355324, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:03.859860: step 7216, loss 0.167636, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:03.969086: step 7217, loss 0.127581, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:04.079882: step 7218, loss 0.121754, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:04.195051: step 7219, loss 0.146699, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:04.312431: step 7220, loss 0.1991, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:04.433071: step 7221, loss 0.0840117, acc 1, learning_rate 0.0001
2017-10-09T15:21:04.551300: step 7222, loss 0.265614, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:04.671960: step 7223, loss 0.159697, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:04.775115: step 7224, loss 0.181483, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:04.894135: step 7225, loss 0.181318, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:05.012514: step 7226, loss 0.29476, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:05.126257: step 7227, loss 0.315569, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:05.245353: step 7228, loss 0.177769, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:05.361863: step 7229, loss 0.211725, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:05.469387: step 7230, loss 0.110766, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:05.581024: step 7231, loss 0.0709241, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:05.695140: step 7232, loss 0.195053, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:05.810625: step 7233, loss 0.137762, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:05.935177: step 7234, loss 0.163361, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:06.050592: step 7235, loss 0.181303, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:06.161872: step 7236, loss 0.163187, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:06.271912: step 7237, loss 0.136356, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:06.393430: step 7238, loss 0.206901, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:06.511942: step 7239, loss 0.359476, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:06.628060: step 7240, loss 0.231172, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:06.906808: step 7240, loss 0.288643, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7240

2017-10-09T15:21:07.658384: step 7241, loss 0.236962, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:07.776578: step 7242, loss 0.207287, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:07.900683: step 7243, loss 0.181193, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:08.014336: step 7244, loss 0.201304, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:08.121313: step 7245, loss 0.290843, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:08.232523: step 7246, loss 0.221428, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:08.346898: step 7247, loss 0.347535, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:08.469493: step 7248, loss 0.184816, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:08.593809: step 7249, loss 0.176899, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:08.712288: step 7250, loss 0.216993, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:08.821182: step 7251, loss 0.199134, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:08.920638: step 7252, loss 0.0803213, acc 0.980392, learning_rate 0.0001
2017-10-09T15:21:09.036246: step 7253, loss 0.180256, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:09.153566: step 7254, loss 0.0734317, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:09.273835: step 7255, loss 0.173931, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:09.400761: step 7256, loss 0.176939, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:09.510356: step 7257, loss 0.256084, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:09.618368: step 7258, loss 0.177761, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:09.727553: step 7259, loss 0.21483, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:09.836244: step 7260, loss 0.266533, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:09.951564: step 7261, loss 0.204862, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:10.063621: step 7262, loss 0.235826, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:10.180203: step 7263, loss 0.227631, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:10.292498: step 7264, loss 0.174251, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:10.404198: step 7265, loss 0.288245, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:10.516898: step 7266, loss 0.231554, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:10.627144: step 7267, loss 0.156051, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:10.737820: step 7268, loss 0.0989245, acc 1, learning_rate 0.0001
2017-10-09T15:21:10.856303: step 7269, loss 0.282787, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:10.978222: step 7270, loss 0.264351, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:11.086276: step 7271, loss 0.136453, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:11.194605: step 7272, loss 0.124858, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:11.307777: step 7273, loss 0.159169, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:11.424360: step 7274, loss 0.135542, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:11.538442: step 7275, loss 0.17962, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:11.650501: step 7276, loss 0.186175, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:11.761172: step 7277, loss 0.189931, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:11.875510: step 7278, loss 0.3204, acc 0.84375, learning_rate 0.0001
2017-10-09T15:21:11.982963: step 7279, loss 0.161775, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:12.092116: step 7280, loss 0.256862, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:12.355227: step 7280, loss 0.2857, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7280

2017-10-09T15:21:12.899765: step 7281, loss 0.200406, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:13.007928: step 7282, loss 0.239767, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:13.118751: step 7283, loss 0.120997, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:13.228316: step 7284, loss 0.332912, acc 0.859375, learning_rate 0.0001
2017-10-09T15:21:13.342380: step 7285, loss 0.174708, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:13.452906: step 7286, loss 0.128382, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:13.567312: step 7287, loss 0.27463, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:13.680218: step 7288, loss 0.188629, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:13.792968: step 7289, loss 0.170168, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:13.908298: step 7290, loss 0.295363, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:14.018435: step 7291, loss 0.113163, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:14.127710: step 7292, loss 0.258715, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:14.233006: step 7293, loss 0.101131, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:14.343099: step 7294, loss 0.186192, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:14.448325: step 7295, loss 0.278741, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:14.555105: step 7296, loss 0.260228, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:14.660456: step 7297, loss 0.161013, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:14.775979: step 7298, loss 0.210243, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:14.887087: step 7299, loss 0.212356, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:14.993897: step 7300, loss 0.228294, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:15.111043: step 7301, loss 0.0912405, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:15.224149: step 7302, loss 0.13377, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:15.343507: step 7303, loss 0.299341, acc 0.859375, learning_rate 0.0001
2017-10-09T15:21:15.459954: step 7304, loss 0.232914, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:15.570625: step 7305, loss 0.239508, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:15.684392: step 7306, loss 0.249512, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:15.799399: step 7307, loss 0.132269, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:15.907835: step 7308, loss 0.209022, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:16.019229: step 7309, loss 0.213905, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:16.131995: step 7310, loss 0.235714, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:16.244613: step 7311, loss 0.0892347, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:16.352774: step 7312, loss 0.221553, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:16.465353: step 7313, loss 0.191849, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:16.577730: step 7314, loss 0.209393, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:16.690832: step 7315, loss 0.141667, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:16.804679: step 7316, loss 0.289879, acc 0.859375, learning_rate 0.0001
2017-10-09T15:21:16.910061: step 7317, loss 0.1666, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:17.021060: step 7318, loss 0.178815, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:17.136801: step 7319, loss 0.113182, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:17.251877: step 7320, loss 0.217581, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:17.522488: step 7320, loss 0.288208, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7320

2017-10-09T15:21:18.129438: step 7321, loss 0.135135, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:18.239537: step 7322, loss 0.12514, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:18.352879: step 7323, loss 0.210679, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:18.460108: step 7324, loss 0.282665, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:18.573453: step 7325, loss 0.117491, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:18.688292: step 7326, loss 0.43539, acc 0.8125, learning_rate 0.0001
2017-10-09T15:21:18.801014: step 7327, loss 0.172178, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:18.913778: step 7328, loss 0.342457, acc 0.859375, learning_rate 0.0001
2017-10-09T15:21:19.025894: step 7329, loss 0.173926, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:19.138562: step 7330, loss 0.188972, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:19.244874: step 7331, loss 0.283129, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:19.360978: step 7332, loss 0.216548, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:19.477304: step 7333, loss 0.145248, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:19.593415: step 7334, loss 0.130915, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:19.708541: step 7335, loss 0.176886, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:19.822740: step 7336, loss 0.258721, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:19.930149: step 7337, loss 0.278133, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:20.040767: step 7338, loss 0.108532, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:20.155808: step 7339, loss 0.159185, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:20.261788: step 7340, loss 0.118465, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:20.366488: step 7341, loss 0.130811, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:20.475419: step 7342, loss 0.233434, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:20.590879: step 7343, loss 0.30301, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:20.699421: step 7344, loss 0.187664, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:20.813407: step 7345, loss 0.265468, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:20.929055: step 7346, loss 0.197277, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:21.041358: step 7347, loss 0.266273, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:21.151454: step 7348, loss 0.224895, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:21.262143: step 7349, loss 0.236294, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:21.354903: step 7350, loss 0.115775, acc 0.960784, learning_rate 0.0001
2017-10-09T15:21:21.473284: step 7351, loss 0.261868, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:21.585421: step 7352, loss 0.268879, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:21.695615: step 7353, loss 0.20102, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:21.813883: step 7354, loss 0.137093, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:21.933564: step 7355, loss 0.17169, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:22.049562: step 7356, loss 0.190904, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:22.164090: step 7357, loss 0.299828, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:22.279773: step 7358, loss 0.162649, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:22.396555: step 7359, loss 0.178887, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:22.509091: step 7360, loss 0.214948, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:22.778965: step 7360, loss 0.284975, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7360

2017-10-09T15:21:23.393707: step 7361, loss 0.156951, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:23.512770: step 7362, loss 0.299364, acc 0.859375, learning_rate 0.0001
2017-10-09T15:21:23.623998: step 7363, loss 0.151816, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:23.741730: step 7364, loss 0.256906, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:23.854523: step 7365, loss 0.273681, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:23.960957: step 7366, loss 0.217106, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:24.067656: step 7367, loss 0.259586, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:24.184886: step 7368, loss 0.245453, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:24.307799: step 7369, loss 0.110824, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:24.422386: step 7370, loss 0.19162, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:24.539164: step 7371, loss 0.163238, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:24.656198: step 7372, loss 0.254202, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:24.767892: step 7373, loss 0.241989, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:24.881192: step 7374, loss 0.169454, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:24.994826: step 7375, loss 0.275657, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:25.108325: step 7376, loss 0.291728, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:25.224777: step 7377, loss 0.185469, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:25.336944: step 7378, loss 0.0751004, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:25.453282: step 7379, loss 0.126999, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:25.570986: step 7380, loss 0.232413, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:25.684047: step 7381, loss 0.314791, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:25.795937: step 7382, loss 0.234439, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:25.909118: step 7383, loss 0.232086, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:26.023812: step 7384, loss 0.171671, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:26.143057: step 7385, loss 0.114661, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:26.256958: step 7386, loss 0.212329, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:26.378796: step 7387, loss 0.234273, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:26.494134: step 7388, loss 0.190544, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:26.609952: step 7389, loss 0.183908, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:26.724676: step 7390, loss 0.26374, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:26.840897: step 7391, loss 0.0926806, acc 1, learning_rate 0.0001
2017-10-09T15:21:26.957130: step 7392, loss 0.124428, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:27.068266: step 7393, loss 0.214664, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:27.184206: step 7394, loss 0.152259, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:27.297850: step 7395, loss 0.162706, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:27.412723: step 7396, loss 0.288735, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:27.530799: step 7397, loss 0.171111, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:27.646881: step 7398, loss 0.218349, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:27.761049: step 7399, loss 0.154779, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:27.878406: step 7400, loss 0.218591, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:28.149132: step 7400, loss 0.283481, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7400

2017-10-09T15:21:28.832158: step 7401, loss 0.178989, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:28.949496: step 7402, loss 0.166482, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:29.062690: step 7403, loss 0.148357, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:29.180835: step 7404, loss 0.185539, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:29.296514: step 7405, loss 0.143669, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:29.416310: step 7406, loss 0.416949, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:29.538068: step 7407, loss 0.12692, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:29.659841: step 7408, loss 0.185267, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:29.778251: step 7409, loss 0.240804, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:29.902670: step 7410, loss 0.243349, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:30.017179: step 7411, loss 0.137427, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:30.138811: step 7412, loss 0.21145, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:30.255892: step 7413, loss 0.0901884, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:30.377596: step 7414, loss 0.254553, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:30.491228: step 7415, loss 0.109427, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:30.612154: step 7416, loss 0.196461, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:30.725685: step 7417, loss 0.442243, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:30.843061: step 7418, loss 0.202867, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:30.960886: step 7419, loss 0.270395, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:31.070799: step 7420, loss 0.21959, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:31.189442: step 7421, loss 0.234547, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:31.311315: step 7422, loss 0.363969, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:31.426577: step 7423, loss 0.257154, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:31.538850: step 7424, loss 0.139099, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:31.660352: step 7425, loss 0.0950745, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:31.779105: step 7426, loss 0.147327, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:31.896633: step 7427, loss 0.236474, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:32.019700: step 7428, loss 0.144394, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:32.141348: step 7429, loss 0.189131, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:32.260329: step 7430, loss 0.105231, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:32.376229: step 7431, loss 0.1485, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:32.498058: step 7432, loss 0.158408, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:32.619725: step 7433, loss 0.233696, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:32.735184: step 7434, loss 0.223288, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:32.853727: step 7435, loss 0.346164, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:32.967150: step 7436, loss 0.17772, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:33.085762: step 7437, loss 0.0733112, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:33.199899: step 7438, loss 0.256263, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:33.317223: step 7439, loss 0.0947694, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:33.430561: step 7440, loss 0.11661, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:33.735122: step 7440, loss 0.284349, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7440

2017-10-09T15:21:34.322005: step 7441, loss 0.139151, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:34.441689: step 7442, loss 0.20948, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:34.554809: step 7443, loss 0.242416, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:34.667162: step 7444, loss 0.171409, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:34.773708: step 7445, loss 0.210448, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:34.887105: step 7446, loss 0.16639, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:34.997762: step 7447, loss 0.0860205, acc 1, learning_rate 0.0001
2017-10-09T15:21:35.086488: step 7448, loss 0.2434, acc 0.921569, learning_rate 0.0001
2017-10-09T15:21:35.194479: step 7449, loss 0.237507, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:35.306383: step 7450, loss 0.314326, acc 0.859375, learning_rate 0.0001
2017-10-09T15:21:35.413064: step 7451, loss 0.138298, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:35.522506: step 7452, loss 0.170013, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:35.630743: step 7453, loss 0.248804, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:35.742733: step 7454, loss 0.24726, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:35.858698: step 7455, loss 0.171828, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:35.978959: step 7456, loss 0.215028, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:36.093677: step 7457, loss 0.255558, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:36.205898: step 7458, loss 0.27889, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:36.319537: step 7459, loss 0.217169, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:36.434023: step 7460, loss 0.173703, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:36.540029: step 7461, loss 0.120217, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:36.656778: step 7462, loss 0.370664, acc 0.84375, learning_rate 0.0001
2017-10-09T15:21:36.764276: step 7463, loss 0.0851516, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:36.870743: step 7464, loss 0.117336, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:36.982988: step 7465, loss 0.188829, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:37.089268: step 7466, loss 0.22744, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:37.203255: step 7467, loss 0.185339, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:37.314331: step 7468, loss 0.198298, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:37.432008: step 7469, loss 0.128938, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:37.546587: step 7470, loss 0.276196, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:37.664260: step 7471, loss 0.238055, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:37.773727: step 7472, loss 0.135043, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:37.882187: step 7473, loss 0.286385, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:37.990305: step 7474, loss 0.26785, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:38.105258: step 7475, loss 0.209217, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:38.217901: step 7476, loss 0.125609, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:38.342830: step 7477, loss 0.127892, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:38.459068: step 7478, loss 0.190581, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:38.567893: step 7479, loss 0.254655, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:38.673755: step 7480, loss 0.249982, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:38.939932: step 7480, loss 0.286173, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7480

2017-10-09T15:21:39.552571: step 7481, loss 0.270428, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:39.662222: step 7482, loss 0.107765, acc 1, learning_rate 0.0001
2017-10-09T15:21:39.780001: step 7483, loss 0.22301, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:39.890631: step 7484, loss 0.101249, acc 1, learning_rate 0.0001
2017-10-09T15:21:40.002788: step 7485, loss 0.119601, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:40.113518: step 7486, loss 0.267614, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:40.225938: step 7487, loss 0.174715, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:40.339736: step 7488, loss 0.2097, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:40.456003: step 7489, loss 0.238975, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:40.567647: step 7490, loss 0.193963, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:40.679915: step 7491, loss 0.212187, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:40.785706: step 7492, loss 0.215016, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:40.898646: step 7493, loss 0.115687, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:41.020166: step 7494, loss 0.222962, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:41.137794: step 7495, loss 0.199945, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:41.253362: step 7496, loss 0.134398, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:41.366270: step 7497, loss 0.233466, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:41.470153: step 7498, loss 0.154613, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:41.579036: step 7499, loss 0.213134, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:41.687674: step 7500, loss 0.112435, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:41.803325: step 7501, loss 0.126477, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:41.917498: step 7502, loss 0.202644, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:42.039936: step 7503, loss 0.217547, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:42.153675: step 7504, loss 0.248023, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:42.261219: step 7505, loss 0.12301, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:42.370850: step 7506, loss 0.208291, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:42.485049: step 7507, loss 0.132663, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:42.602977: step 7508, loss 0.132607, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:42.718052: step 7509, loss 0.196486, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:42.842046: step 7510, loss 0.17783, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:42.968784: step 7511, loss 0.250182, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:43.080781: step 7512, loss 0.0938948, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:43.190263: step 7513, loss 0.125473, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:43.296913: step 7514, loss 0.213249, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:43.409603: step 7515, loss 0.246111, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:43.517649: step 7516, loss 0.242204, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:43.629248: step 7517, loss 0.129388, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:43.744015: step 7518, loss 0.135516, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:43.850606: step 7519, loss 0.273128, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:43.964576: step 7520, loss 0.148261, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:44.234481: step 7520, loss 0.285587, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7520

2017-10-09T15:21:44.847523: step 7521, loss 0.0793713, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:44.963850: step 7522, loss 0.120797, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:45.080680: step 7523, loss 0.175595, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:45.191959: step 7524, loss 0.14661, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:45.301677: step 7525, loss 0.187762, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:45.408593: step 7526, loss 0.227623, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:45.522383: step 7527, loss 0.221726, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:45.640086: step 7528, loss 0.14008, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:45.759286: step 7529, loss 0.115087, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:45.877582: step 7530, loss 0.324364, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:45.992654: step 7531, loss 0.264089, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:46.102591: step 7532, loss 0.263113, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:46.217317: step 7533, loss 0.320853, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:46.329135: step 7534, loss 0.167249, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:46.442151: step 7535, loss 0.156991, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:46.556810: step 7536, loss 0.122469, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:46.668825: step 7537, loss 0.132795, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:46.776503: step 7538, loss 0.261879, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:46.889750: step 7539, loss 0.154694, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:47.006149: step 7540, loss 0.118729, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:47.120051: step 7541, loss 0.166428, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:47.227565: step 7542, loss 0.125899, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:47.340884: step 7543, loss 0.184521, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:47.447377: step 7544, loss 0.158487, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:47.560289: step 7545, loss 0.219165, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:47.651302: step 7546, loss 0.28994, acc 0.862745, learning_rate 0.0001
2017-10-09T15:21:47.765357: step 7547, loss 0.163345, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:47.883656: step 7548, loss 0.248713, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:47.997651: step 7549, loss 0.123268, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:48.106826: step 7550, loss 0.180653, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:48.228296: step 7551, loss 0.216126, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:48.335358: step 7552, loss 0.15313, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:48.449649: step 7553, loss 0.109705, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:48.568612: step 7554, loss 0.350378, acc 0.859375, learning_rate 0.0001
2017-10-09T15:21:48.676703: step 7555, loss 0.218478, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:48.791030: step 7556, loss 0.144658, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:48.906108: step 7557, loss 0.296193, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:49.016997: step 7558, loss 0.158728, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:49.127515: step 7559, loss 0.20986, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:49.244325: step 7560, loss 0.293755, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:49.523209: step 7560, loss 0.283598, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7560

2017-10-09T15:21:50.200949: step 7561, loss 0.176039, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:50.311954: step 7562, loss 0.238684, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:50.425377: step 7563, loss 0.2343, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:50.546678: step 7564, loss 0.163027, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:50.663194: step 7565, loss 0.181072, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:50.775919: step 7566, loss 0.286333, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:50.892158: step 7567, loss 0.139574, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:51.002578: step 7568, loss 0.119482, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:51.114189: step 7569, loss 0.317703, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:51.225505: step 7570, loss 0.206671, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:51.334626: step 7571, loss 0.173535, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:51.444043: step 7572, loss 0.201487, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:51.556112: step 7573, loss 0.213318, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:51.667845: step 7574, loss 0.238249, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:51.775981: step 7575, loss 0.189013, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:51.890312: step 7576, loss 0.185097, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:52.001983: step 7577, loss 0.188206, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:52.112098: step 7578, loss 0.224902, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:52.224290: step 7579, loss 0.202186, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:52.334834: step 7580, loss 0.188599, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:52.446555: step 7581, loss 0.228939, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:52.568062: step 7582, loss 0.174636, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:52.683140: step 7583, loss 0.267981, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:52.793549: step 7584, loss 0.176303, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:52.905865: step 7585, loss 0.196288, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:53.024284: step 7586, loss 0.17042, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:53.140038: step 7587, loss 0.131125, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:53.247928: step 7588, loss 0.284603, acc 0.875, learning_rate 0.0001
2017-10-09T15:21:53.362312: step 7589, loss 0.217645, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:53.478382: step 7590, loss 0.17315, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:53.587904: step 7591, loss 0.152869, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:53.698821: step 7592, loss 0.234237, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:53.808404: step 7593, loss 0.208703, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:53.924664: step 7594, loss 0.131656, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:54.038622: step 7595, loss 0.177767, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:54.150248: step 7596, loss 0.175158, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:54.270639: step 7597, loss 0.159085, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:54.378810: step 7598, loss 0.169783, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:54.501256: step 7599, loss 0.152296, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:54.624093: step 7600, loss 0.398785, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-09T15:21:54.902835: step 7600, loss 0.286511, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7600

2017-10-09T15:21:55.437489: step 7601, loss 0.264783, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:55.549426: step 7602, loss 0.142123, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:55.660832: step 7603, loss 0.0879983, acc 1, learning_rate 0.0001
2017-10-09T15:21:55.768330: step 7604, loss 0.134125, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:55.881471: step 7605, loss 0.231422, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:55.993306: step 7606, loss 0.138986, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:56.106910: step 7607, loss 0.179297, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:56.216502: step 7608, loss 0.232388, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:56.330164: step 7609, loss 0.0886487, acc 1, learning_rate 0.0001
2017-10-09T15:21:56.453563: step 7610, loss 0.255461, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:56.573378: step 7611, loss 0.125906, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:56.682808: step 7612, loss 0.0975896, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:56.793123: step 7613, loss 0.153263, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:56.907428: step 7614, loss 0.295308, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:57.019580: step 7615, loss 0.156365, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:57.132118: step 7616, loss 0.358021, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:57.243107: step 7617, loss 0.24936, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:57.358414: step 7618, loss 0.211393, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:57.467322: step 7619, loss 0.192397, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:57.577311: step 7620, loss 0.195847, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:57.683140: step 7621, loss 0.257283, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:57.788773: step 7622, loss 0.336724, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:57.903768: step 7623, loss 0.0838522, acc 0.984375, learning_rate 0.0001
2017-10-09T15:21:58.016449: step 7624, loss 0.140114, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:58.128175: step 7625, loss 0.183172, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:58.236302: step 7626, loss 0.244328, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:58.345453: step 7627, loss 0.155422, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:58.470864: step 7628, loss 0.152429, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:58.590711: step 7629, loss 0.209394, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:58.700503: step 7630, loss 0.195627, acc 0.953125, learning_rate 0.0001
2017-10-09T15:21:58.806866: step 7631, loss 0.191748, acc 0.90625, learning_rate 0.0001
2017-10-09T15:21:58.921408: step 7632, loss 0.264992, acc 0.890625, learning_rate 0.0001
2017-10-09T15:21:59.034022: step 7633, loss 0.111284, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:59.150524: step 7634, loss 0.274918, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:59.266637: step 7635, loss 0.185635, acc 0.921875, learning_rate 0.0001
2017-10-09T15:21:59.376822: step 7636, loss 0.0927552, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:59.489410: step 7637, loss 0.158811, acc 0.9375, learning_rate 0.0001
2017-10-09T15:21:59.608277: step 7638, loss 0.143236, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:59.721199: step 7639, loss 0.0970707, acc 0.96875, learning_rate 0.0001
2017-10-09T15:21:59.827988: step 7640, loss 0.21198, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:22:00.107915: step 7640, loss 0.282451, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7640

2017-10-09T15:22:00.711838: step 7641, loss 0.101397, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:00.830393: step 7642, loss 0.245681, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:00.951073: step 7643, loss 0.0989455, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:01.048368: step 7644, loss 0.131394, acc 0.980392, learning_rate 0.0001
2017-10-09T15:22:01.177637: step 7645, loss 0.206192, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:01.294476: step 7646, loss 0.195405, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:01.415504: step 7647, loss 0.237018, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:01.533448: step 7648, loss 0.212367, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:01.642695: step 7649, loss 0.263063, acc 0.875, learning_rate 0.0001
2017-10-09T15:22:01.755326: step 7650, loss 0.168314, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:01.865788: step 7651, loss 0.170942, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:01.974904: step 7652, loss 0.13334, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:02.084516: step 7653, loss 0.170191, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:02.201266: step 7654, loss 0.13976, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:02.321089: step 7655, loss 0.155664, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:02.433862: step 7656, loss 0.233758, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:02.550473: step 7657, loss 0.157755, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:02.665599: step 7658, loss 0.18769, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:02.776037: step 7659, loss 0.14543, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:02.893430: step 7660, loss 0.211591, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:03.004263: step 7661, loss 0.134847, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:03.111779: step 7662, loss 0.125031, acc 1, learning_rate 0.0001
2017-10-09T15:22:03.219705: step 7663, loss 0.170351, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:03.328546: step 7664, loss 0.199979, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:03.440170: step 7665, loss 0.276215, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:03.559785: step 7666, loss 0.219353, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:03.668510: step 7667, loss 0.184544, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:03.783699: step 7668, loss 0.166938, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:03.895840: step 7669, loss 0.183597, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:04.008525: step 7670, loss 0.16273, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:04.125289: step 7671, loss 0.218311, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:04.242468: step 7672, loss 0.133558, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:04.352770: step 7673, loss 0.142275, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:04.468997: step 7674, loss 0.323189, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:04.584327: step 7675, loss 0.220285, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:04.697690: step 7676, loss 0.307321, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:04.812565: step 7677, loss 0.201552, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:04.923368: step 7678, loss 0.130919, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:05.055325: step 7679, loss 0.169231, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:05.163570: step 7680, loss 0.182763, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-09T15:22:05.447926: step 7680, loss 0.280565, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7680

2017-10-09T15:22:06.070094: step 7681, loss 0.156638, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:06.179992: step 7682, loss 0.171703, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:06.292843: step 7683, loss 0.247813, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:06.407291: step 7684, loss 0.116132, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:06.521903: step 7685, loss 0.153307, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:06.631702: step 7686, loss 0.27674, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:06.754959: step 7687, loss 0.178331, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:06.868527: step 7688, loss 0.225573, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:06.990346: step 7689, loss 0.120606, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:07.108830: step 7690, loss 0.25841, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:07.224809: step 7691, loss 0.141262, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:07.342557: step 7692, loss 0.23785, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:07.459730: step 7693, loss 0.187898, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:07.581307: step 7694, loss 0.168478, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:07.726169: step 7695, loss 0.237453, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:07.839345: step 7696, loss 0.15845, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:07.951258: step 7697, loss 0.258861, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:08.060283: step 7698, loss 0.203646, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:08.171513: step 7699, loss 0.337475, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:08.289913: step 7700, loss 0.191252, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:08.411871: step 7701, loss 0.127632, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:08.524470: step 7702, loss 0.187712, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:08.638264: step 7703, loss 0.200419, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:08.747626: step 7704, loss 0.149123, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:08.858828: step 7705, loss 0.17684, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:08.973281: step 7706, loss 0.252244, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:09.087914: step 7707, loss 0.176187, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:09.202168: step 7708, loss 0.225942, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:09.318342: step 7709, loss 0.19294, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:09.434114: step 7710, loss 0.241531, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:09.544273: step 7711, loss 0.118697, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:09.649989: step 7712, loss 0.19882, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:09.769780: step 7713, loss 0.175945, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:09.882189: step 7714, loss 0.15711, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:09.991767: step 7715, loss 0.207056, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:10.107497: step 7716, loss 0.235263, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:10.221989: step 7717, loss 0.193508, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:10.331527: step 7718, loss 0.0957274, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:10.441294: step 7719, loss 0.188748, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:10.550602: step 7720, loss 0.089153, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-09T15:22:10.844853: step 7720, loss 0.281624, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7720

2017-10-09T15:22:11.540952: step 7721, loss 0.237272, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:11.656513: step 7722, loss 0.150673, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:11.771256: step 7723, loss 0.172083, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:11.879554: step 7724, loss 0.114403, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:11.992807: step 7725, loss 0.139684, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:12.105464: step 7726, loss 0.209026, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:12.217505: step 7727, loss 0.22549, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:12.338189: step 7728, loss 0.14743, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:12.453305: step 7729, loss 0.147397, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:12.562538: step 7730, loss 0.202283, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:12.675567: step 7731, loss 0.13498, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:12.787975: step 7732, loss 0.110757, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:12.896856: step 7733, loss 0.1576, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:13.007738: step 7734, loss 0.1945, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:13.124011: step 7735, loss 0.26403, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:13.239963: step 7736, loss 0.133904, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:13.353161: step 7737, loss 0.115798, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:13.467323: step 7738, loss 0.0973048, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:13.582292: step 7739, loss 0.186239, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:13.700724: step 7740, loss 0.370506, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:13.813949: step 7741, loss 0.161365, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:13.914808: step 7742, loss 0.230583, acc 0.921569, learning_rate 0.0001
2017-10-09T15:22:14.031483: step 7743, loss 0.311914, acc 0.875, learning_rate 0.0001
2017-10-09T15:22:14.141778: step 7744, loss 0.128117, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:14.256138: step 7745, loss 0.107697, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:14.374334: step 7746, loss 0.165239, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:14.490714: step 7747, loss 0.140749, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:14.603765: step 7748, loss 0.0730137, acc 1, learning_rate 0.0001
2017-10-09T15:22:14.715554: step 7749, loss 0.272406, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:14.827033: step 7750, loss 0.180729, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:14.936439: step 7751, loss 0.215016, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:15.055533: step 7752, loss 0.193497, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:15.170903: step 7753, loss 0.193844, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:15.290580: step 7754, loss 0.123859, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:15.400125: step 7755, loss 0.174422, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:15.508200: step 7756, loss 0.277747, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:15.616581: step 7757, loss 0.143649, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:15.730028: step 7758, loss 0.225607, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:15.850559: step 7759, loss 0.196352, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:15.964715: step 7760, loss 0.174497, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-09T15:22:16.265418: step 7760, loss 0.281058, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7760

2017-10-09T15:22:16.806902: step 7761, loss 0.27647, acc 0.875, learning_rate 0.0001
2017-10-09T15:22:16.927426: step 7762, loss 0.128819, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:17.036562: step 7763, loss 0.261927, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:17.148801: step 7764, loss 0.247075, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:17.260691: step 7765, loss 0.182208, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:17.375816: step 7766, loss 0.177295, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:17.492438: step 7767, loss 0.248669, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:17.606648: step 7768, loss 0.149982, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:17.721145: step 7769, loss 0.217604, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:17.840038: step 7770, loss 0.155856, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:17.961618: step 7771, loss 0.232287, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:18.074713: step 7772, loss 0.100971, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:18.183435: step 7773, loss 0.148845, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:18.293707: step 7774, loss 0.336735, acc 0.875, learning_rate 0.0001
2017-10-09T15:22:18.408285: step 7775, loss 0.245029, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:18.528964: step 7776, loss 0.201045, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:18.644054: step 7777, loss 0.329663, acc 0.859375, learning_rate 0.0001
2017-10-09T15:22:18.764799: step 7778, loss 0.19477, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:18.888558: step 7779, loss 0.219204, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:19.008094: step 7780, loss 0.2219, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:19.123624: step 7781, loss 0.182921, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:19.244129: step 7782, loss 0.20256, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:19.363768: step 7783, loss 0.252771, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:19.484761: step 7784, loss 0.140472, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:19.604727: step 7785, loss 0.195973, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:19.727113: step 7786, loss 0.176744, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:19.845718: step 7787, loss 0.199822, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:19.964153: step 7788, loss 0.152204, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:20.082889: step 7789, loss 0.243987, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:20.209864: step 7790, loss 0.233541, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:20.342401: step 7791, loss 0.204437, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:20.461886: step 7792, loss 0.324806, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:20.586092: step 7793, loss 0.214715, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:20.707204: step 7794, loss 0.162766, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:20.836366: step 7795, loss 0.118803, acc 1, learning_rate 0.0001
2017-10-09T15:22:20.954061: step 7796, loss 0.231376, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:21.072705: step 7797, loss 0.181771, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:21.187905: step 7798, loss 0.197176, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:21.311717: step 7799, loss 0.185309, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:21.430637: step 7800, loss 0.135914, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-09T15:22:21.709087: step 7800, loss 0.284078, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7800

2017-10-09T15:22:22.358915: step 7801, loss 0.137759, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:22.470189: step 7802, loss 0.167231, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:22.587788: step 7803, loss 0.180439, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:22.712804: step 7804, loss 0.15004, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:22.833357: step 7805, loss 0.214413, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:22.946227: step 7806, loss 0.237592, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:23.058393: step 7807, loss 0.141862, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:23.173682: step 7808, loss 0.157561, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:23.284158: step 7809, loss 0.142305, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:23.395344: step 7810, loss 0.291934, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:23.509571: step 7811, loss 0.192437, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:23.626856: step 7812, loss 0.379834, acc 0.84375, learning_rate 0.0001
2017-10-09T15:22:23.741801: step 7813, loss 0.250299, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:23.849965: step 7814, loss 0.242081, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:23.963615: step 7815, loss 0.118473, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:24.081997: step 7816, loss 0.126714, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:24.200310: step 7817, loss 0.144631, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:24.316744: step 7818, loss 0.170929, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:24.435977: step 7819, loss 0.19927, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:24.552728: step 7820, loss 0.238428, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:24.663289: step 7821, loss 0.128903, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:24.776613: step 7822, loss 0.242504, acc 0.875, learning_rate 0.0001
2017-10-09T15:22:24.888124: step 7823, loss 0.123348, acc 0.96875, learning_rate 0.0001
2017-10-09T15:22:24.998495: step 7824, loss 0.149299, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:25.119515: step 7825, loss 0.197164, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:25.235224: step 7826, loss 0.223951, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:25.349528: step 7827, loss 0.202618, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:25.461658: step 7828, loss 0.219996, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:25.579569: step 7829, loss 0.200731, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:25.696993: step 7830, loss 0.1973, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:25.808910: step 7831, loss 0.325554, acc 0.84375, learning_rate 0.0001
2017-10-09T15:22:25.924899: step 7832, loss 0.217278, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:26.041820: step 7833, loss 0.173254, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:26.161951: step 7834, loss 0.212123, acc 0.90625, learning_rate 0.0001
2017-10-09T15:22:26.283628: step 7835, loss 0.198352, acc 0.890625, learning_rate 0.0001
2017-10-09T15:22:26.405244: step 7836, loss 0.0862189, acc 0.984375, learning_rate 0.0001
2017-10-09T15:22:26.515016: step 7837, loss 0.173354, acc 0.953125, learning_rate 0.0001
2017-10-09T15:22:26.634346: step 7838, loss 0.175627, acc 0.921875, learning_rate 0.0001
2017-10-09T15:22:26.743814: step 7839, loss 0.158804, acc 0.9375, learning_rate 0.0001
2017-10-09T15:22:26.835420: step 7840, loss 0.305431, acc 0.901961, learning_rate 0.0001

Evaluation:
2017-10-09T15:22:27.111274: step 7840, loss 0.281036, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507579543/checkpoints/model-7840

