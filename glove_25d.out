
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6954
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
Vocabulary Size: 46116
Train/Dev split: 6259/695
Writing to /home/sheep/bigdata/runs/1506709920

Load glove file /home/sheep/bigdata/glove.twitter.27B.25d.txt
glove file has been loaded

2017-09-29T13:32:16.722283: step 1, loss 5.89264, acc 0.171875, learning_rate 0.005
2017-09-29T13:32:16.939292: step 2, loss 3.96856, acc 0.265625, learning_rate 0.00498
2017-09-29T13:32:17.146462: step 3, loss 4.20016, acc 0.34375, learning_rate 0.00496008
2017-09-29T13:32:17.355116: step 4, loss 3.61383, acc 0.375, learning_rate 0.00494024
2017-09-29T13:32:17.576542: step 5, loss 2.90135, acc 0.390625, learning_rate 0.00492049
2017-09-29T13:32:17.788216: step 6, loss 3.14168, acc 0.328125, learning_rate 0.00490081
2017-09-29T13:32:17.995865: step 7, loss 3.40279, acc 0.21875, learning_rate 0.00488121
2017-09-29T13:32:18.196070: step 8, loss 2.84824, acc 0.25, learning_rate 0.0048617
2017-09-29T13:32:18.384939: step 9, loss 3.27876, acc 0.21875, learning_rate 0.00484226
2017-09-29T13:32:18.568706: step 10, loss 2.20202, acc 0.359375, learning_rate 0.00482291
2017-09-29T13:32:18.765721: step 11, loss 2.15357, acc 0.4375, learning_rate 0.00480363
2017-09-29T13:32:18.947638: step 12, loss 1.97941, acc 0.46875, learning_rate 0.00478443
2017-09-29T13:32:19.130764: step 13, loss 2.48344, acc 0.390625, learning_rate 0.00476531
2017-09-29T13:32:19.319920: step 14, loss 1.75313, acc 0.578125, learning_rate 0.00474627
2017-09-29T13:32:19.510630: step 15, loss 2.71357, acc 0.421875, learning_rate 0.0047273
2017-09-29T13:32:19.696542: step 16, loss 1.73324, acc 0.578125, learning_rate 0.00470841
2017-09-29T13:32:19.883014: step 17, loss 2.40252, acc 0.453125, learning_rate 0.0046896
2017-09-29T13:32:20.070935: step 18, loss 1.90997, acc 0.421875, learning_rate 0.00467087
2017-09-29T13:32:20.259467: step 19, loss 1.78899, acc 0.453125, learning_rate 0.00465221
2017-09-29T13:32:20.447347: step 20, loss 1.7695, acc 0.390625, learning_rate 0.00463363
2017-09-29T13:32:20.636435: step 21, loss 1.59183, acc 0.484375, learning_rate 0.00461513
2017-09-29T13:32:20.821429: step 22, loss 2.11462, acc 0.4375, learning_rate 0.0045967
2017-09-29T13:32:21.006197: step 23, loss 1.63655, acc 0.421875, learning_rate 0.00457834
2017-09-29T13:32:21.188364: step 24, loss 1.18489, acc 0.578125, learning_rate 0.00456006
2017-09-29T13:32:21.379839: step 25, loss 1.39141, acc 0.625, learning_rate 0.00454186
2017-09-29T13:32:21.568966: step 26, loss 1.21087, acc 0.609375, learning_rate 0.00452373
2017-09-29T13:32:21.753087: step 27, loss 0.980836, acc 0.703125, learning_rate 0.00450567
2017-09-29T13:32:21.934397: step 28, loss 1.41654, acc 0.53125, learning_rate 0.00448769
2017-09-29T13:32:22.124302: step 29, loss 0.960012, acc 0.671875, learning_rate 0.00446978
2017-09-29T13:32:22.309240: step 30, loss 0.860406, acc 0.75, learning_rate 0.00445194
2017-09-29T13:32:22.510717: step 31, loss 0.850955, acc 0.703125, learning_rate 0.00443418
2017-09-29T13:32:22.693176: step 32, loss 1.01252, acc 0.609375, learning_rate 0.00441649
2017-09-29T13:32:22.880174: step 33, loss 1.01619, acc 0.703125, learning_rate 0.00439887
2017-09-29T13:32:23.072460: step 34, loss 1.04273, acc 0.671875, learning_rate 0.00438132
2017-09-29T13:32:23.255606: step 35, loss 1.02024, acc 0.578125, learning_rate 0.00436385
2017-09-29T13:32:23.441841: step 36, loss 1.01155, acc 0.6875, learning_rate 0.00434644
2017-09-29T13:32:23.644777: step 37, loss 0.961711, acc 0.75, learning_rate 0.00432911
2017-09-29T13:32:23.849848: step 38, loss 0.665164, acc 0.78125, learning_rate 0.00431185
2017-09-29T13:32:24.057323: step 39, loss 0.787316, acc 0.734375, learning_rate 0.00429465
2017-09-29T13:32:24.249763: step 40, loss 0.513733, acc 0.828125, learning_rate 0.00427753

Evaluation:
2017-09-29T13:32:24.851860: step 40, loss 0.424906, acc 0.853237

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-40

2017-09-29T13:32:25.577485: step 41, loss 0.681961, acc 0.765625, learning_rate 0.00426048
2017-09-29T13:32:25.770378: step 42, loss 0.599046, acc 0.8125, learning_rate 0.0042435
2017-09-29T13:32:25.954970: step 43, loss 0.830664, acc 0.75, learning_rate 0.00422659
2017-09-29T13:32:26.140394: step 44, loss 0.459716, acc 0.796875, learning_rate 0.00420974
2017-09-29T13:32:26.324139: step 45, loss 0.715798, acc 0.796875, learning_rate 0.00419297
2017-09-29T13:32:26.509045: step 46, loss 0.453747, acc 0.859375, learning_rate 0.00417626
2017-09-29T13:32:26.694639: step 47, loss 0.832133, acc 0.671875, learning_rate 0.00415962
2017-09-29T13:32:26.878199: step 48, loss 0.679756, acc 0.71875, learning_rate 0.00414305
2017-09-29T13:32:27.060546: step 49, loss 0.803552, acc 0.703125, learning_rate 0.00412655
2017-09-29T13:32:27.244427: step 50, loss 0.51175, acc 0.828125, learning_rate 0.00411011
2017-09-29T13:32:27.424828: step 51, loss 0.692932, acc 0.734375, learning_rate 0.00409375
2017-09-29T13:32:27.608329: step 52, loss 0.679008, acc 0.765625, learning_rate 0.00407744
2017-09-29T13:32:27.790904: step 53, loss 0.74888, acc 0.703125, learning_rate 0.00406121
2017-09-29T13:32:27.974983: step 54, loss 0.714469, acc 0.75, learning_rate 0.00404504
2017-09-29T13:32:28.160322: step 55, loss 0.599211, acc 0.828125, learning_rate 0.00402894
2017-09-29T13:32:28.344507: step 56, loss 0.713422, acc 0.734375, learning_rate 0.0040129
2017-09-29T13:32:28.542443: step 57, loss 0.82913, acc 0.71875, learning_rate 0.00399693
2017-09-29T13:32:28.727443: step 58, loss 0.482613, acc 0.828125, learning_rate 0.00398102
2017-09-29T13:32:28.911268: step 59, loss 0.472435, acc 0.796875, learning_rate 0.00396518
2017-09-29T13:32:29.096995: step 60, loss 0.561114, acc 0.828125, learning_rate 0.00394941
2017-09-29T13:32:29.277994: step 61, loss 0.607977, acc 0.796875, learning_rate 0.00393369
2017-09-29T13:32:29.462922: step 62, loss 0.663917, acc 0.8125, learning_rate 0.00391804
2017-09-29T13:32:29.648916: step 63, loss 0.484703, acc 0.84375, learning_rate 0.00390246
2017-09-29T13:32:29.828987: step 64, loss 0.67101, acc 0.8125, learning_rate 0.00388694
2017-09-29T13:32:30.011004: step 65, loss 0.502101, acc 0.84375, learning_rate 0.00387148
2017-09-29T13:32:30.192534: step 66, loss 0.472988, acc 0.796875, learning_rate 0.00385609
2017-09-29T13:32:30.374732: step 67, loss 0.446536, acc 0.859375, learning_rate 0.00384076
2017-09-29T13:32:30.561081: step 68, loss 0.476026, acc 0.859375, learning_rate 0.00382549
2017-09-29T13:32:30.773719: step 69, loss 0.600606, acc 0.875, learning_rate 0.00381028
2017-09-29T13:32:30.956165: step 70, loss 0.523558, acc 0.796875, learning_rate 0.00379514
2017-09-29T13:32:31.139213: step 71, loss 0.826415, acc 0.71875, learning_rate 0.00378005
2017-09-29T13:32:31.321940: step 72, loss 0.52611, acc 0.859375, learning_rate 0.00376503
2017-09-29T13:32:31.505835: step 73, loss 0.513414, acc 0.8125, learning_rate 0.00375007
2017-09-29T13:32:31.690214: step 74, loss 0.340954, acc 0.875, learning_rate 0.00373517
2017-09-29T13:32:31.871732: step 75, loss 0.437656, acc 0.859375, learning_rate 0.00372034
2017-09-29T13:32:32.054996: step 76, loss 0.354438, acc 0.890625, learning_rate 0.00370556
2017-09-29T13:32:32.240533: step 77, loss 0.652822, acc 0.765625, learning_rate 0.00369084
2017-09-29T13:32:32.426053: step 78, loss 0.482006, acc 0.8125, learning_rate 0.00367619
2017-09-29T13:32:32.618774: step 79, loss 0.599421, acc 0.828125, learning_rate 0.00366159
2017-09-29T13:32:32.811911: step 80, loss 0.373973, acc 0.859375, learning_rate 0.00364705

Evaluation:
2017-09-29T13:32:33.362248: step 80, loss 0.32299, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-80

2017-09-29T13:32:34.095551: step 81, loss 0.930473, acc 0.734375, learning_rate 0.00363257
2017-09-29T13:32:34.278855: step 82, loss 0.458199, acc 0.828125, learning_rate 0.00361815
2017-09-29T13:32:34.467666: step 83, loss 0.34166, acc 0.875, learning_rate 0.00360379
2017-09-29T13:32:34.661567: step 84, loss 0.364111, acc 0.875, learning_rate 0.00358949
2017-09-29T13:32:34.846748: step 85, loss 0.513055, acc 0.84375, learning_rate 0.00357525
2017-09-29T13:32:35.025532: step 86, loss 0.383483, acc 0.84375, learning_rate 0.00356106
2017-09-29T13:32:35.216105: step 87, loss 0.338401, acc 0.859375, learning_rate 0.00354694
2017-09-29T13:32:35.411012: step 88, loss 0.317893, acc 0.875, learning_rate 0.00353287
2017-09-29T13:32:35.596317: step 89, loss 0.811519, acc 0.75, learning_rate 0.00351885
2017-09-29T13:32:35.786526: step 90, loss 0.476826, acc 0.859375, learning_rate 0.0035049
2017-09-29T13:32:35.970647: step 91, loss 0.610269, acc 0.8125, learning_rate 0.003491
2017-09-29T13:32:36.149353: step 92, loss 0.432138, acc 0.90625, learning_rate 0.00347716
2017-09-29T13:32:36.331602: step 93, loss 0.474193, acc 0.828125, learning_rate 0.00346338
2017-09-29T13:32:36.518016: step 94, loss 0.508985, acc 0.8125, learning_rate 0.00344965
2017-09-29T13:32:36.705914: step 95, loss 0.437871, acc 0.859375, learning_rate 0.00343597
2017-09-29T13:32:36.887510: step 96, loss 0.614312, acc 0.84375, learning_rate 0.00342236
2017-09-29T13:32:37.069801: step 97, loss 0.366764, acc 0.828125, learning_rate 0.0034088
2017-09-29T13:32:37.229606: step 98, loss 0.450888, acc 0.823529, learning_rate 0.00339529
2017-09-29T13:32:37.426597: step 99, loss 0.598398, acc 0.8125, learning_rate 0.00338184
2017-09-29T13:32:37.640159: step 100, loss 0.394155, acc 0.84375, learning_rate 0.00336844
2017-09-29T13:32:37.829494: step 101, loss 0.272412, acc 0.875, learning_rate 0.0033551
2017-09-29T13:32:38.019993: step 102, loss 0.440563, acc 0.875, learning_rate 0.00334182
2017-09-29T13:32:38.215385: step 103, loss 0.288795, acc 0.90625, learning_rate 0.00332858
2017-09-29T13:32:38.405969: step 104, loss 0.282173, acc 0.921875, learning_rate 0.00331541
2017-09-29T13:32:38.597541: step 105, loss 0.393855, acc 0.859375, learning_rate 0.00330228
2017-09-29T13:32:38.791620: step 106, loss 0.359635, acc 0.859375, learning_rate 0.00328921
2017-09-29T13:32:38.994276: step 107, loss 0.208665, acc 0.9375, learning_rate 0.00327619
2017-09-29T13:32:39.207066: step 108, loss 0.304773, acc 0.875, learning_rate 0.00326323
2017-09-29T13:32:39.399418: step 109, loss 0.484365, acc 0.890625, learning_rate 0.00325032
2017-09-29T13:32:39.591743: step 110, loss 0.389063, acc 0.875, learning_rate 0.00323746
2017-09-29T13:32:39.781341: step 111, loss 0.424455, acc 0.859375, learning_rate 0.00322465
2017-09-29T13:32:39.965853: step 112, loss 0.383432, acc 0.84375, learning_rate 0.0032119
2017-09-29T13:32:40.166886: step 113, loss 0.39147, acc 0.921875, learning_rate 0.0031992
2017-09-29T13:32:40.372354: step 114, loss 0.233315, acc 0.9375, learning_rate 0.00318655
2017-09-29T13:32:40.567658: step 115, loss 0.346333, acc 0.875, learning_rate 0.00317395
2017-09-29T13:32:40.759488: step 116, loss 0.287173, acc 0.90625, learning_rate 0.0031614
2017-09-29T13:32:40.947875: step 117, loss 0.308716, acc 0.859375, learning_rate 0.0031489
2017-09-29T13:32:41.130123: step 118, loss 0.302954, acc 0.890625, learning_rate 0.00313646
2017-09-29T13:32:41.312998: step 119, loss 0.387361, acc 0.875, learning_rate 0.00312407
2017-09-29T13:32:41.499081: step 120, loss 0.337747, acc 0.859375, learning_rate 0.00311172

Evaluation:
2017-09-29T13:32:42.048158: step 120, loss 0.289522, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-120

2017-09-29T13:32:42.765631: step 121, loss 0.451407, acc 0.84375, learning_rate 0.00309943
2017-09-29T13:32:42.946191: step 122, loss 0.236708, acc 0.90625, learning_rate 0.00308719
2017-09-29T13:32:43.135478: step 123, loss 0.317801, acc 0.859375, learning_rate 0.00307499
2017-09-29T13:32:43.319207: step 124, loss 0.334233, acc 0.90625, learning_rate 0.00306285
2017-09-29T13:32:43.501640: step 125, loss 0.314587, acc 0.84375, learning_rate 0.00305076
2017-09-29T13:32:43.694987: step 126, loss 0.385093, acc 0.859375, learning_rate 0.00303871
2017-09-29T13:32:43.887693: step 127, loss 0.360429, acc 0.859375, learning_rate 0.00302672
2017-09-29T13:32:44.070699: step 128, loss 0.332514, acc 0.875, learning_rate 0.00301477
2017-09-29T13:32:44.254527: step 129, loss 0.291211, acc 0.890625, learning_rate 0.00300287
2017-09-29T13:32:44.447702: step 130, loss 0.531132, acc 0.84375, learning_rate 0.00299102
2017-09-29T13:32:44.642196: step 131, loss 0.360502, acc 0.859375, learning_rate 0.00297922
2017-09-29T13:32:44.845919: step 132, loss 0.253196, acc 0.921875, learning_rate 0.00296747
2017-09-29T13:32:45.047993: step 133, loss 0.274059, acc 0.875, learning_rate 0.00295577
2017-09-29T13:32:45.240910: step 134, loss 0.298933, acc 0.890625, learning_rate 0.00294411
2017-09-29T13:32:45.427953: step 135, loss 0.364841, acc 0.859375, learning_rate 0.0029325
2017-09-29T13:32:45.609953: step 136, loss 0.226144, acc 0.90625, learning_rate 0.00292094
2017-09-29T13:32:45.801284: step 137, loss 0.29823, acc 0.90625, learning_rate 0.00290943
2017-09-29T13:32:45.982790: step 138, loss 0.234443, acc 0.9375, learning_rate 0.00289796
2017-09-29T13:32:46.164210: step 139, loss 0.219027, acc 0.875, learning_rate 0.00288654
2017-09-29T13:32:46.344322: step 140, loss 0.567141, acc 0.796875, learning_rate 0.00287516
2017-09-29T13:32:46.542949: step 141, loss 0.313839, acc 0.875, learning_rate 0.00286384
2017-09-29T13:32:46.725449: step 142, loss 0.255142, acc 0.921875, learning_rate 0.00285256
2017-09-29T13:32:46.907344: step 143, loss 0.502135, acc 0.828125, learning_rate 0.00284132
2017-09-29T13:32:47.087591: step 144, loss 0.291487, acc 0.921875, learning_rate 0.00283013
2017-09-29T13:32:47.269100: step 145, loss 0.27235, acc 0.9375, learning_rate 0.00281899
2017-09-29T13:32:47.454573: step 146, loss 0.376378, acc 0.90625, learning_rate 0.00280789
2017-09-29T13:32:47.640946: step 147, loss 0.322067, acc 0.90625, learning_rate 0.00279684
2017-09-29T13:32:47.824297: step 148, loss 0.30783, acc 0.921875, learning_rate 0.00278583
2017-09-29T13:32:48.008002: step 149, loss 0.288049, acc 0.90625, learning_rate 0.00277486
2017-09-29T13:32:48.204070: step 150, loss 0.229621, acc 0.9375, learning_rate 0.00276395
2017-09-29T13:32:48.385817: step 151, loss 0.291378, acc 0.90625, learning_rate 0.00275307
2017-09-29T13:32:48.577200: step 152, loss 0.400688, acc 0.8125, learning_rate 0.00274224
2017-09-29T13:32:48.762155: step 153, loss 0.572425, acc 0.8125, learning_rate 0.00273146
2017-09-29T13:32:48.947911: step 154, loss 0.356818, acc 0.859375, learning_rate 0.00272072
2017-09-29T13:32:49.133074: step 155, loss 0.328314, acc 0.84375, learning_rate 0.00271002
2017-09-29T13:32:49.316642: step 156, loss 0.203811, acc 0.953125, learning_rate 0.00269937
2017-09-29T13:32:49.498289: step 157, loss 0.285681, acc 0.875, learning_rate 0.00268876
2017-09-29T13:32:49.684909: step 158, loss 0.449307, acc 0.84375, learning_rate 0.00267819
2017-09-29T13:32:49.867703: step 159, loss 0.307554, acc 0.890625, learning_rate 0.00266767
2017-09-29T13:32:50.046824: step 160, loss 0.230905, acc 0.921875, learning_rate 0.00265719

Evaluation:
2017-09-29T13:32:50.597257: step 160, loss 0.26984, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-160

2017-09-29T13:32:51.309149: step 161, loss 0.168723, acc 0.953125, learning_rate 0.00264675
2017-09-29T13:32:51.504784: step 162, loss 0.244959, acc 0.890625, learning_rate 0.00263635
2017-09-29T13:32:51.688677: step 163, loss 0.241765, acc 0.921875, learning_rate 0.002626
2017-09-29T13:32:51.873943: step 164, loss 0.201163, acc 0.921875, learning_rate 0.00261569
2017-09-29T13:32:52.059289: step 165, loss 0.218669, acc 0.90625, learning_rate 0.00260542
2017-09-29T13:32:52.242888: step 166, loss 0.415619, acc 0.859375, learning_rate 0.0025952
2017-09-29T13:32:52.431351: step 167, loss 0.0916061, acc 0.984375, learning_rate 0.00258501
2017-09-29T13:32:52.611947: step 168, loss 0.301219, acc 0.84375, learning_rate 0.00257487
2017-09-29T13:32:52.794226: step 169, loss 0.165702, acc 0.90625, learning_rate 0.00256477
2017-09-29T13:32:52.974142: step 170, loss 0.177351, acc 0.953125, learning_rate 0.0025547
2017-09-29T13:32:53.155048: step 171, loss 0.224366, acc 0.921875, learning_rate 0.00254469
2017-09-29T13:32:53.338216: step 172, loss 0.266076, acc 0.890625, learning_rate 0.00253471
2017-09-29T13:32:53.525509: step 173, loss 0.220625, acc 0.921875, learning_rate 0.00252477
2017-09-29T13:32:53.711595: step 174, loss 0.249401, acc 0.890625, learning_rate 0.00251487
2017-09-29T13:32:53.892521: step 175, loss 0.245047, acc 0.890625, learning_rate 0.00250501
2017-09-29T13:32:54.074643: step 176, loss 0.153494, acc 0.953125, learning_rate 0.0024952
2017-09-29T13:32:54.259509: step 177, loss 0.362152, acc 0.875, learning_rate 0.00248542
2017-09-29T13:32:54.444089: step 178, loss 0.285263, acc 0.90625, learning_rate 0.00247568
2017-09-29T13:32:54.627082: step 179, loss 0.194663, acc 0.9375, learning_rate 0.00246599
2017-09-29T13:32:54.825170: step 180, loss 0.258058, acc 0.921875, learning_rate 0.00245633
2017-09-29T13:32:55.014052: step 181, loss 0.222124, acc 0.890625, learning_rate 0.00244671
2017-09-29T13:32:55.197225: step 182, loss 0.256636, acc 0.90625, learning_rate 0.00243713
2017-09-29T13:32:55.384366: step 183, loss 0.236142, acc 0.921875, learning_rate 0.00242759
2017-09-29T13:32:55.570462: step 184, loss 0.304203, acc 0.90625, learning_rate 0.00241809
2017-09-29T13:32:55.765302: step 185, loss 0.172224, acc 0.953125, learning_rate 0.00240863
2017-09-29T13:32:55.955124: step 186, loss 0.277663, acc 0.921875, learning_rate 0.00239921
2017-09-29T13:32:56.139441: step 187, loss 0.243232, acc 0.9375, learning_rate 0.00238982
2017-09-29T13:32:56.325235: step 188, loss 0.31427, acc 0.890625, learning_rate 0.00238048
2017-09-29T13:32:56.510044: step 189, loss 0.236204, acc 0.90625, learning_rate 0.00237117
2017-09-29T13:32:56.696407: step 190, loss 0.327619, acc 0.90625, learning_rate 0.0023619
2017-09-29T13:32:56.878868: step 191, loss 0.267838, acc 0.875, learning_rate 0.00235267
2017-09-29T13:32:57.060100: step 192, loss 0.088312, acc 0.96875, learning_rate 0.00234347
2017-09-29T13:32:57.244179: step 193, loss 0.168141, acc 0.890625, learning_rate 0.00233431
2017-09-29T13:32:57.429218: step 194, loss 0.306126, acc 0.890625, learning_rate 0.00232519
2017-09-29T13:32:57.611180: step 195, loss 0.247317, acc 0.90625, learning_rate 0.00231611
2017-09-29T13:32:57.763457: step 196, loss 0.647799, acc 0.803922, learning_rate 0.00230707
2017-09-29T13:32:57.949889: step 197, loss 0.198561, acc 0.921875, learning_rate 0.00229806
2017-09-29T13:32:58.134240: step 198, loss 0.131445, acc 0.9375, learning_rate 0.00228908
2017-09-29T13:32:58.316367: step 199, loss 0.167305, acc 0.9375, learning_rate 0.00228015
2017-09-29T13:32:58.499366: step 200, loss 0.199192, acc 0.921875, learning_rate 0.00227125

Evaluation:
2017-09-29T13:32:59.042870: step 200, loss 0.27452, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-200

2017-09-29T13:32:59.782715: step 201, loss 0.127207, acc 0.96875, learning_rate 0.00226239
2017-09-29T13:32:59.965949: step 202, loss 0.103997, acc 0.953125, learning_rate 0.00225356
2017-09-29T13:33:00.156192: step 203, loss 0.212544, acc 0.9375, learning_rate 0.00224477
2017-09-29T13:33:00.336579: step 204, loss 0.245891, acc 0.953125, learning_rate 0.00223602
2017-09-29T13:33:00.530376: step 205, loss 0.172091, acc 0.9375, learning_rate 0.0022273
2017-09-29T13:33:00.716637: step 206, loss 0.298015, acc 0.890625, learning_rate 0.00221862
2017-09-29T13:33:00.910118: step 207, loss 0.242036, acc 0.875, learning_rate 0.00220997
2017-09-29T13:33:01.092216: step 208, loss 0.212254, acc 0.890625, learning_rate 0.00220136
2017-09-29T13:33:01.287625: step 209, loss 0.120826, acc 0.984375, learning_rate 0.00219278
2017-09-29T13:33:01.474794: step 210, loss 0.0925329, acc 0.96875, learning_rate 0.00218424
2017-09-29T13:33:01.661987: step 211, loss 0.0695481, acc 0.96875, learning_rate 0.00217573
2017-09-29T13:33:01.851551: step 212, loss 0.152619, acc 0.9375, learning_rate 0.00216726
2017-09-29T13:33:02.042982: step 213, loss 0.242864, acc 0.921875, learning_rate 0.00215882
2017-09-29T13:33:02.235670: step 214, loss 0.104702, acc 0.984375, learning_rate 0.00215041
2017-09-29T13:33:02.424959: step 215, loss 0.215492, acc 0.921875, learning_rate 0.00214204
2017-09-29T13:33:02.609553: step 216, loss 0.244909, acc 0.953125, learning_rate 0.00213371
2017-09-29T13:33:02.804573: step 217, loss 0.211764, acc 0.9375, learning_rate 0.00212541
2017-09-29T13:33:03.013177: step 218, loss 0.189952, acc 0.9375, learning_rate 0.00211714
2017-09-29T13:33:03.201189: step 219, loss 0.225057, acc 0.9375, learning_rate 0.00210891
2017-09-29T13:33:03.392397: step 220, loss 0.27562, acc 0.90625, learning_rate 0.00210071
2017-09-29T13:33:03.588651: step 221, loss 0.153717, acc 0.953125, learning_rate 0.00209254
2017-09-29T13:33:03.780975: step 222, loss 0.273777, acc 0.890625, learning_rate 0.00208441
2017-09-29T13:33:03.971533: step 223, loss 0.0641978, acc 0.984375, learning_rate 0.00207631
2017-09-29T13:33:04.163150: step 224, loss 0.16268, acc 0.96875, learning_rate 0.00206824
2017-09-29T13:33:04.344997: step 225, loss 0.25776, acc 0.921875, learning_rate 0.00206021
2017-09-29T13:33:04.532609: step 226, loss 0.246433, acc 0.90625, learning_rate 0.00205221
2017-09-29T13:33:04.720189: step 227, loss 0.16097, acc 0.9375, learning_rate 0.00204424
2017-09-29T13:33:04.915297: step 228, loss 0.21129, acc 0.9375, learning_rate 0.0020363
2017-09-29T13:33:05.112637: step 229, loss 0.232037, acc 0.90625, learning_rate 0.0020284
2017-09-29T13:33:05.295074: step 230, loss 0.191721, acc 0.9375, learning_rate 0.00202053
2017-09-29T13:33:05.479426: step 231, loss 0.409175, acc 0.859375, learning_rate 0.00201269
2017-09-29T13:33:05.661365: step 232, loss 0.178133, acc 0.9375, learning_rate 0.00200488
2017-09-29T13:33:05.853671: step 233, loss 0.174486, acc 0.90625, learning_rate 0.00199711
2017-09-29T13:33:06.050312: step 234, loss 0.208603, acc 0.953125, learning_rate 0.00198936
2017-09-29T13:33:06.234666: step 235, loss 0.231822, acc 0.875, learning_rate 0.00198165
2017-09-29T13:33:06.419168: step 236, loss 0.241955, acc 0.90625, learning_rate 0.00197397
2017-09-29T13:33:06.604307: step 237, loss 0.29408, acc 0.875, learning_rate 0.00196632
2017-09-29T13:33:06.790524: step 238, loss 0.331168, acc 0.875, learning_rate 0.0019587
2017-09-29T13:33:06.977602: step 239, loss 0.100637, acc 0.984375, learning_rate 0.00195112
2017-09-29T13:33:07.158992: step 240, loss 0.13624, acc 0.9375, learning_rate 0.00194356

Evaluation:
2017-09-29T13:33:07.702789: step 240, loss 0.253066, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-240

2017-09-29T13:33:08.453467: step 241, loss 0.153068, acc 0.96875, learning_rate 0.00193604
2017-09-29T13:33:08.642925: step 242, loss 0.230242, acc 0.890625, learning_rate 0.00192854
2017-09-29T13:33:08.830879: step 243, loss 0.18874, acc 0.921875, learning_rate 0.00192108
2017-09-29T13:33:09.013780: step 244, loss 0.118713, acc 0.953125, learning_rate 0.00191364
2017-09-29T13:33:09.197938: step 245, loss 0.130152, acc 0.9375, learning_rate 0.00190624
2017-09-29T13:33:09.380925: step 246, loss 0.154101, acc 0.921875, learning_rate 0.00189887
2017-09-29T13:33:09.574797: step 247, loss 0.267092, acc 0.921875, learning_rate 0.00189153
2017-09-29T13:33:09.763779: step 248, loss 0.193686, acc 0.90625, learning_rate 0.00188421
2017-09-29T13:33:09.944939: step 249, loss 0.285036, acc 0.875, learning_rate 0.00187693
2017-09-29T13:33:10.124975: step 250, loss 0.262709, acc 0.921875, learning_rate 0.00186968
2017-09-29T13:33:10.306552: step 251, loss 0.253892, acc 0.890625, learning_rate 0.00186245
2017-09-29T13:33:10.490724: step 252, loss 0.184839, acc 0.921875, learning_rate 0.00185526
2017-09-29T13:33:10.676219: step 253, loss 0.202841, acc 0.9375, learning_rate 0.0018481
2017-09-29T13:33:10.862423: step 254, loss 0.230313, acc 0.9375, learning_rate 0.00184096
2017-09-29T13:33:11.052799: step 255, loss 0.243594, acc 0.921875, learning_rate 0.00183385
2017-09-29T13:33:11.237436: step 256, loss 0.0985205, acc 0.953125, learning_rate 0.00182678
2017-09-29T13:33:11.420905: step 257, loss 0.223743, acc 0.921875, learning_rate 0.00181973
2017-09-29T13:33:11.603746: step 258, loss 0.130646, acc 0.953125, learning_rate 0.00181271
2017-09-29T13:33:11.804862: step 259, loss 0.215149, acc 0.921875, learning_rate 0.00180572
2017-09-29T13:33:11.991773: step 260, loss 0.170822, acc 0.921875, learning_rate 0.00179876
2017-09-29T13:33:12.173839: step 261, loss 0.216331, acc 0.890625, learning_rate 0.00179182
2017-09-29T13:33:12.358135: step 262, loss 0.235863, acc 0.890625, learning_rate 0.00178492
2017-09-29T13:33:12.539542: step 263, loss 0.208654, acc 0.890625, learning_rate 0.00177804
2017-09-29T13:33:12.725998: step 264, loss 0.150071, acc 0.9375, learning_rate 0.00177119
2017-09-29T13:33:12.915599: step 265, loss 0.267523, acc 0.90625, learning_rate 0.00176437
2017-09-29T13:33:13.098420: step 266, loss 0.118616, acc 0.953125, learning_rate 0.00175758
2017-09-29T13:33:13.285971: step 267, loss 0.26081, acc 0.890625, learning_rate 0.00175081
2017-09-29T13:33:13.477670: step 268, loss 0.346341, acc 0.875, learning_rate 0.00174407
2017-09-29T13:33:13.663865: step 269, loss 0.277092, acc 0.90625, learning_rate 0.00173736
2017-09-29T13:33:13.858304: step 270, loss 0.230612, acc 0.921875, learning_rate 0.00173068
2017-09-29T13:33:14.068520: step 271, loss 0.178647, acc 0.921875, learning_rate 0.00172402
2017-09-29T13:33:14.259370: step 272, loss 0.107247, acc 0.96875, learning_rate 0.00171739
2017-09-29T13:33:14.456768: step 273, loss 0.128705, acc 0.953125, learning_rate 0.00171079
2017-09-29T13:33:14.646995: step 274, loss 0.290167, acc 0.9375, learning_rate 0.00170422
2017-09-29T13:33:14.843565: step 275, loss 0.240359, acc 0.921875, learning_rate 0.00169767
2017-09-29T13:33:15.031198: step 276, loss 0.221609, acc 0.90625, learning_rate 0.00169115
2017-09-29T13:33:15.214794: step 277, loss 0.130593, acc 0.96875, learning_rate 0.00168465
2017-09-29T13:33:15.404915: step 278, loss 0.186866, acc 0.90625, learning_rate 0.00167818
2017-09-29T13:33:15.592177: step 279, loss 0.142835, acc 0.96875, learning_rate 0.00167174
2017-09-29T13:33:15.789093: step 280, loss 0.180545, acc 0.9375, learning_rate 0.00166533

Evaluation:
2017-09-29T13:33:16.364355: step 280, loss 0.277406, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-280

2017-09-29T13:33:17.126896: step 281, loss 0.174331, acc 0.953125, learning_rate 0.00165894
2017-09-29T13:33:17.312938: step 282, loss 0.216686, acc 0.953125, learning_rate 0.00165257
2017-09-29T13:33:17.513851: step 283, loss 0.168008, acc 0.921875, learning_rate 0.00164624
2017-09-29T13:33:17.697005: step 284, loss 0.478746, acc 0.875, learning_rate 0.00163993
2017-09-29T13:33:17.892282: step 285, loss 0.0860967, acc 0.96875, learning_rate 0.00163364
2017-09-29T13:33:18.099535: step 286, loss 0.294646, acc 0.875, learning_rate 0.00162738
2017-09-29T13:33:18.289410: step 287, loss 0.226456, acc 0.9375, learning_rate 0.00162115
2017-09-29T13:33:18.479064: step 288, loss 0.230262, acc 0.921875, learning_rate 0.00161494
2017-09-29T13:33:18.667981: step 289, loss 0.12042, acc 0.953125, learning_rate 0.00160875
2017-09-29T13:33:18.848897: step 290, loss 0.151771, acc 0.953125, learning_rate 0.00160259
2017-09-29T13:33:19.030490: step 291, loss 0.229758, acc 0.90625, learning_rate 0.00159646
2017-09-29T13:33:19.214150: step 292, loss 0.261385, acc 0.9375, learning_rate 0.00159035
2017-09-29T13:33:19.405497: step 293, loss 0.207482, acc 0.953125, learning_rate 0.00158427
2017-09-29T13:33:19.560272: step 294, loss 0.196384, acc 0.941176, learning_rate 0.00157821
2017-09-29T13:33:19.752000: step 295, loss 0.0588341, acc 0.984375, learning_rate 0.00157218
2017-09-29T13:33:19.942524: step 296, loss 0.147191, acc 0.953125, learning_rate 0.00156617
2017-09-29T13:33:20.133518: step 297, loss 0.162327, acc 0.9375, learning_rate 0.00156018
2017-09-29T13:33:20.332364: step 298, loss 0.357382, acc 0.921875, learning_rate 0.00155422
2017-09-29T13:33:20.530092: step 299, loss 0.212993, acc 0.890625, learning_rate 0.00154829
2017-09-29T13:33:20.710809: step 300, loss 0.0944962, acc 0.984375, learning_rate 0.00154238
2017-09-29T13:33:20.897675: step 301, loss 0.0803922, acc 0.984375, learning_rate 0.00153649
2017-09-29T13:33:21.097685: step 302, loss 0.0673547, acc 1, learning_rate 0.00153063
2017-09-29T13:33:21.286276: step 303, loss 0.0912191, acc 0.96875, learning_rate 0.00152479
2017-09-29T13:33:21.488061: step 304, loss 0.134636, acc 0.953125, learning_rate 0.00151897
2017-09-29T13:33:21.676256: step 305, loss 0.102602, acc 0.953125, learning_rate 0.00151318
2017-09-29T13:33:21.867255: step 306, loss 0.138609, acc 0.96875, learning_rate 0.00150741
2017-09-29T13:33:22.064914: step 307, loss 0.128572, acc 0.953125, learning_rate 0.00150167
2017-09-29T13:33:22.252266: step 308, loss 0.156618, acc 0.921875, learning_rate 0.00149594
2017-09-29T13:33:22.443797: step 309, loss 0.230303, acc 0.9375, learning_rate 0.00149025
2017-09-29T13:33:22.634647: step 310, loss 0.261192, acc 0.875, learning_rate 0.00148457
2017-09-29T13:33:22.830436: step 311, loss 0.236807, acc 0.90625, learning_rate 0.00147892
2017-09-29T13:33:23.018042: step 312, loss 0.234175, acc 0.90625, learning_rate 0.00147329
2017-09-29T13:33:23.204887: step 313, loss 0.185556, acc 0.90625, learning_rate 0.00146769
2017-09-29T13:33:23.390879: step 314, loss 0.168119, acc 0.953125, learning_rate 0.0014621
2017-09-29T13:33:23.585848: step 315, loss 0.156469, acc 0.921875, learning_rate 0.00145654
2017-09-29T13:33:23.779993: step 316, loss 0.153525, acc 0.96875, learning_rate 0.00145101
2017-09-29T13:33:24.007841: step 317, loss 0.178187, acc 0.96875, learning_rate 0.00144549
2017-09-29T13:33:24.254052: step 318, loss 0.11359, acc 0.96875, learning_rate 0.00144
2017-09-29T13:33:24.486026: step 319, loss 0.13013, acc 0.953125, learning_rate 0.00143453
2017-09-29T13:33:24.681919: step 320, loss 0.154613, acc 0.953125, learning_rate 0.00142908

Evaluation:
2017-09-29T13:33:25.232150: step 320, loss 0.23335, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-320

2017-09-29T13:33:25.961564: step 321, loss 0.106562, acc 0.96875, learning_rate 0.00142366
2017-09-29T13:33:26.149071: step 322, loss 0.0629471, acc 0.984375, learning_rate 0.00141826
2017-09-29T13:33:26.338297: step 323, loss 0.0989622, acc 0.9375, learning_rate 0.00141288
2017-09-29T13:33:26.534980: step 324, loss 0.174051, acc 0.9375, learning_rate 0.00140752
2017-09-29T13:33:26.728271: step 325, loss 0.0748421, acc 0.984375, learning_rate 0.00140218
2017-09-29T13:33:26.917369: step 326, loss 0.101959, acc 0.96875, learning_rate 0.00139686
2017-09-29T13:33:27.107155: step 327, loss 0.215967, acc 0.921875, learning_rate 0.00139157
2017-09-29T13:33:27.292556: step 328, loss 0.10663, acc 0.96875, learning_rate 0.0013863
2017-09-29T13:33:27.479724: step 329, loss 0.211132, acc 0.921875, learning_rate 0.00138105
2017-09-29T13:33:27.668117: step 330, loss 0.12192, acc 0.921875, learning_rate 0.00137582
2017-09-29T13:33:27.863282: step 331, loss 0.249819, acc 0.90625, learning_rate 0.00137061
2017-09-29T13:33:28.047862: step 332, loss 0.0892222, acc 0.96875, learning_rate 0.00136543
2017-09-29T13:33:28.244142: step 333, loss 0.165989, acc 0.953125, learning_rate 0.00136026
2017-09-29T13:33:28.442810: step 334, loss 0.137214, acc 0.953125, learning_rate 0.00135512
2017-09-29T13:33:28.628035: step 335, loss 0.0811139, acc 0.96875, learning_rate 0.00134999
2017-09-29T13:33:28.810758: step 336, loss 0.118972, acc 0.984375, learning_rate 0.00134489
2017-09-29T13:33:28.996539: step 337, loss 0.248042, acc 0.890625, learning_rate 0.00133981
2017-09-29T13:33:29.192638: step 338, loss 0.14148, acc 0.96875, learning_rate 0.00133475
2017-09-29T13:33:29.385104: step 339, loss 0.16142, acc 0.9375, learning_rate 0.00132971
2017-09-29T13:33:29.597660: step 340, loss 0.0929744, acc 0.984375, learning_rate 0.00132469
2017-09-29T13:33:29.813005: step 341, loss 0.155463, acc 0.953125, learning_rate 0.00131969
2017-09-29T13:33:30.019449: step 342, loss 0.0531659, acc 0.984375, learning_rate 0.00131471
2017-09-29T13:33:30.200893: step 343, loss 0.108309, acc 0.96875, learning_rate 0.00130975
2017-09-29T13:33:30.383603: step 344, loss 0.135301, acc 0.953125, learning_rate 0.00130482
2017-09-29T13:33:30.568773: step 345, loss 0.202958, acc 0.90625, learning_rate 0.0012999
2017-09-29T13:33:30.753024: step 346, loss 0.171709, acc 0.953125, learning_rate 0.001295
2017-09-29T13:33:30.944834: step 347, loss 0.177496, acc 0.9375, learning_rate 0.00129012
2017-09-29T13:33:31.130047: step 348, loss 0.151997, acc 0.9375, learning_rate 0.00128527
2017-09-29T13:33:31.320341: step 349, loss 0.134673, acc 0.9375, learning_rate 0.00128043
2017-09-29T13:33:31.512122: step 350, loss 0.12228, acc 0.96875, learning_rate 0.00127561
2017-09-29T13:33:31.702175: step 351, loss 0.134218, acc 0.984375, learning_rate 0.00127081
2017-09-29T13:33:31.888433: step 352, loss 0.134994, acc 0.953125, learning_rate 0.00126603
2017-09-29T13:33:32.070761: step 353, loss 0.105427, acc 0.953125, learning_rate 0.00126127
2017-09-29T13:33:32.252404: step 354, loss 0.14134, acc 0.953125, learning_rate 0.00125653
2017-09-29T13:33:32.448217: step 355, loss 0.0812154, acc 0.984375, learning_rate 0.00125181
2017-09-29T13:33:32.631570: step 356, loss 0.196159, acc 0.9375, learning_rate 0.00124711
2017-09-29T13:33:32.813247: step 357, loss 0.193019, acc 0.90625, learning_rate 0.00124243
2017-09-29T13:33:32.993975: step 358, loss 0.0799158, acc 0.96875, learning_rate 0.00123777
2017-09-29T13:33:33.178141: step 359, loss 0.105195, acc 0.96875, learning_rate 0.00123312
2017-09-29T13:33:33.361623: step 360, loss 0.102642, acc 0.96875, learning_rate 0.0012285

Evaluation:
2017-09-29T13:33:33.906782: step 360, loss 0.235232, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-360

2017-09-29T13:33:34.647801: step 361, loss 0.131172, acc 0.96875, learning_rate 0.00122389
2017-09-29T13:33:34.836307: step 362, loss 0.104198, acc 0.984375, learning_rate 0.0012193
2017-09-29T13:33:35.041051: step 363, loss 0.200505, acc 0.921875, learning_rate 0.00121473
2017-09-29T13:33:35.237835: step 364, loss 0.0621332, acc 1, learning_rate 0.00121018
2017-09-29T13:33:35.419498: step 365, loss 0.193571, acc 0.953125, learning_rate 0.00120565
2017-09-29T13:33:35.605827: step 366, loss 0.116989, acc 0.96875, learning_rate 0.00120114
2017-09-29T13:33:35.808329: step 367, loss 0.136959, acc 0.984375, learning_rate 0.00119664
2017-09-29T13:33:35.989728: step 368, loss 0.223191, acc 0.9375, learning_rate 0.00119217
2017-09-29T13:33:36.194832: step 369, loss 0.040975, acc 1, learning_rate 0.00118771
2017-09-29T13:33:36.392657: step 370, loss 0.130799, acc 0.953125, learning_rate 0.00118327
2017-09-29T13:33:36.579909: step 371, loss 0.204797, acc 0.921875, learning_rate 0.00117885
2017-09-29T13:33:36.778331: step 372, loss 0.379762, acc 0.859375, learning_rate 0.00117445
2017-09-29T13:33:36.975967: step 373, loss 0.184764, acc 0.9375, learning_rate 0.00117006
2017-09-29T13:33:37.171829: step 374, loss 0.161709, acc 0.984375, learning_rate 0.00116569
2017-09-29T13:33:37.354793: step 375, loss 0.211605, acc 0.953125, learning_rate 0.00116134
2017-09-29T13:33:37.556302: step 376, loss 0.129798, acc 0.953125, learning_rate 0.00115701
2017-09-29T13:33:37.763105: step 377, loss 0.103973, acc 0.953125, learning_rate 0.0011527
2017-09-29T13:33:37.951180: step 378, loss 0.12944, acc 0.984375, learning_rate 0.0011484
2017-09-29T13:33:38.148048: step 379, loss 0.154678, acc 0.9375, learning_rate 0.00114412
2017-09-29T13:33:38.335890: step 380, loss 0.103096, acc 0.96875, learning_rate 0.00113986
2017-09-29T13:33:38.530423: step 381, loss 0.137534, acc 0.9375, learning_rate 0.00113561
2017-09-29T13:33:38.729532: step 382, loss 0.136728, acc 0.9375, learning_rate 0.00113139
2017-09-29T13:33:38.928996: step 383, loss 0.103096, acc 0.96875, learning_rate 0.00112718
2017-09-29T13:33:39.124256: step 384, loss 0.209496, acc 0.90625, learning_rate 0.00112298
2017-09-29T13:33:39.318855: step 385, loss 0.145119, acc 0.953125, learning_rate 0.00111881
2017-09-29T13:33:39.523427: step 386, loss 0.105683, acc 0.96875, learning_rate 0.00111465
2017-09-29T13:33:39.716545: step 387, loss 0.185435, acc 0.9375, learning_rate 0.00111051
2017-09-29T13:33:39.918471: step 388, loss 0.213044, acc 0.9375, learning_rate 0.00110638
2017-09-29T13:33:40.106258: step 389, loss 0.116271, acc 0.96875, learning_rate 0.00110228
2017-09-29T13:33:40.302670: step 390, loss 0.131223, acc 0.96875, learning_rate 0.00109818
2017-09-29T13:33:40.489871: step 391, loss 0.129354, acc 0.96875, learning_rate 0.00109411
2017-09-29T13:33:40.645440: step 392, loss 0.0686008, acc 1, learning_rate 0.00109005
2017-09-29T13:33:40.830428: step 393, loss 0.14599, acc 0.96875, learning_rate 0.00108601
2017-09-29T13:33:41.024019: step 394, loss 0.0618927, acc 1, learning_rate 0.00108199
2017-09-29T13:33:41.216190: step 395, loss 0.140562, acc 0.953125, learning_rate 0.00107798
2017-09-29T13:33:41.400291: step 396, loss 0.0398616, acc 1, learning_rate 0.00107399
2017-09-29T13:33:41.582612: step 397, loss 0.107544, acc 0.953125, learning_rate 0.00107001
2017-09-29T13:33:41.768289: step 398, loss 0.0455333, acc 0.984375, learning_rate 0.00106605
2017-09-29T13:33:41.964298: step 399, loss 0.175574, acc 0.921875, learning_rate 0.00106211
2017-09-29T13:33:42.162624: step 400, loss 0.114309, acc 0.96875, learning_rate 0.00105818

Evaluation:
2017-09-29T13:33:42.717707: step 400, loss 0.220646, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-400

2017-09-29T13:33:43.367504: step 401, loss 0.22563, acc 0.921875, learning_rate 0.00105427
2017-09-29T13:33:43.566440: step 402, loss 0.186849, acc 0.921875, learning_rate 0.00105037
2017-09-29T13:33:43.751071: step 403, loss 0.205326, acc 0.9375, learning_rate 0.0010465
2017-09-29T13:33:43.936184: step 404, loss 0.0975721, acc 0.984375, learning_rate 0.00104263
2017-09-29T13:33:44.124551: step 405, loss 0.112496, acc 0.953125, learning_rate 0.00103878
2017-09-29T13:33:44.311405: step 406, loss 0.0296438, acc 1, learning_rate 0.00103495
2017-09-29T13:33:44.514071: step 407, loss 0.0911071, acc 0.984375, learning_rate 0.00103114
2017-09-29T13:33:44.727725: step 408, loss 0.0759441, acc 0.96875, learning_rate 0.00102734
2017-09-29T13:33:44.937006: step 409, loss 0.154222, acc 0.953125, learning_rate 0.00102355
2017-09-29T13:33:45.164070: step 410, loss 0.0718565, acc 0.984375, learning_rate 0.00101978
2017-09-29T13:33:45.348628: step 411, loss 0.174491, acc 0.921875, learning_rate 0.00101603
2017-09-29T13:33:45.554754: step 412, loss 0.0580178, acc 0.96875, learning_rate 0.00101229
2017-09-29T13:33:45.757263: step 413, loss 0.115299, acc 0.953125, learning_rate 0.00100856
2017-09-29T13:33:45.960253: step 414, loss 0.110657, acc 0.96875, learning_rate 0.00100486
2017-09-29T13:33:46.156224: step 415, loss 0.0979633, acc 0.953125, learning_rate 0.00100116
2017-09-29T13:33:46.360205: step 416, loss 0.0882657, acc 0.96875, learning_rate 0.000997483
2017-09-29T13:33:46.553329: step 417, loss 0.120005, acc 0.984375, learning_rate 0.00099382
2017-09-29T13:33:46.751304: step 418, loss 0.0581984, acc 0.984375, learning_rate 0.000990172
2017-09-29T13:33:46.944724: step 419, loss 0.10222, acc 0.953125, learning_rate 0.000986538
2017-09-29T13:33:47.139270: step 420, loss 0.134324, acc 0.953125, learning_rate 0.00098292
2017-09-29T13:33:47.349137: step 421, loss 0.0703874, acc 0.96875, learning_rate 0.000979316
2017-09-29T13:33:47.547914: step 422, loss 0.078538, acc 0.953125, learning_rate 0.000975727
2017-09-29T13:33:47.744047: step 423, loss 0.0439427, acc 0.984375, learning_rate 0.000972152
2017-09-29T13:33:47.944538: step 424, loss 0.150139, acc 0.96875, learning_rate 0.000968592
2017-09-29T13:33:48.136948: step 425, loss 0.253072, acc 0.9375, learning_rate 0.000965047
2017-09-29T13:33:48.324729: step 426, loss 0.173113, acc 0.953125, learning_rate 0.000961516
2017-09-29T13:33:48.528345: step 427, loss 0.135418, acc 0.96875, learning_rate 0.000958
2017-09-29T13:33:48.730445: step 428, loss 0.0723525, acc 0.96875, learning_rate 0.000954497
2017-09-29T13:33:48.916859: step 429, loss 0.207928, acc 0.9375, learning_rate 0.00095101
2017-09-29T13:33:49.105391: step 430, loss 0.0960532, acc 0.984375, learning_rate 0.000947536
2017-09-29T13:33:49.302456: step 431, loss 0.0669133, acc 0.984375, learning_rate 0.000944076
2017-09-29T13:33:49.505614: step 432, loss 0.123486, acc 0.953125, learning_rate 0.000940631
2017-09-29T13:33:49.703851: step 433, loss 0.079108, acc 0.984375, learning_rate 0.0009372
2017-09-29T13:33:49.905749: step 434, loss 0.106897, acc 0.96875, learning_rate 0.000933783
2017-09-29T13:33:50.100564: step 435, loss 0.178954, acc 0.921875, learning_rate 0.000930379
2017-09-29T13:33:50.291935: step 436, loss 0.0755475, acc 0.984375, learning_rate 0.00092699
2017-09-29T13:33:50.483604: step 437, loss 0.0920426, acc 0.96875, learning_rate 0.000923614
2017-09-29T13:33:50.690544: step 438, loss 0.0783173, acc 0.984375, learning_rate 0.000920253
2017-09-29T13:33:50.904621: step 439, loss 0.151435, acc 0.9375, learning_rate 0.000916905
2017-09-29T13:33:51.108461: step 440, loss 0.124951, acc 1, learning_rate 0.00091357

Evaluation:
2017-09-29T13:33:51.659289: step 440, loss 0.226621, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-440

2017-09-29T13:33:52.365670: step 441, loss 0.192114, acc 0.90625, learning_rate 0.000910249
2017-09-29T13:33:52.552602: step 442, loss 0.0950283, acc 0.96875, learning_rate 0.000906942
2017-09-29T13:33:52.747962: step 443, loss 0.147671, acc 0.953125, learning_rate 0.000903648
2017-09-29T13:33:52.939529: step 444, loss 0.14487, acc 0.953125, learning_rate 0.000900368
2017-09-29T13:33:53.128991: step 445, loss 0.168326, acc 0.953125, learning_rate 0.000897101
2017-09-29T13:33:53.316757: step 446, loss 0.0982434, acc 0.96875, learning_rate 0.000893848
2017-09-29T13:33:53.504964: step 447, loss 0.25185, acc 0.9375, learning_rate 0.000890607
2017-09-29T13:33:53.688409: step 448, loss 0.0723645, acc 0.984375, learning_rate 0.00088738
2017-09-29T13:33:53.871463: step 449, loss 0.143549, acc 0.96875, learning_rate 0.000884166
2017-09-29T13:33:54.058557: step 450, loss 0.21715, acc 0.921875, learning_rate 0.000880966
2017-09-29T13:33:54.241587: step 451, loss 0.170004, acc 0.9375, learning_rate 0.000877778
2017-09-29T13:33:54.427719: step 452, loss 0.245962, acc 0.9375, learning_rate 0.000874603
2017-09-29T13:33:54.616433: step 453, loss 0.233553, acc 0.921875, learning_rate 0.000871441
2017-09-29T13:33:54.809589: step 454, loss 0.0983375, acc 0.984375, learning_rate 0.000868293
2017-09-29T13:33:54.991488: step 455, loss 0.108786, acc 0.953125, learning_rate 0.000865157
2017-09-29T13:33:55.177578: step 456, loss 0.0946324, acc 0.96875, learning_rate 0.000862033
2017-09-29T13:33:55.361905: step 457, loss 0.135628, acc 0.9375, learning_rate 0.000858923
2017-09-29T13:33:55.548206: step 458, loss 0.116368, acc 0.96875, learning_rate 0.000855825
2017-09-29T13:33:55.733877: step 459, loss 0.136186, acc 0.9375, learning_rate 0.00085274
2017-09-29T13:33:55.919262: step 460, loss 0.141277, acc 0.96875, learning_rate 0.000849668
2017-09-29T13:33:56.112006: step 461, loss 0.170498, acc 0.9375, learning_rate 0.000846608
2017-09-29T13:33:56.302965: step 462, loss 0.143796, acc 0.953125, learning_rate 0.00084356
2017-09-29T13:33:56.495330: step 463, loss 0.121136, acc 0.953125, learning_rate 0.000840525
2017-09-29T13:33:56.682087: step 464, loss 0.0676039, acc 0.984375, learning_rate 0.000837502
2017-09-29T13:33:56.866451: step 465, loss 0.110453, acc 0.953125, learning_rate 0.000834492
2017-09-29T13:33:57.051936: step 466, loss 0.119465, acc 0.984375, learning_rate 0.000831494
2017-09-29T13:33:57.233352: step 467, loss 0.0904863, acc 0.953125, learning_rate 0.000828508
2017-09-29T13:33:57.432802: step 468, loss 0.146417, acc 0.953125, learning_rate 0.000825535
2017-09-29T13:33:57.637182: step 469, loss 0.17161, acc 0.9375, learning_rate 0.000822573
2017-09-29T13:33:57.844502: step 470, loss 0.144744, acc 0.953125, learning_rate 0.000819624
2017-09-29T13:33:58.050016: step 471, loss 0.114748, acc 0.984375, learning_rate 0.000816687
2017-09-29T13:33:58.259759: step 472, loss 0.249372, acc 0.921875, learning_rate 0.000813761
2017-09-29T13:33:58.446244: step 473, loss 0.185273, acc 0.9375, learning_rate 0.000810848
2017-09-29T13:33:58.630289: step 474, loss 0.0662023, acc 0.984375, learning_rate 0.000807946
2017-09-29T13:33:58.831510: step 475, loss 0.126667, acc 0.96875, learning_rate 0.000805057
2017-09-29T13:33:59.038392: step 476, loss 0.128166, acc 0.96875, learning_rate 0.000802179
2017-09-29T13:33:59.243080: step 477, loss 0.0565975, acc 1, learning_rate 0.000799313
2017-09-29T13:33:59.427985: step 478, loss 0.153269, acc 0.921875, learning_rate 0.000796458
2017-09-29T13:33:59.624159: step 479, loss 0.121921, acc 0.953125, learning_rate 0.000793616
2017-09-29T13:33:59.829370: step 480, loss 0.0595365, acc 0.984375, learning_rate 0.000790784

Evaluation:
2017-09-29T13:34:00.397916: step 480, loss 0.221992, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-480

2017-09-29T13:34:01.193768: step 481, loss 0.148623, acc 0.9375, learning_rate 0.000787965
2017-09-29T13:34:01.384814: step 482, loss 0.116875, acc 0.953125, learning_rate 0.000785157
2017-09-29T13:34:01.566232: step 483, loss 0.216129, acc 0.90625, learning_rate 0.00078236
2017-09-29T13:34:01.749556: step 484, loss 0.0720045, acc 0.96875, learning_rate 0.000779575
2017-09-29T13:34:01.967430: step 485, loss 0.0875429, acc 0.984375, learning_rate 0.000776801
2017-09-29T13:34:02.173329: step 486, loss 0.0982586, acc 0.953125, learning_rate 0.000774038
2017-09-29T13:34:02.376707: step 487, loss 0.196552, acc 0.9375, learning_rate 0.000771287
2017-09-29T13:34:02.575670: step 488, loss 0.0650496, acc 0.984375, learning_rate 0.000768547
2017-09-29T13:34:02.759827: step 489, loss 0.220254, acc 0.953125, learning_rate 0.000765818
2017-09-29T13:34:02.914720: step 490, loss 0.13398, acc 0.960784, learning_rate 0.000763101
2017-09-29T13:34:03.098273: step 491, loss 0.08661, acc 0.953125, learning_rate 0.000760394
2017-09-29T13:34:03.283091: step 492, loss 0.133659, acc 0.96875, learning_rate 0.000757698
2017-09-29T13:34:03.468347: step 493, loss 0.0501443, acc 1, learning_rate 0.000755014
2017-09-29T13:34:03.655206: step 494, loss 0.0746507, acc 0.96875, learning_rate 0.00075234
2017-09-29T13:34:03.845227: step 495, loss 0.132833, acc 0.953125, learning_rate 0.000749677
2017-09-29T13:34:04.032201: step 496, loss 0.0448891, acc 1, learning_rate 0.000747026
2017-09-29T13:34:04.217027: step 497, loss 0.129062, acc 0.9375, learning_rate 0.000744385
2017-09-29T13:34:04.413016: step 498, loss 0.0886624, acc 0.96875, learning_rate 0.000741754
2017-09-29T13:34:04.596557: step 499, loss 0.0510711, acc 0.984375, learning_rate 0.000739135
2017-09-29T13:34:04.783786: step 500, loss 0.105905, acc 0.96875, learning_rate 0.000736526
2017-09-29T13:34:04.977578: step 501, loss 0.0477076, acc 0.984375, learning_rate 0.000733928
2017-09-29T13:34:05.158691: step 502, loss 0.125326, acc 0.953125, learning_rate 0.00073134
2017-09-29T13:34:05.344253: step 503, loss 0.056794, acc 0.984375, learning_rate 0.000728763
2017-09-29T13:34:05.537290: step 504, loss 0.0839772, acc 0.984375, learning_rate 0.000726197
2017-09-29T13:34:05.723326: step 505, loss 0.0667367, acc 0.96875, learning_rate 0.000723641
2017-09-29T13:34:05.910200: step 506, loss 0.119455, acc 0.921875, learning_rate 0.000721095
2017-09-29T13:34:06.091604: step 507, loss 0.0629556, acc 0.984375, learning_rate 0.00071856
2017-09-29T13:34:06.283453: step 508, loss 0.178784, acc 0.953125, learning_rate 0.000716036
2017-09-29T13:34:06.488567: step 509, loss 0.0690046, acc 0.984375, learning_rate 0.000713521
2017-09-29T13:34:06.679062: step 510, loss 0.0835352, acc 0.96875, learning_rate 0.000711017
2017-09-29T13:34:06.868560: step 511, loss 0.0642656, acc 0.984375, learning_rate 0.000708523
2017-09-29T13:34:07.053346: step 512, loss 0.0594299, acc 0.984375, learning_rate 0.000706039
2017-09-29T13:34:07.244014: step 513, loss 0.100015, acc 0.984375, learning_rate 0.000703565
2017-09-29T13:34:07.431969: step 514, loss 0.0465384, acc 1, learning_rate 0.000701102
2017-09-29T13:34:07.618826: step 515, loss 0.0913954, acc 0.984375, learning_rate 0.000698648
2017-09-29T13:34:07.803398: step 516, loss 0.0455313, acc 0.984375, learning_rate 0.000696204
2017-09-29T13:34:07.991189: step 517, loss 0.123165, acc 0.953125, learning_rate 0.000693771
2017-09-29T13:34:08.178998: step 518, loss 0.138104, acc 0.9375, learning_rate 0.000691347
2017-09-29T13:34:08.363838: step 519, loss 0.126924, acc 0.96875, learning_rate 0.000688934
2017-09-29T13:34:08.546748: step 520, loss 0.0991935, acc 0.96875, learning_rate 0.00068653

Evaluation:
2017-09-29T13:34:09.089791: step 520, loss 0.225237, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-520

2017-09-29T13:34:09.732114: step 521, loss 0.132809, acc 0.953125, learning_rate 0.000684136
2017-09-29T13:34:09.915110: step 522, loss 0.0542195, acc 0.96875, learning_rate 0.000681751
2017-09-29T13:34:10.103358: step 523, loss 0.0673992, acc 0.984375, learning_rate 0.000679377
2017-09-29T13:34:10.289819: step 524, loss 0.0691603, acc 0.984375, learning_rate 0.000677012
2017-09-29T13:34:10.476136: step 525, loss 0.117812, acc 0.96875, learning_rate 0.000674657
2017-09-29T13:34:10.658277: step 526, loss 0.0827289, acc 0.984375, learning_rate 0.000672311
2017-09-29T13:34:10.848374: step 527, loss 0.0441058, acc 1, learning_rate 0.000669975
2017-09-29T13:34:11.036108: step 528, loss 0.0546815, acc 0.984375, learning_rate 0.000667648
2017-09-29T13:34:11.221625: step 529, loss 0.115077, acc 0.984375, learning_rate 0.000665331
2017-09-29T13:34:11.411609: step 530, loss 0.0773668, acc 0.96875, learning_rate 0.000663024
2017-09-29T13:34:11.594506: step 531, loss 0.083585, acc 0.96875, learning_rate 0.000660726
2017-09-29T13:34:11.778875: step 532, loss 0.122572, acc 0.953125, learning_rate 0.000658437
2017-09-29T13:34:11.960549: step 533, loss 0.143826, acc 0.953125, learning_rate 0.000656158
2017-09-29T13:34:12.145310: step 534, loss 0.0546692, acc 0.984375, learning_rate 0.000653888
2017-09-29T13:34:12.327154: step 535, loss 0.245079, acc 0.953125, learning_rate 0.000651627
2017-09-29T13:34:12.522351: step 536, loss 0.207858, acc 0.9375, learning_rate 0.000649375
2017-09-29T13:34:12.707529: step 537, loss 0.0977178, acc 0.96875, learning_rate 0.000647133
2017-09-29T13:34:12.889882: step 538, loss 0.11051, acc 0.953125, learning_rate 0.000644899
2017-09-29T13:34:13.072914: step 539, loss 0.137991, acc 0.96875, learning_rate 0.000642675
2017-09-29T13:34:13.255273: step 540, loss 0.0736101, acc 0.96875, learning_rate 0.00064046
2017-09-29T13:34:13.440901: step 541, loss 0.084684, acc 0.984375, learning_rate 0.000638254
2017-09-29T13:34:13.625089: step 542, loss 0.0333249, acc 1, learning_rate 0.000636057
2017-09-29T13:34:13.805589: step 543, loss 0.218792, acc 0.921875, learning_rate 0.000633869
2017-09-29T13:34:13.991589: step 544, loss 0.0702441, acc 0.984375, learning_rate 0.00063169
2017-09-29T13:34:14.172494: step 545, loss 0.212667, acc 0.921875, learning_rate 0.00062952
2017-09-29T13:34:14.369902: step 546, loss 0.116144, acc 0.96875, learning_rate 0.000627358
2017-09-29T13:34:14.567815: step 547, loss 0.0605276, acc 0.984375, learning_rate 0.000625206
2017-09-29T13:34:14.762434: step 548, loss 0.115209, acc 0.953125, learning_rate 0.000623062
2017-09-29T13:34:14.947933: step 549, loss 0.0431964, acc 1, learning_rate 0.000620927
2017-09-29T13:34:15.135752: step 550, loss 0.0949297, acc 0.984375, learning_rate 0.000618801
2017-09-29T13:34:15.331838: step 551, loss 0.074032, acc 0.96875, learning_rate 0.000616683
2017-09-29T13:34:15.538864: step 552, loss 0.0921587, acc 0.96875, learning_rate 0.000614574
2017-09-29T13:34:15.733685: step 553, loss 0.141972, acc 0.96875, learning_rate 0.000612474
2017-09-29T13:34:15.920177: step 554, loss 0.0482122, acc 1, learning_rate 0.000610382
2017-09-29T13:34:16.100915: step 555, loss 0.121466, acc 0.953125, learning_rate 0.000608299
2017-09-29T13:34:16.288183: step 556, loss 0.1789, acc 0.9375, learning_rate 0.000606224
2017-09-29T13:34:16.483438: step 557, loss 0.093932, acc 0.953125, learning_rate 0.000604158
2017-09-29T13:34:16.667545: step 558, loss 0.101841, acc 0.96875, learning_rate 0.0006021
2017-09-29T13:34:16.854252: step 559, loss 0.102306, acc 0.953125, learning_rate 0.00060005
2017-09-29T13:34:17.039782: step 560, loss 0.0971941, acc 0.96875, learning_rate 0.000598009

Evaluation:
2017-09-29T13:34:17.590017: step 560, loss 0.230252, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-560

2017-09-29T13:34:18.323791: step 561, loss 0.0967162, acc 0.96875, learning_rate 0.000595977
2017-09-29T13:34:18.527379: step 562, loss 0.183721, acc 0.921875, learning_rate 0.000593952
2017-09-29T13:34:18.718977: step 563, loss 0.164231, acc 0.984375, learning_rate 0.000591936
2017-09-29T13:34:18.908635: step 564, loss 0.0651313, acc 0.96875, learning_rate 0.000589928
2017-09-29T13:34:19.099809: step 565, loss 0.0880271, acc 0.984375, learning_rate 0.000587928
2017-09-29T13:34:19.292575: step 566, loss 0.154463, acc 0.9375, learning_rate 0.000585937
2017-09-29T13:34:19.489466: step 567, loss 0.0976941, acc 0.96875, learning_rate 0.000583953
2017-09-29T13:34:19.680537: step 568, loss 0.180599, acc 0.9375, learning_rate 0.000581978
2017-09-29T13:34:19.864867: step 569, loss 0.171546, acc 0.96875, learning_rate 0.00058001
2017-09-29T13:34:20.062409: step 570, loss 0.050921, acc 0.984375, learning_rate 0.000578051
2017-09-29T13:34:20.246314: step 571, loss 0.165808, acc 0.96875, learning_rate 0.0005761
2017-09-29T13:34:20.446558: step 572, loss 0.0986889, acc 0.984375, learning_rate 0.000574157
2017-09-29T13:34:20.646472: step 573, loss 0.0797672, acc 0.96875, learning_rate 0.000572221
2017-09-29T13:34:20.841153: step 574, loss 0.0729623, acc 0.984375, learning_rate 0.000570294
2017-09-29T13:34:21.033938: step 575, loss 0.0778374, acc 0.984375, learning_rate 0.000568374
2017-09-29T13:34:21.245597: step 576, loss 0.229004, acc 0.890625, learning_rate 0.000566462
2017-09-29T13:34:21.454206: step 577, loss 0.143204, acc 0.9375, learning_rate 0.000564558
2017-09-29T13:34:21.648920: step 578, loss 0.134651, acc 0.953125, learning_rate 0.000562662
2017-09-29T13:34:21.853157: step 579, loss 0.234906, acc 0.921875, learning_rate 0.000560774
2017-09-29T13:34:22.076010: step 580, loss 0.0207474, acc 1, learning_rate 0.000558893
2017-09-29T13:34:22.306963: step 581, loss 0.0823254, acc 0.984375, learning_rate 0.00055702
2017-09-29T13:34:22.519218: step 582, loss 0.0576032, acc 1, learning_rate 0.000555154
2017-09-29T13:34:22.723548: step 583, loss 0.114672, acc 0.953125, learning_rate 0.000553296
2017-09-29T13:34:22.921772: step 584, loss 0.135578, acc 0.953125, learning_rate 0.000551446
2017-09-29T13:34:23.119176: step 585, loss 0.0706108, acc 1, learning_rate 0.000549604
2017-09-29T13:34:23.312350: step 586, loss 0.108982, acc 0.96875, learning_rate 0.000547768
2017-09-29T13:34:23.507364: step 587, loss 0.0342103, acc 0.984375, learning_rate 0.000545941
2017-09-29T13:34:23.668686: step 588, loss 0.172157, acc 0.941176, learning_rate 0.00054412
2017-09-29T13:34:23.869865: step 589, loss 0.0878351, acc 0.984375, learning_rate 0.000542308
2017-09-29T13:34:24.059953: step 590, loss 0.145121, acc 0.96875, learning_rate 0.000540502
2017-09-29T13:34:24.255628: step 591, loss 0.0790666, acc 0.96875, learning_rate 0.000538704
2017-09-29T13:34:24.447358: step 592, loss 0.0240777, acc 1, learning_rate 0.000536914
2017-09-29T13:34:24.635015: step 593, loss 0.0764302, acc 0.984375, learning_rate 0.00053513
2017-09-29T13:34:24.827639: step 594, loss 0.0661674, acc 0.984375, learning_rate 0.000533354
2017-09-29T13:34:25.016555: step 595, loss 0.13942, acc 0.9375, learning_rate 0.000531585
2017-09-29T13:34:25.243043: step 596, loss 0.0359226, acc 1, learning_rate 0.000529824
2017-09-29T13:34:25.439693: step 597, loss 0.0762396, acc 0.96875, learning_rate 0.000528069
2017-09-29T13:34:25.635704: step 598, loss 0.15318, acc 0.921875, learning_rate 0.000526322
2017-09-29T13:34:25.828759: step 599, loss 0.0545795, acc 0.984375, learning_rate 0.000524582
2017-09-29T13:34:26.016925: step 600, loss 0.115074, acc 0.953125, learning_rate 0.000522849

Evaluation:
2017-09-29T13:34:26.615205: step 600, loss 0.216211, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-600

2017-09-29T13:34:27.443192: step 601, loss 0.11693, acc 0.96875, learning_rate 0.000521123
2017-09-29T13:34:27.642323: step 602, loss 0.0648913, acc 0.984375, learning_rate 0.000519404
2017-09-29T13:34:27.824794: step 603, loss 0.256142, acc 0.90625, learning_rate 0.000517692
2017-09-29T13:34:28.010113: step 604, loss 0.0798331, acc 0.96875, learning_rate 0.000515987
2017-09-29T13:34:28.195754: step 605, loss 0.115871, acc 0.984375, learning_rate 0.000514289
2017-09-29T13:34:28.377328: step 606, loss 0.0300985, acc 0.984375, learning_rate 0.000512598
2017-09-29T13:34:28.563171: step 607, loss 0.175954, acc 0.9375, learning_rate 0.000510914
2017-09-29T13:34:28.750932: step 608, loss 0.143673, acc 0.96875, learning_rate 0.000509237
2017-09-29T13:34:28.935677: step 609, loss 0.157717, acc 0.953125, learning_rate 0.000507566
2017-09-29T13:34:29.122600: step 610, loss 0.111423, acc 0.953125, learning_rate 0.000505903
2017-09-29T13:34:29.306302: step 611, loss 0.0955684, acc 0.984375, learning_rate 0.000504246
2017-09-29T13:34:29.518308: step 612, loss 0.0477984, acc 0.984375, learning_rate 0.000502596
2017-09-29T13:34:29.719219: step 613, loss 0.18557, acc 0.953125, learning_rate 0.000500953
2017-09-29T13:34:29.919210: step 614, loss 0.0801613, acc 0.984375, learning_rate 0.000499316
2017-09-29T13:34:30.110853: step 615, loss 0.0462675, acc 0.984375, learning_rate 0.000497686
2017-09-29T13:34:30.295614: step 616, loss 0.170627, acc 0.9375, learning_rate 0.000496063
2017-09-29T13:34:30.490788: step 617, loss 0.065053, acc 1, learning_rate 0.000494446
2017-09-29T13:34:30.712364: step 618, loss 0.104815, acc 0.953125, learning_rate 0.000492836
2017-09-29T13:34:30.922153: step 619, loss 0.156726, acc 0.953125, learning_rate 0.000491233
2017-09-29T13:34:31.142887: step 620, loss 0.0317905, acc 1, learning_rate 0.000489636
2017-09-29T13:34:31.363473: step 621, loss 0.0748118, acc 1, learning_rate 0.000488045
2017-09-29T13:34:31.614339: step 622, loss 0.0930535, acc 0.96875, learning_rate 0.000486461
2017-09-29T13:34:31.809447: step 623, loss 0.0565935, acc 0.984375, learning_rate 0.000484884
2017-09-29T13:34:32.019944: step 624, loss 0.102224, acc 0.984375, learning_rate 0.000483313
2017-09-29T13:34:32.225920: step 625, loss 0.100506, acc 0.984375, learning_rate 0.000481748
2017-09-29T13:34:32.447982: step 626, loss 0.0465407, acc 1, learning_rate 0.00048019
2017-09-29T13:34:32.673965: step 627, loss 0.0470361, acc 0.984375, learning_rate 0.000478638
2017-09-29T13:34:32.902054: step 628, loss 0.169651, acc 0.9375, learning_rate 0.000477093
2017-09-29T13:34:33.104666: step 629, loss 0.0742037, acc 1, learning_rate 0.000475554
2017-09-29T13:34:33.302835: step 630, loss 0.109789, acc 0.953125, learning_rate 0.000474021
2017-09-29T13:34:33.511597: step 631, loss 0.0877827, acc 0.984375, learning_rate 0.000472494
2017-09-29T13:34:33.707474: step 632, loss 0.041844, acc 1, learning_rate 0.000470974
2017-09-29T13:34:33.898502: step 633, loss 0.154665, acc 0.953125, learning_rate 0.000469459
2017-09-29T13:34:34.097377: step 634, loss 0.121157, acc 0.96875, learning_rate 0.000467951
2017-09-29T13:34:34.295113: step 635, loss 0.0413568, acc 1, learning_rate 0.000466449
2017-09-29T13:34:34.486259: step 636, loss 0.0956377, acc 0.96875, learning_rate 0.000464954
2017-09-29T13:34:34.672588: step 637, loss 0.103158, acc 0.953125, learning_rate 0.000463464
2017-09-29T13:34:34.862787: step 638, loss 0.101592, acc 0.953125, learning_rate 0.00046198
2017-09-29T13:34:35.060687: step 639, loss 0.141669, acc 0.9375, learning_rate 0.000460503
2017-09-29T13:34:35.250941: step 640, loss 0.11327, acc 0.96875, learning_rate 0.000459031

Evaluation:
2017-09-29T13:34:35.847112: step 640, loss 0.230279, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-640

2017-09-29T13:34:36.515102: step 641, loss 0.102253, acc 0.96875, learning_rate 0.000457566
2017-09-29T13:34:36.729715: step 642, loss 0.0858588, acc 0.984375, learning_rate 0.000456106
2017-09-29T13:34:36.934550: step 643, loss 0.0787175, acc 0.984375, learning_rate 0.000454653
2017-09-29T13:34:37.135697: step 644, loss 0.0701977, acc 0.96875, learning_rate 0.000453205
2017-09-29T13:34:37.320900: step 645, loss 0.119829, acc 0.921875, learning_rate 0.000451764
2017-09-29T13:34:37.515364: step 646, loss 0.125202, acc 0.984375, learning_rate 0.000450328
2017-09-29T13:34:37.702288: step 647, loss 0.130898, acc 0.96875, learning_rate 0.000448898
2017-09-29T13:34:37.913322: step 648, loss 0.0852141, acc 0.953125, learning_rate 0.000447474
2017-09-29T13:34:38.109980: step 649, loss 0.0572298, acc 0.984375, learning_rate 0.000446055
2017-09-29T13:34:38.297141: step 650, loss 0.0822517, acc 0.953125, learning_rate 0.000444643
2017-09-29T13:34:38.482298: step 651, loss 0.196151, acc 0.9375, learning_rate 0.000443236
2017-09-29T13:34:38.666862: step 652, loss 0.11368, acc 0.953125, learning_rate 0.000441835
2017-09-29T13:34:38.868296: step 653, loss 0.137816, acc 0.9375, learning_rate 0.00044044
2017-09-29T13:34:39.085315: step 654, loss 0.105603, acc 0.953125, learning_rate 0.00043905
2017-09-29T13:34:39.289863: step 655, loss 0.100459, acc 0.984375, learning_rate 0.000437666
2017-09-29T13:34:39.541916: step 656, loss 0.0684982, acc 0.984375, learning_rate 0.000436288
2017-09-29T13:34:39.805523: step 657, loss 0.0503738, acc 0.984375, learning_rate 0.000434915
2017-09-29T13:34:40.062916: step 658, loss 0.0823278, acc 0.984375, learning_rate 0.000433548
2017-09-29T13:34:40.277921: step 659, loss 0.166003, acc 0.921875, learning_rate 0.000432187
2017-09-29T13:34:40.485261: step 660, loss 0.120485, acc 0.9375, learning_rate 0.000430831
2017-09-29T13:34:40.672838: step 661, loss 0.0728195, acc 0.984375, learning_rate 0.000429481
2017-09-29T13:34:40.863870: step 662, loss 0.0583691, acc 0.96875, learning_rate 0.000428136
2017-09-29T13:34:41.053240: step 663, loss 0.109906, acc 0.953125, learning_rate 0.000426796
2017-09-29T13:34:41.240474: step 664, loss 0.13321, acc 0.921875, learning_rate 0.000425463
2017-09-29T13:34:41.436520: step 665, loss 0.171774, acc 0.9375, learning_rate 0.000424134
2017-09-29T13:34:41.624681: step 666, loss 0.112051, acc 0.953125, learning_rate 0.000422811
2017-09-29T13:34:41.811574: step 667, loss 0.158712, acc 0.9375, learning_rate 0.000421493
2017-09-29T13:34:41.999251: step 668, loss 0.0279378, acc 1, learning_rate 0.000420181
2017-09-29T13:34:42.184604: step 669, loss 0.0695296, acc 0.984375, learning_rate 0.000418874
2017-09-29T13:34:42.365725: step 670, loss 0.0547174, acc 0.984375, learning_rate 0.000417573
2017-09-29T13:34:42.550874: step 671, loss 0.102493, acc 0.984375, learning_rate 0.000416276
2017-09-29T13:34:42.741869: step 672, loss 0.0533565, acc 1, learning_rate 0.000414985
2017-09-29T13:34:42.926452: step 673, loss 0.119155, acc 0.953125, learning_rate 0.0004137
2017-09-29T13:34:43.109277: step 674, loss 0.119946, acc 0.921875, learning_rate 0.000412419
2017-09-29T13:34:43.290854: step 675, loss 0.134294, acc 0.953125, learning_rate 0.000411144
2017-09-29T13:34:43.501103: step 676, loss 0.217788, acc 0.96875, learning_rate 0.000409874
2017-09-29T13:34:43.687747: step 677, loss 0.0382959, acc 1, learning_rate 0.000408609
2017-09-29T13:34:43.873511: step 678, loss 0.0475465, acc 0.984375, learning_rate 0.00040735
2017-09-29T13:34:44.070446: step 679, loss 0.065148, acc 0.984375, learning_rate 0.000406095
2017-09-29T13:34:44.258556: step 680, loss 0.0703157, acc 0.984375, learning_rate 0.000404846

Evaluation:
2017-09-29T13:34:44.823529: step 680, loss 0.215756, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-680

2017-09-29T13:34:45.548954: step 681, loss 0.144112, acc 0.96875, learning_rate 0.000403601
2017-09-29T13:34:45.748428: step 682, loss 0.0969916, acc 0.953125, learning_rate 0.000402362
2017-09-29T13:34:45.968108: step 683, loss 0.0999236, acc 0.984375, learning_rate 0.000401128
2017-09-29T13:34:46.174334: step 684, loss 0.193389, acc 0.921875, learning_rate 0.000399899
2017-09-29T13:34:46.366361: step 685, loss 0.0589117, acc 0.96875, learning_rate 0.000398675
2017-09-29T13:34:46.526540: step 686, loss 0.192798, acc 0.921569, learning_rate 0.000397456
2017-09-29T13:34:46.726076: step 687, loss 0.183644, acc 0.953125, learning_rate 0.000396241
2017-09-29T13:34:46.919197: step 688, loss 0.0260691, acc 1, learning_rate 0.000395032
2017-09-29T13:34:47.113762: step 689, loss 0.0955886, acc 0.953125, learning_rate 0.000393828
2017-09-29T13:34:47.304336: step 690, loss 0.0804706, acc 0.984375, learning_rate 0.000392629
2017-09-29T13:34:47.518049: step 691, loss 0.112388, acc 0.96875, learning_rate 0.000391434
2017-09-29T13:34:47.716516: step 692, loss 0.12558, acc 0.96875, learning_rate 0.000390245
2017-09-29T13:34:47.910165: step 693, loss 0.101729, acc 0.984375, learning_rate 0.00038906
2017-09-29T13:34:48.114302: step 694, loss 0.0233514, acc 1, learning_rate 0.00038788
2017-09-29T13:34:48.305784: step 695, loss 0.0700433, acc 1, learning_rate 0.000386705
2017-09-29T13:34:48.514261: step 696, loss 0.0947155, acc 0.984375, learning_rate 0.000385535
2017-09-29T13:34:48.737297: step 697, loss 0.0688448, acc 0.984375, learning_rate 0.000384369
2017-09-29T13:34:48.955672: step 698, loss 0.117795, acc 0.9375, learning_rate 0.000383209
2017-09-29T13:34:49.197737: step 699, loss 0.088307, acc 0.984375, learning_rate 0.000382053
2017-09-29T13:34:49.403934: step 700, loss 0.0733145, acc 0.984375, learning_rate 0.000380901
2017-09-29T13:34:49.597376: step 701, loss 0.0809916, acc 0.984375, learning_rate 0.000379755
2017-09-29T13:34:49.793026: step 702, loss 0.079529, acc 0.953125, learning_rate 0.000378613
2017-09-29T13:34:50.006861: step 703, loss 0.102644, acc 0.984375, learning_rate 0.000377476
2017-09-29T13:34:50.243565: step 704, loss 0.078014, acc 0.96875, learning_rate 0.000376343
2017-09-29T13:34:50.490908: step 705, loss 0.139271, acc 0.953125, learning_rate 0.000375215
2017-09-29T13:34:50.690379: step 706, loss 0.0904173, acc 0.96875, learning_rate 0.000374092
2017-09-29T13:34:50.876559: step 707, loss 0.10121, acc 0.96875, learning_rate 0.000372973
2017-09-29T13:34:51.066678: step 708, loss 0.0836745, acc 0.96875, learning_rate 0.000371859
2017-09-29T13:34:51.267522: step 709, loss 0.0785909, acc 0.96875, learning_rate 0.000370749
2017-09-29T13:34:51.458424: step 710, loss 0.0761521, acc 0.96875, learning_rate 0.000369644
2017-09-29T13:34:51.648715: step 711, loss 0.0929274, acc 0.96875, learning_rate 0.000368543
2017-09-29T13:34:51.852795: step 712, loss 0.0642246, acc 0.984375, learning_rate 0.000367447
2017-09-29T13:34:52.042492: step 713, loss 0.129784, acc 0.984375, learning_rate 0.000366356
2017-09-29T13:34:52.231057: step 714, loss 0.10936, acc 0.96875, learning_rate 0.000365268
2017-09-29T13:34:52.422373: step 715, loss 0.0813824, acc 0.984375, learning_rate 0.000364186
2017-09-29T13:34:52.623645: step 716, loss 0.16485, acc 0.96875, learning_rate 0.000363107
2017-09-29T13:34:52.815902: step 717, loss 0.0610094, acc 0.984375, learning_rate 0.000362033
2017-09-29T13:34:53.018888: step 718, loss 0.0784563, acc 0.96875, learning_rate 0.000360964
2017-09-29T13:34:53.215736: step 719, loss 0.211708, acc 0.9375, learning_rate 0.000359899
2017-09-29T13:34:53.410296: step 720, loss 0.174246, acc 0.96875, learning_rate 0.000358838

Evaluation:
2017-09-29T13:34:53.966329: step 720, loss 0.222443, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-720

2017-09-29T13:34:54.778867: step 721, loss 0.0499044, acc 0.984375, learning_rate 0.000357781
2017-09-29T13:34:54.962634: step 722, loss 0.11074, acc 0.9375, learning_rate 0.000356729
2017-09-29T13:34:55.157574: step 723, loss 0.211802, acc 0.9375, learning_rate 0.000355681
2017-09-29T13:34:55.340963: step 724, loss 0.0555531, acc 1, learning_rate 0.000354637
2017-09-29T13:34:55.528301: step 725, loss 0.123374, acc 0.953125, learning_rate 0.000353598
2017-09-29T13:34:55.712330: step 726, loss 0.107493, acc 0.96875, learning_rate 0.000352563
2017-09-29T13:34:55.898139: step 727, loss 0.0637854, acc 0.984375, learning_rate 0.000351532
2017-09-29T13:34:56.088603: step 728, loss 0.163727, acc 0.953125, learning_rate 0.000350505
2017-09-29T13:34:56.293576: step 729, loss 0.071913, acc 0.984375, learning_rate 0.000349483
2017-09-29T13:34:56.500011: step 730, loss 0.106704, acc 0.96875, learning_rate 0.000348465
2017-09-29T13:34:56.687854: step 731, loss 0.103017, acc 0.96875, learning_rate 0.00034745
2017-09-29T13:34:56.880238: step 732, loss 0.0305345, acc 1, learning_rate 0.00034644
2017-09-29T13:34:57.067305: step 733, loss 0.0550637, acc 1, learning_rate 0.000345434
2017-09-29T13:34:57.252812: step 734, loss 0.205088, acc 0.90625, learning_rate 0.000344433
2017-09-29T13:34:57.434863: step 735, loss 0.0494467, acc 1, learning_rate 0.000343435
2017-09-29T13:34:57.617675: step 736, loss 0.0868827, acc 0.96875, learning_rate 0.000342441
2017-09-29T13:34:57.798691: step 737, loss 0.0887613, acc 0.96875, learning_rate 0.000341452
2017-09-29T13:34:57.981235: step 738, loss 0.0813662, acc 0.984375, learning_rate 0.000340466
2017-09-29T13:34:58.169656: step 739, loss 0.0957687, acc 0.984375, learning_rate 0.000339485
2017-09-29T13:34:58.353833: step 740, loss 0.129219, acc 0.96875, learning_rate 0.000338507
2017-09-29T13:34:58.566365: step 741, loss 0.0364574, acc 1, learning_rate 0.000337534
2017-09-29T13:34:58.755772: step 742, loss 0.0368248, acc 1, learning_rate 0.000336564
2017-09-29T13:34:58.948012: step 743, loss 0.044076, acc 0.984375, learning_rate 0.000335598
2017-09-29T13:34:59.134364: step 744, loss 0.0121671, acc 1, learning_rate 0.000334637
2017-09-29T13:34:59.320293: step 745, loss 0.123223, acc 0.953125, learning_rate 0.000333679
2017-09-29T13:34:59.505889: step 746, loss 0.0686625, acc 0.984375, learning_rate 0.000332725
2017-09-29T13:34:59.710395: step 747, loss 0.129843, acc 0.9375, learning_rate 0.000331775
2017-09-29T13:34:59.898869: step 748, loss 0.0948212, acc 0.984375, learning_rate 0.000330829
2017-09-29T13:35:00.090954: step 749, loss 0.032786, acc 1, learning_rate 0.000329887
2017-09-29T13:35:00.278938: step 750, loss 0.111228, acc 0.953125, learning_rate 0.000328949
2017-09-29T13:35:00.469225: step 751, loss 0.0847078, acc 0.984375, learning_rate 0.000328014
2017-09-29T13:35:00.672633: step 752, loss 0.0831803, acc 0.96875, learning_rate 0.000327083
2017-09-29T13:35:00.877558: step 753, loss 0.0836252, acc 0.984375, learning_rate 0.000326157
2017-09-29T13:35:01.083951: step 754, loss 0.0689507, acc 0.984375, learning_rate 0.000325233
2017-09-29T13:35:01.274250: step 755, loss 0.127183, acc 0.953125, learning_rate 0.000324314
2017-09-29T13:35:01.461792: step 756, loss 0.117694, acc 0.921875, learning_rate 0.000323399
2017-09-29T13:35:01.645820: step 757, loss 0.0670178, acc 0.984375, learning_rate 0.000322487
2017-09-29T13:35:01.848014: step 758, loss 0.132174, acc 0.953125, learning_rate 0.000321579
2017-09-29T13:35:02.039050: step 759, loss 0.139361, acc 0.9375, learning_rate 0.000320674
2017-09-29T13:35:02.227916: step 760, loss 0.105051, acc 0.96875, learning_rate 0.000319773

Evaluation:
2017-09-29T13:35:02.780733: step 760, loss 0.222653, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-760

2017-09-29T13:35:03.447516: step 761, loss 0.0584964, acc 0.96875, learning_rate 0.000318876
2017-09-29T13:35:03.634153: step 762, loss 0.0789745, acc 0.984375, learning_rate 0.000317983
2017-09-29T13:35:03.821337: step 763, loss 0.0729138, acc 0.96875, learning_rate 0.000317093
2017-09-29T13:35:04.007665: step 764, loss 0.11465, acc 0.9375, learning_rate 0.000316207
2017-09-29T13:35:04.192990: step 765, loss 0.0802502, acc 0.96875, learning_rate 0.000315325
2017-09-29T13:35:04.375895: step 766, loss 0.0557649, acc 0.984375, learning_rate 0.000314446
2017-09-29T13:35:04.571994: step 767, loss 0.111168, acc 0.96875, learning_rate 0.00031357
2017-09-29T13:35:04.762031: step 768, loss 0.15016, acc 0.921875, learning_rate 0.000312699
2017-09-29T13:35:04.953287: step 769, loss 0.051536, acc 1, learning_rate 0.00031183
2017-09-29T13:35:05.139794: step 770, loss 0.0879163, acc 0.953125, learning_rate 0.000310966
2017-09-29T13:35:05.324921: step 771, loss 0.0670054, acc 0.984375, learning_rate 0.000310105
2017-09-29T13:35:05.512503: step 772, loss 0.0410682, acc 1, learning_rate 0.000309247
2017-09-29T13:35:05.703494: step 773, loss 0.0693689, acc 0.984375, learning_rate 0.000308393
2017-09-29T13:35:05.894360: step 774, loss 0.0844208, acc 0.984375, learning_rate 0.000307542
2017-09-29T13:35:06.081376: step 775, loss 0.0821242, acc 0.984375, learning_rate 0.000306695
2017-09-29T13:35:06.271275: step 776, loss 0.0621181, acc 0.96875, learning_rate 0.000305852
2017-09-29T13:35:06.456234: step 777, loss 0.0946636, acc 0.984375, learning_rate 0.000305011
2017-09-29T13:35:06.658495: step 778, loss 0.115021, acc 0.984375, learning_rate 0.000304174
2017-09-29T13:35:06.854383: step 779, loss 0.108914, acc 0.984375, learning_rate 0.000303341
2017-09-29T13:35:07.038860: step 780, loss 0.0590724, acc 1, learning_rate 0.000302511
2017-09-29T13:35:07.225450: step 781, loss 0.0247093, acc 1, learning_rate 0.000301684
2017-09-29T13:35:07.415891: step 782, loss 0.111672, acc 0.9375, learning_rate 0.000300861
2017-09-29T13:35:07.599595: step 783, loss 0.0261461, acc 1, learning_rate 0.000300041
2017-09-29T13:35:07.757981: step 784, loss 0.031031, acc 1, learning_rate 0.000299225
2017-09-29T13:35:07.942811: step 785, loss 0.0148053, acc 1, learning_rate 0.000298412
2017-09-29T13:35:08.128750: step 786, loss 0.108882, acc 0.96875, learning_rate 0.000297602
2017-09-29T13:35:08.332486: step 787, loss 0.0798849, acc 0.96875, learning_rate 0.000296795
2017-09-29T13:35:08.523938: step 788, loss 0.0901891, acc 0.953125, learning_rate 0.000295992
2017-09-29T13:35:08.709988: step 789, loss 0.0940093, acc 0.984375, learning_rate 0.000295192
2017-09-29T13:35:08.896555: step 790, loss 0.0575733, acc 0.984375, learning_rate 0.000294395
2017-09-29T13:35:09.081664: step 791, loss 0.136907, acc 0.953125, learning_rate 0.000293602
2017-09-29T13:35:09.267563: step 792, loss 0.074728, acc 0.984375, learning_rate 0.000292812
2017-09-29T13:35:09.455041: step 793, loss 0.154221, acc 0.953125, learning_rate 0.000292025
2017-09-29T13:35:09.644462: step 794, loss 0.103987, acc 0.9375, learning_rate 0.000291241
2017-09-29T13:35:09.831052: step 795, loss 0.131038, acc 0.9375, learning_rate 0.00029046
2017-09-29T13:35:10.019389: step 796, loss 0.0355837, acc 1, learning_rate 0.000289683
2017-09-29T13:35:10.202215: step 797, loss 0.100065, acc 0.953125, learning_rate 0.000288908
2017-09-29T13:35:10.390975: step 798, loss 0.118933, acc 0.9375, learning_rate 0.000288137
2017-09-29T13:35:10.588693: step 799, loss 0.0660614, acc 0.984375, learning_rate 0.000287369
2017-09-29T13:35:10.781746: step 800, loss 0.107268, acc 0.9375, learning_rate 0.000286605

Evaluation:
2017-09-29T13:35:11.365080: step 800, loss 0.224278, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-800

2017-09-29T13:35:12.127421: step 801, loss 0.14279, acc 0.9375, learning_rate 0.000285843
2017-09-29T13:35:12.313472: step 802, loss 0.059998, acc 0.984375, learning_rate 0.000285084
2017-09-29T13:35:12.507102: step 803, loss 0.0599072, acc 0.96875, learning_rate 0.000284329
2017-09-29T13:35:12.694457: step 804, loss 0.0681193, acc 0.984375, learning_rate 0.000283577
2017-09-29T13:35:12.880258: step 805, loss 0.0647265, acc 0.984375, learning_rate 0.000282827
2017-09-29T13:35:13.081528: step 806, loss 0.108923, acc 0.96875, learning_rate 0.000282081
2017-09-29T13:35:13.278448: step 807, loss 0.0883482, acc 0.96875, learning_rate 0.000281338
2017-09-29T13:35:13.468448: step 808, loss 0.0339405, acc 1, learning_rate 0.000280598
2017-09-29T13:35:13.657476: step 809, loss 0.0523528, acc 0.984375, learning_rate 0.00027986
2017-09-29T13:35:13.847762: step 810, loss 0.0805458, acc 0.984375, learning_rate 0.000279126
2017-09-29T13:35:14.033966: step 811, loss 0.0636602, acc 0.984375, learning_rate 0.000278395
2017-09-29T13:35:14.221114: step 812, loss 0.0949793, acc 0.96875, learning_rate 0.000277667
2017-09-29T13:35:14.409111: step 813, loss 0.142127, acc 0.96875, learning_rate 0.000276942
2017-09-29T13:35:14.599128: step 814, loss 0.0448456, acc 0.984375, learning_rate 0.00027622
2017-09-29T13:35:14.788239: step 815, loss 0.0808124, acc 0.96875, learning_rate 0.0002755
2017-09-29T13:35:14.975777: step 816, loss 0.125308, acc 0.96875, learning_rate 0.000274784
2017-09-29T13:35:15.163840: step 817, loss 0.212401, acc 0.921875, learning_rate 0.000274071
2017-09-29T13:35:15.357891: step 818, loss 0.12436, acc 0.96875, learning_rate 0.00027336
2017-09-29T13:35:15.541807: step 819, loss 0.144328, acc 0.9375, learning_rate 0.000272652
2017-09-29T13:35:15.743425: step 820, loss 0.0462879, acc 1, learning_rate 0.000271948
2017-09-29T13:35:15.930094: step 821, loss 0.0851872, acc 0.96875, learning_rate 0.000271246
2017-09-29T13:35:16.113351: step 822, loss 0.0758627, acc 0.96875, learning_rate 0.000270547
2017-09-29T13:35:16.300050: step 823, loss 0.071995, acc 0.96875, learning_rate 0.000269851
2017-09-29T13:35:16.497204: step 824, loss 0.0638079, acc 1, learning_rate 0.000269157
2017-09-29T13:35:16.680655: step 825, loss 0.0832071, acc 0.953125, learning_rate 0.000268467
2017-09-29T13:35:16.864965: step 826, loss 0.0912241, acc 0.984375, learning_rate 0.000267779
2017-09-29T13:35:17.055622: step 827, loss 0.0411486, acc 1, learning_rate 0.000267094
2017-09-29T13:35:17.250846: step 828, loss 0.145981, acc 0.953125, learning_rate 0.000266412
2017-09-29T13:35:17.441806: step 829, loss 0.0808771, acc 0.984375, learning_rate 0.000265733
2017-09-29T13:35:17.636240: step 830, loss 0.102654, acc 0.953125, learning_rate 0.000265057
2017-09-29T13:35:17.845893: step 831, loss 0.0671476, acc 0.984375, learning_rate 0.000264383
2017-09-29T13:35:18.034012: step 832, loss 0.0625626, acc 0.984375, learning_rate 0.000263712
2017-09-29T13:35:18.224686: step 833, loss 0.0424905, acc 1, learning_rate 0.000263044
2017-09-29T13:35:18.409570: step 834, loss 0.075046, acc 0.984375, learning_rate 0.000262378
2017-09-29T13:35:18.596133: step 835, loss 0.0706991, acc 0.96875, learning_rate 0.000261715
2017-09-29T13:35:18.781733: step 836, loss 0.0976794, acc 0.953125, learning_rate 0.000261055
2017-09-29T13:35:18.964792: step 837, loss 0.0933815, acc 0.96875, learning_rate 0.000260398
2017-09-29T13:35:19.148181: step 838, loss 0.088461, acc 0.96875, learning_rate 0.000259743
2017-09-29T13:35:19.332496: step 839, loss 0.0651171, acc 0.984375, learning_rate 0.000259091
2017-09-29T13:35:19.515521: step 840, loss 0.0771684, acc 0.96875, learning_rate 0.000258442

Evaluation:
2017-09-29T13:35:20.035116: step 840, loss 0.218496, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-840

2017-09-29T13:35:20.838999: step 841, loss 0.0229932, acc 1, learning_rate 0.000257795
2017-09-29T13:35:21.024856: step 842, loss 0.0551899, acc 0.984375, learning_rate 0.000257151
2017-09-29T13:35:21.219647: step 843, loss 0.0400068, acc 0.984375, learning_rate 0.00025651
2017-09-29T13:35:21.402431: step 844, loss 0.0803103, acc 0.96875, learning_rate 0.000255871
2017-09-29T13:35:21.588433: step 845, loss 0.0839263, acc 0.984375, learning_rate 0.000255235
2017-09-29T13:35:21.773370: step 846, loss 0.0637192, acc 0.96875, learning_rate 0.000254601
2017-09-29T13:35:21.972805: step 847, loss 0.0955717, acc 0.96875, learning_rate 0.00025397
2017-09-29T13:35:22.157984: step 848, loss 0.0895314, acc 0.984375, learning_rate 0.000253341
2017-09-29T13:35:22.351743: step 849, loss 0.0736227, acc 0.96875, learning_rate 0.000252716
2017-09-29T13:35:22.552808: step 850, loss 0.0928471, acc 0.96875, learning_rate 0.000252092
2017-09-29T13:35:22.752801: step 851, loss 0.0817364, acc 0.96875, learning_rate 0.000251471
2017-09-29T13:35:22.944107: step 852, loss 0.094637, acc 0.96875, learning_rate 0.000250853
2017-09-29T13:35:23.145495: step 853, loss 0.144098, acc 0.9375, learning_rate 0.000250237
2017-09-29T13:35:23.350352: step 854, loss 0.0738252, acc 0.984375, learning_rate 0.000249624
2017-09-29T13:35:23.540836: step 855, loss 0.0708753, acc 0.984375, learning_rate 0.000249013
2017-09-29T13:35:23.728346: step 856, loss 0.0482397, acc 0.984375, learning_rate 0.000248405
2017-09-29T13:35:23.911436: step 857, loss 0.127448, acc 0.921875, learning_rate 0.000247799
2017-09-29T13:35:24.120518: step 858, loss 0.0550396, acc 0.984375, learning_rate 0.000247196
2017-09-29T13:35:24.303205: step 859, loss 0.180114, acc 0.953125, learning_rate 0.000246595
2017-09-29T13:35:24.488181: step 860, loss 0.137809, acc 0.953125, learning_rate 0.000245997
2017-09-29T13:35:24.673427: step 861, loss 0.156108, acc 0.9375, learning_rate 0.000245401
2017-09-29T13:35:24.856740: step 862, loss 0.0624733, acc 0.984375, learning_rate 0.000244808
2017-09-29T13:35:25.052852: step 863, loss 0.080748, acc 0.984375, learning_rate 0.000244216
2017-09-29T13:35:25.237697: step 864, loss 0.121034, acc 0.953125, learning_rate 0.000243628
2017-09-29T13:35:25.421009: step 865, loss 0.0943265, acc 0.96875, learning_rate 0.000243042
2017-09-29T13:35:25.607379: step 866, loss 0.114542, acc 0.953125, learning_rate 0.000242458
2017-09-29T13:35:25.798066: step 867, loss 0.0677931, acc 0.984375, learning_rate 0.000241876
2017-09-29T13:35:25.982049: step 868, loss 0.0787073, acc 0.984375, learning_rate 0.000241297
2017-09-29T13:35:26.160606: step 869, loss 0.0448481, acc 0.984375, learning_rate 0.00024072
2017-09-29T13:35:26.370491: step 870, loss 0.0864532, acc 0.96875, learning_rate 0.000240146
2017-09-29T13:35:26.558896: step 871, loss 0.0473507, acc 1, learning_rate 0.000239574
2017-09-29T13:35:26.745533: step 872, loss 0.148477, acc 0.9375, learning_rate 0.000239004
2017-09-29T13:35:26.940826: step 873, loss 0.130237, acc 0.9375, learning_rate 0.000238437
2017-09-29T13:35:27.129842: step 874, loss 0.12798, acc 0.953125, learning_rate 0.000237872
2017-09-29T13:35:27.330987: step 875, loss 0.0516692, acc 0.984375, learning_rate 0.000237309
2017-09-29T13:35:27.525339: step 876, loss 0.142047, acc 0.9375, learning_rate 0.000236749
2017-09-29T13:35:27.713965: step 877, loss 0.102261, acc 0.96875, learning_rate 0.00023619
2017-09-29T13:35:27.902173: step 878, loss 0.0542687, acc 0.96875, learning_rate 0.000235635
2017-09-29T13:35:28.095539: step 879, loss 0.0338764, acc 1, learning_rate 0.000235081
2017-09-29T13:35:28.285856: step 880, loss 0.0689219, acc 0.984375, learning_rate 0.00023453

Evaluation:
2017-09-29T13:35:28.826026: step 880, loss 0.218653, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-880

2017-09-29T13:35:29.460538: step 881, loss 0.136437, acc 0.9375, learning_rate 0.00023398
2017-09-29T13:35:29.614185: step 882, loss 0.139888, acc 0.960784, learning_rate 0.000233434
2017-09-29T13:35:29.798275: step 883, loss 0.0901821, acc 0.953125, learning_rate 0.000232889
2017-09-29T13:35:29.986494: step 884, loss 0.105424, acc 0.953125, learning_rate 0.000232346
2017-09-29T13:35:30.169840: step 885, loss 0.0894478, acc 0.953125, learning_rate 0.000231806
2017-09-29T13:35:30.351633: step 886, loss 0.0808015, acc 0.96875, learning_rate 0.000231268
2017-09-29T13:35:30.535282: step 887, loss 0.045102, acc 1, learning_rate 0.000230732
2017-09-29T13:35:30.727125: step 888, loss 0.0716419, acc 0.984375, learning_rate 0.000230199
2017-09-29T13:35:30.936947: step 889, loss 0.0592843, acc 1, learning_rate 0.000229667
2017-09-29T13:35:31.141163: step 890, loss 0.0881162, acc 0.984375, learning_rate 0.000229138
2017-09-29T13:35:31.344875: step 891, loss 0.0364676, acc 0.984375, learning_rate 0.000228611
2017-09-29T13:35:31.549355: step 892, loss 0.0848717, acc 0.984375, learning_rate 0.000228086
2017-09-29T13:35:31.752689: step 893, loss 0.102964, acc 0.96875, learning_rate 0.000227563
2017-09-29T13:35:31.950118: step 894, loss 0.0609736, acc 0.984375, learning_rate 0.000227043
2017-09-29T13:35:32.141523: step 895, loss 0.0396873, acc 0.984375, learning_rate 0.000226524
2017-09-29T13:35:32.332625: step 896, loss 0.154217, acc 0.9375, learning_rate 0.000226008
2017-09-29T13:35:32.528944: step 897, loss 0.0402996, acc 0.984375, learning_rate 0.000225493
2017-09-29T13:35:32.727585: step 898, loss 0.0530018, acc 0.984375, learning_rate 0.000224981
2017-09-29T13:35:32.914904: step 899, loss 0.101845, acc 0.953125, learning_rate 0.000224471
2017-09-29T13:35:33.106880: step 900, loss 0.100208, acc 0.96875, learning_rate 0.000223963
2017-09-29T13:35:33.296049: step 901, loss 0.0403549, acc 1, learning_rate 0.000223457
2017-09-29T13:35:33.499454: step 902, loss 0.132962, acc 0.9375, learning_rate 0.000222953
2017-09-29T13:35:33.686928: step 903, loss 0.0533228, acc 0.984375, learning_rate 0.000222451
2017-09-29T13:35:33.887962: step 904, loss 0.116348, acc 0.9375, learning_rate 0.000221951
2017-09-29T13:35:34.078886: step 905, loss 0.0964436, acc 0.953125, learning_rate 0.000221453
2017-09-29T13:35:34.274737: step 906, loss 0.170831, acc 0.9375, learning_rate 0.000220958
2017-09-29T13:35:34.465305: step 907, loss 0.121957, acc 0.953125, learning_rate 0.000220464
2017-09-29T13:35:34.655384: step 908, loss 0.068435, acc 0.96875, learning_rate 0.000219972
2017-09-29T13:35:34.850429: step 909, loss 0.0677523, acc 0.984375, learning_rate 0.000219483
2017-09-29T13:35:35.048113: step 910, loss 0.0825899, acc 0.984375, learning_rate 0.000218995
2017-09-29T13:35:35.236174: step 911, loss 0.0907105, acc 0.96875, learning_rate 0.000218509
2017-09-29T13:35:35.425344: step 912, loss 0.0967623, acc 0.953125, learning_rate 0.000218025
2017-09-29T13:35:35.614159: step 913, loss 0.0652294, acc 1, learning_rate 0.000217544
2017-09-29T13:35:35.802014: step 914, loss 0.137098, acc 0.9375, learning_rate 0.000217064
2017-09-29T13:35:35.994295: step 915, loss 0.0641803, acc 0.96875, learning_rate 0.000216586
2017-09-29T13:35:36.189460: step 916, loss 0.0453132, acc 1, learning_rate 0.00021611
2017-09-29T13:35:36.381423: step 917, loss 0.0630806, acc 0.984375, learning_rate 0.000215636
2017-09-29T13:35:36.577086: step 918, loss 0.113714, acc 0.953125, learning_rate 0.000215164
2017-09-29T13:35:36.771564: step 919, loss 0.0647861, acc 0.984375, learning_rate 0.000214694
2017-09-29T13:35:36.960776: step 920, loss 0.0820331, acc 0.96875, learning_rate 0.000214226

Evaluation:
2017-09-29T13:35:37.507166: step 920, loss 0.218947, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-920

2017-09-29T13:35:38.227143: step 921, loss 0.0905262, acc 0.984375, learning_rate 0.00021376
2017-09-29T13:35:38.410259: step 922, loss 0.05003, acc 1, learning_rate 0.000213295
2017-09-29T13:35:38.601218: step 923, loss 0.14643, acc 0.9375, learning_rate 0.000212833
2017-09-29T13:35:38.797362: step 924, loss 0.0964631, acc 0.96875, learning_rate 0.000212372
2017-09-29T13:35:38.997768: step 925, loss 0.0751238, acc 0.984375, learning_rate 0.000211914
2017-09-29T13:35:39.186533: step 926, loss 0.0733871, acc 0.96875, learning_rate 0.000211457
2017-09-29T13:35:39.374219: step 927, loss 0.0453666, acc 0.984375, learning_rate 0.000211002
2017-09-29T13:35:39.584476: step 928, loss 0.0208363, acc 1, learning_rate 0.000210549
2017-09-29T13:35:39.774257: step 929, loss 0.0302034, acc 1, learning_rate 0.000210098
2017-09-29T13:35:39.964089: step 930, loss 0.0741058, acc 1, learning_rate 0.000209648
2017-09-29T13:35:40.153052: step 931, loss 0.113228, acc 0.953125, learning_rate 0.000209201
2017-09-29T13:35:40.342551: step 932, loss 0.109031, acc 0.96875, learning_rate 0.000208755
2017-09-29T13:35:40.537396: step 933, loss 0.103505, acc 0.96875, learning_rate 0.000208311
2017-09-29T13:35:40.736018: step 934, loss 0.0937033, acc 0.96875, learning_rate 0.000207869
2017-09-29T13:35:40.935017: step 935, loss 0.0560289, acc 0.984375, learning_rate 0.000207429
2017-09-29T13:35:41.124060: step 936, loss 0.0680228, acc 0.984375, learning_rate 0.00020699
2017-09-29T13:35:41.310341: step 937, loss 0.0594206, acc 0.984375, learning_rate 0.000206554
2017-09-29T13:35:41.502018: step 938, loss 0.0873559, acc 0.953125, learning_rate 0.000206119
2017-09-29T13:35:41.691111: step 939, loss 0.0740496, acc 0.96875, learning_rate 0.000205685
2017-09-29T13:35:41.886096: step 940, loss 0.0577146, acc 1, learning_rate 0.000205254
2017-09-29T13:35:42.083248: step 941, loss 0.0730047, acc 1, learning_rate 0.000204824
2017-09-29T13:35:42.275571: step 942, loss 0.100529, acc 0.984375, learning_rate 0.000204397
2017-09-29T13:35:42.470701: step 943, loss 0.0699828, acc 0.984375, learning_rate 0.00020397
2017-09-29T13:35:42.684609: step 944, loss 0.0366131, acc 1, learning_rate 0.000203546
2017-09-29T13:35:42.905252: step 945, loss 0.0846865, acc 0.984375, learning_rate 0.000203123
2017-09-29T13:35:43.121930: step 946, loss 0.0648168, acc 0.984375, learning_rate 0.000202702
2017-09-29T13:35:43.323937: step 947, loss 0.0473695, acc 1, learning_rate 0.000202283
2017-09-29T13:35:43.523069: step 948, loss 0.1262, acc 0.9375, learning_rate 0.000201866
2017-09-29T13:35:43.718539: step 949, loss 0.0903832, acc 0.96875, learning_rate 0.00020145
2017-09-29T13:35:43.928990: step 950, loss 0.126087, acc 0.9375, learning_rate 0.000201036
2017-09-29T13:35:44.116731: step 951, loss 0.0955303, acc 0.96875, learning_rate 0.000200623
2017-09-29T13:35:44.306412: step 952, loss 0.0232076, acc 1, learning_rate 0.000200213
2017-09-29T13:35:44.501199: step 953, loss 0.0950411, acc 0.984375, learning_rate 0.000199804
2017-09-29T13:35:44.692850: step 954, loss 0.0913905, acc 0.96875, learning_rate 0.000199396
2017-09-29T13:35:44.884714: step 955, loss 0.109381, acc 0.953125, learning_rate 0.000198991
2017-09-29T13:35:45.083404: step 956, loss 0.0884715, acc 0.96875, learning_rate 0.000198587
2017-09-29T13:35:45.277628: step 957, loss 0.0855735, acc 0.953125, learning_rate 0.000198184
2017-09-29T13:35:45.481938: step 958, loss 0.0237006, acc 1, learning_rate 0.000197783
2017-09-29T13:35:45.678720: step 959, loss 0.0513975, acc 0.96875, learning_rate 0.000197384
2017-09-29T13:35:45.885042: step 960, loss 0.0723561, acc 0.984375, learning_rate 0.000196987

Evaluation:
2017-09-29T13:35:46.464970: step 960, loss 0.217618, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-960

2017-09-29T13:35:47.218948: step 961, loss 0.0668245, acc 0.96875, learning_rate 0.000196591
2017-09-29T13:35:47.409500: step 962, loss 0.043383, acc 1, learning_rate 0.000196197
2017-09-29T13:35:47.613555: step 963, loss 0.256716, acc 0.9375, learning_rate 0.000195804
2017-09-29T13:35:47.805843: step 964, loss 0.0796207, acc 0.984375, learning_rate 0.000195413
2017-09-29T13:35:47.995343: step 965, loss 0.0223178, acc 1, learning_rate 0.000195023
2017-09-29T13:35:48.204419: step 966, loss 0.129327, acc 0.9375, learning_rate 0.000194636
2017-09-29T13:35:48.400068: step 967, loss 0.0819339, acc 0.984375, learning_rate 0.000194249
2017-09-29T13:35:48.591754: step 968, loss 0.145151, acc 0.921875, learning_rate 0.000193865
2017-09-29T13:35:48.781917: step 969, loss 0.0480405, acc 1, learning_rate 0.000193482
2017-09-29T13:35:48.974499: step 970, loss 0.120172, acc 0.96875, learning_rate 0.0001931
2017-09-29T13:35:49.164472: step 971, loss 0.0588502, acc 0.984375, learning_rate 0.00019272
2017-09-29T13:35:49.353984: step 972, loss 0.0652112, acc 0.984375, learning_rate 0.000192341
2017-09-29T13:35:49.546296: step 973, loss 0.0496558, acc 0.984375, learning_rate 0.000191965
2017-09-29T13:35:49.737928: step 974, loss 0.0333579, acc 0.984375, learning_rate 0.000191589
2017-09-29T13:35:49.924568: step 975, loss 0.127275, acc 0.953125, learning_rate 0.000191215
2017-09-29T13:35:50.115734: step 976, loss 0.0485836, acc 1, learning_rate 0.000190843
2017-09-29T13:35:50.304658: step 977, loss 0.106036, acc 0.96875, learning_rate 0.000190472
2017-09-29T13:35:50.505419: step 978, loss 0.138207, acc 0.9375, learning_rate 0.000190103
2017-09-29T13:35:50.697012: step 979, loss 0.0410122, acc 1, learning_rate 0.000189735
2017-09-29T13:35:50.850888: step 980, loss 0.0792564, acc 1, learning_rate 0.000189369
2017-09-29T13:35:51.046726: step 981, loss 0.0409606, acc 1, learning_rate 0.000189004
2017-09-29T13:35:51.234178: step 982, loss 0.0521027, acc 0.984375, learning_rate 0.000188641
2017-09-29T13:35:51.425962: step 983, loss 0.0857969, acc 0.96875, learning_rate 0.000188279
2017-09-29T13:35:51.613844: step 984, loss 0.0470753, acc 0.984375, learning_rate 0.000187919
2017-09-29T13:35:51.801590: step 985, loss 0.0503116, acc 0.984375, learning_rate 0.00018756
2017-09-29T13:35:51.988821: step 986, loss 0.052525, acc 1, learning_rate 0.000187202
2017-09-29T13:35:52.189595: step 987, loss 0.125712, acc 0.96875, learning_rate 0.000186846
2017-09-29T13:35:52.379234: step 988, loss 0.116081, acc 0.9375, learning_rate 0.000186492
2017-09-29T13:35:52.571209: step 989, loss 0.108589, acc 0.984375, learning_rate 0.000186139
2017-09-29T13:35:52.759985: step 990, loss 0.0857948, acc 0.96875, learning_rate 0.000185787
2017-09-29T13:35:52.950990: step 991, loss 0.0467698, acc 1, learning_rate 0.000185437
2017-09-29T13:35:53.139545: step 992, loss 0.0482534, acc 1, learning_rate 0.000185088
2017-09-29T13:35:53.336700: step 993, loss 0.0458223, acc 1, learning_rate 0.000184741
2017-09-29T13:35:53.526613: step 994, loss 0.130303, acc 0.9375, learning_rate 0.000184395
2017-09-29T13:35:53.726613: step 995, loss 0.115551, acc 0.96875, learning_rate 0.000184051
2017-09-29T13:35:53.914484: step 996, loss 0.0812168, acc 0.96875, learning_rate 0.000183708
2017-09-29T13:35:54.111517: step 997, loss 0.0731551, acc 0.984375, learning_rate 0.000183366
2017-09-29T13:35:54.312265: step 998, loss 0.179251, acc 0.953125, learning_rate 0.000183026
2017-09-29T13:35:54.510401: step 999, loss 0.104978, acc 0.953125, learning_rate 0.000182687
2017-09-29T13:35:54.703913: step 1000, loss 0.0808513, acc 0.96875, learning_rate 0.000182349

Evaluation:
2017-09-29T13:35:55.250239: step 1000, loss 0.21775, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1000

2017-09-29T13:35:56.062388: step 1001, loss 0.036188, acc 1, learning_rate 0.000182013
2017-09-29T13:35:56.252841: step 1002, loss 0.0901968, acc 0.984375, learning_rate 0.000181678
2017-09-29T13:35:56.446799: step 1003, loss 0.0759898, acc 0.984375, learning_rate 0.000181345
2017-09-29T13:35:56.637853: step 1004, loss 0.181878, acc 0.90625, learning_rate 0.000181013
2017-09-29T13:35:56.832545: step 1005, loss 0.0642473, acc 0.984375, learning_rate 0.000180682
2017-09-29T13:35:57.022151: step 1006, loss 0.161676, acc 0.9375, learning_rate 0.000180353
2017-09-29T13:35:57.218673: step 1007, loss 0.0863547, acc 0.953125, learning_rate 0.000180025
2017-09-29T13:35:57.408550: step 1008, loss 0.0356684, acc 1, learning_rate 0.000179698
2017-09-29T13:35:57.598982: step 1009, loss 0.0773586, acc 0.984375, learning_rate 0.000179373
2017-09-29T13:35:57.787972: step 1010, loss 0.0426755, acc 1, learning_rate 0.000179049
2017-09-29T13:35:57.976807: step 1011, loss 0.0306051, acc 1, learning_rate 0.000178726
2017-09-29T13:35:58.167247: step 1012, loss 0.129852, acc 0.96875, learning_rate 0.000178405
2017-09-29T13:35:58.356211: step 1013, loss 0.206884, acc 0.9375, learning_rate 0.000178085
2017-09-29T13:35:58.547903: step 1014, loss 0.106643, acc 0.96875, learning_rate 0.000177766
2017-09-29T13:35:58.735588: step 1015, loss 0.0825372, acc 0.96875, learning_rate 0.000177449
2017-09-29T13:35:58.926262: step 1016, loss 0.0810784, acc 0.96875, learning_rate 0.000177133
2017-09-29T13:35:59.129891: step 1017, loss 0.0596983, acc 0.984375, learning_rate 0.000176818
2017-09-29T13:35:59.319489: step 1018, loss 0.0210725, acc 1, learning_rate 0.000176504
2017-09-29T13:35:59.512351: step 1019, loss 0.0831865, acc 0.96875, learning_rate 0.000176192
2017-09-29T13:35:59.699975: step 1020, loss 0.170616, acc 0.953125, learning_rate 0.000175881
2017-09-29T13:35:59.897301: step 1021, loss 0.0878675, acc 0.96875, learning_rate 0.000175571
2017-09-29T13:36:00.092616: step 1022, loss 0.0519436, acc 0.984375, learning_rate 0.000175263
2017-09-29T13:36:00.288494: step 1023, loss 0.0834657, acc 0.953125, learning_rate 0.000174956
2017-09-29T13:36:00.477575: step 1024, loss 0.0847924, acc 0.96875, learning_rate 0.00017465
2017-09-29T13:36:00.666159: step 1025, loss 0.0336743, acc 0.984375, learning_rate 0.000174345
2017-09-29T13:36:00.857223: step 1026, loss 0.0709349, acc 0.984375, learning_rate 0.000174042
2017-09-29T13:36:01.049484: step 1027, loss 0.182624, acc 0.9375, learning_rate 0.000173739
2017-09-29T13:36:01.239162: step 1028, loss 0.0457628, acc 0.96875, learning_rate 0.000173438
2017-09-29T13:36:01.436844: step 1029, loss 0.0726759, acc 0.984375, learning_rate 0.000173139
2017-09-29T13:36:01.635752: step 1030, loss 0.0432497, acc 0.96875, learning_rate 0.00017284
2017-09-29T13:36:01.824797: step 1031, loss 0.048293, acc 1, learning_rate 0.000172543
2017-09-29T13:36:02.012552: step 1032, loss 0.0304397, acc 1, learning_rate 0.000172247
2017-09-29T13:36:02.212425: step 1033, loss 0.0351545, acc 1, learning_rate 0.000171952
2017-09-29T13:36:02.400463: step 1034, loss 0.0417542, acc 1, learning_rate 0.000171658
2017-09-29T13:36:02.594813: step 1035, loss 0.0583746, acc 0.984375, learning_rate 0.000171366
2017-09-29T13:36:02.784670: step 1036, loss 0.0870333, acc 0.96875, learning_rate 0.000171074
2017-09-29T13:36:02.979060: step 1037, loss 0.0917548, acc 0.96875, learning_rate 0.000170784
2017-09-29T13:36:03.171084: step 1038, loss 0.132124, acc 0.96875, learning_rate 0.000170495
2017-09-29T13:36:03.356460: step 1039, loss 0.0641087, acc 0.96875, learning_rate 0.000170208
2017-09-29T13:36:03.547959: step 1040, loss 0.0559957, acc 1, learning_rate 0.000169921

Evaluation:
2017-09-29T13:36:04.080247: step 1040, loss 0.216813, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1040

2017-09-29T13:36:04.727628: step 1041, loss 0.0472949, acc 0.984375, learning_rate 0.000169636
2017-09-29T13:36:04.916184: step 1042, loss 0.0338114, acc 1, learning_rate 0.000169351
2017-09-29T13:36:05.105101: step 1043, loss 0.0597297, acc 0.984375, learning_rate 0.000169068
2017-09-29T13:36:05.294845: step 1044, loss 0.0469007, acc 1, learning_rate 0.000168786
2017-09-29T13:36:05.482390: step 1045, loss 0.035733, acc 1, learning_rate 0.000168506
2017-09-29T13:36:05.672830: step 1046, loss 0.102797, acc 0.953125, learning_rate 0.000168226
2017-09-29T13:36:05.860248: step 1047, loss 0.105052, acc 0.9375, learning_rate 0.000167947
2017-09-29T13:36:06.050390: step 1048, loss 0.0299475, acc 0.984375, learning_rate 0.00016767
2017-09-29T13:36:06.239774: step 1049, loss 0.0484626, acc 0.984375, learning_rate 0.000167394
2017-09-29T13:36:06.432481: step 1050, loss 0.060978, acc 0.984375, learning_rate 0.000167119
2017-09-29T13:36:06.623239: step 1051, loss 0.080079, acc 0.96875, learning_rate 0.000166845
2017-09-29T13:36:06.816708: step 1052, loss 0.161256, acc 0.953125, learning_rate 0.000166572
2017-09-29T13:36:07.006358: step 1053, loss 0.0587353, acc 0.984375, learning_rate 0.0001663
2017-09-29T13:36:07.202724: step 1054, loss 0.0426875, acc 0.984375, learning_rate 0.00016603
2017-09-29T13:36:07.393629: step 1055, loss 0.0505366, acc 0.984375, learning_rate 0.00016576
2017-09-29T13:36:07.608877: step 1056, loss 0.0537233, acc 0.984375, learning_rate 0.000165492
2017-09-29T13:36:07.801653: step 1057, loss 0.070758, acc 0.984375, learning_rate 0.000165224
2017-09-29T13:36:07.993901: step 1058, loss 0.148027, acc 0.9375, learning_rate 0.000164958
2017-09-29T13:36:08.189105: step 1059, loss 0.107747, acc 0.9375, learning_rate 0.000164693
2017-09-29T13:36:08.374686: step 1060, loss 0.101346, acc 0.953125, learning_rate 0.000164429
2017-09-29T13:36:08.566773: step 1061, loss 0.105922, acc 0.953125, learning_rate 0.000164166
2017-09-29T13:36:08.761291: step 1062, loss 0.052722, acc 0.984375, learning_rate 0.000163904
2017-09-29T13:36:08.952090: step 1063, loss 0.0742766, acc 1, learning_rate 0.000163643
2017-09-29T13:36:09.146713: step 1064, loss 0.0934244, acc 0.96875, learning_rate 0.000163383
2017-09-29T13:36:09.335742: step 1065, loss 0.0407231, acc 0.984375, learning_rate 0.000163125
2017-09-29T13:36:09.541978: step 1066, loss 0.0383489, acc 1, learning_rate 0.000162867
2017-09-29T13:36:09.738659: step 1067, loss 0.0652878, acc 0.984375, learning_rate 0.00016261
2017-09-29T13:36:09.930846: step 1068, loss 0.0336315, acc 1, learning_rate 0.000162355
2017-09-29T13:36:10.120588: step 1069, loss 0.0780808, acc 0.96875, learning_rate 0.0001621
2017-09-29T13:36:10.312533: step 1070, loss 0.05291, acc 0.984375, learning_rate 0.000161847
2017-09-29T13:36:10.506322: step 1071, loss 0.0759319, acc 0.96875, learning_rate 0.000161594
2017-09-29T13:36:10.703573: step 1072, loss 0.126008, acc 0.953125, learning_rate 0.000161343
2017-09-29T13:36:10.899408: step 1073, loss 0.0582505, acc 0.984375, learning_rate 0.000161093
2017-09-29T13:36:11.098167: step 1074, loss 0.0325997, acc 1, learning_rate 0.000160843
2017-09-29T13:36:11.286031: step 1075, loss 0.0154143, acc 1, learning_rate 0.000160595
2017-09-29T13:36:11.476819: step 1076, loss 0.0976902, acc 0.953125, learning_rate 0.000160348
2017-09-29T13:36:11.666657: step 1077, loss 0.082605, acc 0.96875, learning_rate 0.000160101
2017-09-29T13:36:11.820162: step 1078, loss 0.0491263, acc 1, learning_rate 0.000159856
2017-09-29T13:36:12.008255: step 1079, loss 0.0767362, acc 0.96875, learning_rate 0.000159612
2017-09-29T13:36:12.206343: step 1080, loss 0.115016, acc 0.96875, learning_rate 0.000159368

Evaluation:
2017-09-29T13:36:12.778474: step 1080, loss 0.219142, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1080

2017-09-29T13:36:13.514253: step 1081, loss 0.0619541, acc 0.984375, learning_rate 0.000159126
2017-09-29T13:36:13.698298: step 1082, loss 0.0469032, acc 1, learning_rate 0.000158885
2017-09-29T13:36:13.889751: step 1083, loss 0.0619332, acc 0.96875, learning_rate 0.000158644
2017-09-29T13:36:14.077419: step 1084, loss 0.042881, acc 1, learning_rate 0.000158405
2017-09-29T13:36:14.259493: step 1085, loss 0.212333, acc 0.90625, learning_rate 0.000158167
2017-09-29T13:36:14.443374: step 1086, loss 0.0239671, acc 1, learning_rate 0.000157929
2017-09-29T13:36:14.628825: step 1087, loss 0.120782, acc 0.96875, learning_rate 0.000157693
2017-09-29T13:36:14.817068: step 1088, loss 0.0453736, acc 0.984375, learning_rate 0.000157457
2017-09-29T13:36:14.999906: step 1089, loss 0.0163514, acc 1, learning_rate 0.000157223
2017-09-29T13:36:15.184872: step 1090, loss 0.129588, acc 0.96875, learning_rate 0.000156989
2017-09-29T13:36:15.367404: step 1091, loss 0.139016, acc 0.921875, learning_rate 0.000156757
2017-09-29T13:36:15.550913: step 1092, loss 0.0776176, acc 0.984375, learning_rate 0.000156525
2017-09-29T13:36:15.741412: step 1093, loss 0.161862, acc 0.953125, learning_rate 0.000156294
2017-09-29T13:36:15.922677: step 1094, loss 0.0545855, acc 0.96875, learning_rate 0.000156064
2017-09-29T13:36:16.111491: step 1095, loss 0.0302904, acc 1, learning_rate 0.000155836
2017-09-29T13:36:16.299426: step 1096, loss 0.0802829, acc 0.984375, learning_rate 0.000155608
2017-09-29T13:36:16.498947: step 1097, loss 0.103253, acc 0.96875, learning_rate 0.000155381
2017-09-29T13:36:16.682805: step 1098, loss 0.0967698, acc 0.96875, learning_rate 0.000155155
2017-09-29T13:36:16.866175: step 1099, loss 0.0814211, acc 0.984375, learning_rate 0.000154929
2017-09-29T13:36:17.049545: step 1100, loss 0.106425, acc 0.953125, learning_rate 0.000154705
2017-09-29T13:36:17.235344: step 1101, loss 0.101298, acc 0.984375, learning_rate 0.000154482
2017-09-29T13:36:17.420975: step 1102, loss 0.100416, acc 0.953125, learning_rate 0.00015426
2017-09-29T13:36:17.605203: step 1103, loss 0.0626548, acc 0.984375, learning_rate 0.000154038
2017-09-29T13:36:17.786121: step 1104, loss 0.0597506, acc 0.984375, learning_rate 0.000153818
2017-09-29T13:36:17.971330: step 1105, loss 0.132765, acc 0.96875, learning_rate 0.000153598
2017-09-29T13:36:18.156715: step 1106, loss 0.0367008, acc 1, learning_rate 0.000153379
2017-09-29T13:36:18.342481: step 1107, loss 0.061691, acc 0.984375, learning_rate 0.000153161
2017-09-29T13:36:18.535383: step 1108, loss 0.0191832, acc 1, learning_rate 0.000152944
2017-09-29T13:36:18.720435: step 1109, loss 0.0902469, acc 0.953125, learning_rate 0.000152728
2017-09-29T13:36:18.901825: step 1110, loss 0.0926058, acc 0.96875, learning_rate 0.000152513
2017-09-29T13:36:19.085427: step 1111, loss 0.0624069, acc 0.984375, learning_rate 0.000152299
2017-09-29T13:36:19.272232: step 1112, loss 0.0515117, acc 0.984375, learning_rate 0.000152085
2017-09-29T13:36:19.455639: step 1113, loss 0.0456578, acc 1, learning_rate 0.000151872
2017-09-29T13:36:19.638093: step 1114, loss 0.0863961, acc 0.96875, learning_rate 0.000151661
2017-09-29T13:36:19.821749: step 1115, loss 0.0630097, acc 0.984375, learning_rate 0.00015145
2017-09-29T13:36:20.010511: step 1116, loss 0.0327745, acc 1, learning_rate 0.00015124
2017-09-29T13:36:20.195503: step 1117, loss 0.113825, acc 0.984375, learning_rate 0.000151031
2017-09-29T13:36:20.378997: step 1118, loss 0.0220917, acc 1, learning_rate 0.000150822
2017-09-29T13:36:20.560057: step 1119, loss 0.0453756, acc 1, learning_rate 0.000150615
2017-09-29T13:36:20.743375: step 1120, loss 0.057819, acc 0.96875, learning_rate 0.000150408

Evaluation:
2017-09-29T13:36:21.270364: step 1120, loss 0.216302, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1120

2017-09-29T13:36:22.079868: step 1121, loss 0.0290782, acc 1, learning_rate 0.000150203
2017-09-29T13:36:22.276644: step 1122, loss 0.146265, acc 0.953125, learning_rate 0.000149998
2017-09-29T13:36:22.464648: step 1123, loss 0.0324214, acc 0.984375, learning_rate 0.000149794
2017-09-29T13:36:22.647724: step 1124, loss 0.0502642, acc 1, learning_rate 0.00014959
2017-09-29T13:36:22.847948: step 1125, loss 0.0704602, acc 0.984375, learning_rate 0.000149388
2017-09-29T13:36:23.040684: step 1126, loss 0.124525, acc 0.96875, learning_rate 0.000149186
2017-09-29T13:36:23.224911: step 1127, loss 0.0283679, acc 1, learning_rate 0.000148986
2017-09-29T13:36:23.413309: step 1128, loss 0.12232, acc 0.9375, learning_rate 0.000148786
2017-09-29T13:36:23.599028: step 1129, loss 0.10142, acc 0.96875, learning_rate 0.000148587
2017-09-29T13:36:23.792675: step 1130, loss 0.069074, acc 0.96875, learning_rate 0.000148388
2017-09-29T13:36:23.979486: step 1131, loss 0.0841203, acc 0.96875, learning_rate 0.000148191
2017-09-29T13:36:24.165838: step 1132, loss 0.111026, acc 0.96875, learning_rate 0.000147994
2017-09-29T13:36:24.350259: step 1133, loss 0.0615854, acc 0.984375, learning_rate 0.000147798
2017-09-29T13:36:24.547958: step 1134, loss 0.0423499, acc 0.984375, learning_rate 0.000147603
2017-09-29T13:36:24.730896: step 1135, loss 0.111916, acc 0.96875, learning_rate 0.000147409
2017-09-29T13:36:24.919681: step 1136, loss 0.0333225, acc 0.984375, learning_rate 0.000147215
2017-09-29T13:36:25.102359: step 1137, loss 0.137143, acc 0.9375, learning_rate 0.000147022
2017-09-29T13:36:25.308030: step 1138, loss 0.0508305, acc 0.984375, learning_rate 0.000146831
2017-09-29T13:36:25.505133: step 1139, loss 0.0568441, acc 0.984375, learning_rate 0.000146639
2017-09-29T13:36:25.694307: step 1140, loss 0.0883755, acc 0.96875, learning_rate 0.000146449
2017-09-29T13:36:25.878549: step 1141, loss 0.0529914, acc 1, learning_rate 0.000146259
2017-09-29T13:36:26.093917: step 1142, loss 0.04589, acc 1, learning_rate 0.000146071
2017-09-29T13:36:26.286457: step 1143, loss 0.0819705, acc 0.984375, learning_rate 0.000145883
2017-09-29T13:36:26.476869: step 1144, loss 0.0587232, acc 1, learning_rate 0.000145695
2017-09-29T13:36:26.668257: step 1145, loss 0.052565, acc 0.96875, learning_rate 0.000145509
2017-09-29T13:36:26.859912: step 1146, loss 0.0332289, acc 1, learning_rate 0.000145323
2017-09-29T13:36:27.051633: step 1147, loss 0.0882507, acc 0.984375, learning_rate 0.000145138
2017-09-29T13:36:27.237359: step 1148, loss 0.0648812, acc 0.984375, learning_rate 0.000144954
2017-09-29T13:36:27.437178: step 1149, loss 0.10668, acc 0.953125, learning_rate 0.00014477
2017-09-29T13:36:27.621639: step 1150, loss 0.0989817, acc 0.96875, learning_rate 0.000144588
2017-09-29T13:36:27.805693: step 1151, loss 0.0933342, acc 0.96875, learning_rate 0.000144406
2017-09-29T13:36:27.990600: step 1152, loss 0.0414885, acc 0.984375, learning_rate 0.000144224
2017-09-29T13:36:28.178732: step 1153, loss 0.145079, acc 0.953125, learning_rate 0.000144044
2017-09-29T13:36:28.369602: step 1154, loss 0.0610938, acc 0.984375, learning_rate 0.000143864
2017-09-29T13:36:28.557473: step 1155, loss 0.055988, acc 0.984375, learning_rate 0.000143685
2017-09-29T13:36:28.749840: step 1156, loss 0.0230481, acc 1, learning_rate 0.000143507
2017-09-29T13:36:28.942902: step 1157, loss 0.103363, acc 0.953125, learning_rate 0.000143329
2017-09-29T13:36:29.131495: step 1158, loss 0.0517319, acc 1, learning_rate 0.000143152
2017-09-29T13:36:29.323087: step 1159, loss 0.132754, acc 0.96875, learning_rate 0.000142976
2017-09-29T13:36:29.507630: step 1160, loss 0.0330485, acc 1, learning_rate 0.000142801

Evaluation:
2017-09-29T13:36:30.065579: step 1160, loss 0.218401, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1160

2017-09-29T13:36:30.722102: step 1161, loss 0.118544, acc 0.953125, learning_rate 0.000142626
2017-09-29T13:36:30.915725: step 1162, loss 0.0644274, acc 0.984375, learning_rate 0.000142452
2017-09-29T13:36:31.105016: step 1163, loss 0.108529, acc 0.953125, learning_rate 0.000142279
2017-09-29T13:36:31.304244: step 1164, loss 0.0609232, acc 0.984375, learning_rate 0.000142106
2017-09-29T13:36:31.499249: step 1165, loss 0.067431, acc 0.96875, learning_rate 0.000141934
2017-09-29T13:36:31.690911: step 1166, loss 0.0398241, acc 1, learning_rate 0.000141763
2017-09-29T13:36:31.886203: step 1167, loss 0.140979, acc 0.953125, learning_rate 0.000141593
2017-09-29T13:36:32.102345: step 1168, loss 0.0859415, acc 0.984375, learning_rate 0.000141423
2017-09-29T13:36:32.305561: step 1169, loss 0.0948843, acc 0.96875, learning_rate 0.000141254
2017-09-29T13:36:32.499298: step 1170, loss 0.052225, acc 0.96875, learning_rate 0.000141085
2017-09-29T13:36:32.701757: step 1171, loss 0.070611, acc 0.953125, learning_rate 0.000140918
2017-09-29T13:36:32.904633: step 1172, loss 0.0714517, acc 0.984375, learning_rate 0.000140751
2017-09-29T13:36:33.096075: step 1173, loss 0.0490713, acc 0.984375, learning_rate 0.000140584
2017-09-29T13:36:33.296203: step 1174, loss 0.0876815, acc 0.96875, learning_rate 0.000140419
2017-09-29T13:36:33.485664: step 1175, loss 0.0740895, acc 1, learning_rate 0.000140254
2017-09-29T13:36:33.640694: step 1176, loss 0.0118211, acc 1, learning_rate 0.000140089
2017-09-29T13:36:33.830208: step 1177, loss 0.0733719, acc 0.984375, learning_rate 0.000139926
2017-09-29T13:36:34.014647: step 1178, loss 0.122476, acc 0.9375, learning_rate 0.000139763
2017-09-29T13:36:34.202744: step 1179, loss 0.0249542, acc 1, learning_rate 0.0001396
2017-09-29T13:36:34.390579: step 1180, loss 0.109136, acc 0.953125, learning_rate 0.000139439
2017-09-29T13:36:34.583363: step 1181, loss 0.0975381, acc 0.953125, learning_rate 0.000139278
2017-09-29T13:36:34.776024: step 1182, loss 0.0746702, acc 0.96875, learning_rate 0.000139118
2017-09-29T13:36:34.968082: step 1183, loss 0.0925679, acc 0.984375, learning_rate 0.000138958
2017-09-29T13:36:35.166203: step 1184, loss 0.118891, acc 0.9375, learning_rate 0.000138799
2017-09-29T13:36:35.350449: step 1185, loss 0.0904872, acc 0.96875, learning_rate 0.00013864
2017-09-29T13:36:35.549849: step 1186, loss 0.0360738, acc 1, learning_rate 0.000138483
2017-09-29T13:36:35.731818: step 1187, loss 0.0444282, acc 0.984375, learning_rate 0.000138326
2017-09-29T13:36:35.919540: step 1188, loss 0.0386628, acc 1, learning_rate 0.000138169
2017-09-29T13:36:36.106053: step 1189, loss 0.0968733, acc 0.953125, learning_rate 0.000138013
2017-09-29T13:36:36.295752: step 1190, loss 0.0955746, acc 0.96875, learning_rate 0.000137858
2017-09-29T13:36:36.484508: step 1191, loss 0.0655061, acc 0.984375, learning_rate 0.000137704
2017-09-29T13:36:36.670369: step 1192, loss 0.0849405, acc 0.953125, learning_rate 0.00013755
2017-09-29T13:36:36.858686: step 1193, loss 0.102534, acc 0.953125, learning_rate 0.000137397
2017-09-29T13:36:37.050850: step 1194, loss 0.101488, acc 0.9375, learning_rate 0.000137244
2017-09-29T13:36:37.238650: step 1195, loss 0.113272, acc 0.96875, learning_rate 0.000137092
2017-09-29T13:36:37.435005: step 1196, loss 0.109623, acc 0.953125, learning_rate 0.000136941
2017-09-29T13:36:37.624272: step 1197, loss 0.0601987, acc 0.96875, learning_rate 0.00013679
2017-09-29T13:36:37.829218: step 1198, loss 0.0276702, acc 1, learning_rate 0.00013664
2017-09-29T13:36:38.028468: step 1199, loss 0.104255, acc 0.984375, learning_rate 0.00013649
2017-09-29T13:36:38.228562: step 1200, loss 0.0220882, acc 1, learning_rate 0.000136341

Evaluation:
2017-09-29T13:36:38.775148: step 1200, loss 0.217371, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1200

2017-09-29T13:36:39.517188: step 1201, loss 0.0874275, acc 0.953125, learning_rate 0.000136193
2017-09-29T13:36:39.748516: step 1202, loss 0.0454753, acc 0.984375, learning_rate 0.000136045
2017-09-29T13:36:39.992011: step 1203, loss 0.0420873, acc 1, learning_rate 0.000135898
2017-09-29T13:36:40.221453: step 1204, loss 0.0559478, acc 0.984375, learning_rate 0.000135751
2017-09-29T13:36:40.412152: step 1205, loss 0.0276423, acc 1, learning_rate 0.000135605
2017-09-29T13:36:40.609227: step 1206, loss 0.0876996, acc 0.96875, learning_rate 0.00013546
2017-09-29T13:36:40.794424: step 1207, loss 0.0929964, acc 0.953125, learning_rate 0.000135315
2017-09-29T13:36:40.995234: step 1208, loss 0.0560112, acc 0.984375, learning_rate 0.000135171
2017-09-29T13:36:41.191202: step 1209, loss 0.031866, acc 1, learning_rate 0.000135028
2017-09-29T13:36:41.389888: step 1210, loss 0.124414, acc 0.96875, learning_rate 0.000134885
2017-09-29T13:36:41.640169: step 1211, loss 0.187073, acc 0.921875, learning_rate 0.000134742
2017-09-29T13:36:41.886481: step 1212, loss 0.0418176, acc 0.984375, learning_rate 0.0001346
2017-09-29T13:36:42.085532: step 1213, loss 0.0929351, acc 0.96875, learning_rate 0.000134459
2017-09-29T13:36:42.329177: step 1214, loss 0.0731196, acc 0.984375, learning_rate 0.000134319
2017-09-29T13:36:42.558227: step 1215, loss 0.0820477, acc 0.96875, learning_rate 0.000134178
2017-09-29T13:36:42.741758: step 1216, loss 0.0593798, acc 0.984375, learning_rate 0.000134039
2017-09-29T13:36:42.927999: step 1217, loss 0.0468185, acc 0.984375, learning_rate 0.0001339
2017-09-29T13:36:43.120993: step 1218, loss 0.116579, acc 0.96875, learning_rate 0.000133762
2017-09-29T13:36:43.360403: step 1219, loss 0.15436, acc 0.9375, learning_rate 0.000133624
2017-09-29T13:36:43.592261: step 1220, loss 0.0573833, acc 0.984375, learning_rate 0.000133487
2017-09-29T13:36:43.780743: step 1221, loss 0.0370204, acc 1, learning_rate 0.00013335
2017-09-29T13:36:43.977093: step 1222, loss 0.1812, acc 0.90625, learning_rate 0.000133214
2017-09-29T13:36:44.171694: step 1223, loss 0.0880525, acc 0.984375, learning_rate 0.000133078
2017-09-29T13:36:44.360374: step 1224, loss 0.0392041, acc 1, learning_rate 0.000132943
2017-09-29T13:36:44.546597: step 1225, loss 0.0287598, acc 0.984375, learning_rate 0.000132809
2017-09-29T13:36:44.732577: step 1226, loss 0.0814554, acc 0.984375, learning_rate 0.000132675
2017-09-29T13:36:44.929493: step 1227, loss 0.0313785, acc 1, learning_rate 0.000132541
2017-09-29T13:36:45.112755: step 1228, loss 0.119732, acc 0.921875, learning_rate 0.000132409
2017-09-29T13:36:45.300845: step 1229, loss 0.0725427, acc 0.96875, learning_rate 0.000132276
2017-09-29T13:36:45.483891: step 1230, loss 0.0968239, acc 0.953125, learning_rate 0.000132145
2017-09-29T13:36:45.668432: step 1231, loss 0.0347761, acc 0.984375, learning_rate 0.000132013
2017-09-29T13:36:45.870395: step 1232, loss 0.0625313, acc 0.96875, learning_rate 0.000131883
2017-09-29T13:36:46.084599: step 1233, loss 0.0830022, acc 0.96875, learning_rate 0.000131753
2017-09-29T13:36:46.300548: step 1234, loss 0.0749318, acc 1, learning_rate 0.000131623
2017-09-29T13:36:46.523359: step 1235, loss 0.0399153, acc 0.984375, learning_rate 0.000131494
2017-09-29T13:36:46.720908: step 1236, loss 0.0693813, acc 0.984375, learning_rate 0.000131365
2017-09-29T13:36:46.915018: step 1237, loss 0.0431188, acc 1, learning_rate 0.000131237
2017-09-29T13:36:47.105383: step 1238, loss 0.105454, acc 0.984375, learning_rate 0.00013111
2017-09-29T13:36:47.301100: step 1239, loss 0.0252348, acc 1, learning_rate 0.000130983
2017-09-29T13:36:47.514074: step 1240, loss 0.100397, acc 0.953125, learning_rate 0.000130856

Evaluation:
2017-09-29T13:36:48.094654: step 1240, loss 0.215753, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1240

2017-09-29T13:36:48.892403: step 1241, loss 0.110529, acc 0.96875, learning_rate 0.00013073
2017-09-29T13:36:49.075201: step 1242, loss 0.0932377, acc 0.96875, learning_rate 0.000130605
2017-09-29T13:36:49.261357: step 1243, loss 0.119485, acc 0.953125, learning_rate 0.00013048
2017-09-29T13:36:49.450450: step 1244, loss 0.0905829, acc 0.984375, learning_rate 0.000130356
2017-09-29T13:36:49.636250: step 1245, loss 0.0740572, acc 0.984375, learning_rate 0.000130232
2017-09-29T13:36:49.820623: step 1246, loss 0.0868436, acc 0.984375, learning_rate 0.000130108
2017-09-29T13:36:50.017457: step 1247, loss 0.0645219, acc 1, learning_rate 0.000129985
2017-09-29T13:36:50.228142: step 1248, loss 0.143913, acc 0.96875, learning_rate 0.000129863
2017-09-29T13:36:50.437295: step 1249, loss 0.0378332, acc 1, learning_rate 0.000129741
2017-09-29T13:36:50.639171: step 1250, loss 0.046024, acc 1, learning_rate 0.00012962
2017-09-29T13:36:50.834737: step 1251, loss 0.146404, acc 0.9375, learning_rate 0.000129499
2017-09-29T13:36:51.021542: step 1252, loss 0.0427229, acc 1, learning_rate 0.000129378
2017-09-29T13:36:51.206301: step 1253, loss 0.0945545, acc 0.96875, learning_rate 0.000129259
2017-09-29T13:36:51.398092: step 1254, loss 0.0700591, acc 0.96875, learning_rate 0.000129139
2017-09-29T13:36:51.590144: step 1255, loss 0.0486997, acc 1, learning_rate 0.00012902
2017-09-29T13:36:51.787698: step 1256, loss 0.103892, acc 0.953125, learning_rate 0.000128902
2017-09-29T13:36:51.984556: step 1257, loss 0.0427997, acc 1, learning_rate 0.000128784
2017-09-29T13:36:52.175525: step 1258, loss 0.0866322, acc 0.96875, learning_rate 0.000128666
2017-09-29T13:36:52.359023: step 1259, loss 0.129699, acc 0.953125, learning_rate 0.000128549
2017-09-29T13:36:52.575252: step 1260, loss 0.0427255, acc 1, learning_rate 0.000128433
2017-09-29T13:36:52.765453: step 1261, loss 0.169984, acc 0.921875, learning_rate 0.000128317
2017-09-29T13:36:52.951459: step 1262, loss 0.0899566, acc 0.984375, learning_rate 0.000128201
2017-09-29T13:36:53.143634: step 1263, loss 0.0554457, acc 0.984375, learning_rate 0.000128086
2017-09-29T13:36:53.349237: step 1264, loss 0.0649047, acc 0.984375, learning_rate 0.000127971
2017-09-29T13:36:53.601237: step 1265, loss 0.042122, acc 0.984375, learning_rate 0.000127857
2017-09-29T13:36:53.794776: step 1266, loss 0.0845033, acc 0.953125, learning_rate 0.000127743
2017-09-29T13:36:53.981582: step 1267, loss 0.0332351, acc 1, learning_rate 0.00012763
2017-09-29T13:36:54.167521: step 1268, loss 0.0377282, acc 0.984375, learning_rate 0.000127517
2017-09-29T13:36:54.355244: step 1269, loss 0.0745104, acc 0.984375, learning_rate 0.000127405
2017-09-29T13:36:54.548514: step 1270, loss 0.074121, acc 0.96875, learning_rate 0.000127293
2017-09-29T13:36:54.735485: step 1271, loss 0.0427555, acc 0.984375, learning_rate 0.000127182
2017-09-29T13:36:54.923136: step 1272, loss 0.0855189, acc 0.96875, learning_rate 0.000127071
2017-09-29T13:36:55.108037: step 1273, loss 0.0428882, acc 0.984375, learning_rate 0.00012696
2017-09-29T13:36:55.264655: step 1274, loss 0.096113, acc 0.980392, learning_rate 0.00012685
2017-09-29T13:36:55.458426: step 1275, loss 0.102809, acc 0.96875, learning_rate 0.000126741
2017-09-29T13:36:55.640994: step 1276, loss 0.0543474, acc 1, learning_rate 0.000126632
2017-09-29T13:36:55.846537: step 1277, loss 0.0339685, acc 1, learning_rate 0.000126523
2017-09-29T13:36:56.042275: step 1278, loss 0.129459, acc 0.96875, learning_rate 0.000126415
2017-09-29T13:36:56.232922: step 1279, loss 0.0942422, acc 0.96875, learning_rate 0.000126307
2017-09-29T13:36:56.428947: step 1280, loss 0.102793, acc 0.984375, learning_rate 0.000126199

Evaluation:
2017-09-29T13:36:56.978345: step 1280, loss 0.216868, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1280

2017-09-29T13:36:57.622064: step 1281, loss 0.0837128, acc 0.984375, learning_rate 0.000126093
2017-09-29T13:36:57.807962: step 1282, loss 0.0736508, acc 0.96875, learning_rate 0.000125986
2017-09-29T13:36:57.992819: step 1283, loss 0.074858, acc 0.984375, learning_rate 0.00012588
2017-09-29T13:36:58.186262: step 1284, loss 0.0911302, acc 0.96875, learning_rate 0.000125774
2017-09-29T13:36:58.373769: step 1285, loss 0.065554, acc 0.96875, learning_rate 0.000125669
2017-09-29T13:36:58.570730: step 1286, loss 0.0784591, acc 0.984375, learning_rate 0.000125564
2017-09-29T13:36:58.753109: step 1287, loss 0.0756343, acc 0.96875, learning_rate 0.00012546
2017-09-29T13:36:58.938834: step 1288, loss 0.105036, acc 0.96875, learning_rate 0.000125356
2017-09-29T13:36:59.125430: step 1289, loss 0.0631516, acc 0.96875, learning_rate 0.000125253
2017-09-29T13:36:59.314707: step 1290, loss 0.061958, acc 0.984375, learning_rate 0.00012515
2017-09-29T13:36:59.499427: step 1291, loss 0.157011, acc 0.953125, learning_rate 0.000125047
2017-09-29T13:36:59.683890: step 1292, loss 0.150509, acc 0.96875, learning_rate 0.000124945
2017-09-29T13:36:59.866900: step 1293, loss 0.0774485, acc 0.96875, learning_rate 0.000124843
2017-09-29T13:37:00.049192: step 1294, loss 0.0270507, acc 1, learning_rate 0.000124741
2017-09-29T13:37:00.234621: step 1295, loss 0.0675849, acc 0.984375, learning_rate 0.00012464
2017-09-29T13:37:00.419506: step 1296, loss 0.0812832, acc 0.96875, learning_rate 0.00012454
2017-09-29T13:37:00.603864: step 1297, loss 0.0483626, acc 0.96875, learning_rate 0.00012444
2017-09-29T13:37:00.791688: step 1298, loss 0.0602306, acc 1, learning_rate 0.00012434
2017-09-29T13:37:00.975967: step 1299, loss 0.0271393, acc 1, learning_rate 0.000124241
2017-09-29T13:37:01.163450: step 1300, loss 0.0709786, acc 0.96875, learning_rate 0.000124142
2017-09-29T13:37:01.344656: step 1301, loss 0.0686325, acc 0.984375, learning_rate 0.000124043
2017-09-29T13:37:01.534625: step 1302, loss 0.0888188, acc 0.984375, learning_rate 0.000123945
2017-09-29T13:37:01.722288: step 1303, loss 0.126603, acc 0.953125, learning_rate 0.000123847
2017-09-29T13:37:01.909591: step 1304, loss 0.0670875, acc 1, learning_rate 0.00012375
2017-09-29T13:37:02.095699: step 1305, loss 0.187449, acc 0.9375, learning_rate 0.000123653
2017-09-29T13:37:02.281580: step 1306, loss 0.0820005, acc 0.984375, learning_rate 0.000123556
2017-09-29T13:37:02.480028: step 1307, loss 0.0465588, acc 0.984375, learning_rate 0.00012346
2017-09-29T13:37:02.673239: step 1308, loss 0.0464737, acc 0.984375, learning_rate 0.000123364
2017-09-29T13:37:02.863103: step 1309, loss 0.0645084, acc 1, learning_rate 0.000123269
2017-09-29T13:37:03.050410: step 1310, loss 0.130712, acc 0.96875, learning_rate 0.000123174
2017-09-29T13:37:03.246111: step 1311, loss 0.0578717, acc 0.984375, learning_rate 0.00012308
2017-09-29T13:37:03.444076: step 1312, loss 0.0404425, acc 0.984375, learning_rate 0.000122985
2017-09-29T13:37:03.638841: step 1313, loss 0.0206321, acc 1, learning_rate 0.000122892
2017-09-29T13:37:03.830436: step 1314, loss 0.0719984, acc 0.96875, learning_rate 0.000122798
2017-09-29T13:37:04.014945: step 1315, loss 0.00847416, acc 1, learning_rate 0.000122705
2017-09-29T13:37:04.199769: step 1316, loss 0.0525012, acc 0.984375, learning_rate 0.000122612
2017-09-29T13:37:04.385458: step 1317, loss 0.0763953, acc 0.984375, learning_rate 0.00012252
2017-09-29T13:37:04.585143: step 1318, loss 0.0235894, acc 1, learning_rate 0.000122428
2017-09-29T13:37:04.769300: step 1319, loss 0.0534393, acc 0.984375, learning_rate 0.000122337
2017-09-29T13:37:04.962468: step 1320, loss 0.0439329, acc 0.984375, learning_rate 0.000122245

Evaluation:
2017-09-29T13:37:05.509917: step 1320, loss 0.215309, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1320

2017-09-29T13:37:06.235269: step 1321, loss 0.112141, acc 0.953125, learning_rate 0.000122155
2017-09-29T13:37:06.424109: step 1322, loss 0.170712, acc 0.9375, learning_rate 0.000122064
2017-09-29T13:37:06.613376: step 1323, loss 0.0648338, acc 0.984375, learning_rate 0.000121974
2017-09-29T13:37:06.797738: step 1324, loss 0.0575543, acc 0.984375, learning_rate 0.000121884
2017-09-29T13:37:06.981450: step 1325, loss 0.103558, acc 0.953125, learning_rate 0.000121795
2017-09-29T13:37:07.171747: step 1326, loss 0.0457527, acc 0.984375, learning_rate 0.000121706
2017-09-29T13:37:07.362008: step 1327, loss 0.13502, acc 0.953125, learning_rate 0.000121618
2017-09-29T13:37:07.549736: step 1328, loss 0.11149, acc 0.96875, learning_rate 0.000121529
2017-09-29T13:37:07.746736: step 1329, loss 0.103273, acc 0.953125, learning_rate 0.000121441
2017-09-29T13:37:07.939662: step 1330, loss 0.0750153, acc 0.96875, learning_rate 0.000121354
2017-09-29T13:37:08.130443: step 1331, loss 0.0974697, acc 0.96875, learning_rate 0.000121267
2017-09-29T13:37:08.332042: step 1332, loss 0.0604914, acc 0.96875, learning_rate 0.00012118
2017-09-29T13:37:08.517835: step 1333, loss 0.0251803, acc 1, learning_rate 0.000121093
2017-09-29T13:37:08.711199: step 1334, loss 0.0112274, acc 1, learning_rate 0.000121007
2017-09-29T13:37:08.905795: step 1335, loss 0.0508704, acc 0.984375, learning_rate 0.000120922
2017-09-29T13:37:09.091067: step 1336, loss 0.0623545, acc 0.96875, learning_rate 0.000120836
2017-09-29T13:37:09.276336: step 1337, loss 0.069102, acc 0.96875, learning_rate 0.000120751
2017-09-29T13:37:09.475975: step 1338, loss 0.083431, acc 0.984375, learning_rate 0.000120666
2017-09-29T13:37:09.673085: step 1339, loss 0.0763574, acc 0.96875, learning_rate 0.000120582
2017-09-29T13:37:09.874015: step 1340, loss 0.0550455, acc 1, learning_rate 0.000120498
2017-09-29T13:37:10.068343: step 1341, loss 0.0611873, acc 1, learning_rate 0.000120414
2017-09-29T13:37:10.263998: step 1342, loss 0.0351633, acc 0.984375, learning_rate 0.000120331
2017-09-29T13:37:10.460105: step 1343, loss 0.0983415, acc 0.96875, learning_rate 0.000120248
2017-09-29T13:37:10.648567: step 1344, loss 0.0822979, acc 0.96875, learning_rate 0.000120165
2017-09-29T13:37:10.832793: step 1345, loss 0.130716, acc 0.953125, learning_rate 0.000120083
2017-09-29T13:37:11.016544: step 1346, loss 0.0336741, acc 0.984375, learning_rate 0.000120001
2017-09-29T13:37:11.201940: step 1347, loss 0.0288951, acc 1, learning_rate 0.00011992
2017-09-29T13:37:11.403544: step 1348, loss 0.0810141, acc 0.984375, learning_rate 0.000119838
2017-09-29T13:37:11.603709: step 1349, loss 0.12177, acc 0.9375, learning_rate 0.000119757
2017-09-29T13:37:11.788329: step 1350, loss 0.0504573, acc 1, learning_rate 0.000119677
2017-09-29T13:37:11.974568: step 1351, loss 0.0668843, acc 0.984375, learning_rate 0.000119596
2017-09-29T13:37:12.163301: step 1352, loss 0.0685111, acc 0.984375, learning_rate 0.000119516
2017-09-29T13:37:12.352670: step 1353, loss 0.0538272, acc 0.984375, learning_rate 0.000119437
2017-09-29T13:37:12.544628: step 1354, loss 0.0759098, acc 0.953125, learning_rate 0.000119357
2017-09-29T13:37:12.740124: step 1355, loss 0.0780492, acc 0.984375, learning_rate 0.000119278
2017-09-29T13:37:12.930932: step 1356, loss 0.0861624, acc 0.96875, learning_rate 0.0001192
2017-09-29T13:37:13.117765: step 1357, loss 0.0590283, acc 0.984375, learning_rate 0.000119121
2017-09-29T13:37:13.300153: step 1358, loss 0.0487337, acc 0.984375, learning_rate 0.000119043
2017-09-29T13:37:13.485367: step 1359, loss 0.130414, acc 0.96875, learning_rate 0.000118965
2017-09-29T13:37:13.670065: step 1360, loss 0.127389, acc 0.96875, learning_rate 0.000118888

Evaluation:
2017-09-29T13:37:14.175041: step 1360, loss 0.214465, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1360

2017-09-29T13:37:14.881487: step 1361, loss 0.0360509, acc 1, learning_rate 0.000118811
2017-09-29T13:37:15.065053: step 1362, loss 0.0949537, acc 0.96875, learning_rate 0.000118734
2017-09-29T13:37:15.248244: step 1363, loss 0.0723715, acc 0.984375, learning_rate 0.000118658
2017-09-29T13:37:15.442023: step 1364, loss 0.0703547, acc 0.984375, learning_rate 0.000118582
2017-09-29T13:37:15.627118: step 1365, loss 0.122553, acc 0.953125, learning_rate 0.000118506
2017-09-29T13:37:15.813696: step 1366, loss 0.0742489, acc 0.984375, learning_rate 0.00011843
2017-09-29T13:37:16.004047: step 1367, loss 0.0552394, acc 0.984375, learning_rate 0.000118355
2017-09-29T13:37:16.195152: step 1368, loss 0.0743321, acc 0.984375, learning_rate 0.00011828
2017-09-29T13:37:16.380943: step 1369, loss 0.159831, acc 0.9375, learning_rate 0.000118205
2017-09-29T13:37:16.595405: step 1370, loss 0.0229544, acc 1, learning_rate 0.000118131
2017-09-29T13:37:16.781537: step 1371, loss 0.0815905, acc 0.984375, learning_rate 0.000118057
2017-09-29T13:37:16.936140: step 1372, loss 0.0806029, acc 0.960784, learning_rate 0.000117983
2017-09-29T13:37:17.121352: step 1373, loss 0.0658084, acc 0.984375, learning_rate 0.00011791
2017-09-29T13:37:17.305513: step 1374, loss 0.0644974, acc 0.984375, learning_rate 0.000117837
2017-09-29T13:37:17.489675: step 1375, loss 0.0185177, acc 1, learning_rate 0.000117764
2017-09-29T13:37:17.683840: step 1376, loss 0.0718051, acc 0.984375, learning_rate 0.000117692
2017-09-29T13:37:17.875889: step 1377, loss 0.0981783, acc 0.96875, learning_rate 0.000117619
2017-09-29T13:37:18.067876: step 1378, loss 0.0712901, acc 0.984375, learning_rate 0.000117547
2017-09-29T13:37:18.255197: step 1379, loss 0.125677, acc 0.9375, learning_rate 0.000117476
2017-09-29T13:37:18.441040: step 1380, loss 0.0790109, acc 0.96875, learning_rate 0.000117404
2017-09-29T13:37:18.628765: step 1381, loss 0.129613, acc 0.9375, learning_rate 0.000117333
2017-09-29T13:37:18.826239: step 1382, loss 0.0643848, acc 0.984375, learning_rate 0.000117263
2017-09-29T13:37:19.014075: step 1383, loss 0.0716529, acc 0.96875, learning_rate 0.000117192
2017-09-29T13:37:19.199882: step 1384, loss 0.0355979, acc 1, learning_rate 0.000117122
2017-09-29T13:37:19.386349: step 1385, loss 0.0874427, acc 0.96875, learning_rate 0.000117052
2017-09-29T13:37:19.571294: step 1386, loss 0.0824124, acc 0.96875, learning_rate 0.000116983
2017-09-29T13:37:19.756378: step 1387, loss 0.0697125, acc 0.96875, learning_rate 0.000116913
2017-09-29T13:37:19.938750: step 1388, loss 0.0637147, acc 0.984375, learning_rate 0.000116844
2017-09-29T13:37:20.124489: step 1389, loss 0.0295039, acc 0.984375, learning_rate 0.000116775
2017-09-29T13:37:20.305755: step 1390, loss 0.0752973, acc 0.984375, learning_rate 0.000116707
2017-09-29T13:37:20.494991: step 1391, loss 0.0687949, acc 0.984375, learning_rate 0.000116639
2017-09-29T13:37:20.684998: step 1392, loss 0.0388593, acc 0.984375, learning_rate 0.000116571
2017-09-29T13:37:20.877269: step 1393, loss 0.0419061, acc 1, learning_rate 0.000116503
2017-09-29T13:37:21.069617: step 1394, loss 0.106901, acc 0.984375, learning_rate 0.000116436
2017-09-29T13:37:21.261492: step 1395, loss 0.132214, acc 0.953125, learning_rate 0.000116369
2017-09-29T13:37:21.452658: step 1396, loss 0.0808288, acc 0.953125, learning_rate 0.000116302
2017-09-29T13:37:21.644599: step 1397, loss 0.0662858, acc 0.96875, learning_rate 0.000116235
2017-09-29T13:37:21.831428: step 1398, loss 0.0782495, acc 0.984375, learning_rate 0.000116169
2017-09-29T13:37:22.014272: step 1399, loss 0.0474941, acc 0.984375, learning_rate 0.000116103
2017-09-29T13:37:22.199769: step 1400, loss 0.0702577, acc 0.96875, learning_rate 0.000116037

Evaluation:
2017-09-29T13:37:22.731766: step 1400, loss 0.213452, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1400

2017-09-29T13:37:23.546491: step 1401, loss 0.0967325, acc 0.984375, learning_rate 0.000115972
2017-09-29T13:37:23.736771: step 1402, loss 0.102497, acc 0.953125, learning_rate 0.000115907
2017-09-29T13:37:23.928165: step 1403, loss 0.0932073, acc 0.96875, learning_rate 0.000115842
2017-09-29T13:37:24.113000: step 1404, loss 0.114766, acc 0.921875, learning_rate 0.000115777
2017-09-29T13:37:24.303018: step 1405, loss 0.0789734, acc 0.96875, learning_rate 0.000115713
2017-09-29T13:37:24.489768: step 1406, loss 0.0486992, acc 1, learning_rate 0.000115649
2017-09-29T13:37:24.679136: step 1407, loss 0.0914692, acc 0.96875, learning_rate 0.000115585
2017-09-29T13:37:24.867736: step 1408, loss 0.0557492, acc 0.984375, learning_rate 0.000115521
2017-09-29T13:37:25.052502: step 1409, loss 0.102882, acc 0.953125, learning_rate 0.000115458
2017-09-29T13:37:25.243386: step 1410, loss 0.0218557, acc 1, learning_rate 0.000115395
2017-09-29T13:37:25.429284: step 1411, loss 0.0402721, acc 1, learning_rate 0.000115332
2017-09-29T13:37:25.612642: step 1412, loss 0.105026, acc 0.953125, learning_rate 0.000115269
2017-09-29T13:37:25.798254: step 1413, loss 0.0972947, acc 0.953125, learning_rate 0.000115207
2017-09-29T13:37:25.993291: step 1414, loss 0.0750702, acc 0.96875, learning_rate 0.000115145
2017-09-29T13:37:26.180514: step 1415, loss 0.117573, acc 0.953125, learning_rate 0.000115083
2017-09-29T13:37:26.370599: step 1416, loss 0.16087, acc 0.921875, learning_rate 0.000115022
2017-09-29T13:37:26.562407: step 1417, loss 0.0526165, acc 0.984375, learning_rate 0.00011496
2017-09-29T13:37:26.751719: step 1418, loss 0.189423, acc 0.9375, learning_rate 0.000114899
2017-09-29T13:37:26.936801: step 1419, loss 0.0417751, acc 1, learning_rate 0.000114838
2017-09-29T13:37:27.127530: step 1420, loss 0.0398764, acc 0.984375, learning_rate 0.000114778
2017-09-29T13:37:27.312942: step 1421, loss 0.0219667, acc 1, learning_rate 0.000114717
2017-09-29T13:37:27.496571: step 1422, loss 0.0392251, acc 1, learning_rate 0.000114657
2017-09-29T13:37:27.682689: step 1423, loss 0.117026, acc 0.953125, learning_rate 0.000114598
2017-09-29T13:37:27.872637: step 1424, loss 0.0491658, acc 0.96875, learning_rate 0.000114538
2017-09-29T13:37:28.059404: step 1425, loss 0.0379036, acc 1, learning_rate 0.000114479
2017-09-29T13:37:28.248638: step 1426, loss 0.0192334, acc 1, learning_rate 0.00011442
2017-09-29T13:37:28.447969: step 1427, loss 0.0698096, acc 0.984375, learning_rate 0.000114361
2017-09-29T13:37:28.638157: step 1428, loss 0.157608, acc 0.953125, learning_rate 0.000114302
2017-09-29T13:37:28.829503: step 1429, loss 0.0332333, acc 1, learning_rate 0.000114244
2017-09-29T13:37:29.017218: step 1430, loss 0.138119, acc 0.96875, learning_rate 0.000114186
2017-09-29T13:37:29.212146: step 1431, loss 0.0554051, acc 0.984375, learning_rate 0.000114128
2017-09-29T13:37:29.404100: step 1432, loss 0.0613051, acc 0.96875, learning_rate 0.00011407
2017-09-29T13:37:29.608441: step 1433, loss 0.105171, acc 0.953125, learning_rate 0.000114013
2017-09-29T13:37:29.803726: step 1434, loss 0.0696701, acc 0.96875, learning_rate 0.000113955
2017-09-29T13:37:29.996835: step 1435, loss 0.0594123, acc 1, learning_rate 0.000113898
2017-09-29T13:37:30.185828: step 1436, loss 0.0597054, acc 1, learning_rate 0.000113842
2017-09-29T13:37:30.374017: step 1437, loss 0.0976035, acc 0.953125, learning_rate 0.000113785
2017-09-29T13:37:30.561744: step 1438, loss 0.118774, acc 0.953125, learning_rate 0.000113729
2017-09-29T13:37:30.745845: step 1439, loss 0.0629888, acc 1, learning_rate 0.000113673
2017-09-29T13:37:30.930221: step 1440, loss 0.0774762, acc 0.96875, learning_rate 0.000113617

Evaluation:
2017-09-29T13:37:31.445682: step 1440, loss 0.215624, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1440

2017-09-29T13:37:32.084458: step 1441, loss 0.0335181, acc 1, learning_rate 0.000113561
2017-09-29T13:37:32.286684: step 1442, loss 0.0891666, acc 0.96875, learning_rate 0.000113506
2017-09-29T13:37:32.475152: step 1443, loss 0.124353, acc 0.953125, learning_rate 0.000113451
2017-09-29T13:37:32.656794: step 1444, loss 0.0376073, acc 0.984375, learning_rate 0.000113396
2017-09-29T13:37:32.853916: step 1445, loss 0.113472, acc 0.953125, learning_rate 0.000113341
2017-09-29T13:37:33.056381: step 1446, loss 0.0340382, acc 0.984375, learning_rate 0.000113287
2017-09-29T13:37:33.247909: step 1447, loss 0.13355, acc 0.96875, learning_rate 0.000113233
2017-09-29T13:37:33.435336: step 1448, loss 0.070739, acc 0.96875, learning_rate 0.000113179
2017-09-29T13:37:33.621850: step 1449, loss 0.0662847, acc 0.984375, learning_rate 0.000113125
2017-09-29T13:37:33.807550: step 1450, loss 0.0283616, acc 1, learning_rate 0.000113071
2017-09-29T13:37:33.990486: step 1451, loss 0.0976831, acc 0.96875, learning_rate 0.000113018
2017-09-29T13:37:34.187815: step 1452, loss 0.105883, acc 0.96875, learning_rate 0.000112965
2017-09-29T13:37:34.375269: step 1453, loss 0.0325181, acc 1, learning_rate 0.000112912
2017-09-29T13:37:34.588536: step 1454, loss 0.0313814, acc 1, learning_rate 0.000112859
2017-09-29T13:37:34.776792: step 1455, loss 0.0721183, acc 0.96875, learning_rate 0.000112807
2017-09-29T13:37:34.964515: step 1456, loss 0.175692, acc 0.9375, learning_rate 0.000112754
2017-09-29T13:37:35.147425: step 1457, loss 0.0318905, acc 1, learning_rate 0.000112702
2017-09-29T13:37:35.332231: step 1458, loss 0.032854, acc 0.984375, learning_rate 0.000112651
2017-09-29T13:37:35.517098: step 1459, loss 0.0543302, acc 0.984375, learning_rate 0.000112599
2017-09-29T13:37:35.699578: step 1460, loss 0.0773269, acc 0.953125, learning_rate 0.000112547
2017-09-29T13:37:35.882101: step 1461, loss 0.0648918, acc 0.984375, learning_rate 0.000112496
2017-09-29T13:37:36.065373: step 1462, loss 0.0896643, acc 0.953125, learning_rate 0.000112445
2017-09-29T13:37:36.250104: step 1463, loss 0.0188565, acc 1, learning_rate 0.000112394
2017-09-29T13:37:36.438887: step 1464, loss 0.0395109, acc 1, learning_rate 0.000112344
2017-09-29T13:37:36.624682: step 1465, loss 0.0597215, acc 0.984375, learning_rate 0.000112293
2017-09-29T13:37:36.813381: step 1466, loss 0.102653, acc 0.953125, learning_rate 0.000112243
2017-09-29T13:37:37.005327: step 1467, loss 0.105366, acc 0.953125, learning_rate 0.000112193
2017-09-29T13:37:37.191694: step 1468, loss 0.108136, acc 0.953125, learning_rate 0.000112144
2017-09-29T13:37:37.379261: step 1469, loss 0.125473, acc 0.953125, learning_rate 0.000112094
2017-09-29T13:37:37.539489: step 1470, loss 0.0322701, acc 1, learning_rate 0.000112045
2017-09-29T13:37:37.724298: step 1471, loss 0.0294372, acc 1, learning_rate 0.000111995
2017-09-29T13:37:37.919915: step 1472, loss 0.0466799, acc 1, learning_rate 0.000111946
2017-09-29T13:37:38.108671: step 1473, loss 0.0845272, acc 0.96875, learning_rate 0.000111898
2017-09-29T13:37:38.295413: step 1474, loss 0.128051, acc 0.953125, learning_rate 0.000111849
2017-09-29T13:37:38.480196: step 1475, loss 0.0817843, acc 0.984375, learning_rate 0.000111801
2017-09-29T13:37:38.663246: step 1476, loss 0.0849937, acc 0.984375, learning_rate 0.000111753
2017-09-29T13:37:38.860660: step 1477, loss 0.0432505, acc 0.984375, learning_rate 0.000111705
2017-09-29T13:37:39.047872: step 1478, loss 0.0596997, acc 0.984375, learning_rate 0.000111657
2017-09-29T13:37:39.232838: step 1479, loss 0.0927725, acc 0.96875, learning_rate 0.000111609
2017-09-29T13:37:39.418040: step 1480, loss 0.0461322, acc 1, learning_rate 0.000111562

Evaluation:
2017-09-29T13:37:39.929316: step 1480, loss 0.215858, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1480

2017-09-29T13:37:40.663458: step 1481, loss 0.0811367, acc 0.984375, learning_rate 0.000111515
2017-09-29T13:37:40.850590: step 1482, loss 0.0550389, acc 0.96875, learning_rate 0.000111468
2017-09-29T13:37:41.038909: step 1483, loss 0.122, acc 0.96875, learning_rate 0.000111421
2017-09-29T13:37:41.228410: step 1484, loss 0.0805487, acc 0.953125, learning_rate 0.000111374
2017-09-29T13:37:41.419057: step 1485, loss 0.0658263, acc 0.96875, learning_rate 0.000111328
2017-09-29T13:37:41.607733: step 1486, loss 0.091436, acc 0.984375, learning_rate 0.000111282
2017-09-29T13:37:41.794795: step 1487, loss 0.0407673, acc 1, learning_rate 0.000111236
2017-09-29T13:37:41.977511: step 1488, loss 0.0806981, acc 0.984375, learning_rate 0.00011119
2017-09-29T13:37:42.167751: step 1489, loss 0.0569674, acc 0.984375, learning_rate 0.000111144
2017-09-29T13:37:42.358482: step 1490, loss 0.0730777, acc 0.96875, learning_rate 0.000111099
2017-09-29T13:37:42.550630: step 1491, loss 0.0662004, acc 0.984375, learning_rate 0.000111053
2017-09-29T13:37:42.737113: step 1492, loss 0.0955827, acc 0.96875, learning_rate 0.000111008
2017-09-29T13:37:42.928875: step 1493, loss 0.150783, acc 0.953125, learning_rate 0.000110963
2017-09-29T13:37:43.112844: step 1494, loss 0.0940597, acc 0.96875, learning_rate 0.000110918
2017-09-29T13:37:43.301515: step 1495, loss 0.0256297, acc 1, learning_rate 0.000110874
2017-09-29T13:37:43.488905: step 1496, loss 0.0329771, acc 1, learning_rate 0.00011083
2017-09-29T13:37:43.676241: step 1497, loss 0.0490076, acc 0.984375, learning_rate 0.000110785
2017-09-29T13:37:43.862765: step 1498, loss 0.108986, acc 0.96875, learning_rate 0.000110741
2017-09-29T13:37:44.049528: step 1499, loss 0.0429758, acc 1, learning_rate 0.000110697
2017-09-29T13:37:44.240111: step 1500, loss 0.0708975, acc 0.984375, learning_rate 0.000110654
2017-09-29T13:37:44.431207: step 1501, loss 0.0845467, acc 0.96875, learning_rate 0.00011061
2017-09-29T13:37:44.623012: step 1502, loss 0.0944032, acc 0.984375, learning_rate 0.000110567
2017-09-29T13:37:44.813182: step 1503, loss 0.0780631, acc 0.96875, learning_rate 0.000110524
2017-09-29T13:37:44.998974: step 1504, loss 0.0803179, acc 0.953125, learning_rate 0.000110481
2017-09-29T13:37:45.184069: step 1505, loss 0.0379486, acc 1, learning_rate 0.000110438
2017-09-29T13:37:45.370968: step 1506, loss 0.0790042, acc 0.96875, learning_rate 0.000110396
2017-09-29T13:37:45.573518: step 1507, loss 0.10456, acc 0.984375, learning_rate 0.000110353
2017-09-29T13:37:45.760652: step 1508, loss 0.0406162, acc 0.984375, learning_rate 0.000110311
2017-09-29T13:37:45.953070: step 1509, loss 0.0602476, acc 0.984375, learning_rate 0.000110269
2017-09-29T13:37:46.136948: step 1510, loss 0.126886, acc 0.953125, learning_rate 0.000110227
2017-09-29T13:37:46.323541: step 1511, loss 0.0268257, acc 1, learning_rate 0.000110185
2017-09-29T13:37:46.511258: step 1512, loss 0.0663045, acc 0.984375, learning_rate 0.000110144
2017-09-29T13:37:46.704213: step 1513, loss 0.110498, acc 0.953125, learning_rate 0.000110102
2017-09-29T13:37:46.892407: step 1514, loss 0.0686588, acc 0.96875, learning_rate 0.000110061
2017-09-29T13:37:47.082092: step 1515, loss 0.0971566, acc 0.953125, learning_rate 0.00011002
2017-09-29T13:37:47.268823: step 1516, loss 0.0336733, acc 0.984375, learning_rate 0.000109979
2017-09-29T13:37:47.474644: step 1517, loss 0.0558087, acc 1, learning_rate 0.000109938
2017-09-29T13:37:47.658831: step 1518, loss 0.0436692, acc 1, learning_rate 0.000109898
2017-09-29T13:37:47.842480: step 1519, loss 0.0475758, acc 0.984375, learning_rate 0.000109857
2017-09-29T13:37:48.035231: step 1520, loss 0.0584982, acc 0.984375, learning_rate 0.000109817

Evaluation:
2017-09-29T13:37:48.535543: step 1520, loss 0.217811, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1520

2017-09-29T13:37:49.334865: step 1521, loss 0.0414322, acc 1, learning_rate 0.000109777
2017-09-29T13:37:49.523996: step 1522, loss 0.0684189, acc 0.96875, learning_rate 0.000109737
2017-09-29T13:37:49.713675: step 1523, loss 0.0822211, acc 0.96875, learning_rate 0.000109697
2017-09-29T13:37:49.906405: step 1524, loss 0.0651106, acc 0.984375, learning_rate 0.000109658
2017-09-29T13:37:50.102299: step 1525, loss 0.0592388, acc 0.96875, learning_rate 0.000109618
2017-09-29T13:37:50.289530: step 1526, loss 0.0989781, acc 0.984375, learning_rate 0.000109579
2017-09-29T13:37:50.485173: step 1527, loss 0.0505172, acc 0.984375, learning_rate 0.00010954
2017-09-29T13:37:50.673110: step 1528, loss 0.0473629, acc 0.96875, learning_rate 0.000109501
2017-09-29T13:37:50.855898: step 1529, loss 0.0702142, acc 0.984375, learning_rate 0.000109462
2017-09-29T13:37:51.041762: step 1530, loss 0.114254, acc 0.96875, learning_rate 0.000109424
2017-09-29T13:37:51.225108: step 1531, loss 0.087141, acc 0.96875, learning_rate 0.000109385
2017-09-29T13:37:51.422839: step 1532, loss 0.0726821, acc 0.984375, learning_rate 0.000109347
2017-09-29T13:37:51.618604: step 1533, loss 0.05068, acc 0.984375, learning_rate 0.000109309
2017-09-29T13:37:51.804297: step 1534, loss 0.0644654, acc 0.96875, learning_rate 0.000109271
2017-09-29T13:37:51.988044: step 1535, loss 0.147287, acc 0.9375, learning_rate 0.000109233
2017-09-29T13:37:52.175953: step 1536, loss 0.0803185, acc 0.984375, learning_rate 0.000109195
2017-09-29T13:37:52.360967: step 1537, loss 0.0881631, acc 0.96875, learning_rate 0.000109158
2017-09-29T13:37:52.555764: step 1538, loss 0.0528514, acc 0.984375, learning_rate 0.00010912
2017-09-29T13:37:52.754289: step 1539, loss 0.0852308, acc 0.953125, learning_rate 0.000109083
2017-09-29T13:37:52.945085: step 1540, loss 0.0501608, acc 0.96875, learning_rate 0.000109046
2017-09-29T13:37:53.140575: step 1541, loss 0.0892149, acc 0.96875, learning_rate 0.000109009
2017-09-29T13:37:53.326092: step 1542, loss 0.13861, acc 0.9375, learning_rate 0.000108972
2017-09-29T13:37:53.514830: step 1543, loss 0.0801817, acc 0.96875, learning_rate 0.000108936
2017-09-29T13:37:53.702737: step 1544, loss 0.0873505, acc 0.96875, learning_rate 0.000108899
2017-09-29T13:37:53.886952: step 1545, loss 0.0965846, acc 0.96875, learning_rate 0.000108863
2017-09-29T13:37:54.072549: step 1546, loss 0.0507433, acc 0.984375, learning_rate 0.000108827
2017-09-29T13:37:54.262631: step 1547, loss 0.0387406, acc 1, learning_rate 0.000108791
2017-09-29T13:37:54.446745: step 1548, loss 0.0981102, acc 0.96875, learning_rate 0.000108755
2017-09-29T13:37:54.641427: step 1549, loss 0.0445186, acc 0.984375, learning_rate 0.000108719
2017-09-29T13:37:54.832687: step 1550, loss 0.0573897, acc 0.96875, learning_rate 0.000108683
2017-09-29T13:37:55.026754: step 1551, loss 0.0384284, acc 1, learning_rate 0.000108648
2017-09-29T13:37:55.223442: step 1552, loss 0.0674084, acc 0.984375, learning_rate 0.000108613
2017-09-29T13:37:55.414568: step 1553, loss 0.0491547, acc 0.984375, learning_rate 0.000108577
2017-09-29T13:37:55.601077: step 1554, loss 0.0476576, acc 1, learning_rate 0.000108542
2017-09-29T13:37:55.786229: step 1555, loss 0.083758, acc 0.984375, learning_rate 0.000108508
2017-09-29T13:37:55.972532: step 1556, loss 0.0371819, acc 1, learning_rate 0.000108473
2017-09-29T13:37:56.154811: step 1557, loss 0.0725102, acc 0.984375, learning_rate 0.000108438
2017-09-29T13:37:56.353376: step 1558, loss 0.0855306, acc 0.96875, learning_rate 0.000108404
2017-09-29T13:37:56.545159: step 1559, loss 0.0925442, acc 0.96875, learning_rate 0.00010837
2017-09-29T13:37:56.729505: step 1560, loss 0.0621288, acc 0.984375, learning_rate 0.000108335

Evaluation:
2017-09-29T13:37:57.247157: step 1560, loss 0.216678, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1560

2017-09-29T13:37:57.900373: step 1561, loss 0.0388767, acc 1, learning_rate 0.000108301
2017-09-29T13:37:58.095410: step 1562, loss 0.0653897, acc 0.96875, learning_rate 0.000108267
2017-09-29T13:37:58.280443: step 1563, loss 0.110271, acc 0.96875, learning_rate 0.000108234
2017-09-29T13:37:58.467431: step 1564, loss 0.14922, acc 0.953125, learning_rate 0.0001082
2017-09-29T13:37:58.655560: step 1565, loss 0.0722657, acc 0.96875, learning_rate 0.000108167
2017-09-29T13:37:58.844344: step 1566, loss 0.0882424, acc 0.96875, learning_rate 0.000108133
2017-09-29T13:37:59.029274: step 1567, loss 0.0680425, acc 0.984375, learning_rate 0.0001081
2017-09-29T13:37:59.183857: step 1568, loss 0.0185545, acc 1, learning_rate 0.000108067
2017-09-29T13:37:59.371418: step 1569, loss 0.167083, acc 0.921875, learning_rate 0.000108034
2017-09-29T13:37:59.558765: step 1570, loss 0.058239, acc 0.984375, learning_rate 0.000108001
2017-09-29T13:37:59.744659: step 1571, loss 0.0871244, acc 0.96875, learning_rate 0.000107969
2017-09-29T13:37:59.927151: step 1572, loss 0.072463, acc 0.984375, learning_rate 0.000107936
2017-09-29T13:38:00.126046: step 1573, loss 0.0407929, acc 1, learning_rate 0.000107904
2017-09-29T13:38:00.310392: step 1574, loss 0.0700122, acc 0.984375, learning_rate 0.000107871
2017-09-29T13:38:00.499887: step 1575, loss 0.0776529, acc 0.96875, learning_rate 0.000107839
2017-09-29T13:38:00.691284: step 1576, loss 0.145055, acc 0.9375, learning_rate 0.000107807
2017-09-29T13:38:00.876775: step 1577, loss 0.072099, acc 0.96875, learning_rate 0.000107775
2017-09-29T13:38:01.058009: step 1578, loss 0.0457047, acc 1, learning_rate 0.000107744
2017-09-29T13:38:01.241454: step 1579, loss 0.0364273, acc 1, learning_rate 0.000107712
2017-09-29T13:38:01.429438: step 1580, loss 0.0317836, acc 1, learning_rate 0.000107681
2017-09-29T13:38:01.611625: step 1581, loss 0.0957991, acc 0.96875, learning_rate 0.000107649
2017-09-29T13:38:01.798386: step 1582, loss 0.0485609, acc 1, learning_rate 0.000107618
2017-09-29T13:38:01.980678: step 1583, loss 0.0576539, acc 0.984375, learning_rate 0.000107587
2017-09-29T13:38:02.164056: step 1584, loss 0.0978673, acc 0.984375, learning_rate 0.000107556
2017-09-29T13:38:02.350684: step 1585, loss 0.0448825, acc 1, learning_rate 0.000107525
2017-09-29T13:38:02.537603: step 1586, loss 0.151092, acc 0.953125, learning_rate 0.000107494
2017-09-29T13:38:02.722687: step 1587, loss 0.127497, acc 0.96875, learning_rate 0.000107464
2017-09-29T13:38:02.902269: step 1588, loss 0.0568793, acc 0.984375, learning_rate 0.000107433
2017-09-29T13:38:03.096834: step 1589, loss 0.0689268, acc 0.984375, learning_rate 0.000107403
2017-09-29T13:38:03.277520: step 1590, loss 0.0471104, acc 1, learning_rate 0.000107373
2017-09-29T13:38:03.471076: step 1591, loss 0.0560835, acc 0.984375, learning_rate 0.000107343
2017-09-29T13:38:03.660387: step 1592, loss 0.0947237, acc 0.984375, learning_rate 0.000107313
2017-09-29T13:38:03.851166: step 1593, loss 0.0596562, acc 0.96875, learning_rate 0.000107283
2017-09-29T13:38:04.040272: step 1594, loss 0.0290359, acc 0.984375, learning_rate 0.000107253
2017-09-29T13:38:04.230493: step 1595, loss 0.0403361, acc 1, learning_rate 0.000107224
2017-09-29T13:38:04.425817: step 1596, loss 0.0750443, acc 0.96875, learning_rate 0.000107194
2017-09-29T13:38:04.609827: step 1597, loss 0.0673396, acc 0.96875, learning_rate 0.000107165
2017-09-29T13:38:04.793662: step 1598, loss 0.0810789, acc 0.984375, learning_rate 0.000107136
2017-09-29T13:38:04.979293: step 1599, loss 0.0851493, acc 0.96875, learning_rate 0.000107106
2017-09-29T13:38:05.159999: step 1600, loss 0.105903, acc 0.96875, learning_rate 0.000107077

Evaluation:
2017-09-29T13:38:05.668208: step 1600, loss 0.218621, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1600

2017-09-29T13:38:06.372951: step 1601, loss 0.0976034, acc 0.96875, learning_rate 0.000107048
2017-09-29T13:38:06.560275: step 1602, loss 0.0478626, acc 0.984375, learning_rate 0.00010702
2017-09-29T13:38:06.747203: step 1603, loss 0.0755833, acc 0.96875, learning_rate 0.000106991
2017-09-29T13:38:06.935488: step 1604, loss 0.0738963, acc 1, learning_rate 0.000106963
2017-09-29T13:38:07.119216: step 1605, loss 0.0411112, acc 0.984375, learning_rate 0.000106934
2017-09-29T13:38:07.311368: step 1606, loss 0.0811172, acc 0.984375, learning_rate 0.000106906
2017-09-29T13:38:07.508786: step 1607, loss 0.0476125, acc 1, learning_rate 0.000106878
2017-09-29T13:38:07.698171: step 1608, loss 0.0514733, acc 1, learning_rate 0.00010685
2017-09-29T13:38:07.883036: step 1609, loss 0.0738715, acc 0.96875, learning_rate 0.000106822
2017-09-29T13:38:08.071260: step 1610, loss 0.0551151, acc 0.984375, learning_rate 0.000106794
2017-09-29T13:38:08.256873: step 1611, loss 0.0684853, acc 0.96875, learning_rate 0.000106766
2017-09-29T13:38:08.447631: step 1612, loss 0.08915, acc 0.953125, learning_rate 0.000106738
2017-09-29T13:38:08.632175: step 1613, loss 0.0408725, acc 1, learning_rate 0.000106711
2017-09-29T13:38:08.818529: step 1614, loss 0.0594897, acc 0.984375, learning_rate 0.000106684
2017-09-29T13:38:09.003908: step 1615, loss 0.0690212, acc 0.984375, learning_rate 0.000106656
2017-09-29T13:38:09.190692: step 1616, loss 0.0457657, acc 0.984375, learning_rate 0.000106629
2017-09-29T13:38:09.378756: step 1617, loss 0.0882344, acc 0.96875, learning_rate 0.000106602
2017-09-29T13:38:09.579477: step 1618, loss 0.0701285, acc 0.984375, learning_rate 0.000106575
2017-09-29T13:38:09.763372: step 1619, loss 0.0300132, acc 1, learning_rate 0.000106548
2017-09-29T13:38:09.945237: step 1620, loss 0.073898, acc 0.96875, learning_rate 0.000106521
2017-09-29T13:38:10.131056: step 1621, loss 0.0726102, acc 0.96875, learning_rate 0.000106495
2017-09-29T13:38:10.323730: step 1622, loss 0.0595054, acc 1, learning_rate 0.000106468
2017-09-29T13:38:10.511937: step 1623, loss 0.0522191, acc 1, learning_rate 0.000106442
2017-09-29T13:38:10.696446: step 1624, loss 0.0738054, acc 0.953125, learning_rate 0.000106416
2017-09-29T13:38:10.882999: step 1625, loss 0.072769, acc 0.96875, learning_rate 0.000106389
2017-09-29T13:38:11.067475: step 1626, loss 0.0439882, acc 1, learning_rate 0.000106363
2017-09-29T13:38:11.249577: step 1627, loss 0.0742481, acc 0.984375, learning_rate 0.000106337
2017-09-29T13:38:11.435431: step 1628, loss 0.0489635, acc 0.984375, learning_rate 0.000106312
2017-09-29T13:38:11.619524: step 1629, loss 0.0660596, acc 0.984375, learning_rate 0.000106286
2017-09-29T13:38:11.810818: step 1630, loss 0.0541337, acc 0.984375, learning_rate 0.00010626
2017-09-29T13:38:11.997296: step 1631, loss 0.0876562, acc 0.953125, learning_rate 0.000106235
2017-09-29T13:38:12.181283: step 1632, loss 0.0401704, acc 0.984375, learning_rate 0.000106209
2017-09-29T13:38:12.365790: step 1633, loss 0.0578837, acc 0.984375, learning_rate 0.000106184
2017-09-29T13:38:12.552686: step 1634, loss 0.0148089, acc 1, learning_rate 0.000106159
2017-09-29T13:38:12.735924: step 1635, loss 0.0423026, acc 1, learning_rate 0.000106133
2017-09-29T13:38:12.922879: step 1636, loss 0.074758, acc 0.984375, learning_rate 0.000106108
2017-09-29T13:38:13.120069: step 1637, loss 0.0761854, acc 0.984375, learning_rate 0.000106083
2017-09-29T13:38:13.302069: step 1638, loss 0.0739833, acc 0.96875, learning_rate 0.000106059
2017-09-29T13:38:13.487442: step 1639, loss 0.0258793, acc 1, learning_rate 0.000106034
2017-09-29T13:38:13.680077: step 1640, loss 0.110394, acc 0.96875, learning_rate 0.000106009

Evaluation:
2017-09-29T13:38:14.194299: step 1640, loss 0.216373, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1640

2017-09-29T13:38:14.990528: step 1641, loss 0.0651526, acc 0.984375, learning_rate 0.000105985
2017-09-29T13:38:15.174032: step 1642, loss 0.0426787, acc 1, learning_rate 0.00010596
2017-09-29T13:38:15.359430: step 1643, loss 0.0401161, acc 1, learning_rate 0.000105936
2017-09-29T13:38:15.559111: step 1644, loss 0.0549303, acc 0.984375, learning_rate 0.000105912
2017-09-29T13:38:15.742932: step 1645, loss 0.0296398, acc 1, learning_rate 0.000105888
2017-09-29T13:38:15.932529: step 1646, loss 0.0699241, acc 0.96875, learning_rate 0.000105864
2017-09-29T13:38:16.123513: step 1647, loss 0.0972067, acc 0.953125, learning_rate 0.00010584
2017-09-29T13:38:16.309058: step 1648, loss 0.0492959, acc 0.984375, learning_rate 0.000105816
2017-09-29T13:38:16.495466: step 1649, loss 0.031104, acc 1, learning_rate 0.000105792
2017-09-29T13:38:16.680314: step 1650, loss 0.0557187, acc 1, learning_rate 0.000105768
2017-09-29T13:38:16.868477: step 1651, loss 0.0429795, acc 0.984375, learning_rate 0.000105745
2017-09-29T13:38:17.052128: step 1652, loss 0.0495899, acc 0.984375, learning_rate 0.000105721
2017-09-29T13:38:17.236547: step 1653, loss 0.108798, acc 0.9375, learning_rate 0.000105698
2017-09-29T13:38:17.419878: step 1654, loss 0.0936045, acc 0.96875, learning_rate 0.000105675
2017-09-29T13:38:17.603795: step 1655, loss 0.0687016, acc 0.984375, learning_rate 0.000105652
2017-09-29T13:38:17.787584: step 1656, loss 0.134503, acc 0.921875, learning_rate 0.000105629
2017-09-29T13:38:17.970975: step 1657, loss 0.0896576, acc 0.953125, learning_rate 0.000105606
2017-09-29T13:38:18.166912: step 1658, loss 0.0625246, acc 0.984375, learning_rate 0.000105583
2017-09-29T13:38:18.358570: step 1659, loss 0.114595, acc 0.9375, learning_rate 0.00010556
2017-09-29T13:38:18.550642: step 1660, loss 0.0550192, acc 0.96875, learning_rate 0.000105537
2017-09-29T13:38:18.746535: step 1661, loss 0.0728516, acc 0.984375, learning_rate 0.000105515
2017-09-29T13:38:18.936860: step 1662, loss 0.0387015, acc 1, learning_rate 0.000105492
2017-09-29T13:38:19.122409: step 1663, loss 0.0361119, acc 1, learning_rate 0.00010547
2017-09-29T13:38:19.310314: step 1664, loss 0.0970533, acc 0.96875, learning_rate 0.000105447
2017-09-29T13:38:19.508011: step 1665, loss 0.0769831, acc 0.953125, learning_rate 0.000105425
2017-09-29T13:38:19.663904: step 1666, loss 0.0325074, acc 1, learning_rate 0.000105403
2017-09-29T13:38:19.848020: step 1667, loss 0.0763121, acc 0.953125, learning_rate 0.000105381
2017-09-29T13:38:20.031277: step 1668, loss 0.134699, acc 0.953125, learning_rate 0.000105359
2017-09-29T13:38:20.222906: step 1669, loss 0.0706916, acc 0.96875, learning_rate 0.000105337
2017-09-29T13:38:20.415613: step 1670, loss 0.0279604, acc 1, learning_rate 0.000105315
2017-09-29T13:38:20.604851: step 1671, loss 0.0330066, acc 1, learning_rate 0.000105294
2017-09-29T13:38:20.794425: step 1672, loss 0.0569674, acc 0.984375, learning_rate 0.000105272
2017-09-29T13:38:20.987044: step 1673, loss 0.0509294, acc 0.984375, learning_rate 0.000105251
2017-09-29T13:38:21.179859: step 1674, loss 0.104881, acc 0.984375, learning_rate 0.000105229
2017-09-29T13:38:21.364425: step 1675, loss 0.0491538, acc 0.984375, learning_rate 0.000105208
2017-09-29T13:38:21.563196: step 1676, loss 0.0757277, acc 0.984375, learning_rate 0.000105186
2017-09-29T13:38:21.748313: step 1677, loss 0.0285011, acc 1, learning_rate 0.000105165
2017-09-29T13:38:21.938948: step 1678, loss 0.0401739, acc 0.984375, learning_rate 0.000105144
2017-09-29T13:38:22.125401: step 1679, loss 0.145937, acc 0.9375, learning_rate 0.000105123
2017-09-29T13:38:22.310781: step 1680, loss 0.086433, acc 0.96875, learning_rate 0.000105102

Evaluation:
2017-09-29T13:38:22.829917: step 1680, loss 0.213522, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1680

2017-09-29T13:38:23.473631: step 1681, loss 0.0563171, acc 0.984375, learning_rate 0.000105081
2017-09-29T13:38:23.655984: step 1682, loss 0.118832, acc 0.953125, learning_rate 0.000105061
2017-09-29T13:38:23.840532: step 1683, loss 0.0533217, acc 0.984375, learning_rate 0.00010504
2017-09-29T13:38:24.023437: step 1684, loss 0.0831903, acc 0.96875, learning_rate 0.00010502
2017-09-29T13:38:24.205958: step 1685, loss 0.151005, acc 0.921875, learning_rate 0.000104999
2017-09-29T13:38:24.390736: step 1686, loss 0.0810837, acc 0.96875, learning_rate 0.000104979
2017-09-29T13:38:24.574995: step 1687, loss 0.0584717, acc 0.984375, learning_rate 0.000104958
2017-09-29T13:38:24.759503: step 1688, loss 0.0563855, acc 0.984375, learning_rate 0.000104938
2017-09-29T13:38:24.946623: step 1689, loss 0.0548126, acc 0.96875, learning_rate 0.000104918
2017-09-29T13:38:25.137923: step 1690, loss 0.0294495, acc 1, learning_rate 0.000104898
2017-09-29T13:38:25.324504: step 1691, loss 0.0863763, acc 0.96875, learning_rate 0.000104878
2017-09-29T13:38:25.509425: step 1692, loss 0.0868449, acc 0.984375, learning_rate 0.000104858
2017-09-29T13:38:25.691684: step 1693, loss 0.064913, acc 0.984375, learning_rate 0.000104838
2017-09-29T13:38:25.887831: step 1694, loss 0.0468577, acc 1, learning_rate 0.000104818
2017-09-29T13:38:26.069487: step 1695, loss 0.113136, acc 0.96875, learning_rate 0.000104799
2017-09-29T13:38:26.250158: step 1696, loss 0.101165, acc 0.984375, learning_rate 0.000104779
2017-09-29T13:38:26.437401: step 1697, loss 0.0796222, acc 0.96875, learning_rate 0.00010476
2017-09-29T13:38:26.625184: step 1698, loss 0.0952006, acc 0.953125, learning_rate 0.00010474
2017-09-29T13:38:26.806526: step 1699, loss 0.082568, acc 0.953125, learning_rate 0.000104721
2017-09-29T13:38:26.998439: step 1700, loss 0.056847, acc 0.984375, learning_rate 0.000104702
2017-09-29T13:38:27.185605: step 1701, loss 0.0637546, acc 0.984375, learning_rate 0.000104682
2017-09-29T13:38:27.371445: step 1702, loss 0.0444278, acc 0.984375, learning_rate 0.000104663
2017-09-29T13:38:27.572147: step 1703, loss 0.0706935, acc 0.984375, learning_rate 0.000104644
2017-09-29T13:38:27.766343: step 1704, loss 0.0676705, acc 0.96875, learning_rate 0.000104625
2017-09-29T13:38:27.949189: step 1705, loss 0.132771, acc 0.9375, learning_rate 0.000104606
2017-09-29T13:38:28.141110: step 1706, loss 0.0626692, acc 0.96875, learning_rate 0.000104588
2017-09-29T13:38:28.333815: step 1707, loss 0.100461, acc 0.9375, learning_rate 0.000104569
2017-09-29T13:38:28.518692: step 1708, loss 0.0730821, acc 0.984375, learning_rate 0.00010455
2017-09-29T13:38:28.706484: step 1709, loss 0.0532318, acc 0.984375, learning_rate 0.000104532
2017-09-29T13:38:28.894409: step 1710, loss 0.0520234, acc 0.984375, learning_rate 0.000104513
2017-09-29T13:38:29.080789: step 1711, loss 0.0276543, acc 1, learning_rate 0.000104495
2017-09-29T13:38:29.266048: step 1712, loss 0.0644956, acc 0.984375, learning_rate 0.000104476
2017-09-29T13:38:29.450588: step 1713, loss 0.0185573, acc 1, learning_rate 0.000104458
2017-09-29T13:38:29.636008: step 1714, loss 0.124238, acc 0.953125, learning_rate 0.00010444
2017-09-29T13:38:29.823070: step 1715, loss 0.120721, acc 0.953125, learning_rate 0.000104422
2017-09-29T13:38:30.006376: step 1716, loss 0.0567847, acc 0.984375, learning_rate 0.000104404
2017-09-29T13:38:30.188737: step 1717, loss 0.0547763, acc 0.984375, learning_rate 0.000104386
2017-09-29T13:38:30.373210: step 1718, loss 0.0449528, acc 1, learning_rate 0.000104368
2017-09-29T13:38:30.558968: step 1719, loss 0.0960194, acc 0.984375, learning_rate 0.00010435
2017-09-29T13:38:30.743419: step 1720, loss 0.0682889, acc 0.984375, learning_rate 0.000104332

Evaluation:
2017-09-29T13:38:31.246866: step 1720, loss 0.212608, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1720

2017-09-29T13:38:31.956414: step 1721, loss 0.0931372, acc 0.96875, learning_rate 0.000104315
2017-09-29T13:38:32.140532: step 1722, loss 0.0374468, acc 1, learning_rate 0.000104297
2017-09-29T13:38:32.335780: step 1723, loss 0.0535645, acc 1, learning_rate 0.000104279
2017-09-29T13:38:32.520074: step 1724, loss 0.0594402, acc 0.984375, learning_rate 0.000104262
2017-09-29T13:38:32.705535: step 1725, loss 0.0468512, acc 1, learning_rate 0.000104245
2017-09-29T13:38:32.888812: step 1726, loss 0.0563993, acc 0.984375, learning_rate 0.000104227
2017-09-29T13:38:33.071077: step 1727, loss 0.166359, acc 0.953125, learning_rate 0.00010421
2017-09-29T13:38:33.261303: step 1728, loss 0.0304424, acc 1, learning_rate 0.000104193
2017-09-29T13:38:33.453273: step 1729, loss 0.0550862, acc 0.984375, learning_rate 0.000104176
2017-09-29T13:38:33.638981: step 1730, loss 0.0292751, acc 1, learning_rate 0.000104159
2017-09-29T13:38:33.828393: step 1731, loss 0.0712882, acc 0.984375, learning_rate 0.000104142
2017-09-29T13:38:34.010063: step 1732, loss 0.082455, acc 0.953125, learning_rate 0.000104125
2017-09-29T13:38:34.193999: step 1733, loss 0.120883, acc 0.921875, learning_rate 0.000104108
2017-09-29T13:38:34.380306: step 1734, loss 0.0362969, acc 0.984375, learning_rate 0.000104091
2017-09-29T13:38:34.565031: step 1735, loss 0.0271482, acc 1, learning_rate 0.000104074
2017-09-29T13:38:34.751569: step 1736, loss 0.102873, acc 0.96875, learning_rate 0.000104058
2017-09-29T13:38:34.934117: step 1737, loss 0.0469695, acc 0.984375, learning_rate 0.000104041
2017-09-29T13:38:35.118012: step 1738, loss 0.0743751, acc 0.984375, learning_rate 0.000104025
2017-09-29T13:38:35.309338: step 1739, loss 0.0468466, acc 0.984375, learning_rate 0.000104008
2017-09-29T13:38:35.497516: step 1740, loss 0.164805, acc 0.921875, learning_rate 0.000103992
2017-09-29T13:38:35.685562: step 1741, loss 0.0283336, acc 1, learning_rate 0.000103976
2017-09-29T13:38:35.870331: step 1742, loss 0.0708715, acc 0.953125, learning_rate 0.000103959
2017-09-29T13:38:36.055208: step 1743, loss 0.029629, acc 1, learning_rate 0.000103943
2017-09-29T13:38:36.241400: step 1744, loss 0.0492946, acc 1, learning_rate 0.000103927
2017-09-29T13:38:36.425847: step 1745, loss 0.0528621, acc 0.984375, learning_rate 0.000103911
2017-09-29T13:38:36.615459: step 1746, loss 0.0549512, acc 0.984375, learning_rate 0.000103895
2017-09-29T13:38:36.800740: step 1747, loss 0.0667413, acc 0.984375, learning_rate 0.000103879
2017-09-29T13:38:36.989894: step 1748, loss 0.0731787, acc 0.96875, learning_rate 0.000103863
2017-09-29T13:38:37.174696: step 1749, loss 0.0333163, acc 1, learning_rate 0.000103848
2017-09-29T13:38:37.359061: step 1750, loss 0.0366046, acc 1, learning_rate 0.000103832
2017-09-29T13:38:37.546220: step 1751, loss 0.0606591, acc 0.96875, learning_rate 0.000103816
2017-09-29T13:38:37.731986: step 1752, loss 0.158367, acc 0.96875, learning_rate 0.000103801
2017-09-29T13:38:37.917878: step 1753, loss 0.0356936, acc 0.984375, learning_rate 0.000103785
2017-09-29T13:38:38.106460: step 1754, loss 0.0568747, acc 0.96875, learning_rate 0.00010377
2017-09-29T13:38:38.297177: step 1755, loss 0.023957, acc 1, learning_rate 0.000103754
2017-09-29T13:38:38.487367: step 1756, loss 0.0963378, acc 0.96875, learning_rate 0.000103739
2017-09-29T13:38:38.672487: step 1757, loss 0.099863, acc 0.96875, learning_rate 0.000103724
2017-09-29T13:38:38.856012: step 1758, loss 0.0903026, acc 0.96875, learning_rate 0.000103709
2017-09-29T13:38:39.040547: step 1759, loss 0.0934494, acc 0.96875, learning_rate 0.000103694
2017-09-29T13:38:39.227904: step 1760, loss 0.0784151, acc 0.96875, learning_rate 0.000103678

Evaluation:
2017-09-29T13:38:39.746630: step 1760, loss 0.213923, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1760

2017-09-29T13:38:40.543446: step 1761, loss 0.0416866, acc 1, learning_rate 0.000103663
2017-09-29T13:38:40.726434: step 1762, loss 0.0887788, acc 0.953125, learning_rate 0.000103648
2017-09-29T13:38:40.911328: step 1763, loss 0.0822443, acc 0.984375, learning_rate 0.000103634
2017-09-29T13:38:41.067859: step 1764, loss 0.0394342, acc 0.980392, learning_rate 0.000103619
2017-09-29T13:38:41.254848: step 1765, loss 0.030113, acc 1, learning_rate 0.000103604
2017-09-29T13:38:41.440195: step 1766, loss 0.0419364, acc 0.96875, learning_rate 0.000103589
2017-09-29T13:38:41.624220: step 1767, loss 0.0532151, acc 1, learning_rate 0.000103575
2017-09-29T13:38:41.807598: step 1768, loss 0.0199324, acc 1, learning_rate 0.00010356
2017-09-29T13:38:41.998474: step 1769, loss 0.1318, acc 0.9375, learning_rate 0.000103545
2017-09-29T13:38:42.184152: step 1770, loss 0.071282, acc 0.984375, learning_rate 0.000103531
2017-09-29T13:38:42.370338: step 1771, loss 0.0477588, acc 1, learning_rate 0.000103517
2017-09-29T13:38:42.555667: step 1772, loss 0.0110086, acc 1, learning_rate 0.000103502
2017-09-29T13:38:42.738862: step 1773, loss 0.0925067, acc 0.953125, learning_rate 0.000103488
2017-09-29T13:38:42.925148: step 1774, loss 0.132729, acc 0.953125, learning_rate 0.000103474
2017-09-29T13:38:43.109288: step 1775, loss 0.0848636, acc 0.96875, learning_rate 0.00010346
2017-09-29T13:38:43.301689: step 1776, loss 0.0857773, acc 0.96875, learning_rate 0.000103445
2017-09-29T13:38:43.489340: step 1777, loss 0.0198001, acc 1, learning_rate 0.000103431
2017-09-29T13:38:43.672057: step 1778, loss 0.0859841, acc 0.984375, learning_rate 0.000103417
2017-09-29T13:38:43.858078: step 1779, loss 0.0726284, acc 0.96875, learning_rate 0.000103403
2017-09-29T13:38:44.042716: step 1780, loss 0.105094, acc 0.96875, learning_rate 0.00010339
2017-09-29T13:38:44.226995: step 1781, loss 0.0570113, acc 1, learning_rate 0.000103376
2017-09-29T13:38:44.415599: step 1782, loss 0.0635491, acc 0.984375, learning_rate 0.000103362
2017-09-29T13:38:44.595279: step 1783, loss 0.0166279, acc 1, learning_rate 0.000103348
2017-09-29T13:38:44.776744: step 1784, loss 0.0290983, acc 1, learning_rate 0.000103335
2017-09-29T13:38:44.964234: step 1785, loss 0.0419272, acc 0.984375, learning_rate 0.000103321
2017-09-29T13:38:45.152781: step 1786, loss 0.0324024, acc 1, learning_rate 0.000103307
2017-09-29T13:38:45.335865: step 1787, loss 0.114635, acc 0.96875, learning_rate 0.000103294
2017-09-29T13:38:45.533536: step 1788, loss 0.0893888, acc 0.96875, learning_rate 0.00010328
2017-09-29T13:38:45.724197: step 1789, loss 0.0655266, acc 0.984375, learning_rate 0.000103267
2017-09-29T13:38:45.906641: step 1790, loss 0.0320524, acc 0.984375, learning_rate 0.000103254
2017-09-29T13:38:46.087561: step 1791, loss 0.0980384, acc 0.953125, learning_rate 0.00010324
2017-09-29T13:38:46.272773: step 1792, loss 0.086779, acc 0.953125, learning_rate 0.000103227
2017-09-29T13:38:46.456748: step 1793, loss 0.0832261, acc 0.953125, learning_rate 0.000103214
2017-09-29T13:38:46.639980: step 1794, loss 0.054994, acc 0.96875, learning_rate 0.000103201
2017-09-29T13:38:46.821425: step 1795, loss 0.0527177, acc 0.984375, learning_rate 0.000103188
2017-09-29T13:38:47.012382: step 1796, loss 0.065293, acc 0.96875, learning_rate 0.000103175
2017-09-29T13:38:47.193735: step 1797, loss 0.0694714, acc 0.96875, learning_rate 0.000103162
2017-09-29T13:38:47.380834: step 1798, loss 0.0796889, acc 0.96875, learning_rate 0.000103149
2017-09-29T13:38:47.565110: step 1799, loss 0.0657983, acc 0.984375, learning_rate 0.000103136
2017-09-29T13:38:47.750051: step 1800, loss 0.0217868, acc 1, learning_rate 0.000103123

Evaluation:
2017-09-29T13:38:48.259680: step 1800, loss 0.213009, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1800

2017-09-29T13:38:48.918214: step 1801, loss 0.0562918, acc 0.984375, learning_rate 0.000103111
2017-09-29T13:38:49.101494: step 1802, loss 0.100595, acc 0.953125, learning_rate 0.000103098
2017-09-29T13:38:49.288614: step 1803, loss 0.0714289, acc 0.984375, learning_rate 0.000103085
2017-09-29T13:38:49.475978: step 1804, loss 0.0371037, acc 0.984375, learning_rate 0.000103073
2017-09-29T13:38:49.660090: step 1805, loss 0.0639285, acc 0.96875, learning_rate 0.00010306
2017-09-29T13:38:49.839766: step 1806, loss 0.0246727, acc 1, learning_rate 0.000103048
2017-09-29T13:38:50.024894: step 1807, loss 0.0507183, acc 0.984375, learning_rate 0.000103035
2017-09-29T13:38:50.207689: step 1808, loss 0.034706, acc 0.984375, learning_rate 0.000103023
2017-09-29T13:38:50.391757: step 1809, loss 0.0682246, acc 0.984375, learning_rate 0.00010301
2017-09-29T13:38:50.575595: step 1810, loss 0.0337117, acc 0.984375, learning_rate 0.000102998
2017-09-29T13:38:50.759268: step 1811, loss 0.0836014, acc 0.984375, learning_rate 0.000102986
2017-09-29T13:38:50.941727: step 1812, loss 0.0707802, acc 0.96875, learning_rate 0.000102974
2017-09-29T13:38:51.124584: step 1813, loss 0.0763146, acc 0.96875, learning_rate 0.000102962
2017-09-29T13:38:51.308286: step 1814, loss 0.103552, acc 0.96875, learning_rate 0.000102949
2017-09-29T13:38:51.504316: step 1815, loss 0.0312412, acc 0.984375, learning_rate 0.000102937
2017-09-29T13:38:51.686104: step 1816, loss 0.0785865, acc 0.984375, learning_rate 0.000102925
2017-09-29T13:38:51.871218: step 1817, loss 0.0599677, acc 0.984375, learning_rate 0.000102913
2017-09-29T13:38:52.059689: step 1818, loss 0.0887699, acc 0.96875, learning_rate 0.000102902
2017-09-29T13:38:52.244179: step 1819, loss 0.129052, acc 0.953125, learning_rate 0.00010289
2017-09-29T13:38:52.430819: step 1820, loss 0.0301263, acc 0.984375, learning_rate 0.000102878
2017-09-29T13:38:52.616092: step 1821, loss 0.0772429, acc 0.984375, learning_rate 0.000102866
2017-09-29T13:38:52.799904: step 1822, loss 0.0851242, acc 0.984375, learning_rate 0.000102855
2017-09-29T13:38:52.990422: step 1823, loss 0.0380512, acc 0.984375, learning_rate 0.000102843
2017-09-29T13:38:53.175435: step 1824, loss 0.0571534, acc 0.96875, learning_rate 0.000102831
2017-09-29T13:38:53.366218: step 1825, loss 0.0460763, acc 1, learning_rate 0.00010282
2017-09-29T13:38:53.556331: step 1826, loss 0.047476, acc 1, learning_rate 0.000102808
2017-09-29T13:38:53.738199: step 1827, loss 0.0923588, acc 0.96875, learning_rate 0.000102797
2017-09-29T13:38:53.924045: step 1828, loss 0.125364, acc 0.953125, learning_rate 0.000102785
2017-09-29T13:38:54.109911: step 1829, loss 0.0474481, acc 1, learning_rate 0.000102774
2017-09-29T13:38:54.289438: step 1830, loss 0.0597631, acc 0.96875, learning_rate 0.000102763
2017-09-29T13:38:54.480398: step 1831, loss 0.036465, acc 0.984375, learning_rate 0.000102751
2017-09-29T13:38:54.669640: step 1832, loss 0.13529, acc 0.953125, learning_rate 0.00010274
2017-09-29T13:38:54.853396: step 1833, loss 0.0383674, acc 0.984375, learning_rate 0.000102729
2017-09-29T13:38:55.037837: step 1834, loss 0.0477173, acc 0.984375, learning_rate 0.000102718
2017-09-29T13:38:55.221456: step 1835, loss 0.120674, acc 0.96875, learning_rate 0.000102707
2017-09-29T13:38:55.410804: step 1836, loss 0.0789149, acc 0.96875, learning_rate 0.000102696
2017-09-29T13:38:55.595530: step 1837, loss 0.0635024, acc 0.96875, learning_rate 0.000102685
2017-09-29T13:38:55.792051: step 1838, loss 0.0366536, acc 1, learning_rate 0.000102674
2017-09-29T13:38:55.978220: step 1839, loss 0.071579, acc 0.984375, learning_rate 0.000102663
2017-09-29T13:38:56.161854: step 1840, loss 0.0778643, acc 0.96875, learning_rate 0.000102652

Evaluation:
2017-09-29T13:38:56.687408: step 1840, loss 0.213846, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1840

2017-09-29T13:38:57.413811: step 1841, loss 0.114957, acc 0.96875, learning_rate 0.000102641
2017-09-29T13:38:57.608674: step 1842, loss 0.0727955, acc 0.984375, learning_rate 0.00010263
2017-09-29T13:38:57.792744: step 1843, loss 0.0467209, acc 1, learning_rate 0.00010262
2017-09-29T13:38:57.977546: step 1844, loss 0.0407451, acc 0.984375, learning_rate 0.000102609
2017-09-29T13:38:58.165065: step 1845, loss 0.0692014, acc 0.984375, learning_rate 0.000102598
2017-09-29T13:38:58.362138: step 1846, loss 0.0758406, acc 0.984375, learning_rate 0.000102588
2017-09-29T13:38:58.553796: step 1847, loss 0.0249261, acc 1, learning_rate 0.000102577
2017-09-29T13:38:58.744311: step 1848, loss 0.0377878, acc 1, learning_rate 0.000102567
2017-09-29T13:38:58.930641: step 1849, loss 0.0930399, acc 0.953125, learning_rate 0.000102556
2017-09-29T13:38:59.118980: step 1850, loss 0.0399412, acc 0.984375, learning_rate 0.000102546
2017-09-29T13:38:59.310920: step 1851, loss 0.0318855, acc 1, learning_rate 0.000102535
2017-09-29T13:38:59.499099: step 1852, loss 0.0617673, acc 0.984375, learning_rate 0.000102525
2017-09-29T13:38:59.684054: step 1853, loss 0.0413899, acc 1, learning_rate 0.000102515
2017-09-29T13:38:59.866829: step 1854, loss 0.0418766, acc 1, learning_rate 0.000102504
2017-09-29T13:39:00.051424: step 1855, loss 0.0584092, acc 0.96875, learning_rate 0.000102494
2017-09-29T13:39:00.237578: step 1856, loss 0.0811289, acc 0.96875, learning_rate 0.000102484
2017-09-29T13:39:00.423579: step 1857, loss 0.0763002, acc 0.96875, learning_rate 0.000102474
2017-09-29T13:39:00.609276: step 1858, loss 0.09587, acc 0.9375, learning_rate 0.000102464
2017-09-29T13:39:00.793310: step 1859, loss 0.0577502, acc 0.984375, learning_rate 0.000102454
2017-09-29T13:39:00.976739: step 1860, loss 0.0337361, acc 0.984375, learning_rate 0.000102444
2017-09-29T13:39:01.161760: step 1861, loss 0.112596, acc 0.953125, learning_rate 0.000102434
2017-09-29T13:39:01.313656: step 1862, loss 0.0662831, acc 1, learning_rate 0.000102424
2017-09-29T13:39:01.500286: step 1863, loss 0.0387535, acc 1, learning_rate 0.000102414
2017-09-29T13:39:01.684091: step 1864, loss 0.0378484, acc 1, learning_rate 0.000102404
2017-09-29T13:39:01.872525: step 1865, loss 0.0607533, acc 0.984375, learning_rate 0.000102394
2017-09-29T13:39:02.064493: step 1866, loss 0.0256777, acc 1, learning_rate 0.000102384
2017-09-29T13:39:02.254162: step 1867, loss 0.0260873, acc 1, learning_rate 0.000102375
2017-09-29T13:39:02.444570: step 1868, loss 0.0894149, acc 0.984375, learning_rate 0.000102365
2017-09-29T13:39:02.639494: step 1869, loss 0.0814391, acc 0.96875, learning_rate 0.000102355
2017-09-29T13:39:02.840537: step 1870, loss 0.0235793, acc 1, learning_rate 0.000102346
2017-09-29T13:39:03.027205: step 1871, loss 0.0481886, acc 0.984375, learning_rate 0.000102336
2017-09-29T13:39:03.212341: step 1872, loss 0.0370969, acc 1, learning_rate 0.000102327
2017-09-29T13:39:03.419632: step 1873, loss 0.0807193, acc 0.96875, learning_rate 0.000102317
2017-09-29T13:39:03.617818: step 1874, loss 0.049132, acc 0.984375, learning_rate 0.000102308
2017-09-29T13:39:03.805360: step 1875, loss 0.0408701, acc 0.984375, learning_rate 0.000102298
2017-09-29T13:39:03.990458: step 1876, loss 0.0356358, acc 0.984375, learning_rate 0.000102289
2017-09-29T13:39:04.177933: step 1877, loss 0.0307443, acc 1, learning_rate 0.000102279
2017-09-29T13:39:04.360185: step 1878, loss 0.0348924, acc 1, learning_rate 0.00010227
2017-09-29T13:39:04.545230: step 1879, loss 0.0426038, acc 0.984375, learning_rate 0.000102261
2017-09-29T13:39:04.731845: step 1880, loss 0.0982328, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-09-29T13:39:05.231542: step 1880, loss 0.218177, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1880

2017-09-29T13:39:06.026114: step 1881, loss 0.0995369, acc 0.96875, learning_rate 0.000102242
2017-09-29T13:39:06.210879: step 1882, loss 0.129375, acc 0.9375, learning_rate 0.000102233
2017-09-29T13:39:06.397376: step 1883, loss 0.0439035, acc 1, learning_rate 0.000102224
2017-09-29T13:39:06.587197: step 1884, loss 0.0937584, acc 0.96875, learning_rate 0.000102215
2017-09-29T13:39:06.783485: step 1885, loss 0.066952, acc 0.984375, learning_rate 0.000102206
2017-09-29T13:39:06.975018: step 1886, loss 0.069887, acc 0.96875, learning_rate 0.000102197
2017-09-29T13:39:07.169439: step 1887, loss 0.0965507, acc 0.984375, learning_rate 0.000102188
2017-09-29T13:39:07.360047: step 1888, loss 0.042815, acc 0.984375, learning_rate 0.000102179
2017-09-29T13:39:07.554126: step 1889, loss 0.1003, acc 0.96875, learning_rate 0.00010217
2017-09-29T13:39:07.747989: step 1890, loss 0.0495923, acc 0.984375, learning_rate 0.000102161
2017-09-29T13:39:07.939505: step 1891, loss 0.0632013, acc 0.984375, learning_rate 0.000102153
2017-09-29T13:39:08.127569: step 1892, loss 0.0888199, acc 0.953125, learning_rate 0.000102144
2017-09-29T13:39:08.310362: step 1893, loss 0.066494, acc 0.96875, learning_rate 0.000102135
2017-09-29T13:39:08.507389: step 1894, loss 0.105369, acc 0.9375, learning_rate 0.000102126
2017-09-29T13:39:08.689990: step 1895, loss 0.0213932, acc 1, learning_rate 0.000102118
2017-09-29T13:39:08.881959: step 1896, loss 0.0377827, acc 0.984375, learning_rate 0.000102109
2017-09-29T13:39:09.068939: step 1897, loss 0.0328364, acc 1, learning_rate 0.0001021
2017-09-29T13:39:09.253650: step 1898, loss 0.0368152, acc 1, learning_rate 0.000102092
2017-09-29T13:39:09.450783: step 1899, loss 0.113293, acc 0.953125, learning_rate 0.000102083
2017-09-29T13:39:09.642918: step 1900, loss 0.0792486, acc 0.984375, learning_rate 0.000102075
2017-09-29T13:39:09.832132: step 1901, loss 0.0891089, acc 0.9375, learning_rate 0.000102066
2017-09-29T13:39:10.020830: step 1902, loss 0.0916753, acc 0.96875, learning_rate 0.000102058
2017-09-29T13:39:10.208292: step 1903, loss 0.115061, acc 0.953125, learning_rate 0.00010205
2017-09-29T13:39:10.395487: step 1904, loss 0.0720697, acc 0.984375, learning_rate 0.000102041
2017-09-29T13:39:10.579436: step 1905, loss 0.0259266, acc 1, learning_rate 0.000102033
2017-09-29T13:39:10.767119: step 1906, loss 0.079547, acc 0.953125, learning_rate 0.000102025
2017-09-29T13:39:10.949610: step 1907, loss 0.104988, acc 0.984375, learning_rate 0.000102016
2017-09-29T13:39:11.134329: step 1908, loss 0.0604312, acc 0.984375, learning_rate 0.000102008
2017-09-29T13:39:11.315750: step 1909, loss 0.124862, acc 0.953125, learning_rate 0.000102
2017-09-29T13:39:11.502052: step 1910, loss 0.0814131, acc 0.96875, learning_rate 0.000101992
2017-09-29T13:39:11.683538: step 1911, loss 0.0590243, acc 0.984375, learning_rate 0.000101984
2017-09-29T13:39:11.869941: step 1912, loss 0.0508642, acc 1, learning_rate 0.000101975
2017-09-29T13:39:12.051744: step 1913, loss 0.107668, acc 0.953125, learning_rate 0.000101967
2017-09-29T13:39:12.238330: step 1914, loss 0.0798175, acc 0.96875, learning_rate 0.000101959
2017-09-29T13:39:12.425411: step 1915, loss 0.0910045, acc 0.96875, learning_rate 0.000101951
2017-09-29T13:39:12.611680: step 1916, loss 0.0800761, acc 0.984375, learning_rate 0.000101943
2017-09-29T13:39:12.810365: step 1917, loss 0.0548388, acc 0.984375, learning_rate 0.000101935
2017-09-29T13:39:12.998799: step 1918, loss 0.0864519, acc 0.96875, learning_rate 0.000101928
2017-09-29T13:39:13.185682: step 1919, loss 0.0910481, acc 0.984375, learning_rate 0.00010192
2017-09-29T13:39:13.372147: step 1920, loss 0.0552394, acc 0.984375, learning_rate 0.000101912

Evaluation:
2017-09-29T13:39:13.927621: step 1920, loss 0.216732, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1920

2017-09-29T13:39:14.578830: step 1921, loss 0.0380988, acc 1, learning_rate 0.000101904
2017-09-29T13:39:14.767920: step 1922, loss 0.12697, acc 0.96875, learning_rate 0.000101896
2017-09-29T13:39:14.964866: step 1923, loss 0.0514169, acc 1, learning_rate 0.000101889
2017-09-29T13:39:15.155662: step 1924, loss 0.0801051, acc 0.96875, learning_rate 0.000101881
2017-09-29T13:39:15.340029: step 1925, loss 0.0903715, acc 0.984375, learning_rate 0.000101873
2017-09-29T13:39:15.542268: step 1926, loss 0.0251384, acc 1, learning_rate 0.000101865
2017-09-29T13:39:15.729741: step 1927, loss 0.0373437, acc 0.984375, learning_rate 0.000101858
2017-09-29T13:39:15.916396: step 1928, loss 0.0401845, acc 0.984375, learning_rate 0.00010185
2017-09-29T13:39:16.112169: step 1929, loss 0.0830578, acc 0.984375, learning_rate 0.000101843
2017-09-29T13:39:16.301872: step 1930, loss 0.0285544, acc 1, learning_rate 0.000101835
2017-09-29T13:39:16.491526: step 1931, loss 0.0341575, acc 1, learning_rate 0.000101828
2017-09-29T13:39:16.680634: step 1932, loss 0.052279, acc 0.984375, learning_rate 0.00010182
2017-09-29T13:39:16.866154: step 1933, loss 0.12313, acc 0.953125, learning_rate 0.000101813
2017-09-29T13:39:17.051124: step 1934, loss 0.0643953, acc 0.984375, learning_rate 0.000101805
2017-09-29T13:39:17.239293: step 1935, loss 0.0564154, acc 0.984375, learning_rate 0.000101798
2017-09-29T13:39:17.425796: step 1936, loss 0.0336744, acc 0.984375, learning_rate 0.000101791
2017-09-29T13:39:17.619475: step 1937, loss 0.0623856, acc 0.984375, learning_rate 0.000101783
2017-09-29T13:39:17.808620: step 1938, loss 0.0494004, acc 0.984375, learning_rate 0.000101776
2017-09-29T13:39:17.996910: step 1939, loss 0.0655636, acc 0.984375, learning_rate 0.000101769
2017-09-29T13:39:18.185373: step 1940, loss 0.0518541, acc 0.984375, learning_rate 0.000101762
2017-09-29T13:39:18.371461: step 1941, loss 0.156264, acc 0.953125, learning_rate 0.000101754
2017-09-29T13:39:18.564540: step 1942, loss 0.0114526, acc 1, learning_rate 0.000101747
2017-09-29T13:39:18.748116: step 1943, loss 0.167946, acc 0.96875, learning_rate 0.00010174
2017-09-29T13:39:18.931745: step 1944, loss 0.087218, acc 0.953125, learning_rate 0.000101733
2017-09-29T13:39:19.118617: step 1945, loss 0.0938551, acc 0.96875, learning_rate 0.000101726
2017-09-29T13:39:19.302450: step 1946, loss 0.0748194, acc 0.984375, learning_rate 0.000101719
2017-09-29T13:39:19.488654: step 1947, loss 0.0253473, acc 1, learning_rate 0.000101712
2017-09-29T13:39:19.671835: step 1948, loss 0.0760819, acc 0.953125, learning_rate 0.000101705
2017-09-29T13:39:19.854605: step 1949, loss 0.0284765, acc 1, learning_rate 0.000101698
2017-09-29T13:39:20.037476: step 1950, loss 0.0689838, acc 0.984375, learning_rate 0.000101691
2017-09-29T13:39:20.219930: step 1951, loss 0.0496533, acc 0.984375, learning_rate 0.000101684
2017-09-29T13:39:20.405890: step 1952, loss 0.0583926, acc 0.96875, learning_rate 0.000101677
2017-09-29T13:39:20.609495: step 1953, loss 0.0333084, acc 1, learning_rate 0.00010167
2017-09-29T13:39:20.794631: step 1954, loss 0.039395, acc 1, learning_rate 0.000101664
2017-09-29T13:39:20.985580: step 1955, loss 0.0523463, acc 0.96875, learning_rate 0.000101657
2017-09-29T13:39:21.174113: step 1956, loss 0.0446601, acc 0.984375, learning_rate 0.00010165
2017-09-29T13:39:21.364473: step 1957, loss 0.0296947, acc 1, learning_rate 0.000101643
2017-09-29T13:39:21.557888: step 1958, loss 0.0482215, acc 0.984375, learning_rate 0.000101637
2017-09-29T13:39:21.742090: step 1959, loss 0.0682443, acc 0.984375, learning_rate 0.00010163
2017-09-29T13:39:21.892957: step 1960, loss 0.11538, acc 0.941176, learning_rate 0.000101623

Evaluation:
2017-09-29T13:39:22.426525: step 1960, loss 0.214142, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-1960

2017-09-29T13:39:23.132087: step 1961, loss 0.0520618, acc 0.984375, learning_rate 0.000101617
2017-09-29T13:39:23.315897: step 1962, loss 0.0283125, acc 1, learning_rate 0.00010161
2017-09-29T13:39:23.500266: step 1963, loss 0.031897, acc 1, learning_rate 0.000101604
2017-09-29T13:39:23.687537: step 1964, loss 0.0385021, acc 0.984375, learning_rate 0.000101597
2017-09-29T13:39:23.871024: step 1965, loss 0.0886687, acc 0.96875, learning_rate 0.00010159
2017-09-29T13:39:24.054509: step 1966, loss 0.0400828, acc 1, learning_rate 0.000101584
2017-09-29T13:39:24.239553: step 1967, loss 0.0183407, acc 1, learning_rate 0.000101577
2017-09-29T13:39:24.432442: step 1968, loss 0.115995, acc 0.96875, learning_rate 0.000101571
2017-09-29T13:39:24.625654: step 1969, loss 0.0671858, acc 0.96875, learning_rate 0.000101565
2017-09-29T13:39:24.823192: step 1970, loss 0.0513668, acc 0.984375, learning_rate 0.000101558
2017-09-29T13:39:25.013399: step 1971, loss 0.0781547, acc 0.984375, learning_rate 0.000101552
2017-09-29T13:39:25.210905: step 1972, loss 0.020282, acc 1, learning_rate 0.000101546
2017-09-29T13:39:25.399261: step 1973, loss 0.0237699, acc 1, learning_rate 0.000101539
2017-09-29T13:39:25.588717: step 1974, loss 0.0810566, acc 0.96875, learning_rate 0.000101533
2017-09-29T13:39:25.778645: step 1975, loss 0.0760181, acc 0.96875, learning_rate 0.000101527
2017-09-29T13:39:25.967301: step 1976, loss 0.118806, acc 0.953125, learning_rate 0.00010152
2017-09-29T13:39:26.154732: step 1977, loss 0.0914828, acc 0.984375, learning_rate 0.000101514
2017-09-29T13:39:26.336718: step 1978, loss 0.114984, acc 0.953125, learning_rate 0.000101508
2017-09-29T13:39:26.545817: step 1979, loss 0.0363781, acc 1, learning_rate 0.000101502
2017-09-29T13:39:26.732520: step 1980, loss 0.0432644, acc 0.984375, learning_rate 0.000101496
2017-09-29T13:39:26.914453: step 1981, loss 0.0667339, acc 0.984375, learning_rate 0.00010149
2017-09-29T13:39:27.095826: step 1982, loss 0.0465884, acc 0.984375, learning_rate 0.000101484
2017-09-29T13:39:27.287713: step 1983, loss 0.0466769, acc 1, learning_rate 0.000101478
2017-09-29T13:39:27.480075: step 1984, loss 0.187091, acc 0.921875, learning_rate 0.000101472
2017-09-29T13:39:27.669043: step 1985, loss 0.0786007, acc 0.96875, learning_rate 0.000101466
2017-09-29T13:39:27.857247: step 1986, loss 0.0318792, acc 1, learning_rate 0.00010146
2017-09-29T13:39:28.043969: step 1987, loss 0.101316, acc 0.953125, learning_rate 0.000101454
2017-09-29T13:39:28.238033: step 1988, loss 0.0652763, acc 0.984375, learning_rate 0.000101448
2017-09-29T13:39:28.430047: step 1989, loss 0.139788, acc 0.953125, learning_rate 0.000101442
2017-09-29T13:39:28.629616: step 1990, loss 0.0569228, acc 1, learning_rate 0.000101436
2017-09-29T13:39:28.813720: step 1991, loss 0.147435, acc 0.953125, learning_rate 0.00010143
2017-09-29T13:39:28.995302: step 1992, loss 0.0280578, acc 1, learning_rate 0.000101424
2017-09-29T13:39:29.179135: step 1993, loss 0.0592207, acc 0.984375, learning_rate 0.000101418
2017-09-29T13:39:29.364772: step 1994, loss 0.0906051, acc 0.96875, learning_rate 0.000101413
2017-09-29T13:39:29.549547: step 1995, loss 0.0896144, acc 0.953125, learning_rate 0.000101407
2017-09-29T13:39:29.735425: step 1996, loss 0.0396584, acc 0.984375, learning_rate 0.000101401
2017-09-29T13:39:29.923108: step 1997, loss 0.0276008, acc 1, learning_rate 0.000101395
2017-09-29T13:39:30.109863: step 1998, loss 0.0584894, acc 0.984375, learning_rate 0.00010139
2017-09-29T13:39:30.299992: step 1999, loss 0.0318861, acc 1, learning_rate 0.000101384
2017-09-29T13:39:30.493113: step 2000, loss 0.0603775, acc 0.984375, learning_rate 0.000101378

Evaluation:
2017-09-29T13:39:31.030050: step 2000, loss 0.21181, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2000

2017-09-29T13:39:31.735120: step 2001, loss 0.069414, acc 0.96875, learning_rate 0.000101373
2017-09-29T13:39:31.918901: step 2002, loss 0.0395781, acc 1, learning_rate 0.000101367
2017-09-29T13:39:32.102655: step 2003, loss 0.133666, acc 0.921875, learning_rate 0.000101362
2017-09-29T13:39:32.289685: step 2004, loss 0.0748937, acc 0.96875, learning_rate 0.000101356
2017-09-29T13:39:32.485803: step 2005, loss 0.10438, acc 0.953125, learning_rate 0.00010135
2017-09-29T13:39:32.675000: step 2006, loss 0.0613567, acc 0.96875, learning_rate 0.000101345
2017-09-29T13:39:32.861584: step 2007, loss 0.0920548, acc 0.96875, learning_rate 0.000101339
2017-09-29T13:39:33.044922: step 2008, loss 0.0331135, acc 1, learning_rate 0.000101334
2017-09-29T13:39:33.228963: step 2009, loss 0.0480512, acc 1, learning_rate 0.000101328
2017-09-29T13:39:33.417256: step 2010, loss 0.0393889, acc 0.984375, learning_rate 0.000101323
2017-09-29T13:39:33.612433: step 2011, loss 0.149009, acc 0.921875, learning_rate 0.000101318
2017-09-29T13:39:33.799989: step 2012, loss 0.017086, acc 1, learning_rate 0.000101312
2017-09-29T13:39:33.986782: step 2013, loss 0.0560158, acc 0.984375, learning_rate 0.000101307
2017-09-29T13:39:34.171304: step 2014, loss 0.0448793, acc 1, learning_rate 0.000101302
2017-09-29T13:39:34.365361: step 2015, loss 0.0972623, acc 0.96875, learning_rate 0.000101296
2017-09-29T13:39:34.560345: step 2016, loss 0.0495789, acc 0.984375, learning_rate 0.000101291
2017-09-29T13:39:34.762737: step 2017, loss 0.0456098, acc 0.984375, learning_rate 0.000101286
2017-09-29T13:39:34.959572: step 2018, loss 0.0229207, acc 1, learning_rate 0.00010128
2017-09-29T13:39:35.180570: step 2019, loss 0.0345649, acc 1, learning_rate 0.000101275
2017-09-29T13:39:35.394866: step 2020, loss 0.0401271, acc 1, learning_rate 0.00010127
2017-09-29T13:39:35.620933: step 2021, loss 0.0828999, acc 0.96875, learning_rate 0.000101265
2017-09-29T13:39:35.822498: step 2022, loss 0.0412873, acc 1, learning_rate 0.00010126
2017-09-29T13:39:36.023117: step 2023, loss 0.0985508, acc 0.96875, learning_rate 0.000101255
2017-09-29T13:39:36.223892: step 2024, loss 0.0981944, acc 0.984375, learning_rate 0.000101249
2017-09-29T13:39:36.424795: step 2025, loss 0.0390703, acc 0.984375, learning_rate 0.000101244
2017-09-29T13:39:36.619523: step 2026, loss 0.0982899, acc 0.984375, learning_rate 0.000101239
2017-09-29T13:39:36.805643: step 2027, loss 0.0467193, acc 0.984375, learning_rate 0.000101234
2017-09-29T13:39:36.990336: step 2028, loss 0.0330158, acc 1, learning_rate 0.000101229
2017-09-29T13:39:37.174944: step 2029, loss 0.0518995, acc 0.984375, learning_rate 0.000101224
2017-09-29T13:39:37.364450: step 2030, loss 0.0329944, acc 1, learning_rate 0.000101219
2017-09-29T13:39:37.552779: step 2031, loss 0.0820245, acc 0.984375, learning_rate 0.000101214
2017-09-29T13:39:37.737510: step 2032, loss 0.0556875, acc 0.96875, learning_rate 0.000101209
2017-09-29T13:39:37.925308: step 2033, loss 0.0968132, acc 0.96875, learning_rate 0.000101204
2017-09-29T13:39:38.123478: step 2034, loss 0.0375024, acc 1, learning_rate 0.000101199
2017-09-29T13:39:38.313246: step 2035, loss 0.0666551, acc 0.984375, learning_rate 0.000101194
2017-09-29T13:39:38.529143: step 2036, loss 0.0717972, acc 0.953125, learning_rate 0.00010119
2017-09-29T13:39:38.725746: step 2037, loss 0.070138, acc 0.984375, learning_rate 0.000101185
2017-09-29T13:39:38.913903: step 2038, loss 0.0916531, acc 0.984375, learning_rate 0.00010118
2017-09-29T13:39:39.102176: step 2039, loss 0.0396797, acc 1, learning_rate 0.000101175
2017-09-29T13:39:39.287596: step 2040, loss 0.0806332, acc 0.984375, learning_rate 0.00010117

Evaluation:
2017-09-29T13:39:39.823604: step 2040, loss 0.213116, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2040

2017-09-29T13:39:40.612917: step 2041, loss 0.094533, acc 0.953125, learning_rate 0.000101166
2017-09-29T13:39:40.796538: step 2042, loss 0.040155, acc 0.984375, learning_rate 0.000101161
2017-09-29T13:39:40.983635: step 2043, loss 0.0879314, acc 0.96875, learning_rate 0.000101156
2017-09-29T13:39:41.166851: step 2044, loss 0.0369917, acc 1, learning_rate 0.000101151
2017-09-29T13:39:41.353142: step 2045, loss 0.0877488, acc 0.953125, learning_rate 0.000101147
2017-09-29T13:39:41.542270: step 2046, loss 0.0417262, acc 0.984375, learning_rate 0.000101142
2017-09-29T13:39:41.723851: step 2047, loss 0.104988, acc 0.953125, learning_rate 0.000101137
2017-09-29T13:39:41.910244: step 2048, loss 0.0507386, acc 0.984375, learning_rate 0.000101133
2017-09-29T13:39:42.091520: step 2049, loss 0.0444383, acc 1, learning_rate 0.000101128
2017-09-29T13:39:42.273906: step 2050, loss 0.107918, acc 0.96875, learning_rate 0.000101123
2017-09-29T13:39:42.465811: step 2051, loss 0.0321188, acc 1, learning_rate 0.000101119
2017-09-29T13:39:42.666118: step 2052, loss 0.0879641, acc 0.96875, learning_rate 0.000101114
2017-09-29T13:39:42.863880: step 2053, loss 0.116728, acc 0.9375, learning_rate 0.00010111
2017-09-29T13:39:43.051223: step 2054, loss 0.0576718, acc 0.984375, learning_rate 0.000101105
2017-09-29T13:39:43.240849: step 2055, loss 0.0536169, acc 0.984375, learning_rate 0.000101101
2017-09-29T13:39:43.430172: step 2056, loss 0.0539128, acc 0.984375, learning_rate 0.000101096
2017-09-29T13:39:43.619212: step 2057, loss 0.122569, acc 0.9375, learning_rate 0.000101092
2017-09-29T13:39:43.781108: step 2058, loss 0.026201, acc 1, learning_rate 0.000101087
2017-09-29T13:39:43.965465: step 2059, loss 0.0831029, acc 0.96875, learning_rate 0.000101083
2017-09-29T13:39:44.149548: step 2060, loss 0.0958563, acc 0.96875, learning_rate 0.000101078
2017-09-29T13:39:44.334134: step 2061, loss 0.0662923, acc 0.984375, learning_rate 0.000101074
2017-09-29T13:39:44.534985: step 2062, loss 0.0195197, acc 1, learning_rate 0.00010107
2017-09-29T13:39:44.717808: step 2063, loss 0.0860635, acc 0.96875, learning_rate 0.000101065
2017-09-29T13:39:44.900679: step 2064, loss 0.0306725, acc 0.984375, learning_rate 0.000101061
2017-09-29T13:39:45.084988: step 2065, loss 0.0370189, acc 1, learning_rate 0.000101057
2017-09-29T13:39:45.277988: step 2066, loss 0.1053, acc 0.953125, learning_rate 0.000101052
2017-09-29T13:39:45.468932: step 2067, loss 0.0406729, acc 1, learning_rate 0.000101048
2017-09-29T13:39:45.657151: step 2068, loss 0.0434362, acc 1, learning_rate 0.000101044
2017-09-29T13:39:45.845934: step 2069, loss 0.0553899, acc 0.984375, learning_rate 0.000101039
2017-09-29T13:39:46.035028: step 2070, loss 0.0388823, acc 0.984375, learning_rate 0.000101035
2017-09-29T13:39:46.238881: step 2071, loss 0.0348002, acc 0.984375, learning_rate 0.000101031
2017-09-29T13:39:46.429830: step 2072, loss 0.110944, acc 0.96875, learning_rate 0.000101027
2017-09-29T13:39:46.621209: step 2073, loss 0.0586683, acc 0.96875, learning_rate 0.000101023
2017-09-29T13:39:46.809188: step 2074, loss 0.0773134, acc 0.96875, learning_rate 0.000101018
2017-09-29T13:39:46.997200: step 2075, loss 0.0428115, acc 0.984375, learning_rate 0.000101014
2017-09-29T13:39:47.182201: step 2076, loss 0.0293945, acc 1, learning_rate 0.00010101
2017-09-29T13:39:47.365524: step 2077, loss 0.0162352, acc 1, learning_rate 0.000101006
2017-09-29T13:39:47.547913: step 2078, loss 0.0886739, acc 0.984375, learning_rate 0.000101002
2017-09-29T13:39:47.740485: step 2079, loss 0.0383163, acc 1, learning_rate 0.000100998
2017-09-29T13:39:47.922163: step 2080, loss 0.0383255, acc 1, learning_rate 0.000100994

Evaluation:
2017-09-29T13:39:48.470411: step 2080, loss 0.214383, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2080

2017-09-29T13:39:49.106741: step 2081, loss 0.0862081, acc 0.96875, learning_rate 0.00010099
2017-09-29T13:39:49.287115: step 2082, loss 0.0625066, acc 0.984375, learning_rate 0.000100986
2017-09-29T13:39:49.476384: step 2083, loss 0.0612174, acc 0.984375, learning_rate 0.000100982
2017-09-29T13:39:49.660489: step 2084, loss 0.0454954, acc 0.984375, learning_rate 0.000100978
2017-09-29T13:39:49.845907: step 2085, loss 0.0323369, acc 1, learning_rate 0.000100974
2017-09-29T13:39:50.032506: step 2086, loss 0.0230699, acc 1, learning_rate 0.00010097
2017-09-29T13:39:50.226693: step 2087, loss 0.0199959, acc 1, learning_rate 0.000100966
2017-09-29T13:39:50.414712: step 2088, loss 0.0332706, acc 1, learning_rate 0.000100962
2017-09-29T13:39:50.603936: step 2089, loss 0.128291, acc 0.96875, learning_rate 0.000100958
2017-09-29T13:39:50.789782: step 2090, loss 0.043021, acc 1, learning_rate 0.000100954
2017-09-29T13:39:50.976333: step 2091, loss 0.0785812, acc 0.96875, learning_rate 0.00010095
2017-09-29T13:39:51.161150: step 2092, loss 0.0779896, acc 0.984375, learning_rate 0.000100946
2017-09-29T13:39:51.347414: step 2093, loss 0.0637882, acc 0.984375, learning_rate 0.000100942
2017-09-29T13:39:51.532774: step 2094, loss 0.069555, acc 0.96875, learning_rate 0.000100938
2017-09-29T13:39:51.717436: step 2095, loss 0.0365703, acc 1, learning_rate 0.000100935
2017-09-29T13:39:51.899062: step 2096, loss 0.0898487, acc 0.96875, learning_rate 0.000100931
2017-09-29T13:39:52.087500: step 2097, loss 0.0931998, acc 0.96875, learning_rate 0.000100927
2017-09-29T13:39:52.270336: step 2098, loss 0.0501778, acc 1, learning_rate 0.000100923
2017-09-29T13:39:52.472187: step 2099, loss 0.101616, acc 0.953125, learning_rate 0.000100919
2017-09-29T13:39:52.655942: step 2100, loss 0.0738883, acc 0.96875, learning_rate 0.000100916
2017-09-29T13:39:52.838736: step 2101, loss 0.0169697, acc 1, learning_rate 0.000100912
2017-09-29T13:39:53.034542: step 2102, loss 0.0357731, acc 0.984375, learning_rate 0.000100908
2017-09-29T13:39:53.216210: step 2103, loss 0.0459668, acc 0.984375, learning_rate 0.000100904
2017-09-29T13:39:53.404519: step 2104, loss 0.0681465, acc 0.984375, learning_rate 0.000100901
2017-09-29T13:39:53.589139: step 2105, loss 0.0386001, acc 1, learning_rate 0.000100897
2017-09-29T13:39:53.779661: step 2106, loss 0.0553163, acc 0.96875, learning_rate 0.000100893
2017-09-29T13:39:53.966676: step 2107, loss 0.0319193, acc 1, learning_rate 0.00010089
2017-09-29T13:39:54.150946: step 2108, loss 0.036437, acc 0.984375, learning_rate 0.000100886
2017-09-29T13:39:54.334986: step 2109, loss 0.0652419, acc 0.984375, learning_rate 0.000100883
2017-09-29T13:39:54.519436: step 2110, loss 0.0650131, acc 0.96875, learning_rate 0.000100879
2017-09-29T13:39:54.702072: step 2111, loss 0.0631068, acc 0.984375, learning_rate 0.000100875
2017-09-29T13:39:54.887247: step 2112, loss 0.0588653, acc 0.96875, learning_rate 0.000100872
2017-09-29T13:39:55.080447: step 2113, loss 0.0694517, acc 0.96875, learning_rate 0.000100868
2017-09-29T13:39:55.266839: step 2114, loss 0.120627, acc 0.953125, learning_rate 0.000100865
2017-09-29T13:39:55.454917: step 2115, loss 0.119291, acc 0.953125, learning_rate 0.000100861
2017-09-29T13:39:55.638495: step 2116, loss 0.0391718, acc 1, learning_rate 0.000100858
2017-09-29T13:39:55.823465: step 2117, loss 0.0923508, acc 0.984375, learning_rate 0.000100854
2017-09-29T13:39:56.005437: step 2118, loss 0.0666555, acc 0.984375, learning_rate 0.000100851
2017-09-29T13:39:56.190631: step 2119, loss 0.0119861, acc 1, learning_rate 0.000100847
2017-09-29T13:39:56.374687: step 2120, loss 0.0305745, acc 1, learning_rate 0.000100844

Evaluation:
2017-09-29T13:39:56.918080: step 2120, loss 0.217436, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2120

2017-09-29T13:39:57.613306: step 2121, loss 0.0591482, acc 0.984375, learning_rate 0.00010084
2017-09-29T13:39:57.798485: step 2122, loss 0.115442, acc 0.96875, learning_rate 0.000100837
2017-09-29T13:39:57.999087: step 2123, loss 0.0668757, acc 0.984375, learning_rate 0.000100833
2017-09-29T13:39:58.182471: step 2124, loss 0.0442422, acc 0.984375, learning_rate 0.00010083
2017-09-29T13:39:58.367115: step 2125, loss 0.0808602, acc 0.96875, learning_rate 0.000100827
2017-09-29T13:39:58.554912: step 2126, loss 0.0873964, acc 0.953125, learning_rate 0.000100823
2017-09-29T13:39:58.742992: step 2127, loss 0.0203961, acc 1, learning_rate 0.00010082
2017-09-29T13:39:58.935003: step 2128, loss 0.0484092, acc 1, learning_rate 0.000100817
2017-09-29T13:39:59.127065: step 2129, loss 0.0153818, acc 1, learning_rate 0.000100813
2017-09-29T13:39:59.317695: step 2130, loss 0.094711, acc 0.96875, learning_rate 0.00010081
2017-09-29T13:39:59.511324: step 2131, loss 0.0368842, acc 0.984375, learning_rate 0.000100807
2017-09-29T13:39:59.702504: step 2132, loss 0.0822314, acc 0.96875, learning_rate 0.000100803
2017-09-29T13:39:59.891571: step 2133, loss 0.0341917, acc 1, learning_rate 0.0001008
2017-09-29T13:40:00.079020: step 2134, loss 0.0375235, acc 1, learning_rate 0.000100797
2017-09-29T13:40:00.261979: step 2135, loss 0.097978, acc 0.984375, learning_rate 0.000100793
2017-09-29T13:40:00.450488: step 2136, loss 0.0564732, acc 0.984375, learning_rate 0.00010079
2017-09-29T13:40:00.634823: step 2137, loss 0.0412682, acc 1, learning_rate 0.000100787
2017-09-29T13:40:00.815883: step 2138, loss 0.0267723, acc 1, learning_rate 0.000100784
2017-09-29T13:40:00.999689: step 2139, loss 0.0723351, acc 0.984375, learning_rate 0.000100781
2017-09-29T13:40:01.196412: step 2140, loss 0.0478091, acc 1, learning_rate 0.000100777
2017-09-29T13:40:01.385599: step 2141, loss 0.0580297, acc 0.96875, learning_rate 0.000100774
2017-09-29T13:40:01.592049: step 2142, loss 0.0583654, acc 0.984375, learning_rate 0.000100771
2017-09-29T13:40:01.781782: step 2143, loss 0.101675, acc 0.96875, learning_rate 0.000100768
2017-09-29T13:40:01.972436: step 2144, loss 0.0354736, acc 1, learning_rate 0.000100765
2017-09-29T13:40:02.158772: step 2145, loss 0.128687, acc 0.9375, learning_rate 0.000100762
2017-09-29T13:40:02.344816: step 2146, loss 0.0525461, acc 1, learning_rate 0.000100759
2017-09-29T13:40:02.546113: step 2147, loss 0.0371266, acc 1, learning_rate 0.000100755
2017-09-29T13:40:02.735192: step 2148, loss 0.057636, acc 0.96875, learning_rate 0.000100752
2017-09-29T13:40:02.919214: step 2149, loss 0.0950352, acc 0.96875, learning_rate 0.000100749
2017-09-29T13:40:03.104559: step 2150, loss 0.0703116, acc 0.984375, learning_rate 0.000100746
2017-09-29T13:40:03.287766: step 2151, loss 0.0853884, acc 0.984375, learning_rate 0.000100743
2017-09-29T13:40:03.473383: step 2152, loss 0.0524915, acc 0.96875, learning_rate 0.00010074
2017-09-29T13:40:03.654427: step 2153, loss 0.0695292, acc 0.96875, learning_rate 0.000100737
2017-09-29T13:40:03.852354: step 2154, loss 0.0178022, acc 1, learning_rate 0.000100734
2017-09-29T13:40:04.034210: step 2155, loss 0.049355, acc 1, learning_rate 0.000100731
2017-09-29T13:40:04.190856: step 2156, loss 0.0419283, acc 1, learning_rate 0.000100728
2017-09-29T13:40:04.378468: step 2157, loss 0.0805816, acc 0.96875, learning_rate 0.000100725
2017-09-29T13:40:04.571597: step 2158, loss 0.0607889, acc 0.984375, learning_rate 0.000100722
2017-09-29T13:40:04.762536: step 2159, loss 0.0416268, acc 0.984375, learning_rate 0.000100719
2017-09-29T13:40:04.948071: step 2160, loss 0.0844207, acc 0.984375, learning_rate 0.000100716

Evaluation:
2017-09-29T13:40:05.490387: step 2160, loss 0.213613, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2160

2017-09-29T13:40:06.275719: step 2161, loss 0.247132, acc 0.890625, learning_rate 0.000100713
2017-09-29T13:40:06.455171: step 2162, loss 0.11434, acc 0.9375, learning_rate 0.000100711
2017-09-29T13:40:06.641635: step 2163, loss 0.0709749, acc 0.96875, learning_rate 0.000100708
2017-09-29T13:40:06.823523: step 2164, loss 0.0499139, acc 0.984375, learning_rate 0.000100705
2017-09-29T13:40:07.007737: step 2165, loss 0.0574231, acc 0.96875, learning_rate 0.000100702
2017-09-29T13:40:07.188914: step 2166, loss 0.0446441, acc 0.984375, learning_rate 0.000100699
2017-09-29T13:40:07.372217: step 2167, loss 0.0671687, acc 0.96875, learning_rate 0.000100696
2017-09-29T13:40:07.573892: step 2168, loss 0.0838007, acc 0.96875, learning_rate 0.000100693
2017-09-29T13:40:07.758910: step 2169, loss 0.0859767, acc 0.953125, learning_rate 0.00010069
2017-09-29T13:40:07.943324: step 2170, loss 0.0666191, acc 0.984375, learning_rate 0.000100688
2017-09-29T13:40:08.129138: step 2171, loss 0.0460324, acc 0.984375, learning_rate 0.000100685
2017-09-29T13:40:08.321010: step 2172, loss 0.100824, acc 0.9375, learning_rate 0.000100682
2017-09-29T13:40:08.506331: step 2173, loss 0.120228, acc 0.96875, learning_rate 0.000100679
2017-09-29T13:40:08.688624: step 2174, loss 0.0211364, acc 1, learning_rate 0.000100677
2017-09-29T13:40:08.881247: step 2175, loss 0.0508074, acc 0.984375, learning_rate 0.000100674
2017-09-29T13:40:09.067036: step 2176, loss 0.0301186, acc 1, learning_rate 0.000100671
2017-09-29T13:40:09.257268: step 2177, loss 0.0512583, acc 1, learning_rate 0.000100668
2017-09-29T13:40:09.443973: step 2178, loss 0.121191, acc 0.9375, learning_rate 0.000100666
2017-09-29T13:40:09.644816: step 2179, loss 0.0234252, acc 1, learning_rate 0.000100663
2017-09-29T13:40:09.829462: step 2180, loss 0.0621013, acc 0.96875, learning_rate 0.00010066
2017-09-29T13:40:10.013709: step 2181, loss 0.0814831, acc 0.96875, learning_rate 0.000100657
2017-09-29T13:40:10.194992: step 2182, loss 0.0354098, acc 1, learning_rate 0.000100655
2017-09-29T13:40:10.388359: step 2183, loss 0.017611, acc 1, learning_rate 0.000100652
2017-09-29T13:40:10.575329: step 2184, loss 0.0691873, acc 0.984375, learning_rate 0.000100649
2017-09-29T13:40:10.758229: step 2185, loss 0.0464569, acc 1, learning_rate 0.000100647
2017-09-29T13:40:10.939266: step 2186, loss 0.11559, acc 0.953125, learning_rate 0.000100644
2017-09-29T13:40:11.118226: step 2187, loss 0.0630503, acc 0.984375, learning_rate 0.000100641
2017-09-29T13:40:11.300367: step 2188, loss 0.170298, acc 0.9375, learning_rate 0.000100639
2017-09-29T13:40:11.483079: step 2189, loss 0.0646386, acc 0.984375, learning_rate 0.000100636
2017-09-29T13:40:11.671223: step 2190, loss 0.0370816, acc 0.984375, learning_rate 0.000100634
2017-09-29T13:40:11.866581: step 2191, loss 0.0558897, acc 0.984375, learning_rate 0.000100631
2017-09-29T13:40:12.055252: step 2192, loss 0.056476, acc 0.984375, learning_rate 0.000100628
2017-09-29T13:40:12.241580: step 2193, loss 0.0565074, acc 1, learning_rate 0.000100626
2017-09-29T13:40:12.428189: step 2194, loss 0.0334588, acc 0.984375, learning_rate 0.000100623
2017-09-29T13:40:12.622311: step 2195, loss 0.0745164, acc 0.953125, learning_rate 0.000100621
2017-09-29T13:40:12.806807: step 2196, loss 0.0544096, acc 0.984375, learning_rate 0.000100618
2017-09-29T13:40:12.995533: step 2197, loss 0.0426238, acc 1, learning_rate 0.000100616
2017-09-29T13:40:13.184434: step 2198, loss 0.0944675, acc 0.96875, learning_rate 0.000100613
2017-09-29T13:40:13.367440: step 2199, loss 0.0403686, acc 1, learning_rate 0.000100611
2017-09-29T13:40:13.563339: step 2200, loss 0.0566988, acc 0.984375, learning_rate 0.000100608

Evaluation:
2017-09-29T13:40:14.119741: step 2200, loss 0.214089, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2200

2017-09-29T13:40:14.746477: step 2201, loss 0.0289536, acc 1, learning_rate 0.000100606
2017-09-29T13:40:14.932208: step 2202, loss 0.0772671, acc 0.984375, learning_rate 0.000100603
2017-09-29T13:40:15.118409: step 2203, loss 0.052642, acc 0.984375, learning_rate 0.000100601
2017-09-29T13:40:15.305166: step 2204, loss 0.118977, acc 0.96875, learning_rate 0.000100598
2017-09-29T13:40:15.492148: step 2205, loss 0.0307673, acc 0.984375, learning_rate 0.000100596
2017-09-29T13:40:15.680682: step 2206, loss 0.0819982, acc 0.96875, learning_rate 0.000100594
2017-09-29T13:40:15.876780: step 2207, loss 0.036374, acc 1, learning_rate 0.000100591
2017-09-29T13:40:16.056684: step 2208, loss 0.107774, acc 0.96875, learning_rate 0.000100589
2017-09-29T13:40:16.242001: step 2209, loss 0.0980025, acc 0.953125, learning_rate 0.000100586
2017-09-29T13:40:16.424392: step 2210, loss 0.0477607, acc 1, learning_rate 0.000100584
2017-09-29T13:40:16.614910: step 2211, loss 0.0775123, acc 0.96875, learning_rate 0.000100581
2017-09-29T13:40:16.804887: step 2212, loss 0.0632851, acc 0.984375, learning_rate 0.000100579
2017-09-29T13:40:16.987115: step 2213, loss 0.0356945, acc 1, learning_rate 0.000100577
2017-09-29T13:40:17.167379: step 2214, loss 0.0908954, acc 0.96875, learning_rate 0.000100574
2017-09-29T13:40:17.351099: step 2215, loss 0.052429, acc 1, learning_rate 0.000100572
2017-09-29T13:40:17.535503: step 2216, loss 0.0442561, acc 1, learning_rate 0.00010057
2017-09-29T13:40:17.723494: step 2217, loss 0.0586804, acc 1, learning_rate 0.000100567
2017-09-29T13:40:17.905475: step 2218, loss 0.0361278, acc 0.984375, learning_rate 0.000100565
2017-09-29T13:40:18.091096: step 2219, loss 0.0869207, acc 0.96875, learning_rate 0.000100563
2017-09-29T13:40:18.280262: step 2220, loss 0.115541, acc 0.9375, learning_rate 0.00010056
2017-09-29T13:40:18.476494: step 2221, loss 0.0886435, acc 0.96875, learning_rate 0.000100558
2017-09-29T13:40:18.663095: step 2222, loss 0.0409379, acc 0.984375, learning_rate 0.000100556
2017-09-29T13:40:18.847694: step 2223, loss 0.0367973, acc 0.984375, learning_rate 0.000100554
2017-09-29T13:40:19.042506: step 2224, loss 0.0560813, acc 0.984375, learning_rate 0.000100551
2017-09-29T13:40:19.229039: step 2225, loss 0.0277893, acc 1, learning_rate 0.000100549
2017-09-29T13:40:19.417783: step 2226, loss 0.0516926, acc 0.984375, learning_rate 0.000100547
2017-09-29T13:40:19.606506: step 2227, loss 0.0561503, acc 0.984375, learning_rate 0.000100545
2017-09-29T13:40:19.792666: step 2228, loss 0.0524125, acc 0.984375, learning_rate 0.000100542
2017-09-29T13:40:19.976800: step 2229, loss 0.11204, acc 0.953125, learning_rate 0.00010054
2017-09-29T13:40:20.159851: step 2230, loss 0.0164449, acc 1, learning_rate 0.000100538
2017-09-29T13:40:20.344619: step 2231, loss 0.0352549, acc 1, learning_rate 0.000100536
2017-09-29T13:40:20.530775: step 2232, loss 0.0229103, acc 1, learning_rate 0.000100534
2017-09-29T13:40:20.712343: step 2233, loss 0.0438878, acc 0.984375, learning_rate 0.000100531
2017-09-29T13:40:20.898417: step 2234, loss 0.0709096, acc 0.96875, learning_rate 0.000100529
2017-09-29T13:40:21.081999: step 2235, loss 0.0707629, acc 0.96875, learning_rate 0.000100527
2017-09-29T13:40:21.270933: step 2236, loss 0.0423725, acc 0.984375, learning_rate 0.000100525
2017-09-29T13:40:21.456634: step 2237, loss 0.0858027, acc 0.96875, learning_rate 0.000100523
2017-09-29T13:40:21.638705: step 2238, loss 0.0531381, acc 0.984375, learning_rate 0.000100521
2017-09-29T13:40:21.824725: step 2239, loss 0.0840171, acc 0.984375, learning_rate 0.000100519
2017-09-29T13:40:22.008997: step 2240, loss 0.111885, acc 0.96875, learning_rate 0.000100516

Evaluation:
2017-09-29T13:40:22.558772: step 2240, loss 0.212654, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2240

2017-09-29T13:40:23.262919: step 2241, loss 0.0526037, acc 0.984375, learning_rate 0.000100514
2017-09-29T13:40:23.449924: step 2242, loss 0.0706416, acc 0.984375, learning_rate 0.000100512
2017-09-29T13:40:23.637990: step 2243, loss 0.0572887, acc 0.984375, learning_rate 0.00010051
2017-09-29T13:40:23.821096: step 2244, loss 0.0609787, acc 0.984375, learning_rate 0.000100508
2017-09-29T13:40:24.007426: step 2245, loss 0.0374206, acc 0.984375, learning_rate 0.000100506
2017-09-29T13:40:24.201944: step 2246, loss 0.0309521, acc 1, learning_rate 0.000100504
2017-09-29T13:40:24.383429: step 2247, loss 0.137038, acc 0.953125, learning_rate 0.000100502
2017-09-29T13:40:24.566696: step 2248, loss 0.0314068, acc 1, learning_rate 0.0001005
2017-09-29T13:40:24.753969: step 2249, loss 0.0495085, acc 0.984375, learning_rate 0.000100498
2017-09-29T13:40:24.941581: step 2250, loss 0.0865261, acc 0.96875, learning_rate 0.000100496
2017-09-29T13:40:25.126254: step 2251, loss 0.00967299, acc 1, learning_rate 0.000100494
2017-09-29T13:40:25.309079: step 2252, loss 0.0852776, acc 0.984375, learning_rate 0.000100492
2017-09-29T13:40:25.516991: step 2253, loss 0.0215474, acc 1, learning_rate 0.00010049
2017-09-29T13:40:25.665448: step 2254, loss 0.109481, acc 0.960784, learning_rate 0.000100488
2017-09-29T13:40:25.852934: step 2255, loss 0.0727096, acc 0.96875, learning_rate 0.000100486
2017-09-29T13:40:26.035273: step 2256, loss 0.0266285, acc 1, learning_rate 0.000100484
2017-09-29T13:40:26.219413: step 2257, loss 0.0398282, acc 1, learning_rate 0.000100482
2017-09-29T13:40:26.416113: step 2258, loss 0.0606108, acc 0.984375, learning_rate 0.00010048
2017-09-29T13:40:26.602237: step 2259, loss 0.017751, acc 1, learning_rate 0.000100478
2017-09-29T13:40:26.792415: step 2260, loss 0.0244614, acc 1, learning_rate 0.000100476
2017-09-29T13:40:26.980604: step 2261, loss 0.0876356, acc 0.984375, learning_rate 0.000100474
2017-09-29T13:40:27.177177: step 2262, loss 0.0945405, acc 0.953125, learning_rate 0.000100472
2017-09-29T13:40:27.367557: step 2263, loss 0.011338, acc 1, learning_rate 0.00010047
2017-09-29T13:40:27.556853: step 2264, loss 0.0290975, acc 1, learning_rate 0.000100468
2017-09-29T13:40:27.739250: step 2265, loss 0.0572188, acc 0.984375, learning_rate 0.000100466
2017-09-29T13:40:27.923278: step 2266, loss 0.0133637, acc 1, learning_rate 0.000100464
2017-09-29T13:40:28.104286: step 2267, loss 0.0402843, acc 1, learning_rate 0.000100462
2017-09-29T13:40:28.287636: step 2268, loss 0.0893475, acc 0.96875, learning_rate 0.000100461
2017-09-29T13:40:28.477899: step 2269, loss 0.0403755, acc 0.96875, learning_rate 0.000100459
2017-09-29T13:40:28.666744: step 2270, loss 0.0507809, acc 0.96875, learning_rate 0.000100457
2017-09-29T13:40:28.850337: step 2271, loss 0.0869746, acc 0.984375, learning_rate 0.000100455
2017-09-29T13:40:29.038041: step 2272, loss 0.0576794, acc 0.984375, learning_rate 0.000100453
2017-09-29T13:40:29.224398: step 2273, loss 0.0434909, acc 0.984375, learning_rate 0.000100451
2017-09-29T13:40:29.417518: step 2274, loss 0.0291139, acc 1, learning_rate 0.000100449
2017-09-29T13:40:29.609714: step 2275, loss 0.0760255, acc 0.953125, learning_rate 0.000100448
2017-09-29T13:40:29.811421: step 2276, loss 0.119167, acc 0.9375, learning_rate 0.000100446
2017-09-29T13:40:29.996466: step 2277, loss 0.115089, acc 0.96875, learning_rate 0.000100444
2017-09-29T13:40:30.184836: step 2278, loss 0.132836, acc 0.953125, learning_rate 0.000100442
2017-09-29T13:40:30.371802: step 2279, loss 0.0739296, acc 0.984375, learning_rate 0.00010044
2017-09-29T13:40:30.574920: step 2280, loss 0.0444494, acc 0.984375, learning_rate 0.000100439

Evaluation:
2017-09-29T13:40:31.121726: step 2280, loss 0.210965, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2280

2017-09-29T13:40:31.932334: step 2281, loss 0.0923102, acc 0.96875, learning_rate 0.000100437
2017-09-29T13:40:32.120506: step 2282, loss 0.0813926, acc 0.96875, learning_rate 0.000100435
2017-09-29T13:40:32.304674: step 2283, loss 0.08308, acc 0.96875, learning_rate 0.000100433
2017-09-29T13:40:32.504111: step 2284, loss 0.0731671, acc 0.96875, learning_rate 0.000100431
2017-09-29T13:40:32.701215: step 2285, loss 0.0392772, acc 0.984375, learning_rate 0.00010043
2017-09-29T13:40:32.883368: step 2286, loss 0.0494139, acc 0.984375, learning_rate 0.000100428
2017-09-29T13:40:33.065754: step 2287, loss 0.0237273, acc 1, learning_rate 0.000100426
2017-09-29T13:40:33.247547: step 2288, loss 0.0485262, acc 1, learning_rate 0.000100424
2017-09-29T13:40:33.435149: step 2289, loss 0.0744391, acc 0.96875, learning_rate 0.000100423
2017-09-29T13:40:33.620218: step 2290, loss 0.0944117, acc 0.953125, learning_rate 0.000100421
2017-09-29T13:40:33.805289: step 2291, loss 0.145773, acc 0.984375, learning_rate 0.000100419
2017-09-29T13:40:33.990665: step 2292, loss 0.0487497, acc 0.984375, learning_rate 0.000100418
2017-09-29T13:40:34.177802: step 2293, loss 0.0653405, acc 0.96875, learning_rate 0.000100416
2017-09-29T13:40:34.360725: step 2294, loss 0.0546962, acc 0.984375, learning_rate 0.000100414
2017-09-29T13:40:34.547538: step 2295, loss 0.0688412, acc 0.96875, learning_rate 0.000100412
2017-09-29T13:40:34.732442: step 2296, loss 0.112169, acc 0.921875, learning_rate 0.000100411
2017-09-29T13:40:34.916034: step 2297, loss 0.0808206, acc 0.953125, learning_rate 0.000100409
2017-09-29T13:40:35.098544: step 2298, loss 0.029779, acc 1, learning_rate 0.000100407
2017-09-29T13:40:35.281641: step 2299, loss 0.139953, acc 0.9375, learning_rate 0.000100406
2017-09-29T13:40:35.471153: step 2300, loss 0.0211581, acc 1, learning_rate 0.000100404
2017-09-29T13:40:35.656814: step 2301, loss 0.0609336, acc 0.96875, learning_rate 0.000100402
2017-09-29T13:40:35.844453: step 2302, loss 0.0805237, acc 0.984375, learning_rate 0.000100401
2017-09-29T13:40:36.028521: step 2303, loss 0.0386341, acc 0.984375, learning_rate 0.000100399
2017-09-29T13:40:36.213965: step 2304, loss 0.0505687, acc 0.984375, learning_rate 0.000100398
2017-09-29T13:40:36.398464: step 2305, loss 0.101987, acc 0.984375, learning_rate 0.000100396
2017-09-29T13:40:36.600745: step 2306, loss 0.0216573, acc 1, learning_rate 0.000100394
2017-09-29T13:40:36.784547: step 2307, loss 0.0362324, acc 1, learning_rate 0.000100393
2017-09-29T13:40:36.969888: step 2308, loss 0.128155, acc 0.953125, learning_rate 0.000100391
2017-09-29T13:40:37.158392: step 2309, loss 0.0585859, acc 0.984375, learning_rate 0.000100389
2017-09-29T13:40:37.342784: step 2310, loss 0.0445614, acc 0.984375, learning_rate 0.000100388
2017-09-29T13:40:37.526812: step 2311, loss 0.108426, acc 0.96875, learning_rate 0.000100386
2017-09-29T13:40:37.711101: step 2312, loss 0.0356344, acc 1, learning_rate 0.000100385
2017-09-29T13:40:37.892709: step 2313, loss 0.0134997, acc 1, learning_rate 0.000100383
2017-09-29T13:40:38.075143: step 2314, loss 0.0420606, acc 1, learning_rate 0.000100382
2017-09-29T13:40:38.264536: step 2315, loss 0.102162, acc 0.953125, learning_rate 0.00010038
2017-09-29T13:40:38.452754: step 2316, loss 0.162008, acc 0.921875, learning_rate 0.000100378
2017-09-29T13:40:38.637546: step 2317, loss 0.103203, acc 0.9375, learning_rate 0.000100377
2017-09-29T13:40:38.823715: step 2318, loss 0.0607489, acc 0.984375, learning_rate 0.000100375
2017-09-29T13:40:39.012923: step 2319, loss 0.0225453, acc 1, learning_rate 0.000100374
2017-09-29T13:40:39.196784: step 2320, loss 0.0462631, acc 0.984375, learning_rate 0.000100372

Evaluation:
2017-09-29T13:40:39.748321: step 2320, loss 0.212561, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2320

2017-09-29T13:40:40.383789: step 2321, loss 0.0668374, acc 0.984375, learning_rate 0.000100371
2017-09-29T13:40:40.572259: step 2322, loss 0.0871625, acc 0.96875, learning_rate 0.000100369
2017-09-29T13:40:40.759081: step 2323, loss 0.0551768, acc 0.984375, learning_rate 0.000100368
2017-09-29T13:40:40.942429: step 2324, loss 0.0345188, acc 1, learning_rate 0.000100366
2017-09-29T13:40:41.124900: step 2325, loss 0.0428315, acc 1, learning_rate 0.000100365
2017-09-29T13:40:41.307912: step 2326, loss 0.0383142, acc 0.984375, learning_rate 0.000100363
2017-09-29T13:40:41.503466: step 2327, loss 0.0105104, acc 1, learning_rate 0.000100362
2017-09-29T13:40:41.689485: step 2328, loss 0.069395, acc 0.984375, learning_rate 0.00010036
2017-09-29T13:40:41.874206: step 2329, loss 0.0681409, acc 0.96875, learning_rate 0.000100359
2017-09-29T13:40:42.056241: step 2330, loss 0.0591095, acc 0.96875, learning_rate 0.000100357
2017-09-29T13:40:42.238502: step 2331, loss 0.0707226, acc 0.984375, learning_rate 0.000100356
2017-09-29T13:40:42.437373: step 2332, loss 0.0764042, acc 0.96875, learning_rate 0.000100354
2017-09-29T13:40:42.628390: step 2333, loss 0.152225, acc 0.9375, learning_rate 0.000100353
2017-09-29T13:40:42.814778: step 2334, loss 0.0150475, acc 1, learning_rate 0.000100352
2017-09-29T13:40:42.998141: step 2335, loss 0.133556, acc 0.953125, learning_rate 0.00010035
2017-09-29T13:40:43.179573: step 2336, loss 0.0350047, acc 0.984375, learning_rate 0.000100349
2017-09-29T13:40:43.361399: step 2337, loss 0.0402272, acc 1, learning_rate 0.000100347
2017-09-29T13:40:43.547637: step 2338, loss 0.0208979, acc 1, learning_rate 0.000100346
2017-09-29T13:40:43.737236: step 2339, loss 0.07873, acc 0.984375, learning_rate 0.000100344
2017-09-29T13:40:43.931437: step 2340, loss 0.0801209, acc 0.96875, learning_rate 0.000100343
2017-09-29T13:40:44.122430: step 2341, loss 0.0193324, acc 1, learning_rate 0.000100342
2017-09-29T13:40:44.302907: step 2342, loss 0.0449769, acc 0.984375, learning_rate 0.00010034
2017-09-29T13:40:44.495895: step 2343, loss 0.0586091, acc 0.984375, learning_rate 0.000100339
2017-09-29T13:40:44.681239: step 2344, loss 0.0591568, acc 1, learning_rate 0.000100338
2017-09-29T13:40:44.863505: step 2345, loss 0.0361504, acc 1, learning_rate 0.000100336
2017-09-29T13:40:45.049170: step 2346, loss 0.0316531, acc 1, learning_rate 0.000100335
2017-09-29T13:40:45.229973: step 2347, loss 0.0335429, acc 0.984375, learning_rate 0.000100333
2017-09-29T13:40:45.424257: step 2348, loss 0.0330867, acc 0.984375, learning_rate 0.000100332
2017-09-29T13:40:45.615735: step 2349, loss 0.147615, acc 0.953125, learning_rate 0.000100331
2017-09-29T13:40:45.806992: step 2350, loss 0.0463341, acc 0.984375, learning_rate 0.000100329
2017-09-29T13:40:46.000544: step 2351, loss 0.12893, acc 0.953125, learning_rate 0.000100328
2017-09-29T13:40:46.156304: step 2352, loss 0.0175637, acc 1, learning_rate 0.000100327
2017-09-29T13:40:46.348908: step 2353, loss 0.0533648, acc 0.984375, learning_rate 0.000100325
2017-09-29T13:40:46.534946: step 2354, loss 0.0562418, acc 0.96875, learning_rate 0.000100324
2017-09-29T13:40:46.719762: step 2355, loss 0.045551, acc 0.984375, learning_rate 0.000100323
2017-09-29T13:40:46.906359: step 2356, loss 0.037957, acc 1, learning_rate 0.000100321
2017-09-29T13:40:47.091287: step 2357, loss 0.0755099, acc 0.96875, learning_rate 0.00010032
2017-09-29T13:40:47.275057: step 2358, loss 0.0529492, acc 0.96875, learning_rate 0.000100319
2017-09-29T13:40:47.467862: step 2359, loss 0.0315002, acc 0.984375, learning_rate 0.000100317
2017-09-29T13:40:47.664127: step 2360, loss 0.0362752, acc 1, learning_rate 0.000100316

Evaluation:
2017-09-29T13:40:48.220440: step 2360, loss 0.21522, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2360

2017-09-29T13:40:48.925421: step 2361, loss 0.0558538, acc 0.984375, learning_rate 0.000100315
2017-09-29T13:40:49.120318: step 2362, loss 0.0343524, acc 0.984375, learning_rate 0.000100314
2017-09-29T13:40:49.309891: step 2363, loss 0.0165009, acc 1, learning_rate 0.000100312
2017-09-29T13:40:49.499542: step 2364, loss 0.0787498, acc 0.96875, learning_rate 0.000100311
2017-09-29T13:40:49.683061: step 2365, loss 0.0809341, acc 0.984375, learning_rate 0.00010031
2017-09-29T13:40:49.865835: step 2366, loss 0.0781191, acc 0.96875, learning_rate 0.000100308
2017-09-29T13:40:50.047760: step 2367, loss 0.0730295, acc 0.984375, learning_rate 0.000100307
2017-09-29T13:40:50.230607: step 2368, loss 0.0341137, acc 1, learning_rate 0.000100306
2017-09-29T13:40:50.413947: step 2369, loss 0.0881178, acc 0.984375, learning_rate 0.000100305
2017-09-29T13:40:50.601259: step 2370, loss 0.0898667, acc 0.984375, learning_rate 0.000100303
2017-09-29T13:40:50.787152: step 2371, loss 0.0868943, acc 0.984375, learning_rate 0.000100302
2017-09-29T13:40:50.977848: step 2372, loss 0.078968, acc 0.984375, learning_rate 0.000100301
2017-09-29T13:40:51.169098: step 2373, loss 0.0584471, acc 0.984375, learning_rate 0.0001003
2017-09-29T13:40:51.363978: step 2374, loss 0.0414789, acc 1, learning_rate 0.000100299
2017-09-29T13:40:51.555045: step 2375, loss 0.0443718, acc 1, learning_rate 0.000100297
2017-09-29T13:40:51.746764: step 2376, loss 0.0307832, acc 0.984375, learning_rate 0.000100296
2017-09-29T13:40:51.935328: step 2377, loss 0.05564, acc 0.96875, learning_rate 0.000100295
2017-09-29T13:40:52.127187: step 2378, loss 0.0166155, acc 1, learning_rate 0.000100294
2017-09-29T13:40:52.314977: step 2379, loss 0.0631736, acc 0.96875, learning_rate 0.000100292
2017-09-29T13:40:52.526265: step 2380, loss 0.0683146, acc 0.984375, learning_rate 0.000100291
2017-09-29T13:40:52.711902: step 2381, loss 0.0603044, acc 0.984375, learning_rate 0.00010029
2017-09-29T13:40:52.895912: step 2382, loss 0.0837903, acc 0.984375, learning_rate 0.000100289
2017-09-29T13:40:53.086649: step 2383, loss 0.0344058, acc 0.984375, learning_rate 0.000100288
2017-09-29T13:40:53.270861: step 2384, loss 0.0286801, acc 1, learning_rate 0.000100287
2017-09-29T13:40:53.453657: step 2385, loss 0.0642997, acc 0.984375, learning_rate 0.000100285
2017-09-29T13:40:53.639457: step 2386, loss 0.0785338, acc 0.96875, learning_rate 0.000100284
2017-09-29T13:40:53.824641: step 2387, loss 0.117556, acc 0.953125, learning_rate 0.000100283
2017-09-29T13:40:54.017333: step 2388, loss 0.0761813, acc 0.96875, learning_rate 0.000100282
2017-09-29T13:40:54.209975: step 2389, loss 0.0863136, acc 0.953125, learning_rate 0.000100281
2017-09-29T13:40:54.397024: step 2390, loss 0.0461191, acc 0.984375, learning_rate 0.00010028
2017-09-29T13:40:54.582659: step 2391, loss 0.0650658, acc 0.984375, learning_rate 0.000100278
2017-09-29T13:40:54.763487: step 2392, loss 0.0247111, acc 1, learning_rate 0.000100277
2017-09-29T13:40:54.947743: step 2393, loss 0.112046, acc 0.953125, learning_rate 0.000100276
2017-09-29T13:40:55.132281: step 2394, loss 0.0558029, acc 0.984375, learning_rate 0.000100275
2017-09-29T13:40:55.318755: step 2395, loss 0.0235999, acc 1, learning_rate 0.000100274
2017-09-29T13:40:55.498781: step 2396, loss 0.0284564, acc 1, learning_rate 0.000100273
2017-09-29T13:40:55.684713: step 2397, loss 0.10948, acc 0.984375, learning_rate 0.000100272
2017-09-29T13:40:55.871935: step 2398, loss 0.0711084, acc 0.984375, learning_rate 0.000100271
2017-09-29T13:40:56.056538: step 2399, loss 0.0453357, acc 0.984375, learning_rate 0.00010027
2017-09-29T13:40:56.237612: step 2400, loss 0.0333683, acc 1, learning_rate 0.000100268

Evaluation:
2017-09-29T13:40:56.786053: step 2400, loss 0.212445, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2400

2017-09-29T13:40:57.490196: step 2401, loss 0.0215626, acc 1, learning_rate 0.000100267
2017-09-29T13:40:57.675167: step 2402, loss 0.0851287, acc 0.96875, learning_rate 0.000100266
2017-09-29T13:40:57.862471: step 2403, loss 0.0533035, acc 1, learning_rate 0.000100265
2017-09-29T13:40:58.043965: step 2404, loss 0.107124, acc 0.96875, learning_rate 0.000100264
2017-09-29T13:40:58.231811: step 2405, loss 0.0196981, acc 1, learning_rate 0.000100263
2017-09-29T13:40:58.425515: step 2406, loss 0.101896, acc 0.953125, learning_rate 0.000100262
2017-09-29T13:40:58.613554: step 2407, loss 0.0200817, acc 1, learning_rate 0.000100261
2017-09-29T13:40:58.798903: step 2408, loss 0.102443, acc 0.9375, learning_rate 0.00010026
2017-09-29T13:40:58.982207: step 2409, loss 0.0812186, acc 0.96875, learning_rate 0.000100259
2017-09-29T13:40:59.168531: step 2410, loss 0.117779, acc 0.9375, learning_rate 0.000100258
2017-09-29T13:40:59.351183: step 2411, loss 0.0581188, acc 0.984375, learning_rate 0.000100257
2017-09-29T13:40:59.538426: step 2412, loss 0.0791588, acc 0.96875, learning_rate 0.000100256
2017-09-29T13:40:59.723939: step 2413, loss 0.0762775, acc 0.984375, learning_rate 0.000100255
2017-09-29T13:40:59.908952: step 2414, loss 0.0603122, acc 0.984375, learning_rate 0.000100253
2017-09-29T13:41:00.092424: step 2415, loss 0.110586, acc 0.953125, learning_rate 0.000100252
2017-09-29T13:41:00.276846: step 2416, loss 0.0878788, acc 0.984375, learning_rate 0.000100251
2017-09-29T13:41:00.460659: step 2417, loss 0.0808776, acc 0.96875, learning_rate 0.00010025
2017-09-29T13:41:00.643421: step 2418, loss 0.0434797, acc 1, learning_rate 0.000100249
2017-09-29T13:41:00.829744: step 2419, loss 0.0517873, acc 0.96875, learning_rate 0.000100248
2017-09-29T13:41:01.014171: step 2420, loss 0.0241101, acc 0.984375, learning_rate 0.000100247
2017-09-29T13:41:01.198462: step 2421, loss 0.0618221, acc 0.96875, learning_rate 0.000100246
2017-09-29T13:41:01.382051: step 2422, loss 0.0332398, acc 0.984375, learning_rate 0.000100245
2017-09-29T13:41:01.566293: step 2423, loss 0.0308754, acc 1, learning_rate 0.000100244
2017-09-29T13:41:01.748972: step 2424, loss 0.0446534, acc 1, learning_rate 0.000100243
2017-09-29T13:41:01.931475: step 2425, loss 0.0330261, acc 1, learning_rate 0.000100242
2017-09-29T13:41:02.113474: step 2426, loss 0.116956, acc 0.96875, learning_rate 0.000100241
2017-09-29T13:41:02.301036: step 2427, loss 0.0294259, acc 1, learning_rate 0.00010024
2017-09-29T13:41:02.493852: step 2428, loss 0.101125, acc 0.953125, learning_rate 0.000100239
2017-09-29T13:41:02.684106: step 2429, loss 0.0661035, acc 1, learning_rate 0.000100238
2017-09-29T13:41:02.869034: step 2430, loss 0.0936305, acc 0.953125, learning_rate 0.000100237
2017-09-29T13:41:03.053975: step 2431, loss 0.0419553, acc 1, learning_rate 0.000100236
2017-09-29T13:41:03.238459: step 2432, loss 0.058882, acc 0.96875, learning_rate 0.000100235
2017-09-29T13:41:03.421439: step 2433, loss 0.223809, acc 0.9375, learning_rate 0.000100235
2017-09-29T13:41:03.613274: step 2434, loss 0.013496, acc 1, learning_rate 0.000100234
2017-09-29T13:41:03.798059: step 2435, loss 0.0492718, acc 1, learning_rate 0.000100233
2017-09-29T13:41:03.985544: step 2436, loss 0.0519342, acc 1, learning_rate 0.000100232
2017-09-29T13:41:04.177216: step 2437, loss 0.0785381, acc 0.984375, learning_rate 0.000100231
2017-09-29T13:41:04.363491: step 2438, loss 0.0463874, acc 1, learning_rate 0.00010023
2017-09-29T13:41:04.573942: step 2439, loss 0.0900532, acc 0.96875, learning_rate 0.000100229
2017-09-29T13:41:04.777879: step 2440, loss 0.0433128, acc 1, learning_rate 0.000100228

Evaluation:
2017-09-29T13:41:05.338999: step 2440, loss 0.21648, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2440

2017-09-29T13:41:06.135251: step 2441, loss 0.106812, acc 0.96875, learning_rate 0.000100227
2017-09-29T13:41:06.320519: step 2442, loss 0.0313074, acc 1, learning_rate 0.000100226
2017-09-29T13:41:06.505988: step 2443, loss 0.0500548, acc 0.984375, learning_rate 0.000100225
2017-09-29T13:41:06.696989: step 2444, loss 0.110212, acc 0.953125, learning_rate 0.000100224
2017-09-29T13:41:06.886105: step 2445, loss 0.0754141, acc 0.96875, learning_rate 0.000100223
2017-09-29T13:41:07.069344: step 2446, loss 0.0395673, acc 1, learning_rate 0.000100222
2017-09-29T13:41:07.254182: step 2447, loss 0.126069, acc 0.953125, learning_rate 0.000100221
2017-09-29T13:41:07.440621: step 2448, loss 0.0645833, acc 0.96875, learning_rate 0.000100221
2017-09-29T13:41:07.631598: step 2449, loss 0.0329641, acc 1, learning_rate 0.00010022
2017-09-29T13:41:07.787332: step 2450, loss 0.0463417, acc 0.980392, learning_rate 0.000100219
2017-09-29T13:41:07.982703: step 2451, loss 0.0383449, acc 0.984375, learning_rate 0.000100218
2017-09-29T13:41:08.170329: step 2452, loss 0.100184, acc 0.96875, learning_rate 0.000100217
2017-09-29T13:41:08.357689: step 2453, loss 0.0774387, acc 0.953125, learning_rate 0.000100216
2017-09-29T13:41:08.546196: step 2454, loss 0.0745743, acc 0.953125, learning_rate 0.000100215
2017-09-29T13:41:08.736543: step 2455, loss 0.0512588, acc 0.984375, learning_rate 0.000100214
2017-09-29T13:41:08.928014: step 2456, loss 0.0151353, acc 1, learning_rate 0.000100213
2017-09-29T13:41:09.118454: step 2457, loss 0.051041, acc 0.984375, learning_rate 0.000100213
2017-09-29T13:41:09.308834: step 2458, loss 0.0301374, acc 1, learning_rate 0.000100212
2017-09-29T13:41:09.503835: step 2459, loss 0.12698, acc 0.953125, learning_rate 0.000100211
2017-09-29T13:41:09.693555: step 2460, loss 0.0230054, acc 1, learning_rate 0.00010021
2017-09-29T13:41:09.882939: step 2461, loss 0.0450234, acc 0.984375, learning_rate 0.000100209
2017-09-29T13:41:10.064610: step 2462, loss 0.0248608, acc 1, learning_rate 0.000100208
2017-09-29T13:41:10.250794: step 2463, loss 0.0585944, acc 0.984375, learning_rate 0.000100207
2017-09-29T13:41:10.440913: step 2464, loss 0.0995114, acc 0.96875, learning_rate 0.000100207
2017-09-29T13:41:10.644504: step 2465, loss 0.0807978, acc 0.984375, learning_rate 0.000100206
2017-09-29T13:41:10.832594: step 2466, loss 0.0596253, acc 0.984375, learning_rate 0.000100205
2017-09-29T13:41:11.023149: step 2467, loss 0.0899073, acc 0.984375, learning_rate 0.000100204
2017-09-29T13:41:11.210389: step 2468, loss 0.0186169, acc 1, learning_rate 0.000100203
2017-09-29T13:41:11.397789: step 2469, loss 0.0698848, acc 0.96875, learning_rate 0.000100202
2017-09-29T13:41:11.584176: step 2470, loss 0.114046, acc 0.9375, learning_rate 0.000100202
2017-09-29T13:41:11.766322: step 2471, loss 0.0389798, acc 0.984375, learning_rate 0.000100201
2017-09-29T13:41:11.946631: step 2472, loss 0.0776497, acc 0.984375, learning_rate 0.0001002
2017-09-29T13:41:12.131641: step 2473, loss 0.0728271, acc 0.984375, learning_rate 0.000100199
2017-09-29T13:41:12.314057: step 2474, loss 0.0492457, acc 1, learning_rate 0.000100198
2017-09-29T13:41:12.501307: step 2475, loss 0.0741992, acc 0.96875, learning_rate 0.000100198
2017-09-29T13:41:12.694534: step 2476, loss 0.0326292, acc 1, learning_rate 0.000100197
2017-09-29T13:41:12.883437: step 2477, loss 0.0711707, acc 0.953125, learning_rate 0.000100196
2017-09-29T13:41:13.078704: step 2478, loss 0.0751368, acc 0.96875, learning_rate 0.000100195
2017-09-29T13:41:13.265924: step 2479, loss 0.0957801, acc 0.953125, learning_rate 0.000100194
2017-09-29T13:41:13.454257: step 2480, loss 0.0673438, acc 0.96875, learning_rate 0.000100194

Evaluation:
2017-09-29T13:41:14.010815: step 2480, loss 0.213623, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2480

2017-09-29T13:41:14.643939: step 2481, loss 0.0247189, acc 1, learning_rate 0.000100193
2017-09-29T13:41:14.828023: step 2482, loss 0.0776585, acc 0.96875, learning_rate 0.000100192
2017-09-29T13:41:15.014313: step 2483, loss 0.0666417, acc 0.984375, learning_rate 0.000100191
2017-09-29T13:41:15.216371: step 2484, loss 0.0757311, acc 0.96875, learning_rate 0.00010019
2017-09-29T13:41:15.405352: step 2485, loss 0.0287994, acc 0.984375, learning_rate 0.00010019
2017-09-29T13:41:15.608205: step 2486, loss 0.0610611, acc 0.984375, learning_rate 0.000100189
2017-09-29T13:41:15.795510: step 2487, loss 0.0529319, acc 0.984375, learning_rate 0.000100188
2017-09-29T13:41:15.984581: step 2488, loss 0.0175524, acc 1, learning_rate 0.000100187
2017-09-29T13:41:16.175029: step 2489, loss 0.0201202, acc 1, learning_rate 0.000100187
2017-09-29T13:41:16.363666: step 2490, loss 0.0615753, acc 0.984375, learning_rate 0.000100186
2017-09-29T13:41:16.548919: step 2491, loss 0.0509979, acc 0.984375, learning_rate 0.000100185
2017-09-29T13:41:16.735604: step 2492, loss 0.025052, acc 0.984375, learning_rate 0.000100184
2017-09-29T13:41:16.926555: step 2493, loss 0.0372398, acc 1, learning_rate 0.000100183
2017-09-29T13:41:17.120286: step 2494, loss 0.0937436, acc 0.96875, learning_rate 0.000100183
2017-09-29T13:41:17.300964: step 2495, loss 0.0759979, acc 0.96875, learning_rate 0.000100182
2017-09-29T13:41:17.493576: step 2496, loss 0.0315411, acc 1, learning_rate 0.000100181
2017-09-29T13:41:17.687712: step 2497, loss 0.0305068, acc 1, learning_rate 0.000100181
2017-09-29T13:41:17.870709: step 2498, loss 0.0245282, acc 1, learning_rate 0.00010018
2017-09-29T13:41:18.065360: step 2499, loss 0.0171127, acc 1, learning_rate 0.000100179
2017-09-29T13:41:18.267275: step 2500, loss 0.0436388, acc 0.984375, learning_rate 0.000100178
2017-09-29T13:41:18.467416: step 2501, loss 0.0737666, acc 0.984375, learning_rate 0.000100178
2017-09-29T13:41:18.662918: step 2502, loss 0.058826, acc 0.953125, learning_rate 0.000100177
2017-09-29T13:41:18.856253: step 2503, loss 0.0738499, acc 0.96875, learning_rate 0.000100176
2017-09-29T13:41:19.041630: step 2504, loss 0.0217028, acc 1, learning_rate 0.000100175
2017-09-29T13:41:19.225342: step 2505, loss 0.0385078, acc 1, learning_rate 0.000100175
2017-09-29T13:41:19.419879: step 2506, loss 0.0702652, acc 0.984375, learning_rate 0.000100174
2017-09-29T13:41:19.604091: step 2507, loss 0.0920255, acc 0.953125, learning_rate 0.000100173
2017-09-29T13:41:19.784953: step 2508, loss 0.023519, acc 1, learning_rate 0.000100173
2017-09-29T13:41:19.964853: step 2509, loss 0.0680624, acc 0.984375, learning_rate 0.000100172
2017-09-29T13:41:20.151548: step 2510, loss 0.0699126, acc 0.96875, learning_rate 0.000100171
2017-09-29T13:41:20.336453: step 2511, loss 0.0730159, acc 0.984375, learning_rate 0.00010017
2017-09-29T13:41:20.516703: step 2512, loss 0.0370082, acc 1, learning_rate 0.00010017
2017-09-29T13:41:20.699452: step 2513, loss 0.083205, acc 0.96875, learning_rate 0.000100169
2017-09-29T13:41:20.879571: step 2514, loss 0.0575493, acc 0.96875, learning_rate 0.000100168
2017-09-29T13:41:21.067202: step 2515, loss 0.0497491, acc 0.984375, learning_rate 0.000100168
2017-09-29T13:41:21.249869: step 2516, loss 0.105149, acc 0.984375, learning_rate 0.000100167
2017-09-29T13:41:21.437557: step 2517, loss 0.0422093, acc 1, learning_rate 0.000100166
2017-09-29T13:41:21.635764: step 2518, loss 0.0329523, acc 1, learning_rate 0.000100166
2017-09-29T13:41:21.819417: step 2519, loss 0.084288, acc 0.984375, learning_rate 0.000100165
2017-09-29T13:41:22.001381: step 2520, loss 0.0585673, acc 0.984375, learning_rate 0.000100164

Evaluation:
2017-09-29T13:41:22.549534: step 2520, loss 0.209473, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2520

2017-09-29T13:41:23.279472: step 2521, loss 0.0872611, acc 0.96875, learning_rate 0.000100164
2017-09-29T13:41:23.472067: step 2522, loss 0.0474074, acc 1, learning_rate 0.000100163
2017-09-29T13:41:23.666626: step 2523, loss 0.0342003, acc 0.984375, learning_rate 0.000100162
2017-09-29T13:41:23.872344: step 2524, loss 0.116469, acc 0.9375, learning_rate 0.000100162
2017-09-29T13:41:24.064093: step 2525, loss 0.0881961, acc 0.96875, learning_rate 0.000100161
2017-09-29T13:41:24.251934: step 2526, loss 0.0599069, acc 0.96875, learning_rate 0.00010016
2017-09-29T13:41:24.445820: step 2527, loss 0.0600938, acc 0.984375, learning_rate 0.00010016
2017-09-29T13:41:24.631024: step 2528, loss 0.0936259, acc 0.96875, learning_rate 0.000100159
2017-09-29T13:41:24.813668: step 2529, loss 0.0456057, acc 0.984375, learning_rate 0.000100158
2017-09-29T13:41:25.002093: step 2530, loss 0.0333095, acc 0.984375, learning_rate 0.000100158
2017-09-29T13:41:25.182385: step 2531, loss 0.0166364, acc 1, learning_rate 0.000100157
2017-09-29T13:41:25.371667: step 2532, loss 0.11113, acc 0.96875, learning_rate 0.000100156
2017-09-29T13:41:25.566175: step 2533, loss 0.0730588, acc 0.953125, learning_rate 0.000100156
2017-09-29T13:41:25.755230: step 2534, loss 0.0932556, acc 0.96875, learning_rate 0.000100155
2017-09-29T13:41:25.944994: step 2535, loss 0.0840427, acc 0.984375, learning_rate 0.000100155
2017-09-29T13:41:26.131709: step 2536, loss 0.0439559, acc 1, learning_rate 0.000100154
2017-09-29T13:41:26.318225: step 2537, loss 0.0628825, acc 0.96875, learning_rate 0.000100153
2017-09-29T13:41:26.505400: step 2538, loss 0.0203928, acc 1, learning_rate 0.000100153
2017-09-29T13:41:26.694350: step 2539, loss 0.053646, acc 1, learning_rate 0.000100152
2017-09-29T13:41:26.879695: step 2540, loss 0.0289917, acc 1, learning_rate 0.000100151
2017-09-29T13:41:27.066381: step 2541, loss 0.119509, acc 0.9375, learning_rate 0.000100151
2017-09-29T13:41:27.252766: step 2542, loss 0.0735842, acc 0.984375, learning_rate 0.00010015
2017-09-29T13:41:27.440918: step 2543, loss 0.118291, acc 0.9375, learning_rate 0.00010015
2017-09-29T13:41:27.625027: step 2544, loss 0.0629213, acc 0.96875, learning_rate 0.000100149
2017-09-29T13:41:27.809630: step 2545, loss 0.108601, acc 0.953125, learning_rate 0.000100148
2017-09-29T13:41:27.997137: step 2546, loss 0.0596276, acc 0.984375, learning_rate 0.000100148
2017-09-29T13:41:28.188742: step 2547, loss 0.0908987, acc 0.953125, learning_rate 0.000100147
2017-09-29T13:41:28.348398: step 2548, loss 0.0559398, acc 0.980392, learning_rate 0.000100147
2017-09-29T13:41:28.539496: step 2549, loss 0.0810877, acc 0.96875, learning_rate 0.000100146
2017-09-29T13:41:28.727119: step 2550, loss 0.0805138, acc 0.953125, learning_rate 0.000100145
2017-09-29T13:41:28.909481: step 2551, loss 0.0311203, acc 0.984375, learning_rate 0.000100145
2017-09-29T13:41:29.092098: step 2552, loss 0.0292662, acc 1, learning_rate 0.000100144
2017-09-29T13:41:29.279814: step 2553, loss 0.055394, acc 0.984375, learning_rate 0.000100144
2017-09-29T13:41:29.472587: step 2554, loss 0.0867285, acc 0.96875, learning_rate 0.000100143
2017-09-29T13:41:29.657781: step 2555, loss 0.0186495, acc 1, learning_rate 0.000100142
2017-09-29T13:41:29.840618: step 2556, loss 0.0650415, acc 0.96875, learning_rate 0.000100142
2017-09-29T13:41:30.027427: step 2557, loss 0.0805965, acc 0.96875, learning_rate 0.000100141
2017-09-29T13:41:30.210256: step 2558, loss 0.0333859, acc 1, learning_rate 0.000100141
2017-09-29T13:41:30.389204: step 2559, loss 0.0228654, acc 1, learning_rate 0.00010014
2017-09-29T13:41:30.571267: step 2560, loss 0.0442509, acc 1, learning_rate 0.00010014

Evaluation:
2017-09-29T13:41:31.125440: step 2560, loss 0.212847, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2560

2017-09-29T13:41:31.921891: step 2561, loss 0.154731, acc 0.953125, learning_rate 0.000100139
2017-09-29T13:41:32.108961: step 2562, loss 0.131167, acc 0.96875, learning_rate 0.000100138
2017-09-29T13:41:32.292004: step 2563, loss 0.0450294, acc 1, learning_rate 0.000100138
2017-09-29T13:41:32.476907: step 2564, loss 0.0337636, acc 0.984375, learning_rate 0.000100137
2017-09-29T13:41:32.659373: step 2565, loss 0.0772571, acc 0.96875, learning_rate 0.000100137
2017-09-29T13:41:32.840813: step 2566, loss 0.017307, acc 1, learning_rate 0.000100136
2017-09-29T13:41:33.022967: step 2567, loss 0.0476139, acc 1, learning_rate 0.000100136
2017-09-29T13:41:33.209725: step 2568, loss 0.0250112, acc 1, learning_rate 0.000100135
2017-09-29T13:41:33.392007: step 2569, loss 0.0378493, acc 1, learning_rate 0.000100134
2017-09-29T13:41:33.583474: step 2570, loss 0.0484895, acc 0.984375, learning_rate 0.000100134
2017-09-29T13:41:33.776863: step 2571, loss 0.0606617, acc 0.96875, learning_rate 0.000100133
2017-09-29T13:41:33.968364: step 2572, loss 0.0731968, acc 0.984375, learning_rate 0.000100133
2017-09-29T13:41:34.158100: step 2573, loss 0.0356735, acc 0.984375, learning_rate 0.000100132
2017-09-29T13:41:34.344052: step 2574, loss 0.136022, acc 0.96875, learning_rate 0.000100132
2017-09-29T13:41:34.545475: step 2575, loss 0.0716861, acc 0.984375, learning_rate 0.000100131
2017-09-29T13:41:34.728407: step 2576, loss 0.0363381, acc 1, learning_rate 0.000100131
2017-09-29T13:41:34.913482: step 2577, loss 0.062375, acc 0.953125, learning_rate 0.00010013
2017-09-29T13:41:35.090913: step 2578, loss 0.109516, acc 0.96875, learning_rate 0.00010013
2017-09-29T13:41:35.277819: step 2579, loss 0.0729742, acc 0.96875, learning_rate 0.000100129
2017-09-29T13:41:35.457233: step 2580, loss 0.0570985, acc 0.984375, learning_rate 0.000100129
2017-09-29T13:41:35.637909: step 2581, loss 0.0705919, acc 0.96875, learning_rate 0.000100128
2017-09-29T13:41:35.819292: step 2582, loss 0.0394666, acc 0.984375, learning_rate 0.000100128
2017-09-29T13:41:36.005769: step 2583, loss 0.0269912, acc 1, learning_rate 0.000100127
2017-09-29T13:41:36.190022: step 2584, loss 0.0288783, acc 0.984375, learning_rate 0.000100126
2017-09-29T13:41:36.372906: step 2585, loss 0.0434543, acc 0.984375, learning_rate 0.000100126
2017-09-29T13:41:36.567679: step 2586, loss 0.0581147, acc 0.984375, learning_rate 0.000100125
2017-09-29T13:41:36.767099: step 2587, loss 0.0477725, acc 1, learning_rate 0.000100125
2017-09-29T13:41:36.965197: step 2588, loss 0.109044, acc 0.953125, learning_rate 0.000100124
2017-09-29T13:41:37.153453: step 2589, loss 0.0647539, acc 0.96875, learning_rate 0.000100124
2017-09-29T13:41:37.344647: step 2590, loss 0.0272527, acc 1, learning_rate 0.000100123
2017-09-29T13:41:37.527615: step 2591, loss 0.111081, acc 0.953125, learning_rate 0.000100123
2017-09-29T13:41:37.726108: step 2592, loss 0.0912319, acc 0.96875, learning_rate 0.000100122
2017-09-29T13:41:37.908610: step 2593, loss 0.0639378, acc 0.984375, learning_rate 0.000100122
2017-09-29T13:41:38.092737: step 2594, loss 0.11236, acc 0.96875, learning_rate 0.000100121
2017-09-29T13:41:38.274349: step 2595, loss 0.0177235, acc 1, learning_rate 0.000100121
2017-09-29T13:41:38.460750: step 2596, loss 0.0478453, acc 0.984375, learning_rate 0.00010012
2017-09-29T13:41:38.646510: step 2597, loss 0.102183, acc 0.953125, learning_rate 0.00010012
2017-09-29T13:41:38.829652: step 2598, loss 0.0438655, acc 1, learning_rate 0.000100119
2017-09-29T13:41:39.011680: step 2599, loss 0.125479, acc 0.953125, learning_rate 0.000100119
2017-09-29T13:41:39.202495: step 2600, loss 0.084938, acc 0.96875, learning_rate 0.000100118

Evaluation:
2017-09-29T13:41:39.782464: step 2600, loss 0.212483, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2600

2017-09-29T13:41:40.423344: step 2601, loss 0.0149035, acc 1, learning_rate 0.000100118
2017-09-29T13:41:40.608651: step 2602, loss 0.0400999, acc 0.984375, learning_rate 0.000100117
2017-09-29T13:41:40.807218: step 2603, loss 0.0483508, acc 0.984375, learning_rate 0.000100117
2017-09-29T13:41:41.000446: step 2604, loss 0.0233847, acc 1, learning_rate 0.000100117
2017-09-29T13:41:41.184667: step 2605, loss 0.060196, acc 0.984375, learning_rate 0.000100116
2017-09-29T13:41:41.367550: step 2606, loss 0.078314, acc 0.953125, learning_rate 0.000100116
2017-09-29T13:41:41.564095: step 2607, loss 0.160705, acc 0.921875, learning_rate 0.000100115
2017-09-29T13:41:41.749574: step 2608, loss 0.00488936, acc 1, learning_rate 0.000100115
2017-09-29T13:41:41.931767: step 2609, loss 0.0249622, acc 1, learning_rate 0.000100114
2017-09-29T13:41:42.115878: step 2610, loss 0.0571942, acc 0.984375, learning_rate 0.000100114
2017-09-29T13:41:42.303919: step 2611, loss 0.0258218, acc 1, learning_rate 0.000100113
2017-09-29T13:41:42.493510: step 2612, loss 0.0782661, acc 0.96875, learning_rate 0.000100113
2017-09-29T13:41:42.693915: step 2613, loss 0.119132, acc 0.953125, learning_rate 0.000100112
2017-09-29T13:41:42.890068: step 2614, loss 0.0862267, acc 0.984375, learning_rate 0.000100112
2017-09-29T13:41:43.084526: step 2615, loss 0.0452864, acc 0.984375, learning_rate 0.000100111
2017-09-29T13:41:43.268472: step 2616, loss 0.0351763, acc 1, learning_rate 0.000100111
2017-09-29T13:41:43.457692: step 2617, loss 0.0585167, acc 0.984375, learning_rate 0.000100111
2017-09-29T13:41:43.644408: step 2618, loss 0.0335371, acc 1, learning_rate 0.00010011
2017-09-29T13:41:43.830304: step 2619, loss 0.0363227, acc 1, learning_rate 0.00010011
2017-09-29T13:41:44.019067: step 2620, loss 0.0884841, acc 0.984375, learning_rate 0.000100109
2017-09-29T13:41:44.224913: step 2621, loss 0.0961142, acc 0.96875, learning_rate 0.000100109
2017-09-29T13:41:44.413975: step 2622, loss 0.0565122, acc 0.96875, learning_rate 0.000100108
2017-09-29T13:41:44.608692: step 2623, loss 0.0445978, acc 1, learning_rate 0.000100108
2017-09-29T13:41:44.802068: step 2624, loss 0.0513664, acc 0.984375, learning_rate 0.000100107
2017-09-29T13:41:44.982310: step 2625, loss 0.0425911, acc 0.984375, learning_rate 0.000100107
2017-09-29T13:41:45.181877: step 2626, loss 0.0942825, acc 0.96875, learning_rate 0.000100107
2017-09-29T13:41:45.383400: step 2627, loss 0.0749434, acc 0.984375, learning_rate 0.000100106
2017-09-29T13:41:45.593438: step 2628, loss 0.0382434, acc 1, learning_rate 0.000100106
2017-09-29T13:41:45.789695: step 2629, loss 0.103356, acc 0.953125, learning_rate 0.000100105
2017-09-29T13:41:45.983404: step 2630, loss 0.136144, acc 0.9375, learning_rate 0.000100105
2017-09-29T13:41:46.167430: step 2631, loss 0.0713364, acc 0.984375, learning_rate 0.000100104
2017-09-29T13:41:46.349807: step 2632, loss 0.0296298, acc 1, learning_rate 0.000100104
2017-09-29T13:41:46.534788: step 2633, loss 0.0936293, acc 0.96875, learning_rate 0.000100104
2017-09-29T13:41:46.719854: step 2634, loss 0.132311, acc 0.953125, learning_rate 0.000100103
2017-09-29T13:41:46.900095: step 2635, loss 0.0657352, acc 0.984375, learning_rate 0.000100103
2017-09-29T13:41:47.082624: step 2636, loss 0.107317, acc 0.953125, learning_rate 0.000100102
2017-09-29T13:41:47.265335: step 2637, loss 0.0793013, acc 0.96875, learning_rate 0.000100102
2017-09-29T13:41:47.451782: step 2638, loss 0.0418008, acc 0.984375, learning_rate 0.000100101
2017-09-29T13:41:47.640090: step 2639, loss 0.0521647, acc 0.984375, learning_rate 0.000100101
2017-09-29T13:41:47.827796: step 2640, loss 0.0612256, acc 0.96875, learning_rate 0.000100101

Evaluation:
2017-09-29T13:41:48.386907: step 2640, loss 0.213497, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2640

2017-09-29T13:41:49.105025: step 2641, loss 0.026428, acc 1, learning_rate 0.0001001
2017-09-29T13:41:49.294139: step 2642, loss 0.0725802, acc 0.96875, learning_rate 0.0001001
2017-09-29T13:41:49.487202: step 2643, loss 0.0510308, acc 0.984375, learning_rate 0.000100099
2017-09-29T13:41:49.674275: step 2644, loss 0.045334, acc 1, learning_rate 0.000100099
2017-09-29T13:41:49.854732: step 2645, loss 0.0623565, acc 0.984375, learning_rate 0.000100099
2017-09-29T13:41:50.006479: step 2646, loss 0.0313002, acc 0.980392, learning_rate 0.000100098
2017-09-29T13:41:50.197683: step 2647, loss 0.0392489, acc 1, learning_rate 0.000100098
2017-09-29T13:41:50.382536: step 2648, loss 0.0332701, acc 0.984375, learning_rate 0.000100097
2017-09-29T13:41:50.567510: step 2649, loss 0.0459018, acc 0.96875, learning_rate 0.000100097
2017-09-29T13:41:50.757116: step 2650, loss 0.0353762, acc 0.984375, learning_rate 0.000100097
2017-09-29T13:41:50.951513: step 2651, loss 0.0246602, acc 1, learning_rate 0.000100096
2017-09-29T13:41:51.138551: step 2652, loss 0.0354903, acc 1, learning_rate 0.000100096
2017-09-29T13:41:51.323462: step 2653, loss 0.0933502, acc 0.96875, learning_rate 0.000100095
2017-09-29T13:41:51.525424: step 2654, loss 0.0311337, acc 0.984375, learning_rate 0.000100095
2017-09-29T13:41:51.711411: step 2655, loss 0.0604657, acc 0.984375, learning_rate 0.000100095
2017-09-29T13:41:51.894971: step 2656, loss 0.0999568, acc 0.953125, learning_rate 0.000100094
2017-09-29T13:41:52.085068: step 2657, loss 0.0305533, acc 1, learning_rate 0.000100094
2017-09-29T13:41:52.276727: step 2658, loss 0.0681956, acc 0.984375, learning_rate 0.000100093
2017-09-29T13:41:52.476947: step 2659, loss 0.0657867, acc 0.984375, learning_rate 0.000100093
2017-09-29T13:41:52.670679: step 2660, loss 0.0865516, acc 0.96875, learning_rate 0.000100093
2017-09-29T13:41:52.858610: step 2661, loss 0.0624872, acc 0.96875, learning_rate 0.000100092
2017-09-29T13:41:53.042160: step 2662, loss 0.0316634, acc 1, learning_rate 0.000100092
2017-09-29T13:41:53.228238: step 2663, loss 0.0619163, acc 0.984375, learning_rate 0.000100092
2017-09-29T13:41:53.412268: step 2664, loss 0.0462631, acc 1, learning_rate 0.000100091
2017-09-29T13:41:53.600142: step 2665, loss 0.0607029, acc 0.984375, learning_rate 0.000100091
2017-09-29T13:41:53.786535: step 2666, loss 0.0985219, acc 0.96875, learning_rate 0.00010009
2017-09-29T13:41:53.974787: step 2667, loss 0.0391859, acc 0.984375, learning_rate 0.00010009
2017-09-29T13:41:54.161891: step 2668, loss 0.0396463, acc 0.984375, learning_rate 0.00010009
2017-09-29T13:41:54.360402: step 2669, loss 0.109803, acc 0.953125, learning_rate 0.000100089
2017-09-29T13:41:54.556897: step 2670, loss 0.0742649, acc 0.984375, learning_rate 0.000100089
2017-09-29T13:41:54.744809: step 2671, loss 0.0402104, acc 1, learning_rate 0.000100089
2017-09-29T13:41:54.934728: step 2672, loss 0.0168894, acc 1, learning_rate 0.000100088
2017-09-29T13:41:55.120683: step 2673, loss 0.0480718, acc 1, learning_rate 0.000100088
2017-09-29T13:41:55.309647: step 2674, loss 0.0427375, acc 0.984375, learning_rate 0.000100088
2017-09-29T13:41:55.504845: step 2675, loss 0.0314411, acc 1, learning_rate 0.000100087
2017-09-29T13:41:55.698656: step 2676, loss 0.032953, acc 1, learning_rate 0.000100087
2017-09-29T13:41:55.892033: step 2677, loss 0.173621, acc 0.9375, learning_rate 0.000100086
2017-09-29T13:41:56.084244: step 2678, loss 0.0125619, acc 1, learning_rate 0.000100086
2017-09-29T13:41:56.270018: step 2679, loss 0.0476697, acc 0.984375, learning_rate 0.000100086
2017-09-29T13:41:56.468673: step 2680, loss 0.0655717, acc 0.984375, learning_rate 0.000100085

Evaluation:
2017-09-29T13:41:57.053405: step 2680, loss 0.213353, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2680

2017-09-29T13:41:57.880632: step 2681, loss 0.0578036, acc 0.984375, learning_rate 0.000100085
2017-09-29T13:41:58.067234: step 2682, loss 0.0422372, acc 0.984375, learning_rate 0.000100085
2017-09-29T13:41:58.275208: step 2683, loss 0.0149667, acc 1, learning_rate 0.000100084
2017-09-29T13:41:58.462566: step 2684, loss 0.0685163, acc 0.96875, learning_rate 0.000100084
2017-09-29T13:41:58.657712: step 2685, loss 0.0506996, acc 1, learning_rate 0.000100084
2017-09-29T13:41:58.840318: step 2686, loss 0.0529949, acc 0.984375, learning_rate 0.000100083
2017-09-29T13:41:59.024798: step 2687, loss 0.113014, acc 0.96875, learning_rate 0.000100083
2017-09-29T13:41:59.207979: step 2688, loss 0.0926699, acc 0.96875, learning_rate 0.000100083
2017-09-29T13:41:59.392490: step 2689, loss 0.0871646, acc 0.984375, learning_rate 0.000100082
2017-09-29T13:41:59.588650: step 2690, loss 0.0239431, acc 1, learning_rate 0.000100082
2017-09-29T13:41:59.776242: step 2691, loss 0.0467008, acc 0.984375, learning_rate 0.000100082
2017-09-29T13:41:59.980868: step 2692, loss 0.0640378, acc 0.96875, learning_rate 0.000100081
2017-09-29T13:42:00.179096: step 2693, loss 0.0409147, acc 1, learning_rate 0.000100081
2017-09-29T13:42:00.371646: step 2694, loss 0.0615051, acc 0.96875, learning_rate 0.000100081
2017-09-29T13:42:00.561768: step 2695, loss 0.0343576, acc 0.984375, learning_rate 0.00010008
2017-09-29T13:42:00.749361: step 2696, loss 0.0457968, acc 1, learning_rate 0.00010008
2017-09-29T13:42:00.933656: step 2697, loss 0.042176, acc 0.984375, learning_rate 0.00010008
2017-09-29T13:42:01.120522: step 2698, loss 0.0207241, acc 1, learning_rate 0.000100079
2017-09-29T13:42:01.306369: step 2699, loss 0.0412618, acc 1, learning_rate 0.000100079
2017-09-29T13:42:01.490177: step 2700, loss 0.0278042, acc 1, learning_rate 0.000100079
2017-09-29T13:42:01.673306: step 2701, loss 0.0537715, acc 0.984375, learning_rate 0.000100078
2017-09-29T13:42:01.860263: step 2702, loss 0.0246286, acc 1, learning_rate 0.000100078
2017-09-29T13:42:02.044813: step 2703, loss 0.0399564, acc 0.984375, learning_rate 0.000100078
2017-09-29T13:42:02.226299: step 2704, loss 0.0639911, acc 0.984375, learning_rate 0.000100077
2017-09-29T13:42:02.411773: step 2705, loss 0.0339511, acc 1, learning_rate 0.000100077
2017-09-29T13:42:02.603678: step 2706, loss 0.0332723, acc 0.984375, learning_rate 0.000100077
2017-09-29T13:42:02.792928: step 2707, loss 0.172702, acc 0.921875, learning_rate 0.000100076
2017-09-29T13:42:02.974003: step 2708, loss 0.0785332, acc 0.96875, learning_rate 0.000100076
2017-09-29T13:42:03.176436: step 2709, loss 0.0761165, acc 0.984375, learning_rate 0.000100076
2017-09-29T13:42:03.369411: step 2710, loss 0.0312101, acc 0.984375, learning_rate 0.000100076
2017-09-29T13:42:03.551608: step 2711, loss 0.128254, acc 0.96875, learning_rate 0.000100075
2017-09-29T13:42:03.735677: step 2712, loss 0.0841885, acc 0.96875, learning_rate 0.000100075
2017-09-29T13:42:03.919026: step 2713, loss 0.0728227, acc 0.96875, learning_rate 0.000100075
2017-09-29T13:42:04.100764: step 2714, loss 0.105118, acc 0.96875, learning_rate 0.000100074
2017-09-29T13:42:04.290888: step 2715, loss 0.0937296, acc 0.953125, learning_rate 0.000100074
2017-09-29T13:42:04.476988: step 2716, loss 0.0258447, acc 1, learning_rate 0.000100074
2017-09-29T13:42:04.676519: step 2717, loss 0.0415521, acc 0.984375, learning_rate 0.000100073
2017-09-29T13:42:04.870189: step 2718, loss 0.0244884, acc 1, learning_rate 0.000100073
2017-09-29T13:42:05.076860: step 2719, loss 0.0235156, acc 1, learning_rate 0.000100073
2017-09-29T13:42:05.271042: step 2720, loss 0.0394543, acc 1, learning_rate 0.000100073

Evaluation:
2017-09-29T13:42:05.814247: step 2720, loss 0.213669, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2720

2017-09-29T13:42:06.473684: step 2721, loss 0.0678557, acc 0.984375, learning_rate 0.000100072
2017-09-29T13:42:06.657937: step 2722, loss 0.0380937, acc 1, learning_rate 0.000100072
2017-09-29T13:42:06.842620: step 2723, loss 0.0727753, acc 0.984375, learning_rate 0.000100072
2017-09-29T13:42:07.026404: step 2724, loss 0.0968003, acc 0.96875, learning_rate 0.000100071
2017-09-29T13:42:07.216009: step 2725, loss 0.180033, acc 0.9375, learning_rate 0.000100071
2017-09-29T13:42:07.400072: step 2726, loss 0.051863, acc 0.984375, learning_rate 0.000100071
2017-09-29T13:42:07.608511: step 2727, loss 0.0658493, acc 0.984375, learning_rate 0.00010007
2017-09-29T13:42:07.814773: step 2728, loss 0.0542577, acc 0.984375, learning_rate 0.00010007
2017-09-29T13:42:08.024007: step 2729, loss 0.0798912, acc 0.96875, learning_rate 0.00010007
2017-09-29T13:42:08.223295: step 2730, loss 0.0582215, acc 0.984375, learning_rate 0.00010007
2017-09-29T13:42:08.414822: step 2731, loss 0.0330962, acc 0.984375, learning_rate 0.000100069
2017-09-29T13:42:08.602592: step 2732, loss 0.027199, acc 1, learning_rate 0.000100069
2017-09-29T13:42:08.796692: step 2733, loss 0.0257546, acc 1, learning_rate 0.000100069
2017-09-29T13:42:08.981502: step 2734, loss 0.0830814, acc 0.953125, learning_rate 0.000100068
2017-09-29T13:42:09.169298: step 2735, loss 0.0707916, acc 0.96875, learning_rate 0.000100068
2017-09-29T13:42:09.351081: step 2736, loss 0.0641856, acc 0.96875, learning_rate 0.000100068
2017-09-29T13:42:09.533162: step 2737, loss 0.028139, acc 1, learning_rate 0.000100068
2017-09-29T13:42:09.725063: step 2738, loss 0.045155, acc 1, learning_rate 0.000100067
2017-09-29T13:42:09.913212: step 2739, loss 0.109866, acc 0.953125, learning_rate 0.000100067
2017-09-29T13:42:10.097275: step 2740, loss 0.0674265, acc 0.96875, learning_rate 0.000100067
2017-09-29T13:42:10.283256: step 2741, loss 0.128466, acc 0.9375, learning_rate 0.000100067
2017-09-29T13:42:10.470894: step 2742, loss 0.0590261, acc 0.984375, learning_rate 0.000100066
2017-09-29T13:42:10.668304: step 2743, loss 0.0313213, acc 1, learning_rate 0.000100066
2017-09-29T13:42:10.837112: step 2744, loss 0.0644559, acc 0.980392, learning_rate 0.000100066
2017-09-29T13:42:11.021228: step 2745, loss 0.0415025, acc 1, learning_rate 0.000100065
2017-09-29T13:42:11.215709: step 2746, loss 0.0250076, acc 1, learning_rate 0.000100065
2017-09-29T13:42:11.401891: step 2747, loss 0.0961169, acc 0.96875, learning_rate 0.000100065
2017-09-29T13:42:11.587459: step 2748, loss 0.0848401, acc 0.953125, learning_rate 0.000100065
2017-09-29T13:42:11.776914: step 2749, loss 0.0818332, acc 0.984375, learning_rate 0.000100064
2017-09-29T13:42:11.960755: step 2750, loss 0.0448511, acc 0.984375, learning_rate 0.000100064
2017-09-29T13:42:12.147482: step 2751, loss 0.0271863, acc 1, learning_rate 0.000100064
2017-09-29T13:42:12.341806: step 2752, loss 0.0537239, acc 0.96875, learning_rate 0.000100064
2017-09-29T13:42:12.536476: step 2753, loss 0.0424041, acc 1, learning_rate 0.000100063
2017-09-29T13:42:12.720438: step 2754, loss 0.0699143, acc 0.953125, learning_rate 0.000100063
2017-09-29T13:42:12.906580: step 2755, loss 0.0570819, acc 0.984375, learning_rate 0.000100063
2017-09-29T13:42:13.097303: step 2756, loss 0.080476, acc 0.953125, learning_rate 0.000100063
2017-09-29T13:42:13.281934: step 2757, loss 0.027146, acc 0.984375, learning_rate 0.000100062
2017-09-29T13:42:13.482027: step 2758, loss 0.0616336, acc 0.96875, learning_rate 0.000100062
2017-09-29T13:42:13.670781: step 2759, loss 0.0405857, acc 1, learning_rate 0.000100062
2017-09-29T13:42:13.849883: step 2760, loss 0.0679416, acc 0.96875, learning_rate 0.000100062

Evaluation:
2017-09-29T13:42:14.424811: step 2760, loss 0.210825, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2760

2017-09-29T13:42:15.143051: step 2761, loss 0.0371735, acc 1, learning_rate 0.000100061
2017-09-29T13:42:15.337608: step 2762, loss 0.0478736, acc 1, learning_rate 0.000100061
2017-09-29T13:42:15.523225: step 2763, loss 0.0770166, acc 0.984375, learning_rate 0.000100061
2017-09-29T13:42:15.709862: step 2764, loss 0.0289092, acc 1, learning_rate 0.000100061
2017-09-29T13:42:15.895453: step 2765, loss 0.0405187, acc 1, learning_rate 0.00010006
2017-09-29T13:42:16.080565: step 2766, loss 0.0442791, acc 0.984375, learning_rate 0.00010006
2017-09-29T13:42:16.267302: step 2767, loss 0.0419559, acc 0.984375, learning_rate 0.00010006
2017-09-29T13:42:16.449606: step 2768, loss 0.0503009, acc 0.984375, learning_rate 0.00010006
2017-09-29T13:42:16.634524: step 2769, loss 0.045783, acc 1, learning_rate 0.000100059
2017-09-29T13:42:16.817441: step 2770, loss 0.0259868, acc 1, learning_rate 0.000100059
2017-09-29T13:42:17.004070: step 2771, loss 0.0412167, acc 1, learning_rate 0.000100059
2017-09-29T13:42:17.194364: step 2772, loss 0.0630483, acc 0.96875, learning_rate 0.000100059
2017-09-29T13:42:17.383184: step 2773, loss 0.146881, acc 0.953125, learning_rate 0.000100058
2017-09-29T13:42:17.568744: step 2774, loss 0.0242339, acc 1, learning_rate 0.000100058
2017-09-29T13:42:17.752823: step 2775, loss 0.117572, acc 0.96875, learning_rate 0.000100058
2017-09-29T13:42:17.935611: step 2776, loss 0.0134087, acc 1, learning_rate 0.000100058
2017-09-29T13:42:18.120207: step 2777, loss 0.0914542, acc 0.984375, learning_rate 0.000100057
2017-09-29T13:42:18.305730: step 2778, loss 0.048235, acc 0.96875, learning_rate 0.000100057
2017-09-29T13:42:18.494341: step 2779, loss 0.0466838, acc 0.984375, learning_rate 0.000100057
2017-09-29T13:42:18.683242: step 2780, loss 0.0464486, acc 0.984375, learning_rate 0.000100057
2017-09-29T13:42:18.867233: step 2781, loss 0.0550203, acc 0.984375, learning_rate 0.000100056
2017-09-29T13:42:19.060350: step 2782, loss 0.0379528, acc 1, learning_rate 0.000100056
2017-09-29T13:42:19.244707: step 2783, loss 0.0394875, acc 1, learning_rate 0.000100056
2017-09-29T13:42:19.434723: step 2784, loss 0.0492076, acc 0.96875, learning_rate 0.000100056
2017-09-29T13:42:19.621584: step 2785, loss 0.0334988, acc 1, learning_rate 0.000100056
2017-09-29T13:42:19.816318: step 2786, loss 0.0551797, acc 0.984375, learning_rate 0.000100055
2017-09-29T13:42:20.000648: step 2787, loss 0.0586539, acc 0.984375, learning_rate 0.000100055
2017-09-29T13:42:20.182947: step 2788, loss 0.0306558, acc 1, learning_rate 0.000100055
2017-09-29T13:42:20.376689: step 2789, loss 0.0793599, acc 0.96875, learning_rate 0.000100055
2017-09-29T13:42:20.577827: step 2790, loss 0.0688211, acc 0.984375, learning_rate 0.000100054
2017-09-29T13:42:20.769056: step 2791, loss 0.0495185, acc 0.984375, learning_rate 0.000100054
2017-09-29T13:42:20.958121: step 2792, loss 0.013883, acc 1, learning_rate 0.000100054
2017-09-29T13:42:21.149282: step 2793, loss 0.0642323, acc 0.96875, learning_rate 0.000100054
2017-09-29T13:42:21.338003: step 2794, loss 0.0405689, acc 0.984375, learning_rate 0.000100054
2017-09-29T13:42:21.527738: step 2795, loss 0.100603, acc 0.96875, learning_rate 0.000100053
2017-09-29T13:42:21.720699: step 2796, loss 0.0358252, acc 0.984375, learning_rate 0.000100053
2017-09-29T13:42:21.908931: step 2797, loss 0.0837663, acc 0.984375, learning_rate 0.000100053
2017-09-29T13:42:22.094911: step 2798, loss 0.0584694, acc 0.984375, learning_rate 0.000100053
2017-09-29T13:42:22.288465: step 2799, loss 0.0553603, acc 0.984375, learning_rate 0.000100052
2017-09-29T13:42:22.478999: step 2800, loss 0.0323445, acc 1, learning_rate 0.000100052

Evaluation:
2017-09-29T13:42:23.028694: step 2800, loss 0.213877, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2800

2017-09-29T13:42:23.833601: step 2801, loss 0.0639437, acc 0.984375, learning_rate 0.000100052
2017-09-29T13:42:24.011726: step 2802, loss 0.0441883, acc 0.984375, learning_rate 0.000100052
2017-09-29T13:42:24.197274: step 2803, loss 0.0863602, acc 0.953125, learning_rate 0.000100052
2017-09-29T13:42:24.394880: step 2804, loss 0.125796, acc 0.9375, learning_rate 0.000100051
2017-09-29T13:42:24.594794: step 2805, loss 0.0621962, acc 0.984375, learning_rate 0.000100051
2017-09-29T13:42:24.793576: step 2806, loss 0.0864437, acc 0.9375, learning_rate 0.000100051
2017-09-29T13:42:24.997434: step 2807, loss 0.075127, acc 0.984375, learning_rate 0.000100051
2017-09-29T13:42:25.192498: step 2808, loss 0.0205635, acc 1, learning_rate 0.000100051
2017-09-29T13:42:25.376120: step 2809, loss 0.0415051, acc 1, learning_rate 0.00010005
2017-09-29T13:42:25.566581: step 2810, loss 0.0754954, acc 0.984375, learning_rate 0.00010005
2017-09-29T13:42:25.757412: step 2811, loss 0.0654489, acc 0.984375, learning_rate 0.00010005
2017-09-29T13:42:25.941959: step 2812, loss 0.0727118, acc 0.96875, learning_rate 0.00010005
2017-09-29T13:42:26.122004: step 2813, loss 0.0345651, acc 1, learning_rate 0.00010005
2017-09-29T13:42:26.307435: step 2814, loss 0.103878, acc 0.953125, learning_rate 0.000100049
2017-09-29T13:42:26.490118: step 2815, loss 0.0599788, acc 0.984375, learning_rate 0.000100049
2017-09-29T13:42:26.681018: step 2816, loss 0.0457274, acc 0.984375, learning_rate 0.000100049
2017-09-29T13:42:26.867541: step 2817, loss 0.0884918, acc 0.984375, learning_rate 0.000100049
2017-09-29T13:42:27.065909: step 2818, loss 0.0621784, acc 0.984375, learning_rate 0.000100049
2017-09-29T13:42:27.261369: step 2819, loss 0.0554858, acc 0.96875, learning_rate 0.000100048
2017-09-29T13:42:27.447298: step 2820, loss 0.0507892, acc 0.96875, learning_rate 0.000100048
2017-09-29T13:42:27.639697: step 2821, loss 0.0442647, acc 0.984375, learning_rate 0.000100048
2017-09-29T13:42:27.824511: step 2822, loss 0.104357, acc 0.96875, learning_rate 0.000100048
2017-09-29T13:42:28.014331: step 2823, loss 0.0426098, acc 0.984375, learning_rate 0.000100048
2017-09-29T13:42:28.202408: step 2824, loss 0.0718374, acc 0.984375, learning_rate 0.000100047
2017-09-29T13:42:28.391094: step 2825, loss 0.0406152, acc 0.984375, learning_rate 0.000100047
2017-09-29T13:42:28.582312: step 2826, loss 0.06889, acc 0.953125, learning_rate 0.000100047
2017-09-29T13:42:28.767848: step 2827, loss 0.0229734, acc 0.984375, learning_rate 0.000100047
2017-09-29T13:42:28.951478: step 2828, loss 0.043884, acc 1, learning_rate 0.000100047
2017-09-29T13:42:29.155877: step 2829, loss 0.0326395, acc 1, learning_rate 0.000100046
2017-09-29T13:42:29.361891: step 2830, loss 0.0323951, acc 1, learning_rate 0.000100046
2017-09-29T13:42:29.559039: step 2831, loss 0.0596912, acc 0.96875, learning_rate 0.000100046
2017-09-29T13:42:29.744325: step 2832, loss 0.01397, acc 1, learning_rate 0.000100046
2017-09-29T13:42:29.937269: step 2833, loss 0.0726967, acc 0.984375, learning_rate 0.000100046
2017-09-29T13:42:30.144508: step 2834, loss 0.0811136, acc 0.96875, learning_rate 0.000100045
2017-09-29T13:42:30.343441: step 2835, loss 0.0608348, acc 0.96875, learning_rate 0.000100045
2017-09-29T13:42:30.543226: step 2836, loss 0.0531122, acc 0.984375, learning_rate 0.000100045
2017-09-29T13:42:30.727551: step 2837, loss 0.0526351, acc 0.984375, learning_rate 0.000100045
2017-09-29T13:42:30.915604: step 2838, loss 0.0158825, acc 1, learning_rate 0.000100045
2017-09-29T13:42:31.108329: step 2839, loss 0.023342, acc 1, learning_rate 0.000100045
2017-09-29T13:42:31.291469: step 2840, loss 0.0506436, acc 0.984375, learning_rate 0.000100044

Evaluation:
2017-09-29T13:42:31.856897: step 2840, loss 0.214357, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2840

2017-09-29T13:42:32.505379: step 2841, loss 0.103779, acc 0.953125, learning_rate 0.000100044
2017-09-29T13:42:32.655130: step 2842, loss 0.0584267, acc 0.980392, learning_rate 0.000100044
2017-09-29T13:42:32.841445: step 2843, loss 0.0276143, acc 1, learning_rate 0.000100044
2017-09-29T13:42:33.025690: step 2844, loss 0.0408733, acc 0.984375, learning_rate 0.000100044
2017-09-29T13:42:33.212485: step 2845, loss 0.0904727, acc 0.96875, learning_rate 0.000100043
2017-09-29T13:42:33.399977: step 2846, loss 0.0611575, acc 0.984375, learning_rate 0.000100043
2017-09-29T13:42:33.587325: step 2847, loss 0.0843734, acc 0.96875, learning_rate 0.000100043
2017-09-29T13:42:33.773466: step 2848, loss 0.0252898, acc 1, learning_rate 0.000100043
2017-09-29T13:42:33.957902: step 2849, loss 0.0386225, acc 1, learning_rate 0.000100043
2017-09-29T13:42:34.147994: step 2850, loss 0.0241645, acc 1, learning_rate 0.000100043
2017-09-29T13:42:34.332269: step 2851, loss 0.126184, acc 0.96875, learning_rate 0.000100042
2017-09-29T13:42:34.518962: step 2852, loss 0.0426081, acc 1, learning_rate 0.000100042
2017-09-29T13:42:34.705184: step 2853, loss 0.0302232, acc 1, learning_rate 0.000100042
2017-09-29T13:42:34.899676: step 2854, loss 0.106635, acc 0.953125, learning_rate 0.000100042
2017-09-29T13:42:35.094494: step 2855, loss 0.0158509, acc 1, learning_rate 0.000100042
2017-09-29T13:42:35.285186: step 2856, loss 0.0531425, acc 0.984375, learning_rate 0.000100042
2017-09-29T13:42:35.468114: step 2857, loss 0.0120489, acc 1, learning_rate 0.000100041
2017-09-29T13:42:35.653942: step 2858, loss 0.0279814, acc 1, learning_rate 0.000100041
2017-09-29T13:42:35.841724: step 2859, loss 0.0530804, acc 0.984375, learning_rate 0.000100041
2017-09-29T13:42:36.026654: step 2860, loss 0.120913, acc 0.953125, learning_rate 0.000100041
2017-09-29T13:42:36.211402: step 2861, loss 0.0823172, acc 0.984375, learning_rate 0.000100041
2017-09-29T13:42:36.401235: step 2862, loss 0.0407588, acc 1, learning_rate 0.000100041
2017-09-29T13:42:36.604284: step 2863, loss 0.134932, acc 0.9375, learning_rate 0.00010004
2017-09-29T13:42:36.787090: step 2864, loss 0.0898613, acc 0.96875, learning_rate 0.00010004
2017-09-29T13:42:36.971182: step 2865, loss 0.0674971, acc 0.984375, learning_rate 0.00010004
2017-09-29T13:42:37.152613: step 2866, loss 0.0425297, acc 1, learning_rate 0.00010004
2017-09-29T13:42:37.336923: step 2867, loss 0.0650912, acc 0.984375, learning_rate 0.00010004
2017-09-29T13:42:37.522405: step 2868, loss 0.0714192, acc 0.96875, learning_rate 0.00010004
2017-09-29T13:42:37.719864: step 2869, loss 0.0494178, acc 0.984375, learning_rate 0.000100039
2017-09-29T13:42:37.913638: step 2870, loss 0.0317459, acc 1, learning_rate 0.000100039
2017-09-29T13:42:38.097255: step 2871, loss 0.0214448, acc 1, learning_rate 0.000100039
2017-09-29T13:42:38.279324: step 2872, loss 0.0295532, acc 1, learning_rate 0.000100039
2017-09-29T13:42:38.462614: step 2873, loss 0.0178556, acc 1, learning_rate 0.000100039
2017-09-29T13:42:38.650255: step 2874, loss 0.0189797, acc 1, learning_rate 0.000100039
2017-09-29T13:42:38.843257: step 2875, loss 0.037799, acc 1, learning_rate 0.000100038
2017-09-29T13:42:39.029434: step 2876, loss 0.0726886, acc 0.96875, learning_rate 0.000100038
2017-09-29T13:42:39.217342: step 2877, loss 0.0282544, acc 0.984375, learning_rate 0.000100038
2017-09-29T13:42:39.400412: step 2878, loss 0.0113375, acc 1, learning_rate 0.000100038
2017-09-29T13:42:39.591388: step 2879, loss 0.0576154, acc 0.984375, learning_rate 0.000100038
2017-09-29T13:42:39.798514: step 2880, loss 0.0277344, acc 1, learning_rate 0.000100038

Evaluation:
2017-09-29T13:42:40.399956: step 2880, loss 0.213193, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2880

2017-09-29T13:42:41.126595: step 2881, loss 0.0499975, acc 0.984375, learning_rate 0.000100038
2017-09-29T13:42:41.315122: step 2882, loss 0.0426874, acc 0.984375, learning_rate 0.000100037
2017-09-29T13:42:41.514101: step 2883, loss 0.116622, acc 0.96875, learning_rate 0.000100037
2017-09-29T13:42:41.699451: step 2884, loss 0.0272605, acc 1, learning_rate 0.000100037
2017-09-29T13:42:41.887957: step 2885, loss 0.0849556, acc 0.953125, learning_rate 0.000100037
2017-09-29T13:42:42.069764: step 2886, loss 0.0362698, acc 0.984375, learning_rate 0.000100037
2017-09-29T13:42:42.256833: step 2887, loss 0.0349893, acc 0.984375, learning_rate 0.000100037
2017-09-29T13:42:42.439300: step 2888, loss 0.0659771, acc 0.984375, learning_rate 0.000100036
2017-09-29T13:42:42.625316: step 2889, loss 0.0759666, acc 0.96875, learning_rate 0.000100036
2017-09-29T13:42:42.823439: step 2890, loss 0.0594831, acc 0.984375, learning_rate 0.000100036
2017-09-29T13:42:43.015531: step 2891, loss 0.0878358, acc 0.984375, learning_rate 0.000100036
2017-09-29T13:42:43.197608: step 2892, loss 0.0204722, acc 1, learning_rate 0.000100036
2017-09-29T13:42:43.391120: step 2893, loss 0.0392056, acc 0.984375, learning_rate 0.000100036
2017-09-29T13:42:43.599115: step 2894, loss 0.0288207, acc 1, learning_rate 0.000100036
2017-09-29T13:42:43.788581: step 2895, loss 0.0374143, acc 1, learning_rate 0.000100035
2017-09-29T13:42:43.970207: step 2896, loss 0.0241918, acc 1, learning_rate 0.000100035
2017-09-29T13:42:44.154553: step 2897, loss 0.0488184, acc 0.984375, learning_rate 0.000100035
2017-09-29T13:42:44.336202: step 2898, loss 0.0607336, acc 0.96875, learning_rate 0.000100035
2017-09-29T13:42:44.521348: step 2899, loss 0.024873, acc 1, learning_rate 0.000100035
2017-09-29T13:42:44.706085: step 2900, loss 0.0497433, acc 0.984375, learning_rate 0.000100035
2017-09-29T13:42:44.893818: step 2901, loss 0.0936726, acc 0.96875, learning_rate 0.000100035
2017-09-29T13:42:45.078687: step 2902, loss 0.0210476, acc 1, learning_rate 0.000100034
2017-09-29T13:42:45.259733: step 2903, loss 0.0495456, acc 1, learning_rate 0.000100034
2017-09-29T13:42:45.446011: step 2904, loss 0.0654197, acc 0.984375, learning_rate 0.000100034
2017-09-29T13:42:45.633456: step 2905, loss 0.0176113, acc 1, learning_rate 0.000100034
2017-09-29T13:42:45.814719: step 2906, loss 0.0360196, acc 1, learning_rate 0.000100034
2017-09-29T13:42:46.010195: step 2907, loss 0.0242485, acc 0.984375, learning_rate 0.000100034
2017-09-29T13:42:46.198290: step 2908, loss 0.147281, acc 0.96875, learning_rate 0.000100034
2017-09-29T13:42:46.382808: step 2909, loss 0.0271767, acc 1, learning_rate 0.000100033
2017-09-29T13:42:46.567908: step 2910, loss 0.0260438, acc 1, learning_rate 0.000100033
2017-09-29T13:42:46.751925: step 2911, loss 0.0217703, acc 1, learning_rate 0.000100033
2017-09-29T13:42:46.933382: step 2912, loss 0.0594941, acc 0.96875, learning_rate 0.000100033
2017-09-29T13:42:47.116189: step 2913, loss 0.0678484, acc 0.96875, learning_rate 0.000100033
2017-09-29T13:42:47.299753: step 2914, loss 0.0412234, acc 1, learning_rate 0.000100033
2017-09-29T13:42:47.499111: step 2915, loss 0.0765978, acc 0.96875, learning_rate 0.000100033
2017-09-29T13:42:47.690749: step 2916, loss 0.0548733, acc 0.984375, learning_rate 0.000100033
2017-09-29T13:42:47.898495: step 2917, loss 0.0795119, acc 0.96875, learning_rate 0.000100032
2017-09-29T13:42:48.106900: step 2918, loss 0.0922799, acc 0.953125, learning_rate 0.000100032
2017-09-29T13:42:48.311837: step 2919, loss 0.0569356, acc 0.96875, learning_rate 0.000100032
2017-09-29T13:42:48.501125: step 2920, loss 0.0190919, acc 1, learning_rate 0.000100032

Evaluation:
2017-09-29T13:42:49.069694: step 2920, loss 0.216314, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2920

2017-09-29T13:42:49.860824: step 2921, loss 0.0778312, acc 0.984375, learning_rate 0.000100032
2017-09-29T13:42:50.054095: step 2922, loss 0.0655134, acc 0.953125, learning_rate 0.000100032
2017-09-29T13:42:50.243827: step 2923, loss 0.0232831, acc 1, learning_rate 0.000100032
2017-09-29T13:42:50.429996: step 2924, loss 0.0484472, acc 0.984375, learning_rate 0.000100031
2017-09-29T13:42:50.611782: step 2925, loss 0.0243945, acc 1, learning_rate 0.000100031
2017-09-29T13:42:50.799161: step 2926, loss 0.033412, acc 0.984375, learning_rate 0.000100031
2017-09-29T13:42:50.979819: step 2927, loss 0.0670419, acc 0.953125, learning_rate 0.000100031
2017-09-29T13:42:51.162330: step 2928, loss 0.0651201, acc 0.984375, learning_rate 0.000100031
2017-09-29T13:42:51.350131: step 2929, loss 0.105547, acc 0.9375, learning_rate 0.000100031
2017-09-29T13:42:51.531169: step 2930, loss 0.0832713, acc 0.96875, learning_rate 0.000100031
2017-09-29T13:42:51.711299: step 2931, loss 0.0449975, acc 0.984375, learning_rate 0.000100031
2017-09-29T13:42:51.894349: step 2932, loss 0.0659226, acc 0.96875, learning_rate 0.00010003
2017-09-29T13:42:52.074766: step 2933, loss 0.0335388, acc 1, learning_rate 0.00010003
2017-09-29T13:42:52.257211: step 2934, loss 0.181399, acc 0.9375, learning_rate 0.00010003
2017-09-29T13:42:52.449910: step 2935, loss 0.118622, acc 0.953125, learning_rate 0.00010003
2017-09-29T13:42:52.635792: step 2936, loss 0.0653575, acc 0.96875, learning_rate 0.00010003
2017-09-29T13:42:52.825151: step 2937, loss 0.0311816, acc 0.984375, learning_rate 0.00010003
2017-09-29T13:42:53.010946: step 2938, loss 0.0677249, acc 0.984375, learning_rate 0.00010003
2017-09-29T13:42:53.194871: step 2939, loss 0.139151, acc 0.9375, learning_rate 0.00010003
2017-09-29T13:42:53.350355: step 2940, loss 0.0565545, acc 0.980392, learning_rate 0.000100029
2017-09-29T13:42:53.552189: step 2941, loss 0.0683914, acc 0.96875, learning_rate 0.000100029
2017-09-29T13:42:53.741019: step 2942, loss 0.0551282, acc 0.984375, learning_rate 0.000100029
2017-09-29T13:42:53.925955: step 2943, loss 0.0316837, acc 1, learning_rate 0.000100029
2017-09-29T13:42:54.115579: step 2944, loss 0.0274919, acc 1, learning_rate 0.000100029
2017-09-29T13:42:54.297138: step 2945, loss 0.0146689, acc 1, learning_rate 0.000100029
2017-09-29T13:42:54.481573: step 2946, loss 0.0219103, acc 1, learning_rate 0.000100029
2017-09-29T13:42:54.664482: step 2947, loss 0.00841921, acc 1, learning_rate 0.000100029
2017-09-29T13:42:54.847293: step 2948, loss 0.0379088, acc 0.984375, learning_rate 0.000100029
2017-09-29T13:42:55.032530: step 2949, loss 0.0174512, acc 1, learning_rate 0.000100028
2017-09-29T13:42:55.216128: step 2950, loss 0.0374733, acc 0.984375, learning_rate 0.000100028
2017-09-29T13:42:55.406729: step 2951, loss 0.0242548, acc 1, learning_rate 0.000100028
2017-09-29T13:42:55.589706: step 2952, loss 0.0243819, acc 1, learning_rate 0.000100028
2017-09-29T13:42:55.777176: step 2953, loss 0.05415, acc 0.96875, learning_rate 0.000100028
2017-09-29T13:42:55.960425: step 2954, loss 0.0571013, acc 0.953125, learning_rate 0.000100028
2017-09-29T13:42:56.146809: step 2955, loss 0.0933908, acc 0.953125, learning_rate 0.000100028
2017-09-29T13:42:56.341281: step 2956, loss 0.0151375, acc 1, learning_rate 0.000100028
2017-09-29T13:42:56.521713: step 2957, loss 0.0598036, acc 0.984375, learning_rate 0.000100028
2017-09-29T13:42:56.706431: step 2958, loss 0.0341734, acc 0.984375, learning_rate 0.000100027
2017-09-29T13:42:56.892681: step 2959, loss 0.0199854, acc 1, learning_rate 0.000100027
2017-09-29T13:42:57.076254: step 2960, loss 0.100309, acc 0.953125, learning_rate 0.000100027

Evaluation:
2017-09-29T13:42:57.590500: step 2960, loss 0.211366, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-2960

2017-09-29T13:42:58.240169: step 2961, loss 0.0939404, acc 0.96875, learning_rate 0.000100027
2017-09-29T13:42:58.427730: step 2962, loss 0.0185556, acc 1, learning_rate 0.000100027
2017-09-29T13:42:58.612911: step 2963, loss 0.0965683, acc 0.953125, learning_rate 0.000100027
2017-09-29T13:42:58.805116: step 2964, loss 0.100835, acc 0.96875, learning_rate 0.000100027
2017-09-29T13:42:58.989052: step 2965, loss 0.0317773, acc 1, learning_rate 0.000100027
2017-09-29T13:42:59.174736: step 2966, loss 0.0436764, acc 0.984375, learning_rate 0.000100027
2017-09-29T13:42:59.359165: step 2967, loss 0.0154238, acc 1, learning_rate 0.000100026
2017-09-29T13:42:59.556329: step 2968, loss 0.203112, acc 0.921875, learning_rate 0.000100026
2017-09-29T13:42:59.739230: step 2969, loss 0.0678187, acc 0.96875, learning_rate 0.000100026
2017-09-29T13:42:59.927052: step 2970, loss 0.0344034, acc 0.984375, learning_rate 0.000100026
2017-09-29T13:43:00.114538: step 2971, loss 0.0469148, acc 0.984375, learning_rate 0.000100026
2017-09-29T13:43:00.299065: step 2972, loss 0.025575, acc 1, learning_rate 0.000100026
2017-09-29T13:43:00.488824: step 2973, loss 0.020496, acc 1, learning_rate 0.000100026
2017-09-29T13:43:00.669560: step 2974, loss 0.0214222, acc 1, learning_rate 0.000100026
2017-09-29T13:43:00.852002: step 2975, loss 0.062106, acc 0.984375, learning_rate 0.000100026
2017-09-29T13:43:01.036583: step 2976, loss 0.0981878, acc 0.953125, learning_rate 0.000100025
2017-09-29T13:43:01.222449: step 2977, loss 0.037974, acc 1, learning_rate 0.000100025
2017-09-29T13:43:01.416149: step 2978, loss 0.0244325, acc 1, learning_rate 0.000100025
2017-09-29T13:43:01.598662: step 2979, loss 0.0452778, acc 1, learning_rate 0.000100025
2017-09-29T13:43:01.799179: step 2980, loss 0.0249769, acc 0.984375, learning_rate 0.000100025
2017-09-29T13:43:01.986035: step 2981, loss 0.0654093, acc 0.984375, learning_rate 0.000100025
2017-09-29T13:43:02.178570: step 2982, loss 0.0639121, acc 0.984375, learning_rate 0.000100025
2017-09-29T13:43:02.360672: step 2983, loss 0.0503636, acc 1, learning_rate 0.000100025
2017-09-29T13:43:02.545458: step 2984, loss 0.0294442, acc 0.984375, learning_rate 0.000100025
2017-09-29T13:43:02.728191: step 2985, loss 0.0428105, acc 0.984375, learning_rate 0.000100025
2017-09-29T13:43:02.913915: step 2986, loss 0.0708096, acc 0.96875, learning_rate 0.000100024
2017-09-29T13:43:03.099038: step 2987, loss 0.0757877, acc 0.96875, learning_rate 0.000100024
2017-09-29T13:43:03.289500: step 2988, loss 0.117006, acc 0.953125, learning_rate 0.000100024
2017-09-29T13:43:03.477334: step 2989, loss 0.09222, acc 0.984375, learning_rate 0.000100024
2017-09-29T13:43:03.662754: step 2990, loss 0.027172, acc 1, learning_rate 0.000100024
2017-09-29T13:43:03.850820: step 2991, loss 0.0590598, acc 0.984375, learning_rate 0.000100024
2017-09-29T13:43:04.039603: step 2992, loss 0.026231, acc 0.984375, learning_rate 0.000100024
2017-09-29T13:43:04.228702: step 2993, loss 0.0336238, acc 0.984375, learning_rate 0.000100024
2017-09-29T13:43:04.421302: step 2994, loss 0.0566258, acc 0.984375, learning_rate 0.000100024
2017-09-29T13:43:04.629561: step 2995, loss 0.0441892, acc 0.96875, learning_rate 0.000100024
2017-09-29T13:43:04.820917: step 2996, loss 0.0566157, acc 0.984375, learning_rate 0.000100023
2017-09-29T13:43:05.011934: step 2997, loss 0.114804, acc 0.96875, learning_rate 0.000100023
2017-09-29T13:43:05.198877: step 2998, loss 0.0179971, acc 1, learning_rate 0.000100023
2017-09-29T13:43:05.399952: step 2999, loss 0.0448411, acc 0.984375, learning_rate 0.000100023
2017-09-29T13:43:05.584629: step 3000, loss 0.084893, acc 0.984375, learning_rate 0.000100023

Evaluation:
2017-09-29T13:43:06.105977: step 3000, loss 0.213772, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3000

2017-09-29T13:43:06.831423: step 3001, loss 0.031474, acc 1, learning_rate 0.000100023
2017-09-29T13:43:07.029071: step 3002, loss 0.0373595, acc 0.984375, learning_rate 0.000100023
2017-09-29T13:43:07.223943: step 3003, loss 0.0902429, acc 0.984375, learning_rate 0.000100023
2017-09-29T13:43:07.414076: step 3004, loss 0.0778006, acc 0.984375, learning_rate 0.000100023
2017-09-29T13:43:07.601427: step 3005, loss 0.0672777, acc 0.984375, learning_rate 0.000100023
2017-09-29T13:43:07.797241: step 3006, loss 0.0211721, acc 1, learning_rate 0.000100023
2017-09-29T13:43:07.988665: step 3007, loss 0.0285679, acc 1, learning_rate 0.000100022
2017-09-29T13:43:08.180070: step 3008, loss 0.0296082, acc 1, learning_rate 0.000100022
2017-09-29T13:43:08.359522: step 3009, loss 0.0586274, acc 0.984375, learning_rate 0.000100022
2017-09-29T13:43:08.543068: step 3010, loss 0.0622227, acc 0.984375, learning_rate 0.000100022
2017-09-29T13:43:08.728940: step 3011, loss 0.106862, acc 0.96875, learning_rate 0.000100022
2017-09-29T13:43:08.920969: step 3012, loss 0.0405755, acc 1, learning_rate 0.000100022
2017-09-29T13:43:09.102406: step 3013, loss 0.0535014, acc 1, learning_rate 0.000100022
2017-09-29T13:43:09.290515: step 3014, loss 0.0484836, acc 0.984375, learning_rate 0.000100022
2017-09-29T13:43:09.483983: step 3015, loss 0.0387415, acc 0.984375, learning_rate 0.000100022
2017-09-29T13:43:09.669115: step 3016, loss 0.0865054, acc 0.96875, learning_rate 0.000100022
2017-09-29T13:43:09.866647: step 3017, loss 0.0562191, acc 1, learning_rate 0.000100022
2017-09-29T13:43:10.064387: step 3018, loss 0.123478, acc 0.953125, learning_rate 0.000100021
2017-09-29T13:43:10.252075: step 3019, loss 0.110192, acc 0.9375, learning_rate 0.000100021
2017-09-29T13:43:10.457218: step 3020, loss 0.0461967, acc 1, learning_rate 0.000100021
2017-09-29T13:43:10.646887: step 3021, loss 0.0518301, acc 0.984375, learning_rate 0.000100021
2017-09-29T13:43:10.834502: step 3022, loss 0.0278411, acc 1, learning_rate 0.000100021
2017-09-29T13:43:11.023869: step 3023, loss 0.0391957, acc 0.984375, learning_rate 0.000100021
2017-09-29T13:43:11.209435: step 3024, loss 0.0190286, acc 1, learning_rate 0.000100021
2017-09-29T13:43:11.396222: step 3025, loss 0.0375346, acc 1, learning_rate 0.000100021
2017-09-29T13:43:11.584993: step 3026, loss 0.0264413, acc 1, learning_rate 0.000100021
2017-09-29T13:43:11.768293: step 3027, loss 0.0504171, acc 0.984375, learning_rate 0.000100021
2017-09-29T13:43:11.952697: step 3028, loss 0.0726335, acc 0.96875, learning_rate 0.000100021
2017-09-29T13:43:12.136503: step 3029, loss 0.107843, acc 0.953125, learning_rate 0.00010002
2017-09-29T13:43:12.325677: step 3030, loss 0.065377, acc 0.96875, learning_rate 0.00010002
2017-09-29T13:43:12.518013: step 3031, loss 0.030285, acc 1, learning_rate 0.00010002
2017-09-29T13:43:12.708562: step 3032, loss 0.0980373, acc 0.984375, learning_rate 0.00010002
2017-09-29T13:43:12.897781: step 3033, loss 0.0872712, acc 0.984375, learning_rate 0.00010002
2017-09-29T13:43:13.087541: step 3034, loss 0.0475928, acc 0.984375, learning_rate 0.00010002
2017-09-29T13:43:13.271013: step 3035, loss 0.0607255, acc 0.984375, learning_rate 0.00010002
2017-09-29T13:43:13.455938: step 3036, loss 0.0525063, acc 0.984375, learning_rate 0.00010002
2017-09-29T13:43:13.638689: step 3037, loss 0.0443223, acc 0.96875, learning_rate 0.00010002
2017-09-29T13:43:13.812282: step 3038, loss 0.0930251, acc 0.960784, learning_rate 0.00010002
2017-09-29T13:43:14.008988: step 3039, loss 0.0403099, acc 0.984375, learning_rate 0.00010002
2017-09-29T13:43:14.210094: step 3040, loss 0.0644921, acc 0.984375, learning_rate 0.00010002

Evaluation:
2017-09-29T13:43:14.746449: step 3040, loss 0.209739, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3040

2017-09-29T13:43:15.482404: step 3041, loss 0.0246815, acc 1, learning_rate 0.00010002
2017-09-29T13:43:15.678529: step 3042, loss 0.0244143, acc 1, learning_rate 0.000100019
2017-09-29T13:43:15.874802: step 3043, loss 0.0450578, acc 0.984375, learning_rate 0.000100019
2017-09-29T13:43:16.077674: step 3044, loss 0.0543189, acc 0.96875, learning_rate 0.000100019
2017-09-29T13:43:16.269163: step 3045, loss 0.0940439, acc 0.953125, learning_rate 0.000100019
2017-09-29T13:43:16.454235: step 3046, loss 0.0236151, acc 0.984375, learning_rate 0.000100019
2017-09-29T13:43:16.638356: step 3047, loss 0.0652067, acc 0.984375, learning_rate 0.000100019
2017-09-29T13:43:16.823901: step 3048, loss 0.0353444, acc 1, learning_rate 0.000100019
2017-09-29T13:43:17.021930: step 3049, loss 0.0230202, acc 1, learning_rate 0.000100019
2017-09-29T13:43:17.205012: step 3050, loss 0.0423624, acc 0.984375, learning_rate 0.000100019
2017-09-29T13:43:17.389256: step 3051, loss 0.089574, acc 0.96875, learning_rate 0.000100019
2017-09-29T13:43:17.572676: step 3052, loss 0.038044, acc 0.984375, learning_rate 0.000100019
2017-09-29T13:43:17.760508: step 3053, loss 0.0306249, acc 0.984375, learning_rate 0.000100019
2017-09-29T13:43:17.951551: step 3054, loss 0.0485608, acc 0.984375, learning_rate 0.000100018
2017-09-29T13:43:18.144700: step 3055, loss 0.0210912, acc 1, learning_rate 0.000100018
2017-09-29T13:43:18.332497: step 3056, loss 0.0457984, acc 0.96875, learning_rate 0.000100018
2017-09-29T13:43:18.526795: step 3057, loss 0.119822, acc 0.96875, learning_rate 0.000100018
2017-09-29T13:43:18.714839: step 3058, loss 0.0125938, acc 1, learning_rate 0.000100018
2017-09-29T13:43:18.906082: step 3059, loss 0.061809, acc 0.96875, learning_rate 0.000100018
2017-09-29T13:43:19.089403: step 3060, loss 0.0329076, acc 0.984375, learning_rate 0.000100018
2017-09-29T13:43:19.278513: step 3061, loss 0.0674156, acc 0.984375, learning_rate 0.000100018
2017-09-29T13:43:19.471618: step 3062, loss 0.118286, acc 0.953125, learning_rate 0.000100018
2017-09-29T13:43:19.661087: step 3063, loss 0.0363363, acc 0.984375, learning_rate 0.000100018
2017-09-29T13:43:19.850114: step 3064, loss 0.0349035, acc 0.984375, learning_rate 0.000100018
2017-09-29T13:43:20.037882: step 3065, loss 0.0150675, acc 1, learning_rate 0.000100018
2017-09-29T13:43:20.225872: step 3066, loss 0.0401773, acc 0.984375, learning_rate 0.000100018
2017-09-29T13:43:20.416001: step 3067, loss 0.0620829, acc 0.984375, learning_rate 0.000100018
2017-09-29T13:43:20.612964: step 3068, loss 0.0200371, acc 1, learning_rate 0.000100017
2017-09-29T13:43:20.805096: step 3069, loss 0.0249341, acc 1, learning_rate 0.000100017
2017-09-29T13:43:21.013112: step 3070, loss 0.0259609, acc 1, learning_rate 0.000100017
2017-09-29T13:43:21.202221: step 3071, loss 0.0601699, acc 0.984375, learning_rate 0.000100017
2017-09-29T13:43:21.412280: step 3072, loss 0.029472, acc 1, learning_rate 0.000100017
2017-09-29T13:43:21.610584: step 3073, loss 0.0318723, acc 1, learning_rate 0.000100017
2017-09-29T13:43:21.794745: step 3074, loss 0.0696815, acc 0.96875, learning_rate 0.000100017
2017-09-29T13:43:21.976610: step 3075, loss 0.0694058, acc 0.984375, learning_rate 0.000100017
2017-09-29T13:43:22.159412: step 3076, loss 0.0467517, acc 0.984375, learning_rate 0.000100017
2017-09-29T13:43:22.342873: step 3077, loss 0.0688329, acc 0.984375, learning_rate 0.000100017
2017-09-29T13:43:22.531883: step 3078, loss 0.0511689, acc 1, learning_rate 0.000100017
2017-09-29T13:43:22.714522: step 3079, loss 0.0677613, acc 0.984375, learning_rate 0.000100017
2017-09-29T13:43:22.903230: step 3080, loss 0.0447394, acc 0.984375, learning_rate 0.000100017

Evaluation:
2017-09-29T13:43:23.440511: step 3080, loss 0.209603, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3080

2017-09-29T13:43:24.244610: step 3081, loss 0.0936524, acc 0.984375, learning_rate 0.000100017
2017-09-29T13:43:24.449002: step 3082, loss 0.0613001, acc 0.96875, learning_rate 0.000100016
2017-09-29T13:43:24.634573: step 3083, loss 0.0199281, acc 1, learning_rate 0.000100016
2017-09-29T13:43:24.822876: step 3084, loss 0.0583147, acc 0.96875, learning_rate 0.000100016
2017-09-29T13:43:25.021204: step 3085, loss 0.0455315, acc 1, learning_rate 0.000100016
2017-09-29T13:43:25.211952: step 3086, loss 0.0587127, acc 0.984375, learning_rate 0.000100016
2017-09-29T13:43:25.401919: step 3087, loss 0.0951656, acc 0.96875, learning_rate 0.000100016
2017-09-29T13:43:25.592789: step 3088, loss 0.00940004, acc 1, learning_rate 0.000100016
2017-09-29T13:43:25.777140: step 3089, loss 0.114613, acc 0.953125, learning_rate 0.000100016
2017-09-29T13:43:25.963419: step 3090, loss 0.182576, acc 0.953125, learning_rate 0.000100016
2017-09-29T13:43:26.146930: step 3091, loss 0.0278665, acc 1, learning_rate 0.000100016
2017-09-29T13:43:26.339886: step 3092, loss 0.0257663, acc 1, learning_rate 0.000100016
2017-09-29T13:43:26.526931: step 3093, loss 0.0880243, acc 0.953125, learning_rate 0.000100016
2017-09-29T13:43:26.722992: step 3094, loss 0.0306489, acc 1, learning_rate 0.000100016
2017-09-29T13:43:26.911151: step 3095, loss 0.0961346, acc 0.9375, learning_rate 0.000100016
2017-09-29T13:43:27.096646: step 3096, loss 0.138405, acc 0.9375, learning_rate 0.000100016
2017-09-29T13:43:27.283171: step 3097, loss 0.0888008, acc 0.953125, learning_rate 0.000100016
2017-09-29T13:43:27.484649: step 3098, loss 0.0433769, acc 0.984375, learning_rate 0.000100015
2017-09-29T13:43:27.677371: step 3099, loss 0.0354125, acc 1, learning_rate 0.000100015
2017-09-29T13:43:27.864619: step 3100, loss 0.101385, acc 0.96875, learning_rate 0.000100015
2017-09-29T13:43:28.049778: step 3101, loss 0.0446609, acc 1, learning_rate 0.000100015
2017-09-29T13:43:28.246767: step 3102, loss 0.0145704, acc 1, learning_rate 0.000100015
2017-09-29T13:43:28.435120: step 3103, loss 0.0816984, acc 0.984375, learning_rate 0.000100015
2017-09-29T13:43:28.624509: step 3104, loss 0.186493, acc 0.953125, learning_rate 0.000100015
2017-09-29T13:43:28.810770: step 3105, loss 0.02894, acc 1, learning_rate 0.000100015
2017-09-29T13:43:29.001800: step 3106, loss 0.0189743, acc 1, learning_rate 0.000100015
2017-09-29T13:43:29.186735: step 3107, loss 0.0228071, acc 1, learning_rate 0.000100015
2017-09-29T13:43:29.374148: step 3108, loss 0.0207776, acc 1, learning_rate 0.000100015
2017-09-29T13:43:29.564184: step 3109, loss 0.0243159, acc 1, learning_rate 0.000100015
2017-09-29T13:43:29.748134: step 3110, loss 0.0308259, acc 0.984375, learning_rate 0.000100015
2017-09-29T13:43:29.935723: step 3111, loss 0.0109747, acc 1, learning_rate 0.000100015
2017-09-29T13:43:30.134546: step 3112, loss 0.0208693, acc 1, learning_rate 0.000100015
2017-09-29T13:43:30.332111: step 3113, loss 0.0664062, acc 0.984375, learning_rate 0.000100015
2017-09-29T13:43:30.527503: step 3114, loss 0.0443646, acc 0.984375, learning_rate 0.000100014
2017-09-29T13:43:30.714455: step 3115, loss 0.0594439, acc 1, learning_rate 0.000100014
2017-09-29T13:43:30.903933: step 3116, loss 0.121086, acc 0.96875, learning_rate 0.000100014
2017-09-29T13:43:31.095750: step 3117, loss 0.0102665, acc 1, learning_rate 0.000100014
2017-09-29T13:43:31.281771: step 3118, loss 0.0243321, acc 1, learning_rate 0.000100014
2017-09-29T13:43:31.470697: step 3119, loss 0.0187333, acc 1, learning_rate 0.000100014
2017-09-29T13:43:31.658052: step 3120, loss 0.0175314, acc 1, learning_rate 0.000100014

Evaluation:
2017-09-29T13:43:32.168841: step 3120, loss 0.212345, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3120

2017-09-29T13:43:32.823201: step 3121, loss 0.0414555, acc 1, learning_rate 0.000100014
2017-09-29T13:43:33.012156: step 3122, loss 0.0414988, acc 0.96875, learning_rate 0.000100014
2017-09-29T13:43:33.201539: step 3123, loss 0.0562344, acc 0.96875, learning_rate 0.000100014
2017-09-29T13:43:33.392202: step 3124, loss 0.0758514, acc 0.953125, learning_rate 0.000100014
2017-09-29T13:43:33.598237: step 3125, loss 0.0415045, acc 0.984375, learning_rate 0.000100014
2017-09-29T13:43:33.788895: step 3126, loss 0.0266435, acc 1, learning_rate 0.000100014
2017-09-29T13:43:33.979780: step 3127, loss 0.114648, acc 0.96875, learning_rate 0.000100014
2017-09-29T13:43:34.163657: step 3128, loss 0.051091, acc 0.96875, learning_rate 0.000100014
2017-09-29T13:43:34.346013: step 3129, loss 0.045116, acc 0.984375, learning_rate 0.000100014
2017-09-29T13:43:34.543221: step 3130, loss 0.0339535, acc 1, learning_rate 0.000100014
2017-09-29T13:43:34.731347: step 3131, loss 0.0541423, acc 0.96875, learning_rate 0.000100014
2017-09-29T13:43:34.918543: step 3132, loss 0.0474933, acc 0.984375, learning_rate 0.000100013
2017-09-29T13:43:35.107485: step 3133, loss 0.0534839, acc 0.984375, learning_rate 0.000100013
2017-09-29T13:43:35.300553: step 3134, loss 0.0941067, acc 0.96875, learning_rate 0.000100013
2017-09-29T13:43:35.488472: step 3135, loss 0.0769012, acc 0.984375, learning_rate 0.000100013
2017-09-29T13:43:35.643292: step 3136, loss 0.0660524, acc 0.980392, learning_rate 0.000100013
2017-09-29T13:43:35.833078: step 3137, loss 0.0805107, acc 0.953125, learning_rate 0.000100013
2017-09-29T13:43:36.019840: step 3138, loss 0.0565295, acc 0.984375, learning_rate 0.000100013
2017-09-29T13:43:36.204768: step 3139, loss 0.0726156, acc 0.96875, learning_rate 0.000100013
2017-09-29T13:43:36.393783: step 3140, loss 0.0214647, acc 1, learning_rate 0.000100013
2017-09-29T13:43:36.592448: step 3141, loss 0.0210946, acc 1, learning_rate 0.000100013
2017-09-29T13:43:36.779528: step 3142, loss 0.154103, acc 0.953125, learning_rate 0.000100013
2017-09-29T13:43:36.968378: step 3143, loss 0.0530201, acc 0.984375, learning_rate 0.000100013
2017-09-29T13:43:37.154358: step 3144, loss 0.120367, acc 0.953125, learning_rate 0.000100013
2017-09-29T13:43:37.339388: step 3145, loss 0.0461062, acc 0.984375, learning_rate 0.000100013
2017-09-29T13:43:37.525633: step 3146, loss 0.110341, acc 0.96875, learning_rate 0.000100013
2017-09-29T13:43:37.714100: step 3147, loss 0.0389314, acc 1, learning_rate 0.000100013
2017-09-29T13:43:37.898451: step 3148, loss 0.0357671, acc 0.984375, learning_rate 0.000100013
2017-09-29T13:43:38.079577: step 3149, loss 0.0210325, acc 1, learning_rate 0.000100013
2017-09-29T13:43:38.258848: step 3150, loss 0.0348516, acc 1, learning_rate 0.000100012
2017-09-29T13:43:38.448317: step 3151, loss 0.0342415, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:38.637461: step 3152, loss 0.0301346, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:38.819596: step 3153, loss 0.0461852, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:39.007855: step 3154, loss 0.0293211, acc 1, learning_rate 0.000100012
2017-09-29T13:43:39.194331: step 3155, loss 0.0499023, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:39.379107: step 3156, loss 0.0660596, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:39.566467: step 3157, loss 0.0909425, acc 0.96875, learning_rate 0.000100012
2017-09-29T13:43:39.748515: step 3158, loss 0.0232537, acc 1, learning_rate 0.000100012
2017-09-29T13:43:39.934081: step 3159, loss 0.0711265, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:40.113615: step 3160, loss 0.109742, acc 0.953125, learning_rate 0.000100012

Evaluation:
2017-09-29T13:43:40.624512: step 3160, loss 0.210007, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3160

2017-09-29T13:43:41.324065: step 3161, loss 0.120456, acc 0.953125, learning_rate 0.000100012
2017-09-29T13:43:41.510276: step 3162, loss 0.0871457, acc 0.96875, learning_rate 0.000100012
2017-09-29T13:43:41.694123: step 3163, loss 0.0853856, acc 0.96875, learning_rate 0.000100012
2017-09-29T13:43:41.880886: step 3164, loss 0.0886943, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:42.065186: step 3165, loss 0.0598851, acc 0.96875, learning_rate 0.000100012
2017-09-29T13:43:42.251963: step 3166, loss 0.0282408, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:42.444564: step 3167, loss 0.0525851, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:42.628813: step 3168, loss 0.0544163, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:42.817404: step 3169, loss 0.0242008, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:43.001098: step 3170, loss 0.0302459, acc 0.984375, learning_rate 0.000100012
2017-09-29T13:43:43.181962: step 3171, loss 0.0406924, acc 1, learning_rate 0.000100011
2017-09-29T13:43:43.375054: step 3172, loss 0.0422811, acc 0.984375, learning_rate 0.000100011
2017-09-29T13:43:43.561572: step 3173, loss 0.0277879, acc 1, learning_rate 0.000100011
2017-09-29T13:43:43.759803: step 3174, loss 0.052187, acc 0.984375, learning_rate 0.000100011
2017-09-29T13:43:43.939223: step 3175, loss 0.00856283, acc 1, learning_rate 0.000100011
2017-09-29T13:43:44.125372: step 3176, loss 0.0703985, acc 0.96875, learning_rate 0.000100011
2017-09-29T13:43:44.319104: step 3177, loss 0.0956393, acc 0.984375, learning_rate 0.000100011
2017-09-29T13:43:44.514214: step 3178, loss 0.0803077, acc 0.96875, learning_rate 0.000100011
2017-09-29T13:43:44.699446: step 3179, loss 0.0549774, acc 0.984375, learning_rate 0.000100011
2017-09-29T13:43:44.888775: step 3180, loss 0.0175236, acc 1, learning_rate 0.000100011
2017-09-29T13:43:45.074537: step 3181, loss 0.133154, acc 0.96875, learning_rate 0.000100011
2017-09-29T13:43:45.274529: step 3182, loss 0.0582135, acc 0.984375, learning_rate 0.000100011
2017-09-29T13:43:45.457533: step 3183, loss 0.0135686, acc 1, learning_rate 0.000100011
2017-09-29T13:43:45.640033: step 3184, loss 0.0747633, acc 0.96875, learning_rate 0.000100011
2017-09-29T13:43:45.823901: step 3185, loss 0.0504518, acc 0.984375, learning_rate 0.000100011
2017-09-29T13:43:46.009160: step 3186, loss 0.020067, acc 1, learning_rate 0.000100011
2017-09-29T13:43:46.197243: step 3187, loss 0.129704, acc 0.953125, learning_rate 0.000100011
2017-09-29T13:43:46.381337: step 3188, loss 0.0547171, acc 0.984375, learning_rate 0.000100011
2017-09-29T13:43:46.566444: step 3189, loss 0.0315019, acc 1, learning_rate 0.000100011
2017-09-29T13:43:46.756825: step 3190, loss 0.0522425, acc 0.984375, learning_rate 0.000100011
2017-09-29T13:43:46.952369: step 3191, loss 0.12238, acc 0.96875, learning_rate 0.000100011
2017-09-29T13:43:47.137746: step 3192, loss 0.0118175, acc 1, learning_rate 0.000100011
2017-09-29T13:43:47.319834: step 3193, loss 0.0444864, acc 1, learning_rate 0.00010001
2017-09-29T13:43:47.506242: step 3194, loss 0.111652, acc 0.984375, learning_rate 0.00010001
2017-09-29T13:43:47.691614: step 3195, loss 0.0366173, acc 1, learning_rate 0.00010001
2017-09-29T13:43:47.873439: step 3196, loss 0.0522563, acc 0.984375, learning_rate 0.00010001
2017-09-29T13:43:48.055072: step 3197, loss 0.0478061, acc 0.984375, learning_rate 0.00010001
2017-09-29T13:43:48.237995: step 3198, loss 0.0684757, acc 0.984375, learning_rate 0.00010001
2017-09-29T13:43:48.425058: step 3199, loss 0.0545298, acc 0.984375, learning_rate 0.00010001
2017-09-29T13:43:48.607915: step 3200, loss 0.0524948, acc 1, learning_rate 0.00010001

Evaluation:
2017-09-29T13:43:49.145435: step 3200, loss 0.21452, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3200

2017-09-29T13:43:49.936188: step 3201, loss 0.0235648, acc 1, learning_rate 0.00010001
2017-09-29T13:43:50.125918: step 3202, loss 0.0557743, acc 0.96875, learning_rate 0.00010001
2017-09-29T13:43:50.330053: step 3203, loss 0.0265197, acc 1, learning_rate 0.00010001
2017-09-29T13:43:50.512235: step 3204, loss 0.0779305, acc 0.96875, learning_rate 0.00010001
2017-09-29T13:43:50.695205: step 3205, loss 0.0666367, acc 0.953125, learning_rate 0.00010001
2017-09-29T13:43:50.881025: step 3206, loss 0.0361845, acc 1, learning_rate 0.00010001
2017-09-29T13:43:51.064222: step 3207, loss 0.0331903, acc 0.984375, learning_rate 0.00010001
2017-09-29T13:43:51.249274: step 3208, loss 0.102186, acc 0.96875, learning_rate 0.00010001
2017-09-29T13:43:51.430104: step 3209, loss 0.0522337, acc 0.96875, learning_rate 0.00010001
2017-09-29T13:43:51.613395: step 3210, loss 0.0226391, acc 1, learning_rate 0.00010001
2017-09-29T13:43:51.794148: step 3211, loss 0.0610241, acc 0.984375, learning_rate 0.00010001
2017-09-29T13:43:51.978163: step 3212, loss 0.0127271, acc 1, learning_rate 0.00010001
2017-09-29T13:43:52.161936: step 3213, loss 0.0216757, acc 1, learning_rate 0.00010001
2017-09-29T13:43:52.345776: step 3214, loss 0.0263492, acc 1, learning_rate 0.00010001
2017-09-29T13:43:52.540282: step 3215, loss 0.0764117, acc 0.96875, learning_rate 0.00010001
2017-09-29T13:43:52.747505: step 3216, loss 0.0131368, acc 1, learning_rate 0.00010001
2017-09-29T13:43:52.953709: step 3217, loss 0.0238204, acc 1, learning_rate 0.000100009
2017-09-29T13:43:53.159030: step 3218, loss 0.0521281, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:53.355160: step 3219, loss 0.0435667, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:53.541248: step 3220, loss 0.0258382, acc 1, learning_rate 0.000100009
2017-09-29T13:43:53.725799: step 3221, loss 0.0584219, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:53.913507: step 3222, loss 0.0350117, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:54.098791: step 3223, loss 0.0266213, acc 1, learning_rate 0.000100009
2017-09-29T13:43:54.293983: step 3224, loss 0.0973742, acc 0.96875, learning_rate 0.000100009
2017-09-29T13:43:54.489721: step 3225, loss 0.0766315, acc 0.953125, learning_rate 0.000100009
2017-09-29T13:43:54.675926: step 3226, loss 0.0504088, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:54.863030: step 3227, loss 0.0274794, acc 1, learning_rate 0.000100009
2017-09-29T13:43:55.056088: step 3228, loss 0.0585285, acc 1, learning_rate 0.000100009
2017-09-29T13:43:55.260096: step 3229, loss 0.0409153, acc 1, learning_rate 0.000100009
2017-09-29T13:43:55.466324: step 3230, loss 0.0351995, acc 1, learning_rate 0.000100009
2017-09-29T13:43:55.652313: step 3231, loss 0.123545, acc 0.96875, learning_rate 0.000100009
2017-09-29T13:43:55.836151: step 3232, loss 0.0506706, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:56.020434: step 3233, loss 0.0492125, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:56.171170: step 3234, loss 0.0672047, acc 0.980392, learning_rate 0.000100009
2017-09-29T13:43:56.356379: step 3235, loss 0.0216312, acc 1, learning_rate 0.000100009
2017-09-29T13:43:56.549774: step 3236, loss 0.0207565, acc 1, learning_rate 0.000100009
2017-09-29T13:43:56.742669: step 3237, loss 0.0188933, acc 1, learning_rate 0.000100009
2017-09-29T13:43:56.926936: step 3238, loss 0.0128201, acc 1, learning_rate 0.000100009
2017-09-29T13:43:57.108905: step 3239, loss 0.0338371, acc 1, learning_rate 0.000100009
2017-09-29T13:43:57.293408: step 3240, loss 0.0505879, acc 0.96875, learning_rate 0.000100009

Evaluation:
2017-09-29T13:43:57.814487: step 3240, loss 0.210757, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3240

2017-09-29T13:43:58.454591: step 3241, loss 0.0666079, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:58.637018: step 3242, loss 0.0679202, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:58.819609: step 3243, loss 0.0465429, acc 1, learning_rate 0.000100009
2017-09-29T13:43:59.001110: step 3244, loss 0.0295279, acc 0.984375, learning_rate 0.000100009
2017-09-29T13:43:59.188573: step 3245, loss 0.0266442, acc 1, learning_rate 0.000100008
2017-09-29T13:43:59.380196: step 3246, loss 0.0367812, acc 1, learning_rate 0.000100008
2017-09-29T13:43:59.565864: step 3247, loss 0.0911103, acc 0.96875, learning_rate 0.000100008
2017-09-29T13:43:59.760847: step 3248, loss 0.0491512, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:43:59.950956: step 3249, loss 0.0829577, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:00.137106: step 3250, loss 0.0915308, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:00.324475: step 3251, loss 0.0925653, acc 0.9375, learning_rate 0.000100008
2017-09-29T13:44:00.519295: step 3252, loss 0.02584, acc 1, learning_rate 0.000100008
2017-09-29T13:44:00.701032: step 3253, loss 0.0592069, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:00.885770: step 3254, loss 0.0276555, acc 1, learning_rate 0.000100008
2017-09-29T13:44:01.071275: step 3255, loss 0.124653, acc 0.953125, learning_rate 0.000100008
2017-09-29T13:44:01.254936: step 3256, loss 0.083565, acc 0.96875, learning_rate 0.000100008
2017-09-29T13:44:01.445679: step 3257, loss 0.0901222, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:01.634763: step 3258, loss 0.0398264, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:01.818373: step 3259, loss 0.0560965, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:02.004121: step 3260, loss 0.0504071, acc 1, learning_rate 0.000100008
2017-09-29T13:44:02.200099: step 3261, loss 0.0752229, acc 0.953125, learning_rate 0.000100008
2017-09-29T13:44:02.381231: step 3262, loss 0.0922305, acc 0.953125, learning_rate 0.000100008
2017-09-29T13:44:02.567275: step 3263, loss 0.0540285, acc 1, learning_rate 0.000100008
2017-09-29T13:44:02.757373: step 3264, loss 0.0330517, acc 1, learning_rate 0.000100008
2017-09-29T13:44:02.948109: step 3265, loss 0.0447056, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:03.134729: step 3266, loss 0.0337803, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:03.318055: step 3267, loss 0.049517, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:03.502302: step 3268, loss 0.0547383, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:03.689045: step 3269, loss 0.0593133, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:03.881485: step 3270, loss 0.0490581, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:04.062046: step 3271, loss 0.0321047, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:04.251645: step 3272, loss 0.0671195, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:04.434273: step 3273, loss 0.0483469, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:04.619448: step 3274, loss 0.0342196, acc 0.984375, learning_rate 0.000100008
2017-09-29T13:44:04.806682: step 3275, loss 0.0783873, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:04.990643: step 3276, loss 0.0282218, acc 1, learning_rate 0.000100007
2017-09-29T13:44:05.173859: step 3277, loss 0.0369162, acc 1, learning_rate 0.000100007
2017-09-29T13:44:05.367455: step 3278, loss 0.018165, acc 1, learning_rate 0.000100007
2017-09-29T13:44:05.559272: step 3279, loss 0.0416796, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:05.754312: step 3280, loss 0.0339771, acc 1, learning_rate 0.000100007

Evaluation:
2017-09-29T13:44:06.344020: step 3280, loss 0.212731, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3280

2017-09-29T13:44:07.078981: step 3281, loss 0.0488307, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:07.267482: step 3282, loss 0.0206637, acc 1, learning_rate 0.000100007
2017-09-29T13:44:07.468920: step 3283, loss 0.0670958, acc 0.96875, learning_rate 0.000100007
2017-09-29T13:44:07.650285: step 3284, loss 0.0182916, acc 1, learning_rate 0.000100007
2017-09-29T13:44:07.834645: step 3285, loss 0.0304726, acc 1, learning_rate 0.000100007
2017-09-29T13:44:08.022588: step 3286, loss 0.0508394, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:08.209260: step 3287, loss 0.0419739, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:08.400578: step 3288, loss 0.0936007, acc 0.96875, learning_rate 0.000100007
2017-09-29T13:44:08.597305: step 3289, loss 0.0680066, acc 0.96875, learning_rate 0.000100007
2017-09-29T13:44:08.792602: step 3290, loss 0.0588107, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:09.004195: step 3291, loss 0.0311978, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:09.196916: step 3292, loss 0.064552, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:09.386508: step 3293, loss 0.109001, acc 0.96875, learning_rate 0.000100007
2017-09-29T13:44:09.576043: step 3294, loss 0.0615829, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:09.759144: step 3295, loss 0.0714197, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:09.955178: step 3296, loss 0.108186, acc 0.953125, learning_rate 0.000100007
2017-09-29T13:44:10.137598: step 3297, loss 0.0352797, acc 1, learning_rate 0.000100007
2017-09-29T13:44:10.330889: step 3298, loss 0.0617242, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:10.534338: step 3299, loss 0.0664639, acc 0.96875, learning_rate 0.000100007
2017-09-29T13:44:10.721785: step 3300, loss 0.0230192, acc 1, learning_rate 0.000100007
2017-09-29T13:44:10.905999: step 3301, loss 0.0319123, acc 1, learning_rate 0.000100007
2017-09-29T13:44:11.092140: step 3302, loss 0.04432, acc 1, learning_rate 0.000100007
2017-09-29T13:44:11.283916: step 3303, loss 0.0372295, acc 1, learning_rate 0.000100007
2017-09-29T13:44:11.476265: step 3304, loss 0.0458909, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:11.674632: step 3305, loss 0.0279796, acc 1, learning_rate 0.000100007
2017-09-29T13:44:11.867271: step 3306, loss 0.0227965, acc 1, learning_rate 0.000100007
2017-09-29T13:44:12.058830: step 3307, loss 0.0391811, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:12.271424: step 3308, loss 0.0622911, acc 0.984375, learning_rate 0.000100007
2017-09-29T13:44:12.459473: step 3309, loss 0.0752591, acc 0.96875, learning_rate 0.000100007
2017-09-29T13:44:12.644665: step 3310, loss 0.0898111, acc 0.953125, learning_rate 0.000100006
2017-09-29T13:44:12.826815: step 3311, loss 0.0616593, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:13.009295: step 3312, loss 0.0301054, acc 1, learning_rate 0.000100006
2017-09-29T13:44:13.189499: step 3313, loss 0.0527678, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:13.372644: step 3314, loss 0.0360188, acc 1, learning_rate 0.000100006
2017-09-29T13:44:13.567053: step 3315, loss 0.0547677, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:13.752904: step 3316, loss 0.0598559, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:13.948743: step 3317, loss 0.0370081, acc 1, learning_rate 0.000100006
2017-09-29T13:44:14.151875: step 3318, loss 0.0285851, acc 1, learning_rate 0.000100006
2017-09-29T13:44:14.342387: step 3319, loss 0.0521084, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:14.540611: step 3320, loss 0.0916096, acc 0.984375, learning_rate 0.000100006

Evaluation:
2017-09-29T13:44:15.074687: step 3320, loss 0.212247, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3320

2017-09-29T13:44:15.882008: step 3321, loss 0.0603868, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:16.061656: step 3322, loss 0.0681525, acc 0.96875, learning_rate 0.000100006
2017-09-29T13:44:16.241895: step 3323, loss 0.0684651, acc 0.96875, learning_rate 0.000100006
2017-09-29T13:44:16.423472: step 3324, loss 0.0343267, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:16.604984: step 3325, loss 0.068565, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:16.790915: step 3326, loss 0.0269817, acc 1, learning_rate 0.000100006
2017-09-29T13:44:16.985661: step 3327, loss 0.06357, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:17.177550: step 3328, loss 0.0263342, acc 1, learning_rate 0.000100006
2017-09-29T13:44:17.365240: step 3329, loss 0.135639, acc 0.953125, learning_rate 0.000100006
2017-09-29T13:44:17.553937: step 3330, loss 0.0854083, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:17.742245: step 3331, loss 0.0710991, acc 0.96875, learning_rate 0.000100006
2017-09-29T13:44:17.897173: step 3332, loss 0.0284878, acc 1, learning_rate 0.000100006
2017-09-29T13:44:18.079030: step 3333, loss 0.0102961, acc 1, learning_rate 0.000100006
2017-09-29T13:44:18.260531: step 3334, loss 0.0252247, acc 1, learning_rate 0.000100006
2017-09-29T13:44:18.446844: step 3335, loss 0.0688877, acc 0.953125, learning_rate 0.000100006
2017-09-29T13:44:18.641761: step 3336, loss 0.0355843, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:18.836614: step 3337, loss 0.0312971, acc 1, learning_rate 0.000100006
2017-09-29T13:44:19.025076: step 3338, loss 0.0572691, acc 1, learning_rate 0.000100006
2017-09-29T13:44:19.203823: step 3339, loss 0.0399548, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:19.389162: step 3340, loss 0.151623, acc 0.96875, learning_rate 0.000100006
2017-09-29T13:44:19.588041: step 3341, loss 0.111087, acc 0.96875, learning_rate 0.000100006
2017-09-29T13:44:19.770517: step 3342, loss 0.0492547, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:19.951627: step 3343, loss 0.0400093, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:20.145744: step 3344, loss 0.0584166, acc 0.96875, learning_rate 0.000100006
2017-09-29T13:44:20.331895: step 3345, loss 0.0216258, acc 1, learning_rate 0.000100006
2017-09-29T13:44:20.531595: step 3346, loss 0.0822847, acc 0.953125, learning_rate 0.000100006
2017-09-29T13:44:20.714617: step 3347, loss 0.102198, acc 0.96875, learning_rate 0.000100006
2017-09-29T13:44:20.899992: step 3348, loss 0.0298357, acc 1, learning_rate 0.000100006
2017-09-29T13:44:21.080384: step 3349, loss 0.0371311, acc 1, learning_rate 0.000100006
2017-09-29T13:44:21.262658: step 3350, loss 0.0550966, acc 0.984375, learning_rate 0.000100006
2017-09-29T13:44:21.452127: step 3351, loss 0.0614125, acc 0.96875, learning_rate 0.000100005
2017-09-29T13:44:21.638520: step 3352, loss 0.101374, acc 0.96875, learning_rate 0.000100005
2017-09-29T13:44:21.829786: step 3353, loss 0.0150201, acc 1, learning_rate 0.000100005
2017-09-29T13:44:22.017422: step 3354, loss 0.0464608, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:22.211439: step 3355, loss 0.0204918, acc 1, learning_rate 0.000100005
2017-09-29T13:44:22.396823: step 3356, loss 0.0242196, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:22.588951: step 3357, loss 0.0295311, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:22.775961: step 3358, loss 0.033014, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:22.956606: step 3359, loss 0.0255676, acc 1, learning_rate 0.000100005
2017-09-29T13:44:23.139401: step 3360, loss 0.0253586, acc 1, learning_rate 0.000100005

Evaluation:
2017-09-29T13:44:23.662907: step 3360, loss 0.211431, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3360

2017-09-29T13:44:24.285088: step 3361, loss 0.0949466, acc 0.96875, learning_rate 0.000100005
2017-09-29T13:44:24.481745: step 3362, loss 0.0287806, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:24.679073: step 3363, loss 0.0316042, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:24.867832: step 3364, loss 0.0829635, acc 0.96875, learning_rate 0.000100005
2017-09-29T13:44:25.068035: step 3365, loss 0.0188329, acc 1, learning_rate 0.000100005
2017-09-29T13:44:25.255999: step 3366, loss 0.0560021, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:25.448250: step 3367, loss 0.0901143, acc 0.9375, learning_rate 0.000100005
2017-09-29T13:44:25.647711: step 3368, loss 0.0303767, acc 1, learning_rate 0.000100005
2017-09-29T13:44:25.848813: step 3369, loss 0.0427243, acc 1, learning_rate 0.000100005
2017-09-29T13:44:26.041713: step 3370, loss 0.00904335, acc 1, learning_rate 0.000100005
2017-09-29T13:44:26.230073: step 3371, loss 0.0560883, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:26.426280: step 3372, loss 0.0185613, acc 1, learning_rate 0.000100005
2017-09-29T13:44:26.624411: step 3373, loss 0.0403991, acc 1, learning_rate 0.000100005
2017-09-29T13:44:26.812025: step 3374, loss 0.090106, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:27.005102: step 3375, loss 0.0286876, acc 1, learning_rate 0.000100005
2017-09-29T13:44:27.191183: step 3376, loss 0.109528, acc 0.953125, learning_rate 0.000100005
2017-09-29T13:44:27.375855: step 3377, loss 0.0639118, acc 0.96875, learning_rate 0.000100005
2017-09-29T13:44:27.561726: step 3378, loss 0.0599767, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:27.752698: step 3379, loss 0.0570648, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:27.937623: step 3380, loss 0.0744856, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:28.134431: step 3381, loss 0.0870401, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:28.324508: step 3382, loss 0.0544938, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:28.507999: step 3383, loss 0.0575643, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:28.687945: step 3384, loss 0.0291417, acc 1, learning_rate 0.000100005
2017-09-29T13:44:28.872585: step 3385, loss 0.0263187, acc 1, learning_rate 0.000100005
2017-09-29T13:44:29.057913: step 3386, loss 0.0255647, acc 1, learning_rate 0.000100005
2017-09-29T13:44:29.245318: step 3387, loss 0.059225, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:29.446897: step 3388, loss 0.0575654, acc 0.96875, learning_rate 0.000100005
2017-09-29T13:44:29.630332: step 3389, loss 0.0632364, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:29.822507: step 3390, loss 0.0181914, acc 1, learning_rate 0.000100005
2017-09-29T13:44:30.018605: step 3391, loss 0.0595331, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:30.210326: step 3392, loss 0.0377315, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:30.394690: step 3393, loss 0.0234236, acc 1, learning_rate 0.000100005
2017-09-29T13:44:30.606341: step 3394, loss 0.0412288, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:30.788637: step 3395, loss 0.0831426, acc 0.953125, learning_rate 0.000100005
2017-09-29T13:44:30.972262: step 3396, loss 0.0794881, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:31.155482: step 3397, loss 0.00819535, acc 1, learning_rate 0.000100005
2017-09-29T13:44:31.335231: step 3398, loss 0.0886353, acc 0.96875, learning_rate 0.000100005
2017-09-29T13:44:31.517365: step 3399, loss 0.0696348, acc 0.984375, learning_rate 0.000100005
2017-09-29T13:44:31.710907: step 3400, loss 0.0497617, acc 0.984375, learning_rate 0.000100004

Evaluation:
2017-09-29T13:44:32.242042: step 3400, loss 0.215096, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3400

2017-09-29T13:44:32.944053: step 3401, loss 0.0373369, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:33.127287: step 3402, loss 0.0338687, acc 1, learning_rate 0.000100004
2017-09-29T13:44:33.308294: step 3403, loss 0.0754345, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:33.489906: step 3404, loss 0.0869032, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:33.675678: step 3405, loss 0.0437528, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:33.863908: step 3406, loss 0.0497184, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:34.063260: step 3407, loss 0.0269423, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:34.259720: step 3408, loss 0.0814834, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:34.448767: step 3409, loss 0.0233329, acc 1, learning_rate 0.000100004
2017-09-29T13:44:34.633545: step 3410, loss 0.020731, acc 1, learning_rate 0.000100004
2017-09-29T13:44:34.814823: step 3411, loss 0.0985121, acc 0.953125, learning_rate 0.000100004
2017-09-29T13:44:34.997906: step 3412, loss 0.0304029, acc 1, learning_rate 0.000100004
2017-09-29T13:44:35.181816: step 3413, loss 0.0206054, acc 1, learning_rate 0.000100004
2017-09-29T13:44:35.363597: step 3414, loss 0.0249619, acc 1, learning_rate 0.000100004
2017-09-29T13:44:35.570380: step 3415, loss 0.0364829, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:35.783677: step 3416, loss 0.0860796, acc 0.9375, learning_rate 0.000100004
2017-09-29T13:44:35.987144: step 3417, loss 0.0568938, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:36.186639: step 3418, loss 0.0227012, acc 1, learning_rate 0.000100004
2017-09-29T13:44:36.392199: step 3419, loss 0.0886594, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:36.597338: step 3420, loss 0.0232481, acc 1, learning_rate 0.000100004
2017-09-29T13:44:36.799433: step 3421, loss 0.0148524, acc 1, learning_rate 0.000100004
2017-09-29T13:44:37.009886: step 3422, loss 0.0480114, acc 1, learning_rate 0.000100004
2017-09-29T13:44:37.206905: step 3423, loss 0.0655533, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:37.391498: step 3424, loss 0.0140929, acc 1, learning_rate 0.000100004
2017-09-29T13:44:37.570954: step 3425, loss 0.0212414, acc 1, learning_rate 0.000100004
2017-09-29T13:44:37.756642: step 3426, loss 0.0503533, acc 1, learning_rate 0.000100004
2017-09-29T13:44:37.943821: step 3427, loss 0.0472753, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:38.126797: step 3428, loss 0.0520408, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:38.309850: step 3429, loss 0.0970421, acc 0.953125, learning_rate 0.000100004
2017-09-29T13:44:38.461489: step 3430, loss 0.0577351, acc 0.960784, learning_rate 0.000100004
2017-09-29T13:44:38.648739: step 3431, loss 0.115225, acc 0.953125, learning_rate 0.000100004
2017-09-29T13:44:38.833495: step 3432, loss 0.0702057, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:39.021581: step 3433, loss 0.0833007, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:39.206570: step 3434, loss 0.0345656, acc 1, learning_rate 0.000100004
2017-09-29T13:44:39.405989: step 3435, loss 0.038828, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:39.607237: step 3436, loss 0.0547089, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:39.797472: step 3437, loss 0.0535043, acc 1, learning_rate 0.000100004
2017-09-29T13:44:39.990553: step 3438, loss 0.0626617, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:40.175729: step 3439, loss 0.120681, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:40.365378: step 3440, loss 0.0805783, acc 0.96875, learning_rate 0.000100004

Evaluation:
2017-09-29T13:44:40.907669: step 3440, loss 0.21242, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3440

2017-09-29T13:44:41.633721: step 3441, loss 0.044284, acc 1, learning_rate 0.000100004
2017-09-29T13:44:41.828276: step 3442, loss 0.0211195, acc 1, learning_rate 0.000100004
2017-09-29T13:44:42.014137: step 3443, loss 0.0601678, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:42.202920: step 3444, loss 0.0320274, acc 1, learning_rate 0.000100004
2017-09-29T13:44:42.386785: step 3445, loss 0.02213, acc 1, learning_rate 0.000100004
2017-09-29T13:44:42.574465: step 3446, loss 0.0587769, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:42.769685: step 3447, loss 0.0405752, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:42.963882: step 3448, loss 0.0476372, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:43.161913: step 3449, loss 0.107412, acc 0.96875, learning_rate 0.000100004
2017-09-29T13:44:43.354923: step 3450, loss 0.0627476, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:43.541636: step 3451, loss 0.0341425, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:43.732160: step 3452, loss 0.0957698, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:43.916518: step 3453, loss 0.0102635, acc 1, learning_rate 0.000100004
2017-09-29T13:44:44.106220: step 3454, loss 0.0525806, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:44.294271: step 3455, loss 0.0148413, acc 1, learning_rate 0.000100004
2017-09-29T13:44:44.482908: step 3456, loss 0.0375758, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:44.672210: step 3457, loss 0.0321575, acc 1, learning_rate 0.000100004
2017-09-29T13:44:44.861542: step 3458, loss 0.027605, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:45.050208: step 3459, loss 0.0693882, acc 0.984375, learning_rate 0.000100004
2017-09-29T13:44:45.244414: step 3460, loss 0.0200251, acc 1, learning_rate 0.000100004
2017-09-29T13:44:45.449669: step 3461, loss 0.0351437, acc 1, learning_rate 0.000100004
2017-09-29T13:44:45.651715: step 3462, loss 0.0171693, acc 1, learning_rate 0.000100003
2017-09-29T13:44:45.835411: step 3463, loss 0.0697496, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:46.019592: step 3464, loss 0.0776008, acc 0.953125, learning_rate 0.000100003
2017-09-29T13:44:46.202931: step 3465, loss 0.0358186, acc 1, learning_rate 0.000100003
2017-09-29T13:44:46.403160: step 3466, loss 0.0363078, acc 1, learning_rate 0.000100003
2017-09-29T13:44:46.594904: step 3467, loss 0.0302228, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:46.779430: step 3468, loss 0.0520834, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:46.961540: step 3469, loss 0.124245, acc 0.9375, learning_rate 0.000100003
2017-09-29T13:44:47.147717: step 3470, loss 0.0485228, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:47.337317: step 3471, loss 0.0382515, acc 1, learning_rate 0.000100003
2017-09-29T13:44:47.541357: step 3472, loss 0.0553692, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:47.753439: step 3473, loss 0.0969121, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:47.952047: step 3474, loss 0.0388777, acc 1, learning_rate 0.000100003
2017-09-29T13:44:48.149919: step 3475, loss 0.0132655, acc 1, learning_rate 0.000100003
2017-09-29T13:44:48.337160: step 3476, loss 0.0480242, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:48.520128: step 3477, loss 0.0570722, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:48.704047: step 3478, loss 0.0429559, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:48.885923: step 3479, loss 0.0685726, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:49.068743: step 3480, loss 0.0682698, acc 0.96875, learning_rate 0.000100003

Evaluation:
2017-09-29T13:44:49.564298: step 3480, loss 0.210324, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3480

2017-09-29T13:44:50.394539: step 3481, loss 0.0176752, acc 1, learning_rate 0.000100003
2017-09-29T13:44:50.582178: step 3482, loss 0.0604839, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:50.773907: step 3483, loss 0.041735, acc 1, learning_rate 0.000100003
2017-09-29T13:44:50.959206: step 3484, loss 0.0689695, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:51.145624: step 3485, loss 0.0730716, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:51.326801: step 3486, loss 0.0588531, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:51.520762: step 3487, loss 0.0244955, acc 1, learning_rate 0.000100003
2017-09-29T13:44:51.702591: step 3488, loss 0.0672796, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:51.883483: step 3489, loss 0.0510104, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:52.067355: step 3490, loss 0.0277356, acc 1, learning_rate 0.000100003
2017-09-29T13:44:52.250900: step 3491, loss 0.0233762, acc 1, learning_rate 0.000100003
2017-09-29T13:44:52.452904: step 3492, loss 0.0549162, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:52.647592: step 3493, loss 0.00801865, acc 1, learning_rate 0.000100003
2017-09-29T13:44:52.860328: step 3494, loss 0.0734164, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:53.067053: step 3495, loss 0.122024, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:53.250968: step 3496, loss 0.0599043, acc 0.953125, learning_rate 0.000100003
2017-09-29T13:44:53.434569: step 3497, loss 0.0577381, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:53.625081: step 3498, loss 0.0605827, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:53.811229: step 3499, loss 0.0641361, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:53.991514: step 3500, loss 0.0495868, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:54.175436: step 3501, loss 0.0196953, acc 1, learning_rate 0.000100003
2017-09-29T13:44:54.360584: step 3502, loss 0.030306, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:54.549750: step 3503, loss 0.103224, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:54.753459: step 3504, loss 0.0763209, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:54.965208: step 3505, loss 0.0788372, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:55.166346: step 3506, loss 0.0114428, acc 1, learning_rate 0.000100003
2017-09-29T13:44:55.364185: step 3507, loss 0.0885966, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:55.563210: step 3508, loss 0.0548199, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:55.757715: step 3509, loss 0.0322934, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:55.949363: step 3510, loss 0.0235247, acc 1, learning_rate 0.000100003
2017-09-29T13:44:56.137502: step 3511, loss 0.0198602, acc 1, learning_rate 0.000100003
2017-09-29T13:44:56.326302: step 3512, loss 0.0756083, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:56.513002: step 3513, loss 0.160186, acc 0.953125, learning_rate 0.000100003
2017-09-29T13:44:56.699768: step 3514, loss 0.0239153, acc 1, learning_rate 0.000100003
2017-09-29T13:44:56.880365: step 3515, loss 0.0298818, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:57.077985: step 3516, loss 0.0719094, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:57.262326: step 3517, loss 0.0344119, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:57.447284: step 3518, loss 0.0644332, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:57.638326: step 3519, loss 0.0172084, acc 1, learning_rate 0.000100003
2017-09-29T13:44:57.825604: step 3520, loss 0.039998, acc 0.984375, learning_rate 0.000100003

Evaluation:
2017-09-29T13:44:58.339743: step 3520, loss 0.213847, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3520

2017-09-29T13:44:59.035619: step 3521, loss 0.0476056, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:59.229471: step 3522, loss 0.0616585, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:44:59.413069: step 3523, loss 0.0209994, acc 1, learning_rate 0.000100003
2017-09-29T13:44:59.605241: step 3524, loss 0.0285074, acc 1, learning_rate 0.000100003
2017-09-29T13:44:59.793826: step 3525, loss 0.0570141, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:44:59.994292: step 3526, loss 0.0548432, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:45:00.180992: step 3527, loss 0.0611276, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:45:00.330099: step 3528, loss 0.0126063, acc 1, learning_rate 0.000100003
2017-09-29T13:45:00.523973: step 3529, loss 0.0443344, acc 1, learning_rate 0.000100003
2017-09-29T13:45:00.708942: step 3530, loss 0.0491422, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:45:00.896504: step 3531, loss 0.0790365, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:45:01.080588: step 3532, loss 0.0487796, acc 1, learning_rate 0.000100003
2017-09-29T13:45:01.266934: step 3533, loss 0.0864026, acc 0.96875, learning_rate 0.000100003
2017-09-29T13:45:01.450804: step 3534, loss 0.0315394, acc 1, learning_rate 0.000100003
2017-09-29T13:45:01.629852: step 3535, loss 0.0688512, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:45:01.813450: step 3536, loss 0.0420209, acc 1, learning_rate 0.000100003
2017-09-29T13:45:02.009981: step 3537, loss 0.0207427, acc 1, learning_rate 0.000100003
2017-09-29T13:45:02.202240: step 3538, loss 0.0124451, acc 1, learning_rate 0.000100003
2017-09-29T13:45:02.384079: step 3539, loss 0.0426533, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:45:02.567479: step 3540, loss 0.0754816, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:45:02.753199: step 3541, loss 0.124453, acc 0.953125, learning_rate 0.000100003
2017-09-29T13:45:02.945909: step 3542, loss 0.0477755, acc 0.984375, learning_rate 0.000100003
2017-09-29T13:45:03.141274: step 3543, loss 0.0200051, acc 1, learning_rate 0.000100003
2017-09-29T13:45:03.332123: step 3544, loss 0.0163088, acc 1, learning_rate 0.000100002
2017-09-29T13:45:03.531041: step 3545, loss 0.0262961, acc 1, learning_rate 0.000100002
2017-09-29T13:45:03.713378: step 3546, loss 0.0657893, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:03.892483: step 3547, loss 0.0617062, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:04.076855: step 3548, loss 0.0337805, acc 1, learning_rate 0.000100002
2017-09-29T13:45:04.262528: step 3549, loss 0.0535401, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:04.450114: step 3550, loss 0.0808987, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:04.640215: step 3551, loss 0.0438996, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:04.846918: step 3552, loss 0.0421079, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:05.039200: step 3553, loss 0.022368, acc 1, learning_rate 0.000100002
2017-09-29T13:45:05.229105: step 3554, loss 0.011746, acc 1, learning_rate 0.000100002
2017-09-29T13:45:05.435149: step 3555, loss 0.059923, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:05.621119: step 3556, loss 0.0369681, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:05.819025: step 3557, loss 0.0194202, acc 1, learning_rate 0.000100002
2017-09-29T13:45:06.010280: step 3558, loss 0.0848281, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:06.193385: step 3559, loss 0.0408434, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:06.379062: step 3560, loss 0.0324085, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-09-29T13:45:06.925755: step 3560, loss 0.210014, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3560

2017-09-29T13:45:07.657302: step 3561, loss 0.0306311, acc 1, learning_rate 0.000100002
2017-09-29T13:45:07.845039: step 3562, loss 0.0425, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:08.036207: step 3563, loss 0.0471467, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:08.227883: step 3564, loss 0.0641405, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:08.417695: step 3565, loss 0.0786874, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:08.605326: step 3566, loss 0.0283414, acc 1, learning_rate 0.000100002
2017-09-29T13:45:08.789936: step 3567, loss 0.0555825, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:09.001130: step 3568, loss 0.0707355, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:09.206385: step 3569, loss 0.0763681, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:09.417875: step 3570, loss 0.0299477, acc 1, learning_rate 0.000100002
2017-09-29T13:45:09.634844: step 3571, loss 0.0279243, acc 1, learning_rate 0.000100002
2017-09-29T13:45:09.839205: step 3572, loss 0.0175647, acc 1, learning_rate 0.000100002
2017-09-29T13:45:10.043293: step 3573, loss 0.0261095, acc 1, learning_rate 0.000100002
2017-09-29T13:45:10.224178: step 3574, loss 0.165311, acc 0.9375, learning_rate 0.000100002
2017-09-29T13:45:10.419957: step 3575, loss 0.0134041, acc 1, learning_rate 0.000100002
2017-09-29T13:45:10.605365: step 3576, loss 0.0162516, acc 1, learning_rate 0.000100002
2017-09-29T13:45:10.791590: step 3577, loss 0.0490026, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:10.975998: step 3578, loss 0.0348608, acc 1, learning_rate 0.000100002
2017-09-29T13:45:11.155401: step 3579, loss 0.0744851, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:11.339232: step 3580, loss 0.0233385, acc 1, learning_rate 0.000100002
2017-09-29T13:45:11.524540: step 3581, loss 0.0378715, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:11.705814: step 3582, loss 0.0361447, acc 1, learning_rate 0.000100002
2017-09-29T13:45:11.889118: step 3583, loss 0.0186282, acc 1, learning_rate 0.000100002
2017-09-29T13:45:12.071472: step 3584, loss 0.0379678, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:12.267993: step 3585, loss 0.0197107, acc 1, learning_rate 0.000100002
2017-09-29T13:45:12.463893: step 3586, loss 0.0484009, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:12.649922: step 3587, loss 0.0243807, acc 1, learning_rate 0.000100002
2017-09-29T13:45:12.835569: step 3588, loss 0.0380822, acc 1, learning_rate 0.000100002
2017-09-29T13:45:13.021190: step 3589, loss 0.054042, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:13.211081: step 3590, loss 0.0205555, acc 1, learning_rate 0.000100002
2017-09-29T13:45:13.395416: step 3591, loss 0.123688, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:13.587522: step 3592, loss 0.0918148, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:13.769500: step 3593, loss 0.0411882, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:13.954849: step 3594, loss 0.0290913, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:14.149025: step 3595, loss 0.042664, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:14.331776: step 3596, loss 0.034987, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:14.512380: step 3597, loss 0.0550894, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:14.710161: step 3598, loss 0.0368247, acc 1, learning_rate 0.000100002
2017-09-29T13:45:14.891949: step 3599, loss 0.0479834, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:15.081696: step 3600, loss 0.0408373, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-09-29T13:45:15.627466: step 3600, loss 0.214993, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3600

2017-09-29T13:45:16.423641: step 3601, loss 0.0390989, acc 1, learning_rate 0.000100002
2017-09-29T13:45:16.611255: step 3602, loss 0.099066, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:16.796571: step 3603, loss 0.0288939, acc 1, learning_rate 0.000100002
2017-09-29T13:45:16.978428: step 3604, loss 0.0131576, acc 1, learning_rate 0.000100002
2017-09-29T13:45:17.168064: step 3605, loss 0.0281415, acc 1, learning_rate 0.000100002
2017-09-29T13:45:17.350671: step 3606, loss 0.0140191, acc 1, learning_rate 0.000100002
2017-09-29T13:45:17.545414: step 3607, loss 0.11118, acc 0.9375, learning_rate 0.000100002
2017-09-29T13:45:17.731159: step 3608, loss 0.02266, acc 1, learning_rate 0.000100002
2017-09-29T13:45:17.922465: step 3609, loss 0.0301911, acc 1, learning_rate 0.000100002
2017-09-29T13:45:18.105906: step 3610, loss 0.0695204, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:18.284343: step 3611, loss 0.0768719, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:18.469873: step 3612, loss 0.0786653, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:18.652572: step 3613, loss 0.0759842, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:18.837279: step 3614, loss 0.0553395, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:19.019291: step 3615, loss 0.0405355, acc 1, learning_rate 0.000100002
2017-09-29T13:45:19.202782: step 3616, loss 0.0443072, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:19.391808: step 3617, loss 0.0270494, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:19.575208: step 3618, loss 0.0348203, acc 1, learning_rate 0.000100002
2017-09-29T13:45:19.762193: step 3619, loss 0.0303627, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:19.942825: step 3620, loss 0.0513114, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:20.125242: step 3621, loss 0.0205153, acc 1, learning_rate 0.000100002
2017-09-29T13:45:20.308431: step 3622, loss 0.0494542, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:20.494194: step 3623, loss 0.0718215, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:20.679850: step 3624, loss 0.0643606, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:20.871762: step 3625, loss 0.074346, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:21.024286: step 3626, loss 0.0399768, acc 1, learning_rate 0.000100002
2017-09-29T13:45:21.203915: step 3627, loss 0.0431899, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:21.408012: step 3628, loss 0.0411516, acc 1, learning_rate 0.000100002
2017-09-29T13:45:21.597849: step 3629, loss 0.0571292, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:21.780051: step 3630, loss 0.0541832, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:21.964673: step 3631, loss 0.0929064, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:22.148577: step 3632, loss 0.0200272, acc 1, learning_rate 0.000100002
2017-09-29T13:45:22.330672: step 3633, loss 0.0940601, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:22.514852: step 3634, loss 0.0464739, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:22.701664: step 3635, loss 0.0472091, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:22.885550: step 3636, loss 0.0472273, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:23.071760: step 3637, loss 0.0427621, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:23.255916: step 3638, loss 0.0846305, acc 0.953125, learning_rate 0.000100002
2017-09-29T13:45:23.438798: step 3639, loss 0.0577224, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:23.630319: step 3640, loss 0.0858491, acc 0.953125, learning_rate 0.000100002

Evaluation:
2017-09-29T13:45:24.140494: step 3640, loss 0.211973, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3640

2017-09-29T13:45:24.776519: step 3641, loss 0.0336197, acc 1, learning_rate 0.000100002
2017-09-29T13:45:24.963143: step 3642, loss 0.108834, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:25.146981: step 3643, loss 0.0174597, acc 1, learning_rate 0.000100002
2017-09-29T13:45:25.328545: step 3644, loss 0.0240322, acc 1, learning_rate 0.000100002
2017-09-29T13:45:25.513603: step 3645, loss 0.0191258, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:25.697453: step 3646, loss 0.0438052, acc 1, learning_rate 0.000100002
2017-09-29T13:45:25.884765: step 3647, loss 0.0382022, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:26.066264: step 3648, loss 0.0107073, acc 1, learning_rate 0.000100002
2017-09-29T13:45:26.249732: step 3649, loss 0.0951171, acc 0.953125, learning_rate 0.000100002
2017-09-29T13:45:26.439404: step 3650, loss 0.0370533, acc 1, learning_rate 0.000100002
2017-09-29T13:45:26.628354: step 3651, loss 0.0565937, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:26.824689: step 3652, loss 0.0266835, acc 1, learning_rate 0.000100002
2017-09-29T13:45:27.008083: step 3653, loss 0.0895559, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:27.192224: step 3654, loss 0.0304746, acc 1, learning_rate 0.000100002
2017-09-29T13:45:27.375625: step 3655, loss 0.0390483, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:27.559292: step 3656, loss 0.0955642, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:27.747069: step 3657, loss 0.0338401, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:27.930362: step 3658, loss 0.0277822, acc 1, learning_rate 0.000100002
2017-09-29T13:45:28.118906: step 3659, loss 0.0367819, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:28.314941: step 3660, loss 0.0541062, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:28.517711: step 3661, loss 0.0109346, acc 1, learning_rate 0.000100002
2017-09-29T13:45:28.704811: step 3662, loss 0.103085, acc 0.953125, learning_rate 0.000100002
2017-09-29T13:45:28.908490: step 3663, loss 0.0579635, acc 0.984375, learning_rate 0.000100002
2017-09-29T13:45:29.095095: step 3664, loss 0.0349734, acc 1, learning_rate 0.000100002
2017-09-29T13:45:29.295968: step 3665, loss 0.0192833, acc 1, learning_rate 0.000100002
2017-09-29T13:45:29.483703: step 3666, loss 0.0620011, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:29.668273: step 3667, loss 0.0904468, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:29.852594: step 3668, loss 0.0914778, acc 0.96875, learning_rate 0.000100002
2017-09-29T13:45:30.041090: step 3669, loss 0.0349808, acc 1, learning_rate 0.000100001
2017-09-29T13:45:30.222071: step 3670, loss 0.0172712, acc 1, learning_rate 0.000100001
2017-09-29T13:45:30.401686: step 3671, loss 0.0316659, acc 1, learning_rate 0.000100001
2017-09-29T13:45:30.590652: step 3672, loss 0.0105051, acc 1, learning_rate 0.000100001
2017-09-29T13:45:30.801962: step 3673, loss 0.0226491, acc 1, learning_rate 0.000100001
2017-09-29T13:45:31.027676: step 3674, loss 0.104414, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:31.235450: step 3675, loss 0.0336015, acc 1, learning_rate 0.000100001
2017-09-29T13:45:31.441586: step 3676, loss 0.0232341, acc 1, learning_rate 0.000100001
2017-09-29T13:45:31.626205: step 3677, loss 0.0524857, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:31.815696: step 3678, loss 0.0825327, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:32.006312: step 3679, loss 0.0136921, acc 1, learning_rate 0.000100001
2017-09-29T13:45:32.198733: step 3680, loss 0.0188953, acc 1, learning_rate 0.000100001

Evaluation:
2017-09-29T13:45:32.740906: step 3680, loss 0.211849, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3680

2017-09-29T13:45:33.487051: step 3681, loss 0.0362961, acc 1, learning_rate 0.000100001
2017-09-29T13:45:33.672236: step 3682, loss 0.0411455, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:33.869054: step 3683, loss 0.023139, acc 1, learning_rate 0.000100001
2017-09-29T13:45:34.069873: step 3684, loss 0.00993885, acc 1, learning_rate 0.000100001
2017-09-29T13:45:34.281126: step 3685, loss 0.016586, acc 1, learning_rate 0.000100001
2017-09-29T13:45:34.500115: step 3686, loss 0.0503028, acc 1, learning_rate 0.000100001
2017-09-29T13:45:34.699287: step 3687, loss 0.0565377, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:34.919686: step 3688, loss 0.0632704, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:35.135425: step 3689, loss 0.0324012, acc 1, learning_rate 0.000100001
2017-09-29T13:45:35.345393: step 3690, loss 0.0436808, acc 1, learning_rate 0.000100001
2017-09-29T13:45:35.542403: step 3691, loss 0.0430843, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:35.741563: step 3692, loss 0.0653686, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:35.943978: step 3693, loss 0.0514579, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:36.161476: step 3694, loss 0.0766933, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:36.367910: step 3695, loss 0.0223893, acc 1, learning_rate 0.000100001
2017-09-29T13:45:36.588783: step 3696, loss 0.0282619, acc 1, learning_rate 0.000100001
2017-09-29T13:45:36.792043: step 3697, loss 0.0633655, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:36.996261: step 3698, loss 0.121697, acc 0.9375, learning_rate 0.000100001
2017-09-29T13:45:37.196381: step 3699, loss 0.0292709, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:37.394987: step 3700, loss 0.0162925, acc 1, learning_rate 0.000100001
2017-09-29T13:45:37.596258: step 3701, loss 0.179243, acc 0.921875, learning_rate 0.000100001
2017-09-29T13:45:37.816979: step 3702, loss 0.0870329, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:38.016862: step 3703, loss 0.0213225, acc 1, learning_rate 0.000100001
2017-09-29T13:45:38.212298: step 3704, loss 0.0698747, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:38.417660: step 3705, loss 0.037938, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:38.640471: step 3706, loss 0.0385957, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:38.843293: step 3707, loss 0.0471689, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:39.061723: step 3708, loss 0.0576539, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:39.266935: step 3709, loss 0.0202463, acc 1, learning_rate 0.000100001
2017-09-29T13:45:39.468913: step 3710, loss 0.0195384, acc 1, learning_rate 0.000100001
2017-09-29T13:45:39.674927: step 3711, loss 0.0420724, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:39.879810: step 3712, loss 0.123735, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:40.080341: step 3713, loss 0.02925, acc 1, learning_rate 0.000100001
2017-09-29T13:45:40.277292: step 3714, loss 0.0286124, acc 1, learning_rate 0.000100001
2017-09-29T13:45:40.483272: step 3715, loss 0.0497663, acc 1, learning_rate 0.000100001
2017-09-29T13:45:40.710617: step 3716, loss 0.0271618, acc 1, learning_rate 0.000100001
2017-09-29T13:45:40.910562: step 3717, loss 0.0607007, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:41.122278: step 3718, loss 0.0436549, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:41.335134: step 3719, loss 0.0163573, acc 1, learning_rate 0.000100001
2017-09-29T13:45:41.540518: step 3720, loss 0.106467, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-09-29T13:45:42.108928: step 3720, loss 0.211579, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3720

2017-09-29T13:45:42.933268: step 3721, loss 0.0485253, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:43.145612: step 3722, loss 0.0575985, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:43.350398: step 3723, loss 0.0360885, acc 1, learning_rate 0.000100001
2017-09-29T13:45:43.526557: step 3724, loss 0.015856, acc 1, learning_rate 0.000100001
2017-09-29T13:45:43.736573: step 3725, loss 0.0521649, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:43.934251: step 3726, loss 0.0405046, acc 1, learning_rate 0.000100001
2017-09-29T13:45:44.134462: step 3727, loss 0.0374796, acc 1, learning_rate 0.000100001
2017-09-29T13:45:44.343144: step 3728, loss 0.00728336, acc 1, learning_rate 0.000100001
2017-09-29T13:45:44.562982: step 3729, loss 0.0491542, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:44.765575: step 3730, loss 0.0666395, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:44.970280: step 3731, loss 0.0179366, acc 1, learning_rate 0.000100001
2017-09-29T13:45:45.188947: step 3732, loss 0.037235, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:45.392821: step 3733, loss 0.013659, acc 1, learning_rate 0.000100001
2017-09-29T13:45:45.592732: step 3734, loss 0.0234512, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:45.798558: step 3735, loss 0.154019, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:46.016176: step 3736, loss 0.0376668, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:46.219462: step 3737, loss 0.033045, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:46.445783: step 3738, loss 0.0377582, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:46.645960: step 3739, loss 0.0376776, acc 1, learning_rate 0.000100001
2017-09-29T13:45:46.845257: step 3740, loss 0.0662488, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:47.044114: step 3741, loss 0.0498286, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:47.247094: step 3742, loss 0.0610318, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:47.461554: step 3743, loss 0.0596418, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:47.666560: step 3744, loss 0.0707089, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:47.865827: step 3745, loss 0.0118114, acc 1, learning_rate 0.000100001
2017-09-29T13:45:48.055115: step 3746, loss 0.119316, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:48.239473: step 3747, loss 0.0523025, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:48.424276: step 3748, loss 0.0241676, acc 1, learning_rate 0.000100001
2017-09-29T13:45:48.606132: step 3749, loss 0.0399574, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:48.792234: step 3750, loss 0.0212902, acc 1, learning_rate 0.000100001
2017-09-29T13:45:48.974875: step 3751, loss 0.0984297, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:49.170361: step 3752, loss 0.0145437, acc 1, learning_rate 0.000100001
2017-09-29T13:45:49.350222: step 3753, loss 0.0869748, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:49.535391: step 3754, loss 0.0847476, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:49.717933: step 3755, loss 0.0394367, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:49.898775: step 3756, loss 0.0665823, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:50.084639: step 3757, loss 0.0402081, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:50.270054: step 3758, loss 0.0448208, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:50.468442: step 3759, loss 0.0951824, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:50.657806: step 3760, loss 0.0135844, acc 1, learning_rate 0.000100001

Evaluation:
2017-09-29T13:45:51.179404: step 3760, loss 0.216033, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3760

2017-09-29T13:45:51.816815: step 3761, loss 0.018874, acc 1, learning_rate 0.000100001
2017-09-29T13:45:52.004145: step 3762, loss 0.0995072, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:52.191717: step 3763, loss 0.0291184, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:52.383797: step 3764, loss 0.0695309, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:52.569379: step 3765, loss 0.00928073, acc 1, learning_rate 0.000100001
2017-09-29T13:45:52.751129: step 3766, loss 0.0198701, acc 1, learning_rate 0.000100001
2017-09-29T13:45:52.934294: step 3767, loss 0.0208896, acc 1, learning_rate 0.000100001
2017-09-29T13:45:53.117174: step 3768, loss 0.0401913, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:53.302121: step 3769, loss 0.0401191, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:53.496865: step 3770, loss 0.0857745, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:53.682758: step 3771, loss 0.0564506, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:53.868231: step 3772, loss 0.0478398, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:54.057451: step 3773, loss 0.0234165, acc 1, learning_rate 0.000100001
2017-09-29T13:45:54.236235: step 3774, loss 0.0677322, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:54.421819: step 3775, loss 0.038417, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:54.605794: step 3776, loss 0.0225205, acc 1, learning_rate 0.000100001
2017-09-29T13:45:54.795975: step 3777, loss 0.029772, acc 1, learning_rate 0.000100001
2017-09-29T13:45:54.980394: step 3778, loss 0.0601285, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:55.171881: step 3779, loss 0.0444639, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:55.357473: step 3780, loss 0.0241553, acc 1, learning_rate 0.000100001
2017-09-29T13:45:55.552511: step 3781, loss 0.0305219, acc 1, learning_rate 0.000100001
2017-09-29T13:45:55.746023: step 3782, loss 0.0321542, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:55.930189: step 3783, loss 0.023727, acc 1, learning_rate 0.000100001
2017-09-29T13:45:56.122443: step 3784, loss 0.0409261, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:56.309713: step 3785, loss 0.0828287, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:56.494031: step 3786, loss 0.0675731, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:45:56.678982: step 3787, loss 0.0202803, acc 1, learning_rate 0.000100001
2017-09-29T13:45:56.875239: step 3788, loss 0.0859432, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:57.073044: step 3789, loss 0.121281, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:45:57.269524: step 3790, loss 0.0417174, acc 1, learning_rate 0.000100001
2017-09-29T13:45:57.472811: step 3791, loss 0.0298316, acc 1, learning_rate 0.000100001
2017-09-29T13:45:57.683056: step 3792, loss 0.0567233, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:57.884037: step 3793, loss 0.0235532, acc 1, learning_rate 0.000100001
2017-09-29T13:45:58.080930: step 3794, loss 0.0442584, acc 1, learning_rate 0.000100001
2017-09-29T13:45:58.284652: step 3795, loss 0.033058, acc 1, learning_rate 0.000100001
2017-09-29T13:45:58.489360: step 3796, loss 0.0472501, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:58.698923: step 3797, loss 0.0779434, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:58.900255: step 3798, loss 0.0696463, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:59.112068: step 3799, loss 0.0615975, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:45:59.315774: step 3800, loss 0.0436716, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-09-29T13:45:59.879774: step 3800, loss 0.213234, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3800

2017-09-29T13:46:00.625495: step 3801, loss 0.0243516, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:00.829288: step 3802, loss 0.0391747, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:01.031360: step 3803, loss 0.0176523, acc 1, learning_rate 0.000100001
2017-09-29T13:46:01.238569: step 3804, loss 0.101702, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:46:01.450140: step 3805, loss 0.12022, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:46:01.659852: step 3806, loss 0.0409664, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:01.864388: step 3807, loss 0.0454199, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:02.067013: step 3808, loss 0.0387026, acc 1, learning_rate 0.000100001
2017-09-29T13:46:02.268719: step 3809, loss 0.124423, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:02.468747: step 3810, loss 0.0615142, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:02.670181: step 3811, loss 0.0193095, acc 1, learning_rate 0.000100001
2017-09-29T13:46:02.872881: step 3812, loss 0.0512373, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:03.074843: step 3813, loss 0.0419624, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:03.275557: step 3814, loss 0.060647, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:03.471944: step 3815, loss 0.103075, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:03.676932: step 3816, loss 0.0585731, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:03.876457: step 3817, loss 0.0860579, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:04.076504: step 3818, loss 0.0137645, acc 1, learning_rate 0.000100001
2017-09-29T13:46:04.275683: step 3819, loss 0.0405357, acc 1, learning_rate 0.000100001
2017-09-29T13:46:04.481007: step 3820, loss 0.0358201, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:04.683799: step 3821, loss 0.0388833, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:04.853203: step 3822, loss 0.12041, acc 0.941176, learning_rate 0.000100001
2017-09-29T13:46:05.055754: step 3823, loss 0.0247764, acc 1, learning_rate 0.000100001
2017-09-29T13:46:05.259362: step 3824, loss 0.0877559, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:05.465092: step 3825, loss 0.0546464, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:05.672495: step 3826, loss 0.0401403, acc 1, learning_rate 0.000100001
2017-09-29T13:46:05.869047: step 3827, loss 0.0560983, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:06.082028: step 3828, loss 0.0290307, acc 1, learning_rate 0.000100001
2017-09-29T13:46:06.278751: step 3829, loss 0.0317165, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:06.480445: step 3830, loss 0.0392039, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:06.687338: step 3831, loss 0.105802, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:06.889630: step 3832, loss 0.0256744, acc 1, learning_rate 0.000100001
2017-09-29T13:46:07.091297: step 3833, loss 0.0552224, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:07.291194: step 3834, loss 0.0248922, acc 1, learning_rate 0.000100001
2017-09-29T13:46:07.501488: step 3835, loss 0.0226088, acc 1, learning_rate 0.000100001
2017-09-29T13:46:07.714515: step 3836, loss 0.0741881, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:07.912247: step 3837, loss 0.0362077, acc 1, learning_rate 0.000100001
2017-09-29T13:46:08.123687: step 3838, loss 0.0451878, acc 1, learning_rate 0.000100001
2017-09-29T13:46:08.332374: step 3839, loss 0.0676729, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:08.540236: step 3840, loss 0.0523037, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-09-29T13:46:09.111464: step 3840, loss 0.211799, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3840

2017-09-29T13:46:09.931825: step 3841, loss 0.0388843, acc 1, learning_rate 0.000100001
2017-09-29T13:46:10.133667: step 3842, loss 0.0476135, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:10.344803: step 3843, loss 0.0318114, acc 1, learning_rate 0.000100001
2017-09-29T13:46:10.549326: step 3844, loss 0.0199856, acc 1, learning_rate 0.000100001
2017-09-29T13:46:10.752593: step 3845, loss 0.0488159, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:10.961398: step 3846, loss 0.0452661, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:11.171185: step 3847, loss 0.0370508, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:11.380445: step 3848, loss 0.024015, acc 1, learning_rate 0.000100001
2017-09-29T13:46:11.582870: step 3849, loss 0.0258354, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:11.782609: step 3850, loss 0.0183155, acc 1, learning_rate 0.000100001
2017-09-29T13:46:11.986178: step 3851, loss 0.0687622, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:12.184523: step 3852, loss 0.0374595, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:12.384531: step 3853, loss 0.0248027, acc 1, learning_rate 0.000100001
2017-09-29T13:46:12.577795: step 3854, loss 0.0443656, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:12.786395: step 3855, loss 0.0660622, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:12.988235: step 3856, loss 0.019512, acc 1, learning_rate 0.000100001
2017-09-29T13:46:13.196242: step 3857, loss 0.0630025, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:13.399058: step 3858, loss 0.023994, acc 1, learning_rate 0.000100001
2017-09-29T13:46:13.621349: step 3859, loss 0.0935171, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:13.824101: step 3860, loss 0.0830615, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:46:14.022569: step 3861, loss 0.069926, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:14.222682: step 3862, loss 0.0325878, acc 1, learning_rate 0.000100001
2017-09-29T13:46:14.422994: step 3863, loss 0.0172416, acc 1, learning_rate 0.000100001
2017-09-29T13:46:14.647415: step 3864, loss 0.0287506, acc 1, learning_rate 0.000100001
2017-09-29T13:46:14.856762: step 3865, loss 0.0148652, acc 1, learning_rate 0.000100001
2017-09-29T13:46:15.057220: step 3866, loss 0.0410072, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:15.258383: step 3867, loss 0.0112226, acc 1, learning_rate 0.000100001
2017-09-29T13:46:15.457936: step 3868, loss 0.0264081, acc 1, learning_rate 0.000100001
2017-09-29T13:46:15.656639: step 3869, loss 0.0868104, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:15.854702: step 3870, loss 0.0505611, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:16.063659: step 3871, loss 0.0945214, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:16.296721: step 3872, loss 0.0463821, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:16.509695: step 3873, loss 0.133122, acc 0.9375, learning_rate 0.000100001
2017-09-29T13:46:16.710188: step 3874, loss 0.0336283, acc 1, learning_rate 0.000100001
2017-09-29T13:46:16.910670: step 3875, loss 0.0338617, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:17.119411: step 3876, loss 0.124415, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:17.348475: step 3877, loss 0.0331816, acc 1, learning_rate 0.000100001
2017-09-29T13:46:17.551856: step 3878, loss 0.0444592, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:17.751407: step 3879, loss 0.0447125, acc 1, learning_rate 0.000100001
2017-09-29T13:46:17.948861: step 3880, loss 0.0453862, acc 1, learning_rate 0.000100001

Evaluation:
2017-09-29T13:46:18.562168: step 3880, loss 0.209228, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3880

2017-09-29T13:46:19.690506: step 3881, loss 0.0384577, acc 1, learning_rate 0.000100001
2017-09-29T13:46:19.882114: step 3882, loss 0.0358933, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:20.085803: step 3883, loss 0.0272765, acc 1, learning_rate 0.000100001
2017-09-29T13:46:20.288345: step 3884, loss 0.0128783, acc 1, learning_rate 0.000100001
2017-09-29T13:46:20.516802: step 3885, loss 0.0317807, acc 1, learning_rate 0.000100001
2017-09-29T13:46:20.724245: step 3886, loss 0.0132153, acc 1, learning_rate 0.000100001
2017-09-29T13:46:20.921160: step 3887, loss 0.00921669, acc 1, learning_rate 0.000100001
2017-09-29T13:46:21.133512: step 3888, loss 0.0302719, acc 1, learning_rate 0.000100001
2017-09-29T13:46:21.358610: step 3889, loss 0.0554131, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:21.569869: step 3890, loss 0.0103003, acc 1, learning_rate 0.000100001
2017-09-29T13:46:21.768888: step 3891, loss 0.124871, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:46:21.969951: step 3892, loss 0.0886389, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:22.170163: step 3893, loss 0.0290111, acc 1, learning_rate 0.000100001
2017-09-29T13:46:22.373614: step 3894, loss 0.0515917, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:22.574358: step 3895, loss 0.0327404, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:22.783124: step 3896, loss 0.0578779, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:22.980622: step 3897, loss 0.0589379, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:23.180673: step 3898, loss 0.0193228, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:23.381636: step 3899, loss 0.018838, acc 1, learning_rate 0.000100001
2017-09-29T13:46:23.588253: step 3900, loss 0.0723938, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:23.797911: step 3901, loss 0.0179114, acc 1, learning_rate 0.000100001
2017-09-29T13:46:24.007044: step 3902, loss 0.0817445, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:24.212763: step 3903, loss 0.015498, acc 1, learning_rate 0.000100001
2017-09-29T13:46:24.413470: step 3904, loss 0.0407604, acc 1, learning_rate 0.000100001
2017-09-29T13:46:24.636239: step 3905, loss 0.064818, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:24.835998: step 3906, loss 0.0748539, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:25.035023: step 3907, loss 0.0745243, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:25.241071: step 3908, loss 0.110864, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:46:25.452432: step 3909, loss 0.0330184, acc 1, learning_rate 0.000100001
2017-09-29T13:46:25.662542: step 3910, loss 0.0413224, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:25.863393: step 3911, loss 0.0442923, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:26.059371: step 3912, loss 0.0343028, acc 1, learning_rate 0.000100001
2017-09-29T13:46:26.269930: step 3913, loss 0.0578428, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:26.472696: step 3914, loss 0.0198738, acc 1, learning_rate 0.000100001
2017-09-29T13:46:26.670628: step 3915, loss 0.0911087, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:46:26.876272: step 3916, loss 0.0369253, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:27.072454: step 3917, loss 0.0294909, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:27.272941: step 3918, loss 0.0452265, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:27.485569: step 3919, loss 0.113561, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:27.653934: step 3920, loss 0.0340544, acc 1, learning_rate 0.000100001

Evaluation:
2017-09-29T13:46:28.237641: step 3920, loss 0.212275, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3920

2017-09-29T13:46:28.982547: step 3921, loss 0.0295435, acc 1, learning_rate 0.000100001
2017-09-29T13:46:29.182218: step 3922, loss 0.0526509, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:29.395896: step 3923, loss 0.015544, acc 1, learning_rate 0.000100001
2017-09-29T13:46:29.595599: step 3924, loss 0.0708199, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:29.801972: step 3925, loss 0.0634251, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:30.008401: step 3926, loss 0.0105174, acc 1, learning_rate 0.000100001
2017-09-29T13:46:30.215426: step 3927, loss 0.0325072, acc 1, learning_rate 0.000100001
2017-09-29T13:46:30.422053: step 3928, loss 0.0126535, acc 1, learning_rate 0.000100001
2017-09-29T13:46:30.626690: step 3929, loss 0.0450483, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:30.827091: step 3930, loss 0.0626134, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:31.017081: step 3931, loss 0.0519564, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:31.220646: step 3932, loss 0.0959045, acc 0.953125, learning_rate 0.000100001
2017-09-29T13:46:31.424181: step 3933, loss 0.0493971, acc 0.984375, learning_rate 0.000100001
2017-09-29T13:46:31.624077: step 3934, loss 0.0155236, acc 1, learning_rate 0.000100001
2017-09-29T13:46:31.831637: step 3935, loss 0.0613316, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:32.037520: step 3936, loss 0.06822, acc 0.96875, learning_rate 0.000100001
2017-09-29T13:46:32.235072: step 3937, loss 0.0768596, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:32.444699: step 3938, loss 0.0177416, acc 1, learning_rate 0.0001
2017-09-29T13:46:32.663244: step 3939, loss 0.0944146, acc 0.953125, learning_rate 0.0001
2017-09-29T13:46:32.863401: step 3940, loss 0.041784, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:33.066929: step 3941, loss 0.0750739, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:33.272234: step 3942, loss 0.0632526, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:33.470723: step 3943, loss 0.0727557, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:33.678292: step 3944, loss 0.0277196, acc 1, learning_rate 0.0001
2017-09-29T13:46:33.883545: step 3945, loss 0.0236298, acc 1, learning_rate 0.0001
2017-09-29T13:46:34.080940: step 3946, loss 0.0699664, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:34.277018: step 3947, loss 0.0329105, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:34.486703: step 3948, loss 0.0646197, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:34.685016: step 3949, loss 0.0222406, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:34.888439: step 3950, loss 0.100775, acc 0.953125, learning_rate 0.0001
2017-09-29T13:46:35.088269: step 3951, loss 0.0395807, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:35.298359: step 3952, loss 0.0388217, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:35.494061: step 3953, loss 0.0362904, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:35.693601: step 3954, loss 0.0308914, acc 1, learning_rate 0.0001
2017-09-29T13:46:35.901476: step 3955, loss 0.0656064, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:36.103279: step 3956, loss 0.0197674, acc 1, learning_rate 0.0001
2017-09-29T13:46:36.307373: step 3957, loss 0.0152277, acc 1, learning_rate 0.0001
2017-09-29T13:46:36.524194: step 3958, loss 0.0357671, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:36.729866: step 3959, loss 0.070066, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:36.931494: step 3960, loss 0.0239748, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:46:37.535444: step 3960, loss 0.214039, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-3960

2017-09-29T13:46:38.363256: step 3961, loss 0.0413279, acc 1, learning_rate 0.0001
2017-09-29T13:46:38.561689: step 3962, loss 0.0503908, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:38.761307: step 3963, loss 0.0794098, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:38.965383: step 3964, loss 0.0389021, acc 1, learning_rate 0.0001
2017-09-29T13:46:39.163583: step 3965, loss 0.0445266, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:39.362068: step 3966, loss 0.0467889, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:39.562168: step 3967, loss 0.0524841, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:39.759224: step 3968, loss 0.0130893, acc 1, learning_rate 0.0001
2017-09-29T13:46:39.957119: step 3969, loss 0.0962385, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:40.157405: step 3970, loss 0.0190428, acc 1, learning_rate 0.0001
2017-09-29T13:46:40.372739: step 3971, loss 0.0143605, acc 1, learning_rate 0.0001
2017-09-29T13:46:40.596980: step 3972, loss 0.0460876, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:40.793225: step 3973, loss 0.0446811, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:40.992859: step 3974, loss 0.0141633, acc 1, learning_rate 0.0001
2017-09-29T13:46:41.194197: step 3975, loss 0.0629869, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:41.406910: step 3976, loss 0.0851879, acc 0.953125, learning_rate 0.0001
2017-09-29T13:46:41.607518: step 3977, loss 0.0218913, acc 1, learning_rate 0.0001
2017-09-29T13:46:41.809717: step 3978, loss 0.0125591, acc 1, learning_rate 0.0001
2017-09-29T13:46:42.010337: step 3979, loss 0.028703, acc 1, learning_rate 0.0001
2017-09-29T13:46:42.211486: step 3980, loss 0.12652, acc 0.953125, learning_rate 0.0001
2017-09-29T13:46:42.411163: step 3981, loss 0.0204051, acc 1, learning_rate 0.0001
2017-09-29T13:46:42.638525: step 3982, loss 0.0330912, acc 1, learning_rate 0.0001
2017-09-29T13:46:42.846502: step 3983, loss 0.0390122, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:43.044666: step 3984, loss 0.0295235, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:43.256607: step 3985, loss 0.0325967, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:43.495259: step 3986, loss 0.0659288, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:43.705829: step 3987, loss 0.0089679, acc 1, learning_rate 0.0001
2017-09-29T13:46:43.910723: step 3988, loss 0.0636012, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:44.121546: step 3989, loss 0.0338977, acc 1, learning_rate 0.0001
2017-09-29T13:46:44.345546: step 3990, loss 0.0662531, acc 0.953125, learning_rate 0.0001
2017-09-29T13:46:44.554308: step 3991, loss 0.0958171, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:44.752641: step 3992, loss 0.0195128, acc 1, learning_rate 0.0001
2017-09-29T13:46:44.968802: step 3993, loss 0.0142637, acc 1, learning_rate 0.0001
2017-09-29T13:46:45.174085: step 3994, loss 0.0373938, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:45.374775: step 3995, loss 0.0739373, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:45.575729: step 3996, loss 0.0403871, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:45.768351: step 3997, loss 0.020658, acc 1, learning_rate 0.0001
2017-09-29T13:46:45.974610: step 3998, loss 0.0923289, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:46.174946: step 3999, loss 0.0314341, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:46.378940: step 4000, loss 0.0394551, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:46:46.952828: step 4000, loss 0.211111, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4000

2017-09-29T13:46:47.815993: step 4001, loss 0.0369067, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:48.016111: step 4002, loss 0.0244685, acc 1, learning_rate 0.0001
2017-09-29T13:46:48.226059: step 4003, loss 0.0573626, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:48.450578: step 4004, loss 0.0549321, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:48.648666: step 4005, loss 0.0293918, acc 1, learning_rate 0.0001
2017-09-29T13:46:48.844272: step 4006, loss 0.0460459, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:49.041621: step 4007, loss 0.0291083, acc 1, learning_rate 0.0001
2017-09-29T13:46:49.241370: step 4008, loss 0.0255822, acc 1, learning_rate 0.0001
2017-09-29T13:46:49.441353: step 4009, loss 0.0571999, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:49.641729: step 4010, loss 0.0351274, acc 1, learning_rate 0.0001
2017-09-29T13:46:49.835849: step 4011, loss 0.0257542, acc 1, learning_rate 0.0001
2017-09-29T13:46:50.037147: step 4012, loss 0.0219036, acc 1, learning_rate 0.0001
2017-09-29T13:46:50.240780: step 4013, loss 0.0394884, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:50.455661: step 4014, loss 0.0211908, acc 1, learning_rate 0.0001
2017-09-29T13:46:50.662235: step 4015, loss 0.0529804, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:50.861176: step 4016, loss 0.0278096, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:51.076268: step 4017, loss 0.0519993, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:51.238938: step 4018, loss 0.084599, acc 0.960784, learning_rate 0.0001
2017-09-29T13:46:51.445959: step 4019, loss 0.0625321, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:51.647816: step 4020, loss 0.0556511, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:51.846938: step 4021, loss 0.0183749, acc 1, learning_rate 0.0001
2017-09-29T13:46:52.061595: step 4022, loss 0.0356939, acc 1, learning_rate 0.0001
2017-09-29T13:46:52.266499: step 4023, loss 0.0665045, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:52.454818: step 4024, loss 0.0634434, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:52.659952: step 4025, loss 0.0251289, acc 1, learning_rate 0.0001
2017-09-29T13:46:52.856216: step 4026, loss 0.0562349, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:53.052457: step 4027, loss 0.010086, acc 1, learning_rate 0.0001
2017-09-29T13:46:53.266248: step 4028, loss 0.0424203, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:53.473417: step 4029, loss 0.00711245, acc 1, learning_rate 0.0001
2017-09-29T13:46:53.683109: step 4030, loss 0.0224937, acc 1, learning_rate 0.0001
2017-09-29T13:46:53.889120: step 4031, loss 0.0612681, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:54.105979: step 4032, loss 0.0305437, acc 1, learning_rate 0.0001
2017-09-29T13:46:54.300632: step 4033, loss 0.076676, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:54.516273: step 4034, loss 0.0205006, acc 1, learning_rate 0.0001
2017-09-29T13:46:54.740289: step 4035, loss 0.0832924, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:54.939402: step 4036, loss 0.031678, acc 1, learning_rate 0.0001
2017-09-29T13:46:55.147728: step 4037, loss 0.0493862, acc 1, learning_rate 0.0001
2017-09-29T13:46:55.353950: step 4038, loss 0.0620866, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:55.554446: step 4039, loss 0.0159132, acc 1, learning_rate 0.0001
2017-09-29T13:46:55.764154: step 4040, loss 0.0283803, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:46:56.379638: step 4040, loss 0.216596, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4040

2017-09-29T13:46:57.133665: step 4041, loss 0.0331716, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:57.330143: step 4042, loss 0.0318094, acc 1, learning_rate 0.0001
2017-09-29T13:46:57.527002: step 4043, loss 0.0686075, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:57.726980: step 4044, loss 0.00820114, acc 1, learning_rate 0.0001
2017-09-29T13:46:57.924398: step 4045, loss 0.0522602, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:58.129853: step 4046, loss 0.037484, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:58.329174: step 4047, loss 0.030939, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:58.544413: step 4048, loss 0.0574756, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:58.744192: step 4049, loss 0.0138097, acc 1, learning_rate 0.0001
2017-09-29T13:46:58.942727: step 4050, loss 0.0754773, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:59.144855: step 4051, loss 0.0144139, acc 1, learning_rate 0.0001
2017-09-29T13:46:59.363577: step 4052, loss 0.030618, acc 0.984375, learning_rate 0.0001
2017-09-29T13:46:59.552199: step 4053, loss 0.10489, acc 0.96875, learning_rate 0.0001
2017-09-29T13:46:59.743698: step 4054, loss 0.0369558, acc 1, learning_rate 0.0001
2017-09-29T13:46:59.928270: step 4055, loss 0.0302958, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:00.120548: step 4056, loss 0.0390575, acc 1, learning_rate 0.0001
2017-09-29T13:47:00.304724: step 4057, loss 0.0971002, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:00.517984: step 4058, loss 0.0761734, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:00.719085: step 4059, loss 0.0148263, acc 1, learning_rate 0.0001
2017-09-29T13:47:00.935163: step 4060, loss 0.0121979, acc 1, learning_rate 0.0001
2017-09-29T13:47:01.133849: step 4061, loss 0.0700803, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:01.342499: step 4062, loss 0.0387847, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:01.562909: step 4063, loss 0.0196608, acc 1, learning_rate 0.0001
2017-09-29T13:47:01.761329: step 4064, loss 0.042543, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:01.957060: step 4065, loss 0.0494536, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:02.156954: step 4066, loss 0.00936184, acc 1, learning_rate 0.0001
2017-09-29T13:47:02.352263: step 4067, loss 0.0830789, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:02.562365: step 4068, loss 0.0348684, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:02.772118: step 4069, loss 0.095172, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:02.964018: step 4070, loss 0.0117861, acc 1, learning_rate 0.0001
2017-09-29T13:47:03.166493: step 4071, loss 0.0590822, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:03.352673: step 4072, loss 0.0293583, acc 1, learning_rate 0.0001
2017-09-29T13:47:03.546442: step 4073, loss 0.0249729, acc 1, learning_rate 0.0001
2017-09-29T13:47:03.731005: step 4074, loss 0.0794571, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:03.924927: step 4075, loss 0.0577734, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:04.130752: step 4076, loss 0.0284521, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:04.323022: step 4077, loss 0.0107531, acc 1, learning_rate 0.0001
2017-09-29T13:47:04.516376: step 4078, loss 0.0370919, acc 1, learning_rate 0.0001
2017-09-29T13:47:04.728421: step 4079, loss 0.0299166, acc 1, learning_rate 0.0001
2017-09-29T13:47:04.921575: step 4080, loss 0.0276338, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:47:05.569458: step 4080, loss 0.212803, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4080

2017-09-29T13:47:06.267988: step 4081, loss 0.0379277, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:06.458549: step 4082, loss 0.0373717, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:06.644548: step 4083, loss 0.0834654, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:06.826814: step 4084, loss 0.0259046, acc 1, learning_rate 0.0001
2017-09-29T13:47:07.011198: step 4085, loss 0.0474542, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:07.195713: step 4086, loss 0.0634537, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:07.381651: step 4087, loss 0.0405103, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:07.565466: step 4088, loss 0.0499689, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:07.749955: step 4089, loss 0.0362368, acc 1, learning_rate 0.0001
2017-09-29T13:47:07.941251: step 4090, loss 0.0876779, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:08.124777: step 4091, loss 0.0441968, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:08.320998: step 4092, loss 0.0441869, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:08.503505: step 4093, loss 0.0886054, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:08.688836: step 4094, loss 0.0518626, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:08.872718: step 4095, loss 0.104483, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:09.054660: step 4096, loss 0.0389003, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:09.238329: step 4097, loss 0.0161461, acc 1, learning_rate 0.0001
2017-09-29T13:47:09.424547: step 4098, loss 0.0565942, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:09.606582: step 4099, loss 0.0430284, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:09.788760: step 4100, loss 0.0413556, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:09.971579: step 4101, loss 0.0529537, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:10.154769: step 4102, loss 0.0890827, acc 0.9375, learning_rate 0.0001
2017-09-29T13:47:10.336421: step 4103, loss 0.0231906, acc 1, learning_rate 0.0001
2017-09-29T13:47:10.524201: step 4104, loss 0.0518057, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:10.707252: step 4105, loss 0.021087, acc 1, learning_rate 0.0001
2017-09-29T13:47:10.891792: step 4106, loss 0.0385001, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:11.070466: step 4107, loss 0.0272239, acc 1, learning_rate 0.0001
2017-09-29T13:47:11.251476: step 4108, loss 0.0426852, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:11.435840: step 4109, loss 0.0464282, acc 1, learning_rate 0.0001
2017-09-29T13:47:11.627032: step 4110, loss 0.0823097, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:11.808866: step 4111, loss 0.0386415, acc 1, learning_rate 0.0001
2017-09-29T13:47:11.990713: step 4112, loss 0.0443788, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:12.185158: step 4113, loss 0.106581, acc 0.9375, learning_rate 0.0001
2017-09-29T13:47:12.364409: step 4114, loss 0.0294342, acc 1, learning_rate 0.0001
2017-09-29T13:47:12.558941: step 4115, loss 0.075223, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:12.709761: step 4116, loss 0.0422941, acc 0.980392, learning_rate 0.0001
2017-09-29T13:47:12.893775: step 4117, loss 0.0749156, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:13.075226: step 4118, loss 0.00946747, acc 1, learning_rate 0.0001
2017-09-29T13:47:13.262820: step 4119, loss 0.0704024, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:13.453415: step 4120, loss 0.0700115, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:47:13.991128: step 4120, loss 0.213341, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4120

2017-09-29T13:47:14.786878: step 4121, loss 0.0363582, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:14.967120: step 4122, loss 0.0422188, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:15.149320: step 4123, loss 0.0519087, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:15.335637: step 4124, loss 0.0700379, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:15.521010: step 4125, loss 0.0653343, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:15.704046: step 4126, loss 0.0312846, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:15.890261: step 4127, loss 0.0361575, acc 1, learning_rate 0.0001
2017-09-29T13:47:16.071362: step 4128, loss 0.0673395, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:16.256013: step 4129, loss 0.0193264, acc 1, learning_rate 0.0001
2017-09-29T13:47:16.438696: step 4130, loss 0.0194378, acc 1, learning_rate 0.0001
2017-09-29T13:47:16.626549: step 4131, loss 0.0371215, acc 1, learning_rate 0.0001
2017-09-29T13:47:16.807773: step 4132, loss 0.0420856, acc 1, learning_rate 0.0001
2017-09-29T13:47:16.992788: step 4133, loss 0.0629589, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:17.175853: step 4134, loss 0.0565892, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:17.359439: step 4135, loss 0.0577354, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:17.541106: step 4136, loss 0.0122291, acc 1, learning_rate 0.0001
2017-09-29T13:47:17.724588: step 4137, loss 0.0219224, acc 1, learning_rate 0.0001
2017-09-29T13:47:17.906775: step 4138, loss 0.0541977, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:18.091427: step 4139, loss 0.101131, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:18.276868: step 4140, loss 0.029246, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:18.473147: step 4141, loss 0.0351062, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:18.651170: step 4142, loss 0.0220333, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:18.829128: step 4143, loss 0.0702152, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:19.013190: step 4144, loss 0.028657, acc 1, learning_rate 0.0001
2017-09-29T13:47:19.195854: step 4145, loss 0.0133275, acc 1, learning_rate 0.0001
2017-09-29T13:47:19.378993: step 4146, loss 0.0609996, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:19.563384: step 4147, loss 0.030059, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:19.748211: step 4148, loss 0.0191558, acc 1, learning_rate 0.0001
2017-09-29T13:47:19.929547: step 4149, loss 0.0336941, acc 1, learning_rate 0.0001
2017-09-29T13:47:20.110760: step 4150, loss 0.0948071, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:20.294084: step 4151, loss 0.0465049, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:20.477520: step 4152, loss 0.0855458, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:20.665596: step 4153, loss 0.0237306, acc 1, learning_rate 0.0001
2017-09-29T13:47:20.849611: step 4154, loss 0.057758, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:21.035777: step 4155, loss 0.0476108, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:21.218558: step 4156, loss 0.029989, acc 1, learning_rate 0.0001
2017-09-29T13:47:21.401602: step 4157, loss 0.0272367, acc 1, learning_rate 0.0001
2017-09-29T13:47:21.589750: step 4158, loss 0.0126954, acc 1, learning_rate 0.0001
2017-09-29T13:47:21.771709: step 4159, loss 0.0199164, acc 1, learning_rate 0.0001
2017-09-29T13:47:21.954190: step 4160, loss 0.0176401, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:47:22.500147: step 4160, loss 0.21301, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4160

2017-09-29T13:47:23.142928: step 4161, loss 0.0560095, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:23.358689: step 4162, loss 0.0141924, acc 1, learning_rate 0.0001
2017-09-29T13:47:23.558311: step 4163, loss 0.0724955, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:23.745937: step 4164, loss 0.0261966, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:23.939987: step 4165, loss 0.0214352, acc 1, learning_rate 0.0001
2017-09-29T13:47:24.141011: step 4166, loss 0.0455857, acc 1, learning_rate 0.0001
2017-09-29T13:47:24.342518: step 4167, loss 0.0213345, acc 1, learning_rate 0.0001
2017-09-29T13:47:24.529381: step 4168, loss 0.0414014, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:24.720826: step 4169, loss 0.0150372, acc 1, learning_rate 0.0001
2017-09-29T13:47:24.901405: step 4170, loss 0.101586, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:25.081278: step 4171, loss 0.0742036, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:25.263990: step 4172, loss 0.0213006, acc 1, learning_rate 0.0001
2017-09-29T13:47:25.446835: step 4173, loss 0.052568, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:25.630442: step 4174, loss 0.0449921, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:25.812104: step 4175, loss 0.0360657, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:26.012300: step 4176, loss 0.0247718, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:26.212883: step 4177, loss 0.0229226, acc 1, learning_rate 0.0001
2017-09-29T13:47:26.415112: step 4178, loss 0.0703774, acc 1, learning_rate 0.0001
2017-09-29T13:47:26.603973: step 4179, loss 0.0107771, acc 1, learning_rate 0.0001
2017-09-29T13:47:26.794813: step 4180, loss 0.0301999, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:26.976679: step 4181, loss 0.047919, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:27.160777: step 4182, loss 0.0291157, acc 1, learning_rate 0.0001
2017-09-29T13:47:27.342696: step 4183, loss 0.0709894, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:27.532008: step 4184, loss 0.0571488, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:27.714704: step 4185, loss 0.126638, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:27.897261: step 4186, loss 0.00859564, acc 1, learning_rate 0.0001
2017-09-29T13:47:28.078249: step 4187, loss 0.063353, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:28.262989: step 4188, loss 0.0159567, acc 1, learning_rate 0.0001
2017-09-29T13:47:28.451591: step 4189, loss 0.0367705, acc 1, learning_rate 0.0001
2017-09-29T13:47:28.639300: step 4190, loss 0.0560816, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:28.823474: step 4191, loss 0.0472491, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:29.003557: step 4192, loss 0.0359537, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:29.183695: step 4193, loss 0.0278198, acc 1, learning_rate 0.0001
2017-09-29T13:47:29.372363: step 4194, loss 0.0289036, acc 1, learning_rate 0.0001
2017-09-29T13:47:29.559263: step 4195, loss 0.050454, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:29.743636: step 4196, loss 0.0401063, acc 1, learning_rate 0.0001
2017-09-29T13:47:29.926097: step 4197, loss 0.0288692, acc 1, learning_rate 0.0001
2017-09-29T13:47:30.112340: step 4198, loss 0.104312, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:30.295340: step 4199, loss 0.065378, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:30.479903: step 4200, loss 0.064863, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:47:31.039785: step 4200, loss 0.213646, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4200

2017-09-29T13:47:31.762947: step 4201, loss 0.015922, acc 1, learning_rate 0.0001
2017-09-29T13:47:31.943025: step 4202, loss 0.0878503, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:32.123191: step 4203, loss 0.0183703, acc 1, learning_rate 0.0001
2017-09-29T13:47:32.306736: step 4204, loss 0.0572942, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:32.494805: step 4205, loss 0.0192876, acc 1, learning_rate 0.0001
2017-09-29T13:47:32.682969: step 4206, loss 0.060972, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:32.867607: step 4207, loss 0.0546333, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:33.050076: step 4208, loss 0.0715183, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:33.244347: step 4209, loss 0.0264698, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:33.431975: step 4210, loss 0.0857491, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:33.615260: step 4211, loss 0.0292834, acc 1, learning_rate 0.0001
2017-09-29T13:47:33.796594: step 4212, loss 0.0124909, acc 1, learning_rate 0.0001
2017-09-29T13:47:33.980081: step 4213, loss 0.0121161, acc 1, learning_rate 0.0001
2017-09-29T13:47:34.132048: step 4214, loss 0.0178876, acc 1, learning_rate 0.0001
2017-09-29T13:47:34.318584: step 4215, loss 0.0604977, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:34.520729: step 4216, loss 0.0738839, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:34.713983: step 4217, loss 0.0320986, acc 1, learning_rate 0.0001
2017-09-29T13:47:34.899124: step 4218, loss 0.047341, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:35.078527: step 4219, loss 0.0786845, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:35.267028: step 4220, loss 0.0607617, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:35.455743: step 4221, loss 0.0495486, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:35.642205: step 4222, loss 0.00832198, acc 1, learning_rate 0.0001
2017-09-29T13:47:35.823257: step 4223, loss 0.025741, acc 1, learning_rate 0.0001
2017-09-29T13:47:36.009267: step 4224, loss 0.0397714, acc 1, learning_rate 0.0001
2017-09-29T13:47:36.200388: step 4225, loss 0.0444481, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:36.385425: step 4226, loss 0.0302063, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:36.569390: step 4227, loss 0.0448435, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:36.758051: step 4228, loss 0.0519368, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:36.942331: step 4229, loss 0.0944228, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:37.123074: step 4230, loss 0.0924651, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:37.303468: step 4231, loss 0.0602454, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:37.493971: step 4232, loss 0.0333334, acc 1, learning_rate 0.0001
2017-09-29T13:47:37.676691: step 4233, loss 0.0186134, acc 1, learning_rate 0.0001
2017-09-29T13:47:37.858998: step 4234, loss 0.0358423, acc 1, learning_rate 0.0001
2017-09-29T13:47:38.039076: step 4235, loss 0.0203843, acc 1, learning_rate 0.0001
2017-09-29T13:47:38.224755: step 4236, loss 0.0806364, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:38.408446: step 4237, loss 0.0660494, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:38.588545: step 4238, loss 0.0172689, acc 1, learning_rate 0.0001
2017-09-29T13:47:38.770164: step 4239, loss 0.0230285, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:38.958671: step 4240, loss 0.0861809, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T13:47:39.505387: step 4240, loss 0.214617, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4240

2017-09-29T13:47:40.284484: step 4241, loss 0.0222791, acc 1, learning_rate 0.0001
2017-09-29T13:47:40.475991: step 4242, loss 0.0111645, acc 1, learning_rate 0.0001
2017-09-29T13:47:40.661382: step 4243, loss 0.0131808, acc 1, learning_rate 0.0001
2017-09-29T13:47:40.840074: step 4244, loss 0.0368111, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:41.025530: step 4245, loss 0.0788645, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:41.211031: step 4246, loss 0.0589027, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:41.395425: step 4247, loss 0.0390492, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:41.579481: step 4248, loss 0.0285957, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:41.771826: step 4249, loss 0.0208039, acc 1, learning_rate 0.0001
2017-09-29T13:47:41.952561: step 4250, loss 0.0186741, acc 1, learning_rate 0.0001
2017-09-29T13:47:42.135546: step 4251, loss 0.0222612, acc 1, learning_rate 0.0001
2017-09-29T13:47:42.328821: step 4252, loss 0.0456876, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:42.512643: step 4253, loss 0.0261905, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:42.695728: step 4254, loss 0.0160755, acc 1, learning_rate 0.0001
2017-09-29T13:47:42.876657: step 4255, loss 0.103367, acc 0.9375, learning_rate 0.0001
2017-09-29T13:47:43.065855: step 4256, loss 0.0273656, acc 1, learning_rate 0.0001
2017-09-29T13:47:43.250253: step 4257, loss 0.0537228, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:43.431488: step 4258, loss 0.0571352, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:43.614218: step 4259, loss 0.049584, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:43.795673: step 4260, loss 0.0435487, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:43.979241: step 4261, loss 0.0240087, acc 1, learning_rate 0.0001
2017-09-29T13:47:44.164797: step 4262, loss 0.0254715, acc 1, learning_rate 0.0001
2017-09-29T13:47:44.347534: step 4263, loss 0.0288998, acc 1, learning_rate 0.0001
2017-09-29T13:47:44.528093: step 4264, loss 0.0589189, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:44.712903: step 4265, loss 0.0262264, acc 1, learning_rate 0.0001
2017-09-29T13:47:44.891916: step 4266, loss 0.0266985, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:45.083083: step 4267, loss 0.0636604, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:45.270247: step 4268, loss 0.0420053, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:45.453913: step 4269, loss 0.0427886, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:45.639091: step 4270, loss 0.0231395, acc 1, learning_rate 0.0001
2017-09-29T13:47:45.823086: step 4271, loss 0.0283108, acc 1, learning_rate 0.0001
2017-09-29T13:47:46.007477: step 4272, loss 0.057868, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:46.191119: step 4273, loss 0.0327535, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:46.376642: step 4274, loss 0.0225014, acc 1, learning_rate 0.0001
2017-09-29T13:47:46.573234: step 4275, loss 0.0123949, acc 1, learning_rate 0.0001
2017-09-29T13:47:46.772084: step 4276, loss 0.113223, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:46.964484: step 4277, loss 0.0859184, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:47.145833: step 4278, loss 0.0254462, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:47.330687: step 4279, loss 0.0287258, acc 1, learning_rate 0.0001
2017-09-29T13:47:47.516441: step 4280, loss 0.019849, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:47:48.074958: step 4280, loss 0.212817, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4280

2017-09-29T13:47:48.709390: step 4281, loss 0.0426164, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:48.892773: step 4282, loss 0.0443311, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:49.075935: step 4283, loss 0.0427851, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:49.267517: step 4284, loss 0.0864803, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:49.453587: step 4285, loss 0.0426209, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:49.633372: step 4286, loss 0.0555218, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:49.823659: step 4287, loss 0.00795693, acc 1, learning_rate 0.0001
2017-09-29T13:47:50.004700: step 4288, loss 0.084151, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:50.188339: step 4289, loss 0.025293, acc 1, learning_rate 0.0001
2017-09-29T13:47:50.369516: step 4290, loss 0.057555, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:50.555075: step 4291, loss 0.0265812, acc 1, learning_rate 0.0001
2017-09-29T13:47:50.743516: step 4292, loss 0.0751018, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:50.924538: step 4293, loss 0.0370388, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:51.105662: step 4294, loss 0.0172779, acc 1, learning_rate 0.0001
2017-09-29T13:47:51.295101: step 4295, loss 0.0504917, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:51.478584: step 4296, loss 0.0822019, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:51.662607: step 4297, loss 0.055002, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:51.851598: step 4298, loss 0.0731179, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:52.029296: step 4299, loss 0.0600097, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:52.217134: step 4300, loss 0.0358694, acc 1, learning_rate 0.0001
2017-09-29T13:47:52.400226: step 4301, loss 0.0409143, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:52.590855: step 4302, loss 0.0604252, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:52.769718: step 4303, loss 0.0523497, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:52.956427: step 4304, loss 0.0197193, acc 1, learning_rate 0.0001
2017-09-29T13:47:53.149085: step 4305, loss 0.0451775, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:53.331979: step 4306, loss 0.0548501, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:53.525096: step 4307, loss 0.0387817, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:53.718820: step 4308, loss 0.0442622, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:53.902586: step 4309, loss 0.110878, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:54.094951: step 4310, loss 0.033564, acc 1, learning_rate 0.0001
2017-09-29T13:47:54.278560: step 4311, loss 0.0596529, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:54.432572: step 4312, loss 0.0333302, acc 1, learning_rate 0.0001
2017-09-29T13:47:54.614890: step 4313, loss 0.0533908, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:54.796960: step 4314, loss 0.0699959, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:54.978785: step 4315, loss 0.0336298, acc 1, learning_rate 0.0001
2017-09-29T13:47:55.162161: step 4316, loss 0.011455, acc 1, learning_rate 0.0001
2017-09-29T13:47:55.346545: step 4317, loss 0.0103779, acc 1, learning_rate 0.0001
2017-09-29T13:47:55.528668: step 4318, loss 0.0611283, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:55.714413: step 4319, loss 0.0848757, acc 0.953125, learning_rate 0.0001
2017-09-29T13:47:55.898555: step 4320, loss 0.0106504, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:47:56.434728: step 4320, loss 0.216172, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4320

2017-09-29T13:47:57.148096: step 4321, loss 0.0638785, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:57.338149: step 4322, loss 0.0498884, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:57.537575: step 4323, loss 0.0587683, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:57.721553: step 4324, loss 0.0139881, acc 1, learning_rate 0.0001
2017-09-29T13:47:57.904525: step 4325, loss 0.0712711, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:58.089194: step 4326, loss 0.0540117, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:58.270037: step 4327, loss 0.0380355, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:58.454920: step 4328, loss 0.0317762, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:58.649853: step 4329, loss 0.0629285, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:58.832711: step 4330, loss 0.0189637, acc 1, learning_rate 0.0001
2017-09-29T13:47:59.014654: step 4331, loss 0.0677959, acc 0.96875, learning_rate 0.0001
2017-09-29T13:47:59.197137: step 4332, loss 0.0124422, acc 1, learning_rate 0.0001
2017-09-29T13:47:59.378485: step 4333, loss 0.0334765, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:59.560548: step 4334, loss 0.026244, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:59.745575: step 4335, loss 0.0591053, acc 0.984375, learning_rate 0.0001
2017-09-29T13:47:59.925289: step 4336, loss 0.0222623, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:00.106440: step 4337, loss 0.043453, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:00.291785: step 4338, loss 0.057961, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:00.474335: step 4339, loss 0.0345489, acc 1, learning_rate 0.0001
2017-09-29T13:48:00.656315: step 4340, loss 0.011962, acc 1, learning_rate 0.0001
2017-09-29T13:48:00.843035: step 4341, loss 0.0157611, acc 1, learning_rate 0.0001
2017-09-29T13:48:01.027790: step 4342, loss 0.0927625, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:01.212676: step 4343, loss 0.0552168, acc 1, learning_rate 0.0001
2017-09-29T13:48:01.404620: step 4344, loss 0.0182405, acc 1, learning_rate 0.0001
2017-09-29T13:48:01.589117: step 4345, loss 0.0621127, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:01.770939: step 4346, loss 0.0983405, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:01.961519: step 4347, loss 0.0106681, acc 1, learning_rate 0.0001
2017-09-29T13:48:02.142560: step 4348, loss 0.0180937, acc 1, learning_rate 0.0001
2017-09-29T13:48:02.323902: step 4349, loss 0.0202979, acc 1, learning_rate 0.0001
2017-09-29T13:48:02.508519: step 4350, loss 0.0572167, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:02.692077: step 4351, loss 0.0555375, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:02.875366: step 4352, loss 0.0526853, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:03.055069: step 4353, loss 0.0154286, acc 1, learning_rate 0.0001
2017-09-29T13:48:03.241553: step 4354, loss 0.0654715, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:03.427627: step 4355, loss 0.0459835, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:03.613097: step 4356, loss 0.0218657, acc 1, learning_rate 0.0001
2017-09-29T13:48:03.793445: step 4357, loss 0.0460476, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:03.973470: step 4358, loss 0.0257964, acc 1, learning_rate 0.0001
2017-09-29T13:48:04.165417: step 4359, loss 0.0338702, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:04.349514: step 4360, loss 0.0402365, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:48:04.886371: step 4360, loss 0.214319, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4360

2017-09-29T13:48:05.679481: step 4361, loss 0.0440666, acc 1, learning_rate 0.0001
2017-09-29T13:48:05.866982: step 4362, loss 0.0696921, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:06.050773: step 4363, loss 0.0205774, acc 1, learning_rate 0.0001
2017-09-29T13:48:06.234309: step 4364, loss 0.0579263, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:06.420214: step 4365, loss 0.0894361, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:06.605911: step 4366, loss 0.00610057, acc 1, learning_rate 0.0001
2017-09-29T13:48:06.784377: step 4367, loss 0.0403094, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:06.977221: step 4368, loss 0.0673982, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:07.163734: step 4369, loss 0.100874, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:07.356291: step 4370, loss 0.0737401, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:07.539251: step 4371, loss 0.0385841, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:07.722827: step 4372, loss 0.0265737, acc 1, learning_rate 0.0001
2017-09-29T13:48:07.903302: step 4373, loss 0.0644485, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:08.085110: step 4374, loss 0.0183989, acc 1, learning_rate 0.0001
2017-09-29T13:48:08.267335: step 4375, loss 0.0605628, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:08.453599: step 4376, loss 0.0648604, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:08.636065: step 4377, loss 0.0103023, acc 1, learning_rate 0.0001
2017-09-29T13:48:08.817908: step 4378, loss 0.119298, acc 0.9375, learning_rate 0.0001
2017-09-29T13:48:09.001962: step 4379, loss 0.0109394, acc 1, learning_rate 0.0001
2017-09-29T13:48:09.182947: step 4380, loss 0.0665965, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:09.366945: step 4381, loss 0.0176292, acc 1, learning_rate 0.0001
2017-09-29T13:48:09.574520: step 4382, loss 0.0421328, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:09.759335: step 4383, loss 0.051742, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:09.945072: step 4384, loss 0.0599669, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:10.125776: step 4385, loss 0.013224, acc 1, learning_rate 0.0001
2017-09-29T13:48:10.313090: step 4386, loss 0.101075, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:10.501783: step 4387, loss 0.0262968, acc 1, learning_rate 0.0001
2017-09-29T13:48:10.682655: step 4388, loss 0.0142321, acc 1, learning_rate 0.0001
2017-09-29T13:48:10.871753: step 4389, loss 0.0152439, acc 1, learning_rate 0.0001
2017-09-29T13:48:11.054227: step 4390, loss 0.0276882, acc 1, learning_rate 0.0001
2017-09-29T13:48:11.236634: step 4391, loss 0.117915, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:11.415775: step 4392, loss 0.0164402, acc 1, learning_rate 0.0001
2017-09-29T13:48:11.601367: step 4393, loss 0.0591221, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:11.783102: step 4394, loss 0.0560341, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:11.971073: step 4395, loss 0.0914185, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:12.152984: step 4396, loss 0.0461371, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:12.333972: step 4397, loss 0.055685, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:12.520890: step 4398, loss 0.0215566, acc 1, learning_rate 0.0001
2017-09-29T13:48:12.702791: step 4399, loss 0.0243012, acc 1, learning_rate 0.0001
2017-09-29T13:48:12.886150: step 4400, loss 0.110506, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T13:48:13.432561: step 4400, loss 0.213838, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4400

2017-09-29T13:48:14.063363: step 4401, loss 0.0307576, acc 1, learning_rate 0.0001
2017-09-29T13:48:14.250495: step 4402, loss 0.0816434, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:14.431753: step 4403, loss 0.0997066, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:14.611488: step 4404, loss 0.0149371, acc 1, learning_rate 0.0001
2017-09-29T13:48:14.793136: step 4405, loss 0.0573886, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:14.980257: step 4406, loss 0.036163, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:15.157592: step 4407, loss 0.0411869, acc 1, learning_rate 0.0001
2017-09-29T13:48:15.338154: step 4408, loss 0.0391474, acc 1, learning_rate 0.0001
2017-09-29T13:48:15.531347: step 4409, loss 0.0201007, acc 1, learning_rate 0.0001
2017-09-29T13:48:15.700978: step 4410, loss 0.0274597, acc 1, learning_rate 0.0001
2017-09-29T13:48:15.908845: step 4411, loss 0.0676687, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:16.109385: step 4412, loss 0.00992882, acc 1, learning_rate 0.0001
2017-09-29T13:48:16.313447: step 4413, loss 0.0673318, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:16.508064: step 4414, loss 0.0690471, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:16.692506: step 4415, loss 0.0149205, acc 1, learning_rate 0.0001
2017-09-29T13:48:16.877271: step 4416, loss 0.0248582, acc 1, learning_rate 0.0001
2017-09-29T13:48:17.070333: step 4417, loss 0.0235175, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:17.258523: step 4418, loss 0.0361277, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:17.443324: step 4419, loss 0.0126893, acc 1, learning_rate 0.0001
2017-09-29T13:48:17.630539: step 4420, loss 0.0172425, acc 1, learning_rate 0.0001
2017-09-29T13:48:17.814826: step 4421, loss 0.0255554, acc 1, learning_rate 0.0001
2017-09-29T13:48:17.996037: step 4422, loss 0.0871957, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:18.184064: step 4423, loss 0.02151, acc 1, learning_rate 0.0001
2017-09-29T13:48:18.367881: step 4424, loss 0.016898, acc 1, learning_rate 0.0001
2017-09-29T13:48:18.553657: step 4425, loss 0.0476977, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:18.737997: step 4426, loss 0.0615257, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:18.919670: step 4427, loss 0.0262211, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:19.104636: step 4428, loss 0.0352665, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:19.288740: step 4429, loss 0.0609839, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:19.484280: step 4430, loss 0.0231023, acc 1, learning_rate 0.0001
2017-09-29T13:48:19.683790: step 4431, loss 0.0203433, acc 1, learning_rate 0.0001
2017-09-29T13:48:19.870829: step 4432, loss 0.0479054, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:20.063020: step 4433, loss 0.0536211, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:20.255448: step 4434, loss 0.0128574, acc 1, learning_rate 0.0001
2017-09-29T13:48:20.447482: step 4435, loss 0.0130696, acc 1, learning_rate 0.0001
2017-09-29T13:48:20.635064: step 4436, loss 0.0459517, acc 1, learning_rate 0.0001
2017-09-29T13:48:20.818475: step 4437, loss 0.0217968, acc 1, learning_rate 0.0001
2017-09-29T13:48:21.004895: step 4438, loss 0.0446227, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:21.196763: step 4439, loss 0.0462709, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:21.389456: step 4440, loss 0.0229057, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:48:21.961364: step 4440, loss 0.213003, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4440

2017-09-29T13:48:22.695472: step 4441, loss 0.0421954, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:22.889056: step 4442, loss 0.028309, acc 1, learning_rate 0.0001
2017-09-29T13:48:23.078156: step 4443, loss 0.0285539, acc 1, learning_rate 0.0001
2017-09-29T13:48:23.266668: step 4444, loss 0.0142238, acc 1, learning_rate 0.0001
2017-09-29T13:48:23.451736: step 4445, loss 0.0298642, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:23.635834: step 4446, loss 0.0409304, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:23.822996: step 4447, loss 0.0498386, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:24.018314: step 4448, loss 0.0155205, acc 1, learning_rate 0.0001
2017-09-29T13:48:24.203793: step 4449, loss 0.0261188, acc 1, learning_rate 0.0001
2017-09-29T13:48:24.386594: step 4450, loss 0.0177492, acc 1, learning_rate 0.0001
2017-09-29T13:48:24.594949: step 4451, loss 0.02001, acc 1, learning_rate 0.0001
2017-09-29T13:48:24.793715: step 4452, loss 0.0731358, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:24.980989: step 4453, loss 0.00754851, acc 1, learning_rate 0.0001
2017-09-29T13:48:25.172441: step 4454, loss 0.0143645, acc 1, learning_rate 0.0001
2017-09-29T13:48:25.362863: step 4455, loss 0.0774339, acc 1, learning_rate 0.0001
2017-09-29T13:48:25.550932: step 4456, loss 0.0863016, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:25.737866: step 4457, loss 0.0397862, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:25.922555: step 4458, loss 0.0658489, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:26.102830: step 4459, loss 0.0322085, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:26.293511: step 4460, loss 0.0126078, acc 1, learning_rate 0.0001
2017-09-29T13:48:26.486572: step 4461, loss 0.0657312, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:26.676398: step 4462, loss 0.0697054, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:26.861876: step 4463, loss 0.0155574, acc 1, learning_rate 0.0001
2017-09-29T13:48:27.053816: step 4464, loss 0.045515, acc 1, learning_rate 0.0001
2017-09-29T13:48:27.248735: step 4465, loss 0.0544709, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:27.450269: step 4466, loss 0.0242866, acc 1, learning_rate 0.0001
2017-09-29T13:48:27.639675: step 4467, loss 0.164064, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:27.825484: step 4468, loss 0.0336467, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:28.013037: step 4469, loss 0.0326402, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:28.199153: step 4470, loss 0.0499842, acc 1, learning_rate 0.0001
2017-09-29T13:48:28.384172: step 4471, loss 0.04177, acc 1, learning_rate 0.0001
2017-09-29T13:48:28.571652: step 4472, loss 0.019176, acc 1, learning_rate 0.0001
2017-09-29T13:48:28.758247: step 4473, loss 0.0499737, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:28.939698: step 4474, loss 0.0421408, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:29.122478: step 4475, loss 0.0399895, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:29.309232: step 4476, loss 0.0830516, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:29.492823: step 4477, loss 0.0399084, acc 1, learning_rate 0.0001
2017-09-29T13:48:29.675326: step 4478, loss 0.0342345, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:29.861603: step 4479, loss 0.0408725, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:30.045289: step 4480, loss 0.052558, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:48:30.596722: step 4480, loss 0.213801, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4480

2017-09-29T13:48:31.304171: step 4481, loss 0.0414656, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:31.485124: step 4482, loss 0.0943099, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:31.670591: step 4483, loss 0.0219065, acc 1, learning_rate 0.0001
2017-09-29T13:48:31.853420: step 4484, loss 0.0377896, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:32.048000: step 4485, loss 0.0707236, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:32.231375: step 4486, loss 0.050935, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:32.415715: step 4487, loss 0.0123512, acc 1, learning_rate 0.0001
2017-09-29T13:48:32.598497: step 4488, loss 0.0213081, acc 1, learning_rate 0.0001
2017-09-29T13:48:32.777569: step 4489, loss 0.0166881, acc 1, learning_rate 0.0001
2017-09-29T13:48:32.958949: step 4490, loss 0.0263689, acc 1, learning_rate 0.0001
2017-09-29T13:48:33.141460: step 4491, loss 0.0683688, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:33.322943: step 4492, loss 0.0567907, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:33.520841: step 4493, loss 0.0537004, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:33.701423: step 4494, loss 0.0415187, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:33.887717: step 4495, loss 0.0892655, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:34.071824: step 4496, loss 0.0856773, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:34.267842: step 4497, loss 0.0289639, acc 1, learning_rate 0.0001
2017-09-29T13:48:34.453623: step 4498, loss 0.0626869, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:34.637482: step 4499, loss 0.0171476, acc 1, learning_rate 0.0001
2017-09-29T13:48:34.820397: step 4500, loss 0.0108468, acc 1, learning_rate 0.0001
2017-09-29T13:48:35.003464: step 4501, loss 0.0686854, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:35.189535: step 4502, loss 0.0402577, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:35.372724: step 4503, loss 0.100194, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:35.557236: step 4504, loss 0.0407538, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:35.742926: step 4505, loss 0.0456372, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:35.921318: step 4506, loss 0.0293581, acc 1, learning_rate 0.0001
2017-09-29T13:48:36.105837: step 4507, loss 0.0520138, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:36.257929: step 4508, loss 0.0528998, acc 0.960784, learning_rate 0.0001
2017-09-29T13:48:36.442922: step 4509, loss 0.00673526, acc 1, learning_rate 0.0001
2017-09-29T13:48:36.626435: step 4510, loss 0.0172679, acc 1, learning_rate 0.0001
2017-09-29T13:48:36.806463: step 4511, loss 0.0424776, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:36.987637: step 4512, loss 0.0493083, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:37.176963: step 4513, loss 0.0635003, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:37.366930: step 4514, loss 0.0344792, acc 1, learning_rate 0.0001
2017-09-29T13:48:37.551612: step 4515, loss 0.0234126, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:37.738193: step 4516, loss 0.0816892, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:37.917914: step 4517, loss 0.0221545, acc 1, learning_rate 0.0001
2017-09-29T13:48:38.099713: step 4518, loss 0.029648, acc 1, learning_rate 0.0001
2017-09-29T13:48:38.285589: step 4519, loss 0.0259474, acc 1, learning_rate 0.0001
2017-09-29T13:48:38.475181: step 4520, loss 0.0368823, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:48:39.026657: step 4520, loss 0.215965, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4520

2017-09-29T13:48:39.818970: step 4521, loss 0.0494641, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:40.004223: step 4522, loss 0.0772153, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:40.193614: step 4523, loss 0.0257405, acc 1, learning_rate 0.0001
2017-09-29T13:48:40.375487: step 4524, loss 0.0138983, acc 1, learning_rate 0.0001
2017-09-29T13:48:40.555419: step 4525, loss 0.0628723, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:40.739410: step 4526, loss 0.0840369, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:40.924238: step 4527, loss 0.0336024, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:41.116865: step 4528, loss 0.0101369, acc 1, learning_rate 0.0001
2017-09-29T13:48:41.303348: step 4529, loss 0.0510617, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:41.488323: step 4530, loss 0.100133, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:41.677135: step 4531, loss 0.058216, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:41.860436: step 4532, loss 0.0963146, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:42.039470: step 4533, loss 0.0336895, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:42.239157: step 4534, loss 0.0568319, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:42.435108: step 4535, loss 0.0264295, acc 1, learning_rate 0.0001
2017-09-29T13:48:42.620612: step 4536, loss 0.0574462, acc 1, learning_rate 0.0001
2017-09-29T13:48:42.802001: step 4537, loss 0.0683252, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:42.985099: step 4538, loss 0.0553329, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:43.165674: step 4539, loss 0.0343497, acc 1, learning_rate 0.0001
2017-09-29T13:48:43.347208: step 4540, loss 0.0631364, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:43.531175: step 4541, loss 0.0244906, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:43.713338: step 4542, loss 0.0313504, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:43.893822: step 4543, loss 0.0564537, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:44.075348: step 4544, loss 0.0420899, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:44.258943: step 4545, loss 0.0110808, acc 1, learning_rate 0.0001
2017-09-29T13:48:44.453959: step 4546, loss 0.0221769, acc 1, learning_rate 0.0001
2017-09-29T13:48:44.641209: step 4547, loss 0.00675251, acc 1, learning_rate 0.0001
2017-09-29T13:48:44.824224: step 4548, loss 0.0765328, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:45.005871: step 4549, loss 0.0427504, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:45.223381: step 4550, loss 0.0287937, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:45.408431: step 4551, loss 0.0207152, acc 1, learning_rate 0.0001
2017-09-29T13:48:45.591022: step 4552, loss 0.017388, acc 1, learning_rate 0.0001
2017-09-29T13:48:45.770807: step 4553, loss 0.111375, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:45.956033: step 4554, loss 0.0249436, acc 1, learning_rate 0.0001
2017-09-29T13:48:46.140744: step 4555, loss 0.0519845, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:46.325083: step 4556, loss 0.00664389, acc 1, learning_rate 0.0001
2017-09-29T13:48:46.502521: step 4557, loss 0.0276953, acc 1, learning_rate 0.0001
2017-09-29T13:48:46.690731: step 4558, loss 0.0560661, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:46.875061: step 4559, loss 0.0631426, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:47.056494: step 4560, loss 0.0244803, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:48:47.612962: step 4560, loss 0.214083, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4560

2017-09-29T13:48:48.246329: step 4561, loss 0.0223487, acc 1, learning_rate 0.0001
2017-09-29T13:48:48.432799: step 4562, loss 0.0359689, acc 1, learning_rate 0.0001
2017-09-29T13:48:48.616257: step 4563, loss 0.063813, acc 0.953125, learning_rate 0.0001
2017-09-29T13:48:48.799290: step 4564, loss 0.0456959, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:48.982862: step 4565, loss 0.0623997, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:49.173198: step 4566, loss 0.0334061, acc 1, learning_rate 0.0001
2017-09-29T13:48:49.371837: step 4567, loss 0.0174924, acc 1, learning_rate 0.0001
2017-09-29T13:48:49.556666: step 4568, loss 0.0527753, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:49.742493: step 4569, loss 0.0461602, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:49.936347: step 4570, loss 0.0456879, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:50.122313: step 4571, loss 0.0452033, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:50.309546: step 4572, loss 0.0438068, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:50.521494: step 4573, loss 0.0301999, acc 1, learning_rate 0.0001
2017-09-29T13:48:50.710787: step 4574, loss 0.0628285, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:50.896611: step 4575, loss 0.0548401, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:51.094271: step 4576, loss 0.0504118, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:51.275793: step 4577, loss 0.0117929, acc 1, learning_rate 0.0001
2017-09-29T13:48:51.468136: step 4578, loss 0.0205405, acc 1, learning_rate 0.0001
2017-09-29T13:48:51.665492: step 4579, loss 0.107814, acc 0.9375, learning_rate 0.0001
2017-09-29T13:48:51.851648: step 4580, loss 0.0565496, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:52.053695: step 4581, loss 0.0301203, acc 1, learning_rate 0.0001
2017-09-29T13:48:52.254590: step 4582, loss 0.0269119, acc 1, learning_rate 0.0001
2017-09-29T13:48:52.451151: step 4583, loss 0.0133555, acc 1, learning_rate 0.0001
2017-09-29T13:48:52.648067: step 4584, loss 0.0461409, acc 1, learning_rate 0.0001
2017-09-29T13:48:52.834365: step 4585, loss 0.0334192, acc 1, learning_rate 0.0001
2017-09-29T13:48:53.017845: step 4586, loss 0.0223621, acc 1, learning_rate 0.0001
2017-09-29T13:48:53.201947: step 4587, loss 0.0422149, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:53.385007: step 4588, loss 0.0226087, acc 1, learning_rate 0.0001
2017-09-29T13:48:53.570167: step 4589, loss 0.0115798, acc 1, learning_rate 0.0001
2017-09-29T13:48:53.764533: step 4590, loss 0.0612847, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:53.951495: step 4591, loss 0.053086, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:54.134539: step 4592, loss 0.00841947, acc 1, learning_rate 0.0001
2017-09-29T13:48:54.320543: step 4593, loss 0.0198972, acc 1, learning_rate 0.0001
2017-09-29T13:48:54.506851: step 4594, loss 0.0233955, acc 1, learning_rate 0.0001
2017-09-29T13:48:54.703059: step 4595, loss 0.0398233, acc 1, learning_rate 0.0001
2017-09-29T13:48:54.893597: step 4596, loss 0.0605667, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:55.088844: step 4597, loss 0.042734, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:55.283054: step 4598, loss 0.042639, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:55.494605: step 4599, loss 0.0325161, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:55.688125: step 4600, loss 0.0218207, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:48:56.246743: step 4600, loss 0.213615, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4600

2017-09-29T13:48:56.959708: step 4601, loss 0.0389556, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:57.153318: step 4602, loss 0.0161495, acc 1, learning_rate 0.0001
2017-09-29T13:48:57.353482: step 4603, loss 0.0385732, acc 1, learning_rate 0.0001
2017-09-29T13:48:57.546919: step 4604, loss 0.023444, acc 1, learning_rate 0.0001
2017-09-29T13:48:57.740482: step 4605, loss 0.0476599, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:57.892864: step 4606, loss 0.0428104, acc 0.980392, learning_rate 0.0001
2017-09-29T13:48:58.087092: step 4607, loss 0.0425757, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:58.280599: step 4608, loss 0.0490186, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:58.467151: step 4609, loss 0.0221431, acc 1, learning_rate 0.0001
2017-09-29T13:48:58.651493: step 4610, loss 0.0860755, acc 0.96875, learning_rate 0.0001
2017-09-29T13:48:58.836658: step 4611, loss 0.102661, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:59.019864: step 4612, loss 0.0304639, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:59.215140: step 4613, loss 0.0417661, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:59.407564: step 4614, loss 0.100135, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:59.594141: step 4615, loss 0.0424179, acc 0.984375, learning_rate 0.0001
2017-09-29T13:48:59.785980: step 4616, loss 0.0249863, acc 1, learning_rate 0.0001
2017-09-29T13:48:59.973819: step 4617, loss 0.0369361, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:00.168556: step 4618, loss 0.0594475, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:00.357701: step 4619, loss 0.0498634, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:00.556132: step 4620, loss 0.0122796, acc 1, learning_rate 0.0001
2017-09-29T13:49:00.741959: step 4621, loss 0.0613275, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:00.921810: step 4622, loss 0.0819352, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:01.108482: step 4623, loss 0.0477833, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:01.295963: step 4624, loss 0.0269021, acc 1, learning_rate 0.0001
2017-09-29T13:49:01.488473: step 4625, loss 0.0465346, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:01.680879: step 4626, loss 0.0805293, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:01.880677: step 4627, loss 0.091818, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:02.070891: step 4628, loss 0.0443338, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:02.276876: step 4629, loss 0.0316869, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:02.469178: step 4630, loss 0.0190816, acc 1, learning_rate 0.0001
2017-09-29T13:49:02.656194: step 4631, loss 0.0733152, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:02.841594: step 4632, loss 0.0288059, acc 1, learning_rate 0.0001
2017-09-29T13:49:03.024829: step 4633, loss 0.0201626, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:03.204253: step 4634, loss 0.0149438, acc 1, learning_rate 0.0001
2017-09-29T13:49:03.387716: step 4635, loss 0.0236939, acc 1, learning_rate 0.0001
2017-09-29T13:49:03.567696: step 4636, loss 0.0113999, acc 1, learning_rate 0.0001
2017-09-29T13:49:03.757423: step 4637, loss 0.0523897, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:03.940383: step 4638, loss 0.0613321, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:04.124336: step 4639, loss 0.0488749, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:04.316917: step 4640, loss 0.0669848, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:49:04.875534: step 4640, loss 0.21598, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4640

2017-09-29T13:49:05.682158: step 4641, loss 0.0307276, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:05.862737: step 4642, loss 0.102649, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:06.043386: step 4643, loss 0.0320371, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:06.229220: step 4644, loss 0.0265316, acc 1, learning_rate 0.0001
2017-09-29T13:49:06.419862: step 4645, loss 0.0392049, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:06.611639: step 4646, loss 0.0494745, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:06.797187: step 4647, loss 0.0370098, acc 1, learning_rate 0.0001
2017-09-29T13:49:06.990457: step 4648, loss 0.0505152, acc 1, learning_rate 0.0001
2017-09-29T13:49:07.187553: step 4649, loss 0.0192253, acc 1, learning_rate 0.0001
2017-09-29T13:49:07.390567: step 4650, loss 0.0502079, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:07.590375: step 4651, loss 0.0654146, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:07.775930: step 4652, loss 0.0179925, acc 1, learning_rate 0.0001
2017-09-29T13:49:07.957285: step 4653, loss 0.044795, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:08.145144: step 4654, loss 0.0241992, acc 1, learning_rate 0.0001
2017-09-29T13:49:08.333690: step 4655, loss 0.0502214, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:08.520561: step 4656, loss 0.0445346, acc 1, learning_rate 0.0001
2017-09-29T13:49:08.707407: step 4657, loss 0.0802985, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:08.888444: step 4658, loss 0.00811123, acc 1, learning_rate 0.0001
2017-09-29T13:49:09.069812: step 4659, loss 0.0098439, acc 1, learning_rate 0.0001
2017-09-29T13:49:09.256468: step 4660, loss 0.0533682, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:09.445393: step 4661, loss 0.0251162, acc 1, learning_rate 0.0001
2017-09-29T13:49:09.627488: step 4662, loss 0.0337916, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:09.810829: step 4663, loss 0.0422677, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:09.996113: step 4664, loss 0.0153728, acc 1, learning_rate 0.0001
2017-09-29T13:49:10.187922: step 4665, loss 0.0418536, acc 1, learning_rate 0.0001
2017-09-29T13:49:10.375060: step 4666, loss 0.0534759, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:10.565679: step 4667, loss 0.0299517, acc 1, learning_rate 0.0001
2017-09-29T13:49:10.760276: step 4668, loss 0.0685034, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:10.956658: step 4669, loss 0.0622127, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:11.143425: step 4670, loss 0.0674946, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:11.327451: step 4671, loss 0.0459093, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:11.517424: step 4672, loss 0.028164, acc 1, learning_rate 0.0001
2017-09-29T13:49:11.710380: step 4673, loss 0.0330376, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:11.896058: step 4674, loss 0.0987561, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:12.077876: step 4675, loss 0.0137569, acc 1, learning_rate 0.0001
2017-09-29T13:49:12.261348: step 4676, loss 0.0543321, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:12.453263: step 4677, loss 0.0618196, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:12.635937: step 4678, loss 0.00879055, acc 1, learning_rate 0.0001
2017-09-29T13:49:12.820218: step 4679, loss 0.0744782, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:13.016381: step 4680, loss 0.0425607, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:49:13.588753: step 4680, loss 0.214166, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4680

2017-09-29T13:49:14.221209: step 4681, loss 0.019493, acc 1, learning_rate 0.0001
2017-09-29T13:49:14.415507: step 4682, loss 0.0154963, acc 1, learning_rate 0.0001
2017-09-29T13:49:14.599596: step 4683, loss 0.0144932, acc 1, learning_rate 0.0001
2017-09-29T13:49:14.787826: step 4684, loss 0.0144301, acc 1, learning_rate 0.0001
2017-09-29T13:49:14.967885: step 4685, loss 0.0855689, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:15.152793: step 4686, loss 0.0203635, acc 1, learning_rate 0.0001
2017-09-29T13:49:15.344331: step 4687, loss 0.0484474, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:15.532999: step 4688, loss 0.0302343, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:15.717956: step 4689, loss 0.0190948, acc 1, learning_rate 0.0001
2017-09-29T13:49:15.900895: step 4690, loss 0.0184725, acc 1, learning_rate 0.0001
2017-09-29T13:49:16.083946: step 4691, loss 0.0106548, acc 1, learning_rate 0.0001
2017-09-29T13:49:16.267086: step 4692, loss 0.0425278, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:16.452549: step 4693, loss 0.0715391, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:16.650072: step 4694, loss 0.0278932, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:16.855801: step 4695, loss 0.0778756, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:17.047474: step 4696, loss 0.0577537, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:17.231553: step 4697, loss 0.111559, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:17.420489: step 4698, loss 0.0254983, acc 1, learning_rate 0.0001
2017-09-29T13:49:17.603766: step 4699, loss 0.0771434, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:17.785884: step 4700, loss 0.0306845, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:17.972822: step 4701, loss 0.00861417, acc 1, learning_rate 0.0001
2017-09-29T13:49:18.156129: step 4702, loss 0.0138438, acc 1, learning_rate 0.0001
2017-09-29T13:49:18.350398: step 4703, loss 0.0914932, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:18.536291: step 4704, loss 0.0535406, acc 0.960784, learning_rate 0.0001
2017-09-29T13:49:18.725168: step 4705, loss 0.0198837, acc 1, learning_rate 0.0001
2017-09-29T13:49:18.905529: step 4706, loss 0.0137709, acc 1, learning_rate 0.0001
2017-09-29T13:49:19.086506: step 4707, loss 0.0159812, acc 1, learning_rate 0.0001
2017-09-29T13:49:19.275401: step 4708, loss 0.0446654, acc 1, learning_rate 0.0001
2017-09-29T13:49:19.456211: step 4709, loss 0.0135915, acc 1, learning_rate 0.0001
2017-09-29T13:49:19.637664: step 4710, loss 0.0515822, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:19.818642: step 4711, loss 0.0539616, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:20.001354: step 4712, loss 0.0234085, acc 1, learning_rate 0.0001
2017-09-29T13:49:20.186721: step 4713, loss 0.0639718, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:20.369735: step 4714, loss 0.0145196, acc 1, learning_rate 0.0001
2017-09-29T13:49:20.557058: step 4715, loss 0.0213755, acc 1, learning_rate 0.0001
2017-09-29T13:49:20.742374: step 4716, loss 0.0751915, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:20.930775: step 4717, loss 0.0250607, acc 1, learning_rate 0.0001
2017-09-29T13:49:21.115677: step 4718, loss 0.0302095, acc 1, learning_rate 0.0001
2017-09-29T13:49:21.304903: step 4719, loss 0.124652, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:21.498276: step 4720, loss 0.0574777, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:49:22.055029: step 4720, loss 0.212264, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4720

2017-09-29T13:49:22.762132: step 4721, loss 0.00827305, acc 1, learning_rate 0.0001
2017-09-29T13:49:22.943568: step 4722, loss 0.122808, acc 0.9375, learning_rate 0.0001
2017-09-29T13:49:23.127501: step 4723, loss 0.0800578, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:23.312259: step 4724, loss 0.0169435, acc 1, learning_rate 0.0001
2017-09-29T13:49:23.498579: step 4725, loss 0.025681, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:23.687826: step 4726, loss 0.0293232, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:23.878217: step 4727, loss 0.04843, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:24.062282: step 4728, loss 0.0838922, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:24.246596: step 4729, loss 0.0516552, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:24.432996: step 4730, loss 0.0306135, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:24.624984: step 4731, loss 0.0418859, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:24.809100: step 4732, loss 0.130066, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:24.991854: step 4733, loss 0.0140197, acc 1, learning_rate 0.0001
2017-09-29T13:49:25.175469: step 4734, loss 0.0370398, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:25.360262: step 4735, loss 0.0398295, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:25.542917: step 4736, loss 0.0219487, acc 1, learning_rate 0.0001
2017-09-29T13:49:25.726703: step 4737, loss 0.0284177, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:25.907614: step 4738, loss 0.0135582, acc 1, learning_rate 0.0001
2017-09-29T13:49:26.085734: step 4739, loss 0.0270891, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:26.267110: step 4740, loss 0.0419314, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:26.452175: step 4741, loss 0.0274192, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:26.634176: step 4742, loss 0.0360442, acc 1, learning_rate 0.0001
2017-09-29T13:49:26.820707: step 4743, loss 0.0973783, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:27.004803: step 4744, loss 0.0970084, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:27.192530: step 4745, loss 0.0357871, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:27.379471: step 4746, loss 0.0456095, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:27.580358: step 4747, loss 0.0773642, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:27.766948: step 4748, loss 0.0128367, acc 1, learning_rate 0.0001
2017-09-29T13:49:27.958117: step 4749, loss 0.0360604, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:28.149959: step 4750, loss 0.0616638, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:28.332090: step 4751, loss 0.0145456, acc 1, learning_rate 0.0001
2017-09-29T13:49:28.514123: step 4752, loss 0.0189313, acc 1, learning_rate 0.0001
2017-09-29T13:49:28.698862: step 4753, loss 0.0348535, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:28.886469: step 4754, loss 0.0762258, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:29.072547: step 4755, loss 0.0111063, acc 1, learning_rate 0.0001
2017-09-29T13:49:29.259469: step 4756, loss 0.0193104, acc 1, learning_rate 0.0001
2017-09-29T13:49:29.467654: step 4757, loss 0.0515354, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:29.658936: step 4758, loss 0.0292729, acc 1, learning_rate 0.0001
2017-09-29T13:49:29.851225: step 4759, loss 0.0176918, acc 1, learning_rate 0.0001
2017-09-29T13:49:30.053075: step 4760, loss 0.0393229, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:49:30.627907: step 4760, loss 0.212028, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4760

2017-09-29T13:49:31.440848: step 4761, loss 0.031742, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:31.625849: step 4762, loss 0.0178499, acc 1, learning_rate 0.0001
2017-09-29T13:49:31.823293: step 4763, loss 0.0132365, acc 1, learning_rate 0.0001
2017-09-29T13:49:32.025583: step 4764, loss 0.0711937, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:32.210352: step 4765, loss 0.0391215, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:32.395706: step 4766, loss 0.0774457, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:32.583265: step 4767, loss 0.163146, acc 0.9375, learning_rate 0.0001
2017-09-29T13:49:32.768985: step 4768, loss 0.0271179, acc 1, learning_rate 0.0001
2017-09-29T13:49:32.954675: step 4769, loss 0.019814, acc 1, learning_rate 0.0001
2017-09-29T13:49:33.145099: step 4770, loss 0.0113487, acc 1, learning_rate 0.0001
2017-09-29T13:49:33.330923: step 4771, loss 0.103294, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:33.515180: step 4772, loss 0.026625, acc 1, learning_rate 0.0001
2017-09-29T13:49:33.700881: step 4773, loss 0.0798025, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:33.884673: step 4774, loss 0.0183095, acc 1, learning_rate 0.0001
2017-09-29T13:49:34.064946: step 4775, loss 0.0140694, acc 1, learning_rate 0.0001
2017-09-29T13:49:34.247028: step 4776, loss 0.0136796, acc 1, learning_rate 0.0001
2017-09-29T13:49:34.434138: step 4777, loss 0.0282769, acc 1, learning_rate 0.0001
2017-09-29T13:49:34.623407: step 4778, loss 0.0562204, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:34.813388: step 4779, loss 0.029586, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:34.998780: step 4780, loss 0.0610958, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:35.193812: step 4781, loss 0.0176234, acc 1, learning_rate 0.0001
2017-09-29T13:49:35.379793: step 4782, loss 0.0500962, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:35.576611: step 4783, loss 0.0588933, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:35.759079: step 4784, loss 0.00866398, acc 1, learning_rate 0.0001
2017-09-29T13:49:35.942455: step 4785, loss 0.071061, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:36.122066: step 4786, loss 0.051868, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:36.310067: step 4787, loss 0.0683507, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:36.499469: step 4788, loss 0.0106838, acc 1, learning_rate 0.0001
2017-09-29T13:49:36.694264: step 4789, loss 0.00726426, acc 1, learning_rate 0.0001
2017-09-29T13:49:36.875625: step 4790, loss 0.0377722, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:37.060875: step 4791, loss 0.0625569, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:37.252741: step 4792, loss 0.0519585, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:37.466320: step 4793, loss 0.0140644, acc 1, learning_rate 0.0001
2017-09-29T13:49:37.659841: step 4794, loss 0.0577577, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:37.851779: step 4795, loss 0.00906354, acc 1, learning_rate 0.0001
2017-09-29T13:49:38.038546: step 4796, loss 0.109395, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:38.230965: step 4797, loss 0.0437188, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:38.433485: step 4798, loss 0.0280714, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:38.620361: step 4799, loss 0.0296729, acc 1, learning_rate 0.0001
2017-09-29T13:49:38.801254: step 4800, loss 0.0477055, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:49:39.341240: step 4800, loss 0.215684, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4800

2017-09-29T13:49:39.984224: step 4801, loss 0.00648737, acc 1, learning_rate 0.0001
2017-09-29T13:49:40.148808: step 4802, loss 0.115357, acc 0.941176, learning_rate 0.0001
2017-09-29T13:49:40.348501: step 4803, loss 0.0307306, acc 1, learning_rate 0.0001
2017-09-29T13:49:40.552185: step 4804, loss 0.0299882, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:40.735487: step 4805, loss 0.0708477, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:40.922094: step 4806, loss 0.0523317, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:41.102446: step 4807, loss 0.0355827, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:41.283405: step 4808, loss 0.0350155, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:41.484914: step 4809, loss 0.0328882, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:41.670996: step 4810, loss 0.0159236, acc 1, learning_rate 0.0001
2017-09-29T13:49:41.854891: step 4811, loss 0.0220352, acc 1, learning_rate 0.0001
2017-09-29T13:49:42.032643: step 4812, loss 0.0476565, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:42.228666: step 4813, loss 0.056526, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:42.413538: step 4814, loss 0.0557403, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:42.599801: step 4815, loss 0.0145253, acc 1, learning_rate 0.0001
2017-09-29T13:49:42.780621: step 4816, loss 0.0264816, acc 1, learning_rate 0.0001
2017-09-29T13:49:42.962832: step 4817, loss 0.0222461, acc 1, learning_rate 0.0001
2017-09-29T13:49:43.142990: step 4818, loss 0.0137574, acc 1, learning_rate 0.0001
2017-09-29T13:49:43.337550: step 4819, loss 0.0616869, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:43.521670: step 4820, loss 0.00826899, acc 1, learning_rate 0.0001
2017-09-29T13:49:43.704649: step 4821, loss 0.0091416, acc 1, learning_rate 0.0001
2017-09-29T13:49:43.894003: step 4822, loss 0.0668792, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:44.076591: step 4823, loss 0.0256242, acc 1, learning_rate 0.0001
2017-09-29T13:49:44.257445: step 4824, loss 0.0212392, acc 1, learning_rate 0.0001
2017-09-29T13:49:44.441753: step 4825, loss 0.0446717, acc 1, learning_rate 0.0001
2017-09-29T13:49:44.624460: step 4826, loss 0.0207859, acc 1, learning_rate 0.0001
2017-09-29T13:49:44.821232: step 4827, loss 0.0121945, acc 1, learning_rate 0.0001
2017-09-29T13:49:45.003944: step 4828, loss 0.0552321, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:45.196421: step 4829, loss 0.063821, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:45.381084: step 4830, loss 0.0244176, acc 1, learning_rate 0.0001
2017-09-29T13:49:45.569484: step 4831, loss 0.0530395, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:45.753188: step 4832, loss 0.0679464, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:45.939473: step 4833, loss 0.0299613, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:46.126815: step 4834, loss 0.0372419, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:46.310616: step 4835, loss 0.0551648, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:46.505796: step 4836, loss 0.0196005, acc 1, learning_rate 0.0001
2017-09-29T13:49:46.695484: step 4837, loss 0.0604941, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:46.883417: step 4838, loss 0.034288, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:47.072909: step 4839, loss 0.029656, acc 1, learning_rate 0.0001
2017-09-29T13:49:47.255120: step 4840, loss 0.0515474, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:49:47.822058: step 4840, loss 0.213272, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4840

2017-09-29T13:49:48.538045: step 4841, loss 0.0271897, acc 1, learning_rate 0.0001
2017-09-29T13:49:48.719505: step 4842, loss 0.0469617, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:48.904303: step 4843, loss 0.0158261, acc 1, learning_rate 0.0001
2017-09-29T13:49:49.087806: step 4844, loss 0.020017, acc 1, learning_rate 0.0001
2017-09-29T13:49:49.272418: step 4845, loss 0.0330118, acc 1, learning_rate 0.0001
2017-09-29T13:49:49.476479: step 4846, loss 0.0799278, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:49.667465: step 4847, loss 0.048517, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:49.848113: step 4848, loss 0.0100436, acc 1, learning_rate 0.0001
2017-09-29T13:49:50.050830: step 4849, loss 0.0443274, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:50.255816: step 4850, loss 0.00906059, acc 1, learning_rate 0.0001
2017-09-29T13:49:50.457098: step 4851, loss 0.0736493, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:50.641217: step 4852, loss 0.0692282, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:50.833988: step 4853, loss 0.0460199, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:51.027965: step 4854, loss 0.0630017, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:51.226050: step 4855, loss 0.0270616, acc 1, learning_rate 0.0001
2017-09-29T13:49:51.431218: step 4856, loss 0.0332907, acc 1, learning_rate 0.0001
2017-09-29T13:49:51.641877: step 4857, loss 0.0074015, acc 1, learning_rate 0.0001
2017-09-29T13:49:51.824985: step 4858, loss 0.0180939, acc 1, learning_rate 0.0001
2017-09-29T13:49:52.013604: step 4859, loss 0.0323359, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:52.194251: step 4860, loss 0.0419105, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:52.378445: step 4861, loss 0.0422703, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:52.574034: step 4862, loss 0.0262077, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:52.763044: step 4863, loss 0.0526417, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:52.946058: step 4864, loss 0.0126584, acc 1, learning_rate 0.0001
2017-09-29T13:49:53.130206: step 4865, loss 0.0216498, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:53.323874: step 4866, loss 0.093091, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:53.513733: step 4867, loss 0.0247115, acc 1, learning_rate 0.0001
2017-09-29T13:49:53.704371: step 4868, loss 0.0750762, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:53.893889: step 4869, loss 0.0138101, acc 1, learning_rate 0.0001
2017-09-29T13:49:54.075221: step 4870, loss 0.0286003, acc 1, learning_rate 0.0001
2017-09-29T13:49:54.255878: step 4871, loss 0.131683, acc 0.953125, learning_rate 0.0001
2017-09-29T13:49:54.452052: step 4872, loss 0.029218, acc 1, learning_rate 0.0001
2017-09-29T13:49:54.634428: step 4873, loss 0.0241919, acc 1, learning_rate 0.0001
2017-09-29T13:49:54.821640: step 4874, loss 0.0372562, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:55.010755: step 4875, loss 0.00697133, acc 1, learning_rate 0.0001
2017-09-29T13:49:55.194955: step 4876, loss 0.0427167, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:55.380795: step 4877, loss 0.0341658, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:55.564672: step 4878, loss 0.0691187, acc 0.96875, learning_rate 0.0001
2017-09-29T13:49:55.760973: step 4879, loss 0.030469, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:55.941106: step 4880, loss 0.0888899, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:49:56.484841: step 4880, loss 0.215459, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4880

2017-09-29T13:49:57.281428: step 4881, loss 0.0308825, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:57.477013: step 4882, loss 0.0226356, acc 1, learning_rate 0.0001
2017-09-29T13:49:57.660565: step 4883, loss 0.0187017, acc 1, learning_rate 0.0001
2017-09-29T13:49:57.855176: step 4884, loss 0.0295478, acc 1, learning_rate 0.0001
2017-09-29T13:49:58.036729: step 4885, loss 0.0352581, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:58.223643: step 4886, loss 0.0730742, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:58.418848: step 4887, loss 0.0329188, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:58.619488: step 4888, loss 0.0450084, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:58.814216: step 4889, loss 0.031857, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:59.005361: step 4890, loss 0.0590726, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:59.195164: step 4891, loss 0.0459348, acc 0.984375, learning_rate 0.0001
2017-09-29T13:49:59.376815: step 4892, loss 0.0355456, acc 1, learning_rate 0.0001
2017-09-29T13:49:59.560465: step 4893, loss 0.032267, acc 1, learning_rate 0.0001
2017-09-29T13:49:59.750608: step 4894, loss 0.00982846, acc 1, learning_rate 0.0001
2017-09-29T13:49:59.934729: step 4895, loss 0.0342258, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:00.117744: step 4896, loss 0.0274435, acc 1, learning_rate 0.0001
2017-09-29T13:50:00.304006: step 4897, loss 0.0546779, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:00.507026: step 4898, loss 0.0352468, acc 1, learning_rate 0.0001
2017-09-29T13:50:00.696353: step 4899, loss 0.0191177, acc 1, learning_rate 0.0001
2017-09-29T13:50:00.849381: step 4900, loss 0.0724538, acc 0.980392, learning_rate 0.0001
2017-09-29T13:50:01.033227: step 4901, loss 0.0406154, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:01.212870: step 4902, loss 0.0187734, acc 1, learning_rate 0.0001
2017-09-29T13:50:01.396501: step 4903, loss 0.0640314, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:01.591443: step 4904, loss 0.0528923, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:01.775589: step 4905, loss 0.0609231, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:01.969439: step 4906, loss 0.0395125, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:02.172908: step 4907, loss 0.0241984, acc 1, learning_rate 0.0001
2017-09-29T13:50:02.376578: step 4908, loss 0.0270982, acc 1, learning_rate 0.0001
2017-09-29T13:50:02.565230: step 4909, loss 0.0207786, acc 1, learning_rate 0.0001
2017-09-29T13:50:02.753162: step 4910, loss 0.0187233, acc 1, learning_rate 0.0001
2017-09-29T13:50:02.939911: step 4911, loss 0.00987052, acc 1, learning_rate 0.0001
2017-09-29T13:50:03.126577: step 4912, loss 0.0334018, acc 1, learning_rate 0.0001
2017-09-29T13:50:03.319744: step 4913, loss 0.0621897, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:03.527668: step 4914, loss 0.0495535, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:03.712796: step 4915, loss 0.0421759, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:03.893378: step 4916, loss 0.0123172, acc 1, learning_rate 0.0001
2017-09-29T13:50:04.087230: step 4917, loss 0.0370908, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:04.273330: step 4918, loss 0.0122513, acc 1, learning_rate 0.0001
2017-09-29T13:50:04.466294: step 4919, loss 0.0671059, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:04.659444: step 4920, loss 0.0480032, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:50:05.279445: step 4920, loss 0.214928, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4920

2017-09-29T13:50:05.930562: step 4921, loss 0.0419427, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:06.114234: step 4922, loss 0.0148155, acc 1, learning_rate 0.0001
2017-09-29T13:50:06.296448: step 4923, loss 0.0341124, acc 1, learning_rate 0.0001
2017-09-29T13:50:06.478621: step 4924, loss 0.0261804, acc 1, learning_rate 0.0001
2017-09-29T13:50:06.678122: step 4925, loss 0.0431743, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:06.863706: step 4926, loss 0.0853592, acc 0.953125, learning_rate 0.0001
2017-09-29T13:50:07.045013: step 4927, loss 0.0441025, acc 1, learning_rate 0.0001
2017-09-29T13:50:07.228434: step 4928, loss 0.0419761, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:07.411005: step 4929, loss 0.0148051, acc 1, learning_rate 0.0001
2017-09-29T13:50:07.594677: step 4930, loss 0.0424282, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:07.793450: step 4931, loss 0.0386998, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:07.974113: step 4932, loss 0.0348807, acc 1, learning_rate 0.0001
2017-09-29T13:50:08.157369: step 4933, loss 0.0468041, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:08.343013: step 4934, loss 0.0275205, acc 1, learning_rate 0.0001
2017-09-29T13:50:08.527578: step 4935, loss 0.0468395, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:08.709772: step 4936, loss 0.0283058, acc 1, learning_rate 0.0001
2017-09-29T13:50:08.894394: step 4937, loss 0.0218493, acc 1, learning_rate 0.0001
2017-09-29T13:50:09.083675: step 4938, loss 0.0764512, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:09.265087: step 4939, loss 0.0517858, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:09.459139: step 4940, loss 0.0659742, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:09.651280: step 4941, loss 0.0365413, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:09.828893: step 4942, loss 0.0425265, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:10.011958: step 4943, loss 0.0459522, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:10.196685: step 4944, loss 0.0280473, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:10.379771: step 4945, loss 0.073725, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:10.574417: step 4946, loss 0.0351812, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:10.758702: step 4947, loss 0.0131951, acc 1, learning_rate 0.0001
2017-09-29T13:50:10.941038: step 4948, loss 0.0507687, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:11.131613: step 4949, loss 0.0180464, acc 1, learning_rate 0.0001
2017-09-29T13:50:11.315924: step 4950, loss 0.0701181, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:11.505081: step 4951, loss 0.0369414, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:11.689831: step 4952, loss 0.0168598, acc 1, learning_rate 0.0001
2017-09-29T13:50:11.872800: step 4953, loss 0.0978711, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:12.055411: step 4954, loss 0.0299047, acc 1, learning_rate 0.0001
2017-09-29T13:50:12.236082: step 4955, loss 0.00554732, acc 1, learning_rate 0.0001
2017-09-29T13:50:12.431566: step 4956, loss 0.0503426, acc 0.953125, learning_rate 0.0001
2017-09-29T13:50:12.612989: step 4957, loss 0.0139413, acc 1, learning_rate 0.0001
2017-09-29T13:50:12.801013: step 4958, loss 0.0421631, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:12.994362: step 4959, loss 0.0644956, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:13.178504: step 4960, loss 0.0897642, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T13:50:13.733402: step 4960, loss 0.214634, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-4960

2017-09-29T13:50:14.441408: step 4961, loss 0.0367637, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:14.621583: step 4962, loss 0.0222469, acc 1, learning_rate 0.0001
2017-09-29T13:50:14.809756: step 4963, loss 0.0110195, acc 1, learning_rate 0.0001
2017-09-29T13:50:14.997142: step 4964, loss 0.0210852, acc 1, learning_rate 0.0001
2017-09-29T13:50:15.181180: step 4965, loss 0.0412788, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:15.372043: step 4966, loss 0.0382867, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:15.558423: step 4967, loss 0.031325, acc 1, learning_rate 0.0001
2017-09-29T13:50:15.744677: step 4968, loss 0.0489113, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:15.962133: step 4969, loss 0.0469693, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:16.144927: step 4970, loss 0.0577867, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:16.332048: step 4971, loss 0.0122524, acc 1, learning_rate 0.0001
2017-09-29T13:50:16.528366: step 4972, loss 0.0747161, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:16.716214: step 4973, loss 0.0539619, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:16.899921: step 4974, loss 0.0653521, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:17.087079: step 4975, loss 0.0164517, acc 1, learning_rate 0.0001
2017-09-29T13:50:17.279145: step 4976, loss 0.0222132, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:17.471390: step 4977, loss 0.0329068, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:17.659469: step 4978, loss 0.0734773, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:17.858221: step 4979, loss 0.0295895, acc 1, learning_rate 0.0001
2017-09-29T13:50:18.044338: step 4980, loss 0.0202149, acc 1, learning_rate 0.0001
2017-09-29T13:50:18.226700: step 4981, loss 0.0170544, acc 1, learning_rate 0.0001
2017-09-29T13:50:18.413313: step 4982, loss 0.0327979, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:18.594615: step 4983, loss 0.0479569, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:18.774552: step 4984, loss 0.0227807, acc 1, learning_rate 0.0001
2017-09-29T13:50:18.955098: step 4985, loss 0.066165, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:19.135203: step 4986, loss 0.0402744, acc 1, learning_rate 0.0001
2017-09-29T13:50:19.322542: step 4987, loss 0.041072, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:19.505846: step 4988, loss 0.0486288, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:19.698095: step 4989, loss 0.0225543, acc 1, learning_rate 0.0001
2017-09-29T13:50:19.887289: step 4990, loss 0.0141729, acc 1, learning_rate 0.0001
2017-09-29T13:50:20.082779: step 4991, loss 0.0643019, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:20.270982: step 4992, loss 0.0942, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:20.457043: step 4993, loss 0.0101698, acc 1, learning_rate 0.0001
2017-09-29T13:50:20.642512: step 4994, loss 0.0455465, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:20.831688: step 4995, loss 0.024965, acc 1, learning_rate 0.0001
2017-09-29T13:50:21.014836: step 4996, loss 0.0218371, acc 1, learning_rate 0.0001
2017-09-29T13:50:21.196807: step 4997, loss 0.0966707, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:21.362998: step 4998, loss 0.0634275, acc 0.980392, learning_rate 0.0001
2017-09-29T13:50:21.561554: step 4999, loss 0.093709, acc 0.9375, learning_rate 0.0001
2017-09-29T13:50:21.755778: step 5000, loss 0.0388626, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:50:22.314990: step 5000, loss 0.217055, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5000

2017-09-29T13:50:23.128997: step 5001, loss 0.0750346, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:23.325455: step 5002, loss 0.00822097, acc 1, learning_rate 0.0001
2017-09-29T13:50:23.529625: step 5003, loss 0.0628345, acc 0.953125, learning_rate 0.0001
2017-09-29T13:50:23.724187: step 5004, loss 0.0295987, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:23.912879: step 5005, loss 0.0513587, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:24.109466: step 5006, loss 0.0335631, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:24.296480: step 5007, loss 0.06792, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:24.500598: step 5008, loss 0.047632, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:24.691639: step 5009, loss 0.00857346, acc 1, learning_rate 0.0001
2017-09-29T13:50:24.874771: step 5010, loss 0.0389862, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:25.056765: step 5011, loss 0.0203863, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:25.246381: step 5012, loss 0.0416323, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:25.440724: step 5013, loss 0.0400108, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:25.635083: step 5014, loss 0.0188136, acc 1, learning_rate 0.0001
2017-09-29T13:50:25.861684: step 5015, loss 0.021429, acc 1, learning_rate 0.0001
2017-09-29T13:50:26.064342: step 5016, loss 0.0369394, acc 1, learning_rate 0.0001
2017-09-29T13:50:26.259235: step 5017, loss 0.0812956, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:26.447743: step 5018, loss 0.0250615, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:26.638812: step 5019, loss 0.0308014, acc 1, learning_rate 0.0001
2017-09-29T13:50:26.837808: step 5020, loss 0.0431878, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:27.024765: step 5021, loss 0.0467426, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:27.211425: step 5022, loss 0.0507857, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:27.406732: step 5023, loss 0.0432234, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:27.596726: step 5024, loss 0.059637, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:27.794723: step 5025, loss 0.0762761, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:27.991481: step 5026, loss 0.0346507, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:28.177921: step 5027, loss 0.0291342, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:28.369777: step 5028, loss 0.054473, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:28.576139: step 5029, loss 0.018611, acc 1, learning_rate 0.0001
2017-09-29T13:50:28.764507: step 5030, loss 0.0471957, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:28.952682: step 5031, loss 0.0293977, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:29.137250: step 5032, loss 0.010628, acc 1, learning_rate 0.0001
2017-09-29T13:50:29.327906: step 5033, loss 0.0129362, acc 1, learning_rate 0.0001
2017-09-29T13:50:29.513412: step 5034, loss 0.0293629, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:29.707347: step 5035, loss 0.0307617, acc 1, learning_rate 0.0001
2017-09-29T13:50:29.889262: step 5036, loss 0.0187164, acc 1, learning_rate 0.0001
2017-09-29T13:50:30.080539: step 5037, loss 0.0339486, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:30.262781: step 5038, loss 0.0291092, acc 1, learning_rate 0.0001
2017-09-29T13:50:30.446248: step 5039, loss 0.0708724, acc 0.953125, learning_rate 0.0001
2017-09-29T13:50:30.630968: step 5040, loss 0.0344888, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:50:31.177799: step 5040, loss 0.21834, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5040

2017-09-29T13:50:31.809634: step 5041, loss 0.0577334, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:31.990304: step 5042, loss 0.08373, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:32.173127: step 5043, loss 0.0162842, acc 1, learning_rate 0.0001
2017-09-29T13:50:32.359811: step 5044, loss 0.0182427, acc 1, learning_rate 0.0001
2017-09-29T13:50:32.544250: step 5045, loss 0.0317249, acc 1, learning_rate 0.0001
2017-09-29T13:50:32.730974: step 5046, loss 0.0447603, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:32.915640: step 5047, loss 0.0362867, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:33.096484: step 5048, loss 0.0867299, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:33.278385: step 5049, loss 0.0457014, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:33.460863: step 5050, loss 0.0521814, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:33.642243: step 5051, loss 0.0812889, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:33.826800: step 5052, loss 0.0192848, acc 1, learning_rate 0.0001
2017-09-29T13:50:34.021422: step 5053, loss 0.0148531, acc 1, learning_rate 0.0001
2017-09-29T13:50:34.207491: step 5054, loss 0.0513409, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:34.389763: step 5055, loss 0.0110377, acc 1, learning_rate 0.0001
2017-09-29T13:50:34.597208: step 5056, loss 0.0315735, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:34.785215: step 5057, loss 0.0721527, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:34.969761: step 5058, loss 0.0432259, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:35.154864: step 5059, loss 0.0794874, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:35.342123: step 5060, loss 0.0196553, acc 1, learning_rate 0.0001
2017-09-29T13:50:35.538513: step 5061, loss 0.0150599, acc 1, learning_rate 0.0001
2017-09-29T13:50:35.735586: step 5062, loss 0.021948, acc 1, learning_rate 0.0001
2017-09-29T13:50:35.919558: step 5063, loss 0.0374055, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:36.103959: step 5064, loss 0.065624, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:36.288646: step 5065, loss 0.0651668, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:36.483431: step 5066, loss 0.00637114, acc 1, learning_rate 0.0001
2017-09-29T13:50:36.673478: step 5067, loss 0.060483, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:36.858305: step 5068, loss 0.0102261, acc 1, learning_rate 0.0001
2017-09-29T13:50:37.046659: step 5069, loss 0.0461177, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:37.231869: step 5070, loss 0.0567947, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:37.414642: step 5071, loss 0.0322922, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:37.608094: step 5072, loss 0.0689788, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:37.790487: step 5073, loss 0.0152473, acc 1, learning_rate 0.0001
2017-09-29T13:50:37.983259: step 5074, loss 0.0698303, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:38.167624: step 5075, loss 0.0762853, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:38.349519: step 5076, loss 0.0409394, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:38.533137: step 5077, loss 0.0481019, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:38.712274: step 5078, loss 0.065186, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:38.897714: step 5079, loss 0.026051, acc 1, learning_rate 0.0001
2017-09-29T13:50:39.078875: step 5080, loss 0.0226355, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:50:39.648721: step 5080, loss 0.214435, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5080

2017-09-29T13:50:40.367848: step 5081, loss 0.00821935, acc 1, learning_rate 0.0001
2017-09-29T13:50:40.570137: step 5082, loss 0.0326209, acc 1, learning_rate 0.0001
2017-09-29T13:50:40.754071: step 5083, loss 0.0212, acc 1, learning_rate 0.0001
2017-09-29T13:50:40.937447: step 5084, loss 0.0674972, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:41.131861: step 5085, loss 0.0219117, acc 1, learning_rate 0.0001
2017-09-29T13:50:41.319185: step 5086, loss 0.0201389, acc 1, learning_rate 0.0001
2017-09-29T13:50:41.503391: step 5087, loss 0.0250438, acc 1, learning_rate 0.0001
2017-09-29T13:50:41.682971: step 5088, loss 0.0514251, acc 1, learning_rate 0.0001
2017-09-29T13:50:41.866950: step 5089, loss 0.00686867, acc 1, learning_rate 0.0001
2017-09-29T13:50:42.054740: step 5090, loss 0.0157503, acc 1, learning_rate 0.0001
2017-09-29T13:50:42.239454: step 5091, loss 0.058464, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:42.426463: step 5092, loss 0.0638189, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:42.612730: step 5093, loss 0.0213693, acc 1, learning_rate 0.0001
2017-09-29T13:50:42.804102: step 5094, loss 0.0667324, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:43.002269: step 5095, loss 0.0175214, acc 1, learning_rate 0.0001
2017-09-29T13:50:43.157660: step 5096, loss 0.0255646, acc 1, learning_rate 0.0001
2017-09-29T13:50:43.338135: step 5097, loss 0.010039, acc 1, learning_rate 0.0001
2017-09-29T13:50:43.521911: step 5098, loss 0.0585769, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:43.716197: step 5099, loss 0.034329, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:43.898870: step 5100, loss 0.0191101, acc 1, learning_rate 0.0001
2017-09-29T13:50:44.080153: step 5101, loss 0.0616105, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:44.262289: step 5102, loss 0.0499881, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:44.443729: step 5103, loss 0.0512635, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:44.626369: step 5104, loss 0.105656, acc 0.953125, learning_rate 0.0001
2017-09-29T13:50:44.811531: step 5105, loss 0.0719948, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:44.992317: step 5106, loss 0.00815403, acc 1, learning_rate 0.0001
2017-09-29T13:50:45.172582: step 5107, loss 0.0389241, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:45.354678: step 5108, loss 0.0107265, acc 1, learning_rate 0.0001
2017-09-29T13:50:45.553991: step 5109, loss 0.0168636, acc 1, learning_rate 0.0001
2017-09-29T13:50:45.738760: step 5110, loss 0.0369827, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:45.925697: step 5111, loss 0.0181164, acc 1, learning_rate 0.0001
2017-09-29T13:50:46.111096: step 5112, loss 0.0211173, acc 1, learning_rate 0.0001
2017-09-29T13:50:46.295803: step 5113, loss 0.0794229, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:46.491097: step 5114, loss 0.0254303, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:46.675901: step 5115, loss 0.0116787, acc 1, learning_rate 0.0001
2017-09-29T13:50:46.861915: step 5116, loss 0.0589839, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:47.045805: step 5117, loss 0.0361226, acc 1, learning_rate 0.0001
2017-09-29T13:50:47.227418: step 5118, loss 0.0955854, acc 0.953125, learning_rate 0.0001
2017-09-29T13:50:47.415067: step 5119, loss 0.056804, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:47.601832: step 5120, loss 0.0119603, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:50:48.170109: step 5120, loss 0.214974, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5120

2017-09-29T13:50:48.892790: step 5121, loss 0.0128163, acc 1, learning_rate 0.0001
2017-09-29T13:50:49.081176: step 5122, loss 0.0121795, acc 1, learning_rate 0.0001
2017-09-29T13:50:49.263149: step 5123, loss 0.0286802, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:49.452893: step 5124, loss 0.0146144, acc 1, learning_rate 0.0001
2017-09-29T13:50:49.638838: step 5125, loss 0.0299918, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:49.833671: step 5126, loss 0.087657, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:50.026729: step 5127, loss 0.0226755, acc 1, learning_rate 0.0001
2017-09-29T13:50:50.232746: step 5128, loss 0.0746022, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:50.446061: step 5129, loss 0.071755, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:50.632095: step 5130, loss 0.0244201, acc 1, learning_rate 0.0001
2017-09-29T13:50:50.823410: step 5131, loss 0.119424, acc 0.9375, learning_rate 0.0001
2017-09-29T13:50:51.006906: step 5132, loss 0.0561646, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:51.194506: step 5133, loss 0.0276533, acc 1, learning_rate 0.0001
2017-09-29T13:50:51.379530: step 5134, loss 0.0411597, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:51.567505: step 5135, loss 0.0290894, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:51.753349: step 5136, loss 0.058011, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:51.965971: step 5137, loss 0.0341542, acc 1, learning_rate 0.0001
2017-09-29T13:50:52.160776: step 5138, loss 0.0564517, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:52.361176: step 5139, loss 0.0122595, acc 1, learning_rate 0.0001
2017-09-29T13:50:52.583669: step 5140, loss 0.0215931, acc 1, learning_rate 0.0001
2017-09-29T13:50:52.782703: step 5141, loss 0.0168007, acc 1, learning_rate 0.0001
2017-09-29T13:50:53.001682: step 5142, loss 0.0183915, acc 1, learning_rate 0.0001
2017-09-29T13:50:53.220372: step 5143, loss 0.0097692, acc 1, learning_rate 0.0001
2017-09-29T13:50:53.425971: step 5144, loss 0.036213, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:53.635632: step 5145, loss 0.0289508, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:53.828981: step 5146, loss 0.0238504, acc 1, learning_rate 0.0001
2017-09-29T13:50:54.026256: step 5147, loss 0.015131, acc 1, learning_rate 0.0001
2017-09-29T13:50:54.226520: step 5148, loss 0.0103938, acc 1, learning_rate 0.0001
2017-09-29T13:50:54.406495: step 5149, loss 0.0101523, acc 1, learning_rate 0.0001
2017-09-29T13:50:54.595914: step 5150, loss 0.0208382, acc 1, learning_rate 0.0001
2017-09-29T13:50:54.790422: step 5151, loss 0.0418216, acc 1, learning_rate 0.0001
2017-09-29T13:50:54.993426: step 5152, loss 0.0912378, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:55.181918: step 5153, loss 0.0146996, acc 1, learning_rate 0.0001
2017-09-29T13:50:55.364534: step 5154, loss 0.0889675, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:55.549496: step 5155, loss 0.0558594, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:55.734341: step 5156, loss 0.0564052, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:55.916876: step 5157, loss 0.0605103, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:56.106671: step 5158, loss 0.00596407, acc 1, learning_rate 0.0001
2017-09-29T13:50:56.294957: step 5159, loss 0.0426294, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:56.487571: step 5160, loss 0.0264264, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:50:57.047539: step 5160, loss 0.216073, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5160

2017-09-29T13:50:57.845063: step 5161, loss 0.0388696, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:58.038812: step 5162, loss 0.0194656, acc 1, learning_rate 0.0001
2017-09-29T13:50:58.228558: step 5163, loss 0.0355979, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:58.422067: step 5164, loss 0.0126202, acc 1, learning_rate 0.0001
2017-09-29T13:50:58.616341: step 5165, loss 0.0422493, acc 0.96875, learning_rate 0.0001
2017-09-29T13:50:58.825196: step 5166, loss 0.0271191, acc 1, learning_rate 0.0001
2017-09-29T13:50:59.009729: step 5167, loss 0.0382832, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:59.207924: step 5168, loss 0.0406108, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:59.421669: step 5169, loss 0.0374271, acc 0.984375, learning_rate 0.0001
2017-09-29T13:50:59.638032: step 5170, loss 0.023275, acc 1, learning_rate 0.0001
2017-09-29T13:50:59.858963: step 5171, loss 0.0449674, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:00.087812: step 5172, loss 0.0222777, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:00.296369: step 5173, loss 0.100904, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:00.483080: step 5174, loss 0.0574772, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:00.671591: step 5175, loss 0.157228, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:00.859488: step 5176, loss 0.0428933, acc 1, learning_rate 0.0001
2017-09-29T13:51:01.054090: step 5177, loss 0.0574353, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:01.242018: step 5178, loss 0.0110274, acc 1, learning_rate 0.0001
2017-09-29T13:51:01.428896: step 5179, loss 0.0409343, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:01.643845: step 5180, loss 0.0973531, acc 0.9375, learning_rate 0.0001
2017-09-29T13:51:01.851962: step 5181, loss 0.0184649, acc 1, learning_rate 0.0001
2017-09-29T13:51:02.052840: step 5182, loss 0.0352277, acc 1, learning_rate 0.0001
2017-09-29T13:51:02.257679: step 5183, loss 0.0126132, acc 1, learning_rate 0.0001
2017-09-29T13:51:02.465458: step 5184, loss 0.0670767, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:02.671255: step 5185, loss 0.0350806, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:02.870044: step 5186, loss 0.0309919, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:03.094077: step 5187, loss 0.00737612, acc 1, learning_rate 0.0001
2017-09-29T13:51:03.325117: step 5188, loss 0.0309608, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:03.574002: step 5189, loss 0.0337608, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:03.809221: step 5190, loss 0.016421, acc 1, learning_rate 0.0001
2017-09-29T13:51:04.035131: step 5191, loss 0.039708, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:04.257105: step 5192, loss 0.0437436, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:04.506481: step 5193, loss 0.0199824, acc 1, learning_rate 0.0001
2017-09-29T13:51:04.684092: step 5194, loss 0.0130359, acc 1, learning_rate 0.0001
2017-09-29T13:51:04.902274: step 5195, loss 0.0279315, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:05.148333: step 5196, loss 0.0259594, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:05.419651: step 5197, loss 0.011689, acc 1, learning_rate 0.0001
2017-09-29T13:51:05.703114: step 5198, loss 0.0333776, acc 1, learning_rate 0.0001
2017-09-29T13:51:05.967956: step 5199, loss 0.00727534, acc 1, learning_rate 0.0001
2017-09-29T13:51:06.206551: step 5200, loss 0.0405931, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:51:06.795172: step 5200, loss 0.217593, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5200

2017-09-29T13:51:07.462889: step 5201, loss 0.0263937, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:07.666779: step 5202, loss 0.0394982, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:07.880703: step 5203, loss 0.0491446, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:08.064107: step 5204, loss 0.0621763, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:08.257158: step 5205, loss 0.0694318, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:08.447374: step 5206, loss 0.012816, acc 1, learning_rate 0.0001
2017-09-29T13:51:08.631842: step 5207, loss 0.0457281, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:08.823387: step 5208, loss 0.0179496, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:09.029668: step 5209, loss 0.0657771, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:09.223270: step 5210, loss 0.063516, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:09.420253: step 5211, loss 0.0250846, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:09.607345: step 5212, loss 0.0418488, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:09.796212: step 5213, loss 0.0090405, acc 1, learning_rate 0.0001
2017-09-29T13:51:09.989853: step 5214, loss 0.0146443, acc 1, learning_rate 0.0001
2017-09-29T13:51:10.208141: step 5215, loss 0.0836399, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:10.426454: step 5216, loss 0.00981501, acc 1, learning_rate 0.0001
2017-09-29T13:51:10.620804: step 5217, loss 0.0228348, acc 1, learning_rate 0.0001
2017-09-29T13:51:10.801500: step 5218, loss 0.0629343, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:10.987909: step 5219, loss 0.0319468, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:11.175448: step 5220, loss 0.0248289, acc 1, learning_rate 0.0001
2017-09-29T13:51:11.357361: step 5221, loss 0.0254257, acc 1, learning_rate 0.0001
2017-09-29T13:51:11.549981: step 5222, loss 0.0352215, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:11.741438: step 5223, loss 0.0525255, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:11.926944: step 5224, loss 0.0438441, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:12.132163: step 5225, loss 0.0861769, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:12.352121: step 5226, loss 0.0216857, acc 1, learning_rate 0.0001
2017-09-29T13:51:12.556583: step 5227, loss 0.025802, acc 1, learning_rate 0.0001
2017-09-29T13:51:12.744381: step 5228, loss 0.0171475, acc 1, learning_rate 0.0001
2017-09-29T13:51:12.928101: step 5229, loss 0.0396107, acc 1, learning_rate 0.0001
2017-09-29T13:51:13.114319: step 5230, loss 0.0578414, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:13.312263: step 5231, loss 0.0338741, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:13.519896: step 5232, loss 0.0865325, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:13.716045: step 5233, loss 0.0156677, acc 1, learning_rate 0.0001
2017-09-29T13:51:13.896824: step 5234, loss 0.0222838, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:14.077708: step 5235, loss 0.0460926, acc 1, learning_rate 0.0001
2017-09-29T13:51:14.277762: step 5236, loss 0.033548, acc 1, learning_rate 0.0001
2017-09-29T13:51:14.508477: step 5237, loss 0.040243, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:14.706544: step 5238, loss 0.0278837, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:14.890073: step 5239, loss 0.0293185, acc 1, learning_rate 0.0001
2017-09-29T13:51:15.070089: step 5240, loss 0.0816055, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T13:51:15.607797: step 5240, loss 0.215677, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5240

2017-09-29T13:51:16.344160: step 5241, loss 0.00842106, acc 1, learning_rate 0.0001
2017-09-29T13:51:16.551087: step 5242, loss 0.0178777, acc 1, learning_rate 0.0001
2017-09-29T13:51:16.747123: step 5243, loss 0.0721758, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:16.939564: step 5244, loss 0.0593075, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:17.133711: step 5245, loss 0.0799427, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:17.358721: step 5246, loss 0.090115, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:17.555437: step 5247, loss 0.0351437, acc 1, learning_rate 0.0001
2017-09-29T13:51:17.763290: step 5248, loss 0.0483424, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:17.984178: step 5249, loss 0.0819397, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:18.190402: step 5250, loss 0.0651793, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:18.425613: step 5251, loss 0.0458593, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:18.614943: step 5252, loss 0.0176651, acc 1, learning_rate 0.0001
2017-09-29T13:51:18.817861: step 5253, loss 0.0304731, acc 1, learning_rate 0.0001
2017-09-29T13:51:19.043959: step 5254, loss 0.0381855, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:19.227994: step 5255, loss 0.0125022, acc 1, learning_rate 0.0001
2017-09-29T13:51:19.410723: step 5256, loss 0.0261712, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:19.596784: step 5257, loss 0.00414561, acc 1, learning_rate 0.0001
2017-09-29T13:51:19.780934: step 5258, loss 0.0257812, acc 1, learning_rate 0.0001
2017-09-29T13:51:19.981713: step 5259, loss 0.0793751, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:20.197619: step 5260, loss 0.0177346, acc 1, learning_rate 0.0001
2017-09-29T13:51:20.411351: step 5261, loss 0.0499533, acc 1, learning_rate 0.0001
2017-09-29T13:51:20.597735: step 5262, loss 0.0110378, acc 1, learning_rate 0.0001
2017-09-29T13:51:20.779732: step 5263, loss 0.0514594, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:20.972649: step 5264, loss 0.0222693, acc 1, learning_rate 0.0001
2017-09-29T13:51:21.163560: step 5265, loss 0.0394732, acc 1, learning_rate 0.0001
2017-09-29T13:51:21.353080: step 5266, loss 0.0208653, acc 1, learning_rate 0.0001
2017-09-29T13:51:21.543229: step 5267, loss 0.023443, acc 1, learning_rate 0.0001
2017-09-29T13:51:21.733034: step 5268, loss 0.0733387, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:21.918051: step 5269, loss 0.0285432, acc 1, learning_rate 0.0001
2017-09-29T13:51:22.105144: step 5270, loss 0.025986, acc 1, learning_rate 0.0001
2017-09-29T13:51:22.293333: step 5271, loss 0.0299767, acc 1, learning_rate 0.0001
2017-09-29T13:51:22.486684: step 5272, loss 0.0875569, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:22.672128: step 5273, loss 0.0411546, acc 1, learning_rate 0.0001
2017-09-29T13:51:22.854229: step 5274, loss 0.0152629, acc 1, learning_rate 0.0001
2017-09-29T13:51:23.040211: step 5275, loss 0.0481036, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:23.238938: step 5276, loss 0.0460225, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:23.441886: step 5277, loss 0.0538202, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:23.644782: step 5278, loss 0.0556266, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:23.836326: step 5279, loss 0.0351267, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:24.040773: step 5280, loss 0.02108, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:51:24.590503: step 5280, loss 0.217735, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5280

2017-09-29T13:51:25.410415: step 5281, loss 0.0388209, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:25.596589: step 5282, loss 0.0159243, acc 1, learning_rate 0.0001
2017-09-29T13:51:25.781617: step 5283, loss 0.0181383, acc 1, learning_rate 0.0001
2017-09-29T13:51:25.962489: step 5284, loss 0.0130669, acc 1, learning_rate 0.0001
2017-09-29T13:51:26.145254: step 5285, loss 0.0479343, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:26.330469: step 5286, loss 0.0728732, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:26.519890: step 5287, loss 0.0146529, acc 1, learning_rate 0.0001
2017-09-29T13:51:26.710596: step 5288, loss 0.0829798, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:26.898315: step 5289, loss 0.039877, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:27.078619: step 5290, loss 0.0421651, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:27.269930: step 5291, loss 0.0382861, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:27.423985: step 5292, loss 0.0314331, acc 1, learning_rate 0.0001
2017-09-29T13:51:27.608009: step 5293, loss 0.0104112, acc 1, learning_rate 0.0001
2017-09-29T13:51:27.789459: step 5294, loss 0.0769888, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:27.973501: step 5295, loss 0.0472025, acc 1, learning_rate 0.0001
2017-09-29T13:51:28.159206: step 5296, loss 0.0158121, acc 1, learning_rate 0.0001
2017-09-29T13:51:28.351038: step 5297, loss 0.0708599, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:28.552570: step 5298, loss 0.0062887, acc 1, learning_rate 0.0001
2017-09-29T13:51:28.734749: step 5299, loss 0.0557444, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:28.923157: step 5300, loss 0.0111274, acc 1, learning_rate 0.0001
2017-09-29T13:51:29.107592: step 5301, loss 0.03961, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:29.294598: step 5302, loss 0.027017, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:29.492259: step 5303, loss 0.0579669, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:29.673970: step 5304, loss 0.00838845, acc 1, learning_rate 0.0001
2017-09-29T13:51:29.867732: step 5305, loss 0.043915, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:30.056476: step 5306, loss 0.0324423, acc 1, learning_rate 0.0001
2017-09-29T13:51:30.245032: step 5307, loss 0.0413683, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:30.429206: step 5308, loss 0.0287403, acc 1, learning_rate 0.0001
2017-09-29T13:51:30.618818: step 5309, loss 0.0388677, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:30.807498: step 5310, loss 0.0137322, acc 1, learning_rate 0.0001
2017-09-29T13:51:30.994227: step 5311, loss 0.00637446, acc 1, learning_rate 0.0001
2017-09-29T13:51:31.180703: step 5312, loss 0.0294404, acc 1, learning_rate 0.0001
2017-09-29T13:51:31.366121: step 5313, loss 0.035899, acc 1, learning_rate 0.0001
2017-09-29T13:51:31.554969: step 5314, loss 0.0245247, acc 1, learning_rate 0.0001
2017-09-29T13:51:31.741325: step 5315, loss 0.0655683, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:31.926336: step 5316, loss 0.0325233, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:32.107916: step 5317, loss 0.0408636, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:32.297291: step 5318, loss 0.022227, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:32.489726: step 5319, loss 0.0329611, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:32.677383: step 5320, loss 0.0639473, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:51:33.210858: step 5320, loss 0.216142, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5320

2017-09-29T13:51:33.872137: step 5321, loss 0.0182379, acc 1, learning_rate 0.0001
2017-09-29T13:51:34.065241: step 5322, loss 0.0120404, acc 1, learning_rate 0.0001
2017-09-29T13:51:34.248350: step 5323, loss 0.0938151, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:34.434009: step 5324, loss 0.126033, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:34.623302: step 5325, loss 0.0855876, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:34.808308: step 5326, loss 0.0316834, acc 1, learning_rate 0.0001
2017-09-29T13:51:34.997830: step 5327, loss 0.0215751, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:35.183070: step 5328, loss 0.0187482, acc 1, learning_rate 0.0001
2017-09-29T13:51:35.366228: step 5329, loss 0.0178569, acc 1, learning_rate 0.0001
2017-09-29T13:51:35.550146: step 5330, loss 0.0457437, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:35.731115: step 5331, loss 0.0291977, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:35.911471: step 5332, loss 0.00604685, acc 1, learning_rate 0.0001
2017-09-29T13:51:36.094393: step 5333, loss 0.0134977, acc 1, learning_rate 0.0001
2017-09-29T13:51:36.278905: step 5334, loss 0.0414514, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:36.463618: step 5335, loss 0.0227319, acc 1, learning_rate 0.0001
2017-09-29T13:51:36.646552: step 5336, loss 0.0114763, acc 1, learning_rate 0.0001
2017-09-29T13:51:36.830378: step 5337, loss 0.0304604, acc 1, learning_rate 0.0001
2017-09-29T13:51:37.016352: step 5338, loss 0.0133303, acc 1, learning_rate 0.0001
2017-09-29T13:51:37.199437: step 5339, loss 0.0352694, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:37.387662: step 5340, loss 0.0501244, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:37.569707: step 5341, loss 0.0319595, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:37.760078: step 5342, loss 0.0595692, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:37.945604: step 5343, loss 0.0230605, acc 1, learning_rate 0.0001
2017-09-29T13:51:38.130858: step 5344, loss 0.0194424, acc 1, learning_rate 0.0001
2017-09-29T13:51:38.319694: step 5345, loss 0.08848, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:38.504989: step 5346, loss 0.130997, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:38.692370: step 5347, loss 0.0526732, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:38.874547: step 5348, loss 0.121555, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:39.059527: step 5349, loss 0.023279, acc 1, learning_rate 0.0001
2017-09-29T13:51:39.241507: step 5350, loss 0.0281169, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:39.426352: step 5351, loss 0.0281851, acc 1, learning_rate 0.0001
2017-09-29T13:51:39.607532: step 5352, loss 0.0145532, acc 1, learning_rate 0.0001
2017-09-29T13:51:39.802145: step 5353, loss 0.0187831, acc 1, learning_rate 0.0001
2017-09-29T13:51:39.981784: step 5354, loss 0.0179082, acc 1, learning_rate 0.0001
2017-09-29T13:51:40.162707: step 5355, loss 0.0276938, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:40.342289: step 5356, loss 0.0141706, acc 1, learning_rate 0.0001
2017-09-29T13:51:40.538256: step 5357, loss 0.0649698, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:40.722029: step 5358, loss 0.0395157, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:40.904349: step 5359, loss 0.0762083, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:41.084073: step 5360, loss 0.0132542, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:51:41.609563: step 5360, loss 0.215484, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5360

2017-09-29T13:51:42.321448: step 5361, loss 0.00854899, acc 1, learning_rate 0.0001
2017-09-29T13:51:42.512308: step 5362, loss 0.0306995, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:42.712454: step 5363, loss 0.0251996, acc 1, learning_rate 0.0001
2017-09-29T13:51:42.900490: step 5364, loss 0.0269448, acc 1, learning_rate 0.0001
2017-09-29T13:51:43.084317: step 5365, loss 0.0282639, acc 1, learning_rate 0.0001
2017-09-29T13:51:43.277369: step 5366, loss 0.0489779, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:43.460527: step 5367, loss 0.00841686, acc 1, learning_rate 0.0001
2017-09-29T13:51:43.652971: step 5368, loss 0.0484217, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:43.851991: step 5369, loss 0.0539403, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:44.041118: step 5370, loss 0.039889, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:44.241628: step 5371, loss 0.0461202, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:44.429641: step 5372, loss 0.0907638, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:44.612432: step 5373, loss 0.0387857, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:44.796711: step 5374, loss 0.0215059, acc 1, learning_rate 0.0001
2017-09-29T13:51:44.981533: step 5375, loss 0.0229444, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:45.165076: step 5376, loss 0.0585729, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:45.344395: step 5377, loss 0.0168764, acc 1, learning_rate 0.0001
2017-09-29T13:51:45.538023: step 5378, loss 0.0381236, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:45.719296: step 5379, loss 0.0469167, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:45.906215: step 5380, loss 0.0242205, acc 1, learning_rate 0.0001
2017-09-29T13:51:46.103857: step 5381, loss 0.0106683, acc 1, learning_rate 0.0001
2017-09-29T13:51:46.297758: step 5382, loss 0.0423879, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:46.492810: step 5383, loss 0.0333128, acc 1, learning_rate 0.0001
2017-09-29T13:51:46.678141: step 5384, loss 0.00721847, acc 1, learning_rate 0.0001
2017-09-29T13:51:46.858592: step 5385, loss 0.023735, acc 1, learning_rate 0.0001
2017-09-29T13:51:47.040079: step 5386, loss 0.071588, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:47.223502: step 5387, loss 0.0135582, acc 1, learning_rate 0.0001
2017-09-29T13:51:47.418561: step 5388, loss 0.0928153, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:47.618851: step 5389, loss 0.0407112, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:47.789344: step 5390, loss 0.0223304, acc 1, learning_rate 0.0001
2017-09-29T13:51:47.992609: step 5391, loss 0.019584, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:48.196254: step 5392, loss 0.0787405, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:48.384733: step 5393, loss 0.0159338, acc 1, learning_rate 0.0001
2017-09-29T13:51:48.570185: step 5394, loss 0.0138285, acc 1, learning_rate 0.0001
2017-09-29T13:51:48.753384: step 5395, loss 0.0243387, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:48.951411: step 5396, loss 0.0802123, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:49.151149: step 5397, loss 0.0263518, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:49.355825: step 5398, loss 0.0344324, acc 1, learning_rate 0.0001
2017-09-29T13:51:49.539849: step 5399, loss 0.0680629, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:49.724526: step 5400, loss 0.0338665, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:51:50.243600: step 5400, loss 0.218905, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5400

2017-09-29T13:51:51.034839: step 5401, loss 0.033078, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:51.219571: step 5402, loss 0.0177358, acc 1, learning_rate 0.0001
2017-09-29T13:51:51.411087: step 5403, loss 0.0182326, acc 1, learning_rate 0.0001
2017-09-29T13:51:51.609980: step 5404, loss 0.0766493, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:51.793229: step 5405, loss 0.0235383, acc 1, learning_rate 0.0001
2017-09-29T13:51:51.977392: step 5406, loss 0.0216549, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:52.161915: step 5407, loss 0.0350769, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:52.341882: step 5408, loss 0.0126052, acc 1, learning_rate 0.0001
2017-09-29T13:51:52.532827: step 5409, loss 0.0388624, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:52.723427: step 5410, loss 0.0790682, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:52.905945: step 5411, loss 0.0122528, acc 1, learning_rate 0.0001
2017-09-29T13:51:53.088608: step 5412, loss 0.00606791, acc 1, learning_rate 0.0001
2017-09-29T13:51:53.275006: step 5413, loss 0.00897242, acc 1, learning_rate 0.0001
2017-09-29T13:51:53.474907: step 5414, loss 0.0167504, acc 1, learning_rate 0.0001
2017-09-29T13:51:53.665829: step 5415, loss 0.0684518, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:53.850018: step 5416, loss 0.039798, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:54.039417: step 5417, loss 0.0541601, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:54.223179: step 5418, loss 0.0305397, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:54.403244: step 5419, loss 0.0288039, acc 1, learning_rate 0.0001
2017-09-29T13:51:54.584846: step 5420, loss 0.0586523, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:54.769997: step 5421, loss 0.01864, acc 1, learning_rate 0.0001
2017-09-29T13:51:54.954062: step 5422, loss 0.0479408, acc 1, learning_rate 0.0001
2017-09-29T13:51:55.133993: step 5423, loss 0.00811076, acc 1, learning_rate 0.0001
2017-09-29T13:51:55.322709: step 5424, loss 0.0504613, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:55.506057: step 5425, loss 0.120299, acc 0.96875, learning_rate 0.0001
2017-09-29T13:51:55.691467: step 5426, loss 0.0271293, acc 1, learning_rate 0.0001
2017-09-29T13:51:55.873107: step 5427, loss 0.0359021, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:56.054009: step 5428, loss 0.0169649, acc 1, learning_rate 0.0001
2017-09-29T13:51:56.233956: step 5429, loss 0.0440033, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:56.419701: step 5430, loss 0.144096, acc 0.953125, learning_rate 0.0001
2017-09-29T13:51:56.600055: step 5431, loss 0.0205495, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:56.786030: step 5432, loss 0.0341438, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:56.968096: step 5433, loss 0.0212757, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:57.149906: step 5434, loss 0.0233835, acc 1, learning_rate 0.0001
2017-09-29T13:51:57.331382: step 5435, loss 0.0477696, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:57.539220: step 5436, loss 0.00770215, acc 1, learning_rate 0.0001
2017-09-29T13:51:57.725656: step 5437, loss 0.0433032, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:57.916549: step 5438, loss 0.0308645, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:58.115090: step 5439, loss 0.0184618, acc 1, learning_rate 0.0001
2017-09-29T13:51:58.303256: step 5440, loss 0.114791, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T13:51:58.828686: step 5440, loss 0.218957, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5440

2017-09-29T13:51:59.458284: step 5441, loss 0.0647637, acc 0.984375, learning_rate 0.0001
2017-09-29T13:51:59.644661: step 5442, loss 0.0224062, acc 1, learning_rate 0.0001
2017-09-29T13:51:59.827971: step 5443, loss 0.0763966, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:00.012297: step 5444, loss 0.0435273, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:00.196291: step 5445, loss 0.02741, acc 1, learning_rate 0.0001
2017-09-29T13:52:00.376824: step 5446, loss 0.0253182, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:00.588897: step 5447, loss 0.0764987, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:00.773502: step 5448, loss 0.0288841, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:00.958457: step 5449, loss 0.00640076, acc 1, learning_rate 0.0001
2017-09-29T13:52:01.167882: step 5450, loss 0.0641414, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:01.365496: step 5451, loss 0.055503, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:01.553283: step 5452, loss 0.066579, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:01.747426: step 5453, loss 0.0470977, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:01.939081: step 5454, loss 0.0687565, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:02.123324: step 5455, loss 0.0255658, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:02.315841: step 5456, loss 0.0451444, acc 1, learning_rate 0.0001
2017-09-29T13:52:02.502698: step 5457, loss 0.0104004, acc 1, learning_rate 0.0001
2017-09-29T13:52:02.691114: step 5458, loss 0.0278686, acc 1, learning_rate 0.0001
2017-09-29T13:52:02.873560: step 5459, loss 0.0767956, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:03.060930: step 5460, loss 0.0212792, acc 1, learning_rate 0.0001
2017-09-29T13:52:03.261540: step 5461, loss 0.0352982, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:03.468712: step 5462, loss 0.016151, acc 1, learning_rate 0.0001
2017-09-29T13:52:03.665729: step 5463, loss 0.022731, acc 1, learning_rate 0.0001
2017-09-29T13:52:03.845741: step 5464, loss 0.042861, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:04.032746: step 5465, loss 0.0302418, acc 1, learning_rate 0.0001
2017-09-29T13:52:04.219632: step 5466, loss 0.0139985, acc 1, learning_rate 0.0001
2017-09-29T13:52:04.404074: step 5467, loss 0.104205, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:04.590565: step 5468, loss 0.0295498, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:04.787243: step 5469, loss 0.0369765, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:04.987337: step 5470, loss 0.0469348, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:05.181792: step 5471, loss 0.0203056, acc 1, learning_rate 0.0001
2017-09-29T13:52:05.384268: step 5472, loss 0.0839897, acc 0.953125, learning_rate 0.0001
2017-09-29T13:52:05.579015: step 5473, loss 0.0158741, acc 1, learning_rate 0.0001
2017-09-29T13:52:05.768099: step 5474, loss 0.0309006, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:05.964211: step 5475, loss 0.0651753, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:06.164727: step 5476, loss 0.0169625, acc 1, learning_rate 0.0001
2017-09-29T13:52:06.357448: step 5477, loss 0.0511451, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:06.551609: step 5478, loss 0.0287755, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:06.750460: step 5479, loss 0.0508488, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:06.944285: step 5480, loss 0.0149492, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:52:07.561026: step 5480, loss 0.21657, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5480

2017-09-29T13:52:08.309018: step 5481, loss 0.0435282, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:08.538187: step 5482, loss 0.0641418, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:08.740863: step 5483, loss 0.00314172, acc 1, learning_rate 0.0001
2017-09-29T13:52:08.941482: step 5484, loss 0.00713881, acc 1, learning_rate 0.0001
2017-09-29T13:52:09.138955: step 5485, loss 0.117516, acc 0.9375, learning_rate 0.0001
2017-09-29T13:52:09.354235: step 5486, loss 0.0459514, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:09.552662: step 5487, loss 0.0104299, acc 1, learning_rate 0.0001
2017-09-29T13:52:09.719110: step 5488, loss 0.0261629, acc 1, learning_rate 0.0001
2017-09-29T13:52:09.932372: step 5489, loss 0.0370856, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:10.129286: step 5490, loss 0.0598419, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:10.309640: step 5491, loss 0.0527679, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:10.495982: step 5492, loss 0.0502709, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:10.681707: step 5493, loss 0.0463972, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:10.865361: step 5494, loss 0.0292328, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:11.047083: step 5495, loss 0.019612, acc 1, learning_rate 0.0001
2017-09-29T13:52:11.228481: step 5496, loss 0.00792092, acc 1, learning_rate 0.0001
2017-09-29T13:52:11.410490: step 5497, loss 0.0100537, acc 1, learning_rate 0.0001
2017-09-29T13:52:11.591966: step 5498, loss 0.0458506, acc 1, learning_rate 0.0001
2017-09-29T13:52:11.779699: step 5499, loss 0.0667891, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:11.962779: step 5500, loss 0.0113243, acc 1, learning_rate 0.0001
2017-09-29T13:52:12.146080: step 5501, loss 0.010069, acc 1, learning_rate 0.0001
2017-09-29T13:52:12.326001: step 5502, loss 0.0840839, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:12.516847: step 5503, loss 0.0124451, acc 1, learning_rate 0.0001
2017-09-29T13:52:12.701555: step 5504, loss 0.0598399, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:12.884237: step 5505, loss 0.0198216, acc 1, learning_rate 0.0001
2017-09-29T13:52:13.073391: step 5506, loss 0.0106656, acc 1, learning_rate 0.0001
2017-09-29T13:52:13.263344: step 5507, loss 0.0257363, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:13.452087: step 5508, loss 0.0588804, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:13.642921: step 5509, loss 0.0756394, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:13.826495: step 5510, loss 0.0840631, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:14.009962: step 5511, loss 0.168899, acc 0.90625, learning_rate 0.0001
2017-09-29T13:52:14.190359: step 5512, loss 0.00698119, acc 1, learning_rate 0.0001
2017-09-29T13:52:14.381844: step 5513, loss 0.0414094, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:14.578533: step 5514, loss 0.0119519, acc 1, learning_rate 0.0001
2017-09-29T13:52:14.760264: step 5515, loss 0.0470766, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:14.946103: step 5516, loss 0.0266373, acc 1, learning_rate 0.0001
2017-09-29T13:52:15.139666: step 5517, loss 0.023319, acc 1, learning_rate 0.0001
2017-09-29T13:52:15.339686: step 5518, loss 0.0460153, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:15.541198: step 5519, loss 0.011136, acc 1, learning_rate 0.0001
2017-09-29T13:52:15.738768: step 5520, loss 0.0405886, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:52:16.349442: step 5520, loss 0.218539, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5520

2017-09-29T13:52:17.072454: step 5521, loss 0.0148428, acc 1, learning_rate 0.0001
2017-09-29T13:52:17.268787: step 5522, loss 0.0742257, acc 0.953125, learning_rate 0.0001
2017-09-29T13:52:17.478321: step 5523, loss 0.0204096, acc 1, learning_rate 0.0001
2017-09-29T13:52:17.693672: step 5524, loss 0.0409068, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:17.905652: step 5525, loss 0.0528158, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:18.119414: step 5526, loss 0.0371578, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:18.317978: step 5527, loss 0.0364432, acc 1, learning_rate 0.0001
2017-09-29T13:52:18.523795: step 5528, loss 0.0567151, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:18.760550: step 5529, loss 0.0609618, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:18.977720: step 5530, loss 0.0102243, acc 1, learning_rate 0.0001
2017-09-29T13:52:19.198525: step 5531, loss 0.0133873, acc 1, learning_rate 0.0001
2017-09-29T13:52:19.402880: step 5532, loss 0.0176824, acc 1, learning_rate 0.0001
2017-09-29T13:52:19.622728: step 5533, loss 0.00984958, acc 1, learning_rate 0.0001
2017-09-29T13:52:19.826628: step 5534, loss 0.0280122, acc 1, learning_rate 0.0001
2017-09-29T13:52:20.032828: step 5535, loss 0.0267714, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:20.233894: step 5536, loss 0.00639571, acc 1, learning_rate 0.0001
2017-09-29T13:52:20.458312: step 5537, loss 0.0106773, acc 1, learning_rate 0.0001
2017-09-29T13:52:20.665480: step 5538, loss 0.0353142, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:20.862358: step 5539, loss 0.0125559, acc 1, learning_rate 0.0001
2017-09-29T13:52:21.060391: step 5540, loss 0.0172975, acc 1, learning_rate 0.0001
2017-09-29T13:52:21.256957: step 5541, loss 0.0312925, acc 1, learning_rate 0.0001
2017-09-29T13:52:21.459376: step 5542, loss 0.0206538, acc 1, learning_rate 0.0001
2017-09-29T13:52:21.666429: step 5543, loss 0.019066, acc 1, learning_rate 0.0001
2017-09-29T13:52:21.865118: step 5544, loss 0.0329504, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:22.065570: step 5545, loss 0.0118357, acc 1, learning_rate 0.0001
2017-09-29T13:52:22.260685: step 5546, loss 0.0447473, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:22.460950: step 5547, loss 0.0620118, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:22.684646: step 5548, loss 0.0126165, acc 1, learning_rate 0.0001
2017-09-29T13:52:22.888183: step 5549, loss 0.0260946, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:23.087627: step 5550, loss 0.0561426, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:23.284285: step 5551, loss 0.023204, acc 1, learning_rate 0.0001
2017-09-29T13:52:23.492970: step 5552, loss 0.021706, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:23.703350: step 5553, loss 0.0536941, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:23.912700: step 5554, loss 0.0362498, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:24.112355: step 5555, loss 0.0310755, acc 1, learning_rate 0.0001
2017-09-29T13:52:24.309662: step 5556, loss 0.0102248, acc 1, learning_rate 0.0001
2017-09-29T13:52:24.509762: step 5557, loss 0.0548119, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:24.708880: step 5558, loss 0.0321927, acc 1, learning_rate 0.0001
2017-09-29T13:52:24.906125: step 5559, loss 0.0697111, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:25.120283: step 5560, loss 0.0187102, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:52:25.823251: step 5560, loss 0.218703, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5560

2017-09-29T13:52:26.662852: step 5561, loss 0.0473317, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:26.863597: step 5562, loss 0.0460939, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:27.072866: step 5563, loss 0.0450447, acc 1, learning_rate 0.0001
2017-09-29T13:52:27.286581: step 5564, loss 0.045037, acc 1, learning_rate 0.0001
2017-09-29T13:52:27.493970: step 5565, loss 0.00710135, acc 1, learning_rate 0.0001
2017-09-29T13:52:27.685440: step 5566, loss 0.0357281, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:27.868473: step 5567, loss 0.0248181, acc 1, learning_rate 0.0001
2017-09-29T13:52:28.052087: step 5568, loss 0.0853173, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:28.234824: step 5569, loss 0.0520699, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:28.420702: step 5570, loss 0.0388383, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:28.604941: step 5571, loss 0.0530201, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:28.805411: step 5572, loss 0.0467078, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:29.001797: step 5573, loss 0.00542984, acc 1, learning_rate 0.0001
2017-09-29T13:52:29.191666: step 5574, loss 0.00747252, acc 1, learning_rate 0.0001
2017-09-29T13:52:29.382548: step 5575, loss 0.139675, acc 0.953125, learning_rate 0.0001
2017-09-29T13:52:29.567273: step 5576, loss 0.0423933, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:29.750237: step 5577, loss 0.0666242, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:29.932289: step 5578, loss 0.0178828, acc 1, learning_rate 0.0001
2017-09-29T13:52:30.113569: step 5579, loss 0.0381708, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:30.307787: step 5580, loss 0.0270549, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:30.514861: step 5581, loss 0.01569, acc 1, learning_rate 0.0001
2017-09-29T13:52:30.715754: step 5582, loss 0.0319745, acc 1, learning_rate 0.0001
2017-09-29T13:52:30.913772: step 5583, loss 0.116446, acc 0.953125, learning_rate 0.0001
2017-09-29T13:52:31.099613: step 5584, loss 0.0858816, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:31.292132: step 5585, loss 0.0239688, acc 1, learning_rate 0.0001
2017-09-29T13:52:31.456174: step 5586, loss 0.0429348, acc 0.980392, learning_rate 0.0001
2017-09-29T13:52:31.655916: step 5587, loss 0.0234303, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:31.850924: step 5588, loss 0.0168136, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:32.040693: step 5589, loss 0.0286659, acc 1, learning_rate 0.0001
2017-09-29T13:52:32.228602: step 5590, loss 0.0366798, acc 1, learning_rate 0.0001
2017-09-29T13:52:32.421305: step 5591, loss 0.0444432, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:32.621045: step 5592, loss 0.0329198, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:32.822580: step 5593, loss 0.0527599, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:33.019190: step 5594, loss 0.0669643, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:33.214333: step 5595, loss 0.0205483, acc 1, learning_rate 0.0001
2017-09-29T13:52:33.423375: step 5596, loss 0.0966347, acc 0.9375, learning_rate 0.0001
2017-09-29T13:52:33.623189: step 5597, loss 0.0648448, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:33.830202: step 5598, loss 0.0275081, acc 1, learning_rate 0.0001
2017-09-29T13:52:34.035948: step 5599, loss 0.017286, acc 1, learning_rate 0.0001
2017-09-29T13:52:34.245770: step 5600, loss 0.0403324, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:52:34.779545: step 5600, loss 0.215612, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5600

2017-09-29T13:52:35.406913: step 5601, loss 0.0408164, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:35.609555: step 5602, loss 0.0258275, acc 1, learning_rate 0.0001
2017-09-29T13:52:35.812681: step 5603, loss 0.0474111, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:36.016727: step 5604, loss 0.0892996, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:36.221315: step 5605, loss 0.0562801, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:36.415807: step 5606, loss 0.04516, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:36.613217: step 5607, loss 0.0181824, acc 1, learning_rate 0.0001
2017-09-29T13:52:36.800896: step 5608, loss 0.0132928, acc 1, learning_rate 0.0001
2017-09-29T13:52:36.982545: step 5609, loss 0.0579864, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:37.164696: step 5610, loss 0.00589031, acc 1, learning_rate 0.0001
2017-09-29T13:52:37.350019: step 5611, loss 0.122333, acc 0.953125, learning_rate 0.0001
2017-09-29T13:52:37.534684: step 5612, loss 0.0301801, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:37.722658: step 5613, loss 0.0471898, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:37.904986: step 5614, loss 0.0328148, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:38.086176: step 5615, loss 0.0164095, acc 1, learning_rate 0.0001
2017-09-29T13:52:38.267490: step 5616, loss 0.00865798, acc 1, learning_rate 0.0001
2017-09-29T13:52:38.452187: step 5617, loss 0.00853947, acc 1, learning_rate 0.0001
2017-09-29T13:52:38.633949: step 5618, loss 0.0215946, acc 1, learning_rate 0.0001
2017-09-29T13:52:38.824088: step 5619, loss 0.00986853, acc 1, learning_rate 0.0001
2017-09-29T13:52:39.007708: step 5620, loss 0.00656005, acc 1, learning_rate 0.0001
2017-09-29T13:52:39.192029: step 5621, loss 0.00540841, acc 1, learning_rate 0.0001
2017-09-29T13:52:39.383604: step 5622, loss 0.0185699, acc 1, learning_rate 0.0001
2017-09-29T13:52:39.567915: step 5623, loss 0.0189383, acc 1, learning_rate 0.0001
2017-09-29T13:52:39.748980: step 5624, loss 0.0129019, acc 1, learning_rate 0.0001
2017-09-29T13:52:39.935010: step 5625, loss 0.00991106, acc 1, learning_rate 0.0001
2017-09-29T13:52:40.114421: step 5626, loss 0.0485637, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:40.298016: step 5627, loss 0.0261545, acc 1, learning_rate 0.0001
2017-09-29T13:52:40.482232: step 5628, loss 0.0796223, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:40.665413: step 5629, loss 0.0775282, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:40.847797: step 5630, loss 0.0576882, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:41.030068: step 5631, loss 0.0501034, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:41.215617: step 5632, loss 0.0453817, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:41.400028: step 5633, loss 0.0655353, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:41.584769: step 5634, loss 0.00829319, acc 1, learning_rate 0.0001
2017-09-29T13:52:41.767049: step 5635, loss 0.0138001, acc 1, learning_rate 0.0001
2017-09-29T13:52:41.948374: step 5636, loss 0.0292679, acc 1, learning_rate 0.0001
2017-09-29T13:52:42.139939: step 5637, loss 0.0204823, acc 1, learning_rate 0.0001
2017-09-29T13:52:42.322380: step 5638, loss 0.014127, acc 1, learning_rate 0.0001
2017-09-29T13:52:42.515234: step 5639, loss 0.0642199, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:42.697603: step 5640, loss 0.039913, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:52:43.237760: step 5640, loss 0.21896, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5640

2017-09-29T13:52:43.943554: step 5641, loss 0.0210293, acc 1, learning_rate 0.0001
2017-09-29T13:52:44.128315: step 5642, loss 0.0425037, acc 1, learning_rate 0.0001
2017-09-29T13:52:44.311468: step 5643, loss 0.0363088, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:44.495712: step 5644, loss 0.0730428, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:44.683516: step 5645, loss 0.153385, acc 0.921875, learning_rate 0.0001
2017-09-29T13:52:44.871294: step 5646, loss 0.0591685, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:45.060312: step 5647, loss 0.0465492, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:45.250973: step 5648, loss 0.0185446, acc 1, learning_rate 0.0001
2017-09-29T13:52:45.442950: step 5649, loss 0.0477991, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:45.634044: step 5650, loss 0.0201232, acc 1, learning_rate 0.0001
2017-09-29T13:52:45.838919: step 5651, loss 0.0565091, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:46.039098: step 5652, loss 0.0512267, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:46.230931: step 5653, loss 0.00527452, acc 1, learning_rate 0.0001
2017-09-29T13:52:46.433308: step 5654, loss 0.00973629, acc 1, learning_rate 0.0001
2017-09-29T13:52:46.624616: step 5655, loss 0.0152402, acc 1, learning_rate 0.0001
2017-09-29T13:52:46.830274: step 5656, loss 0.0141928, acc 1, learning_rate 0.0001
2017-09-29T13:52:47.017611: step 5657, loss 0.00729576, acc 1, learning_rate 0.0001
2017-09-29T13:52:47.224976: step 5658, loss 0.00864758, acc 1, learning_rate 0.0001
2017-09-29T13:52:47.426654: step 5659, loss 0.0320055, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:47.626507: step 5660, loss 0.0326415, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:47.866394: step 5661, loss 0.0379268, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:48.076704: step 5662, loss 0.0229108, acc 1, learning_rate 0.0001
2017-09-29T13:52:48.292896: step 5663, loss 0.0406491, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:48.511053: step 5664, loss 0.0238423, acc 1, learning_rate 0.0001
2017-09-29T13:52:48.745345: step 5665, loss 0.0459147, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:48.940502: step 5666, loss 0.0162219, acc 1, learning_rate 0.0001
2017-09-29T13:52:49.146562: step 5667, loss 0.0245013, acc 1, learning_rate 0.0001
2017-09-29T13:52:49.350559: step 5668, loss 0.0498898, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:49.548186: step 5669, loss 0.0429834, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:49.758572: step 5670, loss 0.102328, acc 0.953125, learning_rate 0.0001
2017-09-29T13:52:49.990258: step 5671, loss 0.0341465, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:50.188450: step 5672, loss 0.149091, acc 0.921875, learning_rate 0.0001
2017-09-29T13:52:50.397314: step 5673, loss 0.0436094, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:50.604101: step 5674, loss 0.00913147, acc 1, learning_rate 0.0001
2017-09-29T13:52:50.820354: step 5675, loss 0.0654451, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:51.035566: step 5676, loss 0.0109622, acc 1, learning_rate 0.0001
2017-09-29T13:52:51.260717: step 5677, loss 0.00695013, acc 1, learning_rate 0.0001
2017-09-29T13:52:51.474822: step 5678, loss 0.013286, acc 1, learning_rate 0.0001
2017-09-29T13:52:51.685667: step 5679, loss 0.0103124, acc 1, learning_rate 0.0001
2017-09-29T13:52:51.896528: step 5680, loss 0.00772076, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:52:52.500347: step 5680, loss 0.214019, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5680

2017-09-29T13:52:53.337168: step 5681, loss 0.0282227, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:53.545203: step 5682, loss 0.0370923, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:53.749184: step 5683, loss 0.024464, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:53.916028: step 5684, loss 0.0567985, acc 0.960784, learning_rate 0.0001
2017-09-29T13:52:54.122471: step 5685, loss 0.110106, acc 0.953125, learning_rate 0.0001
2017-09-29T13:52:54.320405: step 5686, loss 0.0613737, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:54.547202: step 5687, loss 0.0227299, acc 1, learning_rate 0.0001
2017-09-29T13:52:54.756278: step 5688, loss 0.0471379, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:54.957675: step 5689, loss 0.0516173, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:55.170615: step 5690, loss 0.0433556, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:55.369852: step 5691, loss 0.0214148, acc 1, learning_rate 0.0001
2017-09-29T13:52:55.579199: step 5692, loss 0.0112777, acc 1, learning_rate 0.0001
2017-09-29T13:52:55.786942: step 5693, loss 0.0346878, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:55.993305: step 5694, loss 0.0595808, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:56.192078: step 5695, loss 0.103873, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:56.388595: step 5696, loss 0.030542, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:56.592008: step 5697, loss 0.0887799, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:56.794211: step 5698, loss 0.0473127, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:56.999471: step 5699, loss 0.0255154, acc 1, learning_rate 0.0001
2017-09-29T13:52:57.197040: step 5700, loss 0.0200942, acc 1, learning_rate 0.0001
2017-09-29T13:52:57.400057: step 5701, loss 0.0231397, acc 1, learning_rate 0.0001
2017-09-29T13:52:57.631021: step 5702, loss 0.0350393, acc 1, learning_rate 0.0001
2017-09-29T13:52:57.844729: step 5703, loss 0.0974279, acc 0.953125, learning_rate 0.0001
2017-09-29T13:52:58.046361: step 5704, loss 0.0199491, acc 1, learning_rate 0.0001
2017-09-29T13:52:58.260834: step 5705, loss 0.0364854, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:58.469853: step 5706, loss 0.0130292, acc 1, learning_rate 0.0001
2017-09-29T13:52:58.689501: step 5707, loss 0.0511363, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:58.899488: step 5708, loss 0.027523, acc 1, learning_rate 0.0001
2017-09-29T13:52:59.082829: step 5709, loss 0.012352, acc 1, learning_rate 0.0001
2017-09-29T13:52:59.265667: step 5710, loss 0.029706, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:59.453250: step 5711, loss 0.0973739, acc 0.96875, learning_rate 0.0001
2017-09-29T13:52:59.637905: step 5712, loss 0.0227908, acc 0.984375, learning_rate 0.0001
2017-09-29T13:52:59.824012: step 5713, loss 0.0312309, acc 1, learning_rate 0.0001
2017-09-29T13:53:00.006118: step 5714, loss 0.00683678, acc 1, learning_rate 0.0001
2017-09-29T13:53:00.189560: step 5715, loss 0.0778391, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:00.367667: step 5716, loss 0.0236638, acc 1, learning_rate 0.0001
2017-09-29T13:53:00.574145: step 5717, loss 0.0321442, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:00.755870: step 5718, loss 0.0114461, acc 1, learning_rate 0.0001
2017-09-29T13:53:00.938530: step 5719, loss 0.0731308, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:01.122820: step 5720, loss 0.00984431, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:53:01.649663: step 5720, loss 0.214968, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5720

2017-09-29T13:53:02.286876: step 5721, loss 0.0180834, acc 1, learning_rate 0.0001
2017-09-29T13:53:02.469381: step 5722, loss 0.0375424, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:02.655336: step 5723, loss 0.0525465, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:02.845463: step 5724, loss 0.0211104, acc 1, learning_rate 0.0001
2017-09-29T13:53:03.035739: step 5725, loss 0.0226014, acc 1, learning_rate 0.0001
2017-09-29T13:53:03.222414: step 5726, loss 0.0574123, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:03.413512: step 5727, loss 0.0266075, acc 1, learning_rate 0.0001
2017-09-29T13:53:03.601510: step 5728, loss 0.0328629, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:03.800426: step 5729, loss 0.0109944, acc 1, learning_rate 0.0001
2017-09-29T13:53:04.009409: step 5730, loss 0.0248902, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:04.206118: step 5731, loss 0.0188049, acc 1, learning_rate 0.0001
2017-09-29T13:53:04.402797: step 5732, loss 0.0130663, acc 1, learning_rate 0.0001
2017-09-29T13:53:04.591034: step 5733, loss 0.0436927, acc 1, learning_rate 0.0001
2017-09-29T13:53:04.777443: step 5734, loss 0.00785901, acc 1, learning_rate 0.0001
2017-09-29T13:53:04.973641: step 5735, loss 0.0432874, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:05.167672: step 5736, loss 0.0629417, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:05.369918: step 5737, loss 0.039139, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:05.583486: step 5738, loss 0.0187634, acc 1, learning_rate 0.0001
2017-09-29T13:53:05.775832: step 5739, loss 0.0198807, acc 1, learning_rate 0.0001
2017-09-29T13:53:05.962193: step 5740, loss 0.0210251, acc 1, learning_rate 0.0001
2017-09-29T13:53:06.157138: step 5741, loss 0.0304943, acc 1, learning_rate 0.0001
2017-09-29T13:53:06.348895: step 5742, loss 0.0517899, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:06.541106: step 5743, loss 0.0836222, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:06.732247: step 5744, loss 0.0137341, acc 1, learning_rate 0.0001
2017-09-29T13:53:06.923617: step 5745, loss 0.0503395, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:07.108644: step 5746, loss 0.0373916, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:07.292771: step 5747, loss 0.0207371, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:07.485934: step 5748, loss 0.0271555, acc 1, learning_rate 0.0001
2017-09-29T13:53:07.680585: step 5749, loss 0.035465, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:07.871983: step 5750, loss 0.0427342, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:08.086442: step 5751, loss 0.0196727, acc 1, learning_rate 0.0001
2017-09-29T13:53:08.273617: step 5752, loss 0.0778085, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:08.459122: step 5753, loss 0.0459483, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:08.641704: step 5754, loss 0.0226205, acc 1, learning_rate 0.0001
2017-09-29T13:53:08.827978: step 5755, loss 0.040998, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:09.025773: step 5756, loss 0.0145273, acc 1, learning_rate 0.0001
2017-09-29T13:53:09.208379: step 5757, loss 0.0501592, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:09.391239: step 5758, loss 0.0214486, acc 1, learning_rate 0.0001
2017-09-29T13:53:09.579228: step 5759, loss 0.0452791, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:09.761590: step 5760, loss 0.00977913, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:53:10.306999: step 5760, loss 0.214947, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5760

2017-09-29T13:53:11.035921: step 5761, loss 0.0986147, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:11.224788: step 5762, loss 0.0519309, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:11.430977: step 5763, loss 0.126155, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:11.620461: step 5764, loss 0.0109031, acc 1, learning_rate 0.0001
2017-09-29T13:53:11.820639: step 5765, loss 0.0264888, acc 1, learning_rate 0.0001
2017-09-29T13:53:12.021244: step 5766, loss 0.0188509, acc 1, learning_rate 0.0001
2017-09-29T13:53:12.214311: step 5767, loss 0.0114339, acc 1, learning_rate 0.0001
2017-09-29T13:53:12.417086: step 5768, loss 0.0143025, acc 1, learning_rate 0.0001
2017-09-29T13:53:12.599040: step 5769, loss 0.015878, acc 1, learning_rate 0.0001
2017-09-29T13:53:12.779945: step 5770, loss 0.0209121, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:12.965918: step 5771, loss 0.0282027, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:13.148607: step 5772, loss 0.0357565, acc 1, learning_rate 0.0001
2017-09-29T13:53:13.331788: step 5773, loss 0.0828448, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:13.516498: step 5774, loss 0.0494485, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:13.707883: step 5775, loss 0.0363351, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:13.895907: step 5776, loss 0.0162805, acc 1, learning_rate 0.0001
2017-09-29T13:53:14.086122: step 5777, loss 0.0289266, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:14.268660: step 5778, loss 0.0251717, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:14.452265: step 5779, loss 0.00814561, acc 1, learning_rate 0.0001
2017-09-29T13:53:14.633084: step 5780, loss 0.00804062, acc 1, learning_rate 0.0001
2017-09-29T13:53:14.827389: step 5781, loss 0.073837, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:14.981489: step 5782, loss 0.00652601, acc 1, learning_rate 0.0001
2017-09-29T13:53:15.168584: step 5783, loss 0.0126356, acc 1, learning_rate 0.0001
2017-09-29T13:53:15.352083: step 5784, loss 0.0606668, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:15.555998: step 5785, loss 0.0144374, acc 1, learning_rate 0.0001
2017-09-29T13:53:15.762073: step 5786, loss 0.0366285, acc 1, learning_rate 0.0001
2017-09-29T13:53:15.956664: step 5787, loss 0.0114835, acc 1, learning_rate 0.0001
2017-09-29T13:53:16.140588: step 5788, loss 0.0378369, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:16.322998: step 5789, loss 0.011935, acc 1, learning_rate 0.0001
2017-09-29T13:53:16.503692: step 5790, loss 0.0139087, acc 1, learning_rate 0.0001
2017-09-29T13:53:16.686347: step 5791, loss 0.032806, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:16.874301: step 5792, loss 0.0098487, acc 1, learning_rate 0.0001
2017-09-29T13:53:17.055351: step 5793, loss 0.0621643, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:17.235864: step 5794, loss 0.0265649, acc 1, learning_rate 0.0001
2017-09-29T13:53:17.430650: step 5795, loss 0.00447149, acc 1, learning_rate 0.0001
2017-09-29T13:53:17.619007: step 5796, loss 0.0177255, acc 1, learning_rate 0.0001
2017-09-29T13:53:17.807842: step 5797, loss 0.113519, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:17.995957: step 5798, loss 0.0429282, acc 1, learning_rate 0.0001
2017-09-29T13:53:18.201330: step 5799, loss 0.0112809, acc 1, learning_rate 0.0001
2017-09-29T13:53:18.389924: step 5800, loss 0.0482208, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:53:18.963185: step 5800, loss 0.218459, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5800

2017-09-29T13:53:19.760665: step 5801, loss 0.0233735, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:19.956760: step 5802, loss 0.0253919, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:20.135909: step 5803, loss 0.069253, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:20.316956: step 5804, loss 0.0183368, acc 1, learning_rate 0.0001
2017-09-29T13:53:20.512961: step 5805, loss 0.0234222, acc 1, learning_rate 0.0001
2017-09-29T13:53:20.700951: step 5806, loss 0.00845644, acc 1, learning_rate 0.0001
2017-09-29T13:53:20.881105: step 5807, loss 0.0407371, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:21.066363: step 5808, loss 0.0205272, acc 1, learning_rate 0.0001
2017-09-29T13:53:21.253293: step 5809, loss 0.00954803, acc 1, learning_rate 0.0001
2017-09-29T13:53:21.435020: step 5810, loss 0.0272239, acc 1, learning_rate 0.0001
2017-09-29T13:53:21.619189: step 5811, loss 0.0155904, acc 1, learning_rate 0.0001
2017-09-29T13:53:21.809041: step 5812, loss 0.0304263, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:21.990532: step 5813, loss 0.0216203, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:22.169544: step 5814, loss 0.0157792, acc 1, learning_rate 0.0001
2017-09-29T13:53:22.354364: step 5815, loss 0.0570312, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:22.540294: step 5816, loss 0.0773348, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:22.724619: step 5817, loss 0.0375392, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:22.906625: step 5818, loss 0.0702473, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:23.095788: step 5819, loss 0.0646329, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:23.283188: step 5820, loss 0.045716, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:23.485916: step 5821, loss 0.0389292, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:23.668079: step 5822, loss 0.0371687, acc 1, learning_rate 0.0001
2017-09-29T13:53:23.848586: step 5823, loss 0.0339071, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:24.044891: step 5824, loss 0.0390126, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:24.226514: step 5825, loss 0.0223683, acc 1, learning_rate 0.0001
2017-09-29T13:53:24.414897: step 5826, loss 0.0606108, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:24.600054: step 5827, loss 0.0264123, acc 1, learning_rate 0.0001
2017-09-29T13:53:24.786373: step 5828, loss 0.0354387, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:24.967918: step 5829, loss 0.0209864, acc 1, learning_rate 0.0001
2017-09-29T13:53:25.156421: step 5830, loss 0.008912, acc 1, learning_rate 0.0001
2017-09-29T13:53:25.342740: step 5831, loss 0.0356247, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:25.525769: step 5832, loss 0.0604301, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:25.709105: step 5833, loss 0.0402944, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:25.892098: step 5834, loss 0.0705537, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:26.073887: step 5835, loss 0.0250112, acc 1, learning_rate 0.0001
2017-09-29T13:53:26.262889: step 5836, loss 0.0399782, acc 1, learning_rate 0.0001
2017-09-29T13:53:26.446600: step 5837, loss 0.017814, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:26.632471: step 5838, loss 0.105674, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:26.811701: step 5839, loss 0.0310114, acc 1, learning_rate 0.0001
2017-09-29T13:53:26.996895: step 5840, loss 0.0807642, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:53:27.539637: step 5840, loss 0.217884, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5840

2017-09-29T13:53:28.185498: step 5841, loss 0.0278568, acc 1, learning_rate 0.0001
2017-09-29T13:53:28.388997: step 5842, loss 0.0507834, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:28.602435: step 5843, loss 0.00645101, acc 1, learning_rate 0.0001
2017-09-29T13:53:28.805347: step 5844, loss 0.050177, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:29.010031: step 5845, loss 0.0443125, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:29.200794: step 5846, loss 0.0302855, acc 1, learning_rate 0.0001
2017-09-29T13:53:29.390510: step 5847, loss 0.0933721, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:29.573007: step 5848, loss 0.0562202, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:29.754696: step 5849, loss 0.0824407, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:29.940230: step 5850, loss 0.0354816, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:30.124026: step 5851, loss 0.0251052, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:30.306239: step 5852, loss 0.0394932, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:30.492364: step 5853, loss 0.0104589, acc 1, learning_rate 0.0001
2017-09-29T13:53:30.684035: step 5854, loss 0.0945378, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:30.868219: step 5855, loss 0.0170961, acc 1, learning_rate 0.0001
2017-09-29T13:53:31.050108: step 5856, loss 0.0706603, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:31.237450: step 5857, loss 0.00890147, acc 1, learning_rate 0.0001
2017-09-29T13:53:31.423983: step 5858, loss 0.0138604, acc 1, learning_rate 0.0001
2017-09-29T13:53:31.616716: step 5859, loss 0.052466, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:31.799823: step 5860, loss 0.0552902, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:31.995327: step 5861, loss 0.0639269, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:32.186187: step 5862, loss 0.0178596, acc 1, learning_rate 0.0001
2017-09-29T13:53:32.369346: step 5863, loss 0.0472772, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:32.559914: step 5864, loss 0.0388663, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:32.746409: step 5865, loss 0.025799, acc 1, learning_rate 0.0001
2017-09-29T13:53:32.932318: step 5866, loss 0.0130327, acc 1, learning_rate 0.0001
2017-09-29T13:53:33.117742: step 5867, loss 0.0166293, acc 1, learning_rate 0.0001
2017-09-29T13:53:33.303846: step 5868, loss 0.00756865, acc 1, learning_rate 0.0001
2017-09-29T13:53:33.501155: step 5869, loss 0.0404953, acc 1, learning_rate 0.0001
2017-09-29T13:53:33.700915: step 5870, loss 0.0464223, acc 1, learning_rate 0.0001
2017-09-29T13:53:33.880534: step 5871, loss 0.046734, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:34.067813: step 5872, loss 0.0415053, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:34.258800: step 5873, loss 0.0328896, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:34.448235: step 5874, loss 0.00798211, acc 1, learning_rate 0.0001
2017-09-29T13:53:34.633327: step 5875, loss 0.00573827, acc 1, learning_rate 0.0001
2017-09-29T13:53:34.815456: step 5876, loss 0.0180074, acc 1, learning_rate 0.0001
2017-09-29T13:53:34.999138: step 5877, loss 0.00863078, acc 1, learning_rate 0.0001
2017-09-29T13:53:35.191574: step 5878, loss 0.0490842, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:35.387175: step 5879, loss 0.0712653, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:35.547153: step 5880, loss 0.0424266, acc 0.980392, learning_rate 0.0001

Evaluation:
2017-09-29T13:53:36.102673: step 5880, loss 0.218303, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5880

2017-09-29T13:53:36.794976: step 5881, loss 0.0519834, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:36.981369: step 5882, loss 0.0712729, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:37.161690: step 5883, loss 0.0328548, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:37.349678: step 5884, loss 0.0200097, acc 1, learning_rate 0.0001
2017-09-29T13:53:37.533604: step 5885, loss 0.0206967, acc 1, learning_rate 0.0001
2017-09-29T13:53:37.716570: step 5886, loss 0.0125682, acc 1, learning_rate 0.0001
2017-09-29T13:53:37.900706: step 5887, loss 0.0191206, acc 1, learning_rate 0.0001
2017-09-29T13:53:38.084200: step 5888, loss 0.0715354, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:38.268184: step 5889, loss 0.034868, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:38.453315: step 5890, loss 0.0950852, acc 0.9375, learning_rate 0.0001
2017-09-29T13:53:38.638330: step 5891, loss 0.0128685, acc 1, learning_rate 0.0001
2017-09-29T13:53:38.823069: step 5892, loss 0.019133, acc 1, learning_rate 0.0001
2017-09-29T13:53:39.005938: step 5893, loss 0.0155739, acc 1, learning_rate 0.0001
2017-09-29T13:53:39.194727: step 5894, loss 0.00817494, acc 1, learning_rate 0.0001
2017-09-29T13:53:39.376405: step 5895, loss 0.0253531, acc 1, learning_rate 0.0001
2017-09-29T13:53:39.579164: step 5896, loss 0.0459763, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:39.762905: step 5897, loss 0.0646143, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:39.945967: step 5898, loss 0.0194287, acc 1, learning_rate 0.0001
2017-09-29T13:53:40.134199: step 5899, loss 0.00787478, acc 1, learning_rate 0.0001
2017-09-29T13:53:40.316088: step 5900, loss 0.0451605, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:40.501195: step 5901, loss 0.0279665, acc 1, learning_rate 0.0001
2017-09-29T13:53:40.682815: step 5902, loss 0.0277364, acc 1, learning_rate 0.0001
2017-09-29T13:53:40.865816: step 5903, loss 0.0804221, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:41.058725: step 5904, loss 0.00942136, acc 1, learning_rate 0.0001
2017-09-29T13:53:41.244576: step 5905, loss 0.0408442, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:41.428770: step 5906, loss 0.045379, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:41.614637: step 5907, loss 0.00899966, acc 1, learning_rate 0.0001
2017-09-29T13:53:41.797644: step 5908, loss 0.0327957, acc 1, learning_rate 0.0001
2017-09-29T13:53:41.983367: step 5909, loss 0.0195165, acc 1, learning_rate 0.0001
2017-09-29T13:53:42.167240: step 5910, loss 0.0630213, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:42.365221: step 5911, loss 0.0137231, acc 1, learning_rate 0.0001
2017-09-29T13:53:42.554085: step 5912, loss 0.0283391, acc 1, learning_rate 0.0001
2017-09-29T13:53:42.738210: step 5913, loss 0.022513, acc 1, learning_rate 0.0001
2017-09-29T13:53:42.931169: step 5914, loss 0.03061, acc 1, learning_rate 0.0001
2017-09-29T13:53:43.118661: step 5915, loss 0.0233167, acc 1, learning_rate 0.0001
2017-09-29T13:53:43.302567: step 5916, loss 0.0586167, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:43.491058: step 5917, loss 0.0210761, acc 1, learning_rate 0.0001
2017-09-29T13:53:43.673958: step 5918, loss 0.00650014, acc 1, learning_rate 0.0001
2017-09-29T13:53:43.852705: step 5919, loss 0.0406097, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:44.036321: step 5920, loss 0.0732153, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:53:44.582029: step 5920, loss 0.21642, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5920

2017-09-29T13:53:45.377138: step 5921, loss 0.0105896, acc 1, learning_rate 0.0001
2017-09-29T13:53:45.577096: step 5922, loss 0.0341097, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:45.760452: step 5923, loss 0.0502653, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:45.943474: step 5924, loss 0.0553797, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:46.146133: step 5925, loss 0.0415513, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:46.335606: step 5926, loss 0.0123214, acc 1, learning_rate 0.0001
2017-09-29T13:53:46.529715: step 5927, loss 0.0239559, acc 1, learning_rate 0.0001
2017-09-29T13:53:46.709129: step 5928, loss 0.0403788, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:46.893714: step 5929, loss 0.0514937, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:47.076089: step 5930, loss 0.0237836, acc 1, learning_rate 0.0001
2017-09-29T13:53:47.259180: step 5931, loss 0.0188013, acc 1, learning_rate 0.0001
2017-09-29T13:53:47.444037: step 5932, loss 0.0156842, acc 1, learning_rate 0.0001
2017-09-29T13:53:47.644316: step 5933, loss 0.0788232, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:47.830120: step 5934, loss 0.00951559, acc 1, learning_rate 0.0001
2017-09-29T13:53:48.018370: step 5935, loss 0.00657211, acc 1, learning_rate 0.0001
2017-09-29T13:53:48.218673: step 5936, loss 0.0418209, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:48.425814: step 5937, loss 0.0129719, acc 1, learning_rate 0.0001
2017-09-29T13:53:48.612495: step 5938, loss 0.0428809, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:48.795778: step 5939, loss 0.0377491, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:48.979429: step 5940, loss 0.014534, acc 1, learning_rate 0.0001
2017-09-29T13:53:49.169781: step 5941, loss 0.0158905, acc 1, learning_rate 0.0001
2017-09-29T13:53:49.367555: step 5942, loss 0.0122024, acc 1, learning_rate 0.0001
2017-09-29T13:53:49.568385: step 5943, loss 0.0103819, acc 1, learning_rate 0.0001
2017-09-29T13:53:49.770019: step 5944, loss 0.03182, acc 1, learning_rate 0.0001
2017-09-29T13:53:49.976970: step 5945, loss 0.00415288, acc 1, learning_rate 0.0001
2017-09-29T13:53:50.181424: step 5946, loss 0.0162706, acc 1, learning_rate 0.0001
2017-09-29T13:53:50.383239: step 5947, loss 0.0596881, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:50.574144: step 5948, loss 0.0372595, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:50.765027: step 5949, loss 0.0280487, acc 1, learning_rate 0.0001
2017-09-29T13:53:50.950679: step 5950, loss 0.0236817, acc 1, learning_rate 0.0001
2017-09-29T13:53:51.135241: step 5951, loss 0.0264499, acc 1, learning_rate 0.0001
2017-09-29T13:53:51.317323: step 5952, loss 0.0484751, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:51.516235: step 5953, loss 0.0127361, acc 1, learning_rate 0.0001
2017-09-29T13:53:51.699471: step 5954, loss 0.0215163, acc 1, learning_rate 0.0001
2017-09-29T13:53:51.887828: step 5955, loss 0.0120826, acc 1, learning_rate 0.0001
2017-09-29T13:53:52.071772: step 5956, loss 0.039092, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:52.255651: step 5957, loss 0.0152821, acc 1, learning_rate 0.0001
2017-09-29T13:53:52.445318: step 5958, loss 0.0890942, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:52.629567: step 5959, loss 0.0269065, acc 1, learning_rate 0.0001
2017-09-29T13:53:52.816283: step 5960, loss 0.0657433, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:53:53.344166: step 5960, loss 0.217134, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-5960

2017-09-29T13:53:53.972068: step 5961, loss 0.0202428, acc 1, learning_rate 0.0001
2017-09-29T13:53:54.160893: step 5962, loss 0.0854793, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:54.351395: step 5963, loss 0.0521323, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:54.540874: step 5964, loss 0.0181654, acc 1, learning_rate 0.0001
2017-09-29T13:53:54.727746: step 5965, loss 0.0437455, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:54.929392: step 5966, loss 0.0192071, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:55.114374: step 5967, loss 0.0529544, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:55.299172: step 5968, loss 0.00801134, acc 1, learning_rate 0.0001
2017-09-29T13:53:55.483671: step 5969, loss 0.01875, acc 1, learning_rate 0.0001
2017-09-29T13:53:55.664689: step 5970, loss 0.0489153, acc 1, learning_rate 0.0001
2017-09-29T13:53:55.850713: step 5971, loss 0.0196889, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:56.041496: step 5972, loss 0.023034, acc 1, learning_rate 0.0001
2017-09-29T13:53:56.226581: step 5973, loss 0.0104108, acc 1, learning_rate 0.0001
2017-09-29T13:53:56.409601: step 5974, loss 0.00452278, acc 1, learning_rate 0.0001
2017-09-29T13:53:56.597495: step 5975, loss 0.0169309, acc 1, learning_rate 0.0001
2017-09-29T13:53:56.781751: step 5976, loss 0.0935545, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:56.967772: step 5977, loss 0.0413802, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:57.121128: step 5978, loss 0.0365028, acc 0.980392, learning_rate 0.0001
2017-09-29T13:53:57.304115: step 5979, loss 0.0425964, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:57.501744: step 5980, loss 0.121288, acc 0.9375, learning_rate 0.0001
2017-09-29T13:53:57.685171: step 5981, loss 0.007094, acc 1, learning_rate 0.0001
2017-09-29T13:53:57.877493: step 5982, loss 0.0517561, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:58.059025: step 5983, loss 0.0259078, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:58.245532: step 5984, loss 0.0136561, acc 1, learning_rate 0.0001
2017-09-29T13:53:58.429843: step 5985, loss 0.055448, acc 0.953125, learning_rate 0.0001
2017-09-29T13:53:58.613403: step 5986, loss 0.0088437, acc 1, learning_rate 0.0001
2017-09-29T13:53:58.795858: step 5987, loss 0.0138633, acc 1, learning_rate 0.0001
2017-09-29T13:53:58.980862: step 5988, loss 0.0324897, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:59.171382: step 5989, loss 0.0205713, acc 1, learning_rate 0.0001
2017-09-29T13:53:59.357401: step 5990, loss 0.0166262, acc 1, learning_rate 0.0001
2017-09-29T13:53:59.539336: step 5991, loss 0.0647988, acc 0.96875, learning_rate 0.0001
2017-09-29T13:53:59.722448: step 5992, loss 0.0619176, acc 0.984375, learning_rate 0.0001
2017-09-29T13:53:59.907977: step 5993, loss 0.0249128, acc 1, learning_rate 0.0001
2017-09-29T13:54:00.093577: step 5994, loss 0.00890482, acc 1, learning_rate 0.0001
2017-09-29T13:54:00.275021: step 5995, loss 0.0932395, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:00.460451: step 5996, loss 0.0136223, acc 1, learning_rate 0.0001
2017-09-29T13:54:00.640003: step 5997, loss 0.0203492, acc 1, learning_rate 0.0001
2017-09-29T13:54:00.832318: step 5998, loss 0.0448608, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:01.016016: step 5999, loss 0.0982824, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:01.200024: step 6000, loss 0.0359499, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:54:01.720255: step 6000, loss 0.217307, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6000

2017-09-29T13:54:02.415271: step 6001, loss 0.0246154, acc 1, learning_rate 0.0001
2017-09-29T13:54:02.602946: step 6002, loss 0.00525665, acc 1, learning_rate 0.0001
2017-09-29T13:54:02.785100: step 6003, loss 0.0145098, acc 1, learning_rate 0.0001
2017-09-29T13:54:02.975012: step 6004, loss 0.0576647, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:03.163200: step 6005, loss 0.0167694, acc 1, learning_rate 0.0001
2017-09-29T13:54:03.354992: step 6006, loss 0.0459853, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:03.550123: step 6007, loss 0.0392458, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:03.732264: step 6008, loss 0.0283026, acc 1, learning_rate 0.0001
2017-09-29T13:54:03.913393: step 6009, loss 0.0256421, acc 1, learning_rate 0.0001
2017-09-29T13:54:04.099288: step 6010, loss 0.0215937, acc 1, learning_rate 0.0001
2017-09-29T13:54:04.286410: step 6011, loss 0.0363161, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:04.474262: step 6012, loss 0.0146682, acc 1, learning_rate 0.0001
2017-09-29T13:54:04.659696: step 6013, loss 0.06315, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:04.839814: step 6014, loss 0.069635, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:05.030338: step 6015, loss 0.0162135, acc 1, learning_rate 0.0001
2017-09-29T13:54:05.217996: step 6016, loss 0.0626349, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:05.401769: step 6017, loss 0.00980139, acc 1, learning_rate 0.0001
2017-09-29T13:54:05.594061: step 6018, loss 0.0215149, acc 1, learning_rate 0.0001
2017-09-29T13:54:05.775504: step 6019, loss 0.00961814, acc 1, learning_rate 0.0001
2017-09-29T13:54:05.955988: step 6020, loss 0.0474665, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:06.138311: step 6021, loss 0.0419765, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:06.329730: step 6022, loss 0.00582171, acc 1, learning_rate 0.0001
2017-09-29T13:54:06.513205: step 6023, loss 0.0167687, acc 1, learning_rate 0.0001
2017-09-29T13:54:06.697004: step 6024, loss 0.0348677, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:06.883871: step 6025, loss 0.0116492, acc 1, learning_rate 0.0001
2017-09-29T13:54:07.073061: step 6026, loss 0.0277781, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:07.255474: step 6027, loss 0.0148238, acc 1, learning_rate 0.0001
2017-09-29T13:54:07.447950: step 6028, loss 0.122623, acc 0.9375, learning_rate 0.0001
2017-09-29T13:54:07.634925: step 6029, loss 0.0454621, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:07.820882: step 6030, loss 0.0592062, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:08.004909: step 6031, loss 0.0327054, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:08.188619: step 6032, loss 0.0272531, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:08.374936: step 6033, loss 0.0052818, acc 1, learning_rate 0.0001
2017-09-29T13:54:08.554650: step 6034, loss 0.0286525, acc 1, learning_rate 0.0001
2017-09-29T13:54:08.736088: step 6035, loss 0.0380484, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:08.923004: step 6036, loss 0.0515251, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:09.115478: step 6037, loss 0.0065837, acc 1, learning_rate 0.0001
2017-09-29T13:54:09.305904: step 6038, loss 0.0171785, acc 1, learning_rate 0.0001
2017-09-29T13:54:09.505848: step 6039, loss 0.0248738, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:09.702334: step 6040, loss 0.00623932, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:54:10.221274: step 6040, loss 0.22, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6040

2017-09-29T13:54:11.009322: step 6041, loss 0.0257393, acc 1, learning_rate 0.0001
2017-09-29T13:54:11.200773: step 6042, loss 0.0168839, acc 1, learning_rate 0.0001
2017-09-29T13:54:11.385192: step 6043, loss 0.0186834, acc 1, learning_rate 0.0001
2017-09-29T13:54:11.568279: step 6044, loss 0.0121, acc 1, learning_rate 0.0001
2017-09-29T13:54:11.753419: step 6045, loss 0.0153726, acc 1, learning_rate 0.0001
2017-09-29T13:54:11.955842: step 6046, loss 0.106214, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:12.140029: step 6047, loss 0.0383015, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:12.322494: step 6048, loss 0.0173363, acc 1, learning_rate 0.0001
2017-09-29T13:54:12.505720: step 6049, loss 0.0200468, acc 1, learning_rate 0.0001
2017-09-29T13:54:12.690403: step 6050, loss 0.0504559, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:12.891207: step 6051, loss 0.0166656, acc 1, learning_rate 0.0001
2017-09-29T13:54:13.092947: step 6052, loss 0.00824007, acc 1, learning_rate 0.0001
2017-09-29T13:54:13.291585: step 6053, loss 0.0398254, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:13.478240: step 6054, loss 0.0274268, acc 1, learning_rate 0.0001
2017-09-29T13:54:13.658720: step 6055, loss 0.0518914, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:13.862335: step 6056, loss 0.0587705, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:14.052481: step 6057, loss 0.0315176, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:14.235251: step 6058, loss 0.0651727, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:14.425011: step 6059, loss 0.0242982, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:14.617671: step 6060, loss 0.00943685, acc 1, learning_rate 0.0001
2017-09-29T13:54:14.820827: step 6061, loss 0.011733, acc 1, learning_rate 0.0001
2017-09-29T13:54:15.026736: step 6062, loss 0.00582079, acc 1, learning_rate 0.0001
2017-09-29T13:54:15.229839: step 6063, loss 0.04001, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:15.441478: step 6064, loss 0.0587398, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:15.644820: step 6065, loss 0.0268585, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:15.848150: step 6066, loss 0.0712734, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:16.051203: step 6067, loss 0.0251921, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:16.258076: step 6068, loss 0.044889, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:16.466221: step 6069, loss 0.0232576, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:16.671732: step 6070, loss 0.00862369, acc 1, learning_rate 0.0001
2017-09-29T13:54:16.875100: step 6071, loss 0.0804027, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:17.077193: step 6072, loss 0.025261, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:17.271039: step 6073, loss 0.0425091, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:17.456121: step 6074, loss 0.0637697, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:17.639635: step 6075, loss 0.00771794, acc 1, learning_rate 0.0001
2017-09-29T13:54:17.799268: step 6076, loss 0.0131897, acc 1, learning_rate 0.0001
2017-09-29T13:54:17.982528: step 6077, loss 0.0133689, acc 1, learning_rate 0.0001
2017-09-29T13:54:18.168663: step 6078, loss 0.0353524, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:18.356196: step 6079, loss 0.0446135, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:18.540602: step 6080, loss 0.0206055, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:54:19.059440: step 6080, loss 0.220666, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6080

2017-09-29T13:54:19.692993: step 6081, loss 0.0671615, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:19.875531: step 6082, loss 0.0935182, acc 0.953125, learning_rate 0.0001
2017-09-29T13:54:20.068816: step 6083, loss 0.0113334, acc 1, learning_rate 0.0001
2017-09-29T13:54:20.260973: step 6084, loss 0.0155233, acc 1, learning_rate 0.0001
2017-09-29T13:54:20.448181: step 6085, loss 0.041021, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:20.635692: step 6086, loss 0.0128375, acc 1, learning_rate 0.0001
2017-09-29T13:54:20.819895: step 6087, loss 0.0212808, acc 1, learning_rate 0.0001
2017-09-29T13:54:21.006820: step 6088, loss 0.0243891, acc 1, learning_rate 0.0001
2017-09-29T13:54:21.188005: step 6089, loss 0.019606, acc 1, learning_rate 0.0001
2017-09-29T13:54:21.373469: step 6090, loss 0.0350598, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:21.571822: step 6091, loss 0.0482046, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:21.754083: step 6092, loss 0.0640214, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:21.937784: step 6093, loss 0.0103856, acc 1, learning_rate 0.0001
2017-09-29T13:54:22.119507: step 6094, loss 0.0442325, acc 0.953125, learning_rate 0.0001
2017-09-29T13:54:22.302468: step 6095, loss 0.027483, acc 1, learning_rate 0.0001
2017-09-29T13:54:22.494936: step 6096, loss 0.0327842, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:22.678625: step 6097, loss 0.0953027, acc 0.953125, learning_rate 0.0001
2017-09-29T13:54:22.860638: step 6098, loss 0.0456808, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:23.045844: step 6099, loss 0.0288117, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:23.224701: step 6100, loss 0.0150968, acc 1, learning_rate 0.0001
2017-09-29T13:54:23.414801: step 6101, loss 0.0745976, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:23.607527: step 6102, loss 0.0227037, acc 1, learning_rate 0.0001
2017-09-29T13:54:23.789767: step 6103, loss 0.0480804, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:23.973479: step 6104, loss 0.00866703, acc 1, learning_rate 0.0001
2017-09-29T13:54:24.166212: step 6105, loss 0.0421663, acc 1, learning_rate 0.0001
2017-09-29T13:54:24.363303: step 6106, loss 0.0335333, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:24.545988: step 6107, loss 0.0309871, acc 1, learning_rate 0.0001
2017-09-29T13:54:24.728525: step 6108, loss 0.0107303, acc 1, learning_rate 0.0001
2017-09-29T13:54:24.920233: step 6109, loss 0.0175416, acc 1, learning_rate 0.0001
2017-09-29T13:54:25.112384: step 6110, loss 0.137275, acc 0.9375, learning_rate 0.0001
2017-09-29T13:54:25.298102: step 6111, loss 0.0467966, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:25.479581: step 6112, loss 0.0149741, acc 1, learning_rate 0.0001
2017-09-29T13:54:25.672330: step 6113, loss 0.0217817, acc 1, learning_rate 0.0001
2017-09-29T13:54:25.853986: step 6114, loss 0.0368824, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:26.039465: step 6115, loss 0.0134751, acc 1, learning_rate 0.0001
2017-09-29T13:54:26.223979: step 6116, loss 0.0484431, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:26.405941: step 6117, loss 0.0272915, acc 1, learning_rate 0.0001
2017-09-29T13:54:26.589880: step 6118, loss 0.0344632, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:26.773808: step 6119, loss 0.0314874, acc 1, learning_rate 0.0001
2017-09-29T13:54:26.974598: step 6120, loss 0.0139365, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:54:27.552428: step 6120, loss 0.218027, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6120

2017-09-29T13:54:28.262350: step 6121, loss 0.026215, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:28.450419: step 6122, loss 0.00450306, acc 1, learning_rate 0.0001
2017-09-29T13:54:28.640718: step 6123, loss 0.0314496, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:28.824018: step 6124, loss 0.0452734, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:29.009095: step 6125, loss 0.0320017, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:29.191911: step 6126, loss 0.115842, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:29.377689: step 6127, loss 0.0172268, acc 1, learning_rate 0.0001
2017-09-29T13:54:29.566826: step 6128, loss 0.100464, acc 0.953125, learning_rate 0.0001
2017-09-29T13:54:29.750552: step 6129, loss 0.00633412, acc 1, learning_rate 0.0001
2017-09-29T13:54:29.930852: step 6130, loss 0.0169132, acc 1, learning_rate 0.0001
2017-09-29T13:54:30.116350: step 6131, loss 0.0274802, acc 1, learning_rate 0.0001
2017-09-29T13:54:30.298521: step 6132, loss 0.0160997, acc 1, learning_rate 0.0001
2017-09-29T13:54:30.492060: step 6133, loss 0.0203023, acc 1, learning_rate 0.0001
2017-09-29T13:54:30.676399: step 6134, loss 0.0181787, acc 1, learning_rate 0.0001
2017-09-29T13:54:30.884137: step 6135, loss 0.00987839, acc 1, learning_rate 0.0001
2017-09-29T13:54:31.083659: step 6136, loss 0.0284592, acc 1, learning_rate 0.0001
2017-09-29T13:54:31.265796: step 6137, loss 0.0168225, acc 1, learning_rate 0.0001
2017-09-29T13:54:31.459438: step 6138, loss 0.0478761, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:31.642258: step 6139, loss 0.00706354, acc 1, learning_rate 0.0001
2017-09-29T13:54:31.821425: step 6140, loss 0.0298319, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:32.005857: step 6141, loss 0.0273991, acc 1, learning_rate 0.0001
2017-09-29T13:54:32.189714: step 6142, loss 0.102091, acc 0.953125, learning_rate 0.0001
2017-09-29T13:54:32.372824: step 6143, loss 0.0650728, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:32.563582: step 6144, loss 0.0144401, acc 1, learning_rate 0.0001
2017-09-29T13:54:32.744811: step 6145, loss 0.00672593, acc 1, learning_rate 0.0001
2017-09-29T13:54:32.927970: step 6146, loss 0.0345611, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:33.111572: step 6147, loss 0.0296847, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:33.294676: step 6148, loss 0.0221008, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:33.492552: step 6149, loss 0.040112, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:33.687170: step 6150, loss 0.00832205, acc 1, learning_rate 0.0001
2017-09-29T13:54:33.881153: step 6151, loss 0.0252572, acc 1, learning_rate 0.0001
2017-09-29T13:54:34.066261: step 6152, loss 0.0422377, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:34.255444: step 6153, loss 0.00549296, acc 1, learning_rate 0.0001
2017-09-29T13:54:34.449521: step 6154, loss 0.0384038, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:34.629283: step 6155, loss 0.0275939, acc 1, learning_rate 0.0001
2017-09-29T13:54:34.811706: step 6156, loss 0.00871056, acc 1, learning_rate 0.0001
2017-09-29T13:54:34.992925: step 6157, loss 0.0426598, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:35.177514: step 6158, loss 0.0464815, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:35.364862: step 6159, loss 0.0731114, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:35.548616: step 6160, loss 0.0119371, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:54:36.076029: step 6160, loss 0.217122, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6160

2017-09-29T13:54:36.770073: step 6161, loss 0.010995, acc 1, learning_rate 0.0001
2017-09-29T13:54:36.954954: step 6162, loss 0.0473767, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:37.136317: step 6163, loss 0.0332899, acc 1, learning_rate 0.0001
2017-09-29T13:54:37.319167: step 6164, loss 0.026106, acc 1, learning_rate 0.0001
2017-09-29T13:54:37.507906: step 6165, loss 0.0394505, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:37.687940: step 6166, loss 0.00521987, acc 1, learning_rate 0.0001
2017-09-29T13:54:37.869957: step 6167, loss 0.0286029, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:38.052429: step 6168, loss 0.0114291, acc 1, learning_rate 0.0001
2017-09-29T13:54:38.243744: step 6169, loss 0.0166846, acc 1, learning_rate 0.0001
2017-09-29T13:54:38.429664: step 6170, loss 0.018454, acc 1, learning_rate 0.0001
2017-09-29T13:54:38.618244: step 6171, loss 0.0292829, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:38.802586: step 6172, loss 0.00881542, acc 1, learning_rate 0.0001
2017-09-29T13:54:38.994742: step 6173, loss 0.013347, acc 1, learning_rate 0.0001
2017-09-29T13:54:39.148645: step 6174, loss 0.0600741, acc 0.980392, learning_rate 0.0001
2017-09-29T13:54:39.335469: step 6175, loss 0.0643525, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:39.537571: step 6176, loss 0.0268323, acc 1, learning_rate 0.0001
2017-09-29T13:54:39.719161: step 6177, loss 0.0312457, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:39.898188: step 6178, loss 0.0306249, acc 1, learning_rate 0.0001
2017-09-29T13:54:40.081057: step 6179, loss 0.00972854, acc 1, learning_rate 0.0001
2017-09-29T13:54:40.271615: step 6180, loss 0.0272123, acc 1, learning_rate 0.0001
2017-09-29T13:54:40.477567: step 6181, loss 0.068866, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:40.662327: step 6182, loss 0.00315044, acc 1, learning_rate 0.0001
2017-09-29T13:54:40.846445: step 6183, loss 0.0401676, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:41.033712: step 6184, loss 0.0440708, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:41.216904: step 6185, loss 0.0362981, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:41.405969: step 6186, loss 0.0241247, acc 1, learning_rate 0.0001
2017-09-29T13:54:41.589925: step 6187, loss 0.0324046, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:41.773664: step 6188, loss 0.0185421, acc 1, learning_rate 0.0001
2017-09-29T13:54:41.966274: step 6189, loss 0.109237, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:42.158956: step 6190, loss 0.0264724, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:42.343489: step 6191, loss 0.0112869, acc 1, learning_rate 0.0001
2017-09-29T13:54:42.529145: step 6192, loss 0.00629293, acc 1, learning_rate 0.0001
2017-09-29T13:54:42.711460: step 6193, loss 0.0620655, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:42.905262: step 6194, loss 0.0315264, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:43.091251: step 6195, loss 0.0117039, acc 1, learning_rate 0.0001
2017-09-29T13:54:43.274286: step 6196, loss 0.0522374, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:43.457148: step 6197, loss 0.0484688, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:43.644560: step 6198, loss 0.00554244, acc 1, learning_rate 0.0001
2017-09-29T13:54:43.828862: step 6199, loss 0.0316717, acc 1, learning_rate 0.0001
2017-09-29T13:54:44.014130: step 6200, loss 0.0780384, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:54:44.548853: step 6200, loss 0.217575, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6200

2017-09-29T13:54:45.333085: step 6201, loss 0.0660819, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:45.533539: step 6202, loss 0.0306947, acc 1, learning_rate 0.0001
2017-09-29T13:54:45.715704: step 6203, loss 0.0637826, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:45.912871: step 6204, loss 0.00876535, acc 1, learning_rate 0.0001
2017-09-29T13:54:46.097323: step 6205, loss 0.067711, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:46.282175: step 6206, loss 0.0396056, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:46.475524: step 6207, loss 0.0262782, acc 1, learning_rate 0.0001
2017-09-29T13:54:46.662045: step 6208, loss 0.0125596, acc 1, learning_rate 0.0001
2017-09-29T13:54:46.843731: step 6209, loss 0.0258237, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:47.039498: step 6210, loss 0.0554801, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:47.222948: step 6211, loss 0.00856702, acc 1, learning_rate 0.0001
2017-09-29T13:54:47.415919: step 6212, loss 0.0163389, acc 1, learning_rate 0.0001
2017-09-29T13:54:47.598753: step 6213, loss 0.0210042, acc 1, learning_rate 0.0001
2017-09-29T13:54:47.782928: step 6214, loss 0.0116773, acc 1, learning_rate 0.0001
2017-09-29T13:54:47.964305: step 6215, loss 0.0267553, acc 1, learning_rate 0.0001
2017-09-29T13:54:48.147109: step 6216, loss 0.0299268, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:48.339534: step 6217, loss 0.0154828, acc 1, learning_rate 0.0001
2017-09-29T13:54:48.531805: step 6218, loss 0.032092, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:48.724159: step 6219, loss 0.0601117, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:48.912634: step 6220, loss 0.085904, acc 0.953125, learning_rate 0.0001
2017-09-29T13:54:49.114634: step 6221, loss 0.0643701, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:49.304264: step 6222, loss 0.112799, acc 0.953125, learning_rate 0.0001
2017-09-29T13:54:49.490125: step 6223, loss 0.0780338, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:49.681843: step 6224, loss 0.0103477, acc 1, learning_rate 0.0001
2017-09-29T13:54:49.875861: step 6225, loss 0.0116723, acc 1, learning_rate 0.0001
2017-09-29T13:54:50.058818: step 6226, loss 0.0311105, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:50.243765: step 6227, loss 0.052915, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:50.426711: step 6228, loss 0.0731603, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:50.607518: step 6229, loss 0.0145922, acc 1, learning_rate 0.0001
2017-09-29T13:54:50.791001: step 6230, loss 0.0117776, acc 1, learning_rate 0.0001
2017-09-29T13:54:50.975216: step 6231, loss 0.0229938, acc 1, learning_rate 0.0001
2017-09-29T13:54:51.161687: step 6232, loss 0.0328155, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:51.346344: step 6233, loss 0.0250182, acc 1, learning_rate 0.0001
2017-09-29T13:54:51.547248: step 6234, loss 0.0157738, acc 1, learning_rate 0.0001
2017-09-29T13:54:51.748022: step 6235, loss 0.0393563, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:51.952852: step 6236, loss 0.0194712, acc 1, learning_rate 0.0001
2017-09-29T13:54:52.158588: step 6237, loss 0.0120637, acc 1, learning_rate 0.0001
2017-09-29T13:54:52.348302: step 6238, loss 0.0471854, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:52.531486: step 6239, loss 0.0669742, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:52.720058: step 6240, loss 0.00678392, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:54:53.281486: step 6240, loss 0.221555, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6240

2017-09-29T13:54:53.921421: step 6241, loss 0.0557966, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:54.123087: step 6242, loss 0.023493, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:54.307091: step 6243, loss 0.0283751, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:54.499491: step 6244, loss 0.0110508, acc 1, learning_rate 0.0001
2017-09-29T13:54:54.700116: step 6245, loss 0.0713078, acc 0.953125, learning_rate 0.0001
2017-09-29T13:54:54.885484: step 6246, loss 0.0614181, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:55.068927: step 6247, loss 0.0239599, acc 1, learning_rate 0.0001
2017-09-29T13:54:55.251213: step 6248, loss 0.0128061, acc 1, learning_rate 0.0001
2017-09-29T13:54:55.448143: step 6249, loss 0.00322484, acc 1, learning_rate 0.0001
2017-09-29T13:54:55.639692: step 6250, loss 0.0784109, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:55.831641: step 6251, loss 0.0149969, acc 1, learning_rate 0.0001
2017-09-29T13:54:56.023044: step 6252, loss 0.00758488, acc 1, learning_rate 0.0001
2017-09-29T13:54:56.212869: step 6253, loss 0.0343829, acc 1, learning_rate 0.0001
2017-09-29T13:54:56.407978: step 6254, loss 0.0625376, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:56.590117: step 6255, loss 0.0289851, acc 1, learning_rate 0.0001
2017-09-29T13:54:56.784114: step 6256, loss 0.0418711, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:56.978142: step 6257, loss 0.0365508, acc 1, learning_rate 0.0001
2017-09-29T13:54:57.170996: step 6258, loss 0.0175378, acc 1, learning_rate 0.0001
2017-09-29T13:54:57.357850: step 6259, loss 0.0355399, acc 0.96875, learning_rate 0.0001
2017-09-29T13:54:57.556086: step 6260, loss 0.0206218, acc 1, learning_rate 0.0001
2017-09-29T13:54:57.748834: step 6261, loss 0.0658866, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:57.938808: step 6262, loss 0.0310411, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:58.122252: step 6263, loss 0.0281835, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:58.307073: step 6264, loss 0.0230156, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:58.494510: step 6265, loss 0.0395528, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:58.679387: step 6266, loss 0.00826309, acc 1, learning_rate 0.0001
2017-09-29T13:54:58.869557: step 6267, loss 0.0107412, acc 1, learning_rate 0.0001
2017-09-29T13:54:59.063982: step 6268, loss 0.026384, acc 0.984375, learning_rate 0.0001
2017-09-29T13:54:59.249455: step 6269, loss 0.0279458, acc 1, learning_rate 0.0001
2017-09-29T13:54:59.447485: step 6270, loss 0.0233793, acc 1, learning_rate 0.0001
2017-09-29T13:54:59.638375: step 6271, loss 0.00683171, acc 1, learning_rate 0.0001
2017-09-29T13:54:59.792506: step 6272, loss 0.0133516, acc 1, learning_rate 0.0001
2017-09-29T13:54:59.974738: step 6273, loss 0.0166785, acc 1, learning_rate 0.0001
2017-09-29T13:55:00.170196: step 6274, loss 0.00796654, acc 1, learning_rate 0.0001
2017-09-29T13:55:00.351789: step 6275, loss 0.0604625, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:00.537776: step 6276, loss 0.0186856, acc 1, learning_rate 0.0001
2017-09-29T13:55:00.743392: step 6277, loss 0.0337012, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:00.938863: step 6278, loss 0.0963556, acc 0.9375, learning_rate 0.0001
2017-09-29T13:55:01.128329: step 6279, loss 0.018238, acc 1, learning_rate 0.0001
2017-09-29T13:55:01.315197: step 6280, loss 0.0199787, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:55:01.846644: step 6280, loss 0.22259, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6280

2017-09-29T13:55:02.549905: step 6281, loss 0.0142853, acc 1, learning_rate 0.0001
2017-09-29T13:55:02.737720: step 6282, loss 0.0592873, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:02.927388: step 6283, loss 0.0513893, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:03.111962: step 6284, loss 0.0200104, acc 1, learning_rate 0.0001
2017-09-29T13:55:03.294630: step 6285, loss 0.0145042, acc 1, learning_rate 0.0001
2017-09-29T13:55:03.499104: step 6286, loss 0.0301013, acc 1, learning_rate 0.0001
2017-09-29T13:55:03.699376: step 6287, loss 0.00599997, acc 1, learning_rate 0.0001
2017-09-29T13:55:03.884889: step 6288, loss 0.0517318, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:04.070284: step 6289, loss 0.0199883, acc 1, learning_rate 0.0001
2017-09-29T13:55:04.252349: step 6290, loss 0.081987, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:04.444075: step 6291, loss 0.00507967, acc 1, learning_rate 0.0001
2017-09-29T13:55:04.637409: step 6292, loss 0.0118031, acc 1, learning_rate 0.0001
2017-09-29T13:55:04.826053: step 6293, loss 0.00662224, acc 1, learning_rate 0.0001
2017-09-29T13:55:05.009705: step 6294, loss 0.0139934, acc 1, learning_rate 0.0001
2017-09-29T13:55:05.191204: step 6295, loss 0.0298029, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:05.374790: step 6296, loss 0.0332301, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:05.563228: step 6297, loss 0.0429778, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:05.746808: step 6298, loss 0.0372605, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:05.926402: step 6299, loss 0.0083745, acc 1, learning_rate 0.0001
2017-09-29T13:55:06.105994: step 6300, loss 0.00389363, acc 1, learning_rate 0.0001
2017-09-29T13:55:06.291329: step 6301, loss 0.0203373, acc 1, learning_rate 0.0001
2017-09-29T13:55:06.493580: step 6302, loss 0.0413885, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:06.675115: step 6303, loss 0.00423712, acc 1, learning_rate 0.0001
2017-09-29T13:55:06.856424: step 6304, loss 0.0135853, acc 1, learning_rate 0.0001
2017-09-29T13:55:07.040595: step 6305, loss 0.0687696, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:07.225788: step 6306, loss 0.0357412, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:07.410561: step 6307, loss 0.0160265, acc 1, learning_rate 0.0001
2017-09-29T13:55:07.601416: step 6308, loss 0.0196805, acc 1, learning_rate 0.0001
2017-09-29T13:55:07.789722: step 6309, loss 0.0370856, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:07.971017: step 6310, loss 0.0511642, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:08.153363: step 6311, loss 0.0228769, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:08.335708: step 6312, loss 0.0571642, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:08.522653: step 6313, loss 0.0505633, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:08.709918: step 6314, loss 0.0154421, acc 1, learning_rate 0.0001
2017-09-29T13:55:08.894412: step 6315, loss 0.0175511, acc 1, learning_rate 0.0001
2017-09-29T13:55:09.078239: step 6316, loss 0.0563783, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:09.262597: step 6317, loss 0.0638864, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:09.451726: step 6318, loss 0.113188, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:09.633952: step 6319, loss 0.00426044, acc 1, learning_rate 0.0001
2017-09-29T13:55:09.822753: step 6320, loss 0.0412667, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:55:10.351030: step 6320, loss 0.223231, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6320

2017-09-29T13:55:11.132082: step 6321, loss 0.00607191, acc 1, learning_rate 0.0001
2017-09-29T13:55:11.327992: step 6322, loss 0.0448192, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:11.518718: step 6323, loss 0.0161878, acc 1, learning_rate 0.0001
2017-09-29T13:55:11.704129: step 6324, loss 0.0365356, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:11.887917: step 6325, loss 0.018106, acc 1, learning_rate 0.0001
2017-09-29T13:55:12.076377: step 6326, loss 0.0184358, acc 1, learning_rate 0.0001
2017-09-29T13:55:12.258400: step 6327, loss 0.0335386, acc 1, learning_rate 0.0001
2017-09-29T13:55:12.443460: step 6328, loss 0.0283, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:12.627883: step 6329, loss 0.0526156, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:12.816131: step 6330, loss 0.123125, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:13.011292: step 6331, loss 0.0223528, acc 1, learning_rate 0.0001
2017-09-29T13:55:13.196953: step 6332, loss 0.0197144, acc 1, learning_rate 0.0001
2017-09-29T13:55:13.381144: step 6333, loss 0.031615, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:13.564283: step 6334, loss 0.023146, acc 1, learning_rate 0.0001
2017-09-29T13:55:13.750692: step 6335, loss 0.0393616, acc 1, learning_rate 0.0001
2017-09-29T13:55:13.941310: step 6336, loss 0.0753151, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:14.130560: step 6337, loss 0.0108639, acc 1, learning_rate 0.0001
2017-09-29T13:55:14.319732: step 6338, loss 0.0225531, acc 1, learning_rate 0.0001
2017-09-29T13:55:14.511049: step 6339, loss 0.0323515, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:14.699534: step 6340, loss 0.0139242, acc 1, learning_rate 0.0001
2017-09-29T13:55:14.882147: step 6341, loss 0.0395396, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:15.066128: step 6342, loss 0.0619334, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:15.248904: step 6343, loss 0.0503234, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:15.439893: step 6344, loss 0.0785171, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:15.623884: step 6345, loss 0.0256373, acc 1, learning_rate 0.0001
2017-09-29T13:55:15.819030: step 6346, loss 0.0230397, acc 1, learning_rate 0.0001
2017-09-29T13:55:16.038883: step 6347, loss 0.0062865, acc 1, learning_rate 0.0001
2017-09-29T13:55:16.235489: step 6348, loss 0.0141293, acc 1, learning_rate 0.0001
2017-09-29T13:55:16.424549: step 6349, loss 0.0165429, acc 1, learning_rate 0.0001
2017-09-29T13:55:16.607490: step 6350, loss 0.0166786, acc 1, learning_rate 0.0001
2017-09-29T13:55:16.788647: step 6351, loss 0.0493352, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:16.987600: step 6352, loss 0.00674757, acc 1, learning_rate 0.0001
2017-09-29T13:55:17.180076: step 6353, loss 0.01501, acc 1, learning_rate 0.0001
2017-09-29T13:55:17.364860: step 6354, loss 0.0236307, acc 1, learning_rate 0.0001
2017-09-29T13:55:17.555416: step 6355, loss 0.0337384, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:17.745074: step 6356, loss 0.0836967, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:17.931434: step 6357, loss 0.0207683, acc 1, learning_rate 0.0001
2017-09-29T13:55:18.119719: step 6358, loss 0.016386, acc 1, learning_rate 0.0001
2017-09-29T13:55:18.307187: step 6359, loss 0.00526058, acc 1, learning_rate 0.0001
2017-09-29T13:55:18.496995: step 6360, loss 0.00611961, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:55:19.046796: step 6360, loss 0.217011, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6360

2017-09-29T13:55:19.718269: step 6361, loss 0.0280626, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:19.932559: step 6362, loss 0.0178975, acc 1, learning_rate 0.0001
2017-09-29T13:55:20.143733: step 6363, loss 0.0103428, acc 1, learning_rate 0.0001
2017-09-29T13:55:20.350803: step 6364, loss 0.0389343, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:20.538837: step 6365, loss 0.0229808, acc 1, learning_rate 0.0001
2017-09-29T13:55:20.727707: step 6366, loss 0.0351817, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:20.923745: step 6367, loss 0.0481341, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:21.109210: step 6368, loss 0.0584817, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:21.298762: step 6369, loss 0.0565861, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:21.465750: step 6370, loss 0.0413244, acc 0.980392, learning_rate 0.0001
2017-09-29T13:55:21.666459: step 6371, loss 0.0106902, acc 1, learning_rate 0.0001
2017-09-29T13:55:21.855781: step 6372, loss 0.0446955, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:22.045211: step 6373, loss 0.0215922, acc 1, learning_rate 0.0001
2017-09-29T13:55:22.241998: step 6374, loss 0.0472486, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:22.437742: step 6375, loss 0.0602027, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:22.632107: step 6376, loss 0.0651063, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:22.823407: step 6377, loss 0.00372171, acc 1, learning_rate 0.0001
2017-09-29T13:55:23.013274: step 6378, loss 0.0189851, acc 1, learning_rate 0.0001
2017-09-29T13:55:23.202886: step 6379, loss 0.0288237, acc 1, learning_rate 0.0001
2017-09-29T13:55:23.392703: step 6380, loss 0.0355144, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:23.580713: step 6381, loss 0.0284882, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:23.771126: step 6382, loss 0.0766723, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:23.953270: step 6383, loss 0.0372888, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:24.136167: step 6384, loss 0.0549703, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:24.335581: step 6385, loss 0.07038, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:24.524014: step 6386, loss 0.0590778, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:24.717077: step 6387, loss 0.0255033, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:24.916076: step 6388, loss 0.0412419, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:25.108872: step 6389, loss 0.0618981, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:25.303272: step 6390, loss 0.0386034, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:25.499585: step 6391, loss 0.0125099, acc 1, learning_rate 0.0001
2017-09-29T13:55:25.693761: step 6392, loss 0.0369869, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:25.884315: step 6393, loss 0.0280905, acc 1, learning_rate 0.0001
2017-09-29T13:55:26.075106: step 6394, loss 0.0270192, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:26.261497: step 6395, loss 0.0357032, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:26.455270: step 6396, loss 0.0511002, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:26.646611: step 6397, loss 0.0330704, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:26.838102: step 6398, loss 0.0623469, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:27.036487: step 6399, loss 0.039376, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:27.223038: step 6400, loss 0.00906753, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:55:27.785020: step 6400, loss 0.218272, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6400

2017-09-29T13:55:28.535139: step 6401, loss 0.0603452, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:28.724566: step 6402, loss 0.0254371, acc 1, learning_rate 0.0001
2017-09-29T13:55:28.913772: step 6403, loss 0.0370264, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:29.111809: step 6404, loss 0.041099, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:29.300319: step 6405, loss 0.048467, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:29.493495: step 6406, loss 0.02999, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:29.676478: step 6407, loss 0.0242045, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:29.862553: step 6408, loss 0.0267052, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:30.055349: step 6409, loss 0.0553492, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:30.239028: step 6410, loss 0.0951376, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:30.429203: step 6411, loss 0.0731247, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:30.620378: step 6412, loss 0.0163363, acc 1, learning_rate 0.0001
2017-09-29T13:55:30.809751: step 6413, loss 0.00807648, acc 1, learning_rate 0.0001
2017-09-29T13:55:30.999364: step 6414, loss 0.0365299, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:31.199206: step 6415, loss 0.0113106, acc 1, learning_rate 0.0001
2017-09-29T13:55:31.384676: step 6416, loss 0.0165595, acc 1, learning_rate 0.0001
2017-09-29T13:55:31.571001: step 6417, loss 0.0439587, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:31.756708: step 6418, loss 0.0100177, acc 1, learning_rate 0.0001
2017-09-29T13:55:31.939899: step 6419, loss 0.0436509, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:32.123963: step 6420, loss 0.0343406, acc 1, learning_rate 0.0001
2017-09-29T13:55:32.313171: step 6421, loss 0.0205571, acc 1, learning_rate 0.0001
2017-09-29T13:55:32.501735: step 6422, loss 0.0250215, acc 1, learning_rate 0.0001
2017-09-29T13:55:32.684768: step 6423, loss 0.00482256, acc 1, learning_rate 0.0001
2017-09-29T13:55:32.869293: step 6424, loss 0.0093956, acc 1, learning_rate 0.0001
2017-09-29T13:55:33.059456: step 6425, loss 0.0363434, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:33.248016: step 6426, loss 0.0532023, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:33.450549: step 6427, loss 0.0165861, acc 1, learning_rate 0.0001
2017-09-29T13:55:33.638408: step 6428, loss 0.0219335, acc 1, learning_rate 0.0001
2017-09-29T13:55:33.840163: step 6429, loss 0.01351, acc 1, learning_rate 0.0001
2017-09-29T13:55:34.025541: step 6430, loss 0.0442319, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:34.208881: step 6431, loss 0.0649217, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:34.392883: step 6432, loss 0.00728204, acc 1, learning_rate 0.0001
2017-09-29T13:55:34.577050: step 6433, loss 0.0373204, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:34.763227: step 6434, loss 0.102612, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:34.948716: step 6435, loss 0.0102106, acc 1, learning_rate 0.0001
2017-09-29T13:55:35.132826: step 6436, loss 0.0342687, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:35.317861: step 6437, loss 0.0366751, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:35.516577: step 6438, loss 0.0458494, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:35.708424: step 6439, loss 0.0844364, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:35.893637: step 6440, loss 0.0304468, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:55:36.455008: step 6440, loss 0.216909, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6440

2017-09-29T13:55:37.241736: step 6441, loss 0.0217914, acc 1, learning_rate 0.0001
2017-09-29T13:55:37.427402: step 6442, loss 0.0569041, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:37.619612: step 6443, loss 0.00780878, acc 1, learning_rate 0.0001
2017-09-29T13:55:37.802698: step 6444, loss 0.0138609, acc 1, learning_rate 0.0001
2017-09-29T13:55:37.996529: step 6445, loss 0.0541022, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:38.201108: step 6446, loss 0.0617485, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:38.390201: step 6447, loss 0.116931, acc 0.9375, learning_rate 0.0001
2017-09-29T13:55:38.588512: step 6448, loss 0.00937301, acc 1, learning_rate 0.0001
2017-09-29T13:55:38.777486: step 6449, loss 0.0106896, acc 1, learning_rate 0.0001
2017-09-29T13:55:38.978479: step 6450, loss 0.0446014, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:39.165602: step 6451, loss 0.0140478, acc 1, learning_rate 0.0001
2017-09-29T13:55:39.350724: step 6452, loss 0.012783, acc 1, learning_rate 0.0001
2017-09-29T13:55:39.535568: step 6453, loss 0.0235606, acc 1, learning_rate 0.0001
2017-09-29T13:55:39.716421: step 6454, loss 0.0400606, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:39.904057: step 6455, loss 0.0288957, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:40.091561: step 6456, loss 0.0275946, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:40.280050: step 6457, loss 0.0267354, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:40.474433: step 6458, loss 0.0256798, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:40.663870: step 6459, loss 0.0222925, acc 1, learning_rate 0.0001
2017-09-29T13:55:40.858065: step 6460, loss 0.0407217, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:41.040462: step 6461, loss 0.00764531, acc 1, learning_rate 0.0001
2017-09-29T13:55:41.228009: step 6462, loss 0.0293641, acc 1, learning_rate 0.0001
2017-09-29T13:55:41.409595: step 6463, loss 0.0660434, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:41.592674: step 6464, loss 0.0244556, acc 1, learning_rate 0.0001
2017-09-29T13:55:41.779641: step 6465, loss 0.109996, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:41.966641: step 6466, loss 0.065443, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:42.147071: step 6467, loss 0.0134806, acc 1, learning_rate 0.0001
2017-09-29T13:55:42.299233: step 6468, loss 0.0121836, acc 1, learning_rate 0.0001
2017-09-29T13:55:42.484772: step 6469, loss 0.0134096, acc 1, learning_rate 0.0001
2017-09-29T13:55:42.667254: step 6470, loss 0.0632378, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:42.849530: step 6471, loss 0.0141183, acc 1, learning_rate 0.0001
2017-09-29T13:55:43.038988: step 6472, loss 0.0116141, acc 1, learning_rate 0.0001
2017-09-29T13:55:43.219792: step 6473, loss 0.0171963, acc 1, learning_rate 0.0001
2017-09-29T13:55:43.403174: step 6474, loss 0.0569332, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:43.596103: step 6475, loss 0.00735856, acc 1, learning_rate 0.0001
2017-09-29T13:55:43.787695: step 6476, loss 0.0176164, acc 1, learning_rate 0.0001
2017-09-29T13:55:43.972814: step 6477, loss 0.0223875, acc 1, learning_rate 0.0001
2017-09-29T13:55:44.153569: step 6478, loss 0.0711636, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:44.337222: step 6479, loss 0.043638, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:44.532356: step 6480, loss 0.0476628, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:55:45.105718: step 6480, loss 0.22828, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6480

2017-09-29T13:55:45.730149: step 6481, loss 0.0261296, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:45.907441: step 6482, loss 0.00400339, acc 1, learning_rate 0.0001
2017-09-29T13:55:46.089566: step 6483, loss 0.0290358, acc 1, learning_rate 0.0001
2017-09-29T13:55:46.270951: step 6484, loss 0.00877273, acc 1, learning_rate 0.0001
2017-09-29T13:55:46.466155: step 6485, loss 0.0461441, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:46.653655: step 6486, loss 0.0216431, acc 1, learning_rate 0.0001
2017-09-29T13:55:46.834964: step 6487, loss 0.0936221, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:47.018506: step 6488, loss 0.0388336, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:47.204876: step 6489, loss 0.0288641, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:47.389470: step 6490, loss 0.00568854, acc 1, learning_rate 0.0001
2017-09-29T13:55:47.582184: step 6491, loss 0.0342265, acc 1, learning_rate 0.0001
2017-09-29T13:55:47.769998: step 6492, loss 0.00431246, acc 1, learning_rate 0.0001
2017-09-29T13:55:47.952810: step 6493, loss 0.0778186, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:48.142898: step 6494, loss 0.0480782, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:48.327602: step 6495, loss 0.0180201, acc 1, learning_rate 0.0001
2017-09-29T13:55:48.516059: step 6496, loss 0.0246866, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:48.713426: step 6497, loss 0.00690865, acc 1, learning_rate 0.0001
2017-09-29T13:55:48.901035: step 6498, loss 0.0490731, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:49.091843: step 6499, loss 0.0778895, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:49.283474: step 6500, loss 0.00576447, acc 1, learning_rate 0.0001
2017-09-29T13:55:49.482885: step 6501, loss 0.022093, acc 1, learning_rate 0.0001
2017-09-29T13:55:49.673626: step 6502, loss 0.0503805, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:49.863639: step 6503, loss 0.0254929, acc 1, learning_rate 0.0001
2017-09-29T13:55:50.066579: step 6504, loss 0.0104595, acc 1, learning_rate 0.0001
2017-09-29T13:55:50.262529: step 6505, loss 0.076635, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:50.470049: step 6506, loss 0.101982, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:50.672740: step 6507, loss 0.00561471, acc 1, learning_rate 0.0001
2017-09-29T13:55:50.860856: step 6508, loss 0.0274889, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:51.059492: step 6509, loss 0.0613252, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:51.236901: step 6510, loss 0.0881436, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:51.435742: step 6511, loss 0.0286489, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:51.626589: step 6512, loss 0.0228638, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:51.814503: step 6513, loss 0.0707949, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:52.005639: step 6514, loss 0.0139138, acc 1, learning_rate 0.0001
2017-09-29T13:55:52.197145: step 6515, loss 0.0285253, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:52.388955: step 6516, loss 0.0299502, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:52.582579: step 6517, loss 0.0216409, acc 1, learning_rate 0.0001
2017-09-29T13:55:52.767613: step 6518, loss 0.0579535, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:52.951745: step 6519, loss 0.00828812, acc 1, learning_rate 0.0001
2017-09-29T13:55:53.142839: step 6520, loss 0.0211398, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:55:53.727000: step 6520, loss 0.225388, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6520

2017-09-29T13:55:54.451522: step 6521, loss 0.0190749, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:54.637456: step 6522, loss 0.012471, acc 1, learning_rate 0.0001
2017-09-29T13:55:54.822460: step 6523, loss 0.0222612, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:55.012106: step 6524, loss 0.0329235, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:55.204515: step 6525, loss 0.0441256, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:55.392380: step 6526, loss 0.0282732, acc 1, learning_rate 0.0001
2017-09-29T13:55:55.577479: step 6527, loss 0.0107928, acc 1, learning_rate 0.0001
2017-09-29T13:55:55.772900: step 6528, loss 0.0366085, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:55.960176: step 6529, loss 0.025742, acc 1, learning_rate 0.0001
2017-09-29T13:55:56.155655: step 6530, loss 0.0692179, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:56.337504: step 6531, loss 0.036023, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:56.533385: step 6532, loss 0.0712084, acc 0.953125, learning_rate 0.0001
2017-09-29T13:55:56.718635: step 6533, loss 0.0158975, acc 1, learning_rate 0.0001
2017-09-29T13:55:56.906038: step 6534, loss 0.0187256, acc 1, learning_rate 0.0001
2017-09-29T13:55:57.089032: step 6535, loss 0.00418505, acc 1, learning_rate 0.0001
2017-09-29T13:55:57.269575: step 6536, loss 0.0676506, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:57.456221: step 6537, loss 0.0507917, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:57.642822: step 6538, loss 0.0471775, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:57.833928: step 6539, loss 0.0242088, acc 1, learning_rate 0.0001
2017-09-29T13:55:58.018931: step 6540, loss 0.0391398, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:58.217678: step 6541, loss 0.0144728, acc 1, learning_rate 0.0001
2017-09-29T13:55:58.401536: step 6542, loss 0.0186892, acc 1, learning_rate 0.0001
2017-09-29T13:55:58.595596: step 6543, loss 0.0640016, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:58.778630: step 6544, loss 0.0440855, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:58.973467: step 6545, loss 0.018033, acc 1, learning_rate 0.0001
2017-09-29T13:55:59.177365: step 6546, loss 0.0776948, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:59.373531: step 6547, loss 0.024882, acc 0.984375, learning_rate 0.0001
2017-09-29T13:55:59.557654: step 6548, loss 0.0407824, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:59.742399: step 6549, loss 0.0602916, acc 0.96875, learning_rate 0.0001
2017-09-29T13:55:59.927834: step 6550, loss 0.0435053, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:00.118736: step 6551, loss 0.04878, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:00.303787: step 6552, loss 0.113408, acc 0.953125, learning_rate 0.0001
2017-09-29T13:56:00.498320: step 6553, loss 0.0310386, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:00.687107: step 6554, loss 0.0236332, acc 1, learning_rate 0.0001
2017-09-29T13:56:00.869267: step 6555, loss 0.0155695, acc 1, learning_rate 0.0001
2017-09-29T13:56:01.054927: step 6556, loss 0.0331161, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:01.239369: step 6557, loss 0.0134439, acc 1, learning_rate 0.0001
2017-09-29T13:56:01.430208: step 6558, loss 0.0212279, acc 1, learning_rate 0.0001
2017-09-29T13:56:01.620126: step 6559, loss 0.0293452, acc 1, learning_rate 0.0001
2017-09-29T13:56:01.803521: step 6560, loss 0.0190884, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:56:02.348521: step 6560, loss 0.219172, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6560

2017-09-29T13:56:03.072359: step 6561, loss 0.0640566, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:03.270433: step 6562, loss 0.0162033, acc 1, learning_rate 0.0001
2017-09-29T13:56:03.492488: step 6563, loss 0.0138964, acc 1, learning_rate 0.0001
2017-09-29T13:56:03.699928: step 6564, loss 0.00798229, acc 1, learning_rate 0.0001
2017-09-29T13:56:03.925493: step 6565, loss 0.0115088, acc 1, learning_rate 0.0001
2017-09-29T13:56:04.109597: step 6566, loss 0.00870836, acc 1, learning_rate 0.0001
2017-09-29T13:56:04.294164: step 6567, loss 0.0209146, acc 1, learning_rate 0.0001
2017-09-29T13:56:04.478920: step 6568, loss 0.0907318, acc 0.953125, learning_rate 0.0001
2017-09-29T13:56:04.656691: step 6569, loss 0.0325546, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:04.837983: step 6570, loss 0.0197036, acc 1, learning_rate 0.0001
2017-09-29T13:56:05.024391: step 6571, loss 0.0200863, acc 1, learning_rate 0.0001
2017-09-29T13:56:05.218679: step 6572, loss 0.0271472, acc 1, learning_rate 0.0001
2017-09-29T13:56:05.408639: step 6573, loss 0.0143639, acc 1, learning_rate 0.0001
2017-09-29T13:56:05.595548: step 6574, loss 0.0241735, acc 1, learning_rate 0.0001
2017-09-29T13:56:05.780368: step 6575, loss 0.0436646, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:05.961244: step 6576, loss 0.0158531, acc 1, learning_rate 0.0001
2017-09-29T13:56:06.147161: step 6577, loss 0.0281975, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:06.343091: step 6578, loss 0.00884621, acc 1, learning_rate 0.0001
2017-09-29T13:56:06.529245: step 6579, loss 0.0188058, acc 1, learning_rate 0.0001
2017-09-29T13:56:06.729169: step 6580, loss 0.0907148, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:06.924009: step 6581, loss 0.00822426, acc 1, learning_rate 0.0001
2017-09-29T13:56:07.107483: step 6582, loss 0.0213648, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:07.290207: step 6583, loss 0.0142823, acc 1, learning_rate 0.0001
2017-09-29T13:56:07.487696: step 6584, loss 0.0220949, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:07.682799: step 6585, loss 0.017376, acc 1, learning_rate 0.0001
2017-09-29T13:56:07.875068: step 6586, loss 0.0136235, acc 1, learning_rate 0.0001
2017-09-29T13:56:08.055875: step 6587, loss 0.0342703, acc 1, learning_rate 0.0001
2017-09-29T13:56:08.240172: step 6588, loss 0.0454015, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:08.424352: step 6589, loss 0.0962687, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:08.619748: step 6590, loss 0.045436, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:08.801462: step 6591, loss 0.0378948, acc 1, learning_rate 0.0001
2017-09-29T13:56:08.982506: step 6592, loss 0.0276594, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:09.170282: step 6593, loss 0.0321768, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:09.356683: step 6594, loss 0.0378452, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:09.544733: step 6595, loss 0.0427462, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:09.727776: step 6596, loss 0.0135466, acc 1, learning_rate 0.0001
2017-09-29T13:56:09.917753: step 6597, loss 0.0444148, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:10.113600: step 6598, loss 0.013891, acc 1, learning_rate 0.0001
2017-09-29T13:56:10.299292: step 6599, loss 0.0167175, acc 1, learning_rate 0.0001
2017-09-29T13:56:10.482587: step 6600, loss 0.0204102, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:56:11.028431: step 6600, loss 0.219715, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6600

2017-09-29T13:56:11.820519: step 6601, loss 0.0207764, acc 1, learning_rate 0.0001
2017-09-29T13:56:12.008600: step 6602, loss 0.0204212, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:12.203081: step 6603, loss 0.0211222, acc 1, learning_rate 0.0001
2017-09-29T13:56:12.409476: step 6604, loss 0.0427057, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:12.632230: step 6605, loss 0.0366474, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:12.909471: step 6606, loss 0.0358646, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:13.106675: step 6607, loss 0.0271332, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:13.296064: step 6608, loss 0.016792, acc 1, learning_rate 0.0001
2017-09-29T13:56:13.516939: step 6609, loss 0.00418379, acc 1, learning_rate 0.0001
2017-09-29T13:56:13.735727: step 6610, loss 0.0569198, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:13.979333: step 6611, loss 0.011378, acc 1, learning_rate 0.0001
2017-09-29T13:56:14.218242: step 6612, loss 0.0669299, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:14.427371: step 6613, loss 0.0237297, acc 1, learning_rate 0.0001
2017-09-29T13:56:14.623171: step 6614, loss 0.0387699, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:14.814470: step 6615, loss 0.020832, acc 1, learning_rate 0.0001
2017-09-29T13:56:15.004535: step 6616, loss 0.0171215, acc 1, learning_rate 0.0001
2017-09-29T13:56:15.192553: step 6617, loss 0.030858, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:15.378243: step 6618, loss 0.00800717, acc 1, learning_rate 0.0001
2017-09-29T13:56:15.587013: step 6619, loss 0.0303868, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:15.798349: step 6620, loss 0.0396496, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:15.989883: step 6621, loss 0.0816175, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:16.180449: step 6622, loss 0.0862485, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:16.367271: step 6623, loss 0.022026, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:16.555246: step 6624, loss 0.0059559, acc 1, learning_rate 0.0001
2017-09-29T13:56:16.735864: step 6625, loss 0.0643118, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:16.919382: step 6626, loss 0.0397865, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:17.110337: step 6627, loss 0.00660917, acc 1, learning_rate 0.0001
2017-09-29T13:56:17.293324: step 6628, loss 0.0119962, acc 1, learning_rate 0.0001
2017-09-29T13:56:17.477574: step 6629, loss 0.0226437, acc 1, learning_rate 0.0001
2017-09-29T13:56:17.682040: step 6630, loss 0.080114, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:17.865339: step 6631, loss 0.0156782, acc 1, learning_rate 0.0001
2017-09-29T13:56:18.061723: step 6632, loss 0.0490674, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:18.250573: step 6633, loss 0.0179447, acc 1, learning_rate 0.0001
2017-09-29T13:56:18.430846: step 6634, loss 0.0623667, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:18.610649: step 6635, loss 0.0330582, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:18.792998: step 6636, loss 0.0285421, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:18.989810: step 6637, loss 0.106635, acc 0.9375, learning_rate 0.0001
2017-09-29T13:56:19.177583: step 6638, loss 0.0659249, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:19.359028: step 6639, loss 0.00982895, acc 1, learning_rate 0.0001
2017-09-29T13:56:19.551654: step 6640, loss 0.0117627, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:56:20.131768: step 6640, loss 0.222072, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6640

2017-09-29T13:56:20.759944: step 6641, loss 0.0554382, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:20.942375: step 6642, loss 0.0576163, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:21.125660: step 6643, loss 0.0420228, acc 1, learning_rate 0.0001
2017-09-29T13:56:21.309214: step 6644, loss 0.0310169, acc 1, learning_rate 0.0001
2017-09-29T13:56:21.494644: step 6645, loss 0.0218241, acc 1, learning_rate 0.0001
2017-09-29T13:56:21.676822: step 6646, loss 0.0179752, acc 1, learning_rate 0.0001
2017-09-29T13:56:21.861826: step 6647, loss 0.0217442, acc 1, learning_rate 0.0001
2017-09-29T13:56:22.041867: step 6648, loss 0.0295766, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:22.224934: step 6649, loss 0.0206715, acc 1, learning_rate 0.0001
2017-09-29T13:56:22.419649: step 6650, loss 0.0229196, acc 1, learning_rate 0.0001
2017-09-29T13:56:22.638793: step 6651, loss 0.0174846, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:22.843268: step 6652, loss 0.00586804, acc 1, learning_rate 0.0001
2017-09-29T13:56:23.039061: step 6653, loss 0.0208206, acc 1, learning_rate 0.0001
2017-09-29T13:56:23.225431: step 6654, loss 0.0351748, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:23.427256: step 6655, loss 0.032554, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:23.626795: step 6656, loss 0.00997181, acc 1, learning_rate 0.0001
2017-09-29T13:56:23.816421: step 6657, loss 0.0238668, acc 1, learning_rate 0.0001
2017-09-29T13:56:24.010607: step 6658, loss 0.0587871, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:24.198719: step 6659, loss 0.0209691, acc 1, learning_rate 0.0001
2017-09-29T13:56:24.383274: step 6660, loss 0.0129816, acc 1, learning_rate 0.0001
2017-09-29T13:56:24.569818: step 6661, loss 0.0177525, acc 1, learning_rate 0.0001
2017-09-29T13:56:24.759541: step 6662, loss 0.0250573, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:24.941909: step 6663, loss 0.117691, acc 0.9375, learning_rate 0.0001
2017-09-29T13:56:25.096865: step 6664, loss 0.0225616, acc 1, learning_rate 0.0001
2017-09-29T13:56:25.284764: step 6665, loss 0.0650898, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:25.492224: step 6666, loss 0.016231, acc 1, learning_rate 0.0001
2017-09-29T13:56:25.676377: step 6667, loss 0.013344, acc 1, learning_rate 0.0001
2017-09-29T13:56:25.859116: step 6668, loss 0.0423857, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:26.041837: step 6669, loss 0.0382886, acc 1, learning_rate 0.0001
2017-09-29T13:56:26.224849: step 6670, loss 0.00257234, acc 1, learning_rate 0.0001
2017-09-29T13:56:26.408082: step 6671, loss 0.0272677, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:26.595283: step 6672, loss 0.0383209, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:26.777025: step 6673, loss 0.0542787, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:26.959697: step 6674, loss 0.0160945, acc 1, learning_rate 0.0001
2017-09-29T13:56:27.144367: step 6675, loss 0.0205115, acc 1, learning_rate 0.0001
2017-09-29T13:56:27.338640: step 6676, loss 0.0165034, acc 1, learning_rate 0.0001
2017-09-29T13:56:27.531985: step 6677, loss 0.0402374, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:27.718769: step 6678, loss 0.016107, acc 1, learning_rate 0.0001
2017-09-29T13:56:27.904136: step 6679, loss 0.0217173, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:28.089364: step 6680, loss 0.0610514, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:56:28.647054: step 6680, loss 0.219159, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6680

2017-09-29T13:56:29.348303: step 6681, loss 0.0955216, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:29.535704: step 6682, loss 0.00766429, acc 1, learning_rate 0.0001
2017-09-29T13:56:29.716748: step 6683, loss 0.0116773, acc 1, learning_rate 0.0001
2017-09-29T13:56:29.908425: step 6684, loss 0.0188154, acc 1, learning_rate 0.0001
2017-09-29T13:56:30.090756: step 6685, loss 0.0185768, acc 1, learning_rate 0.0001
2017-09-29T13:56:30.281425: step 6686, loss 0.0753396, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:30.466369: step 6687, loss 0.0278503, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:30.647345: step 6688, loss 0.00533334, acc 1, learning_rate 0.0001
2017-09-29T13:56:30.829458: step 6689, loss 0.0224002, acc 1, learning_rate 0.0001
2017-09-29T13:56:31.010923: step 6690, loss 0.0566815, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:31.194261: step 6691, loss 0.02258, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:31.372773: step 6692, loss 0.0710482, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:31.568146: step 6693, loss 0.0109851, acc 1, learning_rate 0.0001
2017-09-29T13:56:31.761763: step 6694, loss 0.0147275, acc 1, learning_rate 0.0001
2017-09-29T13:56:31.943201: step 6695, loss 0.0191623, acc 1, learning_rate 0.0001
2017-09-29T13:56:32.124124: step 6696, loss 0.0661737, acc 0.953125, learning_rate 0.0001
2017-09-29T13:56:32.308105: step 6697, loss 0.0358798, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:32.492732: step 6698, loss 0.00455934, acc 1, learning_rate 0.0001
2017-09-29T13:56:32.675409: step 6699, loss 0.013502, acc 1, learning_rate 0.0001
2017-09-29T13:56:32.866446: step 6700, loss 0.0241484, acc 1, learning_rate 0.0001
2017-09-29T13:56:33.047369: step 6701, loss 0.0227546, acc 1, learning_rate 0.0001
2017-09-29T13:56:33.230938: step 6702, loss 0.0196733, acc 1, learning_rate 0.0001
2017-09-29T13:56:33.416853: step 6703, loss 0.0191302, acc 1, learning_rate 0.0001
2017-09-29T13:56:33.600262: step 6704, loss 0.0596152, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:33.782435: step 6705, loss 0.00649697, acc 1, learning_rate 0.0001
2017-09-29T13:56:33.964594: step 6706, loss 0.0336556, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:34.153270: step 6707, loss 0.0573271, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:34.340135: step 6708, loss 0.0218014, acc 1, learning_rate 0.0001
2017-09-29T13:56:34.525496: step 6709, loss 0.0125565, acc 1, learning_rate 0.0001
2017-09-29T13:56:34.709121: step 6710, loss 0.0285282, acc 1, learning_rate 0.0001
2017-09-29T13:56:34.903840: step 6711, loss 0.0194929, acc 1, learning_rate 0.0001
2017-09-29T13:56:35.093056: step 6712, loss 0.033471, acc 1, learning_rate 0.0001
2017-09-29T13:56:35.282274: step 6713, loss 0.00501263, acc 1, learning_rate 0.0001
2017-09-29T13:56:35.465885: step 6714, loss 0.0215295, acc 1, learning_rate 0.0001
2017-09-29T13:56:35.652377: step 6715, loss 0.0163709, acc 1, learning_rate 0.0001
2017-09-29T13:56:35.842009: step 6716, loss 0.0253499, acc 1, learning_rate 0.0001
2017-09-29T13:56:36.024256: step 6717, loss 0.0495541, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:36.219156: step 6718, loss 0.0326403, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:36.401451: step 6719, loss 0.004672, acc 1, learning_rate 0.0001
2017-09-29T13:56:36.592107: step 6720, loss 0.0340569, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:56:37.147357: step 6720, loss 0.218712, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6720

2017-09-29T13:56:37.966233: step 6721, loss 0.0439833, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:38.150119: step 6722, loss 0.0782062, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:38.334765: step 6723, loss 0.0884737, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:38.521189: step 6724, loss 0.0156052, acc 1, learning_rate 0.0001
2017-09-29T13:56:38.711924: step 6725, loss 0.0177966, acc 1, learning_rate 0.0001
2017-09-29T13:56:38.900534: step 6726, loss 0.0279393, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:39.082428: step 6727, loss 0.0442907, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:39.271492: step 6728, loss 0.0390828, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:39.460038: step 6729, loss 0.0467033, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:39.660360: step 6730, loss 0.0985474, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:39.865006: step 6731, loss 0.00961559, acc 1, learning_rate 0.0001
2017-09-29T13:56:40.067275: step 6732, loss 0.045573, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:40.280592: step 6733, loss 0.0631796, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:40.471249: step 6734, loss 0.0671888, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:40.655146: step 6735, loss 0.0275743, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:40.840506: step 6736, loss 0.0372563, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:41.027332: step 6737, loss 0.0266833, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:41.211793: step 6738, loss 0.00580784, acc 1, learning_rate 0.0001
2017-09-29T13:56:41.394715: step 6739, loss 0.0178761, acc 1, learning_rate 0.0001
2017-09-29T13:56:41.580263: step 6740, loss 0.0227518, acc 1, learning_rate 0.0001
2017-09-29T13:56:41.763105: step 6741, loss 0.0154279, acc 1, learning_rate 0.0001
2017-09-29T13:56:41.944364: step 6742, loss 0.0385091, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:42.128645: step 6743, loss 0.00759605, acc 1, learning_rate 0.0001
2017-09-29T13:56:42.310094: step 6744, loss 0.0493676, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:42.496307: step 6745, loss 0.0237057, acc 1, learning_rate 0.0001
2017-09-29T13:56:42.678722: step 6746, loss 0.0365242, acc 1, learning_rate 0.0001
2017-09-29T13:56:42.862495: step 6747, loss 0.02723, acc 1, learning_rate 0.0001
2017-09-29T13:56:43.041087: step 6748, loss 0.0516463, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:43.224947: step 6749, loss 0.0845216, acc 0.953125, learning_rate 0.0001
2017-09-29T13:56:43.411721: step 6750, loss 0.0162054, acc 1, learning_rate 0.0001
2017-09-29T13:56:43.604662: step 6751, loss 0.0776527, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:43.786374: step 6752, loss 0.0255826, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:43.965888: step 6753, loss 0.00599573, acc 1, learning_rate 0.0001
2017-09-29T13:56:44.148362: step 6754, loss 0.110608, acc 0.953125, learning_rate 0.0001
2017-09-29T13:56:44.332922: step 6755, loss 0.0313556, acc 1, learning_rate 0.0001
2017-09-29T13:56:44.516656: step 6756, loss 0.0375766, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:44.699872: step 6757, loss 0.0597777, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:44.885855: step 6758, loss 0.0115571, acc 1, learning_rate 0.0001
2017-09-29T13:56:45.069303: step 6759, loss 0.00721535, acc 1, learning_rate 0.0001
2017-09-29T13:56:45.254844: step 6760, loss 0.00365841, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:56:45.808745: step 6760, loss 0.222736, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6760

2017-09-29T13:56:46.427082: step 6761, loss 0.0249196, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:46.576774: step 6762, loss 0.0685322, acc 0.980392, learning_rate 0.0001
2017-09-29T13:56:46.758613: step 6763, loss 0.0771882, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:46.938038: step 6764, loss 0.0377723, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:47.122359: step 6765, loss 0.0290106, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:47.305465: step 6766, loss 0.109052, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:47.496275: step 6767, loss 0.0201579, acc 1, learning_rate 0.0001
2017-09-29T13:56:47.680653: step 6768, loss 0.0305053, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:47.862226: step 6769, loss 0.0363032, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:48.043540: step 6770, loss 0.0703125, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:48.238333: step 6771, loss 0.00365171, acc 1, learning_rate 0.0001
2017-09-29T13:56:48.422164: step 6772, loss 0.0246122, acc 1, learning_rate 0.0001
2017-09-29T13:56:48.613774: step 6773, loss 0.0858239, acc 0.9375, learning_rate 0.0001
2017-09-29T13:56:48.796620: step 6774, loss 0.0220769, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:48.978985: step 6775, loss 0.0187591, acc 1, learning_rate 0.0001
2017-09-29T13:56:49.178775: step 6776, loss 0.0109522, acc 1, learning_rate 0.0001
2017-09-29T13:56:49.369676: step 6777, loss 0.0554376, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:49.578286: step 6778, loss 0.0380245, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:49.761679: step 6779, loss 0.120079, acc 0.9375, learning_rate 0.0001
2017-09-29T13:56:49.939287: step 6780, loss 0.0226217, acc 1, learning_rate 0.0001
2017-09-29T13:56:50.122000: step 6781, loss 0.037177, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:50.309315: step 6782, loss 0.0081685, acc 1, learning_rate 0.0001
2017-09-29T13:56:50.497103: step 6783, loss 0.00613293, acc 1, learning_rate 0.0001
2017-09-29T13:56:50.679208: step 6784, loss 0.0332414, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:50.860945: step 6785, loss 0.0104565, acc 1, learning_rate 0.0001
2017-09-29T13:56:51.044649: step 6786, loss 0.00265088, acc 1, learning_rate 0.0001
2017-09-29T13:56:51.227589: step 6787, loss 0.0266598, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:51.405887: step 6788, loss 0.0791892, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:51.592297: step 6789, loss 0.0199563, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:51.776408: step 6790, loss 0.0343653, acc 1, learning_rate 0.0001
2017-09-29T13:56:51.959593: step 6791, loss 0.0301364, acc 1, learning_rate 0.0001
2017-09-29T13:56:52.144717: step 6792, loss 0.0307457, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:52.337679: step 6793, loss 0.0220492, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:52.526115: step 6794, loss 0.051863, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:52.709358: step 6795, loss 0.0244247, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:52.893576: step 6796, loss 0.0558068, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:53.085732: step 6797, loss 0.00583174, acc 1, learning_rate 0.0001
2017-09-29T13:56:53.269708: step 6798, loss 0.024291, acc 1, learning_rate 0.0001
2017-09-29T13:56:53.456137: step 6799, loss 0.0628537, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:53.644223: step 6800, loss 0.00491038, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:56:54.219262: step 6800, loss 0.225175, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6800

2017-09-29T13:56:54.918487: step 6801, loss 0.0177503, acc 1, learning_rate 0.0001
2017-09-29T13:56:55.101579: step 6802, loss 0.0151423, acc 1, learning_rate 0.0001
2017-09-29T13:56:55.286929: step 6803, loss 0.0218291, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:55.493329: step 6804, loss 0.01414, acc 1, learning_rate 0.0001
2017-09-29T13:56:55.678394: step 6805, loss 0.00413983, acc 1, learning_rate 0.0001
2017-09-29T13:56:55.869153: step 6806, loss 0.0799704, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:56.055958: step 6807, loss 0.0147249, acc 1, learning_rate 0.0001
2017-09-29T13:56:56.246215: step 6808, loss 0.0548302, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:56.433799: step 6809, loss 0.00959935, acc 1, learning_rate 0.0001
2017-09-29T13:56:56.620146: step 6810, loss 0.00790106, acc 1, learning_rate 0.0001
2017-09-29T13:56:56.805579: step 6811, loss 0.0685284, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:56.990059: step 6812, loss 0.0126933, acc 1, learning_rate 0.0001
2017-09-29T13:56:57.173849: step 6813, loss 0.014499, acc 1, learning_rate 0.0001
2017-09-29T13:56:57.356357: step 6814, loss 0.0186834, acc 1, learning_rate 0.0001
2017-09-29T13:56:57.543846: step 6815, loss 0.0246232, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:57.733989: step 6816, loss 0.0339421, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:57.913241: step 6817, loss 0.0373043, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:58.097602: step 6818, loss 0.023137, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:58.278477: step 6819, loss 0.0451391, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:58.475622: step 6820, loss 0.038132, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:58.671042: step 6821, loss 0.0777877, acc 0.96875, learning_rate 0.0001
2017-09-29T13:56:58.857729: step 6822, loss 0.0103631, acc 1, learning_rate 0.0001
2017-09-29T13:56:59.057695: step 6823, loss 0.0251286, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:59.251776: step 6824, loss 0.0182582, acc 1, learning_rate 0.0001
2017-09-29T13:56:59.442530: step 6825, loss 0.0192917, acc 1, learning_rate 0.0001
2017-09-29T13:56:59.626944: step 6826, loss 0.0549311, acc 0.984375, learning_rate 0.0001
2017-09-29T13:56:59.812380: step 6827, loss 0.0181112, acc 1, learning_rate 0.0001
2017-09-29T13:57:00.001454: step 6828, loss 0.0118957, acc 1, learning_rate 0.0001
2017-09-29T13:57:00.192005: step 6829, loss 0.0192014, acc 1, learning_rate 0.0001
2017-09-29T13:57:00.381853: step 6830, loss 0.0184538, acc 1, learning_rate 0.0001
2017-09-29T13:57:00.566012: step 6831, loss 0.0301873, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:00.750593: step 6832, loss 0.00983169, acc 1, learning_rate 0.0001
2017-09-29T13:57:00.955832: step 6833, loss 0.00893974, acc 1, learning_rate 0.0001
2017-09-29T13:57:01.143711: step 6834, loss 0.0499265, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:01.340222: step 6835, loss 0.00893759, acc 1, learning_rate 0.0001
2017-09-29T13:57:01.557554: step 6836, loss 0.0170054, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:01.757715: step 6837, loss 0.0333109, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:01.949321: step 6838, loss 0.00641125, acc 1, learning_rate 0.0001
2017-09-29T13:57:02.145999: step 6839, loss 0.0140851, acc 1, learning_rate 0.0001
2017-09-29T13:57:02.351373: step 6840, loss 0.0762628, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T13:57:02.943066: step 6840, loss 0.226588, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6840

2017-09-29T13:57:03.727981: step 6841, loss 0.0293369, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:03.931050: step 6842, loss 0.0258158, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:04.115438: step 6843, loss 0.0355719, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:04.298222: step 6844, loss 0.0541397, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:04.495410: step 6845, loss 0.0103511, acc 1, learning_rate 0.0001
2017-09-29T13:57:04.679132: step 6846, loss 0.00464451, acc 1, learning_rate 0.0001
2017-09-29T13:57:04.861858: step 6847, loss 0.0629787, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:05.050397: step 6848, loss 0.0204535, acc 1, learning_rate 0.0001
2017-09-29T13:57:05.240295: step 6849, loss 0.0192181, acc 1, learning_rate 0.0001
2017-09-29T13:57:05.442422: step 6850, loss 0.0334083, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:05.632957: step 6851, loss 0.0496558, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:05.815203: step 6852, loss 0.0514909, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:06.016704: step 6853, loss 0.00888433, acc 1, learning_rate 0.0001
2017-09-29T13:57:06.228287: step 6854, loss 0.0198975, acc 1, learning_rate 0.0001
2017-09-29T13:57:06.444416: step 6855, loss 0.0128555, acc 1, learning_rate 0.0001
2017-09-29T13:57:06.640011: step 6856, loss 0.0221057, acc 1, learning_rate 0.0001
2017-09-29T13:57:06.835666: step 6857, loss 0.0309059, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:07.043462: step 6858, loss 0.0855471, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:07.254435: step 6859, loss 0.0391691, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:07.429467: step 6860, loss 0.0544295, acc 1, learning_rate 0.0001
2017-09-29T13:57:07.645091: step 6861, loss 0.087651, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:07.853010: step 6862, loss 0.0232528, acc 1, learning_rate 0.0001
2017-09-29T13:57:08.048523: step 6863, loss 0.00560482, acc 1, learning_rate 0.0001
2017-09-29T13:57:08.250990: step 6864, loss 0.038533, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:08.448479: step 6865, loss 0.0462203, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:08.638567: step 6866, loss 0.0101442, acc 1, learning_rate 0.0001
2017-09-29T13:57:08.833598: step 6867, loss 0.0190036, acc 1, learning_rate 0.0001
2017-09-29T13:57:09.016524: step 6868, loss 0.0113475, acc 1, learning_rate 0.0001
2017-09-29T13:57:09.201881: step 6869, loss 0.00625271, acc 1, learning_rate 0.0001
2017-09-29T13:57:09.385541: step 6870, loss 0.0382214, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:09.573680: step 6871, loss 0.0413574, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:09.769694: step 6872, loss 0.0294423, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:09.983593: step 6873, loss 0.0655075, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:10.199933: step 6874, loss 0.010031, acc 1, learning_rate 0.0001
2017-09-29T13:57:10.426706: step 6875, loss 0.0548171, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:10.647054: step 6876, loss 0.0696579, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:10.876637: step 6877, loss 0.037362, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:11.105909: step 6878, loss 0.00537298, acc 1, learning_rate 0.0001
2017-09-29T13:57:11.336634: step 6879, loss 0.00808585, acc 1, learning_rate 0.0001
2017-09-29T13:57:11.551451: step 6880, loss 0.0307357, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:57:12.198008: step 6880, loss 0.221756, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6880

2017-09-29T13:57:12.898271: step 6881, loss 0.00322346, acc 1, learning_rate 0.0001
2017-09-29T13:57:13.101861: step 6882, loss 0.0357419, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:13.300167: step 6883, loss 0.0188747, acc 1, learning_rate 0.0001
2017-09-29T13:57:13.496723: step 6884, loss 0.0319454, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:13.730777: step 6885, loss 0.0266524, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:13.960049: step 6886, loss 0.016459, acc 1, learning_rate 0.0001
2017-09-29T13:57:14.150085: step 6887, loss 0.143639, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:14.334796: step 6888, loss 0.00841673, acc 1, learning_rate 0.0001
2017-09-29T13:57:14.545140: step 6889, loss 0.0452162, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:14.736614: step 6890, loss 0.0366185, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:14.924272: step 6891, loss 0.0428096, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:15.112101: step 6892, loss 0.0177001, acc 1, learning_rate 0.0001
2017-09-29T13:57:15.299255: step 6893, loss 0.0477782, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:15.494089: step 6894, loss 0.0370151, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:15.676340: step 6895, loss 0.00198773, acc 1, learning_rate 0.0001
2017-09-29T13:57:15.867946: step 6896, loss 0.00483347, acc 1, learning_rate 0.0001
2017-09-29T13:57:16.051616: step 6897, loss 0.00713693, acc 1, learning_rate 0.0001
2017-09-29T13:57:16.238996: step 6898, loss 0.0492483, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:16.432871: step 6899, loss 0.030962, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:16.619921: step 6900, loss 0.0431489, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:16.810987: step 6901, loss 0.00917604, acc 1, learning_rate 0.0001
2017-09-29T13:57:16.991333: step 6902, loss 0.0590741, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:17.170710: step 6903, loss 0.0682677, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:17.350039: step 6904, loss 0.0020879, acc 1, learning_rate 0.0001
2017-09-29T13:57:17.531487: step 6905, loss 0.0201804, acc 1, learning_rate 0.0001
2017-09-29T13:57:17.714003: step 6906, loss 0.0227046, acc 1, learning_rate 0.0001
2017-09-29T13:57:17.896330: step 6907, loss 0.0112626, acc 1, learning_rate 0.0001
2017-09-29T13:57:18.078985: step 6908, loss 0.029025, acc 1, learning_rate 0.0001
2017-09-29T13:57:18.261315: step 6909, loss 0.037068, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:18.456812: step 6910, loss 0.076511, acc 0.953125, learning_rate 0.0001
2017-09-29T13:57:18.645945: step 6911, loss 0.0372635, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:18.826913: step 6912, loss 0.00632637, acc 1, learning_rate 0.0001
2017-09-29T13:57:19.006966: step 6913, loss 0.0349965, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:19.187551: step 6914, loss 0.0307485, acc 1, learning_rate 0.0001
2017-09-29T13:57:19.372341: step 6915, loss 0.00510721, acc 1, learning_rate 0.0001
2017-09-29T13:57:19.559189: step 6916, loss 0.0724631, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:19.740723: step 6917, loss 0.0542325, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:19.921566: step 6918, loss 0.0402311, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:20.101698: step 6919, loss 0.0407273, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:20.284519: step 6920, loss 0.12764, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T13:57:20.850657: step 6920, loss 0.224273, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6920

2017-09-29T13:57:21.574514: step 6921, loss 0.0414195, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:21.757445: step 6922, loss 0.0130347, acc 1, learning_rate 0.0001
2017-09-29T13:57:21.956364: step 6923, loss 0.0322958, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:22.200464: step 6924, loss 0.0748169, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:22.424662: step 6925, loss 0.0290644, acc 1, learning_rate 0.0001
2017-09-29T13:57:22.604134: step 6926, loss 0.00902447, acc 1, learning_rate 0.0001
2017-09-29T13:57:22.793052: step 6927, loss 0.047002, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:22.976301: step 6928, loss 0.034895, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:23.157188: step 6929, loss 0.0154383, acc 1, learning_rate 0.0001
2017-09-29T13:57:23.338807: step 6930, loss 0.0177386, acc 1, learning_rate 0.0001
2017-09-29T13:57:23.533653: step 6931, loss 0.0367271, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:23.753342: step 6932, loss 0.0422525, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:23.949709: step 6933, loss 0.03241, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:24.130651: step 6934, loss 0.0273239, acc 1, learning_rate 0.0001
2017-09-29T13:57:24.314603: step 6935, loss 0.0365181, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:24.524216: step 6936, loss 0.0451639, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:24.720173: step 6937, loss 0.0395727, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:24.909133: step 6938, loss 0.0509798, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:25.094886: step 6939, loss 0.0378371, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:25.279207: step 6940, loss 0.0463816, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:25.463828: step 6941, loss 0.0123527, acc 1, learning_rate 0.0001
2017-09-29T13:57:25.653103: step 6942, loss 0.00590391, acc 1, learning_rate 0.0001
2017-09-29T13:57:25.843735: step 6943, loss 0.0638052, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:26.025457: step 6944, loss 0.00716398, acc 1, learning_rate 0.0001
2017-09-29T13:57:26.205255: step 6945, loss 0.00643782, acc 1, learning_rate 0.0001
2017-09-29T13:57:26.396197: step 6946, loss 0.0339678, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:26.582236: step 6947, loss 0.017708, acc 1, learning_rate 0.0001
2017-09-29T13:57:26.771669: step 6948, loss 0.0208049, acc 1, learning_rate 0.0001
2017-09-29T13:57:26.962066: step 6949, loss 0.0103292, acc 1, learning_rate 0.0001
2017-09-29T13:57:27.165974: step 6950, loss 0.0356997, acc 1, learning_rate 0.0001
2017-09-29T13:57:27.353158: step 6951, loss 0.0924651, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:27.545028: step 6952, loss 0.105333, acc 0.953125, learning_rate 0.0001
2017-09-29T13:57:27.730376: step 6953, loss 0.0147991, acc 1, learning_rate 0.0001
2017-09-29T13:57:27.912077: step 6954, loss 0.0234318, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:28.102237: step 6955, loss 0.0497099, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:28.295500: step 6956, loss 0.0147476, acc 1, learning_rate 0.0001
2017-09-29T13:57:28.476918: step 6957, loss 0.0197655, acc 1, learning_rate 0.0001
2017-09-29T13:57:28.626327: step 6958, loss 0.037527, acc 0.980392, learning_rate 0.0001
2017-09-29T13:57:28.807736: step 6959, loss 0.00777932, acc 1, learning_rate 0.0001
2017-09-29T13:57:28.987654: step 6960, loss 0.0514596, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:57:29.582162: step 6960, loss 0.222069, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-6960

2017-09-29T13:57:30.478953: step 6961, loss 0.0248518, acc 1, learning_rate 0.0001
2017-09-29T13:57:30.665772: step 6962, loss 0.0390694, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:30.847755: step 6963, loss 0.0143494, acc 1, learning_rate 0.0001
2017-09-29T13:57:31.029629: step 6964, loss 0.015891, acc 1, learning_rate 0.0001
2017-09-29T13:57:31.211766: step 6965, loss 0.0343047, acc 1, learning_rate 0.0001
2017-09-29T13:57:31.406977: step 6966, loss 0.111217, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:31.593208: step 6967, loss 0.00991362, acc 1, learning_rate 0.0001
2017-09-29T13:57:31.793693: step 6968, loss 0.0218974, acc 1, learning_rate 0.0001
2017-09-29T13:57:32.018653: step 6969, loss 0.0167446, acc 1, learning_rate 0.0001
2017-09-29T13:57:32.199812: step 6970, loss 0.0073799, acc 1, learning_rate 0.0001
2017-09-29T13:57:32.382909: step 6971, loss 0.011899, acc 1, learning_rate 0.0001
2017-09-29T13:57:32.573980: step 6972, loss 0.0351517, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:32.763161: step 6973, loss 0.0234235, acc 1, learning_rate 0.0001
2017-09-29T13:57:32.944275: step 6974, loss 0.0187442, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:33.127099: step 6975, loss 0.0477157, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:33.309516: step 6976, loss 0.00919607, acc 1, learning_rate 0.0001
2017-09-29T13:57:33.493498: step 6977, loss 0.00560343, acc 1, learning_rate 0.0001
2017-09-29T13:57:33.676644: step 6978, loss 0.0274917, acc 1, learning_rate 0.0001
2017-09-29T13:57:33.857724: step 6979, loss 0.0225096, acc 1, learning_rate 0.0001
2017-09-29T13:57:34.040557: step 6980, loss 0.0289743, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:34.223280: step 6981, loss 0.0240039, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:34.415236: step 6982, loss 0.0480377, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:34.607584: step 6983, loss 0.0679574, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:34.789538: step 6984, loss 0.0541665, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:34.969367: step 6985, loss 0.0243449, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:35.157373: step 6986, loss 0.0756643, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:35.342325: step 6987, loss 0.0484379, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:35.543706: step 6988, loss 0.0336363, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:35.753488: step 6989, loss 0.0405252, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:35.958366: step 6990, loss 0.0598883, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:36.159829: step 6991, loss 0.0275443, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:36.344752: step 6992, loss 0.00833295, acc 1, learning_rate 0.0001
2017-09-29T13:57:36.539528: step 6993, loss 0.0440649, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:36.726135: step 6994, loss 0.00586316, acc 1, learning_rate 0.0001
2017-09-29T13:57:36.913877: step 6995, loss 0.0605716, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:37.097699: step 6996, loss 0.0659766, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:37.294076: step 6997, loss 0.00585148, acc 1, learning_rate 0.0001
2017-09-29T13:57:37.477809: step 6998, loss 0.00589788, acc 1, learning_rate 0.0001
2017-09-29T13:57:37.671930: step 6999, loss 0.0594684, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:37.860138: step 7000, loss 0.0249512, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:57:38.407417: step 7000, loss 0.222382, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7000

2017-09-29T13:57:39.022795: step 7001, loss 0.0888768, acc 0.953125, learning_rate 0.0001
2017-09-29T13:57:39.204090: step 7002, loss 0.0128951, acc 1, learning_rate 0.0001
2017-09-29T13:57:39.389051: step 7003, loss 0.0871958, acc 0.953125, learning_rate 0.0001
2017-09-29T13:57:39.571999: step 7004, loss 0.0195127, acc 1, learning_rate 0.0001
2017-09-29T13:57:39.756974: step 7005, loss 0.00754724, acc 1, learning_rate 0.0001
2017-09-29T13:57:39.938653: step 7006, loss 0.0127974, acc 1, learning_rate 0.0001
2017-09-29T13:57:40.121920: step 7007, loss 0.0612442, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:40.314206: step 7008, loss 0.010714, acc 1, learning_rate 0.0001
2017-09-29T13:57:40.502781: step 7009, loss 0.00778159, acc 1, learning_rate 0.0001
2017-09-29T13:57:40.688332: step 7010, loss 0.0679112, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:40.916090: step 7011, loss 0.0312497, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:41.147240: step 7012, loss 0.0184717, acc 1, learning_rate 0.0001
2017-09-29T13:57:41.333177: step 7013, loss 0.0173092, acc 1, learning_rate 0.0001
2017-09-29T13:57:41.517077: step 7014, loss 0.044633, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:41.708170: step 7015, loss 0.0134813, acc 1, learning_rate 0.0001
2017-09-29T13:57:41.888830: step 7016, loss 0.0276277, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:42.070223: step 7017, loss 0.0424301, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:42.257249: step 7018, loss 0.00772517, acc 1, learning_rate 0.0001
2017-09-29T13:57:42.451253: step 7019, loss 0.0400405, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:42.630513: step 7020, loss 0.0368774, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:42.814408: step 7021, loss 0.0256507, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:42.998291: step 7022, loss 0.0587222, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:43.185803: step 7023, loss 0.0115422, acc 1, learning_rate 0.0001
2017-09-29T13:57:43.369409: step 7024, loss 0.0142784, acc 1, learning_rate 0.0001
2017-09-29T13:57:43.556450: step 7025, loss 0.0127991, acc 1, learning_rate 0.0001
2017-09-29T13:57:43.736833: step 7026, loss 0.0224371, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:43.921886: step 7027, loss 0.033339, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:44.106396: step 7028, loss 0.040924, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:44.291274: step 7029, loss 0.00680824, acc 1, learning_rate 0.0001
2017-09-29T13:57:44.474760: step 7030, loss 0.0677423, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:44.657514: step 7031, loss 0.00817362, acc 1, learning_rate 0.0001
2017-09-29T13:57:44.847683: step 7032, loss 0.0162821, acc 1, learning_rate 0.0001
2017-09-29T13:57:45.027186: step 7033, loss 0.0186697, acc 1, learning_rate 0.0001
2017-09-29T13:57:45.209164: step 7034, loss 0.0286731, acc 1, learning_rate 0.0001
2017-09-29T13:57:45.393665: step 7035, loss 0.0272566, acc 1, learning_rate 0.0001
2017-09-29T13:57:45.590499: step 7036, loss 0.0616064, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:45.799579: step 7037, loss 0.0227133, acc 1, learning_rate 0.0001
2017-09-29T13:57:46.005868: step 7038, loss 0.0303226, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:46.201013: step 7039, loss 0.00458115, acc 1, learning_rate 0.0001
2017-09-29T13:57:46.391181: step 7040, loss 0.00328663, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:57:46.971381: step 7040, loss 0.226814, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7040

2017-09-29T13:57:47.693527: step 7041, loss 0.0154916, acc 1, learning_rate 0.0001
2017-09-29T13:57:47.900163: step 7042, loss 0.0419793, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:48.103671: step 7043, loss 0.0423977, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:48.288129: step 7044, loss 0.0518948, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:48.479588: step 7045, loss 0.0429239, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:48.662766: step 7046, loss 0.0766116, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:48.855809: step 7047, loss 0.0287152, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:49.039152: step 7048, loss 0.031129, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:49.224196: step 7049, loss 0.0297289, acc 1, learning_rate 0.0001
2017-09-29T13:57:49.409135: step 7050, loss 0.0808089, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:49.590886: step 7051, loss 0.00698852, acc 1, learning_rate 0.0001
2017-09-29T13:57:49.776862: step 7052, loss 0.00788933, acc 1, learning_rate 0.0001
2017-09-29T13:57:49.958688: step 7053, loss 0.0325058, acc 1, learning_rate 0.0001
2017-09-29T13:57:50.140088: step 7054, loss 0.031714, acc 1, learning_rate 0.0001
2017-09-29T13:57:50.319406: step 7055, loss 0.00988413, acc 1, learning_rate 0.0001
2017-09-29T13:57:50.490216: step 7056, loss 0.15208, acc 0.980392, learning_rate 0.0001
2017-09-29T13:57:50.713536: step 7057, loss 0.0234706, acc 1, learning_rate 0.0001
2017-09-29T13:57:50.903985: step 7058, loss 0.0567838, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:51.090256: step 7059, loss 0.00954034, acc 1, learning_rate 0.0001
2017-09-29T13:57:51.275213: step 7060, loss 0.0108193, acc 1, learning_rate 0.0001
2017-09-29T13:57:51.468995: step 7061, loss 0.0378528, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:51.651584: step 7062, loss 0.0550103, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:51.837871: step 7063, loss 0.0216283, acc 1, learning_rate 0.0001
2017-09-29T13:57:52.026009: step 7064, loss 0.00740901, acc 1, learning_rate 0.0001
2017-09-29T13:57:52.216968: step 7065, loss 0.00499117, acc 1, learning_rate 0.0001
2017-09-29T13:57:52.418711: step 7066, loss 0.0111558, acc 1, learning_rate 0.0001
2017-09-29T13:57:52.614322: step 7067, loss 0.0217327, acc 1, learning_rate 0.0001
2017-09-29T13:57:52.808497: step 7068, loss 0.0175969, acc 1, learning_rate 0.0001
2017-09-29T13:57:52.999562: step 7069, loss 0.0245523, acc 1, learning_rate 0.0001
2017-09-29T13:57:53.187205: step 7070, loss 0.021349, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:53.389742: step 7071, loss 0.0166121, acc 1, learning_rate 0.0001
2017-09-29T13:57:53.602660: step 7072, loss 0.052354, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:53.801740: step 7073, loss 0.0137949, acc 1, learning_rate 0.0001
2017-09-29T13:57:54.000121: step 7074, loss 0.0537034, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:54.207563: step 7075, loss 0.0112477, acc 1, learning_rate 0.0001
2017-09-29T13:57:54.411516: step 7076, loss 0.0148791, acc 1, learning_rate 0.0001
2017-09-29T13:57:54.613463: step 7077, loss 0.0669993, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:54.802862: step 7078, loss 0.00640322, acc 1, learning_rate 0.0001
2017-09-29T13:57:54.993230: step 7079, loss 0.0635269, acc 0.96875, learning_rate 0.0001
2017-09-29T13:57:55.178517: step 7080, loss 0.00847428, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:57:55.765384: step 7080, loss 0.221924, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7080

2017-09-29T13:57:56.652128: step 7081, loss 0.00631174, acc 1, learning_rate 0.0001
2017-09-29T13:57:56.849748: step 7082, loss 0.0257499, acc 1, learning_rate 0.0001
2017-09-29T13:57:57.070662: step 7083, loss 0.00799838, acc 1, learning_rate 0.0001
2017-09-29T13:57:57.299169: step 7084, loss 0.00366356, acc 1, learning_rate 0.0001
2017-09-29T13:57:57.503968: step 7085, loss 0.0402477, acc 1, learning_rate 0.0001
2017-09-29T13:57:57.712207: step 7086, loss 0.0331986, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:57.907224: step 7087, loss 0.0075928, acc 1, learning_rate 0.0001
2017-09-29T13:57:58.099384: step 7088, loss 0.0591181, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:58.289988: step 7089, loss 0.0132661, acc 1, learning_rate 0.0001
2017-09-29T13:57:58.474482: step 7090, loss 0.0101868, acc 1, learning_rate 0.0001
2017-09-29T13:57:58.663912: step 7091, loss 0.01693, acc 1, learning_rate 0.0001
2017-09-29T13:57:58.867232: step 7092, loss 0.0862613, acc 0.953125, learning_rate 0.0001
2017-09-29T13:57:59.051555: step 7093, loss 0.00512267, acc 1, learning_rate 0.0001
2017-09-29T13:57:59.236976: step 7094, loss 0.0504958, acc 0.984375, learning_rate 0.0001
2017-09-29T13:57:59.424516: step 7095, loss 0.00794774, acc 1, learning_rate 0.0001
2017-09-29T13:57:59.608099: step 7096, loss 0.0233073, acc 1, learning_rate 0.0001
2017-09-29T13:57:59.791799: step 7097, loss 0.0071358, acc 1, learning_rate 0.0001
2017-09-29T13:57:59.972589: step 7098, loss 0.00487814, acc 1, learning_rate 0.0001
2017-09-29T13:58:00.152747: step 7099, loss 0.066922, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:00.336142: step 7100, loss 0.0451902, acc 1, learning_rate 0.0001
2017-09-29T13:58:00.517785: step 7101, loss 0.0329751, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:00.704745: step 7102, loss 0.0471082, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:00.927635: step 7103, loss 0.00976934, acc 1, learning_rate 0.0001
2017-09-29T13:58:01.144876: step 7104, loss 0.013268, acc 1, learning_rate 0.0001
2017-09-29T13:58:01.331873: step 7105, loss 0.0603135, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:01.518015: step 7106, loss 0.0151207, acc 1, learning_rate 0.0001
2017-09-29T13:58:01.704004: step 7107, loss 0.010016, acc 1, learning_rate 0.0001
2017-09-29T13:58:01.894978: step 7108, loss 0.0364311, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:02.120991: step 7109, loss 0.0679117, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:02.327906: step 7110, loss 0.0528527, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:02.511394: step 7111, loss 0.0379277, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:02.694909: step 7112, loss 0.0813798, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:02.880767: step 7113, loss 0.0578739, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:03.063182: step 7114, loss 0.0306848, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:03.253627: step 7115, loss 0.0049374, acc 1, learning_rate 0.0001
2017-09-29T13:58:03.437586: step 7116, loss 0.0338767, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:03.618495: step 7117, loss 0.031672, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:03.809690: step 7118, loss 0.0340171, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:04.003079: step 7119, loss 0.00886001, acc 1, learning_rate 0.0001
2017-09-29T13:58:04.225128: step 7120, loss 0.0344393, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:58:04.795067: step 7120, loss 0.223218, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7120

2017-09-29T13:58:05.445874: step 7121, loss 0.0062667, acc 1, learning_rate 0.0001
2017-09-29T13:58:05.646202: step 7122, loss 0.0410962, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:05.840111: step 7123, loss 0.00769719, acc 1, learning_rate 0.0001
2017-09-29T13:58:06.039100: step 7124, loss 0.0229562, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:06.245647: step 7125, loss 0.0679047, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:06.445691: step 7126, loss 0.0328747, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:06.654154: step 7127, loss 0.0346342, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:06.851885: step 7128, loss 0.0102061, acc 1, learning_rate 0.0001
2017-09-29T13:58:07.065838: step 7129, loss 0.0802951, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:07.275396: step 7130, loss 0.0166178, acc 1, learning_rate 0.0001
2017-09-29T13:58:07.473967: step 7131, loss 0.00866175, acc 1, learning_rate 0.0001
2017-09-29T13:58:07.656703: step 7132, loss 0.0474046, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:07.861192: step 7133, loss 0.0545646, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:08.055414: step 7134, loss 0.0398108, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:08.245332: step 7135, loss 0.0231736, acc 1, learning_rate 0.0001
2017-09-29T13:58:08.430490: step 7136, loss 0.0105433, acc 1, learning_rate 0.0001
2017-09-29T13:58:08.614249: step 7137, loss 0.0305795, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:08.813238: step 7138, loss 0.0274641, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:09.028185: step 7139, loss 0.0169091, acc 1, learning_rate 0.0001
2017-09-29T13:58:09.257296: step 7140, loss 0.0100855, acc 1, learning_rate 0.0001
2017-09-29T13:58:09.475289: step 7141, loss 0.0174941, acc 1, learning_rate 0.0001
2017-09-29T13:58:09.691758: step 7142, loss 0.0174824, acc 1, learning_rate 0.0001
2017-09-29T13:58:09.926018: step 7143, loss 0.0403263, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:10.157779: step 7144, loss 0.0411329, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:10.411537: step 7145, loss 0.0705727, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:10.641574: step 7146, loss 0.0681162, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:10.871454: step 7147, loss 0.0282565, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:11.091274: step 7148, loss 0.0504241, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:11.313953: step 7149, loss 0.00311009, acc 1, learning_rate 0.0001
2017-09-29T13:58:11.515414: step 7150, loss 0.0674684, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:11.709084: step 7151, loss 0.0510755, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:11.898873: step 7152, loss 0.0101145, acc 1, learning_rate 0.0001
2017-09-29T13:58:12.088237: step 7153, loss 0.00871295, acc 1, learning_rate 0.0001
2017-09-29T13:58:12.246019: step 7154, loss 0.0287349, acc 0.980392, learning_rate 0.0001
2017-09-29T13:58:12.436789: step 7155, loss 0.0078817, acc 1, learning_rate 0.0001
2017-09-29T13:58:12.621930: step 7156, loss 0.0142099, acc 1, learning_rate 0.0001
2017-09-29T13:58:12.813128: step 7157, loss 0.0383842, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:12.992453: step 7158, loss 0.0328066, acc 1, learning_rate 0.0001
2017-09-29T13:58:13.184851: step 7159, loss 0.037412, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:13.369732: step 7160, loss 0.00971601, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:58:13.919443: step 7160, loss 0.224342, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7160

2017-09-29T13:58:14.624072: step 7161, loss 0.0339194, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:14.810995: step 7162, loss 0.0513376, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:15.000311: step 7163, loss 0.0138945, acc 1, learning_rate 0.0001
2017-09-29T13:58:15.181981: step 7164, loss 0.0100305, acc 1, learning_rate 0.0001
2017-09-29T13:58:15.363246: step 7165, loss 0.00327969, acc 1, learning_rate 0.0001
2017-09-29T13:58:15.547245: step 7166, loss 0.0223487, acc 1, learning_rate 0.0001
2017-09-29T13:58:15.737936: step 7167, loss 0.0156432, acc 1, learning_rate 0.0001
2017-09-29T13:58:15.955686: step 7168, loss 0.0379417, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:16.142243: step 7169, loss 0.0168445, acc 1, learning_rate 0.0001
2017-09-29T13:58:16.328770: step 7170, loss 0.0505221, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:16.533740: step 7171, loss 0.0187802, acc 1, learning_rate 0.0001
2017-09-29T13:58:16.728988: step 7172, loss 0.0239057, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:16.920961: step 7173, loss 0.0825403, acc 0.953125, learning_rate 0.0001
2017-09-29T13:58:17.119430: step 7174, loss 0.0140883, acc 1, learning_rate 0.0001
2017-09-29T13:58:17.316220: step 7175, loss 0.0664922, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:17.521035: step 7176, loss 0.0254395, acc 1, learning_rate 0.0001
2017-09-29T13:58:17.732792: step 7177, loss 0.0715648, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:17.918550: step 7178, loss 0.0400211, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:18.100460: step 7179, loss 0.017668, acc 1, learning_rate 0.0001
2017-09-29T13:58:18.286911: step 7180, loss 0.00736451, acc 1, learning_rate 0.0001
2017-09-29T13:58:18.476514: step 7181, loss 0.0127437, acc 1, learning_rate 0.0001
2017-09-29T13:58:18.682498: step 7182, loss 0.0135252, acc 1, learning_rate 0.0001
2017-09-29T13:58:18.900386: step 7183, loss 0.0471078, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:19.108367: step 7184, loss 0.0400322, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:19.302793: step 7185, loss 0.0629168, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:19.489468: step 7186, loss 0.0248237, acc 1, learning_rate 0.0001
2017-09-29T13:58:19.675050: step 7187, loss 0.0438445, acc 1, learning_rate 0.0001
2017-09-29T13:58:19.856133: step 7188, loss 0.0119126, acc 1, learning_rate 0.0001
2017-09-29T13:58:20.076421: step 7189, loss 0.0172221, acc 1, learning_rate 0.0001
2017-09-29T13:58:20.292670: step 7190, loss 0.0132786, acc 1, learning_rate 0.0001
2017-09-29T13:58:20.484215: step 7191, loss 0.0145463, acc 1, learning_rate 0.0001
2017-09-29T13:58:20.679253: step 7192, loss 0.0257461, acc 1, learning_rate 0.0001
2017-09-29T13:58:20.883961: step 7193, loss 0.0332758, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:21.106838: step 7194, loss 0.0685374, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:21.295019: step 7195, loss 0.0650427, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:21.482872: step 7196, loss 0.0503519, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:21.687952: step 7197, loss 0.0709239, acc 0.953125, learning_rate 0.0001
2017-09-29T13:58:21.875649: step 7198, loss 0.0357188, acc 1, learning_rate 0.0001
2017-09-29T13:58:22.057496: step 7199, loss 0.0562713, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:22.241891: step 7200, loss 0.0141255, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:58:22.810907: step 7200, loss 0.224935, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7200

2017-09-29T13:58:23.512554: step 7201, loss 0.0105667, acc 1, learning_rate 0.0001
2017-09-29T13:58:23.697908: step 7202, loss 0.0164815, acc 1, learning_rate 0.0001
2017-09-29T13:58:23.922032: step 7203, loss 0.0204031, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:24.119620: step 7204, loss 0.00849549, acc 1, learning_rate 0.0001
2017-09-29T13:58:24.301850: step 7205, loss 0.0456433, acc 1, learning_rate 0.0001
2017-09-29T13:58:24.504021: step 7206, loss 0.021322, acc 1, learning_rate 0.0001
2017-09-29T13:58:24.688015: step 7207, loss 0.0355863, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:24.873683: step 7208, loss 0.0413639, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:25.069324: step 7209, loss 0.0259061, acc 1, learning_rate 0.0001
2017-09-29T13:58:25.249877: step 7210, loss 0.00583258, acc 1, learning_rate 0.0001
2017-09-29T13:58:25.476166: step 7211, loss 0.0138949, acc 1, learning_rate 0.0001
2017-09-29T13:58:25.698234: step 7212, loss 0.00398943, acc 1, learning_rate 0.0001
2017-09-29T13:58:25.887331: step 7213, loss 0.110638, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:26.074132: step 7214, loss 0.0145205, acc 1, learning_rate 0.0001
2017-09-29T13:58:26.257235: step 7215, loss 0.0805971, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:26.454888: step 7216, loss 0.0281782, acc 1, learning_rate 0.0001
2017-09-29T13:58:26.646975: step 7217, loss 0.0184163, acc 1, learning_rate 0.0001
2017-09-29T13:58:26.828375: step 7218, loss 0.0555832, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:27.012804: step 7219, loss 0.0589608, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:27.196295: step 7220, loss 0.0133442, acc 1, learning_rate 0.0001
2017-09-29T13:58:27.400587: step 7221, loss 0.0721031, acc 0.953125, learning_rate 0.0001
2017-09-29T13:58:27.589576: step 7222, loss 0.0717848, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:27.769839: step 7223, loss 0.081263, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:27.949788: step 7224, loss 0.00721745, acc 1, learning_rate 0.0001
2017-09-29T13:58:28.131844: step 7225, loss 0.00554615, acc 1, learning_rate 0.0001
2017-09-29T13:58:28.313916: step 7226, loss 0.0225793, acc 1, learning_rate 0.0001
2017-09-29T13:58:28.510232: step 7227, loss 0.0552226, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:28.694704: step 7228, loss 0.0528728, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:28.875866: step 7229, loss 0.00885754, acc 1, learning_rate 0.0001
2017-09-29T13:58:29.060518: step 7230, loss 0.0158105, acc 1, learning_rate 0.0001
2017-09-29T13:58:29.245281: step 7231, loss 0.00839702, acc 1, learning_rate 0.0001
2017-09-29T13:58:29.431764: step 7232, loss 0.0132719, acc 1, learning_rate 0.0001
2017-09-29T13:58:29.620148: step 7233, loss 0.0178652, acc 1, learning_rate 0.0001
2017-09-29T13:58:29.806031: step 7234, loss 0.0111184, acc 1, learning_rate 0.0001
2017-09-29T13:58:29.987427: step 7235, loss 0.00553803, acc 1, learning_rate 0.0001
2017-09-29T13:58:30.188109: step 7236, loss 0.0183187, acc 1, learning_rate 0.0001
2017-09-29T13:58:30.393380: step 7237, loss 0.0075336, acc 1, learning_rate 0.0001
2017-09-29T13:58:30.586327: step 7238, loss 0.017985, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:30.768435: step 7239, loss 0.0387805, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:30.952856: step 7240, loss 0.00665813, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:58:31.509693: step 7240, loss 0.224757, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7240

2017-09-29T13:58:32.286810: step 7241, loss 0.0104339, acc 1, learning_rate 0.0001
2017-09-29T13:58:32.475734: step 7242, loss 0.0271303, acc 1, learning_rate 0.0001
2017-09-29T13:58:32.697899: step 7243, loss 0.0175053, acc 1, learning_rate 0.0001
2017-09-29T13:58:32.912325: step 7244, loss 0.0654488, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:33.101310: step 7245, loss 0.0313071, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:33.290146: step 7246, loss 0.0147439, acc 1, learning_rate 0.0001
2017-09-29T13:58:33.489323: step 7247, loss 0.0572747, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:33.696195: step 7248, loss 0.039587, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:33.894179: step 7249, loss 0.058324, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:34.091402: step 7250, loss 0.0387566, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:34.300805: step 7251, loss 0.040644, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:34.482370: step 7252, loss 0.0362399, acc 1, learning_rate 0.0001
2017-09-29T13:58:34.680864: step 7253, loss 0.035682, acc 1, learning_rate 0.0001
2017-09-29T13:58:34.898958: step 7254, loss 0.0427479, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:35.111397: step 7255, loss 0.00783807, acc 1, learning_rate 0.0001
2017-09-29T13:58:35.293974: step 7256, loss 0.0257979, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:35.478298: step 7257, loss 0.0293954, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:35.660138: step 7258, loss 0.0256852, acc 1, learning_rate 0.0001
2017-09-29T13:58:35.841059: step 7259, loss 0.0181406, acc 1, learning_rate 0.0001
2017-09-29T13:58:36.031297: step 7260, loss 0.00943142, acc 1, learning_rate 0.0001
2017-09-29T13:58:36.224690: step 7261, loss 0.039779, acc 1, learning_rate 0.0001
2017-09-29T13:58:36.410363: step 7262, loss 0.024782, acc 1, learning_rate 0.0001
2017-09-29T13:58:36.591215: step 7263, loss 0.00894141, acc 1, learning_rate 0.0001
2017-09-29T13:58:36.776554: step 7264, loss 0.0128853, acc 1, learning_rate 0.0001
2017-09-29T13:58:36.964840: step 7265, loss 0.0401078, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:37.155404: step 7266, loss 0.00564616, acc 1, learning_rate 0.0001
2017-09-29T13:58:37.341364: step 7267, loss 0.00892777, acc 1, learning_rate 0.0001
2017-09-29T13:58:37.537083: step 7268, loss 0.00997099, acc 1, learning_rate 0.0001
2017-09-29T13:58:37.725803: step 7269, loss 0.0317503, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:37.910482: step 7270, loss 0.0654139, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:38.105118: step 7271, loss 0.0677904, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:38.296537: step 7272, loss 0.0256845, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:38.485049: step 7273, loss 0.0199949, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:38.675654: step 7274, loss 0.0340163, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:38.858897: step 7275, loss 0.044767, acc 1, learning_rate 0.0001
2017-09-29T13:58:39.057969: step 7276, loss 0.0159855, acc 1, learning_rate 0.0001
2017-09-29T13:58:39.241154: step 7277, loss 0.00798948, acc 1, learning_rate 0.0001
2017-09-29T13:58:39.430279: step 7278, loss 0.0636572, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:39.613043: step 7279, loss 0.0320645, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:39.840409: step 7280, loss 0.0120925, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:58:40.410743: step 7280, loss 0.225163, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7280

2017-09-29T13:58:41.079436: step 7281, loss 0.023284, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:41.273374: step 7282, loss 0.00713103, acc 1, learning_rate 0.0001
2017-09-29T13:58:41.478167: step 7283, loss 0.0178209, acc 1, learning_rate 0.0001
2017-09-29T13:58:41.677253: step 7284, loss 0.0257688, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:41.869491: step 7285, loss 0.0162093, acc 1, learning_rate 0.0001
2017-09-29T13:58:42.060908: step 7286, loss 0.0722457, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:42.266598: step 7287, loss 0.0177965, acc 1, learning_rate 0.0001
2017-09-29T13:58:42.466008: step 7288, loss 0.0565424, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:42.656156: step 7289, loss 0.0279031, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:42.849713: step 7290, loss 0.0208911, acc 1, learning_rate 0.0001
2017-09-29T13:58:43.044796: step 7291, loss 0.0136737, acc 1, learning_rate 0.0001
2017-09-29T13:58:43.238875: step 7292, loss 0.0209428, acc 1, learning_rate 0.0001
2017-09-29T13:58:43.424677: step 7293, loss 0.00953347, acc 1, learning_rate 0.0001
2017-09-29T13:58:43.606792: step 7294, loss 0.0425417, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:43.786571: step 7295, loss 0.00860133, acc 1, learning_rate 0.0001
2017-09-29T13:58:43.969606: step 7296, loss 0.0293942, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:44.156667: step 7297, loss 0.00833124, acc 1, learning_rate 0.0001
2017-09-29T13:58:44.341903: step 7298, loss 0.0200072, acc 1, learning_rate 0.0001
2017-09-29T13:58:44.540455: step 7299, loss 0.0478947, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:44.734293: step 7300, loss 0.0142676, acc 1, learning_rate 0.0001
2017-09-29T13:58:44.917689: step 7301, loss 0.0409677, acc 1, learning_rate 0.0001
2017-09-29T13:58:45.114006: step 7302, loss 0.121443, acc 0.953125, learning_rate 0.0001
2017-09-29T13:58:45.319000: step 7303, loss 0.0140238, acc 1, learning_rate 0.0001
2017-09-29T13:58:45.513393: step 7304, loss 0.0125705, acc 1, learning_rate 0.0001
2017-09-29T13:58:45.697923: step 7305, loss 0.0801812, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:45.881436: step 7306, loss 0.0822753, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:46.066170: step 7307, loss 0.0437317, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:46.249701: step 7308, loss 0.0350841, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:46.444289: step 7309, loss 0.0318426, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:46.634381: step 7310, loss 0.0471901, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:46.814393: step 7311, loss 0.0533208, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:46.998292: step 7312, loss 0.0129469, acc 1, learning_rate 0.0001
2017-09-29T13:58:47.181257: step 7313, loss 0.0487378, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:47.362838: step 7314, loss 0.0232501, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:47.553662: step 7315, loss 0.0058954, acc 1, learning_rate 0.0001
2017-09-29T13:58:47.737658: step 7316, loss 0.0218308, acc 1, learning_rate 0.0001
2017-09-29T13:58:47.919795: step 7317, loss 0.0543779, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:48.102592: step 7318, loss 0.0100443, acc 1, learning_rate 0.0001
2017-09-29T13:58:48.284788: step 7319, loss 0.0498201, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:48.476255: step 7320, loss 0.0167951, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:58:49.105859: step 7320, loss 0.220565, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7320

2017-09-29T13:58:49.821162: step 7321, loss 0.0258543, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:50.023582: step 7322, loss 0.0565756, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:50.232204: step 7323, loss 0.0334094, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:50.432813: step 7324, loss 0.01368, acc 1, learning_rate 0.0001
2017-09-29T13:58:50.620325: step 7325, loss 0.0103866, acc 1, learning_rate 0.0001
2017-09-29T13:58:50.802743: step 7326, loss 0.0807562, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:50.981147: step 7327, loss 0.0367085, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:51.185515: step 7328, loss 0.00444731, acc 1, learning_rate 0.0001
2017-09-29T13:58:51.369273: step 7329, loss 0.0116124, acc 1, learning_rate 0.0001
2017-09-29T13:58:51.552169: step 7330, loss 0.0441608, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:51.741346: step 7331, loss 0.0105893, acc 1, learning_rate 0.0001
2017-09-29T13:58:51.920839: step 7332, loss 0.00703959, acc 1, learning_rate 0.0001
2017-09-29T13:58:52.100755: step 7333, loss 0.015391, acc 1, learning_rate 0.0001
2017-09-29T13:58:52.283223: step 7334, loss 0.039372, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:52.476727: step 7335, loss 0.0643715, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:52.670037: step 7336, loss 0.0878977, acc 0.953125, learning_rate 0.0001
2017-09-29T13:58:52.848412: step 7337, loss 0.0196803, acc 1, learning_rate 0.0001
2017-09-29T13:58:53.029928: step 7338, loss 0.00966133, acc 1, learning_rate 0.0001
2017-09-29T13:58:53.212228: step 7339, loss 0.0076482, acc 1, learning_rate 0.0001
2017-09-29T13:58:53.399905: step 7340, loss 0.00684765, acc 1, learning_rate 0.0001
2017-09-29T13:58:53.593050: step 7341, loss 0.0137667, acc 1, learning_rate 0.0001
2017-09-29T13:58:53.775254: step 7342, loss 0.108711, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:53.956677: step 7343, loss 0.00757915, acc 1, learning_rate 0.0001
2017-09-29T13:58:54.139154: step 7344, loss 0.0296888, acc 1, learning_rate 0.0001
2017-09-29T13:58:54.320557: step 7345, loss 0.00942991, acc 1, learning_rate 0.0001
2017-09-29T13:58:54.500450: step 7346, loss 0.00243119, acc 1, learning_rate 0.0001
2017-09-29T13:58:54.678386: step 7347, loss 0.0675222, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:54.860183: step 7348, loss 0.0119155, acc 1, learning_rate 0.0001
2017-09-29T13:58:55.044823: step 7349, loss 0.0325391, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:55.198675: step 7350, loss 0.0727711, acc 0.960784, learning_rate 0.0001
2017-09-29T13:58:55.385166: step 7351, loss 0.00331757, acc 1, learning_rate 0.0001
2017-09-29T13:58:55.568707: step 7352, loss 0.0100102, acc 1, learning_rate 0.0001
2017-09-29T13:58:55.750102: step 7353, loss 0.0627853, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:55.931641: step 7354, loss 0.00437661, acc 1, learning_rate 0.0001
2017-09-29T13:58:56.129213: step 7355, loss 0.0800104, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:56.319993: step 7356, loss 0.0358969, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:56.505350: step 7357, loss 0.00597597, acc 1, learning_rate 0.0001
2017-09-29T13:58:56.685735: step 7358, loss 0.0181178, acc 1, learning_rate 0.0001
2017-09-29T13:58:56.869688: step 7359, loss 0.0616115, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:57.048570: step 7360, loss 0.0937098, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T13:58:57.594984: step 7360, loss 0.228487, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7360

2017-09-29T13:58:58.391045: step 7361, loss 0.00439506, acc 1, learning_rate 0.0001
2017-09-29T13:58:58.580836: step 7362, loss 0.0562789, acc 0.984375, learning_rate 0.0001
2017-09-29T13:58:58.766477: step 7363, loss 0.018364, acc 1, learning_rate 0.0001
2017-09-29T13:58:58.954896: step 7364, loss 0.00425271, acc 1, learning_rate 0.0001
2017-09-29T13:58:59.135944: step 7365, loss 0.0184156, acc 1, learning_rate 0.0001
2017-09-29T13:58:59.326934: step 7366, loss 0.0891857, acc 0.96875, learning_rate 0.0001
2017-09-29T13:58:59.522925: step 7367, loss 0.0120399, acc 1, learning_rate 0.0001
2017-09-29T13:58:59.710301: step 7368, loss 0.0106785, acc 1, learning_rate 0.0001
2017-09-29T13:58:59.890527: step 7369, loss 0.0501644, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:00.079813: step 7370, loss 0.0130443, acc 1, learning_rate 0.0001
2017-09-29T13:59:00.266021: step 7371, loss 0.0035392, acc 1, learning_rate 0.0001
2017-09-29T13:59:00.449699: step 7372, loss 0.0162979, acc 1, learning_rate 0.0001
2017-09-29T13:59:00.633328: step 7373, loss 0.00649201, acc 1, learning_rate 0.0001
2017-09-29T13:59:00.815962: step 7374, loss 0.0170137, acc 1, learning_rate 0.0001
2017-09-29T13:59:01.003375: step 7375, loss 0.0139315, acc 1, learning_rate 0.0001
2017-09-29T13:59:01.194504: step 7376, loss 0.0523086, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:01.378387: step 7377, loss 0.0110593, acc 1, learning_rate 0.0001
2017-09-29T13:59:01.560921: step 7378, loss 0.0241447, acc 1, learning_rate 0.0001
2017-09-29T13:59:01.745242: step 7379, loss 0.00912275, acc 1, learning_rate 0.0001
2017-09-29T13:59:01.926735: step 7380, loss 0.0435003, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:02.111882: step 7381, loss 0.0185159, acc 1, learning_rate 0.0001
2017-09-29T13:59:02.292318: step 7382, loss 0.0263297, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:02.477954: step 7383, loss 0.049907, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:02.670442: step 7384, loss 0.0146545, acc 1, learning_rate 0.0001
2017-09-29T13:59:02.853080: step 7385, loss 0.0449548, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:03.032644: step 7386, loss 0.0451085, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:03.211354: step 7387, loss 0.0605234, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:03.399548: step 7388, loss 0.00707419, acc 1, learning_rate 0.0001
2017-09-29T13:59:03.580301: step 7389, loss 0.015751, acc 1, learning_rate 0.0001
2017-09-29T13:59:03.761651: step 7390, loss 0.019817, acc 1, learning_rate 0.0001
2017-09-29T13:59:03.945069: step 7391, loss 0.0396575, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:04.127087: step 7392, loss 0.0245613, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:04.308156: step 7393, loss 0.0121939, acc 1, learning_rate 0.0001
2017-09-29T13:59:04.509336: step 7394, loss 0.0115008, acc 1, learning_rate 0.0001
2017-09-29T13:59:04.692787: step 7395, loss 0.014622, acc 1, learning_rate 0.0001
2017-09-29T13:59:04.875883: step 7396, loss 0.0348112, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:05.061217: step 7397, loss 0.0110733, acc 1, learning_rate 0.0001
2017-09-29T13:59:05.250025: step 7398, loss 0.0116107, acc 1, learning_rate 0.0001
2017-09-29T13:59:05.437013: step 7399, loss 0.0108104, acc 1, learning_rate 0.0001
2017-09-29T13:59:05.620761: step 7400, loss 0.0250236, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:59:06.174513: step 7400, loss 0.223357, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7400

2017-09-29T13:59:06.793322: step 7401, loss 0.0706372, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:06.975699: step 7402, loss 0.0153165, acc 1, learning_rate 0.0001
2017-09-29T13:59:07.157238: step 7403, loss 0.0163339, acc 1, learning_rate 0.0001
2017-09-29T13:59:07.336209: step 7404, loss 0.00779736, acc 1, learning_rate 0.0001
2017-09-29T13:59:07.528299: step 7405, loss 0.017554, acc 1, learning_rate 0.0001
2017-09-29T13:59:07.715678: step 7406, loss 0.0204354, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:07.896945: step 7407, loss 0.0420536, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:08.080206: step 7408, loss 0.050715, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:08.265633: step 7409, loss 0.00929921, acc 1, learning_rate 0.0001
2017-09-29T13:59:08.450966: step 7410, loss 0.0134302, acc 1, learning_rate 0.0001
2017-09-29T13:59:08.647250: step 7411, loss 0.0513966, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:08.829187: step 7412, loss 0.0129073, acc 1, learning_rate 0.0001
2017-09-29T13:59:09.020495: step 7413, loss 0.0346707, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:09.200217: step 7414, loss 0.0248651, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:09.381420: step 7415, loss 0.0682555, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:09.585682: step 7416, loss 0.0463604, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:09.770087: step 7417, loss 0.0521111, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:09.955884: step 7418, loss 0.01581, acc 1, learning_rate 0.0001
2017-09-29T13:59:10.139500: step 7419, loss 0.0161144, acc 1, learning_rate 0.0001
2017-09-29T13:59:10.322974: step 7420, loss 0.0496745, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:10.506612: step 7421, loss 0.0832417, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:10.695204: step 7422, loss 0.0375535, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:10.879360: step 7423, loss 0.0468537, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:11.062461: step 7424, loss 0.00552241, acc 1, learning_rate 0.0001
2017-09-29T13:59:11.251727: step 7425, loss 0.0739546, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:11.436372: step 7426, loss 0.0628966, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:11.622285: step 7427, loss 0.0133641, acc 1, learning_rate 0.0001
2017-09-29T13:59:11.804665: step 7428, loss 0.00281832, acc 1, learning_rate 0.0001
2017-09-29T13:59:11.985932: step 7429, loss 0.0233766, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:12.170097: step 7430, loss 0.0490863, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:12.361232: step 7431, loss 0.0142811, acc 1, learning_rate 0.0001
2017-09-29T13:59:12.546666: step 7432, loss 0.0407779, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:12.728207: step 7433, loss 0.0153839, acc 1, learning_rate 0.0001
2017-09-29T13:59:12.911212: step 7434, loss 0.0919461, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:13.093318: step 7435, loss 0.0162117, acc 1, learning_rate 0.0001
2017-09-29T13:59:13.273767: step 7436, loss 0.01123, acc 1, learning_rate 0.0001
2017-09-29T13:59:13.456632: step 7437, loss 0.0408136, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:13.642378: step 7438, loss 0.0631869, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:13.822411: step 7439, loss 0.0131717, acc 1, learning_rate 0.0001
2017-09-29T13:59:14.005193: step 7440, loss 0.0213715, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:59:14.563415: step 7440, loss 0.226995, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7440

2017-09-29T13:59:15.269979: step 7441, loss 0.027615, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:15.470398: step 7442, loss 0.0270761, acc 1, learning_rate 0.0001
2017-09-29T13:59:15.660573: step 7443, loss 0.0362969, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:15.851810: step 7444, loss 0.0315988, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:16.040744: step 7445, loss 0.0180678, acc 1, learning_rate 0.0001
2017-09-29T13:59:16.221978: step 7446, loss 0.0305089, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:16.408519: step 7447, loss 0.0430622, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:16.561347: step 7448, loss 0.0122169, acc 1, learning_rate 0.0001
2017-09-29T13:59:16.747042: step 7449, loss 0.0592048, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:16.933180: step 7450, loss 0.0122772, acc 1, learning_rate 0.0001
2017-09-29T13:59:17.114797: step 7451, loss 0.031434, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:17.296068: step 7452, loss 0.0266995, acc 1, learning_rate 0.0001
2017-09-29T13:59:17.493975: step 7453, loss 0.0205032, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:17.678772: step 7454, loss 0.0158481, acc 1, learning_rate 0.0001
2017-09-29T13:59:17.862106: step 7455, loss 0.0164429, acc 1, learning_rate 0.0001
2017-09-29T13:59:18.042909: step 7456, loss 0.0154486, acc 1, learning_rate 0.0001
2017-09-29T13:59:18.228219: step 7457, loss 0.0615597, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:18.414051: step 7458, loss 0.00656147, acc 1, learning_rate 0.0001
2017-09-29T13:59:18.601550: step 7459, loss 0.0289678, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:18.790720: step 7460, loss 0.0338846, acc 1, learning_rate 0.0001
2017-09-29T13:59:18.974039: step 7461, loss 0.0321387, acc 1, learning_rate 0.0001
2017-09-29T13:59:19.158003: step 7462, loss 0.0531301, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:19.340568: step 7463, loss 0.0127005, acc 1, learning_rate 0.0001
2017-09-29T13:59:19.521851: step 7464, loss 0.0769333, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:19.701531: step 7465, loss 0.00747675, acc 1, learning_rate 0.0001
2017-09-29T13:59:19.881265: step 7466, loss 0.00540401, acc 1, learning_rate 0.0001
2017-09-29T13:59:20.065613: step 7467, loss 0.0190357, acc 1, learning_rate 0.0001
2017-09-29T13:59:20.245544: step 7468, loss 0.0321915, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:20.445153: step 7469, loss 0.0213676, acc 1, learning_rate 0.0001
2017-09-29T13:59:20.626672: step 7470, loss 0.00850507, acc 1, learning_rate 0.0001
2017-09-29T13:59:20.808654: step 7471, loss 0.0625487, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:20.989450: step 7472, loss 0.0251979, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:21.174110: step 7473, loss 0.0137556, acc 1, learning_rate 0.0001
2017-09-29T13:59:21.364591: step 7474, loss 0.00522452, acc 1, learning_rate 0.0001
2017-09-29T13:59:21.569441: step 7475, loss 0.0397478, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:21.753986: step 7476, loss 0.0436891, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:21.936479: step 7477, loss 0.0227932, acc 1, learning_rate 0.0001
2017-09-29T13:59:22.116211: step 7478, loss 0.00741785, acc 1, learning_rate 0.0001
2017-09-29T13:59:22.297862: step 7479, loss 0.0109133, acc 1, learning_rate 0.0001
2017-09-29T13:59:22.483278: step 7480, loss 0.028567, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:59:23.027827: step 7480, loss 0.224732, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7480

2017-09-29T13:59:23.813473: step 7481, loss 0.0223183, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:23.993991: step 7482, loss 0.0195185, acc 1, learning_rate 0.0001
2017-09-29T13:59:24.179719: step 7483, loss 0.0665387, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:24.370010: step 7484, loss 0.12072, acc 0.9375, learning_rate 0.0001
2017-09-29T13:59:24.557175: step 7485, loss 0.0498309, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:24.751681: step 7486, loss 0.0201038, acc 1, learning_rate 0.0001
2017-09-29T13:59:24.939053: step 7487, loss 0.0412914, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:25.127011: step 7488, loss 0.049428, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:25.308861: step 7489, loss 0.00890049, acc 1, learning_rate 0.0001
2017-09-29T13:59:25.498451: step 7490, loss 0.0468056, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:25.682904: step 7491, loss 0.0883672, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:25.879739: step 7492, loss 0.0100976, acc 1, learning_rate 0.0001
2017-09-29T13:59:26.074690: step 7493, loss 0.047892, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:26.259440: step 7494, loss 0.0494375, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:26.447550: step 7495, loss 0.00872067, acc 1, learning_rate 0.0001
2017-09-29T13:59:26.631305: step 7496, loss 0.0298388, acc 1, learning_rate 0.0001
2017-09-29T13:59:26.814371: step 7497, loss 0.00830592, acc 1, learning_rate 0.0001
2017-09-29T13:59:26.997438: step 7498, loss 0.00150281, acc 1, learning_rate 0.0001
2017-09-29T13:59:27.184074: step 7499, loss 0.00918218, acc 1, learning_rate 0.0001
2017-09-29T13:59:27.363049: step 7500, loss 0.018139, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:27.560756: step 7501, loss 0.0628554, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:27.743791: step 7502, loss 0.072962, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:27.941385: step 7503, loss 0.0116437, acc 1, learning_rate 0.0001
2017-09-29T13:59:28.135355: step 7504, loss 0.0594626, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:28.325556: step 7505, loss 0.0322543, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:28.514620: step 7506, loss 0.0143169, acc 1, learning_rate 0.0001
2017-09-29T13:59:28.697005: step 7507, loss 0.0527152, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:28.882366: step 7508, loss 0.00923355, acc 1, learning_rate 0.0001
2017-09-29T13:59:29.076565: step 7509, loss 0.050702, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:29.284038: step 7510, loss 0.0651005, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:29.494075: step 7511, loss 0.0180742, acc 1, learning_rate 0.0001
2017-09-29T13:59:29.695479: step 7512, loss 0.00454267, acc 1, learning_rate 0.0001
2017-09-29T13:59:29.895166: step 7513, loss 0.0124218, acc 1, learning_rate 0.0001
2017-09-29T13:59:30.118514: step 7514, loss 0.0270192, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:30.303276: step 7515, loss 0.0265717, acc 1, learning_rate 0.0001
2017-09-29T13:59:30.495201: step 7516, loss 0.0338666, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:30.679982: step 7517, loss 0.00250923, acc 1, learning_rate 0.0001
2017-09-29T13:59:30.864893: step 7518, loss 0.0385539, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:31.052274: step 7519, loss 0.0306076, acc 1, learning_rate 0.0001
2017-09-29T13:59:31.231695: step 7520, loss 0.0283846, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:59:31.788263: step 7520, loss 0.2231, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7520

2017-09-29T13:59:32.423459: step 7521, loss 0.0113519, acc 1, learning_rate 0.0001
2017-09-29T13:59:32.617093: step 7522, loss 0.00789861, acc 1, learning_rate 0.0001
2017-09-29T13:59:32.796781: step 7523, loss 0.00458059, acc 1, learning_rate 0.0001
2017-09-29T13:59:32.976029: step 7524, loss 0.0236356, acc 1, learning_rate 0.0001
2017-09-29T13:59:33.160069: step 7525, loss 0.0109712, acc 1, learning_rate 0.0001
2017-09-29T13:59:33.342742: step 7526, loss 0.0868616, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:33.538958: step 7527, loss 0.0287466, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:33.730361: step 7528, loss 0.00889804, acc 1, learning_rate 0.0001
2017-09-29T13:59:33.921581: step 7529, loss 0.0175423, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:34.120175: step 7530, loss 0.0260897, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:34.308817: step 7531, loss 0.022607, acc 1, learning_rate 0.0001
2017-09-29T13:59:34.493988: step 7532, loss 0.0216203, acc 1, learning_rate 0.0001
2017-09-29T13:59:34.680173: step 7533, loss 0.0366413, acc 1, learning_rate 0.0001
2017-09-29T13:59:34.864453: step 7534, loss 0.00960326, acc 1, learning_rate 0.0001
2017-09-29T13:59:35.045145: step 7535, loss 0.0206481, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:35.235895: step 7536, loss 0.0250384, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:35.420809: step 7537, loss 0.00425646, acc 1, learning_rate 0.0001
2017-09-29T13:59:35.612174: step 7538, loss 0.0102674, acc 1, learning_rate 0.0001
2017-09-29T13:59:35.810501: step 7539, loss 0.0167236, acc 1, learning_rate 0.0001
2017-09-29T13:59:35.997266: step 7540, loss 0.0548761, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:36.179021: step 7541, loss 0.0185077, acc 1, learning_rate 0.0001
2017-09-29T13:59:36.367412: step 7542, loss 0.0656305, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:36.552620: step 7543, loss 0.0147822, acc 1, learning_rate 0.0001
2017-09-29T13:59:36.740221: step 7544, loss 0.062166, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:36.925144: step 7545, loss 0.0581217, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:37.086214: step 7546, loss 0.0600482, acc 0.980392, learning_rate 0.0001
2017-09-29T13:59:37.270424: step 7547, loss 0.0668323, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:37.456898: step 7548, loss 0.0153331, acc 1, learning_rate 0.0001
2017-09-29T13:59:37.651345: step 7549, loss 0.0135719, acc 1, learning_rate 0.0001
2017-09-29T13:59:37.840238: step 7550, loss 0.0185053, acc 1, learning_rate 0.0001
2017-09-29T13:59:38.026881: step 7551, loss 0.0167509, acc 1, learning_rate 0.0001
2017-09-29T13:59:38.214214: step 7552, loss 0.0278827, acc 1, learning_rate 0.0001
2017-09-29T13:59:38.403853: step 7553, loss 0.0193506, acc 1, learning_rate 0.0001
2017-09-29T13:59:38.584221: step 7554, loss 0.00803524, acc 1, learning_rate 0.0001
2017-09-29T13:59:38.770200: step 7555, loss 0.0109734, acc 1, learning_rate 0.0001
2017-09-29T13:59:38.947707: step 7556, loss 0.0669051, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:39.130777: step 7557, loss 0.0155247, acc 1, learning_rate 0.0001
2017-09-29T13:59:39.320948: step 7558, loss 0.0164055, acc 1, learning_rate 0.0001
2017-09-29T13:59:39.523395: step 7559, loss 0.00400266, acc 1, learning_rate 0.0001
2017-09-29T13:59:39.718352: step 7560, loss 0.0182526, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:59:40.255513: step 7560, loss 0.225275, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7560

2017-09-29T13:59:40.963456: step 7561, loss 0.00496954, acc 1, learning_rate 0.0001
2017-09-29T13:59:41.163248: step 7562, loss 0.0225142, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:41.363696: step 7563, loss 0.0241016, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:41.559986: step 7564, loss 0.0337847, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:41.744080: step 7565, loss 0.0111616, acc 1, learning_rate 0.0001
2017-09-29T13:59:41.926949: step 7566, loss 0.00607333, acc 1, learning_rate 0.0001
2017-09-29T13:59:42.107409: step 7567, loss 0.00465345, acc 1, learning_rate 0.0001
2017-09-29T13:59:42.289138: step 7568, loss 0.0140463, acc 1, learning_rate 0.0001
2017-09-29T13:59:42.474340: step 7569, loss 0.0619774, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:42.661593: step 7570, loss 0.0476295, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:42.856186: step 7571, loss 0.022917, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:43.052901: step 7572, loss 0.0677318, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:43.253594: step 7573, loss 0.0321422, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:43.457218: step 7574, loss 0.0693346, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:43.654688: step 7575, loss 0.047215, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:43.836635: step 7576, loss 0.00734472, acc 1, learning_rate 0.0001
2017-09-29T13:59:44.017998: step 7577, loss 0.00328051, acc 1, learning_rate 0.0001
2017-09-29T13:59:44.196412: step 7578, loss 0.0250076, acc 1, learning_rate 0.0001
2017-09-29T13:59:44.389146: step 7579, loss 0.0415371, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:44.582893: step 7580, loss 0.0142552, acc 1, learning_rate 0.0001
2017-09-29T13:59:44.769765: step 7581, loss 0.00465044, acc 1, learning_rate 0.0001
2017-09-29T13:59:44.957606: step 7582, loss 0.0068785, acc 1, learning_rate 0.0001
2017-09-29T13:59:45.138779: step 7583, loss 0.0461959, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:45.318091: step 7584, loss 0.0218156, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:45.516869: step 7585, loss 0.00631438, acc 1, learning_rate 0.0001
2017-09-29T13:59:45.702765: step 7586, loss 0.0552902, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:45.896744: step 7587, loss 0.0287843, acc 1, learning_rate 0.0001
2017-09-29T13:59:46.082505: step 7588, loss 0.0748686, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:46.263140: step 7589, loss 0.0745829, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:46.455645: step 7590, loss 0.00629731, acc 1, learning_rate 0.0001
2017-09-29T13:59:46.636038: step 7591, loss 0.0336731, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:46.822077: step 7592, loss 0.048563, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:47.002482: step 7593, loss 0.0130563, acc 1, learning_rate 0.0001
2017-09-29T13:59:47.181347: step 7594, loss 0.0177077, acc 1, learning_rate 0.0001
2017-09-29T13:59:47.366235: step 7595, loss 0.0254338, acc 1, learning_rate 0.0001
2017-09-29T13:59:47.561262: step 7596, loss 0.0309578, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:47.743697: step 7597, loss 0.00858826, acc 1, learning_rate 0.0001
2017-09-29T13:59:47.927276: step 7598, loss 0.00615465, acc 1, learning_rate 0.0001
2017-09-29T13:59:48.109359: step 7599, loss 0.047797, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:48.295444: step 7600, loss 0.0227666, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T13:59:48.857928: step 7600, loss 0.223096, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7600

2017-09-29T13:59:49.559884: step 7601, loss 0.0829228, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:49.745707: step 7602, loss 0.0417979, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:49.941922: step 7603, loss 0.0151707, acc 1, learning_rate 0.0001
2017-09-29T13:59:50.125977: step 7604, loss 0.0256238, acc 1, learning_rate 0.0001
2017-09-29T13:59:50.310252: step 7605, loss 0.019674, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:50.519835: step 7606, loss 0.0385504, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:50.706969: step 7607, loss 0.00414551, acc 1, learning_rate 0.0001
2017-09-29T13:59:50.906997: step 7608, loss 0.00374765, acc 1, learning_rate 0.0001
2017-09-29T13:59:51.101070: step 7609, loss 0.0203395, acc 1, learning_rate 0.0001
2017-09-29T13:59:51.293257: step 7610, loss 0.0269606, acc 1, learning_rate 0.0001
2017-09-29T13:59:51.510357: step 7611, loss 0.0359905, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:51.692210: step 7612, loss 0.064843, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:51.876298: step 7613, loss 0.0371947, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:52.059669: step 7614, loss 0.0407915, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:52.243575: step 7615, loss 0.0351628, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:52.426120: step 7616, loss 0.0836928, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:52.610899: step 7617, loss 0.0159057, acc 1, learning_rate 0.0001
2017-09-29T13:59:52.795200: step 7618, loss 0.0427635, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:52.979974: step 7619, loss 0.0305397, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:53.162444: step 7620, loss 0.0175759, acc 1, learning_rate 0.0001
2017-09-29T13:59:53.346179: step 7621, loss 0.00675472, acc 1, learning_rate 0.0001
2017-09-29T13:59:53.527212: step 7622, loss 0.0075242, acc 1, learning_rate 0.0001
2017-09-29T13:59:53.712886: step 7623, loss 0.0732123, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:53.895666: step 7624, loss 0.0375717, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:54.081592: step 7625, loss 0.0771532, acc 0.953125, learning_rate 0.0001
2017-09-29T13:59:54.264166: step 7626, loss 0.0182713, acc 1, learning_rate 0.0001
2017-09-29T13:59:54.460865: step 7627, loss 0.027, acc 1, learning_rate 0.0001
2017-09-29T13:59:54.640041: step 7628, loss 0.0273912, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:54.822890: step 7629, loss 0.0100673, acc 1, learning_rate 0.0001
2017-09-29T13:59:55.007632: step 7630, loss 0.0423995, acc 0.96875, learning_rate 0.0001
2017-09-29T13:59:55.213942: step 7631, loss 0.0404318, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:55.395138: step 7632, loss 0.0404981, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:55.583013: step 7633, loss 0.0115721, acc 1, learning_rate 0.0001
2017-09-29T13:59:55.767201: step 7634, loss 0.0321767, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:55.950794: step 7635, loss 0.0538538, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:56.132329: step 7636, loss 0.0327634, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:56.312772: step 7637, loss 0.00711554, acc 1, learning_rate 0.0001
2017-09-29T13:59:56.523867: step 7638, loss 0.0372415, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:56.714110: step 7639, loss 0.0164928, acc 1, learning_rate 0.0001
2017-09-29T13:59:56.896759: step 7640, loss 0.0342488, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T13:59:57.450099: step 7640, loss 0.221322, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7640

2017-09-29T13:59:58.234681: step 7641, loss 0.0471843, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:58.420489: step 7642, loss 0.0226938, acc 1, learning_rate 0.0001
2017-09-29T13:59:58.603621: step 7643, loss 0.0247132, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:58.763281: step 7644, loss 0.0458659, acc 0.980392, learning_rate 0.0001
2017-09-29T13:59:58.955471: step 7645, loss 0.0389936, acc 1, learning_rate 0.0001
2017-09-29T13:59:59.136675: step 7646, loss 0.00336229, acc 1, learning_rate 0.0001
2017-09-29T13:59:59.315642: step 7647, loss 0.0605485, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:59.503729: step 7648, loss 0.0279258, acc 0.984375, learning_rate 0.0001
2017-09-29T13:59:59.688082: step 7649, loss 0.0102544, acc 1, learning_rate 0.0001
2017-09-29T13:59:59.868133: step 7650, loss 0.0290773, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:00.049615: step 7651, loss 0.00844874, acc 1, learning_rate 0.0001
2017-09-29T14:00:00.231455: step 7652, loss 0.0259282, acc 1, learning_rate 0.0001
2017-09-29T14:00:00.416897: step 7653, loss 0.0347768, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:00.606874: step 7654, loss 0.040399, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:00.789006: step 7655, loss 0.0370776, acc 1, learning_rate 0.0001
2017-09-29T14:00:00.969354: step 7656, loss 0.00940849, acc 1, learning_rate 0.0001
2017-09-29T14:00:01.159677: step 7657, loss 0.00353339, acc 1, learning_rate 0.0001
2017-09-29T14:00:01.346538: step 7658, loss 0.0544627, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:01.538397: step 7659, loss 0.022675, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:01.731093: step 7660, loss 0.0044459, acc 1, learning_rate 0.0001
2017-09-29T14:00:01.913789: step 7661, loss 0.0372925, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:02.095057: step 7662, loss 0.0633839, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:02.286736: step 7663, loss 0.0263481, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:02.485044: step 7664, loss 0.0154343, acc 1, learning_rate 0.0001
2017-09-29T14:00:02.668715: step 7665, loss 0.0237277, acc 1, learning_rate 0.0001
2017-09-29T14:00:02.856420: step 7666, loss 0.0368637, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:03.038016: step 7667, loss 0.0322869, acc 1, learning_rate 0.0001
2017-09-29T14:00:03.219118: step 7668, loss 0.00661906, acc 1, learning_rate 0.0001
2017-09-29T14:00:03.401778: step 7669, loss 0.0223508, acc 1, learning_rate 0.0001
2017-09-29T14:00:03.585294: step 7670, loss 0.00596467, acc 1, learning_rate 0.0001
2017-09-29T14:00:03.768396: step 7671, loss 0.00943029, acc 1, learning_rate 0.0001
2017-09-29T14:00:03.947600: step 7672, loss 0.017729, acc 1, learning_rate 0.0001
2017-09-29T14:00:04.131221: step 7673, loss 0.0738338, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:04.322327: step 7674, loss 0.0554436, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:04.507882: step 7675, loss 0.024605, acc 1, learning_rate 0.0001
2017-09-29T14:00:04.690764: step 7676, loss 0.0299078, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:04.873701: step 7677, loss 0.0478151, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:05.054376: step 7678, loss 0.016946, acc 1, learning_rate 0.0001
2017-09-29T14:00:05.239930: step 7679, loss 0.00588825, acc 1, learning_rate 0.0001
2017-09-29T14:00:05.433918: step 7680, loss 0.0375941, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T14:00:06.066029: step 7680, loss 0.225308, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7680

2017-09-29T14:00:06.687151: step 7681, loss 0.013345, acc 1, learning_rate 0.0001
2017-09-29T14:00:06.869304: step 7682, loss 0.0153888, acc 1, learning_rate 0.0001
2017-09-29T14:00:07.047179: step 7683, loss 0.00750305, acc 1, learning_rate 0.0001
2017-09-29T14:00:07.230244: step 7684, loss 0.0149303, acc 1, learning_rate 0.0001
2017-09-29T14:00:07.418038: step 7685, loss 0.0587272, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:07.600592: step 7686, loss 0.0452058, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:07.791126: step 7687, loss 0.0154791, acc 1, learning_rate 0.0001
2017-09-29T14:00:07.983874: step 7688, loss 0.0227073, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:08.171838: step 7689, loss 0.0558982, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:08.352504: step 7690, loss 0.00843096, acc 1, learning_rate 0.0001
2017-09-29T14:00:08.551825: step 7691, loss 0.0055986, acc 1, learning_rate 0.0001
2017-09-29T14:00:08.731582: step 7692, loss 0.0277266, acc 1, learning_rate 0.0001
2017-09-29T14:00:08.927378: step 7693, loss 0.00649559, acc 1, learning_rate 0.0001
2017-09-29T14:00:09.109115: step 7694, loss 0.069181, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:09.297589: step 7695, loss 0.00563343, acc 1, learning_rate 0.0001
2017-09-29T14:00:09.478679: step 7696, loss 0.024458, acc 1, learning_rate 0.0001
2017-09-29T14:00:09.659883: step 7697, loss 0.0463922, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:09.840058: step 7698, loss 0.00548115, acc 1, learning_rate 0.0001
2017-09-29T14:00:10.024870: step 7699, loss 0.0337978, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:10.207811: step 7700, loss 0.00624006, acc 1, learning_rate 0.0001
2017-09-29T14:00:10.391572: step 7701, loss 0.118007, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:10.579854: step 7702, loss 0.00634562, acc 1, learning_rate 0.0001
2017-09-29T14:00:10.766426: step 7703, loss 0.00530628, acc 1, learning_rate 0.0001
2017-09-29T14:00:10.945054: step 7704, loss 0.0245965, acc 1, learning_rate 0.0001
2017-09-29T14:00:11.129002: step 7705, loss 0.00938186, acc 1, learning_rate 0.0001
2017-09-29T14:00:11.311478: step 7706, loss 0.0900355, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:11.500341: step 7707, loss 0.0191439, acc 1, learning_rate 0.0001
2017-09-29T14:00:11.692850: step 7708, loss 0.0327109, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:11.876564: step 7709, loss 0.0155728, acc 1, learning_rate 0.0001
2017-09-29T14:00:12.056119: step 7710, loss 0.00237735, acc 1, learning_rate 0.0001
2017-09-29T14:00:12.240074: step 7711, loss 0.0399835, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:12.423448: step 7712, loss 0.0263034, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:12.606354: step 7713, loss 0.0140759, acc 1, learning_rate 0.0001
2017-09-29T14:00:12.788472: step 7714, loss 0.0395223, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:12.973310: step 7715, loss 0.0537533, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:13.158273: step 7716, loss 0.0922222, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:13.341729: step 7717, loss 0.0495575, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:13.554906: step 7718, loss 0.00601573, acc 1, learning_rate 0.0001
2017-09-29T14:00:13.744067: step 7719, loss 0.0147115, acc 1, learning_rate 0.0001
2017-09-29T14:00:13.933384: step 7720, loss 0.0358842, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T14:00:14.487337: step 7720, loss 0.222406, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7720

2017-09-29T14:00:15.174331: step 7721, loss 0.0364892, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:15.378190: step 7722, loss 0.117752, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:15.584715: step 7723, loss 0.00843336, acc 1, learning_rate 0.0001
2017-09-29T14:00:15.788705: step 7724, loss 0.0230311, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:15.992729: step 7725, loss 0.0131653, acc 1, learning_rate 0.0001
2017-09-29T14:00:16.194853: step 7726, loss 0.0201817, acc 1, learning_rate 0.0001
2017-09-29T14:00:16.401759: step 7727, loss 0.0567459, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:16.592246: step 7728, loss 0.00988554, acc 1, learning_rate 0.0001
2017-09-29T14:00:16.783180: step 7729, loss 0.0161972, acc 1, learning_rate 0.0001
2017-09-29T14:00:16.964382: step 7730, loss 0.0230735, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:17.152642: step 7731, loss 0.0183557, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:17.334914: step 7732, loss 0.0370801, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:17.520969: step 7733, loss 0.0244365, acc 1, learning_rate 0.0001
2017-09-29T14:00:17.707583: step 7734, loss 0.0238061, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:17.889252: step 7735, loss 0.0132533, acc 1, learning_rate 0.0001
2017-09-29T14:00:18.068142: step 7736, loss 0.0152072, acc 1, learning_rate 0.0001
2017-09-29T14:00:18.259598: step 7737, loss 0.0213304, acc 1, learning_rate 0.0001
2017-09-29T14:00:18.440792: step 7738, loss 0.00546154, acc 1, learning_rate 0.0001
2017-09-29T14:00:18.623047: step 7739, loss 0.0412905, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:18.805377: step 7740, loss 0.061751, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:18.983253: step 7741, loss 0.0136688, acc 1, learning_rate 0.0001
2017-09-29T14:00:19.138524: step 7742, loss 0.00730281, acc 1, learning_rate 0.0001
2017-09-29T14:00:19.325063: step 7743, loss 0.0225485, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:19.522824: step 7744, loss 0.016244, acc 1, learning_rate 0.0001
2017-09-29T14:00:19.708816: step 7745, loss 0.014236, acc 1, learning_rate 0.0001
2017-09-29T14:00:19.891986: step 7746, loss 0.00952013, acc 1, learning_rate 0.0001
2017-09-29T14:00:20.087085: step 7747, loss 0.0214902, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:20.270450: step 7748, loss 0.0112614, acc 1, learning_rate 0.0001
2017-09-29T14:00:20.451438: step 7749, loss 0.00798977, acc 1, learning_rate 0.0001
2017-09-29T14:00:20.644778: step 7750, loss 0.0393596, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:20.854888: step 7751, loss 0.00344893, acc 1, learning_rate 0.0001
2017-09-29T14:00:21.043698: step 7752, loss 0.00808222, acc 1, learning_rate 0.0001
2017-09-29T14:00:21.228411: step 7753, loss 0.0294546, acc 1, learning_rate 0.0001
2017-09-29T14:00:21.413327: step 7754, loss 0.0156263, acc 1, learning_rate 0.0001
2017-09-29T14:00:21.602145: step 7755, loss 0.00913922, acc 1, learning_rate 0.0001
2017-09-29T14:00:21.794048: step 7756, loss 0.0278381, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:21.985173: step 7757, loss 0.005006, acc 1, learning_rate 0.0001
2017-09-29T14:00:22.175019: step 7758, loss 0.0155658, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:22.358032: step 7759, loss 0.0330987, acc 1, learning_rate 0.0001
2017-09-29T14:00:22.542784: step 7760, loss 0.0294485, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T14:00:23.105388: step 7760, loss 0.22294, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7760

2017-09-29T14:00:23.883879: step 7761, loss 0.0233307, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:24.070190: step 7762, loss 0.0113204, acc 1, learning_rate 0.0001
2017-09-29T14:00:24.251857: step 7763, loss 0.01844, acc 1, learning_rate 0.0001
2017-09-29T14:00:24.442002: step 7764, loss 0.0435499, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:24.627934: step 7765, loss 0.0158327, acc 1, learning_rate 0.0001
2017-09-29T14:00:24.810010: step 7766, loss 0.116318, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:24.992870: step 7767, loss 0.0235492, acc 1, learning_rate 0.0001
2017-09-29T14:00:25.173394: step 7768, loss 0.0520053, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:25.365347: step 7769, loss 0.0314806, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:25.549682: step 7770, loss 0.0372966, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:25.734073: step 7771, loss 0.00966979, acc 1, learning_rate 0.0001
2017-09-29T14:00:25.918884: step 7772, loss 0.00523278, acc 1, learning_rate 0.0001
2017-09-29T14:00:26.098470: step 7773, loss 0.0344378, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:26.278984: step 7774, loss 0.0171995, acc 1, learning_rate 0.0001
2017-09-29T14:00:26.463849: step 7775, loss 0.0136527, acc 1, learning_rate 0.0001
2017-09-29T14:00:26.648423: step 7776, loss 0.0395785, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:26.835570: step 7777, loss 0.0258684, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:27.018985: step 7778, loss 0.0617223, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:27.204241: step 7779, loss 0.0468656, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:27.385193: step 7780, loss 0.0364553, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:27.588728: step 7781, loss 0.0130456, acc 1, learning_rate 0.0001
2017-09-29T14:00:27.784641: step 7782, loss 0.0281355, acc 1, learning_rate 0.0001
2017-09-29T14:00:27.965832: step 7783, loss 0.0567918, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:28.148133: step 7784, loss 0.0313154, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:28.332527: step 7785, loss 0.013185, acc 1, learning_rate 0.0001
2017-09-29T14:00:28.525100: step 7786, loss 0.0347381, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:28.707600: step 7787, loss 0.0339028, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:28.891059: step 7788, loss 0.0433267, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:29.074915: step 7789, loss 0.011068, acc 1, learning_rate 0.0001
2017-09-29T14:00:29.253363: step 7790, loss 0.0185682, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:29.435881: step 7791, loss 0.0105618, acc 1, learning_rate 0.0001
2017-09-29T14:00:29.620976: step 7792, loss 0.0222855, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:29.800707: step 7793, loss 0.00614858, acc 1, learning_rate 0.0001
2017-09-29T14:00:29.981729: step 7794, loss 0.0855052, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:30.165974: step 7795, loss 0.0169397, acc 1, learning_rate 0.0001
2017-09-29T14:00:30.347585: step 7796, loss 0.0336339, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:30.545353: step 7797, loss 0.019768, acc 1, learning_rate 0.0001
2017-09-29T14:00:30.728308: step 7798, loss 0.0105567, acc 1, learning_rate 0.0001
2017-09-29T14:00:30.908928: step 7799, loss 0.00356025, acc 1, learning_rate 0.0001
2017-09-29T14:00:31.088915: step 7800, loss 0.0268836, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T14:00:31.646666: step 7800, loss 0.223699, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7800

2017-09-29T14:00:32.283023: step 7801, loss 0.0306943, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:32.472870: step 7802, loss 0.0120625, acc 1, learning_rate 0.0001
2017-09-29T14:00:32.667580: step 7803, loss 0.016493, acc 1, learning_rate 0.0001
2017-09-29T14:00:32.859240: step 7804, loss 0.11999, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:33.059100: step 7805, loss 0.0855449, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:33.242468: step 7806, loss 0.113258, acc 0.953125, learning_rate 0.0001
2017-09-29T14:00:33.441114: step 7807, loss 0.0201579, acc 1, learning_rate 0.0001
2017-09-29T14:00:33.633342: step 7808, loss 0.0081992, acc 1, learning_rate 0.0001
2017-09-29T14:00:33.828229: step 7809, loss 0.0543684, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:34.019736: step 7810, loss 0.0681109, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:34.209845: step 7811, loss 0.0152814, acc 1, learning_rate 0.0001
2017-09-29T14:00:34.399179: step 7812, loss 0.035962, acc 1, learning_rate 0.0001
2017-09-29T14:00:34.584375: step 7813, loss 0.0116276, acc 1, learning_rate 0.0001
2017-09-29T14:00:34.767521: step 7814, loss 0.0616732, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:34.959527: step 7815, loss 0.0112416, acc 1, learning_rate 0.0001
2017-09-29T14:00:35.144670: step 7816, loss 0.00585579, acc 1, learning_rate 0.0001
2017-09-29T14:00:35.329873: step 7817, loss 0.00732774, acc 1, learning_rate 0.0001
2017-09-29T14:00:35.527468: step 7818, loss 0.0343022, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:35.719842: step 7819, loss 0.0109822, acc 1, learning_rate 0.0001
2017-09-29T14:00:35.910976: step 7820, loss 0.00465199, acc 1, learning_rate 0.0001
2017-09-29T14:00:36.098297: step 7821, loss 0.0109711, acc 1, learning_rate 0.0001
2017-09-29T14:00:36.287073: step 7822, loss 0.0483498, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:36.501819: step 7823, loss 0.0267305, acc 1, learning_rate 0.0001
2017-09-29T14:00:36.688063: step 7824, loss 0.0151323, acc 1, learning_rate 0.0001
2017-09-29T14:00:36.879538: step 7825, loss 0.0154826, acc 1, learning_rate 0.0001
2017-09-29T14:00:37.060155: step 7826, loss 0.0130076, acc 1, learning_rate 0.0001
2017-09-29T14:00:37.246271: step 7827, loss 0.0145712, acc 1, learning_rate 0.0001
2017-09-29T14:00:37.435339: step 7828, loss 0.019421, acc 1, learning_rate 0.0001
2017-09-29T14:00:37.629456: step 7829, loss 0.0255587, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:37.813608: step 7830, loss 0.00346274, acc 1, learning_rate 0.0001
2017-09-29T14:00:37.995015: step 7831, loss 0.0350702, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:38.195193: step 7832, loss 0.0257563, acc 1, learning_rate 0.0001
2017-09-29T14:00:38.383976: step 7833, loss 0.0301903, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:38.577964: step 7834, loss 0.0315465, acc 1, learning_rate 0.0001
2017-09-29T14:00:38.764273: step 7835, loss 0.00480637, acc 1, learning_rate 0.0001
2017-09-29T14:00:38.967451: step 7836, loss 0.0365987, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:39.151566: step 7837, loss 0.0727638, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:39.334131: step 7838, loss 0.0248685, acc 0.984375, learning_rate 0.0001
2017-09-29T14:00:39.530369: step 7839, loss 0.067474, acc 0.96875, learning_rate 0.0001
2017-09-29T14:00:39.694637: step 7840, loss 0.0186433, acc 1, learning_rate 0.0001

Evaluation:
2017-09-29T14:00:40.258248: step 7840, loss 0.224912, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1506709920/checkpoints/model-7840

