
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=16

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507737805

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-11T11:03:29.378864: step 1, loss 5.85971, acc 0.234375, learning_rate 0.005
2017-10-11T11:03:29.469410: step 2, loss 5.83683, acc 0.234375, learning_rate 0.00498
2017-10-11T11:03:29.545515: step 3, loss 4.38294, acc 0.296875, learning_rate 0.00496008
2017-10-11T11:03:29.620056: step 4, loss 4.51815, acc 0.359375, learning_rate 0.00494024
2017-10-11T11:03:29.692678: step 5, loss 4.91591, acc 0.28125, learning_rate 0.00492049
2017-10-11T11:03:29.763533: step 6, loss 3.16488, acc 0.328125, learning_rate 0.00490081
2017-10-11T11:03:29.837439: step 7, loss 4.3673, acc 0.34375, learning_rate 0.00488121
2017-10-11T11:03:29.911316: step 8, loss 3.43169, acc 0.375, learning_rate 0.0048617
2017-10-11T11:03:29.980139: step 9, loss 4.06104, acc 0.375, learning_rate 0.00484226
2017-10-11T11:03:30.049549: step 10, loss 3.48704, acc 0.3125, learning_rate 0.00482291
2017-10-11T11:03:30.122253: step 11, loss 2.82885, acc 0.34375, learning_rate 0.00480363
2017-10-11T11:03:30.195196: step 12, loss 2.95983, acc 0.28125, learning_rate 0.00478443
2017-10-11T11:03:30.266182: step 13, loss 2.89313, acc 0.34375, learning_rate 0.00476531
2017-10-11T11:03:30.337158: step 14, loss 2.98191, acc 0.40625, learning_rate 0.00474627
2017-10-11T11:03:30.409972: step 15, loss 3.22422, acc 0.328125, learning_rate 0.0047273
2017-10-11T11:03:30.477362: step 16, loss 2.01774, acc 0.5, learning_rate 0.00470841
2017-10-11T11:03:30.548019: step 17, loss 2.7653, acc 0.359375, learning_rate 0.0046896
2017-10-11T11:03:30.619618: step 18, loss 2.51068, acc 0.328125, learning_rate 0.00467087
2017-10-11T11:03:30.688413: step 19, loss 2.34177, acc 0.40625, learning_rate 0.00465221
2017-10-11T11:03:30.759398: step 20, loss 1.74776, acc 0.46875, learning_rate 0.00463363
2017-10-11T11:03:30.827083: step 21, loss 2.09945, acc 0.4375, learning_rate 0.00461513
2017-10-11T11:03:30.899628: step 22, loss 2.26175, acc 0.421875, learning_rate 0.0045967
2017-10-11T11:03:30.970996: step 23, loss 1.54274, acc 0.59375, learning_rate 0.00457834
2017-10-11T11:03:31.040918: step 24, loss 1.96816, acc 0.46875, learning_rate 0.00456006
2017-10-11T11:03:31.109284: step 25, loss 1.71492, acc 0.375, learning_rate 0.00454186
2017-10-11T11:03:31.177162: step 26, loss 2.02268, acc 0.4375, learning_rate 0.00452373
2017-10-11T11:03:31.247708: step 27, loss 1.46998, acc 0.53125, learning_rate 0.00450567
2017-10-11T11:03:31.317811: step 28, loss 1.52595, acc 0.5625, learning_rate 0.00448769
2017-10-11T11:03:31.389949: step 29, loss 1.26047, acc 0.5625, learning_rate 0.00446978
2017-10-11T11:03:31.463613: step 30, loss 1.48397, acc 0.5625, learning_rate 0.00445194
2017-10-11T11:03:31.536072: step 31, loss 1.47771, acc 0.59375, learning_rate 0.00443418
2017-10-11T11:03:31.608425: step 32, loss 1.51087, acc 0.5625, learning_rate 0.00441649
2017-10-11T11:03:31.681107: step 33, loss 1.23052, acc 0.609375, learning_rate 0.00439887
2017-10-11T11:03:31.750803: step 34, loss 1.1935, acc 0.671875, learning_rate 0.00438132
2017-10-11T11:03:31.824697: step 35, loss 1.17157, acc 0.59375, learning_rate 0.00436385
2017-10-11T11:03:31.898206: step 36, loss 1.19639, acc 0.609375, learning_rate 0.00434644
2017-10-11T11:03:31.966908: step 37, loss 1.53971, acc 0.46875, learning_rate 0.00432911
2017-10-11T11:03:32.037665: step 38, loss 1.21824, acc 0.546875, learning_rate 0.00431185
2017-10-11T11:03:32.108491: step 39, loss 1.30777, acc 0.625, learning_rate 0.00429465
2017-10-11T11:03:32.181202: step 40, loss 1.16656, acc 0.671875, learning_rate 0.00427753

Evaluation:
2017-10-11T11:03:32.388515: step 40, loss 0.488515, acc 0.847482

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-40

2017-10-11T11:03:32.993050: step 41, loss 0.847253, acc 0.6875, learning_rate 0.00426048
2017-10-11T11:03:33.062420: step 42, loss 1.09104, acc 0.625, learning_rate 0.0042435
2017-10-11T11:03:33.133813: step 43, loss 1.11655, acc 0.609375, learning_rate 0.00422659
2017-10-11T11:03:33.204040: step 44, loss 1.03613, acc 0.625, learning_rate 0.00420974
2017-10-11T11:03:33.276323: step 45, loss 0.915199, acc 0.734375, learning_rate 0.00419297
2017-10-11T11:03:33.344893: step 46, loss 0.983139, acc 0.59375, learning_rate 0.00417626
2017-10-11T11:03:33.423218: step 47, loss 0.793969, acc 0.75, learning_rate 0.00415962
2017-10-11T11:03:33.493761: step 48, loss 1.1447, acc 0.59375, learning_rate 0.00414305
2017-10-11T11:03:33.566844: step 49, loss 0.648059, acc 0.78125, learning_rate 0.00412655
2017-10-11T11:03:33.636030: step 50, loss 0.926919, acc 0.6875, learning_rate 0.00411011
2017-10-11T11:03:33.707048: step 51, loss 0.968012, acc 0.71875, learning_rate 0.00409375
2017-10-11T11:03:33.776147: step 52, loss 0.836515, acc 0.75, learning_rate 0.00407744
2017-10-11T11:03:33.846573: step 53, loss 0.697398, acc 0.703125, learning_rate 0.00406121
2017-10-11T11:03:33.921327: step 54, loss 0.777992, acc 0.734375, learning_rate 0.00404504
2017-10-11T11:03:33.988280: step 55, loss 0.673199, acc 0.71875, learning_rate 0.00402894
2017-10-11T11:03:34.059689: step 56, loss 0.651333, acc 0.765625, learning_rate 0.0040129
2017-10-11T11:03:34.126937: step 57, loss 1.06968, acc 0.65625, learning_rate 0.00399693
2017-10-11T11:03:34.195748: step 58, loss 0.848802, acc 0.703125, learning_rate 0.00398102
2017-10-11T11:03:34.265273: step 59, loss 0.84546, acc 0.75, learning_rate 0.00396518
2017-10-11T11:03:34.336620: step 60, loss 0.938484, acc 0.71875, learning_rate 0.00394941
2017-10-11T11:03:34.405945: step 61, loss 0.913635, acc 0.71875, learning_rate 0.00393369
2017-10-11T11:03:34.476791: step 62, loss 0.775666, acc 0.75, learning_rate 0.00391804
2017-10-11T11:03:34.546013: step 63, loss 0.808265, acc 0.671875, learning_rate 0.00390246
2017-10-11T11:03:34.619942: step 64, loss 0.881473, acc 0.6875, learning_rate 0.00388694
2017-10-11T11:03:34.693568: step 65, loss 0.56177, acc 0.734375, learning_rate 0.00387148
2017-10-11T11:03:34.765339: step 66, loss 0.571837, acc 0.796875, learning_rate 0.00385609
2017-10-11T11:03:34.837734: step 67, loss 0.55321, acc 0.84375, learning_rate 0.00384076
2017-10-11T11:03:34.910781: step 68, loss 0.812313, acc 0.6875, learning_rate 0.00382549
2017-10-11T11:03:34.977952: step 69, loss 0.86765, acc 0.703125, learning_rate 0.00381028
2017-10-11T11:03:35.048174: step 70, loss 0.746529, acc 0.734375, learning_rate 0.00379514
2017-10-11T11:03:35.126440: step 71, loss 0.604623, acc 0.828125, learning_rate 0.00378005
2017-10-11T11:03:35.199169: step 72, loss 0.602158, acc 0.8125, learning_rate 0.00376503
2017-10-11T11:03:35.271731: step 73, loss 0.721468, acc 0.75, learning_rate 0.00375007
2017-10-11T11:03:35.342839: step 74, loss 0.538125, acc 0.78125, learning_rate 0.00373517
2017-10-11T11:03:35.413455: step 75, loss 0.576372, acc 0.78125, learning_rate 0.00372034
2017-10-11T11:03:35.486499: step 76, loss 0.490169, acc 0.875, learning_rate 0.00370556
2017-10-11T11:03:35.562831: step 77, loss 0.826489, acc 0.71875, learning_rate 0.00369084
2017-10-11T11:03:35.634119: step 78, loss 0.571447, acc 0.78125, learning_rate 0.00367619
2017-10-11T11:03:35.703419: step 79, loss 0.589787, acc 0.796875, learning_rate 0.00366159
2017-10-11T11:03:35.774017: step 80, loss 0.738421, acc 0.703125, learning_rate 0.00364705

Evaluation:
2017-10-11T11:03:35.947642: step 80, loss 0.405045, acc 0.864748

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-80

2017-10-11T11:03:36.516602: step 81, loss 0.710403, acc 0.71875, learning_rate 0.00363257
2017-10-11T11:03:36.588751: step 82, loss 0.758521, acc 0.75, learning_rate 0.00361815
2017-10-11T11:03:36.663316: step 83, loss 0.618309, acc 0.859375, learning_rate 0.00360379
2017-10-11T11:03:36.735563: step 84, loss 0.691716, acc 0.765625, learning_rate 0.00358949
2017-10-11T11:03:36.805126: step 85, loss 0.87492, acc 0.734375, learning_rate 0.00357525
2017-10-11T11:03:36.878575: step 86, loss 0.431353, acc 0.859375, learning_rate 0.00356106
2017-10-11T11:03:36.947818: step 87, loss 0.557891, acc 0.8125, learning_rate 0.00354694
2017-10-11T11:03:37.019462: step 88, loss 0.514147, acc 0.8125, learning_rate 0.00353287
2017-10-11T11:03:37.089269: step 89, loss 0.517069, acc 0.828125, learning_rate 0.00351885
2017-10-11T11:03:37.162261: step 90, loss 0.568992, acc 0.828125, learning_rate 0.0035049
2017-10-11T11:03:37.230693: step 91, loss 0.492951, acc 0.78125, learning_rate 0.003491
2017-10-11T11:03:37.299617: step 92, loss 0.548156, acc 0.828125, learning_rate 0.00347716
2017-10-11T11:03:37.370975: step 93, loss 0.471896, acc 0.859375, learning_rate 0.00346338
2017-10-11T11:03:37.439258: step 94, loss 0.603226, acc 0.75, learning_rate 0.00344965
2017-10-11T11:03:37.507787: step 95, loss 0.639491, acc 0.796875, learning_rate 0.00343597
2017-10-11T11:03:37.579111: step 96, loss 0.493333, acc 0.78125, learning_rate 0.00342236
2017-10-11T11:03:37.652341: step 97, loss 0.878404, acc 0.71875, learning_rate 0.0034088
2017-10-11T11:03:37.714585: step 98, loss 0.665924, acc 0.72549, learning_rate 0.00339529
2017-10-11T11:03:37.787749: step 99, loss 0.60559, acc 0.765625, learning_rate 0.00338184
2017-10-11T11:03:37.859647: step 100, loss 0.698607, acc 0.75, learning_rate 0.00336844
2017-10-11T11:03:37.930093: step 101, loss 0.7919, acc 0.703125, learning_rate 0.0033551
2017-10-11T11:03:38.000968: step 102, loss 0.558648, acc 0.796875, learning_rate 0.00334182
2017-10-11T11:03:38.072248: step 103, loss 0.511307, acc 0.796875, learning_rate 0.00332858
2017-10-11T11:03:38.139986: step 104, loss 0.564548, acc 0.84375, learning_rate 0.00331541
2017-10-11T11:03:38.210932: step 105, loss 0.587548, acc 0.75, learning_rate 0.00330228
2017-10-11T11:03:38.281045: step 106, loss 0.564795, acc 0.78125, learning_rate 0.00328921
2017-10-11T11:03:38.351152: step 107, loss 0.631512, acc 0.796875, learning_rate 0.00327619
2017-10-11T11:03:38.425026: step 108, loss 0.591286, acc 0.828125, learning_rate 0.00326323
2017-10-11T11:03:38.497469: step 109, loss 0.612467, acc 0.796875, learning_rate 0.00325032
2017-10-11T11:03:38.568691: step 110, loss 0.631919, acc 0.71875, learning_rate 0.00323746
2017-10-11T11:03:38.639446: step 111, loss 0.609257, acc 0.75, learning_rate 0.00322465
2017-10-11T11:03:38.713554: step 112, loss 0.394827, acc 0.890625, learning_rate 0.0032119
2017-10-11T11:03:38.784064: step 113, loss 0.37082, acc 0.90625, learning_rate 0.0031992
2017-10-11T11:03:38.858389: step 114, loss 0.595461, acc 0.78125, learning_rate 0.00318655
2017-10-11T11:03:38.930783: step 115, loss 0.437387, acc 0.796875, learning_rate 0.00317395
2017-10-11T11:03:39.003096: step 116, loss 0.814388, acc 0.75, learning_rate 0.0031614
2017-10-11T11:03:39.073812: step 117, loss 0.672688, acc 0.796875, learning_rate 0.0031489
2017-10-11T11:03:39.140684: step 118, loss 0.712698, acc 0.765625, learning_rate 0.00313646
2017-10-11T11:03:39.213161: step 119, loss 0.61242, acc 0.734375, learning_rate 0.00312407
2017-10-11T11:03:39.282529: step 120, loss 0.624579, acc 0.78125, learning_rate 0.00311172

Evaluation:
2017-10-11T11:03:39.429617: step 120, loss 0.374303, acc 0.873381

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-120

2017-10-11T11:03:39.994070: step 121, loss 0.290634, acc 0.921875, learning_rate 0.00309943
2017-10-11T11:03:40.064928: step 122, loss 0.363478, acc 0.859375, learning_rate 0.00308719
2017-10-11T11:03:40.136479: step 123, loss 0.63237, acc 0.796875, learning_rate 0.00307499
2017-10-11T11:03:40.209136: step 124, loss 0.517499, acc 0.828125, learning_rate 0.00306285
2017-10-11T11:03:40.280738: step 125, loss 0.542013, acc 0.828125, learning_rate 0.00305076
2017-10-11T11:03:40.350762: step 126, loss 0.42644, acc 0.859375, learning_rate 0.00303871
2017-10-11T11:03:40.420148: step 127, loss 0.648558, acc 0.78125, learning_rate 0.00302672
2017-10-11T11:03:40.492808: step 128, loss 0.568037, acc 0.765625, learning_rate 0.00301477
2017-10-11T11:03:40.563833: step 129, loss 0.521676, acc 0.78125, learning_rate 0.00300287
2017-10-11T11:03:40.636401: step 130, loss 0.46316, acc 0.796875, learning_rate 0.00299102
2017-10-11T11:03:40.707240: step 131, loss 0.473174, acc 0.828125, learning_rate 0.00297922
2017-10-11T11:03:40.776166: step 132, loss 0.745239, acc 0.75, learning_rate 0.00296747
2017-10-11T11:03:40.846652: step 133, loss 0.546439, acc 0.859375, learning_rate 0.00295577
2017-10-11T11:03:40.916183: step 134, loss 0.555139, acc 0.796875, learning_rate 0.00294411
2017-10-11T11:03:40.989173: step 135, loss 0.400523, acc 0.8125, learning_rate 0.0029325
2017-10-11T11:03:41.060351: step 136, loss 0.396204, acc 0.890625, learning_rate 0.00292094
2017-10-11T11:03:41.131772: step 137, loss 0.436025, acc 0.859375, learning_rate 0.00290943
2017-10-11T11:03:41.201192: step 138, loss 0.88925, acc 0.640625, learning_rate 0.00289796
2017-10-11T11:03:41.270547: step 139, loss 0.441434, acc 0.8125, learning_rate 0.00288654
2017-10-11T11:03:41.347087: step 140, loss 0.35379, acc 0.875, learning_rate 0.00287516
2017-10-11T11:03:41.419570: step 141, loss 0.492147, acc 0.859375, learning_rate 0.00286384
2017-10-11T11:03:41.488914: step 142, loss 0.619739, acc 0.765625, learning_rate 0.00285256
2017-10-11T11:03:41.558781: step 143, loss 0.698257, acc 0.71875, learning_rate 0.00284132
2017-10-11T11:03:41.627584: step 144, loss 0.378311, acc 0.859375, learning_rate 0.00283013
2017-10-11T11:03:41.697069: step 145, loss 0.353325, acc 0.890625, learning_rate 0.00281899
2017-10-11T11:03:41.771367: step 146, loss 0.595818, acc 0.8125, learning_rate 0.00280789
2017-10-11T11:03:41.845295: step 147, loss 0.619434, acc 0.796875, learning_rate 0.00279684
2017-10-11T11:03:41.915553: step 148, loss 0.529245, acc 0.828125, learning_rate 0.00278583
2017-10-11T11:03:41.988181: step 149, loss 0.474061, acc 0.8125, learning_rate 0.00277486
2017-10-11T11:03:42.057953: step 150, loss 0.48121, acc 0.8125, learning_rate 0.00276395
2017-10-11T11:03:42.125826: step 151, loss 0.534997, acc 0.8125, learning_rate 0.00275307
2017-10-11T11:03:42.194791: step 152, loss 0.369381, acc 0.875, learning_rate 0.00274224
2017-10-11T11:03:42.266091: step 153, loss 0.517466, acc 0.8125, learning_rate 0.00273146
2017-10-11T11:03:42.337695: step 154, loss 0.501229, acc 0.84375, learning_rate 0.00272072
2017-10-11T11:03:42.409748: step 155, loss 0.341229, acc 0.890625, learning_rate 0.00271002
2017-10-11T11:03:42.479226: step 156, loss 0.553038, acc 0.859375, learning_rate 0.00269937
2017-10-11T11:03:42.550816: step 157, loss 0.370281, acc 0.9375, learning_rate 0.00268876
2017-10-11T11:03:42.619340: step 158, loss 0.339026, acc 0.90625, learning_rate 0.00267819
2017-10-11T11:03:42.690124: step 159, loss 0.707541, acc 0.78125, learning_rate 0.00266767
2017-10-11T11:03:42.759977: step 160, loss 0.548567, acc 0.75, learning_rate 0.00265719

Evaluation:
2017-10-11T11:03:42.913963: step 160, loss 0.335456, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-160

2017-10-11T11:03:43.481999: step 161, loss 0.455674, acc 0.890625, learning_rate 0.00264675
2017-10-11T11:03:43.552473: step 162, loss 0.404038, acc 0.859375, learning_rate 0.00263635
2017-10-11T11:03:43.619940: step 163, loss 0.598208, acc 0.8125, learning_rate 0.002626
2017-10-11T11:03:43.688689: step 164, loss 0.450241, acc 0.875, learning_rate 0.00261569
2017-10-11T11:03:43.760146: step 165, loss 0.528127, acc 0.796875, learning_rate 0.00260542
2017-10-11T11:03:43.831552: step 166, loss 0.525841, acc 0.828125, learning_rate 0.0025952
2017-10-11T11:03:43.920131: step 167, loss 0.354814, acc 0.859375, learning_rate 0.00258501
2017-10-11T11:03:43.990090: step 168, loss 0.362094, acc 0.875, learning_rate 0.00257487
2017-10-11T11:03:44.059535: step 169, loss 0.403009, acc 0.875, learning_rate 0.00256477
2017-10-11T11:03:44.129715: step 170, loss 0.474234, acc 0.796875, learning_rate 0.0025547
2017-10-11T11:03:44.204003: step 171, loss 0.427644, acc 0.875, learning_rate 0.00254469
2017-10-11T11:03:44.272098: step 172, loss 0.601313, acc 0.765625, learning_rate 0.00253471
2017-10-11T11:03:44.343557: step 173, loss 0.515704, acc 0.859375, learning_rate 0.00252477
2017-10-11T11:03:44.416620: step 174, loss 0.41833, acc 0.875, learning_rate 0.00251487
2017-10-11T11:03:44.487607: step 175, loss 0.439254, acc 0.84375, learning_rate 0.00250501
2017-10-11T11:03:44.557741: step 176, loss 0.317111, acc 0.859375, learning_rate 0.0024952
2017-10-11T11:03:44.630416: step 177, loss 0.454481, acc 0.84375, learning_rate 0.00248542
2017-10-11T11:03:44.699220: step 178, loss 0.470331, acc 0.84375, learning_rate 0.00247568
2017-10-11T11:03:44.774072: step 179, loss 0.396303, acc 0.890625, learning_rate 0.00246599
2017-10-11T11:03:44.847950: step 180, loss 0.54974, acc 0.78125, learning_rate 0.00245633
2017-10-11T11:03:44.918252: step 181, loss 0.676233, acc 0.796875, learning_rate 0.00244671
2017-10-11T11:03:44.993652: step 182, loss 0.565644, acc 0.828125, learning_rate 0.00243713
2017-10-11T11:03:45.062805: step 183, loss 0.408256, acc 0.859375, learning_rate 0.00242759
2017-10-11T11:03:45.135218: step 184, loss 0.650669, acc 0.828125, learning_rate 0.00241809
2017-10-11T11:03:45.206002: step 185, loss 0.536109, acc 0.8125, learning_rate 0.00240863
2017-10-11T11:03:45.278671: step 186, loss 0.422239, acc 0.890625, learning_rate 0.00239921
2017-10-11T11:03:45.347878: step 187, loss 0.416831, acc 0.84375, learning_rate 0.00238982
2017-10-11T11:03:45.418012: step 188, loss 0.329141, acc 0.90625, learning_rate 0.00238048
2017-10-11T11:03:45.489333: step 189, loss 0.435944, acc 0.859375, learning_rate 0.00237117
2017-10-11T11:03:45.557119: step 190, loss 0.509112, acc 0.78125, learning_rate 0.0023619
2017-10-11T11:03:45.624612: step 191, loss 0.568448, acc 0.828125, learning_rate 0.00235267
2017-10-11T11:03:45.693170: step 192, loss 0.491327, acc 0.828125, learning_rate 0.00234347
2017-10-11T11:03:45.761717: step 193, loss 0.329356, acc 0.875, learning_rate 0.00233431
2017-10-11T11:03:45.831112: step 194, loss 0.423802, acc 0.84375, learning_rate 0.00232519
2017-10-11T11:03:45.905068: step 195, loss 0.42179, acc 0.859375, learning_rate 0.00231611
2017-10-11T11:03:45.966920: step 196, loss 0.573624, acc 0.784314, learning_rate 0.00230707
2017-10-11T11:03:46.040991: step 197, loss 0.344471, acc 0.875, learning_rate 0.00229806
2017-10-11T11:03:46.111525: step 198, loss 0.455082, acc 0.796875, learning_rate 0.00228908
2017-10-11T11:03:46.189166: step 199, loss 0.282773, acc 0.890625, learning_rate 0.00228015
2017-10-11T11:03:46.259528: step 200, loss 0.557643, acc 0.75, learning_rate 0.00227125

Evaluation:
2017-10-11T11:03:46.418470: step 200, loss 0.335233, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-200

2017-10-11T11:03:46.984774: step 201, loss 0.628263, acc 0.8125, learning_rate 0.00226239
2017-10-11T11:03:47.057629: step 202, loss 0.49753, acc 0.875, learning_rate 0.00225356
2017-10-11T11:03:47.130155: step 203, loss 0.393245, acc 0.875, learning_rate 0.00224477
2017-10-11T11:03:47.200551: step 204, loss 0.554709, acc 0.78125, learning_rate 0.00223602
2017-10-11T11:03:47.271587: step 205, loss 0.356992, acc 0.875, learning_rate 0.0022273
2017-10-11T11:03:47.344473: step 206, loss 0.390297, acc 0.875, learning_rate 0.00221862
2017-10-11T11:03:47.419248: step 207, loss 0.342583, acc 0.921875, learning_rate 0.00220997
2017-10-11T11:03:47.486622: step 208, loss 0.330372, acc 0.84375, learning_rate 0.00220136
2017-10-11T11:03:47.557703: step 209, loss 0.383315, acc 0.890625, learning_rate 0.00219278
2017-10-11T11:03:47.628738: step 210, loss 0.437333, acc 0.875, learning_rate 0.00218424
2017-10-11T11:03:47.701151: step 211, loss 0.344512, acc 0.90625, learning_rate 0.00217573
2017-10-11T11:03:47.770420: step 212, loss 0.295795, acc 0.875, learning_rate 0.00216726
2017-10-11T11:03:47.840304: step 213, loss 0.535716, acc 0.84375, learning_rate 0.00215882
2017-10-11T11:03:47.910489: step 214, loss 0.495387, acc 0.875, learning_rate 0.00215041
2017-10-11T11:03:47.983460: step 215, loss 0.415493, acc 0.890625, learning_rate 0.00214204
2017-10-11T11:03:48.052923: step 216, loss 0.477331, acc 0.875, learning_rate 0.00213371
2017-10-11T11:03:48.123666: step 217, loss 0.469662, acc 0.859375, learning_rate 0.00212541
2017-10-11T11:03:48.194204: step 218, loss 0.409935, acc 0.859375, learning_rate 0.00211714
2017-10-11T11:03:48.264440: step 219, loss 0.757758, acc 0.796875, learning_rate 0.00210891
2017-10-11T11:03:48.331422: step 220, loss 0.546148, acc 0.796875, learning_rate 0.00210071
2017-10-11T11:03:48.398866: step 221, loss 0.310261, acc 0.921875, learning_rate 0.00209254
2017-10-11T11:03:48.471183: step 222, loss 0.296686, acc 0.921875, learning_rate 0.00208441
2017-10-11T11:03:48.542836: step 223, loss 0.327954, acc 0.890625, learning_rate 0.00207631
2017-10-11T11:03:48.612084: step 224, loss 0.445172, acc 0.90625, learning_rate 0.00206824
2017-10-11T11:03:48.683599: step 225, loss 0.33874, acc 0.90625, learning_rate 0.00206021
2017-10-11T11:03:48.753496: step 226, loss 0.334701, acc 0.875, learning_rate 0.00205221
2017-10-11T11:03:48.822573: step 227, loss 0.413673, acc 0.859375, learning_rate 0.00204424
2017-10-11T11:03:48.893922: step 228, loss 0.762009, acc 0.734375, learning_rate 0.0020363
2017-10-11T11:03:48.963631: step 229, loss 0.485279, acc 0.796875, learning_rate 0.0020284
2017-10-11T11:03:49.034847: step 230, loss 0.402062, acc 0.890625, learning_rate 0.00202053
2017-10-11T11:03:49.105566: step 231, loss 0.534641, acc 0.796875, learning_rate 0.00201269
2017-10-11T11:03:49.173498: step 232, loss 0.432406, acc 0.84375, learning_rate 0.00200488
2017-10-11T11:03:49.242465: step 233, loss 0.368879, acc 0.90625, learning_rate 0.00199711
2017-10-11T11:03:49.312024: step 234, loss 0.37027, acc 0.859375, learning_rate 0.00198936
2017-10-11T11:03:49.383622: step 235, loss 0.328092, acc 0.90625, learning_rate 0.00198165
2017-10-11T11:03:49.453268: step 236, loss 0.45311, acc 0.78125, learning_rate 0.00197397
2017-10-11T11:03:49.526398: step 237, loss 0.616043, acc 0.796875, learning_rate 0.00196632
2017-10-11T11:03:49.597439: step 238, loss 0.394128, acc 0.875, learning_rate 0.0019587
2017-10-11T11:03:49.666598: step 239, loss 0.591451, acc 0.78125, learning_rate 0.00195112
2017-10-11T11:03:49.738364: step 240, loss 0.376373, acc 0.875, learning_rate 0.00194356

Evaluation:
2017-10-11T11:03:49.909440: step 240, loss 0.31166, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-240

2017-10-11T11:03:50.498677: step 241, loss 0.346963, acc 0.859375, learning_rate 0.00193604
2017-10-11T11:03:50.573822: step 242, loss 0.405959, acc 0.859375, learning_rate 0.00192854
2017-10-11T11:03:50.644737: step 243, loss 0.343685, acc 0.890625, learning_rate 0.00192108
2017-10-11T11:03:50.718630: step 244, loss 0.491357, acc 0.828125, learning_rate 0.00191364
2017-10-11T11:03:50.787494: step 245, loss 0.52743, acc 0.796875, learning_rate 0.00190624
2017-10-11T11:03:50.856636: step 246, loss 0.342529, acc 0.890625, learning_rate 0.00189887
2017-10-11T11:03:50.928369: step 247, loss 0.540738, acc 0.796875, learning_rate 0.00189153
2017-10-11T11:03:50.999357: step 248, loss 0.419964, acc 0.859375, learning_rate 0.00188421
2017-10-11T11:03:51.069929: step 249, loss 0.45008, acc 0.859375, learning_rate 0.00187693
2017-10-11T11:03:51.142602: step 250, loss 0.373561, acc 0.875, learning_rate 0.00186968
2017-10-11T11:03:51.212699: step 251, loss 0.407403, acc 0.84375, learning_rate 0.00186245
2017-10-11T11:03:51.285231: step 252, loss 0.267737, acc 0.921875, learning_rate 0.00185526
2017-10-11T11:03:51.356921: step 253, loss 0.515486, acc 0.859375, learning_rate 0.0018481
2017-10-11T11:03:51.431131: step 254, loss 0.505963, acc 0.859375, learning_rate 0.00184096
2017-10-11T11:03:51.502636: step 255, loss 0.412653, acc 0.90625, learning_rate 0.00183385
2017-10-11T11:03:51.572681: step 256, loss 0.397204, acc 0.875, learning_rate 0.00182678
2017-10-11T11:03:51.643508: step 257, loss 0.536386, acc 0.828125, learning_rate 0.00181973
2017-10-11T11:03:51.713710: step 258, loss 0.361439, acc 0.859375, learning_rate 0.00181271
2017-10-11T11:03:51.781838: step 259, loss 0.401463, acc 0.921875, learning_rate 0.00180572
2017-10-11T11:03:51.855626: step 260, loss 0.377922, acc 0.890625, learning_rate 0.00179876
2017-10-11T11:03:51.930333: step 261, loss 0.527202, acc 0.890625, learning_rate 0.00179182
2017-10-11T11:03:52.000054: step 262, loss 0.378653, acc 0.859375, learning_rate 0.00178492
2017-10-11T11:03:52.068102: step 263, loss 0.479748, acc 0.890625, learning_rate 0.00177804
2017-10-11T11:03:52.141378: step 264, loss 0.440858, acc 0.875, learning_rate 0.00177119
2017-10-11T11:03:52.213523: step 265, loss 0.431332, acc 0.859375, learning_rate 0.00176437
2017-10-11T11:03:52.282175: step 266, loss 0.412246, acc 0.84375, learning_rate 0.00175758
2017-10-11T11:03:52.351586: step 267, loss 0.412786, acc 0.859375, learning_rate 0.00175081
2017-10-11T11:03:52.420307: step 268, loss 0.368796, acc 0.90625, learning_rate 0.00174407
2017-10-11T11:03:52.491498: step 269, loss 0.534605, acc 0.796875, learning_rate 0.00173736
2017-10-11T11:03:52.560022: step 270, loss 0.285074, acc 0.90625, learning_rate 0.00173068
2017-10-11T11:03:52.634152: step 271, loss 0.223744, acc 0.890625, learning_rate 0.00172402
2017-10-11T11:03:52.705189: step 272, loss 0.361225, acc 0.875, learning_rate 0.00171739
2017-10-11T11:03:52.774594: step 273, loss 0.260133, acc 0.90625, learning_rate 0.00171079
2017-10-11T11:03:52.845856: step 274, loss 0.513681, acc 0.859375, learning_rate 0.00170422
2017-10-11T11:03:52.919118: step 275, loss 0.378796, acc 0.875, learning_rate 0.00169767
2017-10-11T11:03:52.989091: step 276, loss 0.375978, acc 0.90625, learning_rate 0.00169115
2017-10-11T11:03:53.057776: step 277, loss 0.488748, acc 0.84375, learning_rate 0.00168465
2017-10-11T11:03:53.128003: step 278, loss 0.46349, acc 0.828125, learning_rate 0.00167818
2017-10-11T11:03:53.198269: step 279, loss 0.381926, acc 0.890625, learning_rate 0.00167174
2017-10-11T11:03:53.269524: step 280, loss 0.196298, acc 0.953125, learning_rate 0.00166533

Evaluation:
2017-10-11T11:03:53.419566: step 280, loss 0.306018, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-280

2017-10-11T11:03:53.988823: step 281, loss 0.491585, acc 0.859375, learning_rate 0.00165894
2017-10-11T11:03:54.059278: step 282, loss 0.70884, acc 0.78125, learning_rate 0.00165257
2017-10-11T11:03:54.129625: step 283, loss 0.311919, acc 0.90625, learning_rate 0.00164624
2017-10-11T11:03:54.205485: step 284, loss 0.395695, acc 0.875, learning_rate 0.00163993
2017-10-11T11:03:54.275413: step 285, loss 0.416176, acc 0.890625, learning_rate 0.00163364
2017-10-11T11:03:54.346265: step 286, loss 0.325175, acc 0.921875, learning_rate 0.00162738
2017-10-11T11:03:54.418434: step 287, loss 0.30022, acc 0.90625, learning_rate 0.00162115
2017-10-11T11:03:54.490973: step 288, loss 0.40244, acc 0.890625, learning_rate 0.00161494
2017-10-11T11:03:54.562025: step 289, loss 0.371869, acc 0.828125, learning_rate 0.00160875
2017-10-11T11:03:54.631380: step 290, loss 0.397718, acc 0.859375, learning_rate 0.00160259
2017-10-11T11:03:54.704799: step 291, loss 0.327014, acc 0.953125, learning_rate 0.00159646
2017-10-11T11:03:54.773122: step 292, loss 0.361686, acc 0.90625, learning_rate 0.00159035
2017-10-11T11:03:54.845802: step 293, loss 0.517484, acc 0.828125, learning_rate 0.00158427
2017-10-11T11:03:54.906459: step 294, loss 0.34339, acc 0.882353, learning_rate 0.00157821
2017-10-11T11:03:54.978868: step 295, loss 0.33947, acc 0.859375, learning_rate 0.00157218
2017-10-11T11:03:55.052864: step 296, loss 0.417007, acc 0.859375, learning_rate 0.00156617
2017-10-11T11:03:55.123653: step 297, loss 0.354185, acc 0.875, learning_rate 0.00156018
2017-10-11T11:03:55.193286: step 298, loss 0.371371, acc 0.859375, learning_rate 0.00155422
2017-10-11T11:03:55.263313: step 299, loss 0.32665, acc 0.890625, learning_rate 0.00154829
2017-10-11T11:03:55.333018: step 300, loss 0.230931, acc 0.9375, learning_rate 0.00154238
2017-10-11T11:03:55.405795: step 301, loss 0.564126, acc 0.828125, learning_rate 0.00153649
2017-10-11T11:03:55.474742: step 302, loss 0.366019, acc 0.890625, learning_rate 0.00153063
2017-10-11T11:03:55.545498: step 303, loss 0.386636, acc 0.890625, learning_rate 0.00152479
2017-10-11T11:03:55.616546: step 304, loss 0.425205, acc 0.859375, learning_rate 0.00151897
2017-10-11T11:03:55.687494: step 305, loss 0.334402, acc 0.875, learning_rate 0.00151318
2017-10-11T11:03:55.760352: step 306, loss 0.374622, acc 0.890625, learning_rate 0.00150741
2017-10-11T11:03:55.834882: step 307, loss 0.453106, acc 0.84375, learning_rate 0.00150167
2017-10-11T11:03:55.908408: step 308, loss 0.245203, acc 0.9375, learning_rate 0.00149594
2017-10-11T11:03:55.980566: step 309, loss 0.308057, acc 0.90625, learning_rate 0.00149025
2017-10-11T11:03:56.052918: step 310, loss 0.207542, acc 0.921875, learning_rate 0.00148457
2017-10-11T11:03:56.127508: step 311, loss 0.305321, acc 0.90625, learning_rate 0.00147892
2017-10-11T11:03:56.202396: step 312, loss 0.290241, acc 0.921875, learning_rate 0.00147329
2017-10-11T11:03:56.272039: step 313, loss 0.346413, acc 0.84375, learning_rate 0.00146769
2017-10-11T11:03:56.350244: step 314, loss 0.382943, acc 0.859375, learning_rate 0.0014621
2017-10-11T11:03:56.420380: step 315, loss 0.294283, acc 0.90625, learning_rate 0.00145654
2017-10-11T11:03:56.490463: step 316, loss 0.379074, acc 0.890625, learning_rate 0.00145101
2017-10-11T11:03:56.561805: step 317, loss 0.465316, acc 0.828125, learning_rate 0.00144549
2017-10-11T11:03:56.630013: step 318, loss 0.569979, acc 0.765625, learning_rate 0.00144
2017-10-11T11:03:56.699425: step 319, loss 0.618297, acc 0.84375, learning_rate 0.00143453
2017-10-11T11:03:56.771667: step 320, loss 0.405623, acc 0.875, learning_rate 0.00142908

Evaluation:
2017-10-11T11:03:56.929789: step 320, loss 0.2971, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-320

2017-10-11T11:03:57.515225: step 321, loss 0.611232, acc 0.75, learning_rate 0.00142366
2017-10-11T11:03:57.587508: step 322, loss 0.261964, acc 0.90625, learning_rate 0.00141826
2017-10-11T11:03:57.656650: step 323, loss 0.303777, acc 0.90625, learning_rate 0.00141288
2017-10-11T11:03:57.725636: step 324, loss 0.334224, acc 0.875, learning_rate 0.00140752
2017-10-11T11:03:57.795901: step 325, loss 0.40312, acc 0.8125, learning_rate 0.00140218
2017-10-11T11:03:57.869067: step 326, loss 0.309786, acc 0.875, learning_rate 0.00139686
2017-10-11T11:03:57.943387: step 327, loss 0.344946, acc 0.875, learning_rate 0.00139157
2017-10-11T11:03:58.016473: step 328, loss 0.40258, acc 0.859375, learning_rate 0.0013863
2017-10-11T11:03:58.087059: step 329, loss 0.376025, acc 0.859375, learning_rate 0.00138105
2017-10-11T11:03:58.156357: step 330, loss 0.347822, acc 0.890625, learning_rate 0.00137582
2017-10-11T11:03:58.232213: step 331, loss 0.348483, acc 0.890625, learning_rate 0.00137061
2017-10-11T11:03:58.302841: step 332, loss 0.442208, acc 0.84375, learning_rate 0.00136543
2017-10-11T11:03:58.372287: step 333, loss 0.266888, acc 0.90625, learning_rate 0.00136026
2017-10-11T11:03:58.444239: step 334, loss 0.349967, acc 0.875, learning_rate 0.00135512
2017-10-11T11:03:58.520414: step 335, loss 0.202884, acc 0.96875, learning_rate 0.00134999
2017-10-11T11:03:58.602600: step 336, loss 0.418767, acc 0.828125, learning_rate 0.00134489
2017-10-11T11:03:58.674752: step 337, loss 0.519572, acc 0.8125, learning_rate 0.00133981
2017-10-11T11:03:58.746750: step 338, loss 0.355351, acc 0.8125, learning_rate 0.00133475
2017-10-11T11:03:58.816540: step 339, loss 0.440535, acc 0.890625, learning_rate 0.00132971
2017-10-11T11:03:58.890811: step 340, loss 0.412856, acc 0.90625, learning_rate 0.00132469
2017-10-11T11:03:58.960091: step 341, loss 0.394848, acc 0.859375, learning_rate 0.00131969
2017-10-11T11:03:59.030237: step 342, loss 0.266765, acc 0.921875, learning_rate 0.00131471
2017-10-11T11:03:59.101086: step 343, loss 0.27224, acc 0.90625, learning_rate 0.00130975
2017-10-11T11:03:59.170397: step 344, loss 0.294542, acc 0.921875, learning_rate 0.00130482
2017-10-11T11:03:59.239513: step 345, loss 0.406363, acc 0.890625, learning_rate 0.0012999
2017-10-11T11:03:59.310666: step 346, loss 0.405726, acc 0.859375, learning_rate 0.001295
2017-10-11T11:03:59.386554: step 347, loss 0.501368, acc 0.828125, learning_rate 0.00129012
2017-10-11T11:03:59.456441: step 348, loss 0.245695, acc 0.921875, learning_rate 0.00128527
2017-10-11T11:03:59.528007: step 349, loss 0.335765, acc 0.859375, learning_rate 0.00128043
2017-10-11T11:03:59.599779: step 350, loss 0.340227, acc 0.875, learning_rate 0.00127561
2017-10-11T11:03:59.671487: step 351, loss 0.340675, acc 0.890625, learning_rate 0.00127081
2017-10-11T11:03:59.741700: step 352, loss 0.244768, acc 0.96875, learning_rate 0.00126603
2017-10-11T11:03:59.811526: step 353, loss 0.590314, acc 0.8125, learning_rate 0.00126127
2017-10-11T11:03:59.882812: step 354, loss 0.358878, acc 0.875, learning_rate 0.00125653
2017-10-11T11:03:59.954627: step 355, loss 0.208449, acc 0.96875, learning_rate 0.00125181
2017-10-11T11:04:00.025157: step 356, loss 0.252, acc 0.90625, learning_rate 0.00124711
2017-10-11T11:04:00.095556: step 357, loss 0.27889, acc 0.921875, learning_rate 0.00124243
2017-10-11T11:04:00.169796: step 358, loss 0.313413, acc 0.90625, learning_rate 0.00123777
2017-10-11T11:04:00.239457: step 359, loss 0.394043, acc 0.84375, learning_rate 0.00123312
2017-10-11T11:04:00.309661: step 360, loss 0.388235, acc 0.890625, learning_rate 0.0012285

Evaluation:
2017-10-11T11:04:00.482170: step 360, loss 0.285693, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-360

2017-10-11T11:04:01.061801: step 361, loss 0.270493, acc 0.921875, learning_rate 0.00122389
2017-10-11T11:04:01.131685: step 362, loss 0.250602, acc 0.921875, learning_rate 0.0012193
2017-10-11T11:04:01.204952: step 363, loss 0.300576, acc 0.859375, learning_rate 0.00121473
2017-10-11T11:04:01.279507: step 364, loss 0.478375, acc 0.875, learning_rate 0.00121018
2017-10-11T11:04:01.350376: step 365, loss 0.364833, acc 0.875, learning_rate 0.00120565
2017-10-11T11:04:01.426865: step 366, loss 0.446046, acc 0.84375, learning_rate 0.00120114
2017-10-11T11:04:01.498714: step 367, loss 0.278163, acc 0.890625, learning_rate 0.00119664
2017-10-11T11:04:01.570611: step 368, loss 0.264068, acc 0.921875, learning_rate 0.00119217
2017-10-11T11:04:01.644547: step 369, loss 0.276022, acc 0.90625, learning_rate 0.00118771
2017-10-11T11:04:01.715296: step 370, loss 0.458306, acc 0.859375, learning_rate 0.00118327
2017-10-11T11:04:01.787385: step 371, loss 0.289647, acc 0.890625, learning_rate 0.00117885
2017-10-11T11:04:01.872790: step 372, loss 0.302777, acc 0.890625, learning_rate 0.00117445
2017-10-11T11:04:01.949508: step 373, loss 0.361009, acc 0.90625, learning_rate 0.00117006
2017-10-11T11:04:02.019323: step 374, loss 0.418298, acc 0.828125, learning_rate 0.00116569
2017-10-11T11:04:02.091146: step 375, loss 0.339254, acc 0.890625, learning_rate 0.00116134
2017-10-11T11:04:02.165235: step 376, loss 0.289702, acc 0.921875, learning_rate 0.00115701
2017-10-11T11:04:02.235597: step 377, loss 0.446415, acc 0.890625, learning_rate 0.0011527
2017-10-11T11:04:02.309974: step 378, loss 0.280506, acc 0.921875, learning_rate 0.0011484
2017-10-11T11:04:02.383270: step 379, loss 0.2937, acc 0.890625, learning_rate 0.00114412
2017-10-11T11:04:02.455983: step 380, loss 0.286914, acc 0.9375, learning_rate 0.00113986
2017-10-11T11:04:02.532292: step 381, loss 0.411532, acc 0.84375, learning_rate 0.00113561
2017-10-11T11:04:02.602445: step 382, loss 0.39078, acc 0.859375, learning_rate 0.00113139
2017-10-11T11:04:02.674493: step 383, loss 0.413915, acc 0.875, learning_rate 0.00112718
2017-10-11T11:04:02.746793: step 384, loss 0.236994, acc 0.921875, learning_rate 0.00112298
2017-10-11T11:04:02.818928: step 385, loss 0.270169, acc 0.890625, learning_rate 0.00111881
2017-10-11T11:04:02.892628: step 386, loss 0.297665, acc 0.875, learning_rate 0.00111465
2017-10-11T11:04:02.963921: step 387, loss 0.303392, acc 0.90625, learning_rate 0.00111051
2017-10-11T11:04:03.034705: step 388, loss 0.363742, acc 0.890625, learning_rate 0.00110638
2017-10-11T11:04:03.105586: step 389, loss 0.462151, acc 0.859375, learning_rate 0.00110228
2017-10-11T11:04:03.175211: step 390, loss 0.314812, acc 0.890625, learning_rate 0.00109818
2017-10-11T11:04:03.247708: step 391, loss 0.290203, acc 0.921875, learning_rate 0.00109411
2017-10-11T11:04:03.310061: step 392, loss 0.439709, acc 0.803922, learning_rate 0.00109005
2017-10-11T11:04:03.381698: step 393, loss 0.363824, acc 0.859375, learning_rate 0.00108601
2017-10-11T11:04:03.455791: step 394, loss 0.24241, acc 0.921875, learning_rate 0.00108199
2017-10-11T11:04:03.526977: step 395, loss 0.404419, acc 0.875, learning_rate 0.00107798
2017-10-11T11:04:03.598828: step 396, loss 0.353824, acc 0.875, learning_rate 0.00107399
2017-10-11T11:04:03.668361: step 397, loss 0.341501, acc 0.890625, learning_rate 0.00107001
2017-10-11T11:04:03.740296: step 398, loss 0.24684, acc 0.921875, learning_rate 0.00106605
2017-10-11T11:04:03.812700: step 399, loss 0.236335, acc 0.953125, learning_rate 0.00106211
2017-10-11T11:04:03.884921: step 400, loss 0.336415, acc 0.90625, learning_rate 0.00105818

Evaluation:
2017-10-11T11:04:04.036964: step 400, loss 0.286432, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-400

2017-10-11T11:04:04.536343: step 401, loss 0.216506, acc 0.984375, learning_rate 0.00105427
2017-10-11T11:04:04.610042: step 402, loss 0.353208, acc 0.875, learning_rate 0.00105037
2017-10-11T11:04:04.682640: step 403, loss 0.373161, acc 0.921875, learning_rate 0.0010465
2017-10-11T11:04:04.757008: step 404, loss 0.328829, acc 0.859375, learning_rate 0.00104263
2017-10-11T11:04:04.826861: step 405, loss 0.576017, acc 0.8125, learning_rate 0.00103878
2017-10-11T11:04:04.904613: step 406, loss 0.35641, acc 0.875, learning_rate 0.00103495
2017-10-11T11:04:04.975777: step 407, loss 0.492563, acc 0.84375, learning_rate 0.00103114
2017-10-11T11:04:05.047606: step 408, loss 0.222909, acc 0.90625, learning_rate 0.00102734
2017-10-11T11:04:05.115891: step 409, loss 0.32812, acc 0.9375, learning_rate 0.00102355
2017-10-11T11:04:05.186931: step 410, loss 0.245171, acc 0.890625, learning_rate 0.00101978
2017-10-11T11:04:05.255878: step 411, loss 0.350534, acc 0.875, learning_rate 0.00101603
2017-10-11T11:04:05.330052: step 412, loss 0.336634, acc 0.875, learning_rate 0.00101229
2017-10-11T11:04:05.401951: step 413, loss 0.327144, acc 0.90625, learning_rate 0.00100856
2017-10-11T11:04:05.474121: step 414, loss 0.30623, acc 0.875, learning_rate 0.00100486
2017-10-11T11:04:05.545967: step 415, loss 0.203387, acc 0.96875, learning_rate 0.00100116
2017-10-11T11:04:05.615972: step 416, loss 0.2365, acc 0.921875, learning_rate 0.000997483
2017-10-11T11:04:05.683556: step 417, loss 0.314993, acc 0.921875, learning_rate 0.00099382
2017-10-11T11:04:05.753669: step 418, loss 0.304297, acc 0.890625, learning_rate 0.000990172
2017-10-11T11:04:05.824423: step 419, loss 0.280184, acc 0.953125, learning_rate 0.000986538
2017-10-11T11:04:05.897201: step 420, loss 0.30409, acc 0.875, learning_rate 0.00098292
2017-10-11T11:04:05.968110: step 421, loss 0.193105, acc 0.9375, learning_rate 0.000979316
2017-10-11T11:04:06.039323: step 422, loss 0.334427, acc 0.90625, learning_rate 0.000975727
2017-10-11T11:04:06.109867: step 423, loss 0.338539, acc 0.90625, learning_rate 0.000972152
2017-10-11T11:04:06.179569: step 424, loss 0.245008, acc 0.90625, learning_rate 0.000968592
2017-10-11T11:04:06.250497: step 425, loss 0.316791, acc 0.890625, learning_rate 0.000965047
2017-10-11T11:04:06.318757: step 426, loss 0.342674, acc 0.875, learning_rate 0.000961516
2017-10-11T11:04:06.389060: step 427, loss 0.22248, acc 0.9375, learning_rate 0.000958
2017-10-11T11:04:06.462774: step 428, loss 0.396402, acc 0.921875, learning_rate 0.000954497
2017-10-11T11:04:06.535459: step 429, loss 0.453377, acc 0.859375, learning_rate 0.00095101
2017-10-11T11:04:06.606061: step 430, loss 0.408529, acc 0.8125, learning_rate 0.000947536
2017-10-11T11:04:06.680231: step 431, loss 0.181484, acc 0.9375, learning_rate 0.000944076
2017-10-11T11:04:06.751120: step 432, loss 0.294212, acc 0.90625, learning_rate 0.000940631
2017-10-11T11:04:06.823293: step 433, loss 0.28159, acc 0.90625, learning_rate 0.0009372
2017-10-11T11:04:06.897468: step 434, loss 0.291031, acc 0.890625, learning_rate 0.000933783
2017-10-11T11:04:06.965861: step 435, loss 0.293944, acc 0.875, learning_rate 0.000930379
2017-10-11T11:04:07.036972: step 436, loss 0.41008, acc 0.859375, learning_rate 0.00092699
2017-10-11T11:04:07.110307: step 437, loss 0.2293, acc 0.90625, learning_rate 0.000923614
2017-10-11T11:04:07.186799: step 438, loss 0.26619, acc 0.9375, learning_rate 0.000920253
2017-10-11T11:04:07.260252: step 439, loss 0.351281, acc 0.859375, learning_rate 0.000916905
2017-10-11T11:04:07.330187: step 440, loss 0.482385, acc 0.828125, learning_rate 0.00091357

Evaluation:
2017-10-11T11:04:07.481051: step 440, loss 0.28085, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-440

2017-10-11T11:04:08.042933: step 441, loss 0.254751, acc 0.890625, learning_rate 0.000910249
2017-10-11T11:04:08.112137: step 442, loss 0.27946, acc 0.921875, learning_rate 0.000906942
2017-10-11T11:04:08.184870: step 443, loss 0.272938, acc 0.9375, learning_rate 0.000903648
2017-10-11T11:04:08.255027: step 444, loss 0.289398, acc 0.890625, learning_rate 0.000900368
2017-10-11T11:04:08.325685: step 445, loss 0.389317, acc 0.921875, learning_rate 0.000897101
2017-10-11T11:04:08.393889: step 446, loss 0.54958, acc 0.84375, learning_rate 0.000893848
2017-10-11T11:04:08.467632: step 447, loss 0.316159, acc 0.890625, learning_rate 0.000890607
2017-10-11T11:04:08.538241: step 448, loss 0.359947, acc 0.890625, learning_rate 0.00088738
2017-10-11T11:04:08.609623: step 449, loss 0.303251, acc 0.875, learning_rate 0.000884166
2017-10-11T11:04:08.681453: step 450, loss 0.288566, acc 0.890625, learning_rate 0.000880966
2017-10-11T11:04:08.753100: step 451, loss 0.197395, acc 0.9375, learning_rate 0.000877778
2017-10-11T11:04:08.820284: step 452, loss 0.255917, acc 0.90625, learning_rate 0.000874603
2017-10-11T11:04:08.897249: step 453, loss 0.350277, acc 0.921875, learning_rate 0.000871441
2017-10-11T11:04:08.968231: step 454, loss 0.292554, acc 0.9375, learning_rate 0.000868293
2017-10-11T11:04:09.039507: step 455, loss 0.561691, acc 0.78125, learning_rate 0.000865157
2017-10-11T11:04:09.112451: step 456, loss 0.356823, acc 0.90625, learning_rate 0.000862033
2017-10-11T11:04:09.188557: step 457, loss 0.30588, acc 0.875, learning_rate 0.000858923
2017-10-11T11:04:09.260886: step 458, loss 0.439603, acc 0.859375, learning_rate 0.000855825
2017-10-11T11:04:09.333264: step 459, loss 0.261861, acc 0.921875, learning_rate 0.00085274
2017-10-11T11:04:09.411381: step 460, loss 0.369367, acc 0.875, learning_rate 0.000849668
2017-10-11T11:04:09.481331: step 461, loss 0.276839, acc 0.9375, learning_rate 0.000846608
2017-10-11T11:04:09.552098: step 462, loss 0.365679, acc 0.9375, learning_rate 0.00084356
2017-10-11T11:04:09.621212: step 463, loss 0.369001, acc 0.84375, learning_rate 0.000840525
2017-10-11T11:04:09.690729: step 464, loss 0.391031, acc 0.890625, learning_rate 0.000837502
2017-10-11T11:04:09.761805: step 465, loss 0.460453, acc 0.875, learning_rate 0.000834492
2017-10-11T11:04:09.836146: step 466, loss 0.349158, acc 0.859375, learning_rate 0.000831494
2017-10-11T11:04:09.910880: step 467, loss 0.200529, acc 0.9375, learning_rate 0.000828508
2017-10-11T11:04:09.980001: step 468, loss 0.35127, acc 0.84375, learning_rate 0.000825535
2017-10-11T11:04:10.049171: step 469, loss 0.353242, acc 0.84375, learning_rate 0.000822573
2017-10-11T11:04:10.119176: step 470, loss 0.189267, acc 0.953125, learning_rate 0.000819624
2017-10-11T11:04:10.188710: step 471, loss 0.314453, acc 0.890625, learning_rate 0.000816687
2017-10-11T11:04:10.262908: step 472, loss 0.269829, acc 0.90625, learning_rate 0.000813761
2017-10-11T11:04:10.333471: step 473, loss 0.329853, acc 0.890625, learning_rate 0.000810848
2017-10-11T11:04:10.402134: step 474, loss 0.235054, acc 0.90625, learning_rate 0.000807946
2017-10-11T11:04:10.472762: step 475, loss 0.431945, acc 0.84375, learning_rate 0.000805057
2017-10-11T11:04:10.544609: step 476, loss 0.226632, acc 0.921875, learning_rate 0.000802179
2017-10-11T11:04:10.617079: step 477, loss 0.362319, acc 0.890625, learning_rate 0.000799313
2017-10-11T11:04:10.687897: step 478, loss 0.365792, acc 0.921875, learning_rate 0.000796458
2017-10-11T11:04:10.758118: step 479, loss 0.363281, acc 0.84375, learning_rate 0.000793616
2017-10-11T11:04:10.830736: step 480, loss 0.29496, acc 0.953125, learning_rate 0.000790784

Evaluation:
2017-10-11T11:04:10.986636: step 480, loss 0.278336, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-480

2017-10-11T11:04:11.881302: step 481, loss 0.342065, acc 0.90625, learning_rate 0.000787965
2017-10-11T11:04:11.951050: step 482, loss 0.390541, acc 0.875, learning_rate 0.000785157
2017-10-11T11:04:12.024435: step 483, loss 0.23789, acc 0.9375, learning_rate 0.00078236
2017-10-11T11:04:12.093986: step 484, loss 0.226132, acc 0.96875, learning_rate 0.000779575
2017-10-11T11:04:12.168341: step 485, loss 0.139023, acc 0.984375, learning_rate 0.000776801
2017-10-11T11:04:12.238985: step 486, loss 0.313159, acc 0.890625, learning_rate 0.000774038
2017-10-11T11:04:12.307784: step 487, loss 0.340156, acc 0.875, learning_rate 0.000771287
2017-10-11T11:04:12.379960: step 488, loss 0.201594, acc 0.921875, learning_rate 0.000768547
2017-10-11T11:04:12.452685: step 489, loss 0.352408, acc 0.84375, learning_rate 0.000765818
2017-10-11T11:04:12.516261: step 490, loss 0.245368, acc 0.921569, learning_rate 0.000763101
2017-10-11T11:04:12.584649: step 491, loss 0.209262, acc 0.9375, learning_rate 0.000760394
2017-10-11T11:04:12.652780: step 492, loss 0.246369, acc 0.9375, learning_rate 0.000757698
2017-10-11T11:04:12.725512: step 493, loss 0.179428, acc 0.984375, learning_rate 0.000755014
2017-10-11T11:04:12.796670: step 494, loss 0.357556, acc 0.890625, learning_rate 0.00075234
2017-10-11T11:04:12.870877: step 495, loss 0.340329, acc 0.875, learning_rate 0.000749677
2017-10-11T11:04:12.942196: step 496, loss 0.260985, acc 0.953125, learning_rate 0.000747026
2017-10-11T11:04:13.011258: step 497, loss 0.446561, acc 0.828125, learning_rate 0.000744385
2017-10-11T11:04:13.083235: step 498, loss 0.22865, acc 0.921875, learning_rate 0.000741754
2017-10-11T11:04:13.152694: step 499, loss 0.184505, acc 0.9375, learning_rate 0.000739135
2017-10-11T11:04:13.223932: step 500, loss 0.155838, acc 0.96875, learning_rate 0.000736526
2017-10-11T11:04:13.296354: step 501, loss 0.230905, acc 0.9375, learning_rate 0.000733928
2017-10-11T11:04:13.368608: step 502, loss 0.423336, acc 0.875, learning_rate 0.00073134
2017-10-11T11:04:13.452861: step 503, loss 0.489707, acc 0.8125, learning_rate 0.000728763
2017-10-11T11:04:13.531074: step 504, loss 0.303825, acc 0.9375, learning_rate 0.000726197
2017-10-11T11:04:13.602455: step 505, loss 0.316583, acc 0.890625, learning_rate 0.000723641
2017-10-11T11:04:13.674478: step 506, loss 0.28184, acc 0.921875, learning_rate 0.000721095
2017-10-11T11:04:13.744476: step 507, loss 0.306871, acc 0.890625, learning_rate 0.00071856
2017-10-11T11:04:13.814701: step 508, loss 0.23268, acc 0.921875, learning_rate 0.000716036
2017-10-11T11:04:13.887466: step 509, loss 0.337072, acc 0.890625, learning_rate 0.000713521
2017-10-11T11:04:13.956854: step 510, loss 0.339404, acc 0.875, learning_rate 0.000711017
2017-10-11T11:04:14.026680: step 511, loss 0.340639, acc 0.859375, learning_rate 0.000708523
2017-10-11T11:04:14.096854: step 512, loss 0.37789, acc 0.859375, learning_rate 0.000706039
2017-10-11T11:04:14.169931: step 513, loss 0.449785, acc 0.875, learning_rate 0.000703565
2017-10-11T11:04:14.238794: step 514, loss 0.123889, acc 0.953125, learning_rate 0.000701102
2017-10-11T11:04:14.308014: step 515, loss 0.414406, acc 0.90625, learning_rate 0.000698648
2017-10-11T11:04:14.377354: step 516, loss 0.3382, acc 0.875, learning_rate 0.000696204
2017-10-11T11:04:14.448947: step 517, loss 0.332435, acc 0.890625, learning_rate 0.000693771
2017-10-11T11:04:14.520659: step 518, loss 0.327828, acc 0.890625, learning_rate 0.000691347
2017-10-11T11:04:14.589519: step 519, loss 0.306426, acc 0.9375, learning_rate 0.000688934
2017-10-11T11:04:14.659576: step 520, loss 0.268979, acc 0.921875, learning_rate 0.00068653

Evaluation:
2017-10-11T11:04:14.811116: step 520, loss 0.275636, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-520

2017-10-11T11:04:15.309511: step 521, loss 0.335828, acc 0.859375, learning_rate 0.000684136
2017-10-11T11:04:15.382789: step 522, loss 0.292249, acc 0.921875, learning_rate 0.000681751
2017-10-11T11:04:15.455143: step 523, loss 0.370604, acc 0.875, learning_rate 0.000679377
2017-10-11T11:04:15.527237: step 524, loss 0.402413, acc 0.875, learning_rate 0.000677012
2017-10-11T11:04:15.597091: step 525, loss 0.224801, acc 0.953125, learning_rate 0.000674657
2017-10-11T11:04:15.666666: step 526, loss 0.461086, acc 0.890625, learning_rate 0.000672311
2017-10-11T11:04:15.734876: step 527, loss 0.205879, acc 0.9375, learning_rate 0.000669975
2017-10-11T11:04:15.808302: step 528, loss 0.327256, acc 0.890625, learning_rate 0.000667648
2017-10-11T11:04:15.884107: step 529, loss 0.249461, acc 0.921875, learning_rate 0.000665331
2017-10-11T11:04:15.956083: step 530, loss 0.172101, acc 0.921875, learning_rate 0.000663024
2017-10-11T11:04:16.026198: step 531, loss 0.186615, acc 0.9375, learning_rate 0.000660726
2017-10-11T11:04:16.098785: step 532, loss 0.511916, acc 0.875, learning_rate 0.000658437
2017-10-11T11:04:16.172160: step 533, loss 0.3466, acc 0.9375, learning_rate 0.000656158
2017-10-11T11:04:16.243048: step 534, loss 0.460404, acc 0.84375, learning_rate 0.000653888
2017-10-11T11:04:16.317850: step 535, loss 0.299263, acc 0.890625, learning_rate 0.000651627
2017-10-11T11:04:16.385609: step 536, loss 0.316745, acc 0.890625, learning_rate 0.000649375
2017-10-11T11:04:16.453449: step 537, loss 0.282024, acc 0.890625, learning_rate 0.000647133
2017-10-11T11:04:16.524347: step 538, loss 0.355254, acc 0.90625, learning_rate 0.000644899
2017-10-11T11:04:16.600710: step 539, loss 0.465393, acc 0.828125, learning_rate 0.000642675
2017-10-11T11:04:16.674035: step 540, loss 0.203319, acc 0.96875, learning_rate 0.00064046
2017-10-11T11:04:16.749206: step 541, loss 0.301929, acc 0.84375, learning_rate 0.000638254
2017-10-11T11:04:16.821128: step 542, loss 0.252363, acc 0.9375, learning_rate 0.000636057
2017-10-11T11:04:16.894359: step 543, loss 0.162839, acc 0.953125, learning_rate 0.000633869
2017-10-11T11:04:16.967656: step 544, loss 0.483846, acc 0.859375, learning_rate 0.00063169
2017-10-11T11:04:17.037176: step 545, loss 0.269168, acc 0.921875, learning_rate 0.00062952
2017-10-11T11:04:17.110771: step 546, loss 0.27864, acc 0.90625, learning_rate 0.000627358
2017-10-11T11:04:17.182871: step 547, loss 0.323895, acc 0.890625, learning_rate 0.000625206
2017-10-11T11:04:17.254870: step 548, loss 0.365573, acc 0.875, learning_rate 0.000623062
2017-10-11T11:04:17.325645: step 549, loss 0.273181, acc 0.859375, learning_rate 0.000620927
2017-10-11T11:04:17.397225: step 550, loss 0.216305, acc 0.921875, learning_rate 0.000618801
2017-10-11T11:04:17.468059: step 551, loss 0.206625, acc 0.90625, learning_rate 0.000616683
2017-10-11T11:04:17.538998: step 552, loss 0.269134, acc 0.890625, learning_rate 0.000614574
2017-10-11T11:04:17.607949: step 553, loss 0.332093, acc 0.90625, learning_rate 0.000612474
2017-10-11T11:04:17.682523: step 554, loss 0.23537, acc 0.953125, learning_rate 0.000610382
2017-10-11T11:04:17.752517: step 555, loss 0.192475, acc 0.921875, learning_rate 0.000608299
2017-10-11T11:04:17.824127: step 556, loss 0.250925, acc 0.921875, learning_rate 0.000606224
2017-10-11T11:04:17.901358: step 557, loss 0.214141, acc 0.90625, learning_rate 0.000604158
2017-10-11T11:04:17.979647: step 558, loss 0.346404, acc 0.875, learning_rate 0.0006021
2017-10-11T11:04:18.051393: step 559, loss 0.291949, acc 0.921875, learning_rate 0.00060005
2017-10-11T11:04:18.124499: step 560, loss 0.267945, acc 0.921875, learning_rate 0.000598009

Evaluation:
2017-10-11T11:04:18.305348: step 560, loss 0.273691, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-560

2017-10-11T11:04:18.868849: step 561, loss 0.245761, acc 0.921875, learning_rate 0.000595977
2017-10-11T11:04:18.939052: step 562, loss 0.460279, acc 0.765625, learning_rate 0.000593952
2017-10-11T11:04:19.007648: step 563, loss 0.24168, acc 0.90625, learning_rate 0.000591936
2017-10-11T11:04:19.076923: step 564, loss 0.296909, acc 0.890625, learning_rate 0.000589928
2017-10-11T11:04:19.150344: step 565, loss 0.416914, acc 0.859375, learning_rate 0.000587928
2017-10-11T11:04:19.226630: step 566, loss 0.286178, acc 0.921875, learning_rate 0.000585937
2017-10-11T11:04:19.295655: step 567, loss 0.445401, acc 0.875, learning_rate 0.000583953
2017-10-11T11:04:19.367129: step 568, loss 0.422506, acc 0.859375, learning_rate 0.000581978
2017-10-11T11:04:19.439857: step 569, loss 0.215909, acc 0.9375, learning_rate 0.00058001
2017-10-11T11:04:19.510796: step 570, loss 0.330908, acc 0.921875, learning_rate 0.000578051
2017-10-11T11:04:19.582224: step 571, loss 0.260753, acc 0.9375, learning_rate 0.0005761
2017-10-11T11:04:19.654354: step 572, loss 0.368956, acc 0.828125, learning_rate 0.000574157
2017-10-11T11:04:19.726934: step 573, loss 0.309488, acc 0.84375, learning_rate 0.000572221
2017-10-11T11:04:19.798487: step 574, loss 0.213569, acc 0.890625, learning_rate 0.000570294
2017-10-11T11:04:19.872249: step 575, loss 0.179892, acc 0.953125, learning_rate 0.000568374
2017-10-11T11:04:19.941935: step 576, loss 0.283499, acc 0.9375, learning_rate 0.000566462
2017-10-11T11:04:20.013143: step 577, loss 0.273766, acc 0.9375, learning_rate 0.000564558
2017-10-11T11:04:20.083673: step 578, loss 0.172529, acc 0.953125, learning_rate 0.000562662
2017-10-11T11:04:20.155037: step 579, loss 0.23753, acc 0.953125, learning_rate 0.000560774
2017-10-11T11:04:20.226145: step 580, loss 0.223782, acc 0.953125, learning_rate 0.000558893
2017-10-11T11:04:20.297142: step 581, loss 0.387673, acc 0.84375, learning_rate 0.00055702
2017-10-11T11:04:20.371886: step 582, loss 0.266615, acc 0.859375, learning_rate 0.000555154
2017-10-11T11:04:20.440993: step 583, loss 0.336476, acc 0.875, learning_rate 0.000553296
2017-10-11T11:04:20.512882: step 584, loss 0.361438, acc 0.890625, learning_rate 0.000551446
2017-10-11T11:04:20.585306: step 585, loss 0.296853, acc 0.90625, learning_rate 0.000549604
2017-10-11T11:04:20.655052: step 586, loss 0.301396, acc 0.875, learning_rate 0.000547768
2017-10-11T11:04:20.724409: step 587, loss 0.202698, acc 0.953125, learning_rate 0.000545941
2017-10-11T11:04:20.787148: step 588, loss 0.664736, acc 0.745098, learning_rate 0.00054412
2017-10-11T11:04:20.860935: step 589, loss 0.279627, acc 0.890625, learning_rate 0.000542308
2017-10-11T11:04:20.933507: step 590, loss 0.316861, acc 0.890625, learning_rate 0.000540502
2017-10-11T11:04:21.003626: step 591, loss 0.216087, acc 0.96875, learning_rate 0.000538704
2017-10-11T11:04:21.073589: step 592, loss 0.188902, acc 0.96875, learning_rate 0.000536914
2017-10-11T11:04:21.142865: step 593, loss 0.4405, acc 0.875, learning_rate 0.00053513
2017-10-11T11:04:21.211840: step 594, loss 0.328622, acc 0.90625, learning_rate 0.000533354
2017-10-11T11:04:21.287174: step 595, loss 0.262273, acc 0.90625, learning_rate 0.000531585
2017-10-11T11:04:21.354667: step 596, loss 0.337647, acc 0.890625, learning_rate 0.000529824
2017-10-11T11:04:21.428790: step 597, loss 0.324492, acc 0.890625, learning_rate 0.000528069
2017-10-11T11:04:21.501614: step 598, loss 0.280563, acc 0.890625, learning_rate 0.000526322
2017-10-11T11:04:21.574180: step 599, loss 0.359629, acc 0.875, learning_rate 0.000524582
2017-10-11T11:04:21.646969: step 600, loss 0.406821, acc 0.875, learning_rate 0.000522849

Evaluation:
2017-10-11T11:04:21.800163: step 600, loss 0.269132, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-600

2017-10-11T11:04:22.434433: step 601, loss 0.265692, acc 0.90625, learning_rate 0.000521123
2017-10-11T11:04:22.506049: step 602, loss 0.273247, acc 0.90625, learning_rate 0.000519404
2017-10-11T11:04:22.576534: step 603, loss 0.174249, acc 0.96875, learning_rate 0.000517692
2017-10-11T11:04:22.648027: step 604, loss 0.288285, acc 0.875, learning_rate 0.000515987
2017-10-11T11:04:22.717661: step 605, loss 0.489669, acc 0.796875, learning_rate 0.000514289
2017-10-11T11:04:22.788482: step 606, loss 0.306476, acc 0.890625, learning_rate 0.000512598
2017-10-11T11:04:22.860390: step 607, loss 0.275084, acc 0.890625, learning_rate 0.000510914
2017-10-11T11:04:22.931634: step 608, loss 0.236025, acc 0.9375, learning_rate 0.000509237
2017-10-11T11:04:23.002883: step 609, loss 0.399746, acc 0.875, learning_rate 0.000507566
2017-10-11T11:04:23.073890: step 610, loss 0.207674, acc 0.90625, learning_rate 0.000505903
2017-10-11T11:04:23.144169: step 611, loss 0.202023, acc 0.921875, learning_rate 0.000504246
2017-10-11T11:04:23.214531: step 612, loss 0.412201, acc 0.859375, learning_rate 0.000502596
2017-10-11T11:04:23.288822: step 613, loss 0.340189, acc 0.90625, learning_rate 0.000500953
2017-10-11T11:04:23.359883: step 614, loss 0.297464, acc 0.90625, learning_rate 0.000499316
2017-10-11T11:04:23.429407: step 615, loss 0.278124, acc 0.890625, learning_rate 0.000497686
2017-10-11T11:04:23.500577: step 616, loss 0.208686, acc 0.953125, learning_rate 0.000496063
2017-10-11T11:04:23.570236: step 617, loss 0.345463, acc 0.859375, learning_rate 0.000494446
2017-10-11T11:04:23.641405: step 618, loss 0.291265, acc 0.953125, learning_rate 0.000492836
2017-10-11T11:04:23.711515: step 619, loss 0.29712, acc 0.90625, learning_rate 0.000491233
2017-10-11T11:04:23.789716: step 620, loss 0.332754, acc 0.90625, learning_rate 0.000489636
2017-10-11T11:04:23.862360: step 621, loss 0.182173, acc 0.953125, learning_rate 0.000488045
2017-10-11T11:04:23.935961: step 622, loss 0.313793, acc 0.890625, learning_rate 0.000486461
2017-10-11T11:04:24.008924: step 623, loss 0.281547, acc 0.90625, learning_rate 0.000484884
2017-10-11T11:04:24.077595: step 624, loss 0.209569, acc 0.90625, learning_rate 0.000483313
2017-10-11T11:04:24.147978: step 625, loss 0.219815, acc 0.921875, learning_rate 0.000481748
2017-10-11T11:04:24.223597: step 626, loss 0.284305, acc 0.9375, learning_rate 0.00048019
2017-10-11T11:04:24.295291: step 627, loss 0.228749, acc 0.953125, learning_rate 0.000478638
2017-10-11T11:04:24.372794: step 628, loss 0.219697, acc 0.9375, learning_rate 0.000477093
2017-10-11T11:04:24.443077: step 629, loss 0.163602, acc 0.9375, learning_rate 0.000475554
2017-10-11T11:04:24.517463: step 630, loss 0.286589, acc 0.890625, learning_rate 0.000474021
2017-10-11T11:04:24.588779: step 631, loss 0.285331, acc 0.9375, learning_rate 0.000472494
2017-10-11T11:04:24.658431: step 632, loss 0.272532, acc 0.875, learning_rate 0.000470974
2017-10-11T11:04:24.730356: step 633, loss 0.255969, acc 0.921875, learning_rate 0.000469459
2017-10-11T11:04:24.796483: step 634, loss 0.244043, acc 0.90625, learning_rate 0.000467951
2017-10-11T11:04:24.868489: step 635, loss 0.350009, acc 0.90625, learning_rate 0.000466449
2017-10-11T11:04:24.936939: step 636, loss 0.134658, acc 0.984375, learning_rate 0.000464954
2017-10-11T11:04:25.008074: step 637, loss 0.331409, acc 0.90625, learning_rate 0.000463464
2017-10-11T11:04:25.077655: step 638, loss 0.201223, acc 0.96875, learning_rate 0.00046198
2017-10-11T11:04:25.147348: step 639, loss 0.319605, acc 0.84375, learning_rate 0.000460503
2017-10-11T11:04:25.219025: step 640, loss 0.246827, acc 0.9375, learning_rate 0.000459031

Evaluation:
2017-10-11T11:04:25.382940: step 640, loss 0.270427, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-640

2017-10-11T11:04:25.880585: step 641, loss 0.163749, acc 0.984375, learning_rate 0.000457566
2017-10-11T11:04:25.949250: step 642, loss 0.244067, acc 0.90625, learning_rate 0.000456106
2017-10-11T11:04:26.018635: step 643, loss 0.359082, acc 0.90625, learning_rate 0.000454653
2017-10-11T11:04:26.089777: step 644, loss 0.310593, acc 0.890625, learning_rate 0.000453205
2017-10-11T11:04:26.159101: step 645, loss 0.333667, acc 0.890625, learning_rate 0.000451764
2017-10-11T11:04:26.232760: step 646, loss 0.373268, acc 0.84375, learning_rate 0.000450328
2017-10-11T11:04:26.305328: step 647, loss 0.165985, acc 0.953125, learning_rate 0.000448898
2017-10-11T11:04:26.376088: step 648, loss 0.397153, acc 0.859375, learning_rate 0.000447474
2017-10-11T11:04:26.448589: step 649, loss 0.371319, acc 0.890625, learning_rate 0.000446055
2017-10-11T11:04:26.518094: step 650, loss 0.360669, acc 0.859375, learning_rate 0.000444643
2017-10-11T11:04:26.594395: step 651, loss 0.462536, acc 0.859375, learning_rate 0.000443236
2017-10-11T11:04:26.668547: step 652, loss 0.211447, acc 0.9375, learning_rate 0.000441835
2017-10-11T11:04:26.739777: step 653, loss 0.266308, acc 0.9375, learning_rate 0.00044044
2017-10-11T11:04:26.812357: step 654, loss 0.279396, acc 0.90625, learning_rate 0.00043905
2017-10-11T11:04:26.888728: step 655, loss 0.255569, acc 0.9375, learning_rate 0.000437666
2017-10-11T11:04:26.960743: step 656, loss 0.234698, acc 0.90625, learning_rate 0.000436288
2017-10-11T11:04:27.038723: step 657, loss 0.184725, acc 0.921875, learning_rate 0.000434915
2017-10-11T11:04:27.105876: step 658, loss 0.273516, acc 0.921875, learning_rate 0.000433548
2017-10-11T11:04:27.176875: step 659, loss 0.270878, acc 0.9375, learning_rate 0.000432187
2017-10-11T11:04:27.247539: step 660, loss 0.30285, acc 0.890625, learning_rate 0.000430831
2017-10-11T11:04:27.315976: step 661, loss 0.272772, acc 0.9375, learning_rate 0.000429481
2017-10-11T11:04:27.387376: step 662, loss 0.301806, acc 0.90625, learning_rate 0.000428136
2017-10-11T11:04:27.456416: step 663, loss 0.179363, acc 0.96875, learning_rate 0.000426796
2017-10-11T11:04:27.527590: step 664, loss 0.460072, acc 0.875, learning_rate 0.000425463
2017-10-11T11:04:27.598571: step 665, loss 0.231754, acc 0.921875, learning_rate 0.000424134
2017-10-11T11:04:27.669717: step 666, loss 0.300073, acc 0.890625, learning_rate 0.000422811
2017-10-11T11:04:27.742429: step 667, loss 0.190661, acc 0.953125, learning_rate 0.000421493
2017-10-11T11:04:27.813561: step 668, loss 0.369244, acc 0.828125, learning_rate 0.000420181
2017-10-11T11:04:27.885077: step 669, loss 0.247883, acc 0.890625, learning_rate 0.000418874
2017-10-11T11:04:27.960806: step 670, loss 0.222547, acc 0.9375, learning_rate 0.000417573
2017-10-11T11:04:28.030924: step 671, loss 0.420987, acc 0.875, learning_rate 0.000416276
2017-10-11T11:04:28.102539: step 672, loss 0.307153, acc 0.90625, learning_rate 0.000414985
2017-10-11T11:04:28.170130: step 673, loss 0.354903, acc 0.921875, learning_rate 0.0004137
2017-10-11T11:04:28.245079: step 674, loss 0.219839, acc 0.9375, learning_rate 0.000412419
2017-10-11T11:04:28.317438: step 675, loss 0.325926, acc 0.890625, learning_rate 0.000411144
2017-10-11T11:04:28.388525: step 676, loss 0.275223, acc 0.9375, learning_rate 0.000409874
2017-10-11T11:04:28.461806: step 677, loss 0.264678, acc 0.921875, learning_rate 0.000408609
2017-10-11T11:04:28.533849: step 678, loss 0.228083, acc 0.953125, learning_rate 0.00040735
2017-10-11T11:04:28.607322: step 679, loss 0.33047, acc 0.90625, learning_rate 0.000406095
2017-10-11T11:04:28.676082: step 680, loss 0.262833, acc 0.9375, learning_rate 0.000404846

Evaluation:
2017-10-11T11:04:28.843788: step 680, loss 0.264991, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-680

2017-10-11T11:04:29.403907: step 681, loss 0.246513, acc 0.90625, learning_rate 0.000403601
2017-10-11T11:04:29.474023: step 682, loss 0.303522, acc 0.890625, learning_rate 0.000402362
2017-10-11T11:04:29.546898: step 683, loss 0.214592, acc 0.921875, learning_rate 0.000401128
2017-10-11T11:04:29.621191: step 684, loss 0.225828, acc 0.96875, learning_rate 0.000399899
2017-10-11T11:04:29.691003: step 685, loss 0.332125, acc 0.890625, learning_rate 0.000398675
2017-10-11T11:04:29.753782: step 686, loss 0.329873, acc 0.862745, learning_rate 0.000397456
2017-10-11T11:04:29.826377: step 687, loss 0.247854, acc 0.953125, learning_rate 0.000396241
2017-10-11T11:04:29.897567: step 688, loss 0.221881, acc 0.921875, learning_rate 0.000395032
2017-10-11T11:04:29.967916: step 689, loss 0.235586, acc 0.953125, learning_rate 0.000393828
2017-10-11T11:04:30.040562: step 690, loss 0.189191, acc 0.9375, learning_rate 0.000392629
2017-10-11T11:04:30.112834: step 691, loss 0.285638, acc 0.90625, learning_rate 0.000391434
2017-10-11T11:04:30.185767: step 692, loss 0.282543, acc 0.90625, learning_rate 0.000390245
2017-10-11T11:04:30.255384: step 693, loss 0.29969, acc 0.921875, learning_rate 0.00038906
2017-10-11T11:04:30.325830: step 694, loss 0.186306, acc 0.9375, learning_rate 0.00038788
2017-10-11T11:04:30.397170: step 695, loss 0.273898, acc 0.9375, learning_rate 0.000386705
2017-10-11T11:04:30.469535: step 696, loss 0.376436, acc 0.890625, learning_rate 0.000385535
2017-10-11T11:04:30.542077: step 697, loss 0.157993, acc 0.9375, learning_rate 0.000384369
2017-10-11T11:04:30.609508: step 698, loss 0.334903, acc 0.90625, learning_rate 0.000383209
2017-10-11T11:04:30.678168: step 699, loss 0.275878, acc 0.875, learning_rate 0.000382053
2017-10-11T11:04:30.752924: step 700, loss 0.235649, acc 0.9375, learning_rate 0.000380901
2017-10-11T11:04:30.825144: step 701, loss 0.22108, acc 0.921875, learning_rate 0.000379755
2017-10-11T11:04:30.902167: step 702, loss 0.205281, acc 0.953125, learning_rate 0.000378613
2017-10-11T11:04:30.984353: step 703, loss 0.401543, acc 0.84375, learning_rate 0.000377476
2017-10-11T11:04:31.051339: step 704, loss 0.268273, acc 0.890625, learning_rate 0.000376343
2017-10-11T11:04:31.123998: step 705, loss 0.302119, acc 0.9375, learning_rate 0.000375215
2017-10-11T11:04:31.197158: step 706, loss 0.320018, acc 0.890625, learning_rate 0.000374092
2017-10-11T11:04:31.266636: step 707, loss 0.31639, acc 0.890625, learning_rate 0.000372973
2017-10-11T11:04:31.338723: step 708, loss 0.27472, acc 0.890625, learning_rate 0.000371859
2017-10-11T11:04:31.409953: step 709, loss 0.23616, acc 0.90625, learning_rate 0.000370749
2017-10-11T11:04:31.478809: step 710, loss 0.325558, acc 0.875, learning_rate 0.000369644
2017-10-11T11:04:31.551205: step 711, loss 0.288786, acc 0.90625, learning_rate 0.000368543
2017-10-11T11:04:31.622646: step 712, loss 0.274234, acc 0.921875, learning_rate 0.000367447
2017-10-11T11:04:31.694419: step 713, loss 0.198428, acc 0.96875, learning_rate 0.000366356
2017-10-11T11:04:31.764170: step 714, loss 0.235192, acc 0.90625, learning_rate 0.000365268
2017-10-11T11:04:31.839104: step 715, loss 0.262118, acc 0.953125, learning_rate 0.000364186
2017-10-11T11:04:31.911561: step 716, loss 0.30061, acc 0.890625, learning_rate 0.000363107
2017-10-11T11:04:31.985651: step 717, loss 0.268039, acc 0.921875, learning_rate 0.000362033
2017-10-11T11:04:32.054926: step 718, loss 0.280045, acc 0.84375, learning_rate 0.000360964
2017-10-11T11:04:32.124814: step 719, loss 0.216008, acc 0.9375, learning_rate 0.000359899
2017-10-11T11:04:32.196231: step 720, loss 0.239443, acc 0.921875, learning_rate 0.000358838

Evaluation:
2017-10-11T11:04:32.352864: step 720, loss 0.262693, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-720

2017-10-11T11:04:33.000819: step 721, loss 0.40615, acc 0.796875, learning_rate 0.000357781
2017-10-11T11:04:33.070289: step 722, loss 0.341004, acc 0.859375, learning_rate 0.000356729
2017-10-11T11:04:33.140674: step 723, loss 0.220187, acc 0.90625, learning_rate 0.000355681
2017-10-11T11:04:33.211240: step 724, loss 0.256831, acc 0.875, learning_rate 0.000354637
2017-10-11T11:04:33.285975: step 725, loss 0.292174, acc 0.890625, learning_rate 0.000353598
2017-10-11T11:04:33.359768: step 726, loss 0.303898, acc 0.921875, learning_rate 0.000352563
2017-10-11T11:04:33.430685: step 727, loss 0.333652, acc 0.921875, learning_rate 0.000351532
2017-10-11T11:04:33.500180: step 728, loss 0.441301, acc 0.84375, learning_rate 0.000350505
2017-10-11T11:04:33.577305: step 729, loss 0.183407, acc 0.96875, learning_rate 0.000349483
2017-10-11T11:04:33.649209: step 730, loss 0.311452, acc 0.90625, learning_rate 0.000348465
2017-10-11T11:04:33.722798: step 731, loss 0.324957, acc 0.890625, learning_rate 0.00034745
2017-10-11T11:04:33.794265: step 732, loss 0.174983, acc 0.953125, learning_rate 0.00034644
2017-10-11T11:04:33.869328: step 733, loss 0.236333, acc 0.921875, learning_rate 0.000345434
2017-10-11T11:04:33.939850: step 734, loss 0.269472, acc 0.90625, learning_rate 0.000344433
2017-10-11T11:04:34.011272: step 735, loss 0.19227, acc 0.9375, learning_rate 0.000343435
2017-10-11T11:04:34.082231: step 736, loss 0.27345, acc 0.890625, learning_rate 0.000342441
2017-10-11T11:04:34.152893: step 737, loss 0.242779, acc 0.921875, learning_rate 0.000341452
2017-10-11T11:04:34.225900: step 738, loss 0.228091, acc 0.921875, learning_rate 0.000340466
2017-10-11T11:04:34.297884: step 739, loss 0.233891, acc 0.953125, learning_rate 0.000339485
2017-10-11T11:04:34.368060: step 740, loss 0.230218, acc 0.953125, learning_rate 0.000338507
2017-10-11T11:04:34.436656: step 741, loss 0.408743, acc 0.890625, learning_rate 0.000337534
2017-10-11T11:04:34.510124: step 742, loss 0.264698, acc 0.953125, learning_rate 0.000336564
2017-10-11T11:04:34.580244: step 743, loss 0.276363, acc 0.9375, learning_rate 0.000335598
2017-10-11T11:04:34.650228: step 744, loss 0.243333, acc 0.9375, learning_rate 0.000334637
2017-10-11T11:04:34.721236: step 745, loss 0.209148, acc 0.953125, learning_rate 0.000333679
2017-10-11T11:04:34.792473: step 746, loss 0.189301, acc 0.9375, learning_rate 0.000332725
2017-10-11T11:04:34.863981: step 747, loss 0.283233, acc 0.921875, learning_rate 0.000331775
2017-10-11T11:04:34.932603: step 748, loss 0.326371, acc 0.90625, learning_rate 0.000330829
2017-10-11T11:04:35.001131: step 749, loss 0.235889, acc 0.921875, learning_rate 0.000329887
2017-10-11T11:04:35.071961: step 750, loss 0.183034, acc 0.9375, learning_rate 0.000328949
2017-10-11T11:04:35.144679: step 751, loss 0.305735, acc 0.859375, learning_rate 0.000328014
2017-10-11T11:04:35.214733: step 752, loss 0.27516, acc 0.921875, learning_rate 0.000327083
2017-10-11T11:04:35.286670: step 753, loss 0.375705, acc 0.875, learning_rate 0.000326157
2017-10-11T11:04:35.355059: step 754, loss 0.338402, acc 0.890625, learning_rate 0.000325233
2017-10-11T11:04:35.423974: step 755, loss 0.239332, acc 0.9375, learning_rate 0.000324314
2017-10-11T11:04:35.491764: step 756, loss 0.152053, acc 0.953125, learning_rate 0.000323399
2017-10-11T11:04:35.561968: step 757, loss 0.274824, acc 0.890625, learning_rate 0.000322487
2017-10-11T11:04:35.632769: step 758, loss 0.369023, acc 0.90625, learning_rate 0.000321579
2017-10-11T11:04:35.702399: step 759, loss 0.407565, acc 0.875, learning_rate 0.000320674
2017-10-11T11:04:35.772896: step 760, loss 0.191731, acc 0.9375, learning_rate 0.000319773

Evaluation:
2017-10-11T11:04:35.938734: step 760, loss 0.263456, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-760

2017-10-11T11:04:36.436330: step 761, loss 0.33901, acc 0.890625, learning_rate 0.000318876
2017-10-11T11:04:36.512655: step 762, loss 0.290521, acc 0.921875, learning_rate 0.000317983
2017-10-11T11:04:36.582771: step 763, loss 0.246945, acc 0.9375, learning_rate 0.000317093
2017-10-11T11:04:36.653151: step 764, loss 0.221323, acc 0.953125, learning_rate 0.000316207
2017-10-11T11:04:36.726418: step 765, loss 0.245154, acc 0.921875, learning_rate 0.000315325
2017-10-11T11:04:36.795609: step 766, loss 0.259325, acc 0.921875, learning_rate 0.000314446
2017-10-11T11:04:36.867806: step 767, loss 0.174304, acc 0.921875, learning_rate 0.00031357
2017-10-11T11:04:36.938369: step 768, loss 0.276373, acc 0.90625, learning_rate 0.000312699
2017-10-11T11:04:37.012096: step 769, loss 0.448252, acc 0.859375, learning_rate 0.00031183
2017-10-11T11:04:37.081220: step 770, loss 0.421307, acc 0.8125, learning_rate 0.000310966
2017-10-11T11:04:37.152430: step 771, loss 0.313788, acc 0.90625, learning_rate 0.000310105
2017-10-11T11:04:37.221711: step 772, loss 0.393331, acc 0.859375, learning_rate 0.000309247
2017-10-11T11:04:37.289321: step 773, loss 0.278748, acc 0.90625, learning_rate 0.000308393
2017-10-11T11:04:37.359996: step 774, loss 0.380063, acc 0.859375, learning_rate 0.000307542
2017-10-11T11:04:37.432564: step 775, loss 0.337539, acc 0.859375, learning_rate 0.000306695
2017-10-11T11:04:37.499779: step 776, loss 0.225572, acc 0.921875, learning_rate 0.000305852
2017-10-11T11:04:37.570275: step 777, loss 0.228221, acc 0.921875, learning_rate 0.000305011
2017-10-11T11:04:37.642906: step 778, loss 0.300251, acc 0.890625, learning_rate 0.000304174
2017-10-11T11:04:37.713662: step 779, loss 0.242651, acc 0.890625, learning_rate 0.000303341
2017-10-11T11:04:37.784835: step 780, loss 0.275742, acc 0.921875, learning_rate 0.000302511
2017-10-11T11:04:37.858853: step 781, loss 0.252029, acc 0.90625, learning_rate 0.000301684
2017-10-11T11:04:37.931577: step 782, loss 0.453842, acc 0.84375, learning_rate 0.000300861
2017-10-11T11:04:38.002650: step 783, loss 0.160394, acc 0.9375, learning_rate 0.000300041
2017-10-11T11:04:38.066693: step 784, loss 0.289321, acc 0.862745, learning_rate 0.000299225
2017-10-11T11:04:38.137939: step 785, loss 0.199181, acc 0.953125, learning_rate 0.000298412
2017-10-11T11:04:38.216490: step 786, loss 0.21194, acc 0.9375, learning_rate 0.000297602
2017-10-11T11:04:38.286518: step 787, loss 0.177105, acc 0.96875, learning_rate 0.000296795
2017-10-11T11:04:38.355751: step 788, loss 0.255104, acc 0.953125, learning_rate 0.000295992
2017-10-11T11:04:38.428884: step 789, loss 0.213969, acc 0.90625, learning_rate 0.000295192
2017-10-11T11:04:38.500382: step 790, loss 0.249258, acc 0.90625, learning_rate 0.000294395
2017-10-11T11:04:38.568927: step 791, loss 0.251123, acc 0.890625, learning_rate 0.000293602
2017-10-11T11:04:38.638367: step 792, loss 0.297633, acc 0.90625, learning_rate 0.000292812
2017-10-11T11:04:38.709814: step 793, loss 0.362494, acc 0.875, learning_rate 0.000292025
2017-10-11T11:04:38.778279: step 794, loss 0.236661, acc 0.9375, learning_rate 0.000291241
2017-10-11T11:04:38.849895: step 795, loss 0.296266, acc 0.890625, learning_rate 0.00029046
2017-10-11T11:04:38.918027: step 796, loss 0.203384, acc 0.953125, learning_rate 0.000289683
2017-10-11T11:04:38.990470: step 797, loss 0.206986, acc 0.9375, learning_rate 0.000288908
2017-10-11T11:04:39.061710: step 798, loss 0.311917, acc 0.859375, learning_rate 0.000288137
2017-10-11T11:04:39.134829: step 799, loss 0.32002, acc 0.890625, learning_rate 0.000287369
2017-10-11T11:04:39.202385: step 800, loss 0.300108, acc 0.890625, learning_rate 0.000286605

Evaluation:
2017-10-11T11:04:39.364242: step 800, loss 0.265138, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-800

2017-10-11T11:04:40.046181: step 801, loss 0.309245, acc 0.90625, learning_rate 0.000285843
2017-10-11T11:04:40.117849: step 802, loss 0.0966024, acc 0.96875, learning_rate 0.000285084
2017-10-11T11:04:40.188483: step 803, loss 0.282784, acc 0.90625, learning_rate 0.000284329
2017-10-11T11:04:40.257517: step 804, loss 0.292077, acc 0.875, learning_rate 0.000283577
2017-10-11T11:04:40.328510: step 805, loss 0.396748, acc 0.859375, learning_rate 0.000282827
2017-10-11T11:04:40.397085: step 806, loss 0.162091, acc 0.96875, learning_rate 0.000282081
2017-10-11T11:04:40.468779: step 807, loss 0.178686, acc 0.953125, learning_rate 0.000281338
2017-10-11T11:04:40.540115: step 808, loss 0.225472, acc 0.90625, learning_rate 0.000280598
2017-10-11T11:04:40.609100: step 809, loss 0.327305, acc 0.875, learning_rate 0.00027986
2017-10-11T11:04:40.680278: step 810, loss 0.336695, acc 0.875, learning_rate 0.000279126
2017-10-11T11:04:40.750478: step 811, loss 0.276955, acc 0.875, learning_rate 0.000278395
2017-10-11T11:04:40.822989: step 812, loss 0.275698, acc 0.9375, learning_rate 0.000277667
2017-10-11T11:04:40.900150: step 813, loss 0.388744, acc 0.875, learning_rate 0.000276942
2017-10-11T11:04:40.973880: step 814, loss 0.202384, acc 0.9375, learning_rate 0.00027622
2017-10-11T11:04:41.044951: step 815, loss 0.173022, acc 0.96875, learning_rate 0.0002755
2017-10-11T11:04:41.114435: step 816, loss 0.216153, acc 0.921875, learning_rate 0.000274784
2017-10-11T11:04:41.184688: step 817, loss 0.236728, acc 0.90625, learning_rate 0.000274071
2017-10-11T11:04:41.256404: step 818, loss 0.246182, acc 0.890625, learning_rate 0.00027336
2017-10-11T11:04:41.327947: step 819, loss 0.297161, acc 0.90625, learning_rate 0.000272652
2017-10-11T11:04:41.398865: step 820, loss 0.274613, acc 0.90625, learning_rate 0.000271948
2017-10-11T11:04:41.471081: step 821, loss 0.206201, acc 0.90625, learning_rate 0.000271246
2017-10-11T11:04:41.541238: step 822, loss 0.228037, acc 0.96875, learning_rate 0.000270547
2017-10-11T11:04:41.611194: step 823, loss 0.315554, acc 0.90625, learning_rate 0.000269851
2017-10-11T11:04:41.685151: step 824, loss 0.357396, acc 0.875, learning_rate 0.000269157
2017-10-11T11:04:41.760901: step 825, loss 0.256816, acc 0.9375, learning_rate 0.000268467
2017-10-11T11:04:41.830351: step 826, loss 0.385573, acc 0.875, learning_rate 0.000267779
2017-10-11T11:04:41.904676: step 827, loss 0.332669, acc 0.875, learning_rate 0.000267094
2017-10-11T11:04:41.972911: step 828, loss 0.280572, acc 0.90625, learning_rate 0.000266412
2017-10-11T11:04:42.043873: step 829, loss 0.128071, acc 0.96875, learning_rate 0.000265733
2017-10-11T11:04:42.116835: step 830, loss 0.343989, acc 0.875, learning_rate 0.000265057
2017-10-11T11:04:42.185785: step 831, loss 0.420404, acc 0.84375, learning_rate 0.000264383
2017-10-11T11:04:42.259353: step 832, loss 0.201284, acc 0.90625, learning_rate 0.000263712
2017-10-11T11:04:42.331111: step 833, loss 0.165811, acc 0.921875, learning_rate 0.000263044
2017-10-11T11:04:42.403073: step 834, loss 0.212634, acc 0.9375, learning_rate 0.000262378
2017-10-11T11:04:42.476952: step 835, loss 0.170574, acc 0.984375, learning_rate 0.000261715
2017-10-11T11:04:42.547691: step 836, loss 0.36964, acc 0.875, learning_rate 0.000261055
2017-10-11T11:04:42.617639: step 837, loss 0.299559, acc 0.921875, learning_rate 0.000260398
2017-10-11T11:04:42.689260: step 838, loss 0.289116, acc 0.875, learning_rate 0.000259743
2017-10-11T11:04:42.758879: step 839, loss 0.182955, acc 0.9375, learning_rate 0.000259091
2017-10-11T11:04:42.831093: step 840, loss 0.318639, acc 0.90625, learning_rate 0.000258442

Evaluation:
2017-10-11T11:04:42.992226: step 840, loss 0.260251, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-840

2017-10-11T11:04:43.638111: step 841, loss 0.287169, acc 0.90625, learning_rate 0.000257795
2017-10-11T11:04:43.707351: step 842, loss 0.381195, acc 0.859375, learning_rate 0.000257151
2017-10-11T11:04:43.776543: step 843, loss 0.397592, acc 0.84375, learning_rate 0.00025651
2017-10-11T11:04:43.850284: step 844, loss 0.268715, acc 0.921875, learning_rate 0.000255871
2017-10-11T11:04:43.930447: step 845, loss 0.22774, acc 0.9375, learning_rate 0.000255235
2017-10-11T11:04:44.001874: step 846, loss 0.478923, acc 0.859375, learning_rate 0.000254601
2017-10-11T11:04:44.071706: step 847, loss 0.164814, acc 0.9375, learning_rate 0.00025397
2017-10-11T11:04:44.143857: step 848, loss 0.296507, acc 0.859375, learning_rate 0.000253341
2017-10-11T11:04:44.213905: step 849, loss 0.237242, acc 0.953125, learning_rate 0.000252716
2017-10-11T11:04:44.283689: step 850, loss 0.314412, acc 0.890625, learning_rate 0.000252092
2017-10-11T11:04:44.357843: step 851, loss 0.219129, acc 0.953125, learning_rate 0.000251471
2017-10-11T11:04:44.428322: step 852, loss 0.31625, acc 0.890625, learning_rate 0.000250853
2017-10-11T11:04:44.506115: step 853, loss 0.137256, acc 0.984375, learning_rate 0.000250237
2017-10-11T11:04:44.580011: step 854, loss 0.165904, acc 0.96875, learning_rate 0.000249624
2017-10-11T11:04:44.652997: step 855, loss 0.299513, acc 0.921875, learning_rate 0.000249013
2017-10-11T11:04:44.720222: step 856, loss 0.199024, acc 0.9375, learning_rate 0.000248405
2017-10-11T11:04:44.792422: step 857, loss 0.318762, acc 0.875, learning_rate 0.000247799
2017-10-11T11:04:44.863318: step 858, loss 0.253144, acc 0.9375, learning_rate 0.000247196
2017-10-11T11:04:44.932998: step 859, loss 0.223311, acc 0.890625, learning_rate 0.000246595
2017-10-11T11:04:45.002779: step 860, loss 0.327767, acc 0.890625, learning_rate 0.000245997
2017-10-11T11:04:45.072520: step 861, loss 0.190291, acc 0.953125, learning_rate 0.000245401
2017-10-11T11:04:45.143712: step 862, loss 0.329938, acc 0.875, learning_rate 0.000244808
2017-10-11T11:04:45.215377: step 863, loss 0.166212, acc 0.96875, learning_rate 0.000244216
2017-10-11T11:04:45.290237: step 864, loss 0.463983, acc 0.828125, learning_rate 0.000243628
2017-10-11T11:04:45.362937: step 865, loss 0.235071, acc 0.890625, learning_rate 0.000243042
2017-10-11T11:04:45.430806: step 866, loss 0.204806, acc 0.9375, learning_rate 0.000242458
2017-10-11T11:04:45.500082: step 867, loss 0.296406, acc 0.875, learning_rate 0.000241876
2017-10-11T11:04:45.572969: step 868, loss 0.232241, acc 0.9375, learning_rate 0.000241297
2017-10-11T11:04:45.641936: step 869, loss 0.143789, acc 0.96875, learning_rate 0.00024072
2017-10-11T11:04:45.711421: step 870, loss 0.356252, acc 0.90625, learning_rate 0.000240146
2017-10-11T11:04:45.782609: step 871, loss 0.261976, acc 0.9375, learning_rate 0.000239574
2017-10-11T11:04:45.853737: step 872, loss 0.273907, acc 0.921875, learning_rate 0.000239004
2017-10-11T11:04:45.925718: step 873, loss 0.320713, acc 0.890625, learning_rate 0.000238437
2017-10-11T11:04:45.996162: step 874, loss 0.223615, acc 0.953125, learning_rate 0.000237872
2017-10-11T11:04:46.067797: step 875, loss 0.313107, acc 0.890625, learning_rate 0.000237309
2017-10-11T11:04:46.147527: step 876, loss 0.286531, acc 0.90625, learning_rate 0.000236749
2017-10-11T11:04:46.219302: step 877, loss 0.284139, acc 0.9375, learning_rate 0.00023619
2017-10-11T11:04:46.290612: step 878, loss 0.232956, acc 0.9375, learning_rate 0.000235635
2017-10-11T11:04:46.360270: step 879, loss 0.320345, acc 0.90625, learning_rate 0.000235081
2017-10-11T11:04:46.435720: step 880, loss 0.225022, acc 0.90625, learning_rate 0.00023453

Evaluation:
2017-10-11T11:04:46.593046: step 880, loss 0.261071, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-880

2017-10-11T11:04:47.095764: step 881, loss 0.229548, acc 0.9375, learning_rate 0.00023398
2017-10-11T11:04:47.160176: step 882, loss 0.165032, acc 0.941176, learning_rate 0.000233434
2017-10-11T11:04:47.230945: step 883, loss 0.262754, acc 0.921875, learning_rate 0.000232889
2017-10-11T11:04:47.300756: step 884, loss 0.452811, acc 0.84375, learning_rate 0.000232346
2017-10-11T11:04:47.375159: step 885, loss 0.132702, acc 0.96875, learning_rate 0.000231806
2017-10-11T11:04:47.448873: step 886, loss 0.223012, acc 0.9375, learning_rate 0.000231268
2017-10-11T11:04:47.520049: step 887, loss 0.23701, acc 0.953125, learning_rate 0.000230732
2017-10-11T11:04:47.593984: step 888, loss 0.394916, acc 0.875, learning_rate 0.000230199
2017-10-11T11:04:47.664439: step 889, loss 0.37937, acc 0.859375, learning_rate 0.000229667
2017-10-11T11:04:47.735685: step 890, loss 0.223358, acc 0.921875, learning_rate 0.000229138
2017-10-11T11:04:47.805353: step 891, loss 0.27797, acc 0.921875, learning_rate 0.000228611
2017-10-11T11:04:47.880532: step 892, loss 0.436814, acc 0.859375, learning_rate 0.000228086
2017-10-11T11:04:47.951027: step 893, loss 0.260416, acc 0.953125, learning_rate 0.000227563
2017-10-11T11:04:48.020867: step 894, loss 0.237947, acc 0.953125, learning_rate 0.000227043
2017-10-11T11:04:48.089082: step 895, loss 0.247439, acc 0.9375, learning_rate 0.000226524
2017-10-11T11:04:48.158118: step 896, loss 0.229908, acc 0.96875, learning_rate 0.000226008
2017-10-11T11:04:48.230591: step 897, loss 0.214396, acc 0.953125, learning_rate 0.000225493
2017-10-11T11:04:48.300725: step 898, loss 0.223052, acc 0.9375, learning_rate 0.000224981
2017-10-11T11:04:48.373664: step 899, loss 0.451366, acc 0.84375, learning_rate 0.000224471
2017-10-11T11:04:48.446900: step 900, loss 0.33859, acc 0.859375, learning_rate 0.000223963
2017-10-11T11:04:48.520361: step 901, loss 0.162187, acc 0.953125, learning_rate 0.000223457
2017-10-11T11:04:48.589693: step 902, loss 0.378314, acc 0.90625, learning_rate 0.000222953
2017-10-11T11:04:48.662155: step 903, loss 0.282121, acc 0.890625, learning_rate 0.000222451
2017-10-11T11:04:48.732776: step 904, loss 0.207925, acc 0.9375, learning_rate 0.000221951
2017-10-11T11:04:48.803636: step 905, loss 0.336047, acc 0.890625, learning_rate 0.000221453
2017-10-11T11:04:48.873354: step 906, loss 0.297085, acc 0.890625, learning_rate 0.000220958
2017-10-11T11:04:48.944047: step 907, loss 0.278514, acc 0.890625, learning_rate 0.000220464
2017-10-11T11:04:49.012790: step 908, loss 0.332838, acc 0.875, learning_rate 0.000219972
2017-10-11T11:04:49.082051: step 909, loss 0.260755, acc 0.9375, learning_rate 0.000219483
2017-10-11T11:04:49.153744: step 910, loss 0.404624, acc 0.90625, learning_rate 0.000218995
2017-10-11T11:04:49.222653: step 911, loss 0.342748, acc 0.875, learning_rate 0.000218509
2017-10-11T11:04:49.291972: step 912, loss 0.163906, acc 0.9375, learning_rate 0.000218025
2017-10-11T11:04:49.364774: step 913, loss 0.258235, acc 0.953125, learning_rate 0.000217544
2017-10-11T11:04:49.433445: step 914, loss 0.30614, acc 0.890625, learning_rate 0.000217064
2017-10-11T11:04:49.511561: step 915, loss 0.272167, acc 0.90625, learning_rate 0.000216586
2017-10-11T11:04:49.580538: step 916, loss 0.256903, acc 0.921875, learning_rate 0.00021611
2017-10-11T11:04:49.653077: step 917, loss 0.262044, acc 0.921875, learning_rate 0.000215636
2017-10-11T11:04:49.725157: step 918, loss 0.160594, acc 0.921875, learning_rate 0.000215164
2017-10-11T11:04:49.796359: step 919, loss 0.191661, acc 0.953125, learning_rate 0.000214694
2017-10-11T11:04:49.873719: step 920, loss 0.261792, acc 0.890625, learning_rate 0.000214226

Evaluation:
2017-10-11T11:04:50.040862: step 920, loss 0.259161, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-920

2017-10-11T11:04:50.602999: step 921, loss 0.253701, acc 0.9375, learning_rate 0.00021376
2017-10-11T11:04:50.675072: step 922, loss 0.37065, acc 0.890625, learning_rate 0.000213295
2017-10-11T11:04:50.745905: step 923, loss 0.269684, acc 0.90625, learning_rate 0.000212833
2017-10-11T11:04:50.816605: step 924, loss 0.256168, acc 0.90625, learning_rate 0.000212372
2017-10-11T11:04:50.889196: step 925, loss 0.214218, acc 0.921875, learning_rate 0.000211914
2017-10-11T11:04:50.959516: step 926, loss 0.252605, acc 0.890625, learning_rate 0.000211457
2017-10-11T11:04:51.032897: step 927, loss 0.274991, acc 0.9375, learning_rate 0.000211002
2017-10-11T11:04:51.105175: step 928, loss 0.185122, acc 0.921875, learning_rate 0.000210549
2017-10-11T11:04:51.180624: step 929, loss 0.188836, acc 0.953125, learning_rate 0.000210098
2017-10-11T11:04:51.251862: step 930, loss 0.295016, acc 0.921875, learning_rate 0.000209648
2017-10-11T11:04:51.323296: step 931, loss 0.329149, acc 0.859375, learning_rate 0.000209201
2017-10-11T11:04:51.397832: step 932, loss 0.275475, acc 0.90625, learning_rate 0.000208755
2017-10-11T11:04:51.467559: step 933, loss 0.226776, acc 0.9375, learning_rate 0.000208311
2017-10-11T11:04:51.538834: step 934, loss 0.31144, acc 0.90625, learning_rate 0.000207869
2017-10-11T11:04:51.607341: step 935, loss 0.339163, acc 0.875, learning_rate 0.000207429
2017-10-11T11:04:51.679592: step 936, loss 0.265629, acc 0.90625, learning_rate 0.00020699
2017-10-11T11:04:51.752128: step 937, loss 0.128761, acc 0.96875, learning_rate 0.000206554
2017-10-11T11:04:51.823157: step 938, loss 0.223835, acc 0.921875, learning_rate 0.000206119
2017-10-11T11:04:51.900422: step 939, loss 0.414134, acc 0.828125, learning_rate 0.000205685
2017-10-11T11:04:51.974231: step 940, loss 0.29263, acc 0.875, learning_rate 0.000205254
2017-10-11T11:04:52.043570: step 941, loss 0.406819, acc 0.828125, learning_rate 0.000204824
2017-10-11T11:04:52.116095: step 942, loss 0.454495, acc 0.8125, learning_rate 0.000204397
2017-10-11T11:04:52.190543: step 943, loss 0.280324, acc 0.9375, learning_rate 0.00020397
2017-10-11T11:04:52.262792: step 944, loss 0.259813, acc 0.890625, learning_rate 0.000203546
2017-10-11T11:04:52.338864: step 945, loss 0.29808, acc 0.9375, learning_rate 0.000203123
2017-10-11T11:04:52.409751: step 946, loss 0.437949, acc 0.859375, learning_rate 0.000202702
2017-10-11T11:04:52.479095: step 947, loss 0.274175, acc 0.859375, learning_rate 0.000202283
2017-10-11T11:04:52.552903: step 948, loss 0.201577, acc 0.921875, learning_rate 0.000201866
2017-10-11T11:04:52.622371: step 949, loss 0.203835, acc 0.953125, learning_rate 0.00020145
2017-10-11T11:04:52.690998: step 950, loss 0.208227, acc 0.9375, learning_rate 0.000201036
2017-10-11T11:04:52.766074: step 951, loss 0.349767, acc 0.921875, learning_rate 0.000200623
2017-10-11T11:04:52.836564: step 952, loss 0.174031, acc 0.953125, learning_rate 0.000200213
2017-10-11T11:04:52.906757: step 953, loss 0.355475, acc 0.875, learning_rate 0.000199804
2017-10-11T11:04:52.976952: step 954, loss 0.306006, acc 0.875, learning_rate 0.000199396
2017-10-11T11:04:53.048652: step 955, loss 0.31832, acc 0.875, learning_rate 0.000198991
2017-10-11T11:04:53.119571: step 956, loss 0.293266, acc 0.90625, learning_rate 0.000198587
2017-10-11T11:04:53.192688: step 957, loss 0.28, acc 0.953125, learning_rate 0.000198184
2017-10-11T11:04:53.263268: step 958, loss 0.214062, acc 0.90625, learning_rate 0.000197783
2017-10-11T11:04:53.333163: step 959, loss 0.206377, acc 0.875, learning_rate 0.000197384
2017-10-11T11:04:53.405570: step 960, loss 0.389164, acc 0.921875, learning_rate 0.000196987

Evaluation:
2017-10-11T11:04:53.570183: step 960, loss 0.259009, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-960

2017-10-11T11:04:54.398290: step 961, loss 0.222354, acc 0.921875, learning_rate 0.000196591
2017-10-11T11:04:54.469855: step 962, loss 0.286199, acc 0.90625, learning_rate 0.000196197
2017-10-11T11:04:54.542432: step 963, loss 0.277285, acc 0.90625, learning_rate 0.000195804
2017-10-11T11:04:54.614162: step 964, loss 0.259642, acc 0.921875, learning_rate 0.000195413
2017-10-11T11:04:54.686549: step 965, loss 0.239211, acc 0.9375, learning_rate 0.000195023
2017-10-11T11:04:54.759132: step 966, loss 0.290083, acc 0.890625, learning_rate 0.000194636
2017-10-11T11:04:54.831239: step 967, loss 0.276899, acc 0.90625, learning_rate 0.000194249
2017-10-11T11:04:54.902088: step 968, loss 0.231247, acc 0.9375, learning_rate 0.000193865
2017-10-11T11:04:54.975062: step 969, loss 0.251694, acc 0.90625, learning_rate 0.000193482
2017-10-11T11:04:55.043526: step 970, loss 0.256629, acc 0.953125, learning_rate 0.0001931
2017-10-11T11:04:55.114504: step 971, loss 0.163901, acc 0.96875, learning_rate 0.00019272
2017-10-11T11:04:55.184177: step 972, loss 0.184226, acc 0.953125, learning_rate 0.000192341
2017-10-11T11:04:55.254128: step 973, loss 0.277592, acc 0.921875, learning_rate 0.000191965
2017-10-11T11:04:55.323161: step 974, loss 0.27407, acc 0.9375, learning_rate 0.000191589
2017-10-11T11:04:55.394594: step 975, loss 0.273116, acc 0.921875, learning_rate 0.000191215
2017-10-11T11:04:55.465481: step 976, loss 0.18326, acc 0.90625, learning_rate 0.000190843
2017-10-11T11:04:55.536583: step 977, loss 0.216919, acc 0.9375, learning_rate 0.000190472
2017-10-11T11:04:55.606918: step 978, loss 0.20546, acc 0.9375, learning_rate 0.000190103
2017-10-11T11:04:55.675695: step 979, loss 0.212706, acc 0.953125, learning_rate 0.000189735
2017-10-11T11:04:55.739962: step 980, loss 0.366777, acc 0.843137, learning_rate 0.000189369
2017-10-11T11:04:55.810130: step 981, loss 0.343445, acc 0.875, learning_rate 0.000189004
2017-10-11T11:04:55.885336: step 982, loss 0.427595, acc 0.875, learning_rate 0.000188641
2017-10-11T11:04:55.957777: step 983, loss 0.232959, acc 0.953125, learning_rate 0.000188279
2017-10-11T11:04:56.029499: step 984, loss 0.203026, acc 0.921875, learning_rate 0.000187919
2017-10-11T11:04:56.097992: step 985, loss 0.253008, acc 0.90625, learning_rate 0.00018756
2017-10-11T11:04:56.171514: step 986, loss 0.32068, acc 0.90625, learning_rate 0.000187202
2017-10-11T11:04:56.241036: step 987, loss 0.256955, acc 0.890625, learning_rate 0.000186846
2017-10-11T11:04:56.314980: step 988, loss 0.227596, acc 0.90625, learning_rate 0.000186492
2017-10-11T11:04:56.386533: step 989, loss 0.2579, acc 0.921875, learning_rate 0.000186139
2017-10-11T11:04:56.461506: step 990, loss 0.289073, acc 0.921875, learning_rate 0.000185787
2017-10-11T11:04:56.530306: step 991, loss 0.270324, acc 0.9375, learning_rate 0.000185437
2017-10-11T11:04:56.600448: step 992, loss 0.276522, acc 0.890625, learning_rate 0.000185088
2017-10-11T11:04:56.671789: step 993, loss 0.23526, acc 0.875, learning_rate 0.000184741
2017-10-11T11:04:56.741412: step 994, loss 0.182536, acc 0.9375, learning_rate 0.000184395
2017-10-11T11:04:56.812359: step 995, loss 0.350669, acc 0.859375, learning_rate 0.000184051
2017-10-11T11:04:56.884858: step 996, loss 0.320662, acc 0.890625, learning_rate 0.000183708
2017-10-11T11:04:56.956564: step 997, loss 0.192965, acc 0.984375, learning_rate 0.000183366
2017-10-11T11:04:57.027372: step 998, loss 0.307215, acc 0.90625, learning_rate 0.000183026
2017-10-11T11:04:57.097254: step 999, loss 0.310291, acc 0.890625, learning_rate 0.000182687
2017-10-11T11:04:57.171942: step 1000, loss 0.255453, acc 0.9375, learning_rate 0.000182349

Evaluation:
2017-10-11T11:04:57.333783: step 1000, loss 0.258774, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1000

2017-10-11T11:04:57.971904: step 1001, loss 0.182729, acc 0.953125, learning_rate 0.000182013
2017-10-11T11:04:58.049392: step 1002, loss 0.248586, acc 0.90625, learning_rate 0.000181678
2017-10-11T11:04:58.124797: step 1003, loss 0.235255, acc 0.921875, learning_rate 0.000181345
2017-10-11T11:04:58.195191: step 1004, loss 0.308239, acc 0.890625, learning_rate 0.000181013
2017-10-11T11:04:58.385671: step 1005, loss 0.309992, acc 0.875, learning_rate 0.000180682
2017-10-11T11:04:58.456815: step 1006, loss 0.313441, acc 0.859375, learning_rate 0.000180353
2017-10-11T11:04:58.527958: step 1007, loss 0.233427, acc 0.921875, learning_rate 0.000180025
2017-10-11T11:04:58.599644: step 1008, loss 0.225327, acc 0.890625, learning_rate 0.000179698
2017-10-11T11:04:58.668758: step 1009, loss 0.205796, acc 0.921875, learning_rate 0.000179373
2017-10-11T11:04:58.736732: step 1010, loss 0.249646, acc 0.90625, learning_rate 0.000179049
2017-10-11T11:04:58.806124: step 1011, loss 0.14574, acc 0.953125, learning_rate 0.000178726
2017-10-11T11:04:58.876568: step 1012, loss 0.148394, acc 0.953125, learning_rate 0.000178405
2017-10-11T11:04:58.946333: step 1013, loss 0.285435, acc 0.890625, learning_rate 0.000178085
2017-10-11T11:04:59.018058: step 1014, loss 0.313809, acc 0.9375, learning_rate 0.000177766
2017-10-11T11:04:59.090209: step 1015, loss 0.370884, acc 0.890625, learning_rate 0.000177449
2017-10-11T11:04:59.157515: step 1016, loss 0.271137, acc 0.90625, learning_rate 0.000177133
2017-10-11T11:04:59.225934: step 1017, loss 0.212151, acc 0.921875, learning_rate 0.000176818
2017-10-11T11:04:59.294823: step 1018, loss 0.230672, acc 0.921875, learning_rate 0.000176504
2017-10-11T11:04:59.364470: step 1019, loss 0.254548, acc 0.953125, learning_rate 0.000176192
2017-10-11T11:04:59.433545: step 1020, loss 0.186411, acc 0.984375, learning_rate 0.000175881
2017-10-11T11:04:59.504756: step 1021, loss 0.220242, acc 0.9375, learning_rate 0.000175571
2017-10-11T11:04:59.580330: step 1022, loss 0.233322, acc 0.953125, learning_rate 0.000175263
2017-10-11T11:04:59.649093: step 1023, loss 0.266264, acc 0.90625, learning_rate 0.000174956
2017-10-11T11:04:59.722215: step 1024, loss 0.246833, acc 0.890625, learning_rate 0.00017465
2017-10-11T11:04:59.791618: step 1025, loss 0.184145, acc 0.953125, learning_rate 0.000174345
2017-10-11T11:04:59.865552: step 1026, loss 0.154029, acc 0.96875, learning_rate 0.000174042
2017-10-11T11:04:59.943647: step 1027, loss 0.298467, acc 0.921875, learning_rate 0.000173739
2017-10-11T11:05:00.017007: step 1028, loss 0.267302, acc 0.9375, learning_rate 0.000173438
2017-10-11T11:05:00.092704: step 1029, loss 0.190708, acc 0.921875, learning_rate 0.000173139
2017-10-11T11:05:00.165255: step 1030, loss 0.303002, acc 0.890625, learning_rate 0.00017284
2017-10-11T11:05:00.237104: step 1031, loss 0.316837, acc 0.90625, learning_rate 0.000172543
2017-10-11T11:05:00.308078: step 1032, loss 0.372413, acc 0.875, learning_rate 0.000172247
2017-10-11T11:05:00.378120: step 1033, loss 0.239603, acc 0.921875, learning_rate 0.000171952
2017-10-11T11:05:00.447626: step 1034, loss 0.197566, acc 0.9375, learning_rate 0.000171658
2017-10-11T11:05:00.518703: step 1035, loss 0.281619, acc 0.890625, learning_rate 0.000171366
2017-10-11T11:05:00.589789: step 1036, loss 0.282833, acc 0.90625, learning_rate 0.000171074
2017-10-11T11:05:00.659288: step 1037, loss 0.225115, acc 0.953125, learning_rate 0.000170784
2017-10-11T11:05:00.730287: step 1038, loss 0.451317, acc 0.828125, learning_rate 0.000170495
2017-10-11T11:05:00.802104: step 1039, loss 0.316196, acc 0.90625, learning_rate 0.000170208
2017-10-11T11:05:00.877845: step 1040, loss 0.369128, acc 0.875, learning_rate 0.000169921

Evaluation:
2017-10-11T11:05:01.042258: step 1040, loss 0.258329, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1040

2017-10-11T11:05:01.542520: step 1041, loss 0.269076, acc 0.921875, learning_rate 0.000169636
2017-10-11T11:05:01.613141: step 1042, loss 0.130624, acc 0.96875, learning_rate 0.000169351
2017-10-11T11:05:01.682328: step 1043, loss 0.218949, acc 0.9375, learning_rate 0.000169068
2017-10-11T11:05:01.754651: step 1044, loss 0.332447, acc 0.90625, learning_rate 0.000168786
2017-10-11T11:05:01.828204: step 1045, loss 0.252756, acc 0.953125, learning_rate 0.000168506
2017-10-11T11:05:01.905732: step 1046, loss 0.300173, acc 0.921875, learning_rate 0.000168226
2017-10-11T11:05:01.973995: step 1047, loss 0.349489, acc 0.875, learning_rate 0.000167947
2017-10-11T11:05:02.046246: step 1048, loss 0.279127, acc 0.890625, learning_rate 0.00016767
2017-10-11T11:05:02.117660: step 1049, loss 0.332636, acc 0.875, learning_rate 0.000167394
2017-10-11T11:05:02.186038: step 1050, loss 0.285985, acc 0.90625, learning_rate 0.000167119
2017-10-11T11:05:02.257647: step 1051, loss 0.153518, acc 0.953125, learning_rate 0.000166845
2017-10-11T11:05:02.332985: step 1052, loss 0.252012, acc 0.921875, learning_rate 0.000166572
2017-10-11T11:05:02.404089: step 1053, loss 0.202042, acc 0.953125, learning_rate 0.0001663
2017-10-11T11:05:02.479416: step 1054, loss 0.163462, acc 0.984375, learning_rate 0.00016603
2017-10-11T11:05:02.548666: step 1055, loss 0.40062, acc 0.84375, learning_rate 0.00016576
2017-10-11T11:05:02.621181: step 1056, loss 0.168044, acc 0.921875, learning_rate 0.000165492
2017-10-11T11:05:02.692473: step 1057, loss 0.184401, acc 0.953125, learning_rate 0.000165224
2017-10-11T11:05:02.760128: step 1058, loss 0.242763, acc 0.921875, learning_rate 0.000164958
2017-10-11T11:05:02.829521: step 1059, loss 0.411473, acc 0.875, learning_rate 0.000164693
2017-10-11T11:05:02.899075: step 1060, loss 0.226484, acc 0.90625, learning_rate 0.000164429
2017-10-11T11:05:02.968401: step 1061, loss 0.325477, acc 0.921875, learning_rate 0.000164166
2017-10-11T11:05:03.041266: step 1062, loss 0.184531, acc 0.9375, learning_rate 0.000163904
2017-10-11T11:05:03.113520: step 1063, loss 0.408036, acc 0.875, learning_rate 0.000163643
2017-10-11T11:05:03.187440: step 1064, loss 0.20179, acc 0.953125, learning_rate 0.000163383
2017-10-11T11:05:03.255138: step 1065, loss 0.158007, acc 0.96875, learning_rate 0.000163125
2017-10-11T11:05:03.325268: step 1066, loss 0.225682, acc 0.9375, learning_rate 0.000162867
2017-10-11T11:05:03.394571: step 1067, loss 0.20063, acc 0.96875, learning_rate 0.00016261
2017-10-11T11:05:03.463333: step 1068, loss 0.343746, acc 0.90625, learning_rate 0.000162355
2017-10-11T11:05:03.535360: step 1069, loss 0.215136, acc 0.921875, learning_rate 0.0001621
2017-10-11T11:05:03.613749: step 1070, loss 0.312173, acc 0.90625, learning_rate 0.000161847
2017-10-11T11:05:03.684281: step 1071, loss 0.224636, acc 0.96875, learning_rate 0.000161594
2017-10-11T11:05:03.752600: step 1072, loss 0.223824, acc 0.9375, learning_rate 0.000161343
2017-10-11T11:05:03.828139: step 1073, loss 0.181099, acc 0.96875, learning_rate 0.000161093
2017-10-11T11:05:03.903300: step 1074, loss 0.306992, acc 0.921875, learning_rate 0.000160843
2017-10-11T11:05:03.974643: step 1075, loss 0.280757, acc 0.859375, learning_rate 0.000160595
2017-10-11T11:05:04.045908: step 1076, loss 0.15755, acc 0.96875, learning_rate 0.000160348
2017-10-11T11:05:04.118254: step 1077, loss 0.23936, acc 0.921875, learning_rate 0.000160101
2017-10-11T11:05:04.179421: step 1078, loss 0.190879, acc 0.921569, learning_rate 0.000159856
2017-10-11T11:05:04.254720: step 1079, loss 0.327668, acc 0.875, learning_rate 0.000159612
2017-10-11T11:05:04.325899: step 1080, loss 0.224772, acc 0.921875, learning_rate 0.000159368

Evaluation:
2017-10-11T11:05:04.495043: step 1080, loss 0.257851, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1080

2017-10-11T11:05:05.054942: step 1081, loss 0.373016, acc 0.90625, learning_rate 0.000159126
2017-10-11T11:05:05.129020: step 1082, loss 0.282551, acc 0.921875, learning_rate 0.000158885
2017-10-11T11:05:05.199297: step 1083, loss 0.178797, acc 0.9375, learning_rate 0.000158644
2017-10-11T11:05:05.268798: step 1084, loss 0.210497, acc 0.921875, learning_rate 0.000158405
2017-10-11T11:05:05.342945: step 1085, loss 0.191766, acc 0.96875, learning_rate 0.000158167
2017-10-11T11:05:05.411407: step 1086, loss 0.270131, acc 0.890625, learning_rate 0.000157929
2017-10-11T11:05:05.480344: step 1087, loss 0.267998, acc 0.90625, learning_rate 0.000157693
2017-10-11T11:05:05.552374: step 1088, loss 0.225746, acc 0.9375, learning_rate 0.000157457
2017-10-11T11:05:05.628060: step 1089, loss 0.31808, acc 0.90625, learning_rate 0.000157223
2017-10-11T11:05:05.697696: step 1090, loss 0.258602, acc 0.9375, learning_rate 0.000156989
2017-10-11T11:05:05.772329: step 1091, loss 0.321412, acc 0.875, learning_rate 0.000156757
2017-10-11T11:05:05.846186: step 1092, loss 0.251853, acc 0.9375, learning_rate 0.000156525
2017-10-11T11:05:05.917124: step 1093, loss 0.223739, acc 0.9375, learning_rate 0.000156294
2017-10-11T11:05:05.989104: step 1094, loss 0.358815, acc 0.859375, learning_rate 0.000156064
2017-10-11T11:05:06.062189: step 1095, loss 0.176551, acc 0.9375, learning_rate 0.000155836
2017-10-11T11:05:06.136924: step 1096, loss 0.389986, acc 0.859375, learning_rate 0.000155608
2017-10-11T11:05:06.208348: step 1097, loss 0.172935, acc 0.9375, learning_rate 0.000155381
2017-10-11T11:05:06.278014: step 1098, loss 0.264886, acc 0.90625, learning_rate 0.000155155
2017-10-11T11:05:06.348720: step 1099, loss 0.211928, acc 0.921875, learning_rate 0.000154929
2017-10-11T11:05:06.420316: step 1100, loss 0.309884, acc 0.890625, learning_rate 0.000154705
2017-10-11T11:05:06.490785: step 1101, loss 0.393764, acc 0.859375, learning_rate 0.000154482
2017-10-11T11:05:06.560420: step 1102, loss 0.173331, acc 0.96875, learning_rate 0.00015426
2017-10-11T11:05:06.632228: step 1103, loss 0.376872, acc 0.875, learning_rate 0.000154038
2017-10-11T11:05:06.705822: step 1104, loss 0.305588, acc 0.9375, learning_rate 0.000153818
2017-10-11T11:05:06.775971: step 1105, loss 0.199023, acc 0.953125, learning_rate 0.000153598
2017-10-11T11:05:06.848900: step 1106, loss 0.232901, acc 0.90625, learning_rate 0.000153379
2017-10-11T11:05:06.925315: step 1107, loss 0.327803, acc 0.90625, learning_rate 0.000153161
2017-10-11T11:05:06.995908: step 1108, loss 0.340934, acc 0.84375, learning_rate 0.000152944
2017-10-11T11:05:07.067156: step 1109, loss 0.111178, acc 0.984375, learning_rate 0.000152728
2017-10-11T11:05:07.135413: step 1110, loss 0.22698, acc 0.9375, learning_rate 0.000152513
2017-10-11T11:05:07.207071: step 1111, loss 0.199081, acc 0.9375, learning_rate 0.000152299
2017-10-11T11:05:07.281397: step 1112, loss 0.275501, acc 0.875, learning_rate 0.000152085
2017-10-11T11:05:07.351581: step 1113, loss 0.265966, acc 0.921875, learning_rate 0.000151872
2017-10-11T11:05:07.424035: step 1114, loss 0.554731, acc 0.796875, learning_rate 0.000151661
2017-10-11T11:05:07.494937: step 1115, loss 0.259692, acc 0.9375, learning_rate 0.00015145
2017-10-11T11:05:07.563687: step 1116, loss 0.282791, acc 0.9375, learning_rate 0.00015124
2017-10-11T11:05:07.632411: step 1117, loss 0.26909, acc 0.90625, learning_rate 0.000151031
2017-10-11T11:05:07.704505: step 1118, loss 0.216075, acc 0.96875, learning_rate 0.000150822
2017-10-11T11:05:07.774282: step 1119, loss 0.347266, acc 0.875, learning_rate 0.000150615
2017-10-11T11:05:07.846288: step 1120, loss 0.240866, acc 0.921875, learning_rate 0.000150408

Evaluation:
2017-10-11T11:05:08.016427: step 1120, loss 0.257609, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1120

2017-10-11T11:05:08.843155: step 1121, loss 0.289158, acc 0.859375, learning_rate 0.000150203
2017-10-11T11:05:08.911668: step 1122, loss 0.225661, acc 0.9375, learning_rate 0.000149998
2017-10-11T11:05:08.979285: step 1123, loss 0.23264, acc 0.9375, learning_rate 0.000149794
2017-10-11T11:05:09.052336: step 1124, loss 0.258428, acc 0.890625, learning_rate 0.00014959
2017-10-11T11:05:09.126805: step 1125, loss 0.233287, acc 0.921875, learning_rate 0.000149388
2017-10-11T11:05:09.200888: step 1126, loss 0.382081, acc 0.921875, learning_rate 0.000149186
2017-10-11T11:05:09.275820: step 1127, loss 0.234622, acc 0.953125, learning_rate 0.000148986
2017-10-11T11:05:09.347668: step 1128, loss 0.258224, acc 0.9375, learning_rate 0.000148786
2017-10-11T11:05:09.416315: step 1129, loss 0.407458, acc 0.890625, learning_rate 0.000148587
2017-10-11T11:05:09.486780: step 1130, loss 0.154476, acc 0.96875, learning_rate 0.000148388
2017-10-11T11:05:09.559140: step 1131, loss 0.453581, acc 0.859375, learning_rate 0.000148191
2017-10-11T11:05:09.630735: step 1132, loss 0.273491, acc 0.953125, learning_rate 0.000147994
2017-10-11T11:05:09.699185: step 1133, loss 0.238419, acc 0.921875, learning_rate 0.000147798
2017-10-11T11:05:09.770859: step 1134, loss 0.419074, acc 0.859375, learning_rate 0.000147603
2017-10-11T11:05:09.840245: step 1135, loss 0.222233, acc 0.9375, learning_rate 0.000147409
2017-10-11T11:05:09.914401: step 1136, loss 0.21042, acc 0.90625, learning_rate 0.000147215
2017-10-11T11:05:09.982320: step 1137, loss 0.295018, acc 0.921875, learning_rate 0.000147022
2017-10-11T11:05:10.055799: step 1138, loss 0.149814, acc 0.96875, learning_rate 0.000146831
2017-10-11T11:05:10.127950: step 1139, loss 0.245214, acc 0.921875, learning_rate 0.000146639
2017-10-11T11:05:10.197597: step 1140, loss 0.195266, acc 0.90625, learning_rate 0.000146449
2017-10-11T11:05:10.267436: step 1141, loss 0.291288, acc 0.921875, learning_rate 0.000146259
2017-10-11T11:05:10.348989: step 1142, loss 0.198786, acc 0.9375, learning_rate 0.000146071
2017-10-11T11:05:10.419685: step 1143, loss 0.213709, acc 0.9375, learning_rate 0.000145883
2017-10-11T11:05:10.489934: step 1144, loss 0.189134, acc 0.9375, learning_rate 0.000145695
2017-10-11T11:05:10.564211: step 1145, loss 0.361468, acc 0.828125, learning_rate 0.000145509
2017-10-11T11:05:10.633830: step 1146, loss 0.35286, acc 0.875, learning_rate 0.000145323
2017-10-11T11:05:10.706517: step 1147, loss 0.268728, acc 0.890625, learning_rate 0.000145138
2017-10-11T11:05:10.776697: step 1148, loss 0.170537, acc 0.9375, learning_rate 0.000144954
2017-10-11T11:05:10.848713: step 1149, loss 0.330561, acc 0.859375, learning_rate 0.00014477
2017-10-11T11:05:10.919396: step 1150, loss 0.266788, acc 0.921875, learning_rate 0.000144588
2017-10-11T11:05:10.992312: step 1151, loss 0.208043, acc 0.9375, learning_rate 0.000144406
2017-10-11T11:05:11.062774: step 1152, loss 0.203575, acc 0.9375, learning_rate 0.000144224
2017-10-11T11:05:11.133287: step 1153, loss 0.151789, acc 0.96875, learning_rate 0.000144044
2017-10-11T11:05:11.205759: step 1154, loss 0.277316, acc 0.90625, learning_rate 0.000143864
2017-10-11T11:05:11.274601: step 1155, loss 0.276497, acc 0.890625, learning_rate 0.000143685
2017-10-11T11:05:11.344539: step 1156, loss 0.288706, acc 0.921875, learning_rate 0.000143507
2017-10-11T11:05:11.416174: step 1157, loss 0.263011, acc 0.90625, learning_rate 0.000143329
2017-10-11T11:05:11.490349: step 1158, loss 0.175701, acc 0.96875, learning_rate 0.000143152
2017-10-11T11:05:11.564607: step 1159, loss 0.149724, acc 0.953125, learning_rate 0.000142976
2017-10-11T11:05:11.636459: step 1160, loss 0.208589, acc 0.9375, learning_rate 0.000142801

Evaluation:
2017-10-11T11:05:11.801422: step 1160, loss 0.257216, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1160

2017-10-11T11:05:12.304217: step 1161, loss 0.304741, acc 0.90625, learning_rate 0.000142626
2017-10-11T11:05:12.376005: step 1162, loss 0.143922, acc 0.984375, learning_rate 0.000142452
2017-10-11T11:05:12.446546: step 1163, loss 0.250733, acc 0.921875, learning_rate 0.000142279
2017-10-11T11:05:12.516728: step 1164, loss 0.216389, acc 0.921875, learning_rate 0.000142106
2017-10-11T11:05:12.588417: step 1165, loss 0.221617, acc 0.9375, learning_rate 0.000141934
2017-10-11T11:05:12.657676: step 1166, loss 0.163042, acc 0.953125, learning_rate 0.000141763
2017-10-11T11:05:12.728937: step 1167, loss 0.248649, acc 0.921875, learning_rate 0.000141593
2017-10-11T11:05:12.798378: step 1168, loss 0.1953, acc 0.953125, learning_rate 0.000141423
2017-10-11T11:05:12.876662: step 1169, loss 0.236858, acc 0.9375, learning_rate 0.000141254
2017-10-11T11:05:12.947932: step 1170, loss 0.281398, acc 0.921875, learning_rate 0.000141085
2017-10-11T11:05:13.019399: step 1171, loss 0.143495, acc 0.984375, learning_rate 0.000140918
2017-10-11T11:05:13.091753: step 1172, loss 0.307051, acc 0.90625, learning_rate 0.000140751
2017-10-11T11:05:13.171869: step 1173, loss 0.131586, acc 0.96875, learning_rate 0.000140584
2017-10-11T11:05:13.241421: step 1174, loss 0.41201, acc 0.859375, learning_rate 0.000140419
2017-10-11T11:05:13.312292: step 1175, loss 0.198105, acc 0.953125, learning_rate 0.000140254
2017-10-11T11:05:13.376118: step 1176, loss 0.275715, acc 0.882353, learning_rate 0.000140089
2017-10-11T11:05:13.448805: step 1177, loss 0.135342, acc 0.96875, learning_rate 0.000139926
2017-10-11T11:05:13.524152: step 1178, loss 0.184156, acc 0.984375, learning_rate 0.000139763
2017-10-11T11:05:13.594844: step 1179, loss 0.206135, acc 0.921875, learning_rate 0.0001396
2017-10-11T11:05:13.666415: step 1180, loss 0.167128, acc 0.96875, learning_rate 0.000139439
2017-10-11T11:05:13.738328: step 1181, loss 0.259004, acc 0.90625, learning_rate 0.000139278
2017-10-11T11:05:13.807422: step 1182, loss 0.387085, acc 0.859375, learning_rate 0.000139118
2017-10-11T11:05:13.879111: step 1183, loss 0.201067, acc 0.9375, learning_rate 0.000138958
2017-10-11T11:05:13.950176: step 1184, loss 0.169077, acc 0.9375, learning_rate 0.000138799
2017-10-11T11:05:14.022795: step 1185, loss 0.262526, acc 0.921875, learning_rate 0.00013864
2017-10-11T11:05:14.093205: step 1186, loss 0.194112, acc 0.921875, learning_rate 0.000138483
2017-10-11T11:05:14.163028: step 1187, loss 0.354687, acc 0.859375, learning_rate 0.000138326
2017-10-11T11:05:14.235647: step 1188, loss 0.425389, acc 0.796875, learning_rate 0.000138169
2017-10-11T11:05:14.301862: step 1189, loss 0.19636, acc 0.921875, learning_rate 0.000138013
2017-10-11T11:05:14.373597: step 1190, loss 0.23386, acc 0.90625, learning_rate 0.000137858
2017-10-11T11:05:14.447500: step 1191, loss 0.204869, acc 0.96875, learning_rate 0.000137704
2017-10-11T11:05:14.517482: step 1192, loss 0.480222, acc 0.8125, learning_rate 0.00013755
2017-10-11T11:05:14.592300: step 1193, loss 0.171303, acc 0.984375, learning_rate 0.000137397
2017-10-11T11:05:14.659862: step 1194, loss 0.262376, acc 0.90625, learning_rate 0.000137244
2017-10-11T11:05:14.732140: step 1195, loss 0.210488, acc 0.921875, learning_rate 0.000137092
2017-10-11T11:05:14.804565: step 1196, loss 0.250854, acc 0.90625, learning_rate 0.000136941
2017-10-11T11:05:14.874156: step 1197, loss 0.202898, acc 0.953125, learning_rate 0.00013679
2017-10-11T11:05:14.944589: step 1198, loss 0.276929, acc 0.90625, learning_rate 0.00013664
2017-10-11T11:05:15.015945: step 1199, loss 0.230344, acc 0.9375, learning_rate 0.00013649
2017-10-11T11:05:15.086266: step 1200, loss 0.32092, acc 0.875, learning_rate 0.000136341

Evaluation:
2017-10-11T11:05:15.253749: step 1200, loss 0.255811, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1200

2017-10-11T11:05:15.814707: step 1201, loss 0.231102, acc 0.90625, learning_rate 0.000136193
2017-10-11T11:05:15.885965: step 1202, loss 0.217939, acc 0.90625, learning_rate 0.000136045
2017-10-11T11:05:15.961772: step 1203, loss 0.223867, acc 0.90625, learning_rate 0.000135898
2017-10-11T11:05:16.036536: step 1204, loss 0.285266, acc 0.890625, learning_rate 0.000135751
2017-10-11T11:05:16.106677: step 1205, loss 0.321323, acc 0.890625, learning_rate 0.000135605
2017-10-11T11:05:16.179268: step 1206, loss 0.186874, acc 0.921875, learning_rate 0.00013546
2017-10-11T11:05:16.248226: step 1207, loss 0.24684, acc 0.9375, learning_rate 0.000135315
2017-10-11T11:05:16.320403: step 1208, loss 0.253861, acc 0.921875, learning_rate 0.000135171
2017-10-11T11:05:16.391837: step 1209, loss 0.37759, acc 0.875, learning_rate 0.000135028
2017-10-11T11:05:16.465301: step 1210, loss 0.285663, acc 0.890625, learning_rate 0.000134885
2017-10-11T11:05:16.534721: step 1211, loss 0.284636, acc 0.875, learning_rate 0.000134742
2017-10-11T11:05:16.606165: step 1212, loss 0.245393, acc 0.9375, learning_rate 0.0001346
2017-10-11T11:05:16.678029: step 1213, loss 0.236428, acc 0.90625, learning_rate 0.000134459
2017-10-11T11:05:16.747876: step 1214, loss 0.202712, acc 0.953125, learning_rate 0.000134319
2017-10-11T11:05:16.821030: step 1215, loss 0.426915, acc 0.875, learning_rate 0.000134178
2017-10-11T11:05:16.894368: step 1216, loss 0.199864, acc 0.96875, learning_rate 0.000134039
2017-10-11T11:05:16.971089: step 1217, loss 0.363665, acc 0.875, learning_rate 0.0001339
2017-10-11T11:05:17.044723: step 1218, loss 0.265616, acc 0.921875, learning_rate 0.000133762
2017-10-11T11:05:17.115368: step 1219, loss 0.264602, acc 0.953125, learning_rate 0.000133624
2017-10-11T11:05:17.185016: step 1220, loss 0.173015, acc 0.953125, learning_rate 0.000133487
2017-10-11T11:05:17.259664: step 1221, loss 0.264441, acc 0.90625, learning_rate 0.00013335
2017-10-11T11:05:17.331240: step 1222, loss 0.155031, acc 0.96875, learning_rate 0.000133214
2017-10-11T11:05:17.403238: step 1223, loss 0.224318, acc 0.96875, learning_rate 0.000133078
2017-10-11T11:05:17.473962: step 1224, loss 0.235915, acc 0.921875, learning_rate 0.000132943
2017-10-11T11:05:17.545386: step 1225, loss 0.179867, acc 0.953125, learning_rate 0.000132809
2017-10-11T11:05:17.615413: step 1226, loss 0.204888, acc 0.953125, learning_rate 0.000132675
2017-10-11T11:05:17.685159: step 1227, loss 0.205143, acc 0.9375, learning_rate 0.000132541
2017-10-11T11:05:17.755788: step 1228, loss 0.108237, acc 0.953125, learning_rate 0.000132409
2017-10-11T11:05:17.825662: step 1229, loss 0.139142, acc 0.96875, learning_rate 0.000132276
2017-10-11T11:05:17.895006: step 1230, loss 0.34347, acc 0.890625, learning_rate 0.000132145
2017-10-11T11:05:17.964328: step 1231, loss 0.196288, acc 0.9375, learning_rate 0.000132013
2017-10-11T11:05:18.033854: step 1232, loss 0.178331, acc 0.9375, learning_rate 0.000131883
2017-10-11T11:05:18.105617: step 1233, loss 0.26662, acc 0.921875, learning_rate 0.000131753
2017-10-11T11:05:18.176809: step 1234, loss 0.159195, acc 0.953125, learning_rate 0.000131623
2017-10-11T11:05:18.244946: step 1235, loss 0.263124, acc 0.90625, learning_rate 0.000131494
2017-10-11T11:05:18.317180: step 1236, loss 0.225551, acc 0.921875, learning_rate 0.000131365
2017-10-11T11:05:18.392712: step 1237, loss 0.338224, acc 0.921875, learning_rate 0.000131237
2017-10-11T11:05:18.463860: step 1238, loss 0.1807, acc 0.96875, learning_rate 0.00013111
2017-10-11T11:05:18.538929: step 1239, loss 0.218649, acc 0.953125, learning_rate 0.000130983
2017-10-11T11:05:18.609210: step 1240, loss 0.174112, acc 0.9375, learning_rate 0.000130856

Evaluation:
2017-10-11T11:05:18.776256: step 1240, loss 0.254017, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1240

2017-10-11T11:05:19.739233: step 1241, loss 0.273169, acc 0.90625, learning_rate 0.00013073
2017-10-11T11:05:19.808094: step 1242, loss 0.317408, acc 0.875, learning_rate 0.000130605
2017-10-11T11:05:19.882969: step 1243, loss 0.269029, acc 0.921875, learning_rate 0.00013048
2017-10-11T11:05:19.956431: step 1244, loss 0.146061, acc 0.953125, learning_rate 0.000130356
2017-10-11T11:05:20.033869: step 1245, loss 0.31793, acc 0.890625, learning_rate 0.000130232
2017-10-11T11:05:20.103260: step 1246, loss 0.144826, acc 0.96875, learning_rate 0.000130108
2017-10-11T11:05:20.171298: step 1247, loss 0.17969, acc 0.9375, learning_rate 0.000129985
2017-10-11T11:05:20.242175: step 1248, loss 0.353539, acc 0.875, learning_rate 0.000129863
2017-10-11T11:05:20.310136: step 1249, loss 0.258385, acc 0.921875, learning_rate 0.000129741
2017-10-11T11:05:20.385370: step 1250, loss 0.129332, acc 0.953125, learning_rate 0.00012962
2017-10-11T11:05:20.458970: step 1251, loss 0.268259, acc 0.90625, learning_rate 0.000129499
2017-10-11T11:05:20.531400: step 1252, loss 0.272735, acc 0.90625, learning_rate 0.000129378
2017-10-11T11:05:20.603142: step 1253, loss 0.182423, acc 0.921875, learning_rate 0.000129259
2017-10-11T11:05:20.674316: step 1254, loss 0.236925, acc 0.90625, learning_rate 0.000129139
2017-10-11T11:05:20.744077: step 1255, loss 0.124101, acc 0.953125, learning_rate 0.00012902
2017-10-11T11:05:20.816183: step 1256, loss 0.255327, acc 0.90625, learning_rate 0.000128902
2017-10-11T11:05:20.891363: step 1257, loss 0.317552, acc 0.859375, learning_rate 0.000128784
2017-10-11T11:05:20.961626: step 1258, loss 0.16148, acc 0.953125, learning_rate 0.000128666
2017-10-11T11:05:21.032459: step 1259, loss 0.220503, acc 0.9375, learning_rate 0.000128549
2017-10-11T11:05:21.109679: step 1260, loss 0.340625, acc 0.890625, learning_rate 0.000128433
2017-10-11T11:05:21.179061: step 1261, loss 0.303697, acc 0.890625, learning_rate 0.000128317
2017-10-11T11:05:21.248579: step 1262, loss 0.216233, acc 0.9375, learning_rate 0.000128201
2017-10-11T11:05:21.318904: step 1263, loss 0.230165, acc 0.90625, learning_rate 0.000128086
2017-10-11T11:05:21.388656: step 1264, loss 0.261414, acc 0.9375, learning_rate 0.000127971
2017-10-11T11:05:21.461129: step 1265, loss 0.246594, acc 0.9375, learning_rate 0.000127857
2017-10-11T11:05:21.529325: step 1266, loss 0.3833, acc 0.90625, learning_rate 0.000127743
2017-10-11T11:05:21.604116: step 1267, loss 0.266132, acc 0.890625, learning_rate 0.00012763
2017-10-11T11:05:21.679868: step 1268, loss 0.28704, acc 0.9375, learning_rate 0.000127517
2017-10-11T11:05:21.749657: step 1269, loss 0.198383, acc 0.96875, learning_rate 0.000127405
2017-10-11T11:05:21.820003: step 1270, loss 0.281839, acc 0.890625, learning_rate 0.000127293
2017-10-11T11:05:21.893240: step 1271, loss 0.276962, acc 0.9375, learning_rate 0.000127182
2017-10-11T11:05:21.966264: step 1272, loss 0.151149, acc 0.953125, learning_rate 0.000127071
2017-10-11T11:05:22.034895: step 1273, loss 0.302875, acc 0.921875, learning_rate 0.00012696
2017-10-11T11:05:22.097586: step 1274, loss 0.28608, acc 0.882353, learning_rate 0.00012685
2017-10-11T11:05:22.168088: step 1275, loss 0.257555, acc 0.890625, learning_rate 0.000126741
2017-10-11T11:05:22.236612: step 1276, loss 0.179757, acc 0.9375, learning_rate 0.000126632
2017-10-11T11:05:22.308818: step 1277, loss 0.203702, acc 0.921875, learning_rate 0.000126523
2017-10-11T11:05:22.380843: step 1278, loss 0.281991, acc 0.9375, learning_rate 0.000126415
2017-10-11T11:05:22.453232: step 1279, loss 0.26706, acc 0.953125, learning_rate 0.000126307
2017-10-11T11:05:22.524240: step 1280, loss 0.131199, acc 0.984375, learning_rate 0.000126199

Evaluation:
2017-10-11T11:05:22.691997: step 1280, loss 0.255284, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1280

2017-10-11T11:05:23.191849: step 1281, loss 0.256534, acc 0.921875, learning_rate 0.000126093
2017-10-11T11:05:23.263435: step 1282, loss 0.262456, acc 0.9375, learning_rate 0.000125986
2017-10-11T11:05:23.334144: step 1283, loss 0.212081, acc 0.9375, learning_rate 0.00012588
2017-10-11T11:05:23.406392: step 1284, loss 0.170934, acc 0.953125, learning_rate 0.000125774
2017-10-11T11:05:23.474864: step 1285, loss 0.282017, acc 0.890625, learning_rate 0.000125669
2017-10-11T11:05:23.544790: step 1286, loss 0.18465, acc 0.953125, learning_rate 0.000125564
2017-10-11T11:05:23.619626: step 1287, loss 0.190123, acc 0.921875, learning_rate 0.00012546
2017-10-11T11:05:23.690295: step 1288, loss 0.357649, acc 0.890625, learning_rate 0.000125356
2017-10-11T11:05:23.763000: step 1289, loss 0.200737, acc 0.953125, learning_rate 0.000125253
2017-10-11T11:05:23.832802: step 1290, loss 0.273434, acc 0.9375, learning_rate 0.00012515
2017-10-11T11:05:23.906892: step 1291, loss 0.376375, acc 0.84375, learning_rate 0.000125047
2017-10-11T11:05:23.974808: step 1292, loss 0.448289, acc 0.84375, learning_rate 0.000124945
2017-10-11T11:05:24.043798: step 1293, loss 0.322339, acc 0.890625, learning_rate 0.000124843
2017-10-11T11:05:24.115111: step 1294, loss 0.140039, acc 0.953125, learning_rate 0.000124741
2017-10-11T11:05:24.186116: step 1295, loss 0.322564, acc 0.890625, learning_rate 0.00012464
2017-10-11T11:05:24.258945: step 1296, loss 0.355572, acc 0.890625, learning_rate 0.00012454
2017-10-11T11:05:24.332193: step 1297, loss 0.300808, acc 0.875, learning_rate 0.00012444
2017-10-11T11:05:24.399750: step 1298, loss 0.485497, acc 0.90625, learning_rate 0.00012434
2017-10-11T11:05:24.470638: step 1299, loss 0.214381, acc 0.921875, learning_rate 0.000124241
2017-10-11T11:05:24.541945: step 1300, loss 0.161383, acc 0.9375, learning_rate 0.000124142
2017-10-11T11:05:24.616833: step 1301, loss 0.122456, acc 0.984375, learning_rate 0.000124043
2017-10-11T11:05:24.690184: step 1302, loss 0.221097, acc 0.90625, learning_rate 0.000123945
2017-10-11T11:05:24.761956: step 1303, loss 0.242688, acc 0.921875, learning_rate 0.000123847
2017-10-11T11:05:24.833315: step 1304, loss 0.340394, acc 0.875, learning_rate 0.00012375
2017-10-11T11:05:24.910859: step 1305, loss 0.301085, acc 0.875, learning_rate 0.000123653
2017-10-11T11:05:24.982097: step 1306, loss 0.202659, acc 0.90625, learning_rate 0.000123556
2017-10-11T11:05:25.053910: step 1307, loss 0.233454, acc 0.921875, learning_rate 0.00012346
2017-10-11T11:05:25.123805: step 1308, loss 0.262043, acc 0.890625, learning_rate 0.000123364
2017-10-11T11:05:25.194234: step 1309, loss 0.230965, acc 0.90625, learning_rate 0.000123269
2017-10-11T11:05:25.264211: step 1310, loss 0.236451, acc 0.90625, learning_rate 0.000123174
2017-10-11T11:05:25.334120: step 1311, loss 0.176418, acc 0.9375, learning_rate 0.00012308
2017-10-11T11:05:25.410903: step 1312, loss 0.271929, acc 0.90625, learning_rate 0.000122985
2017-10-11T11:05:25.481022: step 1313, loss 0.231151, acc 0.9375, learning_rate 0.000122892
2017-10-11T11:05:25.553182: step 1314, loss 0.171629, acc 0.984375, learning_rate 0.000122798
2017-10-11T11:05:25.626521: step 1315, loss 0.334853, acc 0.875, learning_rate 0.000122705
2017-10-11T11:05:25.697377: step 1316, loss 0.242828, acc 0.90625, learning_rate 0.000122612
2017-10-11T11:05:25.767737: step 1317, loss 0.159547, acc 0.96875, learning_rate 0.00012252
2017-10-11T11:05:25.840301: step 1318, loss 0.337581, acc 0.890625, learning_rate 0.000122428
2017-10-11T11:05:25.913539: step 1319, loss 0.291747, acc 0.953125, learning_rate 0.000122337
2017-10-11T11:05:25.985425: step 1320, loss 0.235735, acc 0.90625, learning_rate 0.000122245

Evaluation:
2017-10-11T11:05:26.152518: step 1320, loss 0.254144, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1320

2017-10-11T11:05:26.724850: step 1321, loss 0.247142, acc 0.9375, learning_rate 0.000122155
2017-10-11T11:05:26.798817: step 1322, loss 0.278791, acc 0.90625, learning_rate 0.000122064
2017-10-11T11:05:26.870172: step 1323, loss 0.302838, acc 0.890625, learning_rate 0.000121974
2017-10-11T11:05:26.939323: step 1324, loss 0.44499, acc 0.859375, learning_rate 0.000121884
2017-10-11T11:05:27.011885: step 1325, loss 0.30378, acc 0.90625, learning_rate 0.000121795
2017-10-11T11:05:27.080270: step 1326, loss 0.175018, acc 0.96875, learning_rate 0.000121706
2017-10-11T11:05:27.152980: step 1327, loss 0.235251, acc 0.890625, learning_rate 0.000121618
2017-10-11T11:05:27.231135: step 1328, loss 0.304264, acc 0.90625, learning_rate 0.000121529
2017-10-11T11:05:27.302044: step 1329, loss 0.251134, acc 0.921875, learning_rate 0.000121441
2017-10-11T11:05:27.370613: step 1330, loss 0.194535, acc 0.96875, learning_rate 0.000121354
2017-10-11T11:05:27.440637: step 1331, loss 0.30978, acc 0.890625, learning_rate 0.000121267
2017-10-11T11:05:27.510050: step 1332, loss 0.276584, acc 0.90625, learning_rate 0.00012118
2017-10-11T11:05:27.579658: step 1333, loss 0.24984, acc 0.9375, learning_rate 0.000121093
2017-10-11T11:05:27.651596: step 1334, loss 0.275961, acc 0.90625, learning_rate 0.000121007
2017-10-11T11:05:27.727519: step 1335, loss 0.311321, acc 0.953125, learning_rate 0.000120922
2017-10-11T11:05:27.802563: step 1336, loss 0.167994, acc 0.9375, learning_rate 0.000120836
2017-10-11T11:05:27.873065: step 1337, loss 0.286374, acc 0.90625, learning_rate 0.000120751
2017-10-11T11:05:27.948625: step 1338, loss 0.277958, acc 0.90625, learning_rate 0.000120666
2017-10-11T11:05:28.019368: step 1339, loss 0.180606, acc 0.9375, learning_rate 0.000120582
2017-10-11T11:05:28.090391: step 1340, loss 0.323043, acc 0.875, learning_rate 0.000120498
2017-10-11T11:05:28.160748: step 1341, loss 0.336089, acc 0.90625, learning_rate 0.000120414
2017-10-11T11:05:28.234348: step 1342, loss 0.201719, acc 0.875, learning_rate 0.000120331
2017-10-11T11:05:28.303388: step 1343, loss 0.315404, acc 0.890625, learning_rate 0.000120248
2017-10-11T11:05:28.373432: step 1344, loss 0.163222, acc 0.953125, learning_rate 0.000120165
2017-10-11T11:05:28.443978: step 1345, loss 0.233521, acc 0.90625, learning_rate 0.000120083
2017-10-11T11:05:28.514315: step 1346, loss 0.271562, acc 0.9375, learning_rate 0.000120001
2017-10-11T11:05:28.585800: step 1347, loss 0.239154, acc 0.921875, learning_rate 0.00011992
2017-10-11T11:05:28.659094: step 1348, loss 0.263377, acc 0.9375, learning_rate 0.000119838
2017-10-11T11:05:28.732912: step 1349, loss 0.335947, acc 0.875, learning_rate 0.000119757
2017-10-11T11:05:28.806062: step 1350, loss 0.309567, acc 0.875, learning_rate 0.000119677
2017-10-11T11:05:28.881479: step 1351, loss 0.263538, acc 0.875, learning_rate 0.000119596
2017-10-11T11:05:28.956393: step 1352, loss 0.301067, acc 0.90625, learning_rate 0.000119516
2017-10-11T11:05:29.026481: step 1353, loss 0.125728, acc 0.984375, learning_rate 0.000119437
2017-10-11T11:05:29.096388: step 1354, loss 0.295538, acc 0.875, learning_rate 0.000119357
2017-10-11T11:05:29.165870: step 1355, loss 0.206454, acc 0.96875, learning_rate 0.000119278
2017-10-11T11:05:29.238719: step 1356, loss 0.213054, acc 0.953125, learning_rate 0.0001192
2017-10-11T11:05:29.312475: step 1357, loss 0.234602, acc 0.921875, learning_rate 0.000119121
2017-10-11T11:05:29.380974: step 1358, loss 0.236944, acc 0.921875, learning_rate 0.000119043
2017-10-11T11:05:29.452607: step 1359, loss 0.37306, acc 0.875, learning_rate 0.000118965
2017-10-11T11:05:29.526798: step 1360, loss 0.167352, acc 0.984375, learning_rate 0.000118888

Evaluation:
2017-10-11T11:05:29.701903: step 1360, loss 0.254124, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1360

2017-10-11T11:05:30.272686: step 1361, loss 0.253306, acc 0.890625, learning_rate 0.000118811
2017-10-11T11:05:30.341695: step 1362, loss 0.205104, acc 0.9375, learning_rate 0.000118734
2017-10-11T11:05:30.413611: step 1363, loss 0.180042, acc 0.9375, learning_rate 0.000118658
2017-10-11T11:05:30.486750: step 1364, loss 0.288195, acc 0.921875, learning_rate 0.000118582
2017-10-11T11:05:30.559806: step 1365, loss 0.235203, acc 0.9375, learning_rate 0.000118506
2017-10-11T11:05:30.634649: step 1366, loss 0.278261, acc 0.90625, learning_rate 0.00011843
2017-10-11T11:05:30.702380: step 1367, loss 0.309773, acc 0.890625, learning_rate 0.000118355
2017-10-11T11:05:30.773134: step 1368, loss 0.258525, acc 0.921875, learning_rate 0.00011828
2017-10-11T11:05:30.847106: step 1369, loss 0.429383, acc 0.875, learning_rate 0.000118205
2017-10-11T11:05:30.918137: step 1370, loss 0.26279, acc 0.9375, learning_rate 0.000118131
2017-10-11T11:05:30.988947: step 1371, loss 0.151291, acc 0.953125, learning_rate 0.000118057
2017-10-11T11:05:31.053831: step 1372, loss 0.152299, acc 0.980392, learning_rate 0.000117983
2017-10-11T11:05:31.126302: step 1373, loss 0.298538, acc 0.875, learning_rate 0.00011791
2017-10-11T11:05:31.199166: step 1374, loss 0.240581, acc 0.921875, learning_rate 0.000117837
2017-10-11T11:05:31.269211: step 1375, loss 0.348665, acc 0.9375, learning_rate 0.000117764
2017-10-11T11:05:31.342041: step 1376, loss 0.230316, acc 0.90625, learning_rate 0.000117692
2017-10-11T11:05:31.415544: step 1377, loss 0.228035, acc 0.9375, learning_rate 0.000117619
2017-10-11T11:05:31.486017: step 1378, loss 0.192982, acc 0.9375, learning_rate 0.000117547
2017-10-11T11:05:31.558587: step 1379, loss 0.26025, acc 0.921875, learning_rate 0.000117476
2017-10-11T11:05:31.628717: step 1380, loss 0.201675, acc 0.953125, learning_rate 0.000117404
2017-10-11T11:05:31.698583: step 1381, loss 0.198219, acc 0.9375, learning_rate 0.000117333
2017-10-11T11:05:31.768396: step 1382, loss 0.248209, acc 0.90625, learning_rate 0.000117263
2017-10-11T11:05:31.838239: step 1383, loss 0.201491, acc 0.9375, learning_rate 0.000117192
2017-10-11T11:05:31.910874: step 1384, loss 0.243147, acc 0.9375, learning_rate 0.000117122
2017-10-11T11:05:31.982974: step 1385, loss 0.135327, acc 0.96875, learning_rate 0.000117052
2017-10-11T11:05:32.051844: step 1386, loss 0.289086, acc 0.890625, learning_rate 0.000116983
2017-10-11T11:05:32.122611: step 1387, loss 0.296134, acc 0.890625, learning_rate 0.000116913
2017-10-11T11:05:32.193017: step 1388, loss 0.147845, acc 0.96875, learning_rate 0.000116844
2017-10-11T11:05:32.263922: step 1389, loss 0.309332, acc 0.875, learning_rate 0.000116775
2017-10-11T11:05:32.338237: step 1390, loss 0.242039, acc 0.90625, learning_rate 0.000116707
2017-10-11T11:05:32.413593: step 1391, loss 0.250125, acc 0.90625, learning_rate 0.000116639
2017-10-11T11:05:32.485583: step 1392, loss 0.273309, acc 0.90625, learning_rate 0.000116571
2017-10-11T11:05:32.558465: step 1393, loss 0.27648, acc 0.9375, learning_rate 0.000116503
2017-10-11T11:05:32.630115: step 1394, loss 0.320266, acc 0.921875, learning_rate 0.000116436
2017-10-11T11:05:32.702659: step 1395, loss 0.38005, acc 0.890625, learning_rate 0.000116369
2017-10-11T11:05:32.773807: step 1396, loss 0.310658, acc 0.921875, learning_rate 0.000116302
2017-10-11T11:05:32.851205: step 1397, loss 0.204637, acc 0.90625, learning_rate 0.000116235
2017-10-11T11:05:32.922537: step 1398, loss 0.280178, acc 0.921875, learning_rate 0.000116169
2017-10-11T11:05:32.991975: step 1399, loss 0.256845, acc 0.890625, learning_rate 0.000116103
2017-10-11T11:05:33.067015: step 1400, loss 0.237104, acc 0.90625, learning_rate 0.000116037

Evaluation:
2017-10-11T11:05:33.237907: step 1400, loss 0.255026, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1400

2017-10-11T11:05:34.320289: step 1401, loss 0.35092, acc 0.828125, learning_rate 0.000115972
2017-10-11T11:05:34.391341: step 1402, loss 0.194451, acc 0.921875, learning_rate 0.000115907
2017-10-11T11:05:34.462950: step 1403, loss 0.281749, acc 0.890625, learning_rate 0.000115842
2017-10-11T11:05:34.532378: step 1404, loss 0.14145, acc 1, learning_rate 0.000115777
2017-10-11T11:05:34.602266: step 1405, loss 0.179637, acc 0.953125, learning_rate 0.000115713
2017-10-11T11:05:34.674109: step 1406, loss 0.175948, acc 0.921875, learning_rate 0.000115649
2017-10-11T11:05:34.747707: step 1407, loss 0.203409, acc 0.953125, learning_rate 0.000115585
2017-10-11T11:05:34.818326: step 1408, loss 0.26332, acc 0.890625, learning_rate 0.000115521
2017-10-11T11:05:34.893637: step 1409, loss 0.179881, acc 0.9375, learning_rate 0.000115458
2017-10-11T11:05:34.966828: step 1410, loss 0.219452, acc 0.921875, learning_rate 0.000115395
2017-10-11T11:05:35.041255: step 1411, loss 0.324773, acc 0.90625, learning_rate 0.000115332
2017-10-11T11:05:35.112805: step 1412, loss 0.212677, acc 0.921875, learning_rate 0.000115269
2017-10-11T11:05:35.184093: step 1413, loss 0.230357, acc 0.90625, learning_rate 0.000115207
2017-10-11T11:05:35.256481: step 1414, loss 0.213056, acc 0.9375, learning_rate 0.000115145
2017-10-11T11:05:35.324586: step 1415, loss 0.14614, acc 0.921875, learning_rate 0.000115083
2017-10-11T11:05:35.397020: step 1416, loss 0.248149, acc 0.90625, learning_rate 0.000115022
2017-10-11T11:05:35.466408: step 1417, loss 0.27998, acc 0.890625, learning_rate 0.00011496
2017-10-11T11:05:35.538532: step 1418, loss 0.188221, acc 0.953125, learning_rate 0.000114899
2017-10-11T11:05:35.613970: step 1419, loss 0.350401, acc 0.859375, learning_rate 0.000114838
2017-10-11T11:05:35.685203: step 1420, loss 0.241314, acc 0.9375, learning_rate 0.000114778
2017-10-11T11:05:35.755584: step 1421, loss 0.317101, acc 0.90625, learning_rate 0.000114717
2017-10-11T11:05:35.828649: step 1422, loss 0.220568, acc 0.921875, learning_rate 0.000114657
2017-10-11T11:05:35.898288: step 1423, loss 0.0734297, acc 0.984375, learning_rate 0.000114598
2017-10-11T11:05:35.967071: step 1424, loss 0.21341, acc 0.953125, learning_rate 0.000114538
2017-10-11T11:05:36.036556: step 1425, loss 0.248409, acc 0.9375, learning_rate 0.000114479
2017-10-11T11:05:36.117157: step 1426, loss 0.207995, acc 0.90625, learning_rate 0.00011442
2017-10-11T11:05:36.191305: step 1427, loss 0.263317, acc 0.890625, learning_rate 0.000114361
2017-10-11T11:05:36.258821: step 1428, loss 0.181369, acc 0.953125, learning_rate 0.000114302
2017-10-11T11:05:36.328744: step 1429, loss 0.309057, acc 0.921875, learning_rate 0.000114244
2017-10-11T11:05:36.400598: step 1430, loss 0.28922, acc 0.953125, learning_rate 0.000114186
2017-10-11T11:05:36.474212: step 1431, loss 0.309499, acc 0.84375, learning_rate 0.000114128
2017-10-11T11:05:36.542705: step 1432, loss 0.115398, acc 1, learning_rate 0.00011407
2017-10-11T11:05:36.613990: step 1433, loss 0.14537, acc 0.96875, learning_rate 0.000114013
2017-10-11T11:05:36.681979: step 1434, loss 0.310057, acc 0.890625, learning_rate 0.000113955
2017-10-11T11:05:36.750243: step 1435, loss 0.380883, acc 0.875, learning_rate 0.000113898
2017-10-11T11:05:36.820402: step 1436, loss 0.164114, acc 0.96875, learning_rate 0.000113842
2017-10-11T11:05:36.895317: step 1437, loss 0.354838, acc 0.859375, learning_rate 0.000113785
2017-10-11T11:05:36.967731: step 1438, loss 0.32533, acc 0.90625, learning_rate 0.000113729
2017-10-11T11:05:37.042813: step 1439, loss 0.254407, acc 0.921875, learning_rate 0.000113673
2017-10-11T11:05:37.112225: step 1440, loss 0.335169, acc 0.875, learning_rate 0.000113617

Evaluation:
2017-10-11T11:05:37.285608: step 1440, loss 0.255204, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1440

2017-10-11T11:05:37.779503: step 1441, loss 0.234891, acc 0.9375, learning_rate 0.000113561
2017-10-11T11:05:37.851031: step 1442, loss 0.227543, acc 0.90625, learning_rate 0.000113506
2017-10-11T11:05:37.924378: step 1443, loss 0.256971, acc 0.890625, learning_rate 0.000113451
2017-10-11T11:05:37.994082: step 1444, loss 0.36136, acc 0.875, learning_rate 0.000113396
2017-10-11T11:05:38.065818: step 1445, loss 0.388939, acc 0.890625, learning_rate 0.000113341
2017-10-11T11:05:38.136306: step 1446, loss 0.329744, acc 0.875, learning_rate 0.000113287
2017-10-11T11:05:38.206202: step 1447, loss 0.334661, acc 0.875, learning_rate 0.000113233
2017-10-11T11:05:38.275306: step 1448, loss 0.332506, acc 0.90625, learning_rate 0.000113179
2017-10-11T11:05:38.346402: step 1449, loss 0.233989, acc 0.921875, learning_rate 0.000113125
2017-10-11T11:05:38.419178: step 1450, loss 0.231419, acc 0.9375, learning_rate 0.000113071
2017-10-11T11:05:38.490206: step 1451, loss 0.22145, acc 0.921875, learning_rate 0.000113018
2017-10-11T11:05:38.560765: step 1452, loss 0.273996, acc 0.890625, learning_rate 0.000112965
2017-10-11T11:05:38.629920: step 1453, loss 0.258862, acc 0.890625, learning_rate 0.000112912
2017-10-11T11:05:38.699691: step 1454, loss 0.275382, acc 0.921875, learning_rate 0.000112859
2017-10-11T11:05:38.780896: step 1455, loss 0.267193, acc 0.90625, learning_rate 0.000112807
2017-10-11T11:05:38.854496: step 1456, loss 0.209324, acc 0.953125, learning_rate 0.000112754
2017-10-11T11:05:38.924091: step 1457, loss 0.285777, acc 0.890625, learning_rate 0.000112702
2017-10-11T11:05:38.993640: step 1458, loss 0.265221, acc 0.90625, learning_rate 0.000112651
2017-10-11T11:05:39.062442: step 1459, loss 0.331715, acc 0.859375, learning_rate 0.000112599
2017-10-11T11:05:39.131700: step 1460, loss 0.244628, acc 0.890625, learning_rate 0.000112547
2017-10-11T11:05:39.209623: step 1461, loss 0.333632, acc 0.84375, learning_rate 0.000112496
2017-10-11T11:05:39.282444: step 1462, loss 0.225192, acc 0.9375, learning_rate 0.000112445
2017-10-11T11:05:39.357153: step 1463, loss 0.267235, acc 0.90625, learning_rate 0.000112394
2017-10-11T11:05:39.431346: step 1464, loss 0.405676, acc 0.828125, learning_rate 0.000112344
2017-10-11T11:05:39.500220: step 1465, loss 0.277215, acc 0.90625, learning_rate 0.000112293
2017-10-11T11:05:39.571486: step 1466, loss 0.226685, acc 0.9375, learning_rate 0.000112243
2017-10-11T11:05:39.643744: step 1467, loss 0.376059, acc 0.859375, learning_rate 0.000112193
2017-10-11T11:05:39.715930: step 1468, loss 0.132857, acc 0.953125, learning_rate 0.000112144
2017-10-11T11:05:39.789935: step 1469, loss 0.237863, acc 0.9375, learning_rate 0.000112094
2017-10-11T11:05:39.854496: step 1470, loss 0.190673, acc 0.941176, learning_rate 0.000112045
2017-10-11T11:05:39.925881: step 1471, loss 0.319966, acc 0.84375, learning_rate 0.000111995
2017-10-11T11:05:39.995329: step 1472, loss 0.192209, acc 0.9375, learning_rate 0.000111946
2017-10-11T11:05:40.066001: step 1473, loss 0.196387, acc 0.9375, learning_rate 0.000111898
2017-10-11T11:05:40.134551: step 1474, loss 0.28707, acc 0.921875, learning_rate 0.000111849
2017-10-11T11:05:40.205388: step 1475, loss 0.212773, acc 0.90625, learning_rate 0.000111801
2017-10-11T11:05:40.278945: step 1476, loss 0.193079, acc 0.9375, learning_rate 0.000111753
2017-10-11T11:05:40.348595: step 1477, loss 0.243752, acc 0.875, learning_rate 0.000111705
2017-10-11T11:05:40.419351: step 1478, loss 0.351106, acc 0.875, learning_rate 0.000111657
2017-10-11T11:05:40.493663: step 1479, loss 0.205742, acc 0.96875, learning_rate 0.000111609
2017-10-11T11:05:40.565190: step 1480, loss 0.238575, acc 0.921875, learning_rate 0.000111562

Evaluation:
2017-10-11T11:05:40.741563: step 1480, loss 0.252783, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1480

2017-10-11T11:05:41.306239: step 1481, loss 0.288015, acc 0.9375, learning_rate 0.000111515
2017-10-11T11:05:41.376158: step 1482, loss 0.242672, acc 0.90625, learning_rate 0.000111468
2017-10-11T11:05:41.447265: step 1483, loss 0.230566, acc 0.9375, learning_rate 0.000111421
2017-10-11T11:05:41.522387: step 1484, loss 0.137588, acc 0.953125, learning_rate 0.000111374
2017-10-11T11:05:41.592711: step 1485, loss 0.35018, acc 0.921875, learning_rate 0.000111328
2017-10-11T11:05:41.662572: step 1486, loss 0.235599, acc 0.890625, learning_rate 0.000111282
2017-10-11T11:05:41.733693: step 1487, loss 0.20067, acc 0.90625, learning_rate 0.000111236
2017-10-11T11:05:41.805518: step 1488, loss 0.188085, acc 0.9375, learning_rate 0.00011119
2017-10-11T11:05:41.882019: step 1489, loss 0.269252, acc 0.890625, learning_rate 0.000111144
2017-10-11T11:05:41.959266: step 1490, loss 0.311009, acc 0.921875, learning_rate 0.000111099
2017-10-11T11:05:42.030088: step 1491, loss 0.335732, acc 0.90625, learning_rate 0.000111053
2017-10-11T11:05:42.101648: step 1492, loss 0.260363, acc 0.90625, learning_rate 0.000111008
2017-10-11T11:05:42.171527: step 1493, loss 0.240303, acc 0.90625, learning_rate 0.000110963
2017-10-11T11:05:42.242444: step 1494, loss 0.398061, acc 0.890625, learning_rate 0.000110918
2017-10-11T11:05:42.311357: step 1495, loss 0.377056, acc 0.921875, learning_rate 0.000110874
2017-10-11T11:05:42.380917: step 1496, loss 0.258496, acc 0.921875, learning_rate 0.00011083
2017-10-11T11:05:42.449928: step 1497, loss 0.439348, acc 0.875, learning_rate 0.000110785
2017-10-11T11:05:42.522705: step 1498, loss 0.165593, acc 0.96875, learning_rate 0.000110741
2017-10-11T11:05:42.594317: step 1499, loss 0.165021, acc 0.921875, learning_rate 0.000110697
2017-10-11T11:05:42.667974: step 1500, loss 0.171434, acc 0.953125, learning_rate 0.000110654
2017-10-11T11:05:42.738404: step 1501, loss 0.41565, acc 0.84375, learning_rate 0.00011061
2017-10-11T11:05:42.814416: step 1502, loss 0.212382, acc 0.953125, learning_rate 0.000110567
2017-10-11T11:05:42.891031: step 1503, loss 0.20368, acc 0.921875, learning_rate 0.000110524
2017-10-11T11:05:42.961943: step 1504, loss 0.170081, acc 0.9375, learning_rate 0.000110481
2017-10-11T11:05:43.033190: step 1505, loss 0.275814, acc 0.921875, learning_rate 0.000110438
2017-10-11T11:05:43.102431: step 1506, loss 0.281316, acc 0.875, learning_rate 0.000110396
2017-10-11T11:05:43.175763: step 1507, loss 0.134228, acc 0.984375, learning_rate 0.000110353
2017-10-11T11:05:43.246661: step 1508, loss 0.316107, acc 0.90625, learning_rate 0.000110311
2017-10-11T11:05:43.318973: step 1509, loss 0.185457, acc 0.9375, learning_rate 0.000110269
2017-10-11T11:05:43.387938: step 1510, loss 0.142804, acc 0.96875, learning_rate 0.000110227
2017-10-11T11:05:43.455738: step 1511, loss 0.294546, acc 0.921875, learning_rate 0.000110185
2017-10-11T11:05:43.529140: step 1512, loss 0.180562, acc 0.953125, learning_rate 0.000110144
2017-10-11T11:05:43.603106: step 1513, loss 0.183064, acc 0.953125, learning_rate 0.000110102
2017-10-11T11:05:43.672461: step 1514, loss 0.247313, acc 0.921875, learning_rate 0.000110061
2017-10-11T11:05:43.744569: step 1515, loss 0.302668, acc 0.9375, learning_rate 0.00011002
2017-10-11T11:05:43.822800: step 1516, loss 0.268789, acc 0.890625, learning_rate 0.000109979
2017-10-11T11:05:43.896659: step 1517, loss 0.351184, acc 0.890625, learning_rate 0.000109938
2017-10-11T11:05:43.984074: step 1518, loss 0.261541, acc 0.90625, learning_rate 0.000109898
2017-10-11T11:05:44.062306: step 1519, loss 0.185272, acc 0.921875, learning_rate 0.000109857
2017-10-11T11:05:44.133981: step 1520, loss 0.371494, acc 0.890625, learning_rate 0.000109817

Evaluation:
2017-10-11T11:05:44.305912: step 1520, loss 0.252978, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1520

2017-10-11T11:05:44.941902: step 1521, loss 0.297005, acc 0.875, learning_rate 0.000109777
2017-10-11T11:05:45.013470: step 1522, loss 0.190889, acc 0.921875, learning_rate 0.000109737
2017-10-11T11:05:45.087203: step 1523, loss 0.158594, acc 0.96875, learning_rate 0.000109697
2017-10-11T11:05:45.159184: step 1524, loss 0.339313, acc 0.90625, learning_rate 0.000109658
2017-10-11T11:05:45.229667: step 1525, loss 0.27949, acc 0.921875, learning_rate 0.000109618
2017-10-11T11:05:45.298539: step 1526, loss 0.209998, acc 0.9375, learning_rate 0.000109579
2017-10-11T11:05:45.370615: step 1527, loss 0.204351, acc 0.921875, learning_rate 0.00010954
2017-10-11T11:05:45.440733: step 1528, loss 0.236846, acc 0.921875, learning_rate 0.000109501
2017-10-11T11:05:45.511837: step 1529, loss 0.248608, acc 0.953125, learning_rate 0.000109462
2017-10-11T11:05:45.580362: step 1530, loss 0.228425, acc 0.921875, learning_rate 0.000109424
2017-10-11T11:05:45.652609: step 1531, loss 0.158354, acc 0.953125, learning_rate 0.000109385
2017-10-11T11:05:45.724461: step 1532, loss 0.32553, acc 0.921875, learning_rate 0.000109347
2017-10-11T11:05:45.797177: step 1533, loss 0.248364, acc 0.953125, learning_rate 0.000109309
2017-10-11T11:05:45.870873: step 1534, loss 0.291884, acc 0.9375, learning_rate 0.000109271
2017-10-11T11:05:45.939753: step 1535, loss 0.256833, acc 0.890625, learning_rate 0.000109233
2017-10-11T11:05:46.011216: step 1536, loss 0.310052, acc 0.890625, learning_rate 0.000109195
2017-10-11T11:05:46.090619: step 1537, loss 0.243305, acc 0.90625, learning_rate 0.000109158
2017-10-11T11:05:46.163711: step 1538, loss 0.379213, acc 0.84375, learning_rate 0.00010912
2017-10-11T11:05:46.235908: step 1539, loss 0.23604, acc 0.96875, learning_rate 0.000109083
2017-10-11T11:05:46.307253: step 1540, loss 0.122924, acc 1, learning_rate 0.000109046
2017-10-11T11:05:46.377378: step 1541, loss 0.478788, acc 0.859375, learning_rate 0.000109009
2017-10-11T11:05:46.448542: step 1542, loss 0.326968, acc 0.859375, learning_rate 0.000108972
2017-10-11T11:05:46.518308: step 1543, loss 0.29305, acc 0.90625, learning_rate 0.000108936
2017-10-11T11:05:46.589526: step 1544, loss 0.212447, acc 0.921875, learning_rate 0.000108899
2017-10-11T11:05:46.658757: step 1545, loss 0.151762, acc 0.984375, learning_rate 0.000108863
2017-10-11T11:05:46.734157: step 1546, loss 0.240023, acc 0.921875, learning_rate 0.000108827
2017-10-11T11:05:46.806146: step 1547, loss 0.325591, acc 0.921875, learning_rate 0.000108791
2017-10-11T11:05:46.879049: step 1548, loss 0.181954, acc 0.953125, learning_rate 0.000108755
2017-10-11T11:05:46.949497: step 1549, loss 0.181591, acc 0.9375, learning_rate 0.000108719
2017-10-11T11:05:47.018273: step 1550, loss 0.195224, acc 0.9375, learning_rate 0.000108683
2017-10-11T11:05:47.087715: step 1551, loss 0.231979, acc 0.9375, learning_rate 0.000108648
2017-10-11T11:05:47.158296: step 1552, loss 0.251267, acc 0.90625, learning_rate 0.000108613
2017-10-11T11:05:47.226992: step 1553, loss 0.225217, acc 0.953125, learning_rate 0.000108577
2017-10-11T11:05:47.302321: step 1554, loss 0.370387, acc 0.875, learning_rate 0.000108542
2017-10-11T11:05:47.375235: step 1555, loss 0.248585, acc 0.90625, learning_rate 0.000108508
2017-10-11T11:05:47.448550: step 1556, loss 0.196265, acc 0.921875, learning_rate 0.000108473
2017-10-11T11:05:47.522350: step 1557, loss 0.219566, acc 0.921875, learning_rate 0.000108438
2017-10-11T11:05:47.594633: step 1558, loss 0.22338, acc 0.921875, learning_rate 0.000108404
2017-10-11T11:05:47.676261: step 1559, loss 0.16629, acc 0.9375, learning_rate 0.00010837
2017-10-11T11:05:47.744282: step 1560, loss 0.188996, acc 0.953125, learning_rate 0.000108335

Evaluation:
2017-10-11T11:05:47.922283: step 1560, loss 0.252945, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1560

2017-10-11T11:05:48.556233: step 1561, loss 0.110217, acc 0.984375, learning_rate 0.000108301
2017-10-11T11:05:48.627897: step 1562, loss 0.207828, acc 0.890625, learning_rate 0.000108267
2017-10-11T11:05:48.698029: step 1563, loss 0.297015, acc 0.875, learning_rate 0.000108234
2017-10-11T11:05:48.765693: step 1564, loss 0.379809, acc 0.890625, learning_rate 0.0001082
2017-10-11T11:05:48.838184: step 1565, loss 0.170574, acc 0.9375, learning_rate 0.000108167
2017-10-11T11:05:48.913403: step 1566, loss 0.242987, acc 0.90625, learning_rate 0.000108133
2017-10-11T11:05:48.984078: step 1567, loss 0.30107, acc 0.890625, learning_rate 0.0001081
2017-10-11T11:05:49.047206: step 1568, loss 0.108106, acc 0.980392, learning_rate 0.000108067
2017-10-11T11:05:49.122621: step 1569, loss 0.188259, acc 0.953125, learning_rate 0.000108034
2017-10-11T11:05:49.192336: step 1570, loss 0.25349, acc 0.9375, learning_rate 0.000108001
2017-10-11T11:05:49.266937: step 1571, loss 0.372074, acc 0.859375, learning_rate 0.000107969
2017-10-11T11:05:49.337401: step 1572, loss 0.253749, acc 0.921875, learning_rate 0.000107936
2017-10-11T11:05:49.407627: step 1573, loss 0.209031, acc 0.9375, learning_rate 0.000107904
2017-10-11T11:05:49.477786: step 1574, loss 0.143182, acc 0.984375, learning_rate 0.000107871
2017-10-11T11:05:49.546550: step 1575, loss 0.223055, acc 0.921875, learning_rate 0.000107839
2017-10-11T11:05:49.615422: step 1576, loss 0.233198, acc 0.890625, learning_rate 0.000107807
2017-10-11T11:05:49.688303: step 1577, loss 0.223287, acc 0.921875, learning_rate 0.000107775
2017-10-11T11:05:49.758476: step 1578, loss 0.252246, acc 0.921875, learning_rate 0.000107744
2017-10-11T11:05:49.830277: step 1579, loss 0.267876, acc 0.921875, learning_rate 0.000107712
2017-10-11T11:05:49.903644: step 1580, loss 0.29443, acc 0.890625, learning_rate 0.000107681
2017-10-11T11:05:49.975748: step 1581, loss 0.193153, acc 0.9375, learning_rate 0.000107649
2017-10-11T11:05:50.049420: step 1582, loss 0.185581, acc 0.953125, learning_rate 0.000107618
2017-10-11T11:05:50.118970: step 1583, loss 0.270269, acc 0.921875, learning_rate 0.000107587
2017-10-11T11:05:50.189782: step 1584, loss 0.166188, acc 0.953125, learning_rate 0.000107556
2017-10-11T11:05:50.264179: step 1585, loss 0.414492, acc 0.828125, learning_rate 0.000107525
2017-10-11T11:05:50.335060: step 1586, loss 0.159982, acc 0.953125, learning_rate 0.000107494
2017-10-11T11:05:50.404693: step 1587, loss 0.262148, acc 0.90625, learning_rate 0.000107464
2017-10-11T11:05:50.478020: step 1588, loss 0.201012, acc 0.890625, learning_rate 0.000107433
2017-10-11T11:05:50.547541: step 1589, loss 0.32964, acc 0.90625, learning_rate 0.000107403
2017-10-11T11:05:50.625499: step 1590, loss 0.141191, acc 0.953125, learning_rate 0.000107373
2017-10-11T11:05:50.700417: step 1591, loss 0.278563, acc 0.890625, learning_rate 0.000107343
2017-10-11T11:05:50.774784: step 1592, loss 0.241922, acc 0.9375, learning_rate 0.000107313
2017-10-11T11:05:50.845336: step 1593, loss 0.267939, acc 0.890625, learning_rate 0.000107283
2017-10-11T11:05:50.914870: step 1594, loss 0.251815, acc 0.921875, learning_rate 0.000107253
2017-10-11T11:05:50.985669: step 1595, loss 0.180411, acc 0.953125, learning_rate 0.000107224
2017-10-11T11:05:51.052027: step 1596, loss 0.204954, acc 0.9375, learning_rate 0.000107194
2017-10-11T11:05:51.122061: step 1597, loss 0.201702, acc 0.953125, learning_rate 0.000107165
2017-10-11T11:05:51.191844: step 1598, loss 0.156968, acc 0.953125, learning_rate 0.000107136
2017-10-11T11:05:51.264249: step 1599, loss 0.280324, acc 0.875, learning_rate 0.000107106
2017-10-11T11:05:51.335952: step 1600, loss 0.269158, acc 0.9375, learning_rate 0.000107077

Evaluation:
2017-10-11T11:05:51.512708: step 1600, loss 0.251517, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1600

2017-10-11T11:05:52.116423: step 1601, loss 0.246275, acc 0.890625, learning_rate 0.000107048
2017-10-11T11:05:52.189034: step 1602, loss 0.259677, acc 0.984375, learning_rate 0.00010702
2017-10-11T11:05:52.257916: step 1603, loss 0.210255, acc 0.890625, learning_rate 0.000106991
2017-10-11T11:05:52.334454: step 1604, loss 0.356725, acc 0.859375, learning_rate 0.000106963
2017-10-11T11:05:52.408290: step 1605, loss 0.201258, acc 0.9375, learning_rate 0.000106934
2017-10-11T11:05:52.480752: step 1606, loss 0.189239, acc 0.953125, learning_rate 0.000106906
2017-10-11T11:05:52.551500: step 1607, loss 0.370974, acc 0.90625, learning_rate 0.000106878
2017-10-11T11:05:52.623640: step 1608, loss 0.205302, acc 0.9375, learning_rate 0.00010685
2017-10-11T11:05:52.694412: step 1609, loss 0.302194, acc 0.890625, learning_rate 0.000106822
2017-10-11T11:05:52.765087: step 1610, loss 0.278075, acc 0.890625, learning_rate 0.000106794
2017-10-11T11:05:52.839541: step 1611, loss 0.273158, acc 0.9375, learning_rate 0.000106766
2017-10-11T11:05:52.912004: step 1612, loss 0.232963, acc 0.90625, learning_rate 0.000106738
2017-10-11T11:05:52.983175: step 1613, loss 0.168131, acc 0.96875, learning_rate 0.000106711
2017-10-11T11:05:53.056949: step 1614, loss 0.281952, acc 0.890625, learning_rate 0.000106684
2017-10-11T11:05:53.129341: step 1615, loss 0.176174, acc 0.9375, learning_rate 0.000106656
2017-10-11T11:05:53.198450: step 1616, loss 0.180647, acc 0.921875, learning_rate 0.000106629
2017-10-11T11:05:53.270949: step 1617, loss 0.150688, acc 0.9375, learning_rate 0.000106602
2017-10-11T11:05:53.343423: step 1618, loss 0.295628, acc 0.875, learning_rate 0.000106575
2017-10-11T11:05:53.413349: step 1619, loss 0.202947, acc 0.953125, learning_rate 0.000106548
2017-10-11T11:05:53.489287: step 1620, loss 0.349994, acc 0.875, learning_rate 0.000106521
2017-10-11T11:05:53.561383: step 1621, loss 0.326159, acc 0.890625, learning_rate 0.000106495
2017-10-11T11:05:53.635268: step 1622, loss 0.270386, acc 0.921875, learning_rate 0.000106468
2017-10-11T11:05:53.708276: step 1623, loss 0.190422, acc 0.96875, learning_rate 0.000106442
2017-10-11T11:05:53.779432: step 1624, loss 0.290101, acc 0.90625, learning_rate 0.000106416
2017-10-11T11:05:53.856953: step 1625, loss 0.269143, acc 0.921875, learning_rate 0.000106389
2017-10-11T11:05:53.927757: step 1626, loss 0.188009, acc 0.9375, learning_rate 0.000106363
2017-10-11T11:05:53.996963: step 1627, loss 0.36957, acc 0.859375, learning_rate 0.000106337
2017-10-11T11:05:54.066285: step 1628, loss 0.256035, acc 0.890625, learning_rate 0.000106312
2017-10-11T11:05:54.137696: step 1629, loss 0.207996, acc 0.921875, learning_rate 0.000106286
2017-10-11T11:05:54.209740: step 1630, loss 0.187841, acc 0.921875, learning_rate 0.00010626
2017-10-11T11:05:54.282082: step 1631, loss 0.262984, acc 0.921875, learning_rate 0.000106235
2017-10-11T11:05:54.350748: step 1632, loss 0.364264, acc 0.890625, learning_rate 0.000106209
2017-10-11T11:05:54.421334: step 1633, loss 0.358545, acc 0.875, learning_rate 0.000106184
2017-10-11T11:05:54.494662: step 1634, loss 0.189744, acc 0.921875, learning_rate 0.000106159
2017-10-11T11:05:54.564065: step 1635, loss 0.207549, acc 0.9375, learning_rate 0.000106133
2017-10-11T11:05:54.642224: step 1636, loss 0.130863, acc 0.96875, learning_rate 0.000106108
2017-10-11T11:05:54.712250: step 1637, loss 0.225118, acc 0.90625, learning_rate 0.000106083
2017-10-11T11:05:54.786686: step 1638, loss 0.274046, acc 0.890625, learning_rate 0.000106059
2017-10-11T11:05:54.859559: step 1639, loss 0.206052, acc 0.9375, learning_rate 0.000106034
2017-10-11T11:05:54.929693: step 1640, loss 0.258068, acc 0.890625, learning_rate 0.000106009

Evaluation:
2017-10-11T11:05:55.102073: step 1640, loss 0.251623, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1640

2017-10-11T11:05:55.738040: step 1641, loss 0.214378, acc 0.953125, learning_rate 0.000105985
2017-10-11T11:05:55.804866: step 1642, loss 0.257606, acc 0.875, learning_rate 0.00010596
2017-10-11T11:05:55.878685: step 1643, loss 0.263908, acc 0.875, learning_rate 0.000105936
2017-10-11T11:05:55.952961: step 1644, loss 0.439792, acc 0.84375, learning_rate 0.000105912
2017-10-11T11:05:56.021890: step 1645, loss 0.319206, acc 0.890625, learning_rate 0.000105888
2017-10-11T11:05:56.091083: step 1646, loss 0.266869, acc 0.921875, learning_rate 0.000105864
2017-10-11T11:05:56.163576: step 1647, loss 0.151481, acc 0.96875, learning_rate 0.00010584
2017-10-11T11:05:56.237187: step 1648, loss 0.374703, acc 0.921875, learning_rate 0.000105816
2017-10-11T11:05:56.309417: step 1649, loss 0.368622, acc 0.875, learning_rate 0.000105792
2017-10-11T11:05:56.380860: step 1650, loss 0.266766, acc 0.890625, learning_rate 0.000105768
2017-10-11T11:05:56.452033: step 1651, loss 0.209559, acc 0.9375, learning_rate 0.000105745
2017-10-11T11:05:56.523749: step 1652, loss 0.264102, acc 0.953125, learning_rate 0.000105721
2017-10-11T11:05:56.595988: step 1653, loss 0.266648, acc 0.90625, learning_rate 0.000105698
2017-10-11T11:05:56.665325: step 1654, loss 0.256392, acc 0.921875, learning_rate 0.000105675
2017-10-11T11:05:56.738614: step 1655, loss 0.326249, acc 0.890625, learning_rate 0.000105652
2017-10-11T11:05:56.811432: step 1656, loss 0.149488, acc 0.96875, learning_rate 0.000105629
2017-10-11T11:05:56.888126: step 1657, loss 0.210832, acc 0.953125, learning_rate 0.000105606
2017-10-11T11:05:56.959221: step 1658, loss 0.297482, acc 0.890625, learning_rate 0.000105583
2017-10-11T11:05:57.031978: step 1659, loss 0.295666, acc 0.890625, learning_rate 0.00010556
2017-10-11T11:05:57.102497: step 1660, loss 0.349754, acc 0.875, learning_rate 0.000105537
2017-10-11T11:05:57.172409: step 1661, loss 0.264306, acc 0.9375, learning_rate 0.000105515
2017-10-11T11:05:57.242061: step 1662, loss 0.165806, acc 0.953125, learning_rate 0.000105492
2017-10-11T11:05:57.314080: step 1663, loss 0.182766, acc 0.9375, learning_rate 0.00010547
2017-10-11T11:05:57.386000: step 1664, loss 0.379966, acc 0.859375, learning_rate 0.000105447
2017-10-11T11:05:57.457753: step 1665, loss 0.149451, acc 0.984375, learning_rate 0.000105425
2017-10-11T11:05:57.519990: step 1666, loss 0.247642, acc 0.921569, learning_rate 0.000105403
2017-10-11T11:05:57.595154: step 1667, loss 0.199629, acc 0.9375, learning_rate 0.000105381
2017-10-11T11:05:57.668752: step 1668, loss 0.38126, acc 0.875, learning_rate 0.000105359
2017-10-11T11:05:57.743337: step 1669, loss 0.25026, acc 0.921875, learning_rate 0.000105337
2017-10-11T11:05:57.813536: step 1670, loss 0.206864, acc 0.921875, learning_rate 0.000105315
2017-10-11T11:05:57.890917: step 1671, loss 0.228145, acc 0.90625, learning_rate 0.000105294
2017-10-11T11:05:57.966604: step 1672, loss 0.320522, acc 0.921875, learning_rate 0.000105272
2017-10-11T11:05:58.037511: step 1673, loss 0.257832, acc 0.9375, learning_rate 0.000105251
2017-10-11T11:05:58.109490: step 1674, loss 0.235695, acc 0.921875, learning_rate 0.000105229
2017-10-11T11:05:58.183389: step 1675, loss 0.161715, acc 0.953125, learning_rate 0.000105208
2017-10-11T11:05:58.260326: step 1676, loss 0.29014, acc 0.90625, learning_rate 0.000105186
2017-10-11T11:05:58.333074: step 1677, loss 0.152056, acc 0.953125, learning_rate 0.000105165
2017-10-11T11:05:58.402162: step 1678, loss 0.216143, acc 0.9375, learning_rate 0.000105144
2017-10-11T11:05:58.474142: step 1679, loss 0.147855, acc 0.953125, learning_rate 0.000105123
2017-10-11T11:05:58.545307: step 1680, loss 0.235886, acc 0.890625, learning_rate 0.000105102

Evaluation:
2017-10-11T11:05:58.718465: step 1680, loss 0.251972, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1680

2017-10-11T11:05:59.227309: step 1681, loss 0.212698, acc 0.9375, learning_rate 0.000105081
2017-10-11T11:05:59.297381: step 1682, loss 0.249266, acc 0.9375, learning_rate 0.000105061
2017-10-11T11:05:59.369266: step 1683, loss 0.099245, acc 0.96875, learning_rate 0.00010504
2017-10-11T11:05:59.439445: step 1684, loss 0.24517, acc 0.921875, learning_rate 0.00010502
2017-10-11T11:05:59.510321: step 1685, loss 0.229481, acc 0.890625, learning_rate 0.000104999
2017-10-11T11:05:59.581997: step 1686, loss 0.172804, acc 0.9375, learning_rate 0.000104979
2017-10-11T11:05:59.651993: step 1687, loss 0.281399, acc 0.890625, learning_rate 0.000104958
2017-10-11T11:05:59.722406: step 1688, loss 0.206689, acc 0.90625, learning_rate 0.000104938
2017-10-11T11:05:59.791814: step 1689, loss 0.285963, acc 0.921875, learning_rate 0.000104918
2017-10-11T11:05:59.861357: step 1690, loss 0.254494, acc 0.921875, learning_rate 0.000104898
2017-10-11T11:05:59.933718: step 1691, loss 0.106162, acc 1, learning_rate 0.000104878
2017-10-11T11:06:00.004750: step 1692, loss 0.404585, acc 0.875, learning_rate 0.000104858
2017-10-11T11:06:00.077064: step 1693, loss 0.301743, acc 0.90625, learning_rate 0.000104838
2017-10-11T11:06:00.148284: step 1694, loss 0.167212, acc 0.953125, learning_rate 0.000104818
2017-10-11T11:06:00.218125: step 1695, loss 0.227505, acc 0.921875, learning_rate 0.000104799
2017-10-11T11:06:00.288840: step 1696, loss 0.190376, acc 0.953125, learning_rate 0.000104779
2017-10-11T11:06:00.357278: step 1697, loss 0.264372, acc 0.90625, learning_rate 0.00010476
2017-10-11T11:06:00.429586: step 1698, loss 0.161558, acc 0.96875, learning_rate 0.00010474
2017-10-11T11:06:00.499608: step 1699, loss 0.119005, acc 0.984375, learning_rate 0.000104721
2017-10-11T11:06:00.574646: step 1700, loss 0.0885653, acc 0.984375, learning_rate 0.000104702
2017-10-11T11:06:00.646935: step 1701, loss 0.304694, acc 0.875, learning_rate 0.000104682
2017-10-11T11:06:00.716525: step 1702, loss 0.204101, acc 0.9375, learning_rate 0.000104663
2017-10-11T11:06:00.785203: step 1703, loss 0.200548, acc 0.9375, learning_rate 0.000104644
2017-10-11T11:06:00.857673: step 1704, loss 0.258731, acc 0.921875, learning_rate 0.000104625
2017-10-11T11:06:00.929934: step 1705, loss 0.346032, acc 0.8125, learning_rate 0.000104606
2017-10-11T11:06:01.003852: step 1706, loss 0.142099, acc 0.953125, learning_rate 0.000104588
2017-10-11T11:06:01.076928: step 1707, loss 0.17298, acc 0.9375, learning_rate 0.000104569
2017-10-11T11:06:01.148304: step 1708, loss 0.228693, acc 0.9375, learning_rate 0.00010455
2017-10-11T11:06:01.221070: step 1709, loss 0.262642, acc 0.921875, learning_rate 0.000104532
2017-10-11T11:06:01.292986: step 1710, loss 0.242791, acc 0.90625, learning_rate 0.000104513
2017-10-11T11:06:01.364701: step 1711, loss 0.297386, acc 0.875, learning_rate 0.000104495
2017-10-11T11:06:01.433724: step 1712, loss 0.306267, acc 0.890625, learning_rate 0.000104476
2017-10-11T11:06:01.503739: step 1713, loss 0.259568, acc 0.921875, learning_rate 0.000104458
2017-10-11T11:06:01.573468: step 1714, loss 0.187972, acc 0.90625, learning_rate 0.00010444
2017-10-11T11:06:01.643687: step 1715, loss 0.219018, acc 0.953125, learning_rate 0.000104422
2017-10-11T11:06:01.712845: step 1716, loss 0.164113, acc 0.96875, learning_rate 0.000104404
2017-10-11T11:06:01.785126: step 1717, loss 0.370114, acc 0.859375, learning_rate 0.000104386
2017-10-11T11:06:01.855863: step 1718, loss 0.234975, acc 0.9375, learning_rate 0.000104368
2017-10-11T11:06:01.925621: step 1719, loss 0.28447, acc 0.859375, learning_rate 0.00010435
2017-10-11T11:06:01.997752: step 1720, loss 0.218642, acc 0.984375, learning_rate 0.000104332

Evaluation:
2017-10-11T11:06:02.171986: step 1720, loss 0.252476, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1720

2017-10-11T11:06:03.033965: step 1721, loss 0.368716, acc 0.90625, learning_rate 0.000104315
2017-10-11T11:06:03.103966: step 1722, loss 0.303031, acc 0.890625, learning_rate 0.000104297
2017-10-11T11:06:03.177730: step 1723, loss 0.331556, acc 0.921875, learning_rate 0.000104279
2017-10-11T11:06:03.247058: step 1724, loss 0.207612, acc 0.9375, learning_rate 0.000104262
2017-10-11T11:06:03.319551: step 1725, loss 0.378335, acc 0.84375, learning_rate 0.000104245
2017-10-11T11:06:03.389745: step 1726, loss 0.138564, acc 0.96875, learning_rate 0.000104227
2017-10-11T11:06:03.459983: step 1727, loss 0.243075, acc 0.890625, learning_rate 0.00010421
2017-10-11T11:06:03.530514: step 1728, loss 0.217054, acc 0.9375, learning_rate 0.000104193
2017-10-11T11:06:03.604077: step 1729, loss 0.292188, acc 0.890625, learning_rate 0.000104176
2017-10-11T11:06:03.678951: step 1730, loss 0.252418, acc 0.9375, learning_rate 0.000104159
2017-10-11T11:06:03.753288: step 1731, loss 0.20389, acc 0.9375, learning_rate 0.000104142
2017-10-11T11:06:03.825409: step 1732, loss 0.231693, acc 0.921875, learning_rate 0.000104125
2017-10-11T11:06:03.898073: step 1733, loss 0.125694, acc 0.96875, learning_rate 0.000104108
2017-10-11T11:06:03.968382: step 1734, loss 0.285844, acc 0.890625, learning_rate 0.000104091
2017-10-11T11:06:04.038153: step 1735, loss 0.455028, acc 0.828125, learning_rate 0.000104074
2017-10-11T11:06:04.110033: step 1736, loss 0.234247, acc 0.96875, learning_rate 0.000104058
2017-10-11T11:06:04.181857: step 1737, loss 0.142847, acc 0.984375, learning_rate 0.000104041
2017-10-11T11:06:04.254417: step 1738, loss 0.344514, acc 0.859375, learning_rate 0.000104025
2017-10-11T11:06:04.326949: step 1739, loss 0.335959, acc 0.890625, learning_rate 0.000104008
2017-10-11T11:06:04.396034: step 1740, loss 0.24826, acc 0.90625, learning_rate 0.000103992
2017-10-11T11:06:04.472098: step 1741, loss 0.26054, acc 0.921875, learning_rate 0.000103976
2017-10-11T11:06:04.543612: step 1742, loss 0.220862, acc 0.921875, learning_rate 0.000103959
2017-10-11T11:06:04.620944: step 1743, loss 0.261824, acc 0.90625, learning_rate 0.000103943
2017-10-11T11:06:04.690950: step 1744, loss 0.392806, acc 0.875, learning_rate 0.000103927
2017-10-11T11:06:04.762254: step 1745, loss 0.431499, acc 0.84375, learning_rate 0.000103911
2017-10-11T11:06:04.831117: step 1746, loss 0.353684, acc 0.921875, learning_rate 0.000103895
2017-10-11T11:06:04.910767: step 1747, loss 0.178701, acc 0.921875, learning_rate 0.000103879
2017-10-11T11:06:04.980783: step 1748, loss 0.250943, acc 0.921875, learning_rate 0.000103863
2017-10-11T11:06:05.051168: step 1749, loss 0.178124, acc 0.9375, learning_rate 0.000103848
2017-10-11T11:06:05.120900: step 1750, loss 0.210886, acc 0.9375, learning_rate 0.000103832
2017-10-11T11:06:05.189350: step 1751, loss 0.288304, acc 0.921875, learning_rate 0.000103816
2017-10-11T11:06:05.258652: step 1752, loss 0.177899, acc 0.9375, learning_rate 0.000103801
2017-10-11T11:06:05.329821: step 1753, loss 0.227197, acc 0.90625, learning_rate 0.000103785
2017-10-11T11:06:05.402898: step 1754, loss 0.145837, acc 0.953125, learning_rate 0.00010377
2017-10-11T11:06:05.474614: step 1755, loss 0.156796, acc 0.96875, learning_rate 0.000103754
2017-10-11T11:06:05.544847: step 1756, loss 0.170637, acc 0.96875, learning_rate 0.000103739
2017-10-11T11:06:05.616030: step 1757, loss 0.299952, acc 0.9375, learning_rate 0.000103724
2017-10-11T11:06:05.686514: step 1758, loss 0.257315, acc 0.9375, learning_rate 0.000103709
2017-10-11T11:06:05.759269: step 1759, loss 0.263925, acc 0.90625, learning_rate 0.000103694
2017-10-11T11:06:05.830416: step 1760, loss 0.188228, acc 0.984375, learning_rate 0.000103678

Evaluation:
2017-10-11T11:06:06.005982: step 1760, loss 0.250599, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1760

2017-10-11T11:06:06.640770: step 1761, loss 0.193107, acc 0.953125, learning_rate 0.000103663
2017-10-11T11:06:06.712292: step 1762, loss 0.220125, acc 0.90625, learning_rate 0.000103648
2017-10-11T11:06:06.783726: step 1763, loss 0.300749, acc 0.921875, learning_rate 0.000103634
2017-10-11T11:06:06.848807: step 1764, loss 0.226617, acc 0.941176, learning_rate 0.000103619
2017-10-11T11:06:06.919466: step 1765, loss 0.315485, acc 0.953125, learning_rate 0.000103604
2017-10-11T11:06:06.989792: step 1766, loss 0.237129, acc 0.921875, learning_rate 0.000103589
2017-10-11T11:06:07.060508: step 1767, loss 0.316169, acc 0.875, learning_rate 0.000103575
2017-10-11T11:06:07.130030: step 1768, loss 0.136365, acc 0.96875, learning_rate 0.00010356
2017-10-11T11:06:07.199839: step 1769, loss 0.308343, acc 0.890625, learning_rate 0.000103545
2017-10-11T11:06:07.269787: step 1770, loss 0.211522, acc 0.90625, learning_rate 0.000103531
2017-10-11T11:06:07.342220: step 1771, loss 0.37537, acc 0.859375, learning_rate 0.000103517
2017-10-11T11:06:07.415037: step 1772, loss 0.222736, acc 0.9375, learning_rate 0.000103502
2017-10-11T11:06:07.486756: step 1773, loss 0.118473, acc 0.984375, learning_rate 0.000103488
2017-10-11T11:06:07.557823: step 1774, loss 0.213154, acc 0.9375, learning_rate 0.000103474
2017-10-11T11:06:07.624777: step 1775, loss 0.234053, acc 0.90625, learning_rate 0.00010346
2017-10-11T11:06:07.693935: step 1776, loss 0.166671, acc 0.9375, learning_rate 0.000103445
2017-10-11T11:06:07.769055: step 1777, loss 0.247511, acc 0.953125, learning_rate 0.000103431
2017-10-11T11:06:07.841922: step 1778, loss 0.369651, acc 0.890625, learning_rate 0.000103417
2017-10-11T11:06:07.913782: step 1779, loss 0.177665, acc 0.953125, learning_rate 0.000103403
2017-10-11T11:06:07.988232: step 1780, loss 0.240967, acc 0.921875, learning_rate 0.00010339
2017-10-11T11:06:08.059643: step 1781, loss 0.160978, acc 0.9375, learning_rate 0.000103376
2017-10-11T11:06:08.133896: step 1782, loss 0.124246, acc 0.984375, learning_rate 0.000103362
2017-10-11T11:06:08.203224: step 1783, loss 0.213531, acc 0.953125, learning_rate 0.000103348
2017-10-11T11:06:08.282976: step 1784, loss 0.294804, acc 0.890625, learning_rate 0.000103335
2017-10-11T11:06:08.353651: step 1785, loss 0.146105, acc 0.984375, learning_rate 0.000103321
2017-10-11T11:06:08.423932: step 1786, loss 0.337097, acc 0.890625, learning_rate 0.000103307
2017-10-11T11:06:08.491769: step 1787, loss 0.24626, acc 0.921875, learning_rate 0.000103294
2017-10-11T11:06:08.558924: step 1788, loss 0.187664, acc 0.9375, learning_rate 0.00010328
2017-10-11T11:06:08.633889: step 1789, loss 0.206661, acc 0.953125, learning_rate 0.000103267
2017-10-11T11:06:08.707833: step 1790, loss 0.248611, acc 0.9375, learning_rate 0.000103254
2017-10-11T11:06:08.782934: step 1791, loss 0.230673, acc 0.9375, learning_rate 0.00010324
2017-10-11T11:06:08.853796: step 1792, loss 0.238994, acc 0.921875, learning_rate 0.000103227
2017-10-11T11:06:08.923844: step 1793, loss 0.165931, acc 0.953125, learning_rate 0.000103214
2017-10-11T11:06:08.998284: step 1794, loss 0.338156, acc 0.890625, learning_rate 0.000103201
2017-10-11T11:06:09.068009: step 1795, loss 0.198249, acc 0.921875, learning_rate 0.000103188
2017-10-11T11:06:09.139430: step 1796, loss 0.245344, acc 0.921875, learning_rate 0.000103175
2017-10-11T11:06:09.209751: step 1797, loss 0.167536, acc 0.953125, learning_rate 0.000103162
2017-10-11T11:06:09.280755: step 1798, loss 0.276196, acc 0.859375, learning_rate 0.000103149
2017-10-11T11:06:09.350374: step 1799, loss 0.32189, acc 0.890625, learning_rate 0.000103136
2017-10-11T11:06:09.420768: step 1800, loss 0.17393, acc 0.921875, learning_rate 0.000103123

Evaluation:
2017-10-11T11:06:09.597246: step 1800, loss 0.251077, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1800

2017-10-11T11:06:10.106010: step 1801, loss 0.183759, acc 0.953125, learning_rate 0.000103111
2017-10-11T11:06:10.182679: step 1802, loss 0.24097, acc 0.90625, learning_rate 0.000103098
2017-10-11T11:06:10.252417: step 1803, loss 0.187649, acc 0.953125, learning_rate 0.000103085
2017-10-11T11:06:10.324239: step 1804, loss 0.190519, acc 0.953125, learning_rate 0.000103073
2017-10-11T11:06:10.398982: step 1805, loss 0.229763, acc 0.96875, learning_rate 0.00010306
2017-10-11T11:06:10.472982: step 1806, loss 0.214069, acc 0.921875, learning_rate 0.000103048
2017-10-11T11:06:10.542888: step 1807, loss 0.198338, acc 0.96875, learning_rate 0.000103035
2017-10-11T11:06:10.617711: step 1808, loss 0.180593, acc 0.953125, learning_rate 0.000103023
2017-10-11T11:06:10.692633: step 1809, loss 0.179612, acc 0.96875, learning_rate 0.00010301
2017-10-11T11:06:10.764480: step 1810, loss 0.293752, acc 0.921875, learning_rate 0.000102998
2017-10-11T11:06:10.838844: step 1811, loss 0.157859, acc 0.9375, learning_rate 0.000102986
2017-10-11T11:06:10.911838: step 1812, loss 0.361414, acc 0.890625, learning_rate 0.000102974
2017-10-11T11:06:10.982397: step 1813, loss 0.303052, acc 0.875, learning_rate 0.000102962
2017-10-11T11:06:11.053204: step 1814, loss 0.26588, acc 0.90625, learning_rate 0.000102949
2017-10-11T11:06:11.125157: step 1815, loss 0.233344, acc 0.921875, learning_rate 0.000102937
2017-10-11T11:06:11.198895: step 1816, loss 0.27813, acc 0.90625, learning_rate 0.000102925
2017-10-11T11:06:11.273614: step 1817, loss 0.311414, acc 0.890625, learning_rate 0.000102913
2017-10-11T11:06:11.345906: step 1818, loss 0.25267, acc 0.875, learning_rate 0.000102902
2017-10-11T11:06:11.418464: step 1819, loss 0.428996, acc 0.859375, learning_rate 0.00010289
2017-10-11T11:06:11.489690: step 1820, loss 0.215874, acc 0.921875, learning_rate 0.000102878
2017-10-11T11:06:11.564113: step 1821, loss 0.276274, acc 0.890625, learning_rate 0.000102866
2017-10-11T11:06:11.639023: step 1822, loss 0.186704, acc 0.953125, learning_rate 0.000102855
2017-10-11T11:06:11.710950: step 1823, loss 0.298543, acc 0.90625, learning_rate 0.000102843
2017-10-11T11:06:11.782293: step 1824, loss 0.326474, acc 0.90625, learning_rate 0.000102831
2017-10-11T11:06:11.855780: step 1825, loss 0.259187, acc 0.890625, learning_rate 0.00010282
2017-10-11T11:06:11.929876: step 1826, loss 0.239367, acc 0.9375, learning_rate 0.000102808
2017-10-11T11:06:12.002571: step 1827, loss 0.174222, acc 0.953125, learning_rate 0.000102797
2017-10-11T11:06:12.073528: step 1828, loss 0.162896, acc 0.953125, learning_rate 0.000102785
2017-10-11T11:06:12.148107: step 1829, loss 0.180017, acc 0.9375, learning_rate 0.000102774
2017-10-11T11:06:12.217233: step 1830, loss 0.183603, acc 0.9375, learning_rate 0.000102763
2017-10-11T11:06:12.289028: step 1831, loss 0.148475, acc 0.953125, learning_rate 0.000102751
2017-10-11T11:06:12.359010: step 1832, loss 0.24051, acc 0.921875, learning_rate 0.00010274
2017-10-11T11:06:12.434505: step 1833, loss 0.18937, acc 0.9375, learning_rate 0.000102729
2017-10-11T11:06:12.505093: step 1834, loss 0.145731, acc 0.984375, learning_rate 0.000102718
2017-10-11T11:06:12.578039: step 1835, loss 0.211274, acc 0.921875, learning_rate 0.000102707
2017-10-11T11:06:12.650466: step 1836, loss 0.353225, acc 0.890625, learning_rate 0.000102696
2017-10-11T11:06:12.722565: step 1837, loss 0.377768, acc 0.859375, learning_rate 0.000102685
2017-10-11T11:06:12.793441: step 1838, loss 0.405839, acc 0.828125, learning_rate 0.000102674
2017-10-11T11:06:12.866693: step 1839, loss 0.324257, acc 0.875, learning_rate 0.000102663
2017-10-11T11:06:12.946231: step 1840, loss 0.36426, acc 0.890625, learning_rate 0.000102652

Evaluation:
2017-10-11T11:06:13.125778: step 1840, loss 0.250146, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1840

2017-10-11T11:06:13.691552: step 1841, loss 0.214694, acc 0.9375, learning_rate 0.000102641
2017-10-11T11:06:13.762034: step 1842, loss 0.293359, acc 0.9375, learning_rate 0.00010263
2017-10-11T11:06:13.835196: step 1843, loss 0.221642, acc 0.921875, learning_rate 0.00010262
2017-10-11T11:06:13.909508: step 1844, loss 0.18166, acc 0.9375, learning_rate 0.000102609
2017-10-11T11:06:13.980968: step 1845, loss 0.160818, acc 0.96875, learning_rate 0.000102598
2017-10-11T11:06:14.053483: step 1846, loss 0.377838, acc 0.875, learning_rate 0.000102588
2017-10-11T11:06:14.123589: step 1847, loss 0.236882, acc 0.90625, learning_rate 0.000102577
2017-10-11T11:06:14.193266: step 1848, loss 0.162062, acc 0.953125, learning_rate 0.000102567
2017-10-11T11:06:14.265719: step 1849, loss 0.263803, acc 0.921875, learning_rate 0.000102556
2017-10-11T11:06:14.339063: step 1850, loss 0.19237, acc 0.96875, learning_rate 0.000102546
2017-10-11T11:06:14.413808: step 1851, loss 0.250258, acc 0.9375, learning_rate 0.000102535
2017-10-11T11:06:14.488679: step 1852, loss 0.334969, acc 0.875, learning_rate 0.000102525
2017-10-11T11:06:14.563124: step 1853, loss 0.283738, acc 0.921875, learning_rate 0.000102515
2017-10-11T11:06:14.633911: step 1854, loss 0.16794, acc 0.96875, learning_rate 0.000102504
2017-10-11T11:06:14.703310: step 1855, loss 0.152556, acc 0.953125, learning_rate 0.000102494
2017-10-11T11:06:14.774149: step 1856, loss 0.276557, acc 0.875, learning_rate 0.000102484
2017-10-11T11:06:14.845386: step 1857, loss 0.193616, acc 0.921875, learning_rate 0.000102474
2017-10-11T11:06:14.915693: step 1858, loss 0.22391, acc 0.90625, learning_rate 0.000102464
2017-10-11T11:06:14.988878: step 1859, loss 0.12253, acc 0.96875, learning_rate 0.000102454
2017-10-11T11:06:15.060135: step 1860, loss 0.174345, acc 0.9375, learning_rate 0.000102444
2017-10-11T11:06:15.134172: step 1861, loss 0.175205, acc 0.953125, learning_rate 0.000102434
2017-10-11T11:06:15.196605: step 1862, loss 0.356934, acc 0.921569, learning_rate 0.000102424
2017-10-11T11:06:15.268201: step 1863, loss 0.21391, acc 0.9375, learning_rate 0.000102414
2017-10-11T11:06:15.342414: step 1864, loss 0.331892, acc 0.859375, learning_rate 0.000102404
2017-10-11T11:06:15.415425: step 1865, loss 0.18169, acc 0.953125, learning_rate 0.000102394
2017-10-11T11:06:15.488382: step 1866, loss 0.193924, acc 0.921875, learning_rate 0.000102384
2017-10-11T11:06:15.560203: step 1867, loss 0.238726, acc 0.921875, learning_rate 0.000102375
2017-10-11T11:06:15.633836: step 1868, loss 0.290874, acc 0.890625, learning_rate 0.000102365
2017-10-11T11:06:15.704783: step 1869, loss 0.22686, acc 0.921875, learning_rate 0.000102355
2017-10-11T11:06:15.775400: step 1870, loss 0.217685, acc 0.921875, learning_rate 0.000102346
2017-10-11T11:06:15.845742: step 1871, loss 0.31293, acc 0.9375, learning_rate 0.000102336
2017-10-11T11:06:15.917438: step 1872, loss 0.278957, acc 0.921875, learning_rate 0.000102327
2017-10-11T11:06:15.989347: step 1873, loss 0.171863, acc 0.9375, learning_rate 0.000102317
2017-10-11T11:06:16.059769: step 1874, loss 0.2813, acc 0.890625, learning_rate 0.000102308
2017-10-11T11:06:16.130762: step 1875, loss 0.126417, acc 0.984375, learning_rate 0.000102298
2017-10-11T11:06:16.203632: step 1876, loss 0.217745, acc 0.9375, learning_rate 0.000102289
2017-10-11T11:06:16.276916: step 1877, loss 0.220312, acc 0.90625, learning_rate 0.000102279
2017-10-11T11:06:16.349948: step 1878, loss 0.285325, acc 0.9375, learning_rate 0.00010227
2017-10-11T11:06:16.421742: step 1879, loss 0.175244, acc 0.921875, learning_rate 0.000102261
2017-10-11T11:06:16.494997: step 1880, loss 0.1329, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-10-11T11:06:16.671920: step 1880, loss 0.24942, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1880

2017-10-11T11:06:17.314580: step 1881, loss 0.160755, acc 0.96875, learning_rate 0.000102242
2017-10-11T11:06:17.387066: step 1882, loss 0.329719, acc 0.828125, learning_rate 0.000102233
2017-10-11T11:06:17.460967: step 1883, loss 0.291923, acc 0.9375, learning_rate 0.000102224
2017-10-11T11:06:17.531009: step 1884, loss 0.318428, acc 0.90625, learning_rate 0.000102215
2017-10-11T11:06:17.601716: step 1885, loss 0.103775, acc 0.984375, learning_rate 0.000102206
2017-10-11T11:06:17.674262: step 1886, loss 0.219793, acc 0.890625, learning_rate 0.000102197
2017-10-11T11:06:17.745493: step 1887, loss 0.309359, acc 0.890625, learning_rate 0.000102188
2017-10-11T11:06:17.816953: step 1888, loss 0.407134, acc 0.875, learning_rate 0.000102179
2017-10-11T11:06:17.891397: step 1889, loss 0.201624, acc 0.953125, learning_rate 0.00010217
2017-10-11T11:06:17.962540: step 1890, loss 0.279538, acc 0.875, learning_rate 0.000102161
2017-10-11T11:06:18.031606: step 1891, loss 0.201759, acc 0.9375, learning_rate 0.000102153
2017-10-11T11:06:18.107822: step 1892, loss 0.231404, acc 0.9375, learning_rate 0.000102144
2017-10-11T11:06:18.178501: step 1893, loss 0.285935, acc 0.921875, learning_rate 0.000102135
2017-10-11T11:06:18.246361: step 1894, loss 0.209256, acc 0.9375, learning_rate 0.000102126
2017-10-11T11:06:18.319211: step 1895, loss 0.167298, acc 0.953125, learning_rate 0.000102118
2017-10-11T11:06:18.392858: step 1896, loss 0.213868, acc 0.953125, learning_rate 0.000102109
2017-10-11T11:06:18.465040: step 1897, loss 0.260674, acc 0.9375, learning_rate 0.0001021
2017-10-11T11:06:18.540364: step 1898, loss 0.472422, acc 0.875, learning_rate 0.000102092
2017-10-11T11:06:18.612224: step 1899, loss 0.230902, acc 0.9375, learning_rate 0.000102083
2017-10-11T11:06:18.682173: step 1900, loss 0.270486, acc 0.90625, learning_rate 0.000102075
2017-10-11T11:06:18.752667: step 1901, loss 0.290758, acc 0.875, learning_rate 0.000102066
2017-10-11T11:06:18.826719: step 1902, loss 0.184453, acc 0.921875, learning_rate 0.000102058
2017-10-11T11:06:18.899910: step 1903, loss 0.24139, acc 0.9375, learning_rate 0.00010205
2017-10-11T11:06:18.967291: step 1904, loss 0.234323, acc 0.953125, learning_rate 0.000102041
2017-10-11T11:06:19.040465: step 1905, loss 0.332759, acc 0.859375, learning_rate 0.000102033
2017-10-11T11:06:19.109654: step 1906, loss 0.135991, acc 0.984375, learning_rate 0.000102025
2017-10-11T11:06:19.180195: step 1907, loss 0.157126, acc 0.953125, learning_rate 0.000102016
2017-10-11T11:06:19.253898: step 1908, loss 0.158247, acc 0.96875, learning_rate 0.000102008
2017-10-11T11:06:19.322202: step 1909, loss 0.182218, acc 0.96875, learning_rate 0.000102
2017-10-11T11:06:19.393332: step 1910, loss 0.171119, acc 0.9375, learning_rate 0.000101992
2017-10-11T11:06:19.465774: step 1911, loss 0.33968, acc 0.890625, learning_rate 0.000101984
2017-10-11T11:06:19.541009: step 1912, loss 0.217451, acc 0.9375, learning_rate 0.000101975
2017-10-11T11:06:19.614192: step 1913, loss 0.276447, acc 0.921875, learning_rate 0.000101967
2017-10-11T11:06:19.683433: step 1914, loss 0.278627, acc 0.90625, learning_rate 0.000101959
2017-10-11T11:06:19.753831: step 1915, loss 0.188836, acc 0.921875, learning_rate 0.000101951
2017-10-11T11:06:19.822414: step 1916, loss 0.310069, acc 0.875, learning_rate 0.000101943
2017-10-11T11:06:19.893324: step 1917, loss 0.180003, acc 0.953125, learning_rate 0.000101935
2017-10-11T11:06:19.965277: step 1918, loss 0.188166, acc 0.953125, learning_rate 0.000101928
2017-10-11T11:06:20.034404: step 1919, loss 0.328433, acc 0.90625, learning_rate 0.00010192
2017-10-11T11:06:20.104880: step 1920, loss 0.331045, acc 0.90625, learning_rate 0.000101912

Evaluation:
2017-10-11T11:06:20.276627: step 1920, loss 0.249291, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1920

2017-10-11T11:06:20.774045: step 1921, loss 0.197148, acc 0.921875, learning_rate 0.000101904
2017-10-11T11:06:20.845913: step 1922, loss 0.277344, acc 0.875, learning_rate 0.000101896
2017-10-11T11:06:20.915158: step 1923, loss 0.375339, acc 0.875, learning_rate 0.000101889
2017-10-11T11:06:20.986547: step 1924, loss 0.344845, acc 0.875, learning_rate 0.000101881
2017-10-11T11:06:21.054439: step 1925, loss 0.261424, acc 0.921875, learning_rate 0.000101873
2017-10-11T11:06:21.133071: step 1926, loss 0.408259, acc 0.921875, learning_rate 0.000101865
2017-10-11T11:06:21.203511: step 1927, loss 0.215186, acc 0.9375, learning_rate 0.000101858
2017-10-11T11:06:21.274779: step 1928, loss 0.292082, acc 0.890625, learning_rate 0.00010185
2017-10-11T11:06:21.347016: step 1929, loss 0.179767, acc 0.9375, learning_rate 0.000101843
2017-10-11T11:06:21.419479: step 1930, loss 0.25434, acc 0.921875, learning_rate 0.000101835
2017-10-11T11:06:21.499014: step 1931, loss 0.198378, acc 0.96875, learning_rate 0.000101828
2017-10-11T11:06:21.577943: step 1932, loss 0.187634, acc 0.890625, learning_rate 0.00010182
2017-10-11T11:06:21.645878: step 1933, loss 0.342817, acc 0.875, learning_rate 0.000101813
2017-10-11T11:06:21.716551: step 1934, loss 0.276914, acc 0.890625, learning_rate 0.000101805
2017-10-11T11:06:21.792117: step 1935, loss 0.261648, acc 0.921875, learning_rate 0.000101798
2017-10-11T11:06:21.865729: step 1936, loss 0.298005, acc 0.875, learning_rate 0.000101791
2017-10-11T11:06:21.938547: step 1937, loss 0.362502, acc 0.890625, learning_rate 0.000101783
2017-10-11T11:06:22.009944: step 1938, loss 0.151959, acc 0.921875, learning_rate 0.000101776
2017-10-11T11:06:22.084590: step 1939, loss 0.207901, acc 0.9375, learning_rate 0.000101769
2017-10-11T11:06:22.156810: step 1940, loss 0.354291, acc 0.84375, learning_rate 0.000101762
2017-10-11T11:06:22.229166: step 1941, loss 0.195437, acc 0.953125, learning_rate 0.000101754
2017-10-11T11:06:22.300654: step 1942, loss 0.213113, acc 0.953125, learning_rate 0.000101747
2017-10-11T11:06:22.371958: step 1943, loss 0.232359, acc 0.921875, learning_rate 0.00010174
2017-10-11T11:06:22.442412: step 1944, loss 0.196009, acc 0.953125, learning_rate 0.000101733
2017-10-11T11:06:22.513786: step 1945, loss 0.303136, acc 0.921875, learning_rate 0.000101726
2017-10-11T11:06:22.587078: step 1946, loss 0.317198, acc 0.859375, learning_rate 0.000101719
2017-10-11T11:06:22.657085: step 1947, loss 0.27514, acc 0.90625, learning_rate 0.000101712
2017-10-11T11:06:22.725490: step 1948, loss 0.343975, acc 0.921875, learning_rate 0.000101705
2017-10-11T11:06:22.794115: step 1949, loss 0.277538, acc 0.890625, learning_rate 0.000101698
2017-10-11T11:06:22.868096: step 1950, loss 0.230534, acc 0.921875, learning_rate 0.000101691
2017-10-11T11:06:22.942070: step 1951, loss 0.209962, acc 0.953125, learning_rate 0.000101684
2017-10-11T11:06:23.013638: step 1952, loss 0.271578, acc 0.90625, learning_rate 0.000101677
2017-10-11T11:06:23.090013: step 1953, loss 0.281963, acc 0.9375, learning_rate 0.00010167
2017-10-11T11:06:23.163735: step 1954, loss 0.277627, acc 0.921875, learning_rate 0.000101664
2017-10-11T11:06:23.230678: step 1955, loss 0.275671, acc 0.875, learning_rate 0.000101657
2017-10-11T11:06:23.300831: step 1956, loss 0.21192, acc 0.921875, learning_rate 0.00010165
2017-10-11T11:06:23.370957: step 1957, loss 0.234278, acc 0.9375, learning_rate 0.000101643
2017-10-11T11:06:23.443434: step 1958, loss 0.231305, acc 0.890625, learning_rate 0.000101637
2017-10-11T11:06:23.516626: step 1959, loss 0.258082, acc 0.921875, learning_rate 0.00010163
2017-10-11T11:06:23.581527: step 1960, loss 0.220586, acc 0.921569, learning_rate 0.000101623

Evaluation:
2017-10-11T11:06:23.753020: step 1960, loss 0.249497, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-1960

2017-10-11T11:06:24.330005: step 1961, loss 0.141642, acc 0.984375, learning_rate 0.000101617
2017-10-11T11:06:24.400804: step 1962, loss 0.219731, acc 0.96875, learning_rate 0.00010161
2017-10-11T11:06:24.473393: step 1963, loss 0.365398, acc 0.890625, learning_rate 0.000101604
2017-10-11T11:06:24.543121: step 1964, loss 0.263601, acc 0.921875, learning_rate 0.000101597
2017-10-11T11:06:24.613131: step 1965, loss 0.126382, acc 0.96875, learning_rate 0.00010159
2017-10-11T11:06:24.683651: step 1966, loss 0.176802, acc 0.9375, learning_rate 0.000101584
2017-10-11T11:06:24.753646: step 1967, loss 0.186668, acc 0.96875, learning_rate 0.000101577
2017-10-11T11:06:24.825053: step 1968, loss 0.181939, acc 0.921875, learning_rate 0.000101571
2017-10-11T11:06:24.897094: step 1969, loss 0.211181, acc 0.9375, learning_rate 0.000101565
2017-10-11T11:06:24.964266: step 1970, loss 0.138145, acc 0.9375, learning_rate 0.000101558
2017-10-11T11:06:25.036594: step 1971, loss 0.2169, acc 0.9375, learning_rate 0.000101552
2017-10-11T11:06:25.106283: step 1972, loss 0.222866, acc 0.9375, learning_rate 0.000101546
2017-10-11T11:06:25.176769: step 1973, loss 0.390859, acc 0.84375, learning_rate 0.000101539
2017-10-11T11:06:25.248173: step 1974, loss 0.16355, acc 0.96875, learning_rate 0.000101533
2017-10-11T11:06:25.318760: step 1975, loss 0.341377, acc 0.90625, learning_rate 0.000101527
2017-10-11T11:06:25.391764: step 1976, loss 0.239954, acc 0.921875, learning_rate 0.00010152
2017-10-11T11:06:25.460775: step 1977, loss 0.265499, acc 0.921875, learning_rate 0.000101514
2017-10-11T11:06:25.531797: step 1978, loss 0.14322, acc 0.96875, learning_rate 0.000101508
2017-10-11T11:06:25.601961: step 1979, loss 0.256175, acc 0.9375, learning_rate 0.000101502
2017-10-11T11:06:25.671631: step 1980, loss 0.181239, acc 0.953125, learning_rate 0.000101496
2017-10-11T11:06:25.741586: step 1981, loss 0.354081, acc 0.875, learning_rate 0.00010149
2017-10-11T11:06:25.812161: step 1982, loss 0.199636, acc 0.9375, learning_rate 0.000101484
2017-10-11T11:06:25.881925: step 1983, loss 0.215975, acc 0.890625, learning_rate 0.000101478
2017-10-11T11:06:25.953773: step 1984, loss 0.315422, acc 0.890625, learning_rate 0.000101472
2017-10-11T11:06:26.029496: step 1985, loss 0.205703, acc 0.921875, learning_rate 0.000101466
2017-10-11T11:06:26.098624: step 1986, loss 0.179195, acc 0.9375, learning_rate 0.00010146
2017-10-11T11:06:26.170289: step 1987, loss 0.187455, acc 0.96875, learning_rate 0.000101454
2017-10-11T11:06:26.240926: step 1988, loss 0.360727, acc 0.875, learning_rate 0.000101448
2017-10-11T11:06:26.315271: step 1989, loss 0.259234, acc 0.90625, learning_rate 0.000101442
2017-10-11T11:06:26.383220: step 1990, loss 0.201177, acc 0.953125, learning_rate 0.000101436
2017-10-11T11:06:26.457465: step 1991, loss 0.219583, acc 0.9375, learning_rate 0.00010143
2017-10-11T11:06:26.530306: step 1992, loss 0.210684, acc 0.9375, learning_rate 0.000101424
2017-10-11T11:06:26.600976: step 1993, loss 0.226125, acc 0.921875, learning_rate 0.000101418
2017-10-11T11:06:26.670308: step 1994, loss 0.299266, acc 0.890625, learning_rate 0.000101413
2017-10-11T11:06:26.741604: step 1995, loss 0.129391, acc 0.96875, learning_rate 0.000101407
2017-10-11T11:06:26.817015: step 1996, loss 0.239206, acc 0.921875, learning_rate 0.000101401
2017-10-11T11:06:26.891490: step 1997, loss 0.215532, acc 0.9375, learning_rate 0.000101395
2017-10-11T11:06:26.965661: step 1998, loss 0.215569, acc 0.9375, learning_rate 0.00010139
2017-10-11T11:06:27.048158: step 1999, loss 0.192077, acc 0.9375, learning_rate 0.000101384
2017-10-11T11:06:27.134192: step 2000, loss 0.256894, acc 0.921875, learning_rate 0.000101378

Evaluation:
2017-10-11T11:06:27.307315: step 2000, loss 0.249252, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2000

2017-10-11T11:06:28.165488: step 2001, loss 0.15084, acc 0.96875, learning_rate 0.000101373
2017-10-11T11:06:28.235175: step 2002, loss 0.234383, acc 0.921875, learning_rate 0.000101367
2017-10-11T11:06:28.307139: step 2003, loss 0.227595, acc 0.9375, learning_rate 0.000101362
2017-10-11T11:06:28.377921: step 2004, loss 0.319534, acc 0.921875, learning_rate 0.000101356
2017-10-11T11:06:28.448787: step 2005, loss 0.217126, acc 0.9375, learning_rate 0.00010135
2017-10-11T11:06:28.527017: step 2006, loss 0.225504, acc 0.90625, learning_rate 0.000101345
2017-10-11T11:06:28.601420: step 2007, loss 0.336424, acc 0.875, learning_rate 0.000101339
2017-10-11T11:06:28.671983: step 2008, loss 0.213834, acc 0.921875, learning_rate 0.000101334
2017-10-11T11:06:28.742447: step 2009, loss 0.209659, acc 0.921875, learning_rate 0.000101328
2017-10-11T11:06:28.814176: step 2010, loss 0.405685, acc 0.859375, learning_rate 0.000101323
2017-10-11T11:06:28.885802: step 2011, loss 0.169308, acc 0.9375, learning_rate 0.000101318
2017-10-11T11:06:28.956145: step 2012, loss 0.252761, acc 0.921875, learning_rate 0.000101312
2017-10-11T11:06:29.025995: step 2013, loss 0.259966, acc 0.875, learning_rate 0.000101307
2017-10-11T11:06:29.098317: step 2014, loss 0.204492, acc 0.921875, learning_rate 0.000101302
2017-10-11T11:06:29.168760: step 2015, loss 0.326542, acc 0.9375, learning_rate 0.000101296
2017-10-11T11:06:29.240704: step 2016, loss 0.116279, acc 0.984375, learning_rate 0.000101291
2017-10-11T11:06:29.313687: step 2017, loss 0.321173, acc 0.890625, learning_rate 0.000101286
2017-10-11T11:06:29.387348: step 2018, loss 0.270225, acc 0.9375, learning_rate 0.00010128
2017-10-11T11:06:29.461525: step 2019, loss 0.112874, acc 0.984375, learning_rate 0.000101275
2017-10-11T11:06:29.535184: step 2020, loss 0.157003, acc 0.953125, learning_rate 0.00010127
2017-10-11T11:06:29.605143: step 2021, loss 0.227058, acc 0.96875, learning_rate 0.000101265
2017-10-11T11:06:29.675202: step 2022, loss 0.277077, acc 0.921875, learning_rate 0.00010126
2017-10-11T11:06:29.746920: step 2023, loss 0.10121, acc 0.984375, learning_rate 0.000101255
2017-10-11T11:06:29.819050: step 2024, loss 0.203884, acc 0.921875, learning_rate 0.000101249
2017-10-11T11:06:29.894027: step 2025, loss 0.32776, acc 0.890625, learning_rate 0.000101244
2017-10-11T11:06:29.966286: step 2026, loss 0.21348, acc 0.9375, learning_rate 0.000101239
2017-10-11T11:06:30.040782: step 2027, loss 0.284566, acc 0.890625, learning_rate 0.000101234
2017-10-11T11:06:30.116968: step 2028, loss 0.299209, acc 0.921875, learning_rate 0.000101229
2017-10-11T11:06:30.190571: step 2029, loss 0.26286, acc 0.90625, learning_rate 0.000101224
2017-10-11T11:06:30.260085: step 2030, loss 0.142637, acc 0.953125, learning_rate 0.000101219
2017-10-11T11:06:30.330328: step 2031, loss 0.18546, acc 0.96875, learning_rate 0.000101214
2017-10-11T11:06:30.399564: step 2032, loss 0.225951, acc 0.921875, learning_rate 0.000101209
2017-10-11T11:06:30.473924: step 2033, loss 0.179709, acc 0.953125, learning_rate 0.000101204
2017-10-11T11:06:30.546563: step 2034, loss 0.240779, acc 0.890625, learning_rate 0.000101199
2017-10-11T11:06:30.618727: step 2035, loss 0.185953, acc 0.921875, learning_rate 0.000101194
2017-10-11T11:06:30.689235: step 2036, loss 0.137621, acc 0.953125, learning_rate 0.00010119
2017-10-11T11:06:30.761497: step 2037, loss 0.103154, acc 0.984375, learning_rate 0.000101185
2017-10-11T11:06:30.831438: step 2038, loss 0.304216, acc 0.859375, learning_rate 0.00010118
2017-10-11T11:06:30.904521: step 2039, loss 0.179348, acc 0.96875, learning_rate 0.000101175
2017-10-11T11:06:30.983116: step 2040, loss 0.37626, acc 0.859375, learning_rate 0.00010117

Evaluation:
2017-10-11T11:06:31.173002: step 2040, loss 0.248401, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2040

2017-10-11T11:06:31.829612: step 2041, loss 0.218485, acc 0.9375, learning_rate 0.000101166
2017-10-11T11:06:31.903336: step 2042, loss 0.384067, acc 0.875, learning_rate 0.000101161
2017-10-11T11:06:31.976114: step 2043, loss 0.253261, acc 0.90625, learning_rate 0.000101156
2017-10-11T11:06:32.048083: step 2044, loss 0.212724, acc 0.953125, learning_rate 0.000101151
2017-10-11T11:06:32.118633: step 2045, loss 0.188957, acc 0.984375, learning_rate 0.000101147
2017-10-11T11:06:32.188687: step 2046, loss 0.165219, acc 0.9375, learning_rate 0.000101142
2017-10-11T11:06:32.262903: step 2047, loss 0.370518, acc 0.890625, learning_rate 0.000101137
2017-10-11T11:06:32.338756: step 2048, loss 0.203933, acc 0.921875, learning_rate 0.000101133
2017-10-11T11:06:32.413785: step 2049, loss 0.257203, acc 0.90625, learning_rate 0.000101128
2017-10-11T11:06:32.489036: step 2050, loss 0.292715, acc 0.90625, learning_rate 0.000101123
2017-10-11T11:06:32.561600: step 2051, loss 0.266065, acc 0.921875, learning_rate 0.000101119
2017-10-11T11:06:32.633039: step 2052, loss 0.158369, acc 0.984375, learning_rate 0.000101114
2017-10-11T11:06:32.703530: step 2053, loss 0.176539, acc 0.953125, learning_rate 0.00010111
2017-10-11T11:06:32.772891: step 2054, loss 0.185881, acc 0.953125, learning_rate 0.000101105
2017-10-11T11:06:32.849620: step 2055, loss 0.289357, acc 0.875, learning_rate 0.000101101
2017-10-11T11:06:32.921007: step 2056, loss 0.195348, acc 0.9375, learning_rate 0.000101096
2017-10-11T11:06:32.991794: step 2057, loss 0.247316, acc 0.9375, learning_rate 0.000101092
2017-10-11T11:06:33.052319: step 2058, loss 0.389436, acc 0.843137, learning_rate 0.000101087
2017-10-11T11:06:33.123327: step 2059, loss 0.15902, acc 0.953125, learning_rate 0.000101083
2017-10-11T11:06:33.194903: step 2060, loss 0.145849, acc 0.953125, learning_rate 0.000101078
2017-10-11T11:06:33.266718: step 2061, loss 0.449929, acc 0.859375, learning_rate 0.000101074
2017-10-11T11:06:33.339116: step 2062, loss 0.362073, acc 0.859375, learning_rate 0.00010107
2017-10-11T11:06:33.411141: step 2063, loss 0.169802, acc 0.96875, learning_rate 0.000101065
2017-10-11T11:06:33.482275: step 2064, loss 0.101207, acc 0.984375, learning_rate 0.000101061
2017-10-11T11:06:33.552888: step 2065, loss 0.292752, acc 0.90625, learning_rate 0.000101057
2017-10-11T11:06:33.625592: step 2066, loss 0.25814, acc 0.90625, learning_rate 0.000101052
2017-10-11T11:06:33.696304: step 2067, loss 0.228952, acc 0.953125, learning_rate 0.000101048
2017-10-11T11:06:33.762855: step 2068, loss 0.133904, acc 0.96875, learning_rate 0.000101044
2017-10-11T11:06:33.831583: step 2069, loss 0.219331, acc 0.953125, learning_rate 0.000101039
2017-10-11T11:06:33.910307: step 2070, loss 0.199201, acc 0.9375, learning_rate 0.000101035
2017-10-11T11:06:33.978414: step 2071, loss 0.358055, acc 0.890625, learning_rate 0.000101031
2017-10-11T11:06:34.049723: step 2072, loss 0.224158, acc 0.921875, learning_rate 0.000101027
2017-10-11T11:06:34.120635: step 2073, loss 0.210628, acc 0.9375, learning_rate 0.000101023
2017-10-11T11:06:34.191073: step 2074, loss 0.382084, acc 0.859375, learning_rate 0.000101018
2017-10-11T11:06:34.262342: step 2075, loss 0.148799, acc 0.96875, learning_rate 0.000101014
2017-10-11T11:06:34.331754: step 2076, loss 0.395835, acc 0.859375, learning_rate 0.00010101
2017-10-11T11:06:34.404550: step 2077, loss 0.220188, acc 0.890625, learning_rate 0.000101006
2017-10-11T11:06:34.473256: step 2078, loss 0.265364, acc 0.953125, learning_rate 0.000101002
2017-10-11T11:06:34.543291: step 2079, loss 0.243852, acc 0.90625, learning_rate 0.000100998
2017-10-11T11:06:34.613676: step 2080, loss 0.375555, acc 0.890625, learning_rate 0.000100994

Evaluation:
2017-10-11T11:06:34.786242: step 2080, loss 0.247083, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2080

2017-10-11T11:06:35.313626: step 2081, loss 0.309845, acc 0.890625, learning_rate 0.00010099
2017-10-11T11:06:35.385733: step 2082, loss 0.314746, acc 0.875, learning_rate 0.000100986
2017-10-11T11:06:35.454996: step 2083, loss 0.296612, acc 0.90625, learning_rate 0.000100982
2017-10-11T11:06:35.526935: step 2084, loss 0.108414, acc 0.96875, learning_rate 0.000100978
2017-10-11T11:06:35.596936: step 2085, loss 0.2714, acc 0.921875, learning_rate 0.000100974
2017-10-11T11:06:35.665535: step 2086, loss 0.187049, acc 0.953125, learning_rate 0.00010097
2017-10-11T11:06:35.735226: step 2087, loss 0.30755, acc 0.875, learning_rate 0.000100966
2017-10-11T11:06:35.804619: step 2088, loss 0.152081, acc 0.96875, learning_rate 0.000100962
2017-10-11T11:06:35.876484: step 2089, loss 0.233818, acc 0.9375, learning_rate 0.000100958
2017-10-11T11:06:35.949410: step 2090, loss 0.265527, acc 0.890625, learning_rate 0.000100954
2017-10-11T11:06:36.020054: step 2091, loss 0.273109, acc 0.90625, learning_rate 0.00010095
2017-10-11T11:06:36.091018: step 2092, loss 0.320328, acc 0.890625, learning_rate 0.000100946
2017-10-11T11:06:36.162310: step 2093, loss 0.318221, acc 0.890625, learning_rate 0.000100942
2017-10-11T11:06:36.231290: step 2094, loss 0.269919, acc 0.90625, learning_rate 0.000100938
2017-10-11T11:06:36.305310: step 2095, loss 0.272375, acc 0.890625, learning_rate 0.000100935
2017-10-11T11:06:36.375217: step 2096, loss 0.181089, acc 0.9375, learning_rate 0.000100931
2017-10-11T11:06:36.448090: step 2097, loss 0.164929, acc 0.9375, learning_rate 0.000100927
2017-10-11T11:06:36.516441: step 2098, loss 0.305291, acc 0.90625, learning_rate 0.000100923
2017-10-11T11:06:36.591102: step 2099, loss 0.233343, acc 0.90625, learning_rate 0.000100919
2017-10-11T11:06:36.663104: step 2100, loss 0.260714, acc 0.890625, learning_rate 0.000100916
2017-10-11T11:06:36.734566: step 2101, loss 0.148287, acc 0.953125, learning_rate 0.000100912
2017-10-11T11:06:36.805101: step 2102, loss 0.241565, acc 0.9375, learning_rate 0.000100908
2017-10-11T11:06:36.881297: step 2103, loss 0.210704, acc 0.921875, learning_rate 0.000100904
2017-10-11T11:06:36.951737: step 2104, loss 0.189662, acc 0.953125, learning_rate 0.000100901
2017-10-11T11:06:37.021417: step 2105, loss 0.272413, acc 0.921875, learning_rate 0.000100897
2017-10-11T11:06:37.095306: step 2106, loss 0.14967, acc 0.984375, learning_rate 0.000100893
2017-10-11T11:06:37.168000: step 2107, loss 0.222147, acc 0.9375, learning_rate 0.00010089
2017-10-11T11:06:37.241725: step 2108, loss 0.347543, acc 0.84375, learning_rate 0.000100886
2017-10-11T11:06:37.312130: step 2109, loss 0.167729, acc 0.9375, learning_rate 0.000100883
2017-10-11T11:06:37.385217: step 2110, loss 0.281631, acc 0.890625, learning_rate 0.000100879
2017-10-11T11:06:37.456760: step 2111, loss 0.297246, acc 0.90625, learning_rate 0.000100875
2017-10-11T11:06:37.525501: step 2112, loss 0.243555, acc 0.921875, learning_rate 0.000100872
2017-10-11T11:06:37.597872: step 2113, loss 0.227269, acc 0.875, learning_rate 0.000100868
2017-10-11T11:06:37.669103: step 2114, loss 0.437778, acc 0.921875, learning_rate 0.000100865
2017-10-11T11:06:37.738097: step 2115, loss 0.233941, acc 0.890625, learning_rate 0.000100861
2017-10-11T11:06:37.809005: step 2116, loss 0.120627, acc 0.984375, learning_rate 0.000100858
2017-10-11T11:06:37.883438: step 2117, loss 0.153558, acc 0.953125, learning_rate 0.000100854
2017-10-11T11:06:37.954126: step 2118, loss 0.205784, acc 0.9375, learning_rate 0.000100851
2017-10-11T11:06:38.026404: step 2119, loss 0.258391, acc 0.921875, learning_rate 0.000100847
2017-10-11T11:06:38.099679: step 2120, loss 0.2314, acc 0.921875, learning_rate 0.000100844

Evaluation:
2017-10-11T11:06:38.271151: step 2120, loss 0.247943, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2120

2017-10-11T11:06:38.831503: step 2121, loss 0.132516, acc 0.96875, learning_rate 0.00010084
2017-10-11T11:06:38.915281: step 2122, loss 0.248822, acc 0.90625, learning_rate 0.000100837
2017-10-11T11:06:38.983780: step 2123, loss 0.179821, acc 0.921875, learning_rate 0.000100833
2017-10-11T11:06:39.052084: step 2124, loss 0.131899, acc 0.96875, learning_rate 0.00010083
2017-10-11T11:06:39.123290: step 2125, loss 0.157838, acc 0.953125, learning_rate 0.000100827
2017-10-11T11:06:39.192993: step 2126, loss 0.234557, acc 0.9375, learning_rate 0.000100823
2017-10-11T11:06:39.266184: step 2127, loss 0.241206, acc 0.921875, learning_rate 0.00010082
2017-10-11T11:06:39.336904: step 2128, loss 0.17976, acc 0.9375, learning_rate 0.000100817
2017-10-11T11:06:39.406175: step 2129, loss 0.311759, acc 0.875, learning_rate 0.000100813
2017-10-11T11:06:39.478751: step 2130, loss 0.172782, acc 0.953125, learning_rate 0.00010081
2017-10-11T11:06:39.550539: step 2131, loss 0.238034, acc 0.9375, learning_rate 0.000100807
2017-10-11T11:06:39.622322: step 2132, loss 0.251521, acc 0.9375, learning_rate 0.000100803
2017-10-11T11:06:39.692329: step 2133, loss 0.141862, acc 0.96875, learning_rate 0.0001008
2017-10-11T11:06:39.765026: step 2134, loss 0.212764, acc 0.921875, learning_rate 0.000100797
2017-10-11T11:06:39.836085: step 2135, loss 0.115522, acc 1, learning_rate 0.000100793
2017-10-11T11:06:39.907644: step 2136, loss 0.262342, acc 0.921875, learning_rate 0.00010079
2017-10-11T11:06:39.983917: step 2137, loss 0.283168, acc 0.875, learning_rate 0.000100787
2017-10-11T11:06:40.053515: step 2138, loss 0.200772, acc 0.96875, learning_rate 0.000100784
2017-10-11T11:06:40.126001: step 2139, loss 0.20916, acc 0.953125, learning_rate 0.000100781
2017-10-11T11:06:40.196407: step 2140, loss 0.238418, acc 0.9375, learning_rate 0.000100777
2017-10-11T11:06:40.267996: step 2141, loss 0.170795, acc 0.96875, learning_rate 0.000100774
2017-10-11T11:06:40.340596: step 2142, loss 0.195541, acc 0.9375, learning_rate 0.000100771
2017-10-11T11:06:40.416050: step 2143, loss 0.192667, acc 0.9375, learning_rate 0.000100768
2017-10-11T11:06:40.486076: step 2144, loss 0.200992, acc 0.9375, learning_rate 0.000100765
2017-10-11T11:06:40.556314: step 2145, loss 0.235687, acc 0.9375, learning_rate 0.000100762
2017-10-11T11:06:40.627189: step 2146, loss 0.193113, acc 0.9375, learning_rate 0.000100759
2017-10-11T11:06:40.695180: step 2147, loss 0.174776, acc 0.9375, learning_rate 0.000100755
2017-10-11T11:06:40.765360: step 2148, loss 0.146263, acc 0.984375, learning_rate 0.000100752
2017-10-11T11:06:40.835294: step 2149, loss 0.282604, acc 0.890625, learning_rate 0.000100749
2017-10-11T11:06:40.911663: step 2150, loss 0.251878, acc 0.90625, learning_rate 0.000100746
2017-10-11T11:06:40.983570: step 2151, loss 0.4171, acc 0.875, learning_rate 0.000100743
2017-10-11T11:06:41.057088: step 2152, loss 0.303649, acc 0.921875, learning_rate 0.00010074
2017-10-11T11:06:41.129103: step 2153, loss 0.195203, acc 0.921875, learning_rate 0.000100737
2017-10-11T11:06:41.198177: step 2154, loss 0.314215, acc 0.890625, learning_rate 0.000100734
2017-10-11T11:06:41.269552: step 2155, loss 0.313823, acc 0.890625, learning_rate 0.000100731
2017-10-11T11:06:41.332996: step 2156, loss 0.247238, acc 0.882353, learning_rate 0.000100728
2017-10-11T11:06:41.407180: step 2157, loss 0.180362, acc 0.9375, learning_rate 0.000100725
2017-10-11T11:06:41.480056: step 2158, loss 0.256226, acc 0.9375, learning_rate 0.000100722
2017-10-11T11:06:41.549736: step 2159, loss 0.213211, acc 0.96875, learning_rate 0.000100719
2017-10-11T11:06:41.621127: step 2160, loss 0.278875, acc 0.953125, learning_rate 0.000100716

Evaluation:
2017-10-11T11:06:41.798797: step 2160, loss 0.247574, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2160

2017-10-11T11:06:42.549831: step 2161, loss 0.243289, acc 0.90625, learning_rate 0.000100713
2017-10-11T11:06:42.621716: step 2162, loss 0.253994, acc 0.9375, learning_rate 0.000100711
2017-10-11T11:06:42.695162: step 2163, loss 0.163804, acc 0.953125, learning_rate 0.000100708
2017-10-11T11:06:42.769451: step 2164, loss 0.414885, acc 0.890625, learning_rate 0.000100705
2017-10-11T11:06:42.841201: step 2165, loss 0.208266, acc 0.9375, learning_rate 0.000100702
2017-10-11T11:06:42.911128: step 2166, loss 0.229633, acc 0.921875, learning_rate 0.000100699
2017-10-11T11:06:42.986520: step 2167, loss 0.236079, acc 0.9375, learning_rate 0.000100696
2017-10-11T11:06:43.060519: step 2168, loss 0.171806, acc 0.96875, learning_rate 0.000100693
2017-10-11T11:06:43.132773: step 2169, loss 0.251972, acc 0.9375, learning_rate 0.00010069
2017-10-11T11:06:43.203350: step 2170, loss 0.218521, acc 0.9375, learning_rate 0.000100688
2017-10-11T11:06:43.273245: step 2171, loss 0.268179, acc 0.875, learning_rate 0.000100685
2017-10-11T11:06:43.345324: step 2172, loss 0.304023, acc 0.90625, learning_rate 0.000100682
2017-10-11T11:06:43.415925: step 2173, loss 0.165457, acc 0.984375, learning_rate 0.000100679
2017-10-11T11:06:43.493478: step 2174, loss 0.271161, acc 0.875, learning_rate 0.000100677
2017-10-11T11:06:43.570259: step 2175, loss 0.180456, acc 0.953125, learning_rate 0.000100674
2017-10-11T11:06:43.642894: step 2176, loss 0.186408, acc 0.921875, learning_rate 0.000100671
2017-10-11T11:06:43.715354: step 2177, loss 0.17838, acc 0.96875, learning_rate 0.000100668
2017-10-11T11:06:43.788050: step 2178, loss 0.197999, acc 0.9375, learning_rate 0.000100666
2017-10-11T11:06:43.860140: step 2179, loss 0.255981, acc 0.890625, learning_rate 0.000100663
2017-10-11T11:06:43.931146: step 2180, loss 0.246252, acc 0.921875, learning_rate 0.00010066
2017-10-11T11:06:43.999685: step 2181, loss 0.383808, acc 0.890625, learning_rate 0.000100657
2017-10-11T11:06:44.070558: step 2182, loss 0.276147, acc 0.953125, learning_rate 0.000100655
2017-10-11T11:06:44.141685: step 2183, loss 0.230331, acc 0.90625, learning_rate 0.000100652
2017-10-11T11:06:44.210589: step 2184, loss 0.358917, acc 0.90625, learning_rate 0.000100649
2017-10-11T11:06:44.280201: step 2185, loss 0.101195, acc 0.96875, learning_rate 0.000100647
2017-10-11T11:06:44.351499: step 2186, loss 0.277287, acc 0.9375, learning_rate 0.000100644
2017-10-11T11:06:44.424473: step 2187, loss 0.2231, acc 0.90625, learning_rate 0.000100641
2017-10-11T11:06:44.493897: step 2188, loss 0.436354, acc 0.8125, learning_rate 0.000100639
2017-10-11T11:06:44.563885: step 2189, loss 0.105903, acc 0.96875, learning_rate 0.000100636
2017-10-11T11:06:44.633402: step 2190, loss 0.24927, acc 0.921875, learning_rate 0.000100634
2017-10-11T11:06:44.706229: step 2191, loss 0.194327, acc 0.921875, learning_rate 0.000100631
2017-10-11T11:06:44.779759: step 2192, loss 0.261014, acc 0.90625, learning_rate 0.000100628
2017-10-11T11:06:44.853334: step 2193, loss 0.252737, acc 0.953125, learning_rate 0.000100626
2017-10-11T11:06:44.926104: step 2194, loss 0.198862, acc 0.921875, learning_rate 0.000100623
2017-10-11T11:06:45.002615: step 2195, loss 0.187382, acc 0.953125, learning_rate 0.000100621
2017-10-11T11:06:45.072937: step 2196, loss 0.17838, acc 0.921875, learning_rate 0.000100618
2017-10-11T11:06:45.144708: step 2197, loss 0.138649, acc 0.96875, learning_rate 0.000100616
2017-10-11T11:06:45.215748: step 2198, loss 0.154822, acc 0.953125, learning_rate 0.000100613
2017-10-11T11:06:45.288212: step 2199, loss 0.139044, acc 0.953125, learning_rate 0.000100611
2017-10-11T11:06:45.359224: step 2200, loss 0.252609, acc 0.921875, learning_rate 0.000100608

Evaluation:
2017-10-11T11:06:45.533631: step 2200, loss 0.246933, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2200

2017-10-11T11:06:46.053392: step 2201, loss 0.168744, acc 0.96875, learning_rate 0.000100606
2017-10-11T11:06:46.123013: step 2202, loss 0.328793, acc 0.890625, learning_rate 0.000100603
2017-10-11T11:06:46.199574: step 2203, loss 0.22002, acc 0.921875, learning_rate 0.000100601
2017-10-11T11:06:46.269507: step 2204, loss 0.243471, acc 0.890625, learning_rate 0.000100598
2017-10-11T11:06:46.337462: step 2205, loss 0.15581, acc 0.96875, learning_rate 0.000100596
2017-10-11T11:06:46.407753: step 2206, loss 0.201348, acc 0.90625, learning_rate 0.000100594
2017-10-11T11:06:46.479868: step 2207, loss 0.329496, acc 0.875, learning_rate 0.000100591
2017-10-11T11:06:46.552060: step 2208, loss 0.201807, acc 0.953125, learning_rate 0.000100589
2017-10-11T11:06:46.622785: step 2209, loss 0.259209, acc 0.921875, learning_rate 0.000100586
2017-10-11T11:06:46.692131: step 2210, loss 0.263602, acc 0.890625, learning_rate 0.000100584
2017-10-11T11:06:46.763171: step 2211, loss 0.122316, acc 0.984375, learning_rate 0.000100581
2017-10-11T11:06:46.833503: step 2212, loss 0.202065, acc 0.9375, learning_rate 0.000100579
2017-10-11T11:06:46.906210: step 2213, loss 0.258374, acc 0.9375, learning_rate 0.000100577
2017-10-11T11:06:46.978779: step 2214, loss 0.204434, acc 0.9375, learning_rate 0.000100574
2017-10-11T11:06:47.051853: step 2215, loss 0.215885, acc 0.953125, learning_rate 0.000100572
2017-10-11T11:06:47.123772: step 2216, loss 0.166498, acc 0.96875, learning_rate 0.00010057
2017-10-11T11:06:47.193683: step 2217, loss 0.103887, acc 0.984375, learning_rate 0.000100567
2017-10-11T11:06:47.264245: step 2218, loss 0.310553, acc 0.921875, learning_rate 0.000100565
2017-10-11T11:06:47.335210: step 2219, loss 0.228275, acc 0.953125, learning_rate 0.000100563
2017-10-11T11:06:47.409204: step 2220, loss 0.22198, acc 0.921875, learning_rate 0.00010056
2017-10-11T11:06:47.484785: step 2221, loss 0.174434, acc 0.9375, learning_rate 0.000100558
2017-10-11T11:06:47.557563: step 2222, loss 0.275101, acc 0.875, learning_rate 0.000100556
2017-10-11T11:06:47.628982: step 2223, loss 0.363132, acc 0.875, learning_rate 0.000100554
2017-10-11T11:06:47.698464: step 2224, loss 0.186393, acc 0.953125, learning_rate 0.000100551
2017-10-11T11:06:47.771488: step 2225, loss 0.183001, acc 0.9375, learning_rate 0.000100549
2017-10-11T11:06:47.849086: step 2226, loss 0.295929, acc 0.953125, learning_rate 0.000100547
2017-10-11T11:06:47.921430: step 2227, loss 0.329611, acc 0.890625, learning_rate 0.000100545
2017-10-11T11:06:47.994314: step 2228, loss 0.254142, acc 0.921875, learning_rate 0.000100542
2017-10-11T11:06:48.065306: step 2229, loss 0.230825, acc 0.953125, learning_rate 0.00010054
2017-10-11T11:06:48.139238: step 2230, loss 0.22125, acc 0.90625, learning_rate 0.000100538
2017-10-11T11:06:48.212311: step 2231, loss 0.196425, acc 0.9375, learning_rate 0.000100536
2017-10-11T11:06:48.282859: step 2232, loss 0.197359, acc 0.921875, learning_rate 0.000100534
2017-10-11T11:06:48.353599: step 2233, loss 0.207374, acc 0.953125, learning_rate 0.000100531
2017-10-11T11:06:48.422289: step 2234, loss 0.289461, acc 0.921875, learning_rate 0.000100529
2017-10-11T11:06:48.493071: step 2235, loss 0.205387, acc 0.921875, learning_rate 0.000100527
2017-10-11T11:06:48.565134: step 2236, loss 0.283594, acc 0.90625, learning_rate 0.000100525
2017-10-11T11:06:48.635072: step 2237, loss 0.217818, acc 0.96875, learning_rate 0.000100523
2017-10-11T11:06:48.706943: step 2238, loss 0.299917, acc 0.890625, learning_rate 0.000100521
2017-10-11T11:06:48.777587: step 2239, loss 0.231486, acc 0.921875, learning_rate 0.000100519
2017-10-11T11:06:48.848176: step 2240, loss 0.179128, acc 0.9375, learning_rate 0.000100516

Evaluation:
2017-10-11T11:06:49.038142: step 2240, loss 0.246427, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2240

2017-10-11T11:06:49.613141: step 2241, loss 0.325679, acc 0.890625, learning_rate 0.000100514
2017-10-11T11:06:49.688354: step 2242, loss 0.128471, acc 0.953125, learning_rate 0.000100512
2017-10-11T11:06:49.764560: step 2243, loss 0.225664, acc 0.921875, learning_rate 0.00010051
2017-10-11T11:06:49.835120: step 2244, loss 0.158293, acc 0.953125, learning_rate 0.000100508
2017-10-11T11:06:49.905394: step 2245, loss 0.252791, acc 0.921875, learning_rate 0.000100506
2017-10-11T11:06:49.976013: step 2246, loss 0.274725, acc 0.9375, learning_rate 0.000100504
2017-10-11T11:06:50.049296: step 2247, loss 0.221295, acc 0.953125, learning_rate 0.000100502
2017-10-11T11:06:50.119861: step 2248, loss 0.217973, acc 0.9375, learning_rate 0.0001005
2017-10-11T11:06:50.188313: step 2249, loss 0.205719, acc 0.921875, learning_rate 0.000100498
2017-10-11T11:06:50.258644: step 2250, loss 0.217661, acc 0.9375, learning_rate 0.000100496
2017-10-11T11:06:50.330247: step 2251, loss 0.188981, acc 0.953125, learning_rate 0.000100494
2017-10-11T11:06:50.400074: step 2252, loss 0.139777, acc 0.96875, learning_rate 0.000100492
2017-10-11T11:06:50.470980: step 2253, loss 0.178879, acc 0.96875, learning_rate 0.00010049
2017-10-11T11:06:50.534366: step 2254, loss 0.223497, acc 0.941176, learning_rate 0.000100488
2017-10-11T11:06:50.603042: step 2255, loss 0.314909, acc 0.90625, learning_rate 0.000100486
2017-10-11T11:06:50.673188: step 2256, loss 0.292918, acc 0.921875, learning_rate 0.000100484
2017-10-11T11:06:50.746114: step 2257, loss 0.174347, acc 0.953125, learning_rate 0.000100482
2017-10-11T11:06:50.819770: step 2258, loss 0.179531, acc 0.90625, learning_rate 0.00010048
2017-10-11T11:06:50.900513: step 2259, loss 0.239418, acc 0.90625, learning_rate 0.000100478
2017-10-11T11:06:50.975057: step 2260, loss 0.226243, acc 0.9375, learning_rate 0.000100476
2017-10-11T11:06:51.048157: step 2261, loss 0.131688, acc 0.984375, learning_rate 0.000100474
2017-10-11T11:06:51.119320: step 2262, loss 0.142147, acc 0.953125, learning_rate 0.000100472
2017-10-11T11:06:51.189742: step 2263, loss 0.170192, acc 0.9375, learning_rate 0.00010047
2017-10-11T11:06:51.260132: step 2264, loss 0.2504, acc 0.90625, learning_rate 0.000100468
2017-10-11T11:06:51.331779: step 2265, loss 0.228645, acc 0.921875, learning_rate 0.000100466
2017-10-11T11:06:51.403094: step 2266, loss 0.244388, acc 0.921875, learning_rate 0.000100464
2017-10-11T11:06:51.474078: step 2267, loss 0.283803, acc 0.90625, learning_rate 0.000100462
2017-10-11T11:06:51.543395: step 2268, loss 0.154969, acc 0.984375, learning_rate 0.000100461
2017-10-11T11:06:51.611635: step 2269, loss 0.207298, acc 0.953125, learning_rate 0.000100459
2017-10-11T11:06:51.681086: step 2270, loss 0.18299, acc 0.984375, learning_rate 0.000100457
2017-10-11T11:06:51.753967: step 2271, loss 0.324189, acc 0.859375, learning_rate 0.000100455
2017-10-11T11:06:51.824099: step 2272, loss 0.319476, acc 0.890625, learning_rate 0.000100453
2017-10-11T11:06:51.898064: step 2273, loss 0.22157, acc 0.90625, learning_rate 0.000100451
2017-10-11T11:06:51.968209: step 2274, loss 0.151352, acc 0.953125, learning_rate 0.000100449
2017-10-11T11:06:52.039860: step 2275, loss 0.176359, acc 0.953125, learning_rate 0.000100448
2017-10-11T11:06:52.110021: step 2276, loss 0.301009, acc 0.890625, learning_rate 0.000100446
2017-10-11T11:06:52.180601: step 2277, loss 0.190655, acc 0.921875, learning_rate 0.000100444
2017-10-11T11:06:52.251757: step 2278, loss 0.259204, acc 0.953125, learning_rate 0.000100442
2017-10-11T11:06:52.324116: step 2279, loss 0.262287, acc 0.90625, learning_rate 0.00010044
2017-10-11T11:06:52.394842: step 2280, loss 0.307813, acc 0.90625, learning_rate 0.000100439

Evaluation:
2017-10-11T11:06:52.571365: step 2280, loss 0.245813, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2280

2017-10-11T11:06:53.216316: step 2281, loss 0.300673, acc 0.90625, learning_rate 0.000100437
2017-10-11T11:06:53.290827: step 2282, loss 0.272698, acc 0.90625, learning_rate 0.000100435
2017-10-11T11:06:53.364533: step 2283, loss 0.38765, acc 0.84375, learning_rate 0.000100433
2017-10-11T11:06:53.435024: step 2284, loss 0.12701, acc 0.96875, learning_rate 0.000100431
2017-10-11T11:06:53.504574: step 2285, loss 0.29669, acc 0.890625, learning_rate 0.00010043
2017-10-11T11:06:53.578316: step 2286, loss 0.285338, acc 0.921875, learning_rate 0.000100428
2017-10-11T11:06:53.649154: step 2287, loss 0.237687, acc 0.9375, learning_rate 0.000100426
2017-10-11T11:06:53.721792: step 2288, loss 0.231889, acc 0.9375, learning_rate 0.000100424
2017-10-11T11:06:53.791529: step 2289, loss 0.246232, acc 0.953125, learning_rate 0.000100423
2017-10-11T11:06:53.864979: step 2290, loss 0.288273, acc 0.921875, learning_rate 0.000100421
2017-10-11T11:06:53.939768: step 2291, loss 0.25129, acc 0.9375, learning_rate 0.000100419
2017-10-11T11:06:54.013666: step 2292, loss 0.137498, acc 0.984375, learning_rate 0.000100418
2017-10-11T11:06:54.083614: step 2293, loss 0.210568, acc 0.921875, learning_rate 0.000100416
2017-10-11T11:06:54.153549: step 2294, loss 0.220951, acc 0.9375, learning_rate 0.000100414
2017-10-11T11:06:54.226220: step 2295, loss 0.137598, acc 0.953125, learning_rate 0.000100412
2017-10-11T11:06:54.296969: step 2296, loss 0.196675, acc 0.90625, learning_rate 0.000100411
2017-10-11T11:06:54.366656: step 2297, loss 0.162144, acc 0.953125, learning_rate 0.000100409
2017-10-11T11:06:54.440829: step 2298, loss 0.341861, acc 0.875, learning_rate 0.000100407
2017-10-11T11:06:54.514493: step 2299, loss 0.412163, acc 0.859375, learning_rate 0.000100406
2017-10-11T11:06:54.588786: step 2300, loss 0.256407, acc 0.921875, learning_rate 0.000100404
2017-10-11T11:06:54.661996: step 2301, loss 0.249572, acc 0.90625, learning_rate 0.000100402
2017-10-11T11:06:54.736457: step 2302, loss 0.266911, acc 0.875, learning_rate 0.000100401
2017-10-11T11:06:54.807504: step 2303, loss 0.104955, acc 0.96875, learning_rate 0.000100399
2017-10-11T11:06:54.879099: step 2304, loss 0.261321, acc 0.90625, learning_rate 0.000100398
2017-10-11T11:06:54.952091: step 2305, loss 0.151745, acc 0.9375, learning_rate 0.000100396
2017-10-11T11:06:55.024267: step 2306, loss 0.116936, acc 0.953125, learning_rate 0.000100394
2017-10-11T11:06:55.096705: step 2307, loss 0.278147, acc 0.890625, learning_rate 0.000100393
2017-10-11T11:06:55.170610: step 2308, loss 0.254702, acc 0.953125, learning_rate 0.000100391
2017-10-11T11:06:55.243084: step 2309, loss 0.287207, acc 0.90625, learning_rate 0.000100389
2017-10-11T11:06:55.315973: step 2310, loss 0.279293, acc 0.90625, learning_rate 0.000100388
2017-10-11T11:06:55.385287: step 2311, loss 0.116323, acc 0.984375, learning_rate 0.000100386
2017-10-11T11:06:55.457989: step 2312, loss 0.149223, acc 0.953125, learning_rate 0.000100385
2017-10-11T11:06:55.528519: step 2313, loss 0.211435, acc 0.9375, learning_rate 0.000100383
2017-10-11T11:06:55.600828: step 2314, loss 0.254816, acc 0.921875, learning_rate 0.000100382
2017-10-11T11:06:55.672000: step 2315, loss 0.214482, acc 0.9375, learning_rate 0.00010038
2017-10-11T11:06:55.742261: step 2316, loss 0.38064, acc 0.859375, learning_rate 0.000100378
2017-10-11T11:06:55.815054: step 2317, loss 0.132588, acc 0.96875, learning_rate 0.000100377
2017-10-11T11:06:55.901181: step 2318, loss 0.330168, acc 0.875, learning_rate 0.000100375
2017-10-11T11:06:55.979383: step 2319, loss 0.254673, acc 0.921875, learning_rate 0.000100374
2017-10-11T11:06:56.050919: step 2320, loss 0.238349, acc 0.9375, learning_rate 0.000100372

Evaluation:
2017-10-11T11:06:56.229940: step 2320, loss 0.245406, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2320

2017-10-11T11:06:56.905313: step 2321, loss 0.214251, acc 0.9375, learning_rate 0.000100371
2017-10-11T11:06:56.989806: step 2322, loss 0.166072, acc 0.9375, learning_rate 0.000100369
2017-10-11T11:06:57.082146: step 2323, loss 0.094869, acc 0.984375, learning_rate 0.000100368
2017-10-11T11:06:57.166578: step 2324, loss 0.175473, acc 0.984375, learning_rate 0.000100366
2017-10-11T11:06:57.248901: step 2325, loss 0.339066, acc 0.796875, learning_rate 0.000100365
2017-10-11T11:06:57.326823: step 2326, loss 0.118932, acc 0.96875, learning_rate 0.000100363
2017-10-11T11:06:57.410860: step 2327, loss 0.339269, acc 0.90625, learning_rate 0.000100362
2017-10-11T11:06:57.482702: step 2328, loss 0.220321, acc 0.921875, learning_rate 0.00010036
2017-10-11T11:06:57.567234: step 2329, loss 0.249427, acc 0.890625, learning_rate 0.000100359
2017-10-11T11:06:57.648039: step 2330, loss 0.242608, acc 0.953125, learning_rate 0.000100357
2017-10-11T11:06:57.725756: step 2331, loss 0.189192, acc 0.9375, learning_rate 0.000100356
2017-10-11T11:06:57.797637: step 2332, loss 0.148239, acc 0.921875, learning_rate 0.000100354
2017-10-11T11:06:57.869176: step 2333, loss 0.260346, acc 0.90625, learning_rate 0.000100353
2017-10-11T11:06:57.944439: step 2334, loss 0.331017, acc 0.890625, learning_rate 0.000100352
2017-10-11T11:06:58.014720: step 2335, loss 0.145923, acc 0.953125, learning_rate 0.00010035
2017-10-11T11:06:58.086903: step 2336, loss 0.281003, acc 0.890625, learning_rate 0.000100349
2017-10-11T11:06:58.161323: step 2337, loss 0.298027, acc 0.921875, learning_rate 0.000100347
2017-10-11T11:06:58.231623: step 2338, loss 0.278275, acc 0.9375, learning_rate 0.000100346
2017-10-11T11:06:58.305055: step 2339, loss 0.267045, acc 0.9375, learning_rate 0.000100344
2017-10-11T11:06:58.376083: step 2340, loss 0.216445, acc 0.96875, learning_rate 0.000100343
2017-10-11T11:06:58.449967: step 2341, loss 0.186957, acc 0.9375, learning_rate 0.000100342
2017-10-11T11:06:58.521903: step 2342, loss 0.233759, acc 0.90625, learning_rate 0.00010034
2017-10-11T11:06:58.594036: step 2343, loss 0.189741, acc 0.921875, learning_rate 0.000100339
2017-10-11T11:06:58.668226: step 2344, loss 0.198452, acc 0.953125, learning_rate 0.000100338
2017-10-11T11:06:58.739984: step 2345, loss 0.222107, acc 0.921875, learning_rate 0.000100336
2017-10-11T11:06:58.812488: step 2346, loss 0.390206, acc 0.859375, learning_rate 0.000100335
2017-10-11T11:06:58.885609: step 2347, loss 0.246927, acc 0.953125, learning_rate 0.000100333
2017-10-11T11:06:58.963298: step 2348, loss 0.184452, acc 0.953125, learning_rate 0.000100332
2017-10-11T11:06:59.035556: step 2349, loss 0.333718, acc 0.84375, learning_rate 0.000100331
2017-10-11T11:06:59.108911: step 2350, loss 0.168, acc 0.953125, learning_rate 0.000100329
2017-10-11T11:06:59.179413: step 2351, loss 0.351311, acc 0.875, learning_rate 0.000100328
2017-10-11T11:06:59.240663: step 2352, loss 0.181792, acc 0.921569, learning_rate 0.000100327
2017-10-11T11:06:59.312778: step 2353, loss 0.136219, acc 0.9375, learning_rate 0.000100325
2017-10-11T11:06:59.383787: step 2354, loss 0.156879, acc 0.96875, learning_rate 0.000100324
2017-10-11T11:06:59.454005: step 2355, loss 0.146133, acc 0.984375, learning_rate 0.000100323
2017-10-11T11:06:59.524593: step 2356, loss 0.273962, acc 0.9375, learning_rate 0.000100321
2017-10-11T11:06:59.595797: step 2357, loss 0.167702, acc 0.9375, learning_rate 0.00010032
2017-10-11T11:06:59.669304: step 2358, loss 0.361417, acc 0.890625, learning_rate 0.000100319
2017-10-11T11:06:59.738396: step 2359, loss 0.284643, acc 0.921875, learning_rate 0.000100317
2017-10-11T11:06:59.808906: step 2360, loss 0.0783747, acc 0.984375, learning_rate 0.000100316

Evaluation:
2017-10-11T11:06:59.997729: step 2360, loss 0.24562, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2360

2017-10-11T11:07:00.566306: step 2361, loss 0.26465, acc 0.9375, learning_rate 0.000100315
2017-10-11T11:07:00.634203: step 2362, loss 0.327602, acc 0.875, learning_rate 0.000100314
2017-10-11T11:07:00.705524: step 2363, loss 0.299234, acc 0.90625, learning_rate 0.000100312
2017-10-11T11:07:00.775580: step 2364, loss 0.557098, acc 0.84375, learning_rate 0.000100311
2017-10-11T11:07:00.850351: step 2365, loss 0.183316, acc 0.953125, learning_rate 0.00010031
2017-10-11T11:07:00.921277: step 2366, loss 0.25163, acc 0.953125, learning_rate 0.000100308
2017-10-11T11:07:00.991337: step 2367, loss 0.291275, acc 0.90625, learning_rate 0.000100307
2017-10-11T11:07:01.060586: step 2368, loss 0.234769, acc 0.9375, learning_rate 0.000100306
2017-10-11T11:07:01.133902: step 2369, loss 0.111411, acc 0.984375, learning_rate 0.000100305
2017-10-11T11:07:01.206672: step 2370, loss 0.216237, acc 0.921875, learning_rate 0.000100303
2017-10-11T11:07:01.281136: step 2371, loss 0.278892, acc 0.921875, learning_rate 0.000100302
2017-10-11T11:07:01.356840: step 2372, loss 0.0812912, acc 0.984375, learning_rate 0.000100301
2017-10-11T11:07:01.429090: step 2373, loss 0.356591, acc 0.921875, learning_rate 0.0001003
2017-10-11T11:07:01.501020: step 2374, loss 0.138191, acc 0.953125, learning_rate 0.000100299
2017-10-11T11:07:01.572843: step 2375, loss 0.377525, acc 0.859375, learning_rate 0.000100297
2017-10-11T11:07:01.643683: step 2376, loss 0.162995, acc 0.953125, learning_rate 0.000100296
2017-10-11T11:07:01.711666: step 2377, loss 0.164063, acc 0.953125, learning_rate 0.000100295
2017-10-11T11:07:01.786038: step 2378, loss 0.245605, acc 0.921875, learning_rate 0.000100294
2017-10-11T11:07:01.862830: step 2379, loss 0.195598, acc 0.953125, learning_rate 0.000100292
2017-10-11T11:07:01.930347: step 2380, loss 0.286914, acc 0.890625, learning_rate 0.000100291
2017-10-11T11:07:01.998453: step 2381, loss 0.178146, acc 0.96875, learning_rate 0.00010029
2017-10-11T11:07:02.069115: step 2382, loss 0.171343, acc 0.953125, learning_rate 0.000100289
2017-10-11T11:07:02.141648: step 2383, loss 0.241958, acc 0.9375, learning_rate 0.000100288
2017-10-11T11:07:02.216769: step 2384, loss 0.202748, acc 0.9375, learning_rate 0.000100287
2017-10-11T11:07:02.289949: step 2385, loss 0.175141, acc 0.96875, learning_rate 0.000100285
2017-10-11T11:07:02.362188: step 2386, loss 0.151228, acc 0.9375, learning_rate 0.000100284
2017-10-11T11:07:02.434748: step 2387, loss 0.255466, acc 0.90625, learning_rate 0.000100283
2017-10-11T11:07:02.508436: step 2388, loss 0.188592, acc 0.921875, learning_rate 0.000100282
2017-10-11T11:07:02.578615: step 2389, loss 0.243154, acc 0.921875, learning_rate 0.000100281
2017-10-11T11:07:02.646602: step 2390, loss 0.376473, acc 0.890625, learning_rate 0.00010028
2017-10-11T11:07:02.715663: step 2391, loss 0.16263, acc 0.953125, learning_rate 0.000100278
2017-10-11T11:07:02.785279: step 2392, loss 0.190247, acc 0.9375, learning_rate 0.000100277
2017-10-11T11:07:02.858898: step 2393, loss 0.177255, acc 0.9375, learning_rate 0.000100276
2017-10-11T11:07:02.931041: step 2394, loss 0.220161, acc 0.953125, learning_rate 0.000100275
2017-10-11T11:07:03.003883: step 2395, loss 0.191485, acc 0.9375, learning_rate 0.000100274
2017-10-11T11:07:03.076984: step 2396, loss 0.215336, acc 0.921875, learning_rate 0.000100273
2017-10-11T11:07:03.147363: step 2397, loss 0.248168, acc 0.9375, learning_rate 0.000100272
2017-10-11T11:07:03.217467: step 2398, loss 0.152946, acc 0.984375, learning_rate 0.000100271
2017-10-11T11:07:03.286498: step 2399, loss 0.235303, acc 0.921875, learning_rate 0.00010027
2017-10-11T11:07:03.355932: step 2400, loss 0.221316, acc 0.90625, learning_rate 0.000100268

Evaluation:
2017-10-11T11:07:03.528772: step 2400, loss 0.244905, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2400

2017-10-11T11:07:04.168449: step 2401, loss 0.259962, acc 0.90625, learning_rate 0.000100267
2017-10-11T11:07:04.240822: step 2402, loss 0.1584, acc 0.984375, learning_rate 0.000100266
2017-10-11T11:07:04.312810: step 2403, loss 0.269301, acc 0.90625, learning_rate 0.000100265
2017-10-11T11:07:04.388917: step 2404, loss 0.218246, acc 0.953125, learning_rate 0.000100264
2017-10-11T11:07:04.457403: step 2405, loss 0.16818, acc 0.96875, learning_rate 0.000100263
2017-10-11T11:07:04.527990: step 2406, loss 0.264412, acc 0.90625, learning_rate 0.000100262
2017-10-11T11:07:04.599080: step 2407, loss 0.203511, acc 0.921875, learning_rate 0.000100261
2017-10-11T11:07:04.673670: step 2408, loss 0.253113, acc 0.90625, learning_rate 0.00010026
2017-10-11T11:07:04.744117: step 2409, loss 0.207791, acc 0.890625, learning_rate 0.000100259
2017-10-11T11:07:04.818062: step 2410, loss 0.143727, acc 1, learning_rate 0.000100258
2017-10-11T11:07:04.890265: step 2411, loss 0.330407, acc 0.90625, learning_rate 0.000100257
2017-10-11T11:07:04.960390: step 2412, loss 0.214392, acc 0.9375, learning_rate 0.000100256
2017-10-11T11:07:05.033411: step 2413, loss 0.268937, acc 0.953125, learning_rate 0.000100255
2017-10-11T11:07:05.105768: step 2414, loss 0.17909, acc 0.96875, learning_rate 0.000100253
2017-10-11T11:07:05.177768: step 2415, loss 0.189699, acc 0.953125, learning_rate 0.000100252
2017-10-11T11:07:05.247675: step 2416, loss 0.336035, acc 0.84375, learning_rate 0.000100251
2017-10-11T11:07:05.318867: step 2417, loss 0.17737, acc 0.9375, learning_rate 0.00010025
2017-10-11T11:07:05.387855: step 2418, loss 0.123036, acc 0.953125, learning_rate 0.000100249
2017-10-11T11:07:05.459709: step 2419, loss 0.29564, acc 0.875, learning_rate 0.000100248
2017-10-11T11:07:05.544317: step 2420, loss 0.198216, acc 0.953125, learning_rate 0.000100247
2017-10-11T11:07:05.623706: step 2421, loss 0.100597, acc 0.96875, learning_rate 0.000100246
2017-10-11T11:07:05.700762: step 2422, loss 0.273796, acc 0.921875, learning_rate 0.000100245
2017-10-11T11:07:05.770991: step 2423, loss 0.400312, acc 0.84375, learning_rate 0.000100244
2017-10-11T11:07:05.840327: step 2424, loss 0.172405, acc 0.96875, learning_rate 0.000100243
2017-10-11T11:07:05.910148: step 2425, loss 0.275613, acc 0.875, learning_rate 0.000100242
2017-10-11T11:07:05.987617: step 2426, loss 0.124712, acc 0.984375, learning_rate 0.000100241
2017-10-11T11:07:06.062931: step 2427, loss 0.226356, acc 0.921875, learning_rate 0.00010024
2017-10-11T11:07:06.139478: step 2428, loss 0.192336, acc 0.9375, learning_rate 0.000100239
2017-10-11T11:07:06.210581: step 2429, loss 0.180258, acc 0.953125, learning_rate 0.000100238
2017-10-11T11:07:06.280038: step 2430, loss 0.138148, acc 0.96875, learning_rate 0.000100237
2017-10-11T11:07:06.352237: step 2431, loss 0.213069, acc 0.96875, learning_rate 0.000100236
2017-10-11T11:07:06.425644: step 2432, loss 0.0989553, acc 0.984375, learning_rate 0.000100235
2017-10-11T11:07:06.495101: step 2433, loss 0.170464, acc 0.96875, learning_rate 0.000100235
2017-10-11T11:07:06.566605: step 2434, loss 0.165273, acc 0.953125, learning_rate 0.000100234
2017-10-11T11:07:06.637476: step 2435, loss 0.221598, acc 0.953125, learning_rate 0.000100233
2017-10-11T11:07:06.710098: step 2436, loss 0.090518, acc 0.984375, learning_rate 0.000100232
2017-10-11T11:07:06.780894: step 2437, loss 0.203997, acc 0.9375, learning_rate 0.000100231
2017-10-11T11:07:06.854508: step 2438, loss 0.268232, acc 0.9375, learning_rate 0.00010023
2017-10-11T11:07:06.927682: step 2439, loss 0.221702, acc 0.890625, learning_rate 0.000100229
2017-10-11T11:07:07.001965: step 2440, loss 0.256961, acc 0.890625, learning_rate 0.000100228

Evaluation:
2017-10-11T11:07:07.179756: step 2440, loss 0.244465, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2440

2017-10-11T11:07:07.821967: step 2441, loss 0.197133, acc 0.921875, learning_rate 0.000100227
2017-10-11T11:07:07.899144: step 2442, loss 0.36845, acc 0.859375, learning_rate 0.000100226
2017-10-11T11:07:07.971129: step 2443, loss 0.220768, acc 0.921875, learning_rate 0.000100225
2017-10-11T11:07:08.041464: step 2444, loss 0.282321, acc 0.921875, learning_rate 0.000100224
2017-10-11T11:07:08.112463: step 2445, loss 0.224461, acc 0.921875, learning_rate 0.000100223
2017-10-11T11:07:08.182578: step 2446, loss 0.300531, acc 0.90625, learning_rate 0.000100222
2017-10-11T11:07:08.251867: step 2447, loss 0.313532, acc 0.90625, learning_rate 0.000100221
2017-10-11T11:07:08.319241: step 2448, loss 0.203931, acc 0.953125, learning_rate 0.000100221
2017-10-11T11:07:08.392027: step 2449, loss 0.300358, acc 0.921875, learning_rate 0.00010022
2017-10-11T11:07:08.454301: step 2450, loss 0.282606, acc 0.921569, learning_rate 0.000100219
2017-10-11T11:07:08.531741: step 2451, loss 0.205757, acc 0.953125, learning_rate 0.000100218
2017-10-11T11:07:08.605950: step 2452, loss 0.214776, acc 0.953125, learning_rate 0.000100217
2017-10-11T11:07:08.679631: step 2453, loss 0.242781, acc 0.921875, learning_rate 0.000100216
2017-10-11T11:07:08.752588: step 2454, loss 0.211459, acc 0.953125, learning_rate 0.000100215
2017-10-11T11:07:08.823502: step 2455, loss 0.189544, acc 0.921875, learning_rate 0.000100214
2017-10-11T11:07:08.900325: step 2456, loss 0.183077, acc 0.953125, learning_rate 0.000100213
2017-10-11T11:07:08.976586: step 2457, loss 0.328413, acc 0.890625, learning_rate 0.000100213
2017-10-11T11:07:09.056219: step 2458, loss 0.182067, acc 0.921875, learning_rate 0.000100212
2017-10-11T11:07:09.131969: step 2459, loss 0.176354, acc 0.96875, learning_rate 0.000100211
2017-10-11T11:07:09.205876: step 2460, loss 0.176184, acc 0.953125, learning_rate 0.00010021
2017-10-11T11:07:09.277562: step 2461, loss 0.252696, acc 0.953125, learning_rate 0.000100209
2017-10-11T11:07:09.351810: step 2462, loss 0.353603, acc 0.890625, learning_rate 0.000100208
2017-10-11T11:07:09.424477: step 2463, loss 0.150254, acc 0.984375, learning_rate 0.000100207
2017-10-11T11:07:09.497614: step 2464, loss 0.239414, acc 0.90625, learning_rate 0.000100207
2017-10-11T11:07:09.568329: step 2465, loss 0.396775, acc 0.875, learning_rate 0.000100206
2017-10-11T11:07:09.638250: step 2466, loss 0.227228, acc 0.90625, learning_rate 0.000100205
2017-10-11T11:07:09.706538: step 2467, loss 0.2074, acc 0.953125, learning_rate 0.000100204
2017-10-11T11:07:09.781955: step 2468, loss 0.173343, acc 0.96875, learning_rate 0.000100203
2017-10-11T11:07:09.851052: step 2469, loss 0.216812, acc 0.9375, learning_rate 0.000100202
2017-10-11T11:07:09.930380: step 2470, loss 0.32066, acc 0.890625, learning_rate 0.000100202
2017-10-11T11:07:10.008752: step 2471, loss 0.19131, acc 0.9375, learning_rate 0.000100201
2017-10-11T11:07:10.079940: step 2472, loss 0.25582, acc 0.9375, learning_rate 0.0001002
2017-10-11T11:07:10.150834: step 2473, loss 0.231516, acc 0.9375, learning_rate 0.000100199
2017-10-11T11:07:10.222153: step 2474, loss 0.232761, acc 0.921875, learning_rate 0.000100198
2017-10-11T11:07:10.291831: step 2475, loss 0.234228, acc 0.9375, learning_rate 0.000100198
2017-10-11T11:07:10.365854: step 2476, loss 0.154613, acc 0.96875, learning_rate 0.000100197
2017-10-11T11:07:10.436183: step 2477, loss 0.319192, acc 0.90625, learning_rate 0.000100196
2017-10-11T11:07:10.509394: step 2478, loss 0.191276, acc 0.9375, learning_rate 0.000100195
2017-10-11T11:07:10.581892: step 2479, loss 0.197723, acc 0.9375, learning_rate 0.000100194
2017-10-11T11:07:10.655385: step 2480, loss 0.135486, acc 0.953125, learning_rate 0.000100194

Evaluation:
2017-10-11T11:07:10.829497: step 2480, loss 0.243727, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2480

2017-10-11T11:07:11.333725: step 2481, loss 0.402504, acc 0.859375, learning_rate 0.000100193
2017-10-11T11:07:11.410393: step 2482, loss 0.315144, acc 0.890625, learning_rate 0.000100192
2017-10-11T11:07:11.483324: step 2483, loss 0.260362, acc 0.90625, learning_rate 0.000100191
2017-10-11T11:07:11.552021: step 2484, loss 0.148702, acc 0.96875, learning_rate 0.00010019
2017-10-11T11:07:11.622275: step 2485, loss 0.315512, acc 0.921875, learning_rate 0.00010019
2017-10-11T11:07:11.694957: step 2486, loss 0.373181, acc 0.875, learning_rate 0.000100189
2017-10-11T11:07:11.766600: step 2487, loss 0.19183, acc 0.9375, learning_rate 0.000100188
2017-10-11T11:07:11.838509: step 2488, loss 0.219058, acc 0.9375, learning_rate 0.000100187
2017-10-11T11:07:11.910601: step 2489, loss 0.250523, acc 0.953125, learning_rate 0.000100187
2017-10-11T11:07:11.979474: step 2490, loss 0.231934, acc 0.953125, learning_rate 0.000100186
2017-10-11T11:07:12.052180: step 2491, loss 0.223578, acc 0.9375, learning_rate 0.000100185
2017-10-11T11:07:12.123025: step 2492, loss 0.21368, acc 0.890625, learning_rate 0.000100184
2017-10-11T11:07:12.192434: step 2493, loss 0.210671, acc 0.921875, learning_rate 0.000100183
2017-10-11T11:07:12.262281: step 2494, loss 0.316542, acc 0.875, learning_rate 0.000100183
2017-10-11T11:07:12.332993: step 2495, loss 0.264046, acc 0.859375, learning_rate 0.000100182
2017-10-11T11:07:12.404080: step 2496, loss 0.189915, acc 0.96875, learning_rate 0.000100181
2017-10-11T11:07:12.472861: step 2497, loss 0.257383, acc 0.90625, learning_rate 0.000100181
2017-10-11T11:07:12.542771: step 2498, loss 0.329344, acc 0.921875, learning_rate 0.00010018
2017-10-11T11:07:12.613244: step 2499, loss 0.290349, acc 0.90625, learning_rate 0.000100179
2017-10-11T11:07:12.686340: step 2500, loss 0.333125, acc 0.890625, learning_rate 0.000100178
2017-10-11T11:07:12.759423: step 2501, loss 0.376492, acc 0.875, learning_rate 0.000100178
2017-10-11T11:07:12.829458: step 2502, loss 0.22635, acc 0.921875, learning_rate 0.000100177
2017-10-11T11:07:12.904951: step 2503, loss 0.273662, acc 0.90625, learning_rate 0.000100176
2017-10-11T11:07:12.976867: step 2504, loss 0.202695, acc 0.90625, learning_rate 0.000100175
2017-10-11T11:07:13.048688: step 2505, loss 0.197021, acc 0.9375, learning_rate 0.000100175
2017-10-11T11:07:13.118793: step 2506, loss 0.182779, acc 0.953125, learning_rate 0.000100174
2017-10-11T11:07:13.192089: step 2507, loss 0.179205, acc 0.96875, learning_rate 0.000100173
2017-10-11T11:07:13.264005: step 2508, loss 0.227101, acc 0.9375, learning_rate 0.000100173
2017-10-11T11:07:13.334465: step 2509, loss 0.244607, acc 0.90625, learning_rate 0.000100172
2017-10-11T11:07:13.402649: step 2510, loss 0.128944, acc 0.96875, learning_rate 0.000100171
2017-10-11T11:07:13.474303: step 2511, loss 0.117822, acc 0.96875, learning_rate 0.00010017
2017-10-11T11:07:13.545282: step 2512, loss 0.258797, acc 0.90625, learning_rate 0.00010017
2017-10-11T11:07:13.615109: step 2513, loss 0.147535, acc 0.96875, learning_rate 0.000100169
2017-10-11T11:07:13.685447: step 2514, loss 0.171845, acc 0.9375, learning_rate 0.000100168
2017-10-11T11:07:13.755785: step 2515, loss 0.184062, acc 0.953125, learning_rate 0.000100168
2017-10-11T11:07:13.826121: step 2516, loss 0.160972, acc 0.984375, learning_rate 0.000100167
2017-10-11T11:07:13.902315: step 2517, loss 0.345004, acc 0.84375, learning_rate 0.000100166
2017-10-11T11:07:13.975690: step 2518, loss 0.163015, acc 0.953125, learning_rate 0.000100166
2017-10-11T11:07:14.051688: step 2519, loss 0.199048, acc 0.953125, learning_rate 0.000100165
2017-10-11T11:07:14.125683: step 2520, loss 0.175898, acc 0.953125, learning_rate 0.000100164

Evaluation:
2017-10-11T11:07:14.305453: step 2520, loss 0.244183, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2520

2017-10-11T11:07:14.887733: step 2521, loss 0.26152, acc 0.921875, learning_rate 0.000100164
2017-10-11T11:07:14.966119: step 2522, loss 0.277386, acc 0.890625, learning_rate 0.000100163
2017-10-11T11:07:15.034028: step 2523, loss 0.185179, acc 0.9375, learning_rate 0.000100162
2017-10-11T11:07:15.106204: step 2524, loss 0.192651, acc 0.9375, learning_rate 0.000100162
2017-10-11T11:07:15.174576: step 2525, loss 0.342485, acc 0.90625, learning_rate 0.000100161
2017-10-11T11:07:15.244972: step 2526, loss 0.16976, acc 0.921875, learning_rate 0.00010016
2017-10-11T11:07:15.319564: step 2527, loss 0.200045, acc 0.921875, learning_rate 0.00010016
2017-10-11T11:07:15.391288: step 2528, loss 0.256893, acc 0.90625, learning_rate 0.000100159
2017-10-11T11:07:15.462050: step 2529, loss 0.287362, acc 0.890625, learning_rate 0.000100158
2017-10-11T11:07:15.533366: step 2530, loss 0.230082, acc 0.921875, learning_rate 0.000100158
2017-10-11T11:07:15.606558: step 2531, loss 0.298504, acc 0.875, learning_rate 0.000100157
2017-10-11T11:07:15.681277: step 2532, loss 0.179293, acc 0.921875, learning_rate 0.000100156
2017-10-11T11:07:15.751935: step 2533, loss 0.183004, acc 0.9375, learning_rate 0.000100156
2017-10-11T11:07:15.823090: step 2534, loss 0.169533, acc 0.96875, learning_rate 0.000100155
2017-10-11T11:07:15.896270: step 2535, loss 0.143238, acc 0.96875, learning_rate 0.000100155
2017-10-11T11:07:15.963448: step 2536, loss 0.214271, acc 0.9375, learning_rate 0.000100154
2017-10-11T11:07:16.039132: step 2537, loss 0.206626, acc 0.953125, learning_rate 0.000100153
2017-10-11T11:07:16.111067: step 2538, loss 0.136283, acc 0.96875, learning_rate 0.000100153
2017-10-11T11:07:16.179721: step 2539, loss 0.125414, acc 0.953125, learning_rate 0.000100152
2017-10-11T11:07:16.250620: step 2540, loss 0.24151, acc 0.953125, learning_rate 0.000100151
2017-10-11T11:07:16.322112: step 2541, loss 0.345244, acc 0.890625, learning_rate 0.000100151
2017-10-11T11:07:16.398547: step 2542, loss 0.199377, acc 0.9375, learning_rate 0.00010015
2017-10-11T11:07:16.472900: step 2543, loss 0.239951, acc 0.921875, learning_rate 0.00010015
2017-10-11T11:07:16.544290: step 2544, loss 0.16933, acc 0.9375, learning_rate 0.000100149
2017-10-11T11:07:16.619780: step 2545, loss 0.172075, acc 0.921875, learning_rate 0.000100148
2017-10-11T11:07:16.692229: step 2546, loss 0.27277, acc 0.9375, learning_rate 0.000100148
2017-10-11T11:07:16.765586: step 2547, loss 0.178699, acc 0.9375, learning_rate 0.000100147
2017-10-11T11:07:16.829611: step 2548, loss 0.126375, acc 0.980392, learning_rate 0.000100147
2017-10-11T11:07:16.902635: step 2549, loss 0.268121, acc 0.890625, learning_rate 0.000100146
2017-10-11T11:07:16.973287: step 2550, loss 0.277686, acc 0.890625, learning_rate 0.000100145
2017-10-11T11:07:17.044934: step 2551, loss 0.16068, acc 0.9375, learning_rate 0.000100145
2017-10-11T11:07:17.117864: step 2552, loss 0.208866, acc 0.953125, learning_rate 0.000100144
2017-10-11T11:07:17.185887: step 2553, loss 0.205546, acc 0.921875, learning_rate 0.000100144
2017-10-11T11:07:17.254648: step 2554, loss 0.251122, acc 0.890625, learning_rate 0.000100143
2017-10-11T11:07:17.325341: step 2555, loss 0.179524, acc 0.9375, learning_rate 0.000100142
2017-10-11T11:07:17.397042: step 2556, loss 0.300886, acc 0.9375, learning_rate 0.000100142
2017-10-11T11:07:17.466788: step 2557, loss 0.232082, acc 0.90625, learning_rate 0.000100141
2017-10-11T11:07:17.539375: step 2558, loss 0.217532, acc 0.953125, learning_rate 0.000100141
2017-10-11T11:07:17.612018: step 2559, loss 0.205183, acc 0.9375, learning_rate 0.00010014
2017-10-11T11:07:17.684044: step 2560, loss 0.265611, acc 0.921875, learning_rate 0.00010014

Evaluation:
2017-10-11T11:07:17.857083: step 2560, loss 0.244651, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2560

2017-10-11T11:07:18.496133: step 2561, loss 0.237667, acc 0.9375, learning_rate 0.000100139
2017-10-11T11:07:18.571179: step 2562, loss 0.209225, acc 0.90625, learning_rate 0.000100138
2017-10-11T11:07:18.643928: step 2563, loss 0.244403, acc 0.875, learning_rate 0.000100138
2017-10-11T11:07:18.719166: step 2564, loss 0.239406, acc 0.921875, learning_rate 0.000100137
2017-10-11T11:07:18.793440: step 2565, loss 0.146574, acc 0.984375, learning_rate 0.000100137
2017-10-11T11:07:18.877122: step 2566, loss 0.350997, acc 0.890625, learning_rate 0.000100136
2017-10-11T11:07:18.947785: step 2567, loss 0.279752, acc 0.9375, learning_rate 0.000100136
2017-10-11T11:07:19.020696: step 2568, loss 0.203449, acc 0.9375, learning_rate 0.000100135
2017-10-11T11:07:19.098346: step 2569, loss 0.238699, acc 0.890625, learning_rate 0.000100134
2017-10-11T11:07:19.169459: step 2570, loss 0.242946, acc 0.96875, learning_rate 0.000100134
2017-10-11T11:07:19.242653: step 2571, loss 0.126163, acc 0.953125, learning_rate 0.000100133
2017-10-11T11:07:19.320688: step 2572, loss 0.308742, acc 0.890625, learning_rate 0.000100133
2017-10-11T11:07:19.392761: step 2573, loss 0.222833, acc 0.921875, learning_rate 0.000100132
2017-10-11T11:07:19.463855: step 2574, loss 0.196138, acc 0.9375, learning_rate 0.000100132
2017-10-11T11:07:19.533853: step 2575, loss 0.113015, acc 0.984375, learning_rate 0.000100131
2017-10-11T11:07:19.605923: step 2576, loss 0.294425, acc 0.921875, learning_rate 0.000100131
2017-10-11T11:07:19.675182: step 2577, loss 0.268306, acc 0.890625, learning_rate 0.00010013
2017-10-11T11:07:19.747287: step 2578, loss 0.306972, acc 0.90625, learning_rate 0.00010013
2017-10-11T11:07:19.818016: step 2579, loss 0.210792, acc 0.9375, learning_rate 0.000100129
2017-10-11T11:07:19.894170: step 2580, loss 0.167642, acc 0.9375, learning_rate 0.000100129
2017-10-11T11:07:19.965528: step 2581, loss 0.349777, acc 0.828125, learning_rate 0.000100128
2017-10-11T11:07:20.038687: step 2582, loss 0.218762, acc 0.90625, learning_rate 0.000100128
2017-10-11T11:07:20.112757: step 2583, loss 0.539929, acc 0.828125, learning_rate 0.000100127
2017-10-11T11:07:20.184487: step 2584, loss 0.276685, acc 0.953125, learning_rate 0.000100126
2017-10-11T11:07:20.254395: step 2585, loss 0.181586, acc 0.921875, learning_rate 0.000100126
2017-10-11T11:07:20.325776: step 2586, loss 0.112513, acc 0.96875, learning_rate 0.000100125
2017-10-11T11:07:20.397580: step 2587, loss 0.199783, acc 0.953125, learning_rate 0.000100125
2017-10-11T11:07:20.467796: step 2588, loss 0.306812, acc 0.875, learning_rate 0.000100124
2017-10-11T11:07:20.540979: step 2589, loss 0.269442, acc 0.890625, learning_rate 0.000100124
2017-10-11T11:07:20.616806: step 2590, loss 0.189411, acc 0.953125, learning_rate 0.000100123
2017-10-11T11:07:20.694162: step 2591, loss 0.28155, acc 0.921875, learning_rate 0.000100123
2017-10-11T11:07:20.765628: step 2592, loss 0.214785, acc 0.9375, learning_rate 0.000100122
2017-10-11T11:07:20.836444: step 2593, loss 0.191134, acc 0.921875, learning_rate 0.000100122
2017-10-11T11:07:20.911970: step 2594, loss 0.177175, acc 0.9375, learning_rate 0.000100121
2017-10-11T11:07:20.986016: step 2595, loss 0.216849, acc 0.921875, learning_rate 0.000100121
2017-10-11T11:07:21.059451: step 2596, loss 0.204027, acc 0.921875, learning_rate 0.00010012
2017-10-11T11:07:21.128731: step 2597, loss 0.189656, acc 0.953125, learning_rate 0.00010012
2017-10-11T11:07:21.197618: step 2598, loss 0.231696, acc 0.953125, learning_rate 0.000100119
2017-10-11T11:07:21.266953: step 2599, loss 0.152763, acc 0.953125, learning_rate 0.000100119
2017-10-11T11:07:21.338565: step 2600, loss 0.13894, acc 0.96875, learning_rate 0.000100118

Evaluation:
2017-10-11T11:07:21.511253: step 2600, loss 0.244435, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2600

2017-10-11T11:07:22.281979: step 2601, loss 0.213263, acc 0.921875, learning_rate 0.000100118
2017-10-11T11:07:22.360988: step 2602, loss 0.121678, acc 0.953125, learning_rate 0.000100117
2017-10-11T11:07:22.433208: step 2603, loss 0.18454, acc 0.9375, learning_rate 0.000100117
2017-10-11T11:07:22.505799: step 2604, loss 0.305181, acc 0.90625, learning_rate 0.000100117
2017-10-11T11:07:22.579050: step 2605, loss 0.218669, acc 0.90625, learning_rate 0.000100116
2017-10-11T11:07:22.651458: step 2606, loss 0.176959, acc 0.953125, learning_rate 0.000100116
2017-10-11T11:07:22.724030: step 2607, loss 0.2912, acc 0.90625, learning_rate 0.000100115
2017-10-11T11:07:22.794189: step 2608, loss 0.323323, acc 0.890625, learning_rate 0.000100115
2017-10-11T11:07:22.865724: step 2609, loss 0.349066, acc 0.90625, learning_rate 0.000100114
2017-10-11T11:07:22.935520: step 2610, loss 0.209503, acc 0.953125, learning_rate 0.000100114
2017-10-11T11:07:23.007720: step 2611, loss 0.256694, acc 0.9375, learning_rate 0.000100113
2017-10-11T11:07:23.078800: step 2612, loss 0.0877209, acc 0.984375, learning_rate 0.000100113
2017-10-11T11:07:23.150496: step 2613, loss 0.292142, acc 0.90625, learning_rate 0.000100112
2017-10-11T11:07:23.218878: step 2614, loss 0.390182, acc 0.890625, learning_rate 0.000100112
2017-10-11T11:07:23.292158: step 2615, loss 0.260442, acc 0.90625, learning_rate 0.000100111
2017-10-11T11:07:23.363218: step 2616, loss 0.298368, acc 0.90625, learning_rate 0.000100111
2017-10-11T11:07:23.436139: step 2617, loss 0.184742, acc 0.953125, learning_rate 0.000100111
2017-10-11T11:07:23.505701: step 2618, loss 0.107003, acc 0.984375, learning_rate 0.00010011
2017-10-11T11:07:23.576986: step 2619, loss 0.314861, acc 0.90625, learning_rate 0.00010011
2017-10-11T11:07:23.645386: step 2620, loss 0.1566, acc 0.921875, learning_rate 0.000100109
2017-10-11T11:07:23.715247: step 2621, loss 0.184168, acc 0.921875, learning_rate 0.000100109
2017-10-11T11:07:23.785675: step 2622, loss 0.286853, acc 0.859375, learning_rate 0.000100108
2017-10-11T11:07:23.855637: step 2623, loss 0.167212, acc 0.953125, learning_rate 0.000100108
2017-10-11T11:07:23.924969: step 2624, loss 0.243548, acc 0.953125, learning_rate 0.000100107
2017-10-11T11:07:23.996918: step 2625, loss 0.188699, acc 0.921875, learning_rate 0.000100107
2017-10-11T11:07:24.065783: step 2626, loss 0.302111, acc 0.90625, learning_rate 0.000100107
2017-10-11T11:07:24.140485: step 2627, loss 0.226721, acc 0.90625, learning_rate 0.000100106
2017-10-11T11:07:24.211768: step 2628, loss 0.172287, acc 0.96875, learning_rate 0.000100106
2017-10-11T11:07:24.285841: step 2629, loss 0.246204, acc 0.921875, learning_rate 0.000100105
2017-10-11T11:07:24.359978: step 2630, loss 0.309532, acc 0.90625, learning_rate 0.000100105
2017-10-11T11:07:24.430385: step 2631, loss 0.381658, acc 0.90625, learning_rate 0.000100104
2017-10-11T11:07:24.503834: step 2632, loss 0.298513, acc 0.90625, learning_rate 0.000100104
2017-10-11T11:07:24.574569: step 2633, loss 0.242289, acc 0.921875, learning_rate 0.000100104
2017-10-11T11:07:24.647167: step 2634, loss 0.225763, acc 0.953125, learning_rate 0.000100103
2017-10-11T11:07:24.716962: step 2635, loss 0.219563, acc 0.921875, learning_rate 0.000100103
2017-10-11T11:07:24.792270: step 2636, loss 0.279116, acc 0.9375, learning_rate 0.000100102
2017-10-11T11:07:24.870498: step 2637, loss 0.210617, acc 0.953125, learning_rate 0.000100102
2017-10-11T11:07:24.947287: step 2638, loss 0.201449, acc 0.9375, learning_rate 0.000100101
2017-10-11T11:07:25.021058: step 2639, loss 0.258682, acc 0.921875, learning_rate 0.000100101
2017-10-11T11:07:25.092358: step 2640, loss 0.269791, acc 0.90625, learning_rate 0.000100101

Evaluation:
2017-10-11T11:07:25.266545: step 2640, loss 0.242729, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2640

2017-10-11T11:07:25.832762: step 2641, loss 0.284442, acc 0.90625, learning_rate 0.0001001
2017-10-11T11:07:25.908705: step 2642, loss 0.171949, acc 0.96875, learning_rate 0.0001001
2017-10-11T11:07:25.978628: step 2643, loss 0.226168, acc 0.90625, learning_rate 0.000100099
2017-10-11T11:07:26.048236: step 2644, loss 0.191542, acc 0.921875, learning_rate 0.000100099
2017-10-11T11:07:26.120753: step 2645, loss 0.141532, acc 0.953125, learning_rate 0.000100099
2017-10-11T11:07:26.186456: step 2646, loss 0.222734, acc 0.901961, learning_rate 0.000100098
2017-10-11T11:07:26.257845: step 2647, loss 0.157279, acc 0.9375, learning_rate 0.000100098
2017-10-11T11:07:26.329701: step 2648, loss 0.153285, acc 0.953125, learning_rate 0.000100097
2017-10-11T11:07:26.404246: step 2649, loss 0.308982, acc 0.890625, learning_rate 0.000100097
2017-10-11T11:07:26.477481: step 2650, loss 0.196333, acc 0.9375, learning_rate 0.000100097
2017-10-11T11:07:26.548885: step 2651, loss 0.261077, acc 0.9375, learning_rate 0.000100096
2017-10-11T11:07:26.616940: step 2652, loss 0.271768, acc 0.890625, learning_rate 0.000100096
2017-10-11T11:07:26.689775: step 2653, loss 0.210849, acc 0.96875, learning_rate 0.000100095
2017-10-11T11:07:26.759701: step 2654, loss 0.128756, acc 0.953125, learning_rate 0.000100095
2017-10-11T11:07:26.833136: step 2655, loss 0.135001, acc 0.96875, learning_rate 0.000100095
2017-10-11T11:07:26.902870: step 2656, loss 0.218436, acc 0.90625, learning_rate 0.000100094
2017-10-11T11:07:26.972269: step 2657, loss 0.267455, acc 0.953125, learning_rate 0.000100094
2017-10-11T11:07:27.042459: step 2658, loss 0.363914, acc 0.90625, learning_rate 0.000100093
2017-10-11T11:07:27.113997: step 2659, loss 0.105747, acc 0.96875, learning_rate 0.000100093
2017-10-11T11:07:27.182698: step 2660, loss 0.158063, acc 0.96875, learning_rate 0.000100093
2017-10-11T11:07:27.259581: step 2661, loss 0.191991, acc 0.9375, learning_rate 0.000100092
2017-10-11T11:07:27.330988: step 2662, loss 0.200446, acc 0.953125, learning_rate 0.000100092
2017-10-11T11:07:27.401708: step 2663, loss 0.138437, acc 0.96875, learning_rate 0.000100092
2017-10-11T11:07:27.479565: step 2664, loss 0.1853, acc 0.890625, learning_rate 0.000100091
2017-10-11T11:07:27.552774: step 2665, loss 0.132458, acc 0.96875, learning_rate 0.000100091
2017-10-11T11:07:27.624319: step 2666, loss 0.369135, acc 0.890625, learning_rate 0.00010009
2017-10-11T11:07:27.696410: step 2667, loss 0.196327, acc 0.953125, learning_rate 0.00010009
2017-10-11T11:07:27.766297: step 2668, loss 0.272971, acc 0.921875, learning_rate 0.00010009
2017-10-11T11:07:27.841078: step 2669, loss 0.275051, acc 0.90625, learning_rate 0.000100089
2017-10-11T11:07:27.909974: step 2670, loss 0.225619, acc 0.90625, learning_rate 0.000100089
2017-10-11T11:07:27.982429: step 2671, loss 0.232258, acc 0.9375, learning_rate 0.000100089
2017-10-11T11:07:28.053330: step 2672, loss 0.221513, acc 0.953125, learning_rate 0.000100088
2017-10-11T11:07:28.123061: step 2673, loss 0.232405, acc 0.921875, learning_rate 0.000100088
2017-10-11T11:07:28.197512: step 2674, loss 0.244025, acc 0.9375, learning_rate 0.000100088
2017-10-11T11:07:28.266210: step 2675, loss 0.254801, acc 0.890625, learning_rate 0.000100087
2017-10-11T11:07:28.338663: step 2676, loss 0.335701, acc 0.921875, learning_rate 0.000100087
2017-10-11T11:07:28.408133: step 2677, loss 0.215054, acc 0.921875, learning_rate 0.000100086
2017-10-11T11:07:28.478686: step 2678, loss 0.216096, acc 0.90625, learning_rate 0.000100086
2017-10-11T11:07:28.551111: step 2679, loss 0.170364, acc 0.953125, learning_rate 0.000100086
2017-10-11T11:07:28.621540: step 2680, loss 0.177829, acc 0.921875, learning_rate 0.000100085

Evaluation:
2017-10-11T11:07:28.791736: step 2680, loss 0.241944, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2680

2017-10-11T11:07:29.434764: step 2681, loss 0.133291, acc 0.96875, learning_rate 0.000100085
2017-10-11T11:07:29.508984: step 2682, loss 0.279414, acc 0.890625, learning_rate 0.000100085
2017-10-11T11:07:29.581708: step 2683, loss 0.168088, acc 0.9375, learning_rate 0.000100084
2017-10-11T11:07:29.657390: step 2684, loss 0.172099, acc 0.953125, learning_rate 0.000100084
2017-10-11T11:07:29.726853: step 2685, loss 0.235904, acc 0.90625, learning_rate 0.000100084
2017-10-11T11:07:29.795096: step 2686, loss 0.237531, acc 0.9375, learning_rate 0.000100083
2017-10-11T11:07:29.865574: step 2687, loss 0.233679, acc 0.9375, learning_rate 0.000100083
2017-10-11T11:07:29.935310: step 2688, loss 0.175628, acc 0.96875, learning_rate 0.000100083
2017-10-11T11:07:30.006696: step 2689, loss 0.162, acc 0.953125, learning_rate 0.000100082
2017-10-11T11:07:30.079453: step 2690, loss 0.251898, acc 0.921875, learning_rate 0.000100082
2017-10-11T11:07:30.152384: step 2691, loss 0.216046, acc 0.90625, learning_rate 0.000100082
2017-10-11T11:07:30.223083: step 2692, loss 0.147864, acc 0.96875, learning_rate 0.000100081
2017-10-11T11:07:30.292381: step 2693, loss 0.264174, acc 0.921875, learning_rate 0.000100081
2017-10-11T11:07:30.365060: step 2694, loss 0.183914, acc 0.90625, learning_rate 0.000100081
2017-10-11T11:07:30.437517: step 2695, loss 0.353039, acc 0.890625, learning_rate 0.00010008
2017-10-11T11:07:30.507035: step 2696, loss 0.178994, acc 0.90625, learning_rate 0.00010008
2017-10-11T11:07:30.580187: step 2697, loss 0.238383, acc 0.921875, learning_rate 0.00010008
2017-10-11T11:07:30.649360: step 2698, loss 0.254389, acc 0.90625, learning_rate 0.000100079
2017-10-11T11:07:30.724038: step 2699, loss 0.18892, acc 0.921875, learning_rate 0.000100079
2017-10-11T11:07:30.797055: step 2700, loss 0.234118, acc 0.875, learning_rate 0.000100079
2017-10-11T11:07:30.871661: step 2701, loss 0.236528, acc 0.90625, learning_rate 0.000100078
2017-10-11T11:07:30.944272: step 2702, loss 0.288222, acc 0.890625, learning_rate 0.000100078
2017-10-11T11:07:31.015630: step 2703, loss 0.306634, acc 0.90625, learning_rate 0.000100078
2017-10-11T11:07:31.084783: step 2704, loss 0.150176, acc 0.953125, learning_rate 0.000100077
2017-10-11T11:07:31.156464: step 2705, loss 0.28574, acc 0.875, learning_rate 0.000100077
2017-10-11T11:07:31.226133: step 2706, loss 0.208914, acc 0.921875, learning_rate 0.000100077
2017-10-11T11:07:31.299090: step 2707, loss 0.282138, acc 0.90625, learning_rate 0.000100076
2017-10-11T11:07:31.372734: step 2708, loss 0.135185, acc 0.953125, learning_rate 0.000100076
2017-10-11T11:07:31.443597: step 2709, loss 0.147367, acc 0.953125, learning_rate 0.000100076
2017-10-11T11:07:31.512315: step 2710, loss 0.137212, acc 0.984375, learning_rate 0.000100076
2017-10-11T11:07:31.584880: step 2711, loss 0.152228, acc 0.96875, learning_rate 0.000100075
2017-10-11T11:07:31.655839: step 2712, loss 0.211378, acc 0.953125, learning_rate 0.000100075
2017-10-11T11:07:31.725386: step 2713, loss 0.186523, acc 0.9375, learning_rate 0.000100075
2017-10-11T11:07:31.797683: step 2714, loss 0.196013, acc 0.9375, learning_rate 0.000100074
2017-10-11T11:07:31.871209: step 2715, loss 0.116903, acc 0.953125, learning_rate 0.000100074
2017-10-11T11:07:31.953307: step 2716, loss 0.282859, acc 0.890625, learning_rate 0.000100074
2017-10-11T11:07:32.023984: step 2717, loss 0.191848, acc 0.921875, learning_rate 0.000100073
2017-10-11T11:07:32.093137: step 2718, loss 0.102536, acc 1, learning_rate 0.000100073
2017-10-11T11:07:32.168692: step 2719, loss 0.329051, acc 0.859375, learning_rate 0.000100073
2017-10-11T11:07:32.237137: step 2720, loss 0.194044, acc 0.96875, learning_rate 0.000100073

Evaluation:
2017-10-11T11:07:32.410415: step 2720, loss 0.241211, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2720

2017-10-11T11:07:32.914554: step 2721, loss 0.320746, acc 0.921875, learning_rate 0.000100072
2017-10-11T11:07:32.984115: step 2722, loss 0.137788, acc 0.96875, learning_rate 0.000100072
2017-10-11T11:07:33.056026: step 2723, loss 0.146898, acc 0.96875, learning_rate 0.000100072
2017-10-11T11:07:33.127633: step 2724, loss 0.208347, acc 0.921875, learning_rate 0.000100071
2017-10-11T11:07:33.201919: step 2725, loss 0.115694, acc 0.96875, learning_rate 0.000100071
2017-10-11T11:07:33.271069: step 2726, loss 0.200184, acc 0.9375, learning_rate 0.000100071
2017-10-11T11:07:33.345323: step 2727, loss 0.132503, acc 0.9375, learning_rate 0.00010007
2017-10-11T11:07:33.414136: step 2728, loss 0.0995739, acc 0.984375, learning_rate 0.00010007
2017-10-11T11:07:33.484771: step 2729, loss 0.198384, acc 0.96875, learning_rate 0.00010007
2017-10-11T11:07:33.558244: step 2730, loss 0.388526, acc 0.84375, learning_rate 0.00010007
2017-10-11T11:07:33.630432: step 2731, loss 0.243962, acc 0.90625, learning_rate 0.000100069
2017-10-11T11:07:33.699302: step 2732, loss 0.23465, acc 0.90625, learning_rate 0.000100069
2017-10-11T11:07:33.770154: step 2733, loss 0.187712, acc 0.9375, learning_rate 0.000100069
2017-10-11T11:07:33.837764: step 2734, loss 0.240501, acc 0.921875, learning_rate 0.000100068
2017-10-11T11:07:33.907646: step 2735, loss 0.334605, acc 0.90625, learning_rate 0.000100068
2017-10-11T11:07:33.982724: step 2736, loss 0.177516, acc 0.9375, learning_rate 0.000100068
2017-10-11T11:07:34.052115: step 2737, loss 0.273907, acc 0.90625, learning_rate 0.000100068
2017-10-11T11:07:34.123501: step 2738, loss 0.290391, acc 0.9375, learning_rate 0.000100067
2017-10-11T11:07:34.194915: step 2739, loss 0.282878, acc 0.90625, learning_rate 0.000100067
2017-10-11T11:07:34.266206: step 2740, loss 0.238127, acc 0.890625, learning_rate 0.000100067
2017-10-11T11:07:34.336383: step 2741, loss 0.238818, acc 0.953125, learning_rate 0.000100067
2017-10-11T11:07:34.408960: step 2742, loss 0.226767, acc 0.9375, learning_rate 0.000100066
2017-10-11T11:07:34.479590: step 2743, loss 0.208119, acc 0.9375, learning_rate 0.000100066
2017-10-11T11:07:34.542001: step 2744, loss 0.205182, acc 0.882353, learning_rate 0.000100066
2017-10-11T11:07:34.612921: step 2745, loss 0.236529, acc 0.9375, learning_rate 0.000100065
2017-10-11T11:07:34.682291: step 2746, loss 0.230312, acc 0.921875, learning_rate 0.000100065
2017-10-11T11:07:34.754218: step 2747, loss 0.18188, acc 0.9375, learning_rate 0.000100065
2017-10-11T11:07:34.825470: step 2748, loss 0.178285, acc 0.96875, learning_rate 0.000100065
2017-10-11T11:07:34.899775: step 2749, loss 0.171213, acc 0.953125, learning_rate 0.000100064
2017-10-11T11:07:34.975306: step 2750, loss 0.161637, acc 0.921875, learning_rate 0.000100064
2017-10-11T11:07:35.051421: step 2751, loss 0.314744, acc 0.921875, learning_rate 0.000100064
2017-10-11T11:07:35.123115: step 2752, loss 0.316349, acc 0.90625, learning_rate 0.000100064
2017-10-11T11:07:35.197446: step 2753, loss 0.30609, acc 0.921875, learning_rate 0.000100063
2017-10-11T11:07:35.267088: step 2754, loss 0.22971, acc 0.921875, learning_rate 0.000100063
2017-10-11T11:07:35.337273: step 2755, loss 0.230135, acc 0.890625, learning_rate 0.000100063
2017-10-11T11:07:35.404485: step 2756, loss 0.325405, acc 0.890625, learning_rate 0.000100063
2017-10-11T11:07:35.473636: step 2757, loss 0.141108, acc 0.9375, learning_rate 0.000100062
2017-10-11T11:07:35.546106: step 2758, loss 0.214321, acc 0.9375, learning_rate 0.000100062
2017-10-11T11:07:35.616346: step 2759, loss 0.333632, acc 0.84375, learning_rate 0.000100062
2017-10-11T11:07:35.686412: step 2760, loss 0.223303, acc 0.9375, learning_rate 0.000100062

Evaluation:
2017-10-11T11:07:35.872410: step 2760, loss 0.241003, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2760

2017-10-11T11:07:36.436960: step 2761, loss 0.213776, acc 0.921875, learning_rate 0.000100061
2017-10-11T11:07:36.509923: step 2762, loss 0.202124, acc 0.953125, learning_rate 0.000100061
2017-10-11T11:07:36.580450: step 2763, loss 0.25398, acc 0.9375, learning_rate 0.000100061
2017-10-11T11:07:36.652958: step 2764, loss 0.304756, acc 0.90625, learning_rate 0.000100061
2017-10-11T11:07:36.724720: step 2765, loss 0.230721, acc 0.90625, learning_rate 0.00010006
2017-10-11T11:07:36.795323: step 2766, loss 0.133981, acc 0.96875, learning_rate 0.00010006
2017-10-11T11:07:36.864419: step 2767, loss 0.242054, acc 0.9375, learning_rate 0.00010006
2017-10-11T11:07:36.943338: step 2768, loss 0.273527, acc 0.875, learning_rate 0.00010006
2017-10-11T11:07:37.017512: step 2769, loss 0.235301, acc 0.921875, learning_rate 0.000100059
2017-10-11T11:07:37.090003: step 2770, loss 0.290728, acc 0.859375, learning_rate 0.000100059
2017-10-11T11:07:37.161550: step 2771, loss 0.278599, acc 0.90625, learning_rate 0.000100059
2017-10-11T11:07:37.235344: step 2772, loss 0.110911, acc 0.96875, learning_rate 0.000100059
2017-10-11T11:07:37.304125: step 2773, loss 0.177968, acc 0.9375, learning_rate 0.000100058
2017-10-11T11:07:37.374645: step 2774, loss 0.24311, acc 0.921875, learning_rate 0.000100058
2017-10-11T11:07:37.446183: step 2775, loss 0.144212, acc 0.96875, learning_rate 0.000100058
2017-10-11T11:07:37.516754: step 2776, loss 0.267045, acc 0.890625, learning_rate 0.000100058
2017-10-11T11:07:37.585921: step 2777, loss 0.184062, acc 0.921875, learning_rate 0.000100057
2017-10-11T11:07:37.656043: step 2778, loss 0.209993, acc 0.9375, learning_rate 0.000100057
2017-10-11T11:07:37.726636: step 2779, loss 0.18714, acc 0.953125, learning_rate 0.000100057
2017-10-11T11:07:37.798017: step 2780, loss 0.233779, acc 0.921875, learning_rate 0.000100057
2017-10-11T11:07:37.886070: step 2781, loss 0.291429, acc 0.90625, learning_rate 0.000100056
2017-10-11T11:07:37.974484: step 2782, loss 0.260284, acc 0.90625, learning_rate 0.000100056
2017-10-11T11:07:38.047321: step 2783, loss 0.257179, acc 0.90625, learning_rate 0.000100056
2017-10-11T11:07:38.114932: step 2784, loss 0.143972, acc 0.96875, learning_rate 0.000100056
2017-10-11T11:07:38.187878: step 2785, loss 0.235876, acc 0.921875, learning_rate 0.000100056
2017-10-11T11:07:38.262772: step 2786, loss 0.212254, acc 0.921875, learning_rate 0.000100055
2017-10-11T11:07:38.336184: step 2787, loss 0.276222, acc 0.9375, learning_rate 0.000100055
2017-10-11T11:07:38.408559: step 2788, loss 0.302485, acc 0.9375, learning_rate 0.000100055
2017-10-11T11:07:38.477834: step 2789, loss 0.298103, acc 0.875, learning_rate 0.000100055
2017-10-11T11:07:38.551458: step 2790, loss 0.387998, acc 0.890625, learning_rate 0.000100054
2017-10-11T11:07:38.623677: step 2791, loss 0.258114, acc 0.9375, learning_rate 0.000100054
2017-10-11T11:07:38.694658: step 2792, loss 0.109709, acc 0.96875, learning_rate 0.000100054
2017-10-11T11:07:38.767489: step 2793, loss 0.298324, acc 0.921875, learning_rate 0.000100054
2017-10-11T11:07:38.844744: step 2794, loss 0.184847, acc 0.9375, learning_rate 0.000100054
2017-10-11T11:07:38.915369: step 2795, loss 0.241044, acc 0.9375, learning_rate 0.000100053
2017-10-11T11:07:38.987264: step 2796, loss 0.205238, acc 0.953125, learning_rate 0.000100053
2017-10-11T11:07:39.054027: step 2797, loss 0.170979, acc 0.9375, learning_rate 0.000100053
2017-10-11T11:07:39.128189: step 2798, loss 0.29403, acc 0.9375, learning_rate 0.000100053
2017-10-11T11:07:39.203412: step 2799, loss 0.25108, acc 0.9375, learning_rate 0.000100052
2017-10-11T11:07:39.274307: step 2800, loss 0.235553, acc 0.9375, learning_rate 0.000100052

Evaluation:
2017-10-11T11:07:39.449918: step 2800, loss 0.241682, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2800

2017-10-11T11:07:40.092163: step 2801, loss 0.131675, acc 0.96875, learning_rate 0.000100052
2017-10-11T11:07:40.163829: step 2802, loss 0.126001, acc 0.96875, learning_rate 0.000100052
2017-10-11T11:07:40.236434: step 2803, loss 0.109133, acc 0.984375, learning_rate 0.000100052
2017-10-11T11:07:40.307362: step 2804, loss 0.118573, acc 0.96875, learning_rate 0.000100051
2017-10-11T11:07:40.379897: step 2805, loss 0.215655, acc 0.9375, learning_rate 0.000100051
2017-10-11T11:07:40.450743: step 2806, loss 0.274861, acc 0.90625, learning_rate 0.000100051
2017-10-11T11:07:40.519532: step 2807, loss 0.176371, acc 0.953125, learning_rate 0.000100051
2017-10-11T11:07:40.588234: step 2808, loss 0.181494, acc 0.921875, learning_rate 0.000100051
2017-10-11T11:07:40.660570: step 2809, loss 0.193059, acc 0.96875, learning_rate 0.00010005
2017-10-11T11:07:40.737712: step 2810, loss 0.155693, acc 0.984375, learning_rate 0.00010005
2017-10-11T11:07:40.807942: step 2811, loss 0.157271, acc 0.9375, learning_rate 0.00010005
2017-10-11T11:07:40.877603: step 2812, loss 0.253037, acc 0.921875, learning_rate 0.00010005
2017-10-11T11:07:40.947541: step 2813, loss 0.231921, acc 0.921875, learning_rate 0.00010005
2017-10-11T11:07:41.017642: step 2814, loss 0.198873, acc 0.921875, learning_rate 0.000100049
2017-10-11T11:07:41.091242: step 2815, loss 0.204953, acc 0.9375, learning_rate 0.000100049
2017-10-11T11:07:41.162703: step 2816, loss 0.137175, acc 0.9375, learning_rate 0.000100049
2017-10-11T11:07:41.236040: step 2817, loss 0.164592, acc 0.96875, learning_rate 0.000100049
2017-10-11T11:07:41.304738: step 2818, loss 0.293513, acc 0.953125, learning_rate 0.000100049
2017-10-11T11:07:41.381368: step 2819, loss 0.119364, acc 0.984375, learning_rate 0.000100048
2017-10-11T11:07:41.455644: step 2820, loss 0.0936269, acc 0.984375, learning_rate 0.000100048
2017-10-11T11:07:41.525922: step 2821, loss 0.333647, acc 0.90625, learning_rate 0.000100048
2017-10-11T11:07:41.597518: step 2822, loss 0.163566, acc 0.921875, learning_rate 0.000100048
2017-10-11T11:07:41.671135: step 2823, loss 0.171735, acc 0.9375, learning_rate 0.000100048
2017-10-11T11:07:41.743616: step 2824, loss 0.292156, acc 0.9375, learning_rate 0.000100047
2017-10-11T11:07:41.814397: step 2825, loss 0.155532, acc 0.953125, learning_rate 0.000100047
2017-10-11T11:07:41.890702: step 2826, loss 0.288166, acc 0.890625, learning_rate 0.000100047
2017-10-11T11:07:41.962393: step 2827, loss 0.172317, acc 0.953125, learning_rate 0.000100047
2017-10-11T11:07:42.035175: step 2828, loss 0.2369, acc 0.921875, learning_rate 0.000100047
2017-10-11T11:07:42.107175: step 2829, loss 0.216604, acc 0.90625, learning_rate 0.000100046
2017-10-11T11:07:42.178730: step 2830, loss 0.196958, acc 0.9375, learning_rate 0.000100046
2017-10-11T11:07:42.247983: step 2831, loss 0.159884, acc 0.9375, learning_rate 0.000100046
2017-10-11T11:07:42.320713: step 2832, loss 0.163994, acc 0.953125, learning_rate 0.000100046
2017-10-11T11:07:42.389462: step 2833, loss 0.188906, acc 0.9375, learning_rate 0.000100046
2017-10-11T11:07:42.464247: step 2834, loss 0.248806, acc 0.921875, learning_rate 0.000100045
2017-10-11T11:07:42.538079: step 2835, loss 0.197627, acc 0.921875, learning_rate 0.000100045
2017-10-11T11:07:42.610298: step 2836, loss 0.235471, acc 0.921875, learning_rate 0.000100045
2017-10-11T11:07:42.682657: step 2837, loss 0.296595, acc 0.890625, learning_rate 0.000100045
2017-10-11T11:07:42.757127: step 2838, loss 0.262986, acc 0.921875, learning_rate 0.000100045
2017-10-11T11:07:42.824176: step 2839, loss 0.163655, acc 0.9375, learning_rate 0.000100045
2017-10-11T11:07:42.906639: step 2840, loss 0.152942, acc 0.9375, learning_rate 0.000100044

Evaluation:
2017-10-11T11:07:43.086041: step 2840, loss 0.24052, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2840

2017-10-11T11:07:43.584677: step 2841, loss 0.134117, acc 0.953125, learning_rate 0.000100044
2017-10-11T11:07:43.645661: step 2842, loss 0.323169, acc 0.862745, learning_rate 0.000100044
2017-10-11T11:07:43.719878: step 2843, loss 0.234441, acc 0.90625, learning_rate 0.000100044
2017-10-11T11:07:43.792560: step 2844, loss 0.260667, acc 0.921875, learning_rate 0.000100044
2017-10-11T11:07:43.863574: step 2845, loss 0.262251, acc 0.90625, learning_rate 0.000100043
2017-10-11T11:07:43.936439: step 2846, loss 0.131995, acc 0.953125, learning_rate 0.000100043
2017-10-11T11:07:44.009349: step 2847, loss 0.292044, acc 0.890625, learning_rate 0.000100043
2017-10-11T11:07:44.078601: step 2848, loss 0.207847, acc 0.9375, learning_rate 0.000100043
2017-10-11T11:07:44.148874: step 2849, loss 0.319663, acc 0.921875, learning_rate 0.000100043
2017-10-11T11:07:44.218729: step 2850, loss 0.134267, acc 0.96875, learning_rate 0.000100043
2017-10-11T11:07:44.288443: step 2851, loss 0.17236, acc 0.9375, learning_rate 0.000100042
2017-10-11T11:07:44.361592: step 2852, loss 0.164462, acc 0.953125, learning_rate 0.000100042
2017-10-11T11:07:44.434721: step 2853, loss 0.203162, acc 0.9375, learning_rate 0.000100042
2017-10-11T11:07:44.508584: step 2854, loss 0.259417, acc 0.921875, learning_rate 0.000100042
2017-10-11T11:07:44.576921: step 2855, loss 0.33101, acc 0.84375, learning_rate 0.000100042
2017-10-11T11:07:44.646165: step 2856, loss 0.324257, acc 0.9375, learning_rate 0.000100042
2017-10-11T11:07:44.721315: step 2857, loss 0.171002, acc 0.96875, learning_rate 0.000100041
2017-10-11T11:07:44.794410: step 2858, loss 0.171852, acc 0.953125, learning_rate 0.000100041
2017-10-11T11:07:44.865281: step 2859, loss 0.194272, acc 0.921875, learning_rate 0.000100041
2017-10-11T11:07:44.935942: step 2860, loss 0.153484, acc 0.96875, learning_rate 0.000100041
2017-10-11T11:07:45.003876: step 2861, loss 0.264137, acc 0.9375, learning_rate 0.000100041
2017-10-11T11:07:45.079939: step 2862, loss 0.195128, acc 0.90625, learning_rate 0.000100041
2017-10-11T11:07:45.152837: step 2863, loss 0.197019, acc 0.953125, learning_rate 0.00010004
2017-10-11T11:07:45.223270: step 2864, loss 0.287254, acc 0.90625, learning_rate 0.00010004
2017-10-11T11:07:45.292844: step 2865, loss 0.115206, acc 0.984375, learning_rate 0.00010004
2017-10-11T11:07:45.363410: step 2866, loss 0.127001, acc 0.984375, learning_rate 0.00010004
2017-10-11T11:07:45.437341: step 2867, loss 0.198345, acc 0.921875, learning_rate 0.00010004
2017-10-11T11:07:45.509266: step 2868, loss 0.152337, acc 0.984375, learning_rate 0.00010004
2017-10-11T11:07:45.578136: step 2869, loss 0.317102, acc 0.90625, learning_rate 0.000100039
2017-10-11T11:07:45.649371: step 2870, loss 0.111309, acc 0.96875, learning_rate 0.000100039
2017-10-11T11:07:45.720561: step 2871, loss 0.181069, acc 0.9375, learning_rate 0.000100039
2017-10-11T11:07:45.792338: step 2872, loss 0.209016, acc 0.9375, learning_rate 0.000100039
2017-10-11T11:07:45.866021: step 2873, loss 0.284842, acc 0.9375, learning_rate 0.000100039
2017-10-11T11:07:45.939635: step 2874, loss 0.277922, acc 0.90625, learning_rate 0.000100039
2017-10-11T11:07:46.012206: step 2875, loss 0.231957, acc 0.90625, learning_rate 0.000100038
2017-10-11T11:07:46.082736: step 2876, loss 0.100211, acc 0.984375, learning_rate 0.000100038
2017-10-11T11:07:46.153676: step 2877, loss 0.152739, acc 0.96875, learning_rate 0.000100038
2017-10-11T11:07:46.221495: step 2878, loss 0.203244, acc 0.9375, learning_rate 0.000100038
2017-10-11T11:07:46.295300: step 2879, loss 0.280086, acc 0.90625, learning_rate 0.000100038
2017-10-11T11:07:46.365595: step 2880, loss 0.209861, acc 0.90625, learning_rate 0.000100038

Evaluation:
2017-10-11T11:07:46.539911: step 2880, loss 0.240098, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2880

2017-10-11T11:07:47.107136: step 2881, loss 0.236453, acc 0.9375, learning_rate 0.000100038
2017-10-11T11:07:47.177217: step 2882, loss 0.0997196, acc 0.984375, learning_rate 0.000100037
2017-10-11T11:07:47.246752: step 2883, loss 0.29517, acc 0.828125, learning_rate 0.000100037
2017-10-11T11:07:47.316436: step 2884, loss 0.194977, acc 0.953125, learning_rate 0.000100037
2017-10-11T11:07:47.388370: step 2885, loss 0.157972, acc 0.96875, learning_rate 0.000100037
2017-10-11T11:07:47.460014: step 2886, loss 0.27013, acc 0.921875, learning_rate 0.000100037
2017-10-11T11:07:47.533693: step 2887, loss 0.229232, acc 0.953125, learning_rate 0.000100037
2017-10-11T11:07:47.604855: step 2888, loss 0.209498, acc 0.890625, learning_rate 0.000100036
2017-10-11T11:07:47.675284: step 2889, loss 0.193134, acc 0.921875, learning_rate 0.000100036
2017-10-11T11:07:47.746465: step 2890, loss 0.301764, acc 0.921875, learning_rate 0.000100036
2017-10-11T11:07:47.819854: step 2891, loss 0.368099, acc 0.84375, learning_rate 0.000100036
2017-10-11T11:07:47.893579: step 2892, loss 0.22485, acc 0.921875, learning_rate 0.000100036
2017-10-11T11:07:47.967043: step 2893, loss 0.20578, acc 0.9375, learning_rate 0.000100036
2017-10-11T11:07:48.039516: step 2894, loss 0.126779, acc 0.984375, learning_rate 0.000100036
2017-10-11T11:07:48.112703: step 2895, loss 0.153171, acc 0.921875, learning_rate 0.000100035
2017-10-11T11:07:48.183724: step 2896, loss 0.397554, acc 0.84375, learning_rate 0.000100035
2017-10-11T11:07:48.259069: step 2897, loss 0.198251, acc 0.90625, learning_rate 0.000100035
2017-10-11T11:07:48.327915: step 2898, loss 0.172069, acc 0.96875, learning_rate 0.000100035
2017-10-11T11:07:48.397412: step 2899, loss 0.19204, acc 0.96875, learning_rate 0.000100035
2017-10-11T11:07:48.470872: step 2900, loss 0.322188, acc 0.875, learning_rate 0.000100035
2017-10-11T11:07:48.540197: step 2901, loss 0.105158, acc 0.984375, learning_rate 0.000100035
2017-10-11T11:07:48.611497: step 2902, loss 0.14714, acc 1, learning_rate 0.000100034
2017-10-11T11:07:48.681276: step 2903, loss 0.236647, acc 0.921875, learning_rate 0.000100034
2017-10-11T11:07:48.755839: step 2904, loss 0.188691, acc 0.9375, learning_rate 0.000100034
2017-10-11T11:07:48.825375: step 2905, loss 0.243206, acc 0.953125, learning_rate 0.000100034
2017-10-11T11:07:48.899328: step 2906, loss 0.31289, acc 0.9375, learning_rate 0.000100034
2017-10-11T11:07:48.970415: step 2907, loss 0.440536, acc 0.84375, learning_rate 0.000100034
2017-10-11T11:07:49.039829: step 2908, loss 0.185726, acc 0.953125, learning_rate 0.000100034
2017-10-11T11:07:49.108913: step 2909, loss 0.194066, acc 0.921875, learning_rate 0.000100033
2017-10-11T11:07:49.181831: step 2910, loss 0.156554, acc 0.9375, learning_rate 0.000100033
2017-10-11T11:07:49.250936: step 2911, loss 0.138863, acc 0.9375, learning_rate 0.000100033
2017-10-11T11:07:49.317333: step 2912, loss 0.278651, acc 0.921875, learning_rate 0.000100033
2017-10-11T11:07:49.395619: step 2913, loss 0.405861, acc 0.84375, learning_rate 0.000100033
2017-10-11T11:07:49.473858: step 2914, loss 0.175742, acc 0.953125, learning_rate 0.000100033
2017-10-11T11:07:49.543504: step 2915, loss 0.33375, acc 0.875, learning_rate 0.000100033
2017-10-11T11:07:49.615247: step 2916, loss 0.135673, acc 0.96875, learning_rate 0.000100033
2017-10-11T11:07:49.689800: step 2917, loss 0.213408, acc 0.921875, learning_rate 0.000100032
2017-10-11T11:07:49.762974: step 2918, loss 0.170064, acc 0.96875, learning_rate 0.000100032
2017-10-11T11:07:49.831503: step 2919, loss 0.278776, acc 0.9375, learning_rate 0.000100032
2017-10-11T11:07:49.905245: step 2920, loss 0.260714, acc 0.921875, learning_rate 0.000100032

Evaluation:
2017-10-11T11:07:50.079601: step 2920, loss 0.239881, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2920

2017-10-11T11:07:50.722495: step 2921, loss 0.133799, acc 0.96875, learning_rate 0.000100032
2017-10-11T11:07:50.796776: step 2922, loss 0.1943, acc 0.921875, learning_rate 0.000100032
2017-10-11T11:07:50.869872: step 2923, loss 0.169015, acc 0.9375, learning_rate 0.000100032
2017-10-11T11:07:50.940986: step 2924, loss 0.129417, acc 0.96875, learning_rate 0.000100031
2017-10-11T11:07:51.008885: step 2925, loss 0.255807, acc 0.953125, learning_rate 0.000100031
2017-10-11T11:07:51.083324: step 2926, loss 0.244303, acc 0.90625, learning_rate 0.000100031
2017-10-11T11:07:51.153354: step 2927, loss 0.275427, acc 0.90625, learning_rate 0.000100031
2017-10-11T11:07:51.224668: step 2928, loss 0.284727, acc 0.859375, learning_rate 0.000100031
2017-10-11T11:07:51.296091: step 2929, loss 0.234075, acc 0.90625, learning_rate 0.000100031
2017-10-11T11:07:51.365683: step 2930, loss 0.213339, acc 0.921875, learning_rate 0.000100031
2017-10-11T11:07:51.439947: step 2931, loss 0.314543, acc 0.921875, learning_rate 0.000100031
2017-10-11T11:07:51.514096: step 2932, loss 0.365604, acc 0.859375, learning_rate 0.00010003
2017-10-11T11:07:51.584985: step 2933, loss 0.192306, acc 0.921875, learning_rate 0.00010003
2017-10-11T11:07:51.658446: step 2934, loss 0.195865, acc 0.921875, learning_rate 0.00010003
2017-10-11T11:07:51.728362: step 2935, loss 0.286837, acc 0.890625, learning_rate 0.00010003
2017-10-11T11:07:51.800593: step 2936, loss 0.302576, acc 0.921875, learning_rate 0.00010003
2017-10-11T11:07:51.875187: step 2937, loss 0.238635, acc 0.921875, learning_rate 0.00010003
2017-10-11T11:07:51.946961: step 2938, loss 0.150551, acc 0.9375, learning_rate 0.00010003
2017-10-11T11:07:52.017051: step 2939, loss 0.255289, acc 0.9375, learning_rate 0.00010003
2017-10-11T11:07:52.078415: step 2940, loss 0.107515, acc 0.980392, learning_rate 0.000100029
2017-10-11T11:07:52.150699: step 2941, loss 0.181338, acc 0.921875, learning_rate 0.000100029
2017-10-11T11:07:52.222156: step 2942, loss 0.359176, acc 0.890625, learning_rate 0.000100029
2017-10-11T11:07:52.289899: step 2943, loss 0.212206, acc 0.9375, learning_rate 0.000100029
2017-10-11T11:07:52.360678: step 2944, loss 0.165937, acc 0.9375, learning_rate 0.000100029
2017-10-11T11:07:52.430541: step 2945, loss 0.192446, acc 0.9375, learning_rate 0.000100029
2017-10-11T11:07:52.504314: step 2946, loss 0.219624, acc 0.953125, learning_rate 0.000100029
2017-10-11T11:07:52.575188: step 2947, loss 0.204303, acc 0.875, learning_rate 0.000100029
2017-10-11T11:07:52.650647: step 2948, loss 0.258265, acc 0.9375, learning_rate 0.000100029
2017-10-11T11:07:52.723699: step 2949, loss 0.242527, acc 0.9375, learning_rate 0.000100028
2017-10-11T11:07:52.796220: step 2950, loss 0.299026, acc 0.890625, learning_rate 0.000100028
2017-10-11T11:07:52.877830: step 2951, loss 0.350534, acc 0.84375, learning_rate 0.000100028
2017-10-11T11:07:52.948601: step 2952, loss 0.304696, acc 0.875, learning_rate 0.000100028
2017-10-11T11:07:53.021645: step 2953, loss 0.208127, acc 0.953125, learning_rate 0.000100028
2017-10-11T11:07:53.094248: step 2954, loss 0.16997, acc 0.953125, learning_rate 0.000100028
2017-10-11T11:07:53.169265: step 2955, loss 0.216391, acc 0.921875, learning_rate 0.000100028
2017-10-11T11:07:53.243451: step 2956, loss 0.288801, acc 0.9375, learning_rate 0.000100028
2017-10-11T11:07:53.317031: step 2957, loss 0.18203, acc 0.953125, learning_rate 0.000100028
2017-10-11T11:07:53.387048: step 2958, loss 0.20474, acc 0.9375, learning_rate 0.000100027
2017-10-11T11:07:53.468959: step 2959, loss 0.268171, acc 0.921875, learning_rate 0.000100027
2017-10-11T11:07:53.553103: step 2960, loss 0.194096, acc 0.90625, learning_rate 0.000100027

Evaluation:
2017-10-11T11:07:53.730004: step 2960, loss 0.240014, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-2960

2017-10-11T11:07:54.239685: step 2961, loss 0.157545, acc 0.96875, learning_rate 0.000100027
2017-10-11T11:07:54.309891: step 2962, loss 0.298564, acc 0.921875, learning_rate 0.000100027
2017-10-11T11:07:54.380712: step 2963, loss 0.181478, acc 0.921875, learning_rate 0.000100027
2017-10-11T11:07:54.453824: step 2964, loss 0.163734, acc 0.9375, learning_rate 0.000100027
2017-10-11T11:07:54.525165: step 2965, loss 0.146101, acc 0.96875, learning_rate 0.000100027
2017-10-11T11:07:54.594691: step 2966, loss 0.257111, acc 0.90625, learning_rate 0.000100027
2017-10-11T11:07:54.665683: step 2967, loss 0.23849, acc 0.921875, learning_rate 0.000100026
2017-10-11T11:07:54.740204: step 2968, loss 0.15182, acc 0.953125, learning_rate 0.000100026
2017-10-11T11:07:54.812983: step 2969, loss 0.226078, acc 0.9375, learning_rate 0.000100026
2017-10-11T11:07:54.888752: step 2970, loss 0.319322, acc 0.90625, learning_rate 0.000100026
2017-10-11T11:07:54.960901: step 2971, loss 0.133333, acc 0.984375, learning_rate 0.000100026
2017-10-11T11:07:55.036228: step 2972, loss 0.354172, acc 0.828125, learning_rate 0.000100026
2017-10-11T11:07:55.109619: step 2973, loss 0.105317, acc 0.984375, learning_rate 0.000100026
2017-10-11T11:07:55.184334: step 2974, loss 0.146715, acc 0.96875, learning_rate 0.000100026
2017-10-11T11:07:55.256026: step 2975, loss 0.208174, acc 0.921875, learning_rate 0.000100026
2017-10-11T11:07:55.330933: step 2976, loss 0.112333, acc 0.96875, learning_rate 0.000100025
2017-10-11T11:07:55.406924: step 2977, loss 0.2301, acc 0.90625, learning_rate 0.000100025
2017-10-11T11:07:55.475236: step 2978, loss 0.300913, acc 0.875, learning_rate 0.000100025
2017-10-11T11:07:55.546773: step 2979, loss 0.137723, acc 0.96875, learning_rate 0.000100025
2017-10-11T11:07:55.621409: step 2980, loss 0.176699, acc 0.984375, learning_rate 0.000100025
2017-10-11T11:07:55.691939: step 2981, loss 0.242975, acc 0.9375, learning_rate 0.000100025
2017-10-11T11:07:55.766398: step 2982, loss 0.117261, acc 0.953125, learning_rate 0.000100025
2017-10-11T11:07:55.838371: step 2983, loss 0.442272, acc 0.875, learning_rate 0.000100025
2017-10-11T11:07:55.912029: step 2984, loss 0.16025, acc 0.9375, learning_rate 0.000100025
2017-10-11T11:07:55.983939: step 2985, loss 0.216436, acc 0.921875, learning_rate 0.000100025
2017-10-11T11:07:56.055919: step 2986, loss 0.171086, acc 0.953125, learning_rate 0.000100024
2017-10-11T11:07:56.129594: step 2987, loss 0.330875, acc 0.90625, learning_rate 0.000100024
2017-10-11T11:07:56.197920: step 2988, loss 0.180533, acc 0.953125, learning_rate 0.000100024
2017-10-11T11:07:56.269544: step 2989, loss 0.396165, acc 0.90625, learning_rate 0.000100024
2017-10-11T11:07:56.339866: step 2990, loss 0.138872, acc 0.96875, learning_rate 0.000100024
2017-10-11T11:07:56.411623: step 2991, loss 0.150156, acc 0.984375, learning_rate 0.000100024
2017-10-11T11:07:56.483924: step 2992, loss 0.284619, acc 0.9375, learning_rate 0.000100024
2017-10-11T11:07:56.557580: step 2993, loss 0.121248, acc 0.96875, learning_rate 0.000100024
2017-10-11T11:07:56.630280: step 2994, loss 0.138709, acc 0.984375, learning_rate 0.000100024
2017-10-11T11:07:56.699143: step 2995, loss 0.060226, acc 0.984375, learning_rate 0.000100024
2017-10-11T11:07:56.768571: step 2996, loss 0.296124, acc 0.921875, learning_rate 0.000100023
2017-10-11T11:07:56.843284: step 2997, loss 0.142273, acc 0.96875, learning_rate 0.000100023
2017-10-11T11:07:56.915574: step 2998, loss 0.212888, acc 0.9375, learning_rate 0.000100023
2017-10-11T11:07:56.990325: step 2999, loss 0.222964, acc 0.921875, learning_rate 0.000100023
2017-10-11T11:07:57.065120: step 3000, loss 0.110841, acc 0.96875, learning_rate 0.000100023

Evaluation:
2017-10-11T11:07:57.236847: step 3000, loss 0.239795, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3000

2017-10-11T11:07:57.810238: step 3001, loss 0.228251, acc 0.90625, learning_rate 0.000100023
2017-10-11T11:07:57.889379: step 3002, loss 0.303483, acc 0.953125, learning_rate 0.000100023
2017-10-11T11:07:57.958715: step 3003, loss 0.262475, acc 0.875, learning_rate 0.000100023
2017-10-11T11:07:58.032500: step 3004, loss 0.183704, acc 0.953125, learning_rate 0.000100023
2017-10-11T11:07:58.107129: step 3005, loss 0.208404, acc 0.921875, learning_rate 0.000100023
2017-10-11T11:07:58.179308: step 3006, loss 0.20272, acc 0.9375, learning_rate 0.000100023
2017-10-11T11:07:58.247977: step 3007, loss 0.257064, acc 0.90625, learning_rate 0.000100022
2017-10-11T11:07:58.316706: step 3008, loss 0.216434, acc 0.90625, learning_rate 0.000100022
2017-10-11T11:07:58.388298: step 3009, loss 0.158966, acc 0.96875, learning_rate 0.000100022
2017-10-11T11:07:58.458230: step 3010, loss 0.240739, acc 0.890625, learning_rate 0.000100022
2017-10-11T11:07:58.532780: step 3011, loss 0.0970561, acc 0.96875, learning_rate 0.000100022
2017-10-11T11:07:58.604779: step 3012, loss 0.212045, acc 0.921875, learning_rate 0.000100022
2017-10-11T11:07:58.675188: step 3013, loss 0.187532, acc 0.90625, learning_rate 0.000100022
2017-10-11T11:07:58.745701: step 3014, loss 0.18258, acc 0.953125, learning_rate 0.000100022
2017-10-11T11:07:58.817968: step 3015, loss 0.276579, acc 0.875, learning_rate 0.000100022
2017-10-11T11:07:58.890738: step 3016, loss 0.348378, acc 0.90625, learning_rate 0.000100022
2017-10-11T11:07:58.968051: step 3017, loss 0.241441, acc 0.90625, learning_rate 0.000100022
2017-10-11T11:07:59.040426: step 3018, loss 0.219007, acc 0.90625, learning_rate 0.000100021
2017-10-11T11:07:59.112273: step 3019, loss 0.297224, acc 0.9375, learning_rate 0.000100021
2017-10-11T11:07:59.185963: step 3020, loss 0.247389, acc 0.90625, learning_rate 0.000100021
2017-10-11T11:07:59.262209: step 3021, loss 0.193616, acc 0.9375, learning_rate 0.000100021
2017-10-11T11:07:59.334501: step 3022, loss 0.193277, acc 0.90625, learning_rate 0.000100021
2017-10-11T11:07:59.406697: step 3023, loss 0.389858, acc 0.90625, learning_rate 0.000100021
2017-10-11T11:07:59.479139: step 3024, loss 0.222298, acc 0.921875, learning_rate 0.000100021
2017-10-11T11:07:59.552898: step 3025, loss 0.240692, acc 0.921875, learning_rate 0.000100021
2017-10-11T11:07:59.623445: step 3026, loss 0.295374, acc 0.921875, learning_rate 0.000100021
2017-10-11T11:07:59.699377: step 3027, loss 0.165959, acc 0.953125, learning_rate 0.000100021
2017-10-11T11:07:59.771707: step 3028, loss 0.186731, acc 0.921875, learning_rate 0.000100021
2017-10-11T11:07:59.842619: step 3029, loss 0.211062, acc 0.9375, learning_rate 0.00010002
2017-10-11T11:07:59.919880: step 3030, loss 0.316111, acc 0.875, learning_rate 0.00010002
2017-10-11T11:07:59.992553: step 3031, loss 0.234723, acc 0.953125, learning_rate 0.00010002
2017-10-11T11:08:00.066417: step 3032, loss 0.191488, acc 0.953125, learning_rate 0.00010002
2017-10-11T11:08:00.136904: step 3033, loss 0.203453, acc 0.9375, learning_rate 0.00010002
2017-10-11T11:08:00.213224: step 3034, loss 0.294161, acc 0.875, learning_rate 0.00010002
2017-10-11T11:08:00.284690: step 3035, loss 0.318492, acc 0.953125, learning_rate 0.00010002
2017-10-11T11:08:00.354892: step 3036, loss 0.218204, acc 0.921875, learning_rate 0.00010002
2017-10-11T11:08:00.425014: step 3037, loss 0.117818, acc 0.984375, learning_rate 0.00010002
2017-10-11T11:08:00.489507: step 3038, loss 0.248476, acc 0.921569, learning_rate 0.00010002
2017-10-11T11:08:00.560518: step 3039, loss 0.262151, acc 0.921875, learning_rate 0.00010002
2017-10-11T11:08:00.633404: step 3040, loss 0.358241, acc 0.859375, learning_rate 0.00010002

Evaluation:
2017-10-11T11:08:00.805360: step 3040, loss 0.239657, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3040

2017-10-11T11:08:01.372932: step 3041, loss 0.170887, acc 0.984375, learning_rate 0.00010002
2017-10-11T11:08:01.444845: step 3042, loss 0.192939, acc 0.90625, learning_rate 0.000100019
2017-10-11T11:08:01.518209: step 3043, loss 0.202176, acc 0.875, learning_rate 0.000100019
2017-10-11T11:08:01.592250: step 3044, loss 0.287775, acc 0.953125, learning_rate 0.000100019
2017-10-11T11:08:01.663785: step 3045, loss 0.173361, acc 0.953125, learning_rate 0.000100019
2017-10-11T11:08:01.737806: step 3046, loss 0.0952955, acc 0.984375, learning_rate 0.000100019
2017-10-11T11:08:01.809312: step 3047, loss 0.312913, acc 0.921875, learning_rate 0.000100019
2017-10-11T11:08:01.884806: step 3048, loss 0.186667, acc 0.9375, learning_rate 0.000100019
2017-10-11T11:08:01.958757: step 3049, loss 0.204244, acc 0.921875, learning_rate 0.000100019
2017-10-11T11:08:02.031085: step 3050, loss 0.140413, acc 0.96875, learning_rate 0.000100019
2017-10-11T11:08:02.102510: step 3051, loss 0.159118, acc 0.96875, learning_rate 0.000100019
2017-10-11T11:08:02.172727: step 3052, loss 0.152113, acc 0.96875, learning_rate 0.000100019
2017-10-11T11:08:02.246382: step 3053, loss 0.151456, acc 0.953125, learning_rate 0.000100019
2017-10-11T11:08:02.317663: step 3054, loss 0.22929, acc 0.90625, learning_rate 0.000100018
2017-10-11T11:08:02.393554: step 3055, loss 0.232104, acc 0.890625, learning_rate 0.000100018
2017-10-11T11:08:02.467534: step 3056, loss 0.141432, acc 0.984375, learning_rate 0.000100018
2017-10-11T11:08:02.542387: step 3057, loss 0.190182, acc 0.96875, learning_rate 0.000100018
2017-10-11T11:08:02.616320: step 3058, loss 0.254759, acc 0.921875, learning_rate 0.000100018
2017-10-11T11:08:02.688850: step 3059, loss 0.155675, acc 0.953125, learning_rate 0.000100018
2017-10-11T11:08:02.759031: step 3060, loss 0.207456, acc 0.9375, learning_rate 0.000100018
2017-10-11T11:08:02.833646: step 3061, loss 0.198908, acc 0.9375, learning_rate 0.000100018
2017-10-11T11:08:02.911869: step 3062, loss 0.234087, acc 0.953125, learning_rate 0.000100018
2017-10-11T11:08:02.982728: step 3063, loss 0.142862, acc 0.9375, learning_rate 0.000100018
2017-10-11T11:08:03.055909: step 3064, loss 0.130802, acc 0.96875, learning_rate 0.000100018
2017-10-11T11:08:03.126662: step 3065, loss 0.205773, acc 0.953125, learning_rate 0.000100018
2017-10-11T11:08:03.214918: step 3066, loss 0.206775, acc 0.921875, learning_rate 0.000100018
2017-10-11T11:08:03.289264: step 3067, loss 0.164182, acc 0.953125, learning_rate 0.000100018
2017-10-11T11:08:03.363268: step 3068, loss 0.200071, acc 0.921875, learning_rate 0.000100017
2017-10-11T11:08:03.433337: step 3069, loss 0.233793, acc 0.9375, learning_rate 0.000100017
2017-10-11T11:08:03.502359: step 3070, loss 0.203768, acc 0.953125, learning_rate 0.000100017
2017-10-11T11:08:03.572814: step 3071, loss 0.291393, acc 0.890625, learning_rate 0.000100017
2017-10-11T11:08:03.646809: step 3072, loss 0.27818, acc 0.890625, learning_rate 0.000100017
2017-10-11T11:08:03.717479: step 3073, loss 0.249214, acc 0.953125, learning_rate 0.000100017
2017-10-11T11:08:03.788219: step 3074, loss 0.282254, acc 0.90625, learning_rate 0.000100017
2017-10-11T11:08:03.862752: step 3075, loss 0.248175, acc 0.96875, learning_rate 0.000100017
2017-10-11T11:08:03.934517: step 3076, loss 0.219162, acc 0.890625, learning_rate 0.000100017
2017-10-11T11:08:04.004417: step 3077, loss 0.151021, acc 0.984375, learning_rate 0.000100017
2017-10-11T11:08:04.074775: step 3078, loss 0.247448, acc 0.921875, learning_rate 0.000100017
2017-10-11T11:08:04.145702: step 3079, loss 0.283132, acc 0.875, learning_rate 0.000100017
2017-10-11T11:08:04.218135: step 3080, loss 0.161151, acc 0.96875, learning_rate 0.000100017

Evaluation:
2017-10-11T11:08:04.398377: step 3080, loss 0.238849, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3080

2017-10-11T11:08:05.125986: step 3081, loss 0.320208, acc 0.875, learning_rate 0.000100017
2017-10-11T11:08:05.199046: step 3082, loss 0.201068, acc 0.921875, learning_rate 0.000100016
2017-10-11T11:08:05.274490: step 3083, loss 0.142874, acc 0.9375, learning_rate 0.000100016
2017-10-11T11:08:05.352718: step 3084, loss 0.247393, acc 0.9375, learning_rate 0.000100016
2017-10-11T11:08:05.425932: step 3085, loss 0.168433, acc 0.953125, learning_rate 0.000100016
2017-10-11T11:08:05.494721: step 3086, loss 0.329231, acc 0.859375, learning_rate 0.000100016
2017-10-11T11:08:05.563773: step 3087, loss 0.352993, acc 0.890625, learning_rate 0.000100016
2017-10-11T11:08:05.642679: step 3088, loss 0.240934, acc 0.9375, learning_rate 0.000100016
2017-10-11T11:08:05.715597: step 3089, loss 0.242304, acc 0.921875, learning_rate 0.000100016
2017-10-11T11:08:05.789051: step 3090, loss 0.19502, acc 0.90625, learning_rate 0.000100016
2017-10-11T11:08:05.865965: step 3091, loss 0.224887, acc 0.96875, learning_rate 0.000100016
2017-10-11T11:08:05.937541: step 3092, loss 0.278253, acc 0.90625, learning_rate 0.000100016
2017-10-11T11:08:06.009582: step 3093, loss 0.0948416, acc 0.96875, learning_rate 0.000100016
2017-10-11T11:08:06.080366: step 3094, loss 0.257728, acc 0.890625, learning_rate 0.000100016
2017-10-11T11:08:06.154098: step 3095, loss 0.230958, acc 0.921875, learning_rate 0.000100016
2017-10-11T11:08:06.226652: step 3096, loss 0.266724, acc 0.953125, learning_rate 0.000100016
2017-10-11T11:08:06.300141: step 3097, loss 0.165955, acc 0.9375, learning_rate 0.000100016
2017-10-11T11:08:06.371337: step 3098, loss 0.203881, acc 0.953125, learning_rate 0.000100015
2017-10-11T11:08:06.447354: step 3099, loss 0.28782, acc 0.890625, learning_rate 0.000100015
2017-10-11T11:08:06.519746: step 3100, loss 0.259699, acc 0.921875, learning_rate 0.000100015
2017-10-11T11:08:06.594809: step 3101, loss 0.217492, acc 0.921875, learning_rate 0.000100015
2017-10-11T11:08:06.669764: step 3102, loss 0.160079, acc 0.953125, learning_rate 0.000100015
2017-10-11T11:08:06.745702: step 3103, loss 0.314387, acc 0.921875, learning_rate 0.000100015
2017-10-11T11:08:06.818708: step 3104, loss 0.169786, acc 0.9375, learning_rate 0.000100015
2017-10-11T11:08:06.894582: step 3105, loss 0.214445, acc 0.890625, learning_rate 0.000100015
2017-10-11T11:08:06.975648: step 3106, loss 0.206291, acc 0.9375, learning_rate 0.000100015
2017-10-11T11:08:07.053305: step 3107, loss 0.152616, acc 0.96875, learning_rate 0.000100015
2017-10-11T11:08:07.128770: step 3108, loss 0.10777, acc 0.984375, learning_rate 0.000100015
2017-10-11T11:08:07.202883: step 3109, loss 0.13475, acc 0.953125, learning_rate 0.000100015
2017-10-11T11:08:07.275971: step 3110, loss 0.281263, acc 0.890625, learning_rate 0.000100015
2017-10-11T11:08:07.348564: step 3111, loss 0.283824, acc 0.90625, learning_rate 0.000100015
2017-10-11T11:08:07.423472: step 3112, loss 0.201936, acc 0.9375, learning_rate 0.000100015
2017-10-11T11:08:07.493829: step 3113, loss 0.211477, acc 0.921875, learning_rate 0.000100015
2017-10-11T11:08:07.566419: step 3114, loss 0.221377, acc 0.921875, learning_rate 0.000100014
2017-10-11T11:08:07.637229: step 3115, loss 0.200308, acc 0.9375, learning_rate 0.000100014
2017-10-11T11:08:07.709244: step 3116, loss 0.133803, acc 0.984375, learning_rate 0.000100014
2017-10-11T11:08:07.778564: step 3117, loss 0.306139, acc 0.921875, learning_rate 0.000100014
2017-10-11T11:08:07.853883: step 3118, loss 0.110408, acc 1, learning_rate 0.000100014
2017-10-11T11:08:07.924823: step 3119, loss 0.160225, acc 0.953125, learning_rate 0.000100014
2017-10-11T11:08:07.998579: step 3120, loss 0.365766, acc 0.84375, learning_rate 0.000100014

Evaluation:
2017-10-11T11:08:08.178666: step 3120, loss 0.238366, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3120

2017-10-11T11:08:08.682343: step 3121, loss 0.142185, acc 0.96875, learning_rate 0.000100014
2017-10-11T11:08:08.753542: step 3122, loss 0.205496, acc 0.921875, learning_rate 0.000100014
2017-10-11T11:08:08.823591: step 3123, loss 0.296014, acc 0.890625, learning_rate 0.000100014
2017-10-11T11:08:08.895906: step 3124, loss 0.249925, acc 0.90625, learning_rate 0.000100014
2017-10-11T11:08:08.968172: step 3125, loss 0.114116, acc 0.953125, learning_rate 0.000100014
2017-10-11T11:08:09.043673: step 3126, loss 0.380789, acc 0.875, learning_rate 0.000100014
2017-10-11T11:08:09.117806: step 3127, loss 0.124349, acc 0.953125, learning_rate 0.000100014
2017-10-11T11:08:09.190874: step 3128, loss 0.151307, acc 0.953125, learning_rate 0.000100014
2017-10-11T11:08:09.262590: step 3129, loss 0.259135, acc 0.921875, learning_rate 0.000100014
2017-10-11T11:08:09.334828: step 3130, loss 0.143612, acc 0.953125, learning_rate 0.000100014
2017-10-11T11:08:09.405577: step 3131, loss 0.149171, acc 0.953125, learning_rate 0.000100014
2017-10-11T11:08:09.476980: step 3132, loss 0.140139, acc 0.96875, learning_rate 0.000100013
2017-10-11T11:08:09.552222: step 3133, loss 0.143449, acc 0.953125, learning_rate 0.000100013
2017-10-11T11:08:09.624170: step 3134, loss 0.220888, acc 0.953125, learning_rate 0.000100013
2017-10-11T11:08:09.701418: step 3135, loss 0.154772, acc 0.9375, learning_rate 0.000100013
2017-10-11T11:08:09.764669: step 3136, loss 0.227284, acc 0.960784, learning_rate 0.000100013
2017-10-11T11:08:09.836328: step 3137, loss 0.314634, acc 0.921875, learning_rate 0.000100013
2017-10-11T11:08:09.910052: step 3138, loss 0.131421, acc 0.96875, learning_rate 0.000100013
2017-10-11T11:08:09.983273: step 3139, loss 0.200355, acc 0.9375, learning_rate 0.000100013
2017-10-11T11:08:10.058820: step 3140, loss 0.160592, acc 0.953125, learning_rate 0.000100013
2017-10-11T11:08:10.135743: step 3141, loss 0.181611, acc 0.9375, learning_rate 0.000100013
2017-10-11T11:08:10.210957: step 3142, loss 0.158969, acc 0.9375, learning_rate 0.000100013
2017-10-11T11:08:10.286051: step 3143, loss 0.141297, acc 0.9375, learning_rate 0.000100013
2017-10-11T11:08:10.358668: step 3144, loss 0.255776, acc 0.90625, learning_rate 0.000100013
2017-10-11T11:08:10.432888: step 3145, loss 0.13075, acc 0.984375, learning_rate 0.000100013
2017-10-11T11:08:10.510037: step 3146, loss 0.117296, acc 0.96875, learning_rate 0.000100013
2017-10-11T11:08:10.583228: step 3147, loss 0.238836, acc 0.890625, learning_rate 0.000100013
2017-10-11T11:08:10.658144: step 3148, loss 0.229312, acc 0.921875, learning_rate 0.000100013
2017-10-11T11:08:10.730665: step 3149, loss 0.189105, acc 0.984375, learning_rate 0.000100013
2017-10-11T11:08:10.810935: step 3150, loss 0.174274, acc 0.953125, learning_rate 0.000100012
2017-10-11T11:08:10.895998: step 3151, loss 0.172509, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:08:10.966851: step 3152, loss 0.154226, acc 0.96875, learning_rate 0.000100012
2017-10-11T11:08:11.038656: step 3153, loss 0.297156, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:08:11.111000: step 3154, loss 0.201077, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:08:11.183088: step 3155, loss 0.235765, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:08:11.253263: step 3156, loss 0.170571, acc 0.953125, learning_rate 0.000100012
2017-10-11T11:08:11.325147: step 3157, loss 0.238189, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:08:11.400211: step 3158, loss 0.209473, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:08:11.472527: step 3159, loss 0.294782, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:08:11.542771: step 3160, loss 0.228098, acc 0.890625, learning_rate 0.000100012

Evaluation:
2017-10-11T11:08:11.722655: step 3160, loss 0.239137, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3160

2017-10-11T11:08:12.302930: step 3161, loss 0.251008, acc 0.90625, learning_rate 0.000100012
2017-10-11T11:08:12.374317: step 3162, loss 0.268663, acc 0.90625, learning_rate 0.000100012
2017-10-11T11:08:12.446201: step 3163, loss 0.254765, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:08:12.519945: step 3164, loss 0.229177, acc 0.90625, learning_rate 0.000100012
2017-10-11T11:08:12.598385: step 3165, loss 0.243723, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:08:12.677277: step 3166, loss 0.287643, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:08:12.752801: step 3167, loss 0.404754, acc 0.875, learning_rate 0.000100012
2017-10-11T11:08:12.825351: step 3168, loss 0.212591, acc 0.96875, learning_rate 0.000100012
2017-10-11T11:08:12.901753: step 3169, loss 0.296935, acc 0.859375, learning_rate 0.000100012
2017-10-11T11:08:12.974548: step 3170, loss 0.153593, acc 0.96875, learning_rate 0.000100012
2017-10-11T11:08:13.044589: step 3171, loss 0.240424, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:08:13.114777: step 3172, loss 0.15235, acc 0.9375, learning_rate 0.000100011
2017-10-11T11:08:13.187676: step 3173, loss 0.199935, acc 0.9375, learning_rate 0.000100011
2017-10-11T11:08:13.257772: step 3174, loss 0.231704, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:08:13.330711: step 3175, loss 0.218086, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:08:13.402726: step 3176, loss 0.265188, acc 0.9375, learning_rate 0.000100011
2017-10-11T11:08:13.477710: step 3177, loss 0.15969, acc 0.96875, learning_rate 0.000100011
2017-10-11T11:08:13.548171: step 3178, loss 0.165192, acc 0.953125, learning_rate 0.000100011
2017-10-11T11:08:13.621584: step 3179, loss 0.156465, acc 0.953125, learning_rate 0.000100011
2017-10-11T11:08:13.694237: step 3180, loss 0.240085, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:08:13.765746: step 3181, loss 0.141635, acc 0.9375, learning_rate 0.000100011
2017-10-11T11:08:13.841227: step 3182, loss 0.152651, acc 0.9375, learning_rate 0.000100011
2017-10-11T11:08:13.913554: step 3183, loss 0.184415, acc 0.921875, learning_rate 0.000100011
2017-10-11T11:08:13.985176: step 3184, loss 0.238591, acc 0.921875, learning_rate 0.000100011
2017-10-11T11:08:14.055445: step 3185, loss 0.201589, acc 0.953125, learning_rate 0.000100011
2017-10-11T11:08:14.130031: step 3186, loss 0.18516, acc 0.9375, learning_rate 0.000100011
2017-10-11T11:08:14.201907: step 3187, loss 0.303065, acc 0.921875, learning_rate 0.000100011
2017-10-11T11:08:14.276694: step 3188, loss 0.299711, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:08:14.348785: step 3189, loss 0.175604, acc 0.953125, learning_rate 0.000100011
2017-10-11T11:08:14.420265: step 3190, loss 0.291141, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:08:14.495134: step 3191, loss 0.353402, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:08:14.566771: step 3192, loss 0.190606, acc 0.921875, learning_rate 0.000100011
2017-10-11T11:08:14.636972: step 3193, loss 0.177297, acc 0.953125, learning_rate 0.00010001
2017-10-11T11:08:14.707287: step 3194, loss 0.159159, acc 0.96875, learning_rate 0.00010001
2017-10-11T11:08:14.778170: step 3195, loss 0.36399, acc 0.890625, learning_rate 0.00010001
2017-10-11T11:08:14.853123: step 3196, loss 0.195907, acc 0.90625, learning_rate 0.00010001
2017-10-11T11:08:14.925109: step 3197, loss 0.117552, acc 0.96875, learning_rate 0.00010001
2017-10-11T11:08:14.996172: step 3198, loss 0.308371, acc 0.90625, learning_rate 0.00010001
2017-10-11T11:08:15.065344: step 3199, loss 0.129476, acc 0.921875, learning_rate 0.00010001
2017-10-11T11:08:15.135374: step 3200, loss 0.171251, acc 0.9375, learning_rate 0.00010001

Evaluation:
2017-10-11T11:08:15.308107: step 3200, loss 0.23966, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3200

2017-10-11T11:08:15.987231: step 3201, loss 0.266898, acc 0.921875, learning_rate 0.00010001
2017-10-11T11:08:16.060815: step 3202, loss 0.184817, acc 0.9375, learning_rate 0.00010001
2017-10-11T11:08:16.131121: step 3203, loss 0.170697, acc 0.953125, learning_rate 0.00010001
2017-10-11T11:08:16.199805: step 3204, loss 0.279452, acc 0.875, learning_rate 0.00010001
2017-10-11T11:08:16.276489: step 3205, loss 0.123454, acc 0.984375, learning_rate 0.00010001
2017-10-11T11:08:16.349985: step 3206, loss 0.316346, acc 0.890625, learning_rate 0.00010001
2017-10-11T11:08:16.422340: step 3207, loss 0.243553, acc 0.890625, learning_rate 0.00010001
2017-10-11T11:08:16.492684: step 3208, loss 0.221614, acc 0.921875, learning_rate 0.00010001
2017-10-11T11:08:16.562195: step 3209, loss 0.282391, acc 0.859375, learning_rate 0.00010001
2017-10-11T11:08:16.633505: step 3210, loss 0.281234, acc 0.875, learning_rate 0.00010001
2017-10-11T11:08:16.704953: step 3211, loss 0.288107, acc 0.875, learning_rate 0.00010001
2017-10-11T11:08:16.774917: step 3212, loss 0.145725, acc 0.96875, learning_rate 0.00010001
2017-10-11T11:08:16.846408: step 3213, loss 0.201829, acc 0.9375, learning_rate 0.00010001
2017-10-11T11:08:16.919420: step 3214, loss 0.121368, acc 0.96875, learning_rate 0.00010001
2017-10-11T11:08:16.997282: step 3215, loss 0.194753, acc 0.90625, learning_rate 0.00010001
2017-10-11T11:08:17.070522: step 3216, loss 0.223244, acc 0.9375, learning_rate 0.00010001
2017-10-11T11:08:17.138321: step 3217, loss 0.227012, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:08:17.212939: step 3218, loss 0.264755, acc 0.875, learning_rate 0.000100009
2017-10-11T11:08:17.286577: step 3219, loss 0.130431, acc 0.96875, learning_rate 0.000100009
2017-10-11T11:08:17.362168: step 3220, loss 0.325323, acc 0.8125, learning_rate 0.000100009
2017-10-11T11:08:17.434077: step 3221, loss 0.289034, acc 0.90625, learning_rate 0.000100009
2017-10-11T11:08:17.504901: step 3222, loss 0.139845, acc 0.984375, learning_rate 0.000100009
2017-10-11T11:08:17.576859: step 3223, loss 0.0993736, acc 1, learning_rate 0.000100009
2017-10-11T11:08:17.652506: step 3224, loss 0.215631, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:08:17.722660: step 3225, loss 0.223915, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:08:17.794796: step 3226, loss 0.106929, acc 0.984375, learning_rate 0.000100009
2017-10-11T11:08:17.868516: step 3227, loss 0.31873, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:08:17.940387: step 3228, loss 0.122353, acc 0.96875, learning_rate 0.000100009
2017-10-11T11:08:18.013605: step 3229, loss 0.211591, acc 0.90625, learning_rate 0.000100009
2017-10-11T11:08:18.085980: step 3230, loss 0.332485, acc 0.90625, learning_rate 0.000100009
2017-10-11T11:08:18.158800: step 3231, loss 0.298225, acc 0.90625, learning_rate 0.000100009
2017-10-11T11:08:18.233549: step 3232, loss 0.215229, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:08:18.305072: step 3233, loss 0.163738, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:08:18.371292: step 3234, loss 0.290144, acc 0.882353, learning_rate 0.000100009
2017-10-11T11:08:18.442492: step 3235, loss 0.155076, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:08:18.517940: step 3236, loss 0.231, acc 0.890625, learning_rate 0.000100009
2017-10-11T11:08:18.596065: step 3237, loss 0.245732, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:08:18.668630: step 3238, loss 0.12216, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:08:18.745382: step 3239, loss 0.200947, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:08:18.818056: step 3240, loss 0.17945, acc 0.9375, learning_rate 0.000100009

Evaluation:
2017-10-11T11:08:18.990025: step 3240, loss 0.239088, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3240

2017-10-11T11:08:19.599849: step 3241, loss 0.193621, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:08:19.669879: step 3242, loss 0.182244, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:08:19.741092: step 3243, loss 0.247603, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:08:19.811291: step 3244, loss 0.146262, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:08:19.883592: step 3245, loss 0.230039, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:08:19.956071: step 3246, loss 0.120592, acc 0.984375, learning_rate 0.000100008
2017-10-11T11:08:20.026777: step 3247, loss 0.184793, acc 0.953125, learning_rate 0.000100008
2017-10-11T11:08:20.095991: step 3248, loss 0.157674, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:08:20.169930: step 3249, loss 0.156391, acc 0.96875, learning_rate 0.000100008
2017-10-11T11:08:20.237816: step 3250, loss 0.120877, acc 0.953125, learning_rate 0.000100008
2017-10-11T11:08:20.311170: step 3251, loss 0.178019, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:08:20.380116: step 3252, loss 0.283637, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:08:20.454981: step 3253, loss 0.201735, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:08:20.528773: step 3254, loss 0.289789, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:08:20.602083: step 3255, loss 0.240374, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:08:20.673967: step 3256, loss 0.165507, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:08:20.748126: step 3257, loss 0.199197, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:08:20.818431: step 3258, loss 0.218298, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:08:20.889969: step 3259, loss 0.324706, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:08:20.961283: step 3260, loss 0.180659, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:08:21.032500: step 3261, loss 0.173588, acc 0.953125, learning_rate 0.000100008
2017-10-11T11:08:21.105501: step 3262, loss 0.263786, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:08:21.182017: step 3263, loss 0.1973, acc 0.953125, learning_rate 0.000100008
2017-10-11T11:08:21.254911: step 3264, loss 0.324265, acc 0.859375, learning_rate 0.000100008
2017-10-11T11:08:21.328370: step 3265, loss 0.300785, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:08:21.401456: step 3266, loss 0.088486, acc 0.96875, learning_rate 0.000100008
2017-10-11T11:08:21.472450: step 3267, loss 0.253138, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:08:21.541929: step 3268, loss 0.141991, acc 0.953125, learning_rate 0.000100008
2017-10-11T11:08:21.615709: step 3269, loss 0.271685, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:08:21.688870: step 3270, loss 0.344353, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:08:21.762819: step 3271, loss 0.252955, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:08:21.833248: step 3272, loss 0.165831, acc 0.96875, learning_rate 0.000100008
2017-10-11T11:08:21.919281: step 3273, loss 0.181847, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:08:21.997242: step 3274, loss 0.173915, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:08:22.071032: step 3275, loss 0.260061, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:08:22.142960: step 3276, loss 0.127807, acc 0.984375, learning_rate 0.000100007
2017-10-11T11:08:22.217078: step 3277, loss 0.140077, acc 0.984375, learning_rate 0.000100007
2017-10-11T11:08:22.287036: step 3278, loss 0.194657, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:08:22.357150: step 3279, loss 0.334387, acc 0.84375, learning_rate 0.000100007
2017-10-11T11:08:22.446645: step 3280, loss 0.182896, acc 0.9375, learning_rate 0.000100007

Evaluation:
2017-10-11T11:08:22.626195: step 3280, loss 0.23875, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3280

2017-10-11T11:08:23.198764: step 3281, loss 0.144501, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:08:23.270270: step 3282, loss 0.176111, acc 0.96875, learning_rate 0.000100007
2017-10-11T11:08:23.340362: step 3283, loss 0.230457, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:08:23.413534: step 3284, loss 0.172218, acc 0.96875, learning_rate 0.000100007
2017-10-11T11:08:23.484823: step 3285, loss 0.209617, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:08:23.554555: step 3286, loss 0.309449, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:08:23.638883: step 3287, loss 0.201795, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:08:23.720058: step 3288, loss 0.0964827, acc 1, learning_rate 0.000100007
2017-10-11T11:08:23.791553: step 3289, loss 0.253281, acc 0.90625, learning_rate 0.000100007
2017-10-11T11:08:23.869997: step 3290, loss 0.27567, acc 0.890625, learning_rate 0.000100007
2017-10-11T11:08:23.946295: step 3291, loss 0.100753, acc 0.984375, learning_rate 0.000100007
2017-10-11T11:08:24.019426: step 3292, loss 0.330071, acc 0.890625, learning_rate 0.000100007
2017-10-11T11:08:24.089226: step 3293, loss 0.178642, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:08:24.159245: step 3294, loss 0.218798, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:08:24.235506: step 3295, loss 0.246203, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:08:24.308384: step 3296, loss 0.114352, acc 0.96875, learning_rate 0.000100007
2017-10-11T11:08:24.380430: step 3297, loss 0.300978, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:08:24.450864: step 3298, loss 0.369462, acc 0.890625, learning_rate 0.000100007
2017-10-11T11:08:24.518201: step 3299, loss 0.128734, acc 0.984375, learning_rate 0.000100007
2017-10-11T11:08:24.590811: step 3300, loss 0.18416, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:08:24.662584: step 3301, loss 0.17033, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:08:24.736838: step 3302, loss 0.138234, acc 0.96875, learning_rate 0.000100007
2017-10-11T11:08:24.813496: step 3303, loss 0.188403, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:08:24.890530: step 3304, loss 0.165868, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:08:24.959417: step 3305, loss 0.312688, acc 0.890625, learning_rate 0.000100007
2017-10-11T11:08:25.032074: step 3306, loss 0.19293, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:08:25.106787: step 3307, loss 0.18544, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:08:25.178800: step 3308, loss 0.241656, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:08:25.249575: step 3309, loss 0.446416, acc 0.84375, learning_rate 0.000100007
2017-10-11T11:08:25.327309: step 3310, loss 0.183859, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:08:25.397276: step 3311, loss 0.177711, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:08:25.470459: step 3312, loss 0.170611, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:25.540970: step 3313, loss 0.213097, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:08:25.611390: step 3314, loss 0.22556, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:25.681020: step 3315, loss 0.240912, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:08:25.752754: step 3316, loss 0.330434, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:08:25.822075: step 3317, loss 0.209022, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:25.895420: step 3318, loss 0.192885, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:25.968012: step 3319, loss 0.173516, acc 0.96875, learning_rate 0.000100006
2017-10-11T11:08:26.041954: step 3320, loss 0.2483, acc 0.9375, learning_rate 0.000100006

Evaluation:
2017-10-11T11:08:26.235334: step 3320, loss 0.236086, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3320

2017-10-11T11:08:26.887269: step 3321, loss 0.188611, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:26.956198: step 3322, loss 0.17679, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:27.027143: step 3323, loss 0.241264, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:08:27.101140: step 3324, loss 0.180489, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:08:27.170973: step 3325, loss 0.181645, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:08:27.241610: step 3326, loss 0.197715, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:27.313772: step 3327, loss 0.15809, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:27.388341: step 3328, loss 0.169866, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:27.460276: step 3329, loss 0.315084, acc 0.859375, learning_rate 0.000100006
2017-10-11T11:08:27.530711: step 3330, loss 0.269633, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:08:27.600488: step 3331, loss 0.443021, acc 0.859375, learning_rate 0.000100006
2017-10-11T11:08:27.662546: step 3332, loss 0.251014, acc 0.921569, learning_rate 0.000100006
2017-10-11T11:08:27.735870: step 3333, loss 0.16733, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:08:27.809812: step 3334, loss 0.188102, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:08:27.890243: step 3335, loss 0.237628, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:08:27.959182: step 3336, loss 0.214947, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:28.032773: step 3337, loss 0.189211, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:28.110361: step 3338, loss 0.182004, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:28.183098: step 3339, loss 0.173084, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:28.252335: step 3340, loss 0.153427, acc 0.96875, learning_rate 0.000100006
2017-10-11T11:08:28.324121: step 3341, loss 0.164302, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:28.395278: step 3342, loss 0.142116, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:28.465754: step 3343, loss 0.183546, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:08:28.536790: step 3344, loss 0.140329, acc 0.984375, learning_rate 0.000100006
2017-10-11T11:08:28.612226: step 3345, loss 0.17215, acc 0.96875, learning_rate 0.000100006
2017-10-11T11:08:28.687723: step 3346, loss 0.169837, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:28.759101: step 3347, loss 0.327038, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:08:28.832126: step 3348, loss 0.17827, acc 0.96875, learning_rate 0.000100006
2017-10-11T11:08:28.914244: step 3349, loss 0.182936, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:08:28.984551: step 3350, loss 0.14379, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:08:29.062373: step 3351, loss 0.210972, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:29.137017: step 3352, loss 0.249669, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:29.210184: step 3353, loss 0.148604, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:08:29.283635: step 3354, loss 0.143505, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:29.358865: step 3355, loss 0.173008, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:29.427479: step 3356, loss 0.154967, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:08:29.502723: step 3357, loss 0.0823619, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:08:29.572394: step 3358, loss 0.199023, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:08:29.641780: step 3359, loss 0.20689, acc 0.890625, learning_rate 0.000100005
2017-10-11T11:08:29.713326: step 3360, loss 0.248963, acc 0.90625, learning_rate 0.000100005

Evaluation:
2017-10-11T11:08:29.898921: step 3360, loss 0.236775, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3360

2017-10-11T11:08:30.395928: step 3361, loss 0.18867, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:08:30.470198: step 3362, loss 0.215085, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:30.547649: step 3363, loss 0.126384, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:30.618356: step 3364, loss 0.293048, acc 0.890625, learning_rate 0.000100005
2017-10-11T11:08:30.691457: step 3365, loss 0.231272, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:30.764789: step 3366, loss 0.249207, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:08:30.835944: step 3367, loss 0.256105, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:30.905757: step 3368, loss 0.277378, acc 0.875, learning_rate 0.000100005
2017-10-11T11:08:30.979008: step 3369, loss 0.103817, acc 0.984375, learning_rate 0.000100005
2017-10-11T11:08:31.052694: step 3370, loss 0.155531, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:31.124730: step 3371, loss 0.212376, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:31.196651: step 3372, loss 0.177913, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:08:31.267934: step 3373, loss 0.312143, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:31.336753: step 3374, loss 0.213722, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:31.413188: step 3375, loss 0.335718, acc 0.875, learning_rate 0.000100005
2017-10-11T11:08:31.483169: step 3376, loss 0.294883, acc 0.875, learning_rate 0.000100005
2017-10-11T11:08:31.550765: step 3377, loss 0.211797, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:31.623681: step 3378, loss 0.409635, acc 0.796875, learning_rate 0.000100005
2017-10-11T11:08:31.693977: step 3379, loss 0.196249, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:08:31.764569: step 3380, loss 0.143904, acc 0.984375, learning_rate 0.000100005
2017-10-11T11:08:31.833612: step 3381, loss 0.218415, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:31.906852: step 3382, loss 0.311564, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:31.979581: step 3383, loss 0.172231, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:32.055218: step 3384, loss 0.223717, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:08:32.129815: step 3385, loss 0.154412, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:32.202540: step 3386, loss 0.240413, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:08:32.272181: step 3387, loss 0.146764, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:32.345608: step 3388, loss 0.143613, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:32.419577: step 3389, loss 0.248442, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:08:32.489500: step 3390, loss 0.259757, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:08:32.560872: step 3391, loss 0.20209, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:08:32.630816: step 3392, loss 0.192163, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:08:32.701310: step 3393, loss 0.183328, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:08:32.772276: step 3394, loss 0.260544, acc 0.890625, learning_rate 0.000100005
2017-10-11T11:08:32.847387: step 3395, loss 0.0933555, acc 1, learning_rate 0.000100005
2017-10-11T11:08:32.930151: step 3396, loss 0.145006, acc 0.984375, learning_rate 0.000100005
2017-10-11T11:08:33.000175: step 3397, loss 0.233344, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:33.073369: step 3398, loss 0.271496, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:08:33.145238: step 3399, loss 0.149321, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:08:33.218214: step 3400, loss 0.231393, acc 0.921875, learning_rate 0.000100004

Evaluation:
2017-10-11T11:08:33.387048: step 3400, loss 0.236945, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3400

2017-10-11T11:08:33.967539: step 3401, loss 0.132819, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:34.041339: step 3402, loss 0.234611, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:08:34.115273: step 3403, loss 0.117056, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:08:34.185514: step 3404, loss 0.195664, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:08:34.253301: step 3405, loss 0.194768, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:34.324959: step 3406, loss 0.353977, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:08:34.398560: step 3407, loss 0.258544, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:08:34.470519: step 3408, loss 0.222348, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:08:34.542575: step 3409, loss 0.175225, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:34.614537: step 3410, loss 0.334931, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:08:34.684127: step 3411, loss 0.109552, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:08:34.758753: step 3412, loss 0.20444, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:34.834904: step 3413, loss 0.150849, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:34.907634: step 3414, loss 0.304464, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:08:34.980353: step 3415, loss 0.171103, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:08:35.050133: step 3416, loss 0.218639, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:08:35.122084: step 3417, loss 0.189055, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:35.194439: step 3418, loss 0.188503, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:35.271267: step 3419, loss 0.214814, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:35.343706: step 3420, loss 0.129332, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:35.413000: step 3421, loss 0.235279, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:35.484115: step 3422, loss 0.161453, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:35.555637: step 3423, loss 0.170273, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:35.625683: step 3424, loss 0.160556, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:35.697918: step 3425, loss 0.296956, acc 0.859375, learning_rate 0.000100004
2017-10-11T11:08:35.769329: step 3426, loss 0.240229, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:08:35.842626: step 3427, loss 0.229421, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:08:35.914967: step 3428, loss 0.158505, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:35.988082: step 3429, loss 0.182689, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:08:36.052945: step 3430, loss 0.372214, acc 0.862745, learning_rate 0.000100004
2017-10-11T11:08:36.123027: step 3431, loss 0.121177, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:08:36.192705: step 3432, loss 0.160291, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:08:36.264139: step 3433, loss 0.134953, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:36.332667: step 3434, loss 0.137153, acc 0.984375, learning_rate 0.000100004
2017-10-11T11:08:36.404506: step 3435, loss 0.262174, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:08:36.480438: step 3436, loss 0.152814, acc 0.984375, learning_rate 0.000100004
2017-10-11T11:08:36.552773: step 3437, loss 0.26312, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:08:36.624290: step 3438, loss 0.306898, acc 0.875, learning_rate 0.000100004
2017-10-11T11:08:36.696518: step 3439, loss 0.158229, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:36.768351: step 3440, loss 0.143503, acc 0.96875, learning_rate 0.000100004

Evaluation:
2017-10-11T11:08:36.934214: step 3440, loss 0.236568, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3440

2017-10-11T11:08:37.494276: step 3441, loss 0.333042, acc 0.875, learning_rate 0.000100004
2017-10-11T11:08:37.566255: step 3442, loss 0.242322, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:08:37.639980: step 3443, loss 0.179293, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:37.714217: step 3444, loss 0.311738, acc 0.875, learning_rate 0.000100004
2017-10-11T11:08:37.787125: step 3445, loss 0.178134, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:37.862217: step 3446, loss 0.271601, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:37.932873: step 3447, loss 0.226515, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:08:38.003111: step 3448, loss 0.37808, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:08:38.072875: step 3449, loss 0.135394, acc 0.984375, learning_rate 0.000100004
2017-10-11T11:08:38.144051: step 3450, loss 0.16318, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:38.214504: step 3451, loss 0.0859181, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:08:38.282894: step 3452, loss 0.279661, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:08:38.351803: step 3453, loss 0.175017, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:38.426163: step 3454, loss 0.166277, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:08:38.495735: step 3455, loss 0.267048, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:38.569919: step 3456, loss 0.175234, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:38.639866: step 3457, loss 0.161925, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:08:38.712222: step 3458, loss 0.291996, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:08:38.783176: step 3459, loss 0.124652, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:38.854967: step 3460, loss 0.289806, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:08:38.925178: step 3461, loss 0.293245, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:08:38.997118: step 3462, loss 0.250101, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:08:39.067605: step 3463, loss 0.248281, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:08:39.143455: step 3464, loss 0.233914, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:39.217753: step 3465, loss 0.205024, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:39.287604: step 3466, loss 0.293513, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:08:39.361921: step 3467, loss 0.300399, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:08:39.432207: step 3468, loss 0.268341, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:08:39.504483: step 3469, loss 0.124556, acc 0.984375, learning_rate 0.000100003
2017-10-11T11:08:39.577433: step 3470, loss 0.179711, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:39.652961: step 3471, loss 0.295166, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:08:39.725906: step 3472, loss 0.122381, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:39.797314: step 3473, loss 0.213337, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:39.881158: step 3474, loss 0.352853, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:08:39.956846: step 3475, loss 0.130171, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:40.026541: step 3476, loss 0.120879, acc 1, learning_rate 0.000100003
2017-10-11T11:08:40.097554: step 3477, loss 0.200567, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:40.172382: step 3478, loss 0.218076, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:08:40.247532: step 3479, loss 0.164231, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:40.323214: step 3480, loss 0.278717, acc 0.90625, learning_rate 0.000100003

Evaluation:
2017-10-11T11:08:40.467610: step 3480, loss 0.235337, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3480

2017-10-11T11:08:41.124724: step 3481, loss 0.203459, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:41.197692: step 3482, loss 0.203424, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:41.269526: step 3483, loss 0.129565, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:41.343950: step 3484, loss 0.134385, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:41.413670: step 3485, loss 0.138318, acc 0.984375, learning_rate 0.000100003
2017-10-11T11:08:41.484549: step 3486, loss 0.080862, acc 1, learning_rate 0.000100003
2017-10-11T11:08:41.556344: step 3487, loss 0.156229, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:41.631114: step 3488, loss 0.114714, acc 1, learning_rate 0.000100003
2017-10-11T11:08:41.706061: step 3489, loss 0.0956004, acc 0.984375, learning_rate 0.000100003
2017-10-11T11:08:41.781233: step 3490, loss 0.149726, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:41.862092: step 3491, loss 0.21515, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:41.938611: step 3492, loss 0.219403, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:08:42.009181: step 3493, loss 0.162338, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:42.079699: step 3494, loss 0.176261, acc 0.984375, learning_rate 0.000100003
2017-10-11T11:08:42.152773: step 3495, loss 0.353352, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:08:42.222446: step 3496, loss 0.0899574, acc 1, learning_rate 0.000100003
2017-10-11T11:08:42.293204: step 3497, loss 0.10943, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:42.364771: step 3498, loss 0.328783, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:42.440990: step 3499, loss 0.127925, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:42.512609: step 3500, loss 0.336996, acc 0.875, learning_rate 0.000100003
2017-10-11T11:08:42.585262: step 3501, loss 0.191896, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:42.655815: step 3502, loss 0.236522, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:08:42.732688: step 3503, loss 0.222099, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:42.805190: step 3504, loss 0.196336, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:42.879475: step 3505, loss 0.159983, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:42.951458: step 3506, loss 0.222536, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:43.021999: step 3507, loss 0.20105, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:43.096860: step 3508, loss 0.24082, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:43.169428: step 3509, loss 0.265385, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:43.241170: step 3510, loss 0.238987, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:08:43.308687: step 3511, loss 0.234842, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:43.380863: step 3512, loss 0.242, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:08:43.456176: step 3513, loss 0.136492, acc 0.984375, learning_rate 0.000100003
2017-10-11T11:08:43.527010: step 3514, loss 0.239824, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:43.597803: step 3515, loss 0.302015, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:08:43.668552: step 3516, loss 0.13602, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:43.739509: step 3517, loss 0.195916, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:43.812163: step 3518, loss 0.17225, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:43.883776: step 3519, loss 0.236507, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:43.954614: step 3520, loss 0.190886, acc 0.9375, learning_rate 0.000100003

Evaluation:
2017-10-11T11:08:44.099823: step 3520, loss 0.235588, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3520

2017-10-11T11:08:44.596167: step 3521, loss 0.251291, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:44.664003: step 3522, loss 0.269476, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:44.737816: step 3523, loss 0.239254, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:44.810930: step 3524, loss 0.141206, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:44.882661: step 3525, loss 0.150714, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:44.962274: step 3526, loss 0.177384, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:45.033682: step 3527, loss 0.239943, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:08:45.098954: step 3528, loss 0.0963105, acc 0.960784, learning_rate 0.000100003
2017-10-11T11:08:45.168559: step 3529, loss 0.301202, acc 0.875, learning_rate 0.000100003
2017-10-11T11:08:45.240130: step 3530, loss 0.196158, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:08:45.311225: step 3531, loss 0.292423, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:08:45.383590: step 3532, loss 0.148566, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:45.457166: step 3533, loss 0.220033, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:08:45.530093: step 3534, loss 0.313174, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:08:45.602466: step 3535, loss 0.131977, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:45.677911: step 3536, loss 0.183161, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:45.750622: step 3537, loss 0.143806, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:45.825298: step 3538, loss 0.327313, acc 0.875, learning_rate 0.000100003
2017-10-11T11:08:45.905883: step 3539, loss 0.13443, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:45.976530: step 3540, loss 0.125485, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:08:46.048269: step 3541, loss 0.238491, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:08:46.121360: step 3542, loss 0.287584, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:08:46.193531: step 3543, loss 0.222122, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:08:46.262784: step 3544, loss 0.299409, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:46.334478: step 3545, loss 0.178298, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:46.407733: step 3546, loss 0.369502, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:46.478477: step 3547, loss 0.234923, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:46.552681: step 3548, loss 0.133924, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:46.623000: step 3549, loss 0.105799, acc 1, learning_rate 0.000100002
2017-10-11T11:08:46.695038: step 3550, loss 0.241285, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:46.768516: step 3551, loss 0.120757, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:46.843882: step 3552, loss 0.0868681, acc 0.984375, learning_rate 0.000100002
2017-10-11T11:08:46.915297: step 3553, loss 0.190106, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:46.984683: step 3554, loss 0.181507, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:47.056567: step 3555, loss 0.198453, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:47.128413: step 3556, loss 0.113586, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:47.201867: step 3557, loss 0.429413, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:47.272231: step 3558, loss 0.130694, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:47.345178: step 3559, loss 0.275234, acc 0.875, learning_rate 0.000100002
2017-10-11T11:08:47.414063: step 3560, loss 0.134767, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-11T11:08:47.560086: step 3560, loss 0.236373, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3560

2017-10-11T11:08:48.126879: step 3561, loss 0.162528, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:48.202044: step 3562, loss 0.18583, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:48.273708: step 3563, loss 0.269646, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:48.352440: step 3564, loss 0.237853, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:48.424770: step 3565, loss 0.133772, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:48.496005: step 3566, loss 0.145451, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:48.567967: step 3567, loss 0.155151, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:48.639398: step 3568, loss 0.248275, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:48.708006: step 3569, loss 0.224341, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:48.781604: step 3570, loss 0.230529, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:48.855119: step 3571, loss 0.270263, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:48.930010: step 3572, loss 0.127654, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:49.002302: step 3573, loss 0.284526, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:49.076775: step 3574, loss 0.270339, acc 0.875, learning_rate 0.000100002
2017-10-11T11:08:49.147844: step 3575, loss 0.199533, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:49.217978: step 3576, loss 0.193402, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:49.288516: step 3577, loss 0.242594, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:49.359930: step 3578, loss 0.291186, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:49.428766: step 3579, loss 0.252956, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:49.501661: step 3580, loss 0.120823, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:49.574308: step 3581, loss 0.298877, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:49.643931: step 3582, loss 0.119616, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:49.717432: step 3583, loss 0.27904, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:49.788170: step 3584, loss 0.260817, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:49.860393: step 3585, loss 0.148125, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:49.932950: step 3586, loss 0.220211, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:50.002061: step 3587, loss 0.149626, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:50.071857: step 3588, loss 0.193254, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:50.144523: step 3589, loss 0.251732, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:50.213551: step 3590, loss 0.172083, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:50.285086: step 3591, loss 0.173996, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:50.358642: step 3592, loss 0.232921, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:50.431328: step 3593, loss 0.244067, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:50.503925: step 3594, loss 0.184111, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:50.575438: step 3595, loss 0.249841, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:50.644717: step 3596, loss 0.181084, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:50.714756: step 3597, loss 0.313509, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:50.784438: step 3598, loss 0.149897, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:50.862171: step 3599, loss 0.172843, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:50.952588: step 3600, loss 0.223802, acc 0.921875, learning_rate 0.000100002

Evaluation:
2017-10-11T11:08:51.098021: step 3600, loss 0.236581, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3600

2017-10-11T11:08:51.732882: step 3601, loss 0.258122, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:51.804948: step 3602, loss 0.213464, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:51.876873: step 3603, loss 0.20219, acc 0.984375, learning_rate 0.000100002
2017-10-11T11:08:51.949007: step 3604, loss 0.175103, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:52.022378: step 3605, loss 0.190719, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:52.089951: step 3606, loss 0.192554, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:52.161455: step 3607, loss 0.367433, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:08:52.236030: step 3608, loss 0.221354, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:52.309778: step 3609, loss 0.20101, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:52.379953: step 3610, loss 0.127454, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:52.450874: step 3611, loss 0.22647, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:52.524410: step 3612, loss 0.163799, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:52.597649: step 3613, loss 0.1265, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:52.670561: step 3614, loss 0.162214, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:52.743606: step 3615, loss 0.234644, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:52.812138: step 3616, loss 0.184239, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:52.884179: step 3617, loss 0.296025, acc 0.875, learning_rate 0.000100002
2017-10-11T11:08:52.951921: step 3618, loss 0.170153, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:53.025547: step 3619, loss 0.195963, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:53.098765: step 3620, loss 0.300515, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:53.171986: step 3621, loss 0.174653, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:53.247036: step 3622, loss 0.3558, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:53.316747: step 3623, loss 0.182759, acc 0.984375, learning_rate 0.000100002
2017-10-11T11:08:53.388768: step 3624, loss 0.266433, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:53.460127: step 3625, loss 0.181376, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:53.523913: step 3626, loss 0.28002, acc 0.941176, learning_rate 0.000100002
2017-10-11T11:08:53.598803: step 3627, loss 0.266728, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:53.668274: step 3628, loss 0.211017, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:53.738590: step 3629, loss 0.145906, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:53.812704: step 3630, loss 0.200977, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:53.887852: step 3631, loss 0.294358, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:53.961403: step 3632, loss 0.158058, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:54.031227: step 3633, loss 0.104602, acc 0.984375, learning_rate 0.000100002
2017-10-11T11:08:54.100244: step 3634, loss 0.301277, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:54.168470: step 3635, loss 0.208943, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:54.239471: step 3636, loss 0.251085, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:54.312205: step 3637, loss 0.0993993, acc 0.984375, learning_rate 0.000100002
2017-10-11T11:08:54.387910: step 3638, loss 0.240465, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:54.456870: step 3639, loss 0.142534, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:54.531642: step 3640, loss 0.130226, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-11T11:08:54.674156: step 3640, loss 0.236581, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3640

2017-10-11T11:08:55.184265: step 3641, loss 0.305159, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:55.259277: step 3642, loss 0.337659, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:55.331730: step 3643, loss 0.231667, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:55.404224: step 3644, loss 0.210795, acc 0.984375, learning_rate 0.000100002
2017-10-11T11:08:55.475486: step 3645, loss 0.20973, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:55.546038: step 3646, loss 0.132077, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:55.618717: step 3647, loss 0.185683, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:55.688403: step 3648, loss 0.132909, acc 1, learning_rate 0.000100002
2017-10-11T11:08:55.758978: step 3649, loss 0.210045, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:55.827090: step 3650, loss 0.174256, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:55.900384: step 3651, loss 0.277337, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:55.969810: step 3652, loss 0.247151, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:56.039259: step 3653, loss 0.147599, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:56.109075: step 3654, loss 0.195758, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:56.181683: step 3655, loss 0.320757, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:08:56.256316: step 3656, loss 0.245957, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:56.328018: step 3657, loss 0.218412, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:56.397260: step 3658, loss 0.249514, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:56.470136: step 3659, loss 0.0959245, acc 0.984375, learning_rate 0.000100002
2017-10-11T11:08:56.541763: step 3660, loss 0.228507, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:56.613972: step 3661, loss 0.167971, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:56.688598: step 3662, loss 0.210403, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:08:56.761313: step 3663, loss 0.270839, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:56.835992: step 3664, loss 0.235788, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:08:56.917753: step 3665, loss 0.163092, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:08:56.991315: step 3666, loss 0.223638, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:08:57.062197: step 3667, loss 0.179053, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:57.134752: step 3668, loss 0.143551, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:08:57.205947: step 3669, loss 0.0552767, acc 1, learning_rate 0.000100001
2017-10-11T11:08:57.278651: step 3670, loss 0.213339, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:57.357603: step 3671, loss 0.161953, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:08:57.426986: step 3672, loss 0.177756, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:08:57.501363: step 3673, loss 0.251754, acc 0.84375, learning_rate 0.000100001
2017-10-11T11:08:57.574276: step 3674, loss 0.27773, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:57.644013: step 3675, loss 0.0882237, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:08:57.716964: step 3676, loss 0.122848, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:08:57.785644: step 3677, loss 0.165005, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:08:57.859229: step 3678, loss 0.322455, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:08:57.933118: step 3679, loss 0.184805, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:08:58.010001: step 3680, loss 0.248812, acc 0.90625, learning_rate 0.000100001

Evaluation:
2017-10-11T11:08:58.154489: step 3680, loss 0.235804, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3680

2017-10-11T11:08:58.716612: step 3681, loss 0.0693749, acc 1, learning_rate 0.000100001
2017-10-11T11:08:58.784494: step 3682, loss 0.199587, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:58.858783: step 3683, loss 0.24006, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:08:58.930044: step 3684, loss 0.135745, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:08:58.999693: step 3685, loss 0.272905, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:08:59.071031: step 3686, loss 0.235655, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:08:59.140821: step 3687, loss 0.386913, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:08:59.210490: step 3688, loss 0.297234, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:59.283261: step 3689, loss 0.241897, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:08:59.361933: step 3690, loss 0.16587, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:08:59.434593: step 3691, loss 0.229388, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:59.504427: step 3692, loss 0.17116, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:59.574857: step 3693, loss 0.273029, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:08:59.642843: step 3694, loss 0.242442, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:59.712771: step 3695, loss 0.172812, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:08:59.785455: step 3696, loss 0.260295, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:08:59.857083: step 3697, loss 0.248026, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:59.928666: step 3698, loss 0.265252, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:08:59.999966: step 3699, loss 0.270329, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:00.069252: step 3700, loss 0.16444, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:00.142126: step 3701, loss 0.194522, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:00.212576: step 3702, loss 0.428556, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:00.285152: step 3703, loss 0.294354, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:00.359443: step 3704, loss 0.278372, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:00.433666: step 3705, loss 0.244242, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:00.507691: step 3706, loss 0.167522, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:00.580715: step 3707, loss 0.18883, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:00.654555: step 3708, loss 0.206414, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:00.724919: step 3709, loss 0.146181, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:00.798535: step 3710, loss 0.157131, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:00.869098: step 3711, loss 0.28112, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:00.938288: step 3712, loss 0.132654, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:01.007152: step 3713, loss 0.0950873, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:01.079911: step 3714, loss 0.213734, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:01.151269: step 3715, loss 0.22567, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:01.226149: step 3716, loss 0.169151, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:01.298770: step 3717, loss 0.191435, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:01.368940: step 3718, loss 0.109226, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:01.442212: step 3719, loss 0.129642, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:01.514638: step 3720, loss 0.232159, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-11T11:09:01.660852: step 3720, loss 0.234793, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3720

2017-10-11T11:09:02.436554: step 3721, loss 0.151625, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:02.511822: step 3722, loss 0.189633, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:02.582638: step 3723, loss 0.192608, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:02.646246: step 3724, loss 0.168449, acc 0.960784, learning_rate 0.000100001
2017-10-11T11:09:02.721051: step 3725, loss 0.18431, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:02.794906: step 3726, loss 0.185976, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:02.866877: step 3727, loss 0.172612, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:02.938687: step 3728, loss 0.149911, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:03.014364: step 3729, loss 0.226182, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:03.086936: step 3730, loss 0.38458, acc 0.875, learning_rate 0.000100001
2017-10-11T11:09:03.157788: step 3731, loss 0.269227, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:03.230910: step 3732, loss 0.339686, acc 0.875, learning_rate 0.000100001
2017-10-11T11:09:03.303890: step 3733, loss 0.200109, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:03.372735: step 3734, loss 0.198204, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:03.448174: step 3735, loss 0.185939, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:03.522875: step 3736, loss 0.180592, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:03.600804: step 3737, loss 0.157588, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:03.667627: step 3738, loss 0.211146, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:03.739528: step 3739, loss 0.110449, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:03.811383: step 3740, loss 0.237534, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:03.892466: step 3741, loss 0.194571, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:03.974329: step 3742, loss 0.306486, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:04.049634: step 3743, loss 0.191422, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:04.120124: step 3744, loss 0.269415, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:04.190168: step 3745, loss 0.194428, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:04.262101: step 3746, loss 0.139129, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:04.337475: step 3747, loss 0.254622, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:04.409729: step 3748, loss 0.187715, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:04.478770: step 3749, loss 0.163189, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:04.552915: step 3750, loss 0.120197, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:04.623905: step 3751, loss 0.209977, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:04.693345: step 3752, loss 0.231972, acc 0.875, learning_rate 0.000100001
2017-10-11T11:09:04.767263: step 3753, loss 0.204524, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:04.841976: step 3754, loss 0.148277, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:04.915325: step 3755, loss 0.139608, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:04.987970: step 3756, loss 0.192974, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:05.064754: step 3757, loss 0.230866, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:05.138106: step 3758, loss 0.177599, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:05.208186: step 3759, loss 0.279447, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:05.284850: step 3760, loss 0.146846, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-11T11:09:05.438098: step 3760, loss 0.235387, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3760

2017-10-11T11:09:05.954677: step 3761, loss 0.160566, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:06.030747: step 3762, loss 0.147925, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:06.103326: step 3763, loss 0.196536, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:06.175036: step 3764, loss 0.297038, acc 0.875, learning_rate 0.000100001
2017-10-11T11:09:06.247372: step 3765, loss 0.189657, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:06.315904: step 3766, loss 0.20902, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:06.392700: step 3767, loss 0.133342, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:06.465438: step 3768, loss 0.160397, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:06.536703: step 3769, loss 0.155902, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:06.607134: step 3770, loss 0.149453, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:06.677869: step 3771, loss 0.177905, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:06.745450: step 3772, loss 0.369895, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:06.817537: step 3773, loss 0.160247, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:06.893864: step 3774, loss 0.226636, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:06.965590: step 3775, loss 0.208109, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:07.040048: step 3776, loss 0.198943, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:07.112625: step 3777, loss 0.081588, acc 1, learning_rate 0.000100001
2017-10-11T11:09:07.184466: step 3778, loss 0.216326, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:07.251875: step 3779, loss 0.143189, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:07.324546: step 3780, loss 0.218537, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:07.393998: step 3781, loss 0.173474, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:07.464441: step 3782, loss 0.230374, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:07.538712: step 3783, loss 0.245471, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:07.607959: step 3784, loss 0.112865, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:07.680962: step 3785, loss 0.252933, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:07.752319: step 3786, loss 0.217186, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:07.822603: step 3787, loss 0.262408, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:07.897069: step 3788, loss 0.225121, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:07.969318: step 3789, loss 0.369406, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:09:08.039656: step 3790, loss 0.244402, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:08.109897: step 3791, loss 0.226867, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:08.178765: step 3792, loss 0.189205, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:08.251206: step 3793, loss 0.267322, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:08.324572: step 3794, loss 0.150081, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:08.395329: step 3795, loss 0.161389, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:08.468579: step 3796, loss 0.250791, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:08.538220: step 3797, loss 0.191043, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:08.611467: step 3798, loss 0.165075, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:08.684877: step 3799, loss 0.138927, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:08.756322: step 3800, loss 0.0950881, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-11T11:09:08.908365: step 3800, loss 0.234526, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3800

2017-10-11T11:09:09.470686: step 3801, loss 0.15201, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:09.543733: step 3802, loss 0.30524, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:09:09.613769: step 3803, loss 0.160651, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:09.683059: step 3804, loss 0.185673, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:09.756445: step 3805, loss 0.213719, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:09.832917: step 3806, loss 0.219998, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:09.906204: step 3807, loss 0.174848, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:09.978030: step 3808, loss 0.142827, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:10.055231: step 3809, loss 0.3054, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:10.133814: step 3810, loss 0.29285, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:10.202252: step 3811, loss 0.188094, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:10.275686: step 3812, loss 0.221791, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:10.346843: step 3813, loss 0.214584, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:10.417692: step 3814, loss 0.27327, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:10.486798: step 3815, loss 0.178429, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:10.555736: step 3816, loss 0.294863, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:10.625782: step 3817, loss 0.158434, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:10.696807: step 3818, loss 0.20794, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:10.769692: step 3819, loss 0.236818, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:10.845330: step 3820, loss 0.21948, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:10.914514: step 3821, loss 0.254768, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:10.982215: step 3822, loss 0.199619, acc 0.901961, learning_rate 0.000100001
2017-10-11T11:09:11.054871: step 3823, loss 0.281519, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:11.125959: step 3824, loss 0.27122, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:11.199697: step 3825, loss 0.177837, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:11.271087: step 3826, loss 0.122485, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:11.343246: step 3827, loss 0.155618, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:11.414820: step 3828, loss 0.153391, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:11.487160: step 3829, loss 0.121102, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:11.555727: step 3830, loss 0.176841, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:11.626420: step 3831, loss 0.0950625, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:11.696179: step 3832, loss 0.282949, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:11.767980: step 3833, loss 0.14513, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:11.842390: step 3834, loss 0.126474, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:11.912397: step 3835, loss 0.188773, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:11.988872: step 3836, loss 0.188528, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:12.060023: step 3837, loss 0.268697, acc 0.875, learning_rate 0.000100001
2017-10-11T11:09:12.132796: step 3838, loss 0.197437, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:12.209872: step 3839, loss 0.14932, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:12.281277: step 3840, loss 0.165333, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-11T11:09:12.419776: step 3840, loss 0.233601, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3840

2017-10-11T11:09:13.061264: step 3841, loss 0.121891, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:13.135279: step 3842, loss 0.281345, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:13.206749: step 3843, loss 0.122706, acc 1, learning_rate 0.000100001
2017-10-11T11:09:13.278833: step 3844, loss 0.153209, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:13.350298: step 3845, loss 0.247089, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:13.420568: step 3846, loss 0.181203, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:13.495317: step 3847, loss 0.332932, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:13.568132: step 3848, loss 0.205166, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:13.638898: step 3849, loss 0.176086, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:13.713271: step 3850, loss 0.211249, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:13.784005: step 3851, loss 0.183161, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:13.855022: step 3852, loss 0.282451, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:13.925069: step 3853, loss 0.214657, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:13.999243: step 3854, loss 0.129917, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:14.081756: step 3855, loss 0.143084, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:14.154336: step 3856, loss 0.208834, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:14.228774: step 3857, loss 0.178282, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:14.299917: step 3858, loss 0.225009, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:14.371355: step 3859, loss 0.102279, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:14.444184: step 3860, loss 0.188185, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:14.513437: step 3861, loss 0.201167, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:14.584098: step 3862, loss 0.203799, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:14.654424: step 3863, loss 0.222377, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:14.727331: step 3864, loss 0.315152, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:14.801205: step 3865, loss 0.192663, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:14.877941: step 3866, loss 0.137769, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:14.948431: step 3867, loss 0.293171, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:15.020878: step 3868, loss 0.198964, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:15.090027: step 3869, loss 0.134846, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:15.164080: step 3870, loss 0.156582, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:15.234062: step 3871, loss 0.134276, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:15.306136: step 3872, loss 0.174385, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:15.379484: step 3873, loss 0.188831, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:15.450415: step 3874, loss 0.276112, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:15.523035: step 3875, loss 0.289018, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:15.597754: step 3876, loss 0.181801, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:15.670750: step 3877, loss 0.229256, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:15.742951: step 3878, loss 0.205455, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:15.817953: step 3879, loss 0.323838, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:15.888467: step 3880, loss 0.0994742, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-11T11:09:16.039221: step 3880, loss 0.233587, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3880

2017-10-11T11:09:16.668706: step 3881, loss 0.221465, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:16.737955: step 3882, loss 0.218549, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:16.809613: step 3883, loss 0.229243, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:16.881413: step 3884, loss 0.134774, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:16.954200: step 3885, loss 0.14648, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:17.029879: step 3886, loss 0.225573, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:17.101440: step 3887, loss 0.135042, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:17.171555: step 3888, loss 0.172759, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:17.245063: step 3889, loss 0.180916, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:17.316544: step 3890, loss 0.24154, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:17.388684: step 3891, loss 0.300961, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:17.459763: step 3892, loss 0.27996, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:17.529353: step 3893, loss 0.0879486, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:17.599621: step 3894, loss 0.244506, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:17.674731: step 3895, loss 0.293927, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:09:17.745275: step 3896, loss 0.211018, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:17.815246: step 3897, loss 0.299147, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:17.890069: step 3898, loss 0.148966, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:17.962595: step 3899, loss 0.225447, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:18.036134: step 3900, loss 0.245453, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:18.108015: step 3901, loss 0.242924, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:18.182436: step 3902, loss 0.259178, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:18.257879: step 3903, loss 0.230224, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:18.330728: step 3904, loss 0.240135, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:18.404256: step 3905, loss 0.15435, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:18.476707: step 3906, loss 0.203087, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:18.549752: step 3907, loss 0.389995, acc 0.875, learning_rate 0.000100001
2017-10-11T11:09:18.629905: step 3908, loss 0.20362, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:18.702855: step 3909, loss 0.0567839, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:09:18.774229: step 3910, loss 0.244402, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:18.847041: step 3911, loss 0.32425, acc 0.875, learning_rate 0.000100001
2017-10-11T11:09:18.919530: step 3912, loss 0.184632, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:18.993405: step 3913, loss 0.171438, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:19.064272: step 3914, loss 0.233021, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:19.136342: step 3915, loss 0.275577, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:19.208203: step 3916, loss 0.170368, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:19.280309: step 3917, loss 0.248249, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:19.350244: step 3918, loss 0.182173, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:19.421066: step 3919, loss 0.165159, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:19.486387: step 3920, loss 0.131722, acc 0.960784, learning_rate 0.000100001

Evaluation:
2017-10-11T11:09:19.629420: step 3920, loss 0.233297, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3920

2017-10-11T11:09:20.199708: step 3921, loss 0.290578, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:09:20.273101: step 3922, loss 0.175613, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:20.348194: step 3923, loss 0.149436, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:20.416576: step 3924, loss 0.29386, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:20.484858: step 3925, loss 0.147602, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:20.557552: step 3926, loss 0.228983, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:20.632520: step 3927, loss 0.283902, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:09:20.703641: step 3928, loss 0.180764, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:09:20.779301: step 3929, loss 0.229326, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:20.851334: step 3930, loss 0.170266, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:20.919959: step 3931, loss 0.178008, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:20.992044: step 3932, loss 0.217856, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:21.063815: step 3933, loss 0.240893, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:21.135193: step 3934, loss 0.17787, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:09:21.206315: step 3935, loss 0.221554, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:09:21.276624: step 3936, loss 0.244108, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:09:21.348150: step 3937, loss 0.190139, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:21.418511: step 3938, loss 0.307122, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:21.487624: step 3939, loss 0.171993, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:21.556327: step 3940, loss 0.171245, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:21.627506: step 3941, loss 0.157509, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:21.698423: step 3942, loss 0.359438, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:21.769918: step 3943, loss 0.208579, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:21.843102: step 3944, loss 0.152771, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:21.912287: step 3945, loss 0.202643, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:21.981122: step 3946, loss 0.139712, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:22.057792: step 3947, loss 0.245717, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:22.126917: step 3948, loss 0.213544, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:22.201277: step 3949, loss 0.368836, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:22.273033: step 3950, loss 0.164064, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:22.348682: step 3951, loss 0.301839, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:22.418555: step 3952, loss 0.0883502, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:22.488889: step 3953, loss 0.154824, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:22.559198: step 3954, loss 0.111784, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:22.630242: step 3955, loss 0.150822, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:22.700200: step 3956, loss 0.144487, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:22.779317: step 3957, loss 0.178288, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:22.852189: step 3958, loss 0.204239, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:22.923331: step 3959, loss 0.187604, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:22.993303: step 3960, loss 0.315229, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:23.138803: step 3960, loss 0.234185, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-3960

2017-10-11T11:09:23.780192: step 3961, loss 0.301962, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:23.851398: step 3962, loss 0.112722, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:23.923620: step 3963, loss 0.0976352, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:23.995237: step 3964, loss 0.347524, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:24.065535: step 3965, loss 0.279275, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:24.137691: step 3966, loss 0.179324, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:24.211280: step 3967, loss 0.225691, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:24.280960: step 3968, loss 0.152776, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:24.352610: step 3969, loss 0.143018, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:24.423007: step 3970, loss 0.176829, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:24.493455: step 3971, loss 0.144206, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:24.563920: step 3972, loss 0.0895551, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:24.634424: step 3973, loss 0.172007, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:24.705604: step 3974, loss 0.210545, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:24.776243: step 3975, loss 0.133173, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:24.851668: step 3976, loss 0.239029, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:24.924805: step 3977, loss 0.120049, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:24.992174: step 3978, loss 0.167062, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:25.063518: step 3979, loss 0.141833, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:25.131303: step 3980, loss 0.233521, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:25.200613: step 3981, loss 0.155131, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:25.269238: step 3982, loss 0.171055, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:25.339241: step 3983, loss 0.2321, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:25.412487: step 3984, loss 0.173168, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:25.482512: step 3985, loss 0.126745, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:25.555397: step 3986, loss 0.193387, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:25.625256: step 3987, loss 0.15276, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:25.699247: step 3988, loss 0.169503, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:25.768976: step 3989, loss 0.256524, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:25.845313: step 3990, loss 0.320656, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:25.918972: step 3991, loss 0.257992, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:25.988796: step 3992, loss 0.192345, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:26.060325: step 3993, loss 0.27383, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:26.133707: step 3994, loss 0.250431, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:26.205176: step 3995, loss 0.158408, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:26.274763: step 3996, loss 0.199288, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:26.345855: step 3997, loss 0.248076, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:26.415620: step 3998, loss 0.198262, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:26.486736: step 3999, loss 0.202429, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:26.561183: step 4000, loss 0.105578, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:26.706948: step 4000, loss 0.2337, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4000

2017-10-11T11:09:27.214878: step 4001, loss 0.165345, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:27.286468: step 4002, loss 0.236913, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:27.358001: step 4003, loss 0.139197, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:27.431232: step 4004, loss 0.135482, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:27.498984: step 4005, loss 0.179485, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:27.570614: step 4006, loss 0.147327, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:27.639865: step 4007, loss 0.364323, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:27.713337: step 4008, loss 0.168675, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:27.783662: step 4009, loss 0.260819, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:27.855182: step 4010, loss 0.24711, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:27.926416: step 4011, loss 0.220912, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:28.000829: step 4012, loss 0.227164, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:28.074204: step 4013, loss 0.222459, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:28.144766: step 4014, loss 0.238963, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:28.214878: step 4015, loss 0.503001, acc 0.828125, learning_rate 0.0001
2017-10-11T11:09:28.285416: step 4016, loss 0.225636, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:28.355387: step 4017, loss 0.187607, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:28.420834: step 4018, loss 0.161752, acc 0.960784, learning_rate 0.0001
2017-10-11T11:09:28.488773: step 4019, loss 0.120938, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:28.556461: step 4020, loss 0.206612, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:28.626341: step 4021, loss 0.30332, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:28.697469: step 4022, loss 0.400037, acc 0.828125, learning_rate 0.0001
2017-10-11T11:09:28.770018: step 4023, loss 0.101666, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:28.840426: step 4024, loss 0.151312, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:28.910749: step 4025, loss 0.155883, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:28.981228: step 4026, loss 0.282849, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:29.054674: step 4027, loss 0.194638, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:29.124966: step 4028, loss 0.162631, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:29.196209: step 4029, loss 0.255057, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:29.265972: step 4030, loss 0.221941, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:29.336071: step 4031, loss 0.159531, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:29.411116: step 4032, loss 0.218884, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:29.478181: step 4033, loss 0.162205, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:29.548341: step 4034, loss 0.210324, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:29.618686: step 4035, loss 0.23142, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:29.688445: step 4036, loss 0.196993, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:29.759512: step 4037, loss 0.218664, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:29.829913: step 4038, loss 0.138494, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:29.905359: step 4039, loss 0.117846, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:29.982733: step 4040, loss 0.238963, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:30.129257: step 4040, loss 0.232498, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4040

2017-10-11T11:09:30.810507: step 4041, loss 0.126671, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:30.887532: step 4042, loss 0.13507, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:30.957687: step 4043, loss 0.202603, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:31.028458: step 4044, loss 0.298118, acc 0.84375, learning_rate 0.0001
2017-10-11T11:09:31.097926: step 4045, loss 0.104725, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:31.169514: step 4046, loss 0.249526, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:31.240941: step 4047, loss 0.274709, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:31.313064: step 4048, loss 0.218691, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:31.384031: step 4049, loss 0.12652, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:31.458935: step 4050, loss 0.254042, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:31.529577: step 4051, loss 0.139977, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:31.600178: step 4052, loss 0.281786, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:31.670690: step 4053, loss 0.134662, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:31.745358: step 4054, loss 0.196913, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:31.818283: step 4055, loss 0.220635, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:31.903194: step 4056, loss 0.0708935, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:31.983445: step 4057, loss 0.245826, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:32.057716: step 4058, loss 0.18024, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:32.128318: step 4059, loss 0.176495, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:32.199907: step 4060, loss 0.129221, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:32.268798: step 4061, loss 0.205125, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:32.339425: step 4062, loss 0.352077, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:32.409584: step 4063, loss 0.160639, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:32.480895: step 4064, loss 0.176018, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:32.552209: step 4065, loss 0.2237, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:32.625529: step 4066, loss 0.124928, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:32.698943: step 4067, loss 0.208157, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:32.770576: step 4068, loss 0.0882082, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:32.844777: step 4069, loss 0.143062, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:32.914511: step 4070, loss 0.145991, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:32.983249: step 4071, loss 0.263767, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:33.053362: step 4072, loss 0.190841, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:33.133509: step 4073, loss 0.226666, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:33.213308: step 4074, loss 0.165811, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:33.286155: step 4075, loss 0.319106, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:33.355714: step 4076, loss 0.300868, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:33.426067: step 4077, loss 0.145488, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:33.495315: step 4078, loss 0.189594, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:33.565350: step 4079, loss 0.182791, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:33.636978: step 4080, loss 0.30945, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:33.782529: step 4080, loss 0.233342, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4080

2017-10-11T11:09:34.369489: step 4081, loss 0.288973, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:34.442516: step 4082, loss 0.221842, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:34.514581: step 4083, loss 0.164758, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:34.584834: step 4084, loss 0.164689, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:34.654606: step 4085, loss 0.277174, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:34.724571: step 4086, loss 0.12718, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:34.793906: step 4087, loss 0.120765, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:34.865893: step 4088, loss 0.18047, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:34.939381: step 4089, loss 0.134701, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:35.012634: step 4090, loss 0.226961, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:35.083005: step 4091, loss 0.2619, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:35.154084: step 4092, loss 0.17676, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:35.226274: step 4093, loss 0.202091, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:35.299537: step 4094, loss 0.206981, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:35.373074: step 4095, loss 0.304024, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:35.448037: step 4096, loss 0.156011, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:35.517409: step 4097, loss 0.178412, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:35.590125: step 4098, loss 0.275541, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:35.664839: step 4099, loss 0.18641, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:35.734358: step 4100, loss 0.150116, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:35.805086: step 4101, loss 0.208131, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:35.875417: step 4102, loss 0.138515, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:35.945404: step 4103, loss 0.186435, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:36.017278: step 4104, loss 0.257033, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:36.090392: step 4105, loss 0.187056, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:36.159142: step 4106, loss 0.221493, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:36.230561: step 4107, loss 0.15938, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:36.299447: step 4108, loss 0.186826, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:36.369853: step 4109, loss 0.344462, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:36.441947: step 4110, loss 0.186809, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:36.511497: step 4111, loss 0.254201, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:36.586034: step 4112, loss 0.179808, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:36.654321: step 4113, loss 0.226556, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:36.727524: step 4114, loss 0.173324, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:36.800611: step 4115, loss 0.267499, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:36.865501: step 4116, loss 0.136734, acc 0.960784, learning_rate 0.0001
2017-10-11T11:09:36.945222: step 4117, loss 0.128157, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:37.015175: step 4118, loss 0.175982, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:37.088730: step 4119, loss 0.122351, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:37.160287: step 4120, loss 0.147744, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:37.310236: step 4120, loss 0.232364, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4120

2017-10-11T11:09:37.953520: step 4121, loss 0.151936, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:38.026255: step 4122, loss 0.079622, acc 1, learning_rate 0.0001
2017-10-11T11:09:38.097131: step 4123, loss 0.189214, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:38.167791: step 4124, loss 0.295575, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:38.239531: step 4125, loss 0.229285, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:38.310498: step 4126, loss 0.223734, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:38.384372: step 4127, loss 0.257195, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:38.455437: step 4128, loss 0.269436, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:38.526709: step 4129, loss 0.215813, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:38.599335: step 4130, loss 0.192973, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:38.670920: step 4131, loss 0.231795, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:38.745276: step 4132, loss 0.243419, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:38.822039: step 4133, loss 0.27347, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:38.895051: step 4134, loss 0.181129, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:38.967747: step 4135, loss 0.190195, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:39.039238: step 4136, loss 0.167764, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:39.109131: step 4137, loss 0.144352, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:39.180451: step 4138, loss 0.0928853, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:39.251249: step 4139, loss 0.0915255, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:39.322337: step 4140, loss 0.212075, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:39.396419: step 4141, loss 0.155131, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:39.467498: step 4142, loss 0.246152, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:39.542654: step 4143, loss 0.109133, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:39.622781: step 4144, loss 0.0675266, acc 1, learning_rate 0.0001
2017-10-11T11:09:39.693983: step 4145, loss 0.281481, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:39.766643: step 4146, loss 0.208389, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:39.839307: step 4147, loss 0.168336, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:39.912252: step 4148, loss 0.228937, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:39.983299: step 4149, loss 0.259436, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:40.053308: step 4150, loss 0.130818, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:40.124665: step 4151, loss 0.12672, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:40.198445: step 4152, loss 0.129015, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:40.269806: step 4153, loss 0.34163, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:40.339520: step 4154, loss 0.197519, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:40.412230: step 4155, loss 0.140963, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:40.482488: step 4156, loss 0.118939, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:40.555197: step 4157, loss 0.169753, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:40.627198: step 4158, loss 0.210354, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:40.697808: step 4159, loss 0.282585, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:40.767167: step 4160, loss 0.287698, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:40.920751: step 4160, loss 0.232753, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4160

2017-10-11T11:09:41.426294: step 4161, loss 0.232121, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:41.496032: step 4162, loss 0.217108, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:41.566025: step 4163, loss 0.192255, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:41.639221: step 4164, loss 0.201803, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:41.710755: step 4165, loss 0.130382, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:41.782594: step 4166, loss 0.189963, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:41.853532: step 4167, loss 0.134306, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:41.924222: step 4168, loss 0.0882959, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:41.997611: step 4169, loss 0.217981, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:42.068876: step 4170, loss 0.0959484, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:42.142409: step 4171, loss 0.171229, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:42.213349: step 4172, loss 0.193337, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:42.282008: step 4173, loss 0.164915, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:42.351567: step 4174, loss 0.236774, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:42.425333: step 4175, loss 0.231954, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:42.496692: step 4176, loss 0.334959, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:42.566900: step 4177, loss 0.142586, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:42.635767: step 4178, loss 0.171394, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:42.710996: step 4179, loss 0.259739, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:42.781606: step 4180, loss 0.294011, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:42.856424: step 4181, loss 0.219835, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:42.930211: step 4182, loss 0.184013, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:43.002171: step 4183, loss 0.114547, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:43.077836: step 4184, loss 0.167561, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:43.149522: step 4185, loss 0.219836, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:43.221647: step 4186, loss 0.109698, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:43.295899: step 4187, loss 0.269738, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:43.367772: step 4188, loss 0.239689, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:43.443099: step 4189, loss 0.153974, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:43.518029: step 4190, loss 0.170873, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:43.592378: step 4191, loss 0.141243, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:43.667448: step 4192, loss 0.26848, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:43.741566: step 4193, loss 0.163599, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:43.818663: step 4194, loss 0.270204, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:43.890909: step 4195, loss 0.261746, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:43.962593: step 4196, loss 0.083736, acc 1, learning_rate 0.0001
2017-10-11T11:09:44.037484: step 4197, loss 0.265112, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:44.108679: step 4198, loss 0.174036, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:44.181997: step 4199, loss 0.219771, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:44.251516: step 4200, loss 0.141252, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:44.404491: step 4200, loss 0.232664, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4200

2017-10-11T11:09:45.547475: step 4201, loss 0.13915, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:45.622063: step 4202, loss 0.252631, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:45.695222: step 4203, loss 0.101551, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:45.768972: step 4204, loss 0.155797, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:45.843776: step 4205, loss 0.397386, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:45.918184: step 4206, loss 0.156181, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:45.990447: step 4207, loss 0.192659, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:46.067009: step 4208, loss 0.105698, acc 1, learning_rate 0.0001
2017-10-11T11:09:46.139801: step 4209, loss 0.216301, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:46.217242: step 4210, loss 0.195442, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:46.290489: step 4211, loss 0.176029, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:46.365309: step 4212, loss 0.161764, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:46.438852: step 4213, loss 0.0870416, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:46.505244: step 4214, loss 0.237181, acc 0.941176, learning_rate 0.0001
2017-10-11T11:09:46.576599: step 4215, loss 0.233953, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:46.645927: step 4216, loss 0.164015, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:46.716147: step 4217, loss 0.151151, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:46.793154: step 4218, loss 0.420559, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:46.869158: step 4219, loss 0.355806, acc 0.84375, learning_rate 0.0001
2017-10-11T11:09:46.940871: step 4220, loss 0.128014, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:47.012378: step 4221, loss 0.198004, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:47.088467: step 4222, loss 0.193464, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:47.162127: step 4223, loss 0.0855999, acc 1, learning_rate 0.0001
2017-10-11T11:09:47.234263: step 4224, loss 0.188064, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:47.306928: step 4225, loss 0.143784, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:47.379288: step 4226, loss 0.229478, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:47.447147: step 4227, loss 0.201163, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:47.515924: step 4228, loss 0.168537, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:47.588162: step 4229, loss 0.136262, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:47.661451: step 4230, loss 0.164777, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:47.731791: step 4231, loss 0.112011, acc 1, learning_rate 0.0001
2017-10-11T11:09:47.802070: step 4232, loss 0.290347, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:47.881478: step 4233, loss 0.176793, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:47.955990: step 4234, loss 0.0919412, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:48.025425: step 4235, loss 0.0805101, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:48.100415: step 4236, loss 0.234807, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:48.175389: step 4237, loss 0.141073, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:48.249266: step 4238, loss 0.211203, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:48.321962: step 4239, loss 0.216734, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:48.393372: step 4240, loss 0.29661, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:48.548385: step 4240, loss 0.232237, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4240

2017-10-11T11:09:49.195361: step 4241, loss 0.178322, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:49.268916: step 4242, loss 0.129612, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:49.336910: step 4243, loss 0.189608, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:49.408739: step 4244, loss 0.172084, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:49.486544: step 4245, loss 0.175701, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:49.559372: step 4246, loss 0.311532, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:49.630479: step 4247, loss 0.174814, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:49.705091: step 4248, loss 0.219351, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:49.775372: step 4249, loss 0.178938, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:49.847296: step 4250, loss 0.214482, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:49.917737: step 4251, loss 0.275372, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:49.989447: step 4252, loss 0.0975588, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:50.062889: step 4253, loss 0.143456, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:50.133775: step 4254, loss 0.280217, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:50.204690: step 4255, loss 0.203082, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:50.274470: step 4256, loss 0.297336, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:50.344678: step 4257, loss 0.154791, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:50.419796: step 4258, loss 0.231905, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:50.490279: step 4259, loss 0.15082, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:50.559475: step 4260, loss 0.236115, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:50.633400: step 4261, loss 0.178881, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:50.705310: step 4262, loss 0.424749, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:50.778914: step 4263, loss 0.35213, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:50.856997: step 4264, loss 0.391785, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:50.930153: step 4265, loss 0.104691, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:51.001687: step 4266, loss 0.189736, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:51.073914: step 4267, loss 0.207324, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:51.150144: step 4268, loss 0.112621, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:51.219074: step 4269, loss 0.160552, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:51.294470: step 4270, loss 0.0995594, acc 1, learning_rate 0.0001
2017-10-11T11:09:51.368656: step 4271, loss 0.176513, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:51.441632: step 4272, loss 0.220415, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:51.514217: step 4273, loss 0.30561, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:51.587345: step 4274, loss 0.250146, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:51.655968: step 4275, loss 0.369255, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:51.730469: step 4276, loss 0.19992, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:51.803438: step 4277, loss 0.266257, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:51.877592: step 4278, loss 0.199691, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:51.953128: step 4279, loss 0.189951, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:52.023075: step 4280, loss 0.338375, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:52.190445: step 4280, loss 0.231255, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4280

2017-10-11T11:09:52.691233: step 4281, loss 0.175892, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:52.765207: step 4282, loss 0.192749, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:52.838158: step 4283, loss 0.117582, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:52.913858: step 4284, loss 0.234533, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:52.987258: step 4285, loss 0.194539, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:53.058345: step 4286, loss 0.107466, acc 1, learning_rate 0.0001
2017-10-11T11:09:53.142786: step 4287, loss 0.208695, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:53.217837: step 4288, loss 0.241971, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:53.292089: step 4289, loss 0.147124, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:53.375620: step 4290, loss 0.162563, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:53.450326: step 4291, loss 0.224465, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:53.526547: step 4292, loss 0.13048, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:53.598258: step 4293, loss 0.0859799, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:53.669911: step 4294, loss 0.185107, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:53.743346: step 4295, loss 0.321606, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:53.815583: step 4296, loss 0.241205, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:53.892159: step 4297, loss 0.107282, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:53.965356: step 4298, loss 0.192719, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:54.041767: step 4299, loss 0.183167, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:54.117206: step 4300, loss 0.175311, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:54.208030: step 4301, loss 0.181514, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:54.287722: step 4302, loss 0.1375, acc 1, learning_rate 0.0001
2017-10-11T11:09:54.363628: step 4303, loss 0.0811028, acc 1, learning_rate 0.0001
2017-10-11T11:09:54.445187: step 4304, loss 0.226425, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:54.521923: step 4305, loss 0.162667, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:54.594849: step 4306, loss 0.173941, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:54.672554: step 4307, loss 0.393697, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:54.747171: step 4308, loss 0.273496, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:54.826966: step 4309, loss 0.22853, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:54.909941: step 4310, loss 0.197186, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:54.983503: step 4311, loss 0.192835, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:55.052389: step 4312, loss 0.149906, acc 0.941176, learning_rate 0.0001
2017-10-11T11:09:55.125627: step 4313, loss 0.206946, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:55.204739: step 4314, loss 0.225697, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:55.280068: step 4315, loss 0.227547, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:55.351649: step 4316, loss 0.178637, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:55.426182: step 4317, loss 0.147898, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:55.508291: step 4318, loss 0.147867, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:55.586174: step 4319, loss 0.23474, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:55.661980: step 4320, loss 0.272359, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:55.819559: step 4320, loss 0.231943, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4320

2017-10-11T11:09:56.395631: step 4321, loss 0.20864, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:56.482196: step 4322, loss 0.166421, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:56.557269: step 4323, loss 0.111406, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:56.632202: step 4324, loss 0.336272, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:56.708133: step 4325, loss 0.190691, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:56.780250: step 4326, loss 0.163555, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:56.858177: step 4327, loss 0.122693, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:56.934828: step 4328, loss 0.185809, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:57.011709: step 4329, loss 0.249667, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:57.091576: step 4330, loss 0.249924, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:57.168768: step 4331, loss 0.171607, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:57.247753: step 4332, loss 0.219258, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:57.326207: step 4333, loss 0.233246, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:57.402034: step 4334, loss 0.194725, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:57.485736: step 4335, loss 0.314868, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:57.560975: step 4336, loss 0.319508, acc 0.859375, learning_rate 0.0001
2017-10-11T11:09:57.639308: step 4337, loss 0.153084, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:57.717200: step 4338, loss 0.32523, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:57.790462: step 4339, loss 0.0988355, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:57.865528: step 4340, loss 0.15623, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:57.941838: step 4341, loss 0.398382, acc 0.875, learning_rate 0.0001
2017-10-11T11:09:58.023525: step 4342, loss 0.233336, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:58.102378: step 4343, loss 0.156202, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:58.177957: step 4344, loss 0.332766, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:58.252457: step 4345, loss 0.104823, acc 1, learning_rate 0.0001
2017-10-11T11:09:58.328848: step 4346, loss 0.213671, acc 0.921875, learning_rate 0.0001
2017-10-11T11:09:58.406462: step 4347, loss 0.275426, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:58.493002: step 4348, loss 0.313488, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:58.570876: step 4349, loss 0.113695, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:58.643593: step 4350, loss 0.226381, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:58.722387: step 4351, loss 0.111423, acc 0.984375, learning_rate 0.0001
2017-10-11T11:09:58.801470: step 4352, loss 0.194207, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:58.881306: step 4353, loss 0.147078, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:58.955849: step 4354, loss 0.135746, acc 0.96875, learning_rate 0.0001
2017-10-11T11:09:59.030215: step 4355, loss 0.253582, acc 0.890625, learning_rate 0.0001
2017-10-11T11:09:59.105771: step 4356, loss 0.281658, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:59.181022: step 4357, loss 0.163085, acc 0.953125, learning_rate 0.0001
2017-10-11T11:09:59.254820: step 4358, loss 0.13929, acc 0.9375, learning_rate 0.0001
2017-10-11T11:09:59.330627: step 4359, loss 0.266152, acc 0.90625, learning_rate 0.0001
2017-10-11T11:09:59.406316: step 4360, loss 0.164104, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:09:59.565888: step 4360, loss 0.230581, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4360

2017-10-11T11:10:00.213192: step 4361, loss 0.21307, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:00.289463: step 4362, loss 0.191836, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:00.361209: step 4363, loss 0.256609, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:00.433913: step 4364, loss 0.181024, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:00.506201: step 4365, loss 0.252058, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:00.576073: step 4366, loss 0.254993, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:00.647835: step 4367, loss 0.169722, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:00.719762: step 4368, loss 0.223224, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:00.790761: step 4369, loss 0.12398, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:00.864239: step 4370, loss 0.184346, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:00.947097: step 4371, loss 0.142784, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:01.026105: step 4372, loss 0.161472, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:01.101015: step 4373, loss 0.249535, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:01.177999: step 4374, loss 0.196867, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:01.252705: step 4375, loss 0.264661, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:01.327057: step 4376, loss 0.148846, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:01.398467: step 4377, loss 0.16147, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:01.473126: step 4378, loss 0.14131, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:01.550107: step 4379, loss 0.195986, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:01.626394: step 4380, loss 0.11315, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:01.700923: step 4381, loss 0.286811, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:01.771419: step 4382, loss 0.197609, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:01.846066: step 4383, loss 0.188746, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:01.917914: step 4384, loss 0.103707, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:01.986784: step 4385, loss 0.205102, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:02.060006: step 4386, loss 0.195576, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:02.137440: step 4387, loss 0.240495, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:02.208121: step 4388, loss 0.21945, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:02.280852: step 4389, loss 0.108122, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:02.357060: step 4390, loss 0.213864, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:02.430275: step 4391, loss 0.324534, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:02.502742: step 4392, loss 0.174188, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:02.578049: step 4393, loss 0.285669, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:02.649856: step 4394, loss 0.117044, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:02.724828: step 4395, loss 0.211353, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:02.795117: step 4396, loss 0.186223, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:02.872002: step 4397, loss 0.171763, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:02.941594: step 4398, loss 0.25648, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:03.013430: step 4399, loss 0.279769, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:03.086337: step 4400, loss 0.322519, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:03.238297: step 4400, loss 0.2312, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4400

2017-10-11T11:10:03.739328: step 4401, loss 0.14851, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:03.812786: step 4402, loss 0.1154, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:03.888454: step 4403, loss 0.0808388, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:03.961798: step 4404, loss 0.145458, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:04.039076: step 4405, loss 0.0766849, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:04.109601: step 4406, loss 0.102402, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:04.179834: step 4407, loss 0.175212, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:04.255674: step 4408, loss 0.245681, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:04.328854: step 4409, loss 0.238457, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:04.391781: step 4410, loss 0.332826, acc 0.901961, learning_rate 0.0001
2017-10-11T11:10:04.461175: step 4411, loss 0.139157, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:04.530874: step 4412, loss 0.112294, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:04.600995: step 4413, loss 0.228991, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:04.673658: step 4414, loss 0.256344, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:04.744192: step 4415, loss 0.245727, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:04.815890: step 4416, loss 0.184068, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:04.891203: step 4417, loss 0.0759887, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:04.962491: step 4418, loss 0.174326, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:05.033394: step 4419, loss 0.189467, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:05.107252: step 4420, loss 0.153725, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:05.179339: step 4421, loss 0.23963, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:05.257210: step 4422, loss 0.142259, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:05.330508: step 4423, loss 0.312556, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:05.403398: step 4424, loss 0.176263, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:05.481416: step 4425, loss 0.143507, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:05.557407: step 4426, loss 0.130177, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:05.632426: step 4427, loss 0.229337, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:05.721160: step 4428, loss 0.1587, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:05.798036: step 4429, loss 0.115408, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:05.881064: step 4430, loss 0.296366, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:05.961369: step 4431, loss 0.265097, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:06.052501: step 4432, loss 0.213318, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:06.144152: step 4433, loss 0.183156, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:06.217864: step 4434, loss 0.141753, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:06.292773: step 4435, loss 0.183466, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:06.367173: step 4436, loss 0.2113, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:06.451478: step 4437, loss 0.224719, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:06.529500: step 4438, loss 0.0849002, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:06.614860: step 4439, loss 0.183566, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:06.691391: step 4440, loss 0.259171, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:06.864679: step 4440, loss 0.23125, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4440

2017-10-11T11:10:07.438401: step 4441, loss 0.151138, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:07.514504: step 4442, loss 0.118182, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:07.600444: step 4443, loss 0.197623, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:07.673436: step 4444, loss 0.249205, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:07.747041: step 4445, loss 0.233831, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:07.820856: step 4446, loss 0.174187, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:07.911121: step 4447, loss 0.200482, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:07.984376: step 4448, loss 0.253042, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:08.059302: step 4449, loss 0.168417, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:08.132329: step 4450, loss 0.12301, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:08.206022: step 4451, loss 0.247113, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:08.277998: step 4452, loss 0.235082, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:08.366465: step 4453, loss 0.093364, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:08.446148: step 4454, loss 0.209357, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:08.529014: step 4455, loss 0.189016, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:08.601292: step 4456, loss 0.193087, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:08.674309: step 4457, loss 0.223412, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:08.750819: step 4458, loss 0.120518, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:08.825226: step 4459, loss 0.286962, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:08.901396: step 4460, loss 0.0719451, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:08.975235: step 4461, loss 0.277523, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:09.050837: step 4462, loss 0.121226, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:09.125903: step 4463, loss 0.156199, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:09.206932: step 4464, loss 0.268323, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:09.281865: step 4465, loss 0.216101, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:09.359154: step 4466, loss 0.184309, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:09.431690: step 4467, loss 0.0973033, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:09.505719: step 4468, loss 0.0896837, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:09.582120: step 4469, loss 0.173699, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:09.664854: step 4470, loss 0.131363, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:09.738921: step 4471, loss 0.110513, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:09.814938: step 4472, loss 0.242646, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:09.889681: step 4473, loss 0.17351, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:09.965303: step 4474, loss 0.160404, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:10.039616: step 4475, loss 0.155921, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:10.111761: step 4476, loss 0.116925, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:10.187376: step 4477, loss 0.151729, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:10.258271: step 4478, loss 0.194479, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:10.333964: step 4479, loss 0.214092, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:10.410963: step 4480, loss 0.135919, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:10.601852: step 4480, loss 0.229912, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4480

2017-10-11T11:10:11.189144: step 4481, loss 0.138557, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:11.265227: step 4482, loss 0.178563, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:11.340125: step 4483, loss 0.130616, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:11.417240: step 4484, loss 0.261697, acc 0.859375, learning_rate 0.0001
2017-10-11T11:10:11.501541: step 4485, loss 0.162635, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:11.580719: step 4486, loss 0.225465, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:11.654546: step 4487, loss 0.176283, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:11.732565: step 4488, loss 0.18783, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:11.807404: step 4489, loss 0.180556, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:11.889319: step 4490, loss 0.18065, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:11.976287: step 4491, loss 0.189132, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:12.051924: step 4492, loss 0.203971, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:12.133370: step 4493, loss 0.210907, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:12.214349: step 4494, loss 0.19899, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:12.301545: step 4495, loss 0.364193, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:12.375907: step 4496, loss 0.126144, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:12.457548: step 4497, loss 0.327294, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:12.532944: step 4498, loss 0.200994, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:12.615023: step 4499, loss 0.138159, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:12.688344: step 4500, loss 0.190772, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:12.765798: step 4501, loss 0.240318, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:12.844319: step 4502, loss 0.288363, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:12.915808: step 4503, loss 0.293221, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:12.992713: step 4504, loss 0.207154, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:13.061091: step 4505, loss 0.137237, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:13.138220: step 4506, loss 0.265383, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:13.208207: step 4507, loss 0.24923, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:13.273138: step 4508, loss 0.157473, acc 0.960784, learning_rate 0.0001
2017-10-11T11:10:13.348339: step 4509, loss 0.110301, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:13.423330: step 4510, loss 0.15461, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:13.497243: step 4511, loss 0.160431, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:13.571678: step 4512, loss 0.193218, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:13.646643: step 4513, loss 0.21293, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:13.721207: step 4514, loss 0.16138, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:13.796920: step 4515, loss 0.138496, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:13.876296: step 4516, loss 0.178581, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:13.951333: step 4517, loss 0.225612, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:14.031385: step 4518, loss 0.280922, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:14.107612: step 4519, loss 0.247489, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:14.182583: step 4520, loss 0.168947, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:14.347314: step 4520, loss 0.230062, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4520

2017-10-11T11:10:15.056711: step 4521, loss 0.166492, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:15.134837: step 4522, loss 0.224905, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:15.212118: step 4523, loss 0.243912, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:15.288085: step 4524, loss 0.106695, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:15.363073: step 4525, loss 0.101009, acc 1, learning_rate 0.0001
2017-10-11T11:10:15.441278: step 4526, loss 0.188211, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:15.515977: step 4527, loss 0.194269, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:15.584025: step 4528, loss 0.157618, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:15.657356: step 4529, loss 0.16573, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:15.769670: step 4530, loss 0.2155, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:15.872999: step 4531, loss 0.171216, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:15.959100: step 4532, loss 0.198145, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:16.064950: step 4533, loss 0.119297, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:16.172251: step 4534, loss 0.177466, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:16.287039: step 4535, loss 0.294216, acc 0.859375, learning_rate 0.0001
2017-10-11T11:10:16.395443: step 4536, loss 0.283994, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:16.506493: step 4537, loss 0.0991956, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:16.619278: step 4538, loss 0.166194, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:16.724763: step 4539, loss 0.248009, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:16.835959: step 4540, loss 0.186352, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:16.917295: step 4541, loss 0.133474, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:17.050824: step 4542, loss 0.226171, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:17.173454: step 4543, loss 0.107351, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:17.287719: step 4544, loss 0.176944, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:17.406008: step 4545, loss 0.191647, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:17.512902: step 4546, loss 0.151361, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:17.644862: step 4547, loss 0.292393, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:17.751459: step 4548, loss 0.140598, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:17.848787: step 4549, loss 0.234465, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:17.956319: step 4550, loss 0.147824, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:18.078287: step 4551, loss 0.150365, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:18.181557: step 4552, loss 0.268469, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:18.296901: step 4553, loss 0.283905, acc 0.859375, learning_rate 0.0001
2017-10-11T11:10:18.409518: step 4554, loss 0.164899, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:18.527273: step 4555, loss 0.242588, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:18.640045: step 4556, loss 0.214954, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:18.752037: step 4557, loss 0.262024, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:18.858066: step 4558, loss 0.247001, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:18.959273: step 4559, loss 0.23779, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:19.080976: step 4560, loss 0.306402, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:19.334492: step 4560, loss 0.229796, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4560

2017-10-11T11:10:20.207918: step 4561, loss 0.11115, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:20.306164: step 4562, loss 0.236638, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:20.461806: step 4563, loss 0.384579, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:20.943163: step 4564, loss 0.235894, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:21.333457: step 4565, loss 0.165324, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:21.429757: step 4566, loss 0.142829, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:21.504670: step 4567, loss 0.146231, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:21.577145: step 4568, loss 0.127602, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:21.650877: step 4569, loss 0.194086, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:21.722053: step 4570, loss 0.193141, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:21.797929: step 4571, loss 0.196487, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:21.868807: step 4572, loss 0.241877, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:21.947190: step 4573, loss 0.169063, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:22.018415: step 4574, loss 0.178309, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:22.089517: step 4575, loss 0.203693, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:22.166791: step 4576, loss 0.19596, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:22.234727: step 4577, loss 0.25275, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:22.310177: step 4578, loss 0.215113, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:22.384477: step 4579, loss 0.22346, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:22.461765: step 4580, loss 0.14812, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:22.533001: step 4581, loss 0.20898, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:22.620213: step 4582, loss 0.148194, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:22.705756: step 4583, loss 0.215119, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:22.775638: step 4584, loss 0.102341, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:22.845989: step 4585, loss 0.356075, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:22.915228: step 4586, loss 0.159491, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:22.984751: step 4587, loss 0.313777, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:23.055885: step 4588, loss 0.23683, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:23.128484: step 4589, loss 0.16431, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:23.198336: step 4590, loss 0.123894, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:23.273260: step 4591, loss 0.146406, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:23.346568: step 4592, loss 0.110348, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:23.421608: step 4593, loss 0.19245, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:23.493150: step 4594, loss 0.166805, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:23.568419: step 4595, loss 0.211944, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:23.639446: step 4596, loss 0.245928, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:23.712325: step 4597, loss 0.202847, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:23.790999: step 4598, loss 0.197059, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:23.863287: step 4599, loss 0.310624, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:23.933640: step 4600, loss 0.2195, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:24.090200: step 4600, loss 0.231011, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4600

2017-10-11T11:10:28.896462: step 4601, loss 0.20951, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:29.008975: step 4602, loss 0.188199, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:29.122593: step 4603, loss 0.111618, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:29.236045: step 4604, loss 0.160734, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:29.348681: step 4605, loss 0.202458, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:29.471087: step 4606, loss 0.121328, acc 0.980392, learning_rate 0.0001
2017-10-11T11:10:30.093241: step 4607, loss 0.164008, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:30.192520: step 4608, loss 0.136859, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:30.324745: step 4609, loss 0.247285, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:30.425134: step 4610, loss 0.19053, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:30.539360: step 4611, loss 0.266747, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:30.652130: step 4612, loss 0.251616, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:30.777239: step 4613, loss 0.105442, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:30.884497: step 4614, loss 0.18684, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:30.992248: step 4615, loss 0.122289, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:31.103336: step 4616, loss 0.230421, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:31.215542: step 4617, loss 0.197388, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:31.318374: step 4618, loss 0.247686, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:31.437854: step 4619, loss 0.137923, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:31.546991: step 4620, loss 0.279945, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:31.661510: step 4621, loss 0.20966, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:31.766124: step 4622, loss 0.211476, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:31.893228: step 4623, loss 0.21271, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:32.002366: step 4624, loss 0.357092, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:32.118458: step 4625, loss 0.192315, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:32.228295: step 4626, loss 0.280294, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:32.536465: step 4627, loss 0.132276, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:32.656000: step 4628, loss 0.127336, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:32.776427: step 4629, loss 0.185089, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:32.900506: step 4630, loss 0.23261, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:33.001589: step 4631, loss 0.193484, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:33.170147: step 4632, loss 0.208391, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:33.250599: step 4633, loss 0.237252, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:33.575528: step 4634, loss 0.204996, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:33.695323: step 4635, loss 0.0931696, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:33.771585: step 4636, loss 0.130172, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:33.844135: step 4637, loss 0.119804, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:33.914764: step 4638, loss 0.223076, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:34.027591: step 4639, loss 0.120154, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:34.103873: step 4640, loss 0.193868, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:34.257058: step 4640, loss 0.229748, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4640

2017-10-11T11:10:34.892098: step 4641, loss 0.285439, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:34.964245: step 4642, loss 0.23183, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:35.033336: step 4643, loss 0.339884, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:35.104287: step 4644, loss 0.0844911, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:35.178227: step 4645, loss 0.215677, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:35.248483: step 4646, loss 0.226973, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:35.324671: step 4647, loss 0.267252, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:35.391901: step 4648, loss 0.179068, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:35.468117: step 4649, loss 0.1735, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:35.536337: step 4650, loss 0.181902, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:35.608265: step 4651, loss 0.161301, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:35.681302: step 4652, loss 0.112254, acc 1, learning_rate 0.0001
2017-10-11T11:10:35.752862: step 4653, loss 0.186949, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:35.823197: step 4654, loss 0.225069, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:35.896154: step 4655, loss 0.156376, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:35.971442: step 4656, loss 0.205991, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:36.044247: step 4657, loss 0.327606, acc 0.84375, learning_rate 0.0001
2017-10-11T11:10:36.116912: step 4658, loss 0.156381, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:36.190749: step 4659, loss 0.214542, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:36.262585: step 4660, loss 0.123037, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:36.334607: step 4661, loss 0.19409, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:36.404023: step 4662, loss 0.185398, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:36.472601: step 4663, loss 0.213245, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:36.545127: step 4664, loss 0.354451, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:36.615184: step 4665, loss 0.171402, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:36.702476: step 4666, loss 0.157031, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:36.785579: step 4667, loss 0.418467, acc 0.859375, learning_rate 0.0001
2017-10-11T11:10:36.864977: step 4668, loss 0.244707, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:36.943191: step 4669, loss 0.249476, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:37.019535: step 4670, loss 0.143828, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:37.100690: step 4671, loss 0.167191, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:37.177578: step 4672, loss 0.166383, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:37.272762: step 4673, loss 0.180872, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:37.371298: step 4674, loss 0.214606, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:37.442308: step 4675, loss 0.129492, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:37.512795: step 4676, loss 0.133759, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:37.582061: step 4677, loss 0.202099, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:37.702453: step 4678, loss 0.242045, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:37.816428: step 4679, loss 0.234731, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:37.931860: step 4680, loss 0.142364, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:38.166404: step 4680, loss 0.229349, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4680

2017-10-11T11:10:39.474629: step 4681, loss 0.116015, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:39.586938: step 4682, loss 0.285432, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:39.707246: step 4683, loss 0.144991, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:39.821202: step 4684, loss 0.0869453, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:39.934744: step 4685, loss 0.206907, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:40.044737: step 4686, loss 0.114815, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:40.155215: step 4687, loss 0.19984, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:40.269996: step 4688, loss 0.261871, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:40.380770: step 4689, loss 0.152522, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:40.487022: step 4690, loss 0.156981, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:40.590365: step 4691, loss 0.0988199, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:40.680451: step 4692, loss 0.147466, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:40.779150: step 4693, loss 0.194531, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:40.883552: step 4694, loss 0.131152, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:40.996866: step 4695, loss 0.284493, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:41.105834: step 4696, loss 0.247458, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:41.222847: step 4697, loss 0.232871, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:41.344902: step 4698, loss 0.253422, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:41.469129: step 4699, loss 0.0822827, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:41.568145: step 4700, loss 0.192455, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:41.689874: step 4701, loss 0.160166, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:41.862161: step 4702, loss 0.177586, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:41.996723: step 4703, loss 0.146347, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:42.062729: step 4704, loss 0.270069, acc 0.941176, learning_rate 0.0001
2017-10-11T11:10:42.138524: step 4705, loss 0.256492, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:42.212192: step 4706, loss 0.207854, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:42.293947: step 4707, loss 0.15672, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:42.368877: step 4708, loss 0.0907965, acc 1, learning_rate 0.0001
2017-10-11T11:10:42.446181: step 4709, loss 0.199041, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:42.521599: step 4710, loss 0.230039, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:42.635538: step 4711, loss 0.25704, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:42.750053: step 4712, loss 0.221981, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:42.863358: step 4713, loss 0.183553, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:42.977108: step 4714, loss 0.266223, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:43.087475: step 4715, loss 0.188712, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:43.198514: step 4716, loss 0.167224, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:43.369486: step 4717, loss 0.164328, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:43.468948: step 4718, loss 0.157342, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:43.574158: step 4719, loss 0.268003, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:43.683900: step 4720, loss 0.237714, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:43.898539: step 4720, loss 0.229191, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4720

2017-10-11T11:10:44.764431: step 4721, loss 0.194135, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:44.888782: step 4722, loss 0.263766, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:45.009016: step 4723, loss 0.136522, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:45.108197: step 4724, loss 0.234469, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:45.214919: step 4725, loss 0.307728, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:45.323281: step 4726, loss 0.166818, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:45.433349: step 4727, loss 0.240309, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:45.545747: step 4728, loss 0.179202, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:45.652012: step 4729, loss 0.257584, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:45.762373: step 4730, loss 0.246398, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:45.873003: step 4731, loss 0.168379, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:45.986476: step 4732, loss 0.115469, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:46.098301: step 4733, loss 0.215845, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:46.208382: step 4734, loss 0.129933, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:46.320833: step 4735, loss 0.228305, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:46.432238: step 4736, loss 0.195824, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:46.516874: step 4737, loss 0.211721, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:46.606075: step 4738, loss 0.255889, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:46.720897: step 4739, loss 0.247673, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:46.839551: step 4740, loss 0.101759, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:46.939117: step 4741, loss 0.0954307, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:47.117245: step 4742, loss 0.225667, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:47.229595: step 4743, loss 0.170704, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:47.298935: step 4744, loss 0.20154, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:47.367689: step 4745, loss 0.152752, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:47.440844: step 4746, loss 0.189207, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:47.511578: step 4747, loss 0.21674, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:47.584351: step 4748, loss 0.113317, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:47.671657: step 4749, loss 0.238729, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:47.755136: step 4750, loss 0.101915, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:47.840890: step 4751, loss 0.198485, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:47.921465: step 4752, loss 0.354148, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:48.006732: step 4753, loss 0.15962, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:48.092593: step 4754, loss 0.183715, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:48.171545: step 4755, loss 0.146492, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:48.287807: step 4756, loss 0.230649, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:48.410839: step 4757, loss 0.159614, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:48.536045: step 4758, loss 0.187793, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:48.647642: step 4759, loss 0.283152, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:48.762530: step 4760, loss 0.184128, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:48.973661: step 4760, loss 0.228865, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4760

2017-10-11T11:10:49.982842: step 4761, loss 0.152237, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:50.090664: step 4762, loss 0.113417, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:50.203607: step 4763, loss 0.201448, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:50.313729: step 4764, loss 0.186146, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:50.424097: step 4765, loss 0.119017, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:50.537982: step 4766, loss 0.118429, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:50.651470: step 4767, loss 0.275447, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:50.764865: step 4768, loss 0.151608, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:50.881911: step 4769, loss 0.344624, acc 0.84375, learning_rate 0.0001
2017-10-11T11:10:50.999181: step 4770, loss 0.179764, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:51.109000: step 4771, loss 0.277629, acc 0.84375, learning_rate 0.0001
2017-10-11T11:10:51.220031: step 4772, loss 0.177734, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:51.332585: step 4773, loss 0.210351, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:51.444844: step 4774, loss 0.267838, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:51.544884: step 4775, loss 0.132476, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:51.632449: step 4776, loss 0.150333, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:51.761208: step 4777, loss 0.151673, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:51.874062: step 4778, loss 0.163385, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:51.987659: step 4779, loss 0.189718, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:52.102814: step 4780, loss 0.142631, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:52.208388: step 4781, loss 0.0981144, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:52.315101: step 4782, loss 0.273057, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:52.436864: step 4783, loss 0.179156, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:52.549119: step 4784, loss 0.195795, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:52.671744: step 4785, loss 0.261963, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:52.786801: step 4786, loss 0.145532, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:52.954698: step 4787, loss 0.119986, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:53.067126: step 4788, loss 0.128277, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:53.151400: step 4789, loss 0.204089, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:53.233851: step 4790, loss 0.217156, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:53.326311: step 4791, loss 0.10393, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:53.406424: step 4792, loss 0.181759, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:53.482042: step 4793, loss 0.1236, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:53.558395: step 4794, loss 0.149938, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:53.633273: step 4795, loss 0.21141, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:53.752597: step 4796, loss 0.146732, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:53.869024: step 4797, loss 0.208764, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:53.977959: step 4798, loss 0.127692, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:54.089455: step 4799, loss 0.108156, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:54.202672: step 4800, loss 0.120929, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:54.392968: step 4800, loss 0.22819, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4800

2017-10-11T11:10:55.176701: step 4801, loss 0.197184, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:55.264856: step 4802, loss 0.326983, acc 0.901961, learning_rate 0.0001
2017-10-11T11:10:55.381928: step 4803, loss 0.160366, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:55.508242: step 4804, loss 0.276143, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:55.629215: step 4805, loss 0.321771, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:55.725272: step 4806, loss 0.11473, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:55.828050: step 4807, loss 0.102528, acc 0.984375, learning_rate 0.0001
2017-10-11T11:10:55.941711: step 4808, loss 0.177273, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:56.054948: step 4809, loss 0.361158, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:56.165214: step 4810, loss 0.174613, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:56.279709: step 4811, loss 0.159997, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:56.394604: step 4812, loss 0.315989, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:56.506811: step 4813, loss 0.218411, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:56.605973: step 4814, loss 0.225957, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:56.711151: step 4815, loss 0.222036, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:56.820346: step 4816, loss 0.230496, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:56.935862: step 4817, loss 0.170341, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:57.043870: step 4818, loss 0.150918, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:57.155393: step 4819, loss 0.169106, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:57.268912: step 4820, loss 0.21646, acc 0.90625, learning_rate 0.0001
2017-10-11T11:10:57.381173: step 4821, loss 0.124515, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:57.493131: step 4822, loss 0.189179, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:57.606208: step 4823, loss 0.119483, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:57.722614: step 4824, loss 0.14714, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:57.829133: step 4825, loss 0.119038, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:57.936763: step 4826, loss 0.171339, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:58.051841: step 4827, loss 0.262154, acc 0.875, learning_rate 0.0001
2017-10-11T11:10:58.185901: step 4828, loss 0.227059, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:58.328288: step 4829, loss 0.116184, acc 0.96875, learning_rate 0.0001
2017-10-11T11:10:58.405478: step 4830, loss 0.228999, acc 0.890625, learning_rate 0.0001
2017-10-11T11:10:58.478865: step 4831, loss 0.168376, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:58.548558: step 4832, loss 0.154731, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:58.620426: step 4833, loss 0.239099, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:58.695594: step 4834, loss 0.239588, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:58.769585: step 4835, loss 0.244474, acc 0.921875, learning_rate 0.0001
2017-10-11T11:10:58.846465: step 4836, loss 0.185274, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:58.924010: step 4837, loss 0.203527, acc 0.9375, learning_rate 0.0001
2017-10-11T11:10:58.996209: step 4838, loss 0.0660141, acc 1, learning_rate 0.0001
2017-10-11T11:10:59.079684: step 4839, loss 0.185759, acc 0.953125, learning_rate 0.0001
2017-10-11T11:10:59.154960: step 4840, loss 0.218271, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:10:59.351632: step 4840, loss 0.229483, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4840

2017-10-11T11:11:00.252555: step 4841, loss 0.234197, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:00.359824: step 4842, loss 0.0982942, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:00.470910: step 4843, loss 0.203068, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:00.576494: step 4844, loss 0.2126, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:00.672879: step 4845, loss 0.172103, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:00.780852: step 4846, loss 0.235926, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:00.870696: step 4847, loss 0.232136, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:00.956576: step 4848, loss 0.143147, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:01.059396: step 4849, loss 0.319708, acc 0.875, learning_rate 0.0001
2017-10-11T11:11:01.164890: step 4850, loss 0.166334, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:01.279327: step 4851, loss 0.0835135, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:01.391590: step 4852, loss 0.246667, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:01.497512: step 4853, loss 0.286336, acc 0.875, learning_rate 0.0001
2017-10-11T11:11:01.615928: step 4854, loss 0.188613, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:01.736891: step 4855, loss 0.157846, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:01.851647: step 4856, loss 0.200224, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:01.965430: step 4857, loss 0.254306, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:02.085075: step 4858, loss 0.222369, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:02.194064: step 4859, loss 0.087855, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:02.303689: step 4860, loss 0.288085, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:02.418052: step 4861, loss 0.188104, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:02.525893: step 4862, loss 0.190665, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:02.650807: step 4863, loss 0.126045, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:02.761309: step 4864, loss 0.186942, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:02.871210: step 4865, loss 0.214312, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:02.982584: step 4866, loss 0.240578, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:03.089144: step 4867, loss 0.325258, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:03.176840: step 4868, loss 0.140053, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:03.271757: step 4869, loss 0.112156, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:03.386200: step 4870, loss 0.197635, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:03.494659: step 4871, loss 0.137335, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:03.603869: step 4872, loss 0.172061, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:03.721592: step 4873, loss 0.156572, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:03.917473: step 4874, loss 0.266805, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:04.238445: step 4875, loss 0.236846, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:04.332846: step 4876, loss 0.187784, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:04.409232: step 4877, loss 0.113444, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:04.478100: step 4878, loss 0.109609, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:04.550727: step 4879, loss 0.140354, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:04.621641: step 4880, loss 0.181189, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:11:34.135628: step 4880, loss 0.228747, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4880

2017-10-11T11:11:40.404781: step 4881, loss 0.1034, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:40.636252: step 4882, loss 0.291055, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:40.754974: step 4883, loss 0.213332, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:40.874816: step 4884, loss 0.180819, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:40.981341: step 4885, loss 0.143924, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:41.087639: step 4886, loss 0.106567, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:41.203840: step 4887, loss 0.222928, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:41.314220: step 4888, loss 0.215703, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:41.421675: step 4889, loss 0.340199, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:41.524845: step 4890, loss 0.130066, acc 1, learning_rate 0.0001
2017-10-11T11:11:41.632819: step 4891, loss 0.164519, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:41.711909: step 4892, loss 0.182003, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:41.818751: step 4893, loss 0.207786, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:41.929247: step 4894, loss 0.227839, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:42.265578: step 4895, loss 0.168287, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:42.336515: step 4896, loss 0.111524, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:42.412131: step 4897, loss 0.186556, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:42.490866: step 4898, loss 0.0837008, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:42.602480: step 4899, loss 0.27101, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:42.702985: step 4900, loss 0.236595, acc 0.921569, learning_rate 0.0001
2017-10-11T11:11:42.817069: step 4901, loss 0.216059, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:42.934001: step 4902, loss 0.111679, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:43.052874: step 4903, loss 0.206886, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:43.159540: step 4904, loss 0.067918, acc 1, learning_rate 0.0001
2017-10-11T11:11:43.282527: step 4905, loss 0.151202, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:43.390178: step 4906, loss 0.193488, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:43.504037: step 4907, loss 0.240706, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:43.619956: step 4908, loss 0.172031, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:43.727908: step 4909, loss 0.266943, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:43.847874: step 4910, loss 0.102866, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:43.958843: step 4911, loss 0.198431, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:44.068293: step 4912, loss 0.121377, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:44.180897: step 4913, loss 0.20198, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:44.297304: step 4914, loss 0.251027, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:44.408590: step 4915, loss 0.262494, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:44.508277: step 4916, loss 0.312647, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:44.623821: step 4917, loss 0.201896, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:44.734941: step 4918, loss 0.0681502, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:44.851779: step 4919, loss 0.10485, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:44.956094: step 4920, loss 0.198961, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:11:45.155866: step 4920, loss 0.227249, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4920

2017-10-11T11:11:45.873918: step 4921, loss 0.145822, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:45.950524: step 4922, loss 0.153487, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:46.024189: step 4923, loss 0.2408, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:46.105661: step 4924, loss 0.323088, acc 0.875, learning_rate 0.0001
2017-10-11T11:11:46.218929: step 4925, loss 0.110766, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:46.340875: step 4926, loss 0.228907, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:46.466726: step 4927, loss 0.144371, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:46.587003: step 4928, loss 0.185353, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:46.695970: step 4929, loss 0.274277, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:46.812442: step 4930, loss 0.219025, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:46.919337: step 4931, loss 0.246802, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:47.022389: step 4932, loss 0.144512, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:47.134209: step 4933, loss 0.132705, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:47.237193: step 4934, loss 0.113295, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:47.351521: step 4935, loss 0.307977, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:47.459037: step 4936, loss 0.130501, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:47.577467: step 4937, loss 0.153173, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:47.694463: step 4938, loss 0.151984, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:47.807096: step 4939, loss 0.131186, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:47.916989: step 4940, loss 0.150934, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:48.026673: step 4941, loss 0.199718, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:48.136339: step 4942, loss 0.207256, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:48.230422: step 4943, loss 0.23598, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:48.332893: step 4944, loss 0.309827, acc 0.875, learning_rate 0.0001
2017-10-11T11:11:48.430735: step 4945, loss 0.273833, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:48.526264: step 4946, loss 0.197753, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:48.643120: step 4947, loss 0.163208, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:48.756018: step 4948, loss 0.137666, acc 1, learning_rate 0.0001
2017-10-11T11:11:48.825727: step 4949, loss 0.0836494, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:48.900866: step 4950, loss 0.312025, acc 0.875, learning_rate 0.0001
2017-10-11T11:11:48.969524: step 4951, loss 0.196856, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:49.040305: step 4952, loss 0.279556, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:49.111731: step 4953, loss 0.191204, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:49.181558: step 4954, loss 0.142793, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:49.252783: step 4955, loss 0.210771, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:49.321380: step 4956, loss 0.135539, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:49.393129: step 4957, loss 0.152427, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:49.464477: step 4958, loss 0.297809, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:49.538006: step 4959, loss 0.201097, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:49.610103: step 4960, loss 0.180191, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:11:49.747180: step 4960, loss 0.227131, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-4960

2017-10-11T11:11:50.659159: step 4961, loss 0.131861, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:50.778897: step 4962, loss 0.165743, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:50.900332: step 4963, loss 0.111396, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:51.014645: step 4964, loss 0.166485, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:51.127091: step 4965, loss 0.214725, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:51.235424: step 4966, loss 0.21792, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:51.341078: step 4967, loss 0.138728, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:51.466781: step 4968, loss 0.21854, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:51.572864: step 4969, loss 0.100876, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:51.743663: step 4970, loss 0.18175, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:51.820430: step 4971, loss 0.265039, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:51.901984: step 4972, loss 0.0832493, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:51.978534: step 4973, loss 0.158646, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:52.053034: step 4974, loss 0.190549, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:52.126079: step 4975, loss 0.184341, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:52.200747: step 4976, loss 0.113347, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:52.281796: step 4977, loss 0.170062, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:52.371145: step 4978, loss 0.162584, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:52.485053: step 4979, loss 0.318203, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:52.578836: step 4980, loss 0.169707, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:52.675463: step 4981, loss 0.171946, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:52.783594: step 4982, loss 0.161411, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:52.899087: step 4983, loss 0.135271, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:53.017441: step 4984, loss 0.156213, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:53.136944: step 4985, loss 0.149219, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:53.249983: step 4986, loss 0.227434, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:53.365175: step 4987, loss 0.133044, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:53.474890: step 4988, loss 0.126151, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:53.589449: step 4989, loss 0.185466, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:53.705443: step 4990, loss 0.296417, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:53.819164: step 4991, loss 0.212764, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:53.931028: step 4992, loss 0.258355, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:54.041517: step 4993, loss 0.162888, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:54.150311: step 4994, loss 0.254754, acc 0.875, learning_rate 0.0001
2017-10-11T11:11:54.260401: step 4995, loss 0.230304, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:54.377681: step 4996, loss 0.143368, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:54.480838: step 4997, loss 0.118829, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:54.563512: step 4998, loss 0.12542, acc 0.960784, learning_rate 0.0001
2017-10-11T11:11:54.661710: step 4999, loss 0.162234, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:54.789044: step 5000, loss 0.170922, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:11:54.982512: step 5000, loss 0.227381, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5000

2017-10-11T11:11:56.079762: step 5001, loss 0.14931, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:56.204530: step 5002, loss 0.160652, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:56.315182: step 5003, loss 0.119158, acc 1, learning_rate 0.0001
2017-10-11T11:11:56.430577: step 5004, loss 0.258962, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:56.541279: step 5005, loss 0.229307, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:56.644712: step 5006, loss 0.106504, acc 1, learning_rate 0.0001
2017-10-11T11:11:56.785043: step 5007, loss 0.160974, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:56.930138: step 5008, loss 0.204712, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:57.012104: step 5009, loss 0.174503, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:57.098315: step 5010, loss 0.105738, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:57.182644: step 5011, loss 0.14625, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:57.265174: step 5012, loss 0.184644, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:57.348240: step 5013, loss 0.197862, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:57.429015: step 5014, loss 0.182566, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:57.513202: step 5015, loss 0.193121, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:57.608900: step 5016, loss 0.189013, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:57.725066: step 5017, loss 0.237093, acc 0.921875, learning_rate 0.0001
2017-10-11T11:11:57.851245: step 5018, loss 0.175719, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:57.972914: step 5019, loss 0.16767, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:58.086452: step 5020, loss 0.193484, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:58.205174: step 5021, loss 0.223302, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:58.319638: step 5022, loss 0.187357, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:58.418979: step 5023, loss 0.113726, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:58.524840: step 5024, loss 0.174836, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:58.614384: step 5025, loss 0.404856, acc 0.859375, learning_rate 0.0001
2017-10-11T11:11:58.724832: step 5026, loss 0.243809, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:58.845600: step 5027, loss 0.297931, acc 0.859375, learning_rate 0.0001
2017-10-11T11:11:58.948893: step 5028, loss 0.264405, acc 0.890625, learning_rate 0.0001
2017-10-11T11:11:59.069834: step 5029, loss 0.209989, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:59.171013: step 5030, loss 0.185807, acc 0.9375, learning_rate 0.0001
2017-10-11T11:11:59.294356: step 5031, loss 0.236612, acc 0.90625, learning_rate 0.0001
2017-10-11T11:11:59.401151: step 5032, loss 0.0780336, acc 0.984375, learning_rate 0.0001
2017-10-11T11:11:59.514866: step 5033, loss 0.157771, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:59.628886: step 5034, loss 0.24875, acc 0.953125, learning_rate 0.0001
2017-10-11T11:11:59.742500: step 5035, loss 0.123018, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:59.844962: step 5036, loss 0.128023, acc 0.96875, learning_rate 0.0001
2017-10-11T11:11:59.953742: step 5037, loss 0.0930196, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:00.062737: step 5038, loss 0.22558, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:00.174001: step 5039, loss 0.199725, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:00.289237: step 5040, loss 0.187522, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:00.471385: step 5040, loss 0.227299, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5040

2017-10-11T11:12:01.278527: step 5041, loss 0.201277, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:01.372999: step 5042, loss 0.137428, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:01.488873: step 5043, loss 0.142459, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:01.613577: step 5044, loss 0.18097, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:01.721393: step 5045, loss 0.225757, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:01.834410: step 5046, loss 0.110935, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:01.938297: step 5047, loss 0.155294, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:02.049470: step 5048, loss 0.117635, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:02.217856: step 5049, loss 0.238262, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:02.326614: step 5050, loss 0.163021, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:02.404586: step 5051, loss 0.165031, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:02.494891: step 5052, loss 0.192143, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:02.569885: step 5053, loss 0.105802, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:02.641327: step 5054, loss 0.148303, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:02.722726: step 5055, loss 0.230248, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:02.802687: step 5056, loss 0.15111, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:02.876797: step 5057, loss 0.169553, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:02.981372: step 5058, loss 0.188902, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:03.091217: step 5059, loss 0.13998, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:03.203878: step 5060, loss 0.221521, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:03.296870: step 5061, loss 0.235487, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:03.400914: step 5062, loss 0.171423, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:03.497463: step 5063, loss 0.13424, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:03.604194: step 5064, loss 0.196213, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:03.711777: step 5065, loss 0.209456, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:03.821913: step 5066, loss 0.153715, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:03.922336: step 5067, loss 0.189541, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:04.039914: step 5068, loss 0.158851, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:04.154125: step 5069, loss 0.275591, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:04.274306: step 5070, loss 0.129022, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:04.389651: step 5071, loss 0.247488, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:04.511934: step 5072, loss 0.204195, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:04.611341: step 5073, loss 0.161283, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:04.737309: step 5074, loss 0.25283, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:04.851184: step 5075, loss 0.245112, acc 0.875, learning_rate 0.0001
2017-10-11T11:12:04.951510: step 5076, loss 0.221189, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:05.056327: step 5077, loss 0.176882, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:05.164449: step 5078, loss 0.170212, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:05.274292: step 5079, loss 0.176069, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:05.382451: step 5080, loss 0.235669, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:05.576943: step 5080, loss 0.22819, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5080

2017-10-11T11:12:06.516952: step 5081, loss 0.144789, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:06.633843: step 5082, loss 0.120221, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:06.756842: step 5083, loss 0.31338, acc 0.84375, learning_rate 0.0001
2017-10-11T11:12:06.881894: step 5084, loss 0.238926, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:06.997115: step 5085, loss 0.160386, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:07.106688: step 5086, loss 0.144131, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:07.214718: step 5087, loss 0.223378, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:07.308914: step 5088, loss 0.140921, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:07.485326: step 5089, loss 0.112015, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:07.587721: step 5090, loss 0.271534, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:07.663408: step 5091, loss 0.177579, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:07.751056: step 5092, loss 0.136717, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:07.827519: step 5093, loss 0.164422, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:07.919094: step 5094, loss 0.157303, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:08.003613: step 5095, loss 0.179141, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:08.072712: step 5096, loss 0.218924, acc 0.921569, learning_rate 0.0001
2017-10-11T11:12:08.150258: step 5097, loss 0.186928, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:08.269802: step 5098, loss 0.180274, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:08.369574: step 5099, loss 0.228661, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:08.485013: step 5100, loss 0.291691, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:08.602237: step 5101, loss 0.108689, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:08.721373: step 5102, loss 0.135758, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:08.830254: step 5103, loss 0.275271, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:08.953560: step 5104, loss 0.122001, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:09.060916: step 5105, loss 0.159099, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:09.156852: step 5106, loss 0.234769, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:09.264636: step 5107, loss 0.176712, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:09.358991: step 5108, loss 0.191424, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:09.484966: step 5109, loss 0.193157, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:09.599589: step 5110, loss 0.251232, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:09.718294: step 5111, loss 0.190558, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:09.831121: step 5112, loss 0.101413, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:09.938307: step 5113, loss 0.311257, acc 0.84375, learning_rate 0.0001
2017-10-11T11:12:10.049463: step 5114, loss 0.195341, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:10.159498: step 5115, loss 0.142637, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:10.272261: step 5116, loss 0.188393, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:10.389208: step 5117, loss 0.137126, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:10.503334: step 5118, loss 0.239823, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:10.611924: step 5119, loss 0.249925, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:10.725452: step 5120, loss 0.241576, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:10.913151: step 5120, loss 0.227822, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5120

2017-10-11T11:12:11.805393: step 5121, loss 0.119578, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:11.907015: step 5122, loss 0.241867, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:12.001090: step 5123, loss 0.153594, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:12.101389: step 5124, loss 0.179011, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:12.207875: step 5125, loss 0.210585, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:12.323737: step 5126, loss 0.333912, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:12.440155: step 5127, loss 0.132778, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:12.539313: step 5128, loss 0.125362, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:12.729530: step 5129, loss 0.156344, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:12.834442: step 5130, loss 0.151947, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:12.907474: step 5131, loss 0.154832, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:12.984756: step 5132, loss 0.317479, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:13.061252: step 5133, loss 0.171893, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:13.134016: step 5134, loss 0.135403, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:13.210301: step 5135, loss 0.214261, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:13.284853: step 5136, loss 0.113421, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:13.367091: step 5137, loss 0.215956, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:13.464081: step 5138, loss 0.278776, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:13.590181: step 5139, loss 0.209935, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:13.698822: step 5140, loss 0.0810675, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:13.811565: step 5141, loss 0.133138, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:13.925587: step 5142, loss 0.117427, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:14.031684: step 5143, loss 0.203926, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:14.136184: step 5144, loss 0.232331, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:14.232862: step 5145, loss 0.148443, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:14.334010: step 5146, loss 0.178447, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:14.433022: step 5147, loss 0.240026, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:14.536639: step 5148, loss 0.142085, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:14.666758: step 5149, loss 0.201768, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:14.769499: step 5150, loss 0.151263, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:14.882127: step 5151, loss 0.135679, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:14.992605: step 5152, loss 0.288093, acc 0.859375, learning_rate 0.0001
2017-10-11T11:12:15.109701: step 5153, loss 0.228027, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:15.228904: step 5154, loss 0.0899387, acc 1, learning_rate 0.0001
2017-10-11T11:12:15.334994: step 5155, loss 0.173099, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:15.454937: step 5156, loss 0.133444, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:15.569411: step 5157, loss 0.0887813, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:15.681337: step 5158, loss 0.144882, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:15.789896: step 5159, loss 0.185293, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:15.906869: step 5160, loss 0.128649, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:16.097324: step 5160, loss 0.22781, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5160

2017-10-11T11:12:17.147588: step 5161, loss 0.179324, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:17.258948: step 5162, loss 0.160827, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:17.363916: step 5163, loss 0.108228, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:17.468848: step 5164, loss 0.0924785, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:17.561846: step 5165, loss 0.118045, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:17.668831: step 5166, loss 0.129899, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:17.770956: step 5167, loss 0.108888, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:17.977058: step 5168, loss 0.124899, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:18.057235: step 5169, loss 0.122254, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:18.131149: step 5170, loss 0.169773, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:18.204257: step 5171, loss 0.230932, acc 0.875, learning_rate 0.0001
2017-10-11T11:12:18.278516: step 5172, loss 0.167768, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:18.354712: step 5173, loss 0.287678, acc 0.875, learning_rate 0.0001
2017-10-11T11:12:18.435058: step 5174, loss 0.173754, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:18.507606: step 5175, loss 0.351506, acc 0.84375, learning_rate 0.0001
2017-10-11T11:12:18.580205: step 5176, loss 0.184181, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:18.676871: step 5177, loss 0.236346, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:18.785155: step 5178, loss 0.120733, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:18.880767: step 5179, loss 0.209107, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:18.985292: step 5180, loss 0.138589, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:19.089137: step 5181, loss 0.136371, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:19.199337: step 5182, loss 0.185482, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:19.308883: step 5183, loss 0.22513, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:19.422038: step 5184, loss 0.155678, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:19.529740: step 5185, loss 0.15327, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:19.635004: step 5186, loss 0.22992, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:19.729897: step 5187, loss 0.157403, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:19.844840: step 5188, loss 0.15665, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:19.944553: step 5189, loss 0.156345, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:20.055836: step 5190, loss 0.118486, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:20.156614: step 5191, loss 0.130907, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:20.261501: step 5192, loss 0.149476, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:20.376871: step 5193, loss 0.183746, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:20.480399: step 5194, loss 0.163752, acc 0.941176, learning_rate 0.0001
2017-10-11T11:12:20.592095: step 5195, loss 0.0929184, acc 1, learning_rate 0.0001
2017-10-11T11:12:20.712422: step 5196, loss 0.223401, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:20.827623: step 5197, loss 0.158633, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:20.948896: step 5198, loss 0.164855, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:21.075919: step 5199, loss 0.29462, acc 0.859375, learning_rate 0.0001
2017-10-11T11:12:21.189636: step 5200, loss 0.128748, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:21.378490: step 5200, loss 0.227844, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5200

2017-10-11T11:12:22.176520: step 5201, loss 0.361942, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:22.283232: step 5202, loss 0.10507, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:22.395247: step 5203, loss 0.120651, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:22.515884: step 5204, loss 0.265971, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:22.619048: step 5205, loss 0.14707, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:22.741357: step 5206, loss 0.179359, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:22.853777: step 5207, loss 0.178235, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:22.964516: step 5208, loss 0.180802, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:23.079712: step 5209, loss 0.188873, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:23.289198: step 5210, loss 0.154938, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:23.370258: step 5211, loss 0.161888, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:23.447206: step 5212, loss 0.12109, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:23.522246: step 5213, loss 0.216269, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:23.597036: step 5214, loss 0.287867, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:23.672211: step 5215, loss 0.160924, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:23.744943: step 5216, loss 0.275872, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:23.823783: step 5217, loss 0.230122, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:23.899344: step 5218, loss 0.232551, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:24.006570: step 5219, loss 0.133457, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:24.117287: step 5220, loss 0.135248, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:24.237111: step 5221, loss 0.281016, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:24.332011: step 5222, loss 0.117296, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:24.466588: step 5223, loss 0.150705, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:24.568080: step 5224, loss 0.15087, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:24.680855: step 5225, loss 0.228245, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:24.770434: step 5226, loss 0.281711, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:24.888495: step 5227, loss 0.138058, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:24.984909: step 5228, loss 0.188814, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:25.087723: step 5229, loss 0.180846, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:25.201072: step 5230, loss 0.206519, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:25.307976: step 5231, loss 0.178807, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:25.433089: step 5232, loss 0.173135, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:25.551242: step 5233, loss 0.0792161, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:25.664552: step 5234, loss 0.238356, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:25.770763: step 5235, loss 0.285708, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:25.881320: step 5236, loss 0.131566, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:25.998847: step 5237, loss 0.113738, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:26.124869: step 5238, loss 0.113112, acc 1, learning_rate 0.0001
2017-10-11T11:12:26.248849: step 5239, loss 0.128548, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:26.348902: step 5240, loss 0.156524, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:26.535844: step 5240, loss 0.226954, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5240

2017-10-11T11:12:27.451496: step 5241, loss 0.0910754, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:27.560387: step 5242, loss 0.188623, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:27.674575: step 5243, loss 0.279081, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:27.784285: step 5244, loss 0.12794, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:27.897042: step 5245, loss 0.181734, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:28.002690: step 5246, loss 0.142919, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:28.121673: step 5247, loss 0.162612, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:28.235329: step 5248, loss 0.138633, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:28.345573: step 5249, loss 0.096708, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:28.519402: step 5250, loss 0.127939, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:28.598169: step 5251, loss 0.242309, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:28.680817: step 5252, loss 0.131881, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:28.756705: step 5253, loss 0.0805641, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:28.830684: step 5254, loss 0.162124, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:28.907913: step 5255, loss 0.244032, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:28.994029: step 5256, loss 0.233115, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:29.068294: step 5257, loss 0.233087, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:29.155077: step 5258, loss 0.162112, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:29.272747: step 5259, loss 0.145244, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:29.394768: step 5260, loss 0.127263, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:29.502746: step 5261, loss 0.14625, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:29.624501: step 5262, loss 0.25716, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:29.741149: step 5263, loss 0.106569, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:29.844708: step 5264, loss 0.108731, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:29.956079: step 5265, loss 0.162304, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:30.072356: step 5266, loss 0.167031, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:30.191526: step 5267, loss 0.102223, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:30.310436: step 5268, loss 0.19292, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:30.429692: step 5269, loss 0.194467, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:30.548854: step 5270, loss 0.311853, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:30.663695: step 5271, loss 0.19269, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:30.787093: step 5272, loss 0.230216, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:30.893786: step 5273, loss 0.186348, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:31.009055: step 5274, loss 0.157003, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:31.125419: step 5275, loss 0.209104, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:31.230777: step 5276, loss 0.212339, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:31.353575: step 5277, loss 0.201904, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:31.465423: step 5278, loss 0.158634, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:31.573217: step 5279, loss 0.168263, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:31.684970: step 5280, loss 0.194744, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:31.872492: step 5280, loss 0.227364, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5280

2017-10-11T11:12:32.912298: step 5281, loss 0.133805, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:33.011004: step 5282, loss 0.169921, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:33.116944: step 5283, loss 0.121004, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:33.239144: step 5284, loss 0.245832, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:33.351149: step 5285, loss 0.194902, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:33.462756: step 5286, loss 0.211183, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:33.581764: step 5287, loss 0.290755, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:33.757330: step 5288, loss 0.263408, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:33.860217: step 5289, loss 0.178354, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:33.935495: step 5290, loss 0.109732, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:34.009656: step 5291, loss 0.104694, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:34.079204: step 5292, loss 0.210418, acc 0.941176, learning_rate 0.0001
2017-10-11T11:12:34.164907: step 5293, loss 0.285847, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:34.247411: step 5294, loss 0.129581, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:34.325475: step 5295, loss 0.183262, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:34.403226: step 5296, loss 0.086159, acc 1, learning_rate 0.0001
2017-10-11T11:12:34.518281: step 5297, loss 0.282097, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:34.627938: step 5298, loss 0.210542, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:34.736830: step 5299, loss 0.181679, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:34.849124: step 5300, loss 0.226253, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:34.960899: step 5301, loss 0.112785, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:35.068397: step 5302, loss 0.244282, acc 0.875, learning_rate 0.0001
2017-10-11T11:12:35.179037: step 5303, loss 0.228433, acc 0.875, learning_rate 0.0001
2017-10-11T11:12:35.282964: step 5304, loss 0.237232, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:35.380901: step 5305, loss 0.153065, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:35.496859: step 5306, loss 0.185293, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:35.604873: step 5307, loss 0.174576, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:35.718193: step 5308, loss 0.120011, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:35.828846: step 5309, loss 0.296918, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:35.936872: step 5310, loss 0.211714, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:36.038488: step 5311, loss 0.12443, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:36.146178: step 5312, loss 0.19058, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:36.264860: step 5313, loss 0.179925, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:36.360946: step 5314, loss 0.111454, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:36.465844: step 5315, loss 0.144258, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:36.560891: step 5316, loss 0.167206, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:36.665814: step 5317, loss 0.120893, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:36.778564: step 5318, loss 0.205775, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:36.893243: step 5319, loss 0.169382, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:37.007205: step 5320, loss 0.114454, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:37.233212: step 5320, loss 0.227046, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5320

2017-10-11T11:12:38.327105: step 5321, loss 0.202119, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:38.448422: step 5322, loss 0.310298, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:38.560918: step 5323, loss 0.240056, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:38.675807: step 5324, loss 0.187582, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:38.795874: step 5325, loss 0.110868, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:38.901639: step 5326, loss 0.103487, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:39.011633: step 5327, loss 0.161523, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:39.144879: step 5328, loss 0.13826, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:39.302521: step 5329, loss 0.236437, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:39.377554: step 5330, loss 0.173204, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:39.460794: step 5331, loss 0.068142, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:39.539326: step 5332, loss 0.17623, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:39.616550: step 5333, loss 0.185754, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:39.690384: step 5334, loss 0.134342, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:39.767865: step 5335, loss 0.15485, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:39.877312: step 5336, loss 0.241119, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:39.988706: step 5337, loss 0.22813, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:40.097163: step 5338, loss 0.177591, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:40.213567: step 5339, loss 0.22169, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:40.329219: step 5340, loss 0.146459, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:40.441186: step 5341, loss 0.163046, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:40.556433: step 5342, loss 0.132798, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:40.675066: step 5343, loss 0.25109, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:40.785118: step 5344, loss 0.15273, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:40.894009: step 5345, loss 0.19491, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:41.011297: step 5346, loss 0.165683, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:41.124961: step 5347, loss 0.118662, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:41.236561: step 5348, loss 0.116842, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:41.349718: step 5349, loss 0.116952, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:41.457067: step 5350, loss 0.152887, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:41.571601: step 5351, loss 0.207241, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:41.685629: step 5352, loss 0.214857, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:41.795049: step 5353, loss 0.140825, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:41.903031: step 5354, loss 0.17145, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:42.026783: step 5355, loss 0.164469, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:42.138799: step 5356, loss 0.208303, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:42.253735: step 5357, loss 0.178381, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:42.366555: step 5358, loss 0.164601, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:42.476394: step 5359, loss 0.143247, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:42.590781: step 5360, loss 0.204918, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:42.782823: step 5360, loss 0.226988, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5360

2017-10-11T11:12:43.687997: step 5361, loss 0.148843, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:43.795743: step 5362, loss 0.0977376, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:43.908116: step 5363, loss 0.206074, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:44.019574: step 5364, loss 0.120915, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:44.134945: step 5365, loss 0.171309, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:44.225000: step 5366, loss 0.132474, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:44.412936: step 5367, loss 0.136735, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:44.491099: step 5368, loss 0.334764, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:44.563899: step 5369, loss 0.156599, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:44.642585: step 5370, loss 0.221613, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:44.717017: step 5371, loss 0.118106, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:44.793431: step 5372, loss 0.218062, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:44.871605: step 5373, loss 0.219679, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:44.947553: step 5374, loss 0.220636, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:45.040104: step 5375, loss 0.125339, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:45.143573: step 5376, loss 0.165701, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:45.258922: step 5377, loss 0.113128, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:45.372624: step 5378, loss 0.138315, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:45.484414: step 5379, loss 0.196376, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:45.597763: step 5380, loss 0.129662, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:45.714662: step 5381, loss 0.313528, acc 0.875, learning_rate 0.0001
2017-10-11T11:12:45.810105: step 5382, loss 0.228275, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:45.927899: step 5383, loss 0.114404, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:46.050863: step 5384, loss 0.139617, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:46.163882: step 5385, loss 0.167129, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:46.271241: step 5386, loss 0.155548, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:46.380836: step 5387, loss 0.152803, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:46.492777: step 5388, loss 0.165403, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:46.611659: step 5389, loss 0.116777, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:46.718473: step 5390, loss 0.211966, acc 0.941176, learning_rate 0.0001
2017-10-11T11:12:46.846227: step 5391, loss 0.117873, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:46.952888: step 5392, loss 0.271737, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:47.064863: step 5393, loss 0.0840353, acc 1, learning_rate 0.0001
2017-10-11T11:12:47.184105: step 5394, loss 0.28422, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:47.291593: step 5395, loss 0.208272, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:47.401480: step 5396, loss 0.116386, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:47.518461: step 5397, loss 0.234201, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:47.623578: step 5398, loss 0.170042, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:47.733560: step 5399, loss 0.201885, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:47.845588: step 5400, loss 0.158505, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:48.034386: step 5400, loss 0.226258, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5400

2017-10-11T11:12:49.034822: step 5401, loss 0.119505, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:49.153946: step 5402, loss 0.133139, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:49.265823: step 5403, loss 0.178376, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:49.383347: step 5404, loss 0.23714, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:49.573258: step 5405, loss 0.133229, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:49.673048: step 5406, loss 0.117276, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:49.751644: step 5407, loss 0.138821, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:49.833780: step 5408, loss 0.135613, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:49.912163: step 5409, loss 0.135664, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:49.999270: step 5410, loss 0.180526, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:50.075460: step 5411, loss 0.316846, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:50.149203: step 5412, loss 0.156514, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:50.225976: step 5413, loss 0.218625, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:50.349471: step 5414, loss 0.233847, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:50.470866: step 5415, loss 0.151244, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:50.589457: step 5416, loss 0.0995187, acc 1, learning_rate 0.0001
2017-10-11T11:12:50.704809: step 5417, loss 0.140367, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:50.810730: step 5418, loss 0.238556, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:50.919176: step 5419, loss 0.151464, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:51.032270: step 5420, loss 0.100273, acc 1, learning_rate 0.0001
2017-10-11T11:12:51.152421: step 5421, loss 0.106881, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:51.262954: step 5422, loss 0.188586, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:51.372658: step 5423, loss 0.174866, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:51.486611: step 5424, loss 0.112506, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:51.595326: step 5425, loss 0.315551, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:51.710508: step 5426, loss 0.124198, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:51.836055: step 5427, loss 0.163561, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:51.943774: step 5428, loss 0.297342, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:52.054756: step 5429, loss 0.168619, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:52.150250: step 5430, loss 0.165236, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:52.244981: step 5431, loss 0.222765, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:52.349545: step 5432, loss 0.197855, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:52.455208: step 5433, loss 0.145173, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:52.574561: step 5434, loss 0.150555, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:52.689904: step 5435, loss 0.178862, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:52.800919: step 5436, loss 0.211862, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:52.909124: step 5437, loss 0.165531, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:53.037086: step 5438, loss 0.097046, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:53.155702: step 5439, loss 0.142935, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:53.259640: step 5440, loss 0.156347, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:53.473665: step 5440, loss 0.225815, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5440

2017-10-11T11:12:54.267157: step 5441, loss 0.11832, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:54.370398: step 5442, loss 0.129574, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:54.476860: step 5443, loss 0.22576, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:54.584241: step 5444, loss 0.179707, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:54.698276: step 5445, loss 0.194513, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:54.900970: step 5446, loss 0.220535, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:54.982566: step 5447, loss 0.117556, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:55.057660: step 5448, loss 0.113571, acc 1, learning_rate 0.0001
2017-10-11T11:12:55.134968: step 5449, loss 0.131501, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:55.208709: step 5450, loss 0.176306, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:55.280048: step 5451, loss 0.191747, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:55.364845: step 5452, loss 0.156013, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:55.436795: step 5453, loss 0.183026, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:55.525062: step 5454, loss 0.167915, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:55.640618: step 5455, loss 0.285041, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:55.753631: step 5456, loss 0.196196, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:55.872282: step 5457, loss 0.355854, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:55.982745: step 5458, loss 0.248274, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:56.103907: step 5459, loss 0.167596, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:56.210160: step 5460, loss 0.332259, acc 0.875, learning_rate 0.0001
2017-10-11T11:12:56.322721: step 5461, loss 0.161804, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:56.434343: step 5462, loss 0.26326, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:56.549188: step 5463, loss 0.153133, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:56.662559: step 5464, loss 0.107735, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:56.772207: step 5465, loss 0.224399, acc 0.890625, learning_rate 0.0001
2017-10-11T11:12:56.891930: step 5466, loss 0.117459, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:57.001707: step 5467, loss 0.165421, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:57.110599: step 5468, loss 0.104063, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:57.224714: step 5469, loss 0.162645, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:57.334888: step 5470, loss 0.2063, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:57.432864: step 5471, loss 0.225988, acc 0.90625, learning_rate 0.0001
2017-10-11T11:12:57.533016: step 5472, loss 0.160327, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:57.648894: step 5473, loss 0.250452, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:57.750546: step 5474, loss 0.12206, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:57.849150: step 5475, loss 0.142414, acc 0.921875, learning_rate 0.0001
2017-10-11T11:12:57.958665: step 5476, loss 0.248268, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:58.069562: step 5477, loss 0.0966753, acc 1, learning_rate 0.0001
2017-10-11T11:12:58.163179: step 5478, loss 0.140175, acc 0.984375, learning_rate 0.0001
2017-10-11T11:12:58.276471: step 5479, loss 0.198319, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:58.395502: step 5480, loss 0.328688, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:12:58.597889: step 5480, loss 0.225813, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5480

2017-10-11T11:12:59.501354: step 5481, loss 0.119789, acc 0.953125, learning_rate 0.0001
2017-10-11T11:12:59.614452: step 5482, loss 0.216373, acc 0.96875, learning_rate 0.0001
2017-10-11T11:12:59.727037: step 5483, loss 0.164852, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:59.836899: step 5484, loss 0.168041, acc 0.9375, learning_rate 0.0001
2017-10-11T11:12:59.949784: step 5485, loss 0.256855, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:00.121139: step 5486, loss 0.145381, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:00.227851: step 5487, loss 0.164663, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:00.290818: step 5488, loss 0.158571, acc 0.960784, learning_rate 0.0001
2017-10-11T11:13:00.367801: step 5489, loss 0.106256, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:00.442427: step 5490, loss 0.16948, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:00.516663: step 5491, loss 0.173795, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:00.604022: step 5492, loss 0.123401, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:00.681229: step 5493, loss 0.272458, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:00.758348: step 5494, loss 0.210053, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:00.872165: step 5495, loss 0.168956, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:00.974186: step 5496, loss 0.268352, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:01.086990: step 5497, loss 0.0735843, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:01.197824: step 5498, loss 0.216369, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:01.311049: step 5499, loss 0.166766, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:01.422719: step 5500, loss 0.109435, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:01.520804: step 5501, loss 0.291756, acc 0.859375, learning_rate 0.0001
2017-10-11T11:13:01.644417: step 5502, loss 0.136733, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:01.761452: step 5503, loss 0.140165, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:01.875128: step 5504, loss 0.260478, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:01.976839: step 5505, loss 0.163118, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:02.101801: step 5506, loss 0.190193, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:02.210530: step 5507, loss 0.199163, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:02.327284: step 5508, loss 0.103993, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:02.431566: step 5509, loss 0.182017, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:02.543910: step 5510, loss 0.175149, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:02.658142: step 5511, loss 0.220141, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:02.771466: step 5512, loss 0.0960761, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:02.875820: step 5513, loss 0.111285, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:02.994339: step 5514, loss 0.165345, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:03.097773: step 5515, loss 0.252389, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:03.193660: step 5516, loss 0.180203, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:03.299154: step 5517, loss 0.203834, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:03.425661: step 5518, loss 0.110286, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:03.537728: step 5519, loss 0.268407, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:03.642112: step 5520, loss 0.174302, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:03.836403: step 5520, loss 0.225558, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5520

2017-10-11T11:13:04.710165: step 5521, loss 0.166458, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:04.801929: step 5522, loss 0.157988, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:04.928908: step 5523, loss 0.182753, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:05.036287: step 5524, loss 0.172193, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:05.152544: step 5525, loss 0.160177, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:05.333325: step 5526, loss 0.149711, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:05.439156: step 5527, loss 0.192371, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:05.521389: step 5528, loss 0.157677, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:05.599365: step 5529, loss 0.197996, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:05.672734: step 5530, loss 0.219182, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:05.750483: step 5531, loss 0.172145, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:05.831035: step 5532, loss 0.123312, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:05.910564: step 5533, loss 0.162584, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:06.004850: step 5534, loss 0.123131, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:06.116346: step 5535, loss 0.128407, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:06.214432: step 5536, loss 0.0869716, acc 1, learning_rate 0.0001
2017-10-11T11:13:06.322936: step 5537, loss 0.109623, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:06.437235: step 5538, loss 0.220604, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:06.545292: step 5539, loss 0.112002, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:06.654326: step 5540, loss 0.108256, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:06.763307: step 5541, loss 0.10406, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:06.873844: step 5542, loss 0.112706, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:06.976031: step 5543, loss 0.146287, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:07.067562: step 5544, loss 0.16758, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:07.173149: step 5545, loss 0.226376, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:07.285393: step 5546, loss 0.274743, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:07.389562: step 5547, loss 0.0992757, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:07.493113: step 5548, loss 0.106318, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:07.608891: step 5549, loss 0.186945, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:07.728407: step 5550, loss 0.177284, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:07.837540: step 5551, loss 0.232456, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:07.952890: step 5552, loss 0.156503, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:08.068894: step 5553, loss 0.288168, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:08.187598: step 5554, loss 0.0923811, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:08.307736: step 5555, loss 0.0713406, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:08.416876: step 5556, loss 0.162978, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:08.525295: step 5557, loss 0.198401, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:08.636425: step 5558, loss 0.0895828, acc 1, learning_rate 0.0001
2017-10-11T11:13:08.753323: step 5559, loss 0.197376, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:08.868451: step 5560, loss 0.0862829, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:09.056935: step 5560, loss 0.22565, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5560

2017-10-11T11:13:10.094217: step 5561, loss 0.253823, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:10.197907: step 5562, loss 0.136306, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:10.299310: step 5563, loss 0.11402, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:10.405897: step 5564, loss 0.15274, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:10.589847: step 5565, loss 0.194555, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:10.693678: step 5566, loss 0.228014, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:10.766949: step 5567, loss 0.220726, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:10.844797: step 5568, loss 0.130871, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:10.917558: step 5569, loss 0.217051, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:10.990347: step 5570, loss 0.116807, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:11.066153: step 5571, loss 0.181711, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:11.139234: step 5572, loss 0.144239, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:11.214710: step 5573, loss 0.248188, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:11.305016: step 5574, loss 0.165792, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:11.413134: step 5575, loss 0.138831, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:11.515052: step 5576, loss 0.212722, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:11.617105: step 5577, loss 0.203805, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:11.727753: step 5578, loss 0.260388, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:11.844827: step 5579, loss 0.106218, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:11.959472: step 5580, loss 0.115143, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:12.068385: step 5581, loss 0.194142, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:12.177411: step 5582, loss 0.157515, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:12.297226: step 5583, loss 0.108018, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:12.404341: step 5584, loss 0.138986, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:12.522832: step 5585, loss 0.115747, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:12.628348: step 5586, loss 0.115974, acc 0.980392, learning_rate 0.0001
2017-10-11T11:13:12.741014: step 5587, loss 0.119757, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:12.850597: step 5588, loss 0.180721, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:12.951079: step 5589, loss 0.14521, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:13.046410: step 5590, loss 0.137154, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:13.156872: step 5591, loss 0.18028, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:13.264671: step 5592, loss 0.112043, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:13.364961: step 5593, loss 0.220787, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:13.474403: step 5594, loss 0.100009, acc 1, learning_rate 0.0001
2017-10-11T11:13:13.584858: step 5595, loss 0.163701, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:13.696915: step 5596, loss 0.213647, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:13.821018: step 5597, loss 0.165799, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:13.931753: step 5598, loss 0.214457, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:14.059622: step 5599, loss 0.114888, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:14.167748: step 5600, loss 0.27664, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:14.340032: step 5600, loss 0.224963, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5600

2017-10-11T11:13:15.197268: step 5601, loss 0.16645, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:15.303086: step 5602, loss 0.252505, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:15.427670: step 5603, loss 0.126726, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:15.540900: step 5604, loss 0.0997014, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:15.656100: step 5605, loss 0.25797, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:15.770016: step 5606, loss 0.349516, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:15.940995: step 5607, loss 0.127505, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:16.060703: step 5608, loss 0.196284, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:16.137902: step 5609, loss 0.129542, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:16.209330: step 5610, loss 0.201623, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:16.283059: step 5611, loss 0.21181, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:16.358097: step 5612, loss 0.127771, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:16.429081: step 5613, loss 0.217923, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:16.505373: step 5614, loss 0.23087, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:16.586834: step 5615, loss 0.0787683, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:16.674086: step 5616, loss 0.19206, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:16.773504: step 5617, loss 0.233641, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:16.881657: step 5618, loss 0.264707, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:16.994912: step 5619, loss 0.158793, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:17.111190: step 5620, loss 0.320636, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:17.231357: step 5621, loss 0.134288, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:17.338235: step 5622, loss 0.106807, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:17.448711: step 5623, loss 0.203791, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:17.563065: step 5624, loss 0.205616, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:17.650116: step 5625, loss 0.0796966, acc 1, learning_rate 0.0001
2017-10-11T11:13:17.761725: step 5626, loss 0.299487, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:17.864864: step 5627, loss 0.175098, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:17.965932: step 5628, loss 0.152853, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:18.069219: step 5629, loss 0.339905, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:18.176851: step 5630, loss 0.11241, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:18.284855: step 5631, loss 0.365346, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:18.404944: step 5632, loss 0.158838, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:18.532704: step 5633, loss 0.118171, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:18.650275: step 5634, loss 0.119939, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:18.745010: step 5635, loss 0.191161, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:18.842145: step 5636, loss 0.174154, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:18.943483: step 5637, loss 0.179904, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:19.060351: step 5638, loss 0.30408, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:19.176861: step 5639, loss 0.19208, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:19.290161: step 5640, loss 0.244515, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:19.490994: step 5640, loss 0.225372, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5640

2017-10-11T11:13:20.330130: step 5641, loss 0.260434, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:20.426177: step 5642, loss 0.153803, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:20.538639: step 5643, loss 0.113395, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:20.648829: step 5644, loss 0.196442, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:20.749570: step 5645, loss 0.149876, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:20.867340: step 5646, loss 0.258344, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:20.986254: step 5647, loss 0.214875, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:21.108528: step 5648, loss 0.190365, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:21.298858: step 5649, loss 0.180417, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:21.388210: step 5650, loss 0.160907, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:21.460768: step 5651, loss 0.0649867, acc 1, learning_rate 0.0001
2017-10-11T11:13:21.533874: step 5652, loss 0.228017, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:21.607681: step 5653, loss 0.234954, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:21.679047: step 5654, loss 0.158604, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:21.750645: step 5655, loss 0.156515, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:21.831732: step 5656, loss 0.203065, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:21.936833: step 5657, loss 0.20314, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:22.058044: step 5658, loss 0.218189, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:22.173121: step 5659, loss 0.246592, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:22.286209: step 5660, loss 0.162811, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:22.393864: step 5661, loss 0.0985206, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:22.492453: step 5662, loss 0.103857, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:22.588915: step 5663, loss 0.200175, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:22.719092: step 5664, loss 0.207334, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:22.830287: step 5665, loss 0.124265, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:22.944063: step 5666, loss 0.21008, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:23.065822: step 5667, loss 0.161921, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:23.174332: step 5668, loss 0.126049, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:23.286525: step 5669, loss 0.232683, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:23.401435: step 5670, loss 0.180742, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:23.519755: step 5671, loss 0.148904, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:23.627091: step 5672, loss 0.2656, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:23.749957: step 5673, loss 0.135184, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:23.858505: step 5674, loss 0.0746854, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:23.976113: step 5675, loss 0.122085, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:24.090028: step 5676, loss 0.198081, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:24.202793: step 5677, loss 0.180145, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:24.324438: step 5678, loss 0.0968028, acc 1, learning_rate 0.0001
2017-10-11T11:13:24.429207: step 5679, loss 0.144321, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:24.531251: step 5680, loss 0.176382, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:24.731277: step 5680, loss 0.225333, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5680

2017-10-11T11:13:25.716390: step 5681, loss 0.135238, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:25.825866: step 5682, loss 0.107051, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:25.940714: step 5683, loss 0.136218, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:26.022367: step 5684, loss 0.151781, acc 0.980392, learning_rate 0.0001
2017-10-11T11:13:26.121000: step 5685, loss 0.198278, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:26.227461: step 5686, loss 0.108457, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:26.326815: step 5687, loss 0.137888, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:26.489051: step 5688, loss 0.162998, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:26.588354: step 5689, loss 0.225549, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:26.666362: step 5690, loss 0.153895, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:26.754063: step 5691, loss 0.116688, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:26.839262: step 5692, loss 0.194772, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:26.928883: step 5693, loss 0.178893, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:27.006917: step 5694, loss 0.102128, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:27.083192: step 5695, loss 0.138821, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:27.164107: step 5696, loss 0.23693, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:27.277370: step 5697, loss 0.109899, acc 1, learning_rate 0.0001
2017-10-11T11:13:27.384926: step 5698, loss 0.130566, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:27.489510: step 5699, loss 0.209398, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:27.613382: step 5700, loss 0.190244, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:27.726287: step 5701, loss 0.201699, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:27.824955: step 5702, loss 0.236176, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:27.945738: step 5703, loss 0.270505, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:28.061908: step 5704, loss 0.114016, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:28.174632: step 5705, loss 0.158323, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:28.279588: step 5706, loss 0.157419, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:28.390654: step 5707, loss 0.247124, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:28.495699: step 5708, loss 0.0975769, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:28.611980: step 5709, loss 0.146531, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:28.717283: step 5710, loss 0.0933326, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:28.831027: step 5711, loss 0.168849, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:28.949655: step 5712, loss 0.140781, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:29.058865: step 5713, loss 0.209964, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:29.175351: step 5714, loss 0.312153, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:29.285781: step 5715, loss 0.128686, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:29.373191: step 5716, loss 0.132952, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:29.465702: step 5717, loss 0.195097, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:29.569518: step 5718, loss 0.0790815, acc 1, learning_rate 0.0001
2017-10-11T11:13:29.668386: step 5719, loss 0.200056, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:29.780882: step 5720, loss 0.234296, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:29.970511: step 5720, loss 0.224537, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5720

2017-10-11T11:13:30.781751: step 5721, loss 0.0580014, acc 1, learning_rate 0.0001
2017-10-11T11:13:30.888034: step 5722, loss 0.166346, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:31.000549: step 5723, loss 0.142295, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:31.107649: step 5724, loss 0.206029, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:31.227221: step 5725, loss 0.234547, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:31.343672: step 5726, loss 0.147709, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:31.456736: step 5727, loss 0.130454, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:31.565162: step 5728, loss 0.080001, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:31.680412: step 5729, loss 0.209259, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:31.852756: step 5730, loss 0.184367, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:31.929276: step 5731, loss 0.276234, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:32.003116: step 5732, loss 0.137572, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:32.077434: step 5733, loss 0.14034, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:32.153304: step 5734, loss 0.0773234, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:32.235955: step 5735, loss 0.16666, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:32.310161: step 5736, loss 0.215331, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:32.383968: step 5737, loss 0.0819002, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:32.478998: step 5738, loss 0.155961, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:32.579605: step 5739, loss 0.206012, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:32.704950: step 5740, loss 0.218553, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:32.806751: step 5741, loss 0.145298, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:32.901028: step 5742, loss 0.153993, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:33.009899: step 5743, loss 0.163785, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:33.119463: step 5744, loss 0.213507, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:33.234088: step 5745, loss 0.16897, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:33.361626: step 5746, loss 0.0745571, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:33.472836: step 5747, loss 0.185109, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:33.584530: step 5748, loss 0.264699, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:33.692414: step 5749, loss 0.157674, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:33.804805: step 5750, loss 0.117998, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:33.906799: step 5751, loss 0.232426, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:34.027767: step 5752, loss 0.159008, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:34.139190: step 5753, loss 0.102792, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:34.252115: step 5754, loss 0.156297, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:34.362484: step 5755, loss 0.216912, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:34.479321: step 5756, loss 0.29272, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:34.593318: step 5757, loss 0.149292, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:34.705477: step 5758, loss 0.144871, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:34.816052: step 5759, loss 0.170809, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:34.930103: step 5760, loss 0.0756565, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:35.119028: step 5760, loss 0.223731, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5760

2017-10-11T11:13:36.167909: step 5761, loss 0.152929, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:36.278225: step 5762, loss 0.130688, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:36.371909: step 5763, loss 0.178862, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:36.495689: step 5764, loss 0.244198, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:36.609880: step 5765, loss 0.225072, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:36.725123: step 5766, loss 0.224807, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:36.904878: step 5767, loss 0.154867, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:37.026264: step 5768, loss 0.245612, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:37.097237: step 5769, loss 0.107676, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:37.173843: step 5770, loss 0.139275, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:37.249758: step 5771, loss 0.107409, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:37.325614: step 5772, loss 0.209798, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:37.404049: step 5773, loss 0.213185, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:37.477412: step 5774, loss 0.146065, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:37.550529: step 5775, loss 0.146288, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:37.668523: step 5776, loss 0.130403, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:37.780704: step 5777, loss 0.227632, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:37.894409: step 5778, loss 0.119621, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:38.003517: step 5779, loss 0.123234, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:38.122305: step 5780, loss 0.202443, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:38.242726: step 5781, loss 0.127419, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:38.344496: step 5782, loss 0.192155, acc 0.941176, learning_rate 0.0001
2017-10-11T11:13:38.462866: step 5783, loss 0.153245, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:38.570448: step 5784, loss 0.122764, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:38.668842: step 5785, loss 0.154588, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:38.764876: step 5786, loss 0.185319, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:38.858760: step 5787, loss 0.148055, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:38.972379: step 5788, loss 0.130367, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:39.086411: step 5789, loss 0.140414, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:39.182607: step 5790, loss 0.076427, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:39.296612: step 5791, loss 0.197627, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:39.400799: step 5792, loss 0.205403, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:39.507492: step 5793, loss 0.0970853, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:39.615721: step 5794, loss 0.0965006, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:39.723841: step 5795, loss 0.261865, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:39.825168: step 5796, loss 0.118084, acc 1, learning_rate 0.0001
2017-10-11T11:13:39.939565: step 5797, loss 0.133134, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:40.054446: step 5798, loss 0.107712, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:40.167417: step 5799, loss 0.10537, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:40.272241: step 5800, loss 0.109443, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:40.465755: step 5800, loss 0.22375, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5800

2017-10-11T11:13:41.664273: step 5801, loss 0.255577, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:41.772928: step 5802, loss 0.20625, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:41.891075: step 5803, loss 0.147558, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:42.047701: step 5804, loss 0.107017, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:42.164191: step 5805, loss 0.134337, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:42.246164: step 5806, loss 0.16944, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:42.325810: step 5807, loss 0.142604, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:42.403341: step 5808, loss 0.262482, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:42.483444: step 5809, loss 0.231867, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:42.567563: step 5810, loss 0.251362, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:42.651622: step 5811, loss 0.117098, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:42.736286: step 5812, loss 0.0852363, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:42.864663: step 5813, loss 0.172547, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:42.984863: step 5814, loss 0.130878, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:43.094865: step 5815, loss 0.195207, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:43.211158: step 5816, loss 0.0932071, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:43.313059: step 5817, loss 0.163158, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:43.419050: step 5818, loss 0.340075, acc 0.859375, learning_rate 0.0001
2017-10-11T11:13:43.520631: step 5819, loss 0.149461, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:43.628875: step 5820, loss 0.309491, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:43.740864: step 5821, loss 0.10143, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:43.857981: step 5822, loss 0.235242, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:43.960603: step 5823, loss 0.122925, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:44.088921: step 5824, loss 0.139609, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:44.205520: step 5825, loss 0.253355, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:44.317442: step 5826, loss 0.0992545, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:44.426820: step 5827, loss 0.103826, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:44.530555: step 5828, loss 0.198936, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:44.623312: step 5829, loss 0.188904, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:44.747703: step 5830, loss 0.232562, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:44.848855: step 5831, loss 0.147018, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:44.974268: step 5832, loss 0.196452, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:45.089665: step 5833, loss 0.24696, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:45.205711: step 5834, loss 0.292759, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:45.320492: step 5835, loss 0.10934, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:45.434526: step 5836, loss 0.163415, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:45.549150: step 5837, loss 0.274015, acc 0.875, learning_rate 0.0001
2017-10-11T11:13:45.662864: step 5838, loss 0.106738, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:45.777076: step 5839, loss 0.0995612, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:45.887197: step 5840, loss 0.0926752, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:46.063460: step 5840, loss 0.222983, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5840

2017-10-11T11:13:46.849428: step 5841, loss 0.247719, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:46.956751: step 5842, loss 0.0827494, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:47.061577: step 5843, loss 0.146384, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:47.182799: step 5844, loss 0.132699, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:47.369378: step 5845, loss 0.216588, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:47.461350: step 5846, loss 0.112496, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:47.541014: step 5847, loss 0.193779, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:47.618655: step 5848, loss 0.240991, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:47.692793: step 5849, loss 0.143248, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:47.768891: step 5850, loss 0.181599, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:47.840800: step 5851, loss 0.180468, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:47.918098: step 5852, loss 0.232441, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:47.998107: step 5853, loss 0.102345, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:48.114870: step 5854, loss 0.179502, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:48.231256: step 5855, loss 0.177016, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:48.340946: step 5856, loss 0.0928896, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:48.456145: step 5857, loss 0.234863, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:48.565848: step 5858, loss 0.213098, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:48.676675: step 5859, loss 0.0911455, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:48.790670: step 5860, loss 0.17929, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:48.904870: step 5861, loss 0.179175, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:49.021408: step 5862, loss 0.179022, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:49.139084: step 5863, loss 0.168511, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:49.256574: step 5864, loss 0.165364, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:49.362098: step 5865, loss 0.0616578, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:49.479957: step 5866, loss 0.187151, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:49.590292: step 5867, loss 0.182383, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:49.700824: step 5868, loss 0.187466, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:49.808823: step 5869, loss 0.147414, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:49.899310: step 5870, loss 0.0953867, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:49.995363: step 5871, loss 0.20174, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:50.112366: step 5872, loss 0.178867, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:50.228069: step 5873, loss 0.25034, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:50.348858: step 5874, loss 0.176909, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:50.455691: step 5875, loss 0.12157, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:50.566191: step 5876, loss 0.24477, acc 0.890625, learning_rate 0.0001
2017-10-11T11:13:50.679713: step 5877, loss 0.117754, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:50.790844: step 5878, loss 0.0682678, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:50.899891: step 5879, loss 0.154025, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:51.006628: step 5880, loss 0.18714, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:51.196569: step 5880, loss 0.222772, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5880

2017-10-11T11:13:52.102888: step 5881, loss 0.167201, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:52.222871: step 5882, loss 0.11793, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:52.337360: step 5883, loss 0.177699, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:52.453494: step 5884, loss 0.14856, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:52.608882: step 5885, loss 0.151426, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:52.742382: step 5886, loss 0.152489, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:52.813685: step 5887, loss 0.180883, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:52.887794: step 5888, loss 0.118611, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:52.967910: step 5889, loss 0.104935, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:53.039738: step 5890, loss 0.153208, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:53.109591: step 5891, loss 0.122048, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:53.196199: step 5892, loss 0.115522, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:53.279672: step 5893, loss 0.0992443, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:53.366490: step 5894, loss 0.0900682, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:53.440790: step 5895, loss 0.0734539, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:53.522841: step 5896, loss 0.174065, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:53.596808: step 5897, loss 0.200798, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:53.671169: step 5898, loss 0.140151, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:53.757786: step 5899, loss 0.103108, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:53.857931: step 5900, loss 0.16353, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:53.969886: step 5901, loss 0.123932, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:54.082212: step 5902, loss 0.204744, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:54.191217: step 5903, loss 0.156538, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:54.306606: step 5904, loss 0.235712, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:54.421204: step 5905, loss 0.20336, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:54.547914: step 5906, loss 0.125066, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:54.661777: step 5907, loss 0.127905, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:54.774310: step 5908, loss 0.127409, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:54.894180: step 5909, loss 0.150149, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:55.009404: step 5910, loss 0.129203, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:55.132860: step 5911, loss 0.125503, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:55.244864: step 5912, loss 0.209514, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:55.363212: step 5913, loss 0.120418, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:55.488027: step 5914, loss 0.201454, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:55.602166: step 5915, loss 0.194563, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:55.707642: step 5916, loss 0.181322, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:55.820504: step 5917, loss 0.13082, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:55.934310: step 5918, loss 0.279636, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:56.036857: step 5919, loss 0.118073, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:56.140847: step 5920, loss 0.220092, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:13:56.351575: step 5920, loss 0.222554, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5920

2017-10-11T11:13:57.363763: step 5921, loss 0.0949266, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:57.474868: step 5922, loss 0.192041, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:57.584981: step 5923, loss 0.216693, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:57.695426: step 5924, loss 0.108368, acc 0.984375, learning_rate 0.0001
2017-10-11T11:13:57.812392: step 5925, loss 0.154128, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:57.924701: step 5926, loss 0.210143, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:58.037099: step 5927, loss 0.181821, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:58.146718: step 5928, loss 0.12561, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:58.244952: step 5929, loss 0.150651, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:58.413207: step 5930, loss 0.325914, acc 0.859375, learning_rate 0.0001
2017-10-11T11:13:58.969038: step 5931, loss 0.0919185, acc 0.96875, learning_rate 0.0001
2017-10-11T11:13:59.045278: step 5932, loss 0.224536, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:59.120778: step 5933, loss 0.180044, acc 0.90625, learning_rate 0.0001
2017-10-11T11:13:59.206904: step 5934, loss 0.150563, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:59.294794: step 5935, loss 0.224269, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:59.382143: step 5936, loss 0.207503, acc 0.921875, learning_rate 0.0001
2017-10-11T11:13:59.457276: step 5937, loss 0.180569, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:59.532658: step 5938, loss 0.218101, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:59.644048: step 5939, loss 0.244501, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:59.758898: step 5940, loss 0.162971, acc 0.9375, learning_rate 0.0001
2017-10-11T11:13:59.885253: step 5941, loss 0.139783, acc 0.953125, learning_rate 0.0001
2017-10-11T11:13:59.991592: step 5942, loss 0.142372, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:00.103165: step 5943, loss 0.301429, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:00.214401: step 5944, loss 0.148069, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:00.331324: step 5945, loss 0.204719, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:00.442385: step 5946, loss 0.105662, acc 1, learning_rate 0.0001
2017-10-11T11:14:00.562172: step 5947, loss 0.152275, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:00.678909: step 5948, loss 0.168776, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:00.793096: step 5949, loss 0.203901, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:00.909404: step 5950, loss 0.164281, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:01.018343: step 5951, loss 0.170057, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:01.132525: step 5952, loss 0.112582, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:01.248602: step 5953, loss 0.152385, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:01.362683: step 5954, loss 0.178775, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:01.467660: step 5955, loss 0.182069, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:01.581675: step 5956, loss 0.155451, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:01.698788: step 5957, loss 0.278865, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:01.808106: step 5958, loss 0.153645, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:01.921590: step 5959, loss 0.230474, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:02.028431: step 5960, loss 0.0826495, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:02.217473: step 5960, loss 0.221808, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-5960

2017-10-11T11:14:02.995240: step 5961, loss 0.160472, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:03.097496: step 5962, loss 0.101481, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:03.211697: step 5963, loss 0.245527, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:03.324983: step 5964, loss 0.30972, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:03.446396: step 5965, loss 0.283281, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:03.556758: step 5966, loss 0.117441, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:03.671518: step 5967, loss 0.324224, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:03.779060: step 5968, loss 0.135401, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:03.899208: step 5969, loss 0.248024, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:04.017467: step 5970, loss 0.201988, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:04.194828: step 5971, loss 0.120283, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:04.294623: step 5972, loss 0.324837, acc 0.84375, learning_rate 0.0001
2017-10-11T11:14:04.371094: step 5973, loss 0.0799928, acc 1, learning_rate 0.0001
2017-10-11T11:14:04.448187: step 5974, loss 0.162667, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:04.523368: step 5975, loss 0.133317, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:04.597622: step 5976, loss 0.195601, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:04.673317: step 5977, loss 0.180553, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:04.736752: step 5978, loss 0.176482, acc 0.941176, learning_rate 0.0001
2017-10-11T11:14:04.850177: step 5979, loss 0.184991, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:04.964184: step 5980, loss 0.175373, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:05.080795: step 5981, loss 0.208637, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:05.195155: step 5982, loss 0.143115, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:05.306235: step 5983, loss 0.117629, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:05.425370: step 5984, loss 0.1506, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:05.549005: step 5985, loss 0.178618, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:05.662236: step 5986, loss 0.191663, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:05.773881: step 5987, loss 0.283592, acc 0.875, learning_rate 0.0001
2017-10-11T11:14:05.888122: step 5988, loss 0.101261, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:06.001271: step 5989, loss 0.160787, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:06.099264: step 5990, loss 0.107697, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:06.209965: step 5991, loss 0.205458, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:06.316855: step 5992, loss 0.148073, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:06.410873: step 5993, loss 0.0792188, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:06.532860: step 5994, loss 0.174374, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:06.635828: step 5995, loss 0.244102, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:06.733380: step 5996, loss 0.239279, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:06.851101: step 5997, loss 0.186, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:06.965411: step 5998, loss 0.114476, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:07.074238: step 5999, loss 0.197551, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:07.188757: step 6000, loss 0.171517, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:07.388982: step 6000, loss 0.22383, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6000

2017-10-11T11:14:08.354355: step 6001, loss 0.0989148, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:08.471971: step 6002, loss 0.189882, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:08.589675: step 6003, loss 0.166002, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:08.702147: step 6004, loss 0.346016, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:08.804828: step 6005, loss 0.123677, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:08.908858: step 6006, loss 0.103196, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:08.999356: step 6007, loss 0.148371, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:09.109731: step 6008, loss 0.204642, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:09.213022: step 6009, loss 0.180654, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:09.375561: step 6010, loss 0.145099, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:09.480827: step 6011, loss 0.260408, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:09.567503: step 6012, loss 0.188745, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:09.646339: step 6013, loss 0.176196, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:09.731199: step 6014, loss 0.23647, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:09.813972: step 6015, loss 0.119954, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:09.897640: step 6016, loss 0.162363, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:09.970620: step 6017, loss 0.23099, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:10.046243: step 6018, loss 0.127994, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:10.159167: step 6019, loss 0.208207, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:10.273119: step 6020, loss 0.1025, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:10.381404: step 6021, loss 0.245364, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:10.489745: step 6022, loss 0.142918, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:10.583980: step 6023, loss 0.179053, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:10.689573: step 6024, loss 0.11996, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:10.791078: step 6025, loss 0.241279, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:10.894030: step 6026, loss 0.246729, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:10.997348: step 6027, loss 0.163388, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:11.101228: step 6028, loss 0.193851, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:11.220758: step 6029, loss 0.103159, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:11.342741: step 6030, loss 0.128138, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:11.448775: step 6031, loss 0.18052, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:11.558815: step 6032, loss 0.192481, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:11.684894: step 6033, loss 0.261876, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:11.809153: step 6034, loss 0.211814, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:11.915413: step 6035, loss 0.18511, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:12.029017: step 6036, loss 0.239795, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:12.130769: step 6037, loss 0.0622961, acc 1, learning_rate 0.0001
2017-10-11T11:14:12.240774: step 6038, loss 0.198933, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:12.350808: step 6039, loss 0.181207, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:12.463944: step 6040, loss 0.227059, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:12.643006: step 6040, loss 0.222134, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6040

2017-10-11T11:14:13.724307: step 6041, loss 0.146959, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:13.836458: step 6042, loss 0.128313, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:13.945602: step 6043, loss 0.256457, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:14.058971: step 6044, loss 0.28396, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:14.169361: step 6045, loss 0.110358, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:14.268849: step 6046, loss 0.211098, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:14.380868: step 6047, loss 0.107215, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:14.484883: step 6048, loss 0.224852, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:14.676099: step 6049, loss 0.115531, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:14.751897: step 6050, loss 0.159477, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:14.828541: step 6051, loss 0.0915794, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:14.900710: step 6052, loss 0.210994, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:14.971370: step 6053, loss 0.253992, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:15.045880: step 6054, loss 0.147664, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:15.133618: step 6055, loss 0.136449, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:15.212115: step 6056, loss 0.153245, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:15.290783: step 6057, loss 0.0901763, acc 1, learning_rate 0.0001
2017-10-11T11:14:15.403401: step 6058, loss 0.286054, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:15.507063: step 6059, loss 0.0965153, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:15.627391: step 6060, loss 0.189305, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:15.753698: step 6061, loss 0.106866, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:15.859047: step 6062, loss 0.158673, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:15.963275: step 6063, loss 0.103667, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:16.087484: step 6064, loss 0.295357, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:16.209400: step 6065, loss 0.194683, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:16.330875: step 6066, loss 0.209734, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:16.453009: step 6067, loss 0.075836, acc 1, learning_rate 0.0001
2017-10-11T11:14:16.570899: step 6068, loss 0.0839968, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:16.689256: step 6069, loss 0.145009, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:16.799974: step 6070, loss 0.187673, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:16.908512: step 6071, loss 0.12878, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:17.008591: step 6072, loss 0.251906, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:17.128244: step 6073, loss 0.0927965, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:17.242956: step 6074, loss 0.156998, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:17.356513: step 6075, loss 0.116178, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:17.459880: step 6076, loss 0.152163, acc 0.960784, learning_rate 0.0001
2017-10-11T11:14:17.568348: step 6077, loss 0.146392, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:17.668786: step 6078, loss 0.118985, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:17.789353: step 6079, loss 0.119172, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:17.897704: step 6080, loss 0.0823791, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:18.092187: step 6080, loss 0.222294, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6080

2017-10-11T11:14:18.871314: step 6081, loss 0.143624, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:18.993827: step 6082, loss 0.0733424, acc 1, learning_rate 0.0001
2017-10-11T11:14:19.108069: step 6083, loss 0.204122, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:19.221153: step 6084, loss 0.128228, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:19.338609: step 6085, loss 0.278121, acc 0.875, learning_rate 0.0001
2017-10-11T11:14:19.459335: step 6086, loss 0.107439, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:19.572988: step 6087, loss 0.183058, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:19.690339: step 6088, loss 0.166292, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:19.832994: step 6089, loss 0.0819626, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:19.987863: step 6090, loss 0.165421, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:20.061519: step 6091, loss 0.0885977, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:20.143023: step 6092, loss 0.189696, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:20.224267: step 6093, loss 0.126424, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:20.296323: step 6094, loss 0.262021, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:20.367738: step 6095, loss 0.203067, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:20.444346: step 6096, loss 0.102127, acc 1, learning_rate 0.0001
2017-10-11T11:14:20.523829: step 6097, loss 0.177785, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:20.610123: step 6098, loss 0.20418, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:20.713067: step 6099, loss 0.199011, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:20.820814: step 6100, loss 0.0707498, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:20.937956: step 6101, loss 0.116469, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:21.060585: step 6102, loss 0.173983, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:21.166323: step 6103, loss 0.157389, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:21.283763: step 6104, loss 0.156095, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:21.402502: step 6105, loss 0.11422, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:21.513882: step 6106, loss 0.190731, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:21.630777: step 6107, loss 0.270348, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:21.755232: step 6108, loss 0.108608, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:21.870559: step 6109, loss 0.237974, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:21.983264: step 6110, loss 0.171934, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:22.096898: step 6111, loss 0.187045, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:22.204274: step 6112, loss 0.215734, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:22.315963: step 6113, loss 0.164954, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:22.422662: step 6114, loss 0.18626, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:22.531854: step 6115, loss 0.19674, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:22.643783: step 6116, loss 0.135939, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:22.751402: step 6117, loss 0.159442, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:22.846200: step 6118, loss 0.188645, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:22.942668: step 6119, loss 0.137952, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:23.035366: step 6120, loss 0.191423, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:23.234505: step 6120, loss 0.221838, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6120

2017-10-11T11:14:24.155556: step 6121, loss 0.207411, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:24.262330: step 6122, loss 0.11512, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:24.372118: step 6123, loss 0.139092, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:24.491010: step 6124, loss 0.150251, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:24.601718: step 6125, loss 0.211127, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:24.708115: step 6126, loss 0.157638, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:24.825670: step 6127, loss 0.138196, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:24.939962: step 6128, loss 0.210773, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:25.055815: step 6129, loss 0.109903, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:25.196902: step 6130, loss 0.0933927, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:25.339950: step 6131, loss 0.160458, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:25.417795: step 6132, loss 0.244018, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:25.498571: step 6133, loss 0.181228, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:25.583143: step 6134, loss 0.217163, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:25.659545: step 6135, loss 0.215571, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:25.736116: step 6136, loss 0.209541, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:25.812515: step 6137, loss 0.162147, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:25.883895: step 6138, loss 0.11926, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:25.980232: step 6139, loss 0.184187, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:26.083775: step 6140, loss 0.195055, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:26.194799: step 6141, loss 0.143128, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:26.300676: step 6142, loss 0.324481, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:26.414009: step 6143, loss 0.0969322, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:26.526046: step 6144, loss 0.209311, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:26.624175: step 6145, loss 0.262507, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:26.730639: step 6146, loss 0.172904, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:26.843867: step 6147, loss 0.101994, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:26.963555: step 6148, loss 0.134106, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:27.084853: step 6149, loss 0.191195, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:27.200275: step 6150, loss 0.137046, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:27.314228: step 6151, loss 0.10932, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:27.433641: step 6152, loss 0.100594, acc 1, learning_rate 0.0001
2017-10-11T11:14:27.552653: step 6153, loss 0.249363, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:27.667462: step 6154, loss 0.144269, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:27.784918: step 6155, loss 0.204826, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:27.888830: step 6156, loss 0.189721, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:28.000400: step 6157, loss 0.136155, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:28.108814: step 6158, loss 0.135886, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:28.224909: step 6159, loss 0.198059, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:28.333047: step 6160, loss 0.113821, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:28.525374: step 6160, loss 0.221959, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6160

2017-10-11T11:14:29.448286: step 6161, loss 0.138141, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:29.559108: step 6162, loss 0.116423, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:29.672345: step 6163, loss 0.064838, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:29.788764: step 6164, loss 0.195013, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:29.892824: step 6165, loss 0.208809, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:29.984158: step 6166, loss 0.165027, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:30.081306: step 6167, loss 0.0958845, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:30.204961: step 6168, loss 0.106534, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:30.304780: step 6169, loss 0.208018, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:30.420877: step 6170, loss 0.215944, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:30.585316: step 6171, loss 0.300745, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:30.693444: step 6172, loss 0.249043, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:30.804458: step 6173, loss 0.129055, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:30.871505: step 6174, loss 0.116527, acc 0.960784, learning_rate 0.0001
2017-10-11T11:14:30.955888: step 6175, loss 0.186926, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:31.030066: step 6176, loss 0.171835, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:31.109686: step 6177, loss 0.0919366, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:31.182336: step 6178, loss 0.219702, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:31.256465: step 6179, loss 0.143507, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:31.365304: step 6180, loss 0.21111, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:31.478474: step 6181, loss 0.157198, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:31.581275: step 6182, loss 0.168833, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:31.692222: step 6183, loss 0.112167, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:31.813066: step 6184, loss 0.175611, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:31.922048: step 6185, loss 0.257629, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:32.041592: step 6186, loss 0.0967858, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:32.151345: step 6187, loss 0.254621, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:32.255914: step 6188, loss 0.203382, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:32.367143: step 6189, loss 0.121007, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:32.480175: step 6190, loss 0.126557, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:32.597028: step 6191, loss 0.11046, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:32.709120: step 6192, loss 0.132521, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:32.826699: step 6193, loss 0.131869, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:32.938508: step 6194, loss 0.205616, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:33.055715: step 6195, loss 0.220868, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:33.167089: step 6196, loss 0.219389, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:33.280623: step 6197, loss 0.161523, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:33.395259: step 6198, loss 0.226386, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:33.505887: step 6199, loss 0.100627, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:33.617948: step 6200, loss 0.151759, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:33.807334: step 6200, loss 0.221814, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6200

2017-10-11T11:14:35.230518: step 6201, loss 0.169853, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:35.341849: step 6202, loss 0.147712, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:35.453248: step 6203, loss 0.32796, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:35.547659: step 6204, loss 0.143107, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:35.687711: step 6205, loss 0.119814, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:35.803019: step 6206, loss 0.177901, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:35.881804: step 6207, loss 0.131585, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:35.961349: step 6208, loss 0.295581, acc 0.859375, learning_rate 0.0001
2017-10-11T11:14:36.037377: step 6209, loss 0.143969, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:36.111010: step 6210, loss 0.1478, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:36.188841: step 6211, loss 0.0959698, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:36.260075: step 6212, loss 0.064413, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:36.335564: step 6213, loss 0.160925, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:36.443863: step 6214, loss 0.172479, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:36.548704: step 6215, loss 0.135236, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:36.664665: step 6216, loss 0.132451, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:36.779613: step 6217, loss 0.298159, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:36.883182: step 6218, loss 0.0680049, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:36.983415: step 6219, loss 0.148573, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:37.081132: step 6220, loss 0.137944, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:37.189132: step 6221, loss 0.201852, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:37.296927: step 6222, loss 0.0962481, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:37.401244: step 6223, loss 0.24993, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:37.511720: step 6224, loss 0.259093, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:37.617313: step 6225, loss 0.290708, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:37.738110: step 6226, loss 0.123593, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:37.840795: step 6227, loss 0.154353, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:37.956475: step 6228, loss 0.229329, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:38.072435: step 6229, loss 0.210006, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:38.185388: step 6230, loss 0.0904192, acc 1, learning_rate 0.0001
2017-10-11T11:14:38.295264: step 6231, loss 0.176252, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:38.396379: step 6232, loss 0.11396, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:38.503413: step 6233, loss 0.240685, acc 0.875, learning_rate 0.0001
2017-10-11T11:14:38.613661: step 6234, loss 0.14142, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:38.710082: step 6235, loss 0.0490624, acc 1, learning_rate 0.0001
2017-10-11T11:14:38.818673: step 6236, loss 0.132195, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:38.921271: step 6237, loss 0.139036, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:39.033011: step 6238, loss 0.256793, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:39.133269: step 6239, loss 0.0907696, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:39.234467: step 6240, loss 0.183587, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:39.414227: step 6240, loss 0.221556, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6240

2017-10-11T11:14:40.214709: step 6241, loss 0.173052, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:40.329321: step 6242, loss 0.170153, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:40.445152: step 6243, loss 0.193652, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:40.566313: step 6244, loss 0.155217, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:40.684205: step 6245, loss 0.202303, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:40.798266: step 6246, loss 0.203579, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:40.936855: step 6247, loss 0.149799, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:41.083264: step 6248, loss 0.179624, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:41.165954: step 6249, loss 0.1461, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:41.236997: step 6250, loss 0.198025, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:41.319467: step 6251, loss 0.190055, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:41.397184: step 6252, loss 0.120473, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:41.469729: step 6253, loss 0.285553, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:41.558614: step 6254, loss 0.193659, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:41.651665: step 6255, loss 0.32013, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:41.744859: step 6256, loss 0.184191, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:41.860304: step 6257, loss 0.156347, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:41.970290: step 6258, loss 0.0796745, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:42.080953: step 6259, loss 0.145717, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:42.190477: step 6260, loss 0.168361, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:42.285942: step 6261, loss 0.199896, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:42.394112: step 6262, loss 0.196452, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:42.510372: step 6263, loss 0.242104, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:42.622608: step 6264, loss 0.264025, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:42.746200: step 6265, loss 0.176844, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:42.855221: step 6266, loss 0.0987939, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:42.979075: step 6267, loss 0.15784, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:43.079854: step 6268, loss 0.263832, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:43.191758: step 6269, loss 0.172824, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:43.311743: step 6270, loss 0.184822, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:43.418602: step 6271, loss 0.216918, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:43.532137: step 6272, loss 0.167597, acc 0.941176, learning_rate 0.0001
2017-10-11T11:14:43.636626: step 6273, loss 0.133743, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:43.731756: step 6274, loss 0.123909, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:43.827864: step 6275, loss 0.178228, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:43.929284: step 6276, loss 0.199852, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:44.047762: step 6277, loss 0.126102, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:44.168237: step 6278, loss 0.168537, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:44.276061: step 6279, loss 0.103898, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:44.398175: step 6280, loss 0.142928, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:44.586951: step 6280, loss 0.221711, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6280

2017-10-11T11:14:45.496546: step 6281, loss 0.199018, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:45.598795: step 6282, loss 0.168116, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:45.721720: step 6283, loss 0.238776, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:45.840852: step 6284, loss 0.123781, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:45.955674: step 6285, loss 0.23763, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:46.095710: step 6286, loss 0.130512, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:46.244735: step 6287, loss 0.144828, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:46.358370: step 6288, loss 0.181828, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:46.435642: step 6289, loss 0.178121, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:46.512599: step 6290, loss 0.078103, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:46.589087: step 6291, loss 0.208228, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:46.667505: step 6292, loss 0.102427, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:46.744296: step 6293, loss 0.0857264, acc 1, learning_rate 0.0001
2017-10-11T11:14:46.823935: step 6294, loss 0.23119, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:46.898900: step 6295, loss 0.0959608, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:47.020020: step 6296, loss 0.179049, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:47.137330: step 6297, loss 0.209585, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:47.239955: step 6298, loss 0.26281, acc 0.875, learning_rate 0.0001
2017-10-11T11:14:47.352210: step 6299, loss 0.308454, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:47.466217: step 6300, loss 0.257152, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:47.576843: step 6301, loss 0.180807, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:47.682944: step 6302, loss 0.124265, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:47.791829: step 6303, loss 0.191336, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:47.900823: step 6304, loss 0.0598082, acc 1, learning_rate 0.0001
2017-10-11T11:14:47.994446: step 6305, loss 0.083975, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:48.090498: step 6306, loss 0.217755, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:48.203128: step 6307, loss 0.171265, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:48.319259: step 6308, loss 0.138344, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:48.432379: step 6309, loss 0.20822, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:48.542283: step 6310, loss 0.16737, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:48.658107: step 6311, loss 0.194383, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:48.771192: step 6312, loss 0.207587, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:48.889527: step 6313, loss 0.10346, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:49.015787: step 6314, loss 0.156507, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:49.133390: step 6315, loss 0.151953, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:49.248880: step 6316, loss 0.177367, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:49.371260: step 6317, loss 0.236566, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:49.488362: step 6318, loss 0.163849, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:49.600923: step 6319, loss 0.0410518, acc 1, learning_rate 0.0001
2017-10-11T11:14:49.706324: step 6320, loss 0.0982638, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:49.892004: step 6320, loss 0.22135, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6320

2017-10-11T11:14:51.442663: step 6321, loss 0.177857, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:51.560165: step 6322, loss 0.118692, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:51.737381: step 6323, loss 0.144334, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:51.842870: step 6324, loss 0.20069, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:51.917053: step 6325, loss 0.157263, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:51.991814: step 6326, loss 0.107294, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:52.067598: step 6327, loss 0.23405, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:52.142531: step 6328, loss 0.0833439, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:52.226561: step 6329, loss 0.120957, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:52.298422: step 6330, loss 0.135397, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:52.373747: step 6331, loss 0.17729, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:52.492016: step 6332, loss 0.169487, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:52.602114: step 6333, loss 0.289481, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:52.709121: step 6334, loss 0.201508, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:52.817437: step 6335, loss 0.136641, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:52.938879: step 6336, loss 0.128779, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:53.055062: step 6337, loss 0.123501, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:53.163663: step 6338, loss 0.0871179, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:53.276668: step 6339, loss 0.152961, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:53.365069: step 6340, loss 0.119093, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:53.465302: step 6341, loss 0.110707, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:53.568800: step 6342, loss 0.18695, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:53.681073: step 6343, loss 0.151613, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:53.780877: step 6344, loss 0.164943, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:53.899953: step 6345, loss 0.189515, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:54.018414: step 6346, loss 0.163365, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:54.116493: step 6347, loss 0.125541, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:54.228158: step 6348, loss 0.127773, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:54.343896: step 6349, loss 0.180444, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:54.461936: step 6350, loss 0.26849, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:54.577692: step 6351, loss 0.139435, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:54.689700: step 6352, loss 0.235374, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:54.802858: step 6353, loss 0.245315, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:54.915169: step 6354, loss 0.147635, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:55.023999: step 6355, loss 0.124125, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:55.136371: step 6356, loss 0.276878, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:55.243832: step 6357, loss 0.115234, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:55.360422: step 6358, loss 0.221476, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:55.481058: step 6359, loss 0.135601, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:55.784317: step 6360, loss 0.219668, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:14:55.990770: step 6360, loss 0.221376, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6360

2017-10-11T11:14:56.815902: step 6361, loss 0.170948, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:56.916724: step 6362, loss 0.0869439, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:57.022692: step 6363, loss 0.28452, acc 0.90625, learning_rate 0.0001
2017-10-11T11:14:57.134696: step 6364, loss 0.187531, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:57.296939: step 6365, loss 0.174658, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:57.428181: step 6366, loss 0.202968, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:57.505918: step 6367, loss 0.12189, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:57.583564: step 6368, loss 0.157622, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:57.657364: step 6369, loss 0.200851, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:57.735949: step 6370, loss 0.11306, acc 0.960784, learning_rate 0.0001
2017-10-11T11:14:57.809774: step 6371, loss 0.14928, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:57.886363: step 6372, loss 0.150143, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:57.963865: step 6373, loss 0.105787, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:58.071133: step 6374, loss 0.0980852, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:58.178529: step 6375, loss 0.13287, acc 0.984375, learning_rate 0.0001
2017-10-11T11:14:58.283847: step 6376, loss 0.259421, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:58.394292: step 6377, loss 0.157814, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:58.505064: step 6378, loss 0.186736, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:58.618426: step 6379, loss 0.209025, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:58.732734: step 6380, loss 0.181025, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:58.846352: step 6381, loss 0.157606, acc 0.9375, learning_rate 0.0001
2017-10-11T11:14:58.949878: step 6382, loss 0.210314, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:59.065153: step 6383, loss 0.138872, acc 0.921875, learning_rate 0.0001
2017-10-11T11:14:59.175308: step 6384, loss 0.128187, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:59.282991: step 6385, loss 0.270454, acc 0.859375, learning_rate 0.0001
2017-10-11T11:14:59.404105: step 6386, loss 0.125619, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:59.517535: step 6387, loss 0.266508, acc 0.890625, learning_rate 0.0001
2017-10-11T11:14:59.623209: step 6388, loss 0.156916, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:59.731681: step 6389, loss 0.143827, acc 0.953125, learning_rate 0.0001
2017-10-11T11:14:59.834667: step 6390, loss 0.129077, acc 0.96875, learning_rate 0.0001
2017-10-11T11:14:59.955011: step 6391, loss 0.163702, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:00.071982: step 6392, loss 0.173097, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:00.184067: step 6393, loss 0.0719761, acc 1, learning_rate 0.0001
2017-10-11T11:15:00.296981: step 6394, loss 0.146883, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:00.410454: step 6395, loss 0.105882, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:00.526438: step 6396, loss 0.236453, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:00.638005: step 6397, loss 0.171048, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:00.750328: step 6398, loss 0.179178, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:00.865438: step 6399, loss 0.159916, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:00.967917: step 6400, loss 0.17869, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:01.158768: step 6400, loss 0.220796, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6400

2017-10-11T11:15:02.068804: step 6401, loss 0.191965, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:02.180699: step 6402, loss 0.132194, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:02.294373: step 6403, loss 0.216471, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:02.409699: step 6404, loss 0.177067, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:02.578882: step 6405, loss 0.179719, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:02.658011: step 6406, loss 0.0649028, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:02.743308: step 6407, loss 0.159638, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:02.815916: step 6408, loss 0.13348, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:02.895153: step 6409, loss 0.0936967, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:02.970798: step 6410, loss 0.215745, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:03.044470: step 6411, loss 0.260087, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:03.119069: step 6412, loss 0.14008, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:03.195324: step 6413, loss 0.192802, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:03.310433: step 6414, loss 0.0972091, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:03.428507: step 6415, loss 0.197747, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:03.542475: step 6416, loss 0.21296, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:03.649962: step 6417, loss 0.0987225, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:03.763777: step 6418, loss 0.245972, acc 0.875, learning_rate 0.0001
2017-10-11T11:15:03.881422: step 6419, loss 0.12046, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:03.994419: step 6420, loss 0.166136, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:04.095260: step 6421, loss 0.108337, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:04.200515: step 6422, loss 0.189746, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:04.315846: step 6423, loss 0.228536, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:04.429451: step 6424, loss 0.138375, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:04.546990: step 6425, loss 0.260053, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:04.658957: step 6426, loss 0.110202, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:04.766570: step 6427, loss 0.0743242, acc 1, learning_rate 0.0001
2017-10-11T11:15:04.879652: step 6428, loss 0.0938545, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:04.991094: step 6429, loss 0.0743932, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:05.098105: step 6430, loss 0.141965, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:05.212730: step 6431, loss 0.328153, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:05.323075: step 6432, loss 0.172036, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:05.436813: step 6433, loss 0.120725, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:05.566554: step 6434, loss 0.116976, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:05.675867: step 6435, loss 0.263666, acc 0.875, learning_rate 0.0001
2017-10-11T11:15:05.785879: step 6436, loss 0.229503, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:05.901365: step 6437, loss 0.325763, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:06.011153: step 6438, loss 0.123714, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:06.128140: step 6439, loss 0.132999, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:06.238565: step 6440, loss 0.121132, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:06.428418: step 6440, loss 0.220415, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6440

2017-10-11T11:15:07.444984: step 6441, loss 0.0906779, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:07.563061: step 6442, loss 0.155207, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:07.744966: step 6443, loss 0.0919829, acc 1, learning_rate 0.0001
2017-10-11T11:15:07.846456: step 6444, loss 0.18909, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:07.925279: step 6445, loss 0.163827, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:08.003846: step 6446, loss 0.142838, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:08.079595: step 6447, loss 0.142048, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:08.154543: step 6448, loss 0.152211, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:08.231813: step 6449, loss 0.154306, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:08.310055: step 6450, loss 0.158131, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:08.385621: step 6451, loss 0.179862, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:08.502567: step 6452, loss 0.337349, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:08.609243: step 6453, loss 0.128618, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:08.719233: step 6454, loss 0.116949, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:08.834179: step 6455, loss 0.138442, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:08.954309: step 6456, loss 0.168635, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:09.069465: step 6457, loss 0.112246, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:09.181954: step 6458, loss 0.087769, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:09.293190: step 6459, loss 0.241161, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:09.391559: step 6460, loss 0.297165, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:09.495309: step 6461, loss 0.30032, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:09.574665: step 6462, loss 0.260062, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:09.688873: step 6463, loss 0.125833, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:09.796159: step 6464, loss 0.149421, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:09.910622: step 6465, loss 0.131649, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:10.036973: step 6466, loss 0.0854012, acc 1, learning_rate 0.0001
2017-10-11T11:15:10.153240: step 6467, loss 0.163707, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:10.250260: step 6468, loss 0.0850195, acc 1, learning_rate 0.0001
2017-10-11T11:15:10.368738: step 6469, loss 0.120871, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:10.485145: step 6470, loss 0.301295, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:10.600519: step 6471, loss 0.0602372, acc 1, learning_rate 0.0001
2017-10-11T11:15:10.701950: step 6472, loss 0.157126, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:10.817170: step 6473, loss 0.19231, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:10.923783: step 6474, loss 0.214499, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:11.025386: step 6475, loss 0.211976, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:11.141741: step 6476, loss 0.261745, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:11.254298: step 6477, loss 0.2118, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:11.382510: step 6478, loss 0.128241, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:11.497484: step 6479, loss 0.157834, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:11.619510: step 6480, loss 0.215614, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:11.820451: step 6480, loss 0.221188, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6480

2017-10-11T11:15:12.528883: step 6481, loss 0.226859, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:12.640730: step 6482, loss 0.180578, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:12.755483: step 6483, loss 0.194788, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:12.875571: step 6484, loss 0.0893608, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:12.984748: step 6485, loss 0.102448, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:13.148876: step 6486, loss 0.142239, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:13.272111: step 6487, loss 0.184587, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:13.358532: step 6488, loss 0.212389, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:13.437120: step 6489, loss 0.15941, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:13.513905: step 6490, loss 0.193441, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:13.603515: step 6491, loss 0.243386, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:13.679769: step 6492, loss 0.194883, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:13.756762: step 6493, loss 0.149493, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:13.833406: step 6494, loss 0.175356, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:13.936855: step 6495, loss 0.180129, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:14.033303: step 6496, loss 0.217354, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:14.135570: step 6497, loss 0.149091, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:14.252867: step 6498, loss 0.0929693, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:14.360551: step 6499, loss 0.156989, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:14.464274: step 6500, loss 0.107403, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:14.575094: step 6501, loss 0.115652, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:14.683531: step 6502, loss 0.156625, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:14.793020: step 6503, loss 0.0917658, acc 1, learning_rate 0.0001
2017-10-11T11:15:14.903812: step 6504, loss 0.135015, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:15.007053: step 6505, loss 0.184994, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:15.117836: step 6506, loss 0.185271, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:15.219514: step 6507, loss 0.114749, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:15.337228: step 6508, loss 0.166058, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:15.449431: step 6509, loss 0.147149, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:15.561868: step 6510, loss 0.105831, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:15.679693: step 6511, loss 0.141234, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:15.788419: step 6512, loss 0.12328, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:15.902095: step 6513, loss 0.235691, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:15.996176: step 6514, loss 0.165305, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:16.093576: step 6515, loss 0.236822, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:16.191156: step 6516, loss 0.0949556, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:16.312249: step 6517, loss 0.240094, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:16.436262: step 6518, loss 0.205739, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:16.548945: step 6519, loss 0.122634, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:16.665044: step 6520, loss 0.290537, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:16.864914: step 6520, loss 0.221215, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6520

2017-10-11T11:15:17.762395: step 6521, loss 0.147074, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:17.871866: step 6522, loss 0.180627, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:17.983185: step 6523, loss 0.0719086, acc 1, learning_rate 0.0001
2017-10-11T11:15:18.078350: step 6524, loss 0.273324, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:18.196062: step 6525, loss 0.163859, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:18.310591: step 6526, loss 0.214308, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:18.509216: step 6527, loss 0.149986, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:18.597270: step 6528, loss 0.219479, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:18.680445: step 6529, loss 0.173078, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:18.763096: step 6530, loss 0.137938, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:18.852591: step 6531, loss 0.15144, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:18.932528: step 6532, loss 0.259446, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:19.005210: step 6533, loss 0.143894, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:19.083127: step 6534, loss 0.252941, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:19.159614: step 6535, loss 0.0934335, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:19.282834: step 6536, loss 0.24147, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:19.399871: step 6537, loss 0.0924243, acc 1, learning_rate 0.0001
2017-10-11T11:15:19.523327: step 6538, loss 0.158688, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:19.634533: step 6539, loss 0.0945672, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:19.759258: step 6540, loss 0.193033, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:19.876302: step 6541, loss 0.115407, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:20.001347: step 6542, loss 0.0891692, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:20.116757: step 6543, loss 0.110822, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:20.225303: step 6544, loss 0.161355, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:20.337080: step 6545, loss 0.175892, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:20.454924: step 6546, loss 0.123629, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:20.556857: step 6547, loss 0.10936, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:20.664856: step 6548, loss 0.122554, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:20.771234: step 6549, loss 0.0899617, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:20.880875: step 6550, loss 0.148891, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:20.988890: step 6551, loss 0.223593, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:21.090646: step 6552, loss 0.276593, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:21.197966: step 6553, loss 0.0801806, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:21.316986: step 6554, loss 0.141448, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:21.430187: step 6555, loss 0.171154, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:21.529233: step 6556, loss 0.100635, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:21.640223: step 6557, loss 0.0897268, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:21.755164: step 6558, loss 0.124607, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:21.872510: step 6559, loss 0.0716445, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:21.978935: step 6560, loss 0.0988028, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:22.183511: step 6560, loss 0.220024, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6560

2017-10-11T11:15:23.087750: step 6561, loss 0.221536, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:23.199617: step 6562, loss 0.211905, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:23.306911: step 6563, loss 0.138198, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:23.416823: step 6564, loss 0.186157, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:23.530500: step 6565, loss 0.125233, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:23.640845: step 6566, loss 0.0916676, acc 1, learning_rate 0.0001
2017-10-11T11:15:23.803869: step 6567, loss 0.133983, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:23.891318: step 6568, loss 0.163149, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:23.966492: step 6569, loss 0.088842, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:24.043750: step 6570, loss 0.192786, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:24.118123: step 6571, loss 0.210488, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:24.193816: step 6572, loss 0.159472, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:24.272244: step 6573, loss 0.096172, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:24.397211: step 6574, loss 0.143423, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:24.514098: step 6575, loss 0.109101, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:24.622948: step 6576, loss 0.216455, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:24.734008: step 6577, loss 0.188992, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:24.847993: step 6578, loss 0.1485, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:24.964438: step 6579, loss 0.108356, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:25.075981: step 6580, loss 0.121458, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:25.195673: step 6581, loss 0.117337, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:25.320882: step 6582, loss 0.175883, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:25.430918: step 6583, loss 0.14217, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:25.543485: step 6584, loss 0.173853, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:25.662758: step 6585, loss 0.208765, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:25.793334: step 6586, loss 0.115468, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:25.902785: step 6587, loss 0.286613, acc 0.875, learning_rate 0.0001
2017-10-11T11:15:26.009218: step 6588, loss 0.160195, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:26.125794: step 6589, loss 0.184317, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:26.243560: step 6590, loss 0.179736, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:26.364423: step 6591, loss 0.132721, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:26.480031: step 6592, loss 0.115247, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:26.597473: step 6593, loss 0.148332, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:26.712992: step 6594, loss 0.204929, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:26.827955: step 6595, loss 0.17299, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:26.939883: step 6596, loss 0.128996, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:27.044847: step 6597, loss 0.219724, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:27.157584: step 6598, loss 0.209563, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:27.272112: step 6599, loss 0.201319, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:27.380033: step 6600, loss 0.0810086, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:27.573598: step 6600, loss 0.219082, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6600

2017-10-11T11:15:28.911012: step 6601, loss 0.176545, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:28.985859: step 6602, loss 0.283888, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:29.058270: step 6603, loss 0.158208, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:29.131856: step 6604, loss 0.133259, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:29.208064: step 6605, loss 0.15428, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:29.286713: step 6606, loss 0.275106, acc 0.875, learning_rate 0.0001
2017-10-11T11:15:29.361987: step 6607, loss 0.162588, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:29.448746: step 6608, loss 0.222446, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:29.575464: step 6609, loss 0.236666, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:29.686976: step 6610, loss 0.139566, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:29.798033: step 6611, loss 0.271476, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:29.912145: step 6612, loss 0.117656, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:30.026807: step 6613, loss 0.17048, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:30.138751: step 6614, loss 0.152454, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:30.251518: step 6615, loss 0.210456, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:30.364530: step 6616, loss 0.222934, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:30.474942: step 6617, loss 0.194158, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:30.577798: step 6618, loss 0.111853, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:30.692750: step 6619, loss 0.232588, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:30.811961: step 6620, loss 0.110427, acc 1, learning_rate 0.0001
2017-10-11T11:15:30.926703: step 6621, loss 0.131267, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:31.051292: step 6622, loss 0.0912799, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:31.174760: step 6623, loss 0.149696, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:31.298781: step 6624, loss 0.102631, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:31.422731: step 6625, loss 0.224915, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:31.534583: step 6626, loss 0.148253, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:31.636493: step 6627, loss 0.222562, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:31.732362: step 6628, loss 0.0761466, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:31.824598: step 6629, loss 0.0914125, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:31.925311: step 6630, loss 0.111991, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:32.040868: step 6631, loss 0.294808, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:32.146849: step 6632, loss 0.133241, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:32.269723: step 6633, loss 0.199996, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:32.388911: step 6634, loss 0.228201, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:32.500889: step 6635, loss 0.219702, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:32.615043: step 6636, loss 0.117298, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:32.743553: step 6637, loss 0.132221, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:32.847852: step 6638, loss 0.247535, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:32.963627: step 6639, loss 0.0769976, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:33.076450: step 6640, loss 0.0800241, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:33.257352: step 6640, loss 0.219132, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6640

2017-10-11T11:15:34.145346: step 6641, loss 0.145247, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:34.232242: step 6642, loss 0.215433, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:34.308785: step 6643, loss 0.193959, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:34.385461: step 6644, loss 0.169728, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:34.459657: step 6645, loss 0.0993476, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:34.544043: step 6646, loss 0.140449, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:34.619059: step 6647, loss 0.242323, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:34.695206: step 6648, loss 0.112477, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:34.772958: step 6649, loss 0.198237, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:34.881717: step 6650, loss 0.227053, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:34.999163: step 6651, loss 0.14875, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:35.110778: step 6652, loss 0.126663, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:35.225349: step 6653, loss 0.178037, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:35.335853: step 6654, loss 0.12606, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:35.445150: step 6655, loss 0.19709, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:35.538193: step 6656, loss 0.169926, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:35.635125: step 6657, loss 0.105396, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:35.737150: step 6658, loss 0.138948, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:35.840617: step 6659, loss 0.176196, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:35.952603: step 6660, loss 0.099866, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:36.069391: step 6661, loss 0.182873, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:36.174351: step 6662, loss 0.0818486, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:36.289065: step 6663, loss 0.218393, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:36.392481: step 6664, loss 0.216722, acc 0.921569, learning_rate 0.0001
2017-10-11T11:15:36.508943: step 6665, loss 0.15641, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:36.618893: step 6666, loss 0.152257, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:36.733119: step 6667, loss 0.10569, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:36.844604: step 6668, loss 0.134595, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:36.952052: step 6669, loss 0.131565, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:37.073709: step 6670, loss 0.183641, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:37.151061: step 6671, loss 0.143787, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:37.260454: step 6672, loss 0.146289, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:37.374036: step 6673, loss 0.216201, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:37.482406: step 6674, loss 0.180583, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:37.596960: step 6675, loss 0.110591, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:37.693885: step 6676, loss 0.127785, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:37.796425: step 6677, loss 0.133364, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:37.892402: step 6678, loss 0.270534, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:38.000185: step 6679, loss 0.141829, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:38.115597: step 6680, loss 0.146011, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:38.304927: step 6680, loss 0.220727, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6680

2017-10-11T11:15:39.235748: step 6681, loss 0.227592, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:39.340837: step 6682, loss 0.211107, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:39.513840: step 6683, loss 0.259479, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:39.606347: step 6684, loss 0.165837, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:39.684003: step 6685, loss 0.174934, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:39.764280: step 6686, loss 0.17228, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:39.842606: step 6687, loss 0.210785, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:39.921978: step 6688, loss 0.172381, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:39.998920: step 6689, loss 0.17908, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:40.075719: step 6690, loss 0.08594, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:40.175875: step 6691, loss 0.126376, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:40.283491: step 6692, loss 0.313034, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:40.388677: step 6693, loss 0.0967548, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:40.490833: step 6694, loss 0.159617, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:40.604375: step 6695, loss 0.160063, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:40.727437: step 6696, loss 0.184308, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:40.845442: step 6697, loss 0.175691, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:40.969366: step 6698, loss 0.223349, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:41.090360: step 6699, loss 0.210819, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:41.205367: step 6700, loss 0.0758124, acc 1, learning_rate 0.0001
2017-10-11T11:15:41.324597: step 6701, loss 0.155335, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:41.432897: step 6702, loss 0.135162, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:41.541209: step 6703, loss 0.136462, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:41.659805: step 6704, loss 0.258381, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:41.773907: step 6705, loss 0.0968656, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:41.888970: step 6706, loss 0.110868, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:42.008669: step 6707, loss 0.0852657, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:42.120015: step 6708, loss 0.102905, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:42.235239: step 6709, loss 0.162734, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:42.334245: step 6710, loss 0.0962773, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:42.449962: step 6711, loss 0.18292, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:42.556638: step 6712, loss 0.100333, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:42.659501: step 6713, loss 0.185553, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:42.775855: step 6714, loss 0.161181, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:42.894704: step 6715, loss 0.102597, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:43.007770: step 6716, loss 0.130797, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:43.122795: step 6717, loss 0.325827, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:43.233489: step 6718, loss 0.159763, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:43.348289: step 6719, loss 0.0886222, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:43.456905: step 6720, loss 0.0942013, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:43.653864: step 6720, loss 0.220675, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6720

2017-10-11T11:15:44.617989: step 6721, loss 0.0774508, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:44.772929: step 6722, loss 0.0967594, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:44.899120: step 6723, loss 0.133745, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:44.971969: step 6724, loss 0.222986, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:45.042332: step 6725, loss 0.0730409, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:45.114940: step 6726, loss 0.109587, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:45.186474: step 6727, loss 0.0888105, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:45.261499: step 6728, loss 0.10542, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:45.336866: step 6729, loss 0.12666, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:45.412380: step 6730, loss 0.182982, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:45.490974: step 6731, loss 0.184458, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:45.569702: step 6732, loss 0.162556, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:45.644909: step 6733, loss 0.153622, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:45.720386: step 6734, loss 0.101586, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:45.837075: step 6735, loss 0.171379, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:46.084850: step 6736, loss 0.213203, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:46.186399: step 6737, loss 0.192253, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:46.309636: step 6738, loss 0.198682, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:46.424692: step 6739, loss 0.123577, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:46.536422: step 6740, loss 0.127577, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:46.650876: step 6741, loss 0.0987381, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:46.759947: step 6742, loss 0.104615, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:46.846305: step 6743, loss 0.141894, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:46.951370: step 6744, loss 0.135429, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:47.054556: step 6745, loss 0.0655207, acc 1, learning_rate 0.0001
2017-10-11T11:15:47.172439: step 6746, loss 0.164129, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:47.297732: step 6747, loss 0.123286, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:47.411005: step 6748, loss 0.180811, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:47.525310: step 6749, loss 0.179129, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:47.630780: step 6750, loss 0.272915, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:47.750100: step 6751, loss 0.125252, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:47.868112: step 6752, loss 0.140388, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:47.974432: step 6753, loss 0.11813, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:48.064945: step 6754, loss 0.175492, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:48.159724: step 6755, loss 0.112435, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:48.267613: step 6756, loss 0.121658, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:48.354261: step 6757, loss 0.24402, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:48.472586: step 6758, loss 0.205722, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:48.587574: step 6759, loss 0.170043, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:48.695984: step 6760, loss 0.28268, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:48.881144: step 6760, loss 0.219642, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6760

2017-10-11T11:15:49.703807: step 6761, loss 0.126646, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:49.789744: step 6762, loss 0.131163, acc 0.960784, learning_rate 0.0001
2017-10-11T11:15:49.905163: step 6763, loss 0.128528, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:50.020974: step 6764, loss 0.199295, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:50.134670: step 6765, loss 0.201284, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:50.257847: step 6766, loss 0.216709, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:50.416055: step 6767, loss 0.101522, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:50.493603: step 6768, loss 0.13495, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:50.575687: step 6769, loss 0.220214, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:50.653962: step 6770, loss 0.184727, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:50.731198: step 6771, loss 0.109504, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:50.807644: step 6772, loss 0.170674, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:50.881842: step 6773, loss 0.141544, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:50.967745: step 6774, loss 0.208807, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:51.057047: step 6775, loss 0.152335, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:51.163112: step 6776, loss 0.0663767, acc 1, learning_rate 0.0001
2017-10-11T11:15:51.272875: step 6777, loss 0.163851, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:51.388410: step 6778, loss 0.147659, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:51.484508: step 6779, loss 0.0798191, acc 1, learning_rate 0.0001
2017-10-11T11:15:51.596405: step 6780, loss 0.227427, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:51.718864: step 6781, loss 0.190412, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:51.822819: step 6782, loss 0.119158, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:51.936189: step 6783, loss 0.192096, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:52.056859: step 6784, loss 0.160867, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:52.168541: step 6785, loss 0.1661, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:52.284289: step 6786, loss 0.148921, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:52.391294: step 6787, loss 0.105333, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:52.509642: step 6788, loss 0.110783, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:52.627604: step 6789, loss 0.219438, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:52.748852: step 6790, loss 0.119891, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:52.869333: step 6791, loss 0.271807, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:52.983890: step 6792, loss 0.195961, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:53.096890: step 6793, loss 0.28473, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:53.214775: step 6794, loss 0.180093, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:53.333023: step 6795, loss 0.173788, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:53.463620: step 6796, loss 0.123566, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:53.567257: step 6797, loss 0.225118, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:53.672857: step 6798, loss 0.0959743, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:53.788830: step 6799, loss 0.181245, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:53.909313: step 6800, loss 0.167001, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:54.102757: step 6800, loss 0.218759, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6800

2017-10-11T11:15:55.028813: step 6801, loss 0.139904, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:55.124878: step 6802, loss 0.216204, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:55.227945: step 6803, loss 0.101344, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:55.333157: step 6804, loss 0.127205, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:55.439430: step 6805, loss 0.105498, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:55.585037: step 6806, loss 0.187603, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:55.717177: step 6807, loss 0.14369, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:55.793447: step 6808, loss 0.195035, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:55.869720: step 6809, loss 0.245863, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:55.956853: step 6810, loss 0.133343, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:56.033029: step 6811, loss 0.114084, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:56.110840: step 6812, loss 0.183931, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:56.186006: step 6813, loss 0.0912135, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:56.260984: step 6814, loss 0.106331, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:56.351081: step 6815, loss 0.115237, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:56.458809: step 6816, loss 0.194549, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:56.550840: step 6817, loss 0.197893, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:56.658027: step 6818, loss 0.117468, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:56.771788: step 6819, loss 0.232312, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:56.877891: step 6820, loss 0.0725453, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:56.999521: step 6821, loss 0.189365, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:57.121999: step 6822, loss 0.127837, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:57.248976: step 6823, loss 0.170546, acc 0.921875, learning_rate 0.0001
2017-10-11T11:15:57.359600: step 6824, loss 0.141368, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:57.474902: step 6825, loss 0.12468, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:57.605804: step 6826, loss 0.247082, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:57.713019: step 6827, loss 0.251099, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:57.829056: step 6828, loss 0.156912, acc 0.953125, learning_rate 0.0001
2017-10-11T11:15:57.929244: step 6829, loss 0.124434, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:58.036850: step 6830, loss 0.0729293, acc 0.984375, learning_rate 0.0001
2017-10-11T11:15:58.163014: step 6831, loss 0.136951, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:58.272113: step 6832, loss 0.217852, acc 0.90625, learning_rate 0.0001
2017-10-11T11:15:58.385652: step 6833, loss 0.169197, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:58.500313: step 6834, loss 0.157071, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:58.613975: step 6835, loss 0.180211, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:58.720759: step 6836, loss 0.124418, acc 0.96875, learning_rate 0.0001
2017-10-11T11:15:58.851347: step 6837, loss 0.162553, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:58.954654: step 6838, loss 0.190498, acc 0.890625, learning_rate 0.0001
2017-10-11T11:15:59.073155: step 6839, loss 0.118235, acc 0.9375, learning_rate 0.0001
2017-10-11T11:15:59.179350: step 6840, loss 0.221407, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:15:59.376463: step 6840, loss 0.218399, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6840

2017-10-11T11:16:00.346626: step 6841, loss 0.201157, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:00.463757: step 6842, loss 0.0560952, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:00.575981: step 6843, loss 0.130095, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:00.689300: step 6844, loss 0.180638, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:00.852898: step 6845, loss 0.288007, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:00.987971: step 6846, loss 0.185757, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:01.060916: step 6847, loss 0.106407, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:01.137225: step 6848, loss 0.113168, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:01.211133: step 6849, loss 0.255661, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:01.284527: step 6850, loss 0.201787, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:01.362509: step 6851, loss 0.217502, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:01.438053: step 6852, loss 0.0884909, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:01.517038: step 6853, loss 0.103835, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:01.603541: step 6854, loss 0.193353, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:01.704894: step 6855, loss 0.236276, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:01.824807: step 6856, loss 0.0854825, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:01.937273: step 6857, loss 0.221775, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:02.045750: step 6858, loss 0.0986349, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:02.152051: step 6859, loss 0.205774, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:02.261574: step 6860, loss 0.139137, acc 0.980392, learning_rate 0.0001
2017-10-11T11:16:02.383466: step 6861, loss 0.153436, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:02.488969: step 6862, loss 0.206059, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:02.602866: step 6863, loss 0.111773, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:02.717544: step 6864, loss 0.174389, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:02.838457: step 6865, loss 0.1956, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:02.953940: step 6866, loss 0.0575482, acc 1, learning_rate 0.0001
2017-10-11T11:16:03.069303: step 6867, loss 0.0936426, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:03.174844: step 6868, loss 0.166895, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:03.282111: step 6869, loss 0.103254, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:03.399915: step 6870, loss 0.196817, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:03.512255: step 6871, loss 0.165419, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:03.625447: step 6872, loss 0.181693, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:03.742658: step 6873, loss 0.244064, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:03.859012: step 6874, loss 0.246098, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:03.974445: step 6875, loss 0.157392, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:04.096972: step 6876, loss 0.282251, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:04.229154: step 6877, loss 0.205952, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:04.344876: step 6878, loss 0.226205, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:04.454674: step 6879, loss 0.186054, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:04.570072: step 6880, loss 0.239767, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:04.752569: step 6880, loss 0.217829, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6880

2017-10-11T11:16:05.665085: step 6881, loss 0.169926, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:05.766303: step 6882, loss 0.235457, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:05.881406: step 6883, loss 0.144373, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:05.992571: step 6884, loss 0.130559, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:06.128886: step 6885, loss 0.168655, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:06.278376: step 6886, loss 0.222758, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:06.355600: step 6887, loss 0.209428, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:06.434266: step 6888, loss 0.15288, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:06.521602: step 6889, loss 0.253382, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:06.593984: step 6890, loss 0.191299, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:06.670248: step 6891, loss 0.141764, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:06.747212: step 6892, loss 0.0720501, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:06.821165: step 6893, loss 0.131232, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:06.907843: step 6894, loss 0.192536, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:07.010201: step 6895, loss 0.120033, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:07.120870: step 6896, loss 0.103335, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:07.254513: step 6897, loss 0.190207, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:07.352674: step 6898, loss 0.0983563, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:07.465287: step 6899, loss 0.137905, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:07.570822: step 6900, loss 0.130982, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:07.671625: step 6901, loss 0.280681, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:07.773185: step 6902, loss 0.156511, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:07.881124: step 6903, loss 0.185795, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:07.991381: step 6904, loss 0.130429, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:08.107285: step 6905, loss 0.132747, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:08.219467: step 6906, loss 0.152291, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:08.343699: step 6907, loss 0.188074, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:08.460049: step 6908, loss 0.0992918, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:08.572816: step 6909, loss 0.146521, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:08.686093: step 6910, loss 0.0877038, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:08.797294: step 6911, loss 0.16712, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:08.910963: step 6912, loss 0.167231, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:09.028803: step 6913, loss 0.167512, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:09.141129: step 6914, loss 0.115492, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:09.254521: step 6915, loss 0.142385, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:09.366953: step 6916, loss 0.220196, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:09.467043: step 6917, loss 0.0623187, acc 1, learning_rate 0.0001
2017-10-11T11:16:09.555374: step 6918, loss 0.141022, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:09.658266: step 6919, loss 0.126622, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:09.773979: step 6920, loss 0.127696, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:09.971900: step 6920, loss 0.217189, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6920

2017-10-11T11:16:10.853955: step 6921, loss 0.1302, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:10.961600: step 6922, loss 0.0923931, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:11.074868: step 6923, loss 0.237861, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:11.190943: step 6924, loss 0.132141, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:11.306810: step 6925, loss 0.157892, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:11.473850: step 6926, loss 0.173914, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:11.592059: step 6927, loss 0.214594, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:11.667782: step 6928, loss 0.169135, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:11.742753: step 6929, loss 0.221871, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:11.823773: step 6930, loss 0.263469, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:11.897824: step 6931, loss 0.0943829, acc 1, learning_rate 0.0001
2017-10-11T11:16:11.976156: step 6932, loss 0.118763, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:12.052857: step 6933, loss 0.191606, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:12.172857: step 6934, loss 0.154387, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:12.288662: step 6935, loss 0.0993679, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:12.405694: step 6936, loss 0.129794, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:12.517526: step 6937, loss 0.164181, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:12.634206: step 6938, loss 0.0779296, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:12.747926: step 6939, loss 0.0722552, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:12.866083: step 6940, loss 0.139881, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:12.975717: step 6941, loss 0.102675, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:13.087459: step 6942, loss 0.143712, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:13.200579: step 6943, loss 0.154224, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:13.315152: step 6944, loss 0.143116, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:13.425502: step 6945, loss 0.198393, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:13.524204: step 6946, loss 0.106122, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:13.644153: step 6947, loss 0.164209, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:13.752462: step 6948, loss 0.10484, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:13.856846: step 6949, loss 0.347714, acc 0.875, learning_rate 0.0001
2017-10-11T11:16:13.970794: step 6950, loss 0.160678, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:14.068819: step 6951, loss 0.16959, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:14.167356: step 6952, loss 0.109957, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:14.272638: step 6953, loss 0.118038, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:14.384830: step 6954, loss 0.17779, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:14.485837: step 6955, loss 0.0912677, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:14.593906: step 6956, loss 0.102527, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:14.694567: step 6957, loss 0.152718, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:14.788923: step 6958, loss 0.14474, acc 0.941176, learning_rate 0.0001
2017-10-11T11:16:14.892586: step 6959, loss 0.0843629, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:14.987499: step 6960, loss 0.154276, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:15.210861: step 6960, loss 0.21731, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-6960

2017-10-11T11:16:16.232464: step 6961, loss 0.124999, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:16.360868: step 6962, loss 0.168707, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:16.473204: step 6963, loss 0.211828, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:16.653314: step 6964, loss 0.167628, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:16.757974: step 6965, loss 0.181077, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:16.831669: step 6966, loss 0.144969, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:16.915723: step 6967, loss 0.240086, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:16.990849: step 6968, loss 0.10564, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:17.078132: step 6969, loss 0.199463, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:17.159013: step 6970, loss 0.177909, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:17.242222: step 6971, loss 0.189941, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:17.315449: step 6972, loss 0.213054, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:17.427776: step 6973, loss 0.18552, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:17.554967: step 6974, loss 0.115213, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:17.673430: step 6975, loss 0.131121, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:17.786230: step 6976, loss 0.0729533, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:17.900041: step 6977, loss 0.1739, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:18.023009: step 6978, loss 0.0693524, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:18.135023: step 6979, loss 0.128868, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:18.247933: step 6980, loss 0.192393, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:18.355875: step 6981, loss 0.102677, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:18.469982: step 6982, loss 0.250373, acc 0.875, learning_rate 0.0001
2017-10-11T11:16:18.573405: step 6983, loss 0.128986, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:18.669010: step 6984, loss 0.195999, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:18.776027: step 6985, loss 0.173457, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:18.868858: step 6986, loss 0.160721, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:18.978347: step 6987, loss 0.0840646, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:19.082862: step 6988, loss 0.141586, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:19.184271: step 6989, loss 0.195449, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:19.276203: step 6990, loss 0.0906638, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:19.381031: step 6991, loss 0.0758381, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:19.483478: step 6992, loss 0.141129, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:19.585450: step 6993, loss 0.0811685, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:19.700320: step 6994, loss 0.149779, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:19.815979: step 6995, loss 0.115237, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:19.933231: step 6996, loss 0.0565696, acc 1, learning_rate 0.0001
2017-10-11T11:16:20.052448: step 6997, loss 0.320415, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:20.173684: step 6998, loss 0.0936103, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:20.287790: step 6999, loss 0.201669, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:20.405952: step 7000, loss 0.202532, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:20.592998: step 7000, loss 0.217184, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7000

2017-10-11T11:16:21.382995: step 7001, loss 0.273955, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:21.482401: step 7002, loss 0.195198, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:21.588811: step 7003, loss 0.156184, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:21.704274: step 7004, loss 0.184908, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:21.827612: step 7005, loss 0.129493, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:21.945098: step 7006, loss 0.223584, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:22.129400: step 7007, loss 0.095485, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:22.227479: step 7008, loss 0.140321, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:22.300875: step 7009, loss 0.197987, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:22.369543: step 7010, loss 0.192669, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:22.444156: step 7011, loss 0.189969, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:22.532334: step 7012, loss 0.144273, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:22.611989: step 7013, loss 0.249944, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:22.693360: step 7014, loss 0.27387, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:22.770010: step 7015, loss 0.192553, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:22.843824: step 7016, loss 0.131882, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:22.919343: step 7017, loss 0.0932404, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:22.998249: step 7018, loss 0.151573, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:23.117786: step 7019, loss 0.17846, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:23.227231: step 7020, loss 0.165136, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:23.342039: step 7021, loss 0.146129, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:23.448315: step 7022, loss 0.13403, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:23.555938: step 7023, loss 0.131002, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:23.652913: step 7024, loss 0.118569, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:23.757002: step 7025, loss 0.138713, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:23.864837: step 7026, loss 0.152422, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:23.961260: step 7027, loss 0.239756, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:24.070175: step 7028, loss 0.230952, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:24.185275: step 7029, loss 0.140604, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:24.285592: step 7030, loss 0.132526, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:24.398413: step 7031, loss 0.145348, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:24.521701: step 7032, loss 0.098718, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:24.635360: step 7033, loss 0.165082, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:24.760894: step 7034, loss 0.190853, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:24.881539: step 7035, loss 0.135052, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:24.988262: step 7036, loss 0.13686, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:25.105233: step 7037, loss 0.130756, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:25.236887: step 7038, loss 0.14622, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:25.336753: step 7039, loss 0.145985, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:25.438582: step 7040, loss 0.100829, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:25.648946: step 7040, loss 0.217449, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7040

2017-10-11T11:16:26.604529: step 7041, loss 0.174874, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:26.720606: step 7042, loss 0.137987, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:26.835440: step 7043, loss 0.18159, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:26.928934: step 7044, loss 0.194247, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:27.036949: step 7045, loss 0.257006, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:27.149735: step 7046, loss 0.10512, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:27.245385: step 7047, loss 0.231573, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:27.337969: step 7048, loss 0.202004, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:27.446595: step 7049, loss 0.191448, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:27.597434: step 7050, loss 0.115738, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:27.725262: step 7051, loss 0.198162, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:27.810127: step 7052, loss 0.0957881, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:27.888290: step 7053, loss 0.132787, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:27.969127: step 7054, loss 0.187054, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:28.046209: step 7055, loss 0.135895, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:28.124272: step 7056, loss 0.187702, acc 0.941176, learning_rate 0.0001
2017-10-11T11:16:28.201681: step 7057, loss 0.0720161, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:28.275033: step 7058, loss 0.194997, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:28.373242: step 7059, loss 0.163867, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:28.493078: step 7060, loss 0.214961, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:28.592866: step 7061, loss 0.0663538, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:28.688899: step 7062, loss 0.167732, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:28.800990: step 7063, loss 0.120186, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:28.910139: step 7064, loss 0.117357, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:29.017595: step 7065, loss 0.10936, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:29.136886: step 7066, loss 0.128633, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:29.246216: step 7067, loss 0.241176, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:29.367006: step 7068, loss 0.0938685, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:29.472730: step 7069, loss 0.0879124, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:29.606435: step 7070, loss 0.196588, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:29.707881: step 7071, loss 0.166638, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:29.801336: step 7072, loss 0.174042, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:29.916683: step 7073, loss 0.144976, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:30.027374: step 7074, loss 0.102434, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:30.124472: step 7075, loss 0.236476, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:30.248281: step 7076, loss 0.123498, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:30.343669: step 7077, loss 0.127054, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:30.463215: step 7078, loss 0.203616, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:30.581984: step 7079, loss 0.127685, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:30.692150: step 7080, loss 0.238804, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:30.882055: step 7080, loss 0.217452, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7080

2017-10-11T11:16:31.961270: step 7081, loss 0.195197, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:32.069471: step 7082, loss 0.145358, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:32.182481: step 7083, loss 0.146614, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:32.298095: step 7084, loss 0.0722, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:32.409035: step 7085, loss 0.202944, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:32.523804: step 7086, loss 0.0744068, acc 1, learning_rate 0.0001
2017-10-11T11:16:32.640350: step 7087, loss 0.175381, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:32.776868: step 7088, loss 0.0947151, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:32.929110: step 7089, loss 0.173832, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:33.013571: step 7090, loss 0.332274, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:33.091868: step 7091, loss 0.15198, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:33.167656: step 7092, loss 0.159617, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:33.249194: step 7093, loss 0.173682, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:33.322304: step 7094, loss 0.236021, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:33.398185: step 7095, loss 0.144461, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:33.483416: step 7096, loss 0.154092, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:33.580859: step 7097, loss 0.123573, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:33.684835: step 7098, loss 0.130908, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:33.784213: step 7099, loss 0.127108, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:33.900799: step 7100, loss 0.121759, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:34.003766: step 7101, loss 0.187828, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:34.120933: step 7102, loss 0.0951666, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:34.233280: step 7103, loss 0.256659, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:34.360249: step 7104, loss 0.121081, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:34.464122: step 7105, loss 0.0563856, acc 1, learning_rate 0.0001
2017-10-11T11:16:34.560867: step 7106, loss 0.208753, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:34.657043: step 7107, loss 0.181569, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:34.757535: step 7108, loss 0.156849, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:34.883897: step 7109, loss 0.0849122, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:34.991971: step 7110, loss 0.113372, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:35.109398: step 7111, loss 0.185883, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:35.212116: step 7112, loss 0.135739, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:35.329753: step 7113, loss 0.180007, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:35.454889: step 7114, loss 0.204449, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:35.564879: step 7115, loss 0.162142, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:35.682056: step 7116, loss 0.25416, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:35.794757: step 7117, loss 0.147708, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:35.905556: step 7118, loss 0.0859816, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:36.019474: step 7119, loss 0.10014, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:36.147698: step 7120, loss 0.129989, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:36.352188: step 7120, loss 0.216538, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7120

2017-10-11T11:16:37.183282: step 7121, loss 0.23202, acc 0.875, learning_rate 0.0001
2017-10-11T11:16:37.279009: step 7122, loss 0.197586, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:37.371940: step 7123, loss 0.126531, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:37.485935: step 7124, loss 0.115452, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:37.610310: step 7125, loss 0.121124, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:37.722781: step 7126, loss 0.200452, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:37.832778: step 7127, loss 0.121801, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:37.950822: step 7128, loss 0.150716, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:38.153081: step 7129, loss 0.132615, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:38.231759: step 7130, loss 0.138542, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:38.312652: step 7131, loss 0.0907137, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:38.395305: step 7132, loss 0.129061, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:38.476459: step 7133, loss 0.093881, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:38.552162: step 7134, loss 0.275978, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:38.625449: step 7135, loss 0.184691, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:38.711305: step 7136, loss 0.154238, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:38.784189: step 7137, loss 0.150148, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:38.896053: step 7138, loss 0.129792, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:39.014327: step 7139, loss 0.174965, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:39.122120: step 7140, loss 0.151805, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:39.231750: step 7141, loss 0.1298, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:39.352333: step 7142, loss 0.128932, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:39.461804: step 7143, loss 0.145747, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:39.571510: step 7144, loss 0.172288, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:39.687627: step 7145, loss 0.174747, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:39.799973: step 7146, loss 0.173085, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:39.913175: step 7147, loss 0.158012, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:40.033102: step 7148, loss 0.197544, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:40.143267: step 7149, loss 0.184262, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:40.271141: step 7150, loss 0.11301, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:40.381458: step 7151, loss 0.165744, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:40.493728: step 7152, loss 0.320278, acc 0.828125, learning_rate 0.0001
2017-10-11T11:16:40.608800: step 7153, loss 0.106948, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:40.704857: step 7154, loss 0.104775, acc 0.960784, learning_rate 0.0001
2017-10-11T11:16:40.824029: step 7155, loss 0.106819, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:40.941114: step 7156, loss 0.114594, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:41.060573: step 7157, loss 0.0934968, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:41.175644: step 7158, loss 0.0863573, acc 1, learning_rate 0.0001
2017-10-11T11:16:41.289944: step 7159, loss 0.18223, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:41.403166: step 7160, loss 0.103863, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:41.593169: step 7160, loss 0.217306, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7160

2017-10-11T11:16:43.012894: step 7161, loss 0.118702, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:43.195230: step 7162, loss 0.16118, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:43.286197: step 7163, loss 0.203727, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:43.360105: step 7164, loss 0.0908654, acc 1, learning_rate 0.0001
2017-10-11T11:16:43.437487: step 7165, loss 0.156311, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:43.514551: step 7166, loss 0.156239, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:43.593639: step 7167, loss 0.201029, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:43.669885: step 7168, loss 0.242597, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:43.755712: step 7169, loss 0.23234, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:43.845099: step 7170, loss 0.078979, acc 1, learning_rate 0.0001
2017-10-11T11:16:43.955108: step 7171, loss 0.220806, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:44.064221: step 7172, loss 0.196436, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:44.176949: step 7173, loss 0.143792, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:44.294672: step 7174, loss 0.0913799, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:44.413660: step 7175, loss 0.100644, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:44.520353: step 7176, loss 0.0897907, acc 1, learning_rate 0.0001
2017-10-11T11:16:44.625875: step 7177, loss 0.0874391, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:44.734209: step 7178, loss 0.0834785, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:44.835696: step 7179, loss 0.13089, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:44.943347: step 7180, loss 0.230926, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:45.038826: step 7181, loss 0.172089, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:45.148965: step 7182, loss 0.153182, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:45.244854: step 7183, loss 0.164368, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:45.348926: step 7184, loss 0.188315, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:45.474644: step 7185, loss 0.152469, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:45.597168: step 7186, loss 0.146817, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:45.707954: step 7187, loss 0.0748593, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:45.826651: step 7188, loss 0.132304, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:45.933430: step 7189, loss 0.161888, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:46.043354: step 7190, loss 0.159745, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:46.153488: step 7191, loss 0.0803344, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:46.273763: step 7192, loss 0.103332, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:46.384457: step 7193, loss 0.166289, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:46.484842: step 7194, loss 0.130365, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:46.592856: step 7195, loss 0.171308, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:46.700853: step 7196, loss 0.276407, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:46.808917: step 7197, loss 0.113045, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:46.909185: step 7198, loss 0.133016, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:47.006898: step 7199, loss 0.133909, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:47.134869: step 7200, loss 0.196197, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:47.338152: step 7200, loss 0.216948, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7200

2017-10-11T11:16:48.251986: step 7201, loss 0.12306, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:48.341228: step 7202, loss 0.232358, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:48.505037: step 7203, loss 0.154336, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:48.603315: step 7204, loss 0.246471, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:48.680376: step 7205, loss 0.153984, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:48.753449: step 7206, loss 0.161203, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:48.840509: step 7207, loss 0.208988, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:48.919077: step 7208, loss 0.165377, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:48.991944: step 7209, loss 0.2023, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:49.066994: step 7210, loss 0.191556, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:49.148044: step 7211, loss 0.16109, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:49.249303: step 7212, loss 0.0795798, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:49.359629: step 7213, loss 0.167457, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:49.460860: step 7214, loss 0.19953, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:49.567275: step 7215, loss 0.149369, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:49.667746: step 7216, loss 0.140456, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:49.784937: step 7217, loss 0.241272, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:49.900848: step 7218, loss 0.145118, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:50.002172: step 7219, loss 0.231292, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:50.106173: step 7220, loss 0.178689, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:50.198300: step 7221, loss 0.119901, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:50.312880: step 7222, loss 0.115487, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:50.441611: step 7223, loss 0.0883892, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:50.541653: step 7224, loss 0.10207, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:50.653610: step 7225, loss 0.0929655, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:50.780871: step 7226, loss 0.185708, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:50.896854: step 7227, loss 0.145372, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:51.008867: step 7228, loss 0.147679, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:51.129416: step 7229, loss 0.140092, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:51.242591: step 7230, loss 0.245091, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:51.364802: step 7231, loss 0.165662, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:51.475464: step 7232, loss 0.155574, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:51.593166: step 7233, loss 0.153146, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:51.705340: step 7234, loss 0.233655, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:51.837216: step 7235, loss 0.123968, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:51.927225: step 7236, loss 0.245195, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:52.027580: step 7237, loss 0.309339, acc 0.875, learning_rate 0.0001
2017-10-11T11:16:52.142981: step 7238, loss 0.117112, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:52.264818: step 7239, loss 0.167798, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:52.374386: step 7240, loss 0.0660189, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:52.569028: step 7240, loss 0.216335, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7240

2017-10-11T11:16:53.440372: step 7241, loss 0.116391, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:53.557799: step 7242, loss 0.153218, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:53.657556: step 7243, loss 0.161985, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:53.847938: step 7244, loss 0.202412, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:53.933420: step 7245, loss 0.152187, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:54.015275: step 7246, loss 0.117461, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:54.095433: step 7247, loss 0.0635509, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:54.178824: step 7248, loss 0.0969251, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:54.257531: step 7249, loss 0.154468, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:54.329843: step 7250, loss 0.131811, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:54.408021: step 7251, loss 0.0962733, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:54.514465: step 7252, loss 0.0785028, acc 0.980392, learning_rate 0.0001
2017-10-11T11:16:54.627179: step 7253, loss 0.208998, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:54.738764: step 7254, loss 0.195883, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:54.830519: step 7255, loss 0.217408, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:54.930808: step 7256, loss 0.122245, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:55.039494: step 7257, loss 0.309884, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:55.152334: step 7258, loss 0.200715, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:55.276140: step 7259, loss 0.181685, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:55.384936: step 7260, loss 0.209078, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:55.487615: step 7261, loss 0.0713979, acc 1, learning_rate 0.0001
2017-10-11T11:16:55.597221: step 7262, loss 0.104636, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:55.709313: step 7263, loss 0.129874, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:55.818583: step 7264, loss 0.0709196, acc 1, learning_rate 0.0001
2017-10-11T11:16:55.928561: step 7265, loss 0.175162, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:56.049388: step 7266, loss 0.172251, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:56.159044: step 7267, loss 0.138933, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:56.261391: step 7268, loss 0.229123, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:56.380916: step 7269, loss 0.100173, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:56.496784: step 7270, loss 0.0857453, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:56.602789: step 7271, loss 0.152135, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:56.716931: step 7272, loss 0.122917, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:56.826207: step 7273, loss 0.1482, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:56.937353: step 7274, loss 0.199905, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:57.047680: step 7275, loss 0.153312, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:57.152542: step 7276, loss 0.109108, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:57.260557: step 7277, loss 0.146655, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:57.380185: step 7278, loss 0.187821, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:57.488894: step 7279, loss 0.0324474, acc 1, learning_rate 0.0001
2017-10-11T11:16:57.604495: step 7280, loss 0.228637, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:16:57.790484: step 7280, loss 0.215063, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7280

2017-10-11T11:16:58.561968: step 7281, loss 0.113779, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:58.657625: step 7282, loss 0.115091, acc 0.984375, learning_rate 0.0001
2017-10-11T11:16:58.770725: step 7283, loss 0.154412, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:58.953312: step 7284, loss 0.0712642, acc 1, learning_rate 0.0001
2017-10-11T11:16:59.063236: step 7285, loss 0.154006, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:59.147931: step 7286, loss 0.196643, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:59.229744: step 7287, loss 0.163443, acc 0.9375, learning_rate 0.0001
2017-10-11T11:16:59.314188: step 7288, loss 0.122767, acc 0.953125, learning_rate 0.0001
2017-10-11T11:16:59.393694: step 7289, loss 0.200305, acc 0.90625, learning_rate 0.0001
2017-10-11T11:16:59.470106: step 7290, loss 0.128488, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:59.545587: step 7291, loss 0.20154, acc 0.921875, learning_rate 0.0001
2017-10-11T11:16:59.618850: step 7292, loss 0.121388, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:59.737535: step 7293, loss 0.288915, acc 0.890625, learning_rate 0.0001
2017-10-11T11:16:59.853089: step 7294, loss 0.0857486, acc 0.96875, learning_rate 0.0001
2017-10-11T11:16:59.965444: step 7295, loss 0.120445, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:00.076297: step 7296, loss 0.0731255, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:00.186689: step 7297, loss 0.129205, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:00.306014: step 7298, loss 0.144831, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:00.421726: step 7299, loss 0.0826126, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:00.534538: step 7300, loss 0.10158, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:00.641355: step 7301, loss 0.1865, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:00.753660: step 7302, loss 0.0831795, acc 1, learning_rate 0.0001
2017-10-11T11:17:00.860513: step 7303, loss 0.123452, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:00.953106: step 7304, loss 0.084911, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:01.061542: step 7305, loss 0.180058, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:01.159983: step 7306, loss 0.190419, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:01.261132: step 7307, loss 0.166376, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:01.382600: step 7308, loss 0.287137, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:01.488608: step 7309, loss 0.115563, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:01.601100: step 7310, loss 0.0842962, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:01.711196: step 7311, loss 0.295864, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:01.826800: step 7312, loss 0.152892, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:01.932576: step 7313, loss 0.104425, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:02.043561: step 7314, loss 0.225284, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:02.157116: step 7315, loss 0.171378, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:02.270079: step 7316, loss 0.0606637, acc 1, learning_rate 0.0001
2017-10-11T11:17:02.372853: step 7317, loss 0.117687, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:02.492820: step 7318, loss 0.156443, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:02.584931: step 7319, loss 0.103772, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:02.685838: step 7320, loss 0.171088, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:02.894905: step 7320, loss 0.215024, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7320

2017-10-11T11:17:03.760866: step 7321, loss 0.15728, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:03.865744: step 7322, loss 0.0551608, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:03.975730: step 7323, loss 0.200473, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:04.094432: step 7324, loss 0.109473, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:04.228918: step 7325, loss 0.168193, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:04.371838: step 7326, loss 0.128255, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:04.455517: step 7327, loss 0.170487, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:04.529952: step 7328, loss 0.148608, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:04.608545: step 7329, loss 0.303299, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:04.686296: step 7330, loss 0.130591, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:04.762369: step 7331, loss 0.165606, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:04.843794: step 7332, loss 0.305021, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:04.938283: step 7333, loss 0.118055, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:05.044535: step 7334, loss 0.180114, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:05.148849: step 7335, loss 0.156288, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:05.263196: step 7336, loss 0.120567, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:05.383855: step 7337, loss 0.211856, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:05.490951: step 7338, loss 0.137464, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:05.596875: step 7339, loss 0.284416, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:05.715858: step 7340, loss 0.0913067, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:05.844450: step 7341, loss 0.0655019, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:05.959955: step 7342, loss 0.389728, acc 0.828125, learning_rate 0.0001
2017-10-11T11:17:06.074779: step 7343, loss 0.212836, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:06.183466: step 7344, loss 0.131234, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:06.294759: step 7345, loss 0.15758, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:06.409678: step 7346, loss 0.141572, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:06.521042: step 7347, loss 0.30804, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:06.638506: step 7348, loss 0.173748, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:06.754405: step 7349, loss 0.0624508, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:06.859753: step 7350, loss 0.198424, acc 0.941176, learning_rate 0.0001
2017-10-11T11:17:06.972378: step 7351, loss 0.134701, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:07.082315: step 7352, loss 0.350269, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:07.196343: step 7353, loss 0.0912185, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:07.304223: step 7354, loss 0.261069, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:07.415666: step 7355, loss 0.172296, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:07.531330: step 7356, loss 0.166165, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:07.633711: step 7357, loss 0.167673, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:07.738107: step 7358, loss 0.0971862, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:07.856696: step 7359, loss 0.304602, acc 0.875, learning_rate 0.0001
2017-10-11T11:17:07.968941: step 7360, loss 0.115181, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:08.157703: step 7360, loss 0.214744, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7360

2017-10-11T11:17:09.179339: step 7361, loss 0.0920233, acc 1, learning_rate 0.0001
2017-10-11T11:17:09.291757: step 7362, loss 0.107094, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:09.472885: step 7363, loss 0.16143, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:09.580463: step 7364, loss 0.149908, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:09.656865: step 7365, loss 0.065206, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:09.731205: step 7366, loss 0.204737, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:09.815975: step 7367, loss 0.173509, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:09.889005: step 7368, loss 0.11166, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:09.962861: step 7369, loss 0.0684194, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:10.039862: step 7370, loss 0.108259, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:10.126153: step 7371, loss 0.269241, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:10.211998: step 7372, loss 0.102499, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:10.303892: step 7373, loss 0.194686, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:10.420291: step 7374, loss 0.125169, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:10.533276: step 7375, loss 0.140601, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:10.635059: step 7376, loss 0.117399, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:10.747462: step 7377, loss 0.179511, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:10.855103: step 7378, loss 0.171829, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:10.971160: step 7379, loss 0.164671, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:11.087569: step 7380, loss 0.0862698, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:11.201637: step 7381, loss 0.134745, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:11.311864: step 7382, loss 0.140827, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:11.419094: step 7383, loss 0.177308, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:11.529900: step 7384, loss 0.151247, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:11.645513: step 7385, loss 0.0562418, acc 1, learning_rate 0.0001
2017-10-11T11:17:11.753989: step 7386, loss 0.0742713, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:11.849611: step 7387, loss 0.185886, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:11.951339: step 7388, loss 0.127686, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:12.053456: step 7389, loss 0.0924473, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:12.163816: step 7390, loss 0.182524, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:12.288457: step 7391, loss 0.183461, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:12.393523: step 7392, loss 0.366237, acc 0.859375, learning_rate 0.0001
2017-10-11T11:17:12.494128: step 7393, loss 0.225165, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:12.607654: step 7394, loss 0.161208, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:12.715304: step 7395, loss 0.107159, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:12.824297: step 7396, loss 0.216076, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:12.931903: step 7397, loss 0.160249, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:13.054323: step 7398, loss 0.345441, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:13.161367: step 7399, loss 0.114853, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:13.275194: step 7400, loss 0.115896, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:13.461006: step 7400, loss 0.213639, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7400

2017-10-11T11:17:14.258571: step 7401, loss 0.141914, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:14.369586: step 7402, loss 0.0950513, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:14.479473: step 7403, loss 0.17311, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:14.589480: step 7404, loss 0.146737, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:14.703960: step 7405, loss 0.191482, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:14.848975: step 7406, loss 0.0738475, acc 1, learning_rate 0.0001
2017-10-11T11:17:15.000219: step 7407, loss 0.110857, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:15.088094: step 7408, loss 0.0495648, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:15.169048: step 7409, loss 0.195519, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:15.241759: step 7410, loss 0.0747074, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:15.319559: step 7411, loss 0.166988, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:15.393378: step 7412, loss 0.15808, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:15.469919: step 7413, loss 0.123203, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:15.550193: step 7414, loss 0.202112, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:15.644834: step 7415, loss 0.117784, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:15.766156: step 7416, loss 0.131075, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:15.883004: step 7417, loss 0.138303, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:15.994751: step 7418, loss 0.0982434, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:16.107677: step 7419, loss 0.149037, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:16.214570: step 7420, loss 0.107324, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:16.331714: step 7421, loss 0.112185, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:16.442015: step 7422, loss 0.189042, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:16.524449: step 7423, loss 0.228482, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:16.620806: step 7424, loss 0.0914519, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:16.734939: step 7425, loss 0.0872665, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:16.844201: step 7426, loss 0.109797, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:16.955419: step 7427, loss 0.123639, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:17.084458: step 7428, loss 0.0801131, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:17.195475: step 7429, loss 0.157548, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:17.306893: step 7430, loss 0.0844378, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:17.420488: step 7431, loss 0.0822086, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:17.540305: step 7432, loss 0.0984237, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:17.663005: step 7433, loss 0.21533, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:17.776219: step 7434, loss 0.272054, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:17.882861: step 7435, loss 0.140416, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:17.995872: step 7436, loss 0.256138, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:18.096842: step 7437, loss 0.256082, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:18.204230: step 7438, loss 0.229316, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:18.320310: step 7439, loss 0.18368, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:18.414667: step 7440, loss 0.201433, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:18.617065: step 7440, loss 0.213733, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7440

2017-10-11T11:17:19.595111: step 7441, loss 0.0655373, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:19.693253: step 7442, loss 0.16845, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:19.803614: step 7443, loss 0.184978, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:19.909147: step 7444, loss 0.202837, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:20.026662: step 7445, loss 0.0617056, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:20.207541: step 7446, loss 0.124339, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:20.308614: step 7447, loss 0.19108, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:20.377582: step 7448, loss 0.117627, acc 0.960784, learning_rate 0.0001
2017-10-11T11:17:20.455729: step 7449, loss 0.0803413, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:20.540147: step 7450, loss 0.092552, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:20.612216: step 7451, loss 0.118742, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:20.701809: step 7452, loss 0.162334, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:20.790530: step 7453, loss 0.21328, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:20.865341: step 7454, loss 0.152244, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:20.974920: step 7455, loss 0.166892, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:21.085870: step 7456, loss 0.105386, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:21.201326: step 7457, loss 0.215578, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:21.307598: step 7458, loss 0.114631, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:21.418740: step 7459, loss 0.129553, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:21.525381: step 7460, loss 0.0799174, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:21.622423: step 7461, loss 0.163416, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:21.729740: step 7462, loss 0.146165, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:21.834605: step 7463, loss 0.135565, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:21.936886: step 7464, loss 0.180868, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:22.048104: step 7465, loss 0.0760765, acc 1, learning_rate 0.0001
2017-10-11T11:17:22.171492: step 7466, loss 0.229935, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:22.282926: step 7467, loss 0.165736, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:22.406961: step 7468, loss 0.205461, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:22.532465: step 7469, loss 0.169354, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:22.638107: step 7470, loss 0.082901, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:22.753789: step 7471, loss 0.064795, acc 1, learning_rate 0.0001
2017-10-11T11:17:22.865654: step 7472, loss 0.105818, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:22.982946: step 7473, loss 0.123475, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:23.093790: step 7474, loss 0.122074, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:23.206895: step 7475, loss 0.110431, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:23.325117: step 7476, loss 0.120443, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:23.438469: step 7477, loss 0.0975349, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:23.569687: step 7478, loss 0.238474, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:23.677139: step 7479, loss 0.130154, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:23.788846: step 7480, loss 0.146841, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:23.979356: step 7480, loss 0.213925, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7480

2017-10-11T11:17:25.018380: step 7481, loss 0.190272, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:25.114411: step 7482, loss 0.155306, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:25.235042: step 7483, loss 0.0862712, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:25.339475: step 7484, loss 0.132888, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:25.521063: step 7485, loss 0.135282, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:25.632088: step 7486, loss 0.116275, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:25.721105: step 7487, loss 0.165841, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:25.799818: step 7488, loss 0.19649, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:25.871758: step 7489, loss 0.159224, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:25.949139: step 7490, loss 0.148129, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:26.029205: step 7491, loss 0.130002, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:26.100732: step 7492, loss 0.268443, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:26.179051: step 7493, loss 0.199836, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:26.296111: step 7494, loss 0.145415, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:26.407734: step 7495, loss 0.107499, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:26.517586: step 7496, loss 0.139969, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:26.629013: step 7497, loss 0.101319, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:26.741738: step 7498, loss 0.153704, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:26.852105: step 7499, loss 0.151972, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:26.961338: step 7500, loss 0.21883, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:27.076383: step 7501, loss 0.0891228, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:27.177788: step 7502, loss 0.170376, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:27.276837: step 7503, loss 0.249226, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:27.379772: step 7504, loss 0.173508, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:27.473487: step 7505, loss 0.0861839, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:27.593578: step 7506, loss 0.238011, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:27.713315: step 7507, loss 0.213127, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:27.830002: step 7508, loss 0.135592, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:27.933302: step 7509, loss 0.140514, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:28.046507: step 7510, loss 0.22073, acc 0.875, learning_rate 0.0001
2017-10-11T11:17:28.149862: step 7511, loss 0.245273, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:28.263995: step 7512, loss 0.151552, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:28.385146: step 7513, loss 0.0881716, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:28.494957: step 7514, loss 0.17317, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:28.595508: step 7515, loss 0.117415, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:28.688924: step 7516, loss 0.215658, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:28.787201: step 7517, loss 0.153014, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:28.890757: step 7518, loss 0.141334, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:28.988884: step 7519, loss 0.123022, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:29.088913: step 7520, loss 0.225718, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:29.280319: step 7520, loss 0.214101, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7520

2017-10-11T11:17:30.060088: step 7521, loss 0.092895, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:30.164730: step 7522, loss 0.197081, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:30.276939: step 7523, loss 0.245386, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:30.389782: step 7524, loss 0.158254, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:30.503276: step 7525, loss 0.125024, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:30.622472: step 7526, loss 0.144431, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:30.739515: step 7527, loss 0.0884082, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:30.925283: step 7528, loss 0.214662, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:31.025175: step 7529, loss 0.190444, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:31.099542: step 7530, loss 0.209737, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:31.176514: step 7531, loss 0.19035, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:31.265991: step 7532, loss 0.191964, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:31.347160: step 7533, loss 0.0927812, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:31.427491: step 7534, loss 0.155423, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:31.510019: step 7535, loss 0.210416, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:31.636402: step 7536, loss 0.105289, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:31.751746: step 7537, loss 0.126279, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:31.862941: step 7538, loss 0.17346, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:31.970732: step 7539, loss 0.181, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:32.080914: step 7540, loss 0.10965, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:32.193924: step 7541, loss 0.141836, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:32.303159: step 7542, loss 0.118523, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:32.413940: step 7543, loss 0.138742, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:32.529399: step 7544, loss 0.11134, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:32.623501: step 7545, loss 0.272899, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:32.717052: step 7546, loss 0.265731, acc 0.960784, learning_rate 0.0001
2017-10-11T11:17:32.819908: step 7547, loss 0.185577, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:32.923367: step 7548, loss 0.197762, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:33.048511: step 7549, loss 0.201147, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:33.157043: step 7550, loss 0.105426, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:33.255716: step 7551, loss 0.159968, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:33.367864: step 7552, loss 0.263784, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:33.473259: step 7553, loss 0.158594, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:33.589863: step 7554, loss 0.162243, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:33.700402: step 7555, loss 0.158103, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:33.812877: step 7556, loss 0.230985, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:33.935475: step 7557, loss 0.114399, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:34.048810: step 7558, loss 0.0951519, acc 1, learning_rate 0.0001
2017-10-11T11:17:34.153859: step 7559, loss 0.130744, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:34.270248: step 7560, loss 0.209708, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:34.471987: step 7560, loss 0.214956, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7560

2017-10-11T11:17:35.376993: step 7561, loss 0.1885, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:35.470701: step 7562, loss 0.0991905, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:35.580196: step 7563, loss 0.162239, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:35.697732: step 7564, loss 0.135566, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:35.808225: step 7565, loss 0.0786166, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:35.928624: step 7566, loss 0.143012, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:36.122643: step 7567, loss 0.126134, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:36.215128: step 7568, loss 0.145559, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:36.285338: step 7569, loss 0.118202, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:36.358043: step 7570, loss 0.15434, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:36.430503: step 7571, loss 0.167954, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:36.500279: step 7572, loss 0.110755, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:36.570972: step 7573, loss 0.0702841, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:36.647739: step 7574, loss 0.135318, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:36.721395: step 7575, loss 0.150627, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:36.804969: step 7576, loss 0.118803, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:36.877323: step 7577, loss 0.121035, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:36.954847: step 7578, loss 0.0857283, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:37.031323: step 7579, loss 0.14865, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:37.102914: step 7580, loss 0.114879, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:37.180094: step 7581, loss 0.116612, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:37.292825: step 7582, loss 0.0568112, acc 1, learning_rate 0.0001
2017-10-11T11:17:37.400841: step 7583, loss 0.128817, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:37.518464: step 7584, loss 0.168112, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:37.631174: step 7585, loss 0.160696, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:37.740952: step 7586, loss 0.257429, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:37.859613: step 7587, loss 0.140745, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:37.977781: step 7588, loss 0.175381, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:38.087963: step 7589, loss 0.168251, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:38.204984: step 7590, loss 0.161301, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:38.322497: step 7591, loss 0.122888, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:38.443443: step 7592, loss 0.205351, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:38.566121: step 7593, loss 0.141288, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:38.680882: step 7594, loss 0.135483, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:38.796684: step 7595, loss 0.150969, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:38.915239: step 7596, loss 0.160695, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:39.048691: step 7597, loss 0.147102, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:39.154976: step 7598, loss 0.181291, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:39.272067: step 7599, loss 0.208522, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:39.387317: step 7600, loss 0.214242, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:39.589073: step 7600, loss 0.21475, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7600

2017-10-11T11:17:40.480329: step 7601, loss 0.163214, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:40.579971: step 7602, loss 0.288926, acc 0.859375, learning_rate 0.0001
2017-10-11T11:17:40.688858: step 7603, loss 0.176576, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:40.807975: step 7604, loss 0.123017, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:40.914841: step 7605, loss 0.1309, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:41.038571: step 7606, loss 0.1298, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:41.152961: step 7607, loss 0.0872223, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:41.267013: step 7608, loss 0.0606049, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:41.391525: step 7609, loss 0.15241, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:41.491227: step 7610, loss 0.142211, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:41.613281: step 7611, loss 0.0837442, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:41.793033: step 7612, loss 0.0703827, acc 1, learning_rate 0.0001
2017-10-11T11:17:41.891357: step 7613, loss 0.200713, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:41.968815: step 7614, loss 0.132041, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:42.047773: step 7615, loss 0.160533, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:42.137505: step 7616, loss 0.174521, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:42.223647: step 7617, loss 0.0774, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:42.300914: step 7618, loss 0.256438, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:42.385192: step 7619, loss 0.126818, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:42.515305: step 7620, loss 0.110319, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:42.617437: step 7621, loss 0.117257, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:42.723825: step 7622, loss 0.282952, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:42.843612: step 7623, loss 0.271878, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:42.960051: step 7624, loss 0.147107, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:43.077433: step 7625, loss 0.176504, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:43.197687: step 7626, loss 0.16673, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:43.314101: step 7627, loss 0.138007, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:43.435875: step 7628, loss 0.158236, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:43.549076: step 7629, loss 0.168985, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:43.655633: step 7630, loss 0.103056, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:43.774069: step 7631, loss 0.252926, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:43.902195: step 7632, loss 0.102845, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:44.007393: step 7633, loss 0.194743, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:44.132826: step 7634, loss 0.2084, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:44.234742: step 7635, loss 0.190063, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:44.343741: step 7636, loss 0.238313, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:44.466298: step 7637, loss 0.126093, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:44.580251: step 7638, loss 0.1195, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:44.696883: step 7639, loss 0.216692, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:44.819991: step 7640, loss 0.188961, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:45.005434: step 7640, loss 0.215246, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7640

2017-10-11T11:17:46.081274: step 7641, loss 0.118197, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:46.189112: step 7642, loss 0.114488, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:46.305242: step 7643, loss 0.152803, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:46.408073: step 7644, loss 0.132544, acc 0.941176, learning_rate 0.0001
2017-10-11T11:17:46.519359: step 7645, loss 0.110381, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:46.632559: step 7646, loss 0.102487, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:46.744763: step 7647, loss 0.124303, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:46.919498: step 7648, loss 0.060286, acc 1, learning_rate 0.0001
2017-10-11T11:17:47.036302: step 7649, loss 0.18574, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:47.114536: step 7650, loss 0.151198, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:47.190376: step 7651, loss 0.072153, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:47.268550: step 7652, loss 0.105885, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:47.347430: step 7653, loss 0.0616061, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:47.432009: step 7654, loss 0.181779, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:47.507423: step 7655, loss 0.0973347, acc 1, learning_rate 0.0001
2017-10-11T11:17:47.583886: step 7656, loss 0.110171, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:47.709351: step 7657, loss 0.100658, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:47.824017: step 7658, loss 0.138148, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:47.936273: step 7659, loss 0.102116, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:48.050133: step 7660, loss 0.0377153, acc 1, learning_rate 0.0001
2017-10-11T11:17:48.169410: step 7661, loss 0.251445, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:48.288595: step 7662, loss 0.182065, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:48.384498: step 7663, loss 0.0939861, acc 1, learning_rate 0.0001
2017-10-11T11:17:48.497936: step 7664, loss 0.0814289, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:48.610588: step 7665, loss 0.200853, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:48.723526: step 7666, loss 0.12681, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:48.839204: step 7667, loss 0.164436, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:48.950471: step 7668, loss 0.141432, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:49.061522: step 7669, loss 0.28442, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:49.180801: step 7670, loss 0.152596, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:49.296741: step 7671, loss 0.179233, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:49.398207: step 7672, loss 0.164168, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:49.497003: step 7673, loss 0.272427, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:49.608384: step 7674, loss 0.131667, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:49.703993: step 7675, loss 0.106323, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:49.806314: step 7676, loss 0.110594, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:49.916799: step 7677, loss 0.146475, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:50.030386: step 7678, loss 0.119393, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:50.156990: step 7679, loss 0.119751, acc 1, learning_rate 0.0001
2017-10-11T11:17:50.265336: step 7680, loss 0.24371, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:50.485297: step 7680, loss 0.214918, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7680

2017-10-11T11:17:51.284166: step 7681, loss 0.0990037, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:51.386325: step 7682, loss 0.174502, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:51.504981: step 7683, loss 0.182649, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:51.610443: step 7684, loss 0.292236, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:51.726044: step 7685, loss 0.107473, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:51.855336: step 7686, loss 0.101049, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:51.969386: step 7687, loss 0.154399, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:52.077015: step 7688, loss 0.141715, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:52.265406: step 7689, loss 0.210135, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:52.354820: step 7690, loss 0.0943111, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:52.433844: step 7691, loss 0.197875, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:52.509867: step 7692, loss 0.140465, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:52.594474: step 7693, loss 0.217846, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:52.670499: step 7694, loss 0.269667, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:52.743283: step 7695, loss 0.230446, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:52.821944: step 7696, loss 0.110782, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:52.894370: step 7697, loss 0.115432, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:53.017153: step 7698, loss 0.11839, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:53.127562: step 7699, loss 0.269218, acc 0.890625, learning_rate 0.0001
2017-10-11T11:17:53.236704: step 7700, loss 0.189425, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:53.351270: step 7701, loss 0.100277, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:53.461661: step 7702, loss 0.218627, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:53.578316: step 7703, loss 0.135276, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:53.691171: step 7704, loss 0.185003, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:53.801238: step 7705, loss 0.22712, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:53.916704: step 7706, loss 0.140017, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:54.028072: step 7707, loss 0.142776, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:54.141889: step 7708, loss 0.117123, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:54.259880: step 7709, loss 0.16836, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:54.372128: step 7710, loss 0.19451, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:54.481998: step 7711, loss 0.0782603, acc 1, learning_rate 0.0001
2017-10-11T11:17:54.598170: step 7712, loss 0.204335, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:54.711841: step 7713, loss 0.0889292, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:54.827774: step 7714, loss 0.168871, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:54.921036: step 7715, loss 0.171157, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:55.028630: step 7716, loss 0.110958, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:55.127314: step 7717, loss 0.142868, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:55.238069: step 7718, loss 0.133376, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:55.338682: step 7719, loss 0.182139, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:55.465028: step 7720, loss 0.130333, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:17:55.635302: step 7720, loss 0.214795, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7720

2017-10-11T11:17:56.666426: step 7721, loss 0.113459, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:56.762473: step 7722, loss 0.120853, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:56.868989: step 7723, loss 0.0968634, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:56.986437: step 7724, loss 0.189711, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:57.097531: step 7725, loss 0.0796829, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:57.226327: step 7726, loss 0.129468, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:57.336840: step 7727, loss 0.146848, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:57.525320: step 7728, loss 0.214152, acc 0.90625, learning_rate 0.0001
2017-10-11T11:17:57.627876: step 7729, loss 0.0873629, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:57.704872: step 7730, loss 0.0973116, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:57.783637: step 7731, loss 0.193034, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:57.856446: step 7732, loss 0.136827, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:57.930296: step 7733, loss 0.121822, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:58.004896: step 7734, loss 0.247658, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:58.080419: step 7735, loss 0.181308, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:58.153569: step 7736, loss 0.271002, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:58.267664: step 7737, loss 0.0745062, acc 1, learning_rate 0.0001
2017-10-11T11:17:58.342401: step 7738, loss 0.0761659, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:58.413534: step 7739, loss 0.0734624, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:58.487319: step 7740, loss 0.154096, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:58.563890: step 7741, loss 0.113032, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:58.636999: step 7742, loss 0.134014, acc 0.941176, learning_rate 0.0001
2017-10-11T11:17:58.744924: step 7743, loss 0.139023, acc 0.96875, learning_rate 0.0001
2017-10-11T11:17:58.860124: step 7744, loss 0.0615582, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:58.979113: step 7745, loss 0.124709, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:59.091934: step 7746, loss 0.204576, acc 0.921875, learning_rate 0.0001
2017-10-11T11:17:59.210813: step 7747, loss 0.120142, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:59.327933: step 7748, loss 0.12895, acc 0.9375, learning_rate 0.0001
2017-10-11T11:17:59.452893: step 7749, loss 0.130298, acc 0.984375, learning_rate 0.0001
2017-10-11T11:17:59.568668: step 7750, loss 0.130948, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:59.673460: step 7751, loss 0.139889, acc 0.953125, learning_rate 0.0001
2017-10-11T11:17:59.790433: step 7752, loss 0.0466917, acc 1, learning_rate 0.0001
2017-10-11T11:17:59.920540: step 7753, loss 0.200386, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:00.026964: step 7754, loss 0.124099, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:00.148872: step 7755, loss 0.215921, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:00.259124: step 7756, loss 0.104684, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:00.366415: step 7757, loss 0.252546, acc 0.90625, learning_rate 0.0001
2017-10-11T11:18:00.496864: step 7758, loss 0.131693, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:00.603113: step 7759, loss 0.0847019, acc 1, learning_rate 0.0001
2017-10-11T11:18:00.713291: step 7760, loss 0.122295, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:18:00.918213: step 7760, loss 0.214067, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7760

2017-10-11T11:18:01.940073: step 7761, loss 0.123815, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:02.035421: step 7762, loss 0.181126, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:02.148877: step 7763, loss 0.129261, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:02.267923: step 7764, loss 0.248731, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:02.376897: step 7765, loss 0.215742, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:02.486338: step 7766, loss 0.0863222, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:02.598942: step 7767, loss 0.218912, acc 0.90625, learning_rate 0.0001
2017-10-11T11:18:02.706748: step 7768, loss 0.135712, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:02.816337: step 7769, loss 0.0995905, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:02.927893: step 7770, loss 0.180275, acc 0.90625, learning_rate 0.0001
2017-10-11T11:18:03.048805: step 7771, loss 0.116217, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:03.212791: step 7772, loss 0.0771337, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:03.290535: step 7773, loss 0.252171, acc 0.921875, learning_rate 0.0001
2017-10-11T11:18:03.366588: step 7774, loss 0.152473, acc 0.921875, learning_rate 0.0001
2017-10-11T11:18:03.441524: step 7775, loss 0.257649, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:03.530662: step 7776, loss 0.121508, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:03.609146: step 7777, loss 0.135099, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:03.683642: step 7778, loss 0.229454, acc 0.90625, learning_rate 0.0001
2017-10-11T11:18:03.760810: step 7779, loss 0.177834, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:03.860838: step 7780, loss 0.203178, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:03.984553: step 7781, loss 0.172683, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:04.097851: step 7782, loss 0.211095, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:04.210766: step 7783, loss 0.122358, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:04.320704: step 7784, loss 0.164098, acc 0.90625, learning_rate 0.0001
2017-10-11T11:18:04.435555: step 7785, loss 0.0448997, acc 1, learning_rate 0.0001
2017-10-11T11:18:04.549312: step 7786, loss 0.109066, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:04.664918: step 7787, loss 0.157895, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:04.774382: step 7788, loss 0.190873, acc 0.921875, learning_rate 0.0001
2017-10-11T11:18:04.891915: step 7789, loss 0.0995371, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:04.998228: step 7790, loss 0.153836, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:05.082685: step 7791, loss 0.225098, acc 0.90625, learning_rate 0.0001
2017-10-11T11:18:05.188242: step 7792, loss 0.150242, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:05.294652: step 7793, loss 0.152876, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:05.405170: step 7794, loss 0.222043, acc 0.90625, learning_rate 0.0001
2017-10-11T11:18:05.508927: step 7795, loss 0.122686, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:05.621953: step 7796, loss 0.14709, acc 0.921875, learning_rate 0.0001
2017-10-11T11:18:05.736497: step 7797, loss 0.129915, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:05.848200: step 7798, loss 0.101246, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:05.957486: step 7799, loss 0.210132, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:06.067854: step 7800, loss 0.131487, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:18:06.255401: step 7800, loss 0.214335, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7800

2017-10-11T11:18:07.068892: step 7801, loss 0.154592, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:07.187130: step 7802, loss 0.116557, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:07.300518: step 7803, loss 0.189889, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:07.405990: step 7804, loss 0.197505, acc 0.890625, learning_rate 0.0001
2017-10-11T11:18:07.521652: step 7805, loss 0.0889926, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:07.620905: step 7806, loss 0.0999543, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:07.748627: step 7807, loss 0.0713216, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:07.865905: step 7808, loss 0.126849, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:07.979278: step 7809, loss 0.139073, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:08.084458: step 7810, loss 0.0638763, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:08.201101: step 7811, loss 0.218353, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:08.325943: step 7812, loss 0.17675, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:08.489483: step 7813, loss 0.176308, acc 0.921875, learning_rate 0.0001
2017-10-11T11:18:08.566153: step 7814, loss 0.116435, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:08.643047: step 7815, loss 0.0785618, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:08.718928: step 7816, loss 0.102252, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:08.794248: step 7817, loss 0.0600544, acc 1, learning_rate 0.0001
2017-10-11T11:18:08.870694: step 7818, loss 0.121791, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:08.946578: step 7819, loss 0.342326, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:09.021439: step 7820, loss 0.274328, acc 0.90625, learning_rate 0.0001
2017-10-11T11:18:09.116895: step 7821, loss 0.135753, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:09.218712: step 7822, loss 0.146894, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:09.331499: step 7823, loss 0.133661, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:09.448346: step 7824, loss 0.160161, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:09.541345: step 7825, loss 0.0715423, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:09.663564: step 7826, loss 0.230669, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:09.774375: step 7827, loss 0.121376, acc 1, learning_rate 0.0001
2017-10-11T11:18:09.882534: step 7828, loss 0.0892458, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:09.999778: step 7829, loss 0.159207, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:10.130824: step 7830, loss 0.268194, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:10.228854: step 7831, loss 0.13131, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:10.342462: step 7832, loss 0.120411, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:10.464185: step 7833, loss 0.112327, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:10.571303: step 7834, loss 0.13599, acc 0.9375, learning_rate 0.0001
2017-10-11T11:18:10.686592: step 7835, loss 0.0947784, acc 0.984375, learning_rate 0.0001
2017-10-11T11:18:10.807624: step 7836, loss 0.142651, acc 0.96875, learning_rate 0.0001
2017-10-11T11:18:10.917483: step 7837, loss 0.200091, acc 0.921875, learning_rate 0.0001
2017-10-11T11:18:11.042294: step 7838, loss 0.151111, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:11.155373: step 7839, loss 0.121585, acc 0.953125, learning_rate 0.0001
2017-10-11T11:18:11.260152: step 7840, loss 0.240027, acc 0.901961, learning_rate 0.0001

Evaluation:
2017-10-11T11:18:11.463537: step 7840, loss 0.21521, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507737805/checkpoints/model-7840

