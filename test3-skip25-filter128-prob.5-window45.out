
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507664331

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T14:38:56.202946: step 1, loss 6.63621, acc 0.21875, learning_rate 0.005
2017-10-10T14:38:56.439820: step 2, loss 5.82103, acc 0.25, learning_rate 0.00498
2017-10-10T14:38:56.666568: step 3, loss 4.89771, acc 0.40625, learning_rate 0.00496008
2017-10-10T14:38:56.874739: step 4, loss 4.69261, acc 0.375, learning_rate 0.00494024
2017-10-10T14:38:57.071223: step 5, loss 7.00168, acc 0.3125, learning_rate 0.00492049
2017-10-10T14:38:57.288664: step 6, loss 3.30896, acc 0.4375, learning_rate 0.00490081
2017-10-10T14:38:57.494249: step 7, loss 3.8861, acc 0.40625, learning_rate 0.00488121
2017-10-10T14:38:57.684855: step 8, loss 4.72062, acc 0.4375, learning_rate 0.0048617
2017-10-10T14:38:57.876860: step 9, loss 4.20251, acc 0.390625, learning_rate 0.00484226
2017-10-10T14:38:58.097017: step 10, loss 3.12076, acc 0.453125, learning_rate 0.00482291
2017-10-10T14:38:58.312829: step 11, loss 2.4071, acc 0.5, learning_rate 0.00480363
2017-10-10T14:38:58.505903: step 12, loss 2.51911, acc 0.5625, learning_rate 0.00478443
2017-10-10T14:38:58.708208: step 13, loss 2.63567, acc 0.59375, learning_rate 0.00476531
2017-10-10T14:38:58.916538: step 14, loss 2.19279, acc 0.625, learning_rate 0.00474627
2017-10-10T14:38:59.122328: step 15, loss 3.67758, acc 0.484375, learning_rate 0.0047273
2017-10-10T14:38:59.323728: step 16, loss 1.84182, acc 0.71875, learning_rate 0.00470841
2017-10-10T14:38:59.526128: step 17, loss 2.79046, acc 0.578125, learning_rate 0.0046896
2017-10-10T14:38:59.730002: step 18, loss 2.05791, acc 0.59375, learning_rate 0.00467087
2017-10-10T14:38:59.931880: step 19, loss 2.31114, acc 0.609375, learning_rate 0.00465221
2017-10-10T14:39:00.135286: step 20, loss 2.13863, acc 0.59375, learning_rate 0.00463363
2017-10-10T14:39:00.336442: step 21, loss 1.96676, acc 0.5625, learning_rate 0.00461513
2017-10-10T14:39:00.535405: step 22, loss 2.7426, acc 0.5625, learning_rate 0.0045967
2017-10-10T14:39:00.727042: step 23, loss 1.7762, acc 0.65625, learning_rate 0.00457834
2017-10-10T14:39:00.932394: step 24, loss 2.14427, acc 0.640625, learning_rate 0.00456006
2017-10-10T14:39:01.134203: step 25, loss 1.93152, acc 0.65625, learning_rate 0.00454186
2017-10-10T14:39:01.328741: step 26, loss 1.61532, acc 0.671875, learning_rate 0.00452373
2017-10-10T14:39:01.517448: step 27, loss 0.572373, acc 0.84375, learning_rate 0.00450567
2017-10-10T14:39:01.729368: step 28, loss 1.26251, acc 0.765625, learning_rate 0.00448769
2017-10-10T14:39:01.905884: step 29, loss 1.88535, acc 0.625, learning_rate 0.00446978
2017-10-10T14:39:02.112409: step 30, loss 1.68737, acc 0.6875, learning_rate 0.00445194
2017-10-10T14:39:02.293022: step 31, loss 0.771392, acc 0.78125, learning_rate 0.00443418
2017-10-10T14:39:02.462000: step 32, loss 2.43685, acc 0.71875, learning_rate 0.00441649
2017-10-10T14:39:02.656874: step 33, loss 1.95814, acc 0.765625, learning_rate 0.00439887
2017-10-10T14:39:02.849099: step 34, loss 1.16935, acc 0.6875, learning_rate 0.00438132
2017-10-10T14:39:03.056399: step 35, loss 1.4082, acc 0.78125, learning_rate 0.00436385
2017-10-10T14:39:03.264444: step 36, loss 1.97672, acc 0.625, learning_rate 0.00434644
2017-10-10T14:39:03.468843: step 37, loss 1.39668, acc 0.6875, learning_rate 0.00432911
2017-10-10T14:39:03.704882: step 38, loss 1.77939, acc 0.609375, learning_rate 0.00431185
2017-10-10T14:39:03.980269: step 39, loss 1.58313, acc 0.6875, learning_rate 0.00429465
2017-10-10T14:39:04.119208: step 40, loss 1.11582, acc 0.671875, learning_rate 0.00427753

Evaluation:
2017-10-10T14:39:04.446829: step 40, loss 0.427358, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-40

2017-10-10T14:39:05.184975: step 41, loss 1.67513, acc 0.671875, learning_rate 0.00426048
2017-10-10T14:39:05.376650: step 42, loss 0.862069, acc 0.765625, learning_rate 0.0042435
2017-10-10T14:39:05.560404: step 43, loss 0.898693, acc 0.84375, learning_rate 0.00422659
2017-10-10T14:39:05.744596: step 44, loss 0.788925, acc 0.796875, learning_rate 0.00420974
2017-10-10T14:39:05.933394: step 45, loss 1.2919, acc 0.75, learning_rate 0.00419297
2017-10-10T14:39:06.144867: step 46, loss 1.54087, acc 0.703125, learning_rate 0.00417626
2017-10-10T14:39:06.331202: step 47, loss 0.962894, acc 0.78125, learning_rate 0.00415962
2017-10-10T14:39:06.528846: step 48, loss 1.36658, acc 0.734375, learning_rate 0.00414305
2017-10-10T14:39:06.741053: step 49, loss 0.736957, acc 0.8125, learning_rate 0.00412655
2017-10-10T14:39:06.916909: step 50, loss 1.35122, acc 0.703125, learning_rate 0.00411011
2017-10-10T14:39:07.127483: step 51, loss 0.754652, acc 0.78125, learning_rate 0.00409375
2017-10-10T14:39:07.311727: step 52, loss 0.739964, acc 0.84375, learning_rate 0.00407744
2017-10-10T14:39:07.512049: step 53, loss 0.990903, acc 0.78125, learning_rate 0.00406121
2017-10-10T14:39:07.665495: step 54, loss 0.886297, acc 0.78125, learning_rate 0.00404504
2017-10-10T14:39:07.848888: step 55, loss 0.952203, acc 0.828125, learning_rate 0.00402894
2017-10-10T14:39:08.057487: step 56, loss 0.883983, acc 0.859375, learning_rate 0.0040129
2017-10-10T14:39:08.254127: step 57, loss 0.94818, acc 0.78125, learning_rate 0.00399693
2017-10-10T14:39:08.446958: step 58, loss 0.618755, acc 0.8125, learning_rate 0.00398102
2017-10-10T14:39:08.648953: step 59, loss 0.915407, acc 0.8125, learning_rate 0.00396518
2017-10-10T14:39:08.828845: step 60, loss 1.55463, acc 0.703125, learning_rate 0.00394941
2017-10-10T14:39:09.026474: step 61, loss 0.868971, acc 0.75, learning_rate 0.00393369
2017-10-10T14:39:09.198841: step 62, loss 0.669924, acc 0.796875, learning_rate 0.00391804
2017-10-10T14:39:09.393999: step 63, loss 0.594535, acc 0.890625, learning_rate 0.00390246
2017-10-10T14:39:09.603059: step 64, loss 0.912721, acc 0.765625, learning_rate 0.00388694
2017-10-10T14:39:09.813327: step 65, loss 0.847812, acc 0.828125, learning_rate 0.00387148
2017-10-10T14:39:10.016516: step 66, loss 0.910109, acc 0.734375, learning_rate 0.00385609
2017-10-10T14:39:10.216090: step 67, loss 0.593155, acc 0.859375, learning_rate 0.00384076
2017-10-10T14:39:10.411741: step 68, loss 1.45706, acc 0.71875, learning_rate 0.00382549
2017-10-10T14:39:10.616716: step 69, loss 0.607556, acc 0.8125, learning_rate 0.00381028
2017-10-10T14:39:10.844847: step 70, loss 0.5805, acc 0.796875, learning_rate 0.00379514
2017-10-10T14:39:11.051187: step 71, loss 1.23658, acc 0.765625, learning_rate 0.00378005
2017-10-10T14:39:11.235086: step 72, loss 0.535003, acc 0.859375, learning_rate 0.00376503
2017-10-10T14:39:11.448846: step 73, loss 1.01633, acc 0.828125, learning_rate 0.00375007
2017-10-10T14:39:11.657166: step 74, loss 0.769479, acc 0.78125, learning_rate 0.00373517
2017-10-10T14:39:11.862451: step 75, loss 0.323689, acc 0.90625, learning_rate 0.00372034
2017-10-10T14:39:12.075105: step 76, loss 0.71714, acc 0.765625, learning_rate 0.00370556
2017-10-10T14:39:12.369336: step 77, loss 0.964949, acc 0.8125, learning_rate 0.00369084
2017-10-10T14:39:12.550155: step 78, loss 0.498119, acc 0.828125, learning_rate 0.00367619
2017-10-10T14:39:12.684665: step 79, loss 0.973123, acc 0.75, learning_rate 0.00366159
2017-10-10T14:39:12.818469: step 80, loss 0.946988, acc 0.78125, learning_rate 0.00364705

Evaluation:
2017-10-10T14:39:13.156417: step 80, loss 0.396915, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-80

2017-10-10T14:39:14.045064: step 81, loss 0.943286, acc 0.75, learning_rate 0.00363257
2017-10-10T14:39:14.224858: step 82, loss 0.803912, acc 0.828125, learning_rate 0.00361815
2017-10-10T14:39:14.433799: step 83, loss 0.636094, acc 0.828125, learning_rate 0.00360379
2017-10-10T14:39:14.648883: step 84, loss 1.0207, acc 0.765625, learning_rate 0.00358949
2017-10-10T14:39:14.859283: step 85, loss 0.812189, acc 0.765625, learning_rate 0.00357525
2017-10-10T14:39:15.066794: step 86, loss 0.840426, acc 0.78125, learning_rate 0.00356106
2017-10-10T14:39:15.276322: step 87, loss 0.660006, acc 0.765625, learning_rate 0.00354694
2017-10-10T14:39:15.470000: step 88, loss 0.679838, acc 0.796875, learning_rate 0.00353287
2017-10-10T14:39:15.666325: step 89, loss 0.895674, acc 0.8125, learning_rate 0.00351885
2017-10-10T14:39:15.868928: step 90, loss 0.700532, acc 0.8125, learning_rate 0.0035049
2017-10-10T14:39:16.079290: step 91, loss 0.73905, acc 0.84375, learning_rate 0.003491
2017-10-10T14:39:16.263813: step 92, loss 0.587946, acc 0.875, learning_rate 0.00347716
2017-10-10T14:39:16.465566: step 93, loss 0.551262, acc 0.890625, learning_rate 0.00346338
2017-10-10T14:39:16.666164: step 94, loss 0.615526, acc 0.796875, learning_rate 0.00344965
2017-10-10T14:39:16.866235: step 95, loss 0.66451, acc 0.84375, learning_rate 0.00343597
2017-10-10T14:39:17.068588: step 96, loss 0.694938, acc 0.875, learning_rate 0.00342236
2017-10-10T14:39:17.267467: step 97, loss 0.848968, acc 0.8125, learning_rate 0.0034088
2017-10-10T14:39:17.444627: step 98, loss 0.900571, acc 0.784314, learning_rate 0.00339529
2017-10-10T14:39:17.654761: step 99, loss 0.457149, acc 0.890625, learning_rate 0.00338184
2017-10-10T14:39:17.860831: step 100, loss 1.1597, acc 0.828125, learning_rate 0.00336844
2017-10-10T14:39:18.060907: step 101, loss 0.981256, acc 0.8125, learning_rate 0.0033551
2017-10-10T14:39:18.248411: step 102, loss 0.52158, acc 0.875, learning_rate 0.00334182
2017-10-10T14:39:18.436890: step 103, loss 0.632705, acc 0.84375, learning_rate 0.00332858
2017-10-10T14:39:18.676934: step 104, loss 0.432431, acc 0.859375, learning_rate 0.00331541
2017-10-10T14:39:18.848885: step 105, loss 1.02851, acc 0.75, learning_rate 0.00330228
2017-10-10T14:39:19.054774: step 106, loss 0.484805, acc 0.890625, learning_rate 0.00328921
2017-10-10T14:39:19.252324: step 107, loss 0.636377, acc 0.8125, learning_rate 0.00327619
2017-10-10T14:39:19.458875: step 108, loss 0.821458, acc 0.8125, learning_rate 0.00326323
2017-10-10T14:39:19.676351: step 109, loss 0.396275, acc 0.890625, learning_rate 0.00325032
2017-10-10T14:39:19.878669: step 110, loss 0.917791, acc 0.765625, learning_rate 0.00323746
2017-10-10T14:39:20.073893: step 111, loss 0.597495, acc 0.875, learning_rate 0.00322465
2017-10-10T14:39:20.269068: step 112, loss 0.486059, acc 0.859375, learning_rate 0.0032119
2017-10-10T14:39:20.453702: step 113, loss 0.464727, acc 0.8125, learning_rate 0.0031992
2017-10-10T14:39:20.733278: step 114, loss 0.61467, acc 0.8125, learning_rate 0.00318655
2017-10-10T14:39:20.912183: step 115, loss 0.375993, acc 0.890625, learning_rate 0.00317395
2017-10-10T14:39:21.045804: step 116, loss 0.677133, acc 0.828125, learning_rate 0.0031614
2017-10-10T14:39:21.181183: step 117, loss 0.421792, acc 0.890625, learning_rate 0.0031489
2017-10-10T14:39:21.326164: step 118, loss 0.796678, acc 0.8125, learning_rate 0.00313646
2017-10-10T14:39:21.458003: step 119, loss 0.39439, acc 0.859375, learning_rate 0.00312407
2017-10-10T14:39:21.589451: step 120, loss 0.717142, acc 0.8125, learning_rate 0.00311172

Evaluation:
2017-10-10T14:39:22.049193: step 120, loss 0.342247, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-120

2017-10-10T14:39:22.989974: step 121, loss 0.279444, acc 0.875, learning_rate 0.00309943
2017-10-10T14:39:23.203942: step 122, loss 0.645422, acc 0.78125, learning_rate 0.00308719
2017-10-10T14:39:23.411145: step 123, loss 0.466002, acc 0.890625, learning_rate 0.00307499
2017-10-10T14:39:23.604881: step 124, loss 0.514939, acc 0.828125, learning_rate 0.00306285
2017-10-10T14:39:23.807158: step 125, loss 0.304777, acc 0.921875, learning_rate 0.00305076
2017-10-10T14:39:24.010951: step 126, loss 0.415881, acc 0.890625, learning_rate 0.00303871
2017-10-10T14:39:24.229385: step 127, loss 0.631096, acc 0.859375, learning_rate 0.00302672
2017-10-10T14:39:24.416885: step 128, loss 0.433679, acc 0.828125, learning_rate 0.00301477
2017-10-10T14:39:24.624999: step 129, loss 0.689622, acc 0.796875, learning_rate 0.00300287
2017-10-10T14:39:24.828592: step 130, loss 0.318154, acc 0.921875, learning_rate 0.00299102
2017-10-10T14:39:25.036856: step 131, loss 0.43516, acc 0.875, learning_rate 0.00297922
2017-10-10T14:39:25.242071: step 132, loss 0.434919, acc 0.859375, learning_rate 0.00296747
2017-10-10T14:39:25.450263: step 133, loss 0.485309, acc 0.84375, learning_rate 0.00295577
2017-10-10T14:39:25.654529: step 134, loss 0.221794, acc 0.921875, learning_rate 0.00294411
2017-10-10T14:39:25.874592: step 135, loss 0.350607, acc 0.921875, learning_rate 0.0029325
2017-10-10T14:39:26.071111: step 136, loss 0.256733, acc 0.90625, learning_rate 0.00292094
2017-10-10T14:39:26.262026: step 137, loss 0.648197, acc 0.796875, learning_rate 0.00290943
2017-10-10T14:39:26.464229: step 138, loss 0.725431, acc 0.8125, learning_rate 0.00289796
2017-10-10T14:39:26.660819: step 139, loss 0.656862, acc 0.859375, learning_rate 0.00288654
2017-10-10T14:39:26.851688: step 140, loss 0.206628, acc 0.90625, learning_rate 0.00287516
2017-10-10T14:39:27.044428: step 141, loss 0.394641, acc 0.859375, learning_rate 0.00286384
2017-10-10T14:39:27.239840: step 142, loss 0.372428, acc 0.859375, learning_rate 0.00285256
2017-10-10T14:39:27.444807: step 143, loss 0.772473, acc 0.765625, learning_rate 0.00284132
2017-10-10T14:39:27.638476: step 144, loss 0.350156, acc 0.859375, learning_rate 0.00283013
2017-10-10T14:39:27.841396: step 145, loss 0.266081, acc 0.90625, learning_rate 0.00281899
2017-10-10T14:39:28.051678: step 146, loss 0.693738, acc 0.875, learning_rate 0.00280789
2017-10-10T14:39:28.254458: step 147, loss 0.432936, acc 0.921875, learning_rate 0.00279684
2017-10-10T14:39:28.475570: step 148, loss 0.269458, acc 0.890625, learning_rate 0.00278583
2017-10-10T14:39:28.677416: step 149, loss 0.539159, acc 0.828125, learning_rate 0.00277486
2017-10-10T14:39:28.864715: step 150, loss 0.524675, acc 0.828125, learning_rate 0.00276395
2017-10-10T14:39:29.069486: step 151, loss 0.493798, acc 0.84375, learning_rate 0.00275307
2017-10-10T14:39:29.381139: step 152, loss 0.53493, acc 0.84375, learning_rate 0.00274224
2017-10-10T14:39:29.539346: step 153, loss 0.455255, acc 0.890625, learning_rate 0.00273146
2017-10-10T14:39:29.677857: step 154, loss 0.262956, acc 0.953125, learning_rate 0.00272072
2017-10-10T14:39:29.814660: step 155, loss 0.346888, acc 0.890625, learning_rate 0.00271002
2017-10-10T14:39:29.949584: step 156, loss 0.579826, acc 0.90625, learning_rate 0.00269937
2017-10-10T14:39:30.078885: step 157, loss 0.14626, acc 0.953125, learning_rate 0.00268876
2017-10-10T14:39:30.220608: step 158, loss 0.223114, acc 0.875, learning_rate 0.00267819
2017-10-10T14:39:30.420180: step 159, loss 0.741403, acc 0.828125, learning_rate 0.00266767
2017-10-10T14:39:30.604865: step 160, loss 0.317494, acc 0.90625, learning_rate 0.00265719

Evaluation:
2017-10-10T14:39:31.064143: step 160, loss 0.306155, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-160

2017-10-10T14:39:31.888505: step 161, loss 0.308059, acc 0.890625, learning_rate 0.00264675
2017-10-10T14:39:32.100098: step 162, loss 0.221225, acc 0.9375, learning_rate 0.00263635
2017-10-10T14:39:32.302322: step 163, loss 0.661936, acc 0.8125, learning_rate 0.002626
2017-10-10T14:39:32.503786: step 164, loss 0.147284, acc 0.921875, learning_rate 0.00261569
2017-10-10T14:39:32.690124: step 165, loss 0.250534, acc 0.90625, learning_rate 0.00260542
2017-10-10T14:39:32.884603: step 166, loss 0.670887, acc 0.875, learning_rate 0.0025952
2017-10-10T14:39:33.080898: step 167, loss 0.154563, acc 0.9375, learning_rate 0.00258501
2017-10-10T14:39:33.280327: step 168, loss 0.488006, acc 0.890625, learning_rate 0.00257487
2017-10-10T14:39:33.475177: step 169, loss 0.336296, acc 0.90625, learning_rate 0.00256477
2017-10-10T14:39:33.676887: step 170, loss 0.261719, acc 0.859375, learning_rate 0.0025547
2017-10-10T14:39:33.877686: step 171, loss 0.499717, acc 0.859375, learning_rate 0.00254469
2017-10-10T14:39:34.083001: step 172, loss 0.421743, acc 0.859375, learning_rate 0.00253471
2017-10-10T14:39:34.314103: step 173, loss 0.383344, acc 0.84375, learning_rate 0.00252477
2017-10-10T14:39:34.523983: step 174, loss 0.559702, acc 0.84375, learning_rate 0.00251487
2017-10-10T14:39:34.741759: step 175, loss 0.342614, acc 0.890625, learning_rate 0.00250501
2017-10-10T14:39:34.939523: step 176, loss 0.134581, acc 0.9375, learning_rate 0.0024952
2017-10-10T14:39:35.136532: step 177, loss 0.306534, acc 0.90625, learning_rate 0.00248542
2017-10-10T14:39:35.338273: step 178, loss 0.379426, acc 0.90625, learning_rate 0.00247568
2017-10-10T14:39:35.533138: step 179, loss 0.388188, acc 0.859375, learning_rate 0.00246599
2017-10-10T14:39:35.739995: step 180, loss 0.535683, acc 0.84375, learning_rate 0.00245633
2017-10-10T14:39:35.946885: step 181, loss 0.724906, acc 0.828125, learning_rate 0.00244671
2017-10-10T14:39:36.168977: step 182, loss 0.959046, acc 0.8125, learning_rate 0.00243713
2017-10-10T14:39:36.366474: step 183, loss 0.244087, acc 0.90625, learning_rate 0.00242759
2017-10-10T14:39:36.564215: step 184, loss 0.769572, acc 0.796875, learning_rate 0.00241809
2017-10-10T14:39:36.752751: step 185, loss 0.434617, acc 0.859375, learning_rate 0.00240863
2017-10-10T14:39:36.980858: step 186, loss 0.197921, acc 0.953125, learning_rate 0.00239921
2017-10-10T14:39:37.192252: step 187, loss 0.252032, acc 0.90625, learning_rate 0.00238982
2017-10-10T14:39:37.390088: step 188, loss 0.352803, acc 0.890625, learning_rate 0.00238048
2017-10-10T14:39:37.584703: step 189, loss 0.264061, acc 0.890625, learning_rate 0.00237117
2017-10-10T14:39:37.882401: step 190, loss 0.485402, acc 0.84375, learning_rate 0.0023619
2017-10-10T14:39:38.046413: step 191, loss 0.956453, acc 0.859375, learning_rate 0.00235267
2017-10-10T14:39:38.180327: step 192, loss 0.433046, acc 0.875, learning_rate 0.00234347
2017-10-10T14:39:38.317792: step 193, loss 0.302008, acc 0.890625, learning_rate 0.00233431
2017-10-10T14:39:38.445680: step 194, loss 0.337636, acc 0.875, learning_rate 0.00232519
2017-10-10T14:39:38.577086: step 195, loss 0.366127, acc 0.921875, learning_rate 0.00231611
2017-10-10T14:39:38.686359: step 196, loss 0.535863, acc 0.843137, learning_rate 0.00230707
2017-10-10T14:39:38.888216: step 197, loss 0.214682, acc 0.9375, learning_rate 0.00229806
2017-10-10T14:39:39.083557: step 198, loss 0.29417, acc 0.84375, learning_rate 0.00228908
2017-10-10T14:39:39.287815: step 199, loss 0.413967, acc 0.90625, learning_rate 0.00228015
2017-10-10T14:39:39.487873: step 200, loss 0.384647, acc 0.875, learning_rate 0.00227125

Evaluation:
2017-10-10T14:39:39.934391: step 200, loss 0.291772, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-200

2017-10-10T14:39:40.887768: step 201, loss 0.4361, acc 0.890625, learning_rate 0.00226239
2017-10-10T14:39:41.093671: step 202, loss 0.283264, acc 0.875, learning_rate 0.00225356
2017-10-10T14:39:41.298433: step 203, loss 0.333354, acc 0.90625, learning_rate 0.00224477
2017-10-10T14:39:41.488296: step 204, loss 0.388038, acc 0.890625, learning_rate 0.00223602
2017-10-10T14:39:41.688051: step 205, loss 0.299846, acc 0.875, learning_rate 0.0022273
2017-10-10T14:39:41.905298: step 206, loss 0.273245, acc 0.921875, learning_rate 0.00221862
2017-10-10T14:39:42.112953: step 207, loss 0.334677, acc 0.859375, learning_rate 0.00220997
2017-10-10T14:39:42.296885: step 208, loss 0.0996859, acc 0.953125, learning_rate 0.00220136
2017-10-10T14:39:42.520963: step 209, loss 0.262931, acc 0.90625, learning_rate 0.00219278
2017-10-10T14:39:42.715241: step 210, loss 0.191331, acc 0.921875, learning_rate 0.00218424
2017-10-10T14:39:42.914399: step 211, loss 0.238265, acc 0.96875, learning_rate 0.00217573
2017-10-10T14:39:43.115468: step 212, loss 0.112076, acc 0.96875, learning_rate 0.00216726
2017-10-10T14:39:43.324193: step 213, loss 0.272497, acc 0.921875, learning_rate 0.00215882
2017-10-10T14:39:43.533948: step 214, loss 0.323099, acc 0.890625, learning_rate 0.00215041
2017-10-10T14:39:43.742565: step 215, loss 0.337719, acc 0.90625, learning_rate 0.00214204
2017-10-10T14:39:43.944880: step 216, loss 0.433712, acc 0.90625, learning_rate 0.00213371
2017-10-10T14:39:44.185766: step 217, loss 0.233352, acc 0.921875, learning_rate 0.00212541
2017-10-10T14:39:44.393122: step 218, loss 0.535936, acc 0.90625, learning_rate 0.00211714
2017-10-10T14:39:44.596223: step 219, loss 0.37356, acc 0.859375, learning_rate 0.00210891
2017-10-10T14:39:44.808539: step 220, loss 0.372616, acc 0.875, learning_rate 0.00210071
2017-10-10T14:39:45.024128: step 221, loss 0.228197, acc 0.921875, learning_rate 0.00209254
2017-10-10T14:39:45.227752: step 222, loss 0.295835, acc 0.875, learning_rate 0.00208441
2017-10-10T14:39:45.423408: step 223, loss 0.190139, acc 0.9375, learning_rate 0.00207631
2017-10-10T14:39:45.627478: step 224, loss 0.253771, acc 0.90625, learning_rate 0.00206824
2017-10-10T14:39:45.824272: step 225, loss 0.30359, acc 0.890625, learning_rate 0.00206021
2017-10-10T14:39:46.036505: step 226, loss 0.24736, acc 0.90625, learning_rate 0.00205221
2017-10-10T14:39:46.349411: step 227, loss 0.25521, acc 0.9375, learning_rate 0.00204424
2017-10-10T14:39:46.515608: step 228, loss 0.269635, acc 0.875, learning_rate 0.0020363
2017-10-10T14:39:46.646927: step 229, loss 0.26472, acc 0.9375, learning_rate 0.0020284
2017-10-10T14:39:46.778866: step 230, loss 0.308555, acc 0.859375, learning_rate 0.00202053
2017-10-10T14:39:46.913768: step 231, loss 0.439905, acc 0.875, learning_rate 0.00201269
2017-10-10T14:39:47.046734: step 232, loss 0.325987, acc 0.90625, learning_rate 0.00200488
2017-10-10T14:39:47.177603: step 233, loss 0.340851, acc 0.890625, learning_rate 0.00199711
2017-10-10T14:39:47.387137: step 234, loss 0.422184, acc 0.875, learning_rate 0.00198936
2017-10-10T14:39:47.594865: step 235, loss 0.274407, acc 0.890625, learning_rate 0.00198165
2017-10-10T14:39:47.792051: step 236, loss 0.164099, acc 0.953125, learning_rate 0.00197397
2017-10-10T14:39:47.989124: step 237, loss 0.542564, acc 0.84375, learning_rate 0.00196632
2017-10-10T14:39:48.202082: step 238, loss 0.192734, acc 0.96875, learning_rate 0.0019587
2017-10-10T14:39:48.411380: step 239, loss 0.380012, acc 0.875, learning_rate 0.00195112
2017-10-10T14:39:48.612776: step 240, loss 0.209704, acc 0.9375, learning_rate 0.00194356

Evaluation:
2017-10-10T14:39:49.046514: step 240, loss 0.266536, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-240

2017-10-10T14:39:50.003432: step 241, loss 0.293891, acc 0.90625, learning_rate 0.00193604
2017-10-10T14:39:50.203509: step 242, loss 0.315643, acc 0.90625, learning_rate 0.00192854
2017-10-10T14:39:50.405748: step 243, loss 0.299041, acc 0.921875, learning_rate 0.00192108
2017-10-10T14:39:50.616566: step 244, loss 0.227365, acc 0.890625, learning_rate 0.00191364
2017-10-10T14:39:50.824534: step 245, loss 0.315165, acc 0.90625, learning_rate 0.00190624
2017-10-10T14:39:51.037353: step 246, loss 0.387858, acc 0.875, learning_rate 0.00189887
2017-10-10T14:39:51.241159: step 247, loss 0.393188, acc 0.90625, learning_rate 0.00189153
2017-10-10T14:39:51.437341: step 248, loss 0.377399, acc 0.890625, learning_rate 0.00188421
2017-10-10T14:39:51.640904: step 249, loss 0.394175, acc 0.875, learning_rate 0.00187693
2017-10-10T14:39:51.828334: step 250, loss 0.304844, acc 0.890625, learning_rate 0.00186968
2017-10-10T14:39:52.026470: step 251, loss 0.473346, acc 0.859375, learning_rate 0.00186245
2017-10-10T14:39:52.214896: step 252, loss 0.12417, acc 0.9375, learning_rate 0.00185526
2017-10-10T14:39:52.399542: step 253, loss 0.24809, acc 0.90625, learning_rate 0.0018481
2017-10-10T14:39:52.599079: step 254, loss 0.328328, acc 0.875, learning_rate 0.00184096
2017-10-10T14:39:52.796885: step 255, loss 0.172808, acc 0.9375, learning_rate 0.00183385
2017-10-10T14:39:52.979461: step 256, loss 0.370846, acc 0.859375, learning_rate 0.00182678
2017-10-10T14:39:53.177872: step 257, loss 0.204517, acc 0.953125, learning_rate 0.00181973
2017-10-10T14:39:53.376882: step 258, loss 0.273579, acc 0.890625, learning_rate 0.00181271
2017-10-10T14:39:53.560059: step 259, loss 0.298438, acc 0.921875, learning_rate 0.00180572
2017-10-10T14:39:53.752140: step 260, loss 0.414035, acc 0.875, learning_rate 0.00179876
2017-10-10T14:39:53.956166: step 261, loss 0.444644, acc 0.890625, learning_rate 0.00179182
2017-10-10T14:39:54.162930: step 262, loss 0.489609, acc 0.84375, learning_rate 0.00178492
2017-10-10T14:39:54.383503: step 263, loss 0.534697, acc 0.796875, learning_rate 0.00177804
2017-10-10T14:39:54.587737: step 264, loss 0.375175, acc 0.921875, learning_rate 0.00177119
2017-10-10T14:39:54.852847: step 265, loss 0.239214, acc 0.953125, learning_rate 0.00176437
2017-10-10T14:39:55.048648: step 266, loss 0.162158, acc 0.953125, learning_rate 0.00175758
2017-10-10T14:39:55.184011: step 267, loss 0.232614, acc 0.9375, learning_rate 0.00175081
2017-10-10T14:39:55.318385: step 268, loss 0.28062, acc 0.953125, learning_rate 0.00174407
2017-10-10T14:39:55.451819: step 269, loss 0.668707, acc 0.796875, learning_rate 0.00173736
2017-10-10T14:39:55.585322: step 270, loss 0.21089, acc 0.921875, learning_rate 0.00173068
2017-10-10T14:39:55.715682: step 271, loss 0.412045, acc 0.921875, learning_rate 0.00172402
2017-10-10T14:39:55.899292: step 272, loss 0.337282, acc 0.890625, learning_rate 0.00171739
2017-10-10T14:39:56.097541: step 273, loss 0.129532, acc 0.96875, learning_rate 0.00171079
2017-10-10T14:39:56.312311: step 274, loss 0.260346, acc 0.921875, learning_rate 0.00170422
2017-10-10T14:39:56.509870: step 275, loss 0.325153, acc 0.921875, learning_rate 0.00169767
2017-10-10T14:39:56.713041: step 276, loss 0.154732, acc 0.90625, learning_rate 0.00169115
2017-10-10T14:39:56.925220: step 277, loss 0.464122, acc 0.875, learning_rate 0.00168465
2017-10-10T14:39:57.138967: step 278, loss 0.262199, acc 0.90625, learning_rate 0.00167818
2017-10-10T14:39:57.324907: step 279, loss 0.26488, acc 0.890625, learning_rate 0.00167174
2017-10-10T14:39:57.516850: step 280, loss 0.135971, acc 0.96875, learning_rate 0.00166533

Evaluation:
2017-10-10T14:39:57.952910: step 280, loss 0.260637, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-280

2017-10-10T14:39:58.933880: step 281, loss 0.299749, acc 0.875, learning_rate 0.00165894
2017-10-10T14:39:59.139145: step 282, loss 0.686459, acc 0.796875, learning_rate 0.00165257
2017-10-10T14:39:59.342071: step 283, loss 0.194795, acc 0.953125, learning_rate 0.00164624
2017-10-10T14:39:59.555132: step 284, loss 0.29299, acc 0.890625, learning_rate 0.00163993
2017-10-10T14:39:59.758906: step 285, loss 0.206202, acc 0.921875, learning_rate 0.00163364
2017-10-10T14:39:59.965370: step 286, loss 0.21817, acc 0.921875, learning_rate 0.00162738
2017-10-10T14:40:00.158439: step 287, loss 0.288556, acc 0.875, learning_rate 0.00162115
2017-10-10T14:40:00.379212: step 288, loss 0.305815, acc 0.9375, learning_rate 0.00161494
2017-10-10T14:40:00.576119: step 289, loss 0.526518, acc 0.796875, learning_rate 0.00160875
2017-10-10T14:40:00.756799: step 290, loss 0.674597, acc 0.765625, learning_rate 0.00160259
2017-10-10T14:40:00.942541: step 291, loss 0.27286, acc 0.921875, learning_rate 0.00159646
2017-10-10T14:40:01.126564: step 292, loss 0.109999, acc 0.9375, learning_rate 0.00159035
2017-10-10T14:40:01.324916: step 293, loss 0.326489, acc 0.921875, learning_rate 0.00158427
2017-10-10T14:40:01.456601: step 294, loss 0.373696, acc 0.882353, learning_rate 0.00157821
2017-10-10T14:40:01.656875: step 295, loss 0.134911, acc 0.953125, learning_rate 0.00157218
2017-10-10T14:40:01.868527: step 296, loss 0.389705, acc 0.875, learning_rate 0.00156617
2017-10-10T14:40:02.075053: step 297, loss 0.248917, acc 0.9375, learning_rate 0.00156018
2017-10-10T14:40:02.267071: step 298, loss 0.167092, acc 0.9375, learning_rate 0.00155422
2017-10-10T14:40:02.468023: step 299, loss 0.287873, acc 0.90625, learning_rate 0.00154829
2017-10-10T14:40:02.667612: step 300, loss 0.125061, acc 0.953125, learning_rate 0.00154238
2017-10-10T14:40:02.880960: step 301, loss 0.289536, acc 0.921875, learning_rate 0.00153649
2017-10-10T14:40:03.081796: step 302, loss 0.262854, acc 0.90625, learning_rate 0.00153063
2017-10-10T14:40:03.401380: step 303, loss 0.151072, acc 0.9375, learning_rate 0.00152479
2017-10-10T14:40:03.561484: step 304, loss 0.237078, acc 0.90625, learning_rate 0.00151897
2017-10-10T14:40:03.696515: step 305, loss 0.055003, acc 1, learning_rate 0.00151318
2017-10-10T14:40:03.827967: step 306, loss 0.575173, acc 0.8125, learning_rate 0.00150741
2017-10-10T14:40:03.961245: step 307, loss 0.16858, acc 0.921875, learning_rate 0.00150167
2017-10-10T14:40:04.094753: step 308, loss 0.175579, acc 0.9375, learning_rate 0.00149594
2017-10-10T14:40:04.306755: step 309, loss 0.120662, acc 0.96875, learning_rate 0.00149025
2017-10-10T14:40:04.515981: step 310, loss 0.121097, acc 0.9375, learning_rate 0.00148457
2017-10-10T14:40:04.714899: step 311, loss 0.122958, acc 0.96875, learning_rate 0.00147892
2017-10-10T14:40:04.931629: step 312, loss 0.1363, acc 0.96875, learning_rate 0.00147329
2017-10-10T14:40:05.138050: step 313, loss 0.211925, acc 0.90625, learning_rate 0.00146769
2017-10-10T14:40:05.340889: step 314, loss 0.37919, acc 0.90625, learning_rate 0.0014621
2017-10-10T14:40:05.528864: step 315, loss 0.251179, acc 0.859375, learning_rate 0.00145654
2017-10-10T14:40:05.721975: step 316, loss 0.338849, acc 0.921875, learning_rate 0.00145101
2017-10-10T14:40:05.916005: step 317, loss 0.338734, acc 0.859375, learning_rate 0.00144549
2017-10-10T14:40:06.115507: step 318, loss 0.370318, acc 0.875, learning_rate 0.00144
2017-10-10T14:40:06.309365: step 319, loss 0.406472, acc 0.875, learning_rate 0.00143453
2017-10-10T14:40:06.497182: step 320, loss 0.152692, acc 0.953125, learning_rate 0.00142908

Evaluation:
2017-10-10T14:40:06.938121: step 320, loss 0.256367, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-320

2017-10-10T14:40:07.712449: step 321, loss 0.214453, acc 0.9375, learning_rate 0.00142366
2017-10-10T14:40:07.924670: step 322, loss 0.130373, acc 0.953125, learning_rate 0.00141826
2017-10-10T14:40:08.126276: step 323, loss 0.249428, acc 0.90625, learning_rate 0.00141288
2017-10-10T14:40:08.337255: step 324, loss 0.255886, acc 0.890625, learning_rate 0.00140752
2017-10-10T14:40:08.537215: step 325, loss 0.390333, acc 0.828125, learning_rate 0.00140218
2017-10-10T14:40:08.740856: step 326, loss 0.208313, acc 0.9375, learning_rate 0.00139686
2017-10-10T14:40:08.934176: step 327, loss 0.244166, acc 0.9375, learning_rate 0.00139157
2017-10-10T14:40:09.117253: step 328, loss 0.282806, acc 0.90625, learning_rate 0.0013863
2017-10-10T14:40:09.308043: step 329, loss 0.109539, acc 0.953125, learning_rate 0.00138105
2017-10-10T14:40:09.501223: step 330, loss 0.238537, acc 0.9375, learning_rate 0.00137582
2017-10-10T14:40:09.702334: step 331, loss 0.209742, acc 0.921875, learning_rate 0.00137061
2017-10-10T14:40:09.903054: step 332, loss 0.234295, acc 0.890625, learning_rate 0.00136543
2017-10-10T14:40:10.096405: step 333, loss 0.271773, acc 0.90625, learning_rate 0.00136026
2017-10-10T14:40:10.279555: step 334, loss 0.202266, acc 0.90625, learning_rate 0.00135512
2017-10-10T14:40:10.457048: step 335, loss 0.117779, acc 0.953125, learning_rate 0.00134999
2017-10-10T14:40:10.684825: step 336, loss 0.350516, acc 0.875, learning_rate 0.00134489
2017-10-10T14:40:10.895885: step 337, loss 0.446031, acc 0.84375, learning_rate 0.00133981
2017-10-10T14:40:11.103659: step 338, loss 0.459839, acc 0.859375, learning_rate 0.00133475
2017-10-10T14:40:11.312392: step 339, loss 0.330845, acc 0.890625, learning_rate 0.00132971
2017-10-10T14:40:11.514333: step 340, loss 0.343095, acc 0.953125, learning_rate 0.00132469
2017-10-10T14:40:11.793057: step 341, loss 0.209663, acc 0.921875, learning_rate 0.00131969
2017-10-10T14:40:11.981963: step 342, loss 0.139355, acc 0.921875, learning_rate 0.00131471
2017-10-10T14:40:12.113442: step 343, loss 0.0842744, acc 0.96875, learning_rate 0.00130975
2017-10-10T14:40:12.245508: step 344, loss 0.226649, acc 0.921875, learning_rate 0.00130482
2017-10-10T14:40:12.389081: step 345, loss 0.253857, acc 0.90625, learning_rate 0.0012999
2017-10-10T14:40:12.527134: step 346, loss 0.341321, acc 0.890625, learning_rate 0.001295
2017-10-10T14:40:12.671271: step 347, loss 0.346433, acc 0.890625, learning_rate 0.00129012
2017-10-10T14:40:12.841658: step 348, loss 0.243686, acc 0.90625, learning_rate 0.00128527
2017-10-10T14:40:13.032602: step 349, loss 0.327065, acc 0.890625, learning_rate 0.00128043
2017-10-10T14:40:13.242657: step 350, loss 0.157399, acc 0.96875, learning_rate 0.00127561
2017-10-10T14:40:13.443202: step 351, loss 0.173365, acc 0.953125, learning_rate 0.00127081
2017-10-10T14:40:13.654617: step 352, loss 0.100346, acc 0.96875, learning_rate 0.00126603
2017-10-10T14:40:13.860297: step 353, loss 0.435789, acc 0.875, learning_rate 0.00126127
2017-10-10T14:40:14.055749: step 354, loss 0.182931, acc 0.9375, learning_rate 0.00125653
2017-10-10T14:40:14.256060: step 355, loss 0.141413, acc 0.9375, learning_rate 0.00125181
2017-10-10T14:40:14.460684: step 356, loss 0.190262, acc 0.953125, learning_rate 0.00124711
2017-10-10T14:40:14.657469: step 357, loss 0.131296, acc 0.96875, learning_rate 0.00124243
2017-10-10T14:40:14.856688: step 358, loss 0.19662, acc 0.921875, learning_rate 0.00123777
2017-10-10T14:40:15.060373: step 359, loss 0.243419, acc 0.90625, learning_rate 0.00123312
2017-10-10T14:40:15.264998: step 360, loss 0.330487, acc 0.90625, learning_rate 0.0012285

Evaluation:
2017-10-10T14:40:15.716596: step 360, loss 0.250579, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-360

2017-10-10T14:40:16.564866: step 361, loss 0.170208, acc 0.9375, learning_rate 0.00122389
2017-10-10T14:40:16.779038: step 362, loss 0.20764, acc 0.90625, learning_rate 0.0012193
2017-10-10T14:40:17.013094: step 363, loss 0.141032, acc 0.953125, learning_rate 0.00121473
2017-10-10T14:40:17.220368: step 364, loss 0.265338, acc 0.9375, learning_rate 0.00121018
2017-10-10T14:40:17.421813: step 365, loss 0.361972, acc 0.859375, learning_rate 0.00120565
2017-10-10T14:40:17.634055: step 366, loss 0.218752, acc 0.921875, learning_rate 0.00120114
2017-10-10T14:40:17.832959: step 367, loss 0.429014, acc 0.875, learning_rate 0.00119664
2017-10-10T14:40:18.043132: step 368, loss 0.151131, acc 0.984375, learning_rate 0.00119217
2017-10-10T14:40:18.256091: step 369, loss 0.172707, acc 0.9375, learning_rate 0.00118771
2017-10-10T14:40:18.451105: step 370, loss 0.315563, acc 0.90625, learning_rate 0.00118327
2017-10-10T14:40:18.653276: step 371, loss 0.0975914, acc 0.96875, learning_rate 0.00117885
2017-10-10T14:40:18.881342: step 372, loss 0.261319, acc 0.90625, learning_rate 0.00117445
2017-10-10T14:40:19.093368: step 373, loss 0.212267, acc 0.921875, learning_rate 0.00117006
2017-10-10T14:40:19.290905: step 374, loss 0.353066, acc 0.90625, learning_rate 0.00116569
2017-10-10T14:40:19.495194: step 375, loss 0.27911, acc 0.90625, learning_rate 0.00116134
2017-10-10T14:40:19.700870: step 376, loss 0.142875, acc 0.953125, learning_rate 0.00115701
2017-10-10T14:40:19.892874: step 377, loss 0.1734, acc 0.953125, learning_rate 0.0011527
2017-10-10T14:40:20.079521: step 378, loss 0.147486, acc 0.96875, learning_rate 0.0011484
2017-10-10T14:40:20.365273: step 379, loss 0.128136, acc 0.9375, learning_rate 0.00114412
2017-10-10T14:40:20.516415: step 380, loss 0.190737, acc 0.953125, learning_rate 0.00113986
2017-10-10T14:40:20.650970: step 381, loss 0.388482, acc 0.859375, learning_rate 0.00113561
2017-10-10T14:40:20.785366: step 382, loss 0.252164, acc 0.921875, learning_rate 0.00113139
2017-10-10T14:40:20.918080: step 383, loss 0.218459, acc 0.90625, learning_rate 0.00112718
2017-10-10T14:40:21.051466: step 384, loss 0.071944, acc 0.984375, learning_rate 0.00112298
2017-10-10T14:40:21.185918: step 385, loss 0.188635, acc 0.953125, learning_rate 0.00111881
2017-10-10T14:40:21.360979: step 386, loss 0.0959416, acc 0.96875, learning_rate 0.00111465
2017-10-10T14:40:21.547879: step 387, loss 0.160497, acc 0.953125, learning_rate 0.00111051
2017-10-10T14:40:21.763344: step 388, loss 0.136034, acc 0.96875, learning_rate 0.00110638
2017-10-10T14:40:21.970874: step 389, loss 0.258773, acc 0.84375, learning_rate 0.00110228
2017-10-10T14:40:22.181545: step 390, loss 0.124184, acc 0.953125, learning_rate 0.00109818
2017-10-10T14:40:22.380838: step 391, loss 0.240226, acc 0.90625, learning_rate 0.00109411
2017-10-10T14:40:22.558113: step 392, loss 0.174996, acc 0.921569, learning_rate 0.00109005
2017-10-10T14:40:22.760480: step 393, loss 0.168046, acc 0.890625, learning_rate 0.00108601
2017-10-10T14:40:22.968769: step 394, loss 0.128321, acc 0.984375, learning_rate 0.00108199
2017-10-10T14:40:23.163940: step 395, loss 0.246677, acc 0.875, learning_rate 0.00107798
2017-10-10T14:40:23.342948: step 396, loss 0.206435, acc 0.9375, learning_rate 0.00107399
2017-10-10T14:40:23.548669: step 397, loss 0.206538, acc 0.921875, learning_rate 0.00107001
2017-10-10T14:40:23.736898: step 398, loss 0.193462, acc 0.90625, learning_rate 0.00106605
2017-10-10T14:40:23.949910: step 399, loss 0.179596, acc 0.9375, learning_rate 0.00106211
2017-10-10T14:40:24.113902: step 400, loss 0.283311, acc 0.875, learning_rate 0.00105818

Evaluation:
2017-10-10T14:40:24.561324: step 400, loss 0.247111, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-400

2017-10-10T14:40:25.522074: step 401, loss 0.121722, acc 0.953125, learning_rate 0.00105427
2017-10-10T14:40:25.734064: step 402, loss 0.196353, acc 0.921875, learning_rate 0.00105037
2017-10-10T14:40:25.950743: step 403, loss 0.19507, acc 0.9375, learning_rate 0.0010465
2017-10-10T14:40:26.142875: step 404, loss 0.31508, acc 0.875, learning_rate 0.00104263
2017-10-10T14:40:26.338901: step 405, loss 0.412324, acc 0.890625, learning_rate 0.00103878
2017-10-10T14:40:26.540147: step 406, loss 0.185171, acc 0.9375, learning_rate 0.00103495
2017-10-10T14:40:26.735500: step 407, loss 0.292, acc 0.875, learning_rate 0.00103114
2017-10-10T14:40:26.942476: step 408, loss 0.106667, acc 0.96875, learning_rate 0.00102734
2017-10-10T14:40:27.148300: step 409, loss 0.220436, acc 0.921875, learning_rate 0.00102355
2017-10-10T14:40:27.358917: step 410, loss 0.292729, acc 0.90625, learning_rate 0.00101978
2017-10-10T14:40:27.563448: step 411, loss 0.204466, acc 0.90625, learning_rate 0.00101603
2017-10-10T14:40:27.788857: step 412, loss 0.165421, acc 0.9375, learning_rate 0.00101229
2017-10-10T14:40:28.016836: step 413, loss 0.21758, acc 0.9375, learning_rate 0.00100856
2017-10-10T14:40:28.219810: step 414, loss 0.229019, acc 0.9375, learning_rate 0.00100486
2017-10-10T14:40:28.412893: step 415, loss 0.0673188, acc 0.984375, learning_rate 0.00100116
2017-10-10T14:40:28.611555: step 416, loss 0.146305, acc 0.953125, learning_rate 0.000997483
2017-10-10T14:40:28.804793: step 417, loss 0.108792, acc 0.96875, learning_rate 0.00099382
2017-10-10T14:40:29.131191: step 418, loss 0.403027, acc 0.890625, learning_rate 0.000990172
2017-10-10T14:40:29.263510: step 419, loss 0.0732059, acc 0.984375, learning_rate 0.000986538
2017-10-10T14:40:29.405350: step 420, loss 0.26542, acc 0.875, learning_rate 0.00098292
2017-10-10T14:40:29.541615: step 421, loss 0.0870535, acc 0.96875, learning_rate 0.000979316
2017-10-10T14:40:29.674203: step 422, loss 0.142592, acc 0.953125, learning_rate 0.000975727
2017-10-10T14:40:29.807894: step 423, loss 0.248738, acc 0.921875, learning_rate 0.000972152
2017-10-10T14:40:30.010797: step 424, loss 0.0816429, acc 0.96875, learning_rate 0.000968592
2017-10-10T14:40:30.206505: step 425, loss 0.160482, acc 0.953125, learning_rate 0.000965047
2017-10-10T14:40:30.409387: step 426, loss 0.170267, acc 0.953125, learning_rate 0.000961516
2017-10-10T14:40:30.617878: step 427, loss 0.0892783, acc 0.984375, learning_rate 0.000958
2017-10-10T14:40:30.801251: step 428, loss 0.196652, acc 0.90625, learning_rate 0.000954497
2017-10-10T14:40:30.996849: step 429, loss 0.270688, acc 0.890625, learning_rate 0.00095101
2017-10-10T14:40:31.185173: step 430, loss 0.275904, acc 0.90625, learning_rate 0.000947536
2017-10-10T14:40:31.384816: step 431, loss 0.213773, acc 0.953125, learning_rate 0.000944076
2017-10-10T14:40:31.590696: step 432, loss 0.362853, acc 0.859375, learning_rate 0.000940631
2017-10-10T14:40:31.788805: step 433, loss 0.219183, acc 0.921875, learning_rate 0.0009372
2017-10-10T14:40:31.992574: step 434, loss 0.167828, acc 0.921875, learning_rate 0.000933783
2017-10-10T14:40:32.197035: step 435, loss 0.128256, acc 0.96875, learning_rate 0.000930379
2017-10-10T14:40:32.393002: step 436, loss 0.163806, acc 0.9375, learning_rate 0.00092699
2017-10-10T14:40:32.576189: step 437, loss 0.0808346, acc 0.96875, learning_rate 0.000923614
2017-10-10T14:40:32.780099: step 438, loss 0.148661, acc 0.953125, learning_rate 0.000920253
2017-10-10T14:40:32.968466: step 439, loss 0.166254, acc 0.953125, learning_rate 0.000916905
2017-10-10T14:40:33.151644: step 440, loss 0.379114, acc 0.890625, learning_rate 0.00091357

Evaluation:
2017-10-10T14:40:33.611243: step 440, loss 0.250593, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-440

2017-10-10T14:40:34.566496: step 441, loss 0.141434, acc 0.9375, learning_rate 0.000910249
2017-10-10T14:40:34.764127: step 442, loss 0.218566, acc 0.9375, learning_rate 0.000906942
2017-10-10T14:40:34.985802: step 443, loss 0.191978, acc 0.9375, learning_rate 0.000903648
2017-10-10T14:40:35.189348: step 444, loss 0.180699, acc 0.953125, learning_rate 0.000900368
2017-10-10T14:40:35.383673: step 445, loss 0.265289, acc 0.90625, learning_rate 0.000897101
2017-10-10T14:40:35.612878: step 446, loss 0.171731, acc 0.921875, learning_rate 0.000893848
2017-10-10T14:40:35.795755: step 447, loss 0.095384, acc 0.96875, learning_rate 0.000890607
2017-10-10T14:40:36.013157: step 448, loss 0.283035, acc 0.890625, learning_rate 0.00088738
2017-10-10T14:40:36.215872: step 449, loss 0.282575, acc 0.9375, learning_rate 0.000884166
2017-10-10T14:40:36.430863: step 450, loss 0.138206, acc 0.96875, learning_rate 0.000880966
2017-10-10T14:40:36.628843: step 451, loss 0.136931, acc 0.96875, learning_rate 0.000877778
2017-10-10T14:40:36.818625: step 452, loss 0.162088, acc 0.9375, learning_rate 0.000874603
2017-10-10T14:40:37.010304: step 453, loss 0.392984, acc 0.890625, learning_rate 0.000871441
2017-10-10T14:40:37.199042: step 454, loss 0.181299, acc 0.90625, learning_rate 0.000868293
2017-10-10T14:40:37.486475: step 455, loss 0.330124, acc 0.890625, learning_rate 0.000865157
2017-10-10T14:40:37.667448: step 456, loss 0.179515, acc 0.9375, learning_rate 0.000862033
2017-10-10T14:40:37.806592: step 457, loss 0.190553, acc 0.953125, learning_rate 0.000858923
2017-10-10T14:40:37.944006: step 458, loss 0.307253, acc 0.84375, learning_rate 0.000855825
2017-10-10T14:40:38.077019: step 459, loss 0.113765, acc 0.9375, learning_rate 0.00085274
2017-10-10T14:40:38.210772: step 460, loss 0.115676, acc 0.953125, learning_rate 0.000849668
2017-10-10T14:40:38.349694: step 461, loss 0.21188, acc 0.921875, learning_rate 0.000846608
2017-10-10T14:40:38.562193: step 462, loss 0.17248, acc 0.96875, learning_rate 0.00084356
2017-10-10T14:40:38.758635: step 463, loss 0.200605, acc 0.9375, learning_rate 0.000840525
2017-10-10T14:40:38.963949: step 464, loss 0.140348, acc 0.9375, learning_rate 0.000837502
2017-10-10T14:40:39.173577: step 465, loss 0.268463, acc 0.90625, learning_rate 0.000834492
2017-10-10T14:40:39.379384: step 466, loss 0.232764, acc 0.953125, learning_rate 0.000831494
2017-10-10T14:40:39.606325: step 467, loss 0.200724, acc 0.90625, learning_rate 0.000828508
2017-10-10T14:40:39.832986: step 468, loss 0.191968, acc 0.953125, learning_rate 0.000825535
2017-10-10T14:40:40.066262: step 469, loss 0.158984, acc 0.9375, learning_rate 0.000822573
2017-10-10T14:40:40.300173: step 470, loss 0.121421, acc 0.96875, learning_rate 0.000819624
2017-10-10T14:40:40.515709: step 471, loss 0.309009, acc 0.921875, learning_rate 0.000816687
2017-10-10T14:40:40.764750: step 472, loss 0.149069, acc 0.96875, learning_rate 0.000813761
2017-10-10T14:40:41.024980: step 473, loss 0.175005, acc 0.9375, learning_rate 0.000810848
2017-10-10T14:40:41.244906: step 474, loss 0.165852, acc 0.96875, learning_rate 0.000807946
2017-10-10T14:40:41.483475: step 475, loss 0.370331, acc 0.890625, learning_rate 0.000805057
2017-10-10T14:40:41.701303: step 476, loss 0.127035, acc 0.96875, learning_rate 0.000802179
2017-10-10T14:40:41.920833: step 477, loss 0.129103, acc 0.953125, learning_rate 0.000799313
2017-10-10T14:40:42.156962: step 478, loss 0.252603, acc 0.921875, learning_rate 0.000796458
2017-10-10T14:40:42.369838: step 479, loss 0.251542, acc 0.921875, learning_rate 0.000793616
2017-10-10T14:40:42.616815: step 480, loss 0.305054, acc 0.9375, learning_rate 0.000790784

Evaluation:
2017-10-10T14:40:43.097991: step 480, loss 0.241906, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-480

2017-10-10T14:40:44.060680: step 481, loss 0.247587, acc 0.921875, learning_rate 0.000787965
2017-10-10T14:40:44.287647: step 482, loss 0.183408, acc 0.953125, learning_rate 0.000785157
2017-10-10T14:40:44.510454: step 483, loss 0.144349, acc 0.9375, learning_rate 0.00078236
2017-10-10T14:40:44.726193: step 484, loss 0.146525, acc 0.953125, learning_rate 0.000779575
2017-10-10T14:40:44.946616: step 485, loss 0.0988271, acc 0.96875, learning_rate 0.000776801
2017-10-10T14:40:45.155957: step 486, loss 0.182566, acc 0.921875, learning_rate 0.000774038
2017-10-10T14:40:45.367294: step 487, loss 0.149965, acc 0.921875, learning_rate 0.000771287
2017-10-10T14:40:45.585438: step 488, loss 0.245439, acc 0.9375, learning_rate 0.000768547
2017-10-10T14:40:45.815405: step 489, loss 0.375427, acc 0.875, learning_rate 0.000765818
2017-10-10T14:40:46.006398: step 490, loss 0.158241, acc 0.980392, learning_rate 0.000763101
2017-10-10T14:40:46.231532: step 491, loss 0.129088, acc 0.9375, learning_rate 0.000760394
2017-10-10T14:40:46.455217: step 492, loss 0.156453, acc 0.9375, learning_rate 0.000757698
2017-10-10T14:40:46.684832: step 493, loss 0.163767, acc 0.9375, learning_rate 0.000755014
2017-10-10T14:40:46.957042: step 494, loss 0.205202, acc 0.96875, learning_rate 0.00075234
2017-10-10T14:40:47.100684: step 495, loss 0.12691, acc 0.9375, learning_rate 0.000749677
2017-10-10T14:40:47.245612: step 496, loss 0.162886, acc 0.96875, learning_rate 0.000747026
2017-10-10T14:40:47.389505: step 497, loss 0.162799, acc 0.96875, learning_rate 0.000744385
2017-10-10T14:40:47.532241: step 498, loss 0.172276, acc 0.953125, learning_rate 0.000741754
2017-10-10T14:40:47.677988: step 499, loss 0.0998436, acc 0.953125, learning_rate 0.000739135
2017-10-10T14:40:47.909483: step 500, loss 0.145059, acc 0.953125, learning_rate 0.000736526
2017-10-10T14:40:48.125995: step 501, loss 0.0848526, acc 0.984375, learning_rate 0.000733928
2017-10-10T14:40:48.357489: step 502, loss 0.187306, acc 0.953125, learning_rate 0.00073134
2017-10-10T14:40:48.573030: step 503, loss 0.232655, acc 0.921875, learning_rate 0.000728763
2017-10-10T14:40:48.791049: step 504, loss 0.195083, acc 0.9375, learning_rate 0.000726197
2017-10-10T14:40:49.016222: step 505, loss 0.134417, acc 0.953125, learning_rate 0.000723641
2017-10-10T14:40:49.234996: step 506, loss 0.152051, acc 0.90625, learning_rate 0.000721095
2017-10-10T14:40:49.435884: step 507, loss 0.264861, acc 0.90625, learning_rate 0.00071856
2017-10-10T14:40:49.651395: step 508, loss 0.177721, acc 0.953125, learning_rate 0.000716036
2017-10-10T14:40:49.879175: step 509, loss 0.285208, acc 0.953125, learning_rate 0.000713521
2017-10-10T14:40:50.111353: step 510, loss 0.0940338, acc 0.984375, learning_rate 0.000711017
2017-10-10T14:40:50.320971: step 511, loss 0.189416, acc 0.921875, learning_rate 0.000708523
2017-10-10T14:40:50.552785: step 512, loss 0.146395, acc 0.9375, learning_rate 0.000706039
2017-10-10T14:40:50.795515: step 513, loss 0.441932, acc 0.90625, learning_rate 0.000703565
2017-10-10T14:40:51.000996: step 514, loss 0.0740159, acc 0.984375, learning_rate 0.000701102
2017-10-10T14:40:51.209785: step 515, loss 0.226713, acc 0.921875, learning_rate 0.000698648
2017-10-10T14:40:51.402391: step 516, loss 0.269817, acc 0.890625, learning_rate 0.000696204
2017-10-10T14:40:51.600941: step 517, loss 0.241684, acc 0.890625, learning_rate 0.000693771
2017-10-10T14:40:51.850074: step 518, loss 0.201035, acc 0.90625, learning_rate 0.000691347
2017-10-10T14:40:52.078191: step 519, loss 0.223844, acc 0.890625, learning_rate 0.000688934
2017-10-10T14:40:52.315628: step 520, loss 0.23434, acc 0.921875, learning_rate 0.00068653

Evaluation:
2017-10-10T14:40:52.799858: step 520, loss 0.241375, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-520

2017-10-10T14:40:53.744745: step 521, loss 0.119878, acc 0.96875, learning_rate 0.000684136
2017-10-10T14:40:53.961328: step 522, loss 0.150312, acc 0.96875, learning_rate 0.000681751
2017-10-10T14:40:54.173005: step 523, loss 0.239654, acc 0.90625, learning_rate 0.000679377
2017-10-10T14:40:54.377122: step 524, loss 0.150851, acc 0.9375, learning_rate 0.000677012
2017-10-10T14:40:54.633880: step 525, loss 0.117581, acc 0.96875, learning_rate 0.000674657
2017-10-10T14:40:54.873080: step 526, loss 0.268688, acc 0.9375, learning_rate 0.000672311
2017-10-10T14:40:55.099940: step 527, loss 0.139698, acc 0.9375, learning_rate 0.000669975
2017-10-10T14:40:55.327614: step 528, loss 0.236647, acc 0.9375, learning_rate 0.000667648
2017-10-10T14:40:55.554251: step 529, loss 0.197115, acc 0.921875, learning_rate 0.000665331
2017-10-10T14:40:55.869069: step 530, loss 0.100122, acc 0.984375, learning_rate 0.000663024
2017-10-10T14:40:56.059306: step 531, loss 0.213981, acc 0.921875, learning_rate 0.000660726
2017-10-10T14:40:56.210685: step 532, loss 0.322296, acc 0.921875, learning_rate 0.000658437
2017-10-10T14:40:56.362228: step 533, loss 0.17133, acc 0.953125, learning_rate 0.000656158
2017-10-10T14:40:56.520675: step 534, loss 0.257347, acc 0.890625, learning_rate 0.000653888
2017-10-10T14:40:56.681565: step 535, loss 0.247647, acc 0.9375, learning_rate 0.000651627
2017-10-10T14:40:56.925609: step 536, loss 0.146053, acc 0.953125, learning_rate 0.000649375
2017-10-10T14:40:57.148889: step 537, loss 0.174511, acc 0.953125, learning_rate 0.000647133
2017-10-10T14:40:57.345690: step 538, loss 0.270155, acc 0.921875, learning_rate 0.000644899
2017-10-10T14:40:57.608909: step 539, loss 0.210498, acc 0.90625, learning_rate 0.000642675
2017-10-10T14:40:57.832140: step 540, loss 0.118984, acc 0.984375, learning_rate 0.00064046
2017-10-10T14:40:58.023654: step 541, loss 0.253864, acc 0.875, learning_rate 0.000638254
2017-10-10T14:40:58.228158: step 542, loss 0.189997, acc 0.953125, learning_rate 0.000636057
2017-10-10T14:40:58.446325: step 543, loss 0.054207, acc 0.984375, learning_rate 0.000633869
2017-10-10T14:40:58.668809: step 544, loss 0.265169, acc 0.90625, learning_rate 0.00063169
2017-10-10T14:40:58.864900: step 545, loss 0.268004, acc 0.921875, learning_rate 0.00062952
2017-10-10T14:40:59.101121: step 546, loss 0.207003, acc 0.921875, learning_rate 0.000627358
2017-10-10T14:40:59.328843: step 547, loss 0.186507, acc 0.9375, learning_rate 0.000625206
2017-10-10T14:40:59.553725: step 548, loss 0.164715, acc 0.9375, learning_rate 0.000623062
2017-10-10T14:40:59.768004: step 549, loss 0.179513, acc 0.921875, learning_rate 0.000620927
2017-10-10T14:40:59.996860: step 550, loss 0.116627, acc 0.96875, learning_rate 0.000618801
2017-10-10T14:41:00.234263: step 551, loss 0.142398, acc 0.90625, learning_rate 0.000616683
2017-10-10T14:41:00.484871: step 552, loss 0.180714, acc 0.9375, learning_rate 0.000614574
2017-10-10T14:41:00.701408: step 553, loss 0.113297, acc 0.953125, learning_rate 0.000612474
2017-10-10T14:41:00.912841: step 554, loss 0.412686, acc 0.90625, learning_rate 0.000610382
2017-10-10T14:41:01.129001: step 555, loss 0.0698544, acc 0.984375, learning_rate 0.000608299
2017-10-10T14:41:01.351186: step 556, loss 0.170779, acc 0.9375, learning_rate 0.000606224
2017-10-10T14:41:01.577797: step 557, loss 0.21961, acc 0.953125, learning_rate 0.000604158
2017-10-10T14:41:01.810166: step 558, loss 0.204606, acc 0.9375, learning_rate 0.0006021
2017-10-10T14:41:02.017490: step 559, loss 0.0942463, acc 0.96875, learning_rate 0.00060005
2017-10-10T14:41:02.230183: step 560, loss 0.122272, acc 0.953125, learning_rate 0.000598009

Evaluation:
2017-10-10T14:41:02.709809: step 560, loss 0.245001, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-560

2017-10-10T14:41:03.698470: step 561, loss 0.262098, acc 0.9375, learning_rate 0.000595977
2017-10-10T14:41:03.938926: step 562, loss 0.248066, acc 0.90625, learning_rate 0.000593952
2017-10-10T14:41:04.159397: step 563, loss 0.114179, acc 0.96875, learning_rate 0.000591936
2017-10-10T14:41:04.389637: step 564, loss 0.121958, acc 0.953125, learning_rate 0.000589928
2017-10-10T14:41:04.611523: step 565, loss 0.0824416, acc 0.96875, learning_rate 0.000587928
2017-10-10T14:41:04.927685: step 566, loss 0.237443, acc 0.890625, learning_rate 0.000585937
2017-10-10T14:41:05.109363: step 567, loss 0.248532, acc 0.90625, learning_rate 0.000583953
2017-10-10T14:41:05.263248: step 568, loss 0.272219, acc 0.890625, learning_rate 0.000581978
2017-10-10T14:41:05.408387: step 569, loss 0.23067, acc 0.921875, learning_rate 0.00058001
2017-10-10T14:41:05.558604: step 570, loss 0.191138, acc 0.921875, learning_rate 0.000578051
2017-10-10T14:41:05.698814: step 571, loss 0.0434904, acc 1, learning_rate 0.0005761
2017-10-10T14:41:05.888252: step 572, loss 0.128761, acc 0.953125, learning_rate 0.000574157
2017-10-10T14:41:06.113972: step 573, loss 0.192755, acc 0.953125, learning_rate 0.000572221
2017-10-10T14:41:06.334484: step 574, loss 0.0286217, acc 1, learning_rate 0.000570294
2017-10-10T14:41:06.562912: step 575, loss 0.16345, acc 0.953125, learning_rate 0.000568374
2017-10-10T14:41:06.776077: step 576, loss 0.180277, acc 0.953125, learning_rate 0.000566462
2017-10-10T14:41:06.995166: step 577, loss 0.184436, acc 0.953125, learning_rate 0.000564558
2017-10-10T14:41:07.203163: step 578, loss 0.219472, acc 0.953125, learning_rate 0.000562662
2017-10-10T14:41:07.440187: step 579, loss 0.182112, acc 0.9375, learning_rate 0.000560774
2017-10-10T14:41:07.691113: step 580, loss 0.358841, acc 0.859375, learning_rate 0.000558893
2017-10-10T14:41:07.981021: step 581, loss 0.213546, acc 0.921875, learning_rate 0.00055702
2017-10-10T14:41:08.241239: step 582, loss 0.125545, acc 0.953125, learning_rate 0.000555154
2017-10-10T14:41:08.531800: step 583, loss 0.244314, acc 0.890625, learning_rate 0.000553296
2017-10-10T14:41:08.780143: step 584, loss 0.235034, acc 0.90625, learning_rate 0.000551446
2017-10-10T14:41:09.100864: step 585, loss 0.135487, acc 0.9375, learning_rate 0.000549604
2017-10-10T14:41:09.322826: step 586, loss 0.205324, acc 0.921875, learning_rate 0.000547768
2017-10-10T14:41:09.573605: step 587, loss 0.0720775, acc 0.984375, learning_rate 0.000545941
2017-10-10T14:41:09.777503: step 588, loss 0.341564, acc 0.882353, learning_rate 0.00054412
2017-10-10T14:41:10.032892: step 589, loss 0.127273, acc 0.953125, learning_rate 0.000542308
2017-10-10T14:41:10.304245: step 590, loss 0.152675, acc 0.96875, learning_rate 0.000540502
2017-10-10T14:41:10.556290: step 591, loss 0.0883061, acc 0.96875, learning_rate 0.000538704
2017-10-10T14:41:10.814021: step 592, loss 0.206042, acc 0.953125, learning_rate 0.000536914
2017-10-10T14:41:11.089071: step 593, loss 0.26297, acc 0.90625, learning_rate 0.00053513
2017-10-10T14:41:11.328635: step 594, loss 0.154581, acc 0.953125, learning_rate 0.000533354
2017-10-10T14:41:11.609238: step 595, loss 0.0901637, acc 0.984375, learning_rate 0.000531585
2017-10-10T14:41:11.916971: step 596, loss 0.188228, acc 0.921875, learning_rate 0.000529824
2017-10-10T14:41:12.195885: step 597, loss 0.202783, acc 0.9375, learning_rate 0.000528069
2017-10-10T14:41:12.492205: step 598, loss 0.219334, acc 0.90625, learning_rate 0.000526322
2017-10-10T14:41:12.759740: step 599, loss 0.242724, acc 0.90625, learning_rate 0.000524582
2017-10-10T14:41:13.043846: step 600, loss 0.254988, acc 0.921875, learning_rate 0.000522849

Evaluation:
2017-10-10T14:41:13.604445: step 600, loss 0.23408, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-600

2017-10-10T14:41:14.645017: step 601, loss 0.188345, acc 0.953125, learning_rate 0.000521123
2017-10-10T14:41:14.832835: step 602, loss 0.152446, acc 0.921875, learning_rate 0.000519404
2017-10-10T14:41:15.109970: step 603, loss 0.142035, acc 0.953125, learning_rate 0.000517692
2017-10-10T14:41:15.400754: step 604, loss 0.153745, acc 0.9375, learning_rate 0.000515987
2017-10-10T14:41:15.732942: step 605, loss 0.14803, acc 0.953125, learning_rate 0.000514289
2017-10-10T14:41:15.929873: step 606, loss 0.157429, acc 0.9375, learning_rate 0.000512598
2017-10-10T14:41:16.132718: step 607, loss 0.112486, acc 0.953125, learning_rate 0.000510914
2017-10-10T14:41:16.333513: step 608, loss 0.183541, acc 0.9375, learning_rate 0.000509237
2017-10-10T14:41:16.524096: step 609, loss 0.151105, acc 0.9375, learning_rate 0.000507566
2017-10-10T14:41:16.736171: step 610, loss 0.109507, acc 0.984375, learning_rate 0.000505903
2017-10-10T14:41:16.994745: step 611, loss 0.187038, acc 0.921875, learning_rate 0.000504246
2017-10-10T14:41:17.245557: step 612, loss 0.190272, acc 0.90625, learning_rate 0.000502596
2017-10-10T14:41:17.496228: step 613, loss 0.168206, acc 0.921875, learning_rate 0.000500953
2017-10-10T14:41:17.778539: step 614, loss 0.185936, acc 0.921875, learning_rate 0.000499316
2017-10-10T14:41:18.076901: step 615, loss 0.257173, acc 0.90625, learning_rate 0.000497686
2017-10-10T14:41:18.348976: step 616, loss 0.101826, acc 0.96875, learning_rate 0.000496063
2017-10-10T14:41:18.604913: step 617, loss 0.180748, acc 0.953125, learning_rate 0.000494446
2017-10-10T14:41:18.874933: step 618, loss 0.121179, acc 0.953125, learning_rate 0.000492836
2017-10-10T14:41:19.136944: step 619, loss 0.159514, acc 0.9375, learning_rate 0.000491233
2017-10-10T14:41:19.434817: step 620, loss 0.328329, acc 0.921875, learning_rate 0.000489636
2017-10-10T14:41:19.708878: step 621, loss 0.0986398, acc 0.96875, learning_rate 0.000488045
2017-10-10T14:41:19.985657: step 622, loss 0.106759, acc 0.984375, learning_rate 0.000486461
2017-10-10T14:41:20.260874: step 623, loss 0.0929268, acc 0.984375, learning_rate 0.000484884
2017-10-10T14:41:20.504897: step 624, loss 0.099976, acc 0.953125, learning_rate 0.000483313
2017-10-10T14:41:20.772806: step 625, loss 0.116305, acc 0.96875, learning_rate 0.000481748
2017-10-10T14:41:21.066283: step 626, loss 0.225416, acc 0.90625, learning_rate 0.00048019
2017-10-10T14:41:21.313044: step 627, loss 0.14477, acc 0.9375, learning_rate 0.000478638
2017-10-10T14:41:21.605595: step 628, loss 0.122545, acc 0.953125, learning_rate 0.000477093
2017-10-10T14:41:21.825303: step 629, loss 0.145599, acc 0.953125, learning_rate 0.000475554
2017-10-10T14:41:22.058132: step 630, loss 0.0949404, acc 0.953125, learning_rate 0.000474021
2017-10-10T14:41:22.284812: step 631, loss 0.147591, acc 0.921875, learning_rate 0.000472494
2017-10-10T14:41:22.477220: step 632, loss 0.127253, acc 0.9375, learning_rate 0.000470974
2017-10-10T14:41:22.756901: step 633, loss 0.19172, acc 0.921875, learning_rate 0.000469459
2017-10-10T14:41:23.009128: step 634, loss 0.261473, acc 0.90625, learning_rate 0.000467951
2017-10-10T14:41:23.276250: step 635, loss 0.250629, acc 0.90625, learning_rate 0.000466449
2017-10-10T14:41:23.564573: step 636, loss 0.0636283, acc 0.984375, learning_rate 0.000464954
2017-10-10T14:41:23.833566: step 637, loss 0.175755, acc 0.953125, learning_rate 0.000463464
2017-10-10T14:41:24.109036: step 638, loss 0.122728, acc 0.9375, learning_rate 0.00046198
2017-10-10T14:41:24.380870: step 639, loss 0.265143, acc 0.90625, learning_rate 0.000460503
2017-10-10T14:41:24.620225: step 640, loss 0.167858, acc 0.90625, learning_rate 0.000459031

Evaluation:
2017-10-10T14:41:25.124920: step 640, loss 0.23567, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-640

2017-10-10T14:41:26.364105: step 641, loss 0.128004, acc 0.96875, learning_rate 0.000457566
2017-10-10T14:41:26.585011: step 642, loss 0.14345, acc 0.921875, learning_rate 0.000456106
2017-10-10T14:41:26.772963: step 643, loss 0.23538, acc 0.953125, learning_rate 0.000454653
2017-10-10T14:41:26.978307: step 644, loss 0.231884, acc 0.921875, learning_rate 0.000453205
2017-10-10T14:41:27.172125: step 645, loss 0.214046, acc 0.890625, learning_rate 0.000451764
2017-10-10T14:41:27.356967: step 646, loss 0.225885, acc 0.9375, learning_rate 0.000450328
2017-10-10T14:41:27.618135: step 647, loss 0.105906, acc 0.96875, learning_rate 0.000448898
2017-10-10T14:41:27.882312: step 648, loss 0.101101, acc 0.984375, learning_rate 0.000447474
2017-10-10T14:41:28.152854: step 649, loss 0.240824, acc 0.90625, learning_rate 0.000446055
2017-10-10T14:41:28.441684: step 650, loss 0.0679127, acc 0.984375, learning_rate 0.000444643
2017-10-10T14:41:28.695602: step 651, loss 0.469367, acc 0.890625, learning_rate 0.000443236
2017-10-10T14:41:29.016945: step 652, loss 0.185018, acc 0.96875, learning_rate 0.000441835
2017-10-10T14:41:29.257590: step 653, loss 0.168537, acc 0.9375, learning_rate 0.00044044
2017-10-10T14:41:29.457327: step 654, loss 0.160449, acc 0.9375, learning_rate 0.00043905
2017-10-10T14:41:29.687199: step 655, loss 0.117753, acc 0.9375, learning_rate 0.000437666
2017-10-10T14:41:29.929117: step 656, loss 0.0665652, acc 1, learning_rate 0.000436288
2017-10-10T14:41:30.189103: step 657, loss 0.109413, acc 0.96875, learning_rate 0.000434915
2017-10-10T14:41:30.475364: step 658, loss 0.110512, acc 0.953125, learning_rate 0.000433548
2017-10-10T14:41:30.723408: step 659, loss 0.127025, acc 0.953125, learning_rate 0.000432187
2017-10-10T14:41:31.003964: step 660, loss 0.186698, acc 0.921875, learning_rate 0.000430831
2017-10-10T14:41:31.269066: step 661, loss 0.0663304, acc 0.984375, learning_rate 0.000429481
2017-10-10T14:41:31.555927: step 662, loss 0.196438, acc 0.875, learning_rate 0.000428136
2017-10-10T14:41:31.838949: step 663, loss 0.140422, acc 0.96875, learning_rate 0.000426796
2017-10-10T14:41:32.099643: step 664, loss 0.280214, acc 0.90625, learning_rate 0.000425463
2017-10-10T14:41:32.361028: step 665, loss 0.110726, acc 0.953125, learning_rate 0.000424134
2017-10-10T14:41:32.632988: step 666, loss 0.193897, acc 0.90625, learning_rate 0.000422811
2017-10-10T14:41:32.889537: step 667, loss 0.122512, acc 0.96875, learning_rate 0.000421493
2017-10-10T14:41:33.161917: step 668, loss 0.123203, acc 0.984375, learning_rate 0.000420181
2017-10-10T14:41:33.449246: step 669, loss 0.200662, acc 0.9375, learning_rate 0.000418874
2017-10-10T14:41:33.708441: step 670, loss 0.183586, acc 0.9375, learning_rate 0.000417573
2017-10-10T14:41:33.952698: step 671, loss 0.188141, acc 0.953125, learning_rate 0.000416276
2017-10-10T14:41:34.192868: step 672, loss 0.118727, acc 0.953125, learning_rate 0.000414985
2017-10-10T14:41:34.446161: step 673, loss 0.302328, acc 0.859375, learning_rate 0.0004137
2017-10-10T14:41:34.691945: step 674, loss 0.0802021, acc 0.96875, learning_rate 0.000412419
2017-10-10T14:41:34.946227: step 675, loss 0.258676, acc 0.90625, learning_rate 0.000411144
2017-10-10T14:41:35.204833: step 676, loss 0.0893795, acc 0.96875, learning_rate 0.000409874
2017-10-10T14:41:35.460391: step 677, loss 0.104759, acc 0.96875, learning_rate 0.000408609
2017-10-10T14:41:35.704447: step 678, loss 0.250503, acc 0.953125, learning_rate 0.00040735
2017-10-10T14:41:35.976944: step 679, loss 0.16805, acc 0.953125, learning_rate 0.000406095
2017-10-10T14:41:36.227973: step 680, loss 0.0914085, acc 0.953125, learning_rate 0.000404846

Evaluation:
2017-10-10T14:41:36.890366: step 680, loss 0.234272, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-680

2017-10-10T14:41:37.791936: step 681, loss 0.0890906, acc 0.96875, learning_rate 0.000403601
2017-10-10T14:41:37.998159: step 682, loss 0.113225, acc 0.953125, learning_rate 0.000402362
2017-10-10T14:41:38.272498: step 683, loss 0.0799246, acc 0.96875, learning_rate 0.000401128
2017-10-10T14:41:38.534213: step 684, loss 0.111082, acc 0.953125, learning_rate 0.000399899
2017-10-10T14:41:38.811323: step 685, loss 0.225576, acc 0.90625, learning_rate 0.000398675
2017-10-10T14:41:39.059212: step 686, loss 0.187493, acc 0.960784, learning_rate 0.000397456
2017-10-10T14:41:39.338247: step 687, loss 0.193057, acc 0.9375, learning_rate 0.000396241
2017-10-10T14:41:39.607432: step 688, loss 0.0935138, acc 0.96875, learning_rate 0.000395032
2017-10-10T14:41:39.884855: step 689, loss 0.111557, acc 0.984375, learning_rate 0.000393828
2017-10-10T14:41:40.125894: step 690, loss 0.0586449, acc 0.96875, learning_rate 0.000392629
2017-10-10T14:41:40.378586: step 691, loss 0.151676, acc 0.9375, learning_rate 0.000391434
2017-10-10T14:41:40.630557: step 692, loss 0.164014, acc 0.953125, learning_rate 0.000390245
2017-10-10T14:41:40.899245: step 693, loss 0.187939, acc 0.953125, learning_rate 0.00038906
2017-10-10T14:41:41.142955: step 694, loss 0.104742, acc 0.953125, learning_rate 0.00038788
2017-10-10T14:41:41.392117: step 695, loss 0.11617, acc 0.9375, learning_rate 0.000386705
2017-10-10T14:41:41.629978: step 696, loss 0.180764, acc 0.9375, learning_rate 0.000385535
2017-10-10T14:41:41.936981: step 697, loss 0.0791736, acc 0.984375, learning_rate 0.000384369
2017-10-10T14:41:42.200047: step 698, loss 0.162075, acc 0.9375, learning_rate 0.000383209
2017-10-10T14:41:42.471552: step 699, loss 0.227205, acc 0.90625, learning_rate 0.000382053
2017-10-10T14:41:42.732967: step 700, loss 0.215857, acc 0.9375, learning_rate 0.000380901
2017-10-10T14:41:42.994774: step 701, loss 0.108883, acc 0.953125, learning_rate 0.000379755
2017-10-10T14:41:43.232597: step 702, loss 0.151653, acc 0.953125, learning_rate 0.000378613
2017-10-10T14:41:43.509052: step 703, loss 0.167543, acc 0.9375, learning_rate 0.000377476
2017-10-10T14:41:43.783292: step 704, loss 0.103879, acc 0.953125, learning_rate 0.000376343
2017-10-10T14:41:44.106421: step 705, loss 0.319062, acc 0.859375, learning_rate 0.000375215
2017-10-10T14:41:44.345160: step 706, loss 0.228851, acc 0.921875, learning_rate 0.000374092
2017-10-10T14:41:44.583691: step 707, loss 0.114028, acc 0.984375, learning_rate 0.000372973
2017-10-10T14:41:44.820786: step 708, loss 0.13113, acc 0.9375, learning_rate 0.000371859
2017-10-10T14:41:45.085458: step 709, loss 0.122808, acc 0.96875, learning_rate 0.000370749
2017-10-10T14:41:45.357045: step 710, loss 0.147697, acc 0.9375, learning_rate 0.000369644
2017-10-10T14:41:45.586438: step 711, loss 0.177574, acc 0.9375, learning_rate 0.000368543
2017-10-10T14:41:45.848889: step 712, loss 0.131274, acc 0.921875, learning_rate 0.000367447
2017-10-10T14:41:46.087870: step 713, loss 0.179859, acc 0.921875, learning_rate 0.000366356
2017-10-10T14:41:46.349151: step 714, loss 0.223758, acc 0.90625, learning_rate 0.000365268
2017-10-10T14:41:46.602195: step 715, loss 0.144943, acc 0.953125, learning_rate 0.000364186
2017-10-10T14:41:46.871006: step 716, loss 0.293068, acc 0.890625, learning_rate 0.000363107
2017-10-10T14:41:47.136945: step 717, loss 0.201452, acc 0.90625, learning_rate 0.000362033
2017-10-10T14:41:47.412942: step 718, loss 0.149297, acc 0.921875, learning_rate 0.000360964
2017-10-10T14:41:47.717999: step 719, loss 0.194227, acc 0.9375, learning_rate 0.000359899
2017-10-10T14:41:48.013860: step 720, loss 0.0930956, acc 0.96875, learning_rate 0.000358838

Evaluation:
2017-10-10T14:41:48.704821: step 720, loss 0.23586, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-720

2017-10-10T14:41:49.506466: step 721, loss 0.234107, acc 0.9375, learning_rate 0.000357781
2017-10-10T14:41:49.777341: step 722, loss 0.16574, acc 0.953125, learning_rate 0.000356729
2017-10-10T14:41:50.027796: step 723, loss 0.267906, acc 0.875, learning_rate 0.000355681
2017-10-10T14:41:50.286871: step 724, loss 0.124451, acc 0.96875, learning_rate 0.000354637
2017-10-10T14:41:50.569089: step 725, loss 0.175919, acc 0.921875, learning_rate 0.000353598
2017-10-10T14:41:50.820236: step 726, loss 0.195567, acc 0.921875, learning_rate 0.000352563
2017-10-10T14:41:51.068862: step 727, loss 0.23012, acc 0.921875, learning_rate 0.000351532
2017-10-10T14:41:51.352517: step 728, loss 0.326331, acc 0.921875, learning_rate 0.000350505
2017-10-10T14:41:51.550122: step 729, loss 0.108197, acc 0.953125, learning_rate 0.000349483
2017-10-10T14:41:51.770435: step 730, loss 0.121999, acc 0.953125, learning_rate 0.000348465
2017-10-10T14:41:51.973048: step 731, loss 0.138715, acc 0.9375, learning_rate 0.00034745
2017-10-10T14:41:52.178717: step 732, loss 0.0994193, acc 0.96875, learning_rate 0.00034644
2017-10-10T14:41:52.397834: step 733, loss 0.0744955, acc 0.984375, learning_rate 0.000345434
2017-10-10T14:41:52.593745: step 734, loss 0.143757, acc 0.953125, learning_rate 0.000344433
2017-10-10T14:41:52.816234: step 735, loss 0.153719, acc 0.9375, learning_rate 0.000343435
2017-10-10T14:41:53.044748: step 736, loss 0.104156, acc 0.96875, learning_rate 0.000342441
2017-10-10T14:41:53.256995: step 737, loss 0.112715, acc 0.96875, learning_rate 0.000341452
2017-10-10T14:41:53.513907: step 738, loss 0.144147, acc 0.9375, learning_rate 0.000340466
2017-10-10T14:41:53.776246: step 739, loss 0.206708, acc 0.921875, learning_rate 0.000339485
2017-10-10T14:41:54.047144: step 740, loss 0.109609, acc 0.953125, learning_rate 0.000338507
2017-10-10T14:41:54.318351: step 741, loss 0.200507, acc 0.90625, learning_rate 0.000337534
2017-10-10T14:41:54.573530: step 742, loss 0.130099, acc 0.9375, learning_rate 0.000336564
2017-10-10T14:41:54.820791: step 743, loss 0.249367, acc 0.921875, learning_rate 0.000335598
2017-10-10T14:41:55.063419: step 744, loss 0.088678, acc 0.96875, learning_rate 0.000334637
2017-10-10T14:41:55.312717: step 745, loss 0.239102, acc 0.90625, learning_rate 0.000333679
2017-10-10T14:41:55.581330: step 746, loss 0.102946, acc 0.96875, learning_rate 0.000332725
2017-10-10T14:41:55.829319: step 747, loss 0.133103, acc 0.984375, learning_rate 0.000331775
2017-10-10T14:41:56.083155: step 748, loss 0.109774, acc 0.984375, learning_rate 0.000330829
2017-10-10T14:41:56.338870: step 749, loss 0.14121, acc 0.9375, learning_rate 0.000329887
2017-10-10T14:41:56.622989: step 750, loss 0.138547, acc 0.96875, learning_rate 0.000328949
2017-10-10T14:41:56.881055: step 751, loss 0.120317, acc 0.953125, learning_rate 0.000328014
2017-10-10T14:41:57.095161: step 752, loss 0.138939, acc 0.96875, learning_rate 0.000327083
2017-10-10T14:41:57.341032: step 753, loss 0.120472, acc 0.953125, learning_rate 0.000326157
2017-10-10T14:41:57.617326: step 754, loss 0.306605, acc 0.90625, learning_rate 0.000325233
2017-10-10T14:41:57.916839: step 755, loss 0.149627, acc 0.953125, learning_rate 0.000324314
2017-10-10T14:41:58.160853: step 756, loss 0.022847, acc 1, learning_rate 0.000323399
2017-10-10T14:41:58.429299: step 757, loss 0.128909, acc 0.9375, learning_rate 0.000322487
2017-10-10T14:41:58.669280: step 758, loss 0.192598, acc 0.9375, learning_rate 0.000321579
2017-10-10T14:41:58.964923: step 759, loss 0.0885304, acc 0.984375, learning_rate 0.000320674
2017-10-10T14:41:59.300351: step 760, loss 0.140703, acc 0.953125, learning_rate 0.000319773

Evaluation:
2017-10-10T14:41:59.751745: step 760, loss 0.240257, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-760

2017-10-10T14:42:00.732076: step 761, loss 0.129834, acc 0.96875, learning_rate 0.000318876
2017-10-10T14:42:01.002437: step 762, loss 0.0875402, acc 0.984375, learning_rate 0.000317983
2017-10-10T14:42:01.255268: step 763, loss 0.146929, acc 0.9375, learning_rate 0.000317093
2017-10-10T14:42:01.533651: step 764, loss 0.0237279, acc 1, learning_rate 0.000316207
2017-10-10T14:42:01.824997: step 765, loss 0.1162, acc 0.96875, learning_rate 0.000315325
2017-10-10T14:42:02.128322: step 766, loss 0.0571417, acc 0.96875, learning_rate 0.000314446
2017-10-10T14:42:02.394306: step 767, loss 0.078655, acc 0.96875, learning_rate 0.00031357
2017-10-10T14:42:02.610318: step 768, loss 0.261387, acc 0.921875, learning_rate 0.000312699
2017-10-10T14:42:02.886096: step 769, loss 0.1884, acc 0.90625, learning_rate 0.00031183
2017-10-10T14:42:03.103108: step 770, loss 0.271043, acc 0.890625, learning_rate 0.000310966
2017-10-10T14:42:03.371735: step 771, loss 0.26315, acc 0.890625, learning_rate 0.000310105
2017-10-10T14:42:03.626710: step 772, loss 0.130934, acc 0.9375, learning_rate 0.000309247
2017-10-10T14:42:03.902159: step 773, loss 0.21111, acc 0.921875, learning_rate 0.000308393
2017-10-10T14:42:04.165203: step 774, loss 0.167911, acc 0.921875, learning_rate 0.000307542
2017-10-10T14:42:04.428842: step 775, loss 0.160619, acc 0.953125, learning_rate 0.000306695
2017-10-10T14:42:04.700854: step 776, loss 0.0599943, acc 0.984375, learning_rate 0.000305852
2017-10-10T14:42:04.952841: step 777, loss 0.0906031, acc 0.96875, learning_rate 0.000305011
2017-10-10T14:42:05.216844: step 778, loss 0.177225, acc 0.921875, learning_rate 0.000304174
2017-10-10T14:42:05.518643: step 779, loss 0.145126, acc 0.9375, learning_rate 0.000303341
2017-10-10T14:42:05.756761: step 780, loss 0.140495, acc 0.9375, learning_rate 0.000302511
2017-10-10T14:42:06.013442: step 781, loss 0.093692, acc 0.96875, learning_rate 0.000301684
2017-10-10T14:42:06.235844: step 782, loss 0.13603, acc 0.953125, learning_rate 0.000300861
2017-10-10T14:42:06.483525: step 783, loss 0.16196, acc 0.953125, learning_rate 0.000300041
2017-10-10T14:42:06.708117: step 784, loss 0.033858, acc 1, learning_rate 0.000299225
2017-10-10T14:42:06.975419: step 785, loss 0.0697453, acc 1, learning_rate 0.000298412
2017-10-10T14:42:07.253605: step 786, loss 0.0576649, acc 1, learning_rate 0.000297602
2017-10-10T14:42:07.488115: step 787, loss 0.148934, acc 0.96875, learning_rate 0.000296795
2017-10-10T14:42:07.696951: step 788, loss 0.235896, acc 0.921875, learning_rate 0.000295992
2017-10-10T14:42:07.970455: step 789, loss 0.113817, acc 0.953125, learning_rate 0.000295192
2017-10-10T14:42:08.173042: step 790, loss 0.143478, acc 0.96875, learning_rate 0.000294395
2017-10-10T14:42:08.382920: step 791, loss 0.146017, acc 0.9375, learning_rate 0.000293602
2017-10-10T14:42:08.611747: step 792, loss 0.101723, acc 0.96875, learning_rate 0.000292812
2017-10-10T14:42:08.865012: step 793, loss 0.275713, acc 0.9375, learning_rate 0.000292025
2017-10-10T14:42:09.109819: step 794, loss 0.142184, acc 0.96875, learning_rate 0.000291241
2017-10-10T14:42:09.343561: step 795, loss 0.147597, acc 0.96875, learning_rate 0.00029046
2017-10-10T14:42:09.612714: step 796, loss 0.147461, acc 0.9375, learning_rate 0.000289683
2017-10-10T14:42:09.986693: step 797, loss 0.150511, acc 0.921875, learning_rate 0.000288908
2017-10-10T14:42:10.216323: step 798, loss 0.257744, acc 0.890625, learning_rate 0.000288137
2017-10-10T14:42:10.432851: step 799, loss 0.145786, acc 0.953125, learning_rate 0.000287369
2017-10-10T14:42:10.641262: step 800, loss 0.0717116, acc 1, learning_rate 0.000286605

Evaluation:
2017-10-10T14:42:11.169011: step 800, loss 0.236127, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-800

2017-10-10T14:42:12.171909: step 801, loss 0.216406, acc 0.90625, learning_rate 0.000285843
2017-10-10T14:42:12.398024: step 802, loss 0.113859, acc 0.9375, learning_rate 0.000285084
2017-10-10T14:42:12.658126: step 803, loss 0.196979, acc 0.921875, learning_rate 0.000284329
2017-10-10T14:42:12.906479: step 804, loss 0.159473, acc 0.9375, learning_rate 0.000283577
2017-10-10T14:42:13.167866: step 805, loss 0.234011, acc 0.921875, learning_rate 0.000282827
2017-10-10T14:42:13.431014: step 806, loss 0.09225, acc 0.96875, learning_rate 0.000282081
2017-10-10T14:42:13.740904: step 807, loss 0.138541, acc 0.96875, learning_rate 0.000281338
2017-10-10T14:42:14.021328: step 808, loss 0.177395, acc 0.921875, learning_rate 0.000280598
2017-10-10T14:42:14.292844: step 809, loss 0.1607, acc 0.953125, learning_rate 0.00027986
2017-10-10T14:42:14.573119: step 810, loss 0.315494, acc 0.90625, learning_rate 0.000279126
2017-10-10T14:42:14.824747: step 811, loss 0.0796369, acc 0.984375, learning_rate 0.000278395
2017-10-10T14:42:15.149998: step 812, loss 0.124697, acc 0.96875, learning_rate 0.000277667
2017-10-10T14:42:15.386823: step 813, loss 0.197672, acc 0.953125, learning_rate 0.000276942
2017-10-10T14:42:15.580271: step 814, loss 0.174379, acc 0.953125, learning_rate 0.00027622
2017-10-10T14:42:15.828836: step 815, loss 0.126606, acc 0.953125, learning_rate 0.0002755
2017-10-10T14:42:16.107819: step 816, loss 0.246791, acc 0.90625, learning_rate 0.000274784
2017-10-10T14:42:16.356475: step 817, loss 0.0940332, acc 0.96875, learning_rate 0.000274071
2017-10-10T14:42:16.626842: step 818, loss 0.0743252, acc 1, learning_rate 0.00027336
2017-10-10T14:42:16.899670: step 819, loss 0.268308, acc 0.9375, learning_rate 0.000272652
2017-10-10T14:42:17.163704: step 820, loss 0.132957, acc 0.96875, learning_rate 0.000271948
2017-10-10T14:42:17.438516: step 821, loss 0.188053, acc 0.921875, learning_rate 0.000271246
2017-10-10T14:42:17.705426: step 822, loss 0.205397, acc 0.921875, learning_rate 0.000270547
2017-10-10T14:42:17.965368: step 823, loss 0.265456, acc 0.890625, learning_rate 0.000269851
2017-10-10T14:42:18.232410: step 824, loss 0.143677, acc 0.984375, learning_rate 0.000269157
2017-10-10T14:42:18.506171: step 825, loss 0.13293, acc 0.96875, learning_rate 0.000268467
2017-10-10T14:42:18.785174: step 826, loss 0.225116, acc 0.9375, learning_rate 0.000267779
2017-10-10T14:42:19.092534: step 827, loss 0.142859, acc 0.9375, learning_rate 0.000267094
2017-10-10T14:42:19.374837: step 828, loss 0.0874519, acc 0.984375, learning_rate 0.000266412
2017-10-10T14:42:19.657931: step 829, loss 0.082484, acc 0.984375, learning_rate 0.000265733
2017-10-10T14:42:19.939528: step 830, loss 0.0917323, acc 0.96875, learning_rate 0.000265057
2017-10-10T14:42:20.204873: step 831, loss 0.195005, acc 0.9375, learning_rate 0.000264383
2017-10-10T14:42:20.452870: step 832, loss 0.117388, acc 0.96875, learning_rate 0.000263712
2017-10-10T14:42:20.830317: step 833, loss 0.184797, acc 0.953125, learning_rate 0.000263044
2017-10-10T14:42:21.029690: step 834, loss 0.0842946, acc 0.984375, learning_rate 0.000262378
2017-10-10T14:42:21.240824: step 835, loss 0.0672182, acc 0.984375, learning_rate 0.000261715
2017-10-10T14:42:21.453837: step 836, loss 0.248428, acc 0.921875, learning_rate 0.000261055
2017-10-10T14:42:21.633660: step 837, loss 0.0959609, acc 0.96875, learning_rate 0.000260398
2017-10-10T14:42:21.835935: step 838, loss 0.190976, acc 0.921875, learning_rate 0.000259743
2017-10-10T14:42:22.086621: step 839, loss 0.136458, acc 0.9375, learning_rate 0.000259091
2017-10-10T14:42:22.317050: step 840, loss 0.187121, acc 0.9375, learning_rate 0.000258442

Evaluation:
2017-10-10T14:42:22.876833: step 840, loss 0.237495, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-840

2017-10-10T14:42:23.872727: step 841, loss 0.145285, acc 0.953125, learning_rate 0.000257795
2017-10-10T14:42:24.140504: step 842, loss 0.193413, acc 0.921875, learning_rate 0.000257151
2017-10-10T14:42:24.396896: step 843, loss 0.164258, acc 0.921875, learning_rate 0.00025651
2017-10-10T14:42:24.657757: step 844, loss 0.130095, acc 0.9375, learning_rate 0.000255871
2017-10-10T14:42:24.897409: step 845, loss 0.0820686, acc 0.96875, learning_rate 0.000255235
2017-10-10T14:42:25.148421: step 846, loss 0.125242, acc 0.953125, learning_rate 0.000254601
2017-10-10T14:42:25.398233: step 847, loss 0.12409, acc 0.9375, learning_rate 0.00025397
2017-10-10T14:42:25.660132: step 848, loss 0.164707, acc 0.921875, learning_rate 0.000253341
2017-10-10T14:42:25.893144: step 849, loss 0.0654814, acc 0.96875, learning_rate 0.000252716
2017-10-10T14:42:26.140246: step 850, loss 0.0633821, acc 0.984375, learning_rate 0.000252092
2017-10-10T14:42:26.394478: step 851, loss 0.17732, acc 0.953125, learning_rate 0.000251471
2017-10-10T14:42:26.664895: step 852, loss 0.104033, acc 0.9375, learning_rate 0.000250853
2017-10-10T14:42:26.900452: step 853, loss 0.169682, acc 0.9375, learning_rate 0.000250237
2017-10-10T14:42:27.163257: step 854, loss 0.0865439, acc 0.984375, learning_rate 0.000249624
2017-10-10T14:42:27.426317: step 855, loss 0.0779108, acc 0.96875, learning_rate 0.000249013
2017-10-10T14:42:27.700907: step 856, loss 0.0803838, acc 0.984375, learning_rate 0.000248405
2017-10-10T14:42:27.966923: step 857, loss 0.232818, acc 0.9375, learning_rate 0.000247799
2017-10-10T14:42:28.214176: step 858, loss 0.172089, acc 0.9375, learning_rate 0.000247196
2017-10-10T14:42:28.481330: step 859, loss 0.0948726, acc 0.96875, learning_rate 0.000246595
2017-10-10T14:42:28.748836: step 860, loss 0.0567811, acc 1, learning_rate 0.000245997
2017-10-10T14:42:29.010951: step 861, loss 0.183734, acc 0.9375, learning_rate 0.000245401
2017-10-10T14:42:29.256754: step 862, loss 0.20213, acc 0.953125, learning_rate 0.000244808
2017-10-10T14:42:29.509183: step 863, loss 0.0529061, acc 0.984375, learning_rate 0.000244216
2017-10-10T14:42:29.753052: step 864, loss 0.10723, acc 0.96875, learning_rate 0.000243628
2017-10-10T14:42:29.993037: step 865, loss 0.105805, acc 0.953125, learning_rate 0.000243042
2017-10-10T14:42:30.232928: step 866, loss 0.0995872, acc 0.96875, learning_rate 0.000242458
2017-10-10T14:42:30.509677: step 867, loss 0.0831765, acc 0.984375, learning_rate 0.000241876
2017-10-10T14:42:30.702877: step 868, loss 0.065498, acc 0.96875, learning_rate 0.000241297
2017-10-10T14:42:30.920906: step 869, loss 0.0509954, acc 1, learning_rate 0.00024072
2017-10-10T14:42:31.156823: step 870, loss 0.293286, acc 0.890625, learning_rate 0.000240146
2017-10-10T14:42:31.391385: step 871, loss 0.173739, acc 0.953125, learning_rate 0.000239574
2017-10-10T14:42:31.750043: step 872, loss 0.0837463, acc 0.96875, learning_rate 0.000239004
2017-10-10T14:42:31.960184: step 873, loss 0.194964, acc 0.96875, learning_rate 0.000238437
2017-10-10T14:42:32.146862: step 874, loss 0.110954, acc 0.953125, learning_rate 0.000237872
2017-10-10T14:42:32.317940: step 875, loss 0.173873, acc 0.9375, learning_rate 0.000237309
2017-10-10T14:42:32.508408: step 876, loss 0.136847, acc 0.9375, learning_rate 0.000236749
2017-10-10T14:42:32.686912: step 877, loss 0.16066, acc 0.953125, learning_rate 0.00023619
2017-10-10T14:42:32.878171: step 878, loss 0.175872, acc 0.9375, learning_rate 0.000235635
2017-10-10T14:42:33.075203: step 879, loss 0.264795, acc 0.9375, learning_rate 0.000235081
2017-10-10T14:42:33.275622: step 880, loss 0.254363, acc 0.9375, learning_rate 0.00023453

Evaluation:
2017-10-10T14:42:33.868915: step 880, loss 0.235288, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-880

2017-10-10T14:42:34.957980: step 881, loss 0.0971022, acc 0.984375, learning_rate 0.00023398
2017-10-10T14:42:35.176859: step 882, loss 0.0970882, acc 0.960784, learning_rate 0.000233434
2017-10-10T14:42:35.466766: step 883, loss 0.109418, acc 0.96875, learning_rate 0.000232889
2017-10-10T14:42:35.716188: step 884, loss 0.259282, acc 0.921875, learning_rate 0.000232346
2017-10-10T14:42:35.971742: step 885, loss 0.150863, acc 0.953125, learning_rate 0.000231806
2017-10-10T14:42:36.271149: step 886, loss 0.0975506, acc 0.953125, learning_rate 0.000231268
2017-10-10T14:42:36.493230: step 887, loss 0.0748491, acc 0.984375, learning_rate 0.000230732
2017-10-10T14:42:36.775796: step 888, loss 0.20873, acc 0.96875, learning_rate 0.000230199
2017-10-10T14:42:37.069600: step 889, loss 0.270714, acc 0.921875, learning_rate 0.000229667
2017-10-10T14:42:37.352851: step 890, loss 0.0777318, acc 0.96875, learning_rate 0.000229138
2017-10-10T14:42:37.674427: step 891, loss 0.237195, acc 0.90625, learning_rate 0.000228611
2017-10-10T14:42:37.894354: step 892, loss 0.187015, acc 0.921875, learning_rate 0.000228086
2017-10-10T14:42:38.127006: step 893, loss 0.100572, acc 0.96875, learning_rate 0.000227563
2017-10-10T14:42:38.356079: step 894, loss 0.139154, acc 0.953125, learning_rate 0.000227043
2017-10-10T14:42:38.615434: step 895, loss 0.237066, acc 0.90625, learning_rate 0.000226524
2017-10-10T14:42:38.902728: step 896, loss 0.0823145, acc 0.96875, learning_rate 0.000226008
2017-10-10T14:42:39.151223: step 897, loss 0.23726, acc 0.890625, learning_rate 0.000225493
2017-10-10T14:42:39.432905: step 898, loss 0.0966444, acc 0.953125, learning_rate 0.000224981
2017-10-10T14:42:39.707737: step 899, loss 0.129323, acc 0.9375, learning_rate 0.000224471
2017-10-10T14:42:40.005226: step 900, loss 0.16421, acc 0.9375, learning_rate 0.000223963
2017-10-10T14:42:40.292865: step 901, loss 0.084313, acc 0.96875, learning_rate 0.000223457
2017-10-10T14:42:40.590046: step 902, loss 0.0718412, acc 0.96875, learning_rate 0.000222953
2017-10-10T14:42:40.859686: step 903, loss 0.124632, acc 0.9375, learning_rate 0.000222451
2017-10-10T14:42:41.123783: step 904, loss 0.166927, acc 0.921875, learning_rate 0.000221951
2017-10-10T14:42:41.388799: step 905, loss 0.170503, acc 0.953125, learning_rate 0.000221453
2017-10-10T14:42:41.668854: step 906, loss 0.118704, acc 0.953125, learning_rate 0.000220958
2017-10-10T14:42:41.908963: step 907, loss 0.135716, acc 0.953125, learning_rate 0.000220464
2017-10-10T14:42:42.169018: step 908, loss 0.246368, acc 0.890625, learning_rate 0.000219972
2017-10-10T14:42:42.419009: step 909, loss 0.108875, acc 0.96875, learning_rate 0.000219483
2017-10-10T14:42:42.716368: step 910, loss 0.347035, acc 0.890625, learning_rate 0.000218995
2017-10-10T14:42:42.973715: step 911, loss 0.0615394, acc 1, learning_rate 0.000218509
2017-10-10T14:42:43.353319: step 912, loss 0.14101, acc 0.9375, learning_rate 0.000218025
2017-10-10T14:42:43.596275: step 913, loss 0.110023, acc 0.96875, learning_rate 0.000217544
2017-10-10T14:42:43.784164: step 914, loss 0.19312, acc 0.9375, learning_rate 0.000217064
2017-10-10T14:42:43.997074: step 915, loss 0.158575, acc 0.90625, learning_rate 0.000216586
2017-10-10T14:42:44.186537: step 916, loss 0.168514, acc 0.9375, learning_rate 0.00021611
2017-10-10T14:42:44.366680: step 917, loss 0.132598, acc 0.96875, learning_rate 0.000215636
2017-10-10T14:42:44.662981: step 918, loss 0.113435, acc 0.96875, learning_rate 0.000215164
2017-10-10T14:42:44.946955: step 919, loss 0.143777, acc 0.953125, learning_rate 0.000214694
2017-10-10T14:42:45.257202: step 920, loss 0.149781, acc 0.96875, learning_rate 0.000214226

Evaluation:
2017-10-10T14:42:45.772942: step 920, loss 0.233544, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-920

2017-10-10T14:42:46.667370: step 921, loss 0.1533, acc 0.953125, learning_rate 0.00021376
2017-10-10T14:42:46.917914: step 922, loss 0.277189, acc 0.921875, learning_rate 0.000213295
2017-10-10T14:42:47.173909: step 923, loss 0.106955, acc 0.953125, learning_rate 0.000212833
2017-10-10T14:42:47.493721: step 924, loss 0.116696, acc 0.96875, learning_rate 0.000212372
2017-10-10T14:42:47.722329: step 925, loss 0.0598492, acc 0.984375, learning_rate 0.000211914
2017-10-10T14:42:47.978705: step 926, loss 0.0781487, acc 0.984375, learning_rate 0.000211457
2017-10-10T14:42:48.288885: step 927, loss 0.11232, acc 0.96875, learning_rate 0.000211002
2017-10-10T14:42:48.513098: step 928, loss 0.149452, acc 0.9375, learning_rate 0.000210549
2017-10-10T14:42:48.781571: step 929, loss 0.0742878, acc 0.96875, learning_rate 0.000210098
2017-10-10T14:42:49.043807: step 930, loss 0.0755675, acc 1, learning_rate 0.000209648
2017-10-10T14:42:49.318170: step 931, loss 0.277802, acc 0.859375, learning_rate 0.000209201
2017-10-10T14:42:49.618304: step 932, loss 0.137598, acc 0.953125, learning_rate 0.000208755
2017-10-10T14:42:49.901013: step 933, loss 0.0467957, acc 0.984375, learning_rate 0.000208311
2017-10-10T14:42:50.144873: step 934, loss 0.260049, acc 0.921875, learning_rate 0.000207869
2017-10-10T14:42:50.419557: step 935, loss 0.285474, acc 0.875, learning_rate 0.000207429
2017-10-10T14:42:50.669828: step 936, loss 0.16094, acc 0.921875, learning_rate 0.00020699
2017-10-10T14:42:50.946724: step 937, loss 0.042693, acc 1, learning_rate 0.000206554
2017-10-10T14:42:51.195277: step 938, loss 0.139399, acc 0.953125, learning_rate 0.000206119
2017-10-10T14:42:51.464262: step 939, loss 0.155265, acc 0.953125, learning_rate 0.000205685
2017-10-10T14:42:51.766683: step 940, loss 0.154842, acc 0.953125, learning_rate 0.000205254
2017-10-10T14:42:52.051487: step 941, loss 0.10932, acc 0.96875, learning_rate 0.000204824
2017-10-10T14:42:52.269134: step 942, loss 0.344486, acc 0.859375, learning_rate 0.000204397
2017-10-10T14:42:52.521834: step 943, loss 0.0823971, acc 0.984375, learning_rate 0.00020397
2017-10-10T14:42:52.873626: step 944, loss 0.112742, acc 0.984375, learning_rate 0.000203546
2017-10-10T14:42:53.100529: step 945, loss 0.0951638, acc 0.984375, learning_rate 0.000203123
2017-10-10T14:42:53.319197: step 946, loss 0.189555, acc 0.921875, learning_rate 0.000202702
2017-10-10T14:42:53.549885: step 947, loss 0.145481, acc 0.96875, learning_rate 0.000202283
2017-10-10T14:42:53.822261: step 948, loss 0.0299381, acc 1, learning_rate 0.000201866
2017-10-10T14:42:54.173273: step 949, loss 0.177753, acc 0.921875, learning_rate 0.00020145
2017-10-10T14:42:54.414317: step 950, loss 0.131438, acc 0.96875, learning_rate 0.000201036
2017-10-10T14:42:54.619794: step 951, loss 0.104308, acc 0.96875, learning_rate 0.000200623
2017-10-10T14:42:54.807520: step 952, loss 0.189346, acc 0.921875, learning_rate 0.000200213
2017-10-10T14:42:55.008140: step 953, loss 0.11969, acc 0.953125, learning_rate 0.000199804
2017-10-10T14:42:55.212761: step 954, loss 0.262424, acc 0.921875, learning_rate 0.000199396
2017-10-10T14:42:55.488506: step 955, loss 0.333019, acc 0.890625, learning_rate 0.000198991
2017-10-10T14:42:55.743106: step 956, loss 0.103174, acc 0.953125, learning_rate 0.000198587
2017-10-10T14:42:55.975739: step 957, loss 0.0996787, acc 0.96875, learning_rate 0.000198184
2017-10-10T14:42:56.237129: step 958, loss 0.117814, acc 0.9375, learning_rate 0.000197783
2017-10-10T14:42:56.466963: step 959, loss 0.119943, acc 0.953125, learning_rate 0.000197384
2017-10-10T14:42:56.735690: step 960, loss 0.344539, acc 0.921875, learning_rate 0.000196987

Evaluation:
2017-10-10T14:42:57.305886: step 960, loss 0.231712, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-960

2017-10-10T14:42:58.480981: step 961, loss 0.14303, acc 0.9375, learning_rate 0.000196591
2017-10-10T14:42:58.742728: step 962, loss 0.111959, acc 0.953125, learning_rate 0.000196197
2017-10-10T14:42:59.008829: step 963, loss 0.123802, acc 0.953125, learning_rate 0.000195804
2017-10-10T14:42:59.316067: step 964, loss 0.161937, acc 0.9375, learning_rate 0.000195413
2017-10-10T14:42:59.621154: step 965, loss 0.0792009, acc 0.984375, learning_rate 0.000195023
2017-10-10T14:42:59.958522: step 966, loss 0.0917116, acc 0.96875, learning_rate 0.000194636
2017-10-10T14:43:00.292129: step 967, loss 0.185446, acc 0.9375, learning_rate 0.000194249
2017-10-10T14:43:00.556867: step 968, loss 0.160422, acc 0.9375, learning_rate 0.000193865
2017-10-10T14:43:00.818867: step 969, loss 0.115343, acc 0.96875, learning_rate 0.000193482
2017-10-10T14:43:01.104857: step 970, loss 0.113594, acc 0.953125, learning_rate 0.0001931
2017-10-10T14:43:01.404245: step 971, loss 0.124141, acc 0.953125, learning_rate 0.00019272
2017-10-10T14:43:01.698316: step 972, loss 0.112089, acc 0.96875, learning_rate 0.000192341
2017-10-10T14:43:01.988837: step 973, loss 0.0798109, acc 0.96875, learning_rate 0.000191965
2017-10-10T14:43:02.356971: step 974, loss 0.174872, acc 0.921875, learning_rate 0.000191589
2017-10-10T14:43:02.606291: step 975, loss 0.137876, acc 0.9375, learning_rate 0.000191215
2017-10-10T14:43:02.862377: step 976, loss 0.0716252, acc 0.984375, learning_rate 0.000190843
2017-10-10T14:43:03.180925: step 977, loss 0.153335, acc 0.96875, learning_rate 0.000190472
2017-10-10T14:43:03.468525: step 978, loss 0.114678, acc 0.96875, learning_rate 0.000190103
2017-10-10T14:43:03.768667: step 979, loss 0.131233, acc 0.96875, learning_rate 0.000189735
2017-10-10T14:43:04.005701: step 980, loss 0.174055, acc 0.921569, learning_rate 0.000189369
2017-10-10T14:43:04.315201: step 981, loss 0.164445, acc 0.9375, learning_rate 0.000189004
2017-10-10T14:43:04.596573: step 982, loss 0.215914, acc 0.90625, learning_rate 0.000188641
2017-10-10T14:43:04.898399: step 983, loss 0.0946546, acc 0.9375, learning_rate 0.000188279
2017-10-10T14:43:05.207590: step 984, loss 0.160002, acc 0.90625, learning_rate 0.000187919
2017-10-10T14:43:05.598807: step 985, loss 0.0682686, acc 0.984375, learning_rate 0.00018756
2017-10-10T14:43:05.881604: step 986, loss 0.148175, acc 0.96875, learning_rate 0.000187202
2017-10-10T14:43:06.119908: step 987, loss 0.141645, acc 0.953125, learning_rate 0.000186846
2017-10-10T14:43:06.339541: step 988, loss 0.161907, acc 0.96875, learning_rate 0.000186492
2017-10-10T14:43:06.620343: step 989, loss 0.0712599, acc 0.984375, learning_rate 0.000186139
2017-10-10T14:43:06.909134: step 990, loss 0.166099, acc 0.90625, learning_rate 0.000185787
2017-10-10T14:43:07.223328: step 991, loss 0.110921, acc 0.96875, learning_rate 0.000185437
2017-10-10T14:43:07.536489: step 992, loss 0.0871819, acc 0.984375, learning_rate 0.000185088
2017-10-10T14:43:07.821417: step 993, loss 0.102806, acc 0.96875, learning_rate 0.000184741
2017-10-10T14:43:08.137270: step 994, loss 0.135841, acc 0.953125, learning_rate 0.000184395
2017-10-10T14:43:08.493215: step 995, loss 0.211466, acc 0.953125, learning_rate 0.000184051
2017-10-10T14:43:08.756856: step 996, loss 0.136351, acc 0.9375, learning_rate 0.000183708
2017-10-10T14:43:09.055305: step 997, loss 0.129141, acc 0.953125, learning_rate 0.000183366
2017-10-10T14:43:09.335681: step 998, loss 0.149419, acc 0.921875, learning_rate 0.000183026
2017-10-10T14:43:09.652167: step 999, loss 0.0778315, acc 1, learning_rate 0.000182687
2017-10-10T14:43:09.961422: step 1000, loss 0.139802, acc 0.9375, learning_rate 0.000182349

Evaluation:
2017-10-10T14:43:10.616361: step 1000, loss 0.233962, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1000

2017-10-10T14:43:11.773606: step 1001, loss 0.119191, acc 0.953125, learning_rate 0.000182013
2017-10-10T14:43:12.048906: step 1002, loss 0.155346, acc 0.9375, learning_rate 0.000181678
2017-10-10T14:43:12.369516: step 1003, loss 0.0863298, acc 0.984375, learning_rate 0.000181345
2017-10-10T14:43:12.672450: step 1004, loss 0.216458, acc 0.9375, learning_rate 0.000181013
2017-10-10T14:43:12.972951: step 1005, loss 0.115017, acc 0.96875, learning_rate 0.000180682
2017-10-10T14:43:13.304109: step 1006, loss 0.119314, acc 0.9375, learning_rate 0.000180353
2017-10-10T14:43:13.592394: step 1007, loss 0.141484, acc 0.953125, learning_rate 0.000180025
2017-10-10T14:43:13.917399: step 1008, loss 0.162888, acc 0.921875, learning_rate 0.000179698
2017-10-10T14:43:14.216075: step 1009, loss 0.0851655, acc 0.984375, learning_rate 0.000179373
2017-10-10T14:43:14.544827: step 1010, loss 0.186426, acc 0.953125, learning_rate 0.000179049
2017-10-10T14:43:14.844481: step 1011, loss 0.0833621, acc 0.96875, learning_rate 0.000178726
2017-10-10T14:43:15.176698: step 1012, loss 0.0536313, acc 0.984375, learning_rate 0.000178405
2017-10-10T14:43:15.496868: step 1013, loss 0.186834, acc 0.921875, learning_rate 0.000178085
2017-10-10T14:43:15.836947: step 1014, loss 0.0550221, acc 1, learning_rate 0.000177766
2017-10-10T14:43:16.168709: step 1015, loss 0.200008, acc 0.90625, learning_rate 0.000177449
2017-10-10T14:43:16.439091: step 1016, loss 0.107545, acc 0.96875, learning_rate 0.000177133
2017-10-10T14:43:16.764456: step 1017, loss 0.0557832, acc 0.984375, learning_rate 0.000176818
2017-10-10T14:43:17.107884: step 1018, loss 0.143821, acc 0.96875, learning_rate 0.000176504
2017-10-10T14:43:17.428830: step 1019, loss 0.158376, acc 0.921875, learning_rate 0.000176192
2017-10-10T14:43:17.737396: step 1020, loss 0.14754, acc 0.96875, learning_rate 0.000175881
2017-10-10T14:43:17.960769: step 1021, loss 0.0721439, acc 0.984375, learning_rate 0.000175571
2017-10-10T14:43:18.225230: step 1022, loss 0.182565, acc 0.90625, learning_rate 0.000175263
2017-10-10T14:43:18.526915: step 1023, loss 0.0482761, acc 0.984375, learning_rate 0.000174956
2017-10-10T14:43:18.867772: step 1024, loss 0.0481364, acc 1, learning_rate 0.00017465
2017-10-10T14:43:19.224836: step 1025, loss 0.111633, acc 0.953125, learning_rate 0.000174345
2017-10-10T14:43:19.532969: step 1026, loss 0.0988169, acc 0.96875, learning_rate 0.000174042
2017-10-10T14:43:19.865502: step 1027, loss 0.0777224, acc 0.984375, learning_rate 0.000173739
2017-10-10T14:43:20.210739: step 1028, loss 0.0683859, acc 0.96875, learning_rate 0.000173438
2017-10-10T14:43:20.522249: step 1029, loss 0.0730722, acc 0.96875, learning_rate 0.000173139
2017-10-10T14:43:20.879102: step 1030, loss 0.196051, acc 0.921875, learning_rate 0.00017284
2017-10-10T14:43:21.236846: step 1031, loss 0.158177, acc 0.921875, learning_rate 0.000172543
2017-10-10T14:43:21.541524: step 1032, loss 0.195403, acc 0.9375, learning_rate 0.000172247
2017-10-10T14:43:21.870531: step 1033, loss 0.135422, acc 0.9375, learning_rate 0.000171952
2017-10-10T14:43:22.228081: step 1034, loss 0.0913358, acc 0.96875, learning_rate 0.000171658
2017-10-10T14:43:22.552944: step 1035, loss 0.219135, acc 0.921875, learning_rate 0.000171366
2017-10-10T14:43:22.892934: step 1036, loss 0.218821, acc 0.953125, learning_rate 0.000171074
2017-10-10T14:43:23.260849: step 1037, loss 0.130134, acc 0.953125, learning_rate 0.000170784
2017-10-10T14:43:23.611898: step 1038, loss 0.149514, acc 0.9375, learning_rate 0.000170495
2017-10-10T14:43:23.943942: step 1039, loss 0.153931, acc 0.9375, learning_rate 0.000170208
2017-10-10T14:43:24.260733: step 1040, loss 0.156317, acc 0.953125, learning_rate 0.000169921

Evaluation:
2017-10-10T14:43:25.084942: step 1040, loss 0.232483, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1040

2017-10-10T14:43:26.088228: step 1041, loss 0.116782, acc 0.9375, learning_rate 0.000169636
2017-10-10T14:43:26.428229: step 1042, loss 0.0905595, acc 0.984375, learning_rate 0.000169351
2017-10-10T14:43:26.761389: step 1043, loss 0.113002, acc 0.96875, learning_rate 0.000169068
2017-10-10T14:43:27.106931: step 1044, loss 0.142966, acc 0.96875, learning_rate 0.000168786
2017-10-10T14:43:27.419467: step 1045, loss 0.101359, acc 0.984375, learning_rate 0.000168506
2017-10-10T14:43:27.705491: step 1046, loss 0.0827966, acc 0.96875, learning_rate 0.000168226
2017-10-10T14:43:28.124139: step 1047, loss 0.102054, acc 0.953125, learning_rate 0.000167947
2017-10-10T14:43:28.458851: step 1048, loss 0.0869543, acc 1, learning_rate 0.00016767
2017-10-10T14:43:28.782207: step 1049, loss 0.20251, acc 0.953125, learning_rate 0.000167394
2017-10-10T14:43:29.086777: step 1050, loss 0.0940444, acc 0.96875, learning_rate 0.000167119
2017-10-10T14:43:29.444874: step 1051, loss 0.130615, acc 0.9375, learning_rate 0.000166845
2017-10-10T14:43:29.788890: step 1052, loss 0.182993, acc 0.90625, learning_rate 0.000166572
2017-10-10T14:43:30.124216: step 1053, loss 0.132449, acc 0.9375, learning_rate 0.0001663
2017-10-10T14:43:30.541203: step 1054, loss 0.114998, acc 0.953125, learning_rate 0.00016603
2017-10-10T14:43:30.910626: step 1055, loss 0.172412, acc 0.921875, learning_rate 0.00016576
2017-10-10T14:43:31.247913: step 1056, loss 0.132467, acc 0.96875, learning_rate 0.000165492
2017-10-10T14:43:31.526290: step 1057, loss 0.0889766, acc 0.984375, learning_rate 0.000165224
2017-10-10T14:43:31.954429: step 1058, loss 0.201905, acc 0.90625, learning_rate 0.000164958
2017-10-10T14:43:32.363227: step 1059, loss 0.211127, acc 0.9375, learning_rate 0.000164693
2017-10-10T14:43:32.727901: step 1060, loss 0.170476, acc 0.9375, learning_rate 0.000164429
2017-10-10T14:43:33.170693: step 1061, loss 0.181002, acc 0.9375, learning_rate 0.000164166
2017-10-10T14:43:33.509053: step 1062, loss 0.0961501, acc 0.9375, learning_rate 0.000163904
2017-10-10T14:43:33.806890: step 1063, loss 0.196021, acc 0.921875, learning_rate 0.000163643
2017-10-10T14:43:34.212833: step 1064, loss 0.151061, acc 0.953125, learning_rate 0.000163383
2017-10-10T14:43:34.605881: step 1065, loss 0.0694704, acc 0.984375, learning_rate 0.000163125
2017-10-10T14:43:34.988890: step 1066, loss 0.124323, acc 0.96875, learning_rate 0.000162867
2017-10-10T14:43:35.332826: step 1067, loss 0.0949278, acc 0.984375, learning_rate 0.00016261
2017-10-10T14:43:35.660636: step 1068, loss 0.125444, acc 0.953125, learning_rate 0.000162355
2017-10-10T14:43:36.046190: step 1069, loss 0.101948, acc 0.96875, learning_rate 0.0001621
2017-10-10T14:43:36.424897: step 1070, loss 0.162283, acc 0.953125, learning_rate 0.000161847
2017-10-10T14:43:36.824883: step 1071, loss 0.164612, acc 0.953125, learning_rate 0.000161594
2017-10-10T14:43:37.185161: step 1072, loss 0.145277, acc 0.953125, learning_rate 0.000161343
2017-10-10T14:43:37.586155: step 1073, loss 0.135452, acc 0.9375, learning_rate 0.000161093
2017-10-10T14:43:37.964879: step 1074, loss 0.112948, acc 0.9375, learning_rate 0.000160843
2017-10-10T14:43:38.361494: step 1075, loss 0.205769, acc 0.9375, learning_rate 0.000160595
2017-10-10T14:43:38.698178: step 1076, loss 0.121535, acc 0.953125, learning_rate 0.000160348
2017-10-10T14:43:39.043676: step 1077, loss 0.0826388, acc 0.96875, learning_rate 0.000160101
2017-10-10T14:43:39.374583: step 1078, loss 0.180928, acc 0.941176, learning_rate 0.000159856
2017-10-10T14:43:39.758556: step 1079, loss 0.0885562, acc 0.96875, learning_rate 0.000159612
2017-10-10T14:43:40.120344: step 1080, loss 0.131723, acc 0.90625, learning_rate 0.000159368

Evaluation:
2017-10-10T14:43:41.183731: step 1080, loss 0.230621, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1080

2017-10-10T14:43:42.813245: step 1081, loss 0.117625, acc 0.96875, learning_rate 0.000159126
2017-10-10T14:43:43.207214: step 1082, loss 0.150794, acc 0.9375, learning_rate 0.000158885
2017-10-10T14:43:43.585556: step 1083, loss 0.146444, acc 0.953125, learning_rate 0.000158644
2017-10-10T14:43:43.996876: step 1084, loss 0.124039, acc 0.921875, learning_rate 0.000158405
2017-10-10T14:43:44.318255: step 1085, loss 0.0795839, acc 0.96875, learning_rate 0.000158167
2017-10-10T14:43:44.731394: step 1086, loss 0.052446, acc 0.984375, learning_rate 0.000157929
2017-10-10T14:43:45.086603: step 1087, loss 0.093054, acc 0.96875, learning_rate 0.000157693
2017-10-10T14:43:45.478060: step 1088, loss 0.144112, acc 0.96875, learning_rate 0.000157457
2017-10-10T14:43:45.786455: step 1089, loss 0.0682315, acc 0.96875, learning_rate 0.000157223
2017-10-10T14:43:46.120822: step 1090, loss 0.164349, acc 0.890625, learning_rate 0.000156989
2017-10-10T14:43:46.469992: step 1091, loss 0.150151, acc 0.9375, learning_rate 0.000156757
2017-10-10T14:43:46.828355: step 1092, loss 0.125657, acc 0.953125, learning_rate 0.000156525
2017-10-10T14:43:47.144917: step 1093, loss 0.0822531, acc 0.953125, learning_rate 0.000156294
2017-10-10T14:43:47.579256: step 1094, loss 0.209416, acc 0.921875, learning_rate 0.000156064
2017-10-10T14:43:47.931308: step 1095, loss 0.147901, acc 0.96875, learning_rate 0.000155836
2017-10-10T14:43:48.304838: step 1096, loss 0.323717, acc 0.90625, learning_rate 0.000155608
2017-10-10T14:43:48.668516: step 1097, loss 0.126806, acc 0.9375, learning_rate 0.000155381
2017-10-10T14:43:49.027325: step 1098, loss 0.263801, acc 0.921875, learning_rate 0.000155155
2017-10-10T14:43:49.414207: step 1099, loss 0.0675956, acc 0.984375, learning_rate 0.000154929
2017-10-10T14:43:49.858557: step 1100, loss 0.250185, acc 0.921875, learning_rate 0.000154705
2017-10-10T14:43:50.297176: step 1101, loss 0.169974, acc 0.921875, learning_rate 0.000154482
2017-10-10T14:43:50.684882: step 1102, loss 0.0872635, acc 0.984375, learning_rate 0.00015426
2017-10-10T14:43:51.100995: step 1103, loss 0.293443, acc 0.890625, learning_rate 0.000154038
2017-10-10T14:43:51.492572: step 1104, loss 0.142331, acc 0.9375, learning_rate 0.000153818
2017-10-10T14:43:51.902799: step 1105, loss 0.0897415, acc 0.96875, learning_rate 0.000153598
2017-10-10T14:43:52.258177: step 1106, loss 0.236068, acc 0.875, learning_rate 0.000153379
2017-10-10T14:43:52.649296: step 1107, loss 0.209465, acc 0.9375, learning_rate 0.000153161
2017-10-10T14:43:53.184447: step 1108, loss 0.152414, acc 0.9375, learning_rate 0.000152944
2017-10-10T14:43:53.525792: step 1109, loss 0.0796314, acc 0.96875, learning_rate 0.000152728
2017-10-10T14:43:53.935742: step 1110, loss 0.121296, acc 0.9375, learning_rate 0.000152513
2017-10-10T14:43:54.388829: step 1111, loss 0.121714, acc 0.9375, learning_rate 0.000152299
2017-10-10T14:43:54.785974: step 1112, loss 0.171541, acc 0.9375, learning_rate 0.000152085
2017-10-10T14:43:55.210453: step 1113, loss 0.0894641, acc 0.96875, learning_rate 0.000151872
2017-10-10T14:43:55.628889: step 1114, loss 0.36372, acc 0.90625, learning_rate 0.000151661
2017-10-10T14:43:55.996897: step 1115, loss 0.134801, acc 0.953125, learning_rate 0.00015145
2017-10-10T14:43:56.463253: step 1116, loss 0.130516, acc 0.984375, learning_rate 0.00015124
2017-10-10T14:43:56.953433: step 1117, loss 0.141356, acc 0.9375, learning_rate 0.000151031
2017-10-10T14:43:57.387195: step 1118, loss 0.0938549, acc 0.96875, learning_rate 0.000150822
2017-10-10T14:43:57.860932: step 1119, loss 0.178077, acc 0.921875, learning_rate 0.000150615
2017-10-10T14:43:58.332852: step 1120, loss 0.106102, acc 0.96875, learning_rate 0.000150408

Evaluation:
2017-10-10T14:43:59.234799: step 1120, loss 0.232459, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1120

2017-10-10T14:44:00.556932: step 1121, loss 0.120993, acc 0.9375, learning_rate 0.000150203
2017-10-10T14:44:00.856895: step 1122, loss 0.0522692, acc 1, learning_rate 0.000149998
2017-10-10T14:44:01.108859: step 1123, loss 0.133177, acc 0.953125, learning_rate 0.000149794
2017-10-10T14:44:01.440605: step 1124, loss 0.127415, acc 0.984375, learning_rate 0.00014959
2017-10-10T14:44:01.882521: step 1125, loss 0.187611, acc 0.9375, learning_rate 0.000149388
2017-10-10T14:44:02.344845: step 1126, loss 0.0544126, acc 0.984375, learning_rate 0.000149186
2017-10-10T14:44:02.726219: step 1127, loss 0.0800523, acc 0.96875, learning_rate 0.000148986
2017-10-10T14:44:03.048736: step 1128, loss 0.13211, acc 0.984375, learning_rate 0.000148786
2017-10-10T14:44:03.516910: step 1129, loss 0.174511, acc 0.921875, learning_rate 0.000148587
2017-10-10T14:44:03.938437: step 1130, loss 0.0946124, acc 0.96875, learning_rate 0.000148388
2017-10-10T14:44:04.373715: step 1131, loss 0.22682, acc 0.90625, learning_rate 0.000148191
2017-10-10T14:44:04.789029: step 1132, loss 0.186605, acc 0.921875, learning_rate 0.000147994
2017-10-10T14:44:05.249502: step 1133, loss 0.147796, acc 0.953125, learning_rate 0.000147798
2017-10-10T14:44:05.681192: step 1134, loss 0.222835, acc 0.921875, learning_rate 0.000147603
2017-10-10T14:44:06.028390: step 1135, loss 0.130984, acc 0.953125, learning_rate 0.000147409
2017-10-10T14:44:06.460108: step 1136, loss 0.155925, acc 0.921875, learning_rate 0.000147215
2017-10-10T14:44:06.807705: step 1137, loss 0.163627, acc 0.921875, learning_rate 0.000147022
2017-10-10T14:44:07.274407: step 1138, loss 0.0775499, acc 0.984375, learning_rate 0.000146831
2017-10-10T14:44:07.690987: step 1139, loss 0.128574, acc 0.953125, learning_rate 0.000146639
2017-10-10T14:44:08.108935: step 1140, loss 0.100982, acc 0.984375, learning_rate 0.000146449
2017-10-10T14:44:08.506102: step 1141, loss 0.215731, acc 0.9375, learning_rate 0.000146259
2017-10-10T14:44:08.908628: step 1142, loss 0.108568, acc 0.984375, learning_rate 0.000146071
2017-10-10T14:44:09.314866: step 1143, loss 0.0892144, acc 0.984375, learning_rate 0.000145883
2017-10-10T14:44:09.760933: step 1144, loss 0.0725192, acc 0.96875, learning_rate 0.000145695
2017-10-10T14:44:10.129515: step 1145, loss 0.311302, acc 0.9375, learning_rate 0.000145509
2017-10-10T14:44:10.548981: step 1146, loss 0.236944, acc 0.9375, learning_rate 0.000145323
2017-10-10T14:44:10.945876: step 1147, loss 0.140527, acc 0.953125, learning_rate 0.000145138
2017-10-10T14:44:11.368873: step 1148, loss 0.178214, acc 0.953125, learning_rate 0.000144954
2017-10-10T14:44:11.788911: step 1149, loss 0.191757, acc 0.953125, learning_rate 0.00014477
2017-10-10T14:44:12.208791: step 1150, loss 0.082121, acc 0.984375, learning_rate 0.000144588
2017-10-10T14:44:12.628354: step 1151, loss 0.0832965, acc 0.984375, learning_rate 0.000144406
2017-10-10T14:44:13.047084: step 1152, loss 0.153147, acc 0.9375, learning_rate 0.000144224
2017-10-10T14:44:13.472920: step 1153, loss 0.0768372, acc 0.96875, learning_rate 0.000144044
2017-10-10T14:44:13.999927: step 1154, loss 0.115644, acc 0.984375, learning_rate 0.000143864
2017-10-10T14:44:14.473021: step 1155, loss 0.0823696, acc 0.984375, learning_rate 0.000143685
2017-10-10T14:44:14.840418: step 1156, loss 0.164747, acc 0.96875, learning_rate 0.000143507
2017-10-10T14:44:15.252118: step 1157, loss 0.0995188, acc 0.984375, learning_rate 0.000143329
2017-10-10T14:44:15.685621: step 1158, loss 0.0674413, acc 0.984375, learning_rate 0.000143152
2017-10-10T14:44:16.085049: step 1159, loss 0.0873458, acc 0.984375, learning_rate 0.000142976
2017-10-10T14:44:16.484100: step 1160, loss 0.0480356, acc 1, learning_rate 0.000142801

Evaluation:
2017-10-10T14:44:17.478103: step 1160, loss 0.232331, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1160

2017-10-10T14:44:19.139085: step 1161, loss 0.162064, acc 0.9375, learning_rate 0.000142626
2017-10-10T14:44:19.481447: step 1162, loss 0.0829906, acc 0.96875, learning_rate 0.000142452
2017-10-10T14:44:19.818785: step 1163, loss 0.181763, acc 0.90625, learning_rate 0.000142279
2017-10-10T14:44:20.171262: step 1164, loss 0.11056, acc 0.96875, learning_rate 0.000142106
2017-10-10T14:44:20.565210: step 1165, loss 0.115651, acc 0.96875, learning_rate 0.000141934
2017-10-10T14:44:20.968135: step 1166, loss 0.118334, acc 0.96875, learning_rate 0.000141763
2017-10-10T14:44:21.395170: step 1167, loss 0.078854, acc 0.984375, learning_rate 0.000141593
2017-10-10T14:44:21.816084: step 1168, loss 0.113558, acc 0.9375, learning_rate 0.000141423
2017-10-10T14:44:22.222435: step 1169, loss 0.1048, acc 0.96875, learning_rate 0.000141254
2017-10-10T14:44:22.558841: step 1170, loss 0.0846796, acc 0.984375, learning_rate 0.000141085
2017-10-10T14:44:22.950080: step 1171, loss 0.0829409, acc 0.96875, learning_rate 0.000140918
2017-10-10T14:44:23.388801: step 1172, loss 0.0969996, acc 0.96875, learning_rate 0.000140751
2017-10-10T14:44:23.766792: step 1173, loss 0.0939987, acc 0.984375, learning_rate 0.000140584
2017-10-10T14:44:24.179485: step 1174, loss 0.211183, acc 0.9375, learning_rate 0.000140419
2017-10-10T14:44:24.578162: step 1175, loss 0.131569, acc 0.9375, learning_rate 0.000140254
2017-10-10T14:44:24.929910: step 1176, loss 0.042782, acc 1, learning_rate 0.000140089
2017-10-10T14:44:25.385781: step 1177, loss 0.0791925, acc 0.984375, learning_rate 0.000139926
2017-10-10T14:44:25.823354: step 1178, loss 0.133665, acc 0.96875, learning_rate 0.000139763
2017-10-10T14:44:26.280999: step 1179, loss 0.0531903, acc 0.984375, learning_rate 0.0001396
2017-10-10T14:44:26.763629: step 1180, loss 0.160061, acc 0.953125, learning_rate 0.000139439
2017-10-10T14:44:27.158665: step 1181, loss 0.0681441, acc 0.984375, learning_rate 0.000139278
2017-10-10T14:44:27.504826: step 1182, loss 0.219943, acc 0.921875, learning_rate 0.000139118
2017-10-10T14:44:27.913125: step 1183, loss 0.099358, acc 0.953125, learning_rate 0.000138958
2017-10-10T14:44:28.313011: step 1184, loss 0.0614786, acc 0.984375, learning_rate 0.000138799
2017-10-10T14:44:28.781054: step 1185, loss 0.171408, acc 0.9375, learning_rate 0.00013864
2017-10-10T14:44:29.222940: step 1186, loss 0.0826596, acc 0.984375, learning_rate 0.000138483
2017-10-10T14:44:29.596984: step 1187, loss 0.192117, acc 0.9375, learning_rate 0.000138326
2017-10-10T14:44:30.106769: step 1188, loss 0.210255, acc 0.921875, learning_rate 0.000138169
2017-10-10T14:44:30.441611: step 1189, loss 0.140421, acc 0.9375, learning_rate 0.000138013
2017-10-10T14:44:30.830631: step 1190, loss 0.0714516, acc 0.984375, learning_rate 0.000137858
2017-10-10T14:44:31.252965: step 1191, loss 0.0952288, acc 0.96875, learning_rate 0.000137704
2017-10-10T14:44:31.641767: step 1192, loss 0.216809, acc 0.9375, learning_rate 0.00013755
2017-10-10T14:44:32.056171: step 1193, loss 0.186141, acc 0.953125, learning_rate 0.000137397
2017-10-10T14:44:32.407465: step 1194, loss 0.0998544, acc 0.953125, learning_rate 0.000137244
2017-10-10T14:44:32.825016: step 1195, loss 0.132248, acc 0.953125, learning_rate 0.000137092
2017-10-10T14:44:33.230497: step 1196, loss 0.220991, acc 0.9375, learning_rate 0.000136941
2017-10-10T14:44:33.641600: step 1197, loss 0.147993, acc 0.9375, learning_rate 0.00013679
2017-10-10T14:44:34.076946: step 1198, loss 0.214409, acc 0.953125, learning_rate 0.00013664
2017-10-10T14:44:34.516931: step 1199, loss 0.123068, acc 0.96875, learning_rate 0.00013649
2017-10-10T14:44:34.919763: step 1200, loss 0.104879, acc 0.984375, learning_rate 0.000136341

Evaluation:
2017-10-10T14:44:35.909913: step 1200, loss 0.230001, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1200

2017-10-10T14:44:37.006492: step 1201, loss 0.0988682, acc 0.953125, learning_rate 0.000136193
2017-10-10T14:44:37.455635: step 1202, loss 0.132673, acc 0.953125, learning_rate 0.000136045
2017-10-10T14:44:37.864867: step 1203, loss 0.0967036, acc 0.96875, learning_rate 0.000135898
2017-10-10T14:44:38.257002: step 1204, loss 0.172879, acc 0.9375, learning_rate 0.000135751
2017-10-10T14:44:38.627905: step 1205, loss 0.155075, acc 0.921875, learning_rate 0.000135605
2017-10-10T14:44:39.058833: step 1206, loss 0.0411914, acc 1, learning_rate 0.00013546
2017-10-10T14:44:39.464964: step 1207, loss 0.0810618, acc 0.984375, learning_rate 0.000135315
2017-10-10T14:44:39.857510: step 1208, loss 0.187403, acc 0.9375, learning_rate 0.000135171
2017-10-10T14:44:40.323019: step 1209, loss 0.118452, acc 0.953125, learning_rate 0.000135028
2017-10-10T14:44:40.705835: step 1210, loss 0.103967, acc 0.984375, learning_rate 0.000134885
2017-10-10T14:44:41.020922: step 1211, loss 0.139732, acc 0.96875, learning_rate 0.000134742
2017-10-10T14:44:41.468879: step 1212, loss 0.103651, acc 0.96875, learning_rate 0.0001346
2017-10-10T14:44:41.888999: step 1213, loss 0.106474, acc 0.953125, learning_rate 0.000134459
2017-10-10T14:44:42.274859: step 1214, loss 0.197136, acc 0.9375, learning_rate 0.000134319
2017-10-10T14:44:42.616007: step 1215, loss 0.197005, acc 0.90625, learning_rate 0.000134178
2017-10-10T14:44:43.028821: step 1216, loss 0.0656888, acc 0.984375, learning_rate 0.000134039
2017-10-10T14:44:43.461427: step 1217, loss 0.140989, acc 0.953125, learning_rate 0.0001339
2017-10-10T14:44:43.833012: step 1218, loss 0.180694, acc 0.921875, learning_rate 0.000133762
2017-10-10T14:44:44.253087: step 1219, loss 0.0924418, acc 0.96875, learning_rate 0.000133624
2017-10-10T14:44:44.647296: step 1220, loss 0.0922695, acc 0.96875, learning_rate 0.000133487
2017-10-10T14:44:45.082518: step 1221, loss 0.10951, acc 0.953125, learning_rate 0.00013335
2017-10-10T14:44:45.436819: step 1222, loss 0.0724345, acc 0.984375, learning_rate 0.000133214
2017-10-10T14:44:45.848626: step 1223, loss 0.116804, acc 0.96875, learning_rate 0.000133078
2017-10-10T14:44:46.224399: step 1224, loss 0.0582071, acc 0.984375, learning_rate 0.000132943
2017-10-10T14:44:46.652182: step 1225, loss 0.133465, acc 0.953125, learning_rate 0.000132809
2017-10-10T14:44:47.085700: step 1226, loss 0.0585426, acc 0.984375, learning_rate 0.000132675
2017-10-10T14:44:47.516870: step 1227, loss 0.116202, acc 0.984375, learning_rate 0.000132541
2017-10-10T14:44:47.939698: step 1228, loss 0.0656849, acc 0.96875, learning_rate 0.000132409
2017-10-10T14:44:48.336892: step 1229, loss 0.0896368, acc 0.96875, learning_rate 0.000132276
2017-10-10T14:44:48.741490: step 1230, loss 0.168999, acc 0.921875, learning_rate 0.000132145
2017-10-10T14:44:49.147451: step 1231, loss 0.0606369, acc 0.96875, learning_rate 0.000132013
2017-10-10T14:44:49.544878: step 1232, loss 0.123096, acc 0.953125, learning_rate 0.000131883
2017-10-10T14:44:49.953251: step 1233, loss 0.185378, acc 0.921875, learning_rate 0.000131753
2017-10-10T14:44:50.363678: step 1234, loss 0.0348993, acc 1, learning_rate 0.000131623
2017-10-10T14:44:50.761167: step 1235, loss 0.12214, acc 0.96875, learning_rate 0.000131494
2017-10-10T14:44:51.241963: step 1236, loss 0.119014, acc 0.96875, learning_rate 0.000131365
2017-10-10T14:44:51.647933: step 1237, loss 0.167634, acc 0.953125, learning_rate 0.000131237
2017-10-10T14:44:51.988824: step 1238, loss 0.133697, acc 0.96875, learning_rate 0.00013111
2017-10-10T14:44:52.318143: step 1239, loss 0.154373, acc 0.9375, learning_rate 0.000130983
2017-10-10T14:44:52.856878: step 1240, loss 0.0862735, acc 0.984375, learning_rate 0.000130856

Evaluation:
2017-10-10T14:44:53.617096: step 1240, loss 0.229285, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1240

2017-10-10T14:44:55.129031: step 1241, loss 0.136039, acc 0.953125, learning_rate 0.00013073
2017-10-10T14:44:55.509020: step 1242, loss 0.199254, acc 0.96875, learning_rate 0.000130605
2017-10-10T14:44:55.904902: step 1243, loss 0.081611, acc 0.984375, learning_rate 0.00013048
2017-10-10T14:44:56.331583: step 1244, loss 0.133505, acc 0.9375, learning_rate 0.000130356
2017-10-10T14:44:56.730507: step 1245, loss 0.204146, acc 0.90625, learning_rate 0.000130232
2017-10-10T14:44:57.165033: step 1246, loss 0.117368, acc 0.96875, learning_rate 0.000130108
2017-10-10T14:44:57.601176: step 1247, loss 0.0808812, acc 0.984375, learning_rate 0.000129985
2017-10-10T14:44:58.022459: step 1248, loss 0.178184, acc 0.9375, learning_rate 0.000129863
2017-10-10T14:44:58.434232: step 1249, loss 0.0497368, acc 0.984375, learning_rate 0.000129741
2017-10-10T14:44:58.900987: step 1250, loss 0.0498422, acc 1, learning_rate 0.00012962
2017-10-10T14:44:59.296305: step 1251, loss 0.127294, acc 0.953125, learning_rate 0.000129499
2017-10-10T14:44:59.751756: step 1252, loss 0.158347, acc 0.9375, learning_rate 0.000129378
2017-10-10T14:45:00.151566: step 1253, loss 0.0374859, acc 1, learning_rate 0.000129259
2017-10-10T14:45:00.536084: step 1254, loss 0.0829377, acc 0.984375, learning_rate 0.000129139
2017-10-10T14:45:00.957413: step 1255, loss 0.0214619, acc 1, learning_rate 0.00012902
2017-10-10T14:45:01.407313: step 1256, loss 0.0833469, acc 0.96875, learning_rate 0.000128902
2017-10-10T14:45:01.888903: step 1257, loss 0.10173, acc 0.96875, learning_rate 0.000128784
2017-10-10T14:45:02.356826: step 1258, loss 0.0866298, acc 0.96875, learning_rate 0.000128666
2017-10-10T14:45:02.717008: step 1259, loss 0.122605, acc 0.96875, learning_rate 0.000128549
2017-10-10T14:45:03.115629: step 1260, loss 0.127797, acc 0.96875, learning_rate 0.000128433
2017-10-10T14:45:03.504863: step 1261, loss 0.0546961, acc 1, learning_rate 0.000128317
2017-10-10T14:45:04.001118: step 1262, loss 0.0795733, acc 0.984375, learning_rate 0.000128201
2017-10-10T14:45:04.416872: step 1263, loss 0.0984798, acc 0.953125, learning_rate 0.000128086
2017-10-10T14:45:04.856872: step 1264, loss 0.110887, acc 0.96875, learning_rate 0.000127971
2017-10-10T14:45:05.243038: step 1265, loss 0.15498, acc 0.953125, learning_rate 0.000127857
2017-10-10T14:45:05.642230: step 1266, loss 0.362015, acc 0.890625, learning_rate 0.000127743
2017-10-10T14:45:06.058986: step 1267, loss 0.0595202, acc 0.984375, learning_rate 0.00012763
2017-10-10T14:45:06.532930: step 1268, loss 0.094443, acc 0.96875, learning_rate 0.000127517
2017-10-10T14:45:07.008855: step 1269, loss 0.118527, acc 0.96875, learning_rate 0.000127405
2017-10-10T14:45:07.427490: step 1270, loss 0.12005, acc 0.96875, learning_rate 0.000127293
2017-10-10T14:45:07.801073: step 1271, loss 0.15791, acc 0.921875, learning_rate 0.000127182
2017-10-10T14:45:08.238213: step 1272, loss 0.17764, acc 0.921875, learning_rate 0.000127071
2017-10-10T14:45:08.652931: step 1273, loss 0.17002, acc 0.9375, learning_rate 0.00012696
2017-10-10T14:45:09.080868: step 1274, loss 0.131128, acc 0.980392, learning_rate 0.00012685
2017-10-10T14:45:09.512848: step 1275, loss 0.0826541, acc 0.96875, learning_rate 0.000126741
2017-10-10T14:45:09.880850: step 1276, loss 0.0606807, acc 0.953125, learning_rate 0.000126632
2017-10-10T14:45:10.211220: step 1277, loss 0.106311, acc 0.984375, learning_rate 0.000126523
2017-10-10T14:45:10.624271: step 1278, loss 0.137075, acc 0.96875, learning_rate 0.000126415
2017-10-10T14:45:11.083485: step 1279, loss 0.0911403, acc 0.96875, learning_rate 0.000126307
2017-10-10T14:45:11.512830: step 1280, loss 0.0260872, acc 1, learning_rate 0.000126199

Evaluation:
2017-10-10T14:45:12.269243: step 1280, loss 0.23203, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1280

2017-10-10T14:45:13.544613: step 1281, loss 0.0823723, acc 0.96875, learning_rate 0.000126093
2017-10-10T14:45:13.947181: step 1282, loss 0.0960757, acc 0.96875, learning_rate 0.000125986
2017-10-10T14:45:14.341898: step 1283, loss 0.109048, acc 0.96875, learning_rate 0.00012588
2017-10-10T14:45:14.772822: step 1284, loss 0.0714147, acc 0.96875, learning_rate 0.000125774
2017-10-10T14:45:15.169425: step 1285, loss 0.09292, acc 0.96875, learning_rate 0.000125669
2017-10-10T14:45:15.554040: step 1286, loss 0.175834, acc 0.921875, learning_rate 0.000125564
2017-10-10T14:45:15.981988: step 1287, loss 0.0346974, acc 0.984375, learning_rate 0.00012546
2017-10-10T14:45:16.368092: step 1288, loss 0.140222, acc 0.9375, learning_rate 0.000125356
2017-10-10T14:45:16.806881: step 1289, loss 0.178841, acc 0.921875, learning_rate 0.000125253
2017-10-10T14:45:17.198839: step 1290, loss 0.137715, acc 0.9375, learning_rate 0.00012515
2017-10-10T14:45:17.633179: step 1291, loss 0.216183, acc 0.953125, learning_rate 0.000125047
2017-10-10T14:45:18.073503: step 1292, loss 0.354395, acc 0.921875, learning_rate 0.000124945
2017-10-10T14:45:18.492009: step 1293, loss 0.17221, acc 0.921875, learning_rate 0.000124843
2017-10-10T14:45:18.808196: step 1294, loss 0.215453, acc 0.921875, learning_rate 0.000124741
2017-10-10T14:45:19.281033: step 1295, loss 0.179459, acc 0.921875, learning_rate 0.00012464
2017-10-10T14:45:19.695576: step 1296, loss 0.267651, acc 0.90625, learning_rate 0.00012454
2017-10-10T14:45:20.184911: step 1297, loss 0.196247, acc 0.953125, learning_rate 0.00012444
2017-10-10T14:45:20.590561: step 1298, loss 0.233017, acc 0.9375, learning_rate 0.00012434
2017-10-10T14:45:20.923489: step 1299, loss 0.0762119, acc 0.96875, learning_rate 0.000124241
2017-10-10T14:45:21.326716: step 1300, loss 0.0878736, acc 0.96875, learning_rate 0.000124142
2017-10-10T14:45:21.674913: step 1301, loss 0.0723015, acc 0.96875, learning_rate 0.000124043
2017-10-10T14:45:22.095264: step 1302, loss 0.118625, acc 0.953125, learning_rate 0.000123945
2017-10-10T14:45:22.485348: step 1303, loss 0.163579, acc 0.90625, learning_rate 0.000123847
2017-10-10T14:45:22.844612: step 1304, loss 0.139093, acc 0.953125, learning_rate 0.00012375
2017-10-10T14:45:23.318973: step 1305, loss 0.128259, acc 0.96875, learning_rate 0.000123653
2017-10-10T14:45:23.731850: step 1306, loss 0.0604717, acc 0.984375, learning_rate 0.000123556
2017-10-10T14:45:24.196077: step 1307, loss 0.131292, acc 0.9375, learning_rate 0.00012346
2017-10-10T14:45:24.632270: step 1308, loss 0.0619894, acc 1, learning_rate 0.000123364
2017-10-10T14:45:25.039961: step 1309, loss 0.036527, acc 1, learning_rate 0.000123269
2017-10-10T14:45:25.441877: step 1310, loss 0.122201, acc 0.96875, learning_rate 0.000123174
2017-10-10T14:45:25.953297: step 1311, loss 0.195824, acc 0.90625, learning_rate 0.00012308
2017-10-10T14:45:26.449157: step 1312, loss 0.122748, acc 0.953125, learning_rate 0.000122985
2017-10-10T14:45:26.831262: step 1313, loss 0.102639, acc 0.953125, learning_rate 0.000122892
2017-10-10T14:45:27.226809: step 1314, loss 0.141917, acc 0.96875, learning_rate 0.000122798
2017-10-10T14:45:27.667873: step 1315, loss 0.0510483, acc 1, learning_rate 0.000122705
2017-10-10T14:45:28.079462: step 1316, loss 0.170496, acc 0.921875, learning_rate 0.000122612
2017-10-10T14:45:28.500933: step 1317, loss 0.0823353, acc 0.984375, learning_rate 0.00012252
2017-10-10T14:45:28.933332: step 1318, loss 0.259597, acc 0.921875, learning_rate 0.000122428
2017-10-10T14:45:29.328845: step 1319, loss 0.217511, acc 0.921875, learning_rate 0.000122337
2017-10-10T14:45:29.811144: step 1320, loss 0.10107, acc 0.953125, learning_rate 0.000122245

Evaluation:
2017-10-10T14:45:30.794594: step 1320, loss 0.230281, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1320

2017-10-10T14:45:32.082771: step 1321, loss 0.165806, acc 0.953125, learning_rate 0.000122155
2017-10-10T14:45:32.557520: step 1322, loss 0.235897, acc 0.9375, learning_rate 0.000122064
2017-10-10T14:45:33.036545: step 1323, loss 0.152872, acc 0.96875, learning_rate 0.000121974
2017-10-10T14:45:33.508814: step 1324, loss 0.165865, acc 0.921875, learning_rate 0.000121884
2017-10-10T14:45:33.933114: step 1325, loss 0.107888, acc 0.96875, learning_rate 0.000121795
2017-10-10T14:45:34.318445: step 1326, loss 0.163156, acc 0.96875, learning_rate 0.000121706
2017-10-10T14:45:34.698254: step 1327, loss 0.0966538, acc 0.96875, learning_rate 0.000121618
2017-10-10T14:45:35.146841: step 1328, loss 0.100456, acc 0.9375, learning_rate 0.000121529
2017-10-10T14:45:35.500886: step 1329, loss 0.12425, acc 0.9375, learning_rate 0.000121441
2017-10-10T14:45:35.873996: step 1330, loss 0.068481, acc 0.96875, learning_rate 0.000121354
2017-10-10T14:45:36.329040: step 1331, loss 0.164035, acc 0.96875, learning_rate 0.000121267
2017-10-10T14:45:36.774810: step 1332, loss 0.120085, acc 0.953125, learning_rate 0.00012118
2017-10-10T14:45:37.178816: step 1333, loss 0.102624, acc 0.953125, learning_rate 0.000121093
2017-10-10T14:45:37.585146: step 1334, loss 0.0386725, acc 1, learning_rate 0.000121007
2017-10-10T14:45:38.021190: step 1335, loss 0.137537, acc 0.953125, learning_rate 0.000120922
2017-10-10T14:45:38.432601: step 1336, loss 0.138634, acc 0.953125, learning_rate 0.000120836
2017-10-10T14:45:38.824982: step 1337, loss 0.148167, acc 0.9375, learning_rate 0.000120751
2017-10-10T14:45:39.287558: step 1338, loss 0.158346, acc 0.96875, learning_rate 0.000120666
2017-10-10T14:45:39.696428: step 1339, loss 0.14422, acc 0.953125, learning_rate 0.000120582
2017-10-10T14:45:40.078669: step 1340, loss 0.204059, acc 0.921875, learning_rate 0.000120498
2017-10-10T14:45:40.520860: step 1341, loss 0.132613, acc 0.9375, learning_rate 0.000120414
2017-10-10T14:45:40.936936: step 1342, loss 0.145, acc 0.953125, learning_rate 0.000120331
2017-10-10T14:45:41.372911: step 1343, loss 0.093678, acc 0.96875, learning_rate 0.000120248
2017-10-10T14:45:41.833878: step 1344, loss 0.109508, acc 0.96875, learning_rate 0.000120165
2017-10-10T14:45:42.348882: step 1345, loss 0.0392031, acc 1, learning_rate 0.000120083
2017-10-10T14:45:42.869035: step 1346, loss 0.125456, acc 0.953125, learning_rate 0.000120001
2017-10-10T14:45:43.161130: step 1347, loss 0.0925196, acc 0.96875, learning_rate 0.00011992
2017-10-10T14:45:43.440994: step 1348, loss 0.195211, acc 0.921875, learning_rate 0.000119838
2017-10-10T14:45:43.743514: step 1349, loss 0.124992, acc 0.953125, learning_rate 0.000119757
2017-10-10T14:45:44.082256: step 1350, loss 0.176416, acc 0.921875, learning_rate 0.000119677
2017-10-10T14:45:44.471441: step 1351, loss 0.155864, acc 0.9375, learning_rate 0.000119596
2017-10-10T14:45:44.831482: step 1352, loss 0.180709, acc 0.921875, learning_rate 0.000119516
2017-10-10T14:45:45.248850: step 1353, loss 0.144349, acc 0.96875, learning_rate 0.000119437
2017-10-10T14:45:45.752895: step 1354, loss 0.148535, acc 0.96875, learning_rate 0.000119357
2017-10-10T14:45:46.180382: step 1355, loss 0.155786, acc 0.921875, learning_rate 0.000119278
2017-10-10T14:45:46.581101: step 1356, loss 0.0739126, acc 0.984375, learning_rate 0.0001192
2017-10-10T14:45:47.026850: step 1357, loss 0.110655, acc 0.953125, learning_rate 0.000119121
2017-10-10T14:45:47.391937: step 1358, loss 0.0866773, acc 0.953125, learning_rate 0.000119043
2017-10-10T14:45:47.790962: step 1359, loss 0.16401, acc 0.9375, learning_rate 0.000118965
2017-10-10T14:45:48.167435: step 1360, loss 0.127034, acc 0.984375, learning_rate 0.000118888

Evaluation:
2017-10-10T14:45:49.024942: step 1360, loss 0.230265, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1360

2017-10-10T14:45:50.201461: step 1361, loss 0.12999, acc 0.9375, learning_rate 0.000118811
2017-10-10T14:45:50.613145: step 1362, loss 0.0938876, acc 0.984375, learning_rate 0.000118734
2017-10-10T14:45:51.095761: step 1363, loss 0.111968, acc 0.96875, learning_rate 0.000118658
2017-10-10T14:45:51.489999: step 1364, loss 0.112903, acc 0.96875, learning_rate 0.000118582
2017-10-10T14:45:51.952874: step 1365, loss 0.128038, acc 0.953125, learning_rate 0.000118506
2017-10-10T14:45:52.359204: step 1366, loss 0.0819792, acc 0.984375, learning_rate 0.00011843
2017-10-10T14:45:52.734883: step 1367, loss 0.104197, acc 0.96875, learning_rate 0.000118355
2017-10-10T14:45:53.132974: step 1368, loss 0.110585, acc 0.96875, learning_rate 0.00011828
2017-10-10T14:45:53.528853: step 1369, loss 0.245382, acc 0.90625, learning_rate 0.000118205
2017-10-10T14:45:53.973502: step 1370, loss 0.091894, acc 0.953125, learning_rate 0.000118131
2017-10-10T14:45:54.365127: step 1371, loss 0.0833339, acc 0.96875, learning_rate 0.000118057
2017-10-10T14:45:54.639629: step 1372, loss 0.0475417, acc 1, learning_rate 0.000117983
2017-10-10T14:45:55.025996: step 1373, loss 0.106714, acc 0.96875, learning_rate 0.00011791
2017-10-10T14:45:55.465061: step 1374, loss 0.0489284, acc 0.984375, learning_rate 0.000117837
2017-10-10T14:45:55.830825: step 1375, loss 0.0528941, acc 1, learning_rate 0.000117764
2017-10-10T14:45:56.204924: step 1376, loss 0.113332, acc 0.96875, learning_rate 0.000117692
2017-10-10T14:45:56.685286: step 1377, loss 0.118359, acc 0.953125, learning_rate 0.000117619
2017-10-10T14:45:57.025866: step 1378, loss 0.129904, acc 0.953125, learning_rate 0.000117547
2017-10-10T14:45:57.416161: step 1379, loss 0.059951, acc 1, learning_rate 0.000117476
2017-10-10T14:45:57.772923: step 1380, loss 0.0852391, acc 0.96875, learning_rate 0.000117404
2017-10-10T14:45:58.185167: step 1381, loss 0.0841671, acc 0.96875, learning_rate 0.000117333
2017-10-10T14:45:58.609961: step 1382, loss 0.0612076, acc 1, learning_rate 0.000117263
2017-10-10T14:45:59.036997: step 1383, loss 0.0930087, acc 0.953125, learning_rate 0.000117192
2017-10-10T14:45:59.456883: step 1384, loss 0.0278546, acc 1, learning_rate 0.000117122
2017-10-10T14:45:59.944975: step 1385, loss 0.0959112, acc 0.96875, learning_rate 0.000117052
2017-10-10T14:46:00.346080: step 1386, loss 0.0612312, acc 0.984375, learning_rate 0.000116983
2017-10-10T14:46:00.675938: step 1387, loss 0.133601, acc 0.96875, learning_rate 0.000116913
2017-10-10T14:46:00.965741: step 1388, loss 0.0367621, acc 0.984375, learning_rate 0.000116844
2017-10-10T14:46:01.385869: step 1389, loss 0.0926473, acc 0.984375, learning_rate 0.000116775
2017-10-10T14:46:01.821090: step 1390, loss 0.17735, acc 0.921875, learning_rate 0.000116707
2017-10-10T14:46:02.215936: step 1391, loss 0.133669, acc 0.953125, learning_rate 0.000116639
2017-10-10T14:46:02.600993: step 1392, loss 0.210919, acc 0.921875, learning_rate 0.000116571
2017-10-10T14:46:03.032796: step 1393, loss 0.0674529, acc 1, learning_rate 0.000116503
2017-10-10T14:46:03.395919: step 1394, loss 0.291037, acc 0.90625, learning_rate 0.000116436
2017-10-10T14:46:03.802502: step 1395, loss 0.267163, acc 0.875, learning_rate 0.000116369
2017-10-10T14:46:04.199740: step 1396, loss 0.0937092, acc 0.96875, learning_rate 0.000116302
2017-10-10T14:46:04.626779: step 1397, loss 0.063098, acc 1, learning_rate 0.000116235
2017-10-10T14:46:05.027805: step 1398, loss 0.253905, acc 0.921875, learning_rate 0.000116169
2017-10-10T14:46:05.464949: step 1399, loss 0.0780356, acc 0.984375, learning_rate 0.000116103
2017-10-10T14:46:05.878809: step 1400, loss 0.0823513, acc 0.96875, learning_rate 0.000116037

Evaluation:
2017-10-10T14:46:06.765685: step 1400, loss 0.228712, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1400

2017-10-10T14:46:08.093306: step 1401, loss 0.139834, acc 0.9375, learning_rate 0.000115972
2017-10-10T14:46:08.445018: step 1402, loss 0.0713268, acc 1, learning_rate 0.000115907
2017-10-10T14:46:08.801023: step 1403, loss 0.139947, acc 0.953125, learning_rate 0.000115842
2017-10-10T14:46:09.273101: step 1404, loss 0.132279, acc 0.921875, learning_rate 0.000115777
2017-10-10T14:46:09.717569: step 1405, loss 0.0962038, acc 0.984375, learning_rate 0.000115713
2017-10-10T14:46:10.109123: step 1406, loss 0.104767, acc 0.921875, learning_rate 0.000115649
2017-10-10T14:46:10.517423: step 1407, loss 0.124378, acc 0.953125, learning_rate 0.000115585
2017-10-10T14:46:10.947053: step 1408, loss 0.156639, acc 0.953125, learning_rate 0.000115521
2017-10-10T14:46:11.359116: step 1409, loss 0.0703465, acc 0.953125, learning_rate 0.000115458
2017-10-10T14:46:11.718210: step 1410, loss 0.11524, acc 0.984375, learning_rate 0.000115395
2017-10-10T14:46:12.140858: step 1411, loss 0.160084, acc 0.90625, learning_rate 0.000115332
2017-10-10T14:46:12.644477: step 1412, loss 0.161918, acc 0.9375, learning_rate 0.000115269
2017-10-10T14:46:13.033117: step 1413, loss 0.0698465, acc 0.984375, learning_rate 0.000115207
2017-10-10T14:46:13.544977: step 1414, loss 0.0854017, acc 0.96875, learning_rate 0.000115145
2017-10-10T14:46:13.900306: step 1415, loss 0.0782509, acc 0.953125, learning_rate 0.000115083
2017-10-10T14:46:14.292871: step 1416, loss 0.114468, acc 0.953125, learning_rate 0.000115022
2017-10-10T14:46:14.688859: step 1417, loss 0.175424, acc 0.96875, learning_rate 0.00011496
2017-10-10T14:46:15.074805: step 1418, loss 0.0820237, acc 0.984375, learning_rate 0.000114899
2017-10-10T14:46:15.461768: step 1419, loss 0.10531, acc 0.96875, learning_rate 0.000114838
2017-10-10T14:46:15.897243: step 1420, loss 0.109326, acc 0.96875, learning_rate 0.000114778
2017-10-10T14:46:16.377616: step 1421, loss 0.105375, acc 0.953125, learning_rate 0.000114717
2017-10-10T14:46:16.887918: step 1422, loss 0.0892767, acc 0.953125, learning_rate 0.000114657
2017-10-10T14:46:17.329434: step 1423, loss 0.0568028, acc 0.984375, learning_rate 0.000114598
2017-10-10T14:46:17.662233: step 1424, loss 0.046909, acc 0.984375, learning_rate 0.000114538
2017-10-10T14:46:18.013797: step 1425, loss 0.154183, acc 0.9375, learning_rate 0.000114479
2017-10-10T14:46:18.420891: step 1426, loss 0.0690083, acc 0.984375, learning_rate 0.00011442
2017-10-10T14:46:18.784854: step 1427, loss 0.146331, acc 0.953125, learning_rate 0.000114361
2017-10-10T14:46:19.223152: step 1428, loss 0.0305384, acc 1, learning_rate 0.000114302
2017-10-10T14:46:19.708872: step 1429, loss 0.274018, acc 0.890625, learning_rate 0.000114244
2017-10-10T14:46:20.110532: step 1430, loss 0.0557409, acc 0.984375, learning_rate 0.000114186
2017-10-10T14:46:20.480008: step 1431, loss 0.212239, acc 0.921875, learning_rate 0.000114128
2017-10-10T14:46:20.857090: step 1432, loss 0.0826836, acc 0.96875, learning_rate 0.00011407
2017-10-10T14:46:21.259515: step 1433, loss 0.0888627, acc 0.984375, learning_rate 0.000114013
2017-10-10T14:46:21.634592: step 1434, loss 0.112195, acc 0.96875, learning_rate 0.000113955
2017-10-10T14:46:22.041790: step 1435, loss 0.132964, acc 0.9375, learning_rate 0.000113898
2017-10-10T14:46:22.445690: step 1436, loss 0.102507, acc 0.96875, learning_rate 0.000113842
2017-10-10T14:46:22.893306: step 1437, loss 0.168488, acc 0.9375, learning_rate 0.000113785
2017-10-10T14:46:23.241001: step 1438, loss 0.121764, acc 0.953125, learning_rate 0.000113729
2017-10-10T14:46:23.686061: step 1439, loss 0.193068, acc 0.9375, learning_rate 0.000113673
2017-10-10T14:46:24.116388: step 1440, loss 0.150912, acc 0.9375, learning_rate 0.000113617

Evaluation:
2017-10-10T14:46:25.096851: step 1440, loss 0.228539, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1440

2017-10-10T14:46:26.397253: step 1441, loss 0.184882, acc 0.90625, learning_rate 0.000113561
2017-10-10T14:46:26.822359: step 1442, loss 0.0554193, acc 1, learning_rate 0.000113506
2017-10-10T14:46:27.199830: step 1443, loss 0.225568, acc 0.921875, learning_rate 0.000113451
2017-10-10T14:46:27.555494: step 1444, loss 0.0807972, acc 0.984375, learning_rate 0.000113396
2017-10-10T14:46:27.992792: step 1445, loss 0.116214, acc 0.953125, learning_rate 0.000113341
2017-10-10T14:46:28.428279: step 1446, loss 0.0756592, acc 0.96875, learning_rate 0.000113287
2017-10-10T14:46:28.867826: step 1447, loss 0.175457, acc 0.9375, learning_rate 0.000113233
2017-10-10T14:46:29.293255: step 1448, loss 0.153856, acc 0.9375, learning_rate 0.000113179
2017-10-10T14:46:29.729196: step 1449, loss 0.0901988, acc 0.96875, learning_rate 0.000113125
2017-10-10T14:46:30.088858: step 1450, loss 0.186938, acc 0.9375, learning_rate 0.000113071
2017-10-10T14:46:30.547347: step 1451, loss 0.0630058, acc 0.96875, learning_rate 0.000113018
2017-10-10T14:46:30.952839: step 1452, loss 0.180946, acc 0.953125, learning_rate 0.000112965
2017-10-10T14:46:31.372899: step 1453, loss 0.111565, acc 0.953125, learning_rate 0.000112912
2017-10-10T14:46:31.853812: step 1454, loss 0.104509, acc 0.96875, learning_rate 0.000112859
2017-10-10T14:46:32.282087: step 1455, loss 0.103767, acc 0.953125, learning_rate 0.000112807
2017-10-10T14:46:32.636931: step 1456, loss 0.116498, acc 0.984375, learning_rate 0.000112754
2017-10-10T14:46:33.055274: step 1457, loss 0.119178, acc 0.96875, learning_rate 0.000112702
2017-10-10T14:46:33.463351: step 1458, loss 0.144465, acc 0.9375, learning_rate 0.000112651
2017-10-10T14:46:33.927703: step 1459, loss 0.125952, acc 0.96875, learning_rate 0.000112599
2017-10-10T14:46:34.284164: step 1460, loss 0.177689, acc 0.9375, learning_rate 0.000112547
2017-10-10T14:46:34.676823: step 1461, loss 0.16252, acc 0.921875, learning_rate 0.000112496
2017-10-10T14:46:35.020889: step 1462, loss 0.0903177, acc 0.96875, learning_rate 0.000112445
2017-10-10T14:46:35.405028: step 1463, loss 0.132629, acc 0.984375, learning_rate 0.000112394
2017-10-10T14:46:35.773063: step 1464, loss 0.162668, acc 0.953125, learning_rate 0.000112344
2017-10-10T14:46:36.204911: step 1465, loss 0.1093, acc 0.96875, learning_rate 0.000112293
2017-10-10T14:46:36.659745: step 1466, loss 0.16797, acc 0.9375, learning_rate 0.000112243
2017-10-10T14:46:37.083455: step 1467, loss 0.0606988, acc 0.96875, learning_rate 0.000112193
2017-10-10T14:46:37.466231: step 1468, loss 0.044889, acc 0.96875, learning_rate 0.000112144
2017-10-10T14:46:37.850031: step 1469, loss 0.0878879, acc 0.953125, learning_rate 0.000112094
2017-10-10T14:46:38.146284: step 1470, loss 0.0398106, acc 1, learning_rate 0.000112045
2017-10-10T14:46:38.641831: step 1471, loss 0.229371, acc 0.921875, learning_rate 0.000111995
2017-10-10T14:46:39.037059: step 1472, loss 0.117924, acc 0.953125, learning_rate 0.000111946
2017-10-10T14:46:39.461051: step 1473, loss 0.0607843, acc 0.984375, learning_rate 0.000111898
2017-10-10T14:46:39.914777: step 1474, loss 0.192416, acc 0.9375, learning_rate 0.000111849
2017-10-10T14:46:40.337652: step 1475, loss 0.0523759, acc 0.984375, learning_rate 0.000111801
2017-10-10T14:46:40.754589: step 1476, loss 0.100699, acc 0.96875, learning_rate 0.000111753
2017-10-10T14:46:41.132819: step 1477, loss 0.129919, acc 0.96875, learning_rate 0.000111705
2017-10-10T14:46:41.542408: step 1478, loss 0.116166, acc 0.96875, learning_rate 0.000111657
2017-10-10T14:46:41.988905: step 1479, loss 0.0959155, acc 0.96875, learning_rate 0.000111609
2017-10-10T14:46:42.371110: step 1480, loss 0.205002, acc 0.9375, learning_rate 0.000111562

Evaluation:
2017-10-10T14:46:43.259068: step 1480, loss 0.230093, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1480

2017-10-10T14:46:44.506906: step 1481, loss 0.132516, acc 0.9375, learning_rate 0.000111515
2017-10-10T14:46:44.899646: step 1482, loss 0.111348, acc 0.921875, learning_rate 0.000111468
2017-10-10T14:46:45.260790: step 1483, loss 0.175175, acc 0.9375, learning_rate 0.000111421
2017-10-10T14:46:45.664052: step 1484, loss 0.0648692, acc 0.96875, learning_rate 0.000111374
2017-10-10T14:46:46.049050: step 1485, loss 0.15603, acc 0.953125, learning_rate 0.000111328
2017-10-10T14:46:46.492986: step 1486, loss 0.117816, acc 0.953125, learning_rate 0.000111282
2017-10-10T14:46:46.885058: step 1487, loss 0.110988, acc 0.96875, learning_rate 0.000111236
2017-10-10T14:46:47.350399: step 1488, loss 0.0473352, acc 0.984375, learning_rate 0.00011119
2017-10-10T14:46:47.724999: step 1489, loss 0.0816408, acc 0.984375, learning_rate 0.000111144
2017-10-10T14:46:48.155735: step 1490, loss 0.201972, acc 0.953125, learning_rate 0.000111099
2017-10-10T14:46:48.644006: step 1491, loss 0.0933591, acc 0.984375, learning_rate 0.000111053
2017-10-10T14:46:49.081622: step 1492, loss 0.105048, acc 0.9375, learning_rate 0.000111008
2017-10-10T14:46:49.443487: step 1493, loss 0.148966, acc 0.9375, learning_rate 0.000110963
2017-10-10T14:46:49.920897: step 1494, loss 0.228614, acc 0.9375, learning_rate 0.000110918
2017-10-10T14:46:50.436841: step 1495, loss 0.0551017, acc 1, learning_rate 0.000110874
2017-10-10T14:46:50.813705: step 1496, loss 0.0399584, acc 1, learning_rate 0.00011083
2017-10-10T14:46:51.141076: step 1497, loss 0.151862, acc 0.9375, learning_rate 0.000110785
2017-10-10T14:46:51.512888: step 1498, loss 0.0498448, acc 0.984375, learning_rate 0.000110741
2017-10-10T14:46:51.959326: step 1499, loss 0.0592725, acc 0.984375, learning_rate 0.000110697
2017-10-10T14:46:52.334572: step 1500, loss 0.197329, acc 0.9375, learning_rate 0.000110654
2017-10-10T14:46:52.728930: step 1501, loss 0.201488, acc 0.953125, learning_rate 0.00011061
2017-10-10T14:46:53.172913: step 1502, loss 0.0782238, acc 0.96875, learning_rate 0.000110567
2017-10-10T14:46:53.520981: step 1503, loss 0.223683, acc 0.9375, learning_rate 0.000110524
2017-10-10T14:46:53.864199: step 1504, loss 0.098098, acc 0.96875, learning_rate 0.000110481
2017-10-10T14:46:54.273398: step 1505, loss 0.0430797, acc 1, learning_rate 0.000110438
2017-10-10T14:46:54.645286: step 1506, loss 0.211541, acc 0.953125, learning_rate 0.000110396
2017-10-10T14:46:55.031548: step 1507, loss 0.0628085, acc 0.984375, learning_rate 0.000110353
2017-10-10T14:46:55.433524: step 1508, loss 0.0764727, acc 0.96875, learning_rate 0.000110311
2017-10-10T14:46:55.880942: step 1509, loss 0.109177, acc 0.96875, learning_rate 0.000110269
2017-10-10T14:46:56.324722: step 1510, loss 0.12244, acc 0.96875, learning_rate 0.000110227
2017-10-10T14:46:56.719789: step 1511, loss 0.0584187, acc 0.984375, learning_rate 0.000110185
2017-10-10T14:46:57.106269: step 1512, loss 0.0851319, acc 0.984375, learning_rate 0.000110144
2017-10-10T14:46:57.525839: step 1513, loss 0.061626, acc 0.984375, learning_rate 0.000110102
2017-10-10T14:46:57.902845: step 1514, loss 0.0926895, acc 1, learning_rate 0.000110061
2017-10-10T14:46:58.324459: step 1515, loss 0.0744854, acc 1, learning_rate 0.00011002
2017-10-10T14:46:58.747662: step 1516, loss 0.124478, acc 0.953125, learning_rate 0.000109979
2017-10-10T14:46:59.161992: step 1517, loss 0.176379, acc 0.921875, learning_rate 0.000109938
2017-10-10T14:46:59.581189: step 1518, loss 0.0803042, acc 0.984375, learning_rate 0.000109898
2017-10-10T14:47:00.050996: step 1519, loss 0.130076, acc 0.953125, learning_rate 0.000109857
2017-10-10T14:47:00.400466: step 1520, loss 0.134354, acc 0.9375, learning_rate 0.000109817

Evaluation:
2017-10-10T14:47:01.367576: step 1520, loss 0.230999, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1520

2017-10-10T14:47:02.995259: step 1521, loss 0.152576, acc 0.9375, learning_rate 0.000109777
2017-10-10T14:47:03.355773: step 1522, loss 0.0648351, acc 0.96875, learning_rate 0.000109737
2017-10-10T14:47:03.748954: step 1523, loss 0.0872719, acc 0.96875, learning_rate 0.000109697
2017-10-10T14:47:04.115741: step 1524, loss 0.118657, acc 0.953125, learning_rate 0.000109658
2017-10-10T14:47:04.540930: step 1525, loss 0.118984, acc 0.953125, learning_rate 0.000109618
2017-10-10T14:47:04.950501: step 1526, loss 0.0904167, acc 0.984375, learning_rate 0.000109579
2017-10-10T14:47:05.384857: step 1527, loss 0.157015, acc 0.96875, learning_rate 0.00010954
2017-10-10T14:47:05.838547: step 1528, loss 0.133797, acc 0.9375, learning_rate 0.000109501
2017-10-10T14:47:06.290766: step 1529, loss 0.150851, acc 0.953125, learning_rate 0.000109462
2017-10-10T14:47:06.644837: step 1530, loss 0.153817, acc 0.921875, learning_rate 0.000109424
2017-10-10T14:47:07.083640: step 1531, loss 0.194499, acc 0.9375, learning_rate 0.000109385
2017-10-10T14:47:07.586025: step 1532, loss 0.112071, acc 0.96875, learning_rate 0.000109347
2017-10-10T14:47:07.920800: step 1533, loss 0.141303, acc 0.953125, learning_rate 0.000109309
2017-10-10T14:47:08.373285: step 1534, loss 0.241548, acc 0.953125, learning_rate 0.000109271
2017-10-10T14:47:08.772934: step 1535, loss 0.0738137, acc 0.984375, learning_rate 0.000109233
2017-10-10T14:47:09.119493: step 1536, loss 0.185636, acc 0.9375, learning_rate 0.000109195
2017-10-10T14:47:09.551581: step 1537, loss 0.0880691, acc 0.984375, learning_rate 0.000109158
2017-10-10T14:47:09.972955: step 1538, loss 0.0967045, acc 0.96875, learning_rate 0.00010912
2017-10-10T14:47:10.387386: step 1539, loss 0.113161, acc 0.96875, learning_rate 0.000109083
2017-10-10T14:47:10.838894: step 1540, loss 0.0212926, acc 1, learning_rate 0.000109046
2017-10-10T14:47:11.221907: step 1541, loss 0.188206, acc 0.921875, learning_rate 0.000109009
2017-10-10T14:47:11.669240: step 1542, loss 0.116246, acc 0.96875, learning_rate 0.000108972
2017-10-10T14:47:12.094659: step 1543, loss 0.233917, acc 0.875, learning_rate 0.000108936
2017-10-10T14:47:12.515646: step 1544, loss 0.0864823, acc 0.984375, learning_rate 0.000108899
2017-10-10T14:47:12.911196: step 1545, loss 0.0940177, acc 0.953125, learning_rate 0.000108863
2017-10-10T14:47:13.366398: step 1546, loss 0.206253, acc 0.953125, learning_rate 0.000108827
2017-10-10T14:47:13.779230: step 1547, loss 0.144364, acc 0.96875, learning_rate 0.000108791
2017-10-10T14:47:14.151015: step 1548, loss 0.141482, acc 0.9375, learning_rate 0.000108755
2017-10-10T14:47:14.532838: step 1549, loss 0.0660932, acc 0.96875, learning_rate 0.000108719
2017-10-10T14:47:14.957009: step 1550, loss 0.0810948, acc 0.984375, learning_rate 0.000108683
2017-10-10T14:47:15.360895: step 1551, loss 0.0706886, acc 0.984375, learning_rate 0.000108648
2017-10-10T14:47:15.781598: step 1552, loss 0.0945459, acc 0.96875, learning_rate 0.000108613
2017-10-10T14:47:16.225131: step 1553, loss 0.105388, acc 0.96875, learning_rate 0.000108577
2017-10-10T14:47:16.645173: step 1554, loss 0.2164, acc 0.9375, learning_rate 0.000108542
2017-10-10T14:47:17.036367: step 1555, loss 0.161133, acc 0.921875, learning_rate 0.000108508
2017-10-10T14:47:17.425210: step 1556, loss 0.097567, acc 0.96875, learning_rate 0.000108473
2017-10-10T14:47:17.791334: step 1557, loss 0.0684221, acc 0.96875, learning_rate 0.000108438
2017-10-10T14:47:18.192902: step 1558, loss 0.0506664, acc 0.984375, learning_rate 0.000108404
2017-10-10T14:47:18.608575: step 1559, loss 0.132634, acc 0.953125, learning_rate 0.00010837
2017-10-10T14:47:19.001381: step 1560, loss 0.061019, acc 0.96875, learning_rate 0.000108335

Evaluation:
2017-10-10T14:47:20.049069: step 1560, loss 0.229858, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1560

2017-10-10T14:47:21.405791: step 1561, loss 0.0408707, acc 0.984375, learning_rate 0.000108301
2017-10-10T14:47:21.801057: step 1562, loss 0.077687, acc 0.96875, learning_rate 0.000108267
2017-10-10T14:47:22.256789: step 1563, loss 0.1115, acc 0.953125, learning_rate 0.000108234
2017-10-10T14:47:22.681292: step 1564, loss 0.340479, acc 0.890625, learning_rate 0.0001082
2017-10-10T14:47:23.190571: step 1565, loss 0.100307, acc 0.96875, learning_rate 0.000108167
2017-10-10T14:47:23.685055: step 1566, loss 0.0674337, acc 0.96875, learning_rate 0.000108133
2017-10-10T14:47:24.084866: step 1567, loss 0.162956, acc 0.953125, learning_rate 0.0001081
2017-10-10T14:47:24.455715: step 1568, loss 0.168423, acc 0.960784, learning_rate 0.000108067
2017-10-10T14:47:24.792849: step 1569, loss 0.0672313, acc 0.984375, learning_rate 0.000108034
2017-10-10T14:47:25.200894: step 1570, loss 0.0612266, acc 1, learning_rate 0.000108001
2017-10-10T14:47:25.588795: step 1571, loss 0.127046, acc 0.953125, learning_rate 0.000107969
2017-10-10T14:47:26.012327: step 1572, loss 0.0909568, acc 0.984375, learning_rate 0.000107936
2017-10-10T14:47:26.446297: step 1573, loss 0.0840744, acc 0.984375, learning_rate 0.000107904
2017-10-10T14:47:26.883299: step 1574, loss 0.0926961, acc 0.96875, learning_rate 0.000107871
2017-10-10T14:47:27.260262: step 1575, loss 0.109866, acc 0.96875, learning_rate 0.000107839
2017-10-10T14:47:27.649052: step 1576, loss 0.116215, acc 0.96875, learning_rate 0.000107807
2017-10-10T14:47:28.013080: step 1577, loss 0.1905, acc 0.90625, learning_rate 0.000107775
2017-10-10T14:47:28.431563: step 1578, loss 0.0769459, acc 1, learning_rate 0.000107744
2017-10-10T14:47:28.868849: step 1579, loss 0.130559, acc 0.984375, learning_rate 0.000107712
2017-10-10T14:47:29.256929: step 1580, loss 0.0736182, acc 0.96875, learning_rate 0.000107681
2017-10-10T14:47:29.648419: step 1581, loss 0.0721301, acc 0.984375, learning_rate 0.000107649
2017-10-10T14:47:30.042590: step 1582, loss 0.0933064, acc 0.953125, learning_rate 0.000107618
2017-10-10T14:47:30.401805: step 1583, loss 0.144035, acc 0.9375, learning_rate 0.000107587
2017-10-10T14:47:30.780938: step 1584, loss 0.0619169, acc 1, learning_rate 0.000107556
2017-10-10T14:47:31.204131: step 1585, loss 0.241954, acc 0.921875, learning_rate 0.000107525
2017-10-10T14:47:31.696862: step 1586, loss 0.102898, acc 0.96875, learning_rate 0.000107494
2017-10-10T14:47:32.116019: step 1587, loss 0.124402, acc 0.921875, learning_rate 0.000107464
2017-10-10T14:47:32.518215: step 1588, loss 0.060906, acc 0.984375, learning_rate 0.000107433
2017-10-10T14:47:32.988231: step 1589, loss 0.159228, acc 0.953125, learning_rate 0.000107403
2017-10-10T14:47:33.418751: step 1590, loss 0.0702489, acc 0.984375, learning_rate 0.000107373
2017-10-10T14:47:33.741430: step 1591, loss 0.121676, acc 0.9375, learning_rate 0.000107343
2017-10-10T14:47:34.112946: step 1592, loss 0.212639, acc 0.9375, learning_rate 0.000107313
2017-10-10T14:47:34.596981: step 1593, loss 0.113819, acc 0.96875, learning_rate 0.000107283
2017-10-10T14:47:35.028819: step 1594, loss 0.0887028, acc 0.96875, learning_rate 0.000107253
2017-10-10T14:47:35.485005: step 1595, loss 0.0639459, acc 0.984375, learning_rate 0.000107224
2017-10-10T14:47:35.887918: step 1596, loss 0.187172, acc 0.921875, learning_rate 0.000107194
2017-10-10T14:47:36.284903: step 1597, loss 0.0984178, acc 0.96875, learning_rate 0.000107165
2017-10-10T14:47:36.618284: step 1598, loss 0.0568421, acc 0.96875, learning_rate 0.000107136
2017-10-10T14:47:37.024039: step 1599, loss 0.117407, acc 0.953125, learning_rate 0.000107106
2017-10-10T14:47:37.439956: step 1600, loss 0.145046, acc 0.953125, learning_rate 0.000107077

Evaluation:
2017-10-10T14:47:38.284430: step 1600, loss 0.226448, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1600

2017-10-10T14:47:39.604967: step 1601, loss 0.149894, acc 0.96875, learning_rate 0.000107048
2017-10-10T14:47:40.058335: step 1602, loss 0.0718356, acc 0.96875, learning_rate 0.00010702
2017-10-10T14:47:40.471072: step 1603, loss 0.0661376, acc 1, learning_rate 0.000106991
2017-10-10T14:47:40.857708: step 1604, loss 0.174539, acc 0.953125, learning_rate 0.000106963
2017-10-10T14:47:41.208943: step 1605, loss 0.0501293, acc 1, learning_rate 0.000106934
2017-10-10T14:47:41.567758: step 1606, loss 0.162301, acc 0.90625, learning_rate 0.000106906
2017-10-10T14:47:42.007613: step 1607, loss 0.219367, acc 0.9375, learning_rate 0.000106878
2017-10-10T14:47:42.433337: step 1608, loss 0.138457, acc 0.96875, learning_rate 0.00010685
2017-10-10T14:47:42.876954: step 1609, loss 0.162247, acc 0.9375, learning_rate 0.000106822
2017-10-10T14:47:43.315616: step 1610, loss 0.118633, acc 0.96875, learning_rate 0.000106794
2017-10-10T14:47:43.667295: step 1611, loss 0.0902, acc 0.96875, learning_rate 0.000106766
2017-10-10T14:47:44.106168: step 1612, loss 0.0904083, acc 0.96875, learning_rate 0.000106738
2017-10-10T14:47:44.532861: step 1613, loss 0.11694, acc 0.96875, learning_rate 0.000106711
2017-10-10T14:47:45.036457: step 1614, loss 0.0993953, acc 0.953125, learning_rate 0.000106684
2017-10-10T14:47:45.412978: step 1615, loss 0.167066, acc 0.9375, learning_rate 0.000106656
2017-10-10T14:47:45.818271: step 1616, loss 0.0933827, acc 0.953125, learning_rate 0.000106629
2017-10-10T14:47:46.187877: step 1617, loss 0.118097, acc 0.96875, learning_rate 0.000106602
2017-10-10T14:47:46.610892: step 1618, loss 0.227125, acc 0.859375, learning_rate 0.000106575
2017-10-10T14:47:47.012859: step 1619, loss 0.125735, acc 0.984375, learning_rate 0.000106548
2017-10-10T14:47:47.349862: step 1620, loss 0.130052, acc 0.953125, learning_rate 0.000106521
2017-10-10T14:47:47.701038: step 1621, loss 0.0724738, acc 0.984375, learning_rate 0.000106495
2017-10-10T14:47:48.063972: step 1622, loss 0.202952, acc 0.96875, learning_rate 0.000106468
2017-10-10T14:47:48.482742: step 1623, loss 0.069262, acc 0.984375, learning_rate 0.000106442
2017-10-10T14:47:48.884563: step 1624, loss 0.162064, acc 0.953125, learning_rate 0.000106416
2017-10-10T14:47:49.307916: step 1625, loss 0.192384, acc 0.90625, learning_rate 0.000106389
2017-10-10T14:47:49.760896: step 1626, loss 0.0815056, acc 0.96875, learning_rate 0.000106363
2017-10-10T14:47:50.184686: step 1627, loss 0.168297, acc 0.953125, learning_rate 0.000106337
2017-10-10T14:47:50.626742: step 1628, loss 0.133012, acc 0.953125, learning_rate 0.000106312
2017-10-10T14:47:51.049069: step 1629, loss 0.0802193, acc 0.96875, learning_rate 0.000106286
2017-10-10T14:47:51.418532: step 1630, loss 0.081477, acc 0.96875, learning_rate 0.00010626
2017-10-10T14:47:51.856227: step 1631, loss 0.14665, acc 0.953125, learning_rate 0.000106235
2017-10-10T14:47:52.255992: step 1632, loss 0.186808, acc 0.953125, learning_rate 0.000106209
2017-10-10T14:47:52.696944: step 1633, loss 0.13389, acc 0.953125, learning_rate 0.000106184
2017-10-10T14:47:53.104271: step 1634, loss 0.092444, acc 0.953125, learning_rate 0.000106159
2017-10-10T14:47:53.596165: step 1635, loss 0.0933453, acc 0.96875, learning_rate 0.000106133
2017-10-10T14:47:54.020662: step 1636, loss 0.0387634, acc 1, learning_rate 0.000106108
2017-10-10T14:47:54.432498: step 1637, loss 0.128089, acc 0.953125, learning_rate 0.000106083
2017-10-10T14:47:54.833086: step 1638, loss 0.109816, acc 0.953125, learning_rate 0.000106059
2017-10-10T14:47:55.322253: step 1639, loss 0.107734, acc 0.984375, learning_rate 0.000106034
2017-10-10T14:47:55.753247: step 1640, loss 0.0964834, acc 0.96875, learning_rate 0.000106009

Evaluation:
2017-10-10T14:47:56.570487: step 1640, loss 0.226803, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1640

2017-10-10T14:47:57.740933: step 1641, loss 0.113143, acc 0.953125, learning_rate 0.000105985
2017-10-10T14:47:58.083382: step 1642, loss 0.160699, acc 0.9375, learning_rate 0.00010596
2017-10-10T14:47:58.459724: step 1643, loss 0.158837, acc 0.953125, learning_rate 0.000105936
2017-10-10T14:47:58.873990: step 1644, loss 0.108425, acc 0.96875, learning_rate 0.000105912
2017-10-10T14:47:59.278174: step 1645, loss 0.134605, acc 0.96875, learning_rate 0.000105888
2017-10-10T14:47:59.784176: step 1646, loss 0.178443, acc 0.9375, learning_rate 0.000105864
2017-10-10T14:48:00.185054: step 1647, loss 0.0310604, acc 1, learning_rate 0.00010584
2017-10-10T14:48:00.578993: step 1648, loss 0.152821, acc 0.96875, learning_rate 0.000105816
2017-10-10T14:48:00.994295: step 1649, loss 0.264024, acc 0.9375, learning_rate 0.000105792
2017-10-10T14:48:01.426733: step 1650, loss 0.127238, acc 0.9375, learning_rate 0.000105768
2017-10-10T14:48:01.870659: step 1651, loss 0.0222211, acc 1, learning_rate 0.000105745
2017-10-10T14:48:02.306804: step 1652, loss 0.0716566, acc 0.984375, learning_rate 0.000105721
2017-10-10T14:48:02.692851: step 1653, loss 0.0938673, acc 0.96875, learning_rate 0.000105698
2017-10-10T14:48:03.103120: step 1654, loss 0.0908618, acc 0.984375, learning_rate 0.000105675
2017-10-10T14:48:03.525828: step 1655, loss 0.103699, acc 0.96875, learning_rate 0.000105652
2017-10-10T14:48:03.916355: step 1656, loss 0.0986301, acc 0.953125, learning_rate 0.000105629
2017-10-10T14:48:04.356051: step 1657, loss 0.10097, acc 0.96875, learning_rate 0.000105606
2017-10-10T14:48:04.764272: step 1658, loss 0.103334, acc 0.953125, learning_rate 0.000105583
2017-10-10T14:48:05.184503: step 1659, loss 0.13204, acc 0.96875, learning_rate 0.00010556
2017-10-10T14:48:05.540959: step 1660, loss 0.0796804, acc 0.984375, learning_rate 0.000105537
2017-10-10T14:48:05.944976: step 1661, loss 0.126841, acc 0.96875, learning_rate 0.000105515
2017-10-10T14:48:06.394513: step 1662, loss 0.066648, acc 0.984375, learning_rate 0.000105492
2017-10-10T14:48:06.820463: step 1663, loss 0.139399, acc 0.9375, learning_rate 0.00010547
2017-10-10T14:48:07.268832: step 1664, loss 0.10569, acc 0.9375, learning_rate 0.000105447
2017-10-10T14:48:07.705003: step 1665, loss 0.0439275, acc 1, learning_rate 0.000105425
2017-10-10T14:48:08.068051: step 1666, loss 0.146841, acc 0.941176, learning_rate 0.000105403
2017-10-10T14:48:08.504003: step 1667, loss 0.0704736, acc 0.984375, learning_rate 0.000105381
2017-10-10T14:48:08.916882: step 1668, loss 0.212006, acc 0.9375, learning_rate 0.000105359
2017-10-10T14:48:09.313679: step 1669, loss 0.0809037, acc 0.984375, learning_rate 0.000105337
2017-10-10T14:48:09.730361: step 1670, loss 0.125288, acc 0.96875, learning_rate 0.000105315
2017-10-10T14:48:10.070919: step 1671, loss 0.0610498, acc 0.96875, learning_rate 0.000105294
2017-10-10T14:48:10.391066: step 1672, loss 0.0871397, acc 0.953125, learning_rate 0.000105272
2017-10-10T14:48:10.800786: step 1673, loss 0.158349, acc 0.9375, learning_rate 0.000105251
2017-10-10T14:48:11.216166: step 1674, loss 0.115571, acc 0.953125, learning_rate 0.000105229
2017-10-10T14:48:11.568926: step 1675, loss 0.105798, acc 0.96875, learning_rate 0.000105208
2017-10-10T14:48:11.956856: step 1676, loss 0.111817, acc 0.96875, learning_rate 0.000105186
2017-10-10T14:48:12.436974: step 1677, loss 0.15317, acc 0.9375, learning_rate 0.000105165
2017-10-10T14:48:12.876096: step 1678, loss 0.113607, acc 0.96875, learning_rate 0.000105144
2017-10-10T14:48:13.409283: step 1679, loss 0.135072, acc 0.9375, learning_rate 0.000105123
2017-10-10T14:48:13.828133: step 1680, loss 0.120567, acc 0.9375, learning_rate 0.000105102

Evaluation:
2017-10-10T14:48:14.558961: step 1680, loss 0.230819, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1680

2017-10-10T14:48:15.907981: step 1681, loss 0.203447, acc 0.953125, learning_rate 0.000105081
2017-10-10T14:48:16.361098: step 1682, loss 0.101798, acc 0.96875, learning_rate 0.000105061
2017-10-10T14:48:16.749324: step 1683, loss 0.0977685, acc 0.96875, learning_rate 0.00010504
2017-10-10T14:48:17.224551: step 1684, loss 0.149086, acc 0.921875, learning_rate 0.00010502
2017-10-10T14:48:17.649425: step 1685, loss 0.127765, acc 0.953125, learning_rate 0.000104999
2017-10-10T14:48:18.057752: step 1686, loss 0.0568624, acc 0.984375, learning_rate 0.000104979
2017-10-10T14:48:18.414158: step 1687, loss 0.059676, acc 0.984375, learning_rate 0.000104958
2017-10-10T14:48:18.844930: step 1688, loss 0.184995, acc 0.9375, learning_rate 0.000104938
2017-10-10T14:48:19.223297: step 1689, loss 0.165703, acc 0.9375, learning_rate 0.000104918
2017-10-10T14:48:19.651152: step 1690, loss 0.116468, acc 0.953125, learning_rate 0.000104898
2017-10-10T14:48:20.145317: step 1691, loss 0.0414266, acc 1, learning_rate 0.000104878
2017-10-10T14:48:20.517871: step 1692, loss 0.149534, acc 0.953125, learning_rate 0.000104858
2017-10-10T14:48:20.892978: step 1693, loss 0.26135, acc 0.9375, learning_rate 0.000104838
2017-10-10T14:48:21.302119: step 1694, loss 0.0637571, acc 0.984375, learning_rate 0.000104818
2017-10-10T14:48:21.681150: step 1695, loss 0.106384, acc 0.953125, learning_rate 0.000104799
2017-10-10T14:48:22.127373: step 1696, loss 0.0772657, acc 0.953125, learning_rate 0.000104779
2017-10-10T14:48:22.490901: step 1697, loss 0.0529905, acc 0.984375, learning_rate 0.00010476
2017-10-10T14:48:22.879945: step 1698, loss 0.130867, acc 0.953125, learning_rate 0.00010474
2017-10-10T14:48:23.290202: step 1699, loss 0.0281053, acc 0.984375, learning_rate 0.000104721
2017-10-10T14:48:23.744573: step 1700, loss 0.0366577, acc 1, learning_rate 0.000104702
2017-10-10T14:48:24.117133: step 1701, loss 0.0908698, acc 0.96875, learning_rate 0.000104682
2017-10-10T14:48:24.616841: step 1702, loss 0.052749, acc 1, learning_rate 0.000104663
2017-10-10T14:48:25.058115: step 1703, loss 0.0564463, acc 0.96875, learning_rate 0.000104644
2017-10-10T14:48:25.462008: step 1704, loss 0.113451, acc 0.953125, learning_rate 0.000104625
2017-10-10T14:48:25.901534: step 1705, loss 0.14198, acc 0.953125, learning_rate 0.000104606
2017-10-10T14:48:26.401150: step 1706, loss 0.0624368, acc 0.984375, learning_rate 0.000104588
2017-10-10T14:48:26.814180: step 1707, loss 0.179235, acc 0.9375, learning_rate 0.000104569
2017-10-10T14:48:27.220811: step 1708, loss 0.0727519, acc 1, learning_rate 0.00010455
2017-10-10T14:48:27.668827: step 1709, loss 0.0559617, acc 1, learning_rate 0.000104532
2017-10-10T14:48:28.089142: step 1710, loss 0.140619, acc 0.953125, learning_rate 0.000104513
2017-10-10T14:48:28.515018: step 1711, loss 0.0904326, acc 0.96875, learning_rate 0.000104495
2017-10-10T14:48:28.892957: step 1712, loss 0.161083, acc 0.953125, learning_rate 0.000104476
2017-10-10T14:48:29.285177: step 1713, loss 0.109588, acc 0.96875, learning_rate 0.000104458
2017-10-10T14:48:29.714261: step 1714, loss 0.0494814, acc 1, learning_rate 0.00010444
2017-10-10T14:48:30.156873: step 1715, loss 0.0922847, acc 0.96875, learning_rate 0.000104422
2017-10-10T14:48:30.565444: step 1716, loss 0.0494772, acc 1, learning_rate 0.000104404
2017-10-10T14:48:30.929658: step 1717, loss 0.12061, acc 0.953125, learning_rate 0.000104386
2017-10-10T14:48:31.374487: step 1718, loss 0.103882, acc 0.96875, learning_rate 0.000104368
2017-10-10T14:48:31.738544: step 1719, loss 0.154551, acc 0.9375, learning_rate 0.00010435
2017-10-10T14:48:32.096079: step 1720, loss 0.127039, acc 0.953125, learning_rate 0.000104332

Evaluation:
2017-10-10T14:48:32.965915: step 1720, loss 0.229436, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1720

2017-10-10T14:48:34.307307: step 1721, loss 0.122574, acc 0.96875, learning_rate 0.000104315
2017-10-10T14:48:34.716956: step 1722, loss 0.135536, acc 0.96875, learning_rate 0.000104297
2017-10-10T14:48:35.165094: step 1723, loss 0.143411, acc 0.96875, learning_rate 0.000104279
2017-10-10T14:48:35.608905: step 1724, loss 0.0826237, acc 0.96875, learning_rate 0.000104262
2017-10-10T14:48:36.015253: step 1725, loss 0.182393, acc 0.9375, learning_rate 0.000104245
2017-10-10T14:48:36.335963: step 1726, loss 0.0412492, acc 0.984375, learning_rate 0.000104227
2017-10-10T14:48:36.732908: step 1727, loss 0.0950632, acc 0.984375, learning_rate 0.00010421
2017-10-10T14:48:37.164958: step 1728, loss 0.0820959, acc 0.984375, learning_rate 0.000104193
2017-10-10T14:48:37.592872: step 1729, loss 0.154633, acc 0.9375, learning_rate 0.000104176
2017-10-10T14:48:38.042095: step 1730, loss 0.117169, acc 0.953125, learning_rate 0.000104159
2017-10-10T14:48:38.457953: step 1731, loss 0.10389, acc 0.984375, learning_rate 0.000104142
2017-10-10T14:48:38.872933: step 1732, loss 0.139548, acc 0.9375, learning_rate 0.000104125
2017-10-10T14:48:39.357384: step 1733, loss 0.0258656, acc 1, learning_rate 0.000104108
2017-10-10T14:48:39.718816: step 1734, loss 0.119226, acc 0.953125, learning_rate 0.000104091
2017-10-10T14:48:40.149364: step 1735, loss 0.170874, acc 0.90625, learning_rate 0.000104074
2017-10-10T14:48:40.552941: step 1736, loss 0.11206, acc 0.96875, learning_rate 0.000104058
2017-10-10T14:48:40.984497: step 1737, loss 0.0380397, acc 0.984375, learning_rate 0.000104041
2017-10-10T14:48:41.361141: step 1738, loss 0.141477, acc 0.9375, learning_rate 0.000104025
2017-10-10T14:48:41.780880: step 1739, loss 0.174534, acc 0.921875, learning_rate 0.000104008
2017-10-10T14:48:42.188960: step 1740, loss 0.156938, acc 0.90625, learning_rate 0.000103992
2017-10-10T14:48:42.637280: step 1741, loss 0.0886611, acc 0.96875, learning_rate 0.000103976
2017-10-10T14:48:43.005038: step 1742, loss 0.128024, acc 0.96875, learning_rate 0.000103959
2017-10-10T14:48:43.404977: step 1743, loss 0.171935, acc 0.953125, learning_rate 0.000103943
2017-10-10T14:48:43.833258: step 1744, loss 0.151445, acc 0.921875, learning_rate 0.000103927
2017-10-10T14:48:44.274881: step 1745, loss 0.170449, acc 0.921875, learning_rate 0.000103911
2017-10-10T14:48:44.661399: step 1746, loss 0.106068, acc 0.9375, learning_rate 0.000103895
2017-10-10T14:48:45.110501: step 1747, loss 0.14675, acc 0.953125, learning_rate 0.000103879
2017-10-10T14:48:45.487846: step 1748, loss 0.0329989, acc 1, learning_rate 0.000103863
2017-10-10T14:48:45.998480: step 1749, loss 0.0718764, acc 0.984375, learning_rate 0.000103848
2017-10-10T14:48:46.461066: step 1750, loss 0.127155, acc 0.953125, learning_rate 0.000103832
2017-10-10T14:48:46.891274: step 1751, loss 0.126102, acc 0.953125, learning_rate 0.000103816
2017-10-10T14:48:47.153057: step 1752, loss 0.0599235, acc 0.96875, learning_rate 0.000103801
2017-10-10T14:48:47.496924: step 1753, loss 0.141826, acc 0.9375, learning_rate 0.000103785
2017-10-10T14:48:47.790881: step 1754, loss 0.108709, acc 0.953125, learning_rate 0.00010377
2017-10-10T14:48:48.198141: step 1755, loss 0.11167, acc 0.984375, learning_rate 0.000103754
2017-10-10T14:48:48.628934: step 1756, loss 0.0784436, acc 0.984375, learning_rate 0.000103739
2017-10-10T14:48:49.030969: step 1757, loss 0.0743816, acc 0.984375, learning_rate 0.000103724
2017-10-10T14:48:49.441151: step 1758, loss 0.0841505, acc 0.953125, learning_rate 0.000103709
2017-10-10T14:48:49.885034: step 1759, loss 0.116873, acc 0.953125, learning_rate 0.000103694
2017-10-10T14:48:50.286497: step 1760, loss 0.0997309, acc 0.984375, learning_rate 0.000103678

Evaluation:
2017-10-10T14:48:51.104902: step 1760, loss 0.226126, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1760

2017-10-10T14:48:52.460844: step 1761, loss 0.136654, acc 0.96875, learning_rate 0.000103663
2017-10-10T14:48:52.977596: step 1762, loss 0.0319114, acc 1, learning_rate 0.000103648
2017-10-10T14:48:53.293300: step 1763, loss 0.101778, acc 1, learning_rate 0.000103634
2017-10-10T14:48:53.620842: step 1764, loss 0.0793404, acc 0.980392, learning_rate 0.000103619
2017-10-10T14:48:54.008769: step 1765, loss 0.126447, acc 0.953125, learning_rate 0.000103604
2017-10-10T14:48:54.427996: step 1766, loss 0.124678, acc 0.96875, learning_rate 0.000103589
2017-10-10T14:48:54.860937: step 1767, loss 0.166985, acc 0.96875, learning_rate 0.000103575
2017-10-10T14:48:55.324533: step 1768, loss 0.0297646, acc 1, learning_rate 0.00010356
2017-10-10T14:48:55.716605: step 1769, loss 0.140621, acc 0.953125, learning_rate 0.000103545
2017-10-10T14:48:56.118029: step 1770, loss 0.0865184, acc 0.96875, learning_rate 0.000103531
2017-10-10T14:48:56.569044: step 1771, loss 0.201463, acc 0.90625, learning_rate 0.000103517
2017-10-10T14:48:56.896428: step 1772, loss 0.192703, acc 0.921875, learning_rate 0.000103502
2017-10-10T14:48:57.259960: step 1773, loss 0.0352202, acc 1, learning_rate 0.000103488
2017-10-10T14:48:57.710898: step 1774, loss 0.0562491, acc 0.96875, learning_rate 0.000103474
2017-10-10T14:48:58.138333: step 1775, loss 0.0796257, acc 0.96875, learning_rate 0.00010346
2017-10-10T14:48:58.560565: step 1776, loss 0.120385, acc 0.953125, learning_rate 0.000103445
2017-10-10T14:48:59.069115: step 1777, loss 0.0256451, acc 1, learning_rate 0.000103431
2017-10-10T14:48:59.433059: step 1778, loss 0.183509, acc 0.9375, learning_rate 0.000103417
2017-10-10T14:48:59.788929: step 1779, loss 0.0762155, acc 0.984375, learning_rate 0.000103403
2017-10-10T14:49:00.253603: step 1780, loss 0.0741241, acc 0.984375, learning_rate 0.00010339
2017-10-10T14:49:00.701080: step 1781, loss 0.144899, acc 0.96875, learning_rate 0.000103376
2017-10-10T14:49:01.123427: step 1782, loss 0.0707494, acc 1, learning_rate 0.000103362
2017-10-10T14:49:01.592993: step 1783, loss 0.0455892, acc 1, learning_rate 0.000103348
2017-10-10T14:49:02.048928: step 1784, loss 0.237693, acc 0.9375, learning_rate 0.000103335
2017-10-10T14:49:02.481028: step 1785, loss 0.0486507, acc 0.984375, learning_rate 0.000103321
2017-10-10T14:49:02.860917: step 1786, loss 0.199112, acc 0.9375, learning_rate 0.000103307
2017-10-10T14:49:03.438113: step 1787, loss 0.0524027, acc 1, learning_rate 0.000103294
2017-10-10T14:49:03.763718: step 1788, loss 0.143101, acc 0.9375, learning_rate 0.00010328
2017-10-10T14:49:04.181283: step 1789, loss 0.0630929, acc 0.984375, learning_rate 0.000103267
2017-10-10T14:49:04.514883: step 1790, loss 0.109176, acc 0.953125, learning_rate 0.000103254
2017-10-10T14:49:04.882721: step 1791, loss 0.106253, acc 0.96875, learning_rate 0.00010324
2017-10-10T14:49:05.211248: step 1792, loss 0.147538, acc 0.96875, learning_rate 0.000103227
2017-10-10T14:49:05.680442: step 1793, loss 0.201515, acc 0.921875, learning_rate 0.000103214
2017-10-10T14:49:06.184991: step 1794, loss 0.226558, acc 0.921875, learning_rate 0.000103201
2017-10-10T14:49:06.613206: step 1795, loss 0.100011, acc 0.953125, learning_rate 0.000103188
2017-10-10T14:49:06.993610: step 1796, loss 0.0723216, acc 1, learning_rate 0.000103175
2017-10-10T14:49:07.432860: step 1797, loss 0.103485, acc 0.96875, learning_rate 0.000103162
2017-10-10T14:49:07.828809: step 1798, loss 0.241507, acc 0.90625, learning_rate 0.000103149
2017-10-10T14:49:08.211217: step 1799, loss 0.181635, acc 0.921875, learning_rate 0.000103136
2017-10-10T14:49:08.638898: step 1800, loss 0.0363169, acc 0.984375, learning_rate 0.000103123

Evaluation:
2017-10-10T14:49:09.472012: step 1800, loss 0.224684, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1800

2017-10-10T14:49:10.780959: step 1801, loss 0.0647471, acc 0.984375, learning_rate 0.000103111
2017-10-10T14:49:11.264317: step 1802, loss 0.216971, acc 0.9375, learning_rate 0.000103098
2017-10-10T14:49:11.638085: step 1803, loss 0.0961134, acc 0.953125, learning_rate 0.000103085
2017-10-10T14:49:11.984657: step 1804, loss 0.0876015, acc 0.984375, learning_rate 0.000103073
2017-10-10T14:49:12.441226: step 1805, loss 0.142271, acc 0.9375, learning_rate 0.00010306
2017-10-10T14:49:12.872058: step 1806, loss 0.105692, acc 0.984375, learning_rate 0.000103048
2017-10-10T14:49:13.316845: step 1807, loss 0.130252, acc 0.921875, learning_rate 0.000103035
2017-10-10T14:49:13.732326: step 1808, loss 0.15104, acc 0.953125, learning_rate 0.000103023
2017-10-10T14:49:14.157162: step 1809, loss 0.0573302, acc 0.984375, learning_rate 0.00010301
2017-10-10T14:49:14.534420: step 1810, loss 0.172623, acc 0.921875, learning_rate 0.000102998
2017-10-10T14:49:15.008948: step 1811, loss 0.0420641, acc 1, learning_rate 0.000102986
2017-10-10T14:49:15.361075: step 1812, loss 0.0918085, acc 0.96875, learning_rate 0.000102974
2017-10-10T14:49:15.789611: step 1813, loss 0.215249, acc 0.921875, learning_rate 0.000102962
2017-10-10T14:49:16.102268: step 1814, loss 0.101661, acc 0.953125, learning_rate 0.000102949
2017-10-10T14:49:16.444856: step 1815, loss 0.117194, acc 0.953125, learning_rate 0.000102937
2017-10-10T14:49:16.923838: step 1816, loss 0.0762779, acc 0.96875, learning_rate 0.000102925
2017-10-10T14:49:17.291269: step 1817, loss 0.128103, acc 0.953125, learning_rate 0.000102913
2017-10-10T14:49:17.702738: step 1818, loss 0.146881, acc 0.96875, learning_rate 0.000102902
2017-10-10T14:49:18.117003: step 1819, loss 0.173853, acc 0.9375, learning_rate 0.00010289
2017-10-10T14:49:18.504524: step 1820, loss 0.119575, acc 0.953125, learning_rate 0.000102878
2017-10-10T14:49:18.889178: step 1821, loss 0.175508, acc 0.90625, learning_rate 0.000102866
2017-10-10T14:49:19.328969: step 1822, loss 0.129561, acc 0.953125, learning_rate 0.000102855
2017-10-10T14:49:19.938559: step 1823, loss 0.121312, acc 0.9375, learning_rate 0.000102843
2017-10-10T14:49:20.224958: step 1824, loss 0.161204, acc 0.9375, learning_rate 0.000102831
2017-10-10T14:49:20.513061: step 1825, loss 0.107297, acc 0.953125, learning_rate 0.00010282
2017-10-10T14:49:20.856578: step 1826, loss 0.118663, acc 0.9375, learning_rate 0.000102808
2017-10-10T14:49:21.191777: step 1827, loss 0.0516676, acc 1, learning_rate 0.000102797
2017-10-10T14:49:21.626137: step 1828, loss 0.118817, acc 0.953125, learning_rate 0.000102785
2017-10-10T14:49:22.059996: step 1829, loss 0.120675, acc 0.953125, learning_rate 0.000102774
2017-10-10T14:49:22.512624: step 1830, loss 0.110756, acc 0.96875, learning_rate 0.000102763
2017-10-10T14:49:22.952930: step 1831, loss 0.0968171, acc 0.96875, learning_rate 0.000102751
2017-10-10T14:49:23.364083: step 1832, loss 0.0863251, acc 0.96875, learning_rate 0.00010274
2017-10-10T14:49:23.733702: step 1833, loss 0.122611, acc 0.953125, learning_rate 0.000102729
2017-10-10T14:49:24.111423: step 1834, loss 0.0624396, acc 0.984375, learning_rate 0.000102718
2017-10-10T14:49:24.508578: step 1835, loss 0.0411403, acc 1, learning_rate 0.000102707
2017-10-10T14:49:24.912037: step 1836, loss 0.183911, acc 0.921875, learning_rate 0.000102696
2017-10-10T14:49:25.337928: step 1837, loss 0.130365, acc 0.953125, learning_rate 0.000102685
2017-10-10T14:49:25.759263: step 1838, loss 0.115025, acc 0.953125, learning_rate 0.000102674
2017-10-10T14:49:26.181041: step 1839, loss 0.145534, acc 0.96875, learning_rate 0.000102663
2017-10-10T14:49:26.548852: step 1840, loss 0.0752437, acc 0.984375, learning_rate 0.000102652

Evaluation:
2017-10-10T14:49:27.440795: step 1840, loss 0.225679, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1840

2017-10-10T14:49:29.083482: step 1841, loss 0.239048, acc 0.9375, learning_rate 0.000102641
2017-10-10T14:49:29.457092: step 1842, loss 0.0879752, acc 0.984375, learning_rate 0.00010263
2017-10-10T14:49:29.873604: step 1843, loss 0.124368, acc 0.953125, learning_rate 0.00010262
2017-10-10T14:49:30.310100: step 1844, loss 0.110424, acc 0.96875, learning_rate 0.000102609
2017-10-10T14:49:30.722724: step 1845, loss 0.106595, acc 0.96875, learning_rate 0.000102598
2017-10-10T14:49:31.083786: step 1846, loss 0.155522, acc 0.9375, learning_rate 0.000102588
2017-10-10T14:49:31.568441: step 1847, loss 0.100629, acc 0.96875, learning_rate 0.000102577
2017-10-10T14:49:32.006456: step 1848, loss 0.16102, acc 0.921875, learning_rate 0.000102567
2017-10-10T14:49:32.401980: step 1849, loss 0.0877137, acc 0.96875, learning_rate 0.000102556
2017-10-10T14:49:32.676373: step 1850, loss 0.0784557, acc 0.984375, learning_rate 0.000102546
2017-10-10T14:49:33.213250: step 1851, loss 0.153638, acc 0.921875, learning_rate 0.000102535
2017-10-10T14:49:33.556870: step 1852, loss 0.150356, acc 0.953125, learning_rate 0.000102525
2017-10-10T14:49:33.955178: step 1853, loss 0.077943, acc 0.984375, learning_rate 0.000102515
2017-10-10T14:49:34.364876: step 1854, loss 0.0515137, acc 0.984375, learning_rate 0.000102504
2017-10-10T14:49:34.702108: step 1855, loss 0.116513, acc 0.96875, learning_rate 0.000102494
2017-10-10T14:49:35.105204: step 1856, loss 0.139838, acc 0.96875, learning_rate 0.000102484
2017-10-10T14:49:35.603592: step 1857, loss 0.108593, acc 0.953125, learning_rate 0.000102474
2017-10-10T14:49:35.956314: step 1858, loss 0.0635302, acc 0.96875, learning_rate 0.000102464
2017-10-10T14:49:36.406993: step 1859, loss 0.0555064, acc 1, learning_rate 0.000102454
2017-10-10T14:49:36.906292: step 1860, loss 0.187613, acc 0.9375, learning_rate 0.000102444
2017-10-10T14:49:37.252819: step 1861, loss 0.0832014, acc 0.984375, learning_rate 0.000102434
2017-10-10T14:49:37.549406: step 1862, loss 0.138615, acc 0.980392, learning_rate 0.000102424
2017-10-10T14:49:37.972678: step 1863, loss 0.0717921, acc 0.96875, learning_rate 0.000102414
2017-10-10T14:49:38.382911: step 1864, loss 0.211173, acc 0.921875, learning_rate 0.000102404
2017-10-10T14:49:38.728827: step 1865, loss 0.032714, acc 1, learning_rate 0.000102394
2017-10-10T14:49:39.155678: step 1866, loss 0.0482384, acc 1, learning_rate 0.000102384
2017-10-10T14:49:39.592940: step 1867, loss 0.150248, acc 0.9375, learning_rate 0.000102375
2017-10-10T14:49:40.005257: step 1868, loss 0.0952563, acc 0.953125, learning_rate 0.000102365
2017-10-10T14:49:40.414407: step 1869, loss 0.196724, acc 0.9375, learning_rate 0.000102355
2017-10-10T14:49:40.844838: step 1870, loss 0.0679197, acc 0.96875, learning_rate 0.000102346
2017-10-10T14:49:41.236146: step 1871, loss 0.0913354, acc 0.96875, learning_rate 0.000102336
2017-10-10T14:49:41.619658: step 1872, loss 0.189794, acc 0.921875, learning_rate 0.000102327
2017-10-10T14:49:42.077023: step 1873, loss 0.0815692, acc 0.984375, learning_rate 0.000102317
2017-10-10T14:49:42.575595: step 1874, loss 0.175139, acc 0.953125, learning_rate 0.000102308
2017-10-10T14:49:42.956852: step 1875, loss 0.0600709, acc 0.984375, learning_rate 0.000102298
2017-10-10T14:49:43.438467: step 1876, loss 0.171309, acc 0.96875, learning_rate 0.000102289
2017-10-10T14:49:43.865234: step 1877, loss 0.0536729, acc 0.984375, learning_rate 0.000102279
2017-10-10T14:49:44.409380: step 1878, loss 0.16217, acc 0.9375, learning_rate 0.00010227
2017-10-10T14:49:44.802963: step 1879, loss 0.0515092, acc 1, learning_rate 0.000102261
2017-10-10T14:49:45.240481: step 1880, loss 0.110053, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-10-10T14:49:46.200349: step 1880, loss 0.22566, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1880

2017-10-10T14:49:47.600863: step 1881, loss 0.153076, acc 0.921875, learning_rate 0.000102242
2017-10-10T14:49:48.030105: step 1882, loss 0.122053, acc 0.96875, learning_rate 0.000102233
2017-10-10T14:49:48.497038: step 1883, loss 0.0523299, acc 1, learning_rate 0.000102224
2017-10-10T14:49:48.880817: step 1884, loss 0.192951, acc 0.921875, learning_rate 0.000102215
2017-10-10T14:49:49.272986: step 1885, loss 0.0635219, acc 1, learning_rate 0.000102206
2017-10-10T14:49:49.686448: step 1886, loss 0.0584198, acc 0.984375, learning_rate 0.000102197
2017-10-10T14:49:50.061260: step 1887, loss 0.130104, acc 0.96875, learning_rate 0.000102188
2017-10-10T14:49:50.476934: step 1888, loss 0.186354, acc 0.921875, learning_rate 0.000102179
2017-10-10T14:49:50.914367: step 1889, loss 0.112151, acc 0.953125, learning_rate 0.00010217
2017-10-10T14:49:51.281035: step 1890, loss 0.109711, acc 0.953125, learning_rate 0.000102161
2017-10-10T14:49:51.692102: step 1891, loss 0.124955, acc 0.9375, learning_rate 0.000102153
2017-10-10T14:49:52.120769: step 1892, loss 0.0459868, acc 1, learning_rate 0.000102144
2017-10-10T14:49:52.511165: step 1893, loss 0.098202, acc 0.984375, learning_rate 0.000102135
2017-10-10T14:49:52.942329: step 1894, loss 0.0517818, acc 1, learning_rate 0.000102126
2017-10-10T14:49:53.362403: step 1895, loss 0.0815057, acc 0.96875, learning_rate 0.000102118
2017-10-10T14:49:53.863835: step 1896, loss 0.0397274, acc 0.984375, learning_rate 0.000102109
2017-10-10T14:49:54.200972: step 1897, loss 0.167249, acc 0.9375, learning_rate 0.0001021
2017-10-10T14:49:54.547681: step 1898, loss 0.271382, acc 0.90625, learning_rate 0.000102092
2017-10-10T14:49:54.898576: step 1899, loss 0.211359, acc 0.9375, learning_rate 0.000102083
2017-10-10T14:49:55.285516: step 1900, loss 0.180452, acc 0.921875, learning_rate 0.000102075
2017-10-10T14:49:55.642329: step 1901, loss 0.200239, acc 0.9375, learning_rate 0.000102066
2017-10-10T14:49:56.027934: step 1902, loss 0.0899088, acc 0.96875, learning_rate 0.000102058
2017-10-10T14:49:56.449052: step 1903, loss 0.144058, acc 0.921875, learning_rate 0.00010205
2017-10-10T14:49:56.861864: step 1904, loss 0.0698647, acc 0.984375, learning_rate 0.000102041
2017-10-10T14:49:57.313886: step 1905, loss 0.0927297, acc 0.96875, learning_rate 0.000102033
2017-10-10T14:49:57.660870: step 1906, loss 0.111033, acc 0.96875, learning_rate 0.000102025
2017-10-10T14:49:58.114895: step 1907, loss 0.0612346, acc 1, learning_rate 0.000102016
2017-10-10T14:49:58.512954: step 1908, loss 0.106273, acc 0.96875, learning_rate 0.000102008
2017-10-10T14:49:58.905732: step 1909, loss 0.0571212, acc 0.984375, learning_rate 0.000102
2017-10-10T14:49:59.312750: step 1910, loss 0.0379189, acc 1, learning_rate 0.000101992
2017-10-10T14:49:59.821173: step 1911, loss 0.119082, acc 0.984375, learning_rate 0.000101984
2017-10-10T14:50:00.191030: step 1912, loss 0.0696356, acc 0.984375, learning_rate 0.000101975
2017-10-10T14:50:00.444572: step 1913, loss 0.193107, acc 0.9375, learning_rate 0.000101967
2017-10-10T14:50:00.802677: step 1914, loss 0.0576128, acc 1, learning_rate 0.000101959
2017-10-10T14:50:01.206762: step 1915, loss 0.0820939, acc 0.984375, learning_rate 0.000101951
2017-10-10T14:50:01.643785: step 1916, loss 0.205554, acc 0.9375, learning_rate 0.000101943
2017-10-10T14:50:02.004230: step 1917, loss 0.129422, acc 0.953125, learning_rate 0.000101935
2017-10-10T14:50:02.393232: step 1918, loss 0.0845224, acc 0.96875, learning_rate 0.000101928
2017-10-10T14:50:02.827125: step 1919, loss 0.0653197, acc 1, learning_rate 0.00010192
2017-10-10T14:50:03.256983: step 1920, loss 0.172975, acc 0.953125, learning_rate 0.000101912

Evaluation:
2017-10-10T14:50:04.183531: step 1920, loss 0.224443, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1920

2017-10-10T14:50:05.626793: step 1921, loss 0.0285651, acc 1, learning_rate 0.000101904
2017-10-10T14:50:06.025925: step 1922, loss 0.182389, acc 0.921875, learning_rate 0.000101896
2017-10-10T14:50:06.426218: step 1923, loss 0.17195, acc 0.96875, learning_rate 0.000101889
2017-10-10T14:50:06.873065: step 1924, loss 0.128815, acc 0.953125, learning_rate 0.000101881
2017-10-10T14:50:07.262332: step 1925, loss 0.0945613, acc 0.984375, learning_rate 0.000101873
2017-10-10T14:50:07.689385: step 1926, loss 0.1372, acc 0.96875, learning_rate 0.000101865
2017-10-10T14:50:08.100867: step 1927, loss 0.200306, acc 0.953125, learning_rate 0.000101858
2017-10-10T14:50:08.509921: step 1928, loss 0.116688, acc 0.96875, learning_rate 0.00010185
2017-10-10T14:50:08.970848: step 1929, loss 0.170561, acc 0.9375, learning_rate 0.000101843
2017-10-10T14:50:09.352026: step 1930, loss 0.0402276, acc 1, learning_rate 0.000101835
2017-10-10T14:50:09.760981: step 1931, loss 0.0684893, acc 0.984375, learning_rate 0.000101828
2017-10-10T14:50:10.285105: step 1932, loss 0.0737034, acc 0.984375, learning_rate 0.00010182
2017-10-10T14:50:10.673348: step 1933, loss 0.163189, acc 0.9375, learning_rate 0.000101813
2017-10-10T14:50:11.049491: step 1934, loss 0.169095, acc 0.9375, learning_rate 0.000101805
2017-10-10T14:50:11.433161: step 1935, loss 0.204109, acc 0.9375, learning_rate 0.000101798
2017-10-10T14:50:11.762742: step 1936, loss 0.0894139, acc 0.953125, learning_rate 0.000101791
2017-10-10T14:50:12.277090: step 1937, loss 0.098519, acc 0.953125, learning_rate 0.000101783
2017-10-10T14:50:12.664895: step 1938, loss 0.0442308, acc 0.984375, learning_rate 0.000101776
2017-10-10T14:50:13.088275: step 1939, loss 0.132464, acc 0.9375, learning_rate 0.000101769
2017-10-10T14:50:13.406985: step 1940, loss 0.145955, acc 0.96875, learning_rate 0.000101762
2017-10-10T14:50:13.754337: step 1941, loss 0.0464693, acc 1, learning_rate 0.000101754
2017-10-10T14:50:14.124870: step 1942, loss 0.130851, acc 0.9375, learning_rate 0.000101747
2017-10-10T14:50:14.512923: step 1943, loss 0.0735177, acc 0.984375, learning_rate 0.00010174
2017-10-10T14:50:14.941113: step 1944, loss 0.0879441, acc 0.984375, learning_rate 0.000101733
2017-10-10T14:50:15.365000: step 1945, loss 0.1037, acc 0.96875, learning_rate 0.000101726
2017-10-10T14:50:15.748617: step 1946, loss 0.108854, acc 0.984375, learning_rate 0.000101719
2017-10-10T14:50:16.094337: step 1947, loss 0.115925, acc 0.96875, learning_rate 0.000101712
2017-10-10T14:50:16.577297: step 1948, loss 0.222133, acc 0.890625, learning_rate 0.000101705
2017-10-10T14:50:16.971383: step 1949, loss 0.175939, acc 0.953125, learning_rate 0.000101698
2017-10-10T14:50:17.384821: step 1950, loss 0.131495, acc 0.9375, learning_rate 0.000101691
2017-10-10T14:50:17.782061: step 1951, loss 0.0539796, acc 0.984375, learning_rate 0.000101684
2017-10-10T14:50:18.200162: step 1952, loss 0.221598, acc 0.90625, learning_rate 0.000101677
2017-10-10T14:50:18.626528: step 1953, loss 0.0620687, acc 0.984375, learning_rate 0.00010167
2017-10-10T14:50:19.077034: step 1954, loss 0.155087, acc 0.953125, learning_rate 0.000101664
2017-10-10T14:50:19.502506: step 1955, loss 0.113904, acc 0.953125, learning_rate 0.000101657
2017-10-10T14:50:19.973766: step 1956, loss 0.0492634, acc 1, learning_rate 0.00010165
2017-10-10T14:50:20.442865: step 1957, loss 0.0621788, acc 0.984375, learning_rate 0.000101643
2017-10-10T14:50:20.856861: step 1958, loss 0.0340601, acc 1, learning_rate 0.000101637
2017-10-10T14:50:21.249653: step 1959, loss 0.138855, acc 0.984375, learning_rate 0.00010163
2017-10-10T14:50:21.674256: step 1960, loss 0.0616377, acc 1, learning_rate 0.000101623

Evaluation:
2017-10-10T14:50:22.484935: step 1960, loss 0.223858, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-1960

2017-10-10T14:50:23.748825: step 1961, loss 0.0547345, acc 0.984375, learning_rate 0.000101617
2017-10-10T14:50:24.201875: step 1962, loss 0.0936148, acc 0.984375, learning_rate 0.00010161
2017-10-10T14:50:24.565987: step 1963, loss 0.125767, acc 0.9375, learning_rate 0.000101604
2017-10-10T14:50:25.053363: step 1964, loss 0.188766, acc 0.90625, learning_rate 0.000101597
2017-10-10T14:50:25.460876: step 1965, loss 0.0547189, acc 0.984375, learning_rate 0.00010159
2017-10-10T14:50:25.884949: step 1966, loss 0.10395, acc 0.953125, learning_rate 0.000101584
2017-10-10T14:50:26.376624: step 1967, loss 0.0733773, acc 1, learning_rate 0.000101577
2017-10-10T14:50:26.823808: step 1968, loss 0.0654611, acc 1, learning_rate 0.000101571
2017-10-10T14:50:27.192978: step 1969, loss 0.197398, acc 0.90625, learning_rate 0.000101565
2017-10-10T14:50:27.535674: step 1970, loss 0.0748166, acc 0.96875, learning_rate 0.000101558
2017-10-10T14:50:27.969019: step 1971, loss 0.130879, acc 0.953125, learning_rate 0.000101552
2017-10-10T14:50:28.393600: step 1972, loss 0.111523, acc 0.953125, learning_rate 0.000101546
2017-10-10T14:50:28.802615: step 1973, loss 0.182024, acc 0.921875, learning_rate 0.000101539
2017-10-10T14:50:29.232844: step 1974, loss 0.0408548, acc 1, learning_rate 0.000101533
2017-10-10T14:50:29.684934: step 1975, loss 0.120688, acc 0.96875, learning_rate 0.000101527
2017-10-10T14:50:30.085162: step 1976, loss 0.13718, acc 0.9375, learning_rate 0.00010152
2017-10-10T14:50:30.584846: step 1977, loss 0.0808045, acc 0.984375, learning_rate 0.000101514
2017-10-10T14:50:31.018301: step 1978, loss 0.0673461, acc 0.953125, learning_rate 0.000101508
2017-10-10T14:50:31.465060: step 1979, loss 0.110475, acc 0.984375, learning_rate 0.000101502
2017-10-10T14:50:31.816850: step 1980, loss 0.113173, acc 0.96875, learning_rate 0.000101496
2017-10-10T14:50:32.288565: step 1981, loss 0.132022, acc 0.9375, learning_rate 0.00010149
2017-10-10T14:50:32.750690: step 1982, loss 0.12739, acc 0.9375, learning_rate 0.000101484
2017-10-10T14:50:33.127312: step 1983, loss 0.145548, acc 0.921875, learning_rate 0.000101478
2017-10-10T14:50:33.504944: step 1984, loss 0.122838, acc 0.96875, learning_rate 0.000101472
2017-10-10T14:50:33.905315: step 1985, loss 0.0578497, acc 0.984375, learning_rate 0.000101466
2017-10-10T14:50:34.316046: step 1986, loss 0.156992, acc 0.9375, learning_rate 0.00010146
2017-10-10T14:50:34.677012: step 1987, loss 0.101389, acc 0.9375, learning_rate 0.000101454
2017-10-10T14:50:35.077230: step 1988, loss 0.21461, acc 0.953125, learning_rate 0.000101448
2017-10-10T14:50:35.512567: step 1989, loss 0.115652, acc 0.96875, learning_rate 0.000101442
2017-10-10T14:50:35.914882: step 1990, loss 0.145995, acc 0.953125, learning_rate 0.000101436
2017-10-10T14:50:36.323455: step 1991, loss 0.0351322, acc 1, learning_rate 0.00010143
2017-10-10T14:50:36.732834: step 1992, loss 0.0622447, acc 0.96875, learning_rate 0.000101424
2017-10-10T14:50:37.186346: step 1993, loss 0.185304, acc 0.90625, learning_rate 0.000101418
2017-10-10T14:50:37.589687: step 1994, loss 0.14209, acc 0.953125, learning_rate 0.000101413
2017-10-10T14:50:38.036846: step 1995, loss 0.109562, acc 0.953125, learning_rate 0.000101407
2017-10-10T14:50:38.428895: step 1996, loss 0.069601, acc 0.96875, learning_rate 0.000101401
2017-10-10T14:50:38.807239: step 1997, loss 0.121027, acc 0.96875, learning_rate 0.000101395
2017-10-10T14:50:39.224391: step 1998, loss 0.207545, acc 0.921875, learning_rate 0.00010139
2017-10-10T14:50:39.762664: step 1999, loss 0.0805096, acc 0.984375, learning_rate 0.000101384
2017-10-10T14:50:40.100025: step 2000, loss 0.227749, acc 0.890625, learning_rate 0.000101378

Evaluation:
2017-10-10T14:50:40.988217: step 2000, loss 0.22356, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2000

2017-10-10T14:50:42.260980: step 2001, loss 0.0367063, acc 1, learning_rate 0.000101373
2017-10-10T14:50:42.699814: step 2002, loss 0.112583, acc 0.953125, learning_rate 0.000101367
2017-10-10T14:50:43.218024: step 2003, loss 0.118841, acc 0.953125, learning_rate 0.000101362
2017-10-10T14:50:43.635963: step 2004, loss 0.150996, acc 0.921875, learning_rate 0.000101356
2017-10-10T14:50:44.013900: step 2005, loss 0.134385, acc 0.953125, learning_rate 0.00010135
2017-10-10T14:50:44.347995: step 2006, loss 0.112759, acc 0.953125, learning_rate 0.000101345
2017-10-10T14:50:44.748832: step 2007, loss 0.0530677, acc 0.984375, learning_rate 0.000101339
2017-10-10T14:50:45.218285: step 2008, loss 0.0906902, acc 0.96875, learning_rate 0.000101334
2017-10-10T14:50:45.583630: step 2009, loss 0.0709702, acc 0.984375, learning_rate 0.000101328
2017-10-10T14:50:46.059144: step 2010, loss 0.141291, acc 0.96875, learning_rate 0.000101323
2017-10-10T14:50:46.508256: step 2011, loss 0.136467, acc 0.9375, learning_rate 0.000101318
2017-10-10T14:50:46.923512: step 2012, loss 0.169323, acc 0.921875, learning_rate 0.000101312
2017-10-10T14:50:47.301014: step 2013, loss 0.223263, acc 0.9375, learning_rate 0.000101307
2017-10-10T14:50:47.760634: step 2014, loss 0.0550266, acc 0.984375, learning_rate 0.000101302
2017-10-10T14:50:48.216894: step 2015, loss 0.0749781, acc 0.984375, learning_rate 0.000101296
2017-10-10T14:50:48.633405: step 2016, loss 0.0940989, acc 0.96875, learning_rate 0.000101291
2017-10-10T14:50:49.024129: step 2017, loss 0.0993758, acc 0.96875, learning_rate 0.000101286
2017-10-10T14:50:49.464444: step 2018, loss 0.0715605, acc 0.984375, learning_rate 0.00010128
2017-10-10T14:50:49.830687: step 2019, loss 0.0636379, acc 0.984375, learning_rate 0.000101275
2017-10-10T14:50:50.166197: step 2020, loss 0.153062, acc 0.96875, learning_rate 0.00010127
2017-10-10T14:50:50.522359: step 2021, loss 0.126338, acc 0.96875, learning_rate 0.000101265
2017-10-10T14:50:50.931807: step 2022, loss 0.161042, acc 0.953125, learning_rate 0.00010126
2017-10-10T14:50:51.354771: step 2023, loss 0.0710688, acc 0.96875, learning_rate 0.000101255
2017-10-10T14:50:51.793053: step 2024, loss 0.131678, acc 0.953125, learning_rate 0.000101249
2017-10-10T14:50:52.173009: step 2025, loss 0.0843446, acc 0.984375, learning_rate 0.000101244
2017-10-10T14:50:52.624973: step 2026, loss 0.0342864, acc 1, learning_rate 0.000101239
2017-10-10T14:50:52.964416: step 2027, loss 0.122492, acc 0.9375, learning_rate 0.000101234
2017-10-10T14:50:53.339233: step 2028, loss 0.0694613, acc 0.96875, learning_rate 0.000101229
2017-10-10T14:50:53.696527: step 2029, loss 0.132963, acc 0.953125, learning_rate 0.000101224
2017-10-10T14:50:54.108096: step 2030, loss 0.17019, acc 0.9375, learning_rate 0.000101219
2017-10-10T14:50:54.661980: step 2031, loss 0.0614625, acc 0.984375, learning_rate 0.000101214
2017-10-10T14:50:55.072387: step 2032, loss 0.0943168, acc 0.953125, learning_rate 0.000101209
2017-10-10T14:50:55.436911: step 2033, loss 0.108006, acc 0.96875, learning_rate 0.000101204
2017-10-10T14:50:55.813148: step 2034, loss 0.092388, acc 0.96875, learning_rate 0.000101199
2017-10-10T14:50:56.206324: step 2035, loss 0.0647651, acc 0.96875, learning_rate 0.000101194
2017-10-10T14:50:56.585048: step 2036, loss 0.0873009, acc 0.984375, learning_rate 0.00010119
2017-10-10T14:50:57.000890: step 2037, loss 0.0930735, acc 0.984375, learning_rate 0.000101185
2017-10-10T14:50:57.377847: step 2038, loss 0.109054, acc 0.96875, learning_rate 0.00010118
2017-10-10T14:50:57.768596: step 2039, loss 0.122835, acc 0.96875, learning_rate 0.000101175
2017-10-10T14:50:58.215390: step 2040, loss 0.142864, acc 0.953125, learning_rate 0.00010117

Evaluation:
2017-10-10T14:50:59.128006: step 2040, loss 0.225208, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2040

2017-10-10T14:51:00.479771: step 2041, loss 0.110993, acc 0.96875, learning_rate 0.000101166
2017-10-10T14:51:00.842642: step 2042, loss 0.174943, acc 0.9375, learning_rate 0.000101161
2017-10-10T14:51:01.236878: step 2043, loss 0.0950025, acc 0.953125, learning_rate 0.000101156
2017-10-10T14:51:01.663377: step 2044, loss 0.192874, acc 0.9375, learning_rate 0.000101151
2017-10-10T14:51:02.026007: step 2045, loss 0.0978938, acc 0.96875, learning_rate 0.000101147
2017-10-10T14:51:02.386006: step 2046, loss 0.0699352, acc 0.984375, learning_rate 0.000101142
2017-10-10T14:51:02.821236: step 2047, loss 0.299384, acc 0.921875, learning_rate 0.000101137
2017-10-10T14:51:03.279271: step 2048, loss 0.0976254, acc 0.9375, learning_rate 0.000101133
2017-10-10T14:51:03.709135: step 2049, loss 0.0857339, acc 0.96875, learning_rate 0.000101128
2017-10-10T14:51:04.141945: step 2050, loss 0.144893, acc 0.9375, learning_rate 0.000101123
2017-10-10T14:51:04.548876: step 2051, loss 0.093819, acc 0.953125, learning_rate 0.000101119
2017-10-10T14:51:04.976994: step 2052, loss 0.0707472, acc 0.984375, learning_rate 0.000101114
2017-10-10T14:51:05.366712: step 2053, loss 0.101504, acc 0.984375, learning_rate 0.00010111
2017-10-10T14:51:05.738892: step 2054, loss 0.123367, acc 0.9375, learning_rate 0.000101105
2017-10-10T14:51:06.189962: step 2055, loss 0.202597, acc 0.921875, learning_rate 0.000101101
2017-10-10T14:51:06.544979: step 2056, loss 0.100295, acc 0.96875, learning_rate 0.000101096
2017-10-10T14:51:06.811548: step 2057, loss 0.200321, acc 0.9375, learning_rate 0.000101092
2017-10-10T14:51:07.160848: step 2058, loss 0.148912, acc 0.921569, learning_rate 0.000101087
2017-10-10T14:51:07.576954: step 2059, loss 0.115106, acc 0.9375, learning_rate 0.000101083
2017-10-10T14:51:07.960900: step 2060, loss 0.0452779, acc 0.984375, learning_rate 0.000101078
2017-10-10T14:51:08.375287: step 2061, loss 0.205109, acc 0.921875, learning_rate 0.000101074
2017-10-10T14:51:08.824932: step 2062, loss 0.138407, acc 0.953125, learning_rate 0.00010107
2017-10-10T14:51:09.236811: step 2063, loss 0.0836361, acc 0.953125, learning_rate 0.000101065
2017-10-10T14:51:09.638313: step 2064, loss 0.063185, acc 1, learning_rate 0.000101061
2017-10-10T14:51:10.096583: step 2065, loss 0.156246, acc 0.953125, learning_rate 0.000101057
2017-10-10T14:51:10.453492: step 2066, loss 0.175761, acc 0.953125, learning_rate 0.000101052
2017-10-10T14:51:10.873361: step 2067, loss 0.100636, acc 0.96875, learning_rate 0.000101048
2017-10-10T14:51:11.277127: step 2068, loss 0.0387715, acc 1, learning_rate 0.000101044
2017-10-10T14:51:11.621512: step 2069, loss 0.0488048, acc 0.984375, learning_rate 0.000101039
2017-10-10T14:51:12.002045: step 2070, loss 0.0979767, acc 0.984375, learning_rate 0.000101035
2017-10-10T14:51:12.350039: step 2071, loss 0.148021, acc 0.96875, learning_rate 0.000101031
2017-10-10T14:51:12.757088: step 2072, loss 0.0918719, acc 0.96875, learning_rate 0.000101027
2017-10-10T14:51:13.117147: step 2073, loss 0.0653183, acc 0.984375, learning_rate 0.000101023
2017-10-10T14:51:13.553058: step 2074, loss 0.154947, acc 0.9375, learning_rate 0.000101018
2017-10-10T14:51:14.005715: step 2075, loss 0.0513347, acc 1, learning_rate 0.000101014
2017-10-10T14:51:14.362195: step 2076, loss 0.0934426, acc 0.984375, learning_rate 0.00010101
2017-10-10T14:51:14.740820: step 2077, loss 0.129077, acc 0.9375, learning_rate 0.000101006
2017-10-10T14:51:15.184869: step 2078, loss 0.297723, acc 0.90625, learning_rate 0.000101002
2017-10-10T14:51:15.607503: step 2079, loss 0.0845661, acc 0.984375, learning_rate 0.000100998
2017-10-10T14:51:16.034558: step 2080, loss 0.160524, acc 0.953125, learning_rate 0.000100994

Evaluation:
2017-10-10T14:52:51.356674: step 2080, loss 0.226456, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2080

2017-10-10T14:57:17.828034: step 2081, loss 0.087552, acc 0.984375, learning_rate 0.00010099
2017-10-10T14:57:24.773530: step 2082, loss 0.112082, acc 0.953125, learning_rate 0.000100986
2017-10-10T14:57:29.933793: step 2083, loss 0.16412, acc 0.96875, learning_rate 0.000100982
2017-10-10T14:57:31.534756: step 2084, loss 0.0993052, acc 0.96875, learning_rate 0.000100978
2017-10-10T14:57:34.612911: step 2085, loss 0.116995, acc 0.953125, learning_rate 0.000100974
2017-10-10T14:57:35.977069: step 2086, loss 0.124153, acc 0.953125, learning_rate 0.00010097
2017-10-10T14:57:39.411289: step 2087, loss 0.102429, acc 0.9375, learning_rate 0.000100966
2017-10-10T14:57:40.780761: step 2088, loss 0.180639, acc 0.953125, learning_rate 0.000100962
2017-10-10T14:57:42.554754: step 2089, loss 0.118465, acc 0.96875, learning_rate 0.000100958
2017-10-10T14:57:43.160579: step 2090, loss 0.0678323, acc 0.96875, learning_rate 0.000100954
2017-10-10T14:57:43.911960: step 2091, loss 0.117742, acc 0.9375, learning_rate 0.00010095
2017-10-10T14:57:44.887976: step 2092, loss 0.162953, acc 0.953125, learning_rate 0.000100946
2017-10-10T14:57:46.380434: step 2093, loss 0.0742825, acc 0.984375, learning_rate 0.000100942
2017-10-10T14:57:47.950066: step 2094, loss 0.176959, acc 0.921875, learning_rate 0.000100938
2017-10-10T14:57:48.554217: step 2095, loss 0.0766746, acc 1, learning_rate 0.000100935
2017-10-10T14:57:49.389505: step 2096, loss 0.0844752, acc 0.96875, learning_rate 0.000100931
2017-10-10T14:57:50.744842: step 2097, loss 0.0562207, acc 1, learning_rate 0.000100927
2017-10-10T14:57:51.345498: step 2098, loss 0.202576, acc 0.921875, learning_rate 0.000100923
2017-10-10T14:57:52.600595: step 2099, loss 0.108527, acc 0.984375, learning_rate 0.000100919
2017-10-10T14:57:53.774931: step 2100, loss 0.188315, acc 0.9375, learning_rate 0.000100916
2017-10-10T14:57:54.413294: step 2101, loss 0.0826837, acc 0.96875, learning_rate 0.000100912
2017-10-10T14:57:55.543178: step 2102, loss 0.0470681, acc 0.96875, learning_rate 0.000100908
2017-10-10T14:57:56.086156: step 2103, loss 0.0596436, acc 0.96875, learning_rate 0.000100904
2017-10-10T14:57:56.598172: step 2104, loss 0.0435032, acc 1, learning_rate 0.000100901
2017-10-10T14:57:57.390480: step 2105, loss 0.144101, acc 0.9375, learning_rate 0.000100897
2017-10-10T14:57:58.474036: step 2106, loss 0.0909567, acc 0.953125, learning_rate 0.000100893
2017-10-10T14:57:59.222782: step 2107, loss 0.0962877, acc 0.984375, learning_rate 0.00010089
2017-10-10T14:57:59.977818: step 2108, loss 0.112353, acc 0.96875, learning_rate 0.000100886
2017-10-10T14:58:00.877398: step 2109, loss 0.117057, acc 0.953125, learning_rate 0.000100883
2017-10-10T14:58:01.509660: step 2110, loss 0.067413, acc 0.984375, learning_rate 0.000100879
2017-10-10T14:58:02.156525: step 2111, loss 0.109698, acc 0.96875, learning_rate 0.000100875
2017-10-10T14:58:02.480848: step 2112, loss 0.229391, acc 0.9375, learning_rate 0.000100872
2017-10-10T14:58:02.963732: step 2113, loss 0.0775103, acc 0.984375, learning_rate 0.000100868
2017-10-10T14:58:03.476936: step 2114, loss 0.194102, acc 0.9375, learning_rate 0.000100865
2017-10-10T14:58:04.113105: step 2115, loss 0.131589, acc 0.96875, learning_rate 0.000100861
2017-10-10T14:58:05.228965: step 2116, loss 0.0733739, acc 0.953125, learning_rate 0.000100858
2017-10-10T14:58:05.796006: step 2117, loss 0.0721782, acc 0.984375, learning_rate 0.000100854
2017-10-10T14:58:06.321413: step 2118, loss 0.0508453, acc 0.984375, learning_rate 0.000100851
2017-10-10T14:58:06.908368: step 2119, loss 0.114373, acc 0.953125, learning_rate 0.000100847
2017-10-10T14:58:07.229133: step 2120, loss 0.106826, acc 0.953125, learning_rate 0.000100844

Evaluation:
2017-10-10T14:58:09.024751: step 2120, loss 0.224601, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2120

2017-10-10T14:58:10.252911: step 2121, loss 0.0549212, acc 0.984375, learning_rate 0.00010084
2017-10-10T14:58:10.752850: step 2122, loss 0.163433, acc 0.953125, learning_rate 0.000100837
2017-10-10T14:58:11.424846: step 2123, loss 0.168979, acc 0.96875, learning_rate 0.000100833
2017-10-10T14:58:12.698186: step 2124, loss 0.096102, acc 0.96875, learning_rate 0.00010083
2017-10-10T14:58:13.575101: step 2125, loss 0.0651962, acc 0.984375, learning_rate 0.000100827
2017-10-10T14:58:13.932481: step 2126, loss 0.140419, acc 0.96875, learning_rate 0.000100823
2017-10-10T14:58:14.337740: step 2127, loss 0.0893037, acc 0.953125, learning_rate 0.00010082
2017-10-10T14:58:14.628837: step 2128, loss 0.074297, acc 0.984375, learning_rate 0.000100817
2017-10-10T14:58:15.123659: step 2129, loss 0.128466, acc 0.953125, learning_rate 0.000100813
2017-10-10T14:58:15.691937: step 2130, loss 0.0983327, acc 0.96875, learning_rate 0.00010081
2017-10-10T14:58:16.181062: step 2131, loss 0.105171, acc 0.96875, learning_rate 0.000100807
2017-10-10T14:58:16.710624: step 2132, loss 0.0781478, acc 0.984375, learning_rate 0.000100803
2017-10-10T14:58:17.075082: step 2133, loss 0.0689342, acc 0.96875, learning_rate 0.0001008
2017-10-10T14:58:17.513420: step 2134, loss 0.156988, acc 0.953125, learning_rate 0.000100797
2017-10-10T14:58:18.090328: step 2135, loss 0.0835776, acc 0.984375, learning_rate 0.000100793
2017-10-10T14:58:18.480956: step 2136, loss 0.0902498, acc 1, learning_rate 0.00010079
2017-10-10T14:58:18.996898: step 2137, loss 0.183048, acc 0.953125, learning_rate 0.000100787
2017-10-10T14:58:19.411316: step 2138, loss 0.103955, acc 0.953125, learning_rate 0.000100784
2017-10-10T14:58:19.859636: step 2139, loss 0.119048, acc 0.953125, learning_rate 0.000100781
2017-10-10T14:58:20.332361: step 2140, loss 0.122047, acc 0.96875, learning_rate 0.000100777
2017-10-10T14:58:20.695289: step 2141, loss 0.0976272, acc 0.953125, learning_rate 0.000100774
2017-10-10T14:58:21.103566: step 2142, loss 0.0977853, acc 0.96875, learning_rate 0.000100771
2017-10-10T14:58:21.587857: step 2143, loss 0.125233, acc 0.953125, learning_rate 0.000100768
2017-10-10T14:58:22.085418: step 2144, loss 0.039458, acc 1, learning_rate 0.000100765
2017-10-10T14:58:22.511439: step 2145, loss 0.0566731, acc 1, learning_rate 0.000100762
2017-10-10T14:58:22.944703: step 2146, loss 0.0838083, acc 0.96875, learning_rate 0.000100759
2017-10-10T14:58:23.398177: step 2147, loss 0.0622312, acc 0.984375, learning_rate 0.000100755
2017-10-10T14:58:23.838037: step 2148, loss 0.123575, acc 0.953125, learning_rate 0.000100752
2017-10-10T14:58:24.220516: step 2149, loss 0.152695, acc 0.96875, learning_rate 0.000100749
2017-10-10T14:58:24.648951: step 2150, loss 0.12429, acc 0.96875, learning_rate 0.000100746
2017-10-10T14:58:25.085484: step 2151, loss 0.143727, acc 0.9375, learning_rate 0.000100743
2017-10-10T14:58:25.420401: step 2152, loss 0.174933, acc 0.984375, learning_rate 0.00010074
2017-10-10T14:58:25.812852: step 2153, loss 0.119469, acc 0.9375, learning_rate 0.000100737
2017-10-10T14:58:26.220581: step 2154, loss 0.0680374, acc 0.984375, learning_rate 0.000100734
2017-10-10T14:58:26.571528: step 2155, loss 0.124227, acc 0.96875, learning_rate 0.000100731
2017-10-10T14:58:26.952342: step 2156, loss 0.22021, acc 0.921569, learning_rate 0.000100728
2017-10-10T14:58:27.635310: step 2157, loss 0.0772986, acc 0.96875, learning_rate 0.000100725
2017-10-10T14:58:28.025282: step 2158, loss 0.0790486, acc 0.953125, learning_rate 0.000100722
2017-10-10T14:58:28.276772: step 2159, loss 0.146496, acc 0.96875, learning_rate 0.000100719
2017-10-10T14:58:28.573048: step 2160, loss 0.1483, acc 0.953125, learning_rate 0.000100716

Evaluation:
2017-10-10T14:58:29.281010: step 2160, loss 0.224443, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2160

2017-10-10T14:58:30.609409: step 2161, loss 0.0960542, acc 0.953125, learning_rate 0.000100713
2017-10-10T14:58:30.884805: step 2162, loss 0.146628, acc 0.9375, learning_rate 0.000100711
2017-10-10T14:58:31.208955: step 2163, loss 0.0480181, acc 0.96875, learning_rate 0.000100708
2017-10-10T14:58:31.527142: step 2164, loss 0.184838, acc 0.9375, learning_rate 0.000100705
2017-10-10T14:58:31.794641: step 2165, loss 0.140198, acc 0.96875, learning_rate 0.000100702
2017-10-10T14:58:32.628912: step 2166, loss 0.184545, acc 0.9375, learning_rate 0.000100699
2017-10-10T14:58:33.052494: step 2167, loss 0.107268, acc 0.953125, learning_rate 0.000100696
2017-10-10T14:58:33.436953: step 2168, loss 0.0628715, acc 0.96875, learning_rate 0.000100693
2017-10-10T14:58:33.875756: step 2169, loss 0.120094, acc 0.953125, learning_rate 0.00010069
2017-10-10T14:58:34.352067: step 2170, loss 0.134819, acc 0.9375, learning_rate 0.000100688
2017-10-10T14:58:34.727098: step 2171, loss 0.130719, acc 0.953125, learning_rate 0.000100685
2017-10-10T14:58:35.046102: step 2172, loss 0.157699, acc 0.921875, learning_rate 0.000100682
2017-10-10T14:58:35.456799: step 2173, loss 0.114451, acc 0.953125, learning_rate 0.000100679
2017-10-10T14:58:35.834494: step 2174, loss 0.11802, acc 0.9375, learning_rate 0.000100677
2017-10-10T14:58:36.183945: step 2175, loss 0.135274, acc 0.9375, learning_rate 0.000100674
2017-10-10T14:58:36.487605: step 2176, loss 0.198945, acc 0.921875, learning_rate 0.000100671
2017-10-10T14:58:36.936986: step 2177, loss 0.0434291, acc 0.984375, learning_rate 0.000100668
2017-10-10T14:58:37.642726: step 2178, loss 0.0919959, acc 0.96875, learning_rate 0.000100666
2017-10-10T14:58:38.038609: step 2179, loss 0.0529835, acc 0.984375, learning_rate 0.000100663
2017-10-10T14:58:38.460441: step 2180, loss 0.0833362, acc 0.96875, learning_rate 0.00010066
2017-10-10T14:58:38.818086: step 2181, loss 0.181695, acc 0.9375, learning_rate 0.000100657
2017-10-10T14:58:39.151248: step 2182, loss 0.107177, acc 0.96875, learning_rate 0.000100655
2017-10-10T14:58:39.447123: step 2183, loss 0.11663, acc 0.953125, learning_rate 0.000100652
2017-10-10T14:58:39.729596: step 2184, loss 0.139067, acc 0.953125, learning_rate 0.000100649
2017-10-10T14:58:40.030556: step 2185, loss 0.0650266, acc 0.96875, learning_rate 0.000100647
2017-10-10T14:58:40.339807: step 2186, loss 0.0768101, acc 0.984375, learning_rate 0.000100644
2017-10-10T14:58:40.668816: step 2187, loss 0.155273, acc 0.9375, learning_rate 0.000100641
2017-10-10T14:58:41.083867: step 2188, loss 0.145905, acc 0.953125, learning_rate 0.000100639
2017-10-10T14:58:41.468786: step 2189, loss 0.0937665, acc 0.984375, learning_rate 0.000100636
2017-10-10T14:58:41.745224: step 2190, loss 0.0346019, acc 0.984375, learning_rate 0.000100634
2017-10-10T14:58:41.999091: step 2191, loss 0.160803, acc 0.9375, learning_rate 0.000100631
2017-10-10T14:58:42.352819: step 2192, loss 0.145918, acc 0.9375, learning_rate 0.000100628
2017-10-10T14:58:42.636299: step 2193, loss 0.199671, acc 0.9375, learning_rate 0.000100626
2017-10-10T14:58:42.965125: step 2194, loss 0.0781624, acc 0.984375, learning_rate 0.000100623
2017-10-10T14:58:43.420958: step 2195, loss 0.044739, acc 1, learning_rate 0.000100621
2017-10-10T14:58:43.932918: step 2196, loss 0.0688723, acc 0.984375, learning_rate 0.000100618
2017-10-10T14:58:44.227982: step 2197, loss 0.0691555, acc 0.984375, learning_rate 0.000100616
2017-10-10T14:58:44.547073: step 2198, loss 0.0958095, acc 0.984375, learning_rate 0.000100613
2017-10-10T14:58:44.875342: step 2199, loss 0.158133, acc 0.96875, learning_rate 0.000100611
2017-10-10T14:58:45.319025: step 2200, loss 0.124624, acc 0.953125, learning_rate 0.000100608

Evaluation:
2017-10-10T14:58:46.084522: step 2200, loss 0.22218, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2200

2017-10-10T14:58:47.486523: step 2201, loss 0.109462, acc 0.953125, learning_rate 0.000100606
2017-10-10T14:58:47.855595: step 2202, loss 0.122194, acc 0.96875, learning_rate 0.000100603
2017-10-10T14:58:48.202486: step 2203, loss 0.118576, acc 0.953125, learning_rate 0.000100601
2017-10-10T14:58:48.567452: step 2204, loss 0.0632856, acc 0.984375, learning_rate 0.000100598
2017-10-10T14:58:49.024914: step 2205, loss 0.117589, acc 0.953125, learning_rate 0.000100596
2017-10-10T14:58:49.453948: step 2206, loss 0.0801887, acc 0.984375, learning_rate 0.000100594
2017-10-10T14:58:49.861975: step 2207, loss 0.171762, acc 0.96875, learning_rate 0.000100591
2017-10-10T14:58:50.430194: step 2208, loss 0.0665468, acc 0.96875, learning_rate 0.000100589
2017-10-10T14:58:50.828018: step 2209, loss 0.0967801, acc 0.953125, learning_rate 0.000100586
2017-10-10T14:58:51.253722: step 2210, loss 0.0617661, acc 0.96875, learning_rate 0.000100584
2017-10-10T14:58:51.644907: step 2211, loss 0.0371693, acc 1, learning_rate 0.000100581
2017-10-10T14:58:52.049119: step 2212, loss 0.0697009, acc 0.96875, learning_rate 0.000100579
2017-10-10T14:58:52.480946: step 2213, loss 0.0975443, acc 0.984375, learning_rate 0.000100577
2017-10-10T14:58:52.918106: step 2214, loss 0.0966816, acc 0.96875, learning_rate 0.000100574
2017-10-10T14:58:53.333503: step 2215, loss 0.092271, acc 0.96875, learning_rate 0.000100572
2017-10-10T14:58:53.758677: step 2216, loss 0.0787164, acc 0.953125, learning_rate 0.00010057
2017-10-10T14:58:54.190338: step 2217, loss 0.077706, acc 0.96875, learning_rate 0.000100567
2017-10-10T14:58:54.579040: step 2218, loss 0.244047, acc 0.96875, learning_rate 0.000100565
2017-10-10T14:58:55.002562: step 2219, loss 0.13052, acc 0.953125, learning_rate 0.000100563
2017-10-10T14:58:55.439725: step 2220, loss 0.0649365, acc 0.984375, learning_rate 0.00010056
2017-10-10T14:58:55.860871: step 2221, loss 0.144998, acc 0.921875, learning_rate 0.000100558
2017-10-10T14:58:56.272876: step 2222, loss 0.181347, acc 0.953125, learning_rate 0.000100556
2017-10-10T14:58:56.711066: step 2223, loss 0.192465, acc 0.90625, learning_rate 0.000100554
2017-10-10T14:58:57.102287: step 2224, loss 0.122989, acc 0.953125, learning_rate 0.000100551
2017-10-10T14:58:57.521217: step 2225, loss 0.144928, acc 0.9375, learning_rate 0.000100549
2017-10-10T14:58:57.903556: step 2226, loss 0.118887, acc 0.953125, learning_rate 0.000100547
2017-10-10T14:58:58.346002: step 2227, loss 0.0527082, acc 0.984375, learning_rate 0.000100545
2017-10-10T14:58:58.736763: step 2228, loss 0.0925035, acc 0.984375, learning_rate 0.000100542
2017-10-10T14:58:59.133183: step 2229, loss 0.116327, acc 0.96875, learning_rate 0.00010054
2017-10-10T14:58:59.562402: step 2230, loss 0.0578975, acc 1, learning_rate 0.000100538
2017-10-10T14:58:59.996093: step 2231, loss 0.156536, acc 0.9375, learning_rate 0.000100536
2017-10-10T14:59:00.496825: step 2232, loss 0.0715495, acc 0.96875, learning_rate 0.000100534
2017-10-10T14:59:00.957701: step 2233, loss 0.0833803, acc 1, learning_rate 0.000100531
2017-10-10T14:59:01.332335: step 2234, loss 0.119646, acc 0.9375, learning_rate 0.000100529
2017-10-10T14:59:01.685114: step 2235, loss 0.0927801, acc 0.953125, learning_rate 0.000100527
2017-10-10T14:59:02.093352: step 2236, loss 0.115024, acc 0.96875, learning_rate 0.000100525
2017-10-10T14:59:02.528103: step 2237, loss 0.0839907, acc 0.96875, learning_rate 0.000100523
2017-10-10T14:59:02.913226: step 2238, loss 0.0861475, acc 0.984375, learning_rate 0.000100521
2017-10-10T14:59:03.296681: step 2239, loss 0.0942672, acc 0.96875, learning_rate 0.000100519
2017-10-10T14:59:03.632931: step 2240, loss 0.0952888, acc 0.96875, learning_rate 0.000100516

Evaluation:
2017-10-10T14:59:04.402208: step 2240, loss 0.221988, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2240

2017-10-10T14:59:05.950180: step 2241, loss 0.14715, acc 0.90625, learning_rate 0.000100514
2017-10-10T14:59:06.328502: step 2242, loss 0.111104, acc 0.9375, learning_rate 0.000100512
2017-10-10T14:59:06.732861: step 2243, loss 0.221707, acc 0.9375, learning_rate 0.00010051
2017-10-10T14:59:07.259875: step 2244, loss 0.105355, acc 0.953125, learning_rate 0.000100508
2017-10-10T14:59:07.653731: step 2245, loss 0.113646, acc 0.953125, learning_rate 0.000100506
2017-10-10T14:59:08.027059: step 2246, loss 0.157073, acc 0.953125, learning_rate 0.000100504
2017-10-10T14:59:08.506721: step 2247, loss 0.053576, acc 1, learning_rate 0.000100502
2017-10-10T14:59:08.894677: step 2248, loss 0.127074, acc 0.953125, learning_rate 0.0001005
2017-10-10T14:59:09.321644: step 2249, loss 0.0749484, acc 0.96875, learning_rate 0.000100498
2017-10-10T14:59:09.757466: step 2250, loss 0.0750069, acc 1, learning_rate 0.000100496
2017-10-10T14:59:10.165175: step 2251, loss 0.231806, acc 0.90625, learning_rate 0.000100494
2017-10-10T14:59:10.637353: step 2252, loss 0.180761, acc 0.921875, learning_rate 0.000100492
2017-10-10T14:59:11.138988: step 2253, loss 0.106674, acc 0.953125, learning_rate 0.00010049
2017-10-10T14:59:11.478664: step 2254, loss 0.118124, acc 0.960784, learning_rate 0.000100488
2017-10-10T14:59:11.908492: step 2255, loss 0.109006, acc 0.96875, learning_rate 0.000100486
2017-10-10T14:59:12.341142: step 2256, loss 0.145678, acc 0.9375, learning_rate 0.000100484
2017-10-10T14:59:12.806331: step 2257, loss 0.188824, acc 0.9375, learning_rate 0.000100482
2017-10-10T14:59:13.173186: step 2258, loss 0.0705482, acc 0.984375, learning_rate 0.00010048
2017-10-10T14:59:13.605006: step 2259, loss 0.0532953, acc 1, learning_rate 0.000100478
2017-10-10T14:59:14.013139: step 2260, loss 0.0494087, acc 1, learning_rate 0.000100476
2017-10-10T14:59:14.459515: step 2261, loss 0.0361301, acc 0.984375, learning_rate 0.000100474
2017-10-10T14:59:14.876831: step 2262, loss 0.120167, acc 0.9375, learning_rate 0.000100472
2017-10-10T14:59:15.238301: step 2263, loss 0.112485, acc 0.984375, learning_rate 0.00010047
2017-10-10T14:59:15.666320: step 2264, loss 0.0333172, acc 1, learning_rate 0.000100468
2017-10-10T14:59:16.118145: step 2265, loss 0.130469, acc 0.953125, learning_rate 0.000100466
2017-10-10T14:59:16.523621: step 2266, loss 0.0922767, acc 0.96875, learning_rate 0.000100464
2017-10-10T14:59:16.935621: step 2267, loss 0.0967022, acc 0.953125, learning_rate 0.000100462
2017-10-10T14:59:17.372873: step 2268, loss 0.108754, acc 0.953125, learning_rate 0.000100461
2017-10-10T14:59:17.832930: step 2269, loss 0.0795769, acc 0.96875, learning_rate 0.000100459
2017-10-10T14:59:18.296492: step 2270, loss 0.135195, acc 0.953125, learning_rate 0.000100457
2017-10-10T14:59:18.604121: step 2271, loss 0.12192, acc 0.953125, learning_rate 0.000100455
2017-10-10T14:59:19.107896: step 2272, loss 0.116542, acc 0.953125, learning_rate 0.000100453
2017-10-10T14:59:19.502532: step 2273, loss 0.156262, acc 0.921875, learning_rate 0.000100451
2017-10-10T14:59:19.860833: step 2274, loss 0.183694, acc 0.953125, learning_rate 0.000100449
2017-10-10T14:59:20.256075: step 2275, loss 0.0857301, acc 0.984375, learning_rate 0.000100448
2017-10-10T14:59:20.664764: step 2276, loss 0.137673, acc 0.9375, learning_rate 0.000100446
2017-10-10T14:59:21.068498: step 2277, loss 0.135423, acc 0.953125, learning_rate 0.000100444
2017-10-10T14:59:21.488013: step 2278, loss 0.0939748, acc 0.96875, learning_rate 0.000100442
2017-10-10T14:59:21.895286: step 2279, loss 0.116738, acc 0.96875, learning_rate 0.00010044
2017-10-10T14:59:22.308104: step 2280, loss 0.180775, acc 0.953125, learning_rate 0.000100439

Evaluation:
2017-10-10T14:59:23.224266: step 2280, loss 0.2226, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2280

2017-10-10T14:59:24.639955: step 2281, loss 0.143626, acc 0.953125, learning_rate 0.000100437
2017-10-10T14:59:25.122459: step 2282, loss 0.0884585, acc 0.96875, learning_rate 0.000100435
2017-10-10T14:59:25.489628: step 2283, loss 0.16356, acc 0.9375, learning_rate 0.000100433
2017-10-10T14:59:25.839463: step 2284, loss 0.0860854, acc 0.96875, learning_rate 0.000100431
2017-10-10T14:59:26.227031: step 2285, loss 0.143215, acc 0.953125, learning_rate 0.00010043
2017-10-10T14:59:26.629398: step 2286, loss 0.0909225, acc 0.953125, learning_rate 0.000100428
2017-10-10T14:59:27.032496: step 2287, loss 0.0967219, acc 0.96875, learning_rate 0.000100426
2017-10-10T14:59:27.449126: step 2288, loss 0.0967988, acc 0.984375, learning_rate 0.000100424
2017-10-10T14:59:27.891905: step 2289, loss 0.160171, acc 0.96875, learning_rate 0.000100423
2017-10-10T14:59:28.316831: step 2290, loss 0.113672, acc 0.96875, learning_rate 0.000100421
2017-10-10T14:59:28.669203: step 2291, loss 0.0989891, acc 0.96875, learning_rate 0.000100419
2017-10-10T14:59:29.096338: step 2292, loss 0.0267373, acc 1, learning_rate 0.000100418
2017-10-10T14:59:29.508843: step 2293, loss 0.12816, acc 0.96875, learning_rate 0.000100416
2017-10-10T14:59:29.870747: step 2294, loss 0.054025, acc 1, learning_rate 0.000100414
2017-10-10T14:59:30.316986: step 2295, loss 0.0415616, acc 1, learning_rate 0.000100412
2017-10-10T14:59:30.697310: step 2296, loss 0.0941894, acc 0.96875, learning_rate 0.000100411
2017-10-10T14:59:31.079205: step 2297, loss 0.0658735, acc 0.96875, learning_rate 0.000100409
2017-10-10T14:59:31.498999: step 2298, loss 0.214956, acc 0.9375, learning_rate 0.000100407
2017-10-10T14:59:31.894882: step 2299, loss 0.230002, acc 0.921875, learning_rate 0.000100406
2017-10-10T14:59:32.279794: step 2300, loss 0.110089, acc 0.96875, learning_rate 0.000100404
2017-10-10T14:59:32.767370: step 2301, loss 0.134249, acc 0.953125, learning_rate 0.000100402
2017-10-10T14:59:33.132258: step 2302, loss 0.134466, acc 0.921875, learning_rate 0.000100401
2017-10-10T14:59:33.469182: step 2303, loss 0.041831, acc 0.984375, learning_rate 0.000100399
2017-10-10T14:59:33.776751: step 2304, loss 0.0630058, acc 0.984375, learning_rate 0.000100398
2017-10-10T14:59:34.092479: step 2305, loss 0.0779529, acc 0.953125, learning_rate 0.000100396
2017-10-10T14:59:34.536970: step 2306, loss 0.0625324, acc 1, learning_rate 0.000100394
2017-10-10T14:59:34.996972: step 2307, loss 0.0870662, acc 0.96875, learning_rate 0.000100393
2017-10-10T14:59:35.398792: step 2308, loss 0.108576, acc 0.96875, learning_rate 0.000100391
2017-10-10T14:59:35.732102: step 2309, loss 0.0898386, acc 0.96875, learning_rate 0.000100389
2017-10-10T14:59:36.135809: step 2310, loss 0.159045, acc 0.9375, learning_rate 0.000100388
2017-10-10T14:59:36.530277: step 2311, loss 0.0542018, acc 0.984375, learning_rate 0.000100386
2017-10-10T14:59:36.956869: step 2312, loss 0.0429704, acc 0.984375, learning_rate 0.000100385
2017-10-10T14:59:37.395139: step 2313, loss 0.094209, acc 0.984375, learning_rate 0.000100383
2017-10-10T14:59:37.759478: step 2314, loss 0.0765648, acc 0.96875, learning_rate 0.000100382
2017-10-10T14:59:38.161975: step 2315, loss 0.104659, acc 0.9375, learning_rate 0.00010038
2017-10-10T14:59:38.560525: step 2316, loss 0.138121, acc 0.953125, learning_rate 0.000100378
2017-10-10T14:59:38.977112: step 2317, loss 0.0384576, acc 1, learning_rate 0.000100377
2017-10-10T14:59:39.408848: step 2318, loss 0.154548, acc 0.9375, learning_rate 0.000100375
2017-10-10T14:59:39.809126: step 2319, loss 0.123508, acc 0.953125, learning_rate 0.000100374
2017-10-10T14:59:40.240086: step 2320, loss 0.145585, acc 0.921875, learning_rate 0.000100372

Evaluation:
2017-10-10T14:59:41.055064: step 2320, loss 0.221614, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2320

2017-10-10T14:59:42.302864: step 2321, loss 0.134256, acc 0.953125, learning_rate 0.000100371
2017-10-10T14:59:42.723434: step 2322, loss 0.0806487, acc 0.953125, learning_rate 0.000100369
2017-10-10T14:59:43.101041: step 2323, loss 0.237194, acc 0.9375, learning_rate 0.000100368
2017-10-10T14:59:43.588895: step 2324, loss 0.0460924, acc 1, learning_rate 0.000100366
2017-10-10T14:59:43.969798: step 2325, loss 0.143719, acc 0.984375, learning_rate 0.000100365
2017-10-10T14:59:44.360957: step 2326, loss 0.0497866, acc 1, learning_rate 0.000100363
2017-10-10T14:59:44.799851: step 2327, loss 0.0814072, acc 0.984375, learning_rate 0.000100362
2017-10-10T14:59:45.173293: step 2328, loss 0.0510918, acc 0.984375, learning_rate 0.00010036
2017-10-10T14:59:45.534730: step 2329, loss 0.137014, acc 0.9375, learning_rate 0.000100359
2017-10-10T14:59:46.048917: step 2330, loss 0.204532, acc 0.921875, learning_rate 0.000100357
2017-10-10T14:59:46.439006: step 2331, loss 0.116541, acc 0.953125, learning_rate 0.000100356
2017-10-10T14:59:46.837594: step 2332, loss 0.0500745, acc 0.984375, learning_rate 0.000100354
2017-10-10T14:59:47.252855: step 2333, loss 0.0778941, acc 0.96875, learning_rate 0.000100353
2017-10-10T14:59:47.673046: step 2334, loss 0.274616, acc 0.890625, learning_rate 0.000100352
2017-10-10T14:59:48.047415: step 2335, loss 0.0675025, acc 0.96875, learning_rate 0.00010035
2017-10-10T14:59:48.443640: step 2336, loss 0.172967, acc 0.953125, learning_rate 0.000100349
2017-10-10T14:59:48.819545: step 2337, loss 0.190388, acc 0.9375, learning_rate 0.000100347
2017-10-10T14:59:49.283227: step 2338, loss 0.0818802, acc 0.953125, learning_rate 0.000100346
2017-10-10T14:59:49.733400: step 2339, loss 0.138541, acc 0.953125, learning_rate 0.000100344
2017-10-10T14:59:50.112382: step 2340, loss 0.0664405, acc 0.984375, learning_rate 0.000100343
2017-10-10T14:59:50.561519: step 2341, loss 0.0465203, acc 0.96875, learning_rate 0.000100342
2017-10-10T14:59:51.024931: step 2342, loss 0.0487844, acc 0.984375, learning_rate 0.00010034
2017-10-10T14:59:51.422409: step 2343, loss 0.168242, acc 0.90625, learning_rate 0.000100339
2017-10-10T14:59:51.889174: step 2344, loss 0.154355, acc 0.921875, learning_rate 0.000100338
2017-10-10T14:59:52.299936: step 2345, loss 0.182898, acc 0.921875, learning_rate 0.000100336
2017-10-10T14:59:52.657969: step 2346, loss 0.223407, acc 0.9375, learning_rate 0.000100335
2017-10-10T14:59:53.005244: step 2347, loss 0.0682539, acc 0.984375, learning_rate 0.000100333
2017-10-10T14:59:53.373791: step 2348, loss 0.091335, acc 0.984375, learning_rate 0.000100332
2017-10-10T14:59:53.761685: step 2349, loss 0.179435, acc 0.9375, learning_rate 0.000100331
2017-10-10T14:59:54.183718: step 2350, loss 0.0601435, acc 0.984375, learning_rate 0.000100329
2017-10-10T14:59:54.671377: step 2351, loss 0.193771, acc 0.953125, learning_rate 0.000100328
2017-10-10T14:59:55.051249: step 2352, loss 0.0500795, acc 0.980392, learning_rate 0.000100327
2017-10-10T14:59:55.461107: step 2353, loss 0.116186, acc 0.953125, learning_rate 0.000100325
2017-10-10T14:59:55.874095: step 2354, loss 0.0744833, acc 0.96875, learning_rate 0.000100324
2017-10-10T14:59:56.353177: step 2355, loss 0.135054, acc 0.953125, learning_rate 0.000100323
2017-10-10T14:59:56.708795: step 2356, loss 0.0556514, acc 0.984375, learning_rate 0.000100321
2017-10-10T14:59:57.133245: step 2357, loss 0.0884363, acc 0.96875, learning_rate 0.00010032
2017-10-10T14:59:57.520244: step 2358, loss 0.113538, acc 0.9375, learning_rate 0.000100319
2017-10-10T14:59:57.991148: step 2359, loss 0.172046, acc 0.9375, learning_rate 0.000100317
2017-10-10T14:59:58.399613: step 2360, loss 0.0960941, acc 0.96875, learning_rate 0.000100316

Evaluation:
2017-10-10T14:59:59.223261: step 2360, loss 0.220281, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2360

2017-10-10T15:00:00.516902: step 2361, loss 0.101808, acc 0.96875, learning_rate 0.000100315
2017-10-10T15:00:00.974145: step 2362, loss 0.118893, acc 0.96875, learning_rate 0.000100314
2017-10-10T15:00:01.360954: step 2363, loss 0.166368, acc 0.9375, learning_rate 0.000100312
2017-10-10T15:00:01.764988: step 2364, loss 0.239574, acc 0.921875, learning_rate 0.000100311
2017-10-10T15:00:02.177096: step 2365, loss 0.0900513, acc 0.953125, learning_rate 0.00010031
2017-10-10T15:00:02.624953: step 2366, loss 0.0782645, acc 0.984375, learning_rate 0.000100308
2017-10-10T15:00:02.988945: step 2367, loss 0.0813314, acc 0.96875, learning_rate 0.000100307
2017-10-10T15:00:03.376238: step 2368, loss 0.119702, acc 0.96875, learning_rate 0.000100306
2017-10-10T15:00:03.785184: step 2369, loss 0.0491659, acc 0.984375, learning_rate 0.000100305
2017-10-10T15:00:04.176446: step 2370, loss 0.124991, acc 0.96875, learning_rate 0.000100303
2017-10-10T15:00:04.568173: step 2371, loss 0.0758601, acc 0.984375, learning_rate 0.000100302
2017-10-10T15:00:05.028827: step 2372, loss 0.0354517, acc 0.984375, learning_rate 0.000100301
2017-10-10T15:00:05.423830: step 2373, loss 0.0740416, acc 0.984375, learning_rate 0.0001003
2017-10-10T15:00:05.806198: step 2374, loss 0.094404, acc 0.96875, learning_rate 0.000100299
2017-10-10T15:00:06.240841: step 2375, loss 0.13902, acc 0.953125, learning_rate 0.000100297
2017-10-10T15:00:06.664970: step 2376, loss 0.100094, acc 0.953125, learning_rate 0.000100296
2017-10-10T15:00:07.030365: step 2377, loss 0.14547, acc 0.9375, learning_rate 0.000100295
2017-10-10T15:00:07.390591: step 2378, loss 0.162417, acc 0.96875, learning_rate 0.000100294
2017-10-10T15:00:07.777222: step 2379, loss 0.0702767, acc 0.984375, learning_rate 0.000100292
2017-10-10T15:00:08.101039: step 2380, loss 0.0544203, acc 0.984375, learning_rate 0.000100291
2017-10-10T15:00:08.580871: step 2381, loss 0.111556, acc 0.953125, learning_rate 0.00010029
2017-10-10T15:00:08.992896: step 2382, loss 0.0335502, acc 1, learning_rate 0.000100289
2017-10-10T15:00:09.348634: step 2383, loss 0.143245, acc 0.96875, learning_rate 0.000100288
2017-10-10T15:00:09.688846: step 2384, loss 0.0934277, acc 0.984375, learning_rate 0.000100287
2017-10-10T15:00:10.144925: step 2385, loss 0.0655275, acc 0.984375, learning_rate 0.000100285
2017-10-10T15:00:10.498325: step 2386, loss 0.0616857, acc 0.96875, learning_rate 0.000100284
2017-10-10T15:00:11.530676: step 2387, loss 0.264235, acc 0.9375, learning_rate 0.000100283
2017-10-10T15:00:11.949853: step 2388, loss 0.0960256, acc 0.953125, learning_rate 0.000100282
2017-10-10T15:00:12.350271: step 2389, loss 0.0901547, acc 0.984375, learning_rate 0.000100281
2017-10-10T15:00:12.714183: step 2390, loss 0.13354, acc 0.953125, learning_rate 0.00010028
2017-10-10T15:00:13.164262: step 2391, loss 0.141002, acc 0.953125, learning_rate 0.000100278
2017-10-10T15:00:13.547596: step 2392, loss 0.0707296, acc 0.984375, learning_rate 0.000100277
2017-10-10T15:00:13.863348: step 2393, loss 0.133341, acc 0.953125, learning_rate 0.000100276
2017-10-10T15:00:14.368480: step 2394, loss 0.0722227, acc 0.96875, learning_rate 0.000100275
2017-10-10T15:00:14.802199: step 2395, loss 0.0834174, acc 0.953125, learning_rate 0.000100274
2017-10-10T15:00:15.088422: step 2396, loss 0.190681, acc 0.9375, learning_rate 0.000100273
2017-10-10T15:00:15.384434: step 2397, loss 0.153612, acc 0.953125, learning_rate 0.000100272
2017-10-10T15:00:15.762909: step 2398, loss 0.0809979, acc 0.96875, learning_rate 0.000100271
2017-10-10T15:00:16.180838: step 2399, loss 0.0687305, acc 0.96875, learning_rate 0.00010027
2017-10-10T15:00:16.661619: step 2400, loss 0.0870422, acc 0.96875, learning_rate 0.000100268

Evaluation:
2017-10-10T15:00:17.515312: step 2400, loss 0.218848, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2400

2017-10-10T15:00:18.757088: step 2401, loss 0.0517504, acc 1, learning_rate 0.000100267
2017-10-10T15:00:19.155684: step 2402, loss 0.131467, acc 0.921875, learning_rate 0.000100266
2017-10-10T15:00:19.593301: step 2403, loss 0.0504281, acc 1, learning_rate 0.000100265
2017-10-10T15:00:19.988606: step 2404, loss 0.101173, acc 0.96875, learning_rate 0.000100264
2017-10-10T15:00:20.369082: step 2405, loss 0.0540147, acc 0.984375, learning_rate 0.000100263
2017-10-10T15:00:20.772685: step 2406, loss 0.198228, acc 0.921875, learning_rate 0.000100262
2017-10-10T15:00:21.121013: step 2407, loss 0.111272, acc 0.953125, learning_rate 0.000100261
2017-10-10T15:00:21.511332: step 2408, loss 0.144821, acc 0.953125, learning_rate 0.00010026
2017-10-10T15:00:21.932567: step 2409, loss 0.0400127, acc 0.984375, learning_rate 0.000100259
2017-10-10T15:00:22.378762: step 2410, loss 0.095869, acc 0.953125, learning_rate 0.000100258
2017-10-10T15:00:22.821079: step 2411, loss 0.16395, acc 0.953125, learning_rate 0.000100257
2017-10-10T15:00:23.224837: step 2412, loss 0.0899469, acc 0.96875, learning_rate 0.000100256
2017-10-10T15:00:23.652936: step 2413, loss 0.19414, acc 0.9375, learning_rate 0.000100255
2017-10-10T15:00:24.025006: step 2414, loss 0.0462302, acc 1, learning_rate 0.000100253
2017-10-10T15:00:24.436765: step 2415, loss 0.0790278, acc 0.96875, learning_rate 0.000100252
2017-10-10T15:00:24.845640: step 2416, loss 0.145631, acc 0.953125, learning_rate 0.000100251
2017-10-10T15:00:25.223193: step 2417, loss 0.108563, acc 0.953125, learning_rate 0.00010025
2017-10-10T15:00:25.741397: step 2418, loss 0.109665, acc 0.96875, learning_rate 0.000100249
2017-10-10T15:00:26.156156: step 2419, loss 0.141638, acc 0.96875, learning_rate 0.000100248
2017-10-10T15:00:26.485639: step 2420, loss 0.150619, acc 0.96875, learning_rate 0.000100247
2017-10-10T15:00:26.956805: step 2421, loss 0.061821, acc 0.953125, learning_rate 0.000100246
2017-10-10T15:00:27.284940: step 2422, loss 0.0822176, acc 0.96875, learning_rate 0.000100245
2017-10-10T15:00:27.624040: step 2423, loss 0.174016, acc 0.921875, learning_rate 0.000100244
2017-10-10T15:00:28.040992: step 2424, loss 0.0812038, acc 0.984375, learning_rate 0.000100243
2017-10-10T15:00:28.453616: step 2425, loss 0.227994, acc 0.90625, learning_rate 0.000100242
2017-10-10T15:00:28.859526: step 2426, loss 0.0728398, acc 0.96875, learning_rate 0.000100241
2017-10-10T15:00:29.270175: step 2427, loss 0.142278, acc 0.9375, learning_rate 0.00010024
2017-10-10T15:00:29.744485: step 2428, loss 0.129793, acc 0.953125, learning_rate 0.000100239
2017-10-10T15:00:30.120841: step 2429, loss 0.173001, acc 0.921875, learning_rate 0.000100238
2017-10-10T15:00:30.533680: step 2430, loss 0.0399261, acc 1, learning_rate 0.000100237
2017-10-10T15:00:30.995332: step 2431, loss 0.0544004, acc 1, learning_rate 0.000100236
2017-10-10T15:00:31.386168: step 2432, loss 0.0347085, acc 1, learning_rate 0.000100235
2017-10-10T15:00:31.852782: step 2433, loss 0.0387943, acc 1, learning_rate 0.000100235
2017-10-10T15:00:32.253108: step 2434, loss 0.0665858, acc 0.96875, learning_rate 0.000100234
2017-10-10T15:00:32.638962: step 2435, loss 0.128037, acc 0.96875, learning_rate 0.000100233
2017-10-10T15:00:33.076598: step 2436, loss 0.14058, acc 0.953125, learning_rate 0.000100232
2017-10-10T15:00:33.446039: step 2437, loss 0.0593517, acc 0.984375, learning_rate 0.000100231
2017-10-10T15:00:33.935402: step 2438, loss 0.121999, acc 0.953125, learning_rate 0.00010023
2017-10-10T15:00:34.302971: step 2439, loss 0.0869928, acc 0.96875, learning_rate 0.000100229
2017-10-10T15:00:34.700595: step 2440, loss 0.112344, acc 0.96875, learning_rate 0.000100228

Evaluation:
2017-10-10T15:00:35.414735: step 2440, loss 0.22397, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2440

2017-10-10T15:00:36.852965: step 2441, loss 0.115059, acc 0.953125, learning_rate 0.000100227
2017-10-10T15:00:37.233352: step 2442, loss 0.163963, acc 0.921875, learning_rate 0.000100226
2017-10-10T15:00:37.618905: step 2443, loss 0.055497, acc 1, learning_rate 0.000100225
2017-10-10T15:00:38.044971: step 2444, loss 0.145755, acc 0.9375, learning_rate 0.000100224
2017-10-10T15:00:38.524883: step 2445, loss 0.114392, acc 0.96875, learning_rate 0.000100223
2017-10-10T15:00:38.922729: step 2446, loss 0.129534, acc 0.9375, learning_rate 0.000100222
2017-10-10T15:00:39.363628: step 2447, loss 0.18051, acc 0.96875, learning_rate 0.000100221
2017-10-10T15:00:39.790859: step 2448, loss 0.109741, acc 0.984375, learning_rate 0.000100221
2017-10-10T15:00:40.149982: step 2449, loss 0.0922433, acc 0.96875, learning_rate 0.00010022
2017-10-10T15:00:40.474581: step 2450, loss 0.169933, acc 0.960784, learning_rate 0.000100219
2017-10-10T15:00:40.940987: step 2451, loss 0.12037, acc 0.953125, learning_rate 0.000100218
2017-10-10T15:00:41.338495: step 2452, loss 0.109842, acc 0.9375, learning_rate 0.000100217
2017-10-10T15:00:41.763589: step 2453, loss 0.110675, acc 0.953125, learning_rate 0.000100216
2017-10-10T15:00:42.139879: step 2454, loss 0.0932539, acc 0.984375, learning_rate 0.000100215
2017-10-10T15:00:42.686290: step 2455, loss 0.0435923, acc 0.984375, learning_rate 0.000100214
2017-10-10T15:00:43.102815: step 2456, loss 0.120549, acc 0.953125, learning_rate 0.000100213
2017-10-10T15:00:43.504868: step 2457, loss 0.135834, acc 0.984375, learning_rate 0.000100213
2017-10-10T15:00:43.896498: step 2458, loss 0.114953, acc 0.96875, learning_rate 0.000100212
2017-10-10T15:00:44.281517: step 2459, loss 0.10227, acc 0.984375, learning_rate 0.000100211
2017-10-10T15:00:44.677631: step 2460, loss 0.214003, acc 0.953125, learning_rate 0.00010021
2017-10-10T15:00:45.104901: step 2461, loss 0.180352, acc 0.9375, learning_rate 0.000100209
2017-10-10T15:00:45.551000: step 2462, loss 0.115638, acc 0.953125, learning_rate 0.000100208
2017-10-10T15:00:45.937607: step 2463, loss 0.0762347, acc 0.984375, learning_rate 0.000100207
2017-10-10T15:00:46.398476: step 2464, loss 0.140285, acc 0.953125, learning_rate 0.000100207
2017-10-10T15:00:46.817268: step 2465, loss 0.1158, acc 0.96875, learning_rate 0.000100206
2017-10-10T15:00:47.242706: step 2466, loss 0.127542, acc 0.953125, learning_rate 0.000100205
2017-10-10T15:00:47.692942: step 2467, loss 0.0413682, acc 1, learning_rate 0.000100204
2017-10-10T15:00:48.072884: step 2468, loss 0.0608764, acc 0.984375, learning_rate 0.000100203
2017-10-10T15:00:48.387088: step 2469, loss 0.0931599, acc 0.96875, learning_rate 0.000100202
2017-10-10T15:00:48.842460: step 2470, loss 0.164226, acc 0.9375, learning_rate 0.000100202
2017-10-10T15:00:49.229114: step 2471, loss 0.0738822, acc 0.984375, learning_rate 0.000100201
2017-10-10T15:00:49.655355: step 2472, loss 0.0494659, acc 0.984375, learning_rate 0.0001002
2017-10-10T15:00:50.050527: step 2473, loss 0.126245, acc 0.96875, learning_rate 0.000100199
2017-10-10T15:00:50.471887: step 2474, loss 0.120067, acc 0.9375, learning_rate 0.000100198
2017-10-10T15:00:50.869396: step 2475, loss 0.116976, acc 0.96875, learning_rate 0.000100198
2017-10-10T15:00:51.304346: step 2476, loss 0.0872425, acc 0.96875, learning_rate 0.000100197
2017-10-10T15:00:51.794558: step 2477, loss 0.173691, acc 0.96875, learning_rate 0.000100196
2017-10-10T15:00:52.158908: step 2478, loss 0.11661, acc 0.96875, learning_rate 0.000100195
2017-10-10T15:00:52.561699: step 2479, loss 0.0929974, acc 0.953125, learning_rate 0.000100194
2017-10-10T15:00:53.036023: step 2480, loss 0.0856282, acc 0.96875, learning_rate 0.000100194

Evaluation:
2017-10-10T15:00:53.888402: step 2480, loss 0.219144, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2480

2017-10-10T15:00:55.225180: step 2481, loss 0.134415, acc 0.953125, learning_rate 0.000100193
2017-10-10T15:00:55.632920: step 2482, loss 0.152373, acc 0.90625, learning_rate 0.000100192
2017-10-10T15:00:56.052328: step 2483, loss 0.148286, acc 0.953125, learning_rate 0.000100191
2017-10-10T15:00:56.450695: step 2484, loss 0.0738519, acc 0.96875, learning_rate 0.00010019
2017-10-10T15:00:56.896527: step 2485, loss 0.093798, acc 0.953125, learning_rate 0.00010019
2017-10-10T15:00:57.291045: step 2486, loss 0.123472, acc 0.96875, learning_rate 0.000100189
2017-10-10T15:00:57.776909: step 2487, loss 0.0811978, acc 0.984375, learning_rate 0.000100188
2017-10-10T15:00:58.257004: step 2488, loss 0.148466, acc 0.953125, learning_rate 0.000100187
2017-10-10T15:00:58.748904: step 2489, loss 0.0839709, acc 0.984375, learning_rate 0.000100187
2017-10-10T15:00:59.073665: step 2490, loss 0.135252, acc 0.984375, learning_rate 0.000100186
2017-10-10T15:00:59.536402: step 2491, loss 0.104197, acc 0.96875, learning_rate 0.000100185
2017-10-10T15:00:59.934059: step 2492, loss 0.161201, acc 0.953125, learning_rate 0.000100184
2017-10-10T15:01:00.276680: step 2493, loss 0.05171, acc 0.984375, learning_rate 0.000100183
2017-10-10T15:01:00.693523: step 2494, loss 0.145163, acc 0.9375, learning_rate 0.000100183
2017-10-10T15:01:01.070288: step 2495, loss 0.0653345, acc 0.984375, learning_rate 0.000100182
2017-10-10T15:01:01.453900: step 2496, loss 0.0562957, acc 1, learning_rate 0.000100181
2017-10-10T15:01:01.880701: step 2497, loss 0.0864287, acc 0.96875, learning_rate 0.000100181
2017-10-10T15:01:02.281090: step 2498, loss 0.251755, acc 0.9375, learning_rate 0.00010018
2017-10-10T15:01:02.692233: step 2499, loss 0.106942, acc 0.96875, learning_rate 0.000100179
2017-10-10T15:01:03.108368: step 2500, loss 0.138587, acc 0.953125, learning_rate 0.000100178
2017-10-10T15:01:03.436218: step 2501, loss 0.102617, acc 0.984375, learning_rate 0.000100178
2017-10-10T15:01:03.967810: step 2502, loss 0.126777, acc 0.96875, learning_rate 0.000100177
2017-10-10T15:01:04.316932: step 2503, loss 0.0956467, acc 0.953125, learning_rate 0.000100176
2017-10-10T15:01:04.745725: step 2504, loss 0.0944576, acc 0.96875, learning_rate 0.000100175
2017-10-10T15:01:05.188552: step 2505, loss 0.0483627, acc 0.984375, learning_rate 0.000100175
2017-10-10T15:01:05.641009: step 2506, loss 0.0800595, acc 0.984375, learning_rate 0.000100174
2017-10-10T15:01:06.082272: step 2507, loss 0.0870462, acc 0.96875, learning_rate 0.000100173
2017-10-10T15:01:06.436852: step 2508, loss 0.179113, acc 0.9375, learning_rate 0.000100173
2017-10-10T15:01:06.816143: step 2509, loss 0.133532, acc 0.953125, learning_rate 0.000100172
2017-10-10T15:01:07.263603: step 2510, loss 0.095031, acc 0.984375, learning_rate 0.000100171
2017-10-10T15:01:07.641008: step 2511, loss 0.103379, acc 0.96875, learning_rate 0.00010017
2017-10-10T15:01:08.087923: step 2512, loss 0.127643, acc 0.9375, learning_rate 0.00010017
2017-10-10T15:01:08.472915: step 2513, loss 0.0559215, acc 1, learning_rate 0.000100169
2017-10-10T15:01:08.921833: step 2514, loss 0.0486473, acc 0.984375, learning_rate 0.000100168
2017-10-10T15:01:09.305095: step 2515, loss 0.0977042, acc 0.96875, learning_rate 0.000100168
2017-10-10T15:01:09.753187: step 2516, loss 0.0757525, acc 0.984375, learning_rate 0.000100167
2017-10-10T15:01:10.080855: step 2517, loss 0.201861, acc 0.890625, learning_rate 0.000100166
2017-10-10T15:01:10.466422: step 2518, loss 0.0822966, acc 0.953125, learning_rate 0.000100166
2017-10-10T15:01:10.865186: step 2519, loss 0.143623, acc 0.9375, learning_rate 0.000100165
2017-10-10T15:01:11.355887: step 2520, loss 0.0915526, acc 0.96875, learning_rate 0.000100164

Evaluation:
2017-10-10T15:01:12.144054: step 2520, loss 0.22002, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2520

2017-10-10T15:01:13.664844: step 2521, loss 0.0697137, acc 0.984375, learning_rate 0.000100164
2017-10-10T15:01:14.072930: step 2522, loss 0.157063, acc 0.921875, learning_rate 0.000100163
2017-10-10T15:01:14.489113: step 2523, loss 0.0412969, acc 0.984375, learning_rate 0.000100162
2017-10-10T15:01:14.923682: step 2524, loss 0.0823157, acc 0.96875, learning_rate 0.000100162
2017-10-10T15:01:15.345269: step 2525, loss 0.100761, acc 0.96875, learning_rate 0.000100161
2017-10-10T15:01:15.816986: step 2526, loss 0.0476715, acc 0.984375, learning_rate 0.00010016
2017-10-10T15:01:16.224865: step 2527, loss 0.122571, acc 0.953125, learning_rate 0.00010016
2017-10-10T15:01:16.709028: step 2528, loss 0.0487006, acc 0.984375, learning_rate 0.000100159
2017-10-10T15:01:17.182648: step 2529, loss 0.179832, acc 0.9375, learning_rate 0.000100158
2017-10-10T15:01:17.605204: step 2530, loss 0.118832, acc 0.96875, learning_rate 0.000100158
2017-10-10T15:01:17.883268: step 2531, loss 0.120502, acc 0.984375, learning_rate 0.000100157
2017-10-10T15:01:18.285030: step 2532, loss 0.0722391, acc 0.984375, learning_rate 0.000100156
2017-10-10T15:01:18.704729: step 2533, loss 0.0262273, acc 1, learning_rate 0.000100156
2017-10-10T15:01:19.054895: step 2534, loss 0.10198, acc 0.96875, learning_rate 0.000100155
2017-10-10T15:01:19.538473: step 2535, loss 0.123693, acc 0.921875, learning_rate 0.000100155
2017-10-10T15:01:19.995511: step 2536, loss 0.101065, acc 0.96875, learning_rate 0.000100154
2017-10-10T15:01:20.357135: step 2537, loss 0.142879, acc 0.953125, learning_rate 0.000100153
2017-10-10T15:01:20.727170: step 2538, loss 0.0716114, acc 0.984375, learning_rate 0.000100153
2017-10-10T15:01:21.148915: step 2539, loss 0.0906068, acc 0.984375, learning_rate 0.000100152
2017-10-10T15:01:21.607104: step 2540, loss 0.16978, acc 0.953125, learning_rate 0.000100151
2017-10-10T15:01:21.991157: step 2541, loss 0.135513, acc 0.96875, learning_rate 0.000100151
2017-10-10T15:01:22.361099: step 2542, loss 0.0444641, acc 0.984375, learning_rate 0.00010015
2017-10-10T15:01:22.734134: step 2543, loss 0.174153, acc 0.890625, learning_rate 0.00010015
2017-10-10T15:01:23.134929: step 2544, loss 0.0540391, acc 0.984375, learning_rate 0.000100149
2017-10-10T15:01:23.507793: step 2545, loss 0.19674, acc 0.921875, learning_rate 0.000100148
2017-10-10T15:01:23.916937: step 2546, loss 0.178845, acc 0.921875, learning_rate 0.000100148
2017-10-10T15:01:24.342868: step 2547, loss 0.146967, acc 0.984375, learning_rate 0.000100147
2017-10-10T15:01:24.692321: step 2548, loss 0.0974592, acc 0.960784, learning_rate 0.000100147
2017-10-10T15:01:25.109094: step 2549, loss 0.08817, acc 0.984375, learning_rate 0.000100146
2017-10-10T15:01:25.520979: step 2550, loss 0.117791, acc 0.96875, learning_rate 0.000100145
2017-10-10T15:01:26.100872: step 2551, loss 0.0964043, acc 0.96875, learning_rate 0.000100145
2017-10-10T15:01:26.469797: step 2552, loss 0.0844148, acc 0.96875, learning_rate 0.000100144
2017-10-10T15:01:26.922343: step 2553, loss 0.0749782, acc 0.96875, learning_rate 0.000100144
2017-10-10T15:01:27.396917: step 2554, loss 0.170097, acc 0.921875, learning_rate 0.000100143
2017-10-10T15:01:27.825842: step 2555, loss 0.0796973, acc 0.96875, learning_rate 0.000100142
2017-10-10T15:01:28.225677: step 2556, loss 0.10183, acc 0.9375, learning_rate 0.000100142
2017-10-10T15:01:28.596852: step 2557, loss 0.0919704, acc 0.953125, learning_rate 0.000100141
2017-10-10T15:01:29.025388: step 2558, loss 0.085427, acc 0.953125, learning_rate 0.000100141
2017-10-10T15:01:29.476924: step 2559, loss 0.0825901, acc 0.96875, learning_rate 0.00010014
2017-10-10T15:01:29.860633: step 2560, loss 0.16281, acc 0.921875, learning_rate 0.00010014

Evaluation:
2017-10-10T15:01:30.740224: step 2560, loss 0.222326, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2560

2017-10-10T15:01:32.080882: step 2561, loss 0.134247, acc 0.953125, learning_rate 0.000100139
2017-10-10T15:01:32.453365: step 2562, loss 0.0401078, acc 0.984375, learning_rate 0.000100138
2017-10-10T15:01:32.951716: step 2563, loss 0.103298, acc 0.96875, learning_rate 0.000100138
2017-10-10T15:01:33.439409: step 2564, loss 0.169123, acc 0.953125, learning_rate 0.000100137
2017-10-10T15:01:33.821751: step 2565, loss 0.0295641, acc 1, learning_rate 0.000100137
2017-10-10T15:01:34.313043: step 2566, loss 0.152524, acc 0.953125, learning_rate 0.000100136
2017-10-10T15:01:34.697774: step 2567, loss 0.081141, acc 0.96875, learning_rate 0.000100136
2017-10-10T15:01:35.026062: step 2568, loss 0.0874274, acc 0.953125, learning_rate 0.000100135
2017-10-10T15:01:35.425469: step 2569, loss 0.17849, acc 0.953125, learning_rate 0.000100134
2017-10-10T15:01:35.813013: step 2570, loss 0.0506155, acc 1, learning_rate 0.000100134
2017-10-10T15:01:36.248839: step 2571, loss 0.0676726, acc 0.96875, learning_rate 0.000100133
2017-10-10T15:01:36.686092: step 2572, loss 0.0566806, acc 1, learning_rate 0.000100133
2017-10-10T15:01:37.091390: step 2573, loss 0.150893, acc 0.953125, learning_rate 0.000100132
2017-10-10T15:01:37.424863: step 2574, loss 0.119957, acc 0.953125, learning_rate 0.000100132
2017-10-10T15:01:37.885003: step 2575, loss 0.0343629, acc 1, learning_rate 0.000100131
2017-10-10T15:01:38.301669: step 2576, loss 0.0657432, acc 1, learning_rate 0.000100131
2017-10-10T15:01:38.716912: step 2577, loss 0.0945822, acc 0.96875, learning_rate 0.00010013
2017-10-10T15:01:39.173211: step 2578, loss 0.198821, acc 0.921875, learning_rate 0.00010013
2017-10-10T15:01:39.602578: step 2579, loss 0.210711, acc 0.921875, learning_rate 0.000100129
2017-10-10T15:01:40.017300: step 2580, loss 0.0583309, acc 0.984375, learning_rate 0.000100129
2017-10-10T15:01:40.416670: step 2581, loss 0.100613, acc 0.96875, learning_rate 0.000100128
2017-10-10T15:01:40.853115: step 2582, loss 0.124868, acc 0.96875, learning_rate 0.000100128
2017-10-10T15:01:41.233059: step 2583, loss 0.400968, acc 0.890625, learning_rate 0.000100127
2017-10-10T15:01:41.830175: step 2584, loss 0.0890866, acc 0.953125, learning_rate 0.000100126
2017-10-10T15:01:42.243342: step 2585, loss 0.0735521, acc 0.984375, learning_rate 0.000100126
2017-10-10T15:01:42.610355: step 2586, loss 0.0544086, acc 1, learning_rate 0.000100125
2017-10-10T15:01:43.026625: step 2587, loss 0.0620618, acc 0.984375, learning_rate 0.000100125
2017-10-10T15:01:43.432289: step 2588, loss 0.0781287, acc 1, learning_rate 0.000100124
2017-10-10T15:01:43.904928: step 2589, loss 0.0906244, acc 0.96875, learning_rate 0.000100124
2017-10-10T15:01:44.257070: step 2590, loss 0.0862547, acc 0.96875, learning_rate 0.000100123
2017-10-10T15:01:44.693417: step 2591, loss 0.0697114, acc 0.984375, learning_rate 0.000100123
2017-10-10T15:01:45.084399: step 2592, loss 0.0447192, acc 0.984375, learning_rate 0.000100122
2017-10-10T15:01:45.544912: step 2593, loss 0.234439, acc 0.9375, learning_rate 0.000100122
2017-10-10T15:01:46.016919: step 2594, loss 0.105958, acc 0.953125, learning_rate 0.000100121
2017-10-10T15:01:46.468853: step 2595, loss 0.0575362, acc 0.96875, learning_rate 0.000100121
2017-10-10T15:01:46.829012: step 2596, loss 0.10819, acc 0.953125, learning_rate 0.00010012
2017-10-10T15:01:47.232829: step 2597, loss 0.0287663, acc 1, learning_rate 0.00010012
2017-10-10T15:01:47.639624: step 2598, loss 0.142951, acc 0.96875, learning_rate 0.000100119
2017-10-10T15:01:48.052823: step 2599, loss 0.0840021, acc 0.96875, learning_rate 0.000100119
2017-10-10T15:01:48.440016: step 2600, loss 0.0891618, acc 0.984375, learning_rate 0.000100118

Evaluation:
2017-10-10T15:01:49.208962: step 2600, loss 0.221659, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2600

2017-10-10T15:01:50.643598: step 2601, loss 0.0793087, acc 0.953125, learning_rate 0.000100118
2017-10-10T15:01:51.026146: step 2602, loss 0.109305, acc 0.953125, learning_rate 0.000100117
2017-10-10T15:01:51.391075: step 2603, loss 0.103474, acc 0.96875, learning_rate 0.000100117
2017-10-10T15:01:51.785689: step 2604, loss 0.177206, acc 0.96875, learning_rate 0.000100117
2017-10-10T15:01:52.199095: step 2605, loss 0.0416258, acc 0.984375, learning_rate 0.000100116
2017-10-10T15:01:52.645035: step 2606, loss 0.0984062, acc 0.96875, learning_rate 0.000100116
2017-10-10T15:01:53.050809: step 2607, loss 0.192936, acc 0.921875, learning_rate 0.000100115
2017-10-10T15:01:53.479563: step 2608, loss 0.078648, acc 0.984375, learning_rate 0.000100115
2017-10-10T15:01:53.930750: step 2609, loss 0.139039, acc 0.9375, learning_rate 0.000100114
2017-10-10T15:01:54.319639: step 2610, loss 0.0425549, acc 0.984375, learning_rate 0.000100114
2017-10-10T15:01:54.716935: step 2611, loss 0.0628371, acc 0.984375, learning_rate 0.000100113
2017-10-10T15:01:55.051129: step 2612, loss 0.0846939, acc 0.984375, learning_rate 0.000100113
2017-10-10T15:01:55.455539: step 2613, loss 0.134533, acc 0.9375, learning_rate 0.000100112
2017-10-10T15:01:55.899561: step 2614, loss 0.158229, acc 0.90625, learning_rate 0.000100112
2017-10-10T15:01:56.284894: step 2615, loss 0.185385, acc 0.921875, learning_rate 0.000100111
2017-10-10T15:01:56.731803: step 2616, loss 0.154601, acc 0.953125, learning_rate 0.000100111
2017-10-10T15:01:57.204807: step 2617, loss 0.086517, acc 0.96875, learning_rate 0.000100111
2017-10-10T15:01:57.657264: step 2618, loss 0.0592115, acc 0.984375, learning_rate 0.00010011
2017-10-10T15:01:58.032988: step 2619, loss 0.12984, acc 0.9375, learning_rate 0.00010011
2017-10-10T15:01:58.449931: step 2620, loss 0.0901382, acc 0.96875, learning_rate 0.000100109
2017-10-10T15:01:58.912393: step 2621, loss 0.0864193, acc 0.96875, learning_rate 0.000100109
2017-10-10T15:01:59.356540: step 2622, loss 0.163806, acc 0.9375, learning_rate 0.000100108
2017-10-10T15:01:59.789677: step 2623, loss 0.0975913, acc 0.96875, learning_rate 0.000100108
2017-10-10T15:02:00.184973: step 2624, loss 0.105697, acc 0.96875, learning_rate 0.000100107
2017-10-10T15:02:00.628994: step 2625, loss 0.0772551, acc 0.953125, learning_rate 0.000100107
2017-10-10T15:02:01.034760: step 2626, loss 0.0488981, acc 1, learning_rate 0.000100107
2017-10-10T15:02:01.461107: step 2627, loss 0.083805, acc 0.984375, learning_rate 0.000100106
2017-10-10T15:02:01.880871: step 2628, loss 0.0466274, acc 0.984375, learning_rate 0.000100106
2017-10-10T15:02:02.261960: step 2629, loss 0.074285, acc 0.984375, learning_rate 0.000100105
2017-10-10T15:02:02.679554: step 2630, loss 0.225996, acc 0.921875, learning_rate 0.000100105
2017-10-10T15:02:03.108987: step 2631, loss 0.147322, acc 0.953125, learning_rate 0.000100104
2017-10-10T15:02:03.518650: step 2632, loss 0.093146, acc 0.953125, learning_rate 0.000100104
2017-10-10T15:02:03.887094: step 2633, loss 0.0645954, acc 0.984375, learning_rate 0.000100104
2017-10-10T15:02:04.280737: step 2634, loss 0.0989941, acc 0.96875, learning_rate 0.000100103
2017-10-10T15:02:04.637526: step 2635, loss 0.0588614, acc 0.984375, learning_rate 0.000100103
2017-10-10T15:02:05.115391: step 2636, loss 0.111179, acc 0.953125, learning_rate 0.000100102
2017-10-10T15:02:05.540671: step 2637, loss 0.0393548, acc 1, learning_rate 0.000100102
2017-10-10T15:02:06.040848: step 2638, loss 0.0920066, acc 0.96875, learning_rate 0.000100101
2017-10-10T15:02:06.403324: step 2639, loss 0.114165, acc 0.9375, learning_rate 0.000100101
2017-10-10T15:02:06.760921: step 2640, loss 0.124858, acc 0.953125, learning_rate 0.000100101

Evaluation:
2017-10-10T15:02:07.668869: step 2640, loss 0.219163, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2640

2017-10-10T15:02:08.790759: step 2641, loss 0.107251, acc 0.984375, learning_rate 0.0001001
2017-10-10T15:02:09.180100: step 2642, loss 0.121549, acc 0.9375, learning_rate 0.0001001
2017-10-10T15:02:09.563138: step 2643, loss 0.0577776, acc 0.984375, learning_rate 0.000100099
2017-10-10T15:02:09.938956: step 2644, loss 0.0854007, acc 0.984375, learning_rate 0.000100099
2017-10-10T15:02:10.340904: step 2645, loss 0.0484871, acc 1, learning_rate 0.000100099
2017-10-10T15:02:10.705864: step 2646, loss 0.109541, acc 0.960784, learning_rate 0.000100098
2017-10-10T15:02:11.163460: step 2647, loss 0.0709922, acc 0.984375, learning_rate 0.000100098
2017-10-10T15:02:11.616902: step 2648, loss 0.0792363, acc 0.96875, learning_rate 0.000100097
2017-10-10T15:02:12.048957: step 2649, loss 0.0672262, acc 0.96875, learning_rate 0.000100097
2017-10-10T15:02:12.456489: step 2650, loss 0.0800598, acc 0.984375, learning_rate 0.000100097
2017-10-10T15:02:12.852968: step 2651, loss 0.0686521, acc 0.96875, learning_rate 0.000100096
2017-10-10T15:02:13.320045: step 2652, loss 0.147618, acc 0.953125, learning_rate 0.000100096
2017-10-10T15:02:13.706621: step 2653, loss 0.0850747, acc 0.96875, learning_rate 0.000100095
2017-10-10T15:02:14.159408: step 2654, loss 0.0458303, acc 0.984375, learning_rate 0.000100095
2017-10-10T15:02:14.506510: step 2655, loss 0.0967826, acc 0.96875, learning_rate 0.000100095
2017-10-10T15:02:14.930117: step 2656, loss 0.147079, acc 0.9375, learning_rate 0.000100094
2017-10-10T15:02:15.360324: step 2657, loss 0.122089, acc 0.953125, learning_rate 0.000100094
2017-10-10T15:02:15.791208: step 2658, loss 0.269937, acc 0.9375, learning_rate 0.000100093
2017-10-10T15:02:16.222454: step 2659, loss 0.0627954, acc 0.984375, learning_rate 0.000100093
2017-10-10T15:02:16.566142: step 2660, loss 0.049158, acc 0.984375, learning_rate 0.000100093
2017-10-10T15:02:16.912492: step 2661, loss 0.152774, acc 0.90625, learning_rate 0.000100092
2017-10-10T15:02:17.252825: step 2662, loss 0.101875, acc 0.96875, learning_rate 0.000100092
2017-10-10T15:02:17.611260: step 2663, loss 0.0621342, acc 1, learning_rate 0.000100092
2017-10-10T15:02:17.969089: step 2664, loss 0.2065, acc 0.921875, learning_rate 0.000100091
2017-10-10T15:02:18.351684: step 2665, loss 0.0677389, acc 0.984375, learning_rate 0.000100091
2017-10-10T15:02:18.656784: step 2666, loss 0.102343, acc 0.96875, learning_rate 0.00010009
2017-10-10T15:02:18.936898: step 2667, loss 0.113171, acc 0.96875, learning_rate 0.00010009
2017-10-10T15:02:19.302003: step 2668, loss 0.13358, acc 0.921875, learning_rate 0.00010009
2017-10-10T15:02:19.630426: step 2669, loss 0.148049, acc 0.9375, learning_rate 0.000100089
2017-10-10T15:02:19.965314: step 2670, loss 0.0934968, acc 0.953125, learning_rate 0.000100089
2017-10-10T15:02:20.318070: step 2671, loss 0.145599, acc 0.953125, learning_rate 0.000100089
2017-10-10T15:02:20.776923: step 2672, loss 0.0414298, acc 0.984375, learning_rate 0.000100088
2017-10-10T15:02:21.179226: step 2673, loss 0.116109, acc 0.953125, learning_rate 0.000100088
2017-10-10T15:02:21.495634: step 2674, loss 0.0770223, acc 0.984375, learning_rate 0.000100088
2017-10-10T15:02:21.905146: step 2675, loss 0.0874159, acc 0.96875, learning_rate 0.000100087
2017-10-10T15:02:22.321418: step 2676, loss 0.124985, acc 0.953125, learning_rate 0.000100087
2017-10-10T15:02:22.783384: step 2677, loss 0.0797861, acc 0.984375, learning_rate 0.000100086
2017-10-10T15:02:23.292100: step 2678, loss 0.102318, acc 0.953125, learning_rate 0.000100086
2017-10-10T15:02:23.603104: step 2679, loss 0.0722745, acc 0.96875, learning_rate 0.000100086
2017-10-10T15:02:23.906353: step 2680, loss 0.0608459, acc 0.984375, learning_rate 0.000100085

Evaluation:
2017-10-10T15:02:24.747165: step 2680, loss 0.221007, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2680

2017-10-10T15:02:26.127392: step 2681, loss 0.0361457, acc 1, learning_rate 0.000100085
2017-10-10T15:02:26.545040: step 2682, loss 0.0518109, acc 0.984375, learning_rate 0.000100085
2017-10-10T15:02:26.963132: step 2683, loss 0.0548521, acc 0.96875, learning_rate 0.000100084
2017-10-10T15:02:27.405222: step 2684, loss 0.0375283, acc 1, learning_rate 0.000100084
2017-10-10T15:02:27.914934: step 2685, loss 0.116009, acc 0.953125, learning_rate 0.000100084
2017-10-10T15:02:28.281558: step 2686, loss 0.100258, acc 0.953125, learning_rate 0.000100083
2017-10-10T15:02:28.706546: step 2687, loss 0.0790277, acc 0.96875, learning_rate 0.000100083
2017-10-10T15:02:29.152071: step 2688, loss 0.0580354, acc 1, learning_rate 0.000100083
2017-10-10T15:02:29.549587: step 2689, loss 0.0916698, acc 0.96875, learning_rate 0.000100082
2017-10-10T15:02:30.077391: step 2690, loss 0.0992033, acc 0.96875, learning_rate 0.000100082
2017-10-10T15:02:30.494707: step 2691, loss 0.0808743, acc 0.984375, learning_rate 0.000100082
2017-10-10T15:02:30.898510: step 2692, loss 0.101701, acc 0.96875, learning_rate 0.000100081
2017-10-10T15:02:31.329076: step 2693, loss 0.119398, acc 0.953125, learning_rate 0.000100081
2017-10-10T15:02:31.828708: step 2694, loss 0.0656192, acc 0.96875, learning_rate 0.000100081
2017-10-10T15:02:32.186130: step 2695, loss 0.180422, acc 0.921875, learning_rate 0.00010008
2017-10-10T15:02:32.664963: step 2696, loss 0.0772387, acc 0.96875, learning_rate 0.00010008
2017-10-10T15:02:33.051122: step 2697, loss 0.0666709, acc 0.984375, learning_rate 0.00010008
2017-10-10T15:02:33.470270: step 2698, loss 0.106983, acc 0.984375, learning_rate 0.000100079
2017-10-10T15:02:33.908716: step 2699, loss 0.116143, acc 0.9375, learning_rate 0.000100079
2017-10-10T15:02:34.365634: step 2700, loss 0.0721304, acc 0.984375, learning_rate 0.000100079
2017-10-10T15:02:34.768328: step 2701, loss 0.0382367, acc 1, learning_rate 0.000100078
2017-10-10T15:02:35.209462: step 2702, loss 0.187243, acc 0.9375, learning_rate 0.000100078
2017-10-10T15:02:35.570046: step 2703, loss 0.140525, acc 0.9375, learning_rate 0.000100078
2017-10-10T15:02:36.009681: step 2704, loss 0.0574765, acc 1, learning_rate 0.000100077
2017-10-10T15:02:36.421926: step 2705, loss 0.193799, acc 0.953125, learning_rate 0.000100077
2017-10-10T15:02:36.851009: step 2706, loss 0.141386, acc 0.984375, learning_rate 0.000100077
2017-10-10T15:02:37.283702: step 2707, loss 0.0747836, acc 0.96875, learning_rate 0.000100076
2017-10-10T15:02:37.696156: step 2708, loss 0.0499145, acc 0.984375, learning_rate 0.000100076
2017-10-10T15:02:38.113056: step 2709, loss 0.113724, acc 0.984375, learning_rate 0.000100076
2017-10-10T15:02:38.505098: step 2710, loss 0.0286717, acc 0.984375, learning_rate 0.000100076
2017-10-10T15:02:39.029014: step 2711, loss 0.183717, acc 0.921875, learning_rate 0.000100075
2017-10-10T15:02:39.462050: step 2712, loss 0.107419, acc 0.984375, learning_rate 0.000100075
2017-10-10T15:02:39.856932: step 2713, loss 0.154221, acc 0.953125, learning_rate 0.000100075
2017-10-10T15:02:40.192029: step 2714, loss 0.122313, acc 0.984375, learning_rate 0.000100074
2017-10-10T15:02:40.540158: step 2715, loss 0.0695362, acc 0.984375, learning_rate 0.000100074
2017-10-10T15:02:40.897915: step 2716, loss 0.127804, acc 0.953125, learning_rate 0.000100074
2017-10-10T15:02:41.302108: step 2717, loss 0.0675668, acc 0.984375, learning_rate 0.000100073
2017-10-10T15:02:41.728556: step 2718, loss 0.0300654, acc 1, learning_rate 0.000100073
2017-10-10T15:02:42.151648: step 2719, loss 0.192784, acc 0.921875, learning_rate 0.000100073
2017-10-10T15:02:42.616491: step 2720, loss 0.0750475, acc 0.984375, learning_rate 0.000100073

Evaluation:
2017-10-10T15:02:43.323978: step 2720, loss 0.219067, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2720

2017-10-10T15:02:44.638672: step 2721, loss 0.0693776, acc 0.984375, learning_rate 0.000100072
2017-10-10T15:02:45.075184: step 2722, loss 0.0729565, acc 0.984375, learning_rate 0.000100072
2017-10-10T15:02:45.462571: step 2723, loss 0.0615883, acc 0.984375, learning_rate 0.000100072
2017-10-10T15:02:45.857339: step 2724, loss 0.0430119, acc 1, learning_rate 0.000100071
2017-10-10T15:02:46.325911: step 2725, loss 0.071328, acc 0.984375, learning_rate 0.000100071
2017-10-10T15:02:46.720596: step 2726, loss 0.0716597, acc 0.984375, learning_rate 0.000100071
2017-10-10T15:02:47.143464: step 2727, loss 0.0510151, acc 1, learning_rate 0.00010007
2017-10-10T15:02:47.562877: step 2728, loss 0.0529647, acc 0.984375, learning_rate 0.00010007
2017-10-10T15:02:47.978119: step 2729, loss 0.195288, acc 0.953125, learning_rate 0.00010007
2017-10-10T15:02:48.436989: step 2730, loss 0.155217, acc 0.953125, learning_rate 0.00010007
2017-10-10T15:02:48.890792: step 2731, loss 0.118981, acc 0.96875, learning_rate 0.000100069
2017-10-10T15:02:49.344886: step 2732, loss 0.0990249, acc 0.96875, learning_rate 0.000100069
2017-10-10T15:02:49.708893: step 2733, loss 0.0805836, acc 0.96875, learning_rate 0.000100069
2017-10-10T15:02:50.170782: step 2734, loss 0.0842299, acc 0.96875, learning_rate 0.000100068
2017-10-10T15:02:50.583438: step 2735, loss 0.0977191, acc 0.953125, learning_rate 0.000100068
2017-10-10T15:02:50.977139: step 2736, loss 0.0825806, acc 0.96875, learning_rate 0.000100068
2017-10-10T15:02:51.428816: step 2737, loss 0.0772807, acc 1, learning_rate 0.000100068
2017-10-10T15:02:51.852403: step 2738, loss 0.120988, acc 0.953125, learning_rate 0.000100067
2017-10-10T15:02:52.376976: step 2739, loss 0.0942905, acc 0.96875, learning_rate 0.000100067
2017-10-10T15:02:52.748615: step 2740, loss 0.0928033, acc 0.984375, learning_rate 0.000100067
2017-10-10T15:02:53.140823: step 2741, loss 0.0909429, acc 0.96875, learning_rate 0.000100067
2017-10-10T15:02:53.519870: step 2742, loss 0.235122, acc 0.953125, learning_rate 0.000100066
2017-10-10T15:02:53.960948: step 2743, loss 0.126612, acc 0.96875, learning_rate 0.000100066
2017-10-10T15:02:54.245926: step 2744, loss 0.0752249, acc 0.960784, learning_rate 0.000100066
2017-10-10T15:02:54.558235: step 2745, loss 0.0945909, acc 0.96875, learning_rate 0.000100065
2017-10-10T15:02:54.973393: step 2746, loss 0.197732, acc 0.921875, learning_rate 0.000100065
2017-10-10T15:02:55.378887: step 2747, loss 0.104679, acc 0.96875, learning_rate 0.000100065
2017-10-10T15:02:55.806546: step 2748, loss 0.099011, acc 0.96875, learning_rate 0.000100065
2017-10-10T15:02:56.200756: step 2749, loss 0.0670587, acc 0.984375, learning_rate 0.000100064
2017-10-10T15:02:56.591107: step 2750, loss 0.0634519, acc 0.96875, learning_rate 0.000100064
2017-10-10T15:02:57.056203: step 2751, loss 0.14327, acc 0.953125, learning_rate 0.000100064
2017-10-10T15:02:57.460858: step 2752, loss 0.146149, acc 0.9375, learning_rate 0.000100064
2017-10-10T15:02:57.814663: step 2753, loss 0.0329395, acc 1, learning_rate 0.000100063
2017-10-10T15:02:58.215263: step 2754, loss 0.125236, acc 0.96875, learning_rate 0.000100063
2017-10-10T15:02:58.664842: step 2755, loss 0.0546967, acc 0.984375, learning_rate 0.000100063
2017-10-10T15:02:59.113116: step 2756, loss 0.201944, acc 0.9375, learning_rate 0.000100063
2017-10-10T15:02:59.501079: step 2757, loss 0.0802533, acc 0.953125, learning_rate 0.000100062
2017-10-10T15:02:59.913375: step 2758, loss 0.107609, acc 0.953125, learning_rate 0.000100062
2017-10-10T15:03:00.325093: step 2759, loss 0.175576, acc 0.96875, learning_rate 0.000100062
2017-10-10T15:03:00.751856: step 2760, loss 0.0938544, acc 0.96875, learning_rate 0.000100062

Evaluation:
2017-10-10T15:03:01.617239: step 2760, loss 0.218526, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2760

2017-10-10T15:03:02.997996: step 2761, loss 0.0398448, acc 1, learning_rate 0.000100061
2017-10-10T15:03:03.453012: step 2762, loss 0.021428, acc 1, learning_rate 0.000100061
2017-10-10T15:03:03.855739: step 2763, loss 0.0487562, acc 0.984375, learning_rate 0.000100061
2017-10-10T15:03:04.277170: step 2764, loss 0.099932, acc 0.953125, learning_rate 0.000100061
2017-10-10T15:03:04.724857: step 2765, loss 0.121515, acc 0.953125, learning_rate 0.00010006
2017-10-10T15:03:05.216876: step 2766, loss 0.0946381, acc 0.96875, learning_rate 0.00010006
2017-10-10T15:03:05.661883: step 2767, loss 0.127423, acc 0.96875, learning_rate 0.00010006
2017-10-10T15:03:06.025123: step 2768, loss 0.132111, acc 0.9375, learning_rate 0.00010006
2017-10-10T15:03:06.494776: step 2769, loss 0.18025, acc 0.953125, learning_rate 0.000100059
2017-10-10T15:03:06.849722: step 2770, loss 0.0810497, acc 0.984375, learning_rate 0.000100059
2017-10-10T15:03:07.189593: step 2771, loss 0.0989172, acc 0.953125, learning_rate 0.000100059
2017-10-10T15:03:07.574809: step 2772, loss 0.0138604, acc 1, learning_rate 0.000100059
2017-10-10T15:03:08.044973: step 2773, loss 0.160742, acc 0.921875, learning_rate 0.000100058
2017-10-10T15:03:08.429957: step 2774, loss 0.112497, acc 0.953125, learning_rate 0.000100058
2017-10-10T15:03:08.840981: step 2775, loss 0.0755436, acc 0.96875, learning_rate 0.000100058
2017-10-10T15:03:09.250634: step 2776, loss 0.120188, acc 0.953125, learning_rate 0.000100058
2017-10-10T15:03:09.653126: step 2777, loss 0.0402889, acc 1, learning_rate 0.000100057
2017-10-10T15:03:10.096825: step 2778, loss 0.0633684, acc 0.984375, learning_rate 0.000100057
2017-10-10T15:03:10.544853: step 2779, loss 0.0645781, acc 0.984375, learning_rate 0.000100057
2017-10-10T15:03:10.918405: step 2780, loss 0.101349, acc 0.96875, learning_rate 0.000100057
2017-10-10T15:03:11.295583: step 2781, loss 0.123537, acc 0.96875, learning_rate 0.000100056
2017-10-10T15:03:11.688079: step 2782, loss 0.0791483, acc 0.984375, learning_rate 0.000100056
2017-10-10T15:03:12.106400: step 2783, loss 0.133104, acc 0.96875, learning_rate 0.000100056
2017-10-10T15:03:12.496841: step 2784, loss 0.0553485, acc 1, learning_rate 0.000100056
2017-10-10T15:03:12.877525: step 2785, loss 0.0730956, acc 1, learning_rate 0.000100056
2017-10-10T15:03:13.318467: step 2786, loss 0.118298, acc 0.96875, learning_rate 0.000100055
2017-10-10T15:03:13.744976: step 2787, loss 0.051902, acc 1, learning_rate 0.000100055
2017-10-10T15:03:14.085089: step 2788, loss 0.0545435, acc 0.984375, learning_rate 0.000100055
2017-10-10T15:03:14.420948: step 2789, loss 0.15563, acc 0.9375, learning_rate 0.000100055
2017-10-10T15:03:14.793325: step 2790, loss 0.13128, acc 0.953125, learning_rate 0.000100054
2017-10-10T15:03:15.191547: step 2791, loss 0.120873, acc 0.953125, learning_rate 0.000100054
2017-10-10T15:03:15.643567: step 2792, loss 0.0929026, acc 0.96875, learning_rate 0.000100054
2017-10-10T15:03:16.076352: step 2793, loss 0.160239, acc 0.9375, learning_rate 0.000100054
2017-10-10T15:03:16.434013: step 2794, loss 0.0508588, acc 0.984375, learning_rate 0.000100054
2017-10-10T15:03:16.836995: step 2795, loss 0.129202, acc 0.96875, learning_rate 0.000100053
2017-10-10T15:03:17.282595: step 2796, loss 0.0644986, acc 0.984375, learning_rate 0.000100053
2017-10-10T15:03:17.683960: step 2797, loss 0.0172035, acc 1, learning_rate 0.000100053
2017-10-10T15:03:18.074714: step 2798, loss 0.0816313, acc 0.984375, learning_rate 0.000100053
2017-10-10T15:03:18.480871: step 2799, loss 0.110298, acc 0.96875, learning_rate 0.000100052
2017-10-10T15:03:19.116287: step 2800, loss 0.0413564, acc 1, learning_rate 0.000100052

Evaluation:
2017-10-10T15:03:19.886309: step 2800, loss 0.221384, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2800

2017-10-10T15:03:21.156841: step 2801, loss 0.0586669, acc 0.984375, learning_rate 0.000100052
2017-10-10T15:03:21.560906: step 2802, loss 0.06287, acc 0.984375, learning_rate 0.000100052
2017-10-10T15:03:21.984654: step 2803, loss 0.0561421, acc 0.984375, learning_rate 0.000100052
2017-10-10T15:03:22.426462: step 2804, loss 0.0636571, acc 0.984375, learning_rate 0.000100051
2017-10-10T15:03:22.845030: step 2805, loss 0.0924271, acc 0.984375, learning_rate 0.000100051
2017-10-10T15:03:23.260023: step 2806, loss 0.189732, acc 0.953125, learning_rate 0.000100051
2017-10-10T15:03:23.671260: step 2807, loss 0.227253, acc 0.9375, learning_rate 0.000100051
2017-10-10T15:03:24.080164: step 2808, loss 0.0544922, acc 0.984375, learning_rate 0.000100051
2017-10-10T15:03:24.483568: step 2809, loss 0.0907441, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:03:24.901104: step 2810, loss 0.113663, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:03:25.267733: step 2811, loss 0.08342, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:03:25.678567: step 2812, loss 0.176006, acc 0.953125, learning_rate 0.00010005
2017-10-10T15:03:26.036540: step 2813, loss 0.083729, acc 1, learning_rate 0.00010005
2017-10-10T15:03:26.492984: step 2814, loss 0.136474, acc 0.9375, learning_rate 0.000100049
2017-10-10T15:03:26.893237: step 2815, loss 0.133668, acc 0.96875, learning_rate 0.000100049
2017-10-10T15:03:27.280780: step 2816, loss 0.0887246, acc 0.984375, learning_rate 0.000100049
2017-10-10T15:03:27.626518: step 2817, loss 0.0282773, acc 1, learning_rate 0.000100049
2017-10-10T15:03:28.080942: step 2818, loss 0.0625022, acc 0.984375, learning_rate 0.000100049
2017-10-10T15:03:28.520872: step 2819, loss 0.0599714, acc 0.984375, learning_rate 0.000100048
2017-10-10T15:03:28.948829: step 2820, loss 0.0896683, acc 0.96875, learning_rate 0.000100048
2017-10-10T15:03:29.371828: step 2821, loss 0.158504, acc 0.96875, learning_rate 0.000100048
2017-10-10T15:03:29.804874: step 2822, loss 0.0820712, acc 0.96875, learning_rate 0.000100048
2017-10-10T15:03:30.303136: step 2823, loss 0.0561237, acc 1, learning_rate 0.000100048
2017-10-10T15:03:30.690778: step 2824, loss 0.0685517, acc 0.96875, learning_rate 0.000100047
2017-10-10T15:03:31.084887: step 2825, loss 0.164265, acc 0.9375, learning_rate 0.000100047
2017-10-10T15:03:31.360513: step 2826, loss 0.123725, acc 0.953125, learning_rate 0.000100047
2017-10-10T15:03:31.807663: step 2827, loss 0.163345, acc 0.9375, learning_rate 0.000100047
2017-10-10T15:03:32.211326: step 2828, loss 0.101855, acc 0.9375, learning_rate 0.000100047
2017-10-10T15:03:32.620008: step 2829, loss 0.0930394, acc 0.96875, learning_rate 0.000100046
2017-10-10T15:03:32.988959: step 2830, loss 0.122774, acc 0.96875, learning_rate 0.000100046
2017-10-10T15:03:33.448847: step 2831, loss 0.118221, acc 0.96875, learning_rate 0.000100046
2017-10-10T15:03:33.893063: step 2832, loss 0.12064, acc 0.953125, learning_rate 0.000100046
2017-10-10T15:03:34.345120: step 2833, loss 0.124209, acc 0.953125, learning_rate 0.000100046
2017-10-10T15:03:34.784849: step 2834, loss 0.105441, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:03:35.157901: step 2835, loss 0.125703, acc 0.9375, learning_rate 0.000100045
2017-10-10T15:03:35.593058: step 2836, loss 0.095163, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:03:35.984891: step 2837, loss 0.132421, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:03:36.408863: step 2838, loss 0.122418, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:03:36.890472: step 2839, loss 0.0837551, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:03:37.349200: step 2840, loss 0.104861, acc 0.96875, learning_rate 0.000100044

Evaluation:
2017-10-10T15:03:38.200949: step 2840, loss 0.218484, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2840

2017-10-10T15:03:39.556953: step 2841, loss 0.0702202, acc 0.984375, learning_rate 0.000100044
2017-10-10T15:03:39.873075: step 2842, loss 0.153139, acc 0.901961, learning_rate 0.000100044
2017-10-10T15:03:40.292984: step 2843, loss 0.0848788, acc 0.984375, learning_rate 0.000100044
2017-10-10T15:03:40.715389: step 2844, loss 0.0890618, acc 0.984375, learning_rate 0.000100044
2017-10-10T15:03:41.181656: step 2845, loss 0.196728, acc 0.921875, learning_rate 0.000100043
2017-10-10T15:03:41.606396: step 2846, loss 0.0439509, acc 1, learning_rate 0.000100043
2017-10-10T15:03:42.040887: step 2847, loss 0.0746157, acc 0.96875, learning_rate 0.000100043
2017-10-10T15:03:42.528120: step 2848, loss 0.0802016, acc 0.96875, learning_rate 0.000100043
2017-10-10T15:03:42.903972: step 2849, loss 0.157122, acc 0.9375, learning_rate 0.000100043
2017-10-10T15:03:43.282305: step 2850, loss 0.121524, acc 0.96875, learning_rate 0.000100043
2017-10-10T15:03:43.658154: step 2851, loss 0.0880998, acc 0.984375, learning_rate 0.000100042
2017-10-10T15:03:44.113415: step 2852, loss 0.129247, acc 0.984375, learning_rate 0.000100042
2017-10-10T15:03:44.544857: step 2853, loss 0.152283, acc 0.96875, learning_rate 0.000100042
2017-10-10T15:03:45.029357: step 2854, loss 0.0551107, acc 0.984375, learning_rate 0.000100042
2017-10-10T15:03:45.442100: step 2855, loss 0.161495, acc 0.953125, learning_rate 0.000100042
2017-10-10T15:03:45.789540: step 2856, loss 0.0871747, acc 0.96875, learning_rate 0.000100042
2017-10-10T15:03:46.168828: step 2857, loss 0.0766807, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:03:46.556083: step 2858, loss 0.0994184, acc 0.953125, learning_rate 0.000100041
2017-10-10T15:03:47.051128: step 2859, loss 0.121708, acc 0.953125, learning_rate 0.000100041
2017-10-10T15:03:47.446634: step 2860, loss 0.0668955, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:03:47.767689: step 2861, loss 0.0994183, acc 0.96875, learning_rate 0.000100041
2017-10-10T15:03:48.199422: step 2862, loss 0.121432, acc 0.9375, learning_rate 0.000100041
2017-10-10T15:03:48.602015: step 2863, loss 0.0736222, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:03:49.040612: step 2864, loss 0.221671, acc 0.90625, learning_rate 0.00010004
2017-10-10T15:03:49.426730: step 2865, loss 0.0681873, acc 0.984375, learning_rate 0.00010004
2017-10-10T15:03:49.977828: step 2866, loss 0.0401094, acc 1, learning_rate 0.00010004
2017-10-10T15:03:50.349331: step 2867, loss 0.0777289, acc 0.984375, learning_rate 0.00010004
2017-10-10T15:03:50.849053: step 2868, loss 0.0865103, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:03:51.344967: step 2869, loss 0.174816, acc 0.921875, learning_rate 0.000100039
2017-10-10T15:03:51.778857: step 2870, loss 0.0514753, acc 1, learning_rate 0.000100039
2017-10-10T15:03:52.177112: step 2871, loss 0.0971377, acc 0.984375, learning_rate 0.000100039
2017-10-10T15:03:52.560235: step 2872, loss 0.106576, acc 0.984375, learning_rate 0.000100039
2017-10-10T15:03:53.005508: step 2873, loss 0.156648, acc 0.953125, learning_rate 0.000100039
2017-10-10T15:03:53.408907: step 2874, loss 0.0838817, acc 0.953125, learning_rate 0.000100039
2017-10-10T15:03:53.931774: step 2875, loss 0.0977574, acc 0.96875, learning_rate 0.000100038
2017-10-10T15:03:54.373059: step 2876, loss 0.0567511, acc 0.96875, learning_rate 0.000100038
2017-10-10T15:03:54.804881: step 2877, loss 0.127303, acc 0.953125, learning_rate 0.000100038
2017-10-10T15:03:55.245762: step 2878, loss 0.068225, acc 0.953125, learning_rate 0.000100038
2017-10-10T15:03:55.640836: step 2879, loss 0.0922808, acc 0.984375, learning_rate 0.000100038
2017-10-10T15:03:56.049344: step 2880, loss 0.0684948, acc 0.953125, learning_rate 0.000100038

Evaluation:
2017-10-10T15:03:56.860450: step 2880, loss 0.22132, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2880

2017-10-10T15:03:58.305411: step 2881, loss 0.0743757, acc 0.984375, learning_rate 0.000100038
2017-10-10T15:03:58.645078: step 2882, loss 0.0771653, acc 0.96875, learning_rate 0.000100037
2017-10-10T15:03:58.962639: step 2883, loss 0.113139, acc 0.953125, learning_rate 0.000100037
2017-10-10T15:03:59.436610: step 2884, loss 0.0479907, acc 0.984375, learning_rate 0.000100037
2017-10-10T15:03:59.835959: step 2885, loss 0.0505748, acc 1, learning_rate 0.000100037
2017-10-10T15:04:00.294702: step 2886, loss 0.0702037, acc 0.984375, learning_rate 0.000100037
2017-10-10T15:04:00.633974: step 2887, loss 0.107038, acc 0.96875, learning_rate 0.000100037
2017-10-10T15:04:00.986829: step 2888, loss 0.234143, acc 0.9375, learning_rate 0.000100036
2017-10-10T15:04:01.407836: step 2889, loss 0.0908214, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:04:01.768961: step 2890, loss 0.0937772, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:04:02.177714: step 2891, loss 0.132481, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:04:02.625750: step 2892, loss 0.0917147, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:04:03.097830: step 2893, loss 0.17625, acc 0.9375, learning_rate 0.000100036
2017-10-10T15:04:03.475191: step 2894, loss 0.163446, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:04:03.820432: step 2895, loss 0.0677226, acc 0.96875, learning_rate 0.000100035
2017-10-10T15:04:04.148836: step 2896, loss 0.0808891, acc 0.984375, learning_rate 0.000100035
2017-10-10T15:04:04.529184: step 2897, loss 0.0920652, acc 0.953125, learning_rate 0.000100035
2017-10-10T15:04:04.906999: step 2898, loss 0.149593, acc 0.9375, learning_rate 0.000100035
2017-10-10T15:04:05.300077: step 2899, loss 0.0573716, acc 0.984375, learning_rate 0.000100035
2017-10-10T15:04:05.656847: step 2900, loss 0.119909, acc 0.96875, learning_rate 0.000100035
2017-10-10T15:04:06.074635: step 2901, loss 0.0449446, acc 1, learning_rate 0.000100035
2017-10-10T15:04:06.475505: step 2902, loss 0.134515, acc 0.953125, learning_rate 0.000100034
2017-10-10T15:04:06.826283: step 2903, loss 0.155889, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:04:07.268867: step 2904, loss 0.0550577, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:04:07.652876: step 2905, loss 0.0994456, acc 0.953125, learning_rate 0.000100034
2017-10-10T15:04:08.009058: step 2906, loss 0.133503, acc 0.953125, learning_rate 0.000100034
2017-10-10T15:04:08.490832: step 2907, loss 0.129855, acc 0.953125, learning_rate 0.000100034
2017-10-10T15:04:08.908401: step 2908, loss 0.0746465, acc 0.984375, learning_rate 0.000100034
2017-10-10T15:04:09.317025: step 2909, loss 0.143727, acc 0.9375, learning_rate 0.000100033
2017-10-10T15:04:09.744855: step 2910, loss 0.065155, acc 1, learning_rate 0.000100033
2017-10-10T15:04:10.121275: step 2911, loss 0.0870449, acc 0.984375, learning_rate 0.000100033
2017-10-10T15:04:10.504866: step 2912, loss 0.102762, acc 0.984375, learning_rate 0.000100033
2017-10-10T15:04:10.917623: step 2913, loss 0.114461, acc 0.953125, learning_rate 0.000100033
2017-10-10T15:04:11.417013: step 2914, loss 0.119637, acc 0.96875, learning_rate 0.000100033
2017-10-10T15:04:11.770671: step 2915, loss 0.179661, acc 0.953125, learning_rate 0.000100033
2017-10-10T15:04:12.106774: step 2916, loss 0.137106, acc 0.9375, learning_rate 0.000100033
2017-10-10T15:04:12.462860: step 2917, loss 0.0732035, acc 0.984375, learning_rate 0.000100032
2017-10-10T15:04:12.868861: step 2918, loss 0.0603498, acc 0.984375, learning_rate 0.000100032
2017-10-10T15:04:13.332594: step 2919, loss 0.159194, acc 0.96875, learning_rate 0.000100032
2017-10-10T15:04:13.740141: step 2920, loss 0.0907359, acc 0.984375, learning_rate 0.000100032

Evaluation:
2017-10-10T15:04:14.545085: step 2920, loss 0.21781, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2920

2017-10-10T15:04:15.873792: step 2921, loss 0.078991, acc 0.96875, learning_rate 0.000100032
2017-10-10T15:04:16.276688: step 2922, loss 0.136609, acc 0.953125, learning_rate 0.000100032
2017-10-10T15:04:16.659882: step 2923, loss 0.0902915, acc 0.96875, learning_rate 0.000100032
2017-10-10T15:04:16.983705: step 2924, loss 0.0187256, acc 1, learning_rate 0.000100031
2017-10-10T15:04:17.410346: step 2925, loss 0.0679789, acc 0.96875, learning_rate 0.000100031
2017-10-10T15:04:17.796908: step 2926, loss 0.131949, acc 0.953125, learning_rate 0.000100031
2017-10-10T15:04:18.148948: step 2927, loss 0.128293, acc 0.953125, learning_rate 0.000100031
2017-10-10T15:04:18.538502: step 2928, loss 0.0978027, acc 0.96875, learning_rate 0.000100031
2017-10-10T15:04:18.987126: step 2929, loss 0.115716, acc 0.96875, learning_rate 0.000100031
2017-10-10T15:04:19.470715: step 2930, loss 0.227307, acc 0.921875, learning_rate 0.000100031
2017-10-10T15:04:19.893068: step 2931, loss 0.1746, acc 0.953125, learning_rate 0.000100031
2017-10-10T15:04:20.362436: step 2932, loss 0.285349, acc 0.890625, learning_rate 0.00010003
2017-10-10T15:04:20.696217: step 2933, loss 0.123416, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:04:21.030238: step 2934, loss 0.0552754, acc 0.984375, learning_rate 0.00010003
2017-10-10T15:04:21.441683: step 2935, loss 0.0771377, acc 0.984375, learning_rate 0.00010003
2017-10-10T15:04:21.871430: step 2936, loss 0.113278, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:04:22.262310: step 2937, loss 0.0511354, acc 1, learning_rate 0.00010003
2017-10-10T15:04:22.641066: step 2938, loss 0.0394771, acc 0.984375, learning_rate 0.00010003
2017-10-10T15:04:23.053822: step 2939, loss 0.123355, acc 0.96875, learning_rate 0.00010003
2017-10-10T15:04:23.373229: step 2940, loss 0.0700541, acc 0.980392, learning_rate 0.000100029
2017-10-10T15:04:23.750071: step 2941, loss 0.0819255, acc 0.984375, learning_rate 0.000100029
2017-10-10T15:04:24.188889: step 2942, loss 0.180549, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:04:24.586952: step 2943, loss 0.131177, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:04:25.069716: step 2944, loss 0.0656015, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:04:25.408896: step 2945, loss 0.0557953, acc 1, learning_rate 0.000100029
2017-10-10T15:04:25.756798: step 2946, loss 0.075048, acc 0.984375, learning_rate 0.000100029
2017-10-10T15:04:26.162286: step 2947, loss 0.0627231, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:04:26.531426: step 2948, loss 0.102006, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:04:26.986272: step 2949, loss 0.0830542, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:04:27.370774: step 2950, loss 0.153057, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:04:27.781183: step 2951, loss 0.0680173, acc 0.984375, learning_rate 0.000100028
2017-10-10T15:04:28.160570: step 2952, loss 0.0782889, acc 1, learning_rate 0.000100028
2017-10-10T15:04:28.582637: step 2953, loss 0.11924, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:04:28.970520: step 2954, loss 0.128184, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:04:29.385513: step 2955, loss 0.0741754, acc 0.984375, learning_rate 0.000100028
2017-10-10T15:04:29.802923: step 2956, loss 0.164675, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:04:30.217111: step 2957, loss 0.0517517, acc 0.984375, learning_rate 0.000100028
2017-10-10T15:04:30.632862: step 2958, loss 0.0674924, acc 0.984375, learning_rate 0.000100027
2017-10-10T15:04:31.002572: step 2959, loss 0.114937, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:04:31.412336: step 2960, loss 0.0505203, acc 0.984375, learning_rate 0.000100027

Evaluation:
2017-10-10T15:04:32.360857: step 2960, loss 0.221042, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-2960

2017-10-10T15:04:33.587589: step 2961, loss 0.0970009, acc 0.984375, learning_rate 0.000100027
2017-10-10T15:04:34.015865: step 2962, loss 0.193088, acc 0.9375, learning_rate 0.000100027
2017-10-10T15:04:34.406298: step 2963, loss 0.0222452, acc 1, learning_rate 0.000100027
2017-10-10T15:04:34.796363: step 2964, loss 0.0508482, acc 0.984375, learning_rate 0.000100027
2017-10-10T15:04:35.236933: step 2965, loss 0.0448546, acc 0.984375, learning_rate 0.000100027
2017-10-10T15:04:35.680982: step 2966, loss 0.105099, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:04:36.133872: step 2967, loss 0.177264, acc 0.921875, learning_rate 0.000100026
2017-10-10T15:04:36.628799: step 2968, loss 0.0272145, acc 1, learning_rate 0.000100026
2017-10-10T15:04:36.989939: step 2969, loss 0.0918193, acc 0.953125, learning_rate 0.000100026
2017-10-10T15:04:37.314729: step 2970, loss 0.0986252, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:04:37.721023: step 2971, loss 0.108133, acc 0.96875, learning_rate 0.000100026
2017-10-10T15:04:38.102876: step 2972, loss 0.212449, acc 0.90625, learning_rate 0.000100026
2017-10-10T15:04:38.537509: step 2973, loss 0.0771655, acc 0.96875, learning_rate 0.000100026
2017-10-10T15:04:38.958530: step 2974, loss 0.0641124, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:04:39.325330: step 2975, loss 0.166415, acc 0.9375, learning_rate 0.000100026
2017-10-10T15:04:39.700959: step 2976, loss 0.0256073, acc 1, learning_rate 0.000100025
2017-10-10T15:04:40.145264: step 2977, loss 0.10471, acc 0.96875, learning_rate 0.000100025
2017-10-10T15:04:40.548507: step 2978, loss 0.13138, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:04:40.989002: step 2979, loss 0.102261, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:04:41.436969: step 2980, loss 0.0447169, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:04:41.856704: step 2981, loss 0.150819, acc 0.96875, learning_rate 0.000100025
2017-10-10T15:04:42.266781: step 2982, loss 0.0789388, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:04:42.664987: step 2983, loss 0.133244, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:04:43.098971: step 2984, loss 0.105916, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:04:43.477632: step 2985, loss 0.0422864, acc 1, learning_rate 0.000100025
2017-10-10T15:04:43.940870: step 2986, loss 0.0379635, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:04:44.380388: step 2987, loss 0.169562, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:04:44.768979: step 2988, loss 0.112827, acc 0.96875, learning_rate 0.000100024
2017-10-10T15:04:45.090984: step 2989, loss 0.240817, acc 0.90625, learning_rate 0.000100024
2017-10-10T15:04:45.457966: step 2990, loss 0.115799, acc 0.9375, learning_rate 0.000100024
2017-10-10T15:04:45.876014: step 2991, loss 0.0635362, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:04:46.280993: step 2992, loss 0.153151, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:04:46.704056: step 2993, loss 0.0425733, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:04:47.117384: step 2994, loss 0.0741524, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:04:47.505104: step 2995, loss 0.0245164, acc 1, learning_rate 0.000100024
2017-10-10T15:04:47.837945: step 2996, loss 0.0628697, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:04:48.253443: step 2997, loss 0.0343224, acc 1, learning_rate 0.000100023
2017-10-10T15:04:48.687448: step 2998, loss 0.11444, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:04:49.096819: step 2999, loss 0.105658, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:04:49.542380: step 3000, loss 0.138768, acc 0.9375, learning_rate 0.000100023

Evaluation:
2017-10-10T15:04:50.349000: step 3000, loss 0.218512, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3000

2017-10-10T15:04:51.488452: step 3001, loss 0.0792091, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:04:52.004856: step 3002, loss 0.107145, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:04:52.465047: step 3003, loss 0.13921, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:04:52.896900: step 3004, loss 0.0490502, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:04:53.396868: step 3005, loss 0.0727546, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:04:53.754168: step 3006, loss 0.0934818, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:04:54.066943: step 3007, loss 0.186742, acc 0.921875, learning_rate 0.000100022
2017-10-10T15:04:54.402600: step 3008, loss 0.21088, acc 0.921875, learning_rate 0.000100022
2017-10-10T15:04:54.844949: step 3009, loss 0.115678, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:04:55.281298: step 3010, loss 0.150471, acc 0.9375, learning_rate 0.000100022
2017-10-10T15:04:55.699426: step 3011, loss 0.0600259, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:04:56.080914: step 3012, loss 0.100354, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:04:56.450544: step 3013, loss 0.104348, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:04:56.908470: step 3014, loss 0.107065, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:04:57.304309: step 3015, loss 0.0624569, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:04:57.708886: step 3016, loss 0.102033, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:04:58.096526: step 3017, loss 0.150668, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:04:58.473346: step 3018, loss 0.0327622, acc 1, learning_rate 0.000100021
2017-10-10T15:04:58.903455: step 3019, loss 0.167076, acc 0.921875, learning_rate 0.000100021
2017-10-10T15:04:59.316832: step 3020, loss 0.034425, acc 1, learning_rate 0.000100021
2017-10-10T15:04:59.738084: step 3021, loss 0.0360887, acc 1, learning_rate 0.000100021
2017-10-10T15:05:00.137051: step 3022, loss 0.104761, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:05:00.530965: step 3023, loss 0.158656, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:05:00.961310: step 3024, loss 0.0555437, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:05:01.357089: step 3025, loss 0.158454, acc 0.9375, learning_rate 0.000100021
2017-10-10T15:05:01.858725: step 3026, loss 0.120676, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:05:02.209091: step 3027, loss 0.0802413, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:05:02.596699: step 3028, loss 0.0788232, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:05:03.000447: step 3029, loss 0.091123, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:05:03.447188: step 3030, loss 0.0831253, acc 0.984375, learning_rate 0.00010002
2017-10-10T15:05:03.897102: step 3031, loss 0.110078, acc 0.984375, learning_rate 0.00010002
2017-10-10T15:05:04.331411: step 3032, loss 0.0546348, acc 1, learning_rate 0.00010002
2017-10-10T15:05:04.724987: step 3033, loss 0.0803082, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:05:05.099411: step 3034, loss 0.290237, acc 0.921875, learning_rate 0.00010002
2017-10-10T15:05:05.616932: step 3035, loss 0.0763437, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:05:06.001802: step 3036, loss 0.146158, acc 0.921875, learning_rate 0.00010002
2017-10-10T15:05:06.389306: step 3037, loss 0.0288881, acc 1, learning_rate 0.00010002
2017-10-10T15:05:06.791473: step 3038, loss 0.0902493, acc 0.960784, learning_rate 0.00010002
2017-10-10T15:05:07.164855: step 3039, loss 0.0918529, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:05:07.593527: step 3040, loss 0.0718245, acc 0.984375, learning_rate 0.00010002

Evaluation:
2017-10-10T15:05:08.410891: step 3040, loss 0.215593, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3040

2017-10-10T15:05:09.876040: step 3041, loss 0.0648151, acc 0.984375, learning_rate 0.00010002
2017-10-10T15:05:10.295571: step 3042, loss 0.0693248, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:05:10.633212: step 3043, loss 0.0565542, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:05:11.036608: step 3044, loss 0.11561, acc 0.9375, learning_rate 0.000100019
2017-10-10T15:05:11.476902: step 3045, loss 0.0470942, acc 1, learning_rate 0.000100019
2017-10-10T15:05:11.893649: step 3046, loss 0.0248737, acc 1, learning_rate 0.000100019
2017-10-10T15:05:12.277001: step 3047, loss 0.0995137, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:05:12.696874: step 3048, loss 0.0285298, acc 1, learning_rate 0.000100019
2017-10-10T15:05:13.111163: step 3049, loss 0.163983, acc 0.921875, learning_rate 0.000100019
2017-10-10T15:05:13.528833: step 3050, loss 0.074639, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:05:13.955188: step 3051, loss 0.0907547, acc 0.953125, learning_rate 0.000100019
2017-10-10T15:05:14.365127: step 3052, loss 0.0457165, acc 1, learning_rate 0.000100019
2017-10-10T15:05:14.753577: step 3053, loss 0.0597772, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:05:15.208956: step 3054, loss 0.182324, acc 0.9375, learning_rate 0.000100018
2017-10-10T15:05:15.591207: step 3055, loss 0.0434257, acc 1, learning_rate 0.000100018
2017-10-10T15:05:15.978825: step 3056, loss 0.101137, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:05:16.388809: step 3057, loss 0.0295243, acc 1, learning_rate 0.000100018
2017-10-10T15:05:16.784931: step 3058, loss 0.0837042, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:05:17.168635: step 3059, loss 0.0914747, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:05:17.609439: step 3060, loss 0.138539, acc 0.9375, learning_rate 0.000100018
2017-10-10T15:05:18.066432: step 3061, loss 0.104229, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:05:18.445817: step 3062, loss 0.230001, acc 0.890625, learning_rate 0.000100018
2017-10-10T15:05:18.859777: step 3063, loss 0.0966387, acc 0.9375, learning_rate 0.000100018
2017-10-10T15:05:19.256987: step 3064, loss 0.0644771, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:05:19.619085: step 3065, loss 0.0560127, acc 1, learning_rate 0.000100018
2017-10-10T15:05:20.024932: step 3066, loss 0.102936, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:05:20.445741: step 3067, loss 0.077286, acc 1, learning_rate 0.000100018
2017-10-10T15:05:20.893609: step 3068, loss 0.0775636, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:21.310920: step 3069, loss 0.125351, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:05:21.657473: step 3070, loss 0.0594771, acc 1, learning_rate 0.000100017
2017-10-10T15:05:22.128904: step 3071, loss 0.0606277, acc 1, learning_rate 0.000100017
2017-10-10T15:05:22.571284: step 3072, loss 0.082744, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:05:22.953111: step 3073, loss 0.0886077, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:05:23.369765: step 3074, loss 0.172383, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:05:23.804827: step 3075, loss 0.0706732, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:05:24.241193: step 3076, loss 0.143706, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:05:24.641056: step 3077, loss 0.070164, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:25.048959: step 3078, loss 0.0764443, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:05:25.406321: step 3079, loss 0.0580817, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:25.767627: step 3080, loss 0.070949, acc 0.984375, learning_rate 0.000100017

Evaluation:
2017-10-10T15:05:26.711188: step 3080, loss 0.218632, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3080

2017-10-10T15:05:27.975000: step 3081, loss 0.205062, acc 0.921875, learning_rate 0.000100017
2017-10-10T15:05:28.345725: step 3082, loss 0.0556665, acc 1, learning_rate 0.000100016
2017-10-10T15:05:28.747359: step 3083, loss 0.0430804, acc 1, learning_rate 0.000100016
2017-10-10T15:05:29.141262: step 3084, loss 0.101968, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:05:29.539380: step 3085, loss 0.104883, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:05:29.924936: step 3086, loss 0.169078, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:05:30.350437: step 3087, loss 0.101951, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:05:30.756899: step 3088, loss 0.189344, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:05:31.216922: step 3089, loss 0.0663493, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:05:31.655703: step 3090, loss 0.0789403, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:05:32.137092: step 3091, loss 0.147586, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:05:32.556905: step 3092, loss 0.169492, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:05:32.890805: step 3093, loss 0.0287778, acc 1, learning_rate 0.000100016
2017-10-10T15:05:33.351439: step 3094, loss 0.0870905, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:05:33.720846: step 3095, loss 0.145613, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:05:34.073440: step 3096, loss 0.0951006, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:05:34.459932: step 3097, loss 0.0439379, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:05:34.921734: step 3098, loss 0.10178, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:05:35.295985: step 3099, loss 0.11059, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:05:35.658727: step 3100, loss 0.147713, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:05:36.096898: step 3101, loss 0.113834, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:36.534877: step 3102, loss 0.0484366, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:05:36.860455: step 3103, loss 0.0535185, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:05:37.237272: step 3104, loss 0.0824232, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:37.699275: step 3105, loss 0.0398975, acc 1, learning_rate 0.000100015
2017-10-10T15:05:38.170010: step 3106, loss 0.127227, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:05:38.635419: step 3107, loss 0.105863, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:39.087205: step 3108, loss 0.038242, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:05:39.522252: step 3109, loss 0.0844956, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:39.938931: step 3110, loss 0.131886, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:40.320919: step 3111, loss 0.152627, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:05:40.740953: step 3112, loss 0.133355, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:05:41.164254: step 3113, loss 0.0544399, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:05:41.500932: step 3114, loss 0.0494507, acc 1, learning_rate 0.000100014
2017-10-10T15:05:41.880171: step 3115, loss 0.136335, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:05:42.280859: step 3116, loss 0.139527, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:42.733492: step 3117, loss 0.271564, acc 0.890625, learning_rate 0.000100014
2017-10-10T15:05:43.152892: step 3118, loss 0.0540713, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:05:43.652891: step 3119, loss 0.0548768, acc 1, learning_rate 0.000100014
2017-10-10T15:05:43.996274: step 3120, loss 0.237617, acc 0.890625, learning_rate 0.000100014

Evaluation:
2017-10-10T15:05:44.813641: step 3120, loss 0.218303, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3120

2017-10-10T15:05:46.098920: step 3121, loss 0.0778673, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:46.431920: step 3122, loss 0.0577338, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:05:46.809160: step 3123, loss 0.151231, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:05:47.243929: step 3124, loss 0.0826689, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:47.649657: step 3125, loss 0.0874148, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:48.029599: step 3126, loss 0.158558, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:05:48.444402: step 3127, loss 0.0512537, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:48.824786: step 3128, loss 0.0397091, acc 1, learning_rate 0.000100014
2017-10-10T15:05:49.228526: step 3129, loss 0.0674232, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:05:49.613045: step 3130, loss 0.0890474, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:50.017018: step 3131, loss 0.0798698, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:05:50.396921: step 3132, loss 0.10101, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:50.867366: step 3133, loss 0.0802783, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:51.277472: step 3134, loss 0.0725044, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:51.661280: step 3135, loss 0.137773, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:52.043027: step 3136, loss 0.0738586, acc 0.980392, learning_rate 0.000100013
2017-10-10T15:05:52.469021: step 3137, loss 0.129972, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:05:52.868570: step 3138, loss 0.0647799, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:53.292061: step 3139, loss 0.12199, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:53.739495: step 3140, loss 0.103968, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:05:54.156975: step 3141, loss 0.0647477, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:54.584953: step 3142, loss 0.0671807, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:55.032994: step 3143, loss 0.0970266, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:55.415738: step 3144, loss 0.106804, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:05:55.809925: step 3145, loss 0.0260709, acc 1, learning_rate 0.000100013
2017-10-10T15:05:56.248035: step 3146, loss 0.0741517, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:56.689821: step 3147, loss 0.148079, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:57.077153: step 3148, loss 0.092671, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:05:57.483558: step 3149, loss 0.0331443, acc 1, learning_rate 0.000100013
2017-10-10T15:05:57.963202: step 3150, loss 0.0805873, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:05:58.364822: step 3151, loss 0.0384838, acc 1, learning_rate 0.000100012
2017-10-10T15:05:58.775557: step 3152, loss 0.0409805, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:05:59.147690: step 3153, loss 0.0743221, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:05:59.558371: step 3154, loss 0.12781, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:06:00.024807: step 3155, loss 0.0488748, acc 1, learning_rate 0.000100012
2017-10-10T15:06:00.428742: step 3156, loss 0.108981, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:06:00.805461: step 3157, loss 0.0714369, acc 1, learning_rate 0.000100012
2017-10-10T15:06:01.129843: step 3158, loss 0.0938073, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:06:01.583548: step 3159, loss 0.0692373, acc 1, learning_rate 0.000100012
2017-10-10T15:06:01.993642: step 3160, loss 0.0422795, acc 1, learning_rate 0.000100012

Evaluation:
2017-10-10T15:06:02.846958: step 3160, loss 0.21672, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3160

2017-10-10T15:06:04.192292: step 3161, loss 0.144365, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:06:04.608992: step 3162, loss 0.143937, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:06:05.092819: step 3163, loss 0.0942463, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:06:05.504825: step 3164, loss 0.174078, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:06:05.901405: step 3165, loss 0.121641, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:06:06.348411: step 3166, loss 0.149183, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:06:06.724906: step 3167, loss 0.100795, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:06:07.161503: step 3168, loss 0.102437, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:06:07.640868: step 3169, loss 0.147618, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:06:08.061164: step 3170, loss 0.126979, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:06:08.498230: step 3171, loss 0.103584, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:06:08.910846: step 3172, loss 0.0351953, acc 1, learning_rate 0.000100011
2017-10-10T15:06:09.289550: step 3173, loss 0.0969826, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:06:09.695270: step 3174, loss 0.204161, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:06:10.100351: step 3175, loss 0.0714933, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:06:10.540289: step 3176, loss 0.0634, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:06:10.892823: step 3177, loss 0.0831062, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:06:11.274080: step 3178, loss 0.0874826, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:06:11.748962: step 3179, loss 0.115428, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:06:12.260858: step 3180, loss 0.144682, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:06:12.653106: step 3181, loss 0.0780473, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:06:12.979428: step 3182, loss 0.0512, acc 1, learning_rate 0.000100011
2017-10-10T15:06:13.367125: step 3183, loss 0.0194199, acc 1, learning_rate 0.000100011
2017-10-10T15:06:13.753112: step 3184, loss 0.0909061, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:06:14.179126: step 3185, loss 0.168066, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:06:14.533095: step 3186, loss 0.0989418, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:06:15.004989: step 3187, loss 0.0686561, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:06:15.420539: step 3188, loss 0.110307, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:06:15.842539: step 3189, loss 0.0693434, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:06:16.310415: step 3190, loss 0.0990236, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:06:16.736944: step 3191, loss 0.275027, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:06:17.088639: step 3192, loss 0.0958682, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:06:17.447515: step 3193, loss 0.0727124, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:06:17.896560: step 3194, loss 0.0536702, acc 1, learning_rate 0.00010001
2017-10-10T15:06:18.353160: step 3195, loss 0.1042, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:18.752879: step 3196, loss 0.0753065, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:19.157606: step 3197, loss 0.0596468, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:06:19.596963: step 3198, loss 0.130459, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:20.060863: step 3199, loss 0.0852556, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:20.485023: step 3200, loss 0.0731896, acc 0.96875, learning_rate 0.00010001

Evaluation:
2017-10-10T15:06:21.358330: step 3200, loss 0.219212, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3200

2017-10-10T15:06:22.608277: step 3201, loss 0.121905, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:06:22.950106: step 3202, loss 0.0593538, acc 1, learning_rate 0.00010001
2017-10-10T15:06:23.396927: step 3203, loss 0.0443799, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:06:23.769484: step 3204, loss 0.170287, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:06:24.112872: step 3205, loss 0.0604653, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:24.547453: step 3206, loss 0.113374, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:24.936903: step 3207, loss 0.0674168, acc 1, learning_rate 0.00010001
2017-10-10T15:06:25.353043: step 3208, loss 0.0462057, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:06:25.756872: step 3209, loss 0.139054, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:06:26.168844: step 3210, loss 0.125809, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:26.539056: step 3211, loss 0.126552, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:26.940969: step 3212, loss 0.0612128, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:27.365114: step 3213, loss 0.0797924, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:06:27.742126: step 3214, loss 0.0949552, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:28.141742: step 3215, loss 0.0887618, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:06:28.556970: step 3216, loss 0.0544035, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:06:28.977085: step 3217, loss 0.131053, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:06:29.407545: step 3218, loss 0.119598, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:06:29.805823: step 3219, loss 0.061522, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:06:30.199237: step 3220, loss 0.0853214, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:06:30.589275: step 3221, loss 0.109863, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:30.930628: step 3222, loss 0.0664057, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:06:31.379740: step 3223, loss 0.0347613, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:06:31.829122: step 3224, loss 0.019379, acc 1, learning_rate 0.000100009
2017-10-10T15:06:32.263508: step 3225, loss 0.131133, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:06:32.620622: step 3226, loss 0.054406, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:33.149006: step 3227, loss 0.180141, acc 0.921875, learning_rate 0.000100009
2017-10-10T15:06:33.609402: step 3228, loss 0.0323781, acc 1, learning_rate 0.000100009
2017-10-10T15:06:33.981489: step 3229, loss 0.0633142, acc 1, learning_rate 0.000100009
2017-10-10T15:06:34.409220: step 3230, loss 0.16661, acc 0.921875, learning_rate 0.000100009
2017-10-10T15:06:34.751402: step 3231, loss 0.0568264, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:06:35.114315: step 3232, loss 0.123623, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:35.507312: step 3233, loss 0.067987, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:35.880968: step 3234, loss 0.0796832, acc 0.980392, learning_rate 0.000100009
2017-10-10T15:06:36.314835: step 3235, loss 0.0461743, acc 1, learning_rate 0.000100009
2017-10-10T15:06:36.683088: step 3236, loss 0.108955, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:37.102134: step 3237, loss 0.103572, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:06:37.508228: step 3238, loss 0.046276, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:06:37.955018: step 3239, loss 0.0258506, acc 1, learning_rate 0.000100009
2017-10-10T15:06:38.345819: step 3240, loss 0.106438, acc 0.96875, learning_rate 0.000100009

Evaluation:
2017-10-10T15:06:39.157028: step 3240, loss 0.216481, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3240

2017-10-10T15:06:40.467545: step 3241, loss 0.120306, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:40.963263: step 3242, loss 0.102088, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:41.395670: step 3243, loss 0.0963992, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:06:41.821985: step 3244, loss 0.0654897, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:42.185066: step 3245, loss 0.209305, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:06:42.669294: step 3246, loss 0.0496785, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:43.082185: step 3247, loss 0.130513, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:43.455732: step 3248, loss 0.0782683, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:43.885139: step 3249, loss 0.121244, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:44.349069: step 3250, loss 0.0237391, acc 1, learning_rate 0.000100008
2017-10-10T15:06:44.760847: step 3251, loss 0.0577917, acc 1, learning_rate 0.000100008
2017-10-10T15:06:45.176657: step 3252, loss 0.138732, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:06:45.628528: step 3253, loss 0.108366, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:46.028635: step 3254, loss 0.316742, acc 0.859375, learning_rate 0.000100008
2017-10-10T15:06:46.493539: step 3255, loss 0.155738, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:46.940887: step 3256, loss 0.0638086, acc 1, learning_rate 0.000100008
2017-10-10T15:06:47.346634: step 3257, loss 0.0937363, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:47.704822: step 3258, loss 0.119098, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:06:48.045295: step 3259, loss 0.228053, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:06:48.496946: step 3260, loss 0.0718542, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:48.956878: step 3261, loss 0.0461916, acc 1, learning_rate 0.000100008
2017-10-10T15:06:49.459055: step 3262, loss 0.0417158, acc 1, learning_rate 0.000100008
2017-10-10T15:06:49.810105: step 3263, loss 0.0685029, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:50.151382: step 3264, loss 0.0897749, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:50.572940: step 3265, loss 0.137473, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:50.952818: step 3266, loss 0.0482336, acc 1, learning_rate 0.000100008
2017-10-10T15:06:51.365152: step 3267, loss 0.181821, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:06:51.800532: step 3268, loss 0.0726388, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:52.223804: step 3269, loss 0.0763272, acc 1, learning_rate 0.000100008
2017-10-10T15:06:52.700987: step 3270, loss 0.0726962, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:53.061007: step 3271, loss 0.0639299, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:53.436297: step 3272, loss 0.0611834, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:53.834547: step 3273, loss 0.0232092, acc 1, learning_rate 0.000100008
2017-10-10T15:06:54.228446: step 3274, loss 0.0940852, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:54.668977: step 3275, loss 0.107134, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:55.066294: step 3276, loss 0.0713274, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:55.460089: step 3277, loss 0.0706336, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:55.874700: step 3278, loss 0.0593799, acc 1, learning_rate 0.000100007
2017-10-10T15:06:56.301176: step 3279, loss 0.132041, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:06:56.637021: step 3280, loss 0.0886642, acc 0.96875, learning_rate 0.000100007

Evaluation:
2017-10-10T15:06:57.493973: step 3280, loss 0.218345, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3280

2017-10-10T15:06:58.911063: step 3281, loss 0.0654699, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:06:59.336940: step 3282, loss 0.15709, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:59.752874: step 3283, loss 0.0722024, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:00.148919: step 3284, loss 0.0495458, acc 1, learning_rate 0.000100007
2017-10-10T15:07:00.507213: step 3285, loss 0.100441, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:07:00.920541: step 3286, loss 0.0645505, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:01.347083: step 3287, loss 0.0685737, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:01.812137: step 3288, loss 0.0551831, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:02.260912: step 3289, loss 0.0977387, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:02.698601: step 3290, loss 0.059337, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:07:03.122590: step 3291, loss 0.0373589, acc 1, learning_rate 0.000100007
2017-10-10T15:07:03.528932: step 3292, loss 0.0864547, acc 1, learning_rate 0.000100007
2017-10-10T15:07:03.952995: step 3293, loss 0.110103, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:07:04.444437: step 3294, loss 0.0629274, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:07:04.877540: step 3295, loss 0.0850778, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:05.360862: step 3296, loss 0.0626812, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:05.804933: step 3297, loss 0.169343, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:07:06.254985: step 3298, loss 0.120224, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:07:06.628036: step 3299, loss 0.0947775, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:07:07.016920: step 3300, loss 0.0670818, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:07:07.492631: step 3301, loss 0.0532623, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:07.879793: step 3302, loss 0.115049, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:07:08.230388: step 3303, loss 0.0942892, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:07:08.646457: step 3304, loss 0.0397082, acc 1, learning_rate 0.000100007
2017-10-10T15:07:09.055082: step 3305, loss 0.187441, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:07:09.471199: step 3306, loss 0.0489837, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:07:09.901245: step 3307, loss 0.0322442, acc 1, learning_rate 0.000100007
2017-10-10T15:07:10.294389: step 3308, loss 0.100287, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:07:10.667803: step 3309, loss 0.263794, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:07:11.061015: step 3310, loss 0.103087, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:11.451671: step 3311, loss 0.0919311, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:11.885706: step 3312, loss 0.0727198, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:12.708864: step 3313, loss 0.0920328, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:07:13.135516: step 3314, loss 0.13454, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:13.516880: step 3315, loss 0.101334, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:13.905212: step 3316, loss 0.198312, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:07:14.341797: step 3317, loss 0.0629345, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:14.767270: step 3318, loss 0.101086, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:07:15.220878: step 3319, loss 0.0305286, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:15.669024: step 3320, loss 0.143887, acc 0.9375, learning_rate 0.000100006

Evaluation:
2017-10-10T15:07:16.492977: step 3320, loss 0.21446, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3320

2017-10-10T15:07:17.585417: step 3321, loss 0.0571686, acc 1, learning_rate 0.000100006
2017-10-10T15:07:17.949097: step 3322, loss 0.0773829, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:18.364490: step 3323, loss 0.116937, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:18.780907: step 3324, loss 0.0796331, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:19.292479: step 3325, loss 0.10319, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:19.609604: step 3326, loss 0.048559, acc 1, learning_rate 0.000100006
2017-10-10T15:07:19.992845: step 3327, loss 0.109245, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:07:20.378889: step 3328, loss 0.0630864, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:20.800914: step 3329, loss 0.115936, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:21.229118: step 3330, loss 0.118061, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:21.611405: step 3331, loss 0.158097, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:07:21.953680: step 3332, loss 0.223739, acc 0.901961, learning_rate 0.000100006
2017-10-10T15:07:22.388764: step 3333, loss 0.0562385, acc 1, learning_rate 0.000100006
2017-10-10T15:07:22.851340: step 3334, loss 0.119466, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:07:23.369832: step 3335, loss 0.135417, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:07:23.776868: step 3336, loss 0.0645726, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:24.148095: step 3337, loss 0.10254, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:24.529343: step 3338, loss 0.0654039, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:24.914155: step 3339, loss 0.0341559, acc 1, learning_rate 0.000100006
2017-10-10T15:07:25.276742: step 3340, loss 0.103575, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:25.655459: step 3341, loss 0.0443489, acc 1, learning_rate 0.000100006
2017-10-10T15:07:26.131621: step 3342, loss 0.0333912, acc 1, learning_rate 0.000100006
2017-10-10T15:07:26.627628: step 3343, loss 0.103475, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:27.044811: step 3344, loss 0.0425494, acc 1, learning_rate 0.000100006
2017-10-10T15:07:27.426534: step 3345, loss 0.0717178, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:27.832349: step 3346, loss 0.217619, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:07:28.267066: step 3347, loss 0.0487935, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:28.697385: step 3348, loss 0.0614191, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:29.164966: step 3349, loss 0.0674698, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:07:29.571408: step 3350, loss 0.0470555, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:07:30.064988: step 3351, loss 0.142402, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:07:30.495490: step 3352, loss 0.0839023, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:07:30.885114: step 3353, loss 0.0223345, acc 1, learning_rate 0.000100005
2017-10-10T15:07:31.276165: step 3354, loss 0.0388986, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:31.679172: step 3355, loss 0.0761326, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:32.180947: step 3356, loss 0.109448, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:07:32.565629: step 3357, loss 0.0579204, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:32.933369: step 3358, loss 0.096053, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:07:33.340962: step 3359, loss 0.0811405, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:33.792234: step 3360, loss 0.064372, acc 1, learning_rate 0.000100005

Evaluation:
2017-10-10T15:07:34.680601: step 3360, loss 0.214565, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3360

2017-10-10T15:07:36.068854: step 3361, loss 0.157229, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:07:36.484995: step 3362, loss 0.0520057, acc 1, learning_rate 0.000100005
2017-10-10T15:07:36.863051: step 3363, loss 0.0199254, acc 1, learning_rate 0.000100005
2017-10-10T15:07:37.216702: step 3364, loss 0.220635, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:07:37.820918: step 3365, loss 0.0491718, acc 1, learning_rate 0.000100005
2017-10-10T15:07:38.232072: step 3366, loss 0.081905, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:38.671067: step 3367, loss 0.0582993, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:39.128076: step 3368, loss 0.0882076, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:39.640489: step 3369, loss 0.0377611, acc 1, learning_rate 0.000100005
2017-10-10T15:07:40.001000: step 3370, loss 0.0588576, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:40.339649: step 3371, loss 0.153967, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:07:40.681281: step 3372, loss 0.0804811, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:41.144818: step 3373, loss 0.110097, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:41.588909: step 3374, loss 0.072938, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:41.984649: step 3375, loss 0.0964899, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:07:42.384323: step 3376, loss 0.135354, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:07:42.791382: step 3377, loss 0.0541013, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:43.199621: step 3378, loss 0.143236, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:07:43.565106: step 3379, loss 0.0784771, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:43.991855: step 3380, loss 0.089011, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:44.429029: step 3381, loss 0.141565, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:44.872954: step 3382, loss 0.106209, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:07:45.300036: step 3383, loss 0.109876, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:45.802047: step 3384, loss 0.129273, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:07:46.201614: step 3385, loss 0.0626027, acc 1, learning_rate 0.000100005
2017-10-10T15:07:46.588886: step 3386, loss 0.161299, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:07:46.997240: step 3387, loss 0.0313169, acc 1, learning_rate 0.000100005
2017-10-10T15:07:47.420956: step 3388, loss 0.134039, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:47.859215: step 3389, loss 0.0701164, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:48.317016: step 3390, loss 0.0920849, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:07:48.816880: step 3391, loss 0.0360017, acc 1, learning_rate 0.000100005
2017-10-10T15:07:49.200980: step 3392, loss 0.119717, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:49.548376: step 3393, loss 0.10384, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:49.955118: step 3394, loss 0.143305, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:07:50.401093: step 3395, loss 0.0855338, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:07:50.830613: step 3396, loss 0.0925823, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:51.264822: step 3397, loss 0.143149, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:51.700634: step 3398, loss 0.0699437, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:07:52.100935: step 3399, loss 0.0364377, acc 1, learning_rate 0.000100005
2017-10-10T15:07:52.577133: step 3400, loss 0.129675, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-10T15:07:53.372472: step 3400, loss 0.217231, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3400

2017-10-10T15:07:54.680934: step 3401, loss 0.0414784, acc 1, learning_rate 0.000100004
2017-10-10T15:07:55.068918: step 3402, loss 0.137484, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:07:55.524938: step 3403, loss 0.0454521, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:07:56.054371: step 3404, loss 0.0818774, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:07:56.387479: step 3405, loss 0.172864, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:07:56.716087: step 3406, loss 0.246687, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:07:57.137180: step 3407, loss 0.144254, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:07:57.581432: step 3408, loss 0.0337768, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:07:58.000756: step 3409, loss 0.0965877, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:07:58.426121: step 3410, loss 0.163563, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:07:58.980915: step 3411, loss 0.0537128, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:07:59.368880: step 3412, loss 0.0793533, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:07:59.748879: step 3413, loss 0.0464633, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:00.143486: step 3414, loss 0.121629, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:00.560813: step 3415, loss 0.0464386, acc 1, learning_rate 0.000100004
2017-10-10T15:08:01.037038: step 3416, loss 0.117549, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:08:01.359263: step 3417, loss 0.106294, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:08:01.735165: step 3418, loss 0.115751, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:08:02.151205: step 3419, loss 0.188309, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:08:02.576638: step 3420, loss 0.0805315, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:02.935237: step 3421, loss 0.140137, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:03.348917: step 3422, loss 0.0585038, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:03.793507: step 3423, loss 0.123157, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:08:04.135382: step 3424, loss 0.0611244, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:04.592990: step 3425, loss 0.10976, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:05.005069: step 3426, loss 0.119614, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:08:05.385106: step 3427, loss 0.0691697, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:05.840627: step 3428, loss 0.0783367, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:06.220638: step 3429, loss 0.0823392, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:06.572853: step 3430, loss 0.127547, acc 0.941176, learning_rate 0.000100004
2017-10-10T15:08:07.001088: step 3431, loss 0.128926, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:08:07.419788: step 3432, loss 0.0795323, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:07.803207: step 3433, loss 0.102516, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:08.192432: step 3434, loss 0.0247359, acc 1, learning_rate 0.000100004
2017-10-10T15:08:08.576921: step 3435, loss 0.132998, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:09.046541: step 3436, loss 0.0310894, acc 1, learning_rate 0.000100004
2017-10-10T15:08:09.512903: step 3437, loss 0.0807206, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:09.984312: step 3438, loss 0.169648, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:08:10.349657: step 3439, loss 0.0419424, acc 1, learning_rate 0.000100004
2017-10-10T15:08:10.767087: step 3440, loss 0.102132, acc 0.96875, learning_rate 0.000100004

Evaluation:
2017-10-10T15:08:11.640886: step 3440, loss 0.219387, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3440

2017-10-10T15:08:13.043559: step 3441, loss 0.161265, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:13.313501: step 3442, loss 0.0998961, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:13.574816: step 3443, loss 0.0811005, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:13.968921: step 3444, loss 0.147176, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:08:14.400992: step 3445, loss 0.0241635, acc 1, learning_rate 0.000100004
2017-10-10T15:08:14.779198: step 3446, loss 0.111627, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:15.139086: step 3447, loss 0.121858, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:08:15.592149: step 3448, loss 0.100757, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:08:16.017000: step 3449, loss 0.0434137, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:16.373031: step 3450, loss 0.110634, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:08:16.752840: step 3451, loss 0.0308224, acc 1, learning_rate 0.000100004
2017-10-10T15:08:17.197105: step 3452, loss 0.0697652, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:17.604727: step 3453, loss 0.0217961, acc 1, learning_rate 0.000100004
2017-10-10T15:08:18.041056: step 3454, loss 0.0945662, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:18.452762: step 3455, loss 0.0989934, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:08:18.859620: step 3456, loss 0.0467212, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:08:19.269021: step 3457, loss 0.0530269, acc 1, learning_rate 0.000100004
2017-10-10T15:08:19.708886: step 3458, loss 0.0931322, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:20.153526: step 3459, loss 0.115092, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:20.582872: step 3460, loss 0.153848, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:08:21.037089: step 3461, loss 0.118105, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:08:21.421207: step 3462, loss 0.0734984, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:21.870156: step 3463, loss 0.117813, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:22.304087: step 3464, loss 0.188215, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:08:22.753169: step 3465, loss 0.0813841, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:23.193901: step 3466, loss 0.130422, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:23.558812: step 3467, loss 0.135585, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:23.959181: step 3468, loss 0.0687045, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:24.377836: step 3469, loss 0.0412275, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:24.751000: step 3470, loss 0.0809231, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:25.163297: step 3471, loss 0.0806572, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:25.616869: step 3472, loss 0.0364396, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:26.049507: step 3473, loss 0.0566473, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:26.358672: step 3474, loss 0.241639, acc 0.90625, learning_rate 0.000100003
2017-10-10T15:08:26.757343: step 3475, loss 0.0774149, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:27.229192: step 3476, loss 0.0562349, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:27.576772: step 3477, loss 0.0460808, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:28.012999: step 3478, loss 0.0436168, acc 1, learning_rate 0.000100003
2017-10-10T15:08:28.426617: step 3479, loss 0.108938, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:28.925129: step 3480, loss 0.158314, acc 0.9375, learning_rate 0.000100003

Evaluation:
2017-10-10T15:08:29.764965: step 3480, loss 0.213993, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3480

2017-10-10T15:08:31.049116: step 3481, loss 0.0811442, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:31.448715: step 3482, loss 0.09651, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:31.928373: step 3483, loss 0.053935, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:32.324810: step 3484, loss 0.0523316, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:32.770196: step 3485, loss 0.0802557, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:33.208840: step 3486, loss 0.0392674, acc 1, learning_rate 0.000100003
2017-10-10T15:08:33.586276: step 3487, loss 0.0250941, acc 1, learning_rate 0.000100003
2017-10-10T15:08:34.002934: step 3488, loss 0.0268513, acc 1, learning_rate 0.000100003
2017-10-10T15:08:34.452399: step 3489, loss 0.0254301, acc 1, learning_rate 0.000100003
2017-10-10T15:08:34.860972: step 3490, loss 0.113435, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:35.302431: step 3491, loss 0.184362, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:08:35.724844: step 3492, loss 0.0508798, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:36.185155: step 3493, loss 0.100499, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:36.601058: step 3494, loss 0.121417, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:37.012939: step 3495, loss 0.0874458, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:37.411794: step 3496, loss 0.0307428, acc 1, learning_rate 0.000100003
2017-10-10T15:08:37.808833: step 3497, loss 0.0769194, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:38.185859: step 3498, loss 0.316803, acc 0.875, learning_rate 0.000100003
2017-10-10T15:08:38.565909: step 3499, loss 0.0515292, acc 1, learning_rate 0.000100003
2017-10-10T15:08:39.028908: step 3500, loss 0.0893219, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:39.428917: step 3501, loss 0.0997741, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:39.817362: step 3502, loss 0.0880728, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:40.210261: step 3503, loss 0.109542, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:40.593218: step 3504, loss 0.0719474, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:41.030088: step 3505, loss 0.0837766, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:41.440926: step 3506, loss 0.117531, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:08:41.852327: step 3507, loss 0.0446835, acc 1, learning_rate 0.000100003
2017-10-10T15:08:42.196913: step 3508, loss 0.0540969, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:42.617693: step 3509, loss 0.186833, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:08:43.021239: step 3510, loss 0.122333, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:43.450511: step 3511, loss 0.082913, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:43.816064: step 3512, loss 0.147028, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:44.281004: step 3513, loss 0.0474017, acc 1, learning_rate 0.000100003
2017-10-10T15:08:44.667561: step 3514, loss 0.107969, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:45.052824: step 3515, loss 0.181697, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:08:45.483078: step 3516, loss 0.0248645, acc 1, learning_rate 0.000100003
2017-10-10T15:08:45.861226: step 3517, loss 0.0586013, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:46.376465: step 3518, loss 0.0318612, acc 1, learning_rate 0.000100003
2017-10-10T15:08:46.723817: step 3519, loss 0.123556, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:47.056930: step 3520, loss 0.0785854, acc 0.984375, learning_rate 0.000100003

Evaluation:
2017-10-10T15:08:47.807743: step 3520, loss 0.215023, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3520

2017-10-10T15:08:49.116501: step 3521, loss 0.17278, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:08:49.550621: step 3522, loss 0.10478, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:49.939020: step 3523, loss 0.0456933, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:50.306287: step 3524, loss 0.103457, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:50.824946: step 3525, loss 0.0527677, acc 1, learning_rate 0.000100003
2017-10-10T15:08:51.245062: step 3526, loss 0.0669758, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:51.621467: step 3527, loss 0.0461796, acc 1, learning_rate 0.000100003
2017-10-10T15:08:51.980873: step 3528, loss 0.0162182, acc 1, learning_rate 0.000100003
2017-10-10T15:08:52.419375: step 3529, loss 0.142787, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:52.785560: step 3530, loss 0.079484, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:53.183376: step 3531, loss 0.230494, acc 0.90625, learning_rate 0.000100003
2017-10-10T15:08:53.588866: step 3532, loss 0.0422387, acc 1, learning_rate 0.000100003
2017-10-10T15:08:53.997956: step 3533, loss 0.255968, acc 0.890625, learning_rate 0.000100003
2017-10-10T15:08:54.405354: step 3534, loss 0.0511405, acc 1, learning_rate 0.000100003
2017-10-10T15:08:54.826197: step 3535, loss 0.0615881, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:55.199657: step 3536, loss 0.0621216, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:55.671732: step 3537, loss 0.0455965, acc 1, learning_rate 0.000100003
2017-10-10T15:08:56.104400: step 3538, loss 0.15878, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:56.469201: step 3539, loss 0.0862107, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:56.866091: step 3540, loss 0.0811117, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:08:57.248957: step 3541, loss 0.108115, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:08:57.658463: step 3542, loss 0.13466, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:08:58.013121: step 3543, loss 0.107809, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:08:58.405047: step 3544, loss 0.111386, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:58.874841: step 3545, loss 0.0616088, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:59.265201: step 3546, loss 0.125793, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:59.657612: step 3547, loss 0.15211, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:00.059646: step 3548, loss 0.0361042, acc 1, learning_rate 0.000100002
2017-10-10T15:09:00.451479: step 3549, loss 0.0864474, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:00.876880: step 3550, loss 0.0482832, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:01.287147: step 3551, loss 0.0521745, acc 1, learning_rate 0.000100002
2017-10-10T15:09:01.752922: step 3552, loss 0.0785298, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:02.189020: step 3553, loss 0.101827, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:02.597918: step 3554, loss 0.0259962, acc 1, learning_rate 0.000100002
2017-10-10T15:09:03.056118: step 3555, loss 0.0701917, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:03.400852: step 3556, loss 0.0313659, acc 1, learning_rate 0.000100002
2017-10-10T15:09:04.516848: step 3557, loss 0.207709, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:09:04.924246: step 3558, loss 0.0729002, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:05.388595: step 3559, loss 0.0753708, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:05.768865: step 3560, loss 0.0628451, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T15:09:06.612982: step 3560, loss 0.216382, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3560

2017-10-10T15:09:07.950682: step 3561, loss 0.0509042, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:08.368562: step 3562, loss 0.0831268, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:08.780821: step 3563, loss 0.145749, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:09.178613: step 3564, loss 0.0399391, acc 1, learning_rate 0.000100002
2017-10-10T15:09:09.611285: step 3565, loss 0.0507122, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:09.986893: step 3566, loss 0.0440073, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:10.379144: step 3567, loss 0.0732772, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:10.877373: step 3568, loss 0.0832835, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:11.267554: step 3569, loss 0.15572, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:11.676941: step 3570, loss 0.0786209, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:12.049149: step 3571, loss 0.156058, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:09:12.467531: step 3572, loss 0.0875946, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:12.941815: step 3573, loss 0.123267, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:13.353846: step 3574, loss 0.12674, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:13.817380: step 3575, loss 0.096846, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:14.174190: step 3576, loss 0.0586829, acc 1, learning_rate 0.000100002
2017-10-10T15:09:14.626193: step 3577, loss 0.218012, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:09:15.016353: step 3578, loss 0.094024, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:15.357009: step 3579, loss 0.064314, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:15.829059: step 3580, loss 0.0484884, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:16.244887: step 3581, loss 0.103296, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:16.649059: step 3582, loss 0.0454083, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:17.079690: step 3583, loss 0.158152, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:09:17.459223: step 3584, loss 0.0821328, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:17.851755: step 3585, loss 0.0515092, acc 1, learning_rate 0.000100002
2017-10-10T15:09:18.261141: step 3586, loss 0.133797, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:18.729050: step 3587, loss 0.0316001, acc 1, learning_rate 0.000100002
2017-10-10T15:09:19.108923: step 3588, loss 0.074924, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:19.637482: step 3589, loss 0.0971253, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:19.982966: step 3590, loss 0.0933333, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:20.404356: step 3591, loss 0.0373666, acc 1, learning_rate 0.000100002
2017-10-10T15:09:20.709371: step 3592, loss 0.0919125, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:21.154536: step 3593, loss 0.0357829, acc 1, learning_rate 0.000100002
2017-10-10T15:09:21.576903: step 3594, loss 0.0687488, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:22.041196: step 3595, loss 0.0601356, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:22.433535: step 3596, loss 0.0720399, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:22.835395: step 3597, loss 0.119626, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:23.300495: step 3598, loss 0.0700838, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:23.725452: step 3599, loss 0.0759088, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:24.103582: step 3600, loss 0.0716093, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T15:09:24.893662: step 3600, loss 0.21438, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3600

2017-10-10T15:09:26.251230: step 3601, loss 0.0514783, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:26.715632: step 3602, loss 0.0662582, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:27.072000: step 3603, loss 0.0888167, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:27.471132: step 3604, loss 0.103414, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:27.915754: step 3605, loss 0.084321, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:28.285153: step 3606, loss 0.131113, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:28.617891: step 3607, loss 0.197212, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:29.026071: step 3608, loss 0.116023, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:29.395774: step 3609, loss 0.117146, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:29.852614: step 3610, loss 0.0473545, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:30.287073: step 3611, loss 0.0548252, acc 1, learning_rate 0.000100002
2017-10-10T15:09:30.752469: step 3612, loss 0.0234923, acc 1, learning_rate 0.000100002
2017-10-10T15:09:31.124996: step 3613, loss 0.0505329, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:31.560619: step 3614, loss 0.0633621, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:31.992925: step 3615, loss 0.0911599, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:32.398316: step 3616, loss 0.0845234, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:32.853001: step 3617, loss 0.0455539, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:33.276963: step 3618, loss 0.0613868, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:33.698982: step 3619, loss 0.0380533, acc 1, learning_rate 0.000100002
2017-10-10T15:09:34.105194: step 3620, loss 0.128478, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:09:34.490841: step 3621, loss 0.0763569, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:34.894917: step 3622, loss 0.112266, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:35.264954: step 3623, loss 0.0519473, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:35.702626: step 3624, loss 0.0926704, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:36.202666: step 3625, loss 0.145726, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:36.567837: step 3626, loss 0.19242, acc 0.901961, learning_rate 0.000100002
2017-10-10T15:09:36.865137: step 3627, loss 0.141401, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:37.230670: step 3628, loss 0.0340048, acc 1, learning_rate 0.000100002
2017-10-10T15:09:37.636363: step 3629, loss 0.0660105, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:38.057198: step 3630, loss 0.103669, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:38.470217: step 3631, loss 0.0639341, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:38.878920: step 3632, loss 0.112837, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:39.329099: step 3633, loss 0.0580537, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:39.682172: step 3634, loss 0.16401, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:09:40.039420: step 3635, loss 0.0717005, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:40.440328: step 3636, loss 0.131864, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:40.776535: step 3637, loss 0.0514045, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:41.149032: step 3638, loss 0.0999762, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:41.604657: step 3639, loss 0.0475675, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:42.039811: step 3640, loss 0.0779473, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T15:09:42.897765: step 3640, loss 0.214724, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3640

2017-10-10T15:09:44.260975: step 3641, loss 0.128785, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:44.612109: step 3642, loss 0.138208, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:09:45.005154: step 3643, loss 0.0394454, acc 1, learning_rate 0.000100002
2017-10-10T15:09:45.436933: step 3644, loss 0.0519289, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:45.856968: step 3645, loss 0.0667055, acc 1, learning_rate 0.000100002
2017-10-10T15:09:46.306046: step 3646, loss 0.0246128, acc 1, learning_rate 0.000100002
2017-10-10T15:09:46.729024: step 3647, loss 0.068053, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:47.095843: step 3648, loss 0.0988402, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:47.489026: step 3649, loss 0.0633572, acc 1, learning_rate 0.000100002
2017-10-10T15:09:47.849001: step 3650, loss 0.0510907, acc 1, learning_rate 0.000100002
2017-10-10T15:09:48.259593: step 3651, loss 0.0904059, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:48.672817: step 3652, loss 0.108292, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:49.074266: step 3653, loss 0.0667694, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:49.504464: step 3654, loss 0.10227, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:49.910098: step 3655, loss 0.0768019, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:50.346801: step 3656, loss 0.0713035, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:50.749318: step 3657, loss 0.098787, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:51.171697: step 3658, loss 0.082254, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:51.596497: step 3659, loss 0.0978738, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:51.993044: step 3660, loss 0.125344, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:52.372841: step 3661, loss 0.105853, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:52.798622: step 3662, loss 0.0509648, acc 1, learning_rate 0.000100002
2017-10-10T15:09:53.287328: step 3663, loss 0.0615704, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:53.629678: step 3664, loss 0.085301, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:53.978002: step 3665, loss 0.0861499, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:54.396895: step 3666, loss 0.0713939, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:54.815462: step 3667, loss 0.0586453, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:55.186359: step 3668, loss 0.0790793, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:55.517078: step 3669, loss 0.0484484, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:55.885757: step 3670, loss 0.0798026, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:56.297131: step 3671, loss 0.0517458, acc 1, learning_rate 0.000100001
2017-10-10T15:09:56.700932: step 3672, loss 0.0712388, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:57.117200: step 3673, loss 0.085132, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:57.537049: step 3674, loss 0.0992604, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:57.960863: step 3675, loss 0.0592933, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:58.420899: step 3676, loss 0.056566, acc 1, learning_rate 0.000100001
2017-10-10T15:09:58.854985: step 3677, loss 0.089531, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:59.217034: step 3678, loss 0.113738, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:59.629006: step 3679, loss 0.0822063, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:00.084887: step 3680, loss 0.107107, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T15:10:00.872967: step 3680, loss 0.214903, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3680

2017-10-10T15:10:02.005187: step 3681, loss 0.0657999, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:02.460936: step 3682, loss 0.0386866, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:02.832576: step 3683, loss 0.0933902, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:03.213836: step 3684, loss 0.0774622, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:03.645280: step 3685, loss 0.175452, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:04.146281: step 3686, loss 0.0481796, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:04.534538: step 3687, loss 0.180077, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:04.880513: step 3688, loss 0.0434075, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:05.348933: step 3689, loss 0.107713, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:05.772971: step 3690, loss 0.0634652, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:06.212422: step 3691, loss 0.110662, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:06.609684: step 3692, loss 0.0949077, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:07.038960: step 3693, loss 0.0908891, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:07.473287: step 3694, loss 0.188336, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:07.895346: step 3695, loss 0.121544, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:08.285586: step 3696, loss 0.0651147, acc 1, learning_rate 0.000100001
2017-10-10T15:10:08.744869: step 3697, loss 0.113069, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:09.248989: step 3698, loss 0.101722, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:09.684960: step 3699, loss 0.130292, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:10.070686: step 3700, loss 0.0684989, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:10.412864: step 3701, loss 0.093683, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:10.837109: step 3702, loss 0.158548, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:11.285807: step 3703, loss 0.0694261, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:11.704915: step 3704, loss 0.105198, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:12.105010: step 3705, loss 0.0538097, acc 1, learning_rate 0.000100001
2017-10-10T15:10:12.538237: step 3706, loss 0.0456789, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:12.906203: step 3707, loss 0.0721893, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:13.329448: step 3708, loss 0.0557986, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:13.852666: step 3709, loss 0.0817432, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:14.212939: step 3710, loss 0.0730777, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:14.596848: step 3711, loss 0.0526694, acc 1, learning_rate 0.000100001
2017-10-10T15:10:14.926979: step 3712, loss 0.121668, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:15.335261: step 3713, loss 0.0339432, acc 1, learning_rate 0.000100001
2017-10-10T15:10:15.754622: step 3714, loss 0.0765412, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:16.165125: step 3715, loss 0.0779181, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:16.550279: step 3716, loss 0.0717161, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:16.936890: step 3717, loss 0.0783695, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:17.311487: step 3718, loss 0.0324738, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:17.691125: step 3719, loss 0.070201, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:18.164950: step 3720, loss 0.17868, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:10:19.002077: step 3720, loss 0.214163, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3720

2017-10-10T15:10:20.380886: step 3721, loss 0.0881025, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:20.776869: step 3722, loss 0.0984372, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:21.193959: step 3723, loss 0.108895, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:21.555317: step 3724, loss 0.106534, acc 0.960784, learning_rate 0.000100001
2017-10-10T15:10:21.941038: step 3725, loss 0.0633956, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:22.361130: step 3726, loss 0.0787761, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:22.805528: step 3727, loss 0.160275, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:23.195868: step 3728, loss 0.0470309, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:23.624075: step 3729, loss 0.0457186, acc 1, learning_rate 0.000100001
2017-10-10T15:10:24.014153: step 3730, loss 0.140489, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:24.439618: step 3731, loss 0.0843901, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:24.859615: step 3732, loss 0.171445, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:25.298978: step 3733, loss 0.0956966, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:25.708518: step 3734, loss 0.0530608, acc 1, learning_rate 0.000100001
2017-10-10T15:10:26.193770: step 3735, loss 0.0863607, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:26.564980: step 3736, loss 0.0713893, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:26.930100: step 3737, loss 0.0832747, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:27.344905: step 3738, loss 0.0738684, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:27.967067: step 3739, loss 0.0398929, acc 1, learning_rate 0.000100001
2017-10-10T15:10:28.291748: step 3740, loss 0.0816473, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:28.717180: step 3741, loss 0.120679, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:29.092472: step 3742, loss 0.102727, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:29.420192: step 3743, loss 0.0404378, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:29.882061: step 3744, loss 0.0862193, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:30.320762: step 3745, loss 0.147142, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:30.779111: step 3746, loss 0.0556113, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:31.217083: step 3747, loss 0.0816454, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:31.628667: step 3748, loss 0.131404, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:32.027773: step 3749, loss 0.0846687, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:32.437060: step 3750, loss 0.105016, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:32.930839: step 3751, loss 0.115855, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:33.318563: step 3752, loss 0.102848, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:33.737684: step 3753, loss 0.060024, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:34.166882: step 3754, loss 0.0759444, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:34.605020: step 3755, loss 0.033931, acc 1, learning_rate 0.000100001
2017-10-10T15:10:35.021137: step 3756, loss 0.0734496, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:35.430494: step 3757, loss 0.0684016, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:35.848964: step 3758, loss 0.104522, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:36.261133: step 3759, loss 0.107265, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:36.743667: step 3760, loss 0.0998508, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T15:10:37.542951: step 3760, loss 0.217279, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3760

2017-10-10T15:10:38.906959: step 3761, loss 0.0745965, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:39.278829: step 3762, loss 0.0573618, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:39.656872: step 3763, loss 0.0572999, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:40.043317: step 3764, loss 0.110373, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:40.387892: step 3765, loss 0.0976489, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:40.897206: step 3766, loss 0.0705184, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:41.232310: step 3767, loss 0.0991997, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:41.560001: step 3768, loss 0.0937807, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:41.932042: step 3769, loss 0.0681113, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:42.396966: step 3770, loss 0.112896, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:42.842579: step 3771, loss 0.0629636, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:43.198033: step 3772, loss 0.129984, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:43.563927: step 3773, loss 0.0940801, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:44.001156: step 3774, loss 0.0977593, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:44.391424: step 3775, loss 0.15699, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:44.800833: step 3776, loss 0.131581, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:45.188563: step 3777, loss 0.0601643, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:45.587134: step 3778, loss 0.0346773, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:45.988382: step 3779, loss 0.100369, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:46.374305: step 3780, loss 0.0675617, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:46.820966: step 3781, loss 0.0343671, acc 1, learning_rate 0.000100001
2017-10-10T15:10:47.287113: step 3782, loss 0.0632998, acc 1, learning_rate 0.000100001
2017-10-10T15:10:47.689008: step 3783, loss 0.0857732, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:48.099455: step 3784, loss 0.0501563, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:48.456614: step 3785, loss 0.0845124, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:48.837392: step 3786, loss 0.0499873, acc 1, learning_rate 0.000100001
2017-10-10T15:10:49.190379: step 3787, loss 0.0947866, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:49.564469: step 3788, loss 0.115747, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:50.019433: step 3789, loss 0.103391, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:50.395341: step 3790, loss 0.094296, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:50.720744: step 3791, loss 0.0564694, acc 1, learning_rate 0.000100001
2017-10-10T15:10:51.145092: step 3792, loss 0.0819311, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:51.499917: step 3793, loss 0.0901045, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:51.877066: step 3794, loss 0.0420712, acc 1, learning_rate 0.000100001
2017-10-10T15:10:52.321633: step 3795, loss 0.0524259, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:52.645184: step 3796, loss 0.17152, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:53.172877: step 3797, loss 0.063734, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:53.551944: step 3798, loss 0.0685519, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:53.855809: step 3799, loss 0.105849, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:54.309213: step 3800, loss 0.0763297, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:10:55.118360: step 3800, loss 0.214223, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3800

2017-10-10T15:10:56.507753: step 3801, loss 0.0231632, acc 1, learning_rate 0.000100001
2017-10-10T15:10:56.912892: step 3802, loss 0.117813, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:57.313033: step 3803, loss 0.0574148, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:57.654781: step 3804, loss 0.202292, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:58.107427: step 3805, loss 0.0637094, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:58.477132: step 3806, loss 0.0761668, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:58.905371: step 3807, loss 0.0567539, acc 1, learning_rate 0.000100001
2017-10-10T15:10:59.428193: step 3808, loss 0.0922953, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:59.754133: step 3809, loss 0.168155, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:00.084861: step 3810, loss 0.134507, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:00.513925: step 3811, loss 0.0784564, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:00.954327: step 3812, loss 0.0691596, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:01.271117: step 3813, loss 0.0483607, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:01.612940: step 3814, loss 0.0972053, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:02.000073: step 3815, loss 0.0324678, acc 1, learning_rate 0.000100001
2017-10-10T15:11:02.452668: step 3816, loss 0.0683225, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:02.850615: step 3817, loss 0.0907253, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:03.287191: step 3818, loss 0.122921, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:03.707742: step 3819, loss 0.0586425, acc 1, learning_rate 0.000100001
2017-10-10T15:11:04.113230: step 3820, loss 0.0792034, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:04.530732: step 3821, loss 0.0900555, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:04.874520: step 3822, loss 0.106668, acc 0.960784, learning_rate 0.000100001
2017-10-10T15:11:05.382317: step 3823, loss 0.15402, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:05.726734: step 3824, loss 0.0836285, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:06.216929: step 3825, loss 0.0801917, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:06.574803: step 3826, loss 0.04283, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:06.960860: step 3827, loss 0.0647396, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:07.472831: step 3828, loss 0.0614821, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:07.852579: step 3829, loss 0.0655061, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:08.197701: step 3830, loss 0.0396542, acc 1, learning_rate 0.000100001
2017-10-10T15:11:08.608620: step 3831, loss 0.0165489, acc 1, learning_rate 0.000100001
2017-10-10T15:11:08.968878: step 3832, loss 0.0468298, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:09.369081: step 3833, loss 0.0189298, acc 1, learning_rate 0.000100001
2017-10-10T15:11:09.776843: step 3834, loss 0.0686453, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:10.189239: step 3835, loss 0.0383647, acc 1, learning_rate 0.000100001
2017-10-10T15:11:10.641114: step 3836, loss 0.103614, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:11.036417: step 3837, loss 0.137864, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:11.490317: step 3838, loss 0.0589234, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:11.961329: step 3839, loss 0.0268808, acc 1, learning_rate 0.000100001
2017-10-10T15:11:12.314503: step 3840, loss 0.0545562, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T15:11:13.132927: step 3840, loss 0.212799, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3840

2017-10-10T15:11:14.530436: step 3841, loss 0.0332465, acc 1, learning_rate 0.000100001
2017-10-10T15:11:14.936881: step 3842, loss 0.178694, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:15.380110: step 3843, loss 0.0753469, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:15.823886: step 3844, loss 0.0901823, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:16.269278: step 3845, loss 0.0932295, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:16.627781: step 3846, loss 0.0969733, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:16.958003: step 3847, loss 0.145773, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:17.302362: step 3848, loss 0.0886984, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:17.752857: step 3849, loss 0.0631603, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:18.227572: step 3850, loss 0.154007, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:18.578075: step 3851, loss 0.0932727, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:18.960251: step 3852, loss 0.132501, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:19.307472: step 3853, loss 0.0436185, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:19.697673: step 3854, loss 0.10851, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:20.112323: step 3855, loss 0.0924963, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:20.549029: step 3856, loss 0.167315, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:20.996966: step 3857, loss 0.060159, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:21.401001: step 3858, loss 0.0913215, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:21.772089: step 3859, loss 0.0545613, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:22.227485: step 3860, loss 0.0281996, acc 1, learning_rate 0.000100001
2017-10-10T15:11:22.617262: step 3861, loss 0.0637715, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:23.087830: step 3862, loss 0.0917626, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:23.453423: step 3863, loss 0.0504543, acc 1, learning_rate 0.000100001
2017-10-10T15:11:23.820921: step 3864, loss 0.113953, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:24.250020: step 3865, loss 0.0387745, acc 1, learning_rate 0.000100001
2017-10-10T15:11:24.638544: step 3866, loss 0.0505011, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:25.079165: step 3867, loss 0.0963009, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:25.517009: step 3868, loss 0.150387, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:25.928946: step 3869, loss 0.0791228, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:26.372577: step 3870, loss 0.0677799, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:26.836812: step 3871, loss 0.117404, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:27.295395: step 3872, loss 0.0675079, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:27.686170: step 3873, loss 0.0401641, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:28.072235: step 3874, loss 0.111835, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:28.461733: step 3875, loss 0.0747508, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:28.883099: step 3876, loss 0.0552827, acc 1, learning_rate 0.000100001
2017-10-10T15:11:29.251333: step 3877, loss 0.109387, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:29.623640: step 3878, loss 0.072891, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:30.060218: step 3879, loss 0.108555, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:30.432891: step 3880, loss 0.0410588, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:11:31.321566: step 3880, loss 0.214631, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3880

2017-10-10T15:11:32.745606: step 3881, loss 0.0605199, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:33.113604: step 3882, loss 0.0975974, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:33.473026: step 3883, loss 0.07687, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:33.971291: step 3884, loss 0.044962, acc 1, learning_rate 0.000100001
2017-10-10T15:11:34.308823: step 3885, loss 0.119846, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:34.666709: step 3886, loss 0.0533501, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:35.095164: step 3887, loss 0.0779712, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:35.521045: step 3888, loss 0.0344395, acc 1, learning_rate 0.000100001
2017-10-10T15:11:35.904898: step 3889, loss 0.0493298, acc 1, learning_rate 0.000100001
2017-10-10T15:11:36.360577: step 3890, loss 0.0353938, acc 1, learning_rate 0.000100001
2017-10-10T15:11:36.802953: step 3891, loss 0.0607062, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:37.213329: step 3892, loss 0.143574, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:37.664981: step 3893, loss 0.0223497, acc 1, learning_rate 0.000100001
2017-10-10T15:11:38.088993: step 3894, loss 0.0707734, acc 1, learning_rate 0.000100001
2017-10-10T15:11:38.505157: step 3895, loss 0.0589862, acc 1, learning_rate 0.000100001
2017-10-10T15:11:38.951885: step 3896, loss 0.0480534, acc 1, learning_rate 0.000100001
2017-10-10T15:11:39.397008: step 3897, loss 0.1684, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:39.836826: step 3898, loss 0.112844, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:40.213789: step 3899, loss 0.235322, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:40.610731: step 3900, loss 0.0923806, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:41.064464: step 3901, loss 0.127614, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:41.425095: step 3902, loss 0.0782952, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:41.845057: step 3903, loss 0.0797049, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:42.304929: step 3904, loss 0.0537009, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:42.721364: step 3905, loss 0.058421, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:43.185075: step 3906, loss 0.103445, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:43.542906: step 3907, loss 0.140723, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:43.968925: step 3908, loss 0.115777, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:44.380956: step 3909, loss 0.0254465, acc 1, learning_rate 0.000100001
2017-10-10T15:11:44.880827: step 3910, loss 0.0352049, acc 1, learning_rate 0.000100001
2017-10-10T15:11:45.281230: step 3911, loss 0.104977, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:45.656606: step 3912, loss 0.0496534, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:46.095892: step 3913, loss 0.0657284, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:46.509451: step 3914, loss 0.0968028, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:46.950845: step 3915, loss 0.104339, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:47.386010: step 3916, loss 0.057152, acc 1, learning_rate 0.000100001
2017-10-10T15:11:47.776953: step 3917, loss 0.0624805, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:48.190506: step 3918, loss 0.0712247, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:48.591698: step 3919, loss 0.0411371, acc 1, learning_rate 0.000100001
2017-10-10T15:11:48.973089: step 3920, loss 0.0244588, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-10T15:11:49.865828: step 3920, loss 0.211874, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3920

2017-10-10T15:11:51.173591: step 3921, loss 0.0992175, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:51.631311: step 3922, loss 0.107749, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:52.121520: step 3923, loss 0.0530595, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:52.543626: step 3924, loss 0.0550326, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:53.008929: step 3925, loss 0.0419409, acc 1, learning_rate 0.000100001
2017-10-10T15:11:53.410362: step 3926, loss 0.101703, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:53.798911: step 3927, loss 0.130383, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:54.207191: step 3928, loss 0.0395749, acc 1, learning_rate 0.000100001
2017-10-10T15:11:54.673253: step 3929, loss 0.113292, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:55.141222: step 3930, loss 0.0756898, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:55.556444: step 3931, loss 0.0454897, acc 1, learning_rate 0.000100001
2017-10-10T15:11:55.952596: step 3932, loss 0.0958711, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:56.285259: step 3933, loss 0.0645558, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:56.660890: step 3934, loss 0.118635, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:57.065811: step 3935, loss 0.0815706, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:57.508875: step 3936, loss 0.0600889, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:57.947543: step 3937, loss 0.0363487, acc 1, learning_rate 0.0001
2017-10-10T15:11:58.316869: step 3938, loss 0.0601262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:58.717064: step 3939, loss 0.0612175, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:59.171685: step 3940, loss 0.0406525, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:59.603411: step 3941, loss 0.0549104, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:00.045911: step 3942, loss 0.110376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:00.481568: step 3943, loss 0.0801624, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:00.850116: step 3944, loss 0.0274444, acc 1, learning_rate 0.0001
2017-10-10T15:12:01.253201: step 3945, loss 0.0441157, acc 1, learning_rate 0.0001
2017-10-10T15:12:01.701097: step 3946, loss 0.0867586, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:02.088939: step 3947, loss 0.0528918, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:02.561290: step 3948, loss 0.0521769, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:02.983687: step 3949, loss 0.0665072, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:03.414460: step 3950, loss 0.119549, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:03.849157: step 3951, loss 0.0516898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:04.249012: step 3952, loss 0.0743628, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:04.652028: step 3953, loss 0.0598968, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:05.087288: step 3954, loss 0.0392518, acc 1, learning_rate 0.0001
2017-10-10T15:12:05.492993: step 3955, loss 0.0694771, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:06.020519: step 3956, loss 0.0255916, acc 1, learning_rate 0.0001
2017-10-10T15:12:06.367785: step 3957, loss 0.0469099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:06.733674: step 3958, loss 0.0749381, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:07.168102: step 3959, loss 0.0797856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:07.564609: step 3960, loss 0.0999142, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:08.252594: step 3960, loss 0.213811, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-3960

2017-10-10T15:12:09.673420: step 3961, loss 0.132478, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:10.090872: step 3962, loss 0.0355979, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:10.505687: step 3963, loss 0.0497489, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:10.929291: step 3964, loss 0.0976139, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:11.371996: step 3965, loss 0.0692307, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:11.785709: step 3966, loss 0.0481096, acc 1, learning_rate 0.0001
2017-10-10T15:12:12.179180: step 3967, loss 0.0750929, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:12.567280: step 3968, loss 0.0255143, acc 1, learning_rate 0.0001
2017-10-10T15:12:12.971308: step 3969, loss 0.069091, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:13.361712: step 3970, loss 0.116421, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:13.829155: step 3971, loss 0.0801627, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:14.194387: step 3972, loss 0.0320382, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:14.674743: step 3973, loss 0.0758111, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:15.073366: step 3974, loss 0.206107, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:15.474686: step 3975, loss 0.0465206, acc 1, learning_rate 0.0001
2017-10-10T15:12:15.880862: step 3976, loss 0.0480771, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:16.276071: step 3977, loss 0.0290772, acc 1, learning_rate 0.0001
2017-10-10T15:12:16.684887: step 3978, loss 0.0552722, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:17.095950: step 3979, loss 0.028569, acc 1, learning_rate 0.0001
2017-10-10T15:12:17.487549: step 3980, loss 0.0902172, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:17.933576: step 3981, loss 0.0158843, acc 1, learning_rate 0.0001
2017-10-10T15:12:18.384679: step 3982, loss 0.0549193, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:18.748965: step 3983, loss 0.0597122, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:19.150751: step 3984, loss 0.113216, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:19.500882: step 3985, loss 0.0444793, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:19.982310: step 3986, loss 0.0939938, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:20.323627: step 3987, loss 0.0421383, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:20.688901: step 3988, loss 0.0961289, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:21.140671: step 3989, loss 0.121659, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:21.512505: step 3990, loss 0.116608, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:21.943756: step 3991, loss 0.114731, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:22.427022: step 3992, loss 0.0282059, acc 1, learning_rate 0.0001
2017-10-10T15:12:22.862127: step 3993, loss 0.0602962, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:23.208021: step 3994, loss 0.197607, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:23.531669: step 3995, loss 0.0889331, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:23.952349: step 3996, loss 0.113792, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:24.345230: step 3997, loss 0.098214, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:24.769069: step 3998, loss 0.0834519, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:25.174712: step 3999, loss 0.124754, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:25.637613: step 4000, loss 0.0411566, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:26.490368: step 4000, loss 0.216226, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4000

2017-10-10T15:12:27.763289: step 4001, loss 0.132975, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.225968: step 4002, loss 0.0844633, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.644254: step 4003, loss 0.0549965, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:29.021054: step 4004, loss 0.0562678, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:29.474651: step 4005, loss 0.0284236, acc 1, learning_rate 0.0001
2017-10-10T15:12:29.881042: step 4006, loss 0.098313, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:30.286820: step 4007, loss 0.0921022, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:30.706041: step 4008, loss 0.0911667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:31.142561: step 4009, loss 0.0844773, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:31.584175: step 4010, loss 0.0860404, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:32.013305: step 4011, loss 0.107376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:32.422941: step 4012, loss 0.169985, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:32.816379: step 4013, loss 0.0578769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:33.230692: step 4014, loss 0.0909646, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:33.592188: step 4015, loss 0.179992, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:33.975562: step 4016, loss 0.0349952, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:34.404829: step 4017, loss 0.0793449, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:34.768942: step 4018, loss 0.0989506, acc 0.960784, learning_rate 0.0001
2017-10-10T15:12:35.218500: step 4019, loss 0.0394764, acc 1, learning_rate 0.0001
2017-10-10T15:12:35.605107: step 4020, loss 0.0627319, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:36.008929: step 4021, loss 0.0757825, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:36.404013: step 4022, loss 0.169635, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:36.782074: step 4023, loss 0.019914, acc 1, learning_rate 0.0001
2017-10-10T15:12:37.178283: step 4024, loss 0.0392781, acc 1, learning_rate 0.0001
2017-10-10T15:12:37.613024: step 4025, loss 0.0643398, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:38.035676: step 4026, loss 0.0749908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:38.460864: step 4027, loss 0.113322, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:38.904916: step 4028, loss 0.0451294, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:39.469760: step 4029, loss 0.0489512, acc 1, learning_rate 0.0001
2017-10-10T15:12:39.802520: step 4030, loss 0.0498749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:40.220829: step 4031, loss 0.0850325, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:40.721488: step 4032, loss 0.0805923, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:41.040839: step 4033, loss 0.0501638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:41.379316: step 4034, loss 0.113784, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:41.727591: step 4035, loss 0.076244, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:42.124467: step 4036, loss 0.0901809, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:42.552912: step 4037, loss 0.0368403, acc 1, learning_rate 0.0001
2017-10-10T15:12:42.983227: step 4038, loss 0.073051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:43.391696: step 4039, loss 0.0804736, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:43.824750: step 4040, loss 0.106391, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:44.638821: step 4040, loss 0.213826, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4040

2017-10-10T15:12:45.997105: step 4041, loss 0.055735, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:46.338379: step 4042, loss 0.0750188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:46.784893: step 4043, loss 0.134117, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:47.224134: step 4044, loss 0.0881773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:47.619439: step 4045, loss 0.120683, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:48.063317: step 4046, loss 0.201005, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:48.486634: step 4047, loss 0.137726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:48.914924: step 4048, loss 0.0966852, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:49.371728: step 4049, loss 0.0223957, acc 1, learning_rate 0.0001
2017-10-10T15:12:49.785596: step 4050, loss 0.129182, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:50.226104: step 4051, loss 0.0460249, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:50.610532: step 4052, loss 0.046254, acc 1, learning_rate 0.0001
2017-10-10T15:12:50.975898: step 4053, loss 0.0786893, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:51.401478: step 4054, loss 0.0268914, acc 1, learning_rate 0.0001
2017-10-10T15:12:51.800535: step 4055, loss 0.0775748, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:52.170772: step 4056, loss 0.063077, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:52.533513: step 4057, loss 0.0417917, acc 1, learning_rate 0.0001
2017-10-10T15:12:52.972847: step 4058, loss 0.0528083, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:53.448959: step 4059, loss 0.12372, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:53.868826: step 4060, loss 0.0703532, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:54.257704: step 4061, loss 0.115941, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:54.644987: step 4062, loss 0.0731406, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:55.056949: step 4063, loss 0.0836308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:55.426218: step 4064, loss 0.124629, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:55.975469: step 4065, loss 0.0510305, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:56.335014: step 4066, loss 0.0865089, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:56.761202: step 4067, loss 0.0979523, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:57.074025: step 4068, loss 0.0657253, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:57.436872: step 4069, loss 0.0753573, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:57.834447: step 4070, loss 0.0893695, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:58.258159: step 4071, loss 0.0959685, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:58.672952: step 4072, loss 0.0681264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:59.087893: step 4073, loss 0.118912, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:59.524821: step 4074, loss 0.0705829, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:59.968906: step 4075, loss 0.25939, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:00.424846: step 4076, loss 0.184436, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:00.851864: step 4077, loss 0.0307574, acc 1, learning_rate 0.0001
2017-10-10T15:13:01.243358: step 4078, loss 0.0852995, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:01.655034: step 4079, loss 0.142193, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:02.094210: step 4080, loss 0.0700772, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:03.006657: step 4080, loss 0.216161, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4080

2017-10-10T15:13:04.604855: step 4081, loss 0.0622027, acc 1, learning_rate 0.0001
2017-10-10T15:13:04.980867: step 4082, loss 0.0570647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:05.356946: step 4083, loss 0.0369493, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:05.746875: step 4084, loss 0.0655127, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:06.207353: step 4085, loss 0.147434, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:06.671494: step 4086, loss 0.0398931, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:07.045014: step 4087, loss 0.0654124, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:07.409074: step 4088, loss 0.0974894, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:07.839112: step 4089, loss 0.0366876, acc 1, learning_rate 0.0001
2017-10-10T15:13:08.265891: step 4090, loss 0.0503304, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:08.729704: step 4091, loss 0.137219, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:09.102764: step 4092, loss 0.0361754, acc 1, learning_rate 0.0001
2017-10-10T15:13:09.504295: step 4093, loss 0.0974842, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:09.940913: step 4094, loss 0.0367331, acc 1, learning_rate 0.0001
2017-10-10T15:13:10.337222: step 4095, loss 0.110517, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:10.700912: step 4096, loss 0.0389646, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:11.142353: step 4097, loss 0.0625662, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:11.548845: step 4098, loss 0.0932163, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:12.009557: step 4099, loss 0.0474818, acc 1, learning_rate 0.0001
2017-10-10T15:13:12.473540: step 4100, loss 0.0376419, acc 1, learning_rate 0.0001
2017-10-10T15:13:12.811849: step 4101, loss 0.106706, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:13.170264: step 4102, loss 0.0252864, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:13.556710: step 4103, loss 0.0371802, acc 1, learning_rate 0.0001
2017-10-10T15:13:13.925131: step 4104, loss 0.104403, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:14.396832: step 4105, loss 0.114794, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:14.762214: step 4106, loss 0.0941421, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:15.124770: step 4107, loss 0.0881059, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:15.586455: step 4108, loss 0.111263, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:15.996186: step 4109, loss 0.276087, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:16.385218: step 4110, loss 0.125032, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:16.775964: step 4111, loss 0.0722148, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:17.155372: step 4112, loss 0.0878049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:17.583193: step 4113, loss 0.0556621, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:17.985047: step 4114, loss 0.0419821, acc 1, learning_rate 0.0001
2017-10-10T15:13:18.465158: step 4115, loss 0.125688, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:18.772979: step 4116, loss 0.0685974, acc 0.980392, learning_rate 0.0001
2017-10-10T15:13:19.232893: step 4117, loss 0.0769171, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:19.673013: step 4118, loss 0.0220833, acc 1, learning_rate 0.0001
2017-10-10T15:13:20.145436: step 4119, loss 0.0717008, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:20.515372: step 4120, loss 0.0396966, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:21.414024: step 4120, loss 0.213284, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4120

2017-10-10T15:13:22.711181: step 4121, loss 0.157029, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:23.151085: step 4122, loss 0.0419923, acc 1, learning_rate 0.0001
2017-10-10T15:13:23.487451: step 4123, loss 0.13216, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:23.918729: step 4124, loss 0.144444, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:24.316352: step 4125, loss 0.0530389, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:24.748063: step 4126, loss 0.0814245, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:25.156333: step 4127, loss 0.104009, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:25.592983: step 4128, loss 0.140908, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:26.004877: step 4129, loss 0.0301777, acc 1, learning_rate 0.0001
2017-10-10T15:13:26.357522: step 4130, loss 0.0595411, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:26.744574: step 4131, loss 0.121058, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:27.132236: step 4132, loss 0.0473525, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:27.566296: step 4133, loss 0.16805, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:27.948139: step 4134, loss 0.0876361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:28.355497: step 4135, loss 0.0768789, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:28.849139: step 4136, loss 0.0777409, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:29.279580: step 4137, loss 0.0781255, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:29.674290: step 4138, loss 0.0832446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:30.021763: step 4139, loss 0.0277361, acc 1, learning_rate 0.0001
2017-10-10T15:13:30.419880: step 4140, loss 0.0813118, acc 1, learning_rate 0.0001
2017-10-10T15:13:30.829561: step 4141, loss 0.109557, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:31.220141: step 4142, loss 0.100491, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:31.664953: step 4143, loss 0.0354133, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:32.025670: step 4144, loss 0.0407141, acc 1, learning_rate 0.0001
2017-10-10T15:13:32.425498: step 4145, loss 0.0821872, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:32.890654: step 4146, loss 0.0528623, acc 1, learning_rate 0.0001
2017-10-10T15:13:33.321137: step 4147, loss 0.0487401, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:33.648526: step 4148, loss 0.0765189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:33.919891: step 4149, loss 0.0832116, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:34.328878: step 4150, loss 0.0625198, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:34.722574: step 4151, loss 0.0777425, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:35.085112: step 4152, loss 0.0460482, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:35.537082: step 4153, loss 0.0800809, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:36.006579: step 4154, loss 0.0924101, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:36.398450: step 4155, loss 0.0387746, acc 1, learning_rate 0.0001
2017-10-10T15:13:36.844382: step 4156, loss 0.0225848, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:37.244508: step 4157, loss 0.159646, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:37.605253: step 4158, loss 0.0842112, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:37.965997: step 4159, loss 0.079794, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:38.360788: step 4160, loss 0.0533665, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:39.215963: step 4160, loss 0.217361, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4160

2017-10-10T15:13:40.588518: step 4161, loss 0.0612355, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:40.995053: step 4162, loss 0.0764274, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:41.395148: step 4163, loss 0.0619474, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:41.825281: step 4164, loss 0.0846735, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:42.282451: step 4165, loss 0.0414907, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:42.719569: step 4166, loss 0.0634276, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:43.100984: step 4167, loss 0.0294976, acc 1, learning_rate 0.0001
2017-10-10T15:13:43.504927: step 4168, loss 0.0837218, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:43.960833: step 4169, loss 0.0829901, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:44.376503: step 4170, loss 0.056462, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:44.808867: step 4171, loss 0.0826999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:45.244864: step 4172, loss 0.0720362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:45.684436: step 4173, loss 0.0509667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:46.028829: step 4174, loss 0.0725868, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:46.428016: step 4175, loss 0.0511616, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:46.655705: step 4176, loss 0.167021, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:46.963837: step 4177, loss 0.0381696, acc 1, learning_rate 0.0001
2017-10-10T15:13:47.345203: step 4178, loss 0.101915, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:47.788780: step 4179, loss 0.149925, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:48.180872: step 4180, loss 0.126328, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:48.576127: step 4181, loss 0.0501581, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:49.012949: step 4182, loss 0.0852956, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:49.381002: step 4183, loss 0.064658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:49.753485: step 4184, loss 0.104708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:50.187689: step 4185, loss 0.0876884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:50.582546: step 4186, loss 0.0622796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:51.046035: step 4187, loss 0.110822, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:51.476921: step 4188, loss 0.110577, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:51.912815: step 4189, loss 0.0814283, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:52.325035: step 4190, loss 0.155031, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:52.737042: step 4191, loss 0.0779895, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:53.151089: step 4192, loss 0.180045, acc 0.890625, learning_rate 0.0001
2017-10-10T15:13:53.626524: step 4193, loss 0.15249, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:54.050825: step 4194, loss 0.0554044, acc 1, learning_rate 0.0001
2017-10-10T15:13:54.445413: step 4195, loss 0.122867, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:54.869099: step 4196, loss 0.0598263, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:55.277512: step 4197, loss 0.11342, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:55.666596: step 4198, loss 0.0779562, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:56.072858: step 4199, loss 0.114529, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:56.547359: step 4200, loss 0.0496037, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:57.520836: step 4200, loss 0.213881, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4200

2017-10-10T15:13:58.808932: step 4201, loss 0.0303927, acc 1, learning_rate 0.0001
2017-10-10T15:13:59.229002: step 4202, loss 0.102041, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:59.669359: step 4203, loss 0.0938219, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:00.091091: step 4204, loss 0.0542643, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:00.477954: step 4205, loss 0.235805, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:00.870821: step 4206, loss 0.0361186, acc 1, learning_rate 0.0001
2017-10-10T15:14:01.224402: step 4207, loss 0.130229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:01.680852: step 4208, loss 0.0572176, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:02.146422: step 4209, loss 0.0579589, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:02.533734: step 4210, loss 0.0632035, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:02.857233: step 4211, loss 0.0401153, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:03.220878: step 4212, loss 0.0393077, acc 1, learning_rate 0.0001
2017-10-10T15:14:03.660814: step 4213, loss 0.033251, acc 1, learning_rate 0.0001
2017-10-10T15:14:04.027781: step 4214, loss 0.172362, acc 0.921569, learning_rate 0.0001
2017-10-10T15:14:04.449255: step 4215, loss 0.054157, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:04.896962: step 4216, loss 0.0610145, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:05.254390: step 4217, loss 0.0794101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:05.692231: step 4218, loss 0.123615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:06.086775: step 4219, loss 0.195588, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:06.561160: step 4220, loss 0.0540054, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:06.989214: step 4221, loss 0.0542828, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:07.358138: step 4222, loss 0.0782796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:07.812237: step 4223, loss 0.0504597, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.238882: step 4224, loss 0.136401, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:08.667404: step 4225, loss 0.0994979, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:09.026245: step 4226, loss 0.120922, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:09.460872: step 4227, loss 0.114679, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:09.872930: step 4228, loss 0.144041, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:10.296085: step 4229, loss 0.0537781, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:10.711671: step 4230, loss 0.0536028, acc 1, learning_rate 0.0001
2017-10-10T15:14:11.057120: step 4231, loss 0.0404134, acc 1, learning_rate 0.0001
2017-10-10T15:14:11.406986: step 4232, loss 0.094012, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:11.794387: step 4233, loss 0.111087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:12.245014: step 4234, loss 0.0532867, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:12.680348: step 4235, loss 0.0406182, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:13.195832: step 4236, loss 0.0417887, acc 1, learning_rate 0.0001
2017-10-10T15:14:13.642606: step 4237, loss 0.0627404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:14.072897: step 4238, loss 0.127461, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:14.515047: step 4239, loss 0.0689503, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:14.888307: step 4240, loss 0.0794749, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:15.761701: step 4240, loss 0.212552, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4240

2017-10-10T15:14:17.099087: step 4241, loss 0.0895583, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:17.497316: step 4242, loss 0.0615357, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:17.888265: step 4243, loss 0.119587, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:18.267967: step 4244, loss 0.0564245, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:18.712907: step 4245, loss 0.0435702, acc 1, learning_rate 0.0001
2017-10-10T15:14:19.140493: step 4246, loss 0.148913, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:19.769404: step 4247, loss 0.0693057, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:20.185012: step 4248, loss 0.0340317, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:20.632582: step 4249, loss 0.106778, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:21.038685: step 4250, loss 0.067781, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:21.452867: step 4251, loss 0.165119, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:21.925270: step 4252, loss 0.0512393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:22.365287: step 4253, loss 0.0465477, acc 1, learning_rate 0.0001
2017-10-10T15:14:22.670809: step 4254, loss 0.1441, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:23.018464: step 4255, loss 0.0326646, acc 1, learning_rate 0.0001
2017-10-10T15:14:23.443590: step 4256, loss 0.157375, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:23.849601: step 4257, loss 0.0350715, acc 1, learning_rate 0.0001
2017-10-10T15:14:24.273455: step 4258, loss 0.0572343, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:24.726880: step 4259, loss 0.0800561, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:25.133357: step 4260, loss 0.0850299, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:25.570704: step 4261, loss 0.0551333, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:26.037837: step 4262, loss 0.0743827, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:26.461005: step 4263, loss 0.0540379, acc 1, learning_rate 0.0001
2017-10-10T15:14:26.849475: step 4264, loss 0.106497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:27.297067: step 4265, loss 0.0230589, acc 1, learning_rate 0.0001
2017-10-10T15:14:27.691462: step 4266, loss 0.0503581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:28.042416: step 4267, loss 0.079213, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:28.495509: step 4268, loss 0.0518929, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:28.933021: step 4269, loss 0.0688637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:29.381076: step 4270, loss 0.0558671, acc 1, learning_rate 0.0001
2017-10-10T15:14:29.857622: step 4271, loss 0.0749098, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:30.247144: step 4272, loss 0.0539527, acc 1, learning_rate 0.0001
2017-10-10T15:14:30.641160: step 4273, loss 0.14579, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:31.019224: step 4274, loss 0.0905599, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:31.399547: step 4275, loss 0.0535027, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:31.809068: step 4276, loss 0.0445528, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:32.185256: step 4277, loss 0.121212, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:32.600422: step 4278, loss 0.107924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:33.045325: step 4279, loss 0.0629146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:33.465207: step 4280, loss 0.110745, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:34.270933: step 4280, loss 0.215287, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4280

2017-10-10T15:14:35.749127: step 4281, loss 0.0712656, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:36.092923: step 4282, loss 0.0660786, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:36.402457: step 4283, loss 0.0777791, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:36.698334: step 4284, loss 0.0636561, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:37.206908: step 4285, loss 0.0820895, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:37.663758: step 4286, loss 0.0607342, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:38.070021: step 4287, loss 0.11289, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:38.475910: step 4288, loss 0.0513241, acc 1, learning_rate 0.0001
2017-10-10T15:14:38.936911: step 4289, loss 0.0529939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:39.333377: step 4290, loss 0.0423622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:39.777278: step 4291, loss 0.071591, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:40.212915: step 4292, loss 0.0690055, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:40.568395: step 4293, loss 0.0386526, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:41.016909: step 4294, loss 0.0608609, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.435659: step 4295, loss 0.14246, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.813176: step 4296, loss 0.0161895, acc 1, learning_rate 0.0001
2017-10-10T15:14:42.230262: step 4297, loss 0.0340649, acc 1, learning_rate 0.0001
2017-10-10T15:14:42.661027: step 4298, loss 0.0381204, acc 1, learning_rate 0.0001
2017-10-10T15:14:43.081098: step 4299, loss 0.0856213, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:43.504998: step 4300, loss 0.147447, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:43.877445: step 4301, loss 0.0732827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:44.372584: step 4302, loss 0.0651272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:44.708851: step 4303, loss 0.0127709, acc 1, learning_rate 0.0001
2017-10-10T15:14:45.025027: step 4304, loss 0.186342, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:45.448823: step 4305, loss 0.0142673, acc 1, learning_rate 0.0001
2017-10-10T15:14:45.840695: step 4306, loss 0.0354907, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:46.285946: step 4307, loss 0.0748329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:46.744495: step 4308, loss 0.079024, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:47.109083: step 4309, loss 0.101356, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:47.470512: step 4310, loss 0.0585171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:47.853126: step 4311, loss 0.099309, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:48.176814: step 4312, loss 0.149298, acc 0.941176, learning_rate 0.0001
2017-10-10T15:14:48.655635: step 4313, loss 0.077307, acc 1, learning_rate 0.0001
2017-10-10T15:14:49.015577: step 4314, loss 0.100213, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:49.443889: step 4315, loss 0.0843552, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:49.850171: step 4316, loss 0.0511602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:50.207029: step 4317, loss 0.0780153, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:50.606243: step 4318, loss 0.120722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:51.043349: step 4319, loss 0.103129, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:51.447494: step 4320, loss 0.0687549, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:52.296193: step 4320, loss 0.21611, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4320

2017-10-10T15:14:53.555181: step 4321, loss 0.112417, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:54.025109: step 4322, loss 0.0967323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:54.392917: step 4323, loss 0.0837208, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:54.805144: step 4324, loss 0.0808725, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:55.217167: step 4325, loss 0.120561, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:55.720187: step 4326, loss 0.117777, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:56.101420: step 4327, loss 0.0382246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:56.472899: step 4328, loss 0.0939476, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:56.899564: step 4329, loss 0.0796456, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:57.413117: step 4330, loss 0.136195, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:57.840914: step 4331, loss 0.0428385, acc 1, learning_rate 0.0001
2017-10-10T15:14:58.225871: step 4332, loss 0.0766461, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:58.669132: step 4333, loss 0.111107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:59.051050: step 4334, loss 0.0647942, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:59.442251: step 4335, loss 0.132514, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:59.815227: step 4336, loss 0.144906, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:00.262114: step 4337, loss 0.046158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:00.685182: step 4338, loss 0.162524, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:01.103097: step 4339, loss 0.0381743, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:01.532258: step 4340, loss 0.0363123, acc 1, learning_rate 0.0001
2017-10-10T15:15:01.956123: step 4341, loss 0.079735, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:02.332533: step 4342, loss 0.0501435, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:02.755780: step 4343, loss 0.0378271, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:03.214431: step 4344, loss 0.161196, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:03.601456: step 4345, loss 0.0182242, acc 1, learning_rate 0.0001
2017-10-10T15:15:04.009132: step 4346, loss 0.0593576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:04.415062: step 4347, loss 0.0661876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:04.815198: step 4348, loss 0.151466, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:05.172804: step 4349, loss 0.111892, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:05.586197: step 4350, loss 0.116562, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:06.044241: step 4351, loss 0.0349829, acc 1, learning_rate 0.0001
2017-10-10T15:15:06.509045: step 4352, loss 0.0438554, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:06.883617: step 4353, loss 0.0740989, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:07.220744: step 4354, loss 0.0598268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:07.562493: step 4355, loss 0.0891435, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:07.926560: step 4356, loss 0.123227, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:08.319483: step 4357, loss 0.0875, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:08.757169: step 4358, loss 0.0478106, acc 1, learning_rate 0.0001
2017-10-10T15:15:09.201492: step 4359, loss 0.117077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:09.488831: step 4360, loss 0.0429688, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:10.270237: step 4360, loss 0.214375, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4360

2017-10-10T15:15:11.471057: step 4361, loss 0.0355, acc 1, learning_rate 0.0001
2017-10-10T15:15:11.884030: step 4362, loss 0.0711459, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:12.223464: step 4363, loss 0.157067, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:12.607599: step 4364, loss 0.0425647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:13.020729: step 4365, loss 0.0951454, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:13.479763: step 4366, loss 0.090699, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:13.852035: step 4367, loss 0.0925005, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:14.309122: step 4368, loss 0.114106, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:14.737553: step 4369, loss 0.0224811, acc 1, learning_rate 0.0001
2017-10-10T15:15:15.111860: step 4370, loss 0.13349, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:15.520243: step 4371, loss 0.0955359, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:15.904983: step 4372, loss 0.0685119, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:16.349542: step 4373, loss 0.0854189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:16.773342: step 4374, loss 0.0414776, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:17.188829: step 4375, loss 0.0709334, acc 1, learning_rate 0.0001
2017-10-10T15:15:17.585000: step 4376, loss 0.0488177, acc 1, learning_rate 0.0001
2017-10-10T15:15:18.037251: step 4377, loss 0.0638209, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:18.439303: step 4378, loss 0.0359346, acc 1, learning_rate 0.0001
2017-10-10T15:15:18.808915: step 4379, loss 0.105382, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:19.171188: step 4380, loss 0.0699843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:19.690323: step 4381, loss 0.064801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:20.005870: step 4382, loss 0.151817, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:20.323526: step 4383, loss 0.0685564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:20.685030: step 4384, loss 0.0355149, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:21.119097: step 4385, loss 0.119613, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:21.513719: step 4386, loss 0.0426233, acc 1, learning_rate 0.0001
2017-10-10T15:15:21.916426: step 4387, loss 0.142271, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:22.328816: step 4388, loss 0.0600635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:22.716877: step 4389, loss 0.0274093, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:23.097177: step 4390, loss 0.0437469, acc 1, learning_rate 0.0001
2017-10-10T15:15:23.516863: step 4391, loss 0.144771, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:24.007090: step 4392, loss 0.0718187, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:24.365486: step 4393, loss 0.178834, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:24.707389: step 4394, loss 0.0737415, acc 1, learning_rate 0.0001
2017-10-10T15:15:25.134459: step 4395, loss 0.0650177, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:25.609036: step 4396, loss 0.106439, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:26.015373: step 4397, loss 0.0386558, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:26.356931: step 4398, loss 0.170127, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:26.720431: step 4399, loss 0.110782, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:27.112850: step 4400, loss 0.0959612, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:27.960117: step 4400, loss 0.21464, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4400

2017-10-10T15:15:29.343061: step 4401, loss 0.0390039, acc 1, learning_rate 0.0001
2017-10-10T15:15:29.625149: step 4402, loss 0.0415747, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:30.003628: step 4403, loss 0.0659269, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:30.393009: step 4404, loss 0.0396405, acc 1, learning_rate 0.0001
2017-10-10T15:15:30.832855: step 4405, loss 0.0390893, acc 1, learning_rate 0.0001
2017-10-10T15:15:31.239424: step 4406, loss 0.0433865, acc 1, learning_rate 0.0001
2017-10-10T15:15:31.707489: step 4407, loss 0.0719164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:32.149971: step 4408, loss 0.0861828, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:32.619157: step 4409, loss 0.0347965, acc 1, learning_rate 0.0001
2017-10-10T15:15:32.945193: step 4410, loss 0.0832326, acc 1, learning_rate 0.0001
2017-10-10T15:15:33.303777: step 4411, loss 0.106279, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:33.676812: step 4412, loss 0.0626763, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:34.101252: step 4413, loss 0.0737515, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:34.530021: step 4414, loss 0.0900401, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:34.956893: step 4415, loss 0.0600072, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:35.372312: step 4416, loss 0.0714048, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:35.814094: step 4417, loss 0.0253831, acc 1, learning_rate 0.0001
2017-10-10T15:15:36.303081: step 4418, loss 0.10069, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:36.716852: step 4419, loss 0.0688613, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:37.144813: step 4420, loss 0.0380409, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:37.523847: step 4421, loss 0.0582659, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:37.983992: step 4422, loss 0.0443867, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:38.411631: step 4423, loss 0.128247, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:38.829440: step 4424, loss 0.104094, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:39.198302: step 4425, loss 0.0304413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:39.633021: step 4426, loss 0.0267568, acc 1, learning_rate 0.0001
2017-10-10T15:15:40.093061: step 4427, loss 0.0630281, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:40.516823: step 4428, loss 0.0619725, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:40.925006: step 4429, loss 0.119992, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:41.317815: step 4430, loss 0.0928738, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:41.753094: step 4431, loss 0.072804, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:42.226223: step 4432, loss 0.0765892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:42.616197: step 4433, loss 0.0558744, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:42.981954: step 4434, loss 0.0385736, acc 1, learning_rate 0.0001
2017-10-10T15:15:43.312886: step 4435, loss 0.0489878, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:43.789097: step 4436, loss 0.165228, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:44.184367: step 4437, loss 0.0472855, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:44.610367: step 4438, loss 0.0334292, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:45.029335: step 4439, loss 0.085004, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:45.372934: step 4440, loss 0.0645505, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:46.304485: step 4440, loss 0.217376, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4440

2017-10-10T15:15:47.700460: step 4441, loss 0.0621069, acc 1, learning_rate 0.0001
2017-10-10T15:15:48.191133: step 4442, loss 0.0714861, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:48.610166: step 4443, loss 0.0594587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:48.975265: step 4444, loss 0.0924444, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:49.379611: step 4445, loss 0.0838136, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:49.871802: step 4446, loss 0.0960822, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:50.246962: step 4447, loss 0.0944797, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:50.654581: step 4448, loss 0.0839895, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:51.082225: step 4449, loss 0.0751384, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:51.503022: step 4450, loss 0.0345513, acc 1, learning_rate 0.0001
2017-10-10T15:15:51.888814: step 4451, loss 0.0246013, acc 1, learning_rate 0.0001
2017-10-10T15:15:52.362167: step 4452, loss 0.193381, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:52.775147: step 4453, loss 0.0659263, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:53.141147: step 4454, loss 0.136389, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:53.584361: step 4455, loss 0.119564, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:53.921024: step 4456, loss 0.0707581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:54.360840: step 4457, loss 0.122502, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:54.764819: step 4458, loss 0.0502881, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:55.224841: step 4459, loss 0.0695599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:55.633884: step 4460, loss 0.02537, acc 1, learning_rate 0.0001
2017-10-10T15:15:56.014151: step 4461, loss 0.129074, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:56.367681: step 4462, loss 0.0694091, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:56.774964: step 4463, loss 0.0468092, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:57.176207: step 4464, loss 0.0750445, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:57.612879: step 4465, loss 0.0534736, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:58.070611: step 4466, loss 0.0392803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:58.499897: step 4467, loss 0.0314225, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:58.976724: step 4468, loss 0.0748072, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:59.355035: step 4469, loss 0.0838867, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:59.654601: step 4470, loss 0.0801996, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:59.956978: step 4471, loss 0.0468303, acc 1, learning_rate 0.0001
2017-10-10T15:16:00.442884: step 4472, loss 0.106984, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:00.781587: step 4473, loss 0.0518569, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:01.115796: step 4474, loss 0.100069, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:01.550132: step 4475, loss 0.0324268, acc 1, learning_rate 0.0001
2017-10-10T15:16:01.987834: step 4476, loss 0.0940906, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:02.491901: step 4477, loss 0.0660743, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:02.906966: step 4478, loss 0.109788, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:03.257919: step 4479, loss 0.0583562, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:03.658443: step 4480, loss 0.0656464, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:04.504005: step 4480, loss 0.213917, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4480

2017-10-10T15:16:05.877028: step 4481, loss 0.129413, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:06.280894: step 4482, loss 0.0550778, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:06.747157: step 4483, loss 0.068248, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:07.220850: step 4484, loss 0.0553527, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:07.624854: step 4485, loss 0.0763385, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:07.985100: step 4486, loss 0.0781887, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:08.440980: step 4487, loss 0.15455, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:08.908843: step 4488, loss 0.107499, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:09.297717: step 4489, loss 0.0978985, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:09.718155: step 4490, loss 0.0478711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:10.115957: step 4491, loss 0.106192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:10.567855: step 4492, loss 0.0540459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:11.014853: step 4493, loss 0.139249, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:11.402817: step 4494, loss 0.127013, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:11.791027: step 4495, loss 0.179443, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:12.236893: step 4496, loss 0.0457477, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:12.716585: step 4497, loss 0.0727229, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:13.118021: step 4498, loss 0.0437842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:13.538335: step 4499, loss 0.0986893, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:13.931973: step 4500, loss 0.0442871, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:14.341318: step 4501, loss 0.0717205, acc 1, learning_rate 0.0001
2017-10-10T15:16:14.772949: step 4502, loss 0.089693, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:15.222594: step 4503, loss 0.147427, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:15.665902: step 4504, loss 0.049555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:16.032804: step 4505, loss 0.0526501, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:16.390479: step 4506, loss 0.0983552, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:16.760959: step 4507, loss 0.120081, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:17.093075: step 4508, loss 0.058885, acc 0.960784, learning_rate 0.0001
2017-10-10T15:16:17.551357: step 4509, loss 0.0865421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:18.002244: step 4510, loss 0.0580694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:18.414420: step 4511, loss 0.0435871, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:18.796820: step 4512, loss 0.0839303, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:19.224170: step 4513, loss 0.132761, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:19.668861: step 4514, loss 0.0490059, acc 1, learning_rate 0.0001
2017-10-10T15:16:20.111349: step 4515, loss 0.0385502, acc 1, learning_rate 0.0001
2017-10-10T15:16:20.518872: step 4516, loss 0.0855602, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:20.911285: step 4517, loss 0.164653, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:21.307627: step 4518, loss 0.0785568, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:21.733922: step 4519, loss 0.107938, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:22.216918: step 4520, loss 0.136192, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:23.094545: step 4520, loss 0.214342, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4520

2017-10-10T15:16:24.456242: step 4521, loss 0.139365, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:24.897128: step 4522, loss 0.105619, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:25.288159: step 4523, loss 0.120187, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:25.756901: step 4524, loss 0.0230573, acc 1, learning_rate 0.0001
2017-10-10T15:16:26.149911: step 4525, loss 0.0578337, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:26.568929: step 4526, loss 0.0657189, acc 1, learning_rate 0.0001
2017-10-10T15:16:27.009113: step 4527, loss 0.0789581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:27.477993: step 4528, loss 0.0498125, acc 1, learning_rate 0.0001
2017-10-10T15:16:27.882865: step 4529, loss 0.111113, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:28.271610: step 4530, loss 0.0584927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:28.716102: step 4531, loss 0.0553505, acc 1, learning_rate 0.0001
2017-10-10T15:16:29.168834: step 4532, loss 0.0456689, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:29.569910: step 4533, loss 0.19046, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:29.966775: step 4534, loss 0.0609467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:30.373525: step 4535, loss 0.0907908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:30.882443: step 4536, loss 0.165608, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:31.285008: step 4537, loss 0.019473, acc 1, learning_rate 0.0001
2017-10-10T15:16:31.691252: step 4538, loss 0.0672164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:32.200831: step 4539, loss 0.0332383, acc 1, learning_rate 0.0001
2017-10-10T15:16:32.599652: step 4540, loss 0.0541763, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:32.976791: step 4541, loss 0.0592095, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:33.318032: step 4542, loss 0.0744143, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:33.721058: step 4543, loss 0.0626689, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:34.167782: step 4544, loss 0.0455124, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:34.568433: step 4545, loss 0.0451681, acc 1, learning_rate 0.0001
2017-10-10T15:16:34.914932: step 4546, loss 0.0410678, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:35.349330: step 4547, loss 0.0755013, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:35.753011: step 4548, loss 0.0521322, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:36.236220: step 4549, loss 0.0493272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:36.665049: step 4550, loss 0.0369344, acc 1, learning_rate 0.0001
2017-10-10T15:16:37.137823: step 4551, loss 0.112828, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:37.502609: step 4552, loss 0.131159, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:37.853248: step 4553, loss 0.122054, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:38.288223: step 4554, loss 0.0419592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:38.759328: step 4555, loss 0.0779363, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:39.181779: step 4556, loss 0.0983543, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:39.568165: step 4557, loss 0.143068, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:39.927238: step 4558, loss 0.144158, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:40.317249: step 4559, loss 0.124756, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:40.710846: step 4560, loss 0.0992723, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:41.611590: step 4560, loss 0.214619, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4560

2017-10-10T15:16:43.680846: step 4561, loss 0.0543221, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:44.087687: step 4562, loss 0.0781197, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:44.481010: step 4563, loss 0.217772, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:44.857578: step 4564, loss 0.0846105, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:45.282725: step 4565, loss 0.0771945, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:45.687725: step 4566, loss 0.0912844, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:46.136836: step 4567, loss 0.0454569, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:46.547467: step 4568, loss 0.0317394, acc 1, learning_rate 0.0001
2017-10-10T15:16:46.999473: step 4569, loss 0.0915039, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:47.376369: step 4570, loss 0.0417098, acc 1, learning_rate 0.0001
2017-10-10T15:16:47.762290: step 4571, loss 0.0745318, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:48.183333: step 4572, loss 0.0560317, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:48.679213: step 4573, loss 0.0846819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:49.119353: step 4574, loss 0.0933094, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:49.423218: step 4575, loss 0.0667227, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:49.753788: step 4576, loss 0.101981, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:50.143372: step 4577, loss 0.113661, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:50.620748: step 4578, loss 0.0725171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:51.128810: step 4579, loss 0.124415, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:51.534953: step 4580, loss 0.0209325, acc 1, learning_rate 0.0001
2017-10-10T15:16:51.965815: step 4581, loss 0.0342086, acc 1, learning_rate 0.0001
2017-10-10T15:16:52.360381: step 4582, loss 0.0716083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:52.828725: step 4583, loss 0.0998761, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:53.193146: step 4584, loss 0.115322, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:53.591226: step 4585, loss 0.0970963, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:54.000408: step 4586, loss 0.0603495, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:54.476175: step 4587, loss 0.0781637, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:54.846126: step 4588, loss 0.132992, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:55.264127: step 4589, loss 0.042301, acc 1, learning_rate 0.0001
2017-10-10T15:16:55.636904: step 4590, loss 0.0417041, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:56.058657: step 4591, loss 0.0963292, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:56.505474: step 4592, loss 0.0517348, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:56.905122: step 4593, loss 0.11754, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:57.318864: step 4594, loss 0.0371444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:57.736477: step 4595, loss 0.111435, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:58.142052: step 4596, loss 0.10104, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:58.524881: step 4597, loss 0.0645624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:58.845804: step 4598, loss 0.0913787, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:59.261028: step 4599, loss 0.109319, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:59.645485: step 4600, loss 0.0724295, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:00.600931: step 4600, loss 0.213307, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4600

2017-10-10T15:17:01.950762: step 4601, loss 0.102235, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:02.384059: step 4602, loss 0.040507, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:02.846551: step 4603, loss 0.0743405, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:03.298795: step 4604, loss 0.0803662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:03.724170: step 4605, loss 0.0555302, acc 1, learning_rate 0.0001
2017-10-10T15:17:04.092865: step 4606, loss 0.0391808, acc 1, learning_rate 0.0001
2017-10-10T15:17:04.497916: step 4607, loss 0.0526393, acc 1, learning_rate 0.0001
2017-10-10T15:17:04.902462: step 4608, loss 0.031805, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:05.309020: step 4609, loss 0.111732, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:05.765148: step 4610, loss 0.0900299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:06.202935: step 4611, loss 0.0784365, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:06.492213: step 4612, loss 0.088327, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:06.800603: step 4613, loss 0.0150425, acc 1, learning_rate 0.0001
2017-10-10T15:17:07.213187: step 4614, loss 0.0788603, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:07.593020: step 4615, loss 0.063588, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:08.037183: step 4616, loss 0.120267, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:08.496509: step 4617, loss 0.0456876, acc 1, learning_rate 0.0001
2017-10-10T15:17:08.970155: step 4618, loss 0.13416, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:09.308917: step 4619, loss 0.0553572, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:09.639050: step 4620, loss 0.161605, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:10.044884: step 4621, loss 0.10172, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:10.420839: step 4622, loss 0.0613697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:10.826466: step 4623, loss 0.0582898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:11.272919: step 4624, loss 0.078557, acc 1, learning_rate 0.0001
2017-10-10T15:17:11.669098: step 4625, loss 0.0651809, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:12.126825: step 4626, loss 0.161347, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:12.585501: step 4627, loss 0.0477729, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:13.000971: step 4628, loss 0.0245759, acc 1, learning_rate 0.0001
2017-10-10T15:17:13.402546: step 4629, loss 0.0589548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:13.780451: step 4630, loss 0.0627959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:14.192979: step 4631, loss 0.0976114, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:14.564832: step 4632, loss 0.0534164, acc 1, learning_rate 0.0001
2017-10-10T15:17:14.960939: step 4633, loss 0.0934397, acc 1, learning_rate 0.0001
2017-10-10T15:17:15.408983: step 4634, loss 0.104198, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:15.768369: step 4635, loss 0.0939354, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:16.272828: step 4636, loss 0.0813499, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:16.702548: step 4637, loss 0.0701402, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:17.102604: step 4638, loss 0.0904005, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:17.541191: step 4639, loss 0.085605, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:17.953678: step 4640, loss 0.0475097, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:18.785529: step 4640, loss 0.210125, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4640

2017-10-10T15:17:20.204930: step 4641, loss 0.131249, acc 0.90625, learning_rate 0.0001
2017-10-10T15:17:20.592832: step 4642, loss 0.040381, acc 1, learning_rate 0.0001
2017-10-10T15:17:20.953421: step 4643, loss 0.0729729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:21.389375: step 4644, loss 0.0153373, acc 1, learning_rate 0.0001
2017-10-10T15:17:21.784961: step 4645, loss 0.0848842, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:22.327504: step 4646, loss 0.085101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:22.692507: step 4647, loss 0.0634436, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:23.084574: step 4648, loss 0.0593023, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:23.442910: step 4649, loss 0.0263194, acc 1, learning_rate 0.0001
2017-10-10T15:17:23.834756: step 4650, loss 0.0480269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:24.244189: step 4651, loss 0.161015, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:24.714933: step 4652, loss 0.0327016, acc 1, learning_rate 0.0001
2017-10-10T15:17:25.103509: step 4653, loss 0.0759254, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:25.608054: step 4654, loss 0.0642707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:25.968709: step 4655, loss 0.0475418, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:26.360102: step 4656, loss 0.0543465, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:26.720371: step 4657, loss 0.135896, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:27.105383: step 4658, loss 0.0707935, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:27.524449: step 4659, loss 0.119217, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:27.944285: step 4660, loss 0.030605, acc 1, learning_rate 0.0001
2017-10-10T15:17:28.387010: step 4661, loss 0.136065, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:28.824215: step 4662, loss 0.0647154, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:29.229253: step 4663, loss 0.100508, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:29.652933: step 4664, loss 0.0794974, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:30.091070: step 4665, loss 0.0405121, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:30.432806: step 4666, loss 0.0727403, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:30.897001: step 4667, loss 0.161668, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:31.300070: step 4668, loss 0.177285, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:31.833009: step 4669, loss 0.102083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:32.175986: step 4670, loss 0.0504332, acc 1, learning_rate 0.0001
2017-10-10T15:17:32.657349: step 4671, loss 0.0847597, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:33.082474: step 4672, loss 0.0835828, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:33.427873: step 4673, loss 0.0500725, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:33.792788: step 4674, loss 0.046096, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:34.176409: step 4675, loss 0.087953, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:34.585131: step 4676, loss 0.0426606, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:35.025186: step 4677, loss 0.0347999, acc 1, learning_rate 0.0001
2017-10-10T15:17:35.449892: step 4678, loss 0.157152, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:35.850722: step 4679, loss 0.0831065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:36.267038: step 4680, loss 0.0706592, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:37.165491: step 4680, loss 0.215354, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4680

2017-10-10T15:17:38.388845: step 4681, loss 0.0493024, acc 1, learning_rate 0.0001
2017-10-10T15:17:38.709126: step 4682, loss 0.0488541, acc 1, learning_rate 0.0001
2017-10-10T15:17:39.156826: step 4683, loss 0.0306438, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:39.523981: step 4684, loss 0.0547091, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:39.884598: step 4685, loss 0.0437316, acc 1, learning_rate 0.0001
2017-10-10T15:17:40.292605: step 4686, loss 0.0505841, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:40.711947: step 4687, loss 0.105307, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:41.048312: step 4688, loss 0.0307116, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:41.453064: step 4689, loss 0.10822, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:41.849412: step 4690, loss 0.103429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:42.267041: step 4691, loss 0.0466264, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:42.739432: step 4692, loss 0.0918152, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:43.189308: step 4693, loss 0.0802645, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:43.605324: step 4694, loss 0.0634272, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:43.965245: step 4695, loss 0.106483, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:44.376917: step 4696, loss 0.0882579, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:44.793130: step 4697, loss 0.11975, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:45.209609: step 4698, loss 0.111761, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:45.615630: step 4699, loss 0.0999003, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:46.129147: step 4700, loss 0.0842646, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:46.524989: step 4701, loss 0.0266052, acc 1, learning_rate 0.0001
2017-10-10T15:17:46.891777: step 4702, loss 0.0629407, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:47.322817: step 4703, loss 0.0250385, acc 1, learning_rate 0.0001
2017-10-10T15:17:47.663452: step 4704, loss 0.0920788, acc 0.960784, learning_rate 0.0001
2017-10-10T15:17:48.120833: step 4705, loss 0.0984953, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:48.499454: step 4706, loss 0.0468279, acc 1, learning_rate 0.0001
2017-10-10T15:17:48.860830: step 4707, loss 0.0337878, acc 1, learning_rate 0.0001
2017-10-10T15:17:49.279942: step 4708, loss 0.0279576, acc 1, learning_rate 0.0001
2017-10-10T15:17:49.646773: step 4709, loss 0.100374, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:50.136732: step 4710, loss 0.102341, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:50.508245: step 4711, loss 0.0232383, acc 1, learning_rate 0.0001
2017-10-10T15:17:50.912279: step 4712, loss 0.0602592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:51.384578: step 4713, loss 0.0469835, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:51.808810: step 4714, loss 0.117113, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:52.232574: step 4715, loss 0.12355, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:52.603751: step 4716, loss 0.0331758, acc 1, learning_rate 0.0001
2017-10-10T15:17:53.061731: step 4717, loss 0.0870584, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:53.468209: step 4718, loss 0.0758189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:53.945406: step 4719, loss 0.107136, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:54.355862: step 4720, loss 0.0780995, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:55.351113: step 4720, loss 0.215869, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4720

2017-10-10T15:17:56.730787: step 4721, loss 0.0857267, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:57.168685: step 4722, loss 0.118269, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:57.619577: step 4723, loss 0.0417914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:58.019847: step 4724, loss 0.119133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:58.415119: step 4725, loss 0.0365881, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:58.864910: step 4726, loss 0.0419868, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:59.252986: step 4727, loss 0.0871639, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:59.664986: step 4728, loss 0.0252054, acc 1, learning_rate 0.0001
2017-10-10T15:18:00.076510: step 4729, loss 0.0665322, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:00.504979: step 4730, loss 0.0620663, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:00.956878: step 4731, loss 0.0775037, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:01.321225: step 4732, loss 0.0574129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:01.700747: step 4733, loss 0.0560098, acc 1, learning_rate 0.0001
2017-10-10T15:18:02.136158: step 4734, loss 0.117201, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:02.532612: step 4735, loss 0.163311, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:02.872897: step 4736, loss 0.100468, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:03.296028: step 4737, loss 0.0536803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:03.650182: step 4738, loss 0.0927989, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:04.028909: step 4739, loss 0.103485, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:04.415025: step 4740, loss 0.0779026, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:04.904946: step 4741, loss 0.0236697, acc 1, learning_rate 0.0001
2017-10-10T15:18:05.276616: step 4742, loss 0.0684661, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:05.669080: step 4743, loss 0.118427, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:06.053266: step 4744, loss 0.160165, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:06.417830: step 4745, loss 0.0620202, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:06.809642: step 4746, loss 0.0963098, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:07.264938: step 4747, loss 0.123769, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:07.692440: step 4748, loss 0.0578639, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:08.121815: step 4749, loss 0.132684, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:08.536939: step 4750, loss 0.104254, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:08.978100: step 4751, loss 0.0583658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:09.406659: step 4752, loss 0.0743616, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:09.848865: step 4753, loss 0.0323226, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:10.264837: step 4754, loss 0.184609, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:10.668920: step 4755, loss 0.109743, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:11.057004: step 4756, loss 0.125084, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:11.540847: step 4757, loss 0.0167614, acc 1, learning_rate 0.0001
2017-10-10T15:18:11.973515: step 4758, loss 0.0526358, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:12.470889: step 4759, loss 0.0568477, acc 1, learning_rate 0.0001
2017-10-10T15:18:12.996850: step 4760, loss 0.100339, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:13.900343: step 4760, loss 0.216487, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4760

2017-10-10T15:18:15.189227: step 4761, loss 0.10269, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:15.579092: step 4762, loss 0.0708455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:15.976870: step 4763, loss 0.083797, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:16.353285: step 4764, loss 0.0387786, acc 1, learning_rate 0.0001
2017-10-10T15:18:16.720971: step 4765, loss 0.0740421, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:17.129107: step 4766, loss 0.0608262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:17.549459: step 4767, loss 0.116118, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:17.992681: step 4768, loss 0.026152, acc 1, learning_rate 0.0001
2017-10-10T15:18:18.416975: step 4769, loss 0.140629, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:18.876649: step 4770, loss 0.0779185, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:19.317101: step 4771, loss 0.105359, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:19.695236: step 4772, loss 0.0541398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:20.121200: step 4773, loss 0.0925479, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:20.536987: step 4774, loss 0.124204, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:20.980824: step 4775, loss 0.0371312, acc 1, learning_rate 0.0001
2017-10-10T15:18:21.410464: step 4776, loss 0.0450179, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:21.853004: step 4777, loss 0.0760171, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:22.259459: step 4778, loss 0.0654193, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:22.649786: step 4779, loss 0.0744346, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:23.049719: step 4780, loss 0.0547181, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:23.448582: step 4781, loss 0.061969, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:23.876144: step 4782, loss 0.133752, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:24.296662: step 4783, loss 0.0928851, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:24.728558: step 4784, loss 0.179079, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:25.161467: step 4785, loss 0.0726179, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:25.716154: step 4786, loss 0.0553701, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:26.027909: step 4787, loss 0.0286272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:26.420184: step 4788, loss 0.123362, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:26.804820: step 4789, loss 0.108891, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:27.220922: step 4790, loss 0.0455953, acc 1, learning_rate 0.0001
2017-10-10T15:18:27.549633: step 4791, loss 0.0239161, acc 1, learning_rate 0.0001
2017-10-10T15:18:27.900441: step 4792, loss 0.102815, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:28.344379: step 4793, loss 0.0901719, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:28.824888: step 4794, loss 0.0458078, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:29.268890: step 4795, loss 0.0966829, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:29.804984: step 4796, loss 0.0363188, acc 1, learning_rate 0.0001
2017-10-10T15:18:30.147504: step 4797, loss 0.10674, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:30.452879: step 4798, loss 0.0366976, acc 1, learning_rate 0.0001
2017-10-10T15:18:30.802518: step 4799, loss 0.0477667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:31.201782: step 4800, loss 0.0543565, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:32.185130: step 4800, loss 0.211269, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4800

2017-10-10T15:18:33.506742: step 4801, loss 0.113357, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:33.902695: step 4802, loss 0.107667, acc 0.960784, learning_rate 0.0001
2017-10-10T15:18:34.302779: step 4803, loss 0.0247314, acc 1, learning_rate 0.0001
2017-10-10T15:18:34.718394: step 4804, loss 0.121918, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:35.144284: step 4805, loss 0.0887365, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:35.557509: step 4806, loss 0.0376357, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:35.944843: step 4807, loss 0.0825587, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:36.429036: step 4808, loss 0.0489443, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:36.788466: step 4809, loss 0.197263, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:37.212832: step 4810, loss 0.0562312, acc 1, learning_rate 0.0001
2017-10-10T15:18:37.600326: step 4811, loss 0.0360354, acc 1, learning_rate 0.0001
2017-10-10T15:18:38.053774: step 4812, loss 0.096351, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:38.428339: step 4813, loss 0.087617, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:38.860207: step 4814, loss 0.0482904, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:39.270855: step 4815, loss 0.122843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:39.596856: step 4816, loss 0.110528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:39.968423: step 4817, loss 0.112116, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:40.352060: step 4818, loss 0.0792879, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:41.035543: step 4819, loss 0.0986185, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:41.395564: step 4820, loss 0.126259, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:41.730824: step 4821, loss 0.0546984, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:42.128840: step 4822, loss 0.108374, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:42.573112: step 4823, loss 0.0837085, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:43.024887: step 4824, loss 0.0643908, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:43.482420: step 4825, loss 0.0574127, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:43.861065: step 4826, loss 0.0613587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:44.271325: step 4827, loss 0.105191, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:44.660955: step 4828, loss 0.0880754, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:45.078288: step 4829, loss 0.0212681, acc 1, learning_rate 0.0001
2017-10-10T15:18:45.448099: step 4830, loss 0.0503219, acc 1, learning_rate 0.0001
2017-10-10T15:18:46.304847: step 4831, loss 0.175303, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:46.805284: step 4832, loss 0.106283, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:47.192929: step 4833, loss 0.0610948, acc 1, learning_rate 0.0001
2017-10-10T15:18:47.552923: step 4834, loss 0.065933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:47.884435: step 4835, loss 0.0981256, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:48.307012: step 4836, loss 0.129244, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:48.697904: step 4837, loss 0.0530139, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:49.106179: step 4838, loss 0.0440311, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:49.504745: step 4839, loss 0.0829418, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:49.931378: step 4840, loss 0.0652538, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:50.879761: step 4840, loss 0.213424, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4840

2017-10-10T15:18:52.301071: step 4841, loss 0.132345, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:52.775072: step 4842, loss 0.0340532, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:53.062449: step 4843, loss 0.0686121, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:53.456886: step 4844, loss 0.0224032, acc 1, learning_rate 0.0001
2017-10-10T15:18:53.831370: step 4845, loss 0.0984754, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:54.264879: step 4846, loss 0.145088, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:54.748887: step 4847, loss 0.0904161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:55.168904: step 4848, loss 0.0716194, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:55.608014: step 4849, loss 0.0767452, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:56.061072: step 4850, loss 0.186873, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:56.584267: step 4851, loss 0.0407822, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:56.949217: step 4852, loss 0.0597752, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:57.334932: step 4853, loss 0.0836699, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:57.766786: step 4854, loss 0.0594056, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:58.176244: step 4855, loss 0.0336255, acc 1, learning_rate 0.0001
2017-10-10T15:18:58.580982: step 4856, loss 0.0536697, acc 1, learning_rate 0.0001
2017-10-10T15:18:59.069023: step 4857, loss 0.0618053, acc 1, learning_rate 0.0001
2017-10-10T15:18:59.540936: step 4858, loss 0.0917715, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:59.940525: step 4859, loss 0.056376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:00.405393: step 4860, loss 0.0647198, acc 1, learning_rate 0.0001
2017-10-10T15:19:00.786395: step 4861, loss 0.071251, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:01.144603: step 4862, loss 0.0459926, acc 1, learning_rate 0.0001
2017-10-10T15:19:01.548051: step 4863, loss 0.015241, acc 1, learning_rate 0.0001
2017-10-10T15:19:01.984360: step 4864, loss 0.074131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:02.403108: step 4865, loss 0.10844, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:02.848867: step 4866, loss 0.109945, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:03.196722: step 4867, loss 0.0622096, acc 1, learning_rate 0.0001
2017-10-10T15:19:03.698656: step 4868, loss 0.0383594, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:04.091872: step 4869, loss 0.0478518, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:04.485355: step 4870, loss 0.0504318, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:04.925099: step 4871, loss 0.0749642, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:05.342251: step 4872, loss 0.0786306, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:05.704913: step 4873, loss 0.0457595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:06.140903: step 4874, loss 0.0940252, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:06.456823: step 4875, loss 0.13835, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:06.821233: step 4876, loss 0.0770522, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:07.256902: step 4877, loss 0.0200874, acc 1, learning_rate 0.0001
2017-10-10T15:19:07.651775: step 4878, loss 0.0242327, acc 1, learning_rate 0.0001
2017-10-10T15:19:08.085944: step 4879, loss 0.0658073, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:08.464726: step 4880, loss 0.0959535, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:09.748865: step 4880, loss 0.210474, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4880

2017-10-10T15:19:11.182167: step 4881, loss 0.028091, acc 1, learning_rate 0.0001
2017-10-10T15:19:11.632857: step 4882, loss 0.120907, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:11.961690: step 4883, loss 0.0913577, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:12.372683: step 4884, loss 0.0492863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:12.836927: step 4885, loss 0.050546, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:13.293004: step 4886, loss 0.0654144, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:13.713190: step 4887, loss 0.0837986, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:14.097900: step 4888, loss 0.0353362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:14.484975: step 4889, loss 0.128951, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:14.909005: step 4890, loss 0.0551412, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:15.345341: step 4891, loss 0.0585724, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:15.741378: step 4892, loss 0.125606, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:16.111580: step 4893, loss 0.0273182, acc 1, learning_rate 0.0001
2017-10-10T15:19:16.554507: step 4894, loss 0.0692941, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:16.966147: step 4895, loss 0.0845834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:17.425586: step 4896, loss 0.0429468, acc 1, learning_rate 0.0001
2017-10-10T15:19:17.800878: step 4897, loss 0.173709, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:18.198851: step 4898, loss 0.0526642, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:18.628930: step 4899, loss 0.109838, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:19.032849: step 4900, loss 0.0562864, acc 0.980392, learning_rate 0.0001
2017-10-10T15:19:19.480836: step 4901, loss 0.113033, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:19.848860: step 4902, loss 0.0569643, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:20.276383: step 4903, loss 0.0399382, acc 1, learning_rate 0.0001
2017-10-10T15:19:20.656869: step 4904, loss 0.0284289, acc 1, learning_rate 0.0001
2017-10-10T15:19:21.004944: step 4905, loss 0.129004, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:21.333141: step 4906, loss 0.0547072, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:21.729264: step 4907, loss 0.127049, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:22.130182: step 4908, loss 0.0371525, acc 1, learning_rate 0.0001
2017-10-10T15:19:22.543786: step 4909, loss 0.168335, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:22.950824: step 4910, loss 0.043812, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:23.344093: step 4911, loss 0.0469099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:23.723807: step 4912, loss 0.0500649, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:24.166772: step 4913, loss 0.0913573, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:24.585884: step 4914, loss 0.0492404, acc 1, learning_rate 0.0001
2017-10-10T15:19:24.995331: step 4915, loss 0.105565, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:25.448943: step 4916, loss 0.0869037, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:25.904840: step 4917, loss 0.0296137, acc 1, learning_rate 0.0001
2017-10-10T15:19:26.332808: step 4918, loss 0.0399208, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:26.821363: step 4919, loss 0.0525129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:27.282236: step 4920, loss 0.0584351, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:28.184834: step 4920, loss 0.210256, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4920

2017-10-10T15:19:29.600878: step 4921, loss 0.0146669, acc 1, learning_rate 0.0001
2017-10-10T15:19:30.071028: step 4922, loss 0.0776914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:30.480028: step 4923, loss 0.0759359, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:30.882764: step 4924, loss 0.0933777, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:31.292951: step 4925, loss 0.0427702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:31.701293: step 4926, loss 0.0466785, acc 1, learning_rate 0.0001
2017-10-10T15:19:32.136556: step 4927, loss 0.0735344, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:32.586723: step 4928, loss 0.0346268, acc 1, learning_rate 0.0001
2017-10-10T15:19:32.932845: step 4929, loss 0.133786, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:33.329168: step 4930, loss 0.0483043, acc 1, learning_rate 0.0001
2017-10-10T15:19:33.753174: step 4931, loss 0.161501, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:34.311537: step 4932, loss 0.0695615, acc 1, learning_rate 0.0001
2017-10-10T15:19:34.743907: step 4933, loss 0.0169438, acc 1, learning_rate 0.0001
2017-10-10T15:19:35.104118: step 4934, loss 0.0962137, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:35.551230: step 4935, loss 0.188596, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:35.976970: step 4936, loss 0.0255766, acc 1, learning_rate 0.0001
2017-10-10T15:19:36.477019: step 4937, loss 0.0650728, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:36.860386: step 4938, loss 0.113607, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:37.306920: step 4939, loss 0.0687959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:37.708385: step 4940, loss 0.0279951, acc 1, learning_rate 0.0001
2017-10-10T15:19:38.055419: step 4941, loss 0.0897991, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:38.409109: step 4942, loss 0.0723788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:38.791716: step 4943, loss 0.0946196, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:39.221019: step 4944, loss 0.161015, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:39.680915: step 4945, loss 0.103983, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:40.066778: step 4946, loss 0.0728807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:40.502290: step 4947, loss 0.108924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:40.961316: step 4948, loss 0.0512681, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:41.308990: step 4949, loss 0.04967, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:41.710300: step 4950, loss 0.0355237, acc 1, learning_rate 0.0001
2017-10-10T15:19:42.211287: step 4951, loss 0.119181, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:42.559855: step 4952, loss 0.0471936, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:42.960183: step 4953, loss 0.0265816, acc 1, learning_rate 0.0001
2017-10-10T15:19:43.367867: step 4954, loss 0.0685303, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:43.768865: step 4955, loss 0.0358376, acc 1, learning_rate 0.0001
2017-10-10T15:19:44.198062: step 4956, loss 0.0528676, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:44.692908: step 4957, loss 0.055167, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:45.104032: step 4958, loss 0.0459977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:45.480893: step 4959, loss 0.109604, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:45.835613: step 4960, loss 0.0784155, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:46.783514: step 4960, loss 0.21172, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-4960

2017-10-10T15:19:48.073082: step 4961, loss 0.0378302, acc 1, learning_rate 0.0001
2017-10-10T15:19:48.487022: step 4962, loss 0.0606109, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:48.932704: step 4963, loss 0.0541419, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:49.372889: step 4964, loss 0.0257887, acc 1, learning_rate 0.0001
2017-10-10T15:19:49.772616: step 4965, loss 0.0598644, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:50.228544: step 4966, loss 0.165431, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:50.628410: step 4967, loss 0.105647, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:51.112732: step 4968, loss 0.0568794, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:51.543351: step 4969, loss 0.0450483, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:51.975659: step 4970, loss 0.0808681, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:52.401105: step 4971, loss 0.0580055, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:52.849118: step 4972, loss 0.083218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:53.292848: step 4973, loss 0.0582988, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:53.730742: step 4974, loss 0.140628, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:54.238251: step 4975, loss 0.104376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:54.598468: step 4976, loss 0.0307101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:54.941803: step 4977, loss 0.0971863, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:55.304898: step 4978, loss 0.0348111, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:55.768530: step 4979, loss 0.183288, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:56.159022: step 4980, loss 0.030692, acc 1, learning_rate 0.0001
2017-10-10T15:19:56.584892: step 4981, loss 0.0562085, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:56.990402: step 4982, loss 0.164122, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:57.407755: step 4983, loss 0.0656597, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:57.849811: step 4984, loss 0.052297, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:58.249160: step 4985, loss 0.128044, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:58.722744: step 4986, loss 0.0254861, acc 1, learning_rate 0.0001
2017-10-10T15:19:59.600823: step 4987, loss 0.0494788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:00.001399: step 4988, loss 0.0323172, acc 1, learning_rate 0.0001
2017-10-10T15:20:00.426654: step 4989, loss 0.0791636, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:00.799671: step 4990, loss 0.154859, acc 0.890625, learning_rate 0.0001
2017-10-10T15:20:01.186550: step 4991, loss 0.0627599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:01.582588: step 4992, loss 0.159159, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:01.994690: step 4993, loss 0.0532495, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:02.365063: step 4994, loss 0.0894146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:02.784852: step 4995, loss 0.0648834, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:03.197890: step 4996, loss 0.0981154, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:03.619490: step 4997, loss 0.0770667, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:03.945943: step 4998, loss 0.0925967, acc 0.960784, learning_rate 0.0001
2017-10-10T15:20:04.388066: step 4999, loss 0.0922138, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:04.834437: step 5000, loss 0.0961406, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:05.799081: step 5000, loss 0.213464, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5000

2017-10-10T15:20:07.085760: step 5001, loss 0.114998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:07.484839: step 5002, loss 0.0673023, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:07.893027: step 5003, loss 0.0511639, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:08.338464: step 5004, loss 0.107956, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:08.760937: step 5005, loss 0.0977564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:09.185254: step 5006, loss 0.053115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:09.663480: step 5007, loss 0.111046, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:10.139656: step 5008, loss 0.0742441, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:10.548914: step 5009, loss 0.0576755, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:10.872932: step 5010, loss 0.0748503, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:11.224990: step 5011, loss 0.0636429, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:11.647821: step 5012, loss 0.0616991, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:12.104857: step 5013, loss 0.00847377, acc 1, learning_rate 0.0001
2017-10-10T15:20:12.541029: step 5014, loss 0.093548, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:12.916977: step 5015, loss 0.0275478, acc 1, learning_rate 0.0001
2017-10-10T15:20:13.336805: step 5016, loss 0.0225665, acc 1, learning_rate 0.0001
2017-10-10T15:20:13.724248: step 5017, loss 0.0870925, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:14.189043: step 5018, loss 0.152643, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:14.596403: step 5019, loss 0.0716247, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:15.005354: step 5020, loss 0.068812, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:15.450127: step 5021, loss 0.11554, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:15.865813: step 5022, loss 0.0281967, acc 1, learning_rate 0.0001
2017-10-10T15:20:16.313727: step 5023, loss 0.0463275, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:16.788177: step 5024, loss 0.0394751, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:17.193021: step 5025, loss 0.117065, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:17.588466: step 5026, loss 0.102697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:18.012848: step 5027, loss 0.106364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:18.345766: step 5028, loss 0.0579683, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:18.780904: step 5029, loss 0.0590808, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:19.203234: step 5030, loss 0.0835621, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:19.560841: step 5031, loss 0.119189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:19.932754: step 5032, loss 0.0436794, acc 1, learning_rate 0.0001
2017-10-10T15:20:20.381521: step 5033, loss 0.0931923, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:20.825000: step 5034, loss 0.049299, acc 1, learning_rate 0.0001
2017-10-10T15:20:21.229248: step 5035, loss 0.0705133, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:21.612850: step 5036, loss 0.0756572, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:22.034853: step 5037, loss 0.0159715, acc 1, learning_rate 0.0001
2017-10-10T15:20:22.436853: step 5038, loss 0.112503, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:22.840938: step 5039, loss 0.07634, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:23.240277: step 5040, loss 0.0429114, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:24.249101: step 5040, loss 0.209953, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5040

2017-10-10T15:20:25.484975: step 5041, loss 0.0924957, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:25.908946: step 5042, loss 0.108466, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:26.317179: step 5043, loss 0.0589483, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:26.700372: step 5044, loss 0.0634382, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:27.192915: step 5045, loss 0.0306367, acc 1, learning_rate 0.0001
2017-10-10T15:20:27.628842: step 5046, loss 0.0165223, acc 1, learning_rate 0.0001
2017-10-10T15:20:27.946905: step 5047, loss 0.0539231, acc 1, learning_rate 0.0001
2017-10-10T15:20:28.342215: step 5048, loss 0.0766535, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:28.717118: step 5049, loss 0.0634307, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:29.125800: step 5050, loss 0.0692728, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:29.491283: step 5051, loss 0.0608953, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:29.951936: step 5052, loss 0.166134, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:30.334575: step 5053, loss 0.067915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:30.732836: step 5054, loss 0.0577977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:31.179440: step 5055, loss 0.0803595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:31.654297: step 5056, loss 0.0460877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:32.083084: step 5057, loss 0.0221872, acc 1, learning_rate 0.0001
2017-10-10T15:20:32.450398: step 5058, loss 0.0628174, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:32.882119: step 5059, loss 0.0834335, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:33.300209: step 5060, loss 0.0680426, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:33.659821: step 5061, loss 0.0611017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:34.045745: step 5062, loss 0.0321189, acc 1, learning_rate 0.0001
2017-10-10T15:20:34.446871: step 5063, loss 0.0667691, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:34.821119: step 5064, loss 0.102335, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:35.212252: step 5065, loss 0.0641332, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:35.664724: step 5066, loss 0.0833364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:36.041255: step 5067, loss 0.108062, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:36.482083: step 5068, loss 0.0493681, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.894387: step 5069, loss 0.0382657, acc 1, learning_rate 0.0001
2017-10-10T15:20:37.244880: step 5070, loss 0.0401578, acc 1, learning_rate 0.0001
2017-10-10T15:20:37.657285: step 5071, loss 0.110222, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:38.073472: step 5072, loss 0.087201, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:38.566205: step 5073, loss 0.0329124, acc 1, learning_rate 0.0001
2017-10-10T15:20:38.924827: step 5074, loss 0.11632, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:39.285383: step 5075, loss 0.217989, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:39.707171: step 5076, loss 0.0657316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:40.171188: step 5077, loss 0.10751, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:40.570157: step 5078, loss 0.0557635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:40.960915: step 5079, loss 0.036504, acc 1, learning_rate 0.0001
2017-10-10T15:20:41.347692: step 5080, loss 0.0675901, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:42.396096: step 5080, loss 0.212028, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5080

2017-10-10T15:20:43.660871: step 5081, loss 0.0227292, acc 1, learning_rate 0.0001
2017-10-10T15:20:44.329518: step 5082, loss 0.0611019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:44.617210: step 5083, loss 0.0918117, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:44.936631: step 5084, loss 0.113034, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:45.257458: step 5085, loss 0.0649918, acc 1, learning_rate 0.0001
2017-10-10T15:20:45.676527: step 5086, loss 0.0699416, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:46.043082: step 5087, loss 0.094655, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:46.463095: step 5088, loss 0.0648045, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:46.885036: step 5089, loss 0.0739232, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:47.349061: step 5090, loss 0.14308, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:47.813711: step 5091, loss 0.0461943, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:48.224936: step 5092, loss 0.0839901, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:48.658070: step 5093, loss 0.0666089, acc 1, learning_rate 0.0001
2017-10-10T15:20:49.044898: step 5094, loss 0.0862624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:49.459919: step 5095, loss 0.0712279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:49.867203: step 5096, loss 0.158026, acc 0.960784, learning_rate 0.0001
2017-10-10T15:20:50.300998: step 5097, loss 0.102425, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:50.729266: step 5098, loss 0.097114, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:51.093017: step 5099, loss 0.0696148, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:51.528127: step 5100, loss 0.0837135, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:51.849878: step 5101, loss 0.0400508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:52.257155: step 5102, loss 0.0569346, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:52.698002: step 5103, loss 0.0631024, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:53.096180: step 5104, loss 0.0705254, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:53.527725: step 5105, loss 0.0466136, acc 1, learning_rate 0.0001
2017-10-10T15:20:53.953854: step 5106, loss 0.0712591, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:54.345095: step 5107, loss 0.0382171, acc 1, learning_rate 0.0001
2017-10-10T15:20:54.766002: step 5108, loss 0.0587408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.127609: step 5109, loss 0.117123, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.492664: step 5110, loss 0.125166, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.861033: step 5111, loss 0.134452, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:56.282394: step 5112, loss 0.0438944, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:56.777287: step 5113, loss 0.187024, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:57.088959: step 5114, loss 0.0870576, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:57.418073: step 5115, loss 0.0235407, acc 1, learning_rate 0.0001
2017-10-10T15:20:57.847221: step 5116, loss 0.0745867, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:58.242530: step 5117, loss 0.036096, acc 1, learning_rate 0.0001
2017-10-10T15:20:58.686491: step 5118, loss 0.07134, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:59.171308: step 5119, loss 0.0883384, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:59.577718: step 5120, loss 0.135675, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:00.650058: step 5120, loss 0.208924, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5120

2017-10-10T15:21:01.880962: step 5121, loss 0.022067, acc 1, learning_rate 0.0001
2017-10-10T15:21:02.265337: step 5122, loss 0.034024, acc 1, learning_rate 0.0001
2017-10-10T15:21:02.648274: step 5123, loss 0.0307837, acc 1, learning_rate 0.0001
2017-10-10T15:21:03.027125: step 5124, loss 0.0857095, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:03.396840: step 5125, loss 0.053388, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:03.808854: step 5126, loss 0.190239, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:04.229711: step 5127, loss 0.0757445, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:04.652551: step 5128, loss 0.051855, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:05.010577: step 5129, loss 0.0522074, acc 1, learning_rate 0.0001
2017-10-10T15:21:05.388713: step 5130, loss 0.0456014, acc 1, learning_rate 0.0001
2017-10-10T15:21:05.778878: step 5131, loss 0.0523811, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:06.174579: step 5132, loss 0.332026, acc 0.890625, learning_rate 0.0001
2017-10-10T15:21:06.596836: step 5133, loss 0.0432989, acc 1, learning_rate 0.0001
2017-10-10T15:21:07.008449: step 5134, loss 0.0535358, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:07.380207: step 5135, loss 0.088926, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:07.714114: step 5136, loss 0.0774038, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:08.120982: step 5137, loss 0.0896574, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:08.544417: step 5138, loss 0.124255, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:08.936929: step 5139, loss 0.0490009, acc 1, learning_rate 0.0001
2017-10-10T15:21:09.331957: step 5140, loss 0.0181294, acc 1, learning_rate 0.0001
2017-10-10T15:21:09.701662: step 5141, loss 0.0597956, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:10.058328: step 5142, loss 0.0652849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:10.437277: step 5143, loss 0.0812626, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:10.804027: step 5144, loss 0.0783831, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:11.172575: step 5145, loss 0.0520171, acc 1, learning_rate 0.0001
2017-10-10T15:21:11.567099: step 5146, loss 0.0637689, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:11.983923: step 5147, loss 0.0853432, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:12.306719: step 5148, loss 0.0402925, acc 1, learning_rate 0.0001
2017-10-10T15:21:12.712137: step 5149, loss 0.0401839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:13.188960: step 5150, loss 0.0547349, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:13.662713: step 5151, loss 0.0373345, acc 1, learning_rate 0.0001
2017-10-10T15:21:14.032818: step 5152, loss 0.132994, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:14.388544: step 5153, loss 0.0701489, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:14.808980: step 5154, loss 0.0603365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:15.262337: step 5155, loss 0.0477972, acc 1, learning_rate 0.0001
2017-10-10T15:21:15.621661: step 5156, loss 0.0848976, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:16.065002: step 5157, loss 0.0408264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:16.460926: step 5158, loss 0.0338721, acc 1, learning_rate 0.0001
2017-10-10T15:21:16.851851: step 5159, loss 0.0245729, acc 1, learning_rate 0.0001
2017-10-10T15:21:17.266984: step 5160, loss 0.0367652, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:18.770360: step 5160, loss 0.210824, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5160

2017-10-10T15:21:20.121081: step 5161, loss 0.118931, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:20.464597: step 5162, loss 0.102182, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:20.890058: step 5163, loss 0.028582, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:21.216909: step 5164, loss 0.0184534, acc 1, learning_rate 0.0001
2017-10-10T15:21:21.605996: step 5165, loss 0.0856042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:22.022100: step 5166, loss 0.0304209, acc 1, learning_rate 0.0001
2017-10-10T15:21:22.403066: step 5167, loss 0.0349031, acc 1, learning_rate 0.0001
2017-10-10T15:21:22.852961: step 5168, loss 0.0429463, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:23.274153: step 5169, loss 0.0407528, acc 1, learning_rate 0.0001
2017-10-10T15:21:23.679461: step 5170, loss 0.147938, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:24.093237: step 5171, loss 0.0846105, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:24.414625: step 5172, loss 0.0272055, acc 1, learning_rate 0.0001
2017-10-10T15:21:24.796811: step 5173, loss 0.172745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:25.180854: step 5174, loss 0.0960673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:25.582096: step 5175, loss 0.179357, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:26.041100: step 5176, loss 0.0600512, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:26.467502: step 5177, loss 0.0865018, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:26.875063: step 5178, loss 0.0260751, acc 1, learning_rate 0.0001
2017-10-10T15:21:27.234537: step 5179, loss 0.117333, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:27.669835: step 5180, loss 0.0737931, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:28.091047: step 5181, loss 0.0470577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:28.497104: step 5182, loss 0.0721092, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:28.878136: step 5183, loss 0.114704, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.279117: step 5184, loss 0.0730873, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.665683: step 5185, loss 0.0584234, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:30.041131: step 5186, loss 0.11074, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:30.481010: step 5187, loss 0.0327399, acc 1, learning_rate 0.0001
2017-10-10T15:21:30.848994: step 5188, loss 0.0241751, acc 1, learning_rate 0.0001
2017-10-10T15:21:31.251152: step 5189, loss 0.109818, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:31.654292: step 5190, loss 0.0997679, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:32.151174: step 5191, loss 0.0309157, acc 1, learning_rate 0.0001
2017-10-10T15:21:32.516906: step 5192, loss 0.0896582, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:32.925042: step 5193, loss 0.071673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:33.302311: step 5194, loss 0.0463824, acc 0.980392, learning_rate 0.0001
2017-10-10T15:21:33.676905: step 5195, loss 0.0455517, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:34.143665: step 5196, loss 0.0951742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:34.568897: step 5197, loss 0.0723409, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:35.017038: step 5198, loss 0.0996429, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:35.392514: step 5199, loss 0.0744212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:35.828849: step 5200, loss 0.017099, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:36.555948: step 5200, loss 0.210738, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5200

2017-10-10T15:21:37.930062: step 5201, loss 0.0804729, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:38.343305: step 5202, loss 0.0704996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:38.756162: step 5203, loss 0.0294152, acc 1, learning_rate 0.0001
2017-10-10T15:21:39.135408: step 5204, loss 0.0698836, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.566086: step 5205, loss 0.0356377, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.954375: step 5206, loss 0.0786999, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:40.341153: step 5207, loss 0.0328327, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:40.749506: step 5208, loss 0.0457809, acc 1, learning_rate 0.0001
2017-10-10T15:21:41.164259: step 5209, loss 0.0991102, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:41.567186: step 5210, loss 0.048625, acc 1, learning_rate 0.0001
2017-10-10T15:21:42.013012: step 5211, loss 0.0520364, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:42.387526: step 5212, loss 0.0556256, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:42.820975: step 5213, loss 0.0542835, acc 1, learning_rate 0.0001
2017-10-10T15:21:43.320967: step 5214, loss 0.0565815, acc 1, learning_rate 0.0001
2017-10-10T15:21:43.721151: step 5215, loss 0.0676884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:44.159952: step 5216, loss 0.142098, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:44.593307: step 5217, loss 0.0315075, acc 1, learning_rate 0.0001
2017-10-10T15:21:45.047488: step 5218, loss 0.149469, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:45.452940: step 5219, loss 0.0313095, acc 1, learning_rate 0.0001
2017-10-10T15:21:45.867382: step 5220, loss 0.0595879, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:46.252811: step 5221, loss 0.02135, acc 1, learning_rate 0.0001
2017-10-10T15:21:46.649161: step 5222, loss 0.033878, acc 1, learning_rate 0.0001
2017-10-10T15:21:46.984931: step 5223, loss 0.0630302, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:47.365124: step 5224, loss 0.0792014, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:47.824713: step 5225, loss 0.0743463, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:48.236345: step 5226, loss 0.0997433, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:48.625182: step 5227, loss 0.209143, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:49.108980: step 5228, loss 0.0439529, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:49.536835: step 5229, loss 0.114432, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:49.988467: step 5230, loss 0.0740692, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:50.389016: step 5231, loss 0.0404888, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:50.839708: step 5232, loss 0.0405122, acc 1, learning_rate 0.0001
2017-10-10T15:21:51.196845: step 5233, loss 0.0352114, acc 1, learning_rate 0.0001
2017-10-10T15:21:51.629027: step 5234, loss 0.0583096, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:52.099425: step 5235, loss 0.0564585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:52.486746: step 5236, loss 0.143364, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:52.818045: step 5237, loss 0.041229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:53.180832: step 5238, loss 0.0564786, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:53.610200: step 5239, loss 0.0249952, acc 1, learning_rate 0.0001
2017-10-10T15:21:54.024947: step 5240, loss 0.0227787, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:54.839430: step 5240, loss 0.209881, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5240

2017-10-10T15:21:56.185435: step 5241, loss 0.0622058, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:56.588830: step 5242, loss 0.0733262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:57.040920: step 5243, loss 0.0932819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:57.498238: step 5244, loss 0.0318988, acc 1, learning_rate 0.0001
2017-10-10T15:21:57.847833: step 5245, loss 0.0665369, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:58.208835: step 5246, loss 0.0594531, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:58.728853: step 5247, loss 0.0886157, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:59.121275: step 5248, loss 0.0541671, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:59.513807: step 5249, loss 0.0563432, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:59.890852: step 5250, loss 0.0791817, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:00.288639: step 5251, loss 0.0554074, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:00.711683: step 5252, loss 0.0444674, acc 1, learning_rate 0.0001
2017-10-10T15:22:01.107710: step 5253, loss 0.0817452, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:01.487558: step 5254, loss 0.0810518, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:01.893875: step 5255, loss 0.128445, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:02.264941: step 5256, loss 0.0804444, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:02.647747: step 5257, loss 0.0778297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:03.035894: step 5258, loss 0.0405834, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:03.425033: step 5259, loss 0.0272602, acc 1, learning_rate 0.0001
2017-10-10T15:22:03.828184: step 5260, loss 0.0413878, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:04.264752: step 5261, loss 0.0277003, acc 1, learning_rate 0.0001
2017-10-10T15:22:04.716896: step 5262, loss 0.0776999, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:05.144978: step 5263, loss 0.0845158, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:05.546375: step 5264, loss 0.138392, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:05.979203: step 5265, loss 0.103731, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:06.404968: step 5266, loss 0.0434863, acc 1, learning_rate 0.0001
2017-10-10T15:22:06.825045: step 5267, loss 0.0419892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:07.272945: step 5268, loss 0.0886896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:07.656819: step 5269, loss 0.0694535, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:08.094864: step 5270, loss 0.14927, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:08.526421: step 5271, loss 0.136559, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:08.947210: step 5272, loss 0.0857282, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:09.228869: step 5273, loss 0.0491565, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:09.688279: step 5274, loss 0.0812645, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:10.039350: step 5275, loss 0.105397, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:10.428924: step 5276, loss 0.116029, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:10.831970: step 5277, loss 0.0743288, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:11.285848: step 5278, loss 0.105158, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:11.780869: step 5279, loss 0.0801806, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:12.202830: step 5280, loss 0.074258, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:13.178963: step 5280, loss 0.217139, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5280

2017-10-10T15:22:14.492787: step 5281, loss 0.0536857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:14.885162: step 5282, loss 0.0462609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:15.307617: step 5283, loss 0.0701136, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:15.695479: step 5284, loss 0.0901394, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:16.128862: step 5285, loss 0.0566903, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:16.541840: step 5286, loss 0.0761441, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:16.967736: step 5287, loss 0.0965176, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:17.404968: step 5288, loss 0.056618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:17.849335: step 5289, loss 0.0326437, acc 1, learning_rate 0.0001
2017-10-10T15:22:18.256298: step 5290, loss 0.0424311, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:18.637102: step 5291, loss 0.0288851, acc 1, learning_rate 0.0001
2017-10-10T15:22:18.989011: step 5292, loss 0.0928465, acc 0.960784, learning_rate 0.0001
2017-10-10T15:22:19.420708: step 5293, loss 0.257353, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:19.786306: step 5294, loss 0.0617091, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:20.169973: step 5295, loss 0.0269278, acc 1, learning_rate 0.0001
2017-10-10T15:22:20.612929: step 5296, loss 0.119588, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:21.015737: step 5297, loss 0.018151, acc 1, learning_rate 0.0001
2017-10-10T15:22:21.431047: step 5298, loss 0.0271667, acc 1, learning_rate 0.0001
2017-10-10T15:22:21.893223: step 5299, loss 0.0273753, acc 1, learning_rate 0.0001
2017-10-10T15:22:22.362849: step 5300, loss 0.0917324, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:22.716197: step 5301, loss 0.0312308, acc 1, learning_rate 0.0001
2017-10-10T15:22:23.114724: step 5302, loss 0.0963027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:23.520997: step 5303, loss 0.113234, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:23.972465: step 5304, loss 0.157131, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:24.445004: step 5305, loss 0.0249514, acc 1, learning_rate 0.0001
2017-10-10T15:22:24.918094: step 5306, loss 0.0576421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:25.328181: step 5307, loss 0.0725383, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:25.694360: step 5308, loss 0.0273312, acc 1, learning_rate 0.0001
2017-10-10T15:22:26.150607: step 5309, loss 0.0736982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:26.466608: step 5310, loss 0.0226382, acc 1, learning_rate 0.0001
2017-10-10T15:22:26.822459: step 5311, loss 0.0341075, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:27.237012: step 5312, loss 0.0302148, acc 1, learning_rate 0.0001
2017-10-10T15:22:27.628681: step 5313, loss 0.0683854, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.016596: step 5314, loss 0.0499033, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.436927: step 5315, loss 0.0174139, acc 1, learning_rate 0.0001
2017-10-10T15:22:28.900912: step 5316, loss 0.0510813, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:29.304949: step 5317, loss 0.104065, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:29.710865: step 5318, loss 0.0546314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:30.147939: step 5319, loss 0.0999302, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:30.559718: step 5320, loss 0.0211698, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:31.325030: step 5320, loss 0.214926, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5320

2017-10-10T15:22:32.721640: step 5321, loss 0.0739817, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:33.101149: step 5322, loss 0.22101, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:33.489294: step 5323, loss 0.0507619, acc 1, learning_rate 0.0001
2017-10-10T15:22:33.926051: step 5324, loss 0.0623732, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:34.378944: step 5325, loss 0.064721, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:34.738228: step 5326, loss 0.0338711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:35.108168: step 5327, loss 0.0643617, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:35.478114: step 5328, loss 0.0337332, acc 1, learning_rate 0.0001
2017-10-10T15:22:35.880856: step 5329, loss 0.0816146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:36.293103: step 5330, loss 0.0645205, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:36.724181: step 5331, loss 0.0179583, acc 1, learning_rate 0.0001
2017-10-10T15:22:37.148855: step 5332, loss 0.0329534, acc 1, learning_rate 0.0001
2017-10-10T15:22:37.565063: step 5333, loss 0.0642482, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:38.024310: step 5334, loss 0.0416159, acc 1, learning_rate 0.0001
2017-10-10T15:22:38.396888: step 5335, loss 0.0513675, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:38.807237: step 5336, loss 0.129734, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:39.244849: step 5337, loss 0.0667761, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:39.752839: step 5338, loss 0.0513492, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:40.116622: step 5339, loss 0.0380961, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:40.477885: step 5340, loss 0.0740594, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:40.871236: step 5341, loss 0.0820535, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:41.356118: step 5342, loss 0.123202, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:41.816997: step 5343, loss 0.179695, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:42.140031: step 5344, loss 0.0547389, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:42.423180: step 5345, loss 0.140597, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:42.754989: step 5346, loss 0.0724216, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:43.139838: step 5347, loss 0.0296024, acc 1, learning_rate 0.0001
2017-10-10T15:22:43.475438: step 5348, loss 0.0308964, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:43.849017: step 5349, loss 0.0333387, acc 1, learning_rate 0.0001
2017-10-10T15:22:44.205815: step 5350, loss 0.140947, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:44.578695: step 5351, loss 0.143172, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:44.921250: step 5352, loss 0.113133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:45.267201: step 5353, loss 0.0341888, acc 1, learning_rate 0.0001
2017-10-10T15:22:45.586944: step 5354, loss 0.0372761, acc 1, learning_rate 0.0001
2017-10-10T15:22:46.005083: step 5355, loss 0.0594839, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:46.319943: step 5356, loss 0.0905959, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:46.590467: step 5357, loss 0.0862278, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:46.892854: step 5358, loss 0.0574426, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:47.203394: step 5359, loss 0.0817254, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:47.537810: step 5360, loss 0.187723, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:48.284242: step 5360, loss 0.210997, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5360

2017-10-10T15:22:49.369319: step 5361, loss 0.0657164, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:49.730639: step 5362, loss 0.0335376, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:50.089082: step 5363, loss 0.078533, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:50.469730: step 5364, loss 0.0820217, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:50.763996: step 5365, loss 0.0649759, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:51.174655: step 5366, loss 0.028492, acc 1, learning_rate 0.0001
2017-10-10T15:22:51.495530: step 5367, loss 0.0429437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:51.833520: step 5368, loss 0.123003, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:52.181158: step 5369, loss 0.0664129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:52.498857: step 5370, loss 0.109586, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:52.788989: step 5371, loss 0.0290175, acc 1, learning_rate 0.0001
2017-10-10T15:22:53.136954: step 5372, loss 0.058884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:53.480939: step 5373, loss 0.0770011, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:53.815627: step 5374, loss 0.127414, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:54.144926: step 5375, loss 0.0508634, acc 1, learning_rate 0.0001
2017-10-10T15:22:54.503081: step 5376, loss 0.0873661, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:54.808991: step 5377, loss 0.0896164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:55.177077: step 5378, loss 0.0478329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:55.560967: step 5379, loss 0.0902624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:55.900161: step 5380, loss 0.115042, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:56.234659: step 5381, loss 0.195386, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:56.498151: step 5382, loss 0.0793042, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:56.763054: step 5383, loss 0.0526079, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:57.044848: step 5384, loss 0.0951843, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:57.391177: step 5385, loss 0.0398259, acc 1, learning_rate 0.0001
2017-10-10T15:22:57.735758: step 5386, loss 0.0517458, acc 1, learning_rate 0.0001
2017-10-10T15:22:58.052906: step 5387, loss 0.10823, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:58.462664: step 5388, loss 0.0793397, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:58.805012: step 5389, loss 0.0119095, acc 1, learning_rate 0.0001
2017-10-10T15:22:59.089002: step 5390, loss 0.071119, acc 0.980392, learning_rate 0.0001
2017-10-10T15:22:59.452995: step 5391, loss 0.033117, acc 1, learning_rate 0.0001
2017-10-10T15:22:59.795073: step 5392, loss 0.0875458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:00.134233: step 5393, loss 0.0149389, acc 1, learning_rate 0.0001
2017-10-10T15:23:00.441043: step 5394, loss 0.0771322, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:00.726532: step 5395, loss 0.093681, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:01.024362: step 5396, loss 0.0430459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:01.384311: step 5397, loss 0.0561354, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:01.809506: step 5398, loss 0.0735116, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:02.148852: step 5399, loss 0.0807392, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:02.424951: step 5400, loss 0.0479236, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:03.112474: step 5400, loss 0.209132, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5400

2017-10-10T15:23:04.371310: step 5401, loss 0.0192539, acc 1, learning_rate 0.0001
2017-10-10T15:23:04.658957: step 5402, loss 0.0495468, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:04.976334: step 5403, loss 0.0531146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:05.279926: step 5404, loss 0.0616225, acc 1, learning_rate 0.0001
2017-10-10T15:23:05.624188: step 5405, loss 0.0624756, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:05.944822: step 5406, loss 0.0266014, acc 1, learning_rate 0.0001
2017-10-10T15:23:06.301974: step 5407, loss 0.0546355, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:06.662582: step 5408, loss 0.0751406, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:06.983692: step 5409, loss 0.0135407, acc 1, learning_rate 0.0001
2017-10-10T15:23:07.228728: step 5410, loss 0.0454921, acc 1, learning_rate 0.0001
2017-10-10T15:23:07.585479: step 5411, loss 0.112868, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:07.921751: step 5412, loss 0.0341047, acc 1, learning_rate 0.0001
2017-10-10T15:23:08.288893: step 5413, loss 0.217552, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:08.626222: step 5414, loss 0.104499, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:08.989599: step 5415, loss 0.0695504, acc 1, learning_rate 0.0001
2017-10-10T15:23:09.344827: step 5416, loss 0.0212649, acc 1, learning_rate 0.0001
2017-10-10T15:23:09.612133: step 5417, loss 0.0150604, acc 1, learning_rate 0.0001
2017-10-10T15:23:09.873118: step 5418, loss 0.145486, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:10.173802: step 5419, loss 0.0400857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:10.508341: step 5420, loss 0.0446672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:10.829160: step 5421, loss 0.0161966, acc 1, learning_rate 0.0001
2017-10-10T15:23:11.123063: step 5422, loss 0.0608082, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:11.511742: step 5423, loss 0.116854, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:11.824933: step 5424, loss 0.0244052, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:12.228914: step 5425, loss 0.0454229, acc 1, learning_rate 0.0001
2017-10-10T15:23:12.565428: step 5426, loss 0.0362853, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:12.999630: step 5427, loss 0.0531235, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:13.291741: step 5428, loss 0.154861, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:13.568352: step 5429, loss 0.114666, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:13.833845: step 5430, loss 0.0354676, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:14.185853: step 5431, loss 0.0956285, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:14.532878: step 5432, loss 0.0766741, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:14.907098: step 5433, loss 0.0768279, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:15.227222: step 5434, loss 0.0551539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:15.568845: step 5435, loss 0.0867958, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:15.913384: step 5436, loss 0.0347671, acc 1, learning_rate 0.0001
2017-10-10T15:23:16.283437: step 5437, loss 0.0689121, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:16.622052: step 5438, loss 0.033596, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:17.033027: step 5439, loss 0.0410648, acc 1, learning_rate 0.0001
2017-10-10T15:23:17.294115: step 5440, loss 0.0586214, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:17.908851: step 5440, loss 0.210885, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5440

2017-10-10T15:23:19.159438: step 5441, loss 0.0440151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:19.540677: step 5442, loss 0.03022, acc 1, learning_rate 0.0001
2017-10-10T15:23:19.868761: step 5443, loss 0.105524, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:20.184287: step 5444, loss 0.108632, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:20.491198: step 5445, loss 0.0983612, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:20.854325: step 5446, loss 0.0812756, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:21.149546: step 5447, loss 0.0699361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:21.473260: step 5448, loss 0.0962383, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:21.840090: step 5449, loss 0.0674363, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:22.138872: step 5450, loss 0.0789192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:22.472981: step 5451, loss 0.0590933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:22.885588: step 5452, loss 0.110841, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:23.147933: step 5453, loss 0.0841676, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:23.469358: step 5454, loss 0.0503616, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:23.833216: step 5455, loss 0.0610524, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:24.119327: step 5456, loss 0.0548062, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:24.432884: step 5457, loss 0.0720227, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:24.712924: step 5458, loss 0.0955884, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:25.051758: step 5459, loss 0.0209267, acc 1, learning_rate 0.0001
2017-10-10T15:23:25.372914: step 5460, loss 0.100854, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:25.728941: step 5461, loss 0.0461111, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:26.107795: step 5462, loss 0.0741403, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:26.447575: step 5463, loss 0.0254701, acc 1, learning_rate 0.0001
2017-10-10T15:23:26.785894: step 5464, loss 0.0320762, acc 1, learning_rate 0.0001
2017-10-10T15:23:27.217093: step 5465, loss 0.037411, acc 1, learning_rate 0.0001
2017-10-10T15:23:27.486665: step 5466, loss 0.0365562, acc 1, learning_rate 0.0001
2017-10-10T15:23:27.803885: step 5467, loss 0.0842981, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:28.060984: step 5468, loss 0.0274039, acc 1, learning_rate 0.0001
2017-10-10T15:23:28.420898: step 5469, loss 0.0951411, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:28.729094: step 5470, loss 0.066535, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:29.062575: step 5471, loss 0.0848553, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.380094: step 5472, loss 0.057557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:29.669271: step 5473, loss 0.152411, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:30.060865: step 5474, loss 0.0537773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.343105: step 5475, loss 0.0518684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.659879: step 5476, loss 0.095174, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:31.073438: step 5477, loss 0.0408075, acc 1, learning_rate 0.0001
2017-10-10T15:23:31.408809: step 5478, loss 0.0392569, acc 1, learning_rate 0.0001
2017-10-10T15:23:31.768128: step 5479, loss 0.0652428, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:32.113304: step 5480, loss 0.0662656, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:32.844100: step 5480, loss 0.210447, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5480

2017-10-10T15:23:34.124269: step 5481, loss 0.0618305, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:34.470192: step 5482, loss 0.0477715, acc 1, learning_rate 0.0001
2017-10-10T15:23:34.833029: step 5483, loss 0.0321998, acc 1, learning_rate 0.0001
2017-10-10T15:23:35.177158: step 5484, loss 0.0870572, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:35.444062: step 5485, loss 0.0255154, acc 1, learning_rate 0.0001
2017-10-10T15:23:35.809870: step 5486, loss 0.0442307, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:36.219076: step 5487, loss 0.0954342, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:36.467488: step 5488, loss 0.101267, acc 0.941176, learning_rate 0.0001
2017-10-10T15:23:36.711066: step 5489, loss 0.0746802, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:37.000848: step 5490, loss 0.125227, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:37.258754: step 5491, loss 0.0597266, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:37.660884: step 5492, loss 0.0536339, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:37.925004: step 5493, loss 0.154176, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:38.213568: step 5494, loss 0.0690505, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:38.573211: step 5495, loss 0.10534, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:38.924537: step 5496, loss 0.140266, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:39.266977: step 5497, loss 0.029898, acc 1, learning_rate 0.0001
2017-10-10T15:23:39.611819: step 5498, loss 0.0923182, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:39.941006: step 5499, loss 0.0497811, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:40.274894: step 5500, loss 0.0897964, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:40.617124: step 5501, loss 0.133547, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:40.928821: step 5502, loss 0.025172, acc 1, learning_rate 0.0001
2017-10-10T15:23:41.256721: step 5503, loss 0.0624729, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:41.542466: step 5504, loss 0.116463, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:41.853655: step 5505, loss 0.0703596, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.169347: step 5506, loss 0.120707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.490659: step 5507, loss 0.0289774, acc 1, learning_rate 0.0001
2017-10-10T15:23:42.849447: step 5508, loss 0.0597405, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:43.181067: step 5509, loss 0.0694511, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:43.521461: step 5510, loss 0.108334, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:43.842548: step 5511, loss 0.118115, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:44.133454: step 5512, loss 0.13714, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:44.466772: step 5513, loss 0.0260333, acc 1, learning_rate 0.0001
2017-10-10T15:23:44.802591: step 5514, loss 0.0223588, acc 1, learning_rate 0.0001
2017-10-10T15:23:45.113062: step 5515, loss 0.0498316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:45.424849: step 5516, loss 0.0653596, acc 1, learning_rate 0.0001
2017-10-10T15:23:45.839506: step 5517, loss 0.0316988, acc 1, learning_rate 0.0001
2017-10-10T15:23:46.152839: step 5518, loss 0.0139469, acc 1, learning_rate 0.0001
2017-10-10T15:23:46.460827: step 5519, loss 0.0896408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:46.749828: step 5520, loss 0.0529422, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:47.448930: step 5520, loss 0.212035, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5520

2017-10-10T15:23:48.577594: step 5521, loss 0.0582297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:48.890052: step 5522, loss 0.0868088, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:49.214063: step 5523, loss 0.0672997, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:49.569073: step 5524, loss 0.0453763, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:49.914847: step 5525, loss 0.0483108, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:50.325370: step 5526, loss 0.0580573, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:50.604955: step 5527, loss 0.0618922, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:50.888989: step 5528, loss 0.0927305, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:51.144919: step 5529, loss 0.102419, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:51.452855: step 5530, loss 0.0443733, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:51.754964: step 5531, loss 0.102392, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:52.099939: step 5532, loss 0.0387852, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:52.438647: step 5533, loss 0.123669, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:52.775224: step 5534, loss 0.0474376, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:53.132976: step 5535, loss 0.0771531, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:53.443704: step 5536, loss 0.0437526, acc 1, learning_rate 0.0001
2017-10-10T15:23:53.761085: step 5537, loss 0.0969886, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:54.136924: step 5538, loss 0.0697253, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:54.460966: step 5539, loss 0.0754153, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:54.766827: step 5540, loss 0.0221088, acc 1, learning_rate 0.0001
2017-10-10T15:23:55.173266: step 5541, loss 0.0279018, acc 1, learning_rate 0.0001
2017-10-10T15:23:55.492158: step 5542, loss 0.0184707, acc 1, learning_rate 0.0001
2017-10-10T15:23:55.853313: step 5543, loss 0.0516315, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:56.225206: step 5544, loss 0.155016, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:56.614834: step 5545, loss 0.0560721, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:56.954374: step 5546, loss 0.0538597, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:57.273118: step 5547, loss 0.0519454, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:57.600830: step 5548, loss 0.0236041, acc 1, learning_rate 0.0001
2017-10-10T15:23:57.937397: step 5549, loss 0.127517, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:58.246762: step 5550, loss 0.0473557, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:58.519983: step 5551, loss 0.10807, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:58.799041: step 5552, loss 0.107582, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:59.153094: step 5553, loss 0.0342729, acc 1, learning_rate 0.0001
2017-10-10T15:23:59.500850: step 5554, loss 0.0337125, acc 1, learning_rate 0.0001
2017-10-10T15:23:59.827201: step 5555, loss 0.0640769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:00.145007: step 5556, loss 0.0347547, acc 1, learning_rate 0.0001
2017-10-10T15:24:00.507784: step 5557, loss 0.0421268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:00.843805: step 5558, loss 0.0785167, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:01.196290: step 5559, loss 0.0814063, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:01.555597: step 5560, loss 0.0625767, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:02.252509: step 5560, loss 0.210831, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5560

2017-10-10T15:24:03.587375: step 5561, loss 0.0538557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:03.954251: step 5562, loss 0.123533, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:04.237105: step 5563, loss 0.0307179, acc 1, learning_rate 0.0001
2017-10-10T15:24:04.505329: step 5564, loss 0.0709115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:04.784871: step 5565, loss 0.0554477, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:05.165938: step 5566, loss 0.0763844, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:05.543252: step 5567, loss 0.060944, acc 1, learning_rate 0.0001
2017-10-10T15:24:05.880970: step 5568, loss 0.0373647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:06.208178: step 5569, loss 0.10636, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:06.572845: step 5570, loss 0.0623365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:06.913031: step 5571, loss 0.0791904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:07.276887: step 5572, loss 0.0197228, acc 1, learning_rate 0.0001
2017-10-10T15:24:07.630830: step 5573, loss 0.0330532, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:08.045433: step 5574, loss 0.072749, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:08.407428: step 5575, loss 0.0301011, acc 1, learning_rate 0.0001
2017-10-10T15:24:08.636817: step 5576, loss 0.0640575, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:08.927775: step 5577, loss 0.107904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:09.210485: step 5578, loss 0.132028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:09.531520: step 5579, loss 0.0768161, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:09.854536: step 5580, loss 0.0262188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:10.179452: step 5581, loss 0.0753042, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:10.504868: step 5582, loss 0.0527604, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:10.885200: step 5583, loss 0.0182527, acc 1, learning_rate 0.0001
2017-10-10T15:24:11.205863: step 5584, loss 0.0550531, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:11.492984: step 5585, loss 0.0869941, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:11.798268: step 5586, loss 0.127923, acc 0.941176, learning_rate 0.0001
2017-10-10T15:24:12.169753: step 5587, loss 0.0630923, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:12.507607: step 5588, loss 0.140536, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:12.854242: step 5589, loss 0.0961406, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:13.161635: step 5590, loss 0.0528803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:13.523064: step 5591, loss 0.0861238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:13.843185: step 5592, loss 0.0694375, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:14.166180: step 5593, loss 0.131535, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:14.551182: step 5594, loss 0.0266285, acc 1, learning_rate 0.0001
2017-10-10T15:24:14.888460: step 5595, loss 0.0410986, acc 1, learning_rate 0.0001
2017-10-10T15:24:15.248685: step 5596, loss 0.0358531, acc 1, learning_rate 0.0001
2017-10-10T15:24:15.576934: step 5597, loss 0.0542823, acc 1, learning_rate 0.0001
2017-10-10T15:24:15.920793: step 5598, loss 0.105121, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:16.260916: step 5599, loss 0.0563295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:16.575895: step 5600, loss 0.113538, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:17.352453: step 5600, loss 0.212201, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5600

2017-10-10T15:24:18.700866: step 5601, loss 0.0930374, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:18.974983: step 5602, loss 0.0977792, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:19.234758: step 5603, loss 0.0783774, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:19.478975: step 5604, loss 0.0388876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:19.848817: step 5605, loss 0.0514513, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:20.201577: step 5606, loss 0.0875139, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:20.548957: step 5607, loss 0.0759007, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:20.881125: step 5608, loss 0.0658276, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:21.199503: step 5609, loss 0.0441721, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:21.554745: step 5610, loss 0.0254926, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:21.912902: step 5611, loss 0.0683433, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:22.244097: step 5612, loss 0.0956726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:22.565150: step 5613, loss 0.110481, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:22.920565: step 5614, loss 0.119472, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:23.232244: step 5615, loss 0.0184921, acc 1, learning_rate 0.0001
2017-10-10T15:24:23.597976: step 5616, loss 0.0415048, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:23.965093: step 5617, loss 0.180258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:24.289541: step 5618, loss 0.0703305, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:24.653170: step 5619, loss 0.0562382, acc 1, learning_rate 0.0001
2017-10-10T15:24:24.962010: step 5620, loss 0.0418994, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:25.287594: step 5621, loss 0.0305425, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:25.650927: step 5622, loss 0.0560119, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:26.001163: step 5623, loss 0.144029, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:26.364092: step 5624, loss 0.126289, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:26.712859: step 5625, loss 0.0554559, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:27.033409: step 5626, loss 0.0502657, acc 1, learning_rate 0.0001
2017-10-10T15:24:27.389541: step 5627, loss 0.031282, acc 1, learning_rate 0.0001
2017-10-10T15:24:27.708979: step 5628, loss 0.0748261, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:28.056419: step 5629, loss 0.101393, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:28.370156: step 5630, loss 0.0286572, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:28.732853: step 5631, loss 0.104156, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:29.168651: step 5632, loss 0.0323109, acc 1, learning_rate 0.0001
2017-10-10T15:24:29.456027: step 5633, loss 0.0134265, acc 1, learning_rate 0.0001
2017-10-10T15:24:29.744947: step 5634, loss 0.0681258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:30.139258: step 5635, loss 0.0503028, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:30.431436: step 5636, loss 0.0712156, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:30.748210: step 5637, loss 0.028565, acc 1, learning_rate 0.0001
2017-10-10T15:24:31.137107: step 5638, loss 0.0521971, acc 1, learning_rate 0.0001
2017-10-10T15:24:31.423140: step 5639, loss 0.0334367, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:31.700099: step 5640, loss 0.0750147, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:32.304980: step 5640, loss 0.209646, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5640

2017-10-10T15:24:33.561286: step 5641, loss 0.187005, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:33.939674: step 5642, loss 0.0527317, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:34.236915: step 5643, loss 0.0472309, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:34.559203: step 5644, loss 0.0842142, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:34.867747: step 5645, loss 0.0553758, acc 1, learning_rate 0.0001
2017-10-10T15:24:35.217494: step 5646, loss 0.0832972, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:35.567983: step 5647, loss 0.0918264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:35.931663: step 5648, loss 0.0665636, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:36.273149: step 5649, loss 0.0671267, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:36.590209: step 5650, loss 0.0500308, acc 1, learning_rate 0.0001
2017-10-10T15:24:36.925569: step 5651, loss 0.0338879, acc 1, learning_rate 0.0001
2017-10-10T15:24:37.241012: step 5652, loss 0.0835537, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:37.568253: step 5653, loss 0.115292, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:37.915436: step 5654, loss 0.0700285, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:38.240880: step 5655, loss 0.0332983, acc 1, learning_rate 0.0001
2017-10-10T15:24:38.557497: step 5656, loss 0.0458914, acc 1, learning_rate 0.0001
2017-10-10T15:24:38.879842: step 5657, loss 0.0315041, acc 1, learning_rate 0.0001
2017-10-10T15:24:39.291646: step 5658, loss 0.12172, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:39.584295: step 5659, loss 0.0490017, acc 1, learning_rate 0.0001
2017-10-10T15:24:39.869451: step 5660, loss 0.0791828, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:40.196476: step 5661, loss 0.0406458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:40.526399: step 5662, loss 0.0142187, acc 1, learning_rate 0.0001
2017-10-10T15:24:40.920895: step 5663, loss 0.0735323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:41.324871: step 5664, loss 0.0638572, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:41.641029: step 5665, loss 0.0444705, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:41.936846: step 5666, loss 0.0542092, acc 1, learning_rate 0.0001
2017-10-10T15:24:42.319450: step 5667, loss 0.0969516, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:42.659864: step 5668, loss 0.0192255, acc 1, learning_rate 0.0001
2017-10-10T15:24:43.049025: step 5669, loss 0.0657498, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:43.419526: step 5670, loss 0.0724347, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:43.750611: step 5671, loss 0.119635, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:44.078705: step 5672, loss 0.0839872, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:44.420908: step 5673, loss 0.0696146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:44.858098: step 5674, loss 0.0329099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:45.189165: step 5675, loss 0.0686866, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:45.442164: step 5676, loss 0.0528293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:45.696955: step 5677, loss 0.023811, acc 1, learning_rate 0.0001
2017-10-10T15:24:46.018277: step 5678, loss 0.0155859, acc 1, learning_rate 0.0001
2017-10-10T15:24:46.356631: step 5679, loss 0.0525115, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:46.697160: step 5680, loss 0.0998461, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:47.388841: step 5680, loss 0.207399, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5680

2017-10-10T15:24:48.676842: step 5681, loss 0.0185865, acc 1, learning_rate 0.0001
2017-10-10T15:24:49.055955: step 5682, loss 0.0300205, acc 1, learning_rate 0.0001
2017-10-10T15:24:49.429307: step 5683, loss 0.0283459, acc 1, learning_rate 0.0001
2017-10-10T15:24:49.701392: step 5684, loss 0.0570012, acc 0.980392, learning_rate 0.0001
2017-10-10T15:24:49.994371: step 5685, loss 0.0748762, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:50.267385: step 5686, loss 0.112843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:50.592187: step 5687, loss 0.0556067, acc 1, learning_rate 0.0001
2017-10-10T15:24:51.002404: step 5688, loss 0.060149, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:51.311015: step 5689, loss 0.0761202, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:51.618736: step 5690, loss 0.0355309, acc 1, learning_rate 0.0001
2017-10-10T15:24:51.939255: step 5691, loss 0.0381551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:52.353579: step 5692, loss 0.0623162, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:52.642925: step 5693, loss 0.0562731, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:52.941066: step 5694, loss 0.0427966, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:53.303955: step 5695, loss 0.0793296, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:53.649094: step 5696, loss 0.0192788, acc 1, learning_rate 0.0001
2017-10-10T15:24:53.980993: step 5697, loss 0.0884644, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:54.346757: step 5698, loss 0.0928217, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:54.685296: step 5699, loss 0.119565, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:55.051547: step 5700, loss 0.0864356, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:55.398131: step 5701, loss 0.0476111, acc 1, learning_rate 0.0001
2017-10-10T15:24:55.776901: step 5702, loss 0.140306, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:56.107686: step 5703, loss 0.0762477, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.428835: step 5704, loss 0.067718, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.825819: step 5705, loss 0.0305814, acc 1, learning_rate 0.0001
2017-10-10T15:24:57.149887: step 5706, loss 0.0521383, acc 1, learning_rate 0.0001
2017-10-10T15:24:57.481040: step 5707, loss 0.0749726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:57.796165: step 5708, loss 0.0211702, acc 1, learning_rate 0.0001
2017-10-10T15:24:58.240372: step 5709, loss 0.13582, acc 0.90625, learning_rate 0.0001
2017-10-10T15:24:58.594784: step 5710, loss 0.0521635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:58.844957: step 5711, loss 0.0317757, acc 1, learning_rate 0.0001
2017-10-10T15:24:59.114913: step 5712, loss 0.05672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:59.374993: step 5713, loss 0.0718427, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:59.756927: step 5714, loss 0.0654183, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:00.016689: step 5715, loss 0.0376988, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:00.340777: step 5716, loss 0.0351397, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:00.614073: step 5717, loss 0.0513214, acc 1, learning_rate 0.0001
2017-10-10T15:25:00.970568: step 5718, loss 0.0301573, acc 1, learning_rate 0.0001
2017-10-10T15:25:01.284967: step 5719, loss 0.0775834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:01.668987: step 5720, loss 0.171664, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:02.392870: step 5720, loss 0.209517, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5720

2017-10-10T15:25:03.465253: step 5721, loss 0.0209977, acc 1, learning_rate 0.0001
2017-10-10T15:25:03.742072: step 5722, loss 0.0159918, acc 1, learning_rate 0.0001
2017-10-10T15:25:04.042859: step 5723, loss 0.0278048, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:04.372945: step 5724, loss 0.0942326, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:04.738155: step 5725, loss 0.0699505, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:05.043437: step 5726, loss 0.0456451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:05.373208: step 5727, loss 0.0250928, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:05.712882: step 5728, loss 0.0855459, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:06.106618: step 5729, loss 0.0496918, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:06.440902: step 5730, loss 0.0656304, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:06.790977: step 5731, loss 0.152568, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:07.131496: step 5732, loss 0.0252675, acc 1, learning_rate 0.0001
2017-10-10T15:25:07.464201: step 5733, loss 0.0313422, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:07.795590: step 5734, loss 0.0350811, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:08.122068: step 5735, loss 0.0605887, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:08.480854: step 5736, loss 0.0505639, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:08.814682: step 5737, loss 0.0330012, acc 1, learning_rate 0.0001
2017-10-10T15:25:09.137306: step 5738, loss 0.0368748, acc 1, learning_rate 0.0001
2017-10-10T15:25:09.488142: step 5739, loss 0.0336036, acc 1, learning_rate 0.0001
2017-10-10T15:25:09.816772: step 5740, loss 0.0885242, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:10.169039: step 5741, loss 0.0615203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:10.549732: step 5742, loss 0.05191, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:10.817758: step 5743, loss 0.0263992, acc 1, learning_rate 0.0001
2017-10-10T15:25:11.121146: step 5744, loss 0.133757, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:11.469989: step 5745, loss 0.107059, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:11.872295: step 5746, loss 0.039758, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:12.239708: step 5747, loss 0.0889641, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:12.532535: step 5748, loss 0.0545713, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:12.822792: step 5749, loss 0.163005, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:13.040446: step 5750, loss 0.0210653, acc 1, learning_rate 0.0001
2017-10-10T15:25:13.365546: step 5751, loss 0.0937651, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:13.712208: step 5752, loss 0.0871077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:14.052889: step 5753, loss 0.03779, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:14.448849: step 5754, loss 0.045947, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:14.764263: step 5755, loss 0.172112, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:15.046228: step 5756, loss 0.0951522, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:15.397401: step 5757, loss 0.0691921, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:15.737091: step 5758, loss 0.0361758, acc 1, learning_rate 0.0001
2017-10-10T15:25:16.048986: step 5759, loss 0.118751, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:16.378782: step 5760, loss 0.0641992, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:17.059643: step 5760, loss 0.209156, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5760

2017-10-10T15:25:18.333182: step 5761, loss 0.103604, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:18.692970: step 5762, loss 0.0718507, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:19.032127: step 5763, loss 0.0687694, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:19.386948: step 5764, loss 0.0526298, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:19.704917: step 5765, loss 0.0441885, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:20.053101: step 5766, loss 0.0570478, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:20.432850: step 5767, loss 0.0512542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:20.819710: step 5768, loss 0.130533, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:21.100840: step 5769, loss 0.0520483, acc 1, learning_rate 0.0001
2017-10-10T15:25:21.404809: step 5770, loss 0.0701863, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:21.739895: step 5771, loss 0.0245724, acc 1, learning_rate 0.0001
2017-10-10T15:25:22.079356: step 5772, loss 0.0442508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:22.423354: step 5773, loss 0.0609723, acc 1, learning_rate 0.0001
2017-10-10T15:25:22.771119: step 5774, loss 0.0457443, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:23.173696: step 5775, loss 0.0395605, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:23.519679: step 5776, loss 0.0534446, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:23.824409: step 5777, loss 0.0406564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:24.205841: step 5778, loss 0.0743329, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:24.537294: step 5779, loss 0.0846318, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:24.865161: step 5780, loss 0.114451, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:25.304009: step 5781, loss 0.0484878, acc 1, learning_rate 0.0001
2017-10-10T15:25:25.577007: step 5782, loss 0.0209139, acc 1, learning_rate 0.0001
2017-10-10T15:25:25.888815: step 5783, loss 0.0234986, acc 1, learning_rate 0.0001
2017-10-10T15:25:26.151444: step 5784, loss 0.056918, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:26.431518: step 5785, loss 0.0224994, acc 1, learning_rate 0.0001
2017-10-10T15:25:26.719599: step 5786, loss 0.0972592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:27.057285: step 5787, loss 0.0233904, acc 1, learning_rate 0.0001
2017-10-10T15:25:27.385027: step 5788, loss 0.070732, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:27.720804: step 5789, loss 0.127083, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:28.034320: step 5790, loss 0.0350874, acc 1, learning_rate 0.0001
2017-10-10T15:25:28.361143: step 5791, loss 0.0412856, acc 1, learning_rate 0.0001
2017-10-10T15:25:28.708954: step 5792, loss 0.0715698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:29.065077: step 5793, loss 0.0521766, acc 1, learning_rate 0.0001
2017-10-10T15:25:29.383547: step 5794, loss 0.0921299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:29.713026: step 5795, loss 0.0863521, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:30.031395: step 5796, loss 0.0330097, acc 1, learning_rate 0.0001
2017-10-10T15:25:30.380818: step 5797, loss 0.0353247, acc 1, learning_rate 0.0001
2017-10-10T15:25:30.730860: step 5798, loss 0.044637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:31.179175: step 5799, loss 0.0476857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:31.484891: step 5800, loss 0.0328323, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:32.176298: step 5800, loss 0.212443, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5800

2017-10-10T15:25:33.391623: step 5801, loss 0.133221, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:33.743181: step 5802, loss 0.0957171, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:34.083857: step 5803, loss 0.163053, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:34.406511: step 5804, loss 0.0186982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:34.737135: step 5805, loss 0.0186682, acc 1, learning_rate 0.0001
2017-10-10T15:25:35.043863: step 5806, loss 0.0896041, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:35.386801: step 5807, loss 0.0711953, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:35.734342: step 5808, loss 0.0818123, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:36.095820: step 5809, loss 0.082569, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:36.397003: step 5810, loss 0.0858524, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:36.695548: step 5811, loss 0.0153881, acc 1, learning_rate 0.0001
2017-10-10T15:25:36.981845: step 5812, loss 0.0596795, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:37.320516: step 5813, loss 0.0682933, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:37.683091: step 5814, loss 0.0702902, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:38.019827: step 5815, loss 0.0735823, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:38.349150: step 5816, loss 0.0950026, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:38.682255: step 5817, loss 0.0289971, acc 1, learning_rate 0.0001
2017-10-10T15:25:39.004363: step 5818, loss 0.116255, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:39.428693: step 5819, loss 0.0646026, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:39.667664: step 5820, loss 0.097976, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:39.947985: step 5821, loss 0.0445497, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:40.207278: step 5822, loss 0.0590494, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:40.497064: step 5823, loss 0.0889786, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:40.876918: step 5824, loss 0.108541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:41.239547: step 5825, loss 0.0771557, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:41.644843: step 5826, loss 0.0336291, acc 1, learning_rate 0.0001
2017-10-10T15:25:41.946234: step 5827, loss 0.0188426, acc 1, learning_rate 0.0001
2017-10-10T15:25:42.257922: step 5828, loss 0.0878648, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:42.596995: step 5829, loss 0.0880003, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:42.991855: step 5830, loss 0.017065, acc 1, learning_rate 0.0001
2017-10-10T15:25:43.314071: step 5831, loss 0.0706219, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:43.708981: step 5832, loss 0.109857, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:44.040723: step 5833, loss 0.0632618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:44.377887: step 5834, loss 0.138882, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:44.764904: step 5835, loss 0.0225096, acc 1, learning_rate 0.0001
2017-10-10T15:25:45.051882: step 5836, loss 0.0620993, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:45.387797: step 5837, loss 0.0464015, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:45.666391: step 5838, loss 0.0173036, acc 1, learning_rate 0.0001
2017-10-10T15:25:46.008631: step 5839, loss 0.0752796, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:46.303585: step 5840, loss 0.0568309, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:46.970855: step 5840, loss 0.210343, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5840

2017-10-10T15:25:48.261239: step 5841, loss 0.0391255, acc 1, learning_rate 0.0001
2017-10-10T15:25:48.585696: step 5842, loss 0.0119041, acc 1, learning_rate 0.0001
2017-10-10T15:25:48.905093: step 5843, loss 0.109313, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:49.250499: step 5844, loss 0.0808941, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:49.610913: step 5845, loss 0.121877, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:49.952906: step 5846, loss 0.0439973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:50.296951: step 5847, loss 0.0936934, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:50.662331: step 5848, loss 0.0640142, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:51.008457: step 5849, loss 0.0155451, acc 1, learning_rate 0.0001
2017-10-10T15:25:51.329885: step 5850, loss 0.0360887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:51.621008: step 5851, loss 0.0513093, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:52.012060: step 5852, loss 0.109476, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:52.285478: step 5853, loss 0.0392245, acc 1, learning_rate 0.0001
2017-10-10T15:25:52.560844: step 5854, loss 0.0784418, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:53.009400: step 5855, loss 0.0936113, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:53.308805: step 5856, loss 0.0408402, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:53.580800: step 5857, loss 0.03643, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:53.864932: step 5858, loss 0.0459211, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:54.184920: step 5859, loss 0.0656424, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:54.481560: step 5860, loss 0.113075, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:54.811060: step 5861, loss 0.0422548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:55.197175: step 5862, loss 0.0403293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:55.508574: step 5863, loss 0.0407369, acc 1, learning_rate 0.0001
2017-10-10T15:25:55.852336: step 5864, loss 0.0447146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:56.200869: step 5865, loss 0.0710369, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:56.541178: step 5866, loss 0.0557356, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:56.895400: step 5867, loss 0.0635463, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:57.247753: step 5868, loss 0.0869222, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:57.544937: step 5869, loss 0.0356401, acc 1, learning_rate 0.0001
2017-10-10T15:25:57.879840: step 5870, loss 0.0169955, acc 1, learning_rate 0.0001
2017-10-10T15:25:58.240969: step 5871, loss 0.1121, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:58.628335: step 5872, loss 0.0945464, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:58.909893: step 5873, loss 0.066737, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:59.196449: step 5874, loss 0.0934782, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:59.515536: step 5875, loss 0.104072, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:59.846581: step 5876, loss 0.0942402, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:00.172431: step 5877, loss 0.00900668, acc 1, learning_rate 0.0001
2017-10-10T15:26:00.528924: step 5878, loss 0.0445832, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:00.829082: step 5879, loss 0.110742, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:01.103634: step 5880, loss 0.0765223, acc 0.960784, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:01.926136: step 5880, loss 0.211739, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5880

2017-10-10T15:26:03.109168: step 5881, loss 0.0651144, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:03.399003: step 5882, loss 0.0709227, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:03.748921: step 5883, loss 0.0670219, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:04.077274: step 5884, loss 0.0457694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:04.423065: step 5885, loss 0.0210494, acc 1, learning_rate 0.0001
2017-10-10T15:26:04.701501: step 5886, loss 0.0701884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:05.113487: step 5887, loss 0.0527204, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:05.446248: step 5888, loss 0.0513525, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:05.776856: step 5889, loss 0.065552, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:06.120870: step 5890, loss 0.0376677, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:06.525294: step 5891, loss 0.0359461, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:06.868967: step 5892, loss 0.0944088, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:07.135007: step 5893, loss 0.0523509, acc 1, learning_rate 0.0001
2017-10-10T15:26:07.399324: step 5894, loss 0.0201492, acc 1, learning_rate 0.0001
2017-10-10T15:26:07.683647: step 5895, loss 0.0102624, acc 1, learning_rate 0.0001
2017-10-10T15:26:08.000856: step 5896, loss 0.0424934, acc 1, learning_rate 0.0001
2017-10-10T15:26:08.333073: step 5897, loss 0.1206, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:08.690485: step 5898, loss 0.0254218, acc 1, learning_rate 0.0001
2017-10-10T15:26:09.032024: step 5899, loss 0.0625427, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:09.423568: step 5900, loss 0.0367331, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:09.692451: step 5901, loss 0.0709895, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:10.001990: step 5902, loss 0.112977, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:10.348957: step 5903, loss 0.0119877, acc 1, learning_rate 0.0001
2017-10-10T15:26:10.660952: step 5904, loss 0.15041, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:11.009023: step 5905, loss 0.0751727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:11.333015: step 5906, loss 0.0257555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:11.737916: step 5907, loss 0.0425552, acc 1, learning_rate 0.0001
2017-10-10T15:26:12.057692: step 5908, loss 0.0622842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:12.488233: step 5909, loss 0.0230913, acc 1, learning_rate 0.0001
2017-10-10T15:26:12.800243: step 5910, loss 0.0770579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:13.093441: step 5911, loss 0.061313, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:13.436383: step 5912, loss 0.0403315, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:13.788940: step 5913, loss 0.0345751, acc 1, learning_rate 0.0001
2017-10-10T15:26:14.118401: step 5914, loss 0.0999954, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:14.429116: step 5915, loss 0.0389257, acc 1, learning_rate 0.0001
2017-10-10T15:26:14.756987: step 5916, loss 0.0447482, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:15.081056: step 5917, loss 0.0798101, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:15.389734: step 5918, loss 0.0907277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:15.722853: step 5919, loss 0.031602, acc 1, learning_rate 0.0001
2017-10-10T15:26:16.037043: step 5920, loss 0.0738252, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:16.753146: step 5920, loss 0.211655, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5920

2017-10-10T15:26:17.982768: step 5921, loss 0.0448546, acc 1, learning_rate 0.0001
2017-10-10T15:26:18.322924: step 5922, loss 0.0593991, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:18.640458: step 5923, loss 0.0606872, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:18.979469: step 5924, loss 0.0324562, acc 1, learning_rate 0.0001
2017-10-10T15:26:19.353793: step 5925, loss 0.0642516, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:19.713154: step 5926, loss 0.11442, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:20.124366: step 5927, loss 0.0675747, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:20.511948: step 5928, loss 0.0597785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:20.787984: step 5929, loss 0.0570616, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:20.996240: step 5930, loss 0.0448066, acc 1, learning_rate 0.0001
2017-10-10T15:26:21.264936: step 5931, loss 0.0144279, acc 1, learning_rate 0.0001
2017-10-10T15:26:21.635239: step 5932, loss 0.194067, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:21.945715: step 5933, loss 0.105831, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:22.276569: step 5934, loss 0.0338355, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:22.683781: step 5935, loss 0.0700689, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:22.978348: step 5936, loss 0.0478236, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:23.240837: step 5937, loss 0.065165, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:23.568403: step 5938, loss 0.0720195, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:23.921273: step 5939, loss 0.0785663, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:24.285039: step 5940, loss 0.0504465, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:24.608997: step 5941, loss 0.0517008, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:24.904958: step 5942, loss 0.0255102, acc 1, learning_rate 0.0001
2017-10-10T15:26:25.219611: step 5943, loss 0.0681929, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:25.565610: step 5944, loss 0.0985719, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:25.893773: step 5945, loss 0.0709593, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:26.200779: step 5946, loss 0.0313389, acc 1, learning_rate 0.0001
2017-10-10T15:26:26.525225: step 5947, loss 0.0434262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:26.833521: step 5948, loss 0.121094, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:27.144916: step 5949, loss 0.0995567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:27.485213: step 5950, loss 0.0642652, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:27.819673: step 5951, loss 0.0302413, acc 1, learning_rate 0.0001
2017-10-10T15:26:28.184938: step 5952, loss 0.0431532, acc 1, learning_rate 0.0001
2017-10-10T15:26:28.513007: step 5953, loss 0.0435793, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:28.840836: step 5954, loss 0.10251, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:29.216813: step 5955, loss 0.0183275, acc 1, learning_rate 0.0001
2017-10-10T15:26:29.529942: step 5956, loss 0.0757557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:29.890153: step 5957, loss 0.112928, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:30.174253: step 5958, loss 0.118258, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:30.502835: step 5959, loss 0.0777107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:30.842700: step 5960, loss 0.0325555, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:31.598861: step 5960, loss 0.209654, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-5960

2017-10-10T15:26:32.786918: step 5961, loss 0.033746, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:33.085943: step 5962, loss 0.078611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:33.365553: step 5963, loss 0.113359, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:33.683302: step 5964, loss 0.126841, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:34.113005: step 5965, loss 0.0660266, acc 1, learning_rate 0.0001
2017-10-10T15:26:34.459190: step 5966, loss 0.0165045, acc 1, learning_rate 0.0001
2017-10-10T15:26:34.732485: step 5967, loss 0.0700618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:35.013683: step 5968, loss 0.0649858, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:35.286082: step 5969, loss 0.030315, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:35.646808: step 5970, loss 0.0271723, acc 1, learning_rate 0.0001
2017-10-10T15:26:35.918285: step 5971, loss 0.04203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:36.209822: step 5972, loss 0.0686977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:36.556205: step 5973, loss 0.0282724, acc 1, learning_rate 0.0001
2017-10-10T15:26:36.872854: step 5974, loss 0.0450348, acc 1, learning_rate 0.0001
2017-10-10T15:26:37.207181: step 5975, loss 0.124813, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:37.568842: step 5976, loss 0.0666592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:37.900853: step 5977, loss 0.054889, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:38.201261: step 5978, loss 0.0497496, acc 1, learning_rate 0.0001
2017-10-10T15:26:38.546366: step 5979, loss 0.0553637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:38.835550: step 5980, loss 0.0443744, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:39.180159: step 5981, loss 0.0730395, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:39.524865: step 5982, loss 0.0167597, acc 1, learning_rate 0.0001
2017-10-10T15:26:39.859253: step 5983, loss 0.0690865, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:40.212855: step 5984, loss 0.0545651, acc 1, learning_rate 0.0001
2017-10-10T15:26:40.594082: step 5985, loss 0.07394, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:40.932827: step 5986, loss 0.0800079, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:41.269990: step 5987, loss 0.131808, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:41.611260: step 5988, loss 0.0363247, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:41.973794: step 5989, loss 0.0771971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:42.326439: step 5990, loss 0.0421592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:42.752862: step 5991, loss 0.046396, acc 1, learning_rate 0.0001
2017-10-10T15:26:43.107601: step 5992, loss 0.039785, acc 1, learning_rate 0.0001
2017-10-10T15:26:43.348849: step 5993, loss 0.0298638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:43.616920: step 5994, loss 0.0294626, acc 1, learning_rate 0.0001
2017-10-10T15:26:43.907544: step 5995, loss 0.128836, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:44.242642: step 5996, loss 0.0775406, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:44.584852: step 5997, loss 0.0536957, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:44.930138: step 5998, loss 0.031956, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:45.305039: step 5999, loss 0.096477, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:45.635543: step 6000, loss 0.0321717, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:46.266353: step 6000, loss 0.210834, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6000

2017-10-10T15:26:47.600978: step 6001, loss 0.0749407, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:47.996821: step 6002, loss 0.0282385, acc 1, learning_rate 0.0001
2017-10-10T15:26:48.286834: step 6003, loss 0.0702013, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:48.567423: step 6004, loss 0.123954, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:48.868610: step 6005, loss 0.0298119, acc 1, learning_rate 0.0001
2017-10-10T15:26:49.236811: step 6006, loss 0.0230582, acc 1, learning_rate 0.0001
2017-10-10T15:26:49.611042: step 6007, loss 0.0484444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:49.976901: step 6008, loss 0.0569563, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:50.355555: step 6009, loss 0.0354384, acc 1, learning_rate 0.0001
2017-10-10T15:26:50.691797: step 6010, loss 0.0569409, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:51.048141: step 6011, loss 0.0610188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:51.359171: step 6012, loss 0.0443945, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:51.706983: step 6013, loss 0.0841173, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:52.045013: step 6014, loss 0.0254708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:52.388112: step 6015, loss 0.0169738, acc 1, learning_rate 0.0001
2017-10-10T15:26:52.731994: step 6016, loss 0.100741, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:53.010316: step 6017, loss 0.0691985, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:53.447443: step 6018, loss 0.0133459, acc 1, learning_rate 0.0001
2017-10-10T15:26:53.728991: step 6019, loss 0.041501, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:53.978989: step 6020, loss 0.0391346, acc 1, learning_rate 0.0001
2017-10-10T15:26:54.212096: step 6021, loss 0.0299625, acc 1, learning_rate 0.0001
2017-10-10T15:26:54.496135: step 6022, loss 0.0628348, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:54.788838: step 6023, loss 0.0755181, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:55.123902: step 6024, loss 0.0326598, acc 1, learning_rate 0.0001
2017-10-10T15:26:55.471430: step 6025, loss 0.0630682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:55.828456: step 6026, loss 0.0453703, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:56.166034: step 6027, loss 0.0731309, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:56.501567: step 6028, loss 0.0965221, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:56.848581: step 6029, loss 0.0364243, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:57.179903: step 6030, loss 0.0525697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:57.551313: step 6031, loss 0.0476545, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:57.885046: step 6032, loss 0.0609747, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:58.204945: step 6033, loss 0.106711, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:58.571465: step 6034, loss 0.0552784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:58.916206: step 6035, loss 0.0336784, acc 1, learning_rate 0.0001
2017-10-10T15:26:59.289645: step 6036, loss 0.0668898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:59.646292: step 6037, loss 0.0198797, acc 1, learning_rate 0.0001
2017-10-10T15:26:59.983736: step 6038, loss 0.107345, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:00.296411: step 6039, loss 0.122938, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:00.650458: step 6040, loss 0.0744563, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:01.320526: step 6040, loss 0.208262, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6040

2017-10-10T15:27:02.366036: step 6041, loss 0.043855, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:02.733810: step 6042, loss 0.0287996, acc 1, learning_rate 0.0001
2017-10-10T15:27:03.068876: step 6043, loss 0.0545304, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:03.403098: step 6044, loss 0.0290415, acc 1, learning_rate 0.0001
2017-10-10T15:27:03.829180: step 6045, loss 0.0462, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:04.154640: step 6046, loss 0.0288855, acc 1, learning_rate 0.0001
2017-10-10T15:27:04.474714: step 6047, loss 0.0302067, acc 1, learning_rate 0.0001
2017-10-10T15:27:04.801299: step 6048, loss 0.122877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:05.093059: step 6049, loss 0.0750321, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:05.458908: step 6050, loss 0.0569341, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:05.748584: step 6051, loss 0.0181825, acc 1, learning_rate 0.0001
2017-10-10T15:27:06.079641: step 6052, loss 0.0406602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:06.453447: step 6053, loss 0.0824688, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:06.824112: step 6054, loss 0.0590768, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:07.135574: step 6055, loss 0.0350004, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:07.468882: step 6056, loss 0.0407935, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:07.800938: step 6057, loss 0.0138702, acc 1, learning_rate 0.0001
2017-10-10T15:27:08.082048: step 6058, loss 0.0762613, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:08.451506: step 6059, loss 0.0185372, acc 1, learning_rate 0.0001
2017-10-10T15:27:08.760889: step 6060, loss 0.0850676, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:09.137503: step 6061, loss 0.038876, acc 1, learning_rate 0.0001
2017-10-10T15:27:09.500379: step 6062, loss 0.0559955, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:09.831449: step 6063, loss 0.0290758, acc 1, learning_rate 0.0001
2017-10-10T15:27:10.147274: step 6064, loss 0.145477, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:10.496899: step 6065, loss 0.0791198, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:10.873180: step 6066, loss 0.0517018, acc 1, learning_rate 0.0001
2017-10-10T15:27:11.237207: step 6067, loss 0.135667, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:11.594072: step 6068, loss 0.0353805, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:11.931197: step 6069, loss 0.0242908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:12.304973: step 6070, loss 0.0833019, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:12.632631: step 6071, loss 0.0570566, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:12.962724: step 6072, loss 0.0729333, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:13.329777: step 6073, loss 0.0581146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:13.653009: step 6074, loss 0.0476496, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:14.065070: step 6075, loss 0.0583662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:14.343727: step 6076, loss 0.0731706, acc 0.960784, learning_rate 0.0001
2017-10-10T15:27:14.646333: step 6077, loss 0.0519314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:15.032416: step 6078, loss 0.0409527, acc 1, learning_rate 0.0001
2017-10-10T15:27:15.438050: step 6079, loss 0.0307293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:15.668957: step 6080, loss 0.0275926, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:16.177831: step 6080, loss 0.211072, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6080

2017-10-10T15:27:17.352174: step 6081, loss 0.0538869, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:17.670613: step 6082, loss 0.0926301, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:18.029314: step 6083, loss 0.0589121, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:18.339429: step 6084, loss 0.0449558, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:18.653051: step 6085, loss 0.0864596, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:18.949237: step 6086, loss 0.0759616, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:19.264800: step 6087, loss 0.100396, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:19.585185: step 6088, loss 0.05769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:19.928012: step 6089, loss 0.00736788, acc 1, learning_rate 0.0001
2017-10-10T15:27:20.281937: step 6090, loss 0.0649128, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:20.616842: step 6091, loss 0.0343495, acc 1, learning_rate 0.0001
2017-10-10T15:27:20.940773: step 6092, loss 0.0635459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:21.266681: step 6093, loss 0.0886347, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:21.621096: step 6094, loss 0.0627314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:21.907268: step 6095, loss 0.129778, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:22.273185: step 6096, loss 0.0450274, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:22.631350: step 6097, loss 0.0834006, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:22.943671: step 6098, loss 0.103375, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:23.295033: step 6099, loss 0.114695, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:23.663403: step 6100, loss 0.0207903, acc 1, learning_rate 0.0001
2017-10-10T15:27:24.080855: step 6101, loss 0.0269607, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:24.388895: step 6102, loss 0.13263, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:24.700341: step 6103, loss 0.0439977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:24.977575: step 6104, loss 0.0317225, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:25.337072: step 6105, loss 0.044519, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:25.688946: step 6106, loss 0.0204684, acc 1, learning_rate 0.0001
2017-10-10T15:27:26.077277: step 6107, loss 0.118849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:26.368986: step 6108, loss 0.0492712, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:26.663655: step 6109, loss 0.102112, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:26.976589: step 6110, loss 0.0887176, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:27.317440: step 6111, loss 0.0522671, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:27.682782: step 6112, loss 0.0517948, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:28.025172: step 6113, loss 0.0402888, acc 1, learning_rate 0.0001
2017-10-10T15:27:28.369360: step 6114, loss 0.0359395, acc 1, learning_rate 0.0001
2017-10-10T15:27:28.686519: step 6115, loss 0.0363144, acc 1, learning_rate 0.0001
2017-10-10T15:27:29.104483: step 6116, loss 0.0669075, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:29.408595: step 6117, loss 0.0506257, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:29.690444: step 6118, loss 0.13929, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:29.968509: step 6119, loss 0.0932085, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:30.331452: step 6120, loss 0.0528151, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:31.014933: step 6120, loss 0.212368, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6120

2017-10-10T15:27:32.228073: step 6121, loss 0.0399331, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:32.551548: step 6122, loss 0.040518, acc 1, learning_rate 0.0001
2017-10-10T15:27:32.848230: step 6123, loss 0.0410676, acc 1, learning_rate 0.0001
2017-10-10T15:27:33.197408: step 6124, loss 0.0617453, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:33.498961: step 6125, loss 0.102249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:33.863500: step 6126, loss 0.103217, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:34.265119: step 6127, loss 0.0918215, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:34.589571: step 6128, loss 0.0337557, acc 1, learning_rate 0.0001
2017-10-10T15:27:34.889822: step 6129, loss 0.0439996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:35.228592: step 6130, loss 0.0708099, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:35.550817: step 6131, loss 0.031713, acc 1, learning_rate 0.0001
2017-10-10T15:27:35.907200: step 6132, loss 0.0866385, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:36.250568: step 6133, loss 0.0670982, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:36.573155: step 6134, loss 0.0485311, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:36.931975: step 6135, loss 0.0557794, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:37.276994: step 6136, loss 0.0905843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:37.568995: step 6137, loss 0.104394, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:37.854536: step 6138, loss 0.111038, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:38.193291: step 6139, loss 0.101011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:38.576313: step 6140, loss 0.0956787, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:38.906698: step 6141, loss 0.0448879, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:39.232631: step 6142, loss 0.106372, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:39.612934: step 6143, loss 0.0280069, acc 1, learning_rate 0.0001
2017-10-10T15:27:39.949328: step 6144, loss 0.119714, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:40.284941: step 6145, loss 0.0648823, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:40.623300: step 6146, loss 0.118657, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:40.967088: step 6147, loss 0.0500204, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:41.292334: step 6148, loss 0.068045, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:41.612831: step 6149, loss 0.0457192, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:41.944869: step 6150, loss 0.0540797, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:42.263612: step 6151, loss 0.0243017, acc 1, learning_rate 0.0001
2017-10-10T15:27:42.692111: step 6152, loss 0.0462685, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:42.988784: step 6153, loss 0.0639688, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:43.272803: step 6154, loss 0.0637371, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:43.550524: step 6155, loss 0.0503226, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:43.839077: step 6156, loss 0.0529578, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:44.163100: step 6157, loss 0.0397211, acc 1, learning_rate 0.0001
2017-10-10T15:27:44.571394: step 6158, loss 0.0350607, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:44.876907: step 6159, loss 0.0365692, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:45.168909: step 6160, loss 0.0812656, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:45.866681: step 6160, loss 0.211683, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6160

2017-10-10T15:27:47.102984: step 6161, loss 0.0372414, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:47.405424: step 6162, loss 0.0337306, acc 1, learning_rate 0.0001
2017-10-10T15:27:47.779375: step 6163, loss 0.0594935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:48.128593: step 6164, loss 0.0965102, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:48.469502: step 6165, loss 0.109401, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:48.764816: step 6166, loss 0.0696283, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:49.066554: step 6167, loss 0.0259467, acc 1, learning_rate 0.0001
2017-10-10T15:27:49.509995: step 6168, loss 0.0206397, acc 1, learning_rate 0.0001
2017-10-10T15:27:49.885999: step 6169, loss 0.129627, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:50.212824: step 6170, loss 0.0284896, acc 1, learning_rate 0.0001
2017-10-10T15:27:50.560856: step 6171, loss 0.055809, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:50.896087: step 6172, loss 0.0233358, acc 1, learning_rate 0.0001
2017-10-10T15:27:51.202655: step 6173, loss 0.0519762, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:51.549390: step 6174, loss 0.0291405, acc 1, learning_rate 0.0001
2017-10-10T15:27:51.893798: step 6175, loss 0.0268843, acc 1, learning_rate 0.0001
2017-10-10T15:27:52.200293: step 6176, loss 0.0262603, acc 1, learning_rate 0.0001
2017-10-10T15:27:52.544379: step 6177, loss 0.0443731, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:52.886096: step 6178, loss 0.118269, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:53.208980: step 6179, loss 0.0683094, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:53.570323: step 6180, loss 0.0742086, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:53.893028: step 6181, loss 0.165476, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:54.236879: step 6182, loss 0.0218625, acc 1, learning_rate 0.0001
2017-10-10T15:27:54.629808: step 6183, loss 0.0764027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:54.908874: step 6184, loss 0.0777936, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:55.189200: step 6185, loss 0.101487, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:55.508908: step 6186, loss 0.029588, acc 1, learning_rate 0.0001
2017-10-10T15:27:55.804484: step 6187, loss 0.0838529, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:56.273739: step 6188, loss 0.114049, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:56.543874: step 6189, loss 0.0303776, acc 1, learning_rate 0.0001
2017-10-10T15:27:56.837718: step 6190, loss 0.115802, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:57.112830: step 6191, loss 0.0346785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:57.440657: step 6192, loss 0.0308785, acc 1, learning_rate 0.0001
2017-10-10T15:27:57.776871: step 6193, loss 0.075366, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:58.129480: step 6194, loss 0.144055, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:58.478599: step 6195, loss 0.0647481, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:58.795558: step 6196, loss 0.065937, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:59.108886: step 6197, loss 0.0434931, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:59.500188: step 6198, loss 0.0412156, acc 1, learning_rate 0.0001
2017-10-10T15:27:59.804928: step 6199, loss 0.0203597, acc 1, learning_rate 0.0001
2017-10-10T15:28:00.128833: step 6200, loss 0.092168, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:00.799424: step 6200, loss 0.211757, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6200

2017-10-10T15:28:02.065241: step 6201, loss 0.0547874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:02.440508: step 6202, loss 0.0910788, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:02.812618: step 6203, loss 0.0735488, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:03.132840: step 6204, loss 0.073047, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:03.483001: step 6205, loss 0.059834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:03.828071: step 6206, loss 0.0832818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:04.152337: step 6207, loss 0.12185, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:04.501232: step 6208, loss 0.0798499, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:04.905228: step 6209, loss 0.0721615, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:05.220840: step 6210, loss 0.061907, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:05.569156: step 6211, loss 0.104267, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:05.832858: step 6212, loss 0.0366717, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:06.187931: step 6213, loss 0.0621602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:06.538816: step 6214, loss 0.0798117, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:06.883223: step 6215, loss 0.07983, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:07.263616: step 6216, loss 0.070711, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:07.569542: step 6217, loss 0.0688457, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:07.912855: step 6218, loss 0.042707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:08.264845: step 6219, loss 0.0327217, acc 1, learning_rate 0.0001
2017-10-10T15:28:08.676237: step 6220, loss 0.0833165, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:08.993119: step 6221, loss 0.0844481, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:09.293259: step 6222, loss 0.0304016, acc 1, learning_rate 0.0001
2017-10-10T15:28:09.737886: step 6223, loss 0.033806, acc 1, learning_rate 0.0001
2017-10-10T15:28:09.970665: step 6224, loss 0.0631569, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:10.275160: step 6225, loss 0.0529384, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:10.584571: step 6226, loss 0.020214, acc 1, learning_rate 0.0001
2017-10-10T15:28:10.892183: step 6227, loss 0.039662, acc 1, learning_rate 0.0001
2017-10-10T15:28:11.136828: step 6228, loss 0.101187, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:11.475981: step 6229, loss 0.126278, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:11.819522: step 6230, loss 0.0722858, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:12.156872: step 6231, loss 0.0797364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:12.457030: step 6232, loss 0.0458363, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:12.753763: step 6233, loss 0.0605429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:13.112437: step 6234, loss 0.0309624, acc 1, learning_rate 0.0001
2017-10-10T15:28:13.467809: step 6235, loss 0.0414484, acc 1, learning_rate 0.0001
2017-10-10T15:28:13.782507: step 6236, loss 0.112826, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:14.118544: step 6237, loss 0.115722, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:14.422358: step 6238, loss 0.0521224, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:14.765328: step 6239, loss 0.0286154, acc 1, learning_rate 0.0001
2017-10-10T15:28:15.089275: step 6240, loss 0.067605, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:15.706161: step 6240, loss 0.209667, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6240

2017-10-10T15:28:16.895924: step 6241, loss 0.0246361, acc 1, learning_rate 0.0001
2017-10-10T15:28:17.265212: step 6242, loss 0.109745, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:17.612820: step 6243, loss 0.0201897, acc 1, learning_rate 0.0001
2017-10-10T15:28:17.938448: step 6244, loss 0.0572891, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:18.320837: step 6245, loss 0.0302873, acc 1, learning_rate 0.0001
2017-10-10T15:28:18.670093: step 6246, loss 0.0441932, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:19.041010: step 6247, loss 0.0181773, acc 1, learning_rate 0.0001
2017-10-10T15:28:19.367971: step 6248, loss 0.0701149, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:19.725269: step 6249, loss 0.0735741, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:20.059062: step 6250, loss 0.031351, acc 1, learning_rate 0.0001
2017-10-10T15:28:20.433894: step 6251, loss 0.0781433, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:20.816564: step 6252, loss 0.0945446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:21.162478: step 6253, loss 0.0282122, acc 1, learning_rate 0.0001
2017-10-10T15:28:21.553197: step 6254, loss 0.12571, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:21.865966: step 6255, loss 0.168021, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:22.296887: step 6256, loss 0.113097, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:22.610938: step 6257, loss 0.0653022, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:22.932018: step 6258, loss 0.0233505, acc 1, learning_rate 0.0001
2017-10-10T15:28:23.348032: step 6259, loss 0.0230142, acc 1, learning_rate 0.0001
2017-10-10T15:28:23.621026: step 6260, loss 0.019205, acc 1, learning_rate 0.0001
2017-10-10T15:28:23.895169: step 6261, loss 0.0610599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:24.174829: step 6262, loss 0.0696952, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:24.549757: step 6263, loss 0.0438042, acc 1, learning_rate 0.0001
2017-10-10T15:28:24.909094: step 6264, loss 0.050165, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:25.223726: step 6265, loss 0.0868919, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:25.580884: step 6266, loss 0.0439693, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:25.996681: step 6267, loss 0.10732, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:26.297652: step 6268, loss 0.0335083, acc 1, learning_rate 0.0001
2017-10-10T15:28:26.560928: step 6269, loss 0.114707, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:26.861308: step 6270, loss 0.094126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:27.156842: step 6271, loss 0.121427, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:27.414755: step 6272, loss 0.0297303, acc 1, learning_rate 0.0001
2017-10-10T15:28:27.699692: step 6273, loss 0.0206498, acc 1, learning_rate 0.0001
2017-10-10T15:28:27.967042: step 6274, loss 0.0540293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:28.240979: step 6275, loss 0.076071, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:28.511483: step 6276, loss 0.0890506, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:28.771239: step 6277, loss 0.0854632, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:29.021342: step 6278, loss 0.135927, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:29.288842: step 6279, loss 0.0400809, acc 1, learning_rate 0.0001
2017-10-10T15:28:29.571215: step 6280, loss 0.0933973, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:30.120684: step 6280, loss 0.21135, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6280

2017-10-10T15:28:31.298557: step 6281, loss 0.0699286, acc 1, learning_rate 0.0001
2017-10-10T15:28:31.521294: step 6282, loss 0.0653522, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:31.749508: step 6283, loss 0.0833948, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:31.962439: step 6284, loss 0.047786, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:32.248313: step 6285, loss 0.129692, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:32.499140: step 6286, loss 0.0316104, acc 1, learning_rate 0.0001
2017-10-10T15:28:32.804850: step 6287, loss 0.0545995, acc 1, learning_rate 0.0001
2017-10-10T15:28:33.080679: step 6288, loss 0.0957763, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:33.356869: step 6289, loss 0.113973, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:33.606641: step 6290, loss 0.0594258, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:33.887482: step 6291, loss 0.024287, acc 1, learning_rate 0.0001
2017-10-10T15:28:34.152634: step 6292, loss 0.0449712, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:34.449507: step 6293, loss 0.0206365, acc 1, learning_rate 0.0001
2017-10-10T15:28:34.805109: step 6294, loss 0.0980811, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:35.062519: step 6295, loss 0.0176981, acc 1, learning_rate 0.0001
2017-10-10T15:28:35.288983: step 6296, loss 0.0454208, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:35.484802: step 6297, loss 0.122278, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:35.692969: step 6298, loss 0.0761722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:35.967665: step 6299, loss 0.0840101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:36.250140: step 6300, loss 0.0503884, acc 1, learning_rate 0.0001
2017-10-10T15:28:36.512993: step 6301, loss 0.101569, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:36.759616: step 6302, loss 0.0314077, acc 1, learning_rate 0.0001
2017-10-10T15:28:37.039155: step 6303, loss 0.0531764, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:37.305721: step 6304, loss 0.0197942, acc 1, learning_rate 0.0001
2017-10-10T15:28:37.563947: step 6305, loss 0.0256595, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:37.829821: step 6306, loss 0.0741313, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:38.100067: step 6307, loss 0.0403161, acc 1, learning_rate 0.0001
2017-10-10T15:28:38.361388: step 6308, loss 0.0678262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:38.616886: step 6309, loss 0.0716392, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:38.864197: step 6310, loss 0.0119554, acc 1, learning_rate 0.0001
2017-10-10T15:28:39.110593: step 6311, loss 0.120664, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:39.397202: step 6312, loss 0.0828164, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:39.673151: step 6313, loss 0.0482221, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:40.009120: step 6314, loss 0.0427791, acc 1, learning_rate 0.0001
2017-10-10T15:28:40.231311: step 6315, loss 0.127042, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:40.444440: step 6316, loss 0.0577592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:40.687707: step 6317, loss 0.0871106, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:40.947959: step 6318, loss 0.0895917, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:41.229820: step 6319, loss 0.0436281, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:41.516873: step 6320, loss 0.051568, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:42.074122: step 6320, loss 0.211122, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6320

2017-10-10T15:28:43.230061: step 6321, loss 0.0625194, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:43.519986: step 6322, loss 0.0336198, acc 1, learning_rate 0.0001
2017-10-10T15:28:43.795677: step 6323, loss 0.0264882, acc 1, learning_rate 0.0001
2017-10-10T15:28:44.066627: step 6324, loss 0.0687685, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:44.332893: step 6325, loss 0.0270153, acc 1, learning_rate 0.0001
2017-10-10T15:28:44.618894: step 6326, loss 0.0170312, acc 1, learning_rate 0.0001
2017-10-10T15:28:44.871799: step 6327, loss 0.0550027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:45.123162: step 6328, loss 0.0438699, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:45.414653: step 6329, loss 0.0299098, acc 1, learning_rate 0.0001
2017-10-10T15:28:45.680955: step 6330, loss 0.0281459, acc 1, learning_rate 0.0001
2017-10-10T15:28:46.020816: step 6331, loss 0.0898643, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:46.220143: step 6332, loss 0.0685049, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:46.438472: step 6333, loss 0.100373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:46.635404: step 6334, loss 0.0371949, acc 1, learning_rate 0.0001
2017-10-10T15:28:46.828620: step 6335, loss 0.0644131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:47.112930: step 6336, loss 0.0518586, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:47.391041: step 6337, loss 0.0470869, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:47.674839: step 6338, loss 0.0263635, acc 1, learning_rate 0.0001
2017-10-10T15:28:47.947803: step 6339, loss 0.0656863, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:48.237243: step 6340, loss 0.0456656, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:48.568916: step 6341, loss 0.0345303, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:48.833051: step 6342, loss 0.0578933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:49.062272: step 6343, loss 0.0875427, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:49.280755: step 6344, loss 0.0727004, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:49.508277: step 6345, loss 0.0304071, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:49.808364: step 6346, loss 0.0843814, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:50.082641: step 6347, loss 0.0456801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:50.321094: step 6348, loss 0.0966013, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:50.585054: step 6349, loss 0.111678, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:50.876833: step 6350, loss 0.070864, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:51.131370: step 6351, loss 0.0609387, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:51.382022: step 6352, loss 0.0618873, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:51.622391: step 6353, loss 0.0838901, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:51.900385: step 6354, loss 0.0536013, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:52.186629: step 6355, loss 0.0527413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:52.448889: step 6356, loss 0.0781718, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:52.709561: step 6357, loss 0.0422399, acc 1, learning_rate 0.0001
2017-10-10T15:28:52.959280: step 6358, loss 0.056732, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:53.296978: step 6359, loss 0.0510469, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:53.559648: step 6360, loss 0.0755833, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:54.116838: step 6360, loss 0.213303, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6360

2017-10-10T15:28:55.265738: step 6361, loss 0.161631, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:55.550698: step 6362, loss 0.0295139, acc 1, learning_rate 0.0001
2017-10-10T15:28:55.799570: step 6363, loss 0.0718067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:56.105483: step 6364, loss 0.0845573, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:56.372908: step 6365, loss 0.0401299, acc 1, learning_rate 0.0001
2017-10-10T15:28:56.718073: step 6366, loss 0.0314818, acc 1, learning_rate 0.0001
2017-10-10T15:28:56.948222: step 6367, loss 0.0479536, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:57.156284: step 6368, loss 0.0220174, acc 1, learning_rate 0.0001
2017-10-10T15:28:57.355376: step 6369, loss 0.0721702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:57.593017: step 6370, loss 0.0144759, acc 1, learning_rate 0.0001
2017-10-10T15:28:57.787284: step 6371, loss 0.0621502, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:58.032820: step 6372, loss 0.036002, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:58.236860: step 6373, loss 0.0161262, acc 1, learning_rate 0.0001
2017-10-10T15:28:58.530069: step 6374, loss 0.042134, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:58.798176: step 6375, loss 0.0779855, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:59.073349: step 6376, loss 0.0619564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:59.353140: step 6377, loss 0.0248048, acc 1, learning_rate 0.0001
2017-10-10T15:28:59.622399: step 6378, loss 0.0885013, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:59.876897: step 6379, loss 0.0653317, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:00.157249: step 6380, loss 0.073546, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:00.454106: step 6381, loss 0.0858001, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:00.712983: step 6382, loss 0.0721719, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:00.994150: step 6383, loss 0.0310267, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:01.257628: step 6384, loss 0.0101997, acc 1, learning_rate 0.0001
2017-10-10T15:29:01.522740: step 6385, loss 0.122993, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:01.787427: step 6386, loss 0.0160517, acc 1, learning_rate 0.0001
2017-10-10T15:29:02.056597: step 6387, loss 0.0581835, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:02.360849: step 6388, loss 0.0328082, acc 1, learning_rate 0.0001
2017-10-10T15:29:02.648860: step 6389, loss 0.0455058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:02.917316: step 6390, loss 0.0788418, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:03.156954: step 6391, loss 0.0116861, acc 1, learning_rate 0.0001
2017-10-10T15:29:03.407453: step 6392, loss 0.0659167, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:03.652543: step 6393, loss 0.0279725, acc 1, learning_rate 0.0001
2017-10-10T15:29:03.908552: step 6394, loss 0.0184461, acc 1, learning_rate 0.0001
2017-10-10T15:29:04.136899: step 6395, loss 0.0554732, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:04.408948: step 6396, loss 0.0842292, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:04.702701: step 6397, loss 0.141844, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:04.975364: step 6398, loss 0.0529358, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:05.260861: step 6399, loss 0.0349893, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:05.502687: step 6400, loss 0.0675712, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:06.071062: step 6400, loss 0.212814, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6400

2017-10-10T15:29:06.938760: step 6401, loss 0.072657, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:07.164377: step 6402, loss 0.0378498, acc 1, learning_rate 0.0001
2017-10-10T15:29:07.343430: step 6403, loss 0.125007, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:07.737156: step 6404, loss 0.104809, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:07.954976: step 6405, loss 0.0876066, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:08.162297: step 6406, loss 0.0645865, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:08.352157: step 6407, loss 0.0621591, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:08.574359: step 6408, loss 0.0370077, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:08.839313: step 6409, loss 0.0233097, acc 1, learning_rate 0.0001
2017-10-10T15:29:09.093462: step 6410, loss 0.0720053, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:09.360747: step 6411, loss 0.0818239, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:09.640277: step 6412, loss 0.0210632, acc 1, learning_rate 0.0001
2017-10-10T15:29:09.928860: step 6413, loss 0.0507867, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:10.272844: step 6414, loss 0.0173353, acc 1, learning_rate 0.0001
2017-10-10T15:29:10.544107: step 6415, loss 0.0822841, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:10.811386: step 6416, loss 0.0972269, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:11.067103: step 6417, loss 0.0307571, acc 1, learning_rate 0.0001
2017-10-10T15:29:11.336841: step 6418, loss 0.0762094, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:11.628326: step 6419, loss 0.0571943, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:11.884182: step 6420, loss 0.0742637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:12.164892: step 6421, loss 0.0198657, acc 1, learning_rate 0.0001
2017-10-10T15:29:12.441709: step 6422, loss 0.0477875, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:12.685323: step 6423, loss 0.0456648, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:12.929411: step 6424, loss 0.01054, acc 1, learning_rate 0.0001
2017-10-10T15:29:13.176676: step 6425, loss 0.059407, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:13.434024: step 6426, loss 0.136452, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:13.704091: step 6427, loss 0.0193276, acc 1, learning_rate 0.0001
2017-10-10T15:29:13.973784: step 6428, loss 0.00811573, acc 1, learning_rate 0.0001
2017-10-10T15:29:14.225272: step 6429, loss 0.00807483, acc 1, learning_rate 0.0001
2017-10-10T15:29:14.470180: step 6430, loss 0.0403721, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:14.743677: step 6431, loss 0.0674332, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:15.023990: step 6432, loss 0.0275362, acc 1, learning_rate 0.0001
2017-10-10T15:29:15.293327: step 6433, loss 0.0257377, acc 1, learning_rate 0.0001
2017-10-10T15:29:15.612842: step 6434, loss 0.0303287, acc 1, learning_rate 0.0001
2017-10-10T15:29:15.882873: step 6435, loss 0.118039, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:16.109885: step 6436, loss 0.0225302, acc 1, learning_rate 0.0001
2017-10-10T15:29:16.340860: step 6437, loss 0.0576042, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:16.598477: step 6438, loss 0.05146, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:16.865140: step 6439, loss 0.0215876, acc 1, learning_rate 0.0001
2017-10-10T15:29:17.123514: step 6440, loss 0.0693601, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:17.698538: step 6440, loss 0.211177, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6440

2017-10-10T15:29:18.781311: step 6441, loss 0.0236016, acc 1, learning_rate 0.0001
2017-10-10T15:29:18.975824: step 6442, loss 0.0417172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:19.165725: step 6443, loss 0.0694969, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:19.362920: step 6444, loss 0.10119, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:19.613038: step 6445, loss 0.0845022, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:19.866998: step 6446, loss 0.0111969, acc 1, learning_rate 0.0001
2017-10-10T15:29:20.175082: step 6447, loss 0.00935126, acc 1, learning_rate 0.0001
2017-10-10T15:29:20.452191: step 6448, loss 0.0140651, acc 1, learning_rate 0.0001
2017-10-10T15:29:20.732017: step 6449, loss 0.058997, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:20.985495: step 6450, loss 0.0349266, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:21.213505: step 6451, loss 0.0947986, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:21.485223: step 6452, loss 0.0597604, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:21.744633: step 6453, loss 0.0345854, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:22.040834: step 6454, loss 0.0328474, acc 1, learning_rate 0.0001
2017-10-10T15:29:22.310893: step 6455, loss 0.0956759, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:22.590132: step 6456, loss 0.0876369, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:22.860836: step 6457, loss 0.0641738, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:23.157302: step 6458, loss 0.0263022, acc 1, learning_rate 0.0001
2017-10-10T15:29:23.405639: step 6459, loss 0.0583145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:23.696226: step 6460, loss 0.0692061, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:23.957562: step 6461, loss 0.148861, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:24.224954: step 6462, loss 0.169165, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:24.565158: step 6463, loss 0.073161, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:24.804864: step 6464, loss 0.0549293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:25.043232: step 6465, loss 0.0697006, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:25.263931: step 6466, loss 0.0186771, acc 1, learning_rate 0.0001
2017-10-10T15:29:25.535843: step 6467, loss 0.0579297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:25.776846: step 6468, loss 0.0352562, acc 0.980392, learning_rate 0.0001
2017-10-10T15:29:26.031534: step 6469, loss 0.0224446, acc 1, learning_rate 0.0001
2017-10-10T15:29:26.277161: step 6470, loss 0.0679611, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:26.520794: step 6471, loss 0.0319934, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:26.802959: step 6472, loss 0.182852, acc 0.921875, learning_rate 0.0001
2017-10-10T15:29:27.065546: step 6473, loss 0.0216917, acc 1, learning_rate 0.0001
2017-10-10T15:29:27.301502: step 6474, loss 0.0341621, acc 1, learning_rate 0.0001
2017-10-10T15:29:27.568126: step 6475, loss 0.0997134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:27.843908: step 6476, loss 0.0801326, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:28.109775: step 6477, loss 0.0680822, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:28.367385: step 6478, loss 0.0543255, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:28.637904: step 6479, loss 0.0347395, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:28.905286: step 6480, loss 0.064688, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:29.497267: step 6480, loss 0.212027, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6480

2017-10-10T15:29:30.437048: step 6481, loss 0.129272, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:30.642163: step 6482, loss 0.0310937, acc 1, learning_rate 0.0001
2017-10-10T15:29:30.841029: step 6483, loss 0.154992, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:31.137327: step 6484, loss 0.0319986, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:31.419242: step 6485, loss 0.0482555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:31.675325: step 6486, loss 0.0607834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:31.924324: step 6487, loss 0.0506611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:32.162610: step 6488, loss 0.0701973, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:32.431426: step 6489, loss 0.0444946, acc 1, learning_rate 0.0001
2017-10-10T15:29:32.663329: step 6490, loss 0.0722185, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:32.898467: step 6491, loss 0.0722446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:33.230101: step 6492, loss 0.0315023, acc 1, learning_rate 0.0001
2017-10-10T15:29:33.482612: step 6493, loss 0.135816, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:33.721296: step 6494, loss 0.043232, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:33.914577: step 6495, loss 0.0718058, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:34.185503: step 6496, loss 0.101837, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:34.450411: step 6497, loss 0.0472738, acc 1, learning_rate 0.0001
2017-10-10T15:29:34.705320: step 6498, loss 0.0328007, acc 1, learning_rate 0.0001
2017-10-10T15:29:34.963005: step 6499, loss 0.0403851, acc 1, learning_rate 0.0001
2017-10-10T15:29:35.234047: step 6500, loss 0.0324856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:35.509784: step 6501, loss 0.0273029, acc 1, learning_rate 0.0001
2017-10-10T15:29:35.793034: step 6502, loss 0.0314973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:36.052877: step 6503, loss 0.0465599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:36.319541: step 6504, loss 0.0310208, acc 1, learning_rate 0.0001
2017-10-10T15:29:36.593151: step 6505, loss 0.0477091, acc 1, learning_rate 0.0001
2017-10-10T15:29:36.879683: step 6506, loss 0.0296532, acc 1, learning_rate 0.0001
2017-10-10T15:29:37.152864: step 6507, loss 0.0407603, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:37.416833: step 6508, loss 0.0716052, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:37.684853: step 6509, loss 0.0770395, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:37.989806: step 6510, loss 0.0488749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:38.284476: step 6511, loss 0.0528374, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:38.553532: step 6512, loss 0.0740376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:38.801210: step 6513, loss 0.0658978, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:39.065653: step 6514, loss 0.0639278, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:39.368831: step 6515, loss 0.108525, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:39.650820: step 6516, loss 0.078226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:39.912342: step 6517, loss 0.109728, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:40.188865: step 6518, loss 0.0534479, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:40.484922: step 6519, loss 0.0183164, acc 1, learning_rate 0.0001
2017-10-10T15:29:40.723842: step 6520, loss 0.0495449, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:41.387981: step 6520, loss 0.211254, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6520

2017-10-10T15:29:42.272967: step 6521, loss 0.0980144, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:42.487588: step 6522, loss 0.132036, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:42.752885: step 6523, loss 0.00719208, acc 1, learning_rate 0.0001
2017-10-10T15:29:42.996911: step 6524, loss 0.137892, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:43.269980: step 6525, loss 0.0638179, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:43.549056: step 6526, loss 0.136003, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:43.841782: step 6527, loss 0.0434964, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:44.096914: step 6528, loss 0.0701127, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:44.370733: step 6529, loss 0.0415249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:44.668739: step 6530, loss 0.0220924, acc 1, learning_rate 0.0001
2017-10-10T15:29:44.930707: step 6531, loss 0.0430882, acc 1, learning_rate 0.0001
2017-10-10T15:29:45.199496: step 6532, loss 0.0499777, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:45.478923: step 6533, loss 0.0476566, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:45.735464: step 6534, loss 0.11088, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:45.990099: step 6535, loss 0.0979913, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:46.268565: step 6536, loss 0.049549, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:46.516761: step 6537, loss 0.0335075, acc 1, learning_rate 0.0001
2017-10-10T15:29:46.782485: step 6538, loss 0.0679408, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:47.040311: step 6539, loss 0.036749, acc 1, learning_rate 0.0001
2017-10-10T15:29:47.301009: step 6540, loss 0.0735731, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:47.592831: step 6541, loss 0.0321751, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:47.844589: step 6542, loss 0.0438141, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:48.099014: step 6543, loss 0.0931264, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:48.351544: step 6544, loss 0.0485098, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:48.638735: step 6545, loss 0.020468, acc 1, learning_rate 0.0001
2017-10-10T15:29:48.870426: step 6546, loss 0.0203809, acc 1, learning_rate 0.0001
2017-10-10T15:29:49.126519: step 6547, loss 0.0387406, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:49.349612: step 6548, loss 0.0708268, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:49.608929: step 6549, loss 0.0445238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:49.876926: step 6550, loss 0.080938, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:50.114918: step 6551, loss 0.100891, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:50.381062: step 6552, loss 0.0954998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:50.652334: step 6553, loss 0.118929, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:50.987743: step 6554, loss 0.0214772, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:51.215750: step 6555, loss 0.118411, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:51.433650: step 6556, loss 0.0289772, acc 1, learning_rate 0.0001
2017-10-10T15:29:51.647520: step 6557, loss 0.0230871, acc 1, learning_rate 0.0001
2017-10-10T15:29:51.904584: step 6558, loss 0.0283489, acc 1, learning_rate 0.0001
2017-10-10T15:29:52.216856: step 6559, loss 0.00861708, acc 1, learning_rate 0.0001
2017-10-10T15:29:52.478034: step 6560, loss 0.0552945, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:52.918672: step 6560, loss 0.211224, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6560

2017-10-10T15:29:54.001958: step 6561, loss 0.0334409, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:54.288859: step 6562, loss 0.122226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:54.592274: step 6563, loss 0.0151871, acc 1, learning_rate 0.0001
2017-10-10T15:29:54.845126: step 6564, loss 0.100403, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:55.120054: step 6565, loss 0.0802633, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:55.374357: step 6566, loss 0.030895, acc 1, learning_rate 0.0001
2017-10-10T15:29:55.646691: step 6567, loss 0.075785, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:55.911377: step 6568, loss 0.0585725, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:56.193043: step 6569, loss 0.0241018, acc 1, learning_rate 0.0001
2017-10-10T15:29:56.436828: step 6570, loss 0.029204, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:56.679693: step 6571, loss 0.113623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:56.970423: step 6572, loss 0.0562314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:57.261371: step 6573, loss 0.085909, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:57.527402: step 6574, loss 0.07151, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:57.797915: step 6575, loss 0.0294924, acc 1, learning_rate 0.0001
2017-10-10T15:29:58.051711: step 6576, loss 0.0330883, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:58.315407: step 6577, loss 0.108775, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:58.592874: step 6578, loss 0.0867458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:58.865917: step 6579, loss 0.0722025, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:59.136822: step 6580, loss 0.0371427, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:59.387626: step 6581, loss 0.0462217, acc 1, learning_rate 0.0001
2017-10-10T15:29:59.664921: step 6582, loss 0.0730464, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:59.943922: step 6583, loss 0.0444561, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:00.182594: step 6584, loss 0.0385745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:00.398656: step 6585, loss 0.0519364, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:00.669477: step 6586, loss 0.0570695, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:00.933537: step 6587, loss 0.042988, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:01.184505: step 6588, loss 0.100886, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:01.477200: step 6589, loss 0.0705725, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:01.752859: step 6590, loss 0.032041, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:02.025838: step 6591, loss 0.0526236, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:02.304358: step 6592, loss 0.052324, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:02.555121: step 6593, loss 0.0420384, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:02.844976: step 6594, loss 0.104859, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:03.160447: step 6595, loss 0.0531386, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:03.385999: step 6596, loss 0.0699864, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:03.568383: step 6597, loss 0.117446, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:03.762600: step 6598, loss 0.149185, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:03.963880: step 6599, loss 0.0973983, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:04.240677: step 6600, loss 0.0346227, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:04.834902: step 6600, loss 0.210898, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6600

2017-10-10T15:30:05.981808: step 6601, loss 0.0303609, acc 1, learning_rate 0.0001
2017-10-10T15:30:06.249541: step 6602, loss 0.037783, acc 1, learning_rate 0.0001
2017-10-10T15:30:06.526539: step 6603, loss 0.0314614, acc 1, learning_rate 0.0001
2017-10-10T15:30:06.788484: step 6604, loss 0.0817814, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:07.044894: step 6605, loss 0.0274447, acc 1, learning_rate 0.0001
2017-10-10T15:30:07.289324: step 6606, loss 0.0471321, acc 1, learning_rate 0.0001
2017-10-10T15:30:07.583310: step 6607, loss 0.0462323, acc 1, learning_rate 0.0001
2017-10-10T15:30:07.836945: step 6608, loss 0.0547755, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:08.085266: step 6609, loss 0.0920685, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:08.396429: step 6610, loss 0.0533147, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:08.636502: step 6611, loss 0.046736, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:08.852850: step 6612, loss 0.0271536, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:09.075654: step 6613, loss 0.0816164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:09.297444: step 6614, loss 0.0943917, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:09.547591: step 6615, loss 0.0643259, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:09.801090: step 6616, loss 0.0401218, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:10.085694: step 6617, loss 0.0759078, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:10.327503: step 6618, loss 0.0123709, acc 1, learning_rate 0.0001
2017-10-10T15:30:10.582283: step 6619, loss 0.104201, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:10.870340: step 6620, loss 0.041405, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:11.145091: step 6621, loss 0.0356463, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:11.452187: step 6622, loss 0.0504679, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:11.695588: step 6623, loss 0.0560803, acc 1, learning_rate 0.0001
2017-10-10T15:30:11.969277: step 6624, loss 0.0778632, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:12.235596: step 6625, loss 0.0716903, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:12.472914: step 6626, loss 0.0418702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:12.757494: step 6627, loss 0.0526096, acc 1, learning_rate 0.0001
2017-10-10T15:30:13.058988: step 6628, loss 0.0406947, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:13.353501: step 6629, loss 0.0180523, acc 1, learning_rate 0.0001
2017-10-10T15:30:13.648909: step 6630, loss 0.040369, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:14.009995: step 6631, loss 0.0398638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:14.200229: step 6632, loss 0.0746413, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:14.417839: step 6633, loss 0.0892616, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:14.617062: step 6634, loss 0.116691, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:14.816971: step 6635, loss 0.144002, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:15.102221: step 6636, loss 0.117688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:15.369600: step 6637, loss 0.0864285, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:15.661172: step 6638, loss 0.0877707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:15.932257: step 6639, loss 0.0132339, acc 1, learning_rate 0.0001
2017-10-10T15:30:16.180131: step 6640, loss 0.0284118, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:16.756914: step 6640, loss 0.209431, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6640

2017-10-10T15:30:17.833556: step 6641, loss 0.0621561, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:18.063570: step 6642, loss 0.193808, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:18.332191: step 6643, loss 0.0800394, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:18.581153: step 6644, loss 0.0389114, acc 1, learning_rate 0.0001
2017-10-10T15:30:18.875407: step 6645, loss 0.0543123, acc 1, learning_rate 0.0001
2017-10-10T15:30:19.149093: step 6646, loss 0.051523, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:19.391268: step 6647, loss 0.137447, acc 0.921875, learning_rate 0.0001
2017-10-10T15:30:19.667652: step 6648, loss 0.0214546, acc 1, learning_rate 0.0001
2017-10-10T15:30:19.939243: step 6649, loss 0.0974047, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:20.213108: step 6650, loss 0.0782112, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:20.512816: step 6651, loss 0.0279854, acc 1, learning_rate 0.0001
2017-10-10T15:30:20.755942: step 6652, loss 0.01965, acc 1, learning_rate 0.0001
2017-10-10T15:30:21.012153: step 6653, loss 0.0425325, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:21.283916: step 6654, loss 0.0924713, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:21.553482: step 6655, loss 0.066732, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:21.820517: step 6656, loss 0.0430174, acc 1, learning_rate 0.0001
2017-10-10T15:30:22.084834: step 6657, loss 0.0699408, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:22.345893: step 6658, loss 0.0527095, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:22.593655: step 6659, loss 0.0659079, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:22.855850: step 6660, loss 0.0549165, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:23.145136: step 6661, loss 0.0368678, acc 1, learning_rate 0.0001
2017-10-10T15:30:23.396441: step 6662, loss 0.0215756, acc 1, learning_rate 0.0001
2017-10-10T15:30:23.674446: step 6663, loss 0.0439803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:23.892054: step 6664, loss 0.0529373, acc 0.980392, learning_rate 0.0001
2017-10-10T15:30:24.148806: step 6665, loss 0.0716106, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:24.422090: step 6666, loss 0.0627086, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:24.686479: step 6667, loss 0.0141842, acc 1, learning_rate 0.0001
2017-10-10T15:30:25.032887: step 6668, loss 0.0634107, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:25.295168: step 6669, loss 0.0374627, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:25.494396: step 6670, loss 0.116897, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:25.700654: step 6671, loss 0.0669703, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:25.909513: step 6672, loss 0.0306788, acc 1, learning_rate 0.0001
2017-10-10T15:30:26.204873: step 6673, loss 0.0300926, acc 1, learning_rate 0.0001
2017-10-10T15:30:26.473113: step 6674, loss 0.0249194, acc 1, learning_rate 0.0001
2017-10-10T15:30:26.708200: step 6675, loss 0.03982, acc 1, learning_rate 0.0001
2017-10-10T15:30:26.948816: step 6676, loss 0.030198, acc 1, learning_rate 0.0001
2017-10-10T15:30:27.261024: step 6677, loss 0.0545004, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:27.529062: step 6678, loss 0.0389159, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:27.819136: step 6679, loss 0.0279946, acc 1, learning_rate 0.0001
2017-10-10T15:30:28.089405: step 6680, loss 0.0699546, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:28.630108: step 6680, loss 0.215418, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6680

2017-10-10T15:30:29.752393: step 6681, loss 0.178715, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:30.019863: step 6682, loss 0.0872834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:30.292958: step 6683, loss 0.134379, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:30.567977: step 6684, loss 0.102333, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:30.836539: step 6685, loss 0.112916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:31.091710: step 6686, loss 0.0900933, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:31.387418: step 6687, loss 0.0691963, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:31.654917: step 6688, loss 0.0296219, acc 1, learning_rate 0.0001
2017-10-10T15:30:31.940848: step 6689, loss 0.0151105, acc 1, learning_rate 0.0001
2017-10-10T15:30:32.200857: step 6690, loss 0.0348291, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:32.450863: step 6691, loss 0.103383, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:32.733109: step 6692, loss 0.122705, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:32.976996: step 6693, loss 0.0637452, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:33.226627: step 6694, loss 0.0475055, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:33.537773: step 6695, loss 0.055511, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:33.771728: step 6696, loss 0.0926032, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:34.062407: step 6697, loss 0.0661743, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:34.322604: step 6698, loss 0.0715522, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:34.572706: step 6699, loss 0.0602019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:34.832997: step 6700, loss 0.0641296, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:35.148452: step 6701, loss 0.0559559, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:35.427659: step 6702, loss 0.0624842, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:35.771069: step 6703, loss 0.0487472, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:35.938625: step 6704, loss 0.0543983, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:36.144575: step 6705, loss 0.0185083, acc 1, learning_rate 0.0001
2017-10-10T15:30:36.364906: step 6706, loss 0.0438588, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:36.570232: step 6707, loss 0.0127829, acc 1, learning_rate 0.0001
2017-10-10T15:30:36.836169: step 6708, loss 0.0337467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:37.089857: step 6709, loss 0.0923432, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:37.372890: step 6710, loss 0.00828051, acc 1, learning_rate 0.0001
2017-10-10T15:30:37.641489: step 6711, loss 0.0292223, acc 1, learning_rate 0.0001
2017-10-10T15:30:37.902877: step 6712, loss 0.0216697, acc 1, learning_rate 0.0001
2017-10-10T15:30:38.164442: step 6713, loss 0.0386186, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:38.425198: step 6714, loss 0.118719, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:38.738379: step 6715, loss 0.0661838, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:39.005731: step 6716, loss 0.0498394, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:39.277299: step 6717, loss 0.0444284, acc 1, learning_rate 0.0001
2017-10-10T15:30:39.543728: step 6718, loss 0.0637479, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:39.790331: step 6719, loss 0.0167976, acc 1, learning_rate 0.0001
2017-10-10T15:30:40.033138: step 6720, loss 0.0467029, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:40.628161: step 6720, loss 0.212474, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6720

2017-10-10T15:30:41.579097: step 6721, loss 0.0375949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:41.873204: step 6722, loss 0.0595703, acc 1, learning_rate 0.0001
2017-10-10T15:30:42.169308: step 6723, loss 0.0171121, acc 1, learning_rate 0.0001
2017-10-10T15:30:42.429057: step 6724, loss 0.142139, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:42.647967: step 6725, loss 0.0096339, acc 1, learning_rate 0.0001
2017-10-10T15:30:42.940833: step 6726, loss 0.0117041, acc 1, learning_rate 0.0001
2017-10-10T15:30:43.188837: step 6727, loss 0.0183085, acc 1, learning_rate 0.0001
2017-10-10T15:30:43.462367: step 6728, loss 0.0390974, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:43.719291: step 6729, loss 0.0168663, acc 1, learning_rate 0.0001
2017-10-10T15:30:44.060219: step 6730, loss 0.0412703, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:44.307380: step 6731, loss 0.0747824, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:44.527599: step 6732, loss 0.104233, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:44.748818: step 6733, loss 0.0666457, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:44.963893: step 6734, loss 0.025433, acc 1, learning_rate 0.0001
2017-10-10T15:30:45.213185: step 6735, loss 0.145372, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:45.484270: step 6736, loss 0.0931648, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:45.756245: step 6737, loss 0.10268, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:46.035732: step 6738, loss 0.055622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:46.277843: step 6739, loss 0.0309315, acc 1, learning_rate 0.0001
2017-10-10T15:30:46.609479: step 6740, loss 0.0494355, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:46.843401: step 6741, loss 0.0202201, acc 1, learning_rate 0.0001
2017-10-10T15:30:47.048901: step 6742, loss 0.0455863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:47.248502: step 6743, loss 0.0573396, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:47.448818: step 6744, loss 0.0935823, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:47.691985: step 6745, loss 0.0174183, acc 1, learning_rate 0.0001
2017-10-10T15:30:47.965628: step 6746, loss 0.101994, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:48.226383: step 6747, loss 0.070737, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:48.511855: step 6748, loss 0.0399213, acc 1, learning_rate 0.0001
2017-10-10T15:30:48.773191: step 6749, loss 0.0717181, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:49.035539: step 6750, loss 0.0542605, acc 1, learning_rate 0.0001
2017-10-10T15:30:49.342483: step 6751, loss 0.0251595, acc 1, learning_rate 0.0001
2017-10-10T15:30:49.654810: step 6752, loss 0.0635172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:49.916518: step 6753, loss 0.0195885, acc 1, learning_rate 0.0001
2017-10-10T15:30:50.184259: step 6754, loss 0.070196, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:50.442782: step 6755, loss 0.0120648, acc 1, learning_rate 0.0001
2017-10-10T15:30:50.732482: step 6756, loss 0.0426734, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:51.028548: step 6757, loss 0.0255239, acc 1, learning_rate 0.0001
2017-10-10T15:30:51.343169: step 6758, loss 0.0410506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:51.604761: step 6759, loss 0.147648, acc 0.921875, learning_rate 0.0001
2017-10-10T15:30:51.889090: step 6760, loss 0.0984534, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:52.444721: step 6760, loss 0.207377, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6760

2017-10-10T15:30:53.460602: step 6761, loss 0.0252766, acc 1, learning_rate 0.0001
2017-10-10T15:30:53.655618: step 6762, loss 0.0355321, acc 1, learning_rate 0.0001
2017-10-10T15:30:53.897974: step 6763, loss 0.0299203, acc 1, learning_rate 0.0001
2017-10-10T15:30:54.168469: step 6764, loss 0.0908901, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:54.440887: step 6765, loss 0.0807362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:54.727332: step 6766, loss 0.0624497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:54.986783: step 6767, loss 0.0323293, acc 1, learning_rate 0.0001
2017-10-10T15:30:55.225137: step 6768, loss 0.121205, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:55.471343: step 6769, loss 0.0532077, acc 1, learning_rate 0.0001
2017-10-10T15:30:55.726902: step 6770, loss 0.0272571, acc 1, learning_rate 0.0001
2017-10-10T15:30:55.980797: step 6771, loss 0.0349537, acc 1, learning_rate 0.0001
2017-10-10T15:30:56.245021: step 6772, loss 0.0939526, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:56.481719: step 6773, loss 0.041856, acc 1, learning_rate 0.0001
2017-10-10T15:30:56.737557: step 6774, loss 0.0507303, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:56.989841: step 6775, loss 0.069561, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:57.308968: step 6776, loss 0.0806897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:57.636635: step 6777, loss 0.0461538, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:57.828900: step 6778, loss 0.05146, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:58.053084: step 6779, loss 0.0594159, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:58.273989: step 6780, loss 0.0897281, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:58.528217: step 6781, loss 0.0444919, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:58.820863: step 6782, loss 0.0485419, acc 1, learning_rate 0.0001
2017-10-10T15:30:59.064064: step 6783, loss 0.0877738, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:59.328299: step 6784, loss 0.0431232, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:59.592847: step 6785, loss 0.0674428, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:59.861170: step 6786, loss 0.0962375, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:00.131281: step 6787, loss 0.016432, acc 1, learning_rate 0.0001
2017-10-10T15:31:00.337309: step 6788, loss 0.0308417, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:00.624900: step 6789, loss 0.0647667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:00.848928: step 6790, loss 0.0299915, acc 1, learning_rate 0.0001
2017-10-10T15:31:01.147167: step 6791, loss 0.0465073, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:01.441922: step 6792, loss 0.0994838, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:01.719971: step 6793, loss 0.0902487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:01.988031: step 6794, loss 0.072807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:02.332724: step 6795, loss 0.0310756, acc 1, learning_rate 0.0001
2017-10-10T15:31:02.562698: step 6796, loss 0.0345694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:02.787907: step 6797, loss 0.127274, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:03.013065: step 6798, loss 0.0201661, acc 1, learning_rate 0.0001
2017-10-10T15:31:03.295976: step 6799, loss 0.102955, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:03.536024: step 6800, loss 0.067091, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:04.104870: step 6800, loss 0.210752, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6800

2017-10-10T15:31:05.199854: step 6801, loss 0.022779, acc 1, learning_rate 0.0001
2017-10-10T15:31:05.443395: step 6802, loss 0.102, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:05.697184: step 6803, loss 0.0255778, acc 1, learning_rate 0.0001
2017-10-10T15:31:05.977148: step 6804, loss 0.055967, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:06.259724: step 6805, loss 0.0184686, acc 1, learning_rate 0.0001
2017-10-10T15:31:06.518732: step 6806, loss 0.100959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:06.794067: step 6807, loss 0.0290799, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:07.094506: step 6808, loss 0.0678979, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:07.396964: step 6809, loss 0.0474706, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:07.665155: step 6810, loss 0.135389, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:08.017977: step 6811, loss 0.0493259, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:08.334813: step 6812, loss 0.10675, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:08.533904: step 6813, loss 0.0286036, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:08.731762: step 6814, loss 0.0595647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:08.925421: step 6815, loss 0.0797029, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:09.206098: step 6816, loss 0.0374789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:09.468178: step 6817, loss 0.0809338, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:09.768562: step 6818, loss 0.0373557, acc 1, learning_rate 0.0001
2017-10-10T15:31:10.033977: step 6819, loss 0.056687, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:10.284831: step 6820, loss 0.0172655, acc 1, learning_rate 0.0001
2017-10-10T15:31:10.520938: step 6821, loss 0.129328, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:10.799290: step 6822, loss 0.0186389, acc 1, learning_rate 0.0001
2017-10-10T15:31:11.145092: step 6823, loss 0.0872403, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:11.331790: step 6824, loss 0.0464842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:11.580815: step 6825, loss 0.0385941, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:11.796823: step 6826, loss 0.0619921, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:12.016588: step 6827, loss 0.0602352, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:12.235420: step 6828, loss 0.036838, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:12.462796: step 6829, loss 0.0437825, acc 1, learning_rate 0.0001
2017-10-10T15:31:12.672437: step 6830, loss 0.0192162, acc 1, learning_rate 0.0001
2017-10-10T15:31:12.889759: step 6831, loss 0.0192311, acc 1, learning_rate 0.0001
2017-10-10T15:31:13.097703: step 6832, loss 0.065892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:13.297197: step 6833, loss 0.0738181, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:13.507503: step 6834, loss 0.0658084, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:13.699153: step 6835, loss 0.0392544, acc 1, learning_rate 0.0001
2017-10-10T15:31:13.905114: step 6836, loss 0.034383, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:14.155192: step 6837, loss 0.106558, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:14.356119: step 6838, loss 0.0408385, acc 1, learning_rate 0.0001
2017-10-10T15:31:14.564309: step 6839, loss 0.0656177, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:14.779788: step 6840, loss 0.120368, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:15.214723: step 6840, loss 0.209828, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6840

2017-10-10T15:31:16.126876: step 6841, loss 0.0763084, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:16.342516: step 6842, loss 0.0181114, acc 1, learning_rate 0.0001
2017-10-10T15:31:16.531443: step 6843, loss 0.0645388, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:16.739153: step 6844, loss 0.0566551, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:16.936006: step 6845, loss 0.14855, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:17.214122: step 6846, loss 0.0674315, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:17.407882: step 6847, loss 0.0190092, acc 1, learning_rate 0.0001
2017-10-10T15:31:17.540842: step 6848, loss 0.0809618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:17.675318: step 6849, loss 0.082104, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:17.806234: step 6850, loss 0.125168, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:17.940218: step 6851, loss 0.0608571, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:18.109879: step 6852, loss 0.0249956, acc 1, learning_rate 0.0001
2017-10-10T15:31:18.303780: step 6853, loss 0.0343188, acc 1, learning_rate 0.0001
2017-10-10T15:31:18.504363: step 6854, loss 0.0676998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:18.707616: step 6855, loss 0.183113, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:18.913956: step 6856, loss 0.042595, acc 1, learning_rate 0.0001
2017-10-10T15:31:19.123027: step 6857, loss 0.0765526, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:19.340492: step 6858, loss 0.0441015, acc 1, learning_rate 0.0001
2017-10-10T15:31:19.536884: step 6859, loss 0.0515688, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:19.725411: step 6860, loss 0.0382413, acc 1, learning_rate 0.0001
2017-10-10T15:31:19.932110: step 6861, loss 0.107638, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:20.132697: step 6862, loss 0.0551064, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:20.337930: step 6863, loss 0.0526768, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:20.539040: step 6864, loss 0.08732, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:20.748765: step 6865, loss 0.0686584, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:20.961037: step 6866, loss 0.011051, acc 1, learning_rate 0.0001
2017-10-10T15:31:21.164864: step 6867, loss 0.0523997, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:21.356061: step 6868, loss 0.0697643, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:21.541560: step 6869, loss 0.0418978, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:21.742405: step 6870, loss 0.0453231, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:21.945759: step 6871, loss 0.0595183, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:22.152898: step 6872, loss 0.0835379, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:22.344850: step 6873, loss 0.038863, acc 1, learning_rate 0.0001
2017-10-10T15:31:22.537727: step 6874, loss 0.137814, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:22.749810: step 6875, loss 0.0770705, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:22.976576: step 6876, loss 0.0358354, acc 1, learning_rate 0.0001
2017-10-10T15:31:23.185236: step 6877, loss 0.0252114, acc 1, learning_rate 0.0001
2017-10-10T15:31:23.389923: step 6878, loss 0.041002, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:23.596178: step 6879, loss 0.0789757, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:23.807266: step 6880, loss 0.0993658, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:24.213194: step 6880, loss 0.209172, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6880

2017-10-10T15:31:25.125110: step 6881, loss 0.0752106, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:25.304995: step 6882, loss 0.0545351, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:25.510750: step 6883, loss 0.0613417, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:25.804440: step 6884, loss 0.0801933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:25.936466: step 6885, loss 0.113003, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:26.074811: step 6886, loss 0.0847095, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:26.214248: step 6887, loss 0.106685, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:26.356820: step 6888, loss 0.0423233, acc 1, learning_rate 0.0001
2017-10-10T15:31:26.511469: step 6889, loss 0.147696, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:26.709111: step 6890, loss 0.146563, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:26.916517: step 6891, loss 0.0404329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:27.122264: step 6892, loss 0.0205952, acc 1, learning_rate 0.0001
2017-10-10T15:31:27.321644: step 6893, loss 0.0194335, acc 1, learning_rate 0.0001
2017-10-10T15:31:27.528853: step 6894, loss 0.111513, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:27.726411: step 6895, loss 0.021514, acc 1, learning_rate 0.0001
2017-10-10T15:31:27.905815: step 6896, loss 0.015947, acc 1, learning_rate 0.0001
2017-10-10T15:31:28.128764: step 6897, loss 0.0713853, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:28.327387: step 6898, loss 0.0202355, acc 1, learning_rate 0.0001
2017-10-10T15:31:28.531496: step 6899, loss 0.0332794, acc 1, learning_rate 0.0001
2017-10-10T15:31:28.740359: step 6900, loss 0.0488897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:28.946427: step 6901, loss 0.0779884, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:29.150123: step 6902, loss 0.102288, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:29.362615: step 6903, loss 0.0753411, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:29.572871: step 6904, loss 0.0290893, acc 1, learning_rate 0.0001
2017-10-10T15:31:29.776280: step 6905, loss 0.0417324, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:29.960847: step 6906, loss 0.0802655, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:30.158984: step 6907, loss 0.0489264, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:30.370719: step 6908, loss 0.0645457, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:30.527675: step 6909, loss 0.0280231, acc 1, learning_rate 0.0001
2017-10-10T15:31:30.719055: step 6910, loss 0.0225113, acc 1, learning_rate 0.0001
2017-10-10T15:31:30.903673: step 6911, loss 0.0201927, acc 1, learning_rate 0.0001
2017-10-10T15:31:31.100436: step 6912, loss 0.0223618, acc 1, learning_rate 0.0001
2017-10-10T15:31:31.291501: step 6913, loss 0.0536762, acc 1, learning_rate 0.0001
2017-10-10T15:31:31.473051: step 6914, loss 0.0364077, acc 1, learning_rate 0.0001
2017-10-10T15:31:31.664330: step 6915, loss 0.0819329, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:31.860861: step 6916, loss 0.0650521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:32.060809: step 6917, loss 0.0286849, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:32.279395: step 6918, loss 0.0204103, acc 1, learning_rate 0.0001
2017-10-10T15:31:32.456961: step 6919, loss 0.0249673, acc 1, learning_rate 0.0001
2017-10-10T15:31:32.624849: step 6920, loss 0.0472015, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:33.056386: step 6920, loss 0.210759, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6920

2017-10-10T15:31:34.084513: step 6921, loss 0.0205072, acc 1, learning_rate 0.0001
2017-10-10T15:31:34.252775: step 6922, loss 0.109665, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:34.390821: step 6923, loss 0.0895129, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:34.532366: step 6924, loss 0.074345, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:34.677120: step 6925, loss 0.045135, acc 1, learning_rate 0.0001
2017-10-10T15:31:34.809552: step 6926, loss 0.0653682, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:34.941568: step 6927, loss 0.0244357, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:35.163253: step 6928, loss 0.0378879, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:35.353955: step 6929, loss 0.0149977, acc 1, learning_rate 0.0001
2017-10-10T15:31:35.542420: step 6930, loss 0.0336133, acc 1, learning_rate 0.0001
2017-10-10T15:31:35.733660: step 6931, loss 0.0466898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:35.933148: step 6932, loss 0.0781821, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:36.149005: step 6933, loss 0.0737369, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:36.367107: step 6934, loss 0.030596, acc 1, learning_rate 0.0001
2017-10-10T15:31:36.568467: step 6935, loss 0.0603097, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:36.763576: step 6936, loss 0.0398899, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:36.978752: step 6937, loss 0.0742747, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:37.176809: step 6938, loss 0.0339695, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:37.376235: step 6939, loss 0.0568538, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:37.581061: step 6940, loss 0.0872014, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:37.784311: step 6941, loss 0.0959079, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:37.992838: step 6942, loss 0.0758723, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:38.172814: step 6943, loss 0.0817969, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:38.364947: step 6944, loss 0.053596, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:38.569508: step 6945, loss 0.040047, acc 1, learning_rate 0.0001
2017-10-10T15:31:38.779809: step 6946, loss 0.050889, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:38.985864: step 6947, loss 0.0538657, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:39.183425: step 6948, loss 0.0353782, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:39.383333: step 6949, loss 0.0975038, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:39.577291: step 6950, loss 0.0325524, acc 1, learning_rate 0.0001
2017-10-10T15:31:39.791439: step 6951, loss 0.069728, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:39.991490: step 6952, loss 0.0192323, acc 1, learning_rate 0.0001
2017-10-10T15:31:40.206531: step 6953, loss 0.0670691, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:40.413146: step 6954, loss 0.0456806, acc 1, learning_rate 0.0001
2017-10-10T15:31:40.625625: step 6955, loss 0.0582769, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:40.826474: step 6956, loss 0.0739762, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:41.029442: step 6957, loss 0.0903841, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:41.206310: step 6958, loss 0.0352709, acc 1, learning_rate 0.0001
2017-10-10T15:31:41.407544: step 6959, loss 0.0327298, acc 1, learning_rate 0.0001
2017-10-10T15:31:41.606636: step 6960, loss 0.097274, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:42.057430: step 6960, loss 0.209702, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-6960

2017-10-10T15:31:42.931228: step 6961, loss 0.0279248, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:43.062132: step 6962, loss 0.0455229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:43.195748: step 6963, loss 0.0262373, acc 1, learning_rate 0.0001
2017-10-10T15:31:43.328602: step 6964, loss 0.0516381, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:43.531708: step 6965, loss 0.105253, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:43.742415: step 6966, loss 0.0828013, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:43.956332: step 6967, loss 0.0920859, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:44.164382: step 6968, loss 0.0121476, acc 1, learning_rate 0.0001
2017-10-10T15:31:44.369681: step 6969, loss 0.049635, acc 1, learning_rate 0.0001
2017-10-10T15:31:44.566206: step 6970, loss 0.108071, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:44.760546: step 6971, loss 0.0465791, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:44.956691: step 6972, loss 0.120353, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:45.145271: step 6973, loss 0.0397802, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:45.337001: step 6974, loss 0.0343497, acc 1, learning_rate 0.0001
2017-10-10T15:31:45.535222: step 6975, loss 0.0820933, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:45.736913: step 6976, loss 0.0475829, acc 1, learning_rate 0.0001
2017-10-10T15:31:45.941279: step 6977, loss 0.0986865, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:46.155227: step 6978, loss 0.0376486, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:46.348063: step 6979, loss 0.1205, acc 0.921875, learning_rate 0.0001
2017-10-10T15:31:46.508280: step 6980, loss 0.116589, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:46.746503: step 6981, loss 0.0404938, acc 1, learning_rate 0.0001
2017-10-10T15:31:46.954920: step 6982, loss 0.0428277, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:47.143036: step 6983, loss 0.0777831, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:47.340598: step 6984, loss 0.0668077, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:47.534765: step 6985, loss 0.0719676, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:47.749212: step 6986, loss 0.0118657, acc 1, learning_rate 0.0001
2017-10-10T15:31:47.961744: step 6987, loss 0.0403802, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:48.170561: step 6988, loss 0.0828715, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:48.355537: step 6989, loss 0.076143, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:48.565038: step 6990, loss 0.0430407, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:48.756432: step 6991, loss 0.0395675, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:48.959465: step 6992, loss 0.0328557, acc 1, learning_rate 0.0001
2017-10-10T15:31:49.153581: step 6993, loss 0.0741284, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:49.342790: step 6994, loss 0.0296267, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:49.532191: step 6995, loss 0.0764539, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:49.720853: step 6996, loss 0.0595366, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:49.902435: step 6997, loss 0.0869131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:50.097393: step 6998, loss 0.0172161, acc 1, learning_rate 0.0001
2017-10-10T15:31:50.294153: step 6999, loss 0.0261012, acc 1, learning_rate 0.0001
2017-10-10T15:31:50.490005: step 7000, loss 0.0698006, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:50.938897: step 7000, loss 0.211919, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7000

2017-10-10T15:31:51.700698: step 7001, loss 0.0650846, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:51.833294: step 7002, loss 0.0186535, acc 1, learning_rate 0.0001
2017-10-10T15:31:51.972901: step 7003, loss 0.0936614, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:52.162313: step 7004, loss 0.0793297, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:52.351138: step 7005, loss 0.0312213, acc 1, learning_rate 0.0001
2017-10-10T15:31:52.572992: step 7006, loss 0.0461439, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:52.794346: step 7007, loss 0.0206749, acc 1, learning_rate 0.0001
2017-10-10T15:31:52.998050: step 7008, loss 0.0357075, acc 1, learning_rate 0.0001
2017-10-10T15:31:53.211635: step 7009, loss 0.155803, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:53.412304: step 7010, loss 0.0384846, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:53.609308: step 7011, loss 0.0708039, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:53.814550: step 7012, loss 0.030934, acc 1, learning_rate 0.0001
2017-10-10T15:31:54.011889: step 7013, loss 0.059514, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:54.221584: step 7014, loss 0.0696539, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:54.435163: step 7015, loss 0.0194275, acc 1, learning_rate 0.0001
2017-10-10T15:31:54.637551: step 7016, loss 0.0211462, acc 1, learning_rate 0.0001
2017-10-10T15:31:54.836195: step 7017, loss 0.0475577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:55.036242: step 7018, loss 0.0706011, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:55.252312: step 7019, loss 0.0785273, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:55.426976: step 7020, loss 0.0623583, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:55.595653: step 7021, loss 0.0225, acc 1, learning_rate 0.0001
2017-10-10T15:31:55.779411: step 7022, loss 0.0270055, acc 1, learning_rate 0.0001
2017-10-10T15:31:55.973284: step 7023, loss 0.0142851, acc 1, learning_rate 0.0001
2017-10-10T15:31:56.161689: step 7024, loss 0.0383364, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:56.352865: step 7025, loss 0.0335505, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:56.544812: step 7026, loss 0.0264813, acc 1, learning_rate 0.0001
2017-10-10T15:31:56.736727: step 7027, loss 0.108007, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:56.930895: step 7028, loss 0.115636, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:57.129395: step 7029, loss 0.0874998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:57.338976: step 7030, loss 0.0336238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:57.532886: step 7031, loss 0.0694641, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:57.746616: step 7032, loss 0.0568346, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:57.954906: step 7033, loss 0.0802203, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:58.164293: step 7034, loss 0.0779189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:58.360849: step 7035, loss 0.0316692, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:58.568179: step 7036, loss 0.0319703, acc 1, learning_rate 0.0001
2017-10-10T15:31:58.751052: step 7037, loss 0.0506882, acc 1, learning_rate 0.0001
2017-10-10T15:31:58.936870: step 7038, loss 0.105011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:59.124878: step 7039, loss 0.0282256, acc 1, learning_rate 0.0001
2017-10-10T15:31:59.314665: step 7040, loss 0.0229086, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:59.786518: step 7040, loss 0.208968, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7040

2017-10-10T15:32:00.551654: step 7041, loss 0.108319, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:00.762197: step 7042, loss 0.0786749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:00.961964: step 7043, loss 0.0299701, acc 1, learning_rate 0.0001
2017-10-10T15:32:01.160659: step 7044, loss 0.0351161, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:01.356866: step 7045, loss 0.067517, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:01.545488: step 7046, loss 0.0408557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:01.745537: step 7047, loss 0.043255, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:01.933262: step 7048, loss 0.0339238, acc 1, learning_rate 0.0001
2017-10-10T15:32:02.126445: step 7049, loss 0.056718, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:02.334226: step 7050, loss 0.0152796, acc 1, learning_rate 0.0001
2017-10-10T15:32:02.544862: step 7051, loss 0.0826866, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:02.728128: step 7052, loss 0.0189189, acc 1, learning_rate 0.0001
2017-10-10T15:32:02.912890: step 7053, loss 0.0325288, acc 1, learning_rate 0.0001
2017-10-10T15:32:03.072239: step 7054, loss 0.0772835, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:03.261106: step 7055, loss 0.110827, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:03.443127: step 7056, loss 0.0310356, acc 1, learning_rate 0.0001
2017-10-10T15:32:03.645948: step 7057, loss 0.0138532, acc 1, learning_rate 0.0001
2017-10-10T15:32:03.813425: step 7058, loss 0.0782169, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:04.005096: step 7059, loss 0.0851167, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:04.211388: step 7060, loss 0.047571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:04.405146: step 7061, loss 0.0407398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:04.598352: step 7062, loss 0.0191817, acc 1, learning_rate 0.0001
2017-10-10T15:32:04.765802: step 7063, loss 0.0150502, acc 1, learning_rate 0.0001
2017-10-10T15:32:04.988873: step 7064, loss 0.0574593, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:05.194996: step 7065, loss 0.0215837, acc 1, learning_rate 0.0001
2017-10-10T15:32:05.395232: step 7066, loss 0.0295995, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:05.592427: step 7067, loss 0.100618, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:05.793267: step 7068, loss 0.0621128, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:06.008208: step 7069, loss 0.0565113, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:06.201474: step 7070, loss 0.0377212, acc 1, learning_rate 0.0001
2017-10-10T15:32:06.401080: step 7071, loss 0.0828882, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:06.591995: step 7072, loss 0.0511494, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:06.792945: step 7073, loss 0.0758851, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:06.990246: step 7074, loss 0.018555, acc 1, learning_rate 0.0001
2017-10-10T15:32:07.204790: step 7075, loss 0.115065, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:07.405650: step 7076, loss 0.0262446, acc 1, learning_rate 0.0001
2017-10-10T15:32:07.600471: step 7077, loss 0.0143186, acc 1, learning_rate 0.0001
2017-10-10T15:32:07.796883: step 7078, loss 0.124027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:07.996855: step 7079, loss 0.0831156, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:08.299274: step 7080, loss 0.0508725, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:08.651366: step 7080, loss 0.208365, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7080

2017-10-10T15:32:09.220881: step 7081, loss 0.0687316, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:09.411577: step 7082, loss 0.0416578, acc 1, learning_rate 0.0001
2017-10-10T15:32:09.639680: step 7083, loss 0.105088, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:09.835536: step 7084, loss 0.0313104, acc 1, learning_rate 0.0001
2017-10-10T15:32:10.038467: step 7085, loss 0.0276574, acc 1, learning_rate 0.0001
2017-10-10T15:32:10.238388: step 7086, loss 0.0384563, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:10.438439: step 7087, loss 0.0330417, acc 1, learning_rate 0.0001
2017-10-10T15:32:10.646083: step 7088, loss 0.0953634, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:10.864835: step 7089, loss 0.0512579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:11.048174: step 7090, loss 0.0471266, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:11.240253: step 7091, loss 0.0596275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:11.453294: step 7092, loss 0.0851949, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:11.685274: step 7093, loss 0.0279671, acc 1, learning_rate 0.0001
2017-10-10T15:32:11.886073: step 7094, loss 0.0173951, acc 1, learning_rate 0.0001
2017-10-10T15:32:12.082890: step 7095, loss 0.068027, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:12.283439: step 7096, loss 0.0154426, acc 1, learning_rate 0.0001
2017-10-10T15:32:12.486616: step 7097, loss 0.0397178, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:12.701323: step 7098, loss 0.0723827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:12.899290: step 7099, loss 0.022269, acc 1, learning_rate 0.0001
2017-10-10T15:32:13.100700: step 7100, loss 0.0210892, acc 1, learning_rate 0.0001
2017-10-10T15:32:13.288140: step 7101, loss 0.0909313, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:13.492298: step 7102, loss 0.0692752, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:13.680673: step 7103, loss 0.0367244, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:13.890853: step 7104, loss 0.0934563, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:14.090756: step 7105, loss 0.0151986, acc 1, learning_rate 0.0001
2017-10-10T15:32:14.293478: step 7106, loss 0.148047, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:14.496880: step 7107, loss 0.0385548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:14.698719: step 7108, loss 0.0302731, acc 1, learning_rate 0.0001
2017-10-10T15:32:14.900106: step 7109, loss 0.0592503, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:15.105685: step 7110, loss 0.0820889, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:15.320060: step 7111, loss 0.0882484, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:15.564822: step 7112, loss 0.0628141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:15.778968: step 7113, loss 0.0929741, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:15.991788: step 7114, loss 0.058977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:16.182353: step 7115, loss 0.0689827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:16.376881: step 7116, loss 0.0401021, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:16.569003: step 7117, loss 0.0115689, acc 1, learning_rate 0.0001
2017-10-10T15:32:16.881639: step 7118, loss 0.0222667, acc 1, learning_rate 0.0001
2017-10-10T15:32:17.023514: step 7119, loss 0.051134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:17.156776: step 7120, loss 0.0518739, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:17.480898: step 7120, loss 0.20831, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7120

2017-10-10T15:32:18.302048: step 7121, loss 0.0476041, acc 1, learning_rate 0.0001
2017-10-10T15:32:18.502678: step 7122, loss 0.049533, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:18.708768: step 7123, loss 0.100364, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:18.903557: step 7124, loss 0.0218052, acc 1, learning_rate 0.0001
2017-10-10T15:32:19.113795: step 7125, loss 0.0276557, acc 1, learning_rate 0.0001
2017-10-10T15:32:19.324693: step 7126, loss 0.0330402, acc 1, learning_rate 0.0001
2017-10-10T15:32:19.544905: step 7127, loss 0.0220195, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:19.762182: step 7128, loss 0.0198067, acc 1, learning_rate 0.0001
2017-10-10T15:32:19.967564: step 7129, loss 0.0471555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:20.173207: step 7130, loss 0.0930018, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:20.370424: step 7131, loss 0.0500333, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:20.576455: step 7132, loss 0.0684963, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:20.794990: step 7133, loss 0.0400117, acc 1, learning_rate 0.0001
2017-10-10T15:32:20.984846: step 7134, loss 0.128936, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:21.172895: step 7135, loss 0.10954, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:21.365305: step 7136, loss 0.0428343, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:21.578686: step 7137, loss 0.0391687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:21.779854: step 7138, loss 0.0544373, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:21.984515: step 7139, loss 0.0461913, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:22.197050: step 7140, loss 0.0182315, acc 1, learning_rate 0.0001
2017-10-10T15:32:22.407410: step 7141, loss 0.0934778, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:22.601370: step 7142, loss 0.0830895, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:22.792136: step 7143, loss 0.0532494, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:22.987354: step 7144, loss 0.0347908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:23.211552: step 7145, loss 0.0842685, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:23.418214: step 7146, loss 0.0862811, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:23.616444: step 7147, loss 0.0751185, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:23.808748: step 7148, loss 0.0528391, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:24.005525: step 7149, loss 0.0989702, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:24.204384: step 7150, loss 0.0450789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:24.421787: step 7151, loss 0.0549001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:24.630673: step 7152, loss 0.159555, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:24.817475: step 7153, loss 0.0532711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:24.991858: step 7154, loss 0.130244, acc 0.921569, learning_rate 0.0001
2017-10-10T15:32:25.283107: step 7155, loss 0.0563555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:25.452781: step 7156, loss 0.0951286, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:25.592539: step 7157, loss 0.0414257, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:25.727656: step 7158, loss 0.0309654, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:25.862551: step 7159, loss 0.0781962, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:26.006614: step 7160, loss 0.030944, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:26.421145: step 7160, loss 0.210384, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7160

2017-10-10T15:32:27.401053: step 7161, loss 0.0358927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:27.613598: step 7162, loss 0.114701, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:27.808063: step 7163, loss 0.0537717, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:28.004633: step 7164, loss 0.0318337, acc 1, learning_rate 0.0001
2017-10-10T15:32:28.217463: step 7165, loss 0.0326659, acc 1, learning_rate 0.0001
2017-10-10T15:32:28.421203: step 7166, loss 0.0753388, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:28.621304: step 7167, loss 0.026614, acc 1, learning_rate 0.0001
2017-10-10T15:32:28.827123: step 7168, loss 0.0735845, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:29.028634: step 7169, loss 0.0578137, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:29.253061: step 7170, loss 0.0473245, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:29.429184: step 7171, loss 0.0594986, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:29.617863: step 7172, loss 0.0529754, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:29.813455: step 7173, loss 0.0240444, acc 1, learning_rate 0.0001
2017-10-10T15:32:29.985217: step 7174, loss 0.043849, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:30.152038: step 7175, loss 0.0932893, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:30.368919: step 7176, loss 0.0541316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:30.542538: step 7177, loss 0.0154748, acc 1, learning_rate 0.0001
2017-10-10T15:32:30.772455: step 7178, loss 0.0407137, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:30.977939: step 7179, loss 0.0376576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:31.192490: step 7180, loss 0.0786388, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:31.389730: step 7181, loss 0.0463761, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:31.586603: step 7182, loss 0.0183465, acc 1, learning_rate 0.0001
2017-10-10T15:32:31.780889: step 7183, loss 0.0523306, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:31.968370: step 7184, loss 0.0610131, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:32.161551: step 7185, loss 0.107716, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:32.360892: step 7186, loss 0.0259905, acc 1, learning_rate 0.0001
2017-10-10T15:32:32.549274: step 7187, loss 0.0281207, acc 1, learning_rate 0.0001
2017-10-10T15:32:32.741079: step 7188, loss 0.115996, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:32.934355: step 7189, loss 0.0229062, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:33.122910: step 7190, loss 0.0356943, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:33.312882: step 7191, loss 0.0275196, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:33.597307: step 7192, loss 0.0399842, acc 1, learning_rate 0.0001
2017-10-10T15:32:33.761907: step 7193, loss 0.0890112, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:33.895812: step 7194, loss 0.0164873, acc 1, learning_rate 0.0001
2017-10-10T15:32:34.027615: step 7195, loss 0.0300895, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:34.169821: step 7196, loss 0.0570658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:34.302767: step 7197, loss 0.0412268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:34.516553: step 7198, loss 0.103739, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:34.722911: step 7199, loss 0.0451215, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:34.921676: step 7200, loss 0.0567877, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:35.373419: step 7200, loss 0.21205, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7200

2017-10-10T15:32:36.313939: step 7201, loss 0.0610743, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:36.508462: step 7202, loss 0.167797, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:36.731500: step 7203, loss 0.119762, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:36.932922: step 7204, loss 0.0389104, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:37.120824: step 7205, loss 0.0128306, acc 1, learning_rate 0.0001
2017-10-10T15:32:37.312787: step 7206, loss 0.0845008, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:37.514039: step 7207, loss 0.0313704, acc 1, learning_rate 0.0001
2017-10-10T15:32:37.731008: step 7208, loss 0.0411125, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:37.905377: step 7209, loss 0.0115066, acc 1, learning_rate 0.0001
2017-10-10T15:32:38.102499: step 7210, loss 0.105589, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:38.281442: step 7211, loss 0.050456, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:38.462273: step 7212, loss 0.0292407, acc 1, learning_rate 0.0001
2017-10-10T15:32:38.636841: step 7213, loss 0.0526214, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:38.821793: step 7214, loss 0.0589126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:39.020872: step 7215, loss 0.0696894, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:39.214889: step 7216, loss 0.0621523, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:39.417380: step 7217, loss 0.12445, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:39.632658: step 7218, loss 0.0534264, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:39.810543: step 7219, loss 0.0499772, acc 1, learning_rate 0.0001
2017-10-10T15:32:40.018224: step 7220, loss 0.088662, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:40.231356: step 7221, loss 0.045247, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:40.451597: step 7222, loss 0.0319607, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:40.661460: step 7223, loss 0.0292487, acc 1, learning_rate 0.0001
2017-10-10T15:32:40.859210: step 7224, loss 0.0448258, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:41.063961: step 7225, loss 0.0176344, acc 1, learning_rate 0.0001
2017-10-10T15:32:41.264955: step 7226, loss 0.055652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:41.460112: step 7227, loss 0.0312135, acc 1, learning_rate 0.0001
2017-10-10T15:32:41.655814: step 7228, loss 0.0506912, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:41.972319: step 7229, loss 0.0270009, acc 1, learning_rate 0.0001
2017-10-10T15:32:42.123986: step 7230, loss 0.122359, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:42.260664: step 7231, loss 0.0302694, acc 1, learning_rate 0.0001
2017-10-10T15:32:42.392442: step 7232, loss 0.0970452, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:42.524093: step 7233, loss 0.0356913, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:42.667832: step 7234, loss 0.0399324, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:42.801915: step 7235, loss 0.0343906, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:42.993581: step 7236, loss 0.135793, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:43.244289: step 7237, loss 0.167434, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:43.466085: step 7238, loss 0.0360772, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:43.673503: step 7239, loss 0.110478, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:43.872880: step 7240, loss 0.0660919, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:44.319450: step 7240, loss 0.215132, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7240

2017-10-10T15:32:45.264728: step 7241, loss 0.0212458, acc 1, learning_rate 0.0001
2017-10-10T15:32:45.474500: step 7242, loss 0.0478562, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:45.672591: step 7243, loss 0.0466095, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:45.880068: step 7244, loss 0.122805, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:46.080886: step 7245, loss 0.0407855, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:46.320815: step 7246, loss 0.108802, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:46.519987: step 7247, loss 0.0107442, acc 1, learning_rate 0.0001
2017-10-10T15:32:46.724178: step 7248, loss 0.0865412, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:46.936698: step 7249, loss 0.0503272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:47.136565: step 7250, loss 0.0823092, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:47.336892: step 7251, loss 0.0351551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:47.510027: step 7252, loss 0.0818321, acc 0.960784, learning_rate 0.0001
2017-10-10T15:32:47.698637: step 7253, loss 0.0378557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:47.897778: step 7254, loss 0.0927257, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:48.096980: step 7255, loss 0.0667626, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:48.316867: step 7256, loss 0.053308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:48.544512: step 7257, loss 0.0899925, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:48.745957: step 7258, loss 0.0461485, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:48.953666: step 7259, loss 0.0891083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:49.162679: step 7260, loss 0.0209798, acc 1, learning_rate 0.0001
2017-10-10T15:32:49.355333: step 7261, loss 0.0531818, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:49.580969: step 7262, loss 0.0729692, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:49.788307: step 7263, loss 0.0512698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:50.069148: step 7264, loss 0.0296852, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:50.263075: step 7265, loss 0.0525137, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:50.396832: step 7266, loss 0.160026, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:50.541420: step 7267, loss 0.0150268, acc 1, learning_rate 0.0001
2017-10-10T15:32:50.673348: step 7268, loss 0.0555787, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:50.810579: step 7269, loss 0.0223584, acc 1, learning_rate 0.0001
2017-10-10T15:32:50.944589: step 7270, loss 0.0301681, acc 1, learning_rate 0.0001
2017-10-10T15:32:51.067449: step 7271, loss 0.0460498, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:51.199461: step 7272, loss 0.081058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:51.328537: step 7273, loss 0.0560246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:51.467007: step 7274, loss 0.0915997, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:51.590431: step 7275, loss 0.0541262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:51.719633: step 7276, loss 0.0690355, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:51.862233: step 7277, loss 0.0458293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:51.990122: step 7278, loss 0.0499184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:52.121095: step 7279, loss 0.0136391, acc 1, learning_rate 0.0001
2017-10-10T15:32:52.243672: step 7280, loss 0.045605, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:52.540347: step 7280, loss 0.209183, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7280

2017-10-10T15:32:53.097830: step 7281, loss 0.0295694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:53.221808: step 7282, loss 0.0485833, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:53.345863: step 7283, loss 0.0757394, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:53.468850: step 7284, loss 0.0391648, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:53.592576: step 7285, loss 0.0190388, acc 1, learning_rate 0.0001
2017-10-10T15:32:53.715458: step 7286, loss 0.147763, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:53.840823: step 7287, loss 0.0413949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:53.963847: step 7288, loss 0.0306312, acc 1, learning_rate 0.0001
2017-10-10T15:32:54.090754: step 7289, loss 0.0259498, acc 1, learning_rate 0.0001
2017-10-10T15:32:54.214201: step 7290, loss 0.0498777, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:54.337478: step 7291, loss 0.0764051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:54.458566: step 7292, loss 0.0407293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:54.584585: step 7293, loss 0.0738303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:54.710854: step 7294, loss 0.0140732, acc 1, learning_rate 0.0001
2017-10-10T15:32:54.835102: step 7295, loss 0.147337, acc 0.9375, learning_rate 0.0001
2017-10-10T15:32:54.959595: step 7296, loss 0.02882, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:55.082963: step 7297, loss 0.0592321, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:55.204077: step 7298, loss 0.0290922, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:55.328155: step 7299, loss 0.0855501, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:55.451131: step 7300, loss 0.0477745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:55.575615: step 7301, loss 0.110963, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:55.700001: step 7302, loss 0.0656985, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:55.822314: step 7303, loss 0.0160755, acc 1, learning_rate 0.0001
2017-10-10T15:32:55.944771: step 7304, loss 0.0135044, acc 1, learning_rate 0.0001
2017-10-10T15:32:56.068732: step 7305, loss 0.0567997, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:56.193043: step 7306, loss 0.0752646, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:56.315489: step 7307, loss 0.0985401, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:56.439943: step 7308, loss 0.0858817, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:56.562601: step 7309, loss 0.0193758, acc 1, learning_rate 0.0001
2017-10-10T15:32:56.686667: step 7310, loss 0.0193143, acc 1, learning_rate 0.0001
2017-10-10T15:32:56.805990: step 7311, loss 0.05088, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:56.930661: step 7312, loss 0.0496597, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:57.056040: step 7313, loss 0.0427352, acc 1, learning_rate 0.0001
2017-10-10T15:32:57.179912: step 7314, loss 0.0738714, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:57.304155: step 7315, loss 0.0921877, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:57.427874: step 7316, loss 0.0396598, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:57.550718: step 7317, loss 0.0687435, acc 0.953125, learning_rate 0.0001
2017-10-10T15:32:57.674061: step 7318, loss 0.0617447, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:57.799108: step 7319, loss 0.0443797, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:57.922470: step 7320, loss 0.0864327, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:32:58.225592: step 7320, loss 0.208189, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7320

2017-10-10T15:32:58.774062: step 7321, loss 0.0402519, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:58.898125: step 7322, loss 0.0188812, acc 1, learning_rate 0.0001
2017-10-10T15:32:59.025045: step 7323, loss 0.0687067, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:59.149278: step 7324, loss 0.0612709, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:59.270213: step 7325, loss 0.0263077, acc 1, learning_rate 0.0001
2017-10-10T15:32:59.393656: step 7326, loss 0.0208642, acc 1, learning_rate 0.0001
2017-10-10T15:32:59.518296: step 7327, loss 0.0974823, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:59.639682: step 7328, loss 0.0513171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:32:59.763695: step 7329, loss 0.0730291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:32:59.886162: step 7330, loss 0.031266, acc 1, learning_rate 0.0001
2017-10-10T15:33:00.011602: step 7331, loss 0.129784, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:00.135291: step 7332, loss 0.0896887, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:00.258867: step 7333, loss 0.0318557, acc 1, learning_rate 0.0001
2017-10-10T15:33:00.382117: step 7334, loss 0.0452771, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:00.507447: step 7335, loss 0.0298093, acc 1, learning_rate 0.0001
2017-10-10T15:33:00.632308: step 7336, loss 0.0517576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:00.754816: step 7337, loss 0.0400307, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:00.879998: step 7338, loss 0.0963416, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:01.003874: step 7339, loss 0.0519286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:01.126501: step 7340, loss 0.0446948, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:01.253027: step 7341, loss 0.0383839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:01.375105: step 7342, loss 0.203539, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:01.499613: step 7343, loss 0.100703, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:01.622273: step 7344, loss 0.0204688, acc 1, learning_rate 0.0001
2017-10-10T15:33:01.744978: step 7345, loss 0.0272934, acc 1, learning_rate 0.0001
2017-10-10T15:33:01.868138: step 7346, loss 0.0461128, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:01.991807: step 7347, loss 0.089621, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:02.115035: step 7348, loss 0.0353017, acc 1, learning_rate 0.0001
2017-10-10T15:33:02.236921: step 7349, loss 0.0312134, acc 1, learning_rate 0.0001
2017-10-10T15:33:02.341286: step 7350, loss 0.0591061, acc 0.980392, learning_rate 0.0001
2017-10-10T15:33:02.466985: step 7351, loss 0.0334656, acc 1, learning_rate 0.0001
2017-10-10T15:33:02.587885: step 7352, loss 0.0740797, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:02.711268: step 7353, loss 0.014212, acc 1, learning_rate 0.0001
2017-10-10T15:33:02.833342: step 7354, loss 0.0723514, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:02.956349: step 7355, loss 0.0313005, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:03.081398: step 7356, loss 0.0566019, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:03.204161: step 7357, loss 0.0565711, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:03.329762: step 7358, loss 0.05505, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:03.454566: step 7359, loss 0.0752415, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:03.579276: step 7360, loss 0.0160886, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:03.887252: step 7360, loss 0.208481, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7360

2017-10-10T15:33:04.438444: step 7361, loss 0.0497504, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:04.560374: step 7362, loss 0.0408101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:04.685755: step 7363, loss 0.0398499, acc 1, learning_rate 0.0001
2017-10-10T15:33:04.812565: step 7364, loss 0.101842, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:04.935807: step 7365, loss 0.0343383, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:05.060529: step 7366, loss 0.0638382, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:05.185719: step 7367, loss 0.0727823, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:05.309614: step 7368, loss 0.0307063, acc 1, learning_rate 0.0001
2017-10-10T15:33:05.434062: step 7369, loss 0.0157638, acc 1, learning_rate 0.0001
2017-10-10T15:33:05.556875: step 7370, loss 0.0241706, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:05.682901: step 7371, loss 0.0915149, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:05.805952: step 7372, loss 0.0317524, acc 1, learning_rate 0.0001
2017-10-10T15:33:06.183495: step 7373, loss 0.0695053, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:06.307290: step 7374, loss 0.0534487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:06.430473: step 7375, loss 0.0437749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:06.554435: step 7376, loss 0.0470452, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:06.679273: step 7377, loss 0.178546, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:06.805068: step 7378, loss 0.0949911, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:06.929437: step 7379, loss 0.0612965, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:07.054541: step 7380, loss 0.0426884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:07.179718: step 7381, loss 0.0798002, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:07.307483: step 7382, loss 0.0447158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:07.430913: step 7383, loss 0.0753254, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:07.554446: step 7384, loss 0.0547205, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:07.679529: step 7385, loss 0.0367461, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:07.801451: step 7386, loss 0.0902783, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:07.927309: step 7387, loss 0.0572851, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:08.051594: step 7388, loss 0.0406599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:08.175400: step 7389, loss 0.0108552, acc 1, learning_rate 0.0001
2017-10-10T15:33:08.298419: step 7390, loss 0.0491557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:08.422943: step 7391, loss 0.0180778, acc 1, learning_rate 0.0001
2017-10-10T15:33:08.545342: step 7392, loss 0.0883128, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:08.666656: step 7393, loss 0.0270629, acc 1, learning_rate 0.0001
2017-10-10T15:33:08.790763: step 7394, loss 0.0448074, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:08.914596: step 7395, loss 0.0513431, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:09.038570: step 7396, loss 0.0581929, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:09.161634: step 7397, loss 0.0677151, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:09.286632: step 7398, loss 0.107271, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:09.410963: step 7399, loss 0.0801805, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:09.533509: step 7400, loss 0.01145, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:09.846440: step 7400, loss 0.208647, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7400

2017-10-10T15:33:10.324162: step 7401, loss 0.0183724, acc 1, learning_rate 0.0001
2017-10-10T15:33:10.445989: step 7402, loss 0.0175097, acc 1, learning_rate 0.0001
2017-10-10T15:33:10.569521: step 7403, loss 0.113209, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:10.694861: step 7404, loss 0.0758109, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:10.818281: step 7405, loss 0.0629733, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:10.944412: step 7406, loss 0.0639267, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:11.068373: step 7407, loss 0.0626729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:11.193191: step 7408, loss 0.024133, acc 1, learning_rate 0.0001
2017-10-10T15:33:11.316550: step 7409, loss 0.0251523, acc 1, learning_rate 0.0001
2017-10-10T15:33:11.439374: step 7410, loss 0.0300569, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:11.563651: step 7411, loss 0.103956, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:11.688984: step 7412, loss 0.0908231, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:11.812938: step 7413, loss 0.0173177, acc 1, learning_rate 0.0001
2017-10-10T15:33:11.937012: step 7414, loss 0.0895764, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:12.059175: step 7415, loss 0.0986673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:12.181846: step 7416, loss 0.021098, acc 1, learning_rate 0.0001
2017-10-10T15:33:12.307070: step 7417, loss 0.0720717, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:12.431153: step 7418, loss 0.0152438, acc 1, learning_rate 0.0001
2017-10-10T15:33:12.554363: step 7419, loss 0.0556844, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:12.678580: step 7420, loss 0.0082456, acc 1, learning_rate 0.0001
2017-10-10T15:33:12.801158: step 7421, loss 0.0510711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:12.925912: step 7422, loss 0.0606878, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:13.048850: step 7423, loss 0.051455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:13.174161: step 7424, loss 0.047897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:13.297745: step 7425, loss 0.0211068, acc 1, learning_rate 0.0001
2017-10-10T15:33:13.422909: step 7426, loss 0.067874, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:13.547496: step 7427, loss 0.0448898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:13.677639: step 7428, loss 0.0324716, acc 1, learning_rate 0.0001
2017-10-10T15:33:13.802417: step 7429, loss 0.0630092, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:13.924828: step 7430, loss 0.028073, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:14.048513: step 7431, loss 0.0694046, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:14.171914: step 7432, loss 0.0469336, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:14.297118: step 7433, loss 0.0403568, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:14.423872: step 7434, loss 0.0589249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:14.547810: step 7435, loss 0.0158727, acc 1, learning_rate 0.0001
2017-10-10T15:33:14.670707: step 7436, loss 0.065012, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:14.795982: step 7437, loss 0.0579838, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:14.918749: step 7438, loss 0.0406235, acc 1, learning_rate 0.0001
2017-10-10T15:33:15.041092: step 7439, loss 0.067751, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:15.163162: step 7440, loss 0.112223, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:15.466294: step 7440, loss 0.208994, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7440

2017-10-10T15:33:16.119328: step 7441, loss 0.0101117, acc 1, learning_rate 0.0001
2017-10-10T15:33:16.242380: step 7442, loss 0.0581564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:16.363943: step 7443, loss 0.0807578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:16.487134: step 7444, loss 0.0500928, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:16.610517: step 7445, loss 0.013647, acc 1, learning_rate 0.0001
2017-10-10T15:33:16.733085: step 7446, loss 0.0296832, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:16.855890: step 7447, loss 0.0231554, acc 1, learning_rate 0.0001
2017-10-10T15:33:16.958593: step 7448, loss 0.034147, acc 1, learning_rate 0.0001
2017-10-10T15:33:17.081520: step 7449, loss 0.0127784, acc 1, learning_rate 0.0001
2017-10-10T15:33:17.205815: step 7450, loss 0.0588949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:17.328081: step 7451, loss 0.0474845, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:17.452159: step 7452, loss 0.0330308, acc 1, learning_rate 0.0001
2017-10-10T15:33:17.575560: step 7453, loss 0.0258976, acc 1, learning_rate 0.0001
2017-10-10T15:33:17.699006: step 7454, loss 0.106689, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:17.825404: step 7455, loss 0.0923557, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:17.947957: step 7456, loss 0.076376, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:18.074900: step 7457, loss 0.0813068, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:18.199721: step 7458, loss 0.0522892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:18.322833: step 7459, loss 0.0376642, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:18.447353: step 7460, loss 0.0279787, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:18.576535: step 7461, loss 0.0451421, acc 1, learning_rate 0.0001
2017-10-10T15:33:18.701879: step 7462, loss 0.110189, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:18.823859: step 7463, loss 0.0556565, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:18.948006: step 7464, loss 0.0488171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:19.071007: step 7465, loss 0.0480711, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:19.195441: step 7466, loss 0.0919388, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:19.318466: step 7467, loss 0.0796959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:19.440712: step 7468, loss 0.0913775, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:19.565311: step 7469, loss 0.0362082, acc 1, learning_rate 0.0001
2017-10-10T15:33:19.688940: step 7470, loss 0.0148727, acc 1, learning_rate 0.0001
2017-10-10T15:33:19.810806: step 7471, loss 0.017961, acc 1, learning_rate 0.0001
2017-10-10T15:33:19.934901: step 7472, loss 0.0254169, acc 1, learning_rate 0.0001
2017-10-10T15:33:20.056222: step 7473, loss 0.059801, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:20.179350: step 7474, loss 0.0357035, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:20.302537: step 7475, loss 0.0520184, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:20.424815: step 7476, loss 0.0404302, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:20.549361: step 7477, loss 0.0230094, acc 1, learning_rate 0.0001
2017-10-10T15:33:20.671572: step 7478, loss 0.0771157, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:20.795483: step 7479, loss 0.0432794, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:20.917354: step 7480, loss 0.0542019, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:21.231735: step 7480, loss 0.212478, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7480

2017-10-10T15:33:21.773986: step 7481, loss 0.0421898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:21.897908: step 7482, loss 0.0391314, acc 1, learning_rate 0.0001
2017-10-10T15:33:22.019827: step 7483, loss 0.00910145, acc 1, learning_rate 0.0001
2017-10-10T15:33:22.142093: step 7484, loss 0.0661132, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:22.266162: step 7485, loss 0.0171157, acc 1, learning_rate 0.0001
2017-10-10T15:33:22.386789: step 7486, loss 0.0871063, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:22.511065: step 7487, loss 0.0622331, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:22.633465: step 7488, loss 0.104813, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:22.756113: step 7489, loss 0.0193737, acc 1, learning_rate 0.0001
2017-10-10T15:33:22.880460: step 7490, loss 0.0106979, acc 1, learning_rate 0.0001
2017-10-10T15:33:23.005858: step 7491, loss 0.0224714, acc 1, learning_rate 0.0001
2017-10-10T15:33:23.126149: step 7492, loss 0.166728, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:23.249017: step 7493, loss 0.0533125, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:23.372244: step 7494, loss 0.027119, acc 1, learning_rate 0.0001
2017-10-10T15:33:23.495959: step 7495, loss 0.0576526, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:23.620401: step 7496, loss 0.08594, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:23.746272: step 7497, loss 0.0272199, acc 1, learning_rate 0.0001
2017-10-10T15:33:23.869515: step 7498, loss 0.0680516, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:23.992667: step 7499, loss 0.0532959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:24.120148: step 7500, loss 0.0837932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:24.246045: step 7501, loss 0.075582, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:24.371442: step 7502, loss 0.0418081, acc 1, learning_rate 0.0001
2017-10-10T15:33:24.493470: step 7503, loss 0.0986284, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:24.617179: step 7504, loss 0.0567738, acc 1, learning_rate 0.0001
2017-10-10T15:33:24.744383: step 7505, loss 0.0231535, acc 1, learning_rate 0.0001
2017-10-10T15:33:24.868945: step 7506, loss 0.0911765, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:24.992039: step 7507, loss 0.0458212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:25.116252: step 7508, loss 0.15501, acc 0.921875, learning_rate 0.0001
2017-10-10T15:33:25.242838: step 7509, loss 0.0503185, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:25.367027: step 7510, loss 0.0600464, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:25.492051: step 7511, loss 0.126425, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:25.617481: step 7512, loss 0.0426266, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:25.741351: step 7513, loss 0.0371424, acc 1, learning_rate 0.0001
2017-10-10T15:33:25.864595: step 7514, loss 0.0214865, acc 1, learning_rate 0.0001
2017-10-10T15:33:25.986924: step 7515, loss 0.0297001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:26.110320: step 7516, loss 0.0397433, acc 1, learning_rate 0.0001
2017-10-10T15:33:26.233425: step 7517, loss 0.141201, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:26.356920: step 7518, loss 0.0580138, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:26.480760: step 7519, loss 0.0282086, acc 1, learning_rate 0.0001
2017-10-10T15:33:26.602250: step 7520, loss 0.0468007, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:26.909467: step 7520, loss 0.211375, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7520

2017-10-10T15:33:27.459128: step 7521, loss 0.0487744, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:27.583268: step 7522, loss 0.129232, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:27.708343: step 7523, loss 0.0302544, acc 1, learning_rate 0.0001
2017-10-10T15:33:27.831137: step 7524, loss 0.0743018, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:27.953433: step 7525, loss 0.03867, acc 1, learning_rate 0.0001
2017-10-10T15:33:28.075427: step 7526, loss 0.0136237, acc 1, learning_rate 0.0001
2017-10-10T15:33:28.195611: step 7527, loss 0.0258258, acc 1, learning_rate 0.0001
2017-10-10T15:33:28.320632: step 7528, loss 0.0167857, acc 1, learning_rate 0.0001
2017-10-10T15:33:28.445188: step 7529, loss 0.104079, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:28.568554: step 7530, loss 0.068922, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:28.691419: step 7531, loss 0.0695092, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:28.814368: step 7532, loss 0.019928, acc 1, learning_rate 0.0001
2017-10-10T15:33:28.937043: step 7533, loss 0.0140413, acc 1, learning_rate 0.0001
2017-10-10T15:33:29.061068: step 7534, loss 0.0507166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:29.182779: step 7535, loss 0.168598, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:29.306217: step 7536, loss 0.0307708, acc 1, learning_rate 0.0001
2017-10-10T15:33:29.432626: step 7537, loss 0.0271167, acc 1, learning_rate 0.0001
2017-10-10T15:33:29.557326: step 7538, loss 0.0407289, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:29.681899: step 7539, loss 0.0396464, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:29.806290: step 7540, loss 0.109192, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:29.929770: step 7541, loss 0.070003, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:30.052601: step 7542, loss 0.0719275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:30.174228: step 7543, loss 0.0494945, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:30.297672: step 7544, loss 0.0659379, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:30.421804: step 7545, loss 0.0700992, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:30.526279: step 7546, loss 0.0839932, acc 0.960784, learning_rate 0.0001
2017-10-10T15:33:30.650439: step 7547, loss 0.0424263, acc 1, learning_rate 0.0001
2017-10-10T15:33:30.775163: step 7548, loss 0.122742, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:30.899998: step 7549, loss 0.0433713, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:31.024531: step 7550, loss 0.0306004, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:31.147426: step 7551, loss 0.0252198, acc 1, learning_rate 0.0001
2017-10-10T15:33:31.271591: step 7552, loss 0.061999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:31.399350: step 7553, loss 0.0402602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:31.523120: step 7554, loss 0.0506568, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:31.650545: step 7555, loss 0.0416448, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:31.774851: step 7556, loss 0.0361767, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:31.899837: step 7557, loss 0.0962726, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:32.022114: step 7558, loss 0.0435304, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:32.146217: step 7559, loss 0.057754, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:32.270103: step 7560, loss 0.062659, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:32.582576: step 7560, loss 0.210827, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7560

2017-10-10T15:33:33.127957: step 7561, loss 0.0470232, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:33.252748: step 7562, loss 0.035201, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:33.375008: step 7563, loss 0.0240072, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:33.498450: step 7564, loss 0.0256206, acc 1, learning_rate 0.0001
2017-10-10T15:33:33.620212: step 7565, loss 0.0585656, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:33.744703: step 7566, loss 0.115448, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:33.867396: step 7567, loss 0.0367785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:33.988925: step 7568, loss 0.0146407, acc 1, learning_rate 0.0001
2017-10-10T15:33:34.113533: step 7569, loss 0.0341463, acc 1, learning_rate 0.0001
2017-10-10T15:33:34.235305: step 7570, loss 0.0372946, acc 1, learning_rate 0.0001
2017-10-10T15:33:34.359300: step 7571, loss 0.0542939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:34.483539: step 7572, loss 0.0298657, acc 1, learning_rate 0.0001
2017-10-10T15:33:34.609320: step 7573, loss 0.0228946, acc 1, learning_rate 0.0001
2017-10-10T15:33:34.733159: step 7574, loss 0.0206996, acc 1, learning_rate 0.0001
2017-10-10T15:33:34.856290: step 7575, loss 0.0146555, acc 1, learning_rate 0.0001
2017-10-10T15:33:34.981375: step 7576, loss 0.0703408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:35.106277: step 7577, loss 0.0485124, acc 1, learning_rate 0.0001
2017-10-10T15:33:35.229809: step 7578, loss 0.04262, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:35.357781: step 7579, loss 0.02636, acc 1, learning_rate 0.0001
2017-10-10T15:33:35.483493: step 7580, loss 0.018141, acc 1, learning_rate 0.0001
2017-10-10T15:33:35.606964: step 7581, loss 0.0424569, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:35.732037: step 7582, loss 0.00966842, acc 1, learning_rate 0.0001
2017-10-10T15:33:35.853218: step 7583, loss 0.0490333, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:35.978841: step 7584, loss 0.0813661, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:36.103273: step 7585, loss 0.0721153, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:36.227379: step 7586, loss 0.056915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:36.350600: step 7587, loss 0.0249957, acc 1, learning_rate 0.0001
2017-10-10T15:33:36.472911: step 7588, loss 0.0428415, acc 1, learning_rate 0.0001
2017-10-10T15:33:36.595373: step 7589, loss 0.0110708, acc 1, learning_rate 0.0001
2017-10-10T15:33:36.717778: step 7590, loss 0.0456628, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:36.842080: step 7591, loss 0.0441921, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:36.963582: step 7592, loss 0.0291735, acc 1, learning_rate 0.0001
2017-10-10T15:33:37.086591: step 7593, loss 0.0314096, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:37.210956: step 7594, loss 0.0520567, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:37.334753: step 7595, loss 0.064006, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:37.458485: step 7596, loss 0.0661126, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:37.582346: step 7597, loss 0.0764553, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:37.706474: step 7598, loss 0.0461191, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:37.831243: step 7599, loss 0.0166265, acc 1, learning_rate 0.0001
2017-10-10T15:33:37.955643: step 7600, loss 0.0226847, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:38.256485: step 7600, loss 0.211817, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7600

2017-10-10T15:33:38.799650: step 7601, loss 0.0248172, acc 1, learning_rate 0.0001
2017-10-10T15:33:38.923140: step 7602, loss 0.0809648, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:39.048377: step 7603, loss 0.0179741, acc 1, learning_rate 0.0001
2017-10-10T15:33:39.173771: step 7604, loss 0.122326, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:39.298006: step 7605, loss 0.0603742, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:39.425178: step 7606, loss 0.0611733, acc 1, learning_rate 0.0001
2017-10-10T15:33:39.550058: step 7607, loss 0.0231164, acc 1, learning_rate 0.0001
2017-10-10T15:33:39.672900: step 7608, loss 0.0202788, acc 1, learning_rate 0.0001
2017-10-10T15:33:39.799321: step 7609, loss 0.0483656, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:39.923270: step 7610, loss 0.0962509, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:40.045884: step 7611, loss 0.0147744, acc 1, learning_rate 0.0001
2017-10-10T15:33:40.172008: step 7612, loss 0.022804, acc 1, learning_rate 0.0001
2017-10-10T15:33:40.293930: step 7613, loss 0.0837547, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:40.420539: step 7614, loss 0.0366917, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:40.543987: step 7615, loss 0.0234898, acc 1, learning_rate 0.0001
2017-10-10T15:33:40.667269: step 7616, loss 0.0325272, acc 1, learning_rate 0.0001
2017-10-10T15:33:40.789036: step 7617, loss 0.0132858, acc 1, learning_rate 0.0001
2017-10-10T15:33:40.911947: step 7618, loss 0.0962144, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:41.036321: step 7619, loss 0.0339015, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:41.162073: step 7620, loss 0.0439336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:41.283459: step 7621, loss 0.0779497, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:41.406820: step 7622, loss 0.0868704, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:41.528799: step 7623, loss 0.0828202, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:41.651749: step 7624, loss 0.0648683, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:41.776167: step 7625, loss 0.0122894, acc 1, learning_rate 0.0001
2017-10-10T15:33:41.898008: step 7626, loss 0.0859032, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:42.019941: step 7627, loss 0.10583, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:42.139702: step 7628, loss 0.102764, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:42.262820: step 7629, loss 0.0812855, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:42.387536: step 7630, loss 0.0306391, acc 1, learning_rate 0.0001
2017-10-10T15:33:42.510980: step 7631, loss 0.0836799, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:42.636422: step 7632, loss 0.0902843, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:42.757422: step 7633, loss 0.0516603, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:42.881495: step 7634, loss 0.0777152, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:43.005750: step 7635, loss 0.0569783, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:43.127943: step 7636, loss 0.042595, acc 1, learning_rate 0.0001
2017-10-10T15:33:43.252396: step 7637, loss 0.0473667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:43.376903: step 7638, loss 0.01895, acc 1, learning_rate 0.0001
2017-10-10T15:33:43.500451: step 7639, loss 0.0395166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:43.623867: step 7640, loss 0.0501087, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:43.918895: step 7640, loss 0.209559, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7640

2017-10-10T15:33:44.467589: step 7641, loss 0.0590016, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:44.590700: step 7642, loss 0.0613484, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:44.714315: step 7643, loss 0.0514951, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:44.817628: step 7644, loss 0.122045, acc 0.960784, learning_rate 0.0001
2017-10-10T15:33:44.942308: step 7645, loss 0.0570604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:45.064219: step 7646, loss 0.050115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:45.189174: step 7647, loss 0.097562, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:45.313834: step 7648, loss 0.0314131, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:45.437619: step 7649, loss 0.0716056, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:45.562728: step 7650, loss 0.0620374, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:45.685175: step 7651, loss 0.0392412, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:45.810490: step 7652, loss 0.0400565, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:45.933258: step 7653, loss 0.0226962, acc 1, learning_rate 0.0001
2017-10-10T15:33:46.058133: step 7654, loss 0.0813318, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:46.182731: step 7655, loss 0.0493166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:46.307528: step 7656, loss 0.0723299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:46.432797: step 7657, loss 0.0203298, acc 1, learning_rate 0.0001
2017-10-10T15:33:46.556136: step 7658, loss 0.0806146, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:46.680718: step 7659, loss 0.0146966, acc 1, learning_rate 0.0001
2017-10-10T15:33:46.802569: step 7660, loss 0.00943957, acc 1, learning_rate 0.0001
2017-10-10T15:33:46.928496: step 7661, loss 0.135002, acc 0.921875, learning_rate 0.0001
2017-10-10T15:33:47.051768: step 7662, loss 0.0347167, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:47.175306: step 7663, loss 0.0897499, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:47.298539: step 7664, loss 0.0307471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:47.421740: step 7665, loss 0.118603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:47.544555: step 7666, loss 0.0614612, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:47.669837: step 7667, loss 0.0126955, acc 1, learning_rate 0.0001
2017-10-10T15:33:47.793835: step 7668, loss 0.0219433, acc 1, learning_rate 0.0001
2017-10-10T15:33:47.919121: step 7669, loss 0.039281, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:48.041874: step 7670, loss 0.0419534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:48.167618: step 7671, loss 0.0443157, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:48.292576: step 7672, loss 0.0690784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:48.414936: step 7673, loss 0.0562726, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:48.537656: step 7674, loss 0.0353521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:48.663144: step 7675, loss 0.0330887, acc 1, learning_rate 0.0001
2017-10-10T15:33:48.788697: step 7676, loss 0.0258659, acc 1, learning_rate 0.0001
2017-10-10T15:33:48.911840: step 7677, loss 0.036092, acc 1, learning_rate 0.0001
2017-10-10T15:33:49.034349: step 7678, loss 0.0411345, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:49.159161: step 7679, loss 0.183359, acc 0.9375, learning_rate 0.0001
2017-10-10T15:33:49.283619: step 7680, loss 0.0379445, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:49.588296: step 7680, loss 0.209707, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7680

2017-10-10T15:33:50.137960: step 7681, loss 0.0550438, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:50.264811: step 7682, loss 0.0928355, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:50.387730: step 7683, loss 0.0704183, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:50.513124: step 7684, loss 0.111352, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:50.638976: step 7685, loss 0.0355321, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:50.762417: step 7686, loss 0.0168397, acc 1, learning_rate 0.0001
2017-10-10T15:33:50.887078: step 7687, loss 0.0225665, acc 1, learning_rate 0.0001
2017-10-10T15:33:51.011279: step 7688, loss 0.0215055, acc 1, learning_rate 0.0001
2017-10-10T15:33:51.134804: step 7689, loss 0.061152, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:51.258631: step 7690, loss 0.0310055, acc 1, learning_rate 0.0001
2017-10-10T15:33:51.381506: step 7691, loss 0.087975, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:51.505792: step 7692, loss 0.0207416, acc 1, learning_rate 0.0001
2017-10-10T15:33:51.630379: step 7693, loss 0.0946991, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:51.753020: step 7694, loss 0.117239, acc 0.921875, learning_rate 0.0001
2017-10-10T15:33:51.876880: step 7695, loss 0.120236, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:52.002741: step 7696, loss 0.113317, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:52.128167: step 7697, loss 0.0323973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:52.248912: step 7698, loss 0.0286214, acc 1, learning_rate 0.0001
2017-10-10T15:33:52.371830: step 7699, loss 0.0732206, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:52.497705: step 7700, loss 0.0451626, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:52.620673: step 7701, loss 0.0293455, acc 1, learning_rate 0.0001
2017-10-10T15:33:52.745777: step 7702, loss 0.0772688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:52.872529: step 7703, loss 0.0287445, acc 1, learning_rate 0.0001
2017-10-10T15:33:52.996560: step 7704, loss 0.089672, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:53.118629: step 7705, loss 0.186071, acc 0.921875, learning_rate 0.0001
2017-10-10T15:33:53.241162: step 7706, loss 0.0698929, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:53.367652: step 7707, loss 0.060687, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:53.489519: step 7708, loss 0.0312342, acc 1, learning_rate 0.0001
2017-10-10T15:33:53.613369: step 7709, loss 0.0668574, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:53.738838: step 7710, loss 0.0455918, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:53.862904: step 7711, loss 0.0318165, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:53.986641: step 7712, loss 0.0590021, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:54.110896: step 7713, loss 0.0265976, acc 1, learning_rate 0.0001
2017-10-10T15:33:54.236429: step 7714, loss 0.101237, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:54.358485: step 7715, loss 0.0699062, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:54.483219: step 7716, loss 0.0386529, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:54.606877: step 7717, loss 0.0199606, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:54.732665: step 7718, loss 0.0269129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:54.855864: step 7719, loss 0.0763762, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:54.979504: step 7720, loss 0.0541341, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:33:55.282382: step 7720, loss 0.209016, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7720

2017-10-10T15:33:55.839738: step 7721, loss 0.0212122, acc 1, learning_rate 0.0001
2017-10-10T15:33:55.963523: step 7722, loss 0.0450988, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:56.086329: step 7723, loss 0.0131139, acc 1, learning_rate 0.0001
2017-10-10T15:33:56.209563: step 7724, loss 0.097, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:56.334022: step 7725, loss 0.05552, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:56.459385: step 7726, loss 0.044153, acc 1, learning_rate 0.0001
2017-10-10T15:33:56.585522: step 7727, loss 0.0535647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:56.709992: step 7728, loss 0.0250346, acc 1, learning_rate 0.0001
2017-10-10T15:33:56.833919: step 7729, loss 0.0106552, acc 1, learning_rate 0.0001
2017-10-10T15:33:56.957078: step 7730, loss 0.040179, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:57.083626: step 7731, loss 0.0262706, acc 1, learning_rate 0.0001
2017-10-10T15:33:57.206581: step 7732, loss 0.0485099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:57.327018: step 7733, loss 0.0429186, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:57.451208: step 7734, loss 0.130011, acc 0.921875, learning_rate 0.0001
2017-10-10T15:33:57.575379: step 7735, loss 0.0433658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:57.699971: step 7736, loss 0.0611147, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:57.824562: step 7737, loss 0.00975151, acc 1, learning_rate 0.0001
2017-10-10T15:33:57.948229: step 7738, loss 0.0235185, acc 1, learning_rate 0.0001
2017-10-10T15:33:58.070966: step 7739, loss 0.0107058, acc 1, learning_rate 0.0001
2017-10-10T15:33:58.193700: step 7740, loss 0.0452579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:58.319262: step 7741, loss 0.0365236, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:58.424018: step 7742, loss 0.0834529, acc 0.960784, learning_rate 0.0001
2017-10-10T15:33:58.547064: step 7743, loss 0.0461576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:58.673041: step 7744, loss 0.0745598, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:58.798074: step 7745, loss 0.0378142, acc 1, learning_rate 0.0001
2017-10-10T15:33:58.920744: step 7746, loss 0.0332083, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:59.045921: step 7747, loss 0.0391494, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:59.171762: step 7748, loss 0.0843293, acc 0.96875, learning_rate 0.0001
2017-10-10T15:33:59.295922: step 7749, loss 0.0863726, acc 0.953125, learning_rate 0.0001
2017-10-10T15:33:59.417733: step 7750, loss 0.0205686, acc 1, learning_rate 0.0001
2017-10-10T15:33:59.541795: step 7751, loss 0.0209976, acc 1, learning_rate 0.0001
2017-10-10T15:33:59.668145: step 7752, loss 0.0547429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:59.792155: step 7753, loss 0.0478782, acc 0.984375, learning_rate 0.0001
2017-10-10T15:33:59.916746: step 7754, loss 0.0259678, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:00.041271: step 7755, loss 0.0316884, acc 1, learning_rate 0.0001
2017-10-10T15:34:00.166215: step 7756, loss 0.0447833, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:00.291469: step 7757, loss 0.098844, acc 0.9375, learning_rate 0.0001
2017-10-10T15:34:00.415632: step 7758, loss 0.101144, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:00.540798: step 7759, loss 0.0655056, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:00.665499: step 7760, loss 0.0644677, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:34:00.970465: step 7760, loss 0.208707, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7760

2017-10-10T15:34:01.452748: step 7761, loss 0.0293254, acc 1, learning_rate 0.0001
2017-10-10T15:34:01.578378: step 7762, loss 0.0499614, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:01.703409: step 7763, loss 0.023281, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:01.830729: step 7764, loss 0.0615074, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:01.955372: step 7765, loss 0.0901233, acc 0.953125, learning_rate 0.0001
2017-10-10T15:34:02.077751: step 7766, loss 0.0760722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:02.201080: step 7767, loss 0.0707199, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:02.326479: step 7768, loss 0.0487768, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:02.449493: step 7769, loss 0.0328405, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:02.573723: step 7770, loss 0.0933045, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:02.695863: step 7771, loss 0.042399, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:02.821496: step 7772, loss 0.0483808, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:02.945275: step 7773, loss 0.0433886, acc 1, learning_rate 0.0001
2017-10-10T15:34:03.069630: step 7774, loss 0.0315015, acc 1, learning_rate 0.0001
2017-10-10T15:34:03.194454: step 7775, loss 0.0913693, acc 0.953125, learning_rate 0.0001
2017-10-10T15:34:03.316455: step 7776, loss 0.0549366, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:03.439597: step 7777, loss 0.0436657, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:03.563031: step 7778, loss 0.0637664, acc 1, learning_rate 0.0001
2017-10-10T15:34:03.688045: step 7779, loss 0.0372073, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:03.812071: step 7780, loss 0.102136, acc 0.9375, learning_rate 0.0001
2017-10-10T15:34:03.939870: step 7781, loss 0.0588941, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:04.061682: step 7782, loss 0.0718591, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:04.186699: step 7783, loss 0.0184652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:04.312080: step 7784, loss 0.0466397, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:04.436535: step 7785, loss 0.00998468, acc 1, learning_rate 0.0001
2017-10-10T15:34:04.561183: step 7786, loss 0.0192759, acc 1, learning_rate 0.0001
2017-10-10T15:34:04.685504: step 7787, loss 0.0432832, acc 1, learning_rate 0.0001
2017-10-10T15:34:04.809309: step 7788, loss 0.0610915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:04.938475: step 7789, loss 0.006798, acc 1, learning_rate 0.0001
2017-10-10T15:34:05.062416: step 7790, loss 0.039564, acc 1, learning_rate 0.0001
2017-10-10T15:34:05.186462: step 7791, loss 0.0563414, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:05.311587: step 7792, loss 0.0556347, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:05.435517: step 7793, loss 0.0397782, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:05.558174: step 7794, loss 0.108066, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:05.683355: step 7795, loss 0.0218337, acc 1, learning_rate 0.0001
2017-10-10T15:34:05.808843: step 7796, loss 0.0497878, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:05.932815: step 7797, loss 0.0116021, acc 1, learning_rate 0.0001
2017-10-10T15:34:06.056066: step 7798, loss 0.0320212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:06.178956: step 7799, loss 0.0459275, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:06.303737: step 7800, loss 0.087312, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:34:06.606475: step 7800, loss 0.21129, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7800

2017-10-10T15:34:07.149372: step 7801, loss 0.0185492, acc 1, learning_rate 0.0001
2017-10-10T15:34:07.274010: step 7802, loss 0.045455, acc 1, learning_rate 0.0001
2017-10-10T15:34:07.396724: step 7803, loss 0.0429931, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:07.520423: step 7804, loss 0.0295834, acc 1, learning_rate 0.0001
2017-10-10T15:34:07.643986: step 7805, loss 0.029871, acc 1, learning_rate 0.0001
2017-10-10T15:34:07.766292: step 7806, loss 0.038088, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:07.890214: step 7807, loss 0.0152808, acc 1, learning_rate 0.0001
2017-10-10T15:34:08.013148: step 7808, loss 0.0796428, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:08.136959: step 7809, loss 0.0423272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:08.260179: step 7810, loss 0.0318413, acc 1, learning_rate 0.0001
2017-10-10T15:34:08.384134: step 7811, loss 0.0715051, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:08.508430: step 7812, loss 0.020215, acc 1, learning_rate 0.0001
2017-10-10T15:34:08.633734: step 7813, loss 0.0268582, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:08.756615: step 7814, loss 0.01573, acc 1, learning_rate 0.0001
2017-10-10T15:34:08.880808: step 7815, loss 0.0430976, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:09.005828: step 7816, loss 0.0519487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:09.129898: step 7817, loss 0.0322764, acc 1, learning_rate 0.0001
2017-10-10T15:34:09.254254: step 7818, loss 0.0461441, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:09.377931: step 7819, loss 0.0811767, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:09.501023: step 7820, loss 0.136296, acc 0.9375, learning_rate 0.0001
2017-10-10T15:34:09.626040: step 7821, loss 0.0330896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:09.748862: step 7822, loss 0.0543268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:09.871954: step 7823, loss 0.038381, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:09.994777: step 7824, loss 0.112798, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:10.115994: step 7825, loss 0.0459171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:10.241988: step 7826, loss 0.0259607, acc 1, learning_rate 0.0001
2017-10-10T15:34:10.363985: step 7827, loss 0.0361297, acc 1, learning_rate 0.0001
2017-10-10T15:34:10.485257: step 7828, loss 0.0188229, acc 1, learning_rate 0.0001
2017-10-10T15:34:10.608259: step 7829, loss 0.0223325, acc 1, learning_rate 0.0001
2017-10-10T15:34:10.733439: step 7830, loss 0.0200338, acc 1, learning_rate 0.0001
2017-10-10T15:34:10.856392: step 7831, loss 0.0615115, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:10.980884: step 7832, loss 0.0136673, acc 1, learning_rate 0.0001
2017-10-10T15:34:11.102929: step 7833, loss 0.0710603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:11.227966: step 7834, loss 0.0184852, acc 1, learning_rate 0.0001
2017-10-10T15:34:11.351592: step 7835, loss 0.0708359, acc 0.96875, learning_rate 0.0001
2017-10-10T15:34:11.473321: step 7836, loss 0.0757599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:11.599288: step 7837, loss 0.0300863, acc 1, learning_rate 0.0001
2017-10-10T15:34:11.720882: step 7838, loss 0.0453107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:11.845709: step 7839, loss 0.0734067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:34:11.948415: step 7840, loss 0.192205, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-10T15:34:12.244351: step 7840, loss 0.210518, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664331/checkpoints/model-7840

