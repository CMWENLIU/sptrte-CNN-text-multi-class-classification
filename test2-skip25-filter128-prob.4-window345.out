
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.4
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507657072

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T12:37:56.317092: step 1, loss 12.5138, acc 0.1875, learning_rate 0.005
2017-10-10T12:37:56.479799: step 2, loss 5.16665, acc 0.28125, learning_rate 0.00498
2017-10-10T12:37:56.638205: step 3, loss 5.56622, acc 0.359375, learning_rate 0.00496008
2017-10-10T12:37:56.799246: step 4, loss 9.49192, acc 0.40625, learning_rate 0.00494024
2017-10-10T12:37:56.967860: step 5, loss 10.2419, acc 0.3125, learning_rate 0.00492049
2017-10-10T12:37:57.131672: step 6, loss 6.04734, acc 0.375, learning_rate 0.00490081
2017-10-10T12:37:57.299694: step 7, loss 6.45498, acc 0.4375, learning_rate 0.00488121
2017-10-10T12:37:57.461588: step 8, loss 6.03129, acc 0.421875, learning_rate 0.0048617
2017-10-10T12:37:57.622267: step 9, loss 5.81169, acc 0.4375, learning_rate 0.00484226
2017-10-10T12:37:57.787292: step 10, loss 4.01217, acc 0.515625, learning_rate 0.00482291
2017-10-10T12:37:57.953021: step 11, loss 3.38762, acc 0.5625, learning_rate 0.00480363
2017-10-10T12:37:58.117496: step 12, loss 3.41816, acc 0.515625, learning_rate 0.00478443
2017-10-10T12:37:58.276992: step 13, loss 3.97989, acc 0.40625, learning_rate 0.00476531
2017-10-10T12:37:58.439990: step 14, loss 2.70074, acc 0.609375, learning_rate 0.00474627
2017-10-10T12:37:58.604137: step 15, loss 4.15759, acc 0.390625, learning_rate 0.0047273
2017-10-10T12:37:58.767074: step 16, loss 2.68951, acc 0.5625, learning_rate 0.00470841
2017-10-10T12:37:58.931569: step 17, loss 2.52781, acc 0.609375, learning_rate 0.0046896
2017-10-10T12:37:59.093630: step 18, loss 2.21929, acc 0.59375, learning_rate 0.00467087
2017-10-10T12:37:59.254595: step 19, loss 2.89491, acc 0.546875, learning_rate 0.00465221
2017-10-10T12:37:59.413117: step 20, loss 1.89389, acc 0.59375, learning_rate 0.00463363
2017-10-10T12:37:59.578445: step 21, loss 2.48389, acc 0.59375, learning_rate 0.00461513
2017-10-10T12:37:59.741302: step 22, loss 3.6271, acc 0.5625, learning_rate 0.0045967
2017-10-10T12:37:59.903532: step 23, loss 1.55947, acc 0.796875, learning_rate 0.00457834
2017-10-10T12:38:00.068062: step 24, loss 3.30404, acc 0.53125, learning_rate 0.00456006
2017-10-10T12:38:00.232237: step 25, loss 2.47349, acc 0.59375, learning_rate 0.00454186
2017-10-10T12:38:00.392727: step 26, loss 1.66034, acc 0.703125, learning_rate 0.00452373
2017-10-10T12:38:00.554504: step 27, loss 1.6121, acc 0.625, learning_rate 0.00450567
2017-10-10T12:38:00.719160: step 28, loss 2.02687, acc 0.6875, learning_rate 0.00448769
2017-10-10T12:38:00.882369: step 29, loss 2.40509, acc 0.546875, learning_rate 0.00446978
2017-10-10T12:38:01.043089: step 30, loss 1.50175, acc 0.703125, learning_rate 0.00445194
2017-10-10T12:38:01.205518: step 31, loss 1.50983, acc 0.71875, learning_rate 0.00443418
2017-10-10T12:38:01.367526: step 32, loss 2.44524, acc 0.65625, learning_rate 0.00441649
2017-10-10T12:38:01.530686: step 33, loss 2.2451, acc 0.65625, learning_rate 0.00439887
2017-10-10T12:38:01.693050: step 34, loss 1.86107, acc 0.6875, learning_rate 0.00438132
2017-10-10T12:38:01.854600: step 35, loss 1.10174, acc 0.78125, learning_rate 0.00436385
2017-10-10T12:38:02.025521: step 36, loss 1.67766, acc 0.71875, learning_rate 0.00434644
2017-10-10T12:38:02.188869: step 37, loss 2.07245, acc 0.6875, learning_rate 0.00432911
2017-10-10T12:38:02.352855: step 38, loss 2.2525, acc 0.640625, learning_rate 0.00431185
2017-10-10T12:38:02.512953: step 39, loss 1.47082, acc 0.703125, learning_rate 0.00429465
2017-10-10T12:38:02.679384: step 40, loss 1.46313, acc 0.640625, learning_rate 0.00427753

Evaluation:
2017-10-10T12:38:03.108099: step 40, loss 0.497549, acc 0.860432

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-40

2017-10-10T12:38:03.773522: step 41, loss 2.19282, acc 0.703125, learning_rate 0.00426048
2017-10-10T12:38:03.942545: step 42, loss 0.950114, acc 0.734375, learning_rate 0.0042435
2017-10-10T12:38:04.102614: step 43, loss 1.71212, acc 0.6875, learning_rate 0.00422659
2017-10-10T12:38:04.264013: step 44, loss 1.88124, acc 0.75, learning_rate 0.00420974
2017-10-10T12:38:04.423378: step 45, loss 1.45138, acc 0.703125, learning_rate 0.00419297
2017-10-10T12:38:04.583657: step 46, loss 2.51852, acc 0.640625, learning_rate 0.00417626
2017-10-10T12:38:04.742822: step 47, loss 1.76761, acc 0.703125, learning_rate 0.00415962
2017-10-10T12:38:04.904717: step 48, loss 1.70924, acc 0.671875, learning_rate 0.00414305
2017-10-10T12:38:05.063529: step 49, loss 1.29334, acc 0.6875, learning_rate 0.00412655
2017-10-10T12:38:05.226282: step 50, loss 1.71174, acc 0.703125, learning_rate 0.00411011
2017-10-10T12:38:05.385428: step 51, loss 0.834816, acc 0.875, learning_rate 0.00409375
2017-10-10T12:38:05.545403: step 52, loss 1.88101, acc 0.734375, learning_rate 0.00407744
2017-10-10T12:38:05.706554: step 53, loss 0.840386, acc 0.796875, learning_rate 0.00406121
2017-10-10T12:38:05.871601: step 54, loss 1.00611, acc 0.75, learning_rate 0.00404504
2017-10-10T12:38:06.030368: step 55, loss 1.45353, acc 0.78125, learning_rate 0.00402894
2017-10-10T12:38:06.188859: step 56, loss 1.37722, acc 0.71875, learning_rate 0.0040129
2017-10-10T12:38:06.347942: step 57, loss 0.99526, acc 0.765625, learning_rate 0.00399693
2017-10-10T12:38:06.506723: step 58, loss 1.8638, acc 0.703125, learning_rate 0.00398102
2017-10-10T12:38:06.664851: step 59, loss 1.24425, acc 0.734375, learning_rate 0.00396518
2017-10-10T12:38:06.825401: step 60, loss 1.62388, acc 0.65625, learning_rate 0.00394941
2017-10-10T12:38:06.985697: step 61, loss 1.18666, acc 0.8125, learning_rate 0.00393369
2017-10-10T12:38:07.142813: step 62, loss 0.53425, acc 0.84375, learning_rate 0.00391804
2017-10-10T12:38:07.299978: step 63, loss 1.40241, acc 0.703125, learning_rate 0.00390246
2017-10-10T12:38:07.458407: step 64, loss 1.42821, acc 0.75, learning_rate 0.00388694
2017-10-10T12:38:07.618507: step 65, loss 0.792904, acc 0.828125, learning_rate 0.00387148
2017-10-10T12:38:07.778070: step 66, loss 0.651166, acc 0.84375, learning_rate 0.00385609
2017-10-10T12:38:07.942769: step 67, loss 0.834572, acc 0.796875, learning_rate 0.00384076
2017-10-10T12:38:08.104420: step 68, loss 1.37211, acc 0.71875, learning_rate 0.00382549
2017-10-10T12:38:08.264228: step 69, loss 1.80028, acc 0.703125, learning_rate 0.00381028
2017-10-10T12:38:08.423644: step 70, loss 1.4426, acc 0.71875, learning_rate 0.00379514
2017-10-10T12:38:08.584974: step 71, loss 0.650218, acc 0.828125, learning_rate 0.00378005
2017-10-10T12:38:08.745469: step 72, loss 0.498406, acc 0.859375, learning_rate 0.00376503
2017-10-10T12:38:08.905409: step 73, loss 1.26383, acc 0.78125, learning_rate 0.00375007
2017-10-10T12:38:09.065770: step 74, loss 0.362198, acc 0.875, learning_rate 0.00373517
2017-10-10T12:38:09.226733: step 75, loss 0.440273, acc 0.890625, learning_rate 0.00372034
2017-10-10T12:38:09.390431: step 76, loss 0.853104, acc 0.78125, learning_rate 0.00370556
2017-10-10T12:38:09.552259: step 77, loss 1.21007, acc 0.75, learning_rate 0.00369084
2017-10-10T12:38:09.713317: step 78, loss 0.762386, acc 0.796875, learning_rate 0.00367619
2017-10-10T12:38:09.878109: step 79, loss 0.789168, acc 0.796875, learning_rate 0.00366159
2017-10-10T12:38:10.040236: step 80, loss 1.21233, acc 0.796875, learning_rate 0.00364705

Evaluation:
2017-10-10T12:38:10.457141: step 80, loss 0.426452, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-80

2017-10-10T12:38:11.101960: step 81, loss 1.41025, acc 0.71875, learning_rate 0.00363257
2017-10-10T12:38:11.263363: step 82, loss 1.40858, acc 0.734375, learning_rate 0.00361815
2017-10-10T12:38:11.420114: step 83, loss 1.20616, acc 0.765625, learning_rate 0.00360379
2017-10-10T12:38:11.579577: step 84, loss 0.961675, acc 0.796875, learning_rate 0.00358949
2017-10-10T12:38:11.738709: step 85, loss 0.955554, acc 0.828125, learning_rate 0.00357525
2017-10-10T12:38:11.901062: step 86, loss 0.733257, acc 0.796875, learning_rate 0.00356106
2017-10-10T12:38:12.060270: step 87, loss 1.0371, acc 0.796875, learning_rate 0.00354694
2017-10-10T12:38:12.220807: step 88, loss 0.563384, acc 0.875, learning_rate 0.00353287
2017-10-10T12:38:12.383501: step 89, loss 0.936487, acc 0.796875, learning_rate 0.00351885
2017-10-10T12:38:12.544017: step 90, loss 0.700793, acc 0.8125, learning_rate 0.0035049
2017-10-10T12:38:12.705046: step 91, loss 1.45569, acc 0.765625, learning_rate 0.003491
2017-10-10T12:38:12.868250: step 92, loss 0.463794, acc 0.90625, learning_rate 0.00347716
2017-10-10T12:38:13.029067: step 93, loss 0.645913, acc 0.859375, learning_rate 0.00346338
2017-10-10T12:38:13.191351: step 94, loss 0.629145, acc 0.796875, learning_rate 0.00344965
2017-10-10T12:38:13.353038: step 95, loss 0.87416, acc 0.796875, learning_rate 0.00343597
2017-10-10T12:38:13.513463: step 96, loss 0.945361, acc 0.8125, learning_rate 0.00342236
2017-10-10T12:38:13.672814: step 97, loss 0.937864, acc 0.765625, learning_rate 0.0034088
2017-10-10T12:38:13.809602: step 98, loss 0.749784, acc 0.784314, learning_rate 0.00339529
2017-10-10T12:38:13.977663: step 99, loss 0.513985, acc 0.859375, learning_rate 0.00338184
2017-10-10T12:38:14.138500: step 100, loss 1.12636, acc 0.75, learning_rate 0.00336844
2017-10-10T12:38:14.300106: step 101, loss 0.699646, acc 0.859375, learning_rate 0.0033551
2017-10-10T12:38:14.463231: step 102, loss 0.537551, acc 0.859375, learning_rate 0.00334182
2017-10-10T12:38:14.625081: step 103, loss 1.08921, acc 0.765625, learning_rate 0.00332858
2017-10-10T12:38:14.783045: step 104, loss 0.656519, acc 0.84375, learning_rate 0.00331541
2017-10-10T12:38:14.946891: step 105, loss 0.84818, acc 0.78125, learning_rate 0.00330228
2017-10-10T12:38:15.108768: step 106, loss 0.628888, acc 0.859375, learning_rate 0.00328921
2017-10-10T12:38:15.271447: step 107, loss 0.908214, acc 0.78125, learning_rate 0.00327619
2017-10-10T12:38:15.432687: step 108, loss 0.944527, acc 0.78125, learning_rate 0.00326323
2017-10-10T12:38:15.591881: step 109, loss 0.586191, acc 0.875, learning_rate 0.00325032
2017-10-10T12:38:15.755814: step 110, loss 0.751184, acc 0.859375, learning_rate 0.00323746
2017-10-10T12:38:15.918539: step 111, loss 0.508198, acc 0.84375, learning_rate 0.00322465
2017-10-10T12:38:16.078197: step 112, loss 0.650733, acc 0.859375, learning_rate 0.0032119
2017-10-10T12:38:16.240414: step 113, loss 0.57257, acc 0.765625, learning_rate 0.0031992
2017-10-10T12:38:16.400255: step 114, loss 0.746763, acc 0.859375, learning_rate 0.00318655
2017-10-10T12:38:16.559898: step 115, loss 0.435343, acc 0.859375, learning_rate 0.00317395
2017-10-10T12:38:16.719525: step 116, loss 1.03435, acc 0.75, learning_rate 0.0031614
2017-10-10T12:38:16.879167: step 117, loss 0.557032, acc 0.859375, learning_rate 0.0031489
2017-10-10T12:38:17.039398: step 118, loss 0.831995, acc 0.828125, learning_rate 0.00313646
2017-10-10T12:38:17.202239: step 119, loss 0.503675, acc 0.890625, learning_rate 0.00312407
2017-10-10T12:38:17.362807: step 120, loss 0.840803, acc 0.796875, learning_rate 0.00311172

Evaluation:
2017-10-10T12:38:17.784907: step 120, loss 0.375287, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-120

2017-10-10T12:38:18.431878: step 121, loss 0.326882, acc 0.921875, learning_rate 0.00309943
2017-10-10T12:38:18.591870: step 122, loss 0.501383, acc 0.859375, learning_rate 0.00308719
2017-10-10T12:38:18.749344: step 123, loss 0.193703, acc 0.953125, learning_rate 0.00307499
2017-10-10T12:38:18.916081: step 124, loss 0.923273, acc 0.78125, learning_rate 0.00306285
2017-10-10T12:38:19.079093: step 125, loss 0.781414, acc 0.796875, learning_rate 0.00305076
2017-10-10T12:38:19.239749: step 126, loss 0.293705, acc 0.921875, learning_rate 0.00303871
2017-10-10T12:38:19.399729: step 127, loss 0.860457, acc 0.828125, learning_rate 0.00302672
2017-10-10T12:38:19.560751: step 128, loss 0.692258, acc 0.828125, learning_rate 0.00301477
2017-10-10T12:38:19.723694: step 129, loss 0.759127, acc 0.78125, learning_rate 0.00300287
2017-10-10T12:38:19.887196: step 130, loss 0.396214, acc 0.828125, learning_rate 0.00299102
2017-10-10T12:38:20.051153: step 131, loss 0.696199, acc 0.8125, learning_rate 0.00297922
2017-10-10T12:38:20.211954: step 132, loss 0.70827, acc 0.84375, learning_rate 0.00296747
2017-10-10T12:38:20.374698: step 133, loss 0.488119, acc 0.875, learning_rate 0.00295577
2017-10-10T12:38:20.536320: step 134, loss 0.200053, acc 0.90625, learning_rate 0.00294411
2017-10-10T12:38:20.699021: step 135, loss 0.463587, acc 0.859375, learning_rate 0.0029325
2017-10-10T12:38:20.861351: step 136, loss 0.515554, acc 0.84375, learning_rate 0.00292094
2017-10-10T12:38:21.023575: step 137, loss 0.431688, acc 0.921875, learning_rate 0.00290943
2017-10-10T12:38:21.182848: step 138, loss 0.862103, acc 0.84375, learning_rate 0.00289796
2017-10-10T12:38:21.340923: step 139, loss 0.590841, acc 0.875, learning_rate 0.00288654
2017-10-10T12:38:21.501961: step 140, loss 0.196991, acc 0.921875, learning_rate 0.00287516
2017-10-10T12:38:21.665052: step 141, loss 0.690297, acc 0.8125, learning_rate 0.00286384
2017-10-10T12:38:21.826486: step 142, loss 0.785786, acc 0.78125, learning_rate 0.00285256
2017-10-10T12:38:22.003082: step 143, loss 0.698613, acc 0.796875, learning_rate 0.00284132
2017-10-10T12:38:22.163720: step 144, loss 0.583277, acc 0.828125, learning_rate 0.00283013
2017-10-10T12:38:22.325210: step 145, loss 0.4278, acc 0.859375, learning_rate 0.00281899
2017-10-10T12:38:22.485406: step 146, loss 0.829016, acc 0.8125, learning_rate 0.00280789
2017-10-10T12:38:22.646610: step 147, loss 0.369431, acc 0.890625, learning_rate 0.00279684
2017-10-10T12:38:22.804661: step 148, loss 0.542968, acc 0.8125, learning_rate 0.00278583
2017-10-10T12:38:22.965061: step 149, loss 0.416003, acc 0.875, learning_rate 0.00277486
2017-10-10T12:38:23.124116: step 150, loss 0.642346, acc 0.84375, learning_rate 0.00276395
2017-10-10T12:38:23.284336: step 151, loss 0.830002, acc 0.796875, learning_rate 0.00275307
2017-10-10T12:38:23.444633: step 152, loss 0.550979, acc 0.859375, learning_rate 0.00274224
2017-10-10T12:38:23.607292: step 153, loss 0.518996, acc 0.875, learning_rate 0.00273146
2017-10-10T12:38:23.771516: step 154, loss 0.756746, acc 0.84375, learning_rate 0.00272072
2017-10-10T12:38:23.936342: step 155, loss 0.195137, acc 0.890625, learning_rate 0.00271002
2017-10-10T12:38:24.096735: step 156, loss 0.952474, acc 0.796875, learning_rate 0.00269937
2017-10-10T12:38:24.256281: step 157, loss 0.204298, acc 0.96875, learning_rate 0.00268876
2017-10-10T12:38:24.416152: step 158, loss 0.442742, acc 0.9375, learning_rate 0.00267819
2017-10-10T12:38:24.575079: step 159, loss 0.734162, acc 0.8125, learning_rate 0.00266767
2017-10-10T12:38:24.735874: step 160, loss 0.405586, acc 0.859375, learning_rate 0.00265719

Evaluation:
2017-10-10T12:38:25.149276: step 160, loss 0.337062, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-160

2017-10-10T12:38:25.785408: step 161, loss 0.173377, acc 0.96875, learning_rate 0.00264675
2017-10-10T12:38:25.949614: step 162, loss 0.298238, acc 0.890625, learning_rate 0.00263635
2017-10-10T12:38:26.112512: step 163, loss 0.818865, acc 0.765625, learning_rate 0.002626
2017-10-10T12:38:26.273227: step 164, loss 0.604723, acc 0.8125, learning_rate 0.00261569
2017-10-10T12:38:26.435302: step 165, loss 0.470617, acc 0.84375, learning_rate 0.00260542
2017-10-10T12:38:26.595582: step 166, loss 0.51822, acc 0.859375, learning_rate 0.0025952
2017-10-10T12:38:26.755899: step 167, loss 0.256403, acc 0.890625, learning_rate 0.00258501
2017-10-10T12:38:26.918319: step 168, loss 0.426806, acc 0.875, learning_rate 0.00257487
2017-10-10T12:38:27.079275: step 169, loss 0.334314, acc 0.875, learning_rate 0.00256477
2017-10-10T12:38:27.242452: step 170, loss 0.63975, acc 0.828125, learning_rate 0.0025547
2017-10-10T12:38:27.406027: step 171, loss 0.610818, acc 0.859375, learning_rate 0.00254469
2017-10-10T12:38:27.567929: step 172, loss 0.558529, acc 0.890625, learning_rate 0.00253471
2017-10-10T12:38:27.729927: step 173, loss 0.600402, acc 0.859375, learning_rate 0.00252477
2017-10-10T12:38:27.892360: step 174, loss 0.511117, acc 0.84375, learning_rate 0.00251487
2017-10-10T12:38:28.053119: step 175, loss 0.481207, acc 0.8125, learning_rate 0.00250501
2017-10-10T12:38:28.214994: step 176, loss 0.399867, acc 0.875, learning_rate 0.0024952
2017-10-10T12:38:28.376703: step 177, loss 0.466475, acc 0.859375, learning_rate 0.00248542
2017-10-10T12:38:28.536120: step 178, loss 0.569706, acc 0.8125, learning_rate 0.00247568
2017-10-10T12:38:28.698208: step 179, loss 0.290816, acc 0.90625, learning_rate 0.00246599
2017-10-10T12:38:28.860633: step 180, loss 0.828806, acc 0.84375, learning_rate 0.00245633
2017-10-10T12:38:29.022278: step 181, loss 0.513777, acc 0.84375, learning_rate 0.00244671
2017-10-10T12:38:29.182597: step 182, loss 0.970606, acc 0.765625, learning_rate 0.00243713
2017-10-10T12:38:29.342692: step 183, loss 0.493321, acc 0.84375, learning_rate 0.00242759
2017-10-10T12:38:29.501328: step 184, loss 0.860565, acc 0.8125, learning_rate 0.00241809
2017-10-10T12:38:29.662584: step 185, loss 0.443496, acc 0.890625, learning_rate 0.00240863
2017-10-10T12:38:29.823169: step 186, loss 0.349332, acc 0.90625, learning_rate 0.00239921
2017-10-10T12:38:29.992949: step 187, loss 0.493633, acc 0.828125, learning_rate 0.00238982
2017-10-10T12:38:30.152722: step 188, loss 0.411353, acc 0.890625, learning_rate 0.00238048
2017-10-10T12:38:30.312094: step 189, loss 0.367651, acc 0.859375, learning_rate 0.00237117
2017-10-10T12:38:30.473919: step 190, loss 0.562854, acc 0.8125, learning_rate 0.0023619
2017-10-10T12:38:30.637648: step 191, loss 0.91271, acc 0.796875, learning_rate 0.00235267
2017-10-10T12:38:30.798848: step 192, loss 0.56092, acc 0.8125, learning_rate 0.00234347
2017-10-10T12:38:30.968077: step 193, loss 0.411982, acc 0.875, learning_rate 0.00233431
2017-10-10T12:38:31.128690: step 194, loss 0.218251, acc 0.921875, learning_rate 0.00232519
2017-10-10T12:38:31.287285: step 195, loss 0.165969, acc 0.9375, learning_rate 0.00231611
2017-10-10T12:38:31.420467: step 196, loss 0.782418, acc 0.843137, learning_rate 0.00230707
2017-10-10T12:38:31.580969: step 197, loss 0.107594, acc 0.984375, learning_rate 0.00229806
2017-10-10T12:38:31.744581: step 198, loss 0.581031, acc 0.875, learning_rate 0.00228908
2017-10-10T12:38:31.907945: step 199, loss 0.494675, acc 0.859375, learning_rate 0.00228015
2017-10-10T12:38:32.067396: step 200, loss 0.39268, acc 0.828125, learning_rate 0.00227125

Evaluation:
2017-10-10T12:38:32.472040: step 200, loss 0.332939, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-200

2017-10-10T12:38:33.132892: step 201, loss 0.466091, acc 0.859375, learning_rate 0.00226239
2017-10-10T12:38:33.290921: step 202, loss 0.862013, acc 0.796875, learning_rate 0.00225356
2017-10-10T12:38:33.451094: step 203, loss 0.448228, acc 0.875, learning_rate 0.00224477
2017-10-10T12:38:33.608198: step 204, loss 0.579196, acc 0.828125, learning_rate 0.00223602
2017-10-10T12:38:33.770435: step 205, loss 0.270037, acc 0.90625, learning_rate 0.0022273
2017-10-10T12:38:33.930880: step 206, loss 0.294816, acc 0.921875, learning_rate 0.00221862
2017-10-10T12:38:34.093714: step 207, loss 0.349352, acc 0.890625, learning_rate 0.00220997
2017-10-10T12:38:34.255152: step 208, loss 0.17638, acc 0.9375, learning_rate 0.00220136
2017-10-10T12:38:34.417260: step 209, loss 0.336785, acc 0.875, learning_rate 0.00219278
2017-10-10T12:38:34.577771: step 210, loss 0.384866, acc 0.859375, learning_rate 0.00218424
2017-10-10T12:38:34.740003: step 211, loss 0.673018, acc 0.796875, learning_rate 0.00217573
2017-10-10T12:38:34.900382: step 212, loss 0.227062, acc 0.90625, learning_rate 0.00216726
2017-10-10T12:38:35.061776: step 213, loss 0.250492, acc 0.90625, learning_rate 0.00215882
2017-10-10T12:38:35.227397: step 214, loss 0.433587, acc 0.90625, learning_rate 0.00215041
2017-10-10T12:38:35.388749: step 215, loss 0.210406, acc 0.90625, learning_rate 0.00214204
2017-10-10T12:38:35.547984: step 216, loss 0.523155, acc 0.875, learning_rate 0.00213371
2017-10-10T12:38:35.707919: step 217, loss 0.451429, acc 0.84375, learning_rate 0.00212541
2017-10-10T12:38:35.871096: step 218, loss 0.56285, acc 0.828125, learning_rate 0.00211714
2017-10-10T12:38:36.029291: step 219, loss 0.653478, acc 0.875, learning_rate 0.00210891
2017-10-10T12:38:36.193318: step 220, loss 0.640913, acc 0.859375, learning_rate 0.00210071
2017-10-10T12:38:36.355785: step 221, loss 0.691267, acc 0.78125, learning_rate 0.00209254
2017-10-10T12:38:36.518962: step 222, loss 0.3063, acc 0.90625, learning_rate 0.00208441
2017-10-10T12:38:36.678923: step 223, loss 0.429037, acc 0.859375, learning_rate 0.00207631
2017-10-10T12:38:36.839914: step 224, loss 0.281667, acc 0.90625, learning_rate 0.00206824
2017-10-10T12:38:37.001071: step 225, loss 0.463344, acc 0.859375, learning_rate 0.00206021
2017-10-10T12:38:37.160664: step 226, loss 0.371609, acc 0.890625, learning_rate 0.00205221
2017-10-10T12:38:37.321722: step 227, loss 0.361039, acc 0.890625, learning_rate 0.00204424
2017-10-10T12:38:37.485026: step 228, loss 0.504683, acc 0.828125, learning_rate 0.0020363
2017-10-10T12:38:37.646717: step 229, loss 0.387002, acc 0.90625, learning_rate 0.0020284
2017-10-10T12:38:37.810051: step 230, loss 0.625206, acc 0.828125, learning_rate 0.00202053
2017-10-10T12:38:37.971131: step 231, loss 0.597093, acc 0.828125, learning_rate 0.00201269
2017-10-10T12:38:38.129610: step 232, loss 0.378925, acc 0.828125, learning_rate 0.00200488
2017-10-10T12:38:38.290991: step 233, loss 0.295006, acc 0.890625, learning_rate 0.00199711
2017-10-10T12:38:38.450583: step 234, loss 0.22175, acc 0.921875, learning_rate 0.00198936
2017-10-10T12:38:38.609333: step 235, loss 0.47826, acc 0.859375, learning_rate 0.00198165
2017-10-10T12:38:38.771175: step 236, loss 0.325911, acc 0.890625, learning_rate 0.00197397
2017-10-10T12:38:38.935195: step 237, loss 0.40688, acc 0.859375, learning_rate 0.00196632
2017-10-10T12:38:39.095327: step 238, loss 0.278203, acc 0.890625, learning_rate 0.0019587
2017-10-10T12:38:39.255845: step 239, loss 0.488828, acc 0.875, learning_rate 0.00195112
2017-10-10T12:38:39.416014: step 240, loss 0.252638, acc 0.90625, learning_rate 0.00194356

Evaluation:
2017-10-10T12:38:39.820492: step 240, loss 0.330881, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-240

2017-10-10T12:38:40.459788: step 241, loss 0.364349, acc 0.875, learning_rate 0.00193604
2017-10-10T12:38:40.622507: step 242, loss 0.430823, acc 0.890625, learning_rate 0.00192854
2017-10-10T12:38:40.787002: step 243, loss 0.435214, acc 0.875, learning_rate 0.00192108
2017-10-10T12:38:40.955761: step 244, loss 0.200249, acc 0.921875, learning_rate 0.00191364
2017-10-10T12:38:41.116803: step 245, loss 0.427822, acc 0.90625, learning_rate 0.00190624
2017-10-10T12:38:41.275289: step 246, loss 0.209591, acc 0.921875, learning_rate 0.00189887
2017-10-10T12:38:41.439378: step 247, loss 0.710509, acc 0.828125, learning_rate 0.00189153
2017-10-10T12:38:41.600224: step 248, loss 0.34027, acc 0.890625, learning_rate 0.00188421
2017-10-10T12:38:41.761734: step 249, loss 0.345287, acc 0.890625, learning_rate 0.00187693
2017-10-10T12:38:41.924801: step 250, loss 0.41416, acc 0.8125, learning_rate 0.00186968
2017-10-10T12:38:42.080991: step 251, loss 0.197296, acc 0.9375, learning_rate 0.00186245
2017-10-10T12:38:42.243012: step 252, loss 0.175841, acc 0.9375, learning_rate 0.00185526
2017-10-10T12:38:42.402266: step 253, loss 0.470741, acc 0.875, learning_rate 0.0018481
2017-10-10T12:38:42.560469: step 254, loss 0.490486, acc 0.859375, learning_rate 0.00184096
2017-10-10T12:38:42.852290: step 255, loss 0.295097, acc 0.890625, learning_rate 0.00183385
2017-10-10T12:38:43.013613: step 256, loss 0.692913, acc 0.78125, learning_rate 0.00182678
2017-10-10T12:38:43.176185: step 257, loss 0.404582, acc 0.890625, learning_rate 0.00181973
2017-10-10T12:38:43.337705: step 258, loss 0.31405, acc 0.875, learning_rate 0.00181271
2017-10-10T12:38:43.501834: step 259, loss 0.234812, acc 0.90625, learning_rate 0.00180572
2017-10-10T12:38:43.660227: step 260, loss 0.296426, acc 0.890625, learning_rate 0.00179876
2017-10-10T12:38:43.816822: step 261, loss 0.621279, acc 0.890625, learning_rate 0.00179182
2017-10-10T12:38:43.982713: step 262, loss 0.429587, acc 0.890625, learning_rate 0.00178492
2017-10-10T12:38:44.143434: step 263, loss 0.481242, acc 0.875, learning_rate 0.00177804
2017-10-10T12:38:44.303004: step 264, loss 0.304877, acc 0.90625, learning_rate 0.00177119
2017-10-10T12:38:44.466574: step 265, loss 0.473936, acc 0.875, learning_rate 0.00176437
2017-10-10T12:38:44.625963: step 266, loss 0.332682, acc 0.859375, learning_rate 0.00175758
2017-10-10T12:38:44.788581: step 267, loss 0.511262, acc 0.8125, learning_rate 0.00175081
2017-10-10T12:38:44.952751: step 268, loss 0.242731, acc 0.9375, learning_rate 0.00174407
2017-10-10T12:38:45.115132: step 269, loss 0.647116, acc 0.828125, learning_rate 0.00173736
2017-10-10T12:38:45.275681: step 270, loss 0.24693, acc 0.9375, learning_rate 0.00173068
2017-10-10T12:38:45.439159: step 271, loss 0.285404, acc 0.921875, learning_rate 0.00172402
2017-10-10T12:38:45.606310: step 272, loss 0.32097, acc 0.875, learning_rate 0.00171739
2017-10-10T12:38:45.767435: step 273, loss 0.170187, acc 0.953125, learning_rate 0.00171079
2017-10-10T12:38:45.936858: step 274, loss 0.575735, acc 0.875, learning_rate 0.00170422
2017-10-10T12:38:46.098643: step 275, loss 0.311055, acc 0.890625, learning_rate 0.00169767
2017-10-10T12:38:46.257900: step 276, loss 0.492557, acc 0.859375, learning_rate 0.00169115
2017-10-10T12:38:46.421052: step 277, loss 0.58961, acc 0.84375, learning_rate 0.00168465
2017-10-10T12:38:46.581442: step 278, loss 0.604829, acc 0.84375, learning_rate 0.00167818
2017-10-10T12:38:46.739556: step 279, loss 0.413369, acc 0.921875, learning_rate 0.00167174
2017-10-10T12:38:46.900736: step 280, loss 0.250675, acc 0.953125, learning_rate 0.00166533

Evaluation:
2017-10-10T12:38:47.303159: step 280, loss 0.291281, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-280

2017-10-10T12:38:47.944955: step 281, loss 0.620037, acc 0.84375, learning_rate 0.00165894
2017-10-10T12:38:48.104526: step 282, loss 1.10487, acc 0.796875, learning_rate 0.00165257
2017-10-10T12:38:48.269250: step 283, loss 0.365646, acc 0.875, learning_rate 0.00164624
2017-10-10T12:38:48.432459: step 284, loss 0.330389, acc 0.890625, learning_rate 0.00163993
2017-10-10T12:38:48.592279: step 285, loss 0.329598, acc 0.84375, learning_rate 0.00163364
2017-10-10T12:38:48.753883: step 286, loss 0.238626, acc 0.921875, learning_rate 0.00162738
2017-10-10T12:38:48.915676: step 287, loss 0.318021, acc 0.875, learning_rate 0.00162115
2017-10-10T12:38:49.075824: step 288, loss 0.365446, acc 0.875, learning_rate 0.00161494
2017-10-10T12:38:49.233956: step 289, loss 0.589238, acc 0.8125, learning_rate 0.00160875
2017-10-10T12:38:49.394264: step 290, loss 0.464724, acc 0.84375, learning_rate 0.00160259
2017-10-10T12:38:49.553838: step 291, loss 0.27627, acc 0.9375, learning_rate 0.00159646
2017-10-10T12:38:49.716683: step 292, loss 0.417286, acc 0.890625, learning_rate 0.00159035
2017-10-10T12:38:49.879447: step 293, loss 0.262539, acc 0.921875, learning_rate 0.00158427
2017-10-10T12:38:50.014543: step 294, loss 0.245041, acc 0.921569, learning_rate 0.00157821
2017-10-10T12:38:50.176159: step 295, loss 0.225621, acc 0.90625, learning_rate 0.00157218
2017-10-10T12:38:50.337221: step 296, loss 0.327983, acc 0.90625, learning_rate 0.00156617
2017-10-10T12:38:50.499932: step 297, loss 0.278019, acc 0.90625, learning_rate 0.00156018
2017-10-10T12:38:50.660893: step 298, loss 0.224895, acc 0.9375, learning_rate 0.00155422
2017-10-10T12:38:50.820895: step 299, loss 0.621206, acc 0.875, learning_rate 0.00154829
2017-10-10T12:38:50.994506: step 300, loss 0.202033, acc 0.9375, learning_rate 0.00154238
2017-10-10T12:38:51.155295: step 301, loss 0.593953, acc 0.8125, learning_rate 0.00153649
2017-10-10T12:38:51.316006: step 302, loss 0.396327, acc 0.875, learning_rate 0.00153063
2017-10-10T12:38:51.478581: step 303, loss 0.350355, acc 0.921875, learning_rate 0.00152479
2017-10-10T12:38:51.639167: step 304, loss 0.219602, acc 0.90625, learning_rate 0.00151897
2017-10-10T12:38:51.800531: step 305, loss 0.218431, acc 0.9375, learning_rate 0.00151318
2017-10-10T12:38:51.969730: step 306, loss 0.408453, acc 0.875, learning_rate 0.00150741
2017-10-10T12:38:52.129666: step 307, loss 0.310273, acc 0.875, learning_rate 0.00150167
2017-10-10T12:38:52.290362: step 308, loss 0.353328, acc 0.921875, learning_rate 0.00149594
2017-10-10T12:38:52.448733: step 309, loss 0.17632, acc 0.90625, learning_rate 0.00149025
2017-10-10T12:38:52.609136: step 310, loss 0.208837, acc 0.9375, learning_rate 0.00148457
2017-10-10T12:38:52.770095: step 311, loss 0.236064, acc 0.9375, learning_rate 0.00147892
2017-10-10T12:38:52.933003: step 312, loss 0.113108, acc 0.953125, learning_rate 0.00147329
2017-10-10T12:38:53.097392: step 313, loss 0.23593, acc 0.921875, learning_rate 0.00146769
2017-10-10T12:38:53.256664: step 314, loss 0.688282, acc 0.796875, learning_rate 0.0014621
2017-10-10T12:38:53.421600: step 315, loss 0.450306, acc 0.859375, learning_rate 0.00145654
2017-10-10T12:38:53.583478: step 316, loss 0.586532, acc 0.796875, learning_rate 0.00145101
2017-10-10T12:38:53.744937: step 317, loss 0.145751, acc 0.9375, learning_rate 0.00144549
2017-10-10T12:38:53.908469: step 318, loss 0.380424, acc 0.875, learning_rate 0.00144
2017-10-10T12:38:54.070192: step 319, loss 0.349783, acc 0.921875, learning_rate 0.00143453
2017-10-10T12:38:54.229865: step 320, loss 0.386439, acc 0.875, learning_rate 0.00142908

Evaluation:
2017-10-10T12:38:54.640244: step 320, loss 0.279982, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-320

2017-10-10T12:38:55.289622: step 321, loss 0.350446, acc 0.890625, learning_rate 0.00142366
2017-10-10T12:38:55.449388: step 322, loss 0.357634, acc 0.890625, learning_rate 0.00141826
2017-10-10T12:38:55.612019: step 323, loss 0.199388, acc 0.953125, learning_rate 0.00141288
2017-10-10T12:38:55.773446: step 324, loss 0.413169, acc 0.875, learning_rate 0.00140752
2017-10-10T12:38:55.935013: step 325, loss 0.468513, acc 0.890625, learning_rate 0.00140218
2017-10-10T12:38:56.094233: step 326, loss 0.153319, acc 0.9375, learning_rate 0.00139686
2017-10-10T12:38:56.252835: step 327, loss 0.0982976, acc 0.96875, learning_rate 0.00139157
2017-10-10T12:38:56.415723: step 328, loss 0.546736, acc 0.84375, learning_rate 0.0013863
2017-10-10T12:38:56.573556: step 329, loss 0.144946, acc 0.953125, learning_rate 0.00138105
2017-10-10T12:38:56.731306: step 330, loss 0.305679, acc 0.875, learning_rate 0.00137582
2017-10-10T12:38:56.892111: step 331, loss 0.224447, acc 0.90625, learning_rate 0.00137061
2017-10-10T12:38:57.053468: step 332, loss 0.276921, acc 0.875, learning_rate 0.00136543
2017-10-10T12:38:57.212966: step 333, loss 0.262949, acc 0.859375, learning_rate 0.00136026
2017-10-10T12:38:57.374474: step 334, loss 0.505734, acc 0.875, learning_rate 0.00135512
2017-10-10T12:38:57.534701: step 335, loss 0.138973, acc 0.9375, learning_rate 0.00134999
2017-10-10T12:38:57.694937: step 336, loss 0.374094, acc 0.875, learning_rate 0.00134489
2017-10-10T12:38:57.855516: step 337, loss 0.295822, acc 0.90625, learning_rate 0.00133981
2017-10-10T12:38:58.018572: step 338, loss 0.448183, acc 0.875, learning_rate 0.00133475
2017-10-10T12:38:58.180529: step 339, loss 0.27011, acc 0.875, learning_rate 0.00132971
2017-10-10T12:38:58.341420: step 340, loss 0.379914, acc 0.875, learning_rate 0.00132469
2017-10-10T12:38:58.503958: step 341, loss 0.12687, acc 0.953125, learning_rate 0.00131969
2017-10-10T12:38:58.661774: step 342, loss 0.311464, acc 0.921875, learning_rate 0.00131471
2017-10-10T12:38:58.823383: step 343, loss 0.186443, acc 0.9375, learning_rate 0.00130975
2017-10-10T12:38:58.986494: step 344, loss 0.233163, acc 0.9375, learning_rate 0.00130482
2017-10-10T12:38:59.144532: step 345, loss 0.347276, acc 0.890625, learning_rate 0.0012999
2017-10-10T12:38:59.305808: step 346, loss 0.30074, acc 0.90625, learning_rate 0.001295
2017-10-10T12:38:59.467009: step 347, loss 0.321116, acc 0.890625, learning_rate 0.00129012
2017-10-10T12:38:59.624192: step 348, loss 0.335604, acc 0.859375, learning_rate 0.00128527
2017-10-10T12:38:59.782785: step 349, loss 0.347734, acc 0.921875, learning_rate 0.00128043
2017-10-10T12:38:59.952141: step 350, loss 0.3107, acc 0.90625, learning_rate 0.00127561
2017-10-10T12:39:00.109649: step 351, loss 0.109641, acc 0.96875, learning_rate 0.00127081
2017-10-10T12:39:00.275465: step 352, loss 0.294833, acc 0.90625, learning_rate 0.00126603
2017-10-10T12:39:00.437574: step 353, loss 0.606993, acc 0.8125, learning_rate 0.00126127
2017-10-10T12:39:00.598561: step 354, loss 0.184332, acc 0.953125, learning_rate 0.00125653
2017-10-10T12:39:00.757909: step 355, loss 0.160244, acc 0.921875, learning_rate 0.00125181
2017-10-10T12:39:00.922346: step 356, loss 0.232809, acc 0.90625, learning_rate 0.00124711
2017-10-10T12:39:01.082716: step 357, loss 0.337987, acc 0.859375, learning_rate 0.00124243
2017-10-10T12:39:01.242764: step 358, loss 0.136547, acc 0.96875, learning_rate 0.00123777
2017-10-10T12:39:01.404497: step 359, loss 0.149959, acc 0.953125, learning_rate 0.00123312
2017-10-10T12:39:01.565289: step 360, loss 0.279942, acc 0.90625, learning_rate 0.0012285

Evaluation:
2017-10-10T12:39:01.979282: step 360, loss 0.275553, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-360

2017-10-10T12:39:02.629039: step 361, loss 0.144833, acc 0.921875, learning_rate 0.00122389
2017-10-10T12:39:02.787406: step 362, loss 0.27378, acc 0.921875, learning_rate 0.0012193
2017-10-10T12:39:02.950088: step 363, loss 0.268106, acc 0.890625, learning_rate 0.00121473
2017-10-10T12:39:03.111241: step 364, loss 0.503125, acc 0.875, learning_rate 0.00121018
2017-10-10T12:39:03.273350: step 365, loss 0.409796, acc 0.90625, learning_rate 0.00120565
2017-10-10T12:39:03.431611: step 366, loss 0.408833, acc 0.875, learning_rate 0.00120114
2017-10-10T12:39:03.592826: step 367, loss 0.340187, acc 0.890625, learning_rate 0.00119664
2017-10-10T12:39:03.753523: step 368, loss 0.16248, acc 0.9375, learning_rate 0.00119217
2017-10-10T12:39:03.919791: step 369, loss 0.191026, acc 0.96875, learning_rate 0.00118771
2017-10-10T12:39:04.080554: step 370, loss 0.301911, acc 0.921875, learning_rate 0.00118327
2017-10-10T12:39:04.242347: step 371, loss 0.197154, acc 0.953125, learning_rate 0.00117885
2017-10-10T12:39:04.405042: step 372, loss 0.273037, acc 0.890625, learning_rate 0.00117445
2017-10-10T12:39:04.565778: step 373, loss 0.231355, acc 0.90625, learning_rate 0.00117006
2017-10-10T12:39:04.725663: step 374, loss 0.6784, acc 0.84375, learning_rate 0.00116569
2017-10-10T12:39:04.883988: step 375, loss 0.234483, acc 0.90625, learning_rate 0.00116134
2017-10-10T12:39:05.043799: step 376, loss 0.254512, acc 0.9375, learning_rate 0.00115701
2017-10-10T12:39:05.205207: step 377, loss 0.374011, acc 0.890625, learning_rate 0.0011527
2017-10-10T12:39:05.365624: step 378, loss 0.129959, acc 0.9375, learning_rate 0.0011484
2017-10-10T12:39:05.527639: step 379, loss 0.173984, acc 0.9375, learning_rate 0.00114412
2017-10-10T12:39:05.689597: step 380, loss 0.339991, acc 0.859375, learning_rate 0.00113986
2017-10-10T12:39:05.855495: step 381, loss 0.445551, acc 0.890625, learning_rate 0.00113561
2017-10-10T12:39:06.014007: step 382, loss 0.208586, acc 0.953125, learning_rate 0.00113139
2017-10-10T12:39:06.172785: step 383, loss 0.32996, acc 0.921875, learning_rate 0.00112718
2017-10-10T12:39:06.333838: step 384, loss 0.0955373, acc 0.96875, learning_rate 0.00112298
2017-10-10T12:39:06.496033: step 385, loss 0.141941, acc 0.96875, learning_rate 0.00111881
2017-10-10T12:39:06.655738: step 386, loss 0.211542, acc 0.921875, learning_rate 0.00111465
2017-10-10T12:39:06.816067: step 387, loss 0.316774, acc 0.890625, learning_rate 0.00111051
2017-10-10T12:39:06.976388: step 388, loss 0.296354, acc 0.90625, learning_rate 0.00110638
2017-10-10T12:39:07.135484: step 389, loss 0.35534, acc 0.875, learning_rate 0.00110228
2017-10-10T12:39:07.295494: step 390, loss 0.217666, acc 0.921875, learning_rate 0.00109818
2017-10-10T12:39:07.458953: step 391, loss 0.334357, acc 0.875, learning_rate 0.00109411
2017-10-10T12:39:07.593056: step 392, loss 0.32682, acc 0.882353, learning_rate 0.00109005
2017-10-10T12:39:07.755774: step 393, loss 0.34962, acc 0.859375, learning_rate 0.00108601
2017-10-10T12:39:07.920866: step 394, loss 0.165295, acc 0.921875, learning_rate 0.00108199
2017-10-10T12:39:08.081033: step 395, loss 0.201909, acc 0.9375, learning_rate 0.00107798
2017-10-10T12:39:08.243394: step 396, loss 0.22172, acc 0.9375, learning_rate 0.00107399
2017-10-10T12:39:08.406607: step 397, loss 0.272069, acc 0.90625, learning_rate 0.00107001
2017-10-10T12:39:08.569407: step 398, loss 0.390947, acc 0.890625, learning_rate 0.00106605
2017-10-10T12:39:08.731341: step 399, loss 0.262308, acc 0.90625, learning_rate 0.00106211
2017-10-10T12:39:08.892242: step 400, loss 0.244023, acc 0.90625, learning_rate 0.00105818

Evaluation:
2017-10-10T12:39:09.302508: step 400, loss 0.261648, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-400

2017-10-10T12:39:09.882921: step 401, loss 0.217571, acc 0.890625, learning_rate 0.00105427
2017-10-10T12:39:10.044300: step 402, loss 0.30125, acc 0.921875, learning_rate 0.00105037
2017-10-10T12:39:10.205958: step 403, loss 0.234246, acc 0.9375, learning_rate 0.0010465
2017-10-10T12:39:10.365693: step 404, loss 0.338936, acc 0.875, learning_rate 0.00104263
2017-10-10T12:39:10.528773: step 405, loss 0.283867, acc 0.90625, learning_rate 0.00103878
2017-10-10T12:39:10.688175: step 406, loss 0.241284, acc 0.921875, learning_rate 0.00103495
2017-10-10T12:39:10.850361: step 407, loss 0.531762, acc 0.8125, learning_rate 0.00103114
2017-10-10T12:39:11.015624: step 408, loss 0.215284, acc 0.921875, learning_rate 0.00102734
2017-10-10T12:39:11.177972: step 409, loss 0.613198, acc 0.859375, learning_rate 0.00102355
2017-10-10T12:39:11.338742: step 410, loss 0.245149, acc 0.890625, learning_rate 0.00101978
2017-10-10T12:39:11.497983: step 411, loss 0.300259, acc 0.9375, learning_rate 0.00101603
2017-10-10T12:39:11.652140: step 412, loss 0.356774, acc 0.90625, learning_rate 0.00101229
2017-10-10T12:39:11.812193: step 413, loss 0.291487, acc 0.890625, learning_rate 0.00100856
2017-10-10T12:39:11.973207: step 414, loss 0.237412, acc 0.96875, learning_rate 0.00100486
2017-10-10T12:39:12.133255: step 415, loss 0.260515, acc 0.9375, learning_rate 0.00100116
2017-10-10T12:39:12.294670: step 416, loss 0.152204, acc 0.9375, learning_rate 0.000997483
2017-10-10T12:39:12.455888: step 417, loss 0.144688, acc 0.9375, learning_rate 0.00099382
2017-10-10T12:39:12.614485: step 418, loss 0.225695, acc 0.90625, learning_rate 0.000990172
2017-10-10T12:39:12.778281: step 419, loss 0.209954, acc 0.90625, learning_rate 0.000986538
2017-10-10T12:39:12.939994: step 420, loss 0.500856, acc 0.875, learning_rate 0.00098292
2017-10-10T12:39:13.100550: step 421, loss 0.23273, acc 0.953125, learning_rate 0.000979316
2017-10-10T12:39:13.263314: step 422, loss 0.303651, acc 0.90625, learning_rate 0.000975727
2017-10-10T12:39:13.425311: step 423, loss 0.358259, acc 0.890625, learning_rate 0.000972152
2017-10-10T12:39:13.586232: step 424, loss 0.191777, acc 0.9375, learning_rate 0.000968592
2017-10-10T12:39:13.748730: step 425, loss 0.126462, acc 0.953125, learning_rate 0.000965047
2017-10-10T12:39:13.909749: step 426, loss 0.219022, acc 0.953125, learning_rate 0.000961516
2017-10-10T12:39:14.070929: step 427, loss 0.0378028, acc 1, learning_rate 0.000958
2017-10-10T12:39:14.229087: step 428, loss 0.250851, acc 0.9375, learning_rate 0.000954497
2017-10-10T12:39:14.389831: step 429, loss 0.452738, acc 0.875, learning_rate 0.00095101
2017-10-10T12:39:14.552697: step 430, loss 0.271147, acc 0.9375, learning_rate 0.000947536
2017-10-10T12:39:14.708909: step 431, loss 0.246205, acc 0.921875, learning_rate 0.000944076
2017-10-10T12:39:14.872941: step 432, loss 0.263338, acc 0.90625, learning_rate 0.000940631
2017-10-10T12:39:15.032741: step 433, loss 0.247001, acc 0.921875, learning_rate 0.0009372
2017-10-10T12:39:15.193225: step 434, loss 0.212467, acc 0.953125, learning_rate 0.000933783
2017-10-10T12:39:15.355468: step 435, loss 0.17402, acc 0.921875, learning_rate 0.000930379
2017-10-10T12:39:15.515969: step 436, loss 0.282175, acc 0.921875, learning_rate 0.00092699
2017-10-10T12:39:15.675073: step 437, loss 0.115474, acc 0.96875, learning_rate 0.000923614
2017-10-10T12:39:15.840768: step 438, loss 0.166555, acc 0.921875, learning_rate 0.000920253
2017-10-10T12:39:16.003407: step 439, loss 0.287618, acc 0.875, learning_rate 0.000916905
2017-10-10T12:39:16.165552: step 440, loss 0.560538, acc 0.828125, learning_rate 0.00091357

Evaluation:
2017-10-10T12:39:16.591368: step 440, loss 0.264536, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-440

2017-10-10T12:39:17.267577: step 441, loss 0.17848, acc 0.90625, learning_rate 0.000910249
2017-10-10T12:39:17.427830: step 442, loss 0.168072, acc 0.953125, learning_rate 0.000906942
2017-10-10T12:39:17.586259: step 443, loss 0.219053, acc 0.921875, learning_rate 0.000903648
2017-10-10T12:39:17.746299: step 444, loss 0.501792, acc 0.890625, learning_rate 0.000900368
2017-10-10T12:39:17.909096: step 445, loss 0.380885, acc 0.890625, learning_rate 0.000897101
2017-10-10T12:39:18.070876: step 446, loss 0.243729, acc 0.875, learning_rate 0.000893848
2017-10-10T12:39:18.233829: step 447, loss 0.0670737, acc 0.984375, learning_rate 0.000890607
2017-10-10T12:39:18.395870: step 448, loss 0.355578, acc 0.90625, learning_rate 0.00088738
2017-10-10T12:39:18.556194: step 449, loss 0.266895, acc 0.953125, learning_rate 0.000884166
2017-10-10T12:39:18.717775: step 450, loss 0.175018, acc 0.921875, learning_rate 0.000880966
2017-10-10T12:39:18.880427: step 451, loss 0.196983, acc 0.9375, learning_rate 0.000877778
2017-10-10T12:39:19.039694: step 452, loss 0.27014, acc 0.921875, learning_rate 0.000874603
2017-10-10T12:39:19.201173: step 453, loss 0.315508, acc 0.875, learning_rate 0.000871441
2017-10-10T12:39:19.363194: step 454, loss 0.119898, acc 0.96875, learning_rate 0.000868293
2017-10-10T12:39:19.523629: step 455, loss 0.41073, acc 0.84375, learning_rate 0.000865157
2017-10-10T12:39:19.681928: step 456, loss 0.283372, acc 0.921875, learning_rate 0.000862033
2017-10-10T12:39:19.844456: step 457, loss 0.213245, acc 0.875, learning_rate 0.000858923
2017-10-10T12:39:20.018750: step 458, loss 0.365047, acc 0.859375, learning_rate 0.000855825
2017-10-10T12:39:20.181092: step 459, loss 0.272794, acc 0.921875, learning_rate 0.00085274
2017-10-10T12:39:20.342057: step 460, loss 0.189827, acc 0.953125, learning_rate 0.000849668
2017-10-10T12:39:20.505083: step 461, loss 0.382798, acc 0.859375, learning_rate 0.000846608
2017-10-10T12:39:20.663353: step 462, loss 0.14604, acc 0.953125, learning_rate 0.00084356
2017-10-10T12:39:20.825533: step 463, loss 0.262433, acc 0.921875, learning_rate 0.000840525
2017-10-10T12:39:20.992187: step 464, loss 0.0560044, acc 0.984375, learning_rate 0.000837502
2017-10-10T12:39:21.150463: step 465, loss 0.0826097, acc 0.984375, learning_rate 0.000834492
2017-10-10T12:39:21.308910: step 466, loss 0.354506, acc 0.9375, learning_rate 0.000831494
2017-10-10T12:39:21.469727: step 467, loss 0.262693, acc 0.90625, learning_rate 0.000828508
2017-10-10T12:39:21.633500: step 468, loss 0.169477, acc 0.953125, learning_rate 0.000825535
2017-10-10T12:39:21.796851: step 469, loss 0.307525, acc 0.921875, learning_rate 0.000822573
2017-10-10T12:39:21.958315: step 470, loss 0.0767212, acc 0.96875, learning_rate 0.000819624
2017-10-10T12:39:22.115855: step 471, loss 0.402742, acc 0.828125, learning_rate 0.000816687
2017-10-10T12:39:22.277631: step 472, loss 0.263233, acc 0.921875, learning_rate 0.000813761
2017-10-10T12:39:22.438739: step 473, loss 0.131135, acc 0.96875, learning_rate 0.000810848
2017-10-10T12:39:22.601175: step 474, loss 0.087659, acc 0.984375, learning_rate 0.000807946
2017-10-10T12:39:22.766182: step 475, loss 0.426192, acc 0.890625, learning_rate 0.000805057
2017-10-10T12:39:22.931446: step 476, loss 0.25609, acc 0.90625, learning_rate 0.000802179
2017-10-10T12:39:23.092254: step 477, loss 0.226934, acc 0.921875, learning_rate 0.000799313
2017-10-10T12:39:23.251112: step 478, loss 0.340757, acc 0.890625, learning_rate 0.000796458
2017-10-10T12:39:23.413932: step 479, loss 0.392008, acc 0.859375, learning_rate 0.000793616
2017-10-10T12:39:23.576058: step 480, loss 0.285135, acc 0.9375, learning_rate 0.000790784

Evaluation:
2017-10-10T12:39:23.996228: step 480, loss 0.2621, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-480

2017-10-10T12:39:24.707472: step 481, loss 0.206831, acc 0.90625, learning_rate 0.000787965
2017-10-10T12:39:24.871071: step 482, loss 0.343144, acc 0.875, learning_rate 0.000785157
2017-10-10T12:39:25.032096: step 483, loss 0.342951, acc 0.859375, learning_rate 0.00078236
2017-10-10T12:39:25.192576: step 484, loss 0.142038, acc 0.96875, learning_rate 0.000779575
2017-10-10T12:39:25.353289: step 485, loss 0.202457, acc 0.96875, learning_rate 0.000776801
2017-10-10T12:39:25.519926: step 486, loss 0.212603, acc 0.9375, learning_rate 0.000774038
2017-10-10T12:39:25.676525: step 487, loss 0.214014, acc 0.9375, learning_rate 0.000771287
2017-10-10T12:39:25.833226: step 488, loss 0.198819, acc 0.9375, learning_rate 0.000768547
2017-10-10T12:39:25.996958: step 489, loss 0.423951, acc 0.875, learning_rate 0.000765818
2017-10-10T12:39:26.128284: step 490, loss 0.219385, acc 0.941176, learning_rate 0.000763101
2017-10-10T12:39:26.289013: step 491, loss 0.204823, acc 0.953125, learning_rate 0.000760394
2017-10-10T12:39:26.450139: step 492, loss 0.257991, acc 0.90625, learning_rate 0.000757698
2017-10-10T12:39:26.614359: step 493, loss 0.129251, acc 0.96875, learning_rate 0.000755014
2017-10-10T12:39:26.777932: step 494, loss 0.307964, acc 0.921875, learning_rate 0.00075234
2017-10-10T12:39:26.948208: step 495, loss 0.429679, acc 0.8125, learning_rate 0.000749677
2017-10-10T12:39:27.106495: step 496, loss 0.265647, acc 0.90625, learning_rate 0.000747026
2017-10-10T12:39:27.270388: step 497, loss 0.385325, acc 0.859375, learning_rate 0.000744385
2017-10-10T12:39:27.430406: step 498, loss 0.42411, acc 0.890625, learning_rate 0.000741754
2017-10-10T12:39:27.590532: step 499, loss 0.0832753, acc 0.953125, learning_rate 0.000739135
2017-10-10T12:39:27.752881: step 500, loss 0.0702245, acc 0.984375, learning_rate 0.000736526
2017-10-10T12:39:27.915264: step 501, loss 0.0862557, acc 0.984375, learning_rate 0.000733928
2017-10-10T12:39:28.073678: step 502, loss 0.349271, acc 0.90625, learning_rate 0.00073134
2017-10-10T12:39:28.237382: step 503, loss 0.249207, acc 0.9375, learning_rate 0.000728763
2017-10-10T12:39:28.399502: step 504, loss 0.237698, acc 0.9375, learning_rate 0.000726197
2017-10-10T12:39:28.559359: step 505, loss 0.104282, acc 0.953125, learning_rate 0.000723641
2017-10-10T12:39:28.718896: step 506, loss 0.305275, acc 0.921875, learning_rate 0.000721095
2017-10-10T12:39:28.884298: step 507, loss 0.103839, acc 0.96875, learning_rate 0.00071856
2017-10-10T12:39:29.053659: step 508, loss 0.089567, acc 0.984375, learning_rate 0.000716036
2017-10-10T12:39:29.214631: step 509, loss 0.248513, acc 0.890625, learning_rate 0.000713521
2017-10-10T12:39:29.377542: step 510, loss 0.253919, acc 0.890625, learning_rate 0.000711017
2017-10-10T12:39:29.536979: step 511, loss 0.234136, acc 0.921875, learning_rate 0.000708523
2017-10-10T12:39:29.698606: step 512, loss 0.206973, acc 0.90625, learning_rate 0.000706039
2017-10-10T12:39:29.863766: step 513, loss 0.375413, acc 0.890625, learning_rate 0.000703565
2017-10-10T12:39:30.025269: step 514, loss 0.121806, acc 0.921875, learning_rate 0.000701102
2017-10-10T12:39:30.185224: step 515, loss 0.201647, acc 0.921875, learning_rate 0.000698648
2017-10-10T12:39:30.344597: step 516, loss 0.270668, acc 0.875, learning_rate 0.000696204
2017-10-10T12:39:30.500149: step 517, loss 0.247192, acc 0.9375, learning_rate 0.000693771
2017-10-10T12:39:30.661924: step 518, loss 0.286778, acc 0.890625, learning_rate 0.000691347
2017-10-10T12:39:30.820819: step 519, loss 0.264152, acc 0.90625, learning_rate 0.000688934
2017-10-10T12:39:30.988324: step 520, loss 0.182728, acc 0.953125, learning_rate 0.00068653

Evaluation:
2017-10-10T12:39:31.386416: step 520, loss 0.259388, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-520

2017-10-10T12:39:31.961804: step 521, loss 0.279848, acc 0.921875, learning_rate 0.000684136
2017-10-10T12:39:32.121468: step 522, loss 0.392361, acc 0.828125, learning_rate 0.000681751
2017-10-10T12:39:32.285196: step 523, loss 0.296384, acc 0.921875, learning_rate 0.000679377
2017-10-10T12:39:32.446492: step 524, loss 0.157924, acc 0.921875, learning_rate 0.000677012
2017-10-10T12:39:32.610336: step 525, loss 0.0828128, acc 0.96875, learning_rate 0.000674657
2017-10-10T12:39:32.771847: step 526, loss 0.319478, acc 0.859375, learning_rate 0.000672311
2017-10-10T12:39:32.942676: step 527, loss 0.0903452, acc 0.984375, learning_rate 0.000669975
2017-10-10T12:39:33.103881: step 528, loss 0.283924, acc 0.90625, learning_rate 0.000667648
2017-10-10T12:39:33.265552: step 529, loss 0.245987, acc 0.921875, learning_rate 0.000665331
2017-10-10T12:39:33.426861: step 530, loss 0.0893075, acc 0.96875, learning_rate 0.000663024
2017-10-10T12:39:33.586311: step 531, loss 0.220454, acc 0.921875, learning_rate 0.000660726
2017-10-10T12:39:33.747746: step 532, loss 0.466547, acc 0.90625, learning_rate 0.000658437
2017-10-10T12:39:33.912764: step 533, loss 0.24841, acc 0.9375, learning_rate 0.000656158
2017-10-10T12:39:34.071048: step 534, loss 0.345748, acc 0.890625, learning_rate 0.000653888
2017-10-10T12:39:34.230685: step 535, loss 0.237601, acc 0.890625, learning_rate 0.000651627
2017-10-10T12:39:34.391531: step 536, loss 0.185273, acc 0.921875, learning_rate 0.000649375
2017-10-10T12:39:34.551870: step 537, loss 0.346901, acc 0.890625, learning_rate 0.000647133
2017-10-10T12:39:34.711947: step 538, loss 0.127776, acc 0.9375, learning_rate 0.000644899
2017-10-10T12:39:34.871051: step 539, loss 0.225588, acc 0.890625, learning_rate 0.000642675
2017-10-10T12:39:35.029621: step 540, loss 0.105826, acc 0.953125, learning_rate 0.00064046
2017-10-10T12:39:35.189841: step 541, loss 0.334902, acc 0.921875, learning_rate 0.000638254
2017-10-10T12:39:35.349855: step 542, loss 0.151001, acc 0.953125, learning_rate 0.000636057
2017-10-10T12:39:35.511357: step 543, loss 0.0858925, acc 0.96875, learning_rate 0.000633869
2017-10-10T12:39:35.672961: step 544, loss 0.21295, acc 0.90625, learning_rate 0.00063169
2017-10-10T12:39:35.829216: step 545, loss 0.121067, acc 0.953125, learning_rate 0.00062952
2017-10-10T12:39:35.993240: step 546, loss 0.125654, acc 0.953125, learning_rate 0.000627358
2017-10-10T12:39:36.154311: step 547, loss 0.196424, acc 0.9375, learning_rate 0.000625206
2017-10-10T12:39:36.315492: step 548, loss 0.234208, acc 0.9375, learning_rate 0.000623062
2017-10-10T12:39:36.476106: step 549, loss 0.280818, acc 0.921875, learning_rate 0.000620927
2017-10-10T12:39:36.640102: step 550, loss 0.133094, acc 0.96875, learning_rate 0.000618801
2017-10-10T12:39:36.803905: step 551, loss 0.128342, acc 0.984375, learning_rate 0.000616683
2017-10-10T12:39:36.970086: step 552, loss 0.24226, acc 0.96875, learning_rate 0.000614574
2017-10-10T12:39:37.126743: step 553, loss 0.187023, acc 0.921875, learning_rate 0.000612474
2017-10-10T12:39:37.292124: step 554, loss 0.3007, acc 0.890625, learning_rate 0.000610382
2017-10-10T12:39:37.451931: step 555, loss 0.0422995, acc 1, learning_rate 0.000608299
2017-10-10T12:39:37.613874: step 556, loss 0.292906, acc 0.84375, learning_rate 0.000606224
2017-10-10T12:39:37.776133: step 557, loss 0.270883, acc 0.9375, learning_rate 0.000604158
2017-10-10T12:39:37.939451: step 558, loss 0.286917, acc 0.890625, learning_rate 0.0006021
2017-10-10T12:39:38.098915: step 559, loss 0.197627, acc 0.9375, learning_rate 0.00060005
2017-10-10T12:39:38.262834: step 560, loss 0.148113, acc 0.9375, learning_rate 0.000598009

Evaluation:
2017-10-10T12:39:38.665239: step 560, loss 0.260729, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-560

2017-10-10T12:39:39.320652: step 561, loss 0.156612, acc 0.953125, learning_rate 0.000595977
2017-10-10T12:39:39.478941: step 562, loss 0.314917, acc 0.890625, learning_rate 0.000593952
2017-10-10T12:39:39.642743: step 563, loss 0.291156, acc 0.90625, learning_rate 0.000591936
2017-10-10T12:39:39.800844: step 564, loss 0.140715, acc 0.953125, learning_rate 0.000589928
2017-10-10T12:39:39.962603: step 565, loss 0.102924, acc 0.953125, learning_rate 0.000587928
2017-10-10T12:39:40.130900: step 566, loss 0.254899, acc 0.890625, learning_rate 0.000585937
2017-10-10T12:39:40.292147: step 567, loss 0.108426, acc 0.96875, learning_rate 0.000583953
2017-10-10T12:39:40.455871: step 568, loss 0.115618, acc 0.953125, learning_rate 0.000581978
2017-10-10T12:39:40.615864: step 569, loss 0.140795, acc 0.9375, learning_rate 0.00058001
2017-10-10T12:39:40.774953: step 570, loss 0.37385, acc 0.90625, learning_rate 0.000578051
2017-10-10T12:39:40.938893: step 571, loss 0.0805544, acc 0.984375, learning_rate 0.0005761
2017-10-10T12:39:41.098108: step 572, loss 0.1923, acc 0.9375, learning_rate 0.000574157
2017-10-10T12:39:41.254886: step 573, loss 0.0989839, acc 0.96875, learning_rate 0.000572221
2017-10-10T12:39:41.415154: step 574, loss 0.109264, acc 0.96875, learning_rate 0.000570294
2017-10-10T12:39:41.577329: step 575, loss 0.19159, acc 0.953125, learning_rate 0.000568374
2017-10-10T12:39:41.736447: step 576, loss 0.315757, acc 0.890625, learning_rate 0.000566462
2017-10-10T12:39:41.901513: step 577, loss 0.286991, acc 0.859375, learning_rate 0.000564558
2017-10-10T12:39:42.060765: step 578, loss 0.0818986, acc 0.984375, learning_rate 0.000562662
2017-10-10T12:39:42.223407: step 579, loss 0.184472, acc 0.875, learning_rate 0.000560774
2017-10-10T12:39:42.385286: step 580, loss 0.313236, acc 0.90625, learning_rate 0.000558893
2017-10-10T12:39:42.546083: step 581, loss 0.44529, acc 0.890625, learning_rate 0.00055702
2017-10-10T12:39:42.706988: step 582, loss 0.103157, acc 0.953125, learning_rate 0.000555154
2017-10-10T12:39:42.870856: step 583, loss 0.198688, acc 0.9375, learning_rate 0.000553296
2017-10-10T12:39:43.027719: step 584, loss 0.231688, acc 0.921875, learning_rate 0.000551446
2017-10-10T12:39:43.194460: step 585, loss 0.231776, acc 0.890625, learning_rate 0.000549604
2017-10-10T12:39:43.355662: step 586, loss 0.302003, acc 0.875, learning_rate 0.000547768
2017-10-10T12:39:43.516557: step 587, loss 0.122598, acc 0.953125, learning_rate 0.000545941
2017-10-10T12:39:43.650932: step 588, loss 0.379058, acc 0.901961, learning_rate 0.00054412
2017-10-10T12:39:43.812896: step 589, loss 0.267791, acc 0.890625, learning_rate 0.000542308
2017-10-10T12:39:43.978238: step 590, loss 0.0926422, acc 0.96875, learning_rate 0.000540502
2017-10-10T12:39:44.138601: step 591, loss 0.127098, acc 0.96875, learning_rate 0.000538704
2017-10-10T12:39:44.299229: step 592, loss 0.205614, acc 0.921875, learning_rate 0.000536914
2017-10-10T12:39:44.461406: step 593, loss 0.312751, acc 0.890625, learning_rate 0.00053513
2017-10-10T12:39:44.623748: step 594, loss 0.139051, acc 0.984375, learning_rate 0.000533354
2017-10-10T12:39:44.782855: step 595, loss 0.19002, acc 0.953125, learning_rate 0.000531585
2017-10-10T12:39:44.951555: step 596, loss 0.23878, acc 0.921875, learning_rate 0.000529824
2017-10-10T12:39:45.115819: step 597, loss 0.259322, acc 0.90625, learning_rate 0.000528069
2017-10-10T12:39:45.277978: step 598, loss 0.24735, acc 0.90625, learning_rate 0.000526322
2017-10-10T12:39:45.438329: step 599, loss 0.188446, acc 0.9375, learning_rate 0.000524582
2017-10-10T12:39:45.600456: step 600, loss 0.150529, acc 0.921875, learning_rate 0.000522849

Evaluation:
2017-10-10T12:39:46.002542: step 600, loss 0.252335, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-600

2017-10-10T12:39:46.713882: step 601, loss 0.202745, acc 0.953125, learning_rate 0.000521123
2017-10-10T12:39:46.874353: step 602, loss 0.238178, acc 0.90625, learning_rate 0.000519404
2017-10-10T12:39:47.036767: step 603, loss 0.205967, acc 0.9375, learning_rate 0.000517692
2017-10-10T12:39:47.196652: step 604, loss 0.256276, acc 0.9375, learning_rate 0.000515987
2017-10-10T12:39:47.358957: step 605, loss 0.154392, acc 0.9375, learning_rate 0.000514289
2017-10-10T12:39:47.520300: step 606, loss 0.240195, acc 0.90625, learning_rate 0.000512598
2017-10-10T12:39:47.680430: step 607, loss 0.118511, acc 0.96875, learning_rate 0.000510914
2017-10-10T12:39:47.841062: step 608, loss 0.210479, acc 0.953125, learning_rate 0.000509237
2017-10-10T12:39:48.001152: step 609, loss 0.223099, acc 0.921875, learning_rate 0.000507566
2017-10-10T12:39:48.159316: step 610, loss 0.243776, acc 0.921875, learning_rate 0.000505903
2017-10-10T12:39:48.318875: step 611, loss 0.104091, acc 0.953125, learning_rate 0.000504246
2017-10-10T12:39:48.480271: step 612, loss 0.327477, acc 0.875, learning_rate 0.000502596
2017-10-10T12:39:48.642989: step 613, loss 0.279967, acc 0.90625, learning_rate 0.000500953
2017-10-10T12:39:48.804465: step 614, loss 0.467477, acc 0.875, learning_rate 0.000499316
2017-10-10T12:39:48.977043: step 615, loss 0.119953, acc 0.953125, learning_rate 0.000497686
2017-10-10T12:39:49.138449: step 616, loss 0.109384, acc 0.953125, learning_rate 0.000496063
2017-10-10T12:39:49.297602: step 617, loss 0.207681, acc 0.90625, learning_rate 0.000494446
2017-10-10T12:39:49.460667: step 618, loss 0.185815, acc 0.9375, learning_rate 0.000492836
2017-10-10T12:39:49.621787: step 619, loss 0.103625, acc 0.9375, learning_rate 0.000491233
2017-10-10T12:39:49.786738: step 620, loss 0.232347, acc 0.90625, learning_rate 0.000489636
2017-10-10T12:39:49.945757: step 621, loss 0.0802594, acc 1, learning_rate 0.000488045
2017-10-10T12:39:50.107311: step 622, loss 0.226519, acc 0.875, learning_rate 0.000486461
2017-10-10T12:39:50.269606: step 623, loss 0.242963, acc 0.921875, learning_rate 0.000484884
2017-10-10T12:39:50.432427: step 624, loss 0.15241, acc 0.953125, learning_rate 0.000483313
2017-10-10T12:39:50.589977: step 625, loss 0.167593, acc 0.96875, learning_rate 0.000481748
2017-10-10T12:39:50.748111: step 626, loss 0.120387, acc 0.953125, learning_rate 0.00048019
2017-10-10T12:39:50.915895: step 627, loss 0.191154, acc 0.921875, learning_rate 0.000478638
2017-10-10T12:39:51.076260: step 628, loss 0.332468, acc 0.890625, learning_rate 0.000477093
2017-10-10T12:39:51.235299: step 629, loss 0.152044, acc 0.96875, learning_rate 0.000475554
2017-10-10T12:39:51.398858: step 630, loss 0.0687713, acc 0.984375, learning_rate 0.000474021
2017-10-10T12:39:51.561133: step 631, loss 0.235702, acc 0.890625, learning_rate 0.000472494
2017-10-10T12:39:51.723659: step 632, loss 0.167711, acc 0.9375, learning_rate 0.000470974
2017-10-10T12:39:51.886474: step 633, loss 0.199026, acc 0.90625, learning_rate 0.000469459
2017-10-10T12:39:52.046226: step 634, loss 0.175179, acc 0.9375, learning_rate 0.000467951
2017-10-10T12:39:52.205099: step 635, loss 0.285024, acc 0.90625, learning_rate 0.000466449
2017-10-10T12:39:52.365942: step 636, loss 0.134289, acc 0.96875, learning_rate 0.000464954
2017-10-10T12:39:52.527431: step 637, loss 0.156759, acc 0.9375, learning_rate 0.000463464
2017-10-10T12:39:52.687498: step 638, loss 0.126126, acc 0.953125, learning_rate 0.00046198
2017-10-10T12:39:52.850309: step 639, loss 0.276366, acc 0.875, learning_rate 0.000460503
2017-10-10T12:39:53.013100: step 640, loss 0.21649, acc 0.9375, learning_rate 0.000459031

Evaluation:
2017-10-10T12:39:53.433518: step 640, loss 0.25471, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-640

2017-10-10T12:39:54.021688: step 641, loss 0.224643, acc 0.953125, learning_rate 0.000457566
2017-10-10T12:39:54.180527: step 642, loss 0.136162, acc 0.953125, learning_rate 0.000456106
2017-10-10T12:39:54.342641: step 643, loss 0.340298, acc 0.921875, learning_rate 0.000454653
2017-10-10T12:39:54.504951: step 644, loss 0.33018, acc 0.828125, learning_rate 0.000453205
2017-10-10T12:39:54.672208: step 645, loss 0.31001, acc 0.859375, learning_rate 0.000451764
2017-10-10T12:39:54.836441: step 646, loss 0.355463, acc 0.859375, learning_rate 0.000450328
2017-10-10T12:39:55.003371: step 647, loss 0.105392, acc 0.953125, learning_rate 0.000448898
2017-10-10T12:39:55.163265: step 648, loss 0.338836, acc 0.859375, learning_rate 0.000447474
2017-10-10T12:39:55.324082: step 649, loss 0.179553, acc 0.921875, learning_rate 0.000446055
2017-10-10T12:39:55.483704: step 650, loss 0.177281, acc 0.96875, learning_rate 0.000444643
2017-10-10T12:39:55.646575: step 651, loss 0.384111, acc 0.90625, learning_rate 0.000443236
2017-10-10T12:39:55.809145: step 652, loss 0.157, acc 0.890625, learning_rate 0.000441835
2017-10-10T12:39:55.974763: step 653, loss 0.210016, acc 0.90625, learning_rate 0.00044044
2017-10-10T12:39:56.137716: step 654, loss 0.246512, acc 0.890625, learning_rate 0.00043905
2017-10-10T12:39:56.295696: step 655, loss 0.223754, acc 0.921875, learning_rate 0.000437666
2017-10-10T12:39:56.457901: step 656, loss 0.195843, acc 0.921875, learning_rate 0.000436288
2017-10-10T12:39:56.618795: step 657, loss 0.17709, acc 0.9375, learning_rate 0.000434915
2017-10-10T12:39:56.789780: step 658, loss 0.188319, acc 0.9375, learning_rate 0.000433548
2017-10-10T12:39:56.962779: step 659, loss 0.261195, acc 0.921875, learning_rate 0.000432187
2017-10-10T12:39:57.126051: step 660, loss 0.265159, acc 0.90625, learning_rate 0.000430831
2017-10-10T12:39:57.288438: step 661, loss 0.0668893, acc 0.984375, learning_rate 0.000429481
2017-10-10T12:39:57.451794: step 662, loss 0.265649, acc 0.890625, learning_rate 0.000428136
2017-10-10T12:39:57.613523: step 663, loss 0.190339, acc 0.9375, learning_rate 0.000426796
2017-10-10T12:39:57.773461: step 664, loss 0.279484, acc 0.890625, learning_rate 0.000425463
2017-10-10T12:39:57.947128: step 665, loss 0.129219, acc 0.96875, learning_rate 0.000424134
2017-10-10T12:39:58.112113: step 666, loss 0.280981, acc 0.90625, learning_rate 0.000422811
2017-10-10T12:39:58.270590: step 667, loss 0.132749, acc 0.9375, learning_rate 0.000421493
2017-10-10T12:39:58.441752: step 668, loss 0.0793265, acc 0.96875, learning_rate 0.000420181
2017-10-10T12:39:58.603332: step 669, loss 0.21576, acc 0.921875, learning_rate 0.000418874
2017-10-10T12:39:58.765505: step 670, loss 0.169936, acc 0.953125, learning_rate 0.000417573
2017-10-10T12:39:58.933934: step 671, loss 0.170617, acc 0.921875, learning_rate 0.000416276
2017-10-10T12:39:59.095494: step 672, loss 0.225315, acc 0.90625, learning_rate 0.000414985
2017-10-10T12:39:59.253812: step 673, loss 0.231806, acc 0.921875, learning_rate 0.0004137
2017-10-10T12:39:59.411876: step 674, loss 0.164847, acc 0.921875, learning_rate 0.000412419
2017-10-10T12:39:59.599504: step 675, loss 0.1405, acc 0.953125, learning_rate 0.000411144
2017-10-10T12:39:59.767249: step 676, loss 0.175099, acc 0.9375, learning_rate 0.000409874
2017-10-10T12:39:59.931535: step 677, loss 0.208352, acc 0.890625, learning_rate 0.000408609
2017-10-10T12:40:00.090790: step 678, loss 0.134143, acc 0.96875, learning_rate 0.00040735
2017-10-10T12:40:00.254360: step 679, loss 0.371742, acc 0.90625, learning_rate 0.000406095
2017-10-10T12:40:00.417924: step 680, loss 0.195266, acc 0.921875, learning_rate 0.000404846

Evaluation:
2017-10-10T12:40:00.832291: step 680, loss 0.246784, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-680

2017-10-10T12:40:01.475830: step 681, loss 0.162758, acc 0.921875, learning_rate 0.000403601
2017-10-10T12:40:01.637191: step 682, loss 0.188915, acc 0.9375, learning_rate 0.000402362
2017-10-10T12:40:01.801813: step 683, loss 0.0854136, acc 0.96875, learning_rate 0.000401128
2017-10-10T12:40:01.970013: step 684, loss 0.183335, acc 0.921875, learning_rate 0.000399899
2017-10-10T12:40:02.129847: step 685, loss 0.168116, acc 0.9375, learning_rate 0.000398675
2017-10-10T12:40:02.265120: step 686, loss 0.0853139, acc 0.960784, learning_rate 0.000397456
2017-10-10T12:40:02.426041: step 687, loss 0.166957, acc 0.984375, learning_rate 0.000396241
2017-10-10T12:40:02.586613: step 688, loss 0.150408, acc 0.9375, learning_rate 0.000395032
2017-10-10T12:40:02.746450: step 689, loss 0.108274, acc 0.921875, learning_rate 0.000393828
2017-10-10T12:40:02.911639: step 690, loss 0.0557604, acc 1, learning_rate 0.000392629
2017-10-10T12:40:03.116250: step 691, loss 0.28312, acc 0.890625, learning_rate 0.000391434
2017-10-10T12:40:03.277562: step 692, loss 0.198531, acc 0.921875, learning_rate 0.000390245
2017-10-10T12:40:03.442325: step 693, loss 0.154248, acc 0.921875, learning_rate 0.00038906
2017-10-10T12:40:03.602692: step 694, loss 0.20152, acc 0.953125, learning_rate 0.00038788
2017-10-10T12:40:03.765040: step 695, loss 0.181774, acc 0.9375, learning_rate 0.000386705
2017-10-10T12:40:03.933495: step 696, loss 0.328544, acc 0.890625, learning_rate 0.000385535
2017-10-10T12:40:04.098062: step 697, loss 0.0361897, acc 1, learning_rate 0.000384369
2017-10-10T12:40:04.261551: step 698, loss 0.104768, acc 0.96875, learning_rate 0.000383209
2017-10-10T12:40:04.424337: step 699, loss 0.291955, acc 0.921875, learning_rate 0.000382053
2017-10-10T12:40:04.589187: step 700, loss 0.183503, acc 0.9375, learning_rate 0.000380901
2017-10-10T12:40:04.751087: step 701, loss 0.0663489, acc 0.96875, learning_rate 0.000379755
2017-10-10T12:40:04.914157: step 702, loss 0.19881, acc 0.921875, learning_rate 0.000378613
2017-10-10T12:40:05.073040: step 703, loss 0.184534, acc 0.9375, learning_rate 0.000377476
2017-10-10T12:40:05.233649: step 704, loss 0.230995, acc 0.953125, learning_rate 0.000376343
2017-10-10T12:40:05.398731: step 705, loss 0.309391, acc 0.890625, learning_rate 0.000375215
2017-10-10T12:40:05.559083: step 706, loss 0.193846, acc 0.90625, learning_rate 0.000374092
2017-10-10T12:40:05.718706: step 707, loss 0.237525, acc 0.921875, learning_rate 0.000372973
2017-10-10T12:40:05.886240: step 708, loss 0.223385, acc 0.9375, learning_rate 0.000371859
2017-10-10T12:40:06.045020: step 709, loss 0.17167, acc 0.9375, learning_rate 0.000370749
2017-10-10T12:40:06.202403: step 710, loss 0.221013, acc 0.90625, learning_rate 0.000369644
2017-10-10T12:40:06.366376: step 711, loss 0.188043, acc 0.875, learning_rate 0.000368543
2017-10-10T12:40:06.528988: step 712, loss 0.259805, acc 0.921875, learning_rate 0.000367447
2017-10-10T12:40:06.691089: step 713, loss 0.0796594, acc 0.953125, learning_rate 0.000366356
2017-10-10T12:40:06.855684: step 714, loss 0.165398, acc 0.921875, learning_rate 0.000365268
2017-10-10T12:40:07.014789: step 715, loss 0.147218, acc 0.90625, learning_rate 0.000364186
2017-10-10T12:40:07.177072: step 716, loss 0.264146, acc 0.890625, learning_rate 0.000363107
2017-10-10T12:40:07.342086: step 717, loss 0.425596, acc 0.875, learning_rate 0.000362033
2017-10-10T12:40:07.524995: step 718, loss 0.243836, acc 0.90625, learning_rate 0.000360964
2017-10-10T12:40:07.692424: step 719, loss 0.27903, acc 0.90625, learning_rate 0.000359899
2017-10-10T12:40:07.854580: step 720, loss 0.17103, acc 0.921875, learning_rate 0.000358838

Evaluation:
2017-10-10T12:40:08.258595: step 720, loss 0.244452, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-720

2017-10-10T12:40:08.995310: step 721, loss 0.326399, acc 0.875, learning_rate 0.000357781
2017-10-10T12:40:09.155904: step 722, loss 0.200872, acc 0.9375, learning_rate 0.000356729
2017-10-10T12:40:09.318491: step 723, loss 0.164955, acc 0.953125, learning_rate 0.000355681
2017-10-10T12:40:09.481202: step 724, loss 0.0863903, acc 0.953125, learning_rate 0.000354637
2017-10-10T12:40:09.645330: step 725, loss 0.175771, acc 0.9375, learning_rate 0.000353598
2017-10-10T12:40:09.803549: step 726, loss 0.210553, acc 0.890625, learning_rate 0.000352563
2017-10-10T12:40:09.966514: step 727, loss 0.202742, acc 0.96875, learning_rate 0.000351532
2017-10-10T12:40:10.128260: step 728, loss 0.264379, acc 0.90625, learning_rate 0.000350505
2017-10-10T12:40:10.288524: step 729, loss 0.064876, acc 0.96875, learning_rate 0.000349483
2017-10-10T12:40:10.444609: step 730, loss 0.153295, acc 0.921875, learning_rate 0.000348465
2017-10-10T12:40:10.605341: step 731, loss 0.210529, acc 0.890625, learning_rate 0.00034745
2017-10-10T12:40:10.767961: step 732, loss 0.135625, acc 0.9375, learning_rate 0.00034644
2017-10-10T12:40:10.930140: step 733, loss 0.127808, acc 0.953125, learning_rate 0.000345434
2017-10-10T12:40:11.091519: step 734, loss 0.227889, acc 0.9375, learning_rate 0.000344433
2017-10-10T12:40:11.252892: step 735, loss 0.179505, acc 0.953125, learning_rate 0.000343435
2017-10-10T12:40:11.414569: step 736, loss 0.101908, acc 0.96875, learning_rate 0.000342441
2017-10-10T12:40:11.575439: step 737, loss 0.0810889, acc 0.984375, learning_rate 0.000341452
2017-10-10T12:40:11.733000: step 738, loss 0.198849, acc 0.921875, learning_rate 0.000340466
2017-10-10T12:40:11.894749: step 739, loss 0.223651, acc 0.921875, learning_rate 0.000339485
2017-10-10T12:40:12.056215: step 740, loss 0.17063, acc 0.953125, learning_rate 0.000338507
2017-10-10T12:40:12.219650: step 741, loss 0.129939, acc 0.953125, learning_rate 0.000337534
2017-10-10T12:40:12.381509: step 742, loss 0.163096, acc 0.921875, learning_rate 0.000336564
2017-10-10T12:40:12.544291: step 743, loss 0.293271, acc 0.921875, learning_rate 0.000335598
2017-10-10T12:40:12.703754: step 744, loss 0.163535, acc 0.953125, learning_rate 0.000334637
2017-10-10T12:40:12.867865: step 745, loss 0.129345, acc 0.9375, learning_rate 0.000333679
2017-10-10T12:40:13.027413: step 746, loss 0.14763, acc 0.96875, learning_rate 0.000332725
2017-10-10T12:40:13.213963: step 747, loss 0.0982541, acc 0.953125, learning_rate 0.000331775
2017-10-10T12:40:13.384594: step 748, loss 0.210995, acc 0.875, learning_rate 0.000330829
2017-10-10T12:40:13.567571: step 749, loss 0.14816, acc 0.953125, learning_rate 0.000329887
2017-10-10T12:40:13.740073: step 750, loss 0.103154, acc 0.953125, learning_rate 0.000328949
2017-10-10T12:40:13.916624: step 751, loss 0.138297, acc 0.96875, learning_rate 0.000328014
2017-10-10T12:40:14.088849: step 752, loss 0.143877, acc 0.953125, learning_rate 0.000327083
2017-10-10T12:40:14.283874: step 753, loss 0.132794, acc 0.96875, learning_rate 0.000326157
2017-10-10T12:40:14.460224: step 754, loss 0.368965, acc 0.90625, learning_rate 0.000325233
2017-10-10T12:40:14.635182: step 755, loss 0.216056, acc 0.9375, learning_rate 0.000324314
2017-10-10T12:40:14.811431: step 756, loss 0.0777716, acc 0.96875, learning_rate 0.000323399
2017-10-10T12:40:14.996884: step 757, loss 0.0916069, acc 0.96875, learning_rate 0.000322487
2017-10-10T12:40:15.185366: step 758, loss 0.196772, acc 0.921875, learning_rate 0.000321579
2017-10-10T12:40:15.354205: step 759, loss 0.208695, acc 0.953125, learning_rate 0.000320674
2017-10-10T12:40:15.525337: step 760, loss 0.121548, acc 0.953125, learning_rate 0.000319773

Evaluation:
2017-10-10T12:40:15.967168: step 760, loss 0.246289, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-760

2017-10-10T12:40:16.561162: step 761, loss 0.239831, acc 0.890625, learning_rate 0.000318876
2017-10-10T12:40:16.729800: step 762, loss 0.0916757, acc 0.96875, learning_rate 0.000317983
2017-10-10T12:40:16.912575: step 763, loss 0.0663156, acc 0.984375, learning_rate 0.000317093
2017-10-10T12:40:17.082457: step 764, loss 0.10302, acc 0.96875, learning_rate 0.000316207
2017-10-10T12:40:17.263848: step 765, loss 0.190696, acc 0.9375, learning_rate 0.000315325
2017-10-10T12:40:17.447447: step 766, loss 0.218662, acc 0.890625, learning_rate 0.000314446
2017-10-10T12:40:17.633774: step 767, loss 0.050925, acc 1, learning_rate 0.00031357
2017-10-10T12:40:17.802817: step 768, loss 0.244655, acc 0.953125, learning_rate 0.000312699
2017-10-10T12:40:17.999685: step 769, loss 0.252518, acc 0.890625, learning_rate 0.00031183
2017-10-10T12:40:18.172835: step 770, loss 0.415509, acc 0.828125, learning_rate 0.000310966
2017-10-10T12:40:18.360127: step 771, loss 0.39317, acc 0.84375, learning_rate 0.000310105
2017-10-10T12:40:18.536185: step 772, loss 0.24378, acc 0.9375, learning_rate 0.000309247
2017-10-10T12:40:18.705248: step 773, loss 0.16159, acc 0.9375, learning_rate 0.000308393
2017-10-10T12:40:18.879302: step 774, loss 0.264647, acc 0.890625, learning_rate 0.000307542
2017-10-10T12:40:19.053647: step 775, loss 0.27168, acc 0.90625, learning_rate 0.000306695
2017-10-10T12:40:19.226749: step 776, loss 0.0796083, acc 0.96875, learning_rate 0.000305852
2017-10-10T12:40:19.403018: step 777, loss 0.103238, acc 0.953125, learning_rate 0.000305011
2017-10-10T12:40:19.583081: step 778, loss 0.205236, acc 0.921875, learning_rate 0.000304174
2017-10-10T12:40:19.755373: step 779, loss 0.243373, acc 0.90625, learning_rate 0.000303341
2017-10-10T12:40:19.928684: step 780, loss 0.232555, acc 0.921875, learning_rate 0.000302511
2017-10-10T12:40:20.101146: step 781, loss 0.228654, acc 0.921875, learning_rate 0.000301684
2017-10-10T12:40:20.276287: step 782, loss 0.157895, acc 0.9375, learning_rate 0.000300861
2017-10-10T12:40:20.450307: step 783, loss 0.115036, acc 0.953125, learning_rate 0.000300041
2017-10-10T12:40:20.592719: step 784, loss 0.10924, acc 0.960784, learning_rate 0.000299225
2017-10-10T12:40:20.766801: step 785, loss 0.157726, acc 0.9375, learning_rate 0.000298412
2017-10-10T12:40:20.943568: step 786, loss 0.112178, acc 0.96875, learning_rate 0.000297602
2017-10-10T12:40:21.116657: step 787, loss 0.0783626, acc 0.96875, learning_rate 0.000296795
2017-10-10T12:40:21.301212: step 788, loss 0.388788, acc 0.921875, learning_rate 0.000295992
2017-10-10T12:40:21.473255: step 789, loss 0.134072, acc 0.953125, learning_rate 0.000295192
2017-10-10T12:40:21.647762: step 790, loss 0.247832, acc 0.90625, learning_rate 0.000294395
2017-10-10T12:40:21.825229: step 791, loss 0.157094, acc 0.9375, learning_rate 0.000293602
2017-10-10T12:40:21.997072: step 792, loss 0.203006, acc 0.90625, learning_rate 0.000292812
2017-10-10T12:40:22.166707: step 793, loss 0.280165, acc 0.921875, learning_rate 0.000292025
2017-10-10T12:40:22.339669: step 794, loss 0.177121, acc 0.953125, learning_rate 0.000291241
2017-10-10T12:40:22.510312: step 795, loss 0.250992, acc 0.921875, learning_rate 0.00029046
2017-10-10T12:40:22.686327: step 796, loss 0.15597, acc 0.9375, learning_rate 0.000289683
2017-10-10T12:40:22.859270: step 797, loss 0.179883, acc 0.921875, learning_rate 0.000288908
2017-10-10T12:40:23.029907: step 798, loss 0.188027, acc 0.9375, learning_rate 0.000288137
2017-10-10T12:40:23.202202: step 799, loss 0.266868, acc 0.921875, learning_rate 0.000287369
2017-10-10T12:40:23.379346: step 800, loss 0.123454, acc 0.953125, learning_rate 0.000286605

Evaluation:
2017-10-10T12:40:23.836515: step 800, loss 0.247661, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-800

2017-10-10T12:40:24.510528: step 801, loss 0.252851, acc 0.890625, learning_rate 0.000285843
2017-10-10T12:40:24.698305: step 802, loss 0.100529, acc 0.984375, learning_rate 0.000285084
2017-10-10T12:40:24.891599: step 803, loss 0.0835811, acc 0.96875, learning_rate 0.000284329
2017-10-10T12:40:25.080282: step 804, loss 0.17103, acc 0.921875, learning_rate 0.000283577
2017-10-10T12:40:25.252746: step 805, loss 0.338905, acc 0.90625, learning_rate 0.000282827
2017-10-10T12:40:25.425828: step 806, loss 0.0935406, acc 0.96875, learning_rate 0.000282081
2017-10-10T12:40:25.598638: step 807, loss 0.121272, acc 0.96875, learning_rate 0.000281338
2017-10-10T12:40:25.766326: step 808, loss 0.177981, acc 0.9375, learning_rate 0.000280598
2017-10-10T12:40:25.936990: step 809, loss 0.178078, acc 0.9375, learning_rate 0.00027986
2017-10-10T12:40:26.104744: step 810, loss 0.204083, acc 0.9375, learning_rate 0.000279126
2017-10-10T12:40:26.275267: step 811, loss 0.160509, acc 0.953125, learning_rate 0.000278395
2017-10-10T12:40:26.441863: step 812, loss 0.166858, acc 0.9375, learning_rate 0.000277667
2017-10-10T12:40:26.610990: step 813, loss 0.325744, acc 0.921875, learning_rate 0.000276942
2017-10-10T12:40:26.782280: step 814, loss 0.225611, acc 0.9375, learning_rate 0.00027622
2017-10-10T12:40:26.970060: step 815, loss 0.189313, acc 0.953125, learning_rate 0.0002755
2017-10-10T12:40:27.143723: step 816, loss 0.112956, acc 0.953125, learning_rate 0.000274784
2017-10-10T12:40:27.313780: step 817, loss 0.141043, acc 0.9375, learning_rate 0.000274071
2017-10-10T12:40:27.486439: step 818, loss 0.207931, acc 0.9375, learning_rate 0.00027336
2017-10-10T12:40:27.657095: step 819, loss 0.2249, acc 0.921875, learning_rate 0.000272652
2017-10-10T12:40:27.832324: step 820, loss 0.189645, acc 0.90625, learning_rate 0.000271948
2017-10-10T12:40:28.010545: step 821, loss 0.173643, acc 0.921875, learning_rate 0.000271246
2017-10-10T12:40:28.192410: step 822, loss 0.0715991, acc 1, learning_rate 0.000270547
2017-10-10T12:40:28.373762: step 823, loss 0.210576, acc 0.921875, learning_rate 0.000269851
2017-10-10T12:40:28.560308: step 824, loss 0.25318, acc 0.921875, learning_rate 0.000269157
2017-10-10T12:40:28.757661: step 825, loss 0.125744, acc 0.9375, learning_rate 0.000268467
2017-10-10T12:40:28.951735: step 826, loss 0.261158, acc 0.921875, learning_rate 0.000267779
2017-10-10T12:40:29.146560: step 827, loss 0.427904, acc 0.890625, learning_rate 0.000267094
2017-10-10T12:40:29.317752: step 828, loss 0.0699876, acc 0.953125, learning_rate 0.000266412
2017-10-10T12:40:29.488724: step 829, loss 0.0543151, acc 0.984375, learning_rate 0.000265733
2017-10-10T12:40:29.660432: step 830, loss 0.266228, acc 0.890625, learning_rate 0.000265057
2017-10-10T12:40:29.833219: step 831, loss 0.331083, acc 0.875, learning_rate 0.000264383
2017-10-10T12:40:30.015352: step 832, loss 0.14836, acc 0.921875, learning_rate 0.000263712
2017-10-10T12:40:30.190412: step 833, loss 0.124736, acc 0.953125, learning_rate 0.000263044
2017-10-10T12:40:30.363154: step 834, loss 0.20522, acc 0.9375, learning_rate 0.000262378
2017-10-10T12:40:30.546963: step 835, loss 0.149845, acc 0.921875, learning_rate 0.000261715
2017-10-10T12:40:30.725774: step 836, loss 0.270782, acc 0.890625, learning_rate 0.000261055
2017-10-10T12:40:30.904532: step 837, loss 0.228682, acc 0.90625, learning_rate 0.000260398
2017-10-10T12:40:31.079961: step 838, loss 0.176182, acc 0.90625, learning_rate 0.000259743
2017-10-10T12:40:31.264376: step 839, loss 0.164897, acc 0.96875, learning_rate 0.000259091
2017-10-10T12:40:31.451196: step 840, loss 0.171403, acc 0.9375, learning_rate 0.000258442

Evaluation:
2017-10-10T12:40:31.887858: step 840, loss 0.242485, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-840

2017-10-10T12:40:32.624841: step 841, loss 0.183766, acc 0.921875, learning_rate 0.000257795
2017-10-10T12:40:32.794798: step 842, loss 0.25386, acc 0.921875, learning_rate 0.000257151
2017-10-10T12:40:32.964277: step 843, loss 0.281712, acc 0.890625, learning_rate 0.00025651
2017-10-10T12:40:33.137743: step 844, loss 0.106921, acc 0.96875, learning_rate 0.000255871
2017-10-10T12:40:33.309080: step 845, loss 0.249043, acc 0.890625, learning_rate 0.000255235
2017-10-10T12:40:33.481911: step 846, loss 0.131189, acc 0.953125, learning_rate 0.000254601
2017-10-10T12:40:33.654063: step 847, loss 0.0981798, acc 0.953125, learning_rate 0.00025397
2017-10-10T12:40:33.846173: step 848, loss 0.279445, acc 0.890625, learning_rate 0.000253341
2017-10-10T12:40:34.031837: step 849, loss 0.143086, acc 0.90625, learning_rate 0.000252716
2017-10-10T12:40:34.210176: step 850, loss 0.122745, acc 0.96875, learning_rate 0.000252092
2017-10-10T12:40:34.377703: step 851, loss 0.115832, acc 0.9375, learning_rate 0.000251471
2017-10-10T12:40:34.551774: step 852, loss 0.236076, acc 0.9375, learning_rate 0.000250853
2017-10-10T12:40:34.721948: step 853, loss 0.0794722, acc 0.984375, learning_rate 0.000250237
2017-10-10T12:40:34.902168: step 854, loss 0.0506123, acc 0.984375, learning_rate 0.000249624
2017-10-10T12:40:35.081249: step 855, loss 0.220746, acc 0.90625, learning_rate 0.000249013
2017-10-10T12:40:35.265000: step 856, loss 0.138498, acc 0.96875, learning_rate 0.000248405
2017-10-10T12:40:35.443134: step 857, loss 0.42673, acc 0.875, learning_rate 0.000247799
2017-10-10T12:40:35.622348: step 858, loss 0.132301, acc 0.9375, learning_rate 0.000247196
2017-10-10T12:40:35.794267: step 859, loss 0.0739133, acc 0.953125, learning_rate 0.000246595
2017-10-10T12:40:35.967132: step 860, loss 0.175459, acc 0.953125, learning_rate 0.000245997
2017-10-10T12:40:36.140175: step 861, loss 0.20376, acc 0.890625, learning_rate 0.000245401
2017-10-10T12:40:36.329342: step 862, loss 0.285357, acc 0.9375, learning_rate 0.000244808
2017-10-10T12:40:36.527372: step 863, loss 0.190607, acc 0.9375, learning_rate 0.000244216
2017-10-10T12:40:36.757034: step 864, loss 0.199462, acc 0.921875, learning_rate 0.000243628
2017-10-10T12:40:37.034824: step 865, loss 0.0625763, acc 0.96875, learning_rate 0.000243042
2017-10-10T12:40:37.324870: step 866, loss 0.269934, acc 0.90625, learning_rate 0.000242458
2017-10-10T12:40:37.610592: step 867, loss 0.198236, acc 0.921875, learning_rate 0.000241876
2017-10-10T12:40:37.908572: step 868, loss 0.119313, acc 0.96875, learning_rate 0.000241297
2017-10-10T12:40:38.189023: step 869, loss 0.17854, acc 0.921875, learning_rate 0.00024072
2017-10-10T12:40:38.473754: step 870, loss 0.259223, acc 0.9375, learning_rate 0.000240146
2017-10-10T12:40:38.759813: step 871, loss 0.123366, acc 0.953125, learning_rate 0.000239574
2017-10-10T12:40:39.043111: step 872, loss 0.141901, acc 0.96875, learning_rate 0.000239004
2017-10-10T12:40:39.321559: step 873, loss 0.320928, acc 0.921875, learning_rate 0.000238437
2017-10-10T12:40:39.595809: step 874, loss 0.157839, acc 0.953125, learning_rate 0.000237872
2017-10-10T12:40:39.881260: step 875, loss 0.329325, acc 0.875, learning_rate 0.000237309
2017-10-10T12:40:40.178142: step 876, loss 0.153027, acc 0.9375, learning_rate 0.000236749
2017-10-10T12:40:40.472370: step 877, loss 0.186292, acc 0.9375, learning_rate 0.00023619
2017-10-10T12:40:40.761157: step 878, loss 0.250644, acc 0.921875, learning_rate 0.000235635
2017-10-10T12:40:41.066165: step 879, loss 0.235471, acc 0.9375, learning_rate 0.000235081
2017-10-10T12:40:41.354307: step 880, loss 0.25154, acc 0.921875, learning_rate 0.00023453

Evaluation:
2017-10-10T12:40:42.016073: step 880, loss 0.243911, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-880

2017-10-10T12:40:43.028880: step 881, loss 0.185165, acc 0.9375, learning_rate 0.00023398
2017-10-10T12:40:43.272638: step 882, loss 0.14397, acc 0.921569, learning_rate 0.000233434
2017-10-10T12:40:43.556782: step 883, loss 0.136846, acc 0.953125, learning_rate 0.000232889
2017-10-10T12:40:43.852519: step 884, loss 0.200716, acc 0.90625, learning_rate 0.000232346
2017-10-10T12:40:44.126097: step 885, loss 0.169503, acc 0.9375, learning_rate 0.000231806
2017-10-10T12:40:44.408610: step 886, loss 0.105399, acc 0.984375, learning_rate 0.000231268
2017-10-10T12:40:44.696258: step 887, loss 0.0540563, acc 0.984375, learning_rate 0.000230732
2017-10-10T12:40:44.982203: step 888, loss 0.324816, acc 0.90625, learning_rate 0.000230199
2017-10-10T12:40:45.247576: step 889, loss 0.246916, acc 0.921875, learning_rate 0.000229667
2017-10-10T12:40:45.522698: step 890, loss 0.0515942, acc 0.984375, learning_rate 0.000229138
2017-10-10T12:40:45.797699: step 891, loss 0.24857, acc 0.90625, learning_rate 0.000228611
2017-10-10T12:40:46.087157: step 892, loss 0.239728, acc 0.921875, learning_rate 0.000228086
2017-10-10T12:40:46.367701: step 893, loss 0.20577, acc 0.96875, learning_rate 0.000227563
2017-10-10T12:40:46.659189: step 894, loss 0.217102, acc 0.9375, learning_rate 0.000227043
2017-10-10T12:40:46.969700: step 895, loss 0.175377, acc 0.890625, learning_rate 0.000226524
2017-10-10T12:40:47.250948: step 896, loss 0.0278269, acc 0.984375, learning_rate 0.000226008
2017-10-10T12:40:47.509364: step 897, loss 0.218415, acc 0.890625, learning_rate 0.000225493
2017-10-10T12:40:47.784881: step 898, loss 0.134346, acc 0.953125, learning_rate 0.000224981
2017-10-10T12:40:48.196587: step 899, loss 0.131355, acc 0.9375, learning_rate 0.000224471
2017-10-10T12:40:48.371523: step 900, loss 0.346244, acc 0.890625, learning_rate 0.000223963
2017-10-10T12:40:48.545968: step 901, loss 0.060629, acc 0.96875, learning_rate 0.000223457
2017-10-10T12:40:48.722084: step 902, loss 0.122331, acc 0.953125, learning_rate 0.000222953
2017-10-10T12:40:48.890618: step 903, loss 0.113636, acc 0.953125, learning_rate 0.000222451
2017-10-10T12:40:49.060038: step 904, loss 0.168115, acc 0.953125, learning_rate 0.000221951
2017-10-10T12:40:49.354288: step 905, loss 0.203831, acc 0.953125, learning_rate 0.000221453
2017-10-10T12:40:49.646688: step 906, loss 0.171241, acc 0.9375, learning_rate 0.000220958
2017-10-10T12:40:49.936141: step 907, loss 0.203918, acc 0.875, learning_rate 0.000220464
2017-10-10T12:40:50.205853: step 908, loss 0.126884, acc 0.953125, learning_rate 0.000219972
2017-10-10T12:40:50.467392: step 909, loss 0.123009, acc 0.953125, learning_rate 0.000219483
2017-10-10T12:40:50.749631: step 910, loss 0.147252, acc 0.921875, learning_rate 0.000218995
2017-10-10T12:40:51.031697: step 911, loss 0.135524, acc 0.953125, learning_rate 0.000218509
2017-10-10T12:40:51.322385: step 912, loss 0.280042, acc 0.90625, learning_rate 0.000218025
2017-10-10T12:40:51.619065: step 913, loss 0.144042, acc 0.96875, learning_rate 0.000217544
2017-10-10T12:40:51.903731: step 914, loss 0.161407, acc 0.9375, learning_rate 0.000217064
2017-10-10T12:40:52.190043: step 915, loss 0.172482, acc 0.9375, learning_rate 0.000216586
2017-10-10T12:40:52.476391: step 916, loss 0.167344, acc 0.9375, learning_rate 0.00021611
2017-10-10T12:40:52.772969: step 917, loss 0.156495, acc 0.9375, learning_rate 0.000215636
2017-10-10T12:40:53.076621: step 918, loss 0.0883821, acc 0.953125, learning_rate 0.000215164
2017-10-10T12:40:53.376366: step 919, loss 0.135464, acc 0.9375, learning_rate 0.000214694
2017-10-10T12:40:53.660914: step 920, loss 0.145536, acc 0.96875, learning_rate 0.000214226

Evaluation:
2017-10-10T12:40:54.348744: step 920, loss 0.240752, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-920

2017-10-10T12:40:55.474846: step 921, loss 0.15892, acc 0.9375, learning_rate 0.00021376
2017-10-10T12:40:55.740022: step 922, loss 0.381967, acc 0.90625, learning_rate 0.000213295
2017-10-10T12:40:56.014702: step 923, loss 0.0897659, acc 0.96875, learning_rate 0.000212833
2017-10-10T12:40:56.300187: step 924, loss 0.168454, acc 0.9375, learning_rate 0.000212372
2017-10-10T12:40:56.596214: step 925, loss 0.1035, acc 0.984375, learning_rate 0.000211914
2017-10-10T12:40:56.881345: step 926, loss 0.0857285, acc 0.953125, learning_rate 0.000211457
2017-10-10T12:40:57.162147: step 927, loss 0.103504, acc 0.984375, learning_rate 0.000211002
2017-10-10T12:40:57.441371: step 928, loss 0.227697, acc 0.921875, learning_rate 0.000210549
2017-10-10T12:40:57.717624: step 929, loss 0.149701, acc 0.984375, learning_rate 0.000210098
2017-10-10T12:40:58.007311: step 930, loss 0.141514, acc 0.921875, learning_rate 0.000209648
2017-10-10T12:40:58.292378: step 931, loss 0.252214, acc 0.890625, learning_rate 0.000209201
2017-10-10T12:40:58.567706: step 932, loss 0.0849697, acc 0.953125, learning_rate 0.000208755
2017-10-10T12:40:58.861628: step 933, loss 0.127876, acc 0.9375, learning_rate 0.000208311
2017-10-10T12:40:59.154889: step 934, loss 0.127683, acc 0.953125, learning_rate 0.000207869
2017-10-10T12:40:59.456517: step 935, loss 0.232814, acc 0.90625, learning_rate 0.000207429
2017-10-10T12:40:59.745909: step 936, loss 0.312701, acc 0.859375, learning_rate 0.00020699
2017-10-10T12:41:00.036533: step 937, loss 0.0387271, acc 1, learning_rate 0.000206554
2017-10-10T12:41:00.412852: step 938, loss 0.131836, acc 0.96875, learning_rate 0.000206119
2017-10-10T12:41:00.740712: step 939, loss 0.277349, acc 0.890625, learning_rate 0.000205685
2017-10-10T12:41:00.925022: step 940, loss 0.183708, acc 0.953125, learning_rate 0.000205254
2017-10-10T12:41:01.099373: step 941, loss 0.154503, acc 0.90625, learning_rate 0.000204824
2017-10-10T12:41:01.271541: step 942, loss 0.280673, acc 0.90625, learning_rate 0.000204397
2017-10-10T12:41:01.442463: step 943, loss 0.200911, acc 0.921875, learning_rate 0.00020397
2017-10-10T12:41:01.615895: step 944, loss 0.211191, acc 0.953125, learning_rate 0.000203546
2017-10-10T12:41:01.913238: step 945, loss 0.202053, acc 0.90625, learning_rate 0.000203123
2017-10-10T12:41:02.196888: step 946, loss 0.159124, acc 0.953125, learning_rate 0.000202702
2017-10-10T12:41:02.487534: step 947, loss 0.17282, acc 0.90625, learning_rate 0.000202283
2017-10-10T12:41:02.776848: step 948, loss 0.0420192, acc 1, learning_rate 0.000201866
2017-10-10T12:41:03.073110: step 949, loss 0.112725, acc 0.96875, learning_rate 0.00020145
2017-10-10T12:41:03.333132: step 950, loss 0.193595, acc 0.9375, learning_rate 0.000201036
2017-10-10T12:41:03.586763: step 951, loss 0.146404, acc 0.921875, learning_rate 0.000200623
2017-10-10T12:41:03.844845: step 952, loss 0.115832, acc 0.984375, learning_rate 0.000200213
2017-10-10T12:41:04.127772: step 953, loss 0.306588, acc 0.890625, learning_rate 0.000199804
2017-10-10T12:41:04.420000: step 954, loss 0.221047, acc 0.890625, learning_rate 0.000199396
2017-10-10T12:41:04.702472: step 955, loss 0.128642, acc 0.96875, learning_rate 0.000198991
2017-10-10T12:41:05.007168: step 956, loss 0.173029, acc 0.921875, learning_rate 0.000198587
2017-10-10T12:41:05.267644: step 957, loss 0.187673, acc 0.9375, learning_rate 0.000198184
2017-10-10T12:41:05.538311: step 958, loss 0.101867, acc 0.96875, learning_rate 0.000197783
2017-10-10T12:41:05.826148: step 959, loss 0.183681, acc 0.96875, learning_rate 0.000197384
2017-10-10T12:41:06.109240: step 960, loss 0.39469, acc 0.890625, learning_rate 0.000196987

Evaluation:
2017-10-10T12:41:06.710224: step 960, loss 0.243394, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-960

2017-10-10T12:41:07.813081: step 961, loss 0.180039, acc 0.953125, learning_rate 0.000196591
2017-10-10T12:41:08.052845: step 962, loss 0.167262, acc 0.921875, learning_rate 0.000196197
2017-10-10T12:41:08.311716: step 963, loss 0.245818, acc 0.9375, learning_rate 0.000195804
2017-10-10T12:41:08.580860: step 964, loss 0.187774, acc 0.9375, learning_rate 0.000195413
2017-10-10T12:41:08.852067: step 965, loss 0.181456, acc 0.9375, learning_rate 0.000195023
2017-10-10T12:41:09.129491: step 966, loss 0.103047, acc 0.953125, learning_rate 0.000194636
2017-10-10T12:41:09.409254: step 967, loss 0.188907, acc 0.9375, learning_rate 0.000194249
2017-10-10T12:41:09.688533: step 968, loss 0.387201, acc 0.875, learning_rate 0.000193865
2017-10-10T12:41:09.971846: step 969, loss 0.233532, acc 0.921875, learning_rate 0.000193482
2017-10-10T12:41:10.264715: step 970, loss 0.178858, acc 0.953125, learning_rate 0.0001931
2017-10-10T12:41:10.551279: step 971, loss 0.30853, acc 0.9375, learning_rate 0.00019272
2017-10-10T12:41:10.838515: step 972, loss 0.125703, acc 0.96875, learning_rate 0.000192341
2017-10-10T12:41:11.136165: step 973, loss 0.0806124, acc 0.984375, learning_rate 0.000191965
2017-10-10T12:41:11.429347: step 974, loss 0.218309, acc 0.921875, learning_rate 0.000191589
2017-10-10T12:41:11.724338: step 975, loss 0.126235, acc 0.953125, learning_rate 0.000191215
2017-10-10T12:41:12.009411: step 976, loss 0.122386, acc 0.9375, learning_rate 0.000190843
2017-10-10T12:41:12.269952: step 977, loss 0.0817533, acc 0.96875, learning_rate 0.000190472
2017-10-10T12:41:12.543574: step 978, loss 0.0923393, acc 0.96875, learning_rate 0.000190103
2017-10-10T12:41:12.884874: step 979, loss 0.0926936, acc 0.984375, learning_rate 0.000189735
2017-10-10T12:41:13.139482: step 980, loss 0.278982, acc 0.862745, learning_rate 0.000189369
2017-10-10T12:41:13.371162: step 981, loss 0.258777, acc 0.9375, learning_rate 0.000189004
2017-10-10T12:41:13.552358: step 982, loss 0.377634, acc 0.875, learning_rate 0.000188641
2017-10-10T12:41:13.733068: step 983, loss 0.195937, acc 0.921875, learning_rate 0.000188279
2017-10-10T12:41:13.906295: step 984, loss 0.107277, acc 0.921875, learning_rate 0.000187919
2017-10-10T12:41:14.091571: step 985, loss 0.116873, acc 0.96875, learning_rate 0.00018756
2017-10-10T12:41:14.340879: step 986, loss 0.154613, acc 0.953125, learning_rate 0.000187202
2017-10-10T12:41:14.600867: step 987, loss 0.221759, acc 0.9375, learning_rate 0.000186846
2017-10-10T12:41:14.909968: step 988, loss 0.105241, acc 0.96875, learning_rate 0.000186492
2017-10-10T12:41:15.164253: step 989, loss 0.159156, acc 0.953125, learning_rate 0.000186139
2017-10-10T12:41:15.447932: step 990, loss 0.121018, acc 0.953125, learning_rate 0.000185787
2017-10-10T12:41:15.739820: step 991, loss 0.17769, acc 0.90625, learning_rate 0.000185437
2017-10-10T12:41:16.027267: step 992, loss 0.223268, acc 0.890625, learning_rate 0.000185088
2017-10-10T12:41:16.299737: step 993, loss 0.0951245, acc 0.984375, learning_rate 0.000184741
2017-10-10T12:41:16.558648: step 994, loss 0.147548, acc 0.921875, learning_rate 0.000184395
2017-10-10T12:41:16.844859: step 995, loss 0.280652, acc 0.921875, learning_rate 0.000184051
2017-10-10T12:41:17.090355: step 996, loss 0.23093, acc 0.90625, learning_rate 0.000183708
2017-10-10T12:41:17.332032: step 997, loss 0.199024, acc 0.921875, learning_rate 0.000183366
2017-10-10T12:41:17.614972: step 998, loss 0.175803, acc 0.953125, learning_rate 0.000183026
2017-10-10T12:41:17.908850: step 999, loss 0.277678, acc 0.90625, learning_rate 0.000182687
2017-10-10T12:41:18.189565: step 1000, loss 0.0355124, acc 1, learning_rate 0.000182349

Evaluation:
2017-10-10T12:41:18.799035: step 1000, loss 0.241264, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1000

2017-10-10T12:41:19.926320: step 1001, loss 0.164762, acc 0.96875, learning_rate 0.000182013
2017-10-10T12:41:20.220441: step 1002, loss 0.104745, acc 0.984375, learning_rate 0.000181678
2017-10-10T12:41:20.518159: step 1003, loss 0.117998, acc 0.9375, learning_rate 0.000181345
2017-10-10T12:41:20.800915: step 1004, loss 0.257094, acc 0.90625, learning_rate 0.000181013
2017-10-10T12:41:21.096816: step 1005, loss 0.124797, acc 0.96875, learning_rate 0.000180682
2017-10-10T12:41:21.370061: step 1006, loss 0.203868, acc 0.921875, learning_rate 0.000180353
2017-10-10T12:41:21.651500: step 1007, loss 0.192362, acc 0.9375, learning_rate 0.000180025
2017-10-10T12:41:21.920051: step 1008, loss 0.110071, acc 0.96875, learning_rate 0.000179698
2017-10-10T12:41:22.203560: step 1009, loss 0.176859, acc 0.921875, learning_rate 0.000179373
2017-10-10T12:41:22.474951: step 1010, loss 0.176718, acc 0.953125, learning_rate 0.000179049
2017-10-10T12:41:22.761172: step 1011, loss 0.220549, acc 0.90625, learning_rate 0.000178726
2017-10-10T12:41:23.048697: step 1012, loss 0.0896052, acc 0.96875, learning_rate 0.000178405
2017-10-10T12:41:23.334638: step 1013, loss 0.165821, acc 0.921875, learning_rate 0.000178085
2017-10-10T12:41:23.605300: step 1014, loss 0.129677, acc 0.953125, learning_rate 0.000177766
2017-10-10T12:41:23.894408: step 1015, loss 0.230262, acc 0.90625, learning_rate 0.000177449
2017-10-10T12:41:24.173133: step 1016, loss 0.15836, acc 0.9375, learning_rate 0.000177133
2017-10-10T12:41:24.459230: step 1017, loss 0.0891444, acc 0.984375, learning_rate 0.000176818
2017-10-10T12:41:24.739006: step 1018, loss 0.204019, acc 0.890625, learning_rate 0.000176504
2017-10-10T12:41:25.042591: step 1019, loss 0.0764797, acc 0.984375, learning_rate 0.000176192
2017-10-10T12:41:25.376900: step 1020, loss 0.166592, acc 0.953125, learning_rate 0.000175881
2017-10-10T12:41:25.720256: step 1021, loss 0.129926, acc 0.96875, learning_rate 0.000175571
2017-10-10T12:41:25.894822: step 1022, loss 0.264024, acc 0.9375, learning_rate 0.000175263
2017-10-10T12:41:26.075695: step 1023, loss 0.213675, acc 0.953125, learning_rate 0.000174956
2017-10-10T12:41:26.249909: step 1024, loss 0.203236, acc 0.921875, learning_rate 0.00017465
2017-10-10T12:41:26.422639: step 1025, loss 0.194744, acc 0.921875, learning_rate 0.000174345
2017-10-10T12:41:26.603966: step 1026, loss 0.137088, acc 0.921875, learning_rate 0.000174042
2017-10-10T12:41:26.888978: step 1027, loss 0.252052, acc 0.90625, learning_rate 0.000173739
2017-10-10T12:41:27.144925: step 1028, loss 0.136109, acc 0.9375, learning_rate 0.000173438
2017-10-10T12:41:27.440360: step 1029, loss 0.0755591, acc 0.96875, learning_rate 0.000173139
2017-10-10T12:41:27.761061: step 1030, loss 0.321701, acc 0.90625, learning_rate 0.00017284
2017-10-10T12:41:28.066006: step 1031, loss 0.260149, acc 0.890625, learning_rate 0.000172543
2017-10-10T12:41:28.355884: step 1032, loss 0.280125, acc 0.953125, learning_rate 0.000172247
2017-10-10T12:41:28.655214: step 1033, loss 0.17071, acc 0.9375, learning_rate 0.000171952
2017-10-10T12:41:28.953640: step 1034, loss 0.121806, acc 0.953125, learning_rate 0.000171658
2017-10-10T12:41:29.244939: step 1035, loss 0.252592, acc 0.890625, learning_rate 0.000171366
2017-10-10T12:41:29.531695: step 1036, loss 0.216849, acc 0.9375, learning_rate 0.000171074
2017-10-10T12:41:29.822562: step 1037, loss 0.107095, acc 0.96875, learning_rate 0.000170784
2017-10-10T12:41:30.099377: step 1038, loss 0.272293, acc 0.90625, learning_rate 0.000170495
2017-10-10T12:41:30.402390: step 1039, loss 0.133342, acc 0.9375, learning_rate 0.000170208
2017-10-10T12:41:30.706530: step 1040, loss 0.11131, acc 0.96875, learning_rate 0.000169921

Evaluation:
2017-10-10T12:41:31.378203: step 1040, loss 0.239076, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1040

2017-10-10T12:41:32.404909: step 1041, loss 0.11416, acc 0.953125, learning_rate 0.000169636
2017-10-10T12:41:32.665281: step 1042, loss 0.0768402, acc 0.984375, learning_rate 0.000169351
2017-10-10T12:41:32.940769: step 1043, loss 0.233232, acc 0.921875, learning_rate 0.000169068
2017-10-10T12:41:33.230531: step 1044, loss 0.176777, acc 0.921875, learning_rate 0.000168786
2017-10-10T12:41:33.520043: step 1045, loss 0.359473, acc 0.828125, learning_rate 0.000168506
2017-10-10T12:41:33.808945: step 1046, loss 0.222733, acc 0.921875, learning_rate 0.000168226
2017-10-10T12:41:34.088854: step 1047, loss 0.147919, acc 0.953125, learning_rate 0.000167947
2017-10-10T12:41:34.383448: step 1048, loss 0.135251, acc 0.953125, learning_rate 0.00016767
2017-10-10T12:41:34.682337: step 1049, loss 0.158128, acc 0.96875, learning_rate 0.000167394
2017-10-10T12:41:34.960859: step 1050, loss 0.155367, acc 0.9375, learning_rate 0.000167119
2017-10-10T12:41:35.238109: step 1051, loss 0.103355, acc 0.96875, learning_rate 0.000166845
2017-10-10T12:41:35.506033: step 1052, loss 0.131716, acc 0.953125, learning_rate 0.000166572
2017-10-10T12:41:35.784380: step 1053, loss 0.159409, acc 0.90625, learning_rate 0.0001663
2017-10-10T12:41:36.056566: step 1054, loss 0.103716, acc 0.96875, learning_rate 0.00016603
2017-10-10T12:41:36.316154: step 1055, loss 0.179963, acc 0.953125, learning_rate 0.00016576
2017-10-10T12:41:36.591743: step 1056, loss 0.269377, acc 0.90625, learning_rate 0.000165492
2017-10-10T12:41:36.881524: step 1057, loss 0.142873, acc 0.9375, learning_rate 0.000165224
2017-10-10T12:41:37.159588: step 1058, loss 0.18621, acc 0.9375, learning_rate 0.000164958
2017-10-10T12:41:37.451880: step 1059, loss 0.204076, acc 0.921875, learning_rate 0.000164693
2017-10-10T12:41:37.727223: step 1060, loss 0.185987, acc 0.953125, learning_rate 0.000164429
2017-10-10T12:41:38.052877: step 1061, loss 0.263478, acc 0.953125, learning_rate 0.000164166
2017-10-10T12:41:38.405365: step 1062, loss 0.13971, acc 0.953125, learning_rate 0.000163904
2017-10-10T12:41:38.592371: step 1063, loss 0.269672, acc 0.921875, learning_rate 0.000163643
2017-10-10T12:41:38.769910: step 1064, loss 0.126889, acc 0.953125, learning_rate 0.000163383
2017-10-10T12:41:38.942092: step 1065, loss 0.134713, acc 0.953125, learning_rate 0.000163125
2017-10-10T12:41:39.118050: step 1066, loss 0.156476, acc 0.953125, learning_rate 0.000162867
2017-10-10T12:41:39.287856: step 1067, loss 0.0943788, acc 0.96875, learning_rate 0.00016261
2017-10-10T12:41:39.583651: step 1068, loss 0.152914, acc 0.9375, learning_rate 0.000162355
2017-10-10T12:41:39.864282: step 1069, loss 0.166665, acc 0.96875, learning_rate 0.0001621
2017-10-10T12:41:40.141299: step 1070, loss 0.20743, acc 0.953125, learning_rate 0.000161847
2017-10-10T12:41:40.415794: step 1071, loss 0.216073, acc 0.921875, learning_rate 0.000161594
2017-10-10T12:41:40.704334: step 1072, loss 0.139524, acc 0.96875, learning_rate 0.000161343
2017-10-10T12:41:40.988077: step 1073, loss 0.111343, acc 0.96875, learning_rate 0.000161093
2017-10-10T12:41:41.281058: step 1074, loss 0.144932, acc 0.9375, learning_rate 0.000160843
2017-10-10T12:41:41.548870: step 1075, loss 0.240182, acc 0.90625, learning_rate 0.000160595
2017-10-10T12:41:41.819615: step 1076, loss 0.197281, acc 0.9375, learning_rate 0.000160348
2017-10-10T12:41:42.095263: step 1077, loss 0.101574, acc 0.96875, learning_rate 0.000160101
2017-10-10T12:41:42.339482: step 1078, loss 0.141243, acc 0.960784, learning_rate 0.000159856
2017-10-10T12:41:42.624574: step 1079, loss 0.211888, acc 0.90625, learning_rate 0.000159612
2017-10-10T12:41:42.922099: step 1080, loss 0.110933, acc 0.96875, learning_rate 0.000159368

Evaluation:
2017-10-10T12:41:43.562493: step 1080, loss 0.239286, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1080

2017-10-10T12:41:44.606905: step 1081, loss 0.145871, acc 0.96875, learning_rate 0.000159126
2017-10-10T12:41:44.900876: step 1082, loss 0.12669, acc 0.953125, learning_rate 0.000158885
2017-10-10T12:41:45.240888: step 1083, loss 0.16085, acc 0.953125, learning_rate 0.000158644
2017-10-10T12:41:45.522905: step 1084, loss 0.0468859, acc 0.984375, learning_rate 0.000158405
2017-10-10T12:41:45.819106: step 1085, loss 0.112699, acc 0.9375, learning_rate 0.000158167
2017-10-10T12:41:46.109636: step 1086, loss 0.145234, acc 0.953125, learning_rate 0.000157929
2017-10-10T12:41:46.397375: step 1087, loss 0.227873, acc 0.890625, learning_rate 0.000157693
2017-10-10T12:41:46.685535: step 1088, loss 0.126651, acc 0.953125, learning_rate 0.000157457
2017-10-10T12:41:46.979774: step 1089, loss 0.20986, acc 0.890625, learning_rate 0.000157223
2017-10-10T12:41:47.266067: step 1090, loss 0.100884, acc 0.953125, learning_rate 0.000156989
2017-10-10T12:41:47.556880: step 1091, loss 0.273821, acc 0.90625, learning_rate 0.000156757
2017-10-10T12:41:47.819725: step 1092, loss 0.246741, acc 0.9375, learning_rate 0.000156525
2017-10-10T12:41:48.098322: step 1093, loss 0.100313, acc 0.96875, learning_rate 0.000156294
2017-10-10T12:41:48.378043: step 1094, loss 0.208112, acc 0.953125, learning_rate 0.000156064
2017-10-10T12:41:48.646653: step 1095, loss 0.194412, acc 0.90625, learning_rate 0.000155836
2017-10-10T12:41:48.938569: step 1096, loss 0.246738, acc 0.921875, learning_rate 0.000155608
2017-10-10T12:41:49.223379: step 1097, loss 0.115064, acc 0.984375, learning_rate 0.000155381
2017-10-10T12:41:49.508597: step 1098, loss 0.153058, acc 0.96875, learning_rate 0.000155155
2017-10-10T12:41:49.788808: step 1099, loss 0.204933, acc 0.921875, learning_rate 0.000154929
2017-10-10T12:41:50.085351: step 1100, loss 0.380995, acc 0.90625, learning_rate 0.000154705
2017-10-10T12:41:50.404902: step 1101, loss 0.183001, acc 0.921875, learning_rate 0.000154482
2017-10-10T12:41:50.763357: step 1102, loss 0.173575, acc 0.953125, learning_rate 0.00015426
2017-10-10T12:41:50.946504: step 1103, loss 0.38149, acc 0.875, learning_rate 0.000154038
2017-10-10T12:41:51.120514: step 1104, loss 0.178552, acc 0.9375, learning_rate 0.000153818
2017-10-10T12:41:51.292771: step 1105, loss 0.131508, acc 0.9375, learning_rate 0.000153598
2017-10-10T12:41:51.464992: step 1106, loss 0.202594, acc 0.90625, learning_rate 0.000153379
2017-10-10T12:41:51.637521: step 1107, loss 0.254835, acc 0.96875, learning_rate 0.000153161
2017-10-10T12:41:51.926054: step 1108, loss 0.162067, acc 0.921875, learning_rate 0.000152944
2017-10-10T12:41:52.225809: step 1109, loss 0.0843764, acc 0.984375, learning_rate 0.000152728
2017-10-10T12:41:52.522974: step 1110, loss 0.099436, acc 0.96875, learning_rate 0.000152513
2017-10-10T12:41:52.813188: step 1111, loss 0.0616097, acc 0.984375, learning_rate 0.000152299
2017-10-10T12:41:53.103071: step 1112, loss 0.17871, acc 0.90625, learning_rate 0.000152085
2017-10-10T12:41:53.393656: step 1113, loss 0.0581241, acc 0.96875, learning_rate 0.000151872
2017-10-10T12:41:53.673070: step 1114, loss 0.48401, acc 0.828125, learning_rate 0.000151661
2017-10-10T12:41:53.993263: step 1115, loss 0.13507, acc 0.953125, learning_rate 0.00015145
2017-10-10T12:41:54.289600: step 1116, loss 0.173696, acc 0.96875, learning_rate 0.00015124
2017-10-10T12:41:54.569362: step 1117, loss 0.182363, acc 0.921875, learning_rate 0.000151031
2017-10-10T12:41:54.859443: step 1118, loss 0.179498, acc 0.953125, learning_rate 0.000150822
2017-10-10T12:41:55.139966: step 1119, loss 0.11014, acc 0.9375, learning_rate 0.000150615
2017-10-10T12:41:55.419413: step 1120, loss 0.0850879, acc 0.984375, learning_rate 0.000150408

Evaluation:
2017-10-10T12:41:56.074978: step 1120, loss 0.242203, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1120

2017-10-10T12:41:57.261762: step 1121, loss 0.124821, acc 0.953125, learning_rate 0.000150203
2017-10-10T12:41:57.459320: step 1122, loss 0.115635, acc 0.9375, learning_rate 0.000149998
2017-10-10T12:41:57.739852: step 1123, loss 0.11812, acc 0.953125, learning_rate 0.000149794
2017-10-10T12:41:58.003706: step 1124, loss 0.118409, acc 0.96875, learning_rate 0.00014959
2017-10-10T12:41:58.296730: step 1125, loss 0.121957, acc 0.953125, learning_rate 0.000149388
2017-10-10T12:41:58.572835: step 1126, loss 0.17108, acc 0.96875, learning_rate 0.000149186
2017-10-10T12:41:58.856901: step 1127, loss 0.185033, acc 0.921875, learning_rate 0.000148986
2017-10-10T12:41:59.158585: step 1128, loss 0.0678528, acc 0.984375, learning_rate 0.000148786
2017-10-10T12:41:59.430824: step 1129, loss 0.222148, acc 0.9375, learning_rate 0.000148587
2017-10-10T12:41:59.708862: step 1130, loss 0.0809662, acc 0.984375, learning_rate 0.000148388
2017-10-10T12:41:59.984538: step 1131, loss 0.141437, acc 0.953125, learning_rate 0.000148191
2017-10-10T12:42:00.268235: step 1132, loss 0.322124, acc 0.90625, learning_rate 0.000147994
2017-10-10T12:42:00.546112: step 1133, loss 0.177049, acc 0.953125, learning_rate 0.000147798
2017-10-10T12:42:00.831447: step 1134, loss 0.165162, acc 0.9375, learning_rate 0.000147603
2017-10-10T12:42:01.104918: step 1135, loss 0.106753, acc 0.96875, learning_rate 0.000147409
2017-10-10T12:42:01.393181: step 1136, loss 0.116445, acc 0.9375, learning_rate 0.000147215
2017-10-10T12:42:01.682184: step 1137, loss 0.166415, acc 0.953125, learning_rate 0.000147022
2017-10-10T12:42:01.964555: step 1138, loss 0.0382909, acc 1, learning_rate 0.000146831
2017-10-10T12:42:02.261780: step 1139, loss 0.126187, acc 0.984375, learning_rate 0.000146639
2017-10-10T12:42:02.547266: step 1140, loss 0.159155, acc 0.890625, learning_rate 0.000146449
2017-10-10T12:42:02.826224: step 1141, loss 0.197059, acc 0.9375, learning_rate 0.000146259
2017-10-10T12:42:03.192857: step 1142, loss 0.122126, acc 0.96875, learning_rate 0.000146071
2017-10-10T12:42:03.523526: step 1143, loss 0.167936, acc 0.9375, learning_rate 0.000145883
2017-10-10T12:42:03.699731: step 1144, loss 0.0823731, acc 0.96875, learning_rate 0.000145695
2017-10-10T12:42:03.876665: step 1145, loss 0.415937, acc 0.84375, learning_rate 0.000145509
2017-10-10T12:42:04.045847: step 1146, loss 0.141269, acc 0.9375, learning_rate 0.000145323
2017-10-10T12:42:04.214006: step 1147, loss 0.238851, acc 0.921875, learning_rate 0.000145138
2017-10-10T12:42:04.392171: step 1148, loss 0.118927, acc 0.921875, learning_rate 0.000144954
2017-10-10T12:42:04.672899: step 1149, loss 0.0954684, acc 0.96875, learning_rate 0.00014477
2017-10-10T12:42:04.949260: step 1150, loss 0.168563, acc 0.921875, learning_rate 0.000144588
2017-10-10T12:42:05.224603: step 1151, loss 0.143235, acc 0.953125, learning_rate 0.000144406
2017-10-10T12:42:05.503852: step 1152, loss 0.0765503, acc 0.984375, learning_rate 0.000144224
2017-10-10T12:42:05.789590: step 1153, loss 0.132967, acc 0.9375, learning_rate 0.000144044
2017-10-10T12:42:06.073493: step 1154, loss 0.135215, acc 0.96875, learning_rate 0.000143864
2017-10-10T12:42:06.382932: step 1155, loss 0.0725368, acc 0.96875, learning_rate 0.000143685
2017-10-10T12:42:06.665557: step 1156, loss 0.150886, acc 0.96875, learning_rate 0.000143507
2017-10-10T12:42:06.941487: step 1157, loss 0.124968, acc 0.921875, learning_rate 0.000143329
2017-10-10T12:42:07.230105: step 1158, loss 0.0528956, acc 0.984375, learning_rate 0.000143152
2017-10-10T12:42:07.521275: step 1159, loss 0.124731, acc 0.9375, learning_rate 0.000142976
2017-10-10T12:42:07.801752: step 1160, loss 0.0303323, acc 1, learning_rate 0.000142801

Evaluation:
2017-10-10T12:42:08.481049: step 1160, loss 0.240527, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1160

2017-10-10T12:42:09.582080: step 1161, loss 0.219881, acc 0.9375, learning_rate 0.000142626
2017-10-10T12:42:09.875626: step 1162, loss 0.131267, acc 0.96875, learning_rate 0.000142452
2017-10-10T12:42:10.179570: step 1163, loss 0.156013, acc 0.9375, learning_rate 0.000142279
2017-10-10T12:42:10.465872: step 1164, loss 0.274966, acc 0.90625, learning_rate 0.000142106
2017-10-10T12:42:10.753770: step 1165, loss 0.102978, acc 0.953125, learning_rate 0.000141934
2017-10-10T12:42:11.041108: step 1166, loss 0.182456, acc 0.96875, learning_rate 0.000141763
2017-10-10T12:42:11.324675: step 1167, loss 0.0715953, acc 0.984375, learning_rate 0.000141593
2017-10-10T12:42:11.626437: step 1168, loss 0.1175, acc 0.96875, learning_rate 0.000141423
2017-10-10T12:42:11.955076: step 1169, loss 0.0970563, acc 0.953125, learning_rate 0.000141254
2017-10-10T12:42:12.259536: step 1170, loss 0.152865, acc 0.96875, learning_rate 0.000141085
2017-10-10T12:42:12.584082: step 1171, loss 0.0586738, acc 0.984375, learning_rate 0.000140918
2017-10-10T12:42:12.902703: step 1172, loss 0.163536, acc 0.921875, learning_rate 0.000140751
2017-10-10T12:42:13.207032: step 1173, loss 0.120591, acc 0.953125, learning_rate 0.000140584
2017-10-10T12:42:13.517810: step 1174, loss 0.269375, acc 0.90625, learning_rate 0.000140419
2017-10-10T12:42:13.822553: step 1175, loss 0.166474, acc 0.9375, learning_rate 0.000140254
2017-10-10T12:42:14.117090: step 1176, loss 0.144564, acc 0.960784, learning_rate 0.000140089
2017-10-10T12:42:14.431959: step 1177, loss 0.128195, acc 0.953125, learning_rate 0.000139926
2017-10-10T12:42:14.738272: step 1178, loss 0.112274, acc 0.96875, learning_rate 0.000139763
2017-10-10T12:42:15.031335: step 1179, loss 0.137532, acc 0.953125, learning_rate 0.0001396
2017-10-10T12:42:15.356576: step 1180, loss 0.151692, acc 0.9375, learning_rate 0.000139439
2017-10-10T12:42:15.666343: step 1181, loss 0.0835155, acc 0.96875, learning_rate 0.000139278
2017-10-10T12:42:16.020880: step 1182, loss 0.241299, acc 0.90625, learning_rate 0.000139118
2017-10-10T12:42:16.391338: step 1183, loss 0.121253, acc 0.984375, learning_rate 0.000138958
2017-10-10T12:42:16.592630: step 1184, loss 0.0902956, acc 0.953125, learning_rate 0.000138799
2017-10-10T12:42:16.791739: step 1185, loss 0.166745, acc 0.953125, learning_rate 0.00013864
2017-10-10T12:42:17.006913: step 1186, loss 0.0850479, acc 0.984375, learning_rate 0.000138483
2017-10-10T12:42:17.197726: step 1187, loss 0.200213, acc 0.9375, learning_rate 0.000138326
2017-10-10T12:42:17.425523: step 1188, loss 0.249535, acc 0.953125, learning_rate 0.000138169
2017-10-10T12:42:17.697199: step 1189, loss 0.147799, acc 0.96875, learning_rate 0.000138013
2017-10-10T12:42:18.001069: step 1190, loss 0.117472, acc 0.953125, learning_rate 0.000137858
2017-10-10T12:42:18.324906: step 1191, loss 0.198368, acc 0.90625, learning_rate 0.000137704
2017-10-10T12:42:18.601099: step 1192, loss 0.31936, acc 0.90625, learning_rate 0.00013755
2017-10-10T12:42:18.910889: step 1193, loss 0.35212, acc 0.875, learning_rate 0.000137397
2017-10-10T12:42:19.165864: step 1194, loss 0.234954, acc 0.921875, learning_rate 0.000137244
2017-10-10T12:42:19.440870: step 1195, loss 0.205623, acc 0.90625, learning_rate 0.000137092
2017-10-10T12:42:19.768975: step 1196, loss 0.0824016, acc 0.96875, learning_rate 0.000136941
2017-10-10T12:42:20.081238: step 1197, loss 0.197638, acc 0.90625, learning_rate 0.00013679
2017-10-10T12:42:20.351661: step 1198, loss 0.253459, acc 0.921875, learning_rate 0.00013664
2017-10-10T12:42:20.627757: step 1199, loss 0.317889, acc 0.90625, learning_rate 0.00013649
2017-10-10T12:42:20.914854: step 1200, loss 0.278518, acc 0.890625, learning_rate 0.000136341

Evaluation:
2017-10-10T12:42:21.655839: step 1200, loss 0.241201, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1200

2017-10-10T12:42:22.780197: step 1201, loss 0.149413, acc 0.953125, learning_rate 0.000136193
2017-10-10T12:42:23.089210: step 1202, loss 0.126039, acc 0.96875, learning_rate 0.000136045
2017-10-10T12:42:23.395720: step 1203, loss 0.094833, acc 0.984375, learning_rate 0.000135898
2017-10-10T12:42:23.709678: step 1204, loss 0.149329, acc 0.9375, learning_rate 0.000135751
2017-10-10T12:42:24.018209: step 1205, loss 0.220241, acc 0.90625, learning_rate 0.000135605
2017-10-10T12:42:24.311119: step 1206, loss 0.312027, acc 0.921875, learning_rate 0.00013546
2017-10-10T12:42:24.625219: step 1207, loss 0.212659, acc 0.953125, learning_rate 0.000135315
2017-10-10T12:42:24.919629: step 1208, loss 0.186809, acc 0.921875, learning_rate 0.000135171
2017-10-10T12:42:25.196716: step 1209, loss 0.195568, acc 0.9375, learning_rate 0.000135028
2017-10-10T12:42:25.481726: step 1210, loss 0.0875006, acc 0.96875, learning_rate 0.000134885
2017-10-10T12:42:25.790561: step 1211, loss 0.250266, acc 0.921875, learning_rate 0.000134742
2017-10-10T12:42:26.098978: step 1212, loss 0.231528, acc 0.9375, learning_rate 0.0001346
2017-10-10T12:42:26.406172: step 1213, loss 0.134685, acc 0.96875, learning_rate 0.000134459
2017-10-10T12:42:26.707597: step 1214, loss 0.208182, acc 0.953125, learning_rate 0.000134319
2017-10-10T12:42:27.013889: step 1215, loss 0.207976, acc 0.9375, learning_rate 0.000134178
2017-10-10T12:42:27.319359: step 1216, loss 0.100606, acc 0.953125, learning_rate 0.000134039
2017-10-10T12:42:27.632503: step 1217, loss 0.191968, acc 0.9375, learning_rate 0.0001339
2017-10-10T12:42:27.936055: step 1218, loss 0.232148, acc 0.921875, learning_rate 0.000133762
2017-10-10T12:42:28.255429: step 1219, loss 0.200916, acc 0.9375, learning_rate 0.000133624
2017-10-10T12:42:28.557325: step 1220, loss 0.169074, acc 0.9375, learning_rate 0.000133487
2017-10-10T12:42:28.885740: step 1221, loss 0.0747122, acc 0.96875, learning_rate 0.00013335
2017-10-10T12:42:29.193366: step 1222, loss 0.064891, acc 0.96875, learning_rate 0.000133214
2017-10-10T12:42:29.567257: step 1223, loss 0.126228, acc 0.96875, learning_rate 0.000133078
2017-10-10T12:42:29.935706: step 1224, loss 0.14835, acc 0.921875, learning_rate 0.000132943
2017-10-10T12:42:30.120119: step 1225, loss 0.111352, acc 0.953125, learning_rate 0.000132809
2017-10-10T12:42:30.310207: step 1226, loss 0.140943, acc 0.96875, learning_rate 0.000132675
2017-10-10T12:42:30.504167: step 1227, loss 0.162173, acc 0.9375, learning_rate 0.000132541
2017-10-10T12:42:30.691163: step 1228, loss 0.118787, acc 0.9375, learning_rate 0.000132409
2017-10-10T12:42:30.876776: step 1229, loss 0.0977238, acc 0.984375, learning_rate 0.000132276
2017-10-10T12:42:31.200773: step 1230, loss 0.315917, acc 0.84375, learning_rate 0.000132145
2017-10-10T12:42:31.508861: step 1231, loss 0.144119, acc 0.953125, learning_rate 0.000132013
2017-10-10T12:42:31.824845: step 1232, loss 0.104613, acc 0.96875, learning_rate 0.000131883
2017-10-10T12:42:32.119757: step 1233, loss 0.257316, acc 0.875, learning_rate 0.000131753
2017-10-10T12:42:32.438355: step 1234, loss 0.0371328, acc 1, learning_rate 0.000131623
2017-10-10T12:42:32.744464: step 1235, loss 0.168737, acc 0.921875, learning_rate 0.000131494
2017-10-10T12:42:33.045871: step 1236, loss 0.192845, acc 0.921875, learning_rate 0.000131365
2017-10-10T12:42:33.369388: step 1237, loss 0.174471, acc 0.9375, learning_rate 0.000131237
2017-10-10T12:42:33.696350: step 1238, loss 0.107123, acc 0.984375, learning_rate 0.00013111
2017-10-10T12:42:33.999794: step 1239, loss 0.229091, acc 0.921875, learning_rate 0.000130983
2017-10-10T12:42:34.310794: step 1240, loss 0.0668959, acc 0.96875, learning_rate 0.000130856

Evaluation:
2017-10-10T12:42:35.121001: step 1240, loss 0.239199, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1240

2017-10-10T12:42:36.492859: step 1241, loss 0.125781, acc 0.984375, learning_rate 0.00013073
2017-10-10T12:42:36.800198: step 1242, loss 0.326587, acc 0.90625, learning_rate 0.000130605
2017-10-10T12:42:37.091632: step 1243, loss 0.158437, acc 0.90625, learning_rate 0.00013048
2017-10-10T12:42:37.393665: step 1244, loss 0.075699, acc 0.984375, learning_rate 0.000130356
2017-10-10T12:42:37.699566: step 1245, loss 0.238172, acc 0.90625, learning_rate 0.000130232
2017-10-10T12:42:38.020320: step 1246, loss 0.110241, acc 0.953125, learning_rate 0.000130108
2017-10-10T12:42:38.332892: step 1247, loss 0.187827, acc 0.90625, learning_rate 0.000129985
2017-10-10T12:42:38.620038: step 1248, loss 0.331877, acc 0.890625, learning_rate 0.000129863
2017-10-10T12:42:38.931685: step 1249, loss 0.13759, acc 0.953125, learning_rate 0.000129741
2017-10-10T12:42:39.239295: step 1250, loss 0.105338, acc 0.984375, learning_rate 0.00012962
2017-10-10T12:42:39.561625: step 1251, loss 0.285809, acc 0.90625, learning_rate 0.000129499
2017-10-10T12:42:39.873845: step 1252, loss 0.243819, acc 0.9375, learning_rate 0.000129378
2017-10-10T12:42:40.166400: step 1253, loss 0.111037, acc 0.96875, learning_rate 0.000129259
2017-10-10T12:42:40.485004: step 1254, loss 0.0561312, acc 1, learning_rate 0.000129139
2017-10-10T12:42:40.801748: step 1255, loss 0.0651272, acc 0.96875, learning_rate 0.00012902
2017-10-10T12:42:41.120703: step 1256, loss 0.159799, acc 0.953125, learning_rate 0.000128902
2017-10-10T12:42:41.421921: step 1257, loss 0.147464, acc 0.96875, learning_rate 0.000128784
2017-10-10T12:42:41.716198: step 1258, loss 0.140876, acc 0.953125, learning_rate 0.000128666
2017-10-10T12:42:42.021361: step 1259, loss 0.165387, acc 0.96875, learning_rate 0.000128549
2017-10-10T12:42:42.365125: step 1260, loss 0.281933, acc 0.890625, learning_rate 0.000128433
2017-10-10T12:42:42.740821: step 1261, loss 0.140419, acc 0.953125, learning_rate 0.000128317
2017-10-10T12:42:43.237615: step 1262, loss 0.111283, acc 0.96875, learning_rate 0.000128201
2017-10-10T12:42:43.696666: step 1263, loss 0.256811, acc 0.90625, learning_rate 0.000128086
2017-10-10T12:42:43.966103: step 1264, loss 0.189301, acc 0.9375, learning_rate 0.000127971
2017-10-10T12:42:44.282069: step 1265, loss 0.0844702, acc 1, learning_rate 0.000127857
2017-10-10T12:42:44.581986: step 1266, loss 0.394911, acc 0.890625, learning_rate 0.000127743
2017-10-10T12:42:45.022067: step 1267, loss 0.0964668, acc 0.96875, learning_rate 0.00012763
2017-10-10T12:42:45.435150: step 1268, loss 0.136803, acc 0.984375, learning_rate 0.000127517
2017-10-10T12:42:45.854991: step 1269, loss 0.0748411, acc 0.984375, learning_rate 0.000127405
2017-10-10T12:42:46.257516: step 1270, loss 0.0896611, acc 0.953125, learning_rate 0.000127293
2017-10-10T12:42:46.677674: step 1271, loss 0.239706, acc 0.921875, learning_rate 0.000127182
2017-10-10T12:42:47.083004: step 1272, loss 0.230221, acc 0.9375, learning_rate 0.000127071
2017-10-10T12:42:47.477375: step 1273, loss 0.0871138, acc 0.984375, learning_rate 0.00012696
2017-10-10T12:42:47.832844: step 1274, loss 0.155623, acc 0.921569, learning_rate 0.00012685
2017-10-10T12:42:48.236988: step 1275, loss 0.0978849, acc 0.96875, learning_rate 0.000126741
2017-10-10T12:42:48.645845: step 1276, loss 0.0970023, acc 0.953125, learning_rate 0.000126632
2017-10-10T12:42:49.052840: step 1277, loss 0.0767863, acc 0.984375, learning_rate 0.000126523
2017-10-10T12:42:49.471129: step 1278, loss 0.0826662, acc 0.96875, learning_rate 0.000126415
2017-10-10T12:42:49.898823: step 1279, loss 0.161971, acc 0.90625, learning_rate 0.000126307
2017-10-10T12:42:50.317637: step 1280, loss 0.0965344, acc 0.953125, learning_rate 0.000126199

Evaluation:
2017-10-10T12:42:51.269660: step 1280, loss 0.243429, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1280

2017-10-10T12:42:52.583094: step 1281, loss 0.105409, acc 0.96875, learning_rate 0.000126093
2017-10-10T12:42:53.023743: step 1282, loss 0.259913, acc 0.921875, learning_rate 0.000125986
2017-10-10T12:42:53.459534: step 1283, loss 0.179854, acc 0.96875, learning_rate 0.00012588
2017-10-10T12:42:53.894626: step 1284, loss 0.0879523, acc 0.96875, learning_rate 0.000125774
2017-10-10T12:42:54.318555: step 1285, loss 0.086169, acc 0.96875, learning_rate 0.000125669
2017-10-10T12:42:54.720892: step 1286, loss 0.121323, acc 0.953125, learning_rate 0.000125564
2017-10-10T12:42:55.147735: step 1287, loss 0.0677564, acc 0.984375, learning_rate 0.00012546
2017-10-10T12:42:55.572864: step 1288, loss 0.141059, acc 0.953125, learning_rate 0.000125356
2017-10-10T12:42:56.009235: step 1289, loss 0.205964, acc 0.90625, learning_rate 0.000125253
2017-10-10T12:42:56.403942: step 1290, loss 0.156882, acc 0.9375, learning_rate 0.00012515
2017-10-10T12:42:56.810978: step 1291, loss 0.143564, acc 0.96875, learning_rate 0.000125047
2017-10-10T12:42:57.228873: step 1292, loss 0.29419, acc 0.875, learning_rate 0.000124945
2017-10-10T12:42:57.607132: step 1293, loss 0.245715, acc 0.90625, learning_rate 0.000124843
2017-10-10T12:42:57.992849: step 1294, loss 0.229012, acc 0.875, learning_rate 0.000124741
2017-10-10T12:42:58.392906: step 1295, loss 0.0809558, acc 0.96875, learning_rate 0.00012464
2017-10-10T12:42:58.752892: step 1296, loss 0.233287, acc 0.921875, learning_rate 0.00012454
2017-10-10T12:42:59.208551: step 1297, loss 0.182436, acc 0.9375, learning_rate 0.00012444
2017-10-10T12:42:59.683553: step 1298, loss 0.207908, acc 0.90625, learning_rate 0.00012434
2017-10-10T12:42:59.968857: step 1299, loss 0.178186, acc 0.953125, learning_rate 0.000124241
2017-10-10T12:43:00.272958: step 1300, loss 0.0373476, acc 0.984375, learning_rate 0.000124142
2017-10-10T12:43:00.543653: step 1301, loss 0.102816, acc 0.953125, learning_rate 0.000124043
2017-10-10T12:43:01.017573: step 1302, loss 0.247937, acc 0.921875, learning_rate 0.000123945
2017-10-10T12:43:01.443772: step 1303, loss 0.115307, acc 0.953125, learning_rate 0.000123847
2017-10-10T12:43:01.749033: step 1304, loss 0.163019, acc 0.9375, learning_rate 0.00012375
2017-10-10T12:43:02.072869: step 1305, loss 0.222934, acc 0.953125, learning_rate 0.000123653
2017-10-10T12:43:02.422162: step 1306, loss 0.0783821, acc 0.96875, learning_rate 0.000123556
2017-10-10T12:43:02.831632: step 1307, loss 0.182506, acc 0.9375, learning_rate 0.00012346
2017-10-10T12:43:03.192675: step 1308, loss 0.113333, acc 0.9375, learning_rate 0.000123364
2017-10-10T12:43:03.583743: step 1309, loss 0.050151, acc 0.96875, learning_rate 0.000123269
2017-10-10T12:43:03.949037: step 1310, loss 0.236649, acc 0.90625, learning_rate 0.000123174
2017-10-10T12:43:04.312911: step 1311, loss 0.165044, acc 0.921875, learning_rate 0.00012308
2017-10-10T12:43:04.641002: step 1312, loss 0.0720556, acc 0.984375, learning_rate 0.000122985
2017-10-10T12:43:05.017375: step 1313, loss 0.218943, acc 0.953125, learning_rate 0.000122892
2017-10-10T12:43:05.476848: step 1314, loss 0.15057, acc 0.9375, learning_rate 0.000122798
2017-10-10T12:43:05.904794: step 1315, loss 0.206674, acc 0.90625, learning_rate 0.000122705
2017-10-10T12:43:06.312808: step 1316, loss 0.122927, acc 0.953125, learning_rate 0.000122612
2017-10-10T12:43:06.714807: step 1317, loss 0.1248, acc 0.984375, learning_rate 0.00012252
2017-10-10T12:43:07.130818: step 1318, loss 0.21964, acc 0.921875, learning_rate 0.000122428
2017-10-10T12:43:07.532007: step 1319, loss 0.249758, acc 0.890625, learning_rate 0.000122337
2017-10-10T12:43:07.916893: step 1320, loss 0.100848, acc 0.96875, learning_rate 0.000122245

Evaluation:
2017-10-10T12:43:08.868193: step 1320, loss 0.240121, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1320

2017-10-10T12:43:10.287259: step 1321, loss 0.10806, acc 0.96875, learning_rate 0.000122155
2017-10-10T12:43:10.676827: step 1322, loss 0.30583, acc 0.890625, learning_rate 0.000122064
2017-10-10T12:43:11.042014: step 1323, loss 0.126072, acc 0.96875, learning_rate 0.000121974
2017-10-10T12:43:11.444082: step 1324, loss 0.285649, acc 0.90625, learning_rate 0.000121884
2017-10-10T12:43:11.781153: step 1325, loss 0.103754, acc 0.96875, learning_rate 0.000121795
2017-10-10T12:43:12.182862: step 1326, loss 0.236554, acc 0.921875, learning_rate 0.000121706
2017-10-10T12:43:12.593042: step 1327, loss 0.0713829, acc 0.984375, learning_rate 0.000121618
2017-10-10T12:43:12.946824: step 1328, loss 0.157313, acc 0.9375, learning_rate 0.000121529
2017-10-10T12:43:13.327244: step 1329, loss 0.138543, acc 0.953125, learning_rate 0.000121441
2017-10-10T12:43:13.743286: step 1330, loss 0.0944021, acc 0.96875, learning_rate 0.000121354
2017-10-10T12:43:14.137118: step 1331, loss 0.216572, acc 0.921875, learning_rate 0.000121267
2017-10-10T12:43:14.523615: step 1332, loss 0.0950776, acc 0.96875, learning_rate 0.00012118
2017-10-10T12:43:14.904142: step 1333, loss 0.214611, acc 0.90625, learning_rate 0.000121093
2017-10-10T12:43:15.297065: step 1334, loss 0.0887019, acc 0.984375, learning_rate 0.000121007
2017-10-10T12:43:15.747528: step 1335, loss 0.277345, acc 0.875, learning_rate 0.000120922
2017-10-10T12:43:16.057676: step 1336, loss 0.0576884, acc 1, learning_rate 0.000120836
2017-10-10T12:43:16.477333: step 1337, loss 0.083162, acc 0.984375, learning_rate 0.000120751
2017-10-10T12:43:16.893007: step 1338, loss 0.219756, acc 0.9375, learning_rate 0.000120666
2017-10-10T12:43:17.328999: step 1339, loss 0.0999204, acc 0.984375, learning_rate 0.000120582
2017-10-10T12:43:17.637028: step 1340, loss 0.261307, acc 0.859375, learning_rate 0.000120498
2017-10-10T12:43:17.949041: step 1341, loss 0.130057, acc 0.953125, learning_rate 0.000120414
2017-10-10T12:43:18.263050: step 1342, loss 0.158238, acc 0.9375, learning_rate 0.000120331
2017-10-10T12:43:18.736911: step 1343, loss 0.0527097, acc 0.984375, learning_rate 0.000120248
2017-10-10T12:43:19.066971: step 1344, loss 0.156825, acc 0.953125, learning_rate 0.000120165
2017-10-10T12:43:19.380115: step 1345, loss 0.159661, acc 0.96875, learning_rate 0.000120083
2017-10-10T12:43:19.678659: step 1346, loss 0.181199, acc 0.9375, learning_rate 0.000120001
2017-10-10T12:43:19.945004: step 1347, loss 0.0646868, acc 0.96875, learning_rate 0.00011992
2017-10-10T12:43:20.249763: step 1348, loss 0.294767, acc 0.890625, learning_rate 0.000119838
2017-10-10T12:43:20.597147: step 1349, loss 0.207782, acc 0.90625, learning_rate 0.000119757
2017-10-10T12:43:21.076877: step 1350, loss 0.165853, acc 0.953125, learning_rate 0.000119677
2017-10-10T12:43:21.528888: step 1351, loss 0.267615, acc 0.921875, learning_rate 0.000119596
2017-10-10T12:43:21.890707: step 1352, loss 0.145829, acc 0.953125, learning_rate 0.000119516
2017-10-10T12:43:22.252838: step 1353, loss 0.156977, acc 0.9375, learning_rate 0.000119437
2017-10-10T12:43:22.669078: step 1354, loss 0.149948, acc 0.9375, learning_rate 0.000119357
2017-10-10T12:43:23.096965: step 1355, loss 0.163788, acc 0.921875, learning_rate 0.000119278
2017-10-10T12:43:23.549435: step 1356, loss 0.120954, acc 0.96875, learning_rate 0.0001192
2017-10-10T12:43:23.958067: step 1357, loss 0.0765015, acc 0.96875, learning_rate 0.000119121
2017-10-10T12:43:24.293412: step 1358, loss 0.12996, acc 0.9375, learning_rate 0.000119043
2017-10-10T12:43:24.634826: step 1359, loss 0.185955, acc 0.90625, learning_rate 0.000118965
2017-10-10T12:43:25.021086: step 1360, loss 0.0896322, acc 0.984375, learning_rate 0.000118888

Evaluation:
2017-10-10T12:43:26.016895: step 1360, loss 0.24291, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1360

2017-10-10T12:43:27.493542: step 1361, loss 0.296487, acc 0.890625, learning_rate 0.000118811
2017-10-10T12:43:27.898426: step 1362, loss 0.0738452, acc 0.96875, learning_rate 0.000118734
2017-10-10T12:43:28.299719: step 1363, loss 0.16471, acc 0.953125, learning_rate 0.000118658
2017-10-10T12:43:28.708421: step 1364, loss 0.205087, acc 0.9375, learning_rate 0.000118582
2017-10-10T12:43:29.117326: step 1365, loss 0.220889, acc 0.96875, learning_rate 0.000118506
2017-10-10T12:43:29.548881: step 1366, loss 0.195818, acc 0.9375, learning_rate 0.00011843
2017-10-10T12:43:29.987160: step 1367, loss 0.210617, acc 0.953125, learning_rate 0.000118355
2017-10-10T12:43:30.413272: step 1368, loss 0.127567, acc 0.9375, learning_rate 0.00011828
2017-10-10T12:43:30.775748: step 1369, loss 0.180228, acc 0.9375, learning_rate 0.000118205
2017-10-10T12:43:31.174975: step 1370, loss 0.13372, acc 0.96875, learning_rate 0.000118131
2017-10-10T12:43:31.626424: step 1371, loss 0.123603, acc 0.96875, learning_rate 0.000118057
2017-10-10T12:43:31.951799: step 1372, loss 0.0530194, acc 1, learning_rate 0.000117983
2017-10-10T12:43:32.364993: step 1373, loss 0.237432, acc 0.9375, learning_rate 0.00011791
2017-10-10T12:43:32.741865: step 1374, loss 0.143178, acc 0.953125, learning_rate 0.000117837
2017-10-10T12:43:33.128082: step 1375, loss 0.0951233, acc 0.96875, learning_rate 0.000117764
2017-10-10T12:43:33.540932: step 1376, loss 0.200266, acc 0.921875, learning_rate 0.000117692
2017-10-10T12:43:33.934462: step 1377, loss 0.0797804, acc 0.96875, learning_rate 0.000117619
2017-10-10T12:43:34.372874: step 1378, loss 0.138693, acc 0.953125, learning_rate 0.000117547
2017-10-10T12:43:34.870794: step 1379, loss 0.149437, acc 0.9375, learning_rate 0.000117476
2017-10-10T12:43:35.141178: step 1380, loss 0.0726959, acc 0.984375, learning_rate 0.000117404
2017-10-10T12:43:35.443349: step 1381, loss 0.135035, acc 0.9375, learning_rate 0.000117333
2017-10-10T12:43:35.785095: step 1382, loss 0.176538, acc 0.953125, learning_rate 0.000117263
2017-10-10T12:43:36.244048: step 1383, loss 0.121192, acc 0.9375, learning_rate 0.000117192
2017-10-10T12:43:36.547489: step 1384, loss 0.0599021, acc 0.984375, learning_rate 0.000117122
2017-10-10T12:43:36.850253: step 1385, loss 0.0885581, acc 0.953125, learning_rate 0.000117052
2017-10-10T12:43:37.130389: step 1386, loss 0.102404, acc 0.953125, learning_rate 0.000116983
2017-10-10T12:43:37.536670: step 1387, loss 0.251546, acc 0.890625, learning_rate 0.000116913
2017-10-10T12:43:37.908858: step 1388, loss 0.135336, acc 0.96875, learning_rate 0.000116844
2017-10-10T12:43:38.343726: step 1389, loss 0.130742, acc 0.9375, learning_rate 0.000116775
2017-10-10T12:43:38.750726: step 1390, loss 0.310929, acc 0.890625, learning_rate 0.000116707
2017-10-10T12:43:39.156994: step 1391, loss 0.175491, acc 0.90625, learning_rate 0.000116639
2017-10-10T12:43:39.585405: step 1392, loss 0.298324, acc 0.921875, learning_rate 0.000116571
2017-10-10T12:43:40.055498: step 1393, loss 0.120623, acc 0.96875, learning_rate 0.000116503
2017-10-10T12:43:40.433101: step 1394, loss 0.2749, acc 0.921875, learning_rate 0.000116436
2017-10-10T12:43:40.876979: step 1395, loss 0.377107, acc 0.859375, learning_rate 0.000116369
2017-10-10T12:43:41.218323: step 1396, loss 0.214831, acc 0.90625, learning_rate 0.000116302
2017-10-10T12:43:41.570344: step 1397, loss 0.191723, acc 0.921875, learning_rate 0.000116235
2017-10-10T12:43:41.934107: step 1398, loss 0.210792, acc 0.921875, learning_rate 0.000116169
2017-10-10T12:43:42.355102: step 1399, loss 0.13734, acc 0.953125, learning_rate 0.000116103
2017-10-10T12:43:42.778757: step 1400, loss 0.156088, acc 0.90625, learning_rate 0.000116037

Evaluation:
2017-10-10T12:43:43.683547: step 1400, loss 0.240112, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1400

2017-10-10T12:43:45.200961: step 1401, loss 0.207927, acc 0.953125, learning_rate 0.000115972
2017-10-10T12:43:45.656860: step 1402, loss 0.137556, acc 0.953125, learning_rate 0.000115907
2017-10-10T12:43:46.086275: step 1403, loss 0.187568, acc 0.9375, learning_rate 0.000115842
2017-10-10T12:43:46.497725: step 1404, loss 0.0928882, acc 0.953125, learning_rate 0.000115777
2017-10-10T12:43:46.881829: step 1405, loss 0.15594, acc 0.96875, learning_rate 0.000115713
2017-10-10T12:43:47.284828: step 1406, loss 0.0923698, acc 0.96875, learning_rate 0.000115649
2017-10-10T12:43:47.688509: step 1407, loss 0.131984, acc 0.96875, learning_rate 0.000115585
2017-10-10T12:43:48.110091: step 1408, loss 0.129586, acc 0.96875, learning_rate 0.000115521
2017-10-10T12:43:48.480839: step 1409, loss 0.178907, acc 0.96875, learning_rate 0.000115458
2017-10-10T12:43:48.817044: step 1410, loss 0.0699994, acc 0.984375, learning_rate 0.000115395
2017-10-10T12:43:49.156429: step 1411, loss 0.118291, acc 0.953125, learning_rate 0.000115332
2017-10-10T12:43:49.551641: step 1412, loss 0.108966, acc 0.96875, learning_rate 0.000115269
2017-10-10T12:43:49.939370: step 1413, loss 0.0476027, acc 0.96875, learning_rate 0.000115207
2017-10-10T12:43:50.258770: step 1414, loss 0.167916, acc 0.921875, learning_rate 0.000115145
2017-10-10T12:43:50.685052: step 1415, loss 0.176371, acc 0.9375, learning_rate 0.000115083
2017-10-10T12:43:51.065397: step 1416, loss 0.196139, acc 0.953125, learning_rate 0.000115022
2017-10-10T12:43:51.477932: step 1417, loss 0.160861, acc 0.953125, learning_rate 0.00011496
2017-10-10T12:43:51.976962: step 1418, loss 0.10999, acc 0.96875, learning_rate 0.000114899
2017-10-10T12:43:52.408861: step 1419, loss 0.34484, acc 0.875, learning_rate 0.000114838
2017-10-10T12:43:52.701033: step 1420, loss 0.117552, acc 0.96875, learning_rate 0.000114778
2017-10-10T12:43:52.995637: step 1421, loss 0.128901, acc 0.921875, learning_rate 0.000114717
2017-10-10T12:43:53.371904: step 1422, loss 0.0982215, acc 0.96875, learning_rate 0.000114657
2017-10-10T12:43:53.796944: step 1423, loss 0.0261026, acc 1, learning_rate 0.000114598
2017-10-10T12:43:54.097725: step 1424, loss 0.0587901, acc 1, learning_rate 0.000114538
2017-10-10T12:43:54.412402: step 1425, loss 0.147496, acc 0.9375, learning_rate 0.000114479
2017-10-10T12:43:54.718067: step 1426, loss 0.278584, acc 0.9375, learning_rate 0.00011442
2017-10-10T12:43:55.148915: step 1427, loss 0.138445, acc 0.96875, learning_rate 0.000114361
2017-10-10T12:43:55.541181: step 1428, loss 0.0569724, acc 1, learning_rate 0.000114302
2017-10-10T12:43:55.908047: step 1429, loss 0.203214, acc 0.9375, learning_rate 0.000114244
2017-10-10T12:43:56.323147: step 1430, loss 0.242307, acc 0.90625, learning_rate 0.000114186
2017-10-10T12:43:56.752549: step 1431, loss 0.220487, acc 0.90625, learning_rate 0.000114128
2017-10-10T12:43:57.169070: step 1432, loss 0.0907401, acc 0.96875, learning_rate 0.00011407
2017-10-10T12:43:57.580994: step 1433, loss 0.141417, acc 0.96875, learning_rate 0.000114013
2017-10-10T12:43:57.941107: step 1434, loss 0.206993, acc 0.921875, learning_rate 0.000113955
2017-10-10T12:43:58.361066: step 1435, loss 0.319983, acc 0.859375, learning_rate 0.000113898
2017-10-10T12:43:58.805539: step 1436, loss 0.0618304, acc 0.984375, learning_rate 0.000113842
2017-10-10T12:43:59.212383: step 1437, loss 0.200472, acc 0.921875, learning_rate 0.000113785
2017-10-10T12:43:59.528940: step 1438, loss 0.159919, acc 0.96875, learning_rate 0.000113729
2017-10-10T12:43:59.912404: step 1439, loss 0.269724, acc 0.90625, learning_rate 0.000113673
2017-10-10T12:44:00.313140: step 1440, loss 0.169915, acc 0.953125, learning_rate 0.000113617

Evaluation:
2017-10-10T12:44:01.272924: step 1440, loss 0.237534, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1440

2017-10-10T12:44:02.605275: step 1441, loss 0.140129, acc 0.90625, learning_rate 0.000113561
2017-10-10T12:44:03.035195: step 1442, loss 0.0957837, acc 1, learning_rate 0.000113506
2017-10-10T12:44:03.465628: step 1443, loss 0.229334, acc 0.890625, learning_rate 0.000113451
2017-10-10T12:44:03.874150: step 1444, loss 0.146175, acc 0.953125, learning_rate 0.000113396
2017-10-10T12:44:04.270042: step 1445, loss 0.11279, acc 0.96875, learning_rate 0.000113341
2017-10-10T12:44:04.670365: step 1446, loss 0.191057, acc 0.953125, learning_rate 0.000113287
2017-10-10T12:44:05.084833: step 1447, loss 0.209059, acc 0.9375, learning_rate 0.000113233
2017-10-10T12:44:05.495083: step 1448, loss 0.222712, acc 0.921875, learning_rate 0.000113179
2017-10-10T12:44:05.897450: step 1449, loss 0.132034, acc 0.96875, learning_rate 0.000113125
2017-10-10T12:44:06.314238: step 1450, loss 0.221147, acc 0.90625, learning_rate 0.000113071
2017-10-10T12:44:06.796450: step 1451, loss 0.122362, acc 0.953125, learning_rate 0.000113018
2017-10-10T12:44:07.202008: step 1452, loss 0.189157, acc 0.96875, learning_rate 0.000112965
2017-10-10T12:44:07.617709: step 1453, loss 0.110613, acc 0.96875, learning_rate 0.000112912
2017-10-10T12:44:08.031482: step 1454, loss 0.228988, acc 0.921875, learning_rate 0.000112859
2017-10-10T12:44:08.438520: step 1455, loss 0.117538, acc 0.9375, learning_rate 0.000112807
2017-10-10T12:44:08.841305: step 1456, loss 0.136161, acc 0.953125, learning_rate 0.000112754
2017-10-10T12:44:09.308720: step 1457, loss 0.198176, acc 0.9375, learning_rate 0.000112702
2017-10-10T12:44:09.763902: step 1458, loss 0.144126, acc 0.921875, learning_rate 0.000112651
2017-10-10T12:44:10.108576: step 1459, loss 0.279078, acc 0.9375, learning_rate 0.000112599
2017-10-10T12:44:10.428018: step 1460, loss 0.156673, acc 0.921875, learning_rate 0.000112547
2017-10-10T12:44:10.746421: step 1461, loss 0.20415, acc 0.921875, learning_rate 0.000112496
2017-10-10T12:44:11.197936: step 1462, loss 0.0381705, acc 1, learning_rate 0.000112445
2017-10-10T12:44:11.672953: step 1463, loss 0.139288, acc 0.96875, learning_rate 0.000112394
2017-10-10T12:44:11.983594: step 1464, loss 0.218465, acc 0.9375, learning_rate 0.000112344
2017-10-10T12:44:12.300194: step 1465, loss 0.124825, acc 0.921875, learning_rate 0.000112293
2017-10-10T12:44:12.608504: step 1466, loss 0.325784, acc 0.90625, learning_rate 0.000112243
2017-10-10T12:44:12.929612: step 1467, loss 0.229579, acc 0.921875, learning_rate 0.000112193
2017-10-10T12:44:13.351617: step 1468, loss 0.0496511, acc 0.984375, learning_rate 0.000112144
2017-10-10T12:44:13.768854: step 1469, loss 0.169468, acc 0.953125, learning_rate 0.000112094
2017-10-10T12:44:14.143525: step 1470, loss 0.0882768, acc 0.960784, learning_rate 0.000112045
2017-10-10T12:44:14.590363: step 1471, loss 0.158314, acc 0.9375, learning_rate 0.000111995
2017-10-10T12:44:15.005558: step 1472, loss 0.130184, acc 0.9375, learning_rate 0.000111946
2017-10-10T12:44:15.396979: step 1473, loss 0.127392, acc 0.953125, learning_rate 0.000111898
2017-10-10T12:44:15.848697: step 1474, loss 0.138912, acc 0.96875, learning_rate 0.000111849
2017-10-10T12:44:16.236979: step 1475, loss 0.0632097, acc 0.984375, learning_rate 0.000111801
2017-10-10T12:44:16.672917: step 1476, loss 0.103925, acc 0.96875, learning_rate 0.000111753
2017-10-10T12:44:17.114542: step 1477, loss 0.0812724, acc 0.96875, learning_rate 0.000111705
2017-10-10T12:44:17.538810: step 1478, loss 0.261445, acc 0.890625, learning_rate 0.000111657
2017-10-10T12:44:17.991225: step 1479, loss 0.1324, acc 0.984375, learning_rate 0.000111609
2017-10-10T12:44:18.409946: step 1480, loss 0.183963, acc 0.953125, learning_rate 0.000111562

Evaluation:
2017-10-10T12:44:19.336876: step 1480, loss 0.233887, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1480

2017-10-10T12:44:20.753049: step 1481, loss 0.183351, acc 0.9375, learning_rate 0.000111515
2017-10-10T12:44:21.153084: step 1482, loss 0.170679, acc 0.953125, learning_rate 0.000111468
2017-10-10T12:44:21.535298: step 1483, loss 0.156247, acc 0.9375, learning_rate 0.000111421
2017-10-10T12:44:21.888350: step 1484, loss 0.105355, acc 0.953125, learning_rate 0.000111374
2017-10-10T12:44:22.320904: step 1485, loss 0.189433, acc 0.921875, learning_rate 0.000111328
2017-10-10T12:44:22.732847: step 1486, loss 0.263709, acc 0.90625, learning_rate 0.000111282
2017-10-10T12:44:23.130083: step 1487, loss 0.199514, acc 0.921875, learning_rate 0.000111236
2017-10-10T12:44:23.552812: step 1488, loss 0.0892769, acc 0.96875, learning_rate 0.00011119
2017-10-10T12:44:23.986561: step 1489, loss 0.105339, acc 0.953125, learning_rate 0.000111144
2017-10-10T12:44:24.358140: step 1490, loss 0.163757, acc 0.96875, learning_rate 0.000111099
2017-10-10T12:44:24.750149: step 1491, loss 0.129078, acc 0.953125, learning_rate 0.000111053
2017-10-10T12:44:25.139676: step 1492, loss 0.196873, acc 0.921875, learning_rate 0.000111008
2017-10-10T12:44:25.500866: step 1493, loss 0.169639, acc 0.9375, learning_rate 0.000110963
2017-10-10T12:44:25.867864: step 1494, loss 0.347243, acc 0.9375, learning_rate 0.000110918
2017-10-10T12:44:26.302854: step 1495, loss 0.259271, acc 0.90625, learning_rate 0.000110874
2017-10-10T12:44:26.713434: step 1496, loss 0.165328, acc 0.9375, learning_rate 0.00011083
2017-10-10T12:44:27.152861: step 1497, loss 0.248486, acc 0.90625, learning_rate 0.000110785
2017-10-10T12:44:27.634333: step 1498, loss 0.157455, acc 0.9375, learning_rate 0.000110741
2017-10-10T12:44:27.969363: step 1499, loss 0.140392, acc 0.96875, learning_rate 0.000110697
2017-10-10T12:44:28.277428: step 1500, loss 0.15267, acc 0.96875, learning_rate 0.000110654
2017-10-10T12:44:28.574905: step 1501, loss 0.167888, acc 0.921875, learning_rate 0.00011061
2017-10-10T12:44:28.993055: step 1502, loss 0.0909003, acc 0.96875, learning_rate 0.000110567
2017-10-10T12:44:29.513820: step 1503, loss 0.131389, acc 0.953125, learning_rate 0.000110524
2017-10-10T12:44:29.788810: step 1504, loss 0.0629063, acc 0.984375, learning_rate 0.000110481
2017-10-10T12:44:30.072831: step 1505, loss 0.13622, acc 0.9375, learning_rate 0.000110438
2017-10-10T12:44:30.458248: step 1506, loss 0.116385, acc 0.953125, learning_rate 0.000110396
2017-10-10T12:44:30.848995: step 1507, loss 0.0923486, acc 0.96875, learning_rate 0.000110353
2017-10-10T12:44:31.287890: step 1508, loss 0.07116, acc 0.96875, learning_rate 0.000110311
2017-10-10T12:44:31.721557: step 1509, loss 0.101359, acc 0.953125, learning_rate 0.000110269
2017-10-10T12:44:32.124092: step 1510, loss 0.0564911, acc 0.984375, learning_rate 0.000110227
2017-10-10T12:44:32.508857: step 1511, loss 0.0632953, acc 0.984375, learning_rate 0.000110185
2017-10-10T12:44:32.872870: step 1512, loss 0.0910773, acc 0.984375, learning_rate 0.000110144
2017-10-10T12:44:33.201106: step 1513, loss 0.0333234, acc 1, learning_rate 0.000110102
2017-10-10T12:44:33.612825: step 1514, loss 0.210995, acc 0.90625, learning_rate 0.000110061
2017-10-10T12:44:33.981972: step 1515, loss 0.238148, acc 0.921875, learning_rate 0.00011002
2017-10-10T12:44:34.337156: step 1516, loss 0.172198, acc 0.9375, learning_rate 0.000109979
2017-10-10T12:44:34.738785: step 1517, loss 0.230611, acc 0.9375, learning_rate 0.000109938
2017-10-10T12:44:35.121306: step 1518, loss 0.175001, acc 0.921875, learning_rate 0.000109898
2017-10-10T12:44:35.511984: step 1519, loss 0.117899, acc 0.953125, learning_rate 0.000109857
2017-10-10T12:44:35.953337: step 1520, loss 0.185608, acc 0.90625, learning_rate 0.000109817

Evaluation:
2017-10-10T12:44:36.876763: step 1520, loss 0.234618, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1520

2017-10-10T12:44:38.347021: step 1521, loss 0.255074, acc 0.90625, learning_rate 0.000109777
2017-10-10T12:44:38.748842: step 1522, loss 0.149314, acc 0.9375, learning_rate 0.000109737
2017-10-10T12:44:39.171665: step 1523, loss 0.0410401, acc 0.984375, learning_rate 0.000109697
2017-10-10T12:44:39.595202: step 1524, loss 0.139275, acc 0.96875, learning_rate 0.000109658
2017-10-10T12:44:40.035799: step 1525, loss 0.193371, acc 0.921875, learning_rate 0.000109618
2017-10-10T12:44:40.394259: step 1526, loss 0.0960406, acc 0.953125, learning_rate 0.000109579
2017-10-10T12:44:40.855721: step 1527, loss 0.160538, acc 0.9375, learning_rate 0.00010954
2017-10-10T12:44:41.303965: step 1528, loss 0.128705, acc 0.921875, learning_rate 0.000109501
2017-10-10T12:44:41.740845: step 1529, loss 0.203736, acc 0.921875, learning_rate 0.000109462
2017-10-10T12:44:42.166983: step 1530, loss 0.244247, acc 0.90625, learning_rate 0.000109424
2017-10-10T12:44:42.572830: step 1531, loss 0.132914, acc 0.9375, learning_rate 0.000109385
2017-10-10T12:44:42.980748: step 1532, loss 0.0839037, acc 0.96875, learning_rate 0.000109347
2017-10-10T12:44:43.364894: step 1533, loss 0.173564, acc 0.9375, learning_rate 0.000109309
2017-10-10T12:44:43.776877: step 1534, loss 0.291877, acc 0.90625, learning_rate 0.000109271
2017-10-10T12:44:44.159557: step 1535, loss 0.108932, acc 0.96875, learning_rate 0.000109233
2017-10-10T12:44:44.501095: step 1536, loss 0.274443, acc 0.9375, learning_rate 0.000109195
2017-10-10T12:44:44.857718: step 1537, loss 0.10649, acc 0.984375, learning_rate 0.000109158
2017-10-10T12:44:45.377165: step 1538, loss 0.270985, acc 0.90625, learning_rate 0.00010912
2017-10-10T12:44:45.800831: step 1539, loss 0.220331, acc 0.921875, learning_rate 0.000109083
2017-10-10T12:44:46.150958: step 1540, loss 0.0878553, acc 0.96875, learning_rate 0.000109046
2017-10-10T12:44:46.476691: step 1541, loss 0.242169, acc 0.921875, learning_rate 0.000109009
2017-10-10T12:44:46.865029: step 1542, loss 0.158562, acc 0.953125, learning_rate 0.000108972
2017-10-10T12:44:47.345359: step 1543, loss 0.273148, acc 0.90625, learning_rate 0.000108936
2017-10-10T12:44:47.615352: step 1544, loss 0.0834121, acc 0.96875, learning_rate 0.000108899
2017-10-10T12:44:47.952863: step 1545, loss 0.0888231, acc 0.96875, learning_rate 0.000108863
2017-10-10T12:44:48.340819: step 1546, loss 0.281915, acc 0.890625, learning_rate 0.000108827
2017-10-10T12:44:48.755482: step 1547, loss 0.270189, acc 0.921875, learning_rate 0.000108791
2017-10-10T12:44:49.202897: step 1548, loss 0.159792, acc 0.9375, learning_rate 0.000108755
2017-10-10T12:44:49.662413: step 1549, loss 0.217339, acc 0.90625, learning_rate 0.000108719
2017-10-10T12:44:50.088289: step 1550, loss 0.0923278, acc 0.96875, learning_rate 0.000108683
2017-10-10T12:44:50.497842: step 1551, loss 0.148752, acc 0.953125, learning_rate 0.000108648
2017-10-10T12:44:50.904890: step 1552, loss 0.0968023, acc 0.984375, learning_rate 0.000108613
2017-10-10T12:44:51.303594: step 1553, loss 0.111136, acc 0.96875, learning_rate 0.000108577
2017-10-10T12:44:51.724824: step 1554, loss 0.199839, acc 0.9375, learning_rate 0.000108542
2017-10-10T12:44:52.196197: step 1555, loss 0.158733, acc 0.9375, learning_rate 0.000108508
2017-10-10T12:44:52.708990: step 1556, loss 0.0844654, acc 0.96875, learning_rate 0.000108473
2017-10-10T12:44:53.114861: step 1557, loss 0.123787, acc 0.953125, learning_rate 0.000108438
2017-10-10T12:44:53.518720: step 1558, loss 0.0880854, acc 0.984375, learning_rate 0.000108404
2017-10-10T12:44:53.944321: step 1559, loss 0.151383, acc 0.921875, learning_rate 0.00010837
2017-10-10T12:44:54.434062: step 1560, loss 0.0867139, acc 0.96875, learning_rate 0.000108335

Evaluation:
2017-10-10T12:44:55.460077: step 1560, loss 0.234517, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1560

2017-10-10T12:44:56.761134: step 1561, loss 0.090243, acc 0.953125, learning_rate 0.000108301
2017-10-10T12:44:57.132408: step 1562, loss 0.0538844, acc 0.984375, learning_rate 0.000108267
2017-10-10T12:44:57.579245: step 1563, loss 0.137512, acc 0.953125, learning_rate 0.000108234
2017-10-10T12:44:58.055631: step 1564, loss 0.2951, acc 0.890625, learning_rate 0.0001082
2017-10-10T12:44:58.480362: step 1565, loss 0.0598353, acc 0.984375, learning_rate 0.000108167
2017-10-10T12:44:58.948933: step 1566, loss 0.116777, acc 0.9375, learning_rate 0.000108133
2017-10-10T12:44:59.368439: step 1567, loss 0.111143, acc 0.953125, learning_rate 0.0001081
2017-10-10T12:44:59.708736: step 1568, loss 0.196199, acc 0.960784, learning_rate 0.000108067
2017-10-10T12:45:00.108118: step 1569, loss 0.124882, acc 0.953125, learning_rate 0.000108034
2017-10-10T12:45:00.562377: step 1570, loss 0.161428, acc 0.9375, learning_rate 0.000108001
2017-10-10T12:45:00.935853: step 1571, loss 0.243748, acc 0.90625, learning_rate 0.000107969
2017-10-10T12:45:01.361545: step 1572, loss 0.214201, acc 0.9375, learning_rate 0.000107936
2017-10-10T12:45:01.804896: step 1573, loss 0.137595, acc 0.9375, learning_rate 0.000107904
2017-10-10T12:45:02.256835: step 1574, loss 0.0849146, acc 0.96875, learning_rate 0.000107871
2017-10-10T12:45:02.636893: step 1575, loss 0.119693, acc 0.953125, learning_rate 0.000107839
2017-10-10T12:45:03.064998: step 1576, loss 0.33009, acc 0.875, learning_rate 0.000107807
2017-10-10T12:45:03.468757: step 1577, loss 0.18538, acc 0.9375, learning_rate 0.000107775
2017-10-10T12:45:03.972898: step 1578, loss 0.252895, acc 0.921875, learning_rate 0.000107744
2017-10-10T12:45:04.295094: step 1579, loss 0.11361, acc 0.96875, learning_rate 0.000107712
2017-10-10T12:45:04.612620: step 1580, loss 0.128384, acc 0.96875, learning_rate 0.000107681
2017-10-10T12:45:04.954966: step 1581, loss 0.0488665, acc 1, learning_rate 0.000107649
2017-10-10T12:45:05.312954: step 1582, loss 0.0800513, acc 0.984375, learning_rate 0.000107618
2017-10-10T12:45:05.825425: step 1583, loss 0.185851, acc 0.921875, learning_rate 0.000107587
2017-10-10T12:45:06.148878: step 1584, loss 0.0594418, acc 1, learning_rate 0.000107556
2017-10-10T12:45:06.442705: step 1585, loss 0.340167, acc 0.859375, learning_rate 0.000107525
2017-10-10T12:45:06.772851: step 1586, loss 0.103662, acc 0.96875, learning_rate 0.000107494
2017-10-10T12:45:07.061580: step 1587, loss 0.171098, acc 0.953125, learning_rate 0.000107464
2017-10-10T12:45:07.456862: step 1588, loss 0.161807, acc 0.9375, learning_rate 0.000107433
2017-10-10T12:45:07.890690: step 1589, loss 0.271012, acc 0.90625, learning_rate 0.000107403
2017-10-10T12:45:08.283633: step 1590, loss 0.0824924, acc 0.96875, learning_rate 0.000107373
2017-10-10T12:45:08.722505: step 1591, loss 0.199924, acc 0.921875, learning_rate 0.000107343
2017-10-10T12:45:09.131141: step 1592, loss 0.211085, acc 0.9375, learning_rate 0.000107313
2017-10-10T12:45:09.568995: step 1593, loss 0.124635, acc 0.953125, learning_rate 0.000107283
2017-10-10T12:45:10.069229: step 1594, loss 0.0584543, acc 1, learning_rate 0.000107253
2017-10-10T12:45:10.472870: step 1595, loss 0.0511401, acc 0.984375, learning_rate 0.000107224
2017-10-10T12:45:10.873103: step 1596, loss 0.108395, acc 0.96875, learning_rate 0.000107194
2017-10-10T12:45:11.316849: step 1597, loss 0.199201, acc 0.9375, learning_rate 0.000107165
2017-10-10T12:45:11.788991: step 1598, loss 0.102112, acc 0.953125, learning_rate 0.000107136
2017-10-10T12:45:12.213649: step 1599, loss 0.203767, acc 0.921875, learning_rate 0.000107106
2017-10-10T12:45:12.649604: step 1600, loss 0.242225, acc 0.90625, learning_rate 0.000107077

Evaluation:
2017-10-10T12:45:13.629474: step 1600, loss 0.234318, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1600

2017-10-10T12:45:15.053729: step 1601, loss 0.116657, acc 0.96875, learning_rate 0.000107048
2017-10-10T12:45:15.456026: step 1602, loss 0.207835, acc 0.921875, learning_rate 0.00010702
2017-10-10T12:45:15.888838: step 1603, loss 0.190005, acc 0.953125, learning_rate 0.000106991
2017-10-10T12:45:16.355903: step 1604, loss 0.149248, acc 0.96875, learning_rate 0.000106963
2017-10-10T12:45:16.793959: step 1605, loss 0.148392, acc 0.921875, learning_rate 0.000106934
2017-10-10T12:45:17.269528: step 1606, loss 0.138351, acc 0.921875, learning_rate 0.000106906
2017-10-10T12:45:17.692246: step 1607, loss 0.207555, acc 0.90625, learning_rate 0.000106878
2017-10-10T12:45:18.134964: step 1608, loss 0.107488, acc 0.953125, learning_rate 0.00010685
2017-10-10T12:45:18.531419: step 1609, loss 0.205211, acc 0.953125, learning_rate 0.000106822
2017-10-10T12:45:18.927873: step 1610, loss 0.116864, acc 0.953125, learning_rate 0.000106794
2017-10-10T12:45:19.328986: step 1611, loss 0.0723192, acc 0.953125, learning_rate 0.000106766
2017-10-10T12:45:19.763810: step 1612, loss 0.142491, acc 0.953125, learning_rate 0.000106738
2017-10-10T12:45:20.177074: step 1613, loss 0.100551, acc 0.96875, learning_rate 0.000106711
2017-10-10T12:45:20.586531: step 1614, loss 0.106901, acc 0.96875, learning_rate 0.000106684
2017-10-10T12:45:21.021277: step 1615, loss 0.162058, acc 0.9375, learning_rate 0.000106656
2017-10-10T12:45:21.407431: step 1616, loss 0.108875, acc 0.953125, learning_rate 0.000106629
2017-10-10T12:45:21.864770: step 1617, loss 0.0595065, acc 0.984375, learning_rate 0.000106602
2017-10-10T12:45:22.391487: step 1618, loss 0.211106, acc 0.921875, learning_rate 0.000106575
2017-10-10T12:45:22.839743: step 1619, loss 0.105404, acc 0.9375, learning_rate 0.000106548
2017-10-10T12:45:23.196773: step 1620, loss 0.143378, acc 0.96875, learning_rate 0.000106521
2017-10-10T12:45:23.546153: step 1621, loss 0.0850383, acc 0.984375, learning_rate 0.000106495
2017-10-10T12:45:23.962918: step 1622, loss 0.205195, acc 0.921875, learning_rate 0.000106468
2017-10-10T12:45:24.448337: step 1623, loss 0.0754353, acc 0.96875, learning_rate 0.000106442
2017-10-10T12:45:24.970414: step 1624, loss 0.175851, acc 0.9375, learning_rate 0.000106416
2017-10-10T12:45:25.321847: step 1625, loss 0.263376, acc 0.890625, learning_rate 0.000106389
2017-10-10T12:45:25.671987: step 1626, loss 0.0681062, acc 0.96875, learning_rate 0.000106363
2017-10-10T12:45:26.111530: step 1627, loss 0.225226, acc 0.90625, learning_rate 0.000106337
2017-10-10T12:45:26.672296: step 1628, loss 0.213061, acc 0.90625, learning_rate 0.000106312
2017-10-10T12:45:27.182343: step 1629, loss 0.0944072, acc 0.984375, learning_rate 0.000106286
2017-10-10T12:45:27.715392: step 1630, loss 0.0898349, acc 0.953125, learning_rate 0.00010626
2017-10-10T12:45:28.260493: step 1631, loss 0.141201, acc 0.96875, learning_rate 0.000106235
2017-10-10T12:45:28.744912: step 1632, loss 0.219615, acc 0.953125, learning_rate 0.000106209
2017-10-10T12:45:29.192688: step 1633, loss 0.342039, acc 0.890625, learning_rate 0.000106184
2017-10-10T12:45:29.625117: step 1634, loss 0.135177, acc 0.953125, learning_rate 0.000106159
2017-10-10T12:45:30.189832: step 1635, loss 0.12427, acc 0.96875, learning_rate 0.000106133
2017-10-10T12:45:30.703955: step 1636, loss 0.0838154, acc 0.96875, learning_rate 0.000106108
2017-10-10T12:45:31.201017: step 1637, loss 0.182093, acc 0.9375, learning_rate 0.000106083
2017-10-10T12:45:31.669159: step 1638, loss 0.118541, acc 0.96875, learning_rate 0.000106059
2017-10-10T12:45:32.162910: step 1639, loss 0.073073, acc 0.984375, learning_rate 0.000106034
2017-10-10T12:45:32.654943: step 1640, loss 0.174752, acc 0.953125, learning_rate 0.000106009

Evaluation:
2017-10-10T12:45:33.900997: step 1640, loss 0.233133, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1640

2017-10-10T12:45:36.548866: step 1641, loss 0.126957, acc 0.984375, learning_rate 0.000105985
2017-10-10T12:45:37.029148: step 1642, loss 0.147592, acc 0.921875, learning_rate 0.00010596
2017-10-10T12:45:37.544850: step 1643, loss 0.306527, acc 0.875, learning_rate 0.000105936
2017-10-10T12:45:38.032984: step 1644, loss 0.124388, acc 0.9375, learning_rate 0.000105912
2017-10-10T12:45:38.558585: step 1645, loss 0.0788349, acc 0.984375, learning_rate 0.000105888
2017-10-10T12:45:39.069067: step 1646, loss 0.125277, acc 0.9375, learning_rate 0.000105864
2017-10-10T12:45:39.560874: step 1647, loss 0.0421235, acc 0.96875, learning_rate 0.00010584
2017-10-10T12:45:40.121081: step 1648, loss 0.193094, acc 0.953125, learning_rate 0.000105816
2017-10-10T12:45:40.597198: step 1649, loss 0.277167, acc 0.921875, learning_rate 0.000105792
2017-10-10T12:45:41.083455: step 1650, loss 0.239093, acc 0.90625, learning_rate 0.000105768
2017-10-10T12:45:41.512620: step 1651, loss 0.0338455, acc 1, learning_rate 0.000105745
2017-10-10T12:45:42.093041: step 1652, loss 0.101631, acc 0.96875, learning_rate 0.000105721
2017-10-10T12:45:42.662599: step 1653, loss 0.121956, acc 0.953125, learning_rate 0.000105698
2017-10-10T12:45:43.212882: step 1654, loss 0.126497, acc 0.984375, learning_rate 0.000105675
2017-10-10T12:45:43.696619: step 1655, loss 0.141044, acc 0.953125, learning_rate 0.000105652
2017-10-10T12:45:44.357033: step 1656, loss 0.135225, acc 0.96875, learning_rate 0.000105629
2017-10-10T12:45:44.924994: step 1657, loss 0.102151, acc 0.953125, learning_rate 0.000105606
2017-10-10T12:45:45.366785: step 1658, loss 0.185756, acc 0.9375, learning_rate 0.000105583
2017-10-10T12:45:45.688855: step 1659, loss 0.193246, acc 0.953125, learning_rate 0.00010556
2017-10-10T12:45:46.256945: step 1660, loss 0.143597, acc 0.9375, learning_rate 0.000105537
2017-10-10T12:45:46.932892: step 1661, loss 0.088626, acc 0.953125, learning_rate 0.000105515
2017-10-10T12:45:47.359771: step 1662, loss 0.0689552, acc 0.96875, learning_rate 0.000105492
2017-10-10T12:45:47.868504: step 1663, loss 0.074227, acc 0.984375, learning_rate 0.00010547
2017-10-10T12:45:48.324836: step 1664, loss 0.1812, acc 0.921875, learning_rate 0.000105447
2017-10-10T12:45:48.791496: step 1665, loss 0.0765309, acc 0.984375, learning_rate 0.000105425
2017-10-10T12:45:49.151686: step 1666, loss 0.0811921, acc 0.980392, learning_rate 0.000105403
2017-10-10T12:45:49.834160: step 1667, loss 0.129352, acc 0.9375, learning_rate 0.000105381
2017-10-10T12:45:50.216109: step 1668, loss 0.207977, acc 0.90625, learning_rate 0.000105359
2017-10-10T12:45:50.613007: step 1669, loss 0.122778, acc 0.9375, learning_rate 0.000105337
2017-10-10T12:45:51.070509: step 1670, loss 0.0737062, acc 0.984375, learning_rate 0.000105315
2017-10-10T12:45:51.516958: step 1671, loss 0.200639, acc 0.9375, learning_rate 0.000105294
2017-10-10T12:45:52.120570: step 1672, loss 0.098791, acc 0.953125, learning_rate 0.000105272
2017-10-10T12:45:52.667635: step 1673, loss 0.119188, acc 0.953125, learning_rate 0.000105251
2017-10-10T12:45:53.172930: step 1674, loss 0.127529, acc 0.9375, learning_rate 0.000105229
2017-10-10T12:45:53.645051: step 1675, loss 0.0885031, acc 0.96875, learning_rate 0.000105208
2017-10-10T12:45:54.212105: step 1676, loss 0.198592, acc 0.90625, learning_rate 0.000105186
2017-10-10T12:45:54.784892: step 1677, loss 0.0846069, acc 0.96875, learning_rate 0.000105165
2017-10-10T12:45:55.387715: step 1678, loss 0.0992976, acc 0.953125, learning_rate 0.000105144
2017-10-10T12:45:55.946251: step 1679, loss 0.060483, acc 0.984375, learning_rate 0.000105123
2017-10-10T12:45:56.476841: step 1680, loss 0.138525, acc 0.96875, learning_rate 0.000105102

Evaluation:
2017-10-10T12:45:57.727365: step 1680, loss 0.234558, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1680

2017-10-10T12:45:59.313003: step 1681, loss 0.258263, acc 0.921875, learning_rate 0.000105081
2017-10-10T12:45:59.844993: step 1682, loss 0.0697287, acc 1, learning_rate 0.000105061
2017-10-10T12:46:00.360209: step 1683, loss 0.0791778, acc 0.953125, learning_rate 0.00010504
2017-10-10T12:46:00.819802: step 1684, loss 0.210696, acc 0.90625, learning_rate 0.00010502
2017-10-10T12:46:01.316987: step 1685, loss 0.0890244, acc 0.96875, learning_rate 0.000104999
2017-10-10T12:46:01.911132: step 1686, loss 0.0489575, acc 0.984375, learning_rate 0.000104979
2017-10-10T12:46:02.412968: step 1687, loss 0.155211, acc 0.953125, learning_rate 0.000104958
2017-10-10T12:46:02.924862: step 1688, loss 0.240864, acc 0.921875, learning_rate 0.000104938
2017-10-10T12:46:03.448257: step 1689, loss 0.159361, acc 0.9375, learning_rate 0.000104918
2017-10-10T12:46:04.016987: step 1690, loss 0.0595556, acc 1, learning_rate 0.000104898
2017-10-10T12:46:04.544893: step 1691, loss 0.112342, acc 0.96875, learning_rate 0.000104878
2017-10-10T12:46:05.101902: step 1692, loss 0.21237, acc 0.9375, learning_rate 0.000104858
2017-10-10T12:46:05.619609: step 1693, loss 0.309645, acc 0.921875, learning_rate 0.000104838
2017-10-10T12:46:06.128894: step 1694, loss 0.0703912, acc 0.984375, learning_rate 0.000104818
2017-10-10T12:46:06.656878: step 1695, loss 0.268551, acc 0.875, learning_rate 0.000104799
2017-10-10T12:46:07.128922: step 1696, loss 0.116824, acc 0.953125, learning_rate 0.000104779
2017-10-10T12:46:07.574166: step 1697, loss 0.0701313, acc 1, learning_rate 0.00010476
2017-10-10T12:46:07.962356: step 1698, loss 0.0489205, acc 0.984375, learning_rate 0.00010474
2017-10-10T12:46:08.360954: step 1699, loss 0.0283297, acc 1, learning_rate 0.000104721
2017-10-10T12:46:08.766581: step 1700, loss 0.048421, acc 0.984375, learning_rate 0.000104702
2017-10-10T12:46:09.296909: step 1701, loss 0.223193, acc 0.90625, learning_rate 0.000104682
2017-10-10T12:46:09.804501: step 1702, loss 0.203414, acc 0.921875, learning_rate 0.000104663
2017-10-10T12:46:10.484664: step 1703, loss 0.0679637, acc 0.984375, learning_rate 0.000104644
2017-10-10T12:46:10.948705: step 1704, loss 0.0954026, acc 0.96875, learning_rate 0.000104625
2017-10-10T12:46:11.409810: step 1705, loss 0.374979, acc 0.859375, learning_rate 0.000104606
2017-10-10T12:46:11.893211: step 1706, loss 0.105184, acc 0.9375, learning_rate 0.000104588
2017-10-10T12:46:12.440906: step 1707, loss 0.23581, acc 0.890625, learning_rate 0.000104569
2017-10-10T12:46:13.089022: step 1708, loss 0.214085, acc 0.921875, learning_rate 0.00010455
2017-10-10T12:46:13.747715: step 1709, loss 0.11673, acc 0.96875, learning_rate 0.000104532
2017-10-10T12:46:14.080817: step 1710, loss 0.254592, acc 0.90625, learning_rate 0.000104513
2017-10-10T12:46:14.501233: step 1711, loss 0.113027, acc 0.984375, learning_rate 0.000104495
2017-10-10T12:46:14.905007: step 1712, loss 0.134222, acc 0.953125, learning_rate 0.000104476
2017-10-10T12:46:15.497320: step 1713, loss 0.0687691, acc 1, learning_rate 0.000104458
2017-10-10T12:46:16.005005: step 1714, loss 0.115165, acc 0.96875, learning_rate 0.00010444
2017-10-10T12:46:16.529111: step 1715, loss 0.127447, acc 0.953125, learning_rate 0.000104422
2017-10-10T12:46:17.129217: step 1716, loss 0.126454, acc 0.9375, learning_rate 0.000104404
2017-10-10T12:46:17.648796: step 1717, loss 0.113721, acc 0.96875, learning_rate 0.000104386
2017-10-10T12:46:18.109159: step 1718, loss 0.211066, acc 0.90625, learning_rate 0.000104368
2017-10-10T12:46:18.590700: step 1719, loss 0.205279, acc 0.921875, learning_rate 0.00010435
2017-10-10T12:46:19.166460: step 1720, loss 0.0912443, acc 0.96875, learning_rate 0.000104332

Evaluation:
2017-10-10T12:46:20.336979: step 1720, loss 0.238982, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1720

2017-10-10T12:46:22.737043: step 1721, loss 0.266451, acc 0.890625, learning_rate 0.000104315
2017-10-10T12:46:23.266812: step 1722, loss 0.231197, acc 0.9375, learning_rate 0.000104297
2017-10-10T12:46:23.818792: step 1723, loss 0.288659, acc 0.875, learning_rate 0.000104279
2017-10-10T12:46:24.210754: step 1724, loss 0.152682, acc 0.953125, learning_rate 0.000104262
2017-10-10T12:46:24.668989: step 1725, loss 0.113227, acc 0.96875, learning_rate 0.000104245
2017-10-10T12:46:25.147588: step 1726, loss 0.0608838, acc 0.96875, learning_rate 0.000104227
2017-10-10T12:46:25.684976: step 1727, loss 0.0826228, acc 0.96875, learning_rate 0.00010421
2017-10-10T12:46:26.268873: step 1728, loss 0.113344, acc 0.96875, learning_rate 0.000104193
2017-10-10T12:46:26.780345: step 1729, loss 0.26144, acc 0.890625, learning_rate 0.000104176
2017-10-10T12:46:27.296393: step 1730, loss 0.171046, acc 0.953125, learning_rate 0.000104159
2017-10-10T12:46:27.848998: step 1731, loss 0.139668, acc 0.9375, learning_rate 0.000104142
2017-10-10T12:46:28.387482: step 1732, loss 0.180089, acc 0.9375, learning_rate 0.000104125
2017-10-10T12:46:28.936993: step 1733, loss 0.108365, acc 0.96875, learning_rate 0.000104108
2017-10-10T12:46:29.418226: step 1734, loss 0.207203, acc 0.90625, learning_rate 0.000104091
2017-10-10T12:46:29.969536: step 1735, loss 0.211851, acc 0.90625, learning_rate 0.000104074
2017-10-10T12:46:30.522775: step 1736, loss 0.166299, acc 0.9375, learning_rate 0.000104058
2017-10-10T12:46:31.055598: step 1737, loss 0.072993, acc 0.984375, learning_rate 0.000104041
2017-10-10T12:46:31.459845: step 1738, loss 0.254913, acc 0.921875, learning_rate 0.000104025
2017-10-10T12:46:31.897081: step 1739, loss 0.226988, acc 0.921875, learning_rate 0.000104008
2017-10-10T12:46:32.472921: step 1740, loss 0.139584, acc 0.953125, learning_rate 0.000103992
2017-10-10T12:46:33.048172: step 1741, loss 0.0568728, acc 0.984375, learning_rate 0.000103976
2017-10-10T12:46:33.560102: step 1742, loss 0.168037, acc 0.921875, learning_rate 0.000103959
2017-10-10T12:46:34.329043: step 1743, loss 0.23244, acc 0.921875, learning_rate 0.000103943
2017-10-10T12:46:34.762548: step 1744, loss 0.237663, acc 0.890625, learning_rate 0.000103927
2017-10-10T12:46:35.223919: step 1745, loss 0.196919, acc 0.9375, learning_rate 0.000103911
2017-10-10T12:46:35.677646: step 1746, loss 0.120063, acc 0.953125, learning_rate 0.000103895
2017-10-10T12:46:36.175028: step 1747, loss 0.0859029, acc 0.984375, learning_rate 0.000103879
2017-10-10T12:46:36.620997: step 1748, loss 0.113178, acc 0.96875, learning_rate 0.000103863
2017-10-10T12:46:37.260968: step 1749, loss 0.06291, acc 0.984375, learning_rate 0.000103848
2017-10-10T12:46:37.713031: step 1750, loss 0.147182, acc 0.9375, learning_rate 0.000103832
2017-10-10T12:46:38.168837: step 1751, loss 0.150057, acc 0.9375, learning_rate 0.000103816
2017-10-10T12:46:38.654030: step 1752, loss 0.136671, acc 0.953125, learning_rate 0.000103801
2017-10-10T12:46:39.200958: step 1753, loss 0.179853, acc 0.96875, learning_rate 0.000103785
2017-10-10T12:46:39.735135: step 1754, loss 0.158477, acc 0.9375, learning_rate 0.00010377
2017-10-10T12:46:40.289108: step 1755, loss 0.191139, acc 0.9375, learning_rate 0.000103754
2017-10-10T12:46:40.729609: step 1756, loss 0.0761565, acc 0.96875, learning_rate 0.000103739
2017-10-10T12:46:41.276029: step 1757, loss 0.18953, acc 0.9375, learning_rate 0.000103724
2017-10-10T12:46:41.820504: step 1758, loss 0.0897877, acc 0.96875, learning_rate 0.000103709
2017-10-10T12:46:42.404839: step 1759, loss 0.115829, acc 0.96875, learning_rate 0.000103694
2017-10-10T12:46:42.948485: step 1760, loss 0.0597691, acc 0.96875, learning_rate 0.000103678

Evaluation:
2017-10-10T12:46:44.236176: step 1760, loss 0.23527, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1760

2017-10-10T12:46:45.986561: step 1761, loss 0.170121, acc 0.9375, learning_rate 0.000103663
2017-10-10T12:46:46.468879: step 1762, loss 0.0546094, acc 1, learning_rate 0.000103648
2017-10-10T12:46:46.962861: step 1763, loss 0.10695, acc 0.96875, learning_rate 0.000103634
2017-10-10T12:46:47.353035: step 1764, loss 0.0848828, acc 1, learning_rate 0.000103619
2017-10-10T12:46:47.862255: step 1765, loss 0.185428, acc 0.921875, learning_rate 0.000103604
2017-10-10T12:46:48.336218: step 1766, loss 0.12804, acc 0.953125, learning_rate 0.000103589
2017-10-10T12:46:48.833720: step 1767, loss 0.177975, acc 0.90625, learning_rate 0.000103575
2017-10-10T12:46:49.236354: step 1768, loss 0.0488615, acc 1, learning_rate 0.00010356
2017-10-10T12:46:49.804917: step 1769, loss 0.0890619, acc 0.96875, learning_rate 0.000103545
2017-10-10T12:46:50.319854: step 1770, loss 0.102395, acc 0.9375, learning_rate 0.000103531
2017-10-10T12:46:50.815942: step 1771, loss 0.256685, acc 0.890625, learning_rate 0.000103517
2017-10-10T12:46:51.326566: step 1772, loss 0.202147, acc 0.890625, learning_rate 0.000103502
2017-10-10T12:46:51.772856: step 1773, loss 0.09507, acc 0.984375, learning_rate 0.000103488
2017-10-10T12:46:52.284933: step 1774, loss 0.111869, acc 0.96875, learning_rate 0.000103474
2017-10-10T12:46:52.817008: step 1775, loss 0.0790373, acc 0.96875, learning_rate 0.00010346
2017-10-10T12:46:53.453335: step 1776, loss 0.106184, acc 0.96875, learning_rate 0.000103445
2017-10-10T12:46:53.972435: step 1777, loss 0.0439053, acc 1, learning_rate 0.000103431
2017-10-10T12:46:54.328940: step 1778, loss 0.354087, acc 0.90625, learning_rate 0.000103417
2017-10-10T12:46:54.822958: step 1779, loss 0.0739882, acc 0.96875, learning_rate 0.000103403
2017-10-10T12:46:55.288430: step 1780, loss 0.130342, acc 0.96875, learning_rate 0.00010339
2017-10-10T12:46:55.797140: step 1781, loss 0.137587, acc 0.96875, learning_rate 0.000103376
2017-10-10T12:46:56.303400: step 1782, loss 0.102291, acc 0.9375, learning_rate 0.000103362
2017-10-10T12:46:56.901050: step 1783, loss 0.124618, acc 0.9375, learning_rate 0.000103348
2017-10-10T12:46:57.327110: step 1784, loss 0.23649, acc 0.921875, learning_rate 0.000103335
2017-10-10T12:46:57.767973: step 1785, loss 0.12334, acc 0.953125, learning_rate 0.000103321
2017-10-10T12:46:58.266760: step 1786, loss 0.17699, acc 0.90625, learning_rate 0.000103307
2017-10-10T12:46:58.818774: step 1787, loss 0.0893068, acc 0.96875, learning_rate 0.000103294
2017-10-10T12:46:59.308473: step 1788, loss 0.156649, acc 0.953125, learning_rate 0.00010328
2017-10-10T12:46:59.832697: step 1789, loss 0.09006, acc 0.984375, learning_rate 0.000103267
2017-10-10T12:47:00.416951: step 1790, loss 0.134128, acc 0.9375, learning_rate 0.000103254
2017-10-10T12:47:00.992961: step 1791, loss 0.104162, acc 0.984375, learning_rate 0.00010324
2017-10-10T12:47:01.445100: step 1792, loss 0.146236, acc 0.921875, learning_rate 0.000103227
2017-10-10T12:47:01.934374: step 1793, loss 0.264352, acc 0.921875, learning_rate 0.000103214
2017-10-10T12:47:02.457469: step 1794, loss 0.145663, acc 0.953125, learning_rate 0.000103201
2017-10-10T12:47:02.976906: step 1795, loss 0.143925, acc 0.921875, learning_rate 0.000103188
2017-10-10T12:47:03.429003: step 1796, loss 0.0954652, acc 0.953125, learning_rate 0.000103175
2017-10-10T12:47:03.922619: step 1797, loss 0.0629401, acc 0.984375, learning_rate 0.000103162
2017-10-10T12:47:04.411234: step 1798, loss 0.207842, acc 0.90625, learning_rate 0.000103149
2017-10-10T12:47:04.872721: step 1799, loss 0.190827, acc 0.90625, learning_rate 0.000103136
2017-10-10T12:47:05.473029: step 1800, loss 0.0451271, acc 1, learning_rate 0.000103123

Evaluation:
2017-10-10T12:47:06.526178: step 1800, loss 0.232899, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1800

2017-10-10T12:47:08.050952: step 1801, loss 0.203531, acc 0.953125, learning_rate 0.000103111
2017-10-10T12:47:08.584937: step 1802, loss 0.259666, acc 0.9375, learning_rate 0.000103098
2017-10-10T12:47:09.124188: step 1803, loss 0.118169, acc 0.96875, learning_rate 0.000103085
2017-10-10T12:47:09.608003: step 1804, loss 0.216639, acc 0.90625, learning_rate 0.000103073
2017-10-10T12:47:10.155843: step 1805, loss 0.197128, acc 0.921875, learning_rate 0.00010306
2017-10-10T12:47:10.720984: step 1806, loss 0.0829406, acc 0.984375, learning_rate 0.000103048
2017-10-10T12:47:11.269069: step 1807, loss 0.276197, acc 0.875, learning_rate 0.000103035
2017-10-10T12:47:11.823085: step 1808, loss 0.175627, acc 0.9375, learning_rate 0.000103023
2017-10-10T12:47:12.391046: step 1809, loss 0.114516, acc 0.96875, learning_rate 0.00010301
2017-10-10T12:47:12.953033: step 1810, loss 0.151798, acc 0.9375, learning_rate 0.000102998
2017-10-10T12:47:13.512886: step 1811, loss 0.120524, acc 0.9375, learning_rate 0.000102986
2017-10-10T12:47:14.085176: step 1812, loss 0.11738, acc 0.984375, learning_rate 0.000102974
2017-10-10T12:47:14.645056: step 1813, loss 0.205598, acc 0.890625, learning_rate 0.000102962
2017-10-10T12:47:15.145813: step 1814, loss 0.185633, acc 0.953125, learning_rate 0.000102949
2017-10-10T12:47:15.720532: step 1815, loss 0.134729, acc 0.96875, learning_rate 0.000102937
2017-10-10T12:47:16.264961: step 1816, loss 0.106664, acc 0.96875, learning_rate 0.000102925
2017-10-10T12:47:16.899128: step 1817, loss 0.184714, acc 0.96875, learning_rate 0.000102913
2017-10-10T12:47:17.328447: step 1818, loss 0.146648, acc 0.96875, learning_rate 0.000102902
2017-10-10T12:47:17.738113: step 1819, loss 0.3915, acc 0.859375, learning_rate 0.00010289
2017-10-10T12:47:18.176877: step 1820, loss 0.124404, acc 0.96875, learning_rate 0.000102878
2017-10-10T12:47:18.691532: step 1821, loss 0.162653, acc 0.921875, learning_rate 0.000102866
2017-10-10T12:47:19.176906: step 1822, loss 0.138323, acc 0.9375, learning_rate 0.000102855
2017-10-10T12:47:19.821106: step 1823, loss 0.109596, acc 0.9375, learning_rate 0.000102843
2017-10-10T12:47:20.317038: step 1824, loss 0.158879, acc 0.921875, learning_rate 0.000102831
2017-10-10T12:47:20.799526: step 1825, loss 0.128819, acc 0.96875, learning_rate 0.00010282
2017-10-10T12:47:21.287564: step 1826, loss 0.301513, acc 0.875, learning_rate 0.000102808
2017-10-10T12:47:21.824880: step 1827, loss 0.125601, acc 0.96875, learning_rate 0.000102797
2017-10-10T12:47:22.335042: step 1828, loss 0.207547, acc 0.890625, learning_rate 0.000102785
2017-10-10T12:47:22.845701: step 1829, loss 0.149122, acc 0.921875, learning_rate 0.000102774
2017-10-10T12:47:23.354216: step 1830, loss 0.102078, acc 0.9375, learning_rate 0.000102763
2017-10-10T12:47:23.959441: step 1831, loss 0.074356, acc 0.96875, learning_rate 0.000102751
2017-10-10T12:47:24.420995: step 1832, loss 0.0921151, acc 0.96875, learning_rate 0.00010274
2017-10-10T12:47:24.789083: step 1833, loss 0.172878, acc 0.921875, learning_rate 0.000102729
2017-10-10T12:47:25.221343: step 1834, loss 0.0729811, acc 0.96875, learning_rate 0.000102718
2017-10-10T12:47:25.698790: step 1835, loss 0.0959445, acc 0.953125, learning_rate 0.000102707
2017-10-10T12:47:26.209159: step 1836, loss 0.180026, acc 0.890625, learning_rate 0.000102696
2017-10-10T12:47:26.753784: step 1837, loss 0.10572, acc 0.96875, learning_rate 0.000102685
2017-10-10T12:47:27.216848: step 1838, loss 0.187062, acc 0.9375, learning_rate 0.000102674
2017-10-10T12:47:27.709133: step 1839, loss 0.237615, acc 0.921875, learning_rate 0.000102663
2017-10-10T12:47:28.290198: step 1840, loss 0.174244, acc 0.921875, learning_rate 0.000102652

Evaluation:
2017-10-10T12:47:29.463165: step 1840, loss 0.233609, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1840

2017-10-10T12:47:30.973044: step 1841, loss 0.268277, acc 0.9375, learning_rate 0.000102641
2017-10-10T12:47:31.540454: step 1842, loss 0.133678, acc 0.984375, learning_rate 0.00010263
2017-10-10T12:47:32.108921: step 1843, loss 0.168775, acc 0.9375, learning_rate 0.00010262
2017-10-10T12:47:32.659995: step 1844, loss 0.144828, acc 0.953125, learning_rate 0.000102609
2017-10-10T12:47:33.208970: step 1845, loss 0.144746, acc 0.9375, learning_rate 0.000102598
2017-10-10T12:47:33.724246: step 1846, loss 0.149956, acc 0.921875, learning_rate 0.000102588
2017-10-10T12:47:34.254100: step 1847, loss 0.149155, acc 0.890625, learning_rate 0.000102577
2017-10-10T12:47:34.804125: step 1848, loss 0.122568, acc 0.953125, learning_rate 0.000102567
2017-10-10T12:47:35.413398: step 1849, loss 0.196262, acc 0.953125, learning_rate 0.000102556
2017-10-10T12:47:35.953020: step 1850, loss 0.110119, acc 0.953125, learning_rate 0.000102546
2017-10-10T12:47:36.508722: step 1851, loss 0.248457, acc 0.921875, learning_rate 0.000102535
2017-10-10T12:47:37.056947: step 1852, loss 0.236815, acc 0.921875, learning_rate 0.000102525
2017-10-10T12:47:37.577984: step 1853, loss 0.158257, acc 0.9375, learning_rate 0.000102515
2017-10-10T12:47:38.059447: step 1854, loss 0.143994, acc 0.9375, learning_rate 0.000102504
2017-10-10T12:47:38.532856: step 1855, loss 0.0849536, acc 0.953125, learning_rate 0.000102494
2017-10-10T12:47:39.073062: step 1856, loss 0.0533756, acc 1, learning_rate 0.000102484
2017-10-10T12:47:39.676776: step 1857, loss 0.171163, acc 0.9375, learning_rate 0.000102474
2017-10-10T12:47:40.156698: step 1858, loss 0.12616, acc 0.921875, learning_rate 0.000102464
2017-10-10T12:47:40.565040: step 1859, loss 0.0913764, acc 0.96875, learning_rate 0.000102454
2017-10-10T12:47:40.925792: step 1860, loss 0.157115, acc 0.921875, learning_rate 0.000102444
2017-10-10T12:47:41.423976: step 1861, loss 0.0527419, acc 1, learning_rate 0.000102434
2017-10-10T12:47:41.946618: step 1862, loss 0.145381, acc 0.960784, learning_rate 0.000102424
2017-10-10T12:47:42.496883: step 1863, loss 0.0589352, acc 1, learning_rate 0.000102414
2017-10-10T12:47:42.955305: step 1864, loss 0.218762, acc 0.9375, learning_rate 0.000102404
2017-10-10T12:47:43.352908: step 1865, loss 0.0419608, acc 1, learning_rate 0.000102394
2017-10-10T12:47:43.852926: step 1866, loss 0.0869542, acc 0.96875, learning_rate 0.000102384
2017-10-10T12:47:44.352840: step 1867, loss 0.185072, acc 0.9375, learning_rate 0.000102375
2017-10-10T12:47:44.878380: step 1868, loss 0.122673, acc 0.953125, learning_rate 0.000102365
2017-10-10T12:47:45.403199: step 1869, loss 0.140762, acc 0.96875, learning_rate 0.000102355
2017-10-10T12:47:45.976865: step 1870, loss 0.0721775, acc 0.984375, learning_rate 0.000102346
2017-10-10T12:47:46.626687: step 1871, loss 0.205871, acc 0.96875, learning_rate 0.000102336
2017-10-10T12:47:47.218494: step 1872, loss 0.149937, acc 0.9375, learning_rate 0.000102327
2017-10-10T12:47:47.652430: step 1873, loss 0.12879, acc 0.96875, learning_rate 0.000102317
2017-10-10T12:47:48.110350: step 1874, loss 0.183924, acc 0.953125, learning_rate 0.000102308
2017-10-10T12:47:48.582181: step 1875, loss 0.132486, acc 0.9375, learning_rate 0.000102298
2017-10-10T12:47:49.087287: step 1876, loss 0.110609, acc 0.96875, learning_rate 0.000102289
2017-10-10T12:47:49.620831: step 1877, loss 0.0313536, acc 1, learning_rate 0.000102279
2017-10-10T12:47:50.300777: step 1878, loss 0.157313, acc 0.953125, learning_rate 0.00010227
2017-10-10T12:47:50.894412: step 1879, loss 0.0925535, acc 0.96875, learning_rate 0.000102261
2017-10-10T12:47:51.460933: step 1880, loss 0.155171, acc 0.921875, learning_rate 0.000102252

Evaluation:
2017-10-10T12:47:52.617029: step 1880, loss 0.235529, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1880

2017-10-10T12:47:54.356919: step 1881, loss 0.116296, acc 0.953125, learning_rate 0.000102242
2017-10-10T12:47:54.864856: step 1882, loss 0.143851, acc 0.921875, learning_rate 0.000102233
2017-10-10T12:47:55.389981: step 1883, loss 0.0922378, acc 0.96875, learning_rate 0.000102224
2017-10-10T12:47:55.924967: step 1884, loss 0.152649, acc 0.9375, learning_rate 0.000102215
2017-10-10T12:47:56.460170: step 1885, loss 0.0345231, acc 0.984375, learning_rate 0.000102206
2017-10-10T12:47:56.963315: step 1886, loss 0.113698, acc 0.9375, learning_rate 0.000102197
2017-10-10T12:47:57.476993: step 1887, loss 0.216709, acc 0.9375, learning_rate 0.000102188
2017-10-10T12:47:58.004889: step 1888, loss 0.274082, acc 0.921875, learning_rate 0.000102179
2017-10-10T12:47:58.488838: step 1889, loss 0.167323, acc 0.953125, learning_rate 0.00010217
2017-10-10T12:47:58.951071: step 1890, loss 0.173339, acc 0.953125, learning_rate 0.000102161
2017-10-10T12:47:59.533073: step 1891, loss 0.0939891, acc 0.953125, learning_rate 0.000102153
2017-10-10T12:48:00.057273: step 1892, loss 0.113027, acc 0.96875, learning_rate 0.000102144
2017-10-10T12:48:00.524930: step 1893, loss 0.163036, acc 0.96875, learning_rate 0.000102135
2017-10-10T12:48:01.154066: step 1894, loss 0.0555644, acc 0.984375, learning_rate 0.000102126
2017-10-10T12:48:01.703172: step 1895, loss 0.0335067, acc 1, learning_rate 0.000102118
2017-10-10T12:48:02.220868: step 1896, loss 0.124612, acc 0.96875, learning_rate 0.000102109
2017-10-10T12:48:02.813110: step 1897, loss 0.117001, acc 0.953125, learning_rate 0.0001021
2017-10-10T12:48:03.266673: step 1898, loss 0.239557, acc 0.9375, learning_rate 0.000102092
2017-10-10T12:48:03.713635: step 1899, loss 0.141227, acc 0.953125, learning_rate 0.000102083
2017-10-10T12:48:04.277926: step 1900, loss 0.18139, acc 0.9375, learning_rate 0.000102075
2017-10-10T12:48:04.931791: step 1901, loss 0.246406, acc 0.90625, learning_rate 0.000102066
2017-10-10T12:48:05.552832: step 1902, loss 0.0785357, acc 0.984375, learning_rate 0.000102058
2017-10-10T12:48:05.967839: step 1903, loss 0.112363, acc 0.96875, learning_rate 0.00010205
2017-10-10T12:48:06.468950: step 1904, loss 0.054118, acc 0.984375, learning_rate 0.000102041
2017-10-10T12:48:07.064954: step 1905, loss 0.149738, acc 0.9375, learning_rate 0.000102033
2017-10-10T12:48:07.656901: step 1906, loss 0.109226, acc 0.984375, learning_rate 0.000102025
2017-10-10T12:48:08.224819: step 1907, loss 0.0755605, acc 0.96875, learning_rate 0.000102016
2017-10-10T12:48:08.818627: step 1908, loss 0.160098, acc 0.9375, learning_rate 0.000102008
2017-10-10T12:48:09.392559: step 1909, loss 0.157887, acc 0.953125, learning_rate 0.000102
2017-10-10T12:48:09.937119: step 1910, loss 0.0812803, acc 0.96875, learning_rate 0.000101992
2017-10-10T12:48:10.401078: step 1911, loss 0.248999, acc 0.90625, learning_rate 0.000101984
2017-10-10T12:48:10.832062: step 1912, loss 0.172379, acc 0.9375, learning_rate 0.000101975
2017-10-10T12:48:11.223187: step 1913, loss 0.280564, acc 0.890625, learning_rate 0.000101967
2017-10-10T12:48:11.781372: step 1914, loss 0.0944121, acc 0.984375, learning_rate 0.000101959
2017-10-10T12:48:12.324653: step 1915, loss 0.167054, acc 0.9375, learning_rate 0.000101951
2017-10-10T12:48:12.918146: step 1916, loss 0.228645, acc 0.9375, learning_rate 0.000101943
2017-10-10T12:48:13.545050: step 1917, loss 0.134985, acc 0.953125, learning_rate 0.000101935
2017-10-10T12:48:14.056893: step 1918, loss 0.127028, acc 0.9375, learning_rate 0.000101928
2017-10-10T12:48:14.649247: step 1919, loss 0.102386, acc 0.96875, learning_rate 0.00010192
2017-10-10T12:48:15.181083: step 1920, loss 0.253055, acc 0.890625, learning_rate 0.000101912

Evaluation:
2017-10-10T12:48:16.368931: step 1920, loss 0.234893, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1920

2017-10-10T12:48:17.631941: step 1921, loss 0.0772645, acc 0.984375, learning_rate 0.000101904
2017-10-10T12:48:18.088594: step 1922, loss 0.22536, acc 0.921875, learning_rate 0.000101896
2017-10-10T12:48:18.602562: step 1923, loss 0.241233, acc 0.90625, learning_rate 0.000101889
2017-10-10T12:48:19.160933: step 1924, loss 0.22181, acc 0.921875, learning_rate 0.000101881
2017-10-10T12:48:19.711077: step 1925, loss 0.114246, acc 0.96875, learning_rate 0.000101873
2017-10-10T12:48:20.216372: step 1926, loss 0.11917, acc 0.9375, learning_rate 0.000101865
2017-10-10T12:48:20.780887: step 1927, loss 0.13205, acc 0.9375, learning_rate 0.000101858
2017-10-10T12:48:21.313080: step 1928, loss 0.146325, acc 0.953125, learning_rate 0.00010185
2017-10-10T12:48:21.812855: step 1929, loss 0.0772542, acc 0.96875, learning_rate 0.000101843
2017-10-10T12:48:22.312890: step 1930, loss 0.106733, acc 0.96875, learning_rate 0.000101835
2017-10-10T12:48:22.828921: step 1931, loss 0.0807552, acc 0.96875, learning_rate 0.000101828
2017-10-10T12:48:23.358046: step 1932, loss 0.122177, acc 0.953125, learning_rate 0.00010182
2017-10-10T12:48:23.940836: step 1933, loss 0.173862, acc 0.921875, learning_rate 0.000101813
2017-10-10T12:48:24.490460: step 1934, loss 0.253854, acc 0.90625, learning_rate 0.000101805
2017-10-10T12:48:25.028344: step 1935, loss 0.203881, acc 0.921875, learning_rate 0.000101798
2017-10-10T12:48:25.608907: step 1936, loss 0.0882719, acc 0.96875, learning_rate 0.000101791
2017-10-10T12:48:26.176848: step 1937, loss 0.120274, acc 0.953125, learning_rate 0.000101783
2017-10-10T12:48:26.658114: step 1938, loss 0.108144, acc 0.9375, learning_rate 0.000101776
2017-10-10T12:48:27.121608: step 1939, loss 0.103482, acc 0.921875, learning_rate 0.000101769
2017-10-10T12:48:27.735786: step 1940, loss 0.175572, acc 0.953125, learning_rate 0.000101762
2017-10-10T12:48:28.210191: step 1941, loss 0.0324007, acc 1, learning_rate 0.000101754
2017-10-10T12:48:28.668961: step 1942, loss 0.0945366, acc 0.953125, learning_rate 0.000101747
2017-10-10T12:48:29.132673: step 1943, loss 0.179624, acc 0.953125, learning_rate 0.00010174
2017-10-10T12:48:29.705052: step 1944, loss 0.120544, acc 0.984375, learning_rate 0.000101733
2017-10-10T12:48:30.239753: step 1945, loss 0.217219, acc 0.953125, learning_rate 0.000101726
2017-10-10T12:48:30.820920: step 1946, loss 0.178556, acc 0.9375, learning_rate 0.000101719
2017-10-10T12:48:31.384719: step 1947, loss 0.199114, acc 0.9375, learning_rate 0.000101712
2017-10-10T12:48:31.808925: step 1948, loss 0.174201, acc 0.953125, learning_rate 0.000101705
2017-10-10T12:48:32.357447: step 1949, loss 0.141972, acc 0.9375, learning_rate 0.000101698
2017-10-10T12:48:32.948868: step 1950, loss 0.114675, acc 0.953125, learning_rate 0.000101691
2017-10-10T12:48:33.532899: step 1951, loss 0.0879356, acc 0.953125, learning_rate 0.000101684
2017-10-10T12:48:33.970724: step 1952, loss 0.195234, acc 0.90625, learning_rate 0.000101677
2017-10-10T12:48:34.364668: step 1953, loss 0.126732, acc 0.9375, learning_rate 0.00010167
2017-10-10T12:48:34.810114: step 1954, loss 0.168692, acc 0.9375, learning_rate 0.000101664
2017-10-10T12:48:35.291586: step 1955, loss 0.128967, acc 0.953125, learning_rate 0.000101657
2017-10-10T12:48:35.856057: step 1956, loss 0.0676211, acc 0.96875, learning_rate 0.00010165
2017-10-10T12:48:36.369952: step 1957, loss 0.110439, acc 0.953125, learning_rate 0.000101643
2017-10-10T12:48:36.890080: step 1958, loss 0.0907773, acc 0.96875, learning_rate 0.000101637
2017-10-10T12:48:37.392909: step 1959, loss 0.274521, acc 0.921875, learning_rate 0.00010163
2017-10-10T12:48:37.847870: step 1960, loss 0.0547311, acc 1, learning_rate 0.000101623

Evaluation:
2017-10-10T12:48:39.060150: step 1960, loss 0.232324, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-1960

2017-10-10T12:48:40.764841: step 1961, loss 0.0771989, acc 0.96875, learning_rate 0.000101617
2017-10-10T12:48:41.349327: step 1962, loss 0.0640481, acc 0.984375, learning_rate 0.00010161
2017-10-10T12:48:41.928958: step 1963, loss 0.140129, acc 0.953125, learning_rate 0.000101604
2017-10-10T12:48:42.437286: step 1964, loss 0.194311, acc 0.953125, learning_rate 0.000101597
2017-10-10T12:48:42.973309: step 1965, loss 0.0812235, acc 0.984375, learning_rate 0.00010159
2017-10-10T12:48:43.524053: step 1966, loss 0.184152, acc 0.9375, learning_rate 0.000101584
2017-10-10T12:48:44.076850: step 1967, loss 0.187317, acc 0.953125, learning_rate 0.000101577
2017-10-10T12:48:44.624870: step 1968, loss 0.151385, acc 0.9375, learning_rate 0.000101571
2017-10-10T12:48:45.234994: step 1969, loss 0.163848, acc 0.9375, learning_rate 0.000101565
2017-10-10T12:48:45.792940: step 1970, loss 0.0960047, acc 0.96875, learning_rate 0.000101558
2017-10-10T12:48:46.305197: step 1971, loss 0.159946, acc 0.921875, learning_rate 0.000101552
2017-10-10T12:48:46.849034: step 1972, loss 0.145638, acc 0.96875, learning_rate 0.000101546
2017-10-10T12:48:47.349310: step 1973, loss 0.247217, acc 0.921875, learning_rate 0.000101539
2017-10-10T12:48:47.902496: step 1974, loss 0.154438, acc 0.90625, learning_rate 0.000101533
2017-10-10T12:48:48.529850: step 1975, loss 0.222636, acc 0.921875, learning_rate 0.000101527
2017-10-10T12:48:48.986928: step 1976, loss 0.142394, acc 0.9375, learning_rate 0.00010152
2017-10-10T12:48:49.418946: step 1977, loss 0.12246, acc 0.9375, learning_rate 0.000101514
2017-10-10T12:48:49.870655: step 1978, loss 0.060468, acc 0.984375, learning_rate 0.000101508
2017-10-10T12:48:50.439651: step 1979, loss 0.168286, acc 0.9375, learning_rate 0.000101502
2017-10-10T12:48:50.998717: step 1980, loss 0.105861, acc 0.953125, learning_rate 0.000101496
2017-10-10T12:48:51.380925: step 1981, loss 0.0985782, acc 0.96875, learning_rate 0.00010149
2017-10-10T12:48:51.812865: step 1982, loss 0.148331, acc 0.953125, learning_rate 0.000101484
2017-10-10T12:48:52.200870: step 1983, loss 0.225361, acc 0.9375, learning_rate 0.000101478
2017-10-10T12:48:52.692768: step 1984, loss 0.125173, acc 0.9375, learning_rate 0.000101472
2017-10-10T12:48:53.296857: step 1985, loss 0.127277, acc 0.96875, learning_rate 0.000101466
2017-10-10T12:48:53.761014: step 1986, loss 0.203842, acc 0.921875, learning_rate 0.00010146
2017-10-10T12:48:54.220560: step 1987, loss 0.0926379, acc 0.953125, learning_rate 0.000101454
2017-10-10T12:48:54.705857: step 1988, loss 0.285581, acc 0.9375, learning_rate 0.000101448
2017-10-10T12:48:55.161045: step 1989, loss 0.132091, acc 0.953125, learning_rate 0.000101442
2017-10-10T12:48:55.680840: step 1990, loss 0.0653532, acc 0.984375, learning_rate 0.000101436
2017-10-10T12:48:56.336433: step 1991, loss 0.114839, acc 0.953125, learning_rate 0.00010143
2017-10-10T12:48:56.912839: step 1992, loss 0.0778554, acc 0.96875, learning_rate 0.000101424
2017-10-10T12:48:57.382230: step 1993, loss 0.17128, acc 0.9375, learning_rate 0.000101418
2017-10-10T12:48:57.772872: step 1994, loss 0.307065, acc 0.90625, learning_rate 0.000101413
2017-10-10T12:48:58.321081: step 1995, loss 0.0651178, acc 0.96875, learning_rate 0.000101407
2017-10-10T12:48:58.900856: step 1996, loss 0.0574647, acc 0.984375, learning_rate 0.000101401
2017-10-10T12:48:59.453125: step 1997, loss 0.153563, acc 0.9375, learning_rate 0.000101395
2017-10-10T12:49:00.017480: step 1998, loss 0.15851, acc 0.953125, learning_rate 0.00010139
2017-10-10T12:49:00.526763: step 1999, loss 0.120901, acc 0.953125, learning_rate 0.000101384
2017-10-10T12:49:01.076673: step 2000, loss 0.228225, acc 0.921875, learning_rate 0.000101378

Evaluation:
2017-10-10T12:49:02.188917: step 2000, loss 0.23291, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2000

2017-10-10T12:49:03.790565: step 2001, loss 0.117313, acc 0.984375, learning_rate 0.000101373
2017-10-10T12:49:04.344892: step 2002, loss 0.0739096, acc 0.984375, learning_rate 0.000101367
2017-10-10T12:49:04.815779: step 2003, loss 0.23548, acc 0.921875, learning_rate 0.000101362
2017-10-10T12:49:05.328971: step 2004, loss 0.19684, acc 0.921875, learning_rate 0.000101356
2017-10-10T12:49:05.868890: step 2005, loss 0.159372, acc 0.953125, learning_rate 0.00010135
2017-10-10T12:49:06.433164: step 2006, loss 0.145664, acc 0.921875, learning_rate 0.000101345
2017-10-10T12:49:06.965508: step 2007, loss 0.137296, acc 0.9375, learning_rate 0.000101339
2017-10-10T12:49:07.457284: step 2008, loss 0.125431, acc 0.9375, learning_rate 0.000101334
2017-10-10T12:49:08.033059: step 2009, loss 0.110887, acc 0.96875, learning_rate 0.000101328
2017-10-10T12:49:08.597007: step 2010, loss 0.292201, acc 0.90625, learning_rate 0.000101323
2017-10-10T12:49:09.085391: step 2011, loss 0.188532, acc 0.921875, learning_rate 0.000101318
2017-10-10T12:49:09.564943: step 2012, loss 0.226338, acc 0.921875, learning_rate 0.000101312
2017-10-10T12:49:10.133577: step 2013, loss 0.229884, acc 0.90625, learning_rate 0.000101307
2017-10-10T12:49:10.660357: step 2014, loss 0.0962144, acc 0.953125, learning_rate 0.000101302
2017-10-10T12:49:11.269242: step 2015, loss 0.139204, acc 0.9375, learning_rate 0.000101296
2017-10-10T12:49:11.680995: step 2016, loss 0.138935, acc 0.953125, learning_rate 0.000101291
2017-10-10T12:49:12.109066: step 2017, loss 0.175819, acc 0.953125, learning_rate 0.000101286
2017-10-10T12:49:12.571095: step 2018, loss 0.149111, acc 0.96875, learning_rate 0.00010128
2017-10-10T12:49:13.098958: step 2019, loss 0.0346979, acc 0.984375, learning_rate 0.000101275
2017-10-10T12:49:13.716226: step 2020, loss 0.0899746, acc 0.96875, learning_rate 0.00010127
2017-10-10T12:49:14.261284: step 2021, loss 0.200161, acc 0.921875, learning_rate 0.000101265
2017-10-10T12:49:14.711563: step 2022, loss 0.208403, acc 0.921875, learning_rate 0.00010126
2017-10-10T12:49:15.198396: step 2023, loss 0.162484, acc 0.953125, learning_rate 0.000101255
2017-10-10T12:49:15.676910: step 2024, loss 0.107229, acc 0.984375, learning_rate 0.000101249
2017-10-10T12:49:16.220720: step 2025, loss 0.104478, acc 0.953125, learning_rate 0.000101244
2017-10-10T12:49:16.745049: step 2026, loss 0.0842545, acc 0.96875, learning_rate 0.000101239
2017-10-10T12:49:17.348978: step 2027, loss 0.166442, acc 0.921875, learning_rate 0.000101234
2017-10-10T12:49:17.915911: step 2028, loss 0.237315, acc 0.953125, learning_rate 0.000101229
2017-10-10T12:49:18.497544: step 2029, loss 0.234556, acc 0.921875, learning_rate 0.000101224
2017-10-10T12:49:19.004895: step 2030, loss 0.100635, acc 0.953125, learning_rate 0.000101219
2017-10-10T12:49:19.568075: step 2031, loss 0.181274, acc 0.9375, learning_rate 0.000101214
2017-10-10T12:49:20.001247: step 2032, loss 0.0746981, acc 0.953125, learning_rate 0.000101209
2017-10-10T12:49:20.344826: step 2033, loss 0.0744847, acc 0.984375, learning_rate 0.000101204
2017-10-10T12:49:20.844995: step 2034, loss 0.065324, acc 0.984375, learning_rate 0.000101199
2017-10-10T12:49:21.395881: step 2035, loss 0.0602935, acc 0.984375, learning_rate 0.000101194
2017-10-10T12:49:21.949955: step 2036, loss 0.128557, acc 0.953125, learning_rate 0.00010119
2017-10-10T12:49:22.539304: step 2037, loss 0.0866613, acc 0.96875, learning_rate 0.000101185
2017-10-10T12:49:23.047963: step 2038, loss 0.146034, acc 0.921875, learning_rate 0.00010118
2017-10-10T12:49:23.547050: step 2039, loss 0.0852138, acc 0.984375, learning_rate 0.000101175
2017-10-10T12:49:24.084924: step 2040, loss 0.15665, acc 0.953125, learning_rate 0.00010117

Evaluation:
2017-10-10T12:49:25.286851: step 2040, loss 0.232374, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2040

2017-10-10T12:49:27.197072: step 2041, loss 0.0818364, acc 0.984375, learning_rate 0.000101166
2017-10-10T12:49:27.708866: step 2042, loss 0.114312, acc 0.953125, learning_rate 0.000101161
2017-10-10T12:49:28.236950: step 2043, loss 0.214709, acc 0.921875, learning_rate 0.000101156
2017-10-10T12:49:28.768912: step 2044, loss 0.117969, acc 0.984375, learning_rate 0.000101151
2017-10-10T12:49:29.324973: step 2045, loss 0.180429, acc 0.953125, learning_rate 0.000101147
2017-10-10T12:49:29.932823: step 2046, loss 0.156632, acc 0.921875, learning_rate 0.000101142
2017-10-10T12:49:30.533055: step 2047, loss 0.154173, acc 0.96875, learning_rate 0.000101137
2017-10-10T12:49:31.096180: step 2048, loss 0.0436223, acc 1, learning_rate 0.000101133
2017-10-10T12:49:31.582894: step 2049, loss 0.190512, acc 0.9375, learning_rate 0.000101128
2017-10-10T12:49:32.140863: step 2050, loss 0.186633, acc 0.953125, learning_rate 0.000101123
2017-10-10T12:49:32.605042: step 2051, loss 0.146559, acc 0.90625, learning_rate 0.000101119
2017-10-10T12:49:33.160947: step 2052, loss 0.124952, acc 0.984375, learning_rate 0.000101114
2017-10-10T12:49:33.744921: step 2053, loss 0.0692552, acc 0.984375, learning_rate 0.00010111
2017-10-10T12:49:34.293012: step 2054, loss 0.113789, acc 0.9375, learning_rate 0.000101105
2017-10-10T12:49:34.685011: step 2055, loss 0.300042, acc 0.875, learning_rate 0.000101101
2017-10-10T12:49:35.098296: step 2056, loss 0.16486, acc 0.921875, learning_rate 0.000101096
2017-10-10T12:49:35.502799: step 2057, loss 0.105468, acc 0.953125, learning_rate 0.000101092
2017-10-10T12:49:35.977290: step 2058, loss 0.213365, acc 0.882353, learning_rate 0.000101087
2017-10-10T12:49:36.516914: step 2059, loss 0.0591433, acc 0.984375, learning_rate 0.000101083
2017-10-10T12:49:37.100844: step 2060, loss 0.0359261, acc 1, learning_rate 0.000101078
2017-10-10T12:49:37.511268: step 2061, loss 0.11377, acc 0.953125, learning_rate 0.000101074
2017-10-10T12:49:37.936935: step 2062, loss 0.179784, acc 0.96875, learning_rate 0.00010107
2017-10-10T12:49:38.405052: step 2063, loss 0.260087, acc 0.875, learning_rate 0.000101065
2017-10-10T12:49:38.900915: step 2064, loss 0.0592454, acc 1, learning_rate 0.000101061
2017-10-10T12:49:39.381359: step 2065, loss 0.11529, acc 0.96875, learning_rate 0.000101057
2017-10-10T12:49:39.867304: step 2066, loss 0.151263, acc 0.921875, learning_rate 0.000101052
2017-10-10T12:49:40.362211: step 2067, loss 0.20347, acc 0.90625, learning_rate 0.000101048
2017-10-10T12:49:40.901071: step 2068, loss 0.0557279, acc 0.984375, learning_rate 0.000101044
2017-10-10T12:49:41.441013: step 2069, loss 0.0922799, acc 0.96875, learning_rate 0.000101039
2017-10-10T12:49:42.105572: step 2070, loss 0.126261, acc 0.9375, learning_rate 0.000101035
2017-10-10T12:49:42.575494: step 2071, loss 0.140478, acc 0.9375, learning_rate 0.000101031
2017-10-10T12:49:43.003913: step 2072, loss 0.212443, acc 0.921875, learning_rate 0.000101027
2017-10-10T12:49:43.436750: step 2073, loss 0.132858, acc 0.953125, learning_rate 0.000101023
2017-10-10T12:49:43.976505: step 2074, loss 0.397758, acc 0.890625, learning_rate 0.000101018
2017-10-10T12:49:44.524050: step 2075, loss 0.118013, acc 0.9375, learning_rate 0.000101014
2017-10-10T12:49:45.096869: step 2076, loss 0.201546, acc 0.9375, learning_rate 0.00010101
2017-10-10T12:49:45.642851: step 2077, loss 0.0689386, acc 0.984375, learning_rate 0.000101006
2017-10-10T12:49:46.196926: step 2078, loss 0.399405, acc 0.890625, learning_rate 0.000101002
2017-10-10T12:49:46.773164: step 2079, loss 0.120793, acc 0.953125, learning_rate 0.000100998
2017-10-10T12:49:47.296131: step 2080, loss 0.162962, acc 0.921875, learning_rate 0.000100994

Evaluation:
2017-10-10T12:49:48.390987: step 2080, loss 0.232354, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2080

2017-10-10T12:49:49.865074: step 2081, loss 0.108319, acc 0.953125, learning_rate 0.00010099
2017-10-10T12:49:50.341295: step 2082, loss 0.175373, acc 0.90625, learning_rate 0.000100986
2017-10-10T12:49:50.855033: step 2083, loss 0.145638, acc 0.921875, learning_rate 0.000100982
2017-10-10T12:49:51.432935: step 2084, loss 0.0570263, acc 0.984375, learning_rate 0.000100978
2017-10-10T12:49:51.921197: step 2085, loss 0.220227, acc 0.953125, learning_rate 0.000100974
2017-10-10T12:49:52.421012: step 2086, loss 0.100319, acc 0.96875, learning_rate 0.00010097
2017-10-10T12:49:52.914099: step 2087, loss 0.218274, acc 0.9375, learning_rate 0.000100966
2017-10-10T12:49:53.418338: step 2088, loss 0.15486, acc 0.984375, learning_rate 0.000100962
2017-10-10T12:49:53.973570: step 2089, loss 0.17786, acc 0.90625, learning_rate 0.000100958
2017-10-10T12:49:54.509090: step 2090, loss 0.12277, acc 0.953125, learning_rate 0.000100954
2017-10-10T12:49:55.005185: step 2091, loss 0.240967, acc 0.890625, learning_rate 0.00010095
2017-10-10T12:49:55.506016: step 2092, loss 0.106232, acc 0.96875, learning_rate 0.000100946
2017-10-10T12:49:55.949329: step 2093, loss 0.0926814, acc 0.984375, learning_rate 0.000100942
2017-10-10T12:49:56.377752: step 2094, loss 0.147152, acc 0.9375, learning_rate 0.000100938
2017-10-10T12:49:56.868452: step 2095, loss 0.114212, acc 0.96875, learning_rate 0.000100935
2017-10-10T12:49:57.461696: step 2096, loss 0.0573748, acc 0.984375, learning_rate 0.000100931
2017-10-10T12:49:57.878181: step 2097, loss 0.0785619, acc 0.984375, learning_rate 0.000100927
2017-10-10T12:49:58.356845: step 2098, loss 0.102544, acc 0.96875, learning_rate 0.000100923
2017-10-10T12:49:58.795947: step 2099, loss 0.161762, acc 0.921875, learning_rate 0.000100919
2017-10-10T12:49:59.383344: step 2100, loss 0.115975, acc 0.953125, learning_rate 0.000100916
2017-10-10T12:50:00.009092: step 2101, loss 0.130791, acc 0.96875, learning_rate 0.000100912
2017-10-10T12:50:00.589147: step 2102, loss 0.0560726, acc 0.984375, learning_rate 0.000100908
2017-10-10T12:50:01.001065: step 2103, loss 0.0243847, acc 1, learning_rate 0.000100904
2017-10-10T12:50:01.411076: step 2104, loss 0.20991, acc 0.953125, learning_rate 0.000100901
2017-10-10T12:50:01.830041: step 2105, loss 0.223071, acc 0.9375, learning_rate 0.000100897
2017-10-10T12:50:02.344878: step 2106, loss 0.157119, acc 0.96875, learning_rate 0.000100893
2017-10-10T12:50:02.867701: step 2107, loss 0.0835698, acc 0.96875, learning_rate 0.00010089
2017-10-10T12:50:03.416840: step 2108, loss 0.0890327, acc 0.984375, learning_rate 0.000100886
2017-10-10T12:50:03.950994: step 2109, loss 0.0584067, acc 1, learning_rate 0.000100883
2017-10-10T12:50:04.519326: step 2110, loss 0.110821, acc 0.953125, learning_rate 0.000100879
2017-10-10T12:50:05.036019: step 2111, loss 0.204356, acc 0.9375, learning_rate 0.000100875
2017-10-10T12:50:05.624913: step 2112, loss 0.200262, acc 0.921875, learning_rate 0.000100872
2017-10-10T12:50:06.200906: step 2113, loss 0.0830004, acc 0.96875, learning_rate 0.000100868
2017-10-10T12:50:06.630194: step 2114, loss 0.151839, acc 0.9375, learning_rate 0.000100865
2017-10-10T12:50:07.093069: step 2115, loss 0.131527, acc 0.9375, learning_rate 0.000100861
2017-10-10T12:50:07.576234: step 2116, loss 0.123972, acc 0.984375, learning_rate 0.000100858
2017-10-10T12:50:08.112953: step 2117, loss 0.185853, acc 0.90625, learning_rate 0.000100854
2017-10-10T12:50:08.696874: step 2118, loss 0.159933, acc 0.96875, learning_rate 0.000100851
2017-10-10T12:50:09.235994: step 2119, loss 0.175498, acc 0.953125, learning_rate 0.000100847
2017-10-10T12:50:09.755170: step 2120, loss 0.161216, acc 0.9375, learning_rate 0.000100844

Evaluation:
2017-10-10T12:50:10.982221: step 2120, loss 0.231813, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2120

2017-10-10T12:50:12.636443: step 2121, loss 0.109005, acc 0.984375, learning_rate 0.00010084
2017-10-10T12:50:13.149082: step 2122, loss 0.226899, acc 0.90625, learning_rate 0.000100837
2017-10-10T12:50:13.611288: step 2123, loss 0.0699128, acc 1, learning_rate 0.000100833
2017-10-10T12:50:14.132844: step 2124, loss 0.0930299, acc 0.96875, learning_rate 0.00010083
2017-10-10T12:50:14.716862: step 2125, loss 0.0381408, acc 1, learning_rate 0.000100827
2017-10-10T12:50:15.234373: step 2126, loss 0.21176, acc 0.9375, learning_rate 0.000100823
2017-10-10T12:50:15.816005: step 2127, loss 0.0830935, acc 0.984375, learning_rate 0.00010082
2017-10-10T12:50:16.262056: step 2128, loss 0.10874, acc 0.953125, learning_rate 0.000100817
2017-10-10T12:50:16.760496: step 2129, loss 0.108129, acc 0.953125, learning_rate 0.000100813
2017-10-10T12:50:17.246428: step 2130, loss 0.0875001, acc 0.96875, learning_rate 0.00010081
2017-10-10T12:50:17.768819: step 2131, loss 0.12087, acc 0.953125, learning_rate 0.000100807
2017-10-10T12:50:18.280072: step 2132, loss 0.143606, acc 0.953125, learning_rate 0.000100803
2017-10-10T12:50:18.788167: step 2133, loss 0.0984254, acc 0.953125, learning_rate 0.0001008
2017-10-10T12:50:19.302088: step 2134, loss 0.121291, acc 0.953125, learning_rate 0.000100797
2017-10-10T12:50:19.814096: step 2135, loss 0.0729813, acc 0.953125, learning_rate 0.000100793
2017-10-10T12:50:20.400243: step 2136, loss 0.111576, acc 0.9375, learning_rate 0.00010079
2017-10-10T12:50:20.980832: step 2137, loss 0.149424, acc 0.953125, learning_rate 0.000100787
2017-10-10T12:50:21.436010: step 2138, loss 0.101714, acc 0.984375, learning_rate 0.000100784
2017-10-10T12:50:21.937603: step 2139, loss 0.220392, acc 0.90625, learning_rate 0.000100781
2017-10-10T12:50:22.564886: step 2140, loss 0.17606, acc 0.921875, learning_rate 0.000100777
2017-10-10T12:50:23.177015: step 2141, loss 0.0935287, acc 0.96875, learning_rate 0.000100774
2017-10-10T12:50:23.728918: step 2142, loss 0.123343, acc 0.96875, learning_rate 0.000100771
2017-10-10T12:50:24.193093: step 2143, loss 0.215192, acc 0.953125, learning_rate 0.000100768
2017-10-10T12:50:24.649134: step 2144, loss 0.0868247, acc 0.96875, learning_rate 0.000100765
2017-10-10T12:50:25.137023: step 2145, loss 0.105409, acc 0.96875, learning_rate 0.000100762
2017-10-10T12:50:25.587107: step 2146, loss 0.164183, acc 0.96875, learning_rate 0.000100759
2017-10-10T12:50:26.167386: step 2147, loss 0.127509, acc 0.953125, learning_rate 0.000100755
2017-10-10T12:50:26.658797: step 2148, loss 0.117611, acc 0.96875, learning_rate 0.000100752
2017-10-10T12:50:27.168085: step 2149, loss 0.202573, acc 0.9375, learning_rate 0.000100749
2017-10-10T12:50:27.665232: step 2150, loss 0.18942, acc 0.90625, learning_rate 0.000100746
2017-10-10T12:50:28.226690: step 2151, loss 0.183506, acc 0.90625, learning_rate 0.000100743
2017-10-10T12:50:28.744882: step 2152, loss 0.25399, acc 0.921875, learning_rate 0.00010074
2017-10-10T12:50:29.340868: step 2153, loss 0.140553, acc 0.96875, learning_rate 0.000100737
2017-10-10T12:50:29.871656: step 2154, loss 0.248854, acc 0.921875, learning_rate 0.000100734
2017-10-10T12:50:30.320902: step 2155, loss 0.106868, acc 0.96875, learning_rate 0.000100731
2017-10-10T12:50:30.772929: step 2156, loss 0.1765, acc 0.901961, learning_rate 0.000100728
2017-10-10T12:50:31.333006: step 2157, loss 0.129981, acc 0.953125, learning_rate 0.000100725
2017-10-10T12:50:31.863159: step 2158, loss 0.0849225, acc 0.953125, learning_rate 0.000100722
2017-10-10T12:50:32.408929: step 2159, loss 0.085767, acc 0.96875, learning_rate 0.000100719
2017-10-10T12:50:32.908848: step 2160, loss 0.137573, acc 0.9375, learning_rate 0.000100716

Evaluation:
2017-10-10T12:50:34.152965: step 2160, loss 0.23004, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2160

2017-10-10T12:50:35.845197: step 2161, loss 0.291571, acc 0.9375, learning_rate 0.000100713
2017-10-10T12:50:36.358083: step 2162, loss 0.11361, acc 0.953125, learning_rate 0.000100711
2017-10-10T12:50:36.896482: step 2163, loss 0.0539643, acc 0.984375, learning_rate 0.000100708
2017-10-10T12:50:37.376889: step 2164, loss 0.121251, acc 0.96875, learning_rate 0.000100705
2017-10-10T12:50:37.916895: step 2165, loss 0.144215, acc 0.9375, learning_rate 0.000100702
2017-10-10T12:50:38.432998: step 2166, loss 0.178637, acc 0.9375, learning_rate 0.000100699
2017-10-10T12:50:38.975138: step 2167, loss 0.116056, acc 0.96875, learning_rate 0.000100696
2017-10-10T12:50:39.476843: step 2168, loss 0.166654, acc 0.953125, learning_rate 0.000100693
2017-10-10T12:50:40.016870: step 2169, loss 0.101167, acc 0.96875, learning_rate 0.00010069
2017-10-10T12:50:40.536761: step 2170, loss 0.142853, acc 0.96875, learning_rate 0.000100688
2017-10-10T12:50:41.048070: step 2171, loss 0.155096, acc 0.921875, learning_rate 0.000100685
2017-10-10T12:50:41.536439: step 2172, loss 0.165687, acc 0.9375, learning_rate 0.000100682
2017-10-10T12:50:42.118154: step 2173, loss 0.249791, acc 0.921875, learning_rate 0.000100679
2017-10-10T12:50:42.600911: step 2174, loss 0.10119, acc 0.96875, learning_rate 0.000100677
2017-10-10T12:50:43.185342: step 2175, loss 0.209206, acc 0.9375, learning_rate 0.000100674
2017-10-10T12:50:43.668455: step 2176, loss 0.0901477, acc 0.953125, learning_rate 0.000100671
2017-10-10T12:50:44.068657: step 2177, loss 0.0537007, acc 0.984375, learning_rate 0.000100668
2017-10-10T12:50:44.436390: step 2178, loss 0.112514, acc 0.953125, learning_rate 0.000100666
2017-10-10T12:50:44.920985: step 2179, loss 0.182107, acc 0.9375, learning_rate 0.000100663
2017-10-10T12:50:45.493039: step 2180, loss 0.150118, acc 0.953125, learning_rate 0.00010066
2017-10-10T12:50:46.086865: step 2181, loss 0.403852, acc 0.84375, learning_rate 0.000100657
2017-10-10T12:50:46.548868: step 2182, loss 0.223868, acc 0.921875, learning_rate 0.000100655
2017-10-10T12:50:47.039317: step 2183, loss 0.219498, acc 0.890625, learning_rate 0.000100652
2017-10-10T12:50:47.597077: step 2184, loss 0.163743, acc 0.953125, learning_rate 0.000100649
2017-10-10T12:50:48.146325: step 2185, loss 0.0792912, acc 0.984375, learning_rate 0.000100647
2017-10-10T12:50:48.672936: step 2186, loss 0.120881, acc 0.96875, learning_rate 0.000100644
2017-10-10T12:50:49.213232: step 2187, loss 0.148771, acc 0.9375, learning_rate 0.000100641
2017-10-10T12:50:49.711915: step 2188, loss 0.207281, acc 0.921875, learning_rate 0.000100639
2017-10-10T12:50:50.252860: step 2189, loss 0.0938905, acc 0.96875, learning_rate 0.000100636
2017-10-10T12:50:50.805044: step 2190, loss 0.0622965, acc 0.984375, learning_rate 0.000100634
2017-10-10T12:50:51.357810: step 2191, loss 0.130262, acc 0.9375, learning_rate 0.000100631
2017-10-10T12:50:51.901789: step 2192, loss 0.168688, acc 0.96875, learning_rate 0.000100628
2017-10-10T12:50:52.525213: step 2193, loss 0.172465, acc 0.921875, learning_rate 0.000100626
2017-10-10T12:50:52.983666: step 2194, loss 0.042133, acc 0.984375, learning_rate 0.000100623
2017-10-10T12:50:53.438651: step 2195, loss 0.0696513, acc 0.96875, learning_rate 0.000100621
2017-10-10T12:50:53.980868: step 2196, loss 0.120262, acc 0.953125, learning_rate 0.000100618
2017-10-10T12:50:54.557854: step 2197, loss 0.0906872, acc 0.953125, learning_rate 0.000100616
2017-10-10T12:50:55.106431: step 2198, loss 0.121963, acc 0.9375, learning_rate 0.000100613
2017-10-10T12:50:55.638880: step 2199, loss 0.0714015, acc 0.984375, learning_rate 0.000100611
2017-10-10T12:50:56.165975: step 2200, loss 0.222916, acc 0.921875, learning_rate 0.000100608

Evaluation:
2017-10-10T12:50:57.358302: step 2200, loss 0.229683, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2200

2017-10-10T12:50:58.854403: step 2201, loss 0.0647868, acc 1, learning_rate 0.000100606
2017-10-10T12:50:59.384667: step 2202, loss 0.221783, acc 0.953125, learning_rate 0.000100603
2017-10-10T12:50:59.851946: step 2203, loss 0.0677854, acc 0.96875, learning_rate 0.000100601
2017-10-10T12:51:00.369768: step 2204, loss 0.129294, acc 0.96875, learning_rate 0.000100598
2017-10-10T12:51:00.838308: step 2205, loss 0.0757576, acc 0.984375, learning_rate 0.000100596
2017-10-10T12:51:01.340916: step 2206, loss 0.0921912, acc 0.953125, learning_rate 0.000100594
2017-10-10T12:51:01.809166: step 2207, loss 0.157332, acc 0.953125, learning_rate 0.000100591
2017-10-10T12:51:02.299638: step 2208, loss 0.0640434, acc 0.984375, learning_rate 0.000100589
2017-10-10T12:51:02.858587: step 2209, loss 0.156593, acc 0.953125, learning_rate 0.000100586
2017-10-10T12:51:03.424875: step 2210, loss 0.0509235, acc 1, learning_rate 0.000100584
2017-10-10T12:51:04.048876: step 2211, loss 0.0802475, acc 0.984375, learning_rate 0.000100581
2017-10-10T12:51:04.578345: step 2212, loss 0.0674045, acc 1, learning_rate 0.000100579
2017-10-10T12:51:05.115934: step 2213, loss 0.142272, acc 0.921875, learning_rate 0.000100577
2017-10-10T12:51:05.748403: step 2214, loss 0.0738814, acc 0.96875, learning_rate 0.000100574
2017-10-10T12:51:06.324837: step 2215, loss 0.163874, acc 0.953125, learning_rate 0.000100572
2017-10-10T12:51:06.772998: step 2216, loss 0.225171, acc 0.953125, learning_rate 0.00010057
2017-10-10T12:51:07.322771: step 2217, loss 0.10333, acc 0.96875, learning_rate 0.000100567
2017-10-10T12:51:07.796808: step 2218, loss 0.212462, acc 0.96875, learning_rate 0.000100565
2017-10-10T12:51:08.380820: step 2219, loss 0.143865, acc 0.96875, learning_rate 0.000100563
2017-10-10T12:51:09.001362: step 2220, loss 0.046297, acc 1, learning_rate 0.00010056
2017-10-10T12:51:09.456858: step 2221, loss 0.162575, acc 0.9375, learning_rate 0.000100558
2017-10-10T12:51:09.944986: step 2222, loss 0.191052, acc 0.9375, learning_rate 0.000100556
2017-10-10T12:51:10.421008: step 2223, loss 0.313496, acc 0.90625, learning_rate 0.000100554
2017-10-10T12:51:10.997784: step 2224, loss 0.157775, acc 0.9375, learning_rate 0.000100551
2017-10-10T12:51:11.421206: step 2225, loss 0.0558851, acc 0.984375, learning_rate 0.000100549
2017-10-10T12:51:12.096540: step 2226, loss 0.168801, acc 0.96875, learning_rate 0.000100547
2017-10-10T12:51:12.596978: step 2227, loss 0.148867, acc 0.9375, learning_rate 0.000100545
2017-10-10T12:51:13.150345: step 2228, loss 0.175828, acc 0.953125, learning_rate 0.000100542
2017-10-10T12:51:13.613128: step 2229, loss 0.0622613, acc 0.984375, learning_rate 0.00010054
2017-10-10T12:51:14.148974: step 2230, loss 0.0631836, acc 0.984375, learning_rate 0.000100538
2017-10-10T12:51:14.757696: step 2231, loss 0.125461, acc 0.921875, learning_rate 0.000100536
2017-10-10T12:51:15.355640: step 2232, loss 0.181668, acc 0.9375, learning_rate 0.000100534
2017-10-10T12:51:15.796030: step 2233, loss 0.151797, acc 0.9375, learning_rate 0.000100531
2017-10-10T12:51:16.261027: step 2234, loss 0.253584, acc 0.921875, learning_rate 0.000100529
2017-10-10T12:51:16.679745: step 2235, loss 0.0676933, acc 0.96875, learning_rate 0.000100527
2017-10-10T12:51:17.226075: step 2236, loss 0.0997711, acc 0.96875, learning_rate 0.000100525
2017-10-10T12:51:17.693071: step 2237, loss 0.0830126, acc 0.984375, learning_rate 0.000100523
2017-10-10T12:51:18.199721: step 2238, loss 0.0695798, acc 0.984375, learning_rate 0.000100521
2017-10-10T12:51:18.774809: step 2239, loss 0.0980404, acc 0.96875, learning_rate 0.000100519
2017-10-10T12:51:19.367397: step 2240, loss 0.0880278, acc 0.96875, learning_rate 0.000100516

Evaluation:
2017-10-10T12:51:20.500840: step 2240, loss 0.229501, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2240

2017-10-10T12:51:22.109110: step 2241, loss 0.126168, acc 0.953125, learning_rate 0.000100514
2017-10-10T12:51:22.692821: step 2242, loss 0.141136, acc 0.953125, learning_rate 0.000100512
2017-10-10T12:51:23.208848: step 2243, loss 0.171393, acc 0.9375, learning_rate 0.00010051
2017-10-10T12:51:23.750917: step 2244, loss 0.0955932, acc 0.96875, learning_rate 0.000100508
2017-10-10T12:51:24.267212: step 2245, loss 0.159605, acc 0.921875, learning_rate 0.000100506
2017-10-10T12:51:24.818387: step 2246, loss 0.141673, acc 0.953125, learning_rate 0.000100504
2017-10-10T12:51:25.357066: step 2247, loss 0.0599001, acc 1, learning_rate 0.000100502
2017-10-10T12:51:25.868855: step 2248, loss 0.180806, acc 0.921875, learning_rate 0.0001005
2017-10-10T12:51:26.431913: step 2249, loss 0.0709639, acc 0.984375, learning_rate 0.000100498
2017-10-10T12:51:27.000414: step 2250, loss 0.112339, acc 0.9375, learning_rate 0.000100496
2017-10-10T12:51:27.545743: step 2251, loss 0.124325, acc 0.9375, learning_rate 0.000100494
2017-10-10T12:51:28.157183: step 2252, loss 0.0851803, acc 0.984375, learning_rate 0.000100492
2017-10-10T12:51:28.634053: step 2253, loss 0.0994675, acc 0.96875, learning_rate 0.00010049
2017-10-10T12:51:28.956888: step 2254, loss 0.105226, acc 0.980392, learning_rate 0.000100488
2017-10-10T12:51:29.414179: step 2255, loss 0.156585, acc 0.9375, learning_rate 0.000100486
2017-10-10T12:51:29.932523: step 2256, loss 0.191161, acc 0.953125, learning_rate 0.000100484
2017-10-10T12:51:30.483831: step 2257, loss 0.239205, acc 0.90625, learning_rate 0.000100482
2017-10-10T12:51:31.004858: step 2258, loss 0.0882693, acc 0.96875, learning_rate 0.00010048
2017-10-10T12:51:31.576871: step 2259, loss 0.0846983, acc 0.984375, learning_rate 0.000100478
2017-10-10T12:51:32.077248: step 2260, loss 0.100508, acc 0.953125, learning_rate 0.000100476
2017-10-10T12:51:32.512136: step 2261, loss 0.0356323, acc 1, learning_rate 0.000100474
2017-10-10T12:51:32.955433: step 2262, loss 0.177953, acc 0.9375, learning_rate 0.000100472
2017-10-10T12:51:33.485092: step 2263, loss 0.0971251, acc 0.96875, learning_rate 0.00010047
2017-10-10T12:51:33.980948: step 2264, loss 0.0521481, acc 0.984375, learning_rate 0.000100468
2017-10-10T12:51:34.521032: step 2265, loss 0.0796972, acc 0.96875, learning_rate 0.000100466
2017-10-10T12:51:34.912951: step 2266, loss 0.227426, acc 0.90625, learning_rate 0.000100464
2017-10-10T12:51:35.372625: step 2267, loss 0.149286, acc 0.9375, learning_rate 0.000100462
2017-10-10T12:51:35.985074: step 2268, loss 0.137946, acc 0.953125, learning_rate 0.000100461
2017-10-10T12:51:36.461812: step 2269, loss 0.154675, acc 0.9375, learning_rate 0.000100459
2017-10-10T12:51:36.957200: step 2270, loss 0.243295, acc 0.90625, learning_rate 0.000100457
2017-10-10T12:51:37.548913: step 2271, loss 0.242973, acc 0.921875, learning_rate 0.000100455
2017-10-10T12:51:38.107816: step 2272, loss 0.120842, acc 0.953125, learning_rate 0.000100453
2017-10-10T12:51:38.569083: step 2273, loss 0.217921, acc 0.90625, learning_rate 0.000100451
2017-10-10T12:51:39.093099: step 2274, loss 0.176425, acc 0.953125, learning_rate 0.000100449
2017-10-10T12:51:39.649000: step 2275, loss 0.208893, acc 0.90625, learning_rate 0.000100448
2017-10-10T12:51:40.199253: step 2276, loss 0.125955, acc 0.9375, learning_rate 0.000100446
2017-10-10T12:51:40.609099: step 2277, loss 0.221754, acc 0.9375, learning_rate 0.000100444
2017-10-10T12:51:41.164883: step 2278, loss 0.179068, acc 0.9375, learning_rate 0.000100442
2017-10-10T12:51:41.705008: step 2279, loss 0.0781706, acc 0.953125, learning_rate 0.00010044
2017-10-10T12:51:42.208613: step 2280, loss 0.188019, acc 0.96875, learning_rate 0.000100439

Evaluation:
2017-10-10T12:51:43.373989: step 2280, loss 0.229637, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2280

2017-10-10T12:51:45.072942: step 2281, loss 0.19807, acc 0.96875, learning_rate 0.000100437
2017-10-10T12:51:45.584916: step 2282, loss 0.250237, acc 0.90625, learning_rate 0.000100435
2017-10-10T12:51:46.123995: step 2283, loss 0.156566, acc 0.953125, learning_rate 0.000100433
2017-10-10T12:51:46.658295: step 2284, loss 0.18982, acc 0.921875, learning_rate 0.000100431
2017-10-10T12:51:47.224831: step 2285, loss 0.245446, acc 0.921875, learning_rate 0.00010043
2017-10-10T12:51:47.764817: step 2286, loss 0.163522, acc 0.9375, learning_rate 0.000100428
2017-10-10T12:51:48.325789: step 2287, loss 0.0919427, acc 0.96875, learning_rate 0.000100426
2017-10-10T12:51:48.848144: step 2288, loss 0.248585, acc 0.9375, learning_rate 0.000100424
2017-10-10T12:51:49.397555: step 2289, loss 0.12787, acc 0.9375, learning_rate 0.000100423
2017-10-10T12:51:49.896846: step 2290, loss 0.22117, acc 0.9375, learning_rate 0.000100421
2017-10-10T12:51:50.423419: step 2291, loss 0.0859485, acc 0.953125, learning_rate 0.000100419
2017-10-10T12:51:51.036828: step 2292, loss 0.0604696, acc 0.984375, learning_rate 0.000100418
2017-10-10T12:51:51.578620: step 2293, loss 0.127858, acc 0.9375, learning_rate 0.000100416
2017-10-10T12:51:51.928228: step 2294, loss 0.113496, acc 0.953125, learning_rate 0.000100414
2017-10-10T12:51:52.273039: step 2295, loss 0.111496, acc 0.96875, learning_rate 0.000100412
2017-10-10T12:51:52.717232: step 2296, loss 0.0822892, acc 0.984375, learning_rate 0.000100411
2017-10-10T12:51:53.256967: step 2297, loss 0.0552627, acc 0.984375, learning_rate 0.000100409
2017-10-10T12:51:53.784903: step 2298, loss 0.203911, acc 0.921875, learning_rate 0.000100407
2017-10-10T12:51:54.335838: step 2299, loss 0.140027, acc 0.96875, learning_rate 0.000100406
2017-10-10T12:51:54.861289: step 2300, loss 0.272806, acc 0.921875, learning_rate 0.000100404
2017-10-10T12:51:55.360899: step 2301, loss 0.168171, acc 0.9375, learning_rate 0.000100402
2017-10-10T12:51:55.828479: step 2302, loss 0.197799, acc 0.921875, learning_rate 0.000100401
2017-10-10T12:51:56.269155: step 2303, loss 0.18737, acc 0.953125, learning_rate 0.000100399
2017-10-10T12:51:56.734095: step 2304, loss 0.108033, acc 0.984375, learning_rate 0.000100398
2017-10-10T12:51:57.239377: step 2305, loss 0.0564, acc 0.96875, learning_rate 0.000100396
2017-10-10T12:51:57.764160: step 2306, loss 0.144411, acc 0.96875, learning_rate 0.000100394
2017-10-10T12:51:58.276909: step 2307, loss 0.118575, acc 0.96875, learning_rate 0.000100393
2017-10-10T12:51:58.774189: step 2308, loss 0.131854, acc 0.953125, learning_rate 0.000100391
2017-10-10T12:51:59.305715: step 2309, loss 0.153789, acc 0.921875, learning_rate 0.000100389
2017-10-10T12:51:59.796965: step 2310, loss 0.144306, acc 0.96875, learning_rate 0.000100388
2017-10-10T12:52:00.349056: step 2311, loss 0.0247723, acc 1, learning_rate 0.000100386
2017-10-10T12:52:00.964877: step 2312, loss 0.0581787, acc 1, learning_rate 0.000100385
2017-10-10T12:52:01.511963: step 2313, loss 0.0942716, acc 0.984375, learning_rate 0.000100383
2017-10-10T12:52:01.999270: step 2314, loss 0.0513782, acc 1, learning_rate 0.000100382
2017-10-10T12:52:02.437741: step 2315, loss 0.155771, acc 0.9375, learning_rate 0.00010038
2017-10-10T12:52:03.011127: step 2316, loss 0.167201, acc 0.9375, learning_rate 0.000100378
2017-10-10T12:52:03.560407: step 2317, loss 0.0723262, acc 0.96875, learning_rate 0.000100377
2017-10-10T12:52:04.103781: step 2318, loss 0.125722, acc 0.96875, learning_rate 0.000100375
2017-10-10T12:52:04.657144: step 2319, loss 0.0764229, acc 0.984375, learning_rate 0.000100374
2017-10-10T12:52:05.165288: step 2320, loss 0.137155, acc 0.9375, learning_rate 0.000100372

Evaluation:
2017-10-10T12:52:06.301502: step 2320, loss 0.230333, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2320

2017-10-10T12:52:07.736926: step 2321, loss 0.126068, acc 0.96875, learning_rate 0.000100371
2017-10-10T12:52:08.212007: step 2322, loss 0.0720308, acc 0.984375, learning_rate 0.000100369
2017-10-10T12:52:08.746512: step 2323, loss 0.0909625, acc 0.96875, learning_rate 0.000100368
2017-10-10T12:52:09.223568: step 2324, loss 0.0408436, acc 1, learning_rate 0.000100366
2017-10-10T12:52:09.789070: step 2325, loss 0.154081, acc 0.953125, learning_rate 0.000100365
2017-10-10T12:52:10.266106: step 2326, loss 0.0347713, acc 0.984375, learning_rate 0.000100363
2017-10-10T12:52:10.758096: step 2327, loss 0.142627, acc 0.953125, learning_rate 0.000100362
2017-10-10T12:52:11.352887: step 2328, loss 0.135946, acc 0.953125, learning_rate 0.00010036
2017-10-10T12:52:11.916527: step 2329, loss 0.134298, acc 0.9375, learning_rate 0.000100359
2017-10-10T12:52:12.456918: step 2330, loss 0.180873, acc 0.890625, learning_rate 0.000100357
2017-10-10T12:52:13.053060: step 2331, loss 0.118641, acc 0.953125, learning_rate 0.000100356
2017-10-10T12:52:13.559983: step 2332, loss 0.0289196, acc 1, learning_rate 0.000100354
2017-10-10T12:52:14.056144: step 2333, loss 0.124419, acc 0.9375, learning_rate 0.000100353
2017-10-10T12:52:14.659645: step 2334, loss 0.292125, acc 0.859375, learning_rate 0.000100352
2017-10-10T12:52:15.136951: step 2335, loss 0.129022, acc 0.9375, learning_rate 0.00010035
2017-10-10T12:52:15.596865: step 2336, loss 0.302897, acc 0.890625, learning_rate 0.000100349
2017-10-10T12:52:16.024766: step 2337, loss 0.157952, acc 0.9375, learning_rate 0.000100347
2017-10-10T12:52:16.540049: step 2338, loss 0.171542, acc 0.9375, learning_rate 0.000100346
2017-10-10T12:52:17.120908: step 2339, loss 0.165884, acc 0.9375, learning_rate 0.000100344
2017-10-10T12:52:17.677907: step 2340, loss 0.177622, acc 0.9375, learning_rate 0.000100343
2017-10-10T12:52:18.189885: step 2341, loss 0.051751, acc 0.984375, learning_rate 0.000100342
2017-10-10T12:52:18.632815: step 2342, loss 0.0433949, acc 0.984375, learning_rate 0.00010034
2017-10-10T12:52:19.180953: step 2343, loss 0.179211, acc 0.921875, learning_rate 0.000100339
2017-10-10T12:52:19.737924: step 2344, loss 0.15602, acc 0.9375, learning_rate 0.000100338
2017-10-10T12:52:20.300866: step 2345, loss 0.204788, acc 0.921875, learning_rate 0.000100336
2017-10-10T12:52:20.857414: step 2346, loss 0.377155, acc 0.875, learning_rate 0.000100335
2017-10-10T12:52:21.409568: step 2347, loss 0.117853, acc 0.953125, learning_rate 0.000100333
2017-10-10T12:52:21.924864: step 2348, loss 0.101537, acc 0.96875, learning_rate 0.000100332
2017-10-10T12:52:22.458649: step 2349, loss 0.165122, acc 0.9375, learning_rate 0.000100331
2017-10-10T12:52:22.901192: step 2350, loss 0.0517489, acc 0.984375, learning_rate 0.000100329
2017-10-10T12:52:23.337027: step 2351, loss 0.232593, acc 0.953125, learning_rate 0.000100328
2017-10-10T12:52:23.859920: step 2352, loss 0.0757432, acc 0.980392, learning_rate 0.000100327
2017-10-10T12:52:24.499398: step 2353, loss 0.22299, acc 0.90625, learning_rate 0.000100325
2017-10-10T12:52:24.845647: step 2354, loss 0.17632, acc 0.890625, learning_rate 0.000100324
2017-10-10T12:52:25.260938: step 2355, loss 0.212156, acc 0.953125, learning_rate 0.000100323
2017-10-10T12:52:25.692197: step 2356, loss 0.057813, acc 0.984375, learning_rate 0.000100321
2017-10-10T12:52:26.233256: step 2357, loss 0.123371, acc 0.953125, learning_rate 0.00010032
2017-10-10T12:52:26.698579: step 2358, loss 0.144088, acc 0.9375, learning_rate 0.000100319
2017-10-10T12:52:27.229098: step 2359, loss 0.123646, acc 0.96875, learning_rate 0.000100317
2017-10-10T12:52:27.744968: step 2360, loss 0.0938284, acc 0.984375, learning_rate 0.000100316

Evaluation:
2017-10-10T12:52:28.920892: step 2360, loss 0.230792, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2360

2017-10-10T12:52:30.556969: step 2361, loss 0.130633, acc 0.96875, learning_rate 0.000100315
2017-10-10T12:52:31.116086: step 2362, loss 0.216815, acc 0.953125, learning_rate 0.000100314
2017-10-10T12:52:31.677094: step 2363, loss 0.222471, acc 0.953125, learning_rate 0.000100312
2017-10-10T12:52:32.241040: step 2364, loss 0.341682, acc 0.90625, learning_rate 0.000100311
2017-10-10T12:52:32.735164: step 2365, loss 0.19681, acc 0.9375, learning_rate 0.00010031
2017-10-10T12:52:33.260425: step 2366, loss 0.17515, acc 0.953125, learning_rate 0.000100308
2017-10-10T12:52:33.765023: step 2367, loss 0.111328, acc 0.96875, learning_rate 0.000100307
2017-10-10T12:52:34.293316: step 2368, loss 0.130092, acc 0.96875, learning_rate 0.000100306
2017-10-10T12:52:34.909052: step 2369, loss 0.061555, acc 0.984375, learning_rate 0.000100305
2017-10-10T12:52:35.445263: step 2370, loss 0.0872314, acc 0.96875, learning_rate 0.000100303
2017-10-10T12:52:35.934840: step 2371, loss 0.115741, acc 0.9375, learning_rate 0.000100302
2017-10-10T12:52:36.483816: step 2372, loss 0.0270352, acc 0.984375, learning_rate 0.000100301
2017-10-10T12:52:36.995113: step 2373, loss 0.213093, acc 0.921875, learning_rate 0.0001003
2017-10-10T12:52:37.565386: step 2374, loss 0.0731049, acc 0.96875, learning_rate 0.000100299
2017-10-10T12:52:38.045063: step 2375, loss 0.255273, acc 0.90625, learning_rate 0.000100297
2017-10-10T12:52:38.472837: step 2376, loss 0.0763155, acc 0.984375, learning_rate 0.000100296
2017-10-10T12:52:38.920116: step 2377, loss 0.149797, acc 0.921875, learning_rate 0.000100295
2017-10-10T12:52:39.488277: step 2378, loss 0.157144, acc 0.953125, learning_rate 0.000100294
2017-10-10T12:52:40.043740: step 2379, loss 0.228839, acc 0.90625, learning_rate 0.000100292
2017-10-10T12:52:40.500250: step 2380, loss 0.128536, acc 0.921875, learning_rate 0.000100291
2017-10-10T12:52:40.949470: step 2381, loss 0.127153, acc 0.953125, learning_rate 0.00010029
2017-10-10T12:52:41.296884: step 2382, loss 0.10475, acc 0.9375, learning_rate 0.000100289
2017-10-10T12:52:41.826382: step 2383, loss 0.255886, acc 0.90625, learning_rate 0.000100288
2017-10-10T12:52:42.344909: step 2384, loss 0.132213, acc 0.9375, learning_rate 0.000100287
2017-10-10T12:52:42.896812: step 2385, loss 0.0590388, acc 1, learning_rate 0.000100285
2017-10-10T12:52:43.430160: step 2386, loss 0.0914795, acc 0.96875, learning_rate 0.000100284
2017-10-10T12:52:43.991589: step 2387, loss 0.280815, acc 0.953125, learning_rate 0.000100283
2017-10-10T12:52:44.542065: step 2388, loss 0.0890415, acc 0.96875, learning_rate 0.000100282
2017-10-10T12:52:45.099368: step 2389, loss 0.137627, acc 0.953125, learning_rate 0.000100281
2017-10-10T12:52:45.664822: step 2390, loss 0.213067, acc 0.90625, learning_rate 0.00010028
2017-10-10T12:52:46.289087: step 2391, loss 0.14311, acc 0.953125, learning_rate 0.000100278
2017-10-10T12:52:46.872253: step 2392, loss 0.128586, acc 0.921875, learning_rate 0.000100277
2017-10-10T12:52:47.501055: step 2393, loss 0.069127, acc 0.96875, learning_rate 0.000100276
2017-10-10T12:52:47.883556: step 2394, loss 0.0886634, acc 0.96875, learning_rate 0.000100275
2017-10-10T12:52:48.297923: step 2395, loss 0.158986, acc 0.953125, learning_rate 0.000100274
2017-10-10T12:52:48.816891: step 2396, loss 0.101306, acc 0.984375, learning_rate 0.000100273
2017-10-10T12:52:49.332750: step 2397, loss 0.0970441, acc 0.96875, learning_rate 0.000100272
2017-10-10T12:52:49.870999: step 2398, loss 0.0897382, acc 0.96875, learning_rate 0.000100271
2017-10-10T12:52:50.468064: step 2399, loss 0.1245, acc 0.96875, learning_rate 0.00010027
2017-10-10T12:52:51.009004: step 2400, loss 0.112842, acc 0.9375, learning_rate 0.000100268

Evaluation:
2017-10-10T12:52:52.417649: step 2400, loss 0.229793, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2400

2017-10-10T12:52:54.133232: step 2401, loss 0.0873303, acc 0.96875, learning_rate 0.000100267
2017-10-10T12:52:54.690883: step 2402, loss 0.133419, acc 0.96875, learning_rate 0.000100266
2017-10-10T12:52:55.261233: step 2403, loss 0.177266, acc 0.953125, learning_rate 0.000100265
2017-10-10T12:52:55.777786: step 2404, loss 0.131803, acc 0.921875, learning_rate 0.000100264
2017-10-10T12:52:56.288464: step 2405, loss 0.102166, acc 0.96875, learning_rate 0.000100263
2017-10-10T12:52:56.800886: step 2406, loss 0.116893, acc 0.953125, learning_rate 0.000100262
2017-10-10T12:52:57.251270: step 2407, loss 0.122193, acc 0.9375, learning_rate 0.000100261
2017-10-10T12:52:57.809548: step 2408, loss 0.138212, acc 0.9375, learning_rate 0.00010026
2017-10-10T12:52:58.288531: step 2409, loss 0.112252, acc 0.96875, learning_rate 0.000100259
2017-10-10T12:52:58.806636: step 2410, loss 0.121466, acc 0.953125, learning_rate 0.000100258
2017-10-10T12:52:59.353082: step 2411, loss 0.335172, acc 0.890625, learning_rate 0.000100257
2017-10-10T12:52:59.840896: step 2412, loss 0.220061, acc 0.921875, learning_rate 0.000100256
2017-10-10T12:53:00.381330: step 2413, loss 0.122758, acc 0.9375, learning_rate 0.000100255
2017-10-10T12:53:01.002196: step 2414, loss 0.160336, acc 0.921875, learning_rate 0.000100253
2017-10-10T12:53:01.449930: step 2415, loss 0.0911817, acc 0.96875, learning_rate 0.000100252
2017-10-10T12:53:01.901741: step 2416, loss 0.180818, acc 0.921875, learning_rate 0.000100251
2017-10-10T12:53:02.420906: step 2417, loss 0.103047, acc 0.96875, learning_rate 0.00010025
2017-10-10T12:53:02.904864: step 2418, loss 0.113676, acc 0.953125, learning_rate 0.000100249
2017-10-10T12:53:03.469501: step 2419, loss 0.103428, acc 0.953125, learning_rate 0.000100248
2017-10-10T12:53:03.965061: step 2420, loss 0.101751, acc 0.96875, learning_rate 0.000100247
2017-10-10T12:53:04.383923: step 2421, loss 0.101088, acc 0.9375, learning_rate 0.000100246
2017-10-10T12:53:04.893209: step 2422, loss 0.104211, acc 0.984375, learning_rate 0.000100245
2017-10-10T12:53:05.440927: step 2423, loss 0.189198, acc 0.9375, learning_rate 0.000100244
2017-10-10T12:53:05.988446: step 2424, loss 0.0772115, acc 0.984375, learning_rate 0.000100243
2017-10-10T12:53:06.568847: step 2425, loss 0.193434, acc 0.9375, learning_rate 0.000100242
2017-10-10T12:53:07.084862: step 2426, loss 0.184609, acc 0.953125, learning_rate 0.000100241
2017-10-10T12:53:07.649939: step 2427, loss 0.126695, acc 0.9375, learning_rate 0.00010024
2017-10-10T12:53:08.146470: step 2428, loss 0.153175, acc 0.9375, learning_rate 0.000100239
2017-10-10T12:53:08.700876: step 2429, loss 0.156239, acc 0.953125, learning_rate 0.000100238
2017-10-10T12:53:09.235276: step 2430, loss 0.0782843, acc 0.984375, learning_rate 0.000100237
2017-10-10T12:53:09.722013: step 2431, loss 0.098335, acc 0.96875, learning_rate 0.000100236
2017-10-10T12:53:10.292872: step 2432, loss 0.0816034, acc 0.96875, learning_rate 0.000100235
2017-10-10T12:53:10.776945: step 2433, loss 0.0689175, acc 0.984375, learning_rate 0.000100235
2017-10-10T12:53:11.234866: step 2434, loss 0.104134, acc 0.96875, learning_rate 0.000100234
2017-10-10T12:53:11.628974: step 2435, loss 0.0608323, acc 1, learning_rate 0.000100233
2017-10-10T12:53:12.177070: step 2436, loss 0.0650479, acc 0.96875, learning_rate 0.000100232
2017-10-10T12:53:12.772976: step 2437, loss 0.124143, acc 0.953125, learning_rate 0.000100231
2017-10-10T12:53:13.348870: step 2438, loss 0.138693, acc 0.921875, learning_rate 0.00010023
2017-10-10T12:53:13.896882: step 2439, loss 0.107446, acc 0.953125, learning_rate 0.000100229
2017-10-10T12:53:14.390754: step 2440, loss 0.179891, acc 0.90625, learning_rate 0.000100228

Evaluation:
2017-10-10T12:53:15.510273: step 2440, loss 0.234287, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2440

2017-10-10T12:53:17.352409: step 2441, loss 0.120065, acc 0.96875, learning_rate 0.000100227
2017-10-10T12:53:17.871489: step 2442, loss 0.338951, acc 0.890625, learning_rate 0.000100226
2017-10-10T12:53:18.396933: step 2443, loss 0.066251, acc 0.96875, learning_rate 0.000100225
2017-10-10T12:53:18.889867: step 2444, loss 0.20156, acc 0.890625, learning_rate 0.000100224
2017-10-10T12:53:19.455789: step 2445, loss 0.206667, acc 0.921875, learning_rate 0.000100223
2017-10-10T12:53:20.039632: step 2446, loss 0.122796, acc 0.9375, learning_rate 0.000100222
2017-10-10T12:53:20.592016: step 2447, loss 0.066375, acc 0.984375, learning_rate 0.000100221
2017-10-10T12:53:21.142677: step 2448, loss 0.139218, acc 0.953125, learning_rate 0.000100221
2017-10-10T12:53:21.665010: step 2449, loss 0.169668, acc 0.9375, learning_rate 0.00010022
2017-10-10T12:53:22.111119: step 2450, loss 0.169436, acc 0.921569, learning_rate 0.000100219
2017-10-10T12:53:22.632831: step 2451, loss 0.118507, acc 0.953125, learning_rate 0.000100218
2017-10-10T12:53:23.220978: step 2452, loss 0.169606, acc 0.953125, learning_rate 0.000100217
2017-10-10T12:53:23.801798: step 2453, loss 0.0976693, acc 0.984375, learning_rate 0.000100216
2017-10-10T12:53:24.266421: step 2454, loss 0.140897, acc 0.953125, learning_rate 0.000100215
2017-10-10T12:53:24.708610: step 2455, loss 0.101296, acc 0.953125, learning_rate 0.000100214
2017-10-10T12:53:25.239319: step 2456, loss 0.0652911, acc 0.984375, learning_rate 0.000100213
2017-10-10T12:53:25.823804: step 2457, loss 0.177594, acc 0.9375, learning_rate 0.000100213
2017-10-10T12:53:26.405028: step 2458, loss 0.0730158, acc 0.984375, learning_rate 0.000100212
2017-10-10T12:53:26.851454: step 2459, loss 0.0961759, acc 0.984375, learning_rate 0.000100211
2017-10-10T12:53:27.312821: step 2460, loss 0.132536, acc 0.953125, learning_rate 0.00010021
2017-10-10T12:53:27.840080: step 2461, loss 0.119831, acc 0.96875, learning_rate 0.000100209
2017-10-10T12:53:28.399344: step 2462, loss 0.116708, acc 0.96875, learning_rate 0.000100208
2017-10-10T12:53:28.910760: step 2463, loss 0.0803939, acc 0.96875, learning_rate 0.000100207
2017-10-10T12:53:29.444910: step 2464, loss 0.189958, acc 0.9375, learning_rate 0.000100207
2017-10-10T12:53:30.000953: step 2465, loss 0.135442, acc 0.953125, learning_rate 0.000100206
2017-10-10T12:53:30.467680: step 2466, loss 0.149094, acc 0.9375, learning_rate 0.000100205
2017-10-10T12:53:31.074232: step 2467, loss 0.134184, acc 0.9375, learning_rate 0.000100204
2017-10-10T12:53:31.577673: step 2468, loss 0.0753994, acc 0.984375, learning_rate 0.000100203
2017-10-10T12:53:32.099684: step 2469, loss 0.0813751, acc 0.984375, learning_rate 0.000100202
2017-10-10T12:53:32.595429: step 2470, loss 0.203338, acc 0.90625, learning_rate 0.000100202
2017-10-10T12:53:33.247186: step 2471, loss 0.0928662, acc 0.96875, learning_rate 0.000100201
2017-10-10T12:53:33.763786: step 2472, loss 0.0561555, acc 0.984375, learning_rate 0.0001002
2017-10-10T12:53:34.228806: step 2473, loss 0.143511, acc 0.9375, learning_rate 0.000100199
2017-10-10T12:53:34.647631: step 2474, loss 0.246871, acc 0.921875, learning_rate 0.000100198
2017-10-10T12:53:35.198028: step 2475, loss 0.159017, acc 0.953125, learning_rate 0.000100198
2017-10-10T12:53:35.752697: step 2476, loss 0.112028, acc 0.9375, learning_rate 0.000100197
2017-10-10T12:53:36.271173: step 2477, loss 0.121038, acc 0.9375, learning_rate 0.000100196
2017-10-10T12:53:36.777131: step 2478, loss 0.133793, acc 0.953125, learning_rate 0.000100195
2017-10-10T12:53:37.272901: step 2479, loss 0.0557943, acc 0.984375, learning_rate 0.000100194
2017-10-10T12:53:37.733492: step 2480, loss 0.146263, acc 0.96875, learning_rate 0.000100194

Evaluation:
2017-10-10T12:53:38.902101: step 2480, loss 0.229625, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2480

2017-10-10T12:53:40.184937: step 2481, loss 0.183755, acc 0.90625, learning_rate 0.000100193
2017-10-10T12:53:40.746615: step 2482, loss 0.148432, acc 0.96875, learning_rate 0.000100192
2017-10-10T12:53:41.308510: step 2483, loss 0.156261, acc 0.96875, learning_rate 0.000100191
2017-10-10T12:53:41.812062: step 2484, loss 0.0650364, acc 0.96875, learning_rate 0.00010019
2017-10-10T12:53:42.282830: step 2485, loss 0.13974, acc 0.984375, learning_rate 0.00010019
2017-10-10T12:53:42.809725: step 2486, loss 0.122476, acc 0.9375, learning_rate 0.000100189
2017-10-10T12:53:43.372874: step 2487, loss 0.109426, acc 0.96875, learning_rate 0.000100188
2017-10-10T12:53:43.912847: step 2488, loss 0.0936549, acc 0.984375, learning_rate 0.000100187
2017-10-10T12:53:44.434514: step 2489, loss 0.127284, acc 0.9375, learning_rate 0.000100187
2017-10-10T12:53:44.989245: step 2490, loss 0.158433, acc 0.953125, learning_rate 0.000100186
2017-10-10T12:53:45.531756: step 2491, loss 0.113631, acc 0.96875, learning_rate 0.000100185
2017-10-10T12:53:46.008931: step 2492, loss 0.110842, acc 0.953125, learning_rate 0.000100184
2017-10-10T12:53:46.480945: step 2493, loss 0.0305076, acc 1, learning_rate 0.000100183
2017-10-10T12:53:47.023203: step 2494, loss 0.165303, acc 0.921875, learning_rate 0.000100183
2017-10-10T12:53:47.506163: step 2495, loss 0.130053, acc 0.953125, learning_rate 0.000100182
2017-10-10T12:53:47.940853: step 2496, loss 0.0905277, acc 0.96875, learning_rate 0.000100181
2017-10-10T12:53:48.376728: step 2497, loss 0.0909681, acc 0.96875, learning_rate 0.000100181
2017-10-10T12:53:48.887566: step 2498, loss 0.220966, acc 0.90625, learning_rate 0.00010018
2017-10-10T12:53:49.394274: step 2499, loss 0.134141, acc 0.921875, learning_rate 0.000100179
2017-10-10T12:53:49.866863: step 2500, loss 0.20707, acc 0.9375, learning_rate 0.000100178
2017-10-10T12:53:50.311338: step 2501, loss 0.186871, acc 0.9375, learning_rate 0.000100178
2017-10-10T12:53:50.790387: step 2502, loss 0.192189, acc 0.90625, learning_rate 0.000100177
2017-10-10T12:53:51.342567: step 2503, loss 0.14777, acc 0.921875, learning_rate 0.000100176
2017-10-10T12:53:51.871069: step 2504, loss 0.11939, acc 0.96875, learning_rate 0.000100175
2017-10-10T12:53:52.370999: step 2505, loss 0.0572563, acc 0.96875, learning_rate 0.000100175
2017-10-10T12:53:52.910874: step 2506, loss 0.090898, acc 0.984375, learning_rate 0.000100174
2017-10-10T12:53:53.470218: step 2507, loss 0.101733, acc 0.984375, learning_rate 0.000100173
2017-10-10T12:53:54.001598: step 2508, loss 0.128975, acc 0.953125, learning_rate 0.000100173
2017-10-10T12:53:54.543230: step 2509, loss 0.135479, acc 0.953125, learning_rate 0.000100172
2017-10-10T12:53:55.107766: step 2510, loss 0.0793468, acc 0.96875, learning_rate 0.000100171
2017-10-10T12:53:55.669119: step 2511, loss 0.0895035, acc 0.96875, learning_rate 0.00010017
2017-10-10T12:53:56.224995: step 2512, loss 0.10986, acc 0.9375, learning_rate 0.00010017
2017-10-10T12:53:56.736881: step 2513, loss 0.0881965, acc 0.96875, learning_rate 0.000100169
2017-10-10T12:53:57.382070: step 2514, loss 0.154394, acc 0.953125, learning_rate 0.000100168
2017-10-10T12:53:57.817609: step 2515, loss 0.0910312, acc 0.96875, learning_rate 0.000100168
2017-10-10T12:53:58.276873: step 2516, loss 0.22206, acc 0.9375, learning_rate 0.000100167
2017-10-10T12:53:58.832839: step 2517, loss 0.194931, acc 0.9375, learning_rate 0.000100166
2017-10-10T12:53:59.351882: step 2518, loss 0.0899396, acc 0.96875, learning_rate 0.000100166
2017-10-10T12:53:59.900990: step 2519, loss 0.126655, acc 0.96875, learning_rate 0.000100165
2017-10-10T12:54:00.480846: step 2520, loss 0.0705304, acc 0.984375, learning_rate 0.000100164

Evaluation:
2017-10-10T12:54:01.715007: step 2520, loss 0.227746, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2520

2017-10-10T12:54:03.363433: step 2521, loss 0.0880513, acc 0.96875, learning_rate 0.000100164
2017-10-10T12:54:03.868969: step 2522, loss 0.225976, acc 0.9375, learning_rate 0.000100163
2017-10-10T12:54:04.421122: step 2523, loss 0.0682145, acc 0.984375, learning_rate 0.000100162
2017-10-10T12:54:04.936992: step 2524, loss 0.0615135, acc 0.96875, learning_rate 0.000100162
2017-10-10T12:54:05.488170: step 2525, loss 0.109733, acc 0.96875, learning_rate 0.000100161
2017-10-10T12:54:05.996361: step 2526, loss 0.0923317, acc 0.96875, learning_rate 0.00010016
2017-10-10T12:54:06.496828: step 2527, loss 0.108786, acc 0.953125, learning_rate 0.00010016
2017-10-10T12:54:07.059568: step 2528, loss 0.067117, acc 0.96875, learning_rate 0.000100159
2017-10-10T12:54:07.600985: step 2529, loss 0.135075, acc 0.96875, learning_rate 0.000100158
2017-10-10T12:54:08.184854: step 2530, loss 0.113268, acc 0.9375, learning_rate 0.000100158
2017-10-10T12:54:08.717906: step 2531, loss 0.0721624, acc 0.96875, learning_rate 0.000100157
2017-10-10T12:54:09.293190: step 2532, loss 0.134299, acc 0.953125, learning_rate 0.000100156
2017-10-10T12:54:09.877797: step 2533, loss 0.154574, acc 0.9375, learning_rate 0.000100156
2017-10-10T12:54:10.417334: step 2534, loss 0.0345212, acc 0.984375, learning_rate 0.000100155
2017-10-10T12:54:10.869899: step 2535, loss 0.265741, acc 0.921875, learning_rate 0.000100155
2017-10-10T12:54:11.290778: step 2536, loss 0.115064, acc 0.953125, learning_rate 0.000100154
2017-10-10T12:54:11.803471: step 2537, loss 0.17172, acc 0.921875, learning_rate 0.000100153
2017-10-10T12:54:12.332914: step 2538, loss 0.128858, acc 0.9375, learning_rate 0.000100153
2017-10-10T12:54:13.012901: step 2539, loss 0.109305, acc 0.96875, learning_rate 0.000100152
2017-10-10T12:54:13.468777: step 2540, loss 0.132321, acc 0.96875, learning_rate 0.000100151
2017-10-10T12:54:13.878257: step 2541, loss 0.0950656, acc 0.953125, learning_rate 0.000100151
2017-10-10T12:54:14.406751: step 2542, loss 0.0685637, acc 0.96875, learning_rate 0.00010015
2017-10-10T12:54:14.960793: step 2543, loss 0.396107, acc 0.890625, learning_rate 0.00010015
2017-10-10T12:54:15.401846: step 2544, loss 0.0431758, acc 0.984375, learning_rate 0.000100149
2017-10-10T12:54:15.952834: step 2545, loss 0.255648, acc 0.9375, learning_rate 0.000100148
2017-10-10T12:54:16.508824: step 2546, loss 0.226323, acc 0.90625, learning_rate 0.000100148
2017-10-10T12:54:17.065403: step 2547, loss 0.167592, acc 0.90625, learning_rate 0.000100147
2017-10-10T12:54:17.556296: step 2548, loss 0.0346688, acc 1, learning_rate 0.000100147
2017-10-10T12:54:18.035889: step 2549, loss 0.183247, acc 0.953125, learning_rate 0.000100146
2017-10-10T12:54:18.556844: step 2550, loss 0.0972266, acc 0.9375, learning_rate 0.000100145
2017-10-10T12:54:19.036977: step 2551, loss 0.111147, acc 0.953125, learning_rate 0.000100145
2017-10-10T12:54:19.523445: step 2552, loss 0.119625, acc 0.96875, learning_rate 0.000100144
2017-10-10T12:54:20.109164: step 2553, loss 0.144962, acc 0.96875, learning_rate 0.000100144
2017-10-10T12:54:20.687399: step 2554, loss 0.206254, acc 0.921875, learning_rate 0.000100143
2017-10-10T12:54:21.115159: step 2555, loss 0.0855945, acc 0.96875, learning_rate 0.000100142
2017-10-10T12:54:21.589002: step 2556, loss 0.128091, acc 0.953125, learning_rate 0.000100142
2017-10-10T12:54:22.105025: step 2557, loss 0.133271, acc 0.953125, learning_rate 0.000100141
2017-10-10T12:54:22.720801: step 2558, loss 0.0668869, acc 0.96875, learning_rate 0.000100141
2017-10-10T12:54:23.261312: step 2559, loss 0.0553531, acc 0.984375, learning_rate 0.00010014
2017-10-10T12:54:23.747722: step 2560, loss 0.225196, acc 0.984375, learning_rate 0.00010014

Evaluation:
2017-10-10T12:54:24.929085: step 2560, loss 0.228491, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2560

2017-10-10T12:54:26.733114: step 2561, loss 0.177825, acc 0.9375, learning_rate 0.000100139
2017-10-10T12:54:27.260401: step 2562, loss 0.0610423, acc 1, learning_rate 0.000100138
2017-10-10T12:54:27.720612: step 2563, loss 0.0437005, acc 1, learning_rate 0.000100138
2017-10-10T12:54:28.198316: step 2564, loss 0.129726, acc 0.953125, learning_rate 0.000100137
2017-10-10T12:54:28.748870: step 2565, loss 0.0589236, acc 0.96875, learning_rate 0.000100137
2017-10-10T12:54:29.307364: step 2566, loss 0.195991, acc 0.96875, learning_rate 0.000100136
2017-10-10T12:54:29.818003: step 2567, loss 0.111787, acc 0.953125, learning_rate 0.000100136
2017-10-10T12:54:30.372985: step 2568, loss 0.0664621, acc 0.984375, learning_rate 0.000100135
2017-10-10T12:54:30.984045: step 2569, loss 0.211674, acc 0.9375, learning_rate 0.000100134
2017-10-10T12:54:31.503203: step 2570, loss 0.052616, acc 1, learning_rate 0.000100134
2017-10-10T12:54:32.021784: step 2571, loss 0.0661837, acc 0.953125, learning_rate 0.000100133
2017-10-10T12:54:32.643121: step 2572, loss 0.0937717, acc 0.96875, learning_rate 0.000100133
2017-10-10T12:54:33.236810: step 2573, loss 0.243665, acc 0.921875, learning_rate 0.000100132
2017-10-10T12:54:33.760416: step 2574, loss 0.127711, acc 0.9375, learning_rate 0.000100132
2017-10-10T12:54:34.197952: step 2575, loss 0.141407, acc 0.96875, learning_rate 0.000100131
2017-10-10T12:54:34.736881: step 2576, loss 0.0624419, acc 0.96875, learning_rate 0.000100131
2017-10-10T12:54:35.283655: step 2577, loss 0.0960133, acc 0.96875, learning_rate 0.00010013
2017-10-10T12:54:35.713979: step 2578, loss 0.177267, acc 0.953125, learning_rate 0.00010013
2017-10-10T12:54:36.114231: step 2579, loss 0.125408, acc 0.953125, learning_rate 0.000100129
2017-10-10T12:54:36.600885: step 2580, loss 0.059927, acc 0.984375, learning_rate 0.000100129
2017-10-10T12:54:37.125158: step 2581, loss 0.213019, acc 0.96875, learning_rate 0.000100128
2017-10-10T12:54:37.728806: step 2582, loss 0.0603548, acc 0.984375, learning_rate 0.000100128
2017-10-10T12:54:38.276496: step 2583, loss 0.481632, acc 0.828125, learning_rate 0.000100127
2017-10-10T12:54:38.808088: step 2584, loss 0.0789917, acc 0.96875, learning_rate 0.000100126
2017-10-10T12:54:39.296930: step 2585, loss 0.110581, acc 0.9375, learning_rate 0.000100126
2017-10-10T12:54:39.801522: step 2586, loss 0.0796537, acc 0.96875, learning_rate 0.000100125
2017-10-10T12:54:40.298068: step 2587, loss 0.0573483, acc 0.984375, learning_rate 0.000100125
2017-10-10T12:54:40.821062: step 2588, loss 0.0781424, acc 0.96875, learning_rate 0.000100124
2017-10-10T12:54:41.253154: step 2589, loss 0.156049, acc 0.953125, learning_rate 0.000100124
2017-10-10T12:54:41.801081: step 2590, loss 0.214924, acc 0.9375, learning_rate 0.000100123
2017-10-10T12:54:42.337225: step 2591, loss 0.235295, acc 0.890625, learning_rate 0.000100123
2017-10-10T12:54:42.805045: step 2592, loss 0.0548274, acc 0.96875, learning_rate 0.000100122
2017-10-10T12:54:43.488468: step 2593, loss 0.141121, acc 0.953125, learning_rate 0.000100122
2017-10-10T12:54:44.065611: step 2594, loss 0.0455283, acc 0.984375, learning_rate 0.000100121
2017-10-10T12:54:44.416929: step 2595, loss 0.13637, acc 0.9375, learning_rate 0.000100121
2017-10-10T12:54:44.825147: step 2596, loss 0.208961, acc 0.90625, learning_rate 0.00010012
2017-10-10T12:54:45.336677: step 2597, loss 0.0853248, acc 0.96875, learning_rate 0.00010012
2017-10-10T12:54:45.847340: step 2598, loss 0.121844, acc 0.953125, learning_rate 0.000100119
2017-10-10T12:54:46.375199: step 2599, loss 0.0975795, acc 0.96875, learning_rate 0.000100119
2017-10-10T12:54:46.947980: step 2600, loss 0.0793523, acc 0.984375, learning_rate 0.000100118

Evaluation:
2017-10-10T12:54:48.064877: step 2600, loss 0.230175, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2600

2017-10-10T12:54:49.552956: step 2601, loss 0.143476, acc 0.9375, learning_rate 0.000100118
2017-10-10T12:54:50.122822: step 2602, loss 0.180278, acc 0.921875, learning_rate 0.000100117
2017-10-10T12:54:50.660853: step 2603, loss 0.169433, acc 0.921875, learning_rate 0.000100117
2017-10-10T12:54:51.167896: step 2604, loss 0.116702, acc 0.953125, learning_rate 0.000100117
2017-10-10T12:54:51.649236: step 2605, loss 0.118966, acc 0.953125, learning_rate 0.000100116
2017-10-10T12:54:52.224848: step 2606, loss 0.0997853, acc 0.953125, learning_rate 0.000100116
2017-10-10T12:54:52.716954: step 2607, loss 0.0976026, acc 0.953125, learning_rate 0.000100115
2017-10-10T12:54:53.264878: step 2608, loss 0.287348, acc 0.90625, learning_rate 0.000100115
2017-10-10T12:54:53.828628: step 2609, loss 0.17997, acc 0.953125, learning_rate 0.000100114
2017-10-10T12:54:54.364936: step 2610, loss 0.0739093, acc 0.953125, learning_rate 0.000100114
2017-10-10T12:54:54.892635: step 2611, loss 0.197394, acc 0.953125, learning_rate 0.000100113
2017-10-10T12:54:55.341236: step 2612, loss 0.158239, acc 0.9375, learning_rate 0.000100113
2017-10-10T12:54:55.872451: step 2613, loss 0.192817, acc 0.921875, learning_rate 0.000100112
2017-10-10T12:54:56.452263: step 2614, loss 0.131306, acc 0.9375, learning_rate 0.000100112
2017-10-10T12:54:57.004852: step 2615, loss 0.219203, acc 0.9375, learning_rate 0.000100111
2017-10-10T12:54:57.448304: step 2616, loss 0.154818, acc 0.953125, learning_rate 0.000100111
2017-10-10T12:54:57.990773: step 2617, loss 0.122855, acc 0.953125, learning_rate 0.000100111
2017-10-10T12:54:58.463103: step 2618, loss 0.0510338, acc 0.984375, learning_rate 0.00010011
2017-10-10T12:54:58.922164: step 2619, loss 0.0875572, acc 0.96875, learning_rate 0.00010011
2017-10-10T12:54:59.343459: step 2620, loss 0.130209, acc 0.9375, learning_rate 0.000100109
2017-10-10T12:54:59.900873: step 2621, loss 0.161875, acc 0.9375, learning_rate 0.000100109
2017-10-10T12:55:00.446518: step 2622, loss 0.118411, acc 0.96875, learning_rate 0.000100108
2017-10-10T12:55:00.934375: step 2623, loss 0.201584, acc 0.921875, learning_rate 0.000100108
2017-10-10T12:55:01.457008: step 2624, loss 0.274163, acc 0.90625, learning_rate 0.000100107
2017-10-10T12:55:01.995351: step 2625, loss 0.0948021, acc 0.953125, learning_rate 0.000100107
2017-10-10T12:55:02.458524: step 2626, loss 0.123512, acc 0.96875, learning_rate 0.000100107
2017-10-10T12:55:02.966440: step 2627, loss 0.11244, acc 0.96875, learning_rate 0.000100106
2017-10-10T12:55:03.482475: step 2628, loss 0.101117, acc 0.96875, learning_rate 0.000100106
2017-10-10T12:55:03.981042: step 2629, loss 0.0624276, acc 0.984375, learning_rate 0.000100105
2017-10-10T12:55:04.464900: step 2630, loss 0.220954, acc 0.921875, learning_rate 0.000100105
2017-10-10T12:55:05.036862: step 2631, loss 0.138683, acc 0.9375, learning_rate 0.000100104
2017-10-10T12:55:05.506885: step 2632, loss 0.10857, acc 0.96875, learning_rate 0.000100104
2017-10-10T12:55:06.061210: step 2633, loss 0.184682, acc 0.9375, learning_rate 0.000100104
2017-10-10T12:55:06.560940: step 2634, loss 0.0549174, acc 0.984375, learning_rate 0.000100103
2017-10-10T12:55:07.120844: step 2635, loss 0.069357, acc 0.984375, learning_rate 0.000100103
2017-10-10T12:55:07.569960: step 2636, loss 0.176922, acc 0.921875, learning_rate 0.000100102
2017-10-10T12:55:07.990015: step 2637, loss 0.0453471, acc 0.984375, learning_rate 0.000100102
2017-10-10T12:55:08.544902: step 2638, loss 0.123101, acc 0.96875, learning_rate 0.000100101
2017-10-10T12:55:09.111716: step 2639, loss 0.113256, acc 0.96875, learning_rate 0.000100101
2017-10-10T12:55:09.556872: step 2640, loss 0.0909582, acc 0.96875, learning_rate 0.000100101

Evaluation:
2017-10-10T12:55:10.691165: step 2640, loss 0.227112, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2640

2017-10-10T12:55:12.317369: step 2641, loss 0.114006, acc 0.984375, learning_rate 0.0001001
2017-10-10T12:55:12.859029: step 2642, loss 0.114712, acc 0.953125, learning_rate 0.0001001
2017-10-10T12:55:13.447793: step 2643, loss 0.277216, acc 0.9375, learning_rate 0.000100099
2017-10-10T12:55:14.071818: step 2644, loss 0.0436271, acc 1, learning_rate 0.000100099
2017-10-10T12:55:14.607264: step 2645, loss 0.158147, acc 0.9375, learning_rate 0.000100099
2017-10-10T12:55:15.099054: step 2646, loss 0.0803326, acc 0.980392, learning_rate 0.000100098
2017-10-10T12:55:15.664892: step 2647, loss 0.14175, acc 0.96875, learning_rate 0.000100098
2017-10-10T12:55:16.211955: step 2648, loss 0.115456, acc 0.9375, learning_rate 0.000100097
2017-10-10T12:55:16.750496: step 2649, loss 0.132125, acc 0.953125, learning_rate 0.000100097
2017-10-10T12:55:17.308842: step 2650, loss 0.0603346, acc 0.984375, learning_rate 0.000100097
2017-10-10T12:55:17.830620: step 2651, loss 0.185858, acc 0.921875, learning_rate 0.000100096
2017-10-10T12:55:18.445213: step 2652, loss 0.185631, acc 0.9375, learning_rate 0.000100096
2017-10-10T12:55:19.000255: step 2653, loss 0.155822, acc 0.953125, learning_rate 0.000100095
2017-10-10T12:55:19.576200: step 2654, loss 0.0698732, acc 1, learning_rate 0.000100095
2017-10-10T12:55:19.996901: step 2655, loss 0.118093, acc 0.953125, learning_rate 0.000100095
2017-10-10T12:55:20.382568: step 2656, loss 0.133473, acc 0.953125, learning_rate 0.000100094
2017-10-10T12:55:20.896934: step 2657, loss 0.093547, acc 0.96875, learning_rate 0.000100094
2017-10-10T12:55:21.468900: step 2658, loss 0.318397, acc 0.90625, learning_rate 0.000100093
2017-10-10T12:55:21.920826: step 2659, loss 0.0736525, acc 0.96875, learning_rate 0.000100093
2017-10-10T12:55:22.353295: step 2660, loss 0.10597, acc 0.9375, learning_rate 0.000100093
2017-10-10T12:55:22.883799: step 2661, loss 0.192665, acc 0.96875, learning_rate 0.000100092
2017-10-10T12:55:23.436907: step 2662, loss 0.167613, acc 0.953125, learning_rate 0.000100092
2017-10-10T12:55:23.984199: step 2663, loss 0.129878, acc 0.953125, learning_rate 0.000100092
2017-10-10T12:55:24.507180: step 2664, loss 0.163083, acc 0.921875, learning_rate 0.000100091
2017-10-10T12:55:25.052865: step 2665, loss 0.0719685, acc 0.96875, learning_rate 0.000100091
2017-10-10T12:55:25.613395: step 2666, loss 0.208232, acc 0.953125, learning_rate 0.00010009
2017-10-10T12:55:26.204896: step 2667, loss 0.142417, acc 0.90625, learning_rate 0.00010009
2017-10-10T12:55:26.804571: step 2668, loss 0.216018, acc 0.9375, learning_rate 0.00010009
2017-10-10T12:55:27.279472: step 2669, loss 0.101151, acc 0.96875, learning_rate 0.000100089
2017-10-10T12:55:27.835400: step 2670, loss 0.132384, acc 0.96875, learning_rate 0.000100089
2017-10-10T12:55:28.395155: step 2671, loss 0.165811, acc 0.9375, learning_rate 0.000100089
2017-10-10T12:55:28.864887: step 2672, loss 0.055896, acc 1, learning_rate 0.000100088
2017-10-10T12:55:29.493076: step 2673, loss 0.165747, acc 0.9375, learning_rate 0.000100088
2017-10-10T12:55:30.013552: step 2674, loss 0.0870525, acc 0.96875, learning_rate 0.000100088
2017-10-10T12:55:30.414290: step 2675, loss 0.252657, acc 0.953125, learning_rate 0.000100087
2017-10-10T12:55:30.847914: step 2676, loss 0.182884, acc 0.9375, learning_rate 0.000100087
2017-10-10T12:55:31.349098: step 2677, loss 0.189008, acc 0.921875, learning_rate 0.000100086
2017-10-10T12:55:31.908856: step 2678, loss 0.134264, acc 0.953125, learning_rate 0.000100086
2017-10-10T12:55:32.463221: step 2679, loss 0.0456916, acc 0.984375, learning_rate 0.000100086
2017-10-10T12:55:33.015078: step 2680, loss 0.066256, acc 0.96875, learning_rate 0.000100085

Evaluation:
2017-10-10T12:55:34.153004: step 2680, loss 0.22795, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2680

2017-10-10T12:55:35.966955: step 2681, loss 0.0923038, acc 0.953125, learning_rate 0.000100085
2017-10-10T12:55:36.419809: step 2682, loss 0.0197294, acc 1, learning_rate 0.000100085
2017-10-10T12:55:36.853204: step 2683, loss 0.0812847, acc 0.96875, learning_rate 0.000100084
2017-10-10T12:55:37.429044: step 2684, loss 0.0535664, acc 0.984375, learning_rate 0.000100084
2017-10-10T12:55:37.996999: step 2685, loss 0.172337, acc 0.921875, learning_rate 0.000100084
2017-10-10T12:55:38.501027: step 2686, loss 0.0984117, acc 0.953125, learning_rate 0.000100083
2017-10-10T12:55:38.957084: step 2687, loss 0.0820071, acc 0.984375, learning_rate 0.000100083
2017-10-10T12:55:39.441045: step 2688, loss 0.0610962, acc 0.984375, learning_rate 0.000100083
2017-10-10T12:55:39.987612: step 2689, loss 0.0945313, acc 0.953125, learning_rate 0.000100082
2017-10-10T12:55:40.509037: step 2690, loss 0.156348, acc 0.9375, learning_rate 0.000100082
2017-10-10T12:55:41.092240: step 2691, loss 0.0941799, acc 0.953125, learning_rate 0.000100082
2017-10-10T12:55:41.613064: step 2692, loss 0.103656, acc 0.96875, learning_rate 0.000100081
2017-10-10T12:55:42.065062: step 2693, loss 0.134241, acc 0.96875, learning_rate 0.000100081
2017-10-10T12:55:42.677380: step 2694, loss 0.0946223, acc 0.96875, learning_rate 0.000100081
2017-10-10T12:55:43.184839: step 2695, loss 0.191114, acc 0.953125, learning_rate 0.00010008
2017-10-10T12:55:43.635833: step 2696, loss 0.0826887, acc 0.984375, learning_rate 0.00010008
2017-10-10T12:55:44.232864: step 2697, loss 0.131915, acc 0.96875, learning_rate 0.00010008
2017-10-10T12:55:44.804885: step 2698, loss 0.164886, acc 0.9375, learning_rate 0.000100079
2017-10-10T12:55:45.260877: step 2699, loss 0.106884, acc 0.953125, learning_rate 0.000100079
2017-10-10T12:55:45.701882: step 2700, loss 0.086316, acc 0.984375, learning_rate 0.000100079
2017-10-10T12:55:46.215596: step 2701, loss 0.14953, acc 0.953125, learning_rate 0.000100078
2017-10-10T12:55:46.735673: step 2702, loss 0.274335, acc 0.921875, learning_rate 0.000100078
2017-10-10T12:55:47.245476: step 2703, loss 0.104234, acc 0.953125, learning_rate 0.000100078
2017-10-10T12:55:47.766871: step 2704, loss 0.10181, acc 0.953125, learning_rate 0.000100077
2017-10-10T12:55:48.289079: step 2705, loss 0.135946, acc 0.921875, learning_rate 0.000100077
2017-10-10T12:55:48.764464: step 2706, loss 0.201246, acc 0.921875, learning_rate 0.000100077
2017-10-10T12:55:49.227568: step 2707, loss 0.173491, acc 0.921875, learning_rate 0.000100076
2017-10-10T12:55:49.712901: step 2708, loss 0.0865747, acc 0.953125, learning_rate 0.000100076
2017-10-10T12:55:50.192248: step 2709, loss 0.0629892, acc 0.96875, learning_rate 0.000100076
2017-10-10T12:55:50.708887: step 2710, loss 0.0443681, acc 1, learning_rate 0.000100076
2017-10-10T12:55:51.272216: step 2711, loss 0.101811, acc 0.96875, learning_rate 0.000100075
2017-10-10T12:55:51.781025: step 2712, loss 0.184703, acc 0.921875, learning_rate 0.000100075
2017-10-10T12:55:52.350628: step 2713, loss 0.142845, acc 0.921875, learning_rate 0.000100075
2017-10-10T12:55:52.869102: step 2714, loss 0.154399, acc 0.9375, learning_rate 0.000100074
2017-10-10T12:55:53.480976: step 2715, loss 0.0799708, acc 0.96875, learning_rate 0.000100074
2017-10-10T12:55:53.920674: step 2716, loss 0.114564, acc 0.96875, learning_rate 0.000100074
2017-10-10T12:55:54.332077: step 2717, loss 0.0670514, acc 0.96875, learning_rate 0.000100073
2017-10-10T12:55:54.872870: step 2718, loss 0.0934995, acc 0.96875, learning_rate 0.000100073
2017-10-10T12:55:55.355766: step 2719, loss 0.227127, acc 0.90625, learning_rate 0.000100073
2017-10-10T12:55:55.961019: step 2720, loss 0.0652129, acc 0.96875, learning_rate 0.000100073

Evaluation:
2017-10-10T12:55:57.159099: step 2720, loss 0.230727, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2720

2017-10-10T12:55:58.591773: step 2721, loss 0.0923538, acc 0.953125, learning_rate 0.000100072
2017-10-10T12:55:59.074919: step 2722, loss 0.137602, acc 0.984375, learning_rate 0.000100072
2017-10-10T12:55:59.574236: step 2723, loss 0.0494945, acc 1, learning_rate 0.000100072
2017-10-10T12:56:00.143730: step 2724, loss 0.0554558, acc 0.984375, learning_rate 0.000100071
2017-10-10T12:56:00.696832: step 2725, loss 0.0515056, acc 0.984375, learning_rate 0.000100071
2017-10-10T12:56:01.215624: step 2726, loss 0.0443295, acc 1, learning_rate 0.000100071
2017-10-10T12:56:01.708888: step 2727, loss 0.05934, acc 0.96875, learning_rate 0.00010007
2017-10-10T12:56:02.185873: step 2728, loss 0.0909097, acc 0.953125, learning_rate 0.00010007
2017-10-10T12:56:02.666553: step 2729, loss 0.118757, acc 0.9375, learning_rate 0.00010007
2017-10-10T12:56:03.233155: step 2730, loss 0.199128, acc 0.921875, learning_rate 0.00010007
2017-10-10T12:56:03.741034: step 2731, loss 0.068706, acc 0.984375, learning_rate 0.000100069
2017-10-10T12:56:04.284859: step 2732, loss 0.0937009, acc 0.984375, learning_rate 0.000100069
2017-10-10T12:56:04.876080: step 2733, loss 0.0358222, acc 1, learning_rate 0.000100069
2017-10-10T12:56:05.449136: step 2734, loss 0.134276, acc 0.953125, learning_rate 0.000100068
2017-10-10T12:56:05.952814: step 2735, loss 0.191153, acc 0.9375, learning_rate 0.000100068
2017-10-10T12:56:06.407288: step 2736, loss 0.165458, acc 0.953125, learning_rate 0.000100068
2017-10-10T12:56:06.854372: step 2737, loss 0.0552187, acc 1, learning_rate 0.000100068
2017-10-10T12:56:07.417130: step 2738, loss 0.129136, acc 0.953125, learning_rate 0.000100067
2017-10-10T12:56:07.917131: step 2739, loss 0.199674, acc 0.9375, learning_rate 0.000100067
2017-10-10T12:56:08.334524: step 2740, loss 0.179007, acc 0.890625, learning_rate 0.000100067
2017-10-10T12:56:08.732888: step 2741, loss 0.149805, acc 0.9375, learning_rate 0.000100067
2017-10-10T12:56:09.251969: step 2742, loss 0.170108, acc 0.921875, learning_rate 0.000100066
2017-10-10T12:56:09.718999: step 2743, loss 0.12907, acc 0.953125, learning_rate 0.000100066
2017-10-10T12:56:10.132913: step 2744, loss 0.0875469, acc 0.941176, learning_rate 0.000100066
2017-10-10T12:56:10.611858: step 2745, loss 0.0870177, acc 0.984375, learning_rate 0.000100065
2017-10-10T12:56:11.153779: step 2746, loss 0.115268, acc 0.953125, learning_rate 0.000100065
2017-10-10T12:56:11.696865: step 2747, loss 0.199019, acc 0.9375, learning_rate 0.000100065
2017-10-10T12:56:12.248922: step 2748, loss 0.0693594, acc 0.984375, learning_rate 0.000100065
2017-10-10T12:56:12.748838: step 2749, loss 0.0414152, acc 0.984375, learning_rate 0.000100064
2017-10-10T12:56:13.312917: step 2750, loss 0.130468, acc 0.953125, learning_rate 0.000100064
2017-10-10T12:56:13.856973: step 2751, loss 0.156753, acc 0.953125, learning_rate 0.000100064
2017-10-10T12:56:14.402419: step 2752, loss 0.236111, acc 0.90625, learning_rate 0.000100064
2017-10-10T12:56:14.924889: step 2753, loss 0.1326, acc 0.953125, learning_rate 0.000100063
2017-10-10T12:56:15.463847: step 2754, loss 0.0954216, acc 0.984375, learning_rate 0.000100063
2017-10-10T12:56:16.089111: step 2755, loss 0.159908, acc 0.9375, learning_rate 0.000100063
2017-10-10T12:56:16.539897: step 2756, loss 0.169405, acc 0.9375, learning_rate 0.000100063
2017-10-10T12:56:17.012849: step 2757, loss 0.109117, acc 0.921875, learning_rate 0.000100062
2017-10-10T12:56:17.420861: step 2758, loss 0.138045, acc 0.9375, learning_rate 0.000100062
2017-10-10T12:56:17.953335: step 2759, loss 0.153073, acc 0.9375, learning_rate 0.000100062
2017-10-10T12:56:18.485775: step 2760, loss 0.0401337, acc 0.984375, learning_rate 0.000100062

Evaluation:
2017-10-10T12:56:19.786565: step 2760, loss 0.228074, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2760

2017-10-10T12:56:21.256821: step 2761, loss 0.0581098, acc 0.984375, learning_rate 0.000100061
2017-10-10T12:56:21.776816: step 2762, loss 0.0890536, acc 0.96875, learning_rate 0.000100061
2017-10-10T12:56:22.334324: step 2763, loss 0.102347, acc 0.96875, learning_rate 0.000100061
2017-10-10T12:56:22.889586: step 2764, loss 0.0990035, acc 0.953125, learning_rate 0.000100061
2017-10-10T12:56:23.469804: step 2765, loss 0.0587114, acc 0.984375, learning_rate 0.00010006
2017-10-10T12:56:23.993124: step 2766, loss 0.132332, acc 0.953125, learning_rate 0.00010006
2017-10-10T12:56:24.463467: step 2767, loss 0.222502, acc 0.9375, learning_rate 0.00010006
2017-10-10T12:56:24.833028: step 2768, loss 0.167348, acc 0.921875, learning_rate 0.00010006
2017-10-10T12:56:25.373235: step 2769, loss 0.117757, acc 0.953125, learning_rate 0.000100059
2017-10-10T12:56:25.876855: step 2770, loss 0.128251, acc 0.984375, learning_rate 0.000100059
2017-10-10T12:56:26.469061: step 2771, loss 0.101237, acc 0.984375, learning_rate 0.000100059
2017-10-10T12:56:27.056947: step 2772, loss 0.0587941, acc 0.984375, learning_rate 0.000100059
2017-10-10T12:56:27.516944: step 2773, loss 0.0910578, acc 0.96875, learning_rate 0.000100058
2017-10-10T12:56:28.080110: step 2774, loss 0.100751, acc 0.96875, learning_rate 0.000100058
2017-10-10T12:56:28.668970: step 2775, loss 0.0985358, acc 0.984375, learning_rate 0.000100058
2017-10-10T12:56:29.121264: step 2776, loss 0.14523, acc 0.9375, learning_rate 0.000100058
2017-10-10T12:56:29.588812: step 2777, loss 0.0557816, acc 0.984375, learning_rate 0.000100057
2017-10-10T12:56:30.176187: step 2778, loss 0.079656, acc 0.984375, learning_rate 0.000100057
2017-10-10T12:56:30.845124: step 2779, loss 0.0385369, acc 1, learning_rate 0.000100057
2017-10-10T12:56:31.292081: step 2780, loss 0.180658, acc 0.9375, learning_rate 0.000100057
2017-10-10T12:56:31.752537: step 2781, loss 0.276834, acc 0.90625, learning_rate 0.000100056
2017-10-10T12:56:32.301763: step 2782, loss 0.17427, acc 0.953125, learning_rate 0.000100056
2017-10-10T12:56:32.867923: step 2783, loss 0.0820692, acc 0.984375, learning_rate 0.000100056
2017-10-10T12:56:33.301038: step 2784, loss 0.0383055, acc 1, learning_rate 0.000100056
2017-10-10T12:56:33.870529: step 2785, loss 0.0640411, acc 0.984375, learning_rate 0.000100056
2017-10-10T12:56:34.326113: step 2786, loss 0.0930994, acc 0.96875, learning_rate 0.000100055
2017-10-10T12:56:34.704882: step 2787, loss 0.101057, acc 0.96875, learning_rate 0.000100055
2017-10-10T12:56:35.248284: step 2788, loss 0.140139, acc 0.953125, learning_rate 0.000100055
2017-10-10T12:56:35.848902: step 2789, loss 0.091606, acc 0.96875, learning_rate 0.000100055
2017-10-10T12:56:36.370415: step 2790, loss 0.140675, acc 0.953125, learning_rate 0.000100054
2017-10-10T12:56:36.908602: step 2791, loss 0.0820621, acc 0.984375, learning_rate 0.000100054
2017-10-10T12:56:37.448842: step 2792, loss 0.0694888, acc 0.96875, learning_rate 0.000100054
2017-10-10T12:56:37.992680: step 2793, loss 0.21581, acc 0.9375, learning_rate 0.000100054
2017-10-10T12:56:38.532391: step 2794, loss 0.100687, acc 0.984375, learning_rate 0.000100054
2017-10-10T12:56:39.168907: step 2795, loss 0.206501, acc 0.96875, learning_rate 0.000100053
2017-10-10T12:56:39.735975: step 2796, loss 0.0979442, acc 0.953125, learning_rate 0.000100053
2017-10-10T12:56:40.230036: step 2797, loss 0.0430538, acc 0.984375, learning_rate 0.000100053
2017-10-10T12:56:40.720489: step 2798, loss 0.058614, acc 0.984375, learning_rate 0.000100053
2017-10-10T12:56:41.318833: step 2799, loss 0.170544, acc 0.9375, learning_rate 0.000100052
2017-10-10T12:56:41.796938: step 2800, loss 0.151428, acc 0.953125, learning_rate 0.000100052

Evaluation:
2017-10-10T12:56:42.933314: step 2800, loss 0.226533, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2800

2017-10-10T12:56:44.720845: step 2801, loss 0.128677, acc 0.9375, learning_rate 0.000100052
2017-10-10T12:56:45.240917: step 2802, loss 0.102953, acc 0.953125, learning_rate 0.000100052
2017-10-10T12:56:45.763881: step 2803, loss 0.124928, acc 0.9375, learning_rate 0.000100052
2017-10-10T12:56:46.315627: step 2804, loss 0.0663343, acc 0.984375, learning_rate 0.000100051
2017-10-10T12:56:46.806562: step 2805, loss 0.151616, acc 0.953125, learning_rate 0.000100051
2017-10-10T12:56:47.316867: step 2806, loss 0.202337, acc 0.9375, learning_rate 0.000100051
2017-10-10T12:56:47.868981: step 2807, loss 0.3381, acc 0.9375, learning_rate 0.000100051
2017-10-10T12:56:48.355897: step 2808, loss 0.0506078, acc 0.984375, learning_rate 0.000100051
2017-10-10T12:56:48.912911: step 2809, loss 0.0877004, acc 0.96875, learning_rate 0.00010005
2017-10-10T12:56:49.457228: step 2810, loss 0.138922, acc 0.9375, learning_rate 0.00010005
2017-10-10T12:56:50.052827: step 2811, loss 0.0927897, acc 0.96875, learning_rate 0.00010005
2017-10-10T12:56:50.601832: step 2812, loss 0.166074, acc 0.921875, learning_rate 0.00010005
2017-10-10T12:56:51.152869: step 2813, loss 0.0438656, acc 1, learning_rate 0.00010005
2017-10-10T12:56:51.790666: step 2814, loss 0.1643, acc 0.90625, learning_rate 0.000100049
2017-10-10T12:56:52.231955: step 2815, loss 0.0514795, acc 0.96875, learning_rate 0.000100049
2017-10-10T12:56:52.637108: step 2816, loss 0.0918197, acc 0.96875, learning_rate 0.000100049
2017-10-10T12:56:53.200934: step 2817, loss 0.0684608, acc 0.984375, learning_rate 0.000100049
2017-10-10T12:56:53.800860: step 2818, loss 0.155522, acc 0.984375, learning_rate 0.000100049
2017-10-10T12:56:54.231156: step 2819, loss 0.086531, acc 0.96875, learning_rate 0.000100048
2017-10-10T12:56:54.569008: step 2820, loss 0.0484737, acc 0.984375, learning_rate 0.000100048
2017-10-10T12:56:55.107282: step 2821, loss 0.201767, acc 0.953125, learning_rate 0.000100048
2017-10-10T12:56:55.569106: step 2822, loss 0.162047, acc 0.953125, learning_rate 0.000100048
2017-10-10T12:56:56.100986: step 2823, loss 0.114818, acc 0.96875, learning_rate 0.000100048
2017-10-10T12:56:56.677082: step 2824, loss 0.15512, acc 0.96875, learning_rate 0.000100047
2017-10-10T12:56:57.244913: step 2825, loss 0.0694594, acc 0.984375, learning_rate 0.000100047
2017-10-10T12:56:57.689074: step 2826, loss 0.0766469, acc 0.96875, learning_rate 0.000100047
2017-10-10T12:56:58.205299: step 2827, loss 0.0858729, acc 0.96875, learning_rate 0.000100047
2017-10-10T12:56:58.809054: step 2828, loss 0.224711, acc 0.9375, learning_rate 0.000100047
2017-10-10T12:56:59.420845: step 2829, loss 0.151269, acc 0.921875, learning_rate 0.000100046
2017-10-10T12:56:59.942297: step 2830, loss 0.17757, acc 0.9375, learning_rate 0.000100046
2017-10-10T12:57:00.437035: step 2831, loss 0.0770106, acc 0.984375, learning_rate 0.000100046
2017-10-10T12:57:00.977088: step 2832, loss 0.0824434, acc 0.96875, learning_rate 0.000100046
2017-10-10T12:57:01.521158: step 2833, loss 0.171548, acc 0.9375, learning_rate 0.000100046
2017-10-10T12:57:02.032912: step 2834, loss 0.282165, acc 0.90625, learning_rate 0.000100045
2017-10-10T12:57:02.621310: step 2835, loss 0.215318, acc 0.90625, learning_rate 0.000100045
2017-10-10T12:57:03.035878: step 2836, loss 0.178902, acc 0.921875, learning_rate 0.000100045
2017-10-10T12:57:03.433186: step 2837, loss 0.0984078, acc 0.96875, learning_rate 0.000100045
2017-10-10T12:57:03.951625: step 2838, loss 0.110323, acc 0.953125, learning_rate 0.000100045
2017-10-10T12:57:04.437709: step 2839, loss 0.0765909, acc 0.984375, learning_rate 0.000100045
2017-10-10T12:57:04.908736: step 2840, loss 0.0397697, acc 1, learning_rate 0.000100044

Evaluation:
2017-10-10T12:57:06.076882: step 2840, loss 0.228066, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2840

2017-10-10T12:57:07.500940: step 2841, loss 0.225623, acc 0.921875, learning_rate 0.000100044
2017-10-10T12:57:07.960820: step 2842, loss 0.22482, acc 0.941176, learning_rate 0.000100044
2017-10-10T12:57:08.516789: step 2843, loss 0.0745897, acc 0.984375, learning_rate 0.000100044
2017-10-10T12:57:09.053902: step 2844, loss 0.0819536, acc 0.984375, learning_rate 0.000100044
2017-10-10T12:57:09.587226: step 2845, loss 0.105449, acc 0.984375, learning_rate 0.000100043
2017-10-10T12:57:10.111350: step 2846, loss 0.0933149, acc 0.96875, learning_rate 0.000100043
2017-10-10T12:57:10.537054: step 2847, loss 0.0815953, acc 0.96875, learning_rate 0.000100043
2017-10-10T12:57:11.050093: step 2848, loss 0.129702, acc 0.9375, learning_rate 0.000100043
2017-10-10T12:57:11.465179: step 2849, loss 0.116654, acc 0.9375, learning_rate 0.000100043
2017-10-10T12:57:11.953109: step 2850, loss 0.0840793, acc 0.96875, learning_rate 0.000100043
2017-10-10T12:57:12.516899: step 2851, loss 0.0555227, acc 1, learning_rate 0.000100042
2017-10-10T12:57:13.059781: step 2852, loss 0.085639, acc 0.96875, learning_rate 0.000100042
2017-10-10T12:57:13.614261: step 2853, loss 0.0894777, acc 0.984375, learning_rate 0.000100042
2017-10-10T12:57:14.176848: step 2854, loss 0.119098, acc 0.953125, learning_rate 0.000100042
2017-10-10T12:57:14.802447: step 2855, loss 0.0861535, acc 0.96875, learning_rate 0.000100042
2017-10-10T12:57:15.255010: step 2856, loss 0.148679, acc 0.953125, learning_rate 0.000100042
2017-10-10T12:57:15.720487: step 2857, loss 0.143353, acc 0.953125, learning_rate 0.000100041
2017-10-10T12:57:16.304064: step 2858, loss 0.162589, acc 0.953125, learning_rate 0.000100041
2017-10-10T12:57:16.877059: step 2859, loss 0.0778354, acc 0.984375, learning_rate 0.000100041
2017-10-10T12:57:17.312833: step 2860, loss 0.102592, acc 0.96875, learning_rate 0.000100041
2017-10-10T12:57:17.772842: step 2861, loss 0.225708, acc 0.90625, learning_rate 0.000100041
2017-10-10T12:57:18.367521: step 2862, loss 0.144324, acc 0.9375, learning_rate 0.000100041
2017-10-10T12:57:18.832870: step 2863, loss 0.130102, acc 0.953125, learning_rate 0.00010004
2017-10-10T12:57:19.351374: step 2864, loss 0.213077, acc 0.96875, learning_rate 0.00010004
2017-10-10T12:57:19.887678: step 2865, loss 0.0633888, acc 0.984375, learning_rate 0.00010004
2017-10-10T12:57:20.440887: step 2866, loss 0.110194, acc 0.953125, learning_rate 0.00010004
2017-10-10T12:57:20.953119: step 2867, loss 0.086921, acc 0.96875, learning_rate 0.00010004
2017-10-10T12:57:21.547815: step 2868, loss 0.0908256, acc 1, learning_rate 0.00010004
2017-10-10T12:57:22.107213: step 2869, loss 0.128019, acc 0.953125, learning_rate 0.000100039
2017-10-10T12:57:22.656912: step 2870, loss 0.107316, acc 0.984375, learning_rate 0.000100039
2017-10-10T12:57:23.257000: step 2871, loss 0.0470468, acc 1, learning_rate 0.000100039
2017-10-10T12:57:23.800852: step 2872, loss 0.0460699, acc 1, learning_rate 0.000100039
2017-10-10T12:57:24.377047: step 2873, loss 0.107726, acc 0.96875, learning_rate 0.000100039
2017-10-10T12:57:24.905260: step 2874, loss 0.120281, acc 0.953125, learning_rate 0.000100039
2017-10-10T12:57:25.548959: step 2875, loss 0.166898, acc 0.90625, learning_rate 0.000100038
2017-10-10T12:57:26.108406: step 2876, loss 0.0913313, acc 0.984375, learning_rate 0.000100038
2017-10-10T12:57:26.496896: step 2877, loss 0.120643, acc 0.953125, learning_rate 0.000100038
2017-10-10T12:57:26.868906: step 2878, loss 0.072311, acc 0.953125, learning_rate 0.000100038
2017-10-10T12:57:27.319378: step 2879, loss 0.15417, acc 0.953125, learning_rate 0.000100038
2017-10-10T12:57:27.872901: step 2880, loss 0.0608705, acc 0.984375, learning_rate 0.000100038

Evaluation:
2017-10-10T12:57:29.048896: step 2880, loss 0.228913, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2880

2017-10-10T12:57:30.636671: step 2881, loss 0.21103, acc 0.9375, learning_rate 0.000100038
2017-10-10T12:57:31.128815: step 2882, loss 0.0915361, acc 0.96875, learning_rate 0.000100037
2017-10-10T12:57:31.657819: step 2883, loss 0.162058, acc 0.921875, learning_rate 0.000100037
2017-10-10T12:57:32.136987: step 2884, loss 0.0651758, acc 0.984375, learning_rate 0.000100037
2017-10-10T12:57:32.711132: step 2885, loss 0.0735064, acc 0.9375, learning_rate 0.000100037
2017-10-10T12:57:33.245789: step 2886, loss 0.0536985, acc 0.984375, learning_rate 0.000100037
2017-10-10T12:57:33.793095: step 2887, loss 0.150809, acc 0.953125, learning_rate 0.000100037
2017-10-10T12:57:34.320386: step 2888, loss 0.234846, acc 0.890625, learning_rate 0.000100036
2017-10-10T12:57:34.927754: step 2889, loss 0.0343354, acc 1, learning_rate 0.000100036
2017-10-10T12:57:35.410273: step 2890, loss 0.107546, acc 0.96875, learning_rate 0.000100036
2017-10-10T12:57:35.916421: step 2891, loss 0.203078, acc 0.9375, learning_rate 0.000100036
2017-10-10T12:57:36.436325: step 2892, loss 0.0642628, acc 1, learning_rate 0.000100036
2017-10-10T12:57:36.952993: step 2893, loss 0.0915172, acc 0.96875, learning_rate 0.000100036
2017-10-10T12:57:37.577020: step 2894, loss 0.177433, acc 0.921875, learning_rate 0.000100036
2017-10-10T12:57:37.989054: step 2895, loss 0.0443567, acc 1, learning_rate 0.000100035
2017-10-10T12:57:38.429005: step 2896, loss 0.178155, acc 0.921875, learning_rate 0.000100035
2017-10-10T12:57:38.738141: step 2897, loss 0.0995859, acc 0.953125, learning_rate 0.000100035
2017-10-10T12:57:39.406904: step 2898, loss 0.115873, acc 0.953125, learning_rate 0.000100035
2017-10-10T12:57:39.906834: step 2899, loss 0.073753, acc 0.984375, learning_rate 0.000100035
2017-10-10T12:57:40.354899: step 2900, loss 0.163971, acc 0.921875, learning_rate 0.000100035
2017-10-10T12:57:40.794548: step 2901, loss 0.0895682, acc 0.96875, learning_rate 0.000100035
2017-10-10T12:57:41.243345: step 2902, loss 0.100246, acc 0.984375, learning_rate 0.000100034
2017-10-10T12:57:41.780924: step 2903, loss 0.183627, acc 0.96875, learning_rate 0.000100034
2017-10-10T12:57:42.332407: step 2904, loss 0.188597, acc 0.90625, learning_rate 0.000100034
2017-10-10T12:57:42.872935: step 2905, loss 0.110674, acc 0.953125, learning_rate 0.000100034
2017-10-10T12:57:43.456833: step 2906, loss 0.186478, acc 0.921875, learning_rate 0.000100034
2017-10-10T12:57:43.911882: step 2907, loss 0.179775, acc 0.953125, learning_rate 0.000100034
2017-10-10T12:57:44.455006: step 2908, loss 0.195732, acc 0.90625, learning_rate 0.000100034
2017-10-10T12:57:45.066225: step 2909, loss 0.0809107, acc 0.96875, learning_rate 0.000100033
2017-10-10T12:57:45.598157: step 2910, loss 0.0686439, acc 0.96875, learning_rate 0.000100033
2017-10-10T12:57:46.136158: step 2911, loss 0.16721, acc 0.9375, learning_rate 0.000100033
2017-10-10T12:57:46.697100: step 2912, loss 0.121072, acc 0.96875, learning_rate 0.000100033
2017-10-10T12:57:47.308958: step 2913, loss 0.130827, acc 0.953125, learning_rate 0.000100033
2017-10-10T12:57:47.842957: step 2914, loss 0.104581, acc 0.96875, learning_rate 0.000100033
2017-10-10T12:57:48.390499: step 2915, loss 0.10194, acc 0.953125, learning_rate 0.000100033
2017-10-10T12:57:49.021201: step 2916, loss 0.121237, acc 0.96875, learning_rate 0.000100033
2017-10-10T12:57:49.442123: step 2917, loss 0.184399, acc 0.953125, learning_rate 0.000100032
2017-10-10T12:57:49.897254: step 2918, loss 0.0800208, acc 0.984375, learning_rate 0.000100032
2017-10-10T12:57:50.442965: step 2919, loss 0.0806708, acc 0.984375, learning_rate 0.000100032
2017-10-10T12:57:50.969788: step 2920, loss 0.129005, acc 0.96875, learning_rate 0.000100032

Evaluation:
2017-10-10T12:57:52.131082: step 2920, loss 0.226495, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2920

2017-10-10T12:57:53.964179: step 2921, loss 0.0709431, acc 0.96875, learning_rate 0.000100032
2017-10-10T12:57:54.533088: step 2922, loss 0.16222, acc 0.953125, learning_rate 0.000100032
2017-10-10T12:57:55.124430: step 2923, loss 0.0983672, acc 0.984375, learning_rate 0.000100032
2017-10-10T12:57:55.578859: step 2924, loss 0.0479798, acc 0.984375, learning_rate 0.000100031
2017-10-10T12:57:56.116623: step 2925, loss 0.0691759, acc 0.984375, learning_rate 0.000100031
2017-10-10T12:57:56.590239: step 2926, loss 0.17228, acc 0.953125, learning_rate 0.000100031
2017-10-10T12:57:57.081151: step 2927, loss 0.09905, acc 0.953125, learning_rate 0.000100031
2017-10-10T12:57:57.673049: step 2928, loss 0.159361, acc 0.9375, learning_rate 0.000100031
2017-10-10T12:57:58.173010: step 2929, loss 0.124324, acc 0.96875, learning_rate 0.000100031
2017-10-10T12:57:58.673005: step 2930, loss 0.173434, acc 0.9375, learning_rate 0.000100031
2017-10-10T12:57:59.127880: step 2931, loss 0.0832431, acc 0.96875, learning_rate 0.000100031
2017-10-10T12:57:59.603863: step 2932, loss 0.1767, acc 0.921875, learning_rate 0.00010003
2017-10-10T12:58:00.108946: step 2933, loss 0.15942, acc 0.96875, learning_rate 0.00010003
2017-10-10T12:58:00.699181: step 2934, loss 0.237248, acc 0.9375, learning_rate 0.00010003
2017-10-10T12:58:01.257524: step 2935, loss 0.16954, acc 0.953125, learning_rate 0.00010003
2017-10-10T12:58:01.701383: step 2936, loss 0.112159, acc 0.953125, learning_rate 0.00010003
2017-10-10T12:58:02.244939: step 2937, loss 0.17065, acc 0.90625, learning_rate 0.00010003
2017-10-10T12:58:02.854721: step 2938, loss 0.0520637, acc 0.984375, learning_rate 0.00010003
2017-10-10T12:58:03.312913: step 2939, loss 0.226901, acc 0.90625, learning_rate 0.00010003
2017-10-10T12:58:03.792981: step 2940, loss 0.121806, acc 0.960784, learning_rate 0.000100029
2017-10-10T12:58:04.264897: step 2941, loss 0.127249, acc 0.953125, learning_rate 0.000100029
2017-10-10T12:58:04.816827: step 2942, loss 0.159963, acc 0.953125, learning_rate 0.000100029
2017-10-10T12:58:05.350306: step 2943, loss 0.171264, acc 0.9375, learning_rate 0.000100029
2017-10-10T12:58:05.940887: step 2944, loss 0.0762675, acc 0.96875, learning_rate 0.000100029
2017-10-10T12:58:06.452013: step 2945, loss 0.101819, acc 0.953125, learning_rate 0.000100029
2017-10-10T12:58:06.983485: step 2946, loss 0.0898501, acc 0.984375, learning_rate 0.000100029
2017-10-10T12:58:07.481123: step 2947, loss 0.0730241, acc 0.96875, learning_rate 0.000100029
2017-10-10T12:58:08.003476: step 2948, loss 0.169045, acc 0.9375, learning_rate 0.000100029
2017-10-10T12:58:08.503220: step 2949, loss 0.120185, acc 0.953125, learning_rate 0.000100028
2017-10-10T12:58:09.000853: step 2950, loss 0.168039, acc 0.9375, learning_rate 0.000100028
2017-10-10T12:58:09.517070: step 2951, loss 0.0749852, acc 0.984375, learning_rate 0.000100028
2017-10-10T12:58:10.080893: step 2952, loss 0.21351, acc 0.90625, learning_rate 0.000100028
2017-10-10T12:58:10.640407: step 2953, loss 0.195607, acc 0.921875, learning_rate 0.000100028
2017-10-10T12:58:11.179376: step 2954, loss 0.0482396, acc 0.984375, learning_rate 0.000100028
2017-10-10T12:58:11.777020: step 2955, loss 0.0959741, acc 0.96875, learning_rate 0.000100028
2017-10-10T12:58:12.253036: step 2956, loss 0.0934675, acc 0.96875, learning_rate 0.000100028
2017-10-10T12:58:12.672971: step 2957, loss 0.0744985, acc 1, learning_rate 0.000100028
2017-10-10T12:58:13.178878: step 2958, loss 0.11796, acc 0.953125, learning_rate 0.000100027
2017-10-10T12:58:13.594962: step 2959, loss 0.161038, acc 0.953125, learning_rate 0.000100027
2017-10-10T12:58:14.101058: step 2960, loss 0.0605728, acc 0.984375, learning_rate 0.000100027

Evaluation:
2017-10-10T12:58:15.273004: step 2960, loss 0.226243, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-2960

2017-10-10T12:58:16.731791: step 2961, loss 0.174695, acc 0.9375, learning_rate 0.000100027
2017-10-10T12:58:17.200027: step 2962, loss 0.18415, acc 0.953125, learning_rate 0.000100027
2017-10-10T12:58:17.779645: step 2963, loss 0.0411256, acc 1, learning_rate 0.000100027
2017-10-10T12:58:18.379140: step 2964, loss 0.0889954, acc 0.96875, learning_rate 0.000100027
2017-10-10T12:58:18.887236: step 2965, loss 0.0457378, acc 1, learning_rate 0.000100027
2017-10-10T12:58:19.342914: step 2966, loss 0.182944, acc 0.9375, learning_rate 0.000100027
2017-10-10T12:58:19.864986: step 2967, loss 0.12502, acc 0.953125, learning_rate 0.000100026
2017-10-10T12:58:20.421191: step 2968, loss 0.0471794, acc 1, learning_rate 0.000100026
2017-10-10T12:58:20.991264: step 2969, loss 0.0598397, acc 1, learning_rate 0.000100026
2017-10-10T12:58:21.601129: step 2970, loss 0.135814, acc 0.953125, learning_rate 0.000100026
2017-10-10T12:58:22.034902: step 2971, loss 0.0647732, acc 0.96875, learning_rate 0.000100026
2017-10-10T12:58:22.575832: step 2972, loss 0.228148, acc 0.90625, learning_rate 0.000100026
2017-10-10T12:58:23.060882: step 2973, loss 0.0482193, acc 0.984375, learning_rate 0.000100026
2017-10-10T12:58:23.593136: step 2974, loss 0.0547043, acc 0.96875, learning_rate 0.000100026
2017-10-10T12:58:24.196892: step 2975, loss 0.163016, acc 0.953125, learning_rate 0.000100026
2017-10-10T12:58:24.700889: step 2976, loss 0.124168, acc 0.96875, learning_rate 0.000100025
2017-10-10T12:58:25.139280: step 2977, loss 0.241672, acc 0.921875, learning_rate 0.000100025
2017-10-10T12:58:25.591804: step 2978, loss 0.113039, acc 0.96875, learning_rate 0.000100025
2017-10-10T12:58:26.077039: step 2979, loss 0.111842, acc 0.953125, learning_rate 0.000100025
2017-10-10T12:58:26.668981: step 2980, loss 0.0910871, acc 0.984375, learning_rate 0.000100025
2017-10-10T12:58:27.209009: step 2981, loss 0.126572, acc 0.96875, learning_rate 0.000100025
2017-10-10T12:58:27.712421: step 2982, loss 0.0421651, acc 1, learning_rate 0.000100025
2017-10-10T12:58:28.278046: step 2983, loss 0.164083, acc 0.953125, learning_rate 0.000100025
2017-10-10T12:58:28.809356: step 2984, loss 0.071796, acc 0.96875, learning_rate 0.000100025
2017-10-10T12:58:29.356239: step 2985, loss 0.126109, acc 0.96875, learning_rate 0.000100025
2017-10-10T12:58:29.861048: step 2986, loss 0.0587705, acc 0.984375, learning_rate 0.000100024
2017-10-10T12:58:30.379346: step 2987, loss 0.0627626, acc 0.96875, learning_rate 0.000100024
2017-10-10T12:58:30.868913: step 2988, loss 0.197127, acc 0.921875, learning_rate 0.000100024
2017-10-10T12:58:31.399952: step 2989, loss 0.205146, acc 0.9375, learning_rate 0.000100024
2017-10-10T12:58:31.880870: step 2990, loss 0.12187, acc 0.984375, learning_rate 0.000100024
2017-10-10T12:58:32.368933: step 2991, loss 0.0687333, acc 0.984375, learning_rate 0.000100024
2017-10-10T12:58:32.921135: step 2992, loss 0.203962, acc 0.921875, learning_rate 0.000100024
2017-10-10T12:58:33.433200: step 2993, loss 0.0804232, acc 0.984375, learning_rate 0.000100024
2017-10-10T12:58:33.976570: step 2994, loss 0.0585035, acc 0.984375, learning_rate 0.000100024
2017-10-10T12:58:34.508922: step 2995, loss 0.0265088, acc 1, learning_rate 0.000100024
2017-10-10T12:58:35.069097: step 2996, loss 0.07019, acc 0.984375, learning_rate 0.000100023
2017-10-10T12:58:35.559283: step 2997, loss 0.0306876, acc 1, learning_rate 0.000100023
2017-10-10T12:58:36.037237: step 2998, loss 0.140447, acc 0.953125, learning_rate 0.000100023
2017-10-10T12:58:36.564970: step 2999, loss 0.119079, acc 0.9375, learning_rate 0.000100023
2017-10-10T12:58:37.109597: step 3000, loss 0.114508, acc 0.984375, learning_rate 0.000100023

Evaluation:
2017-10-10T12:58:38.401949: step 3000, loss 0.228247, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3000

2017-10-10T12:58:39.989951: step 3001, loss 0.104185, acc 0.953125, learning_rate 0.000100023
2017-10-10T12:58:40.548975: step 3002, loss 0.138183, acc 0.96875, learning_rate 0.000100023
2017-10-10T12:58:41.130463: step 3003, loss 0.128574, acc 0.953125, learning_rate 0.000100023
2017-10-10T12:58:41.652971: step 3004, loss 0.0550509, acc 1, learning_rate 0.000100023
2017-10-10T12:58:42.155686: step 3005, loss 0.0616426, acc 0.984375, learning_rate 0.000100023
2017-10-10T12:58:42.752876: step 3006, loss 0.126158, acc 0.953125, learning_rate 0.000100023
2017-10-10T12:58:43.280867: step 3007, loss 0.131233, acc 0.953125, learning_rate 0.000100022
2017-10-10T12:58:43.777377: step 3008, loss 0.241336, acc 0.890625, learning_rate 0.000100022
2017-10-10T12:58:44.336850: step 3009, loss 0.128949, acc 0.96875, learning_rate 0.000100022
2017-10-10T12:58:44.800861: step 3010, loss 0.325861, acc 0.875, learning_rate 0.000100022
2017-10-10T12:58:45.344324: step 3011, loss 0.0933209, acc 0.953125, learning_rate 0.000100022
2017-10-10T12:58:45.851922: step 3012, loss 0.112379, acc 0.953125, learning_rate 0.000100022
2017-10-10T12:58:46.389043: step 3013, loss 0.0967302, acc 0.953125, learning_rate 0.000100022
2017-10-10T12:58:46.969044: step 3014, loss 0.068001, acc 0.984375, learning_rate 0.000100022
2017-10-10T12:58:47.408868: step 3015, loss 0.136088, acc 0.96875, learning_rate 0.000100022
2017-10-10T12:58:47.916849: step 3016, loss 0.118592, acc 0.953125, learning_rate 0.000100022
2017-10-10T12:58:48.227948: step 3017, loss 0.256762, acc 0.90625, learning_rate 0.000100022
2017-10-10T12:58:48.567214: step 3018, loss 0.0848332, acc 0.96875, learning_rate 0.000100021
2017-10-10T12:58:48.988847: step 3019, loss 0.166974, acc 0.953125, learning_rate 0.000100021
2017-10-10T12:58:49.528928: step 3020, loss 0.0967632, acc 0.953125, learning_rate 0.000100021
2017-10-10T12:58:50.108925: step 3021, loss 0.0951067, acc 0.96875, learning_rate 0.000100021
2017-10-10T12:58:50.666716: step 3022, loss 0.124116, acc 0.96875, learning_rate 0.000100021
2017-10-10T12:58:51.173796: step 3023, loss 0.13855, acc 0.96875, learning_rate 0.000100021
2017-10-10T12:58:51.721366: step 3024, loss 0.108728, acc 0.96875, learning_rate 0.000100021
2017-10-10T12:58:52.209880: step 3025, loss 0.0882335, acc 0.96875, learning_rate 0.000100021
2017-10-10T12:58:52.729309: step 3026, loss 0.160997, acc 0.9375, learning_rate 0.000100021
2017-10-10T12:58:53.168345: step 3027, loss 0.0690163, acc 0.984375, learning_rate 0.000100021
2017-10-10T12:58:53.683289: step 3028, loss 0.178477, acc 0.953125, learning_rate 0.000100021
2017-10-10T12:58:54.208519: step 3029, loss 0.0929768, acc 0.953125, learning_rate 0.00010002
2017-10-10T12:58:54.699379: step 3030, loss 0.153489, acc 0.953125, learning_rate 0.00010002
2017-10-10T12:58:55.249105: step 3031, loss 0.14415, acc 0.953125, learning_rate 0.00010002
2017-10-10T12:58:55.720179: step 3032, loss 0.1313, acc 0.96875, learning_rate 0.00010002
2017-10-10T12:58:56.236484: step 3033, loss 0.0679714, acc 0.953125, learning_rate 0.00010002
2017-10-10T12:58:56.709920: step 3034, loss 0.127537, acc 0.96875, learning_rate 0.00010002
2017-10-10T12:58:57.228938: step 3035, loss 0.118881, acc 0.96875, learning_rate 0.00010002
2017-10-10T12:58:57.864763: step 3036, loss 0.152096, acc 0.953125, learning_rate 0.00010002
2017-10-10T12:58:58.300682: step 3037, loss 0.0343298, acc 0.984375, learning_rate 0.00010002
2017-10-10T12:58:58.696732: step 3038, loss 0.110303, acc 0.960784, learning_rate 0.00010002
2017-10-10T12:58:59.123854: step 3039, loss 0.0811848, acc 0.96875, learning_rate 0.00010002
2017-10-10T12:58:59.664072: step 3040, loss 0.0676155, acc 1, learning_rate 0.00010002

Evaluation:
2017-10-10T12:59:00.860928: step 3040, loss 0.223086, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3040

2017-10-10T12:59:02.505258: step 3041, loss 0.119597, acc 0.96875, learning_rate 0.00010002
2017-10-10T12:59:03.008818: step 3042, loss 0.0786944, acc 0.96875, learning_rate 0.000100019
2017-10-10T12:59:03.483938: step 3043, loss 0.0480853, acc 0.984375, learning_rate 0.000100019
2017-10-10T12:59:03.958674: step 3044, loss 0.260254, acc 0.890625, learning_rate 0.000100019
2017-10-10T12:59:04.477547: step 3045, loss 0.0692801, acc 0.984375, learning_rate 0.000100019
2017-10-10T12:59:05.063096: step 3046, loss 0.0550826, acc 0.984375, learning_rate 0.000100019
2017-10-10T12:59:05.552892: step 3047, loss 0.256087, acc 0.890625, learning_rate 0.000100019
2017-10-10T12:59:06.048997: step 3048, loss 0.105616, acc 0.953125, learning_rate 0.000100019
2017-10-10T12:59:06.599015: step 3049, loss 0.186702, acc 0.9375, learning_rate 0.000100019
2017-10-10T12:59:07.101747: step 3050, loss 0.0799616, acc 0.984375, learning_rate 0.000100019
2017-10-10T12:59:07.585036: step 3051, loss 0.0915129, acc 0.96875, learning_rate 0.000100019
2017-10-10T12:59:08.082639: step 3052, loss 0.13128, acc 0.96875, learning_rate 0.000100019
2017-10-10T12:59:08.482466: step 3053, loss 0.0586384, acc 1, learning_rate 0.000100019
2017-10-10T12:59:09.010883: step 3054, loss 0.203627, acc 0.921875, learning_rate 0.000100018
2017-10-10T12:59:09.594002: step 3055, loss 0.0397873, acc 1, learning_rate 0.000100018
2017-10-10T12:59:10.206061: step 3056, loss 0.125137, acc 0.9375, learning_rate 0.000100018
2017-10-10T12:59:10.709213: step 3057, loss 0.0594604, acc 0.984375, learning_rate 0.000100018
2017-10-10T12:59:11.076429: step 3058, loss 0.0910921, acc 0.96875, learning_rate 0.000100018
2017-10-10T12:59:11.452764: step 3059, loss 0.0888988, acc 0.984375, learning_rate 0.000100018
2017-10-10T12:59:11.892850: step 3060, loss 0.104326, acc 0.96875, learning_rate 0.000100018
2017-10-10T12:59:12.492881: step 3061, loss 0.0792251, acc 0.984375, learning_rate 0.000100018
2017-10-10T12:59:12.980349: step 3062, loss 0.205292, acc 0.921875, learning_rate 0.000100018
2017-10-10T12:59:13.452935: step 3063, loss 0.112995, acc 0.953125, learning_rate 0.000100018
2017-10-10T12:59:14.001634: step 3064, loss 0.127209, acc 0.96875, learning_rate 0.000100018
2017-10-10T12:59:14.475908: step 3065, loss 0.0997982, acc 0.953125, learning_rate 0.000100018
2017-10-10T12:59:14.984968: step 3066, loss 0.0935711, acc 0.96875, learning_rate 0.000100018
2017-10-10T12:59:15.449617: step 3067, loss 0.12527, acc 0.953125, learning_rate 0.000100018
2017-10-10T12:59:15.953043: step 3068, loss 0.0361152, acc 1, learning_rate 0.000100017
2017-10-10T12:59:16.433303: step 3069, loss 0.137595, acc 0.9375, learning_rate 0.000100017
2017-10-10T12:59:16.969055: step 3070, loss 0.104686, acc 0.96875, learning_rate 0.000100017
2017-10-10T12:59:17.457274: step 3071, loss 0.143938, acc 0.953125, learning_rate 0.000100017
2017-10-10T12:59:18.060397: step 3072, loss 0.0872096, acc 0.984375, learning_rate 0.000100017
2017-10-10T12:59:18.577407: step 3073, loss 0.0904352, acc 0.953125, learning_rate 0.000100017
2017-10-10T12:59:19.145014: step 3074, loss 0.16365, acc 0.921875, learning_rate 0.000100017
2017-10-10T12:59:19.669371: step 3075, loss 0.0509427, acc 0.984375, learning_rate 0.000100017
2017-10-10T12:59:20.152845: step 3076, loss 0.101588, acc 0.984375, learning_rate 0.000100017
2017-10-10T12:59:20.716875: step 3077, loss 0.235211, acc 0.90625, learning_rate 0.000100017
2017-10-10T12:59:21.283590: step 3078, loss 0.0869658, acc 0.96875, learning_rate 0.000100017
2017-10-10T12:59:21.741041: step 3079, loss 0.120804, acc 0.9375, learning_rate 0.000100017
2017-10-10T12:59:22.179590: step 3080, loss 0.0988782, acc 0.96875, learning_rate 0.000100017

Evaluation:
2017-10-10T12:59:23.323433: step 3080, loss 0.225815, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3080

2017-10-10T12:59:25.072968: step 3081, loss 0.285954, acc 0.890625, learning_rate 0.000100017
2017-10-10T12:59:25.653080: step 3082, loss 0.0611556, acc 0.96875, learning_rate 0.000100016
2017-10-10T12:59:26.120981: step 3083, loss 0.0479824, acc 1, learning_rate 0.000100016
2017-10-10T12:59:26.741006: step 3084, loss 0.109, acc 0.9375, learning_rate 0.000100016
2017-10-10T12:59:27.300823: step 3085, loss 0.148722, acc 0.9375, learning_rate 0.000100016
2017-10-10T12:59:27.855471: step 3086, loss 0.130002, acc 0.9375, learning_rate 0.000100016
2017-10-10T12:59:28.384863: step 3087, loss 0.116313, acc 0.953125, learning_rate 0.000100016
2017-10-10T12:59:28.875035: step 3088, loss 0.125321, acc 0.96875, learning_rate 0.000100016
2017-10-10T12:59:29.386317: step 3089, loss 0.0810036, acc 0.984375, learning_rate 0.000100016
2017-10-10T12:59:29.934784: step 3090, loss 0.0904187, acc 0.96875, learning_rate 0.000100016
2017-10-10T12:59:30.401509: step 3091, loss 0.18128, acc 0.90625, learning_rate 0.000100016
2017-10-10T12:59:30.913419: step 3092, loss 0.118543, acc 0.984375, learning_rate 0.000100016
2017-10-10T12:59:31.349111: step 3093, loss 0.12538, acc 0.96875, learning_rate 0.000100016
2017-10-10T12:59:31.872981: step 3094, loss 0.12565, acc 0.9375, learning_rate 0.000100016
2017-10-10T12:59:32.488932: step 3095, loss 0.133149, acc 0.953125, learning_rate 0.000100016
2017-10-10T12:59:32.792926: step 3096, loss 0.18966, acc 0.890625, learning_rate 0.000100016
2017-10-10T12:59:46.458919: step 3097, loss 0.0643878, acc 0.984375, learning_rate 0.000100016
2017-10-10T12:59:46.723294: step 3098, loss 0.183297, acc 0.953125, learning_rate 0.000100015
2017-10-10T12:59:46.972974: step 3099, loss 0.0725279, acc 0.984375, learning_rate 0.000100015
2017-10-10T12:59:47.281967: step 3100, loss 0.189209, acc 0.921875, learning_rate 0.000100015
2017-10-10T12:59:47.626145: step 3101, loss 0.12255, acc 0.953125, learning_rate 0.000100015
2017-10-10T12:59:47.958948: step 3102, loss 0.0676886, acc 0.96875, learning_rate 0.000100015
2017-10-10T12:59:48.300908: step 3103, loss 0.115121, acc 0.9375, learning_rate 0.000100015
2017-10-10T12:59:48.856819: step 3104, loss 0.0885962, acc 0.96875, learning_rate 0.000100015
2017-10-10T12:59:49.363817: step 3105, loss 0.117707, acc 0.953125, learning_rate 0.000100015
2017-10-10T12:59:49.878111: step 3106, loss 0.0936468, acc 0.984375, learning_rate 0.000100015
2017-10-10T12:59:50.260827: step 3107, loss 0.0857414, acc 0.96875, learning_rate 0.000100015
2017-10-10T12:59:50.799710: step 3108, loss 0.0414399, acc 1, learning_rate 0.000100015
2017-10-10T12:59:51.399485: step 3109, loss 0.191361, acc 0.9375, learning_rate 0.000100015
2017-10-10T12:59:51.928657: step 3110, loss 0.111958, acc 0.9375, learning_rate 0.000100015
2017-10-10T12:59:52.413056: step 3111, loss 0.141158, acc 0.9375, learning_rate 0.000100015
2017-10-10T12:59:52.940678: step 3112, loss 0.0617165, acc 1, learning_rate 0.000100015
2017-10-10T12:59:53.509810: step 3113, loss 0.0441288, acc 1, learning_rate 0.000100015
2017-10-10T12:59:54.065107: step 3114, loss 0.141376, acc 0.96875, learning_rate 0.000100014
2017-10-10T12:59:54.654660: step 3115, loss 0.0887437, acc 0.984375, learning_rate 0.000100014
2017-10-10T12:59:55.187845: step 3116, loss 0.111872, acc 0.96875, learning_rate 0.000100014
2017-10-10T12:59:55.641331: step 3117, loss 0.199297, acc 0.9375, learning_rate 0.000100014
2017-10-10T12:59:56.201166: step 3118, loss 0.0455855, acc 1, learning_rate 0.000100014
2017-10-10T12:59:56.797038: step 3119, loss 0.0739618, acc 0.96875, learning_rate 0.000100014
2017-10-10T12:59:57.253169: step 3120, loss 0.195667, acc 0.9375, learning_rate 0.000100014

Evaluation:
2017-10-10T12:59:58.285124: step 3120, loss 0.22433, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3120

2017-10-10T12:59:59.649169: step 3121, loss 0.0552152, acc 0.984375, learning_rate 0.000100014
2017-10-10T13:00:00.122969: step 3122, loss 0.0484373, acc 0.984375, learning_rate 0.000100014
2017-10-10T13:00:00.660958: step 3123, loss 0.231956, acc 0.9375, learning_rate 0.000100014
2017-10-10T13:00:01.148835: step 3124, loss 0.207033, acc 0.890625, learning_rate 0.000100014
2017-10-10T13:00:01.694382: step 3125, loss 0.0819916, acc 0.984375, learning_rate 0.000100014
2017-10-10T13:00:02.236980: step 3126, loss 0.119539, acc 0.96875, learning_rate 0.000100014
2017-10-10T13:00:02.788088: step 3127, loss 0.0888416, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:00:03.312818: step 3128, loss 0.0328152, acc 1, learning_rate 0.000100014
2017-10-10T13:00:03.917083: step 3129, loss 0.147655, acc 0.9375, learning_rate 0.000100014
2017-10-10T13:00:04.449396: step 3130, loss 0.0633184, acc 0.984375, learning_rate 0.000100014
2017-10-10T13:00:04.967741: step 3131, loss 0.0556711, acc 0.984375, learning_rate 0.000100014
2017-10-10T13:00:05.432508: step 3132, loss 0.0475665, acc 1, learning_rate 0.000100013
2017-10-10T13:00:06.329403: step 3133, loss 0.154742, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:00:06.870143: step 3134, loss 0.132625, acc 0.9375, learning_rate 0.000100013
2017-10-10T13:00:07.434934: step 3135, loss 0.0716955, acc 0.984375, learning_rate 0.000100013
2017-10-10T13:00:07.916667: step 3136, loss 0.115021, acc 0.941176, learning_rate 0.000100013
2017-10-10T13:00:08.458566: step 3137, loss 0.167862, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:00:08.997865: step 3138, loss 0.0668748, acc 0.984375, learning_rate 0.000100013
2017-10-10T13:00:09.517654: step 3139, loss 0.141012, acc 0.921875, learning_rate 0.000100013
2017-10-10T13:00:09.980906: step 3140, loss 0.0691476, acc 0.984375, learning_rate 0.000100013
2017-10-10T13:00:10.616230: step 3141, loss 0.144754, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:00:11.194655: step 3142, loss 0.0718557, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:00:11.548785: step 3143, loss 0.0409374, acc 0.984375, learning_rate 0.000100013
2017-10-10T13:00:11.916774: step 3144, loss 0.10964, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:00:12.252614: step 3145, loss 0.0310175, acc 0.984375, learning_rate 0.000100013
2017-10-10T13:00:12.797377: step 3146, loss 0.114649, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:00:13.296821: step 3147, loss 0.186542, acc 0.921875, learning_rate 0.000100013
2017-10-10T13:00:13.872205: step 3148, loss 0.0706587, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:00:14.418920: step 3149, loss 0.088782, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:00:14.940858: step 3150, loss 0.075454, acc 0.984375, learning_rate 0.000100012
2017-10-10T13:00:15.413105: step 3151, loss 0.0997487, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:00:15.902224: step 3152, loss 0.111032, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:00:16.385170: step 3153, loss 0.143081, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:00:16.972422: step 3154, loss 0.188433, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:00:17.521092: step 3155, loss 0.0979164, acc 0.984375, learning_rate 0.000100012
2017-10-10T13:00:18.034194: step 3156, loss 0.187825, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:00:18.523335: step 3157, loss 0.111974, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:00:18.973143: step 3158, loss 0.0659784, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:00:19.502498: step 3159, loss 0.0754521, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:00:20.081117: step 3160, loss 0.0744954, acc 0.953125, learning_rate 0.000100012

Evaluation:
2017-10-10T13:00:21.128980: step 3160, loss 0.225067, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3160

2017-10-10T13:00:22.647434: step 3161, loss 0.111635, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:00:23.168846: step 3162, loss 0.138983, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:00:23.661062: step 3163, loss 0.141636, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:00:24.250891: step 3164, loss 0.129351, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:00:24.728715: step 3165, loss 0.147705, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:00:25.256910: step 3166, loss 0.131509, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:00:25.790759: step 3167, loss 0.149292, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:00:26.348409: step 3168, loss 0.193845, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:00:26.890788: step 3169, loss 0.200307, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:00:27.380894: step 3170, loss 0.10186, acc 0.96875, learning_rate 0.000100012
2017-10-10T13:00:27.864136: step 3171, loss 0.0808735, acc 0.984375, learning_rate 0.000100011
2017-10-10T13:00:28.333052: step 3172, loss 0.0239124, acc 1, learning_rate 0.000100011
2017-10-10T13:00:28.925947: step 3173, loss 0.109997, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:00:29.452093: step 3174, loss 0.22035, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:00:30.019558: step 3175, loss 0.0360234, acc 0.984375, learning_rate 0.000100011
2017-10-10T13:00:30.553667: step 3176, loss 0.136484, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:00:31.036834: step 3177, loss 0.052436, acc 0.984375, learning_rate 0.000100011
2017-10-10T13:00:31.528426: step 3178, loss 0.131573, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:00:32.037676: step 3179, loss 0.176243, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:00:32.561757: step 3180, loss 0.102979, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:00:33.095677: step 3181, loss 0.0693796, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:00:33.653111: step 3182, loss 0.131722, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:00:34.257045: step 3183, loss 0.0749494, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:00:34.676132: step 3184, loss 0.235321, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:00:35.012831: step 3185, loss 0.134047, acc 0.984375, learning_rate 0.000100011
2017-10-10T13:00:35.338421: step 3186, loss 0.0852557, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:00:35.852857: step 3187, loss 0.0701323, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:00:36.352831: step 3188, loss 0.18573, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:00:36.804433: step 3189, loss 0.202657, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:00:37.271559: step 3190, loss 0.140933, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:00:37.762251: step 3191, loss 0.173991, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:00:38.212473: step 3192, loss 0.135057, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:00:38.726048: step 3193, loss 0.139149, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:00:39.308869: step 3194, loss 0.153081, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:00:39.807370: step 3195, loss 0.157548, acc 0.90625, learning_rate 0.00010001
2017-10-10T13:00:40.337022: step 3196, loss 0.140833, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:00:40.871673: step 3197, loss 0.132528, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:00:41.436138: step 3198, loss 0.207184, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:00:41.901217: step 3199, loss 0.0504066, acc 0.984375, learning_rate 0.00010001
2017-10-10T13:00:42.385010: step 3200, loss 0.194095, acc 0.921875, learning_rate 0.00010001

Evaluation:
2017-10-10T13:00:43.700075: step 3200, loss 0.226006, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3200

2017-10-10T13:00:45.751384: step 3201, loss 0.094684, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:00:46.269750: step 3202, loss 0.0816309, acc 0.984375, learning_rate 0.00010001
2017-10-10T13:00:46.815771: step 3203, loss 0.0953427, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:00:47.356834: step 3204, loss 0.181116, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:00:47.901081: step 3205, loss 0.0954442, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:00:48.405757: step 3206, loss 0.246069, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:00:48.949800: step 3207, loss 0.113843, acc 0.984375, learning_rate 0.00010001
2017-10-10T13:00:49.936521: step 3208, loss 0.0596089, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:00:50.479692: step 3209, loss 0.158644, acc 0.90625, learning_rate 0.00010001
2017-10-10T13:00:51.016230: step 3210, loss 0.122386, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:00:51.573364: step 3211, loss 0.182822, acc 0.90625, learning_rate 0.00010001
2017-10-10T13:00:52.105456: step 3212, loss 0.0381261, acc 1, learning_rate 0.00010001
2017-10-10T13:00:52.610766: step 3213, loss 0.066178, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:00:53.140845: step 3214, loss 0.134137, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:00:53.640736: step 3215, loss 0.0668861, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:00:54.124990: step 3216, loss 0.0355067, acc 1, learning_rate 0.00010001
2017-10-10T13:00:54.664815: step 3217, loss 0.159896, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:00:55.186672: step 3218, loss 0.140606, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:00:55.696831: step 3219, loss 0.0808423, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:00:56.216827: step 3220, loss 0.111228, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:00:56.740264: step 3221, loss 0.20153, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:00:57.272910: step 3222, loss 0.121228, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:00:57.838148: step 3223, loss 0.0607732, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:00:58.276959: step 3224, loss 0.0457951, acc 1, learning_rate 0.000100009
2017-10-10T13:00:58.636409: step 3225, loss 0.185009, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:00:58.956845: step 3226, loss 0.033694, acc 1, learning_rate 0.000100009
2017-10-10T13:00:59.389031: step 3227, loss 0.206051, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:00:59.861479: step 3228, loss 0.0876737, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:01:00.401176: step 3229, loss 0.151885, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:01:00.896043: step 3230, loss 0.215373, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:01:01.345833: step 3231, loss 0.072756, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:01:01.883596: step 3232, loss 0.199424, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:01:02.413630: step 3233, loss 0.0820285, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:01:02.915432: step 3234, loss 0.191879, acc 0.921569, learning_rate 0.000100009
2017-10-10T13:01:03.463947: step 3235, loss 0.0284748, acc 1, learning_rate 0.000100009
2017-10-10T13:01:03.957518: step 3236, loss 0.071901, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:01:04.511953: step 3237, loss 0.104838, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:01:05.024908: step 3238, loss 0.116571, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:01:05.576900: step 3239, loss 0.0521226, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:01:06.140865: step 3240, loss 0.0825628, acc 0.953125, learning_rate 0.000100009

Evaluation:
2017-10-10T13:01:07.316843: step 3240, loss 0.22299, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3240

2017-10-10T13:01:08.689239: step 3241, loss 0.207412, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:01:09.209272: step 3242, loss 0.0944709, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:01:09.636432: step 3243, loss 0.0794921, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:01:10.192576: step 3244, loss 0.107967, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:01:10.739364: step 3245, loss 0.171785, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:01:11.266827: step 3246, loss 0.0827138, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:11.781013: step 3247, loss 0.132835, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:12.304965: step 3248, loss 0.102769, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:12.824518: step 3249, loss 0.131514, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:01:13.326107: step 3250, loss 0.0495915, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:01:13.869740: step 3251, loss 0.0860932, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:01:14.437146: step 3252, loss 0.0962007, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:01:14.961004: step 3253, loss 0.166014, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:01:15.445312: step 3254, loss 0.243081, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:01:15.941203: step 3255, loss 0.0881851, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:01:16.512987: step 3256, loss 0.0690364, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:01:17.096850: step 3257, loss 0.109347, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:01:17.601926: step 3258, loss 0.111724, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:01:18.060854: step 3259, loss 0.145661, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:18.597183: step 3260, loss 0.11493, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:19.105620: step 3261, loss 0.0988357, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:19.600905: step 3262, loss 0.156015, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:20.177057: step 3263, loss 0.101732, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:01:20.862650: step 3264, loss 0.0854075, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:01:21.359496: step 3265, loss 0.0863267, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:01:21.628837: step 3266, loss 0.0528466, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:01:21.932567: step 3267, loss 0.173929, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:01:22.385728: step 3268, loss 0.0682691, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:01:22.880839: step 3269, loss 0.139558, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:23.421286: step 3270, loss 0.150404, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:23.924824: step 3271, loss 0.105578, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:24.459087: step 3272, loss 0.0775495, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:01:24.953773: step 3273, loss 0.0998034, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:01:25.464917: step 3274, loss 0.119249, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:01:26.032888: step 3275, loss 0.225187, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:01:26.568850: step 3276, loss 0.0706424, acc 0.984375, learning_rate 0.000100007
2017-10-10T13:01:27.088901: step 3277, loss 0.116462, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:01:27.637075: step 3278, loss 0.0541411, acc 1, learning_rate 0.000100007
2017-10-10T13:01:28.198133: step 3279, loss 0.189072, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:01:28.721521: step 3280, loss 0.058995, acc 0.984375, learning_rate 0.000100007

Evaluation:
2017-10-10T13:01:29.974515: step 3280, loss 0.224221, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3280

2017-10-10T13:01:31.588985: step 3281, loss 0.0782825, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:01:32.135177: step 3282, loss 0.144538, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:01:32.698285: step 3283, loss 0.210279, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:01:33.236353: step 3284, loss 0.0654459, acc 0.984375, learning_rate 0.000100007
2017-10-10T13:01:33.733014: step 3285, loss 0.08055, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:01:34.287166: step 3286, loss 0.109943, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:01:34.760865: step 3287, loss 0.0819883, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:01:35.228898: step 3288, loss 0.0791777, acc 0.984375, learning_rate 0.000100007
2017-10-10T13:01:35.737129: step 3289, loss 0.133393, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:01:36.237583: step 3290, loss 0.0793764, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:01:36.658481: step 3291, loss 0.130557, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:01:37.164568: step 3292, loss 0.166597, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:01:37.732941: step 3293, loss 0.110536, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:01:38.265175: step 3294, loss 0.10481, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:01:38.845095: step 3295, loss 0.0548191, acc 1, learning_rate 0.000100007
2017-10-10T13:01:39.296868: step 3296, loss 0.0220871, acc 1, learning_rate 0.000100007
2017-10-10T13:01:39.888921: step 3297, loss 0.216346, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:01:40.404932: step 3298, loss 0.223941, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:01:41.007778: step 3299, loss 0.094983, acc 0.984375, learning_rate 0.000100007
2017-10-10T13:01:41.546378: step 3300, loss 0.148058, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:01:42.086094: step 3301, loss 0.107121, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:01:42.612936: step 3302, loss 0.0556877, acc 0.984375, learning_rate 0.000100007
2017-10-10T13:01:43.313773: step 3303, loss 0.121959, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:01:43.761016: step 3304, loss 0.125199, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:01:44.013027: step 3305, loss 0.16978, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:01:44.446248: step 3306, loss 0.109577, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:01:44.771090: step 3307, loss 0.0210404, acc 1, learning_rate 0.000100007
2017-10-10T13:01:45.083761: step 3308, loss 0.090145, acc 0.984375, learning_rate 0.000100007
2017-10-10T13:01:45.564948: step 3309, loss 0.23748, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:01:46.103090: step 3310, loss 0.169571, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:01:46.624887: step 3311, loss 0.0436156, acc 1, learning_rate 0.000100006
2017-10-10T13:01:47.037391: step 3312, loss 0.113591, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:01:47.525255: step 3313, loss 0.0437571, acc 1, learning_rate 0.000100006
2017-10-10T13:01:48.029132: step 3314, loss 0.0687363, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:01:48.482380: step 3315, loss 0.0605605, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:01:49.085538: step 3316, loss 0.210987, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:01:49.529643: step 3317, loss 0.0716914, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:01:50.048863: step 3318, loss 0.114543, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:01:50.658361: step 3319, loss 0.0607369, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:01:51.165018: step 3320, loss 0.17205, acc 0.921875, learning_rate 0.000100006

Evaluation:
2017-10-10T13:01:52.281011: step 3320, loss 0.221562, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3320

2017-10-10T13:01:53.866010: step 3321, loss 0.105384, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:01:54.352936: step 3322, loss 0.168216, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:01:54.873083: step 3323, loss 0.130393, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:01:55.355113: step 3324, loss 0.164463, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:01:55.911736: step 3325, loss 0.0731901, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:01:56.385024: step 3326, loss 0.0906619, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:01:56.881233: step 3327, loss 0.0910911, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:01:57.406866: step 3328, loss 0.0805084, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:01:57.939864: step 3329, loss 0.213515, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:01:58.399989: step 3330, loss 0.0795996, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:01:58.940138: step 3331, loss 0.209816, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:01:59.368067: step 3332, loss 0.231471, acc 0.921569, learning_rate 0.000100006
2017-10-10T13:01:59.801115: step 3333, loss 0.0708836, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:02:00.361900: step 3334, loss 0.17096, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:02:00.892548: step 3335, loss 0.14512, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:02:01.332837: step 3336, loss 0.0675884, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:02:01.758865: step 3337, loss 0.151396, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:02:02.312811: step 3338, loss 0.068361, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:02:02.851720: step 3339, loss 0.0841823, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:02:03.412745: step 3340, loss 0.0989401, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:02:03.958210: step 3341, loss 0.0655708, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:02:04.510303: step 3342, loss 0.0353347, acc 1, learning_rate 0.000100006
2017-10-10T13:02:05.040926: step 3343, loss 0.112617, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:02:05.599904: step 3344, loss 0.0596337, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:02:06.164915: step 3345, loss 0.109989, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:02:06.848624: step 3346, loss 0.157582, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:02:07.427433: step 3347, loss 0.111307, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:02:07.779912: step 3348, loss 0.0756913, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:02:08.112838: step 3349, loss 0.0490533, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:02:08.672850: step 3350, loss 0.0241835, acc 1, learning_rate 0.000100006
2017-10-10T13:02:09.200869: step 3351, loss 0.115619, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:09.771033: step 3352, loss 0.172998, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:10.313012: step 3353, loss 0.0351676, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:10.912844: step 3354, loss 0.0921939, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:11.472967: step 3355, loss 0.120523, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:12.000863: step 3356, loss 0.101443, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:12.552176: step 3357, loss 0.0490173, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:13.108834: step 3358, loss 0.0932619, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:13.644869: step 3359, loss 0.107179, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:14.152988: step 3360, loss 0.0504622, acc 1, learning_rate 0.000100005

Evaluation:
2017-10-10T13:02:15.360846: step 3360, loss 0.222995, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3360

2017-10-10T13:02:16.680935: step 3361, loss 0.17622, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:17.101351: step 3362, loss 0.0638286, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:17.525190: step 3363, loss 0.114592, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:18.079141: step 3364, loss 0.159452, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:18.584093: step 3365, loss 0.0898686, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:19.116740: step 3366, loss 0.107978, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:19.669336: step 3367, loss 0.187955, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:20.274355: step 3368, loss 0.066638, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:20.804872: step 3369, loss 0.10491, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:21.344037: step 3370, loss 0.0650387, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:21.886577: step 3371, loss 0.170759, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:22.426817: step 3372, loss 0.0714764, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:22.946442: step 3373, loss 0.220281, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:02:23.482922: step 3374, loss 0.131483, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:23.881021: step 3375, loss 0.170519, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:02:24.339930: step 3376, loss 0.11661, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:24.810531: step 3377, loss 0.10235, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:25.327140: step 3378, loss 0.19075, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:25.867520: step 3379, loss 0.0546797, acc 1, learning_rate 0.000100005
2017-10-10T13:02:26.349893: step 3380, loss 0.10202, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:26.896919: step 3381, loss 0.0787215, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:27.465154: step 3382, loss 0.127666, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:27.942455: step 3383, loss 0.153719, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:28.438491: step 3384, loss 0.0974229, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:28.983042: step 3385, loss 0.112511, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:29.573630: step 3386, loss 0.138133, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:30.232969: step 3387, loss 0.059319, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:30.718028: step 3388, loss 0.115375, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:31.065042: step 3389, loss 0.186202, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:31.489081: step 3390, loss 0.0809338, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:02:31.991589: step 3391, loss 0.0728876, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:32.449044: step 3392, loss 0.144118, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:33.040893: step 3393, loss 0.140777, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:33.497095: step 3394, loss 0.143427, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:34.076440: step 3395, loss 0.0569768, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:02:34.592980: step 3396, loss 0.136401, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:35.109511: step 3397, loss 0.181623, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:02:35.556619: step 3398, loss 0.143229, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:02:36.136990: step 3399, loss 0.120034, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:02:36.633023: step 3400, loss 0.154739, acc 0.96875, learning_rate 0.000100004

Evaluation:
2017-10-10T13:02:37.697137: step 3400, loss 0.224838, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3400

2017-10-10T13:02:39.264839: step 3401, loss 0.118752, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:02:39.933162: step 3402, loss 0.122707, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:02:40.417033: step 3403, loss 0.127269, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:02:40.884873: step 3404, loss 0.127238, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:41.353170: step 3405, loss 0.11828, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:02:41.878465: step 3406, loss 0.239796, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:02:42.363380: step 3407, loss 0.142776, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:02:42.877023: step 3408, loss 0.0581252, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:43.311018: step 3409, loss 0.107297, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:43.777005: step 3410, loss 0.167014, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:02:44.356919: step 3411, loss 0.104542, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:44.805165: step 3412, loss 0.0660548, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:45.348953: step 3413, loss 0.0996752, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:45.879613: step 3414, loss 0.0716434, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:46.421915: step 3415, loss 0.0797039, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:46.941049: step 3416, loss 0.0902445, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:47.436336: step 3417, loss 0.0844785, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:47.965940: step 3418, loss 0.121409, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:48.484456: step 3419, loss 0.108444, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:02:48.933426: step 3420, loss 0.183079, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:02:49.409005: step 3421, loss 0.0823333, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:49.968987: step 3422, loss 0.201934, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:02:50.505118: step 3423, loss 0.0569889, acc 1, learning_rate 0.000100004
2017-10-10T13:02:50.955997: step 3424, loss 0.112453, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:51.408613: step 3425, loss 0.151644, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:02:51.907565: step 3426, loss 0.188705, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:02:52.487188: step 3427, loss 0.091274, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:53.009056: step 3428, loss 0.0584983, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:53.428092: step 3429, loss 0.135633, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:02:53.724838: step 3430, loss 0.129676, acc 0.960784, learning_rate 0.000100004
2017-10-10T13:02:54.067283: step 3431, loss 0.142221, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:02:54.458412: step 3432, loss 0.0653944, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:54.975766: step 3433, loss 0.0481384, acc 1, learning_rate 0.000100004
2017-10-10T13:02:55.448378: step 3434, loss 0.0540087, acc 1, learning_rate 0.000100004
2017-10-10T13:02:55.997032: step 3435, loss 0.0923862, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:56.549301: step 3436, loss 0.0948201, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:02:57.128899: step 3437, loss 0.179053, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:02:57.647437: step 3438, loss 0.192686, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:02:58.185230: step 3439, loss 0.063335, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:02:58.732870: step 3440, loss 0.130295, acc 0.9375, learning_rate 0.000100004

Evaluation:
2017-10-10T13:02:59.852909: step 3440, loss 0.220245, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3440

2017-10-10T13:03:01.472320: step 3441, loss 0.168579, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:03:02.018259: step 3442, loss 0.110589, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:03:02.616910: step 3443, loss 0.0709475, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:03:03.210956: step 3444, loss 0.170398, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:03:03.612873: step 3445, loss 0.145213, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:03:04.064946: step 3446, loss 0.179732, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:03:04.598816: step 3447, loss 0.169826, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:03:05.116528: step 3448, loss 0.115719, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:03:05.636742: step 3449, loss 0.124012, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:03:06.199979: step 3450, loss 0.115575, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:03:06.805020: step 3451, loss 0.0195945, acc 1, learning_rate 0.000100004
2017-10-10T13:03:07.364920: step 3452, loss 0.1039, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:03:07.952878: step 3453, loss 0.0799172, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:03:08.453420: step 3454, loss 0.122062, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:03:08.981887: step 3455, loss 0.137781, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:03:09.512963: step 3456, loss 0.106904, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:03:10.055967: step 3457, loss 0.153107, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:03:10.599091: step 3458, loss 0.0918639, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:03:11.154129: step 3459, loss 0.0643336, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:03:11.693328: step 3460, loss 0.21203, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:03:12.208932: step 3461, loss 0.174533, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:03:12.706217: step 3462, loss 0.186724, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:13.242823: step 3463, loss 0.175221, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:03:13.740389: step 3464, loss 0.127811, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:14.292896: step 3465, loss 0.19682, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:03:14.808906: step 3466, loss 0.104493, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:15.256915: step 3467, loss 0.204918, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:03:15.925128: step 3468, loss 0.0963974, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:16.426047: step 3469, loss 0.0441299, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:16.728818: step 3470, loss 0.0906694, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:17.044816: step 3471, loss 0.112962, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:17.632388: step 3472, loss 0.052262, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:18.160970: step 3473, loss 0.0723275, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:18.661779: step 3474, loss 0.181935, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:03:19.168851: step 3475, loss 0.0806635, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:19.676057: step 3476, loss 0.0690495, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:20.260296: step 3477, loss 0.164256, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:03:20.817144: step 3478, loss 0.117251, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:21.372906: step 3479, loss 0.0661004, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:21.924191: step 3480, loss 0.221998, acc 0.953125, learning_rate 0.000100003

Evaluation:
2017-10-10T13:03:23.064910: step 3480, loss 0.220787, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3480

2017-10-10T13:03:25.112825: step 3481, loss 0.0915163, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:25.707149: step 3482, loss 0.117776, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:26.246621: step 3483, loss 0.12524, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:03:26.663408: step 3484, loss 0.20319, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:03:27.077619: step 3485, loss 0.085984, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:27.636871: step 3486, loss 0.0381578, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:28.169998: step 3487, loss 0.102978, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:28.768858: step 3488, loss 0.0764448, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:29.320819: step 3489, loss 0.0302714, acc 1, learning_rate 0.000100003
2017-10-10T13:03:29.887966: step 3490, loss 0.0957584, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:30.474062: step 3491, loss 0.115128, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:30.973179: step 3492, loss 0.043778, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:31.478572: step 3493, loss 0.0994568, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:32.000597: step 3494, loss 0.0924893, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:32.543802: step 3495, loss 0.179015, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:33.145907: step 3496, loss 0.0994515, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:33.708871: step 3497, loss 0.0403484, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:34.300510: step 3498, loss 0.241589, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:03:34.831333: step 3499, loss 0.0637826, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:35.345055: step 3500, loss 0.182913, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:03:35.880170: step 3501, loss 0.118808, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:36.362204: step 3502, loss 0.0686902, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:36.908432: step 3503, loss 0.0801922, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:37.402224: step 3504, loss 0.082803, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:37.897339: step 3505, loss 0.137786, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:38.321006: step 3506, loss 0.172562, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:03:38.959099: step 3507, loss 0.0783944, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:39.554652: step 3508, loss 0.109214, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:39.808852: step 3509, loss 0.102899, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:40.177531: step 3510, loss 0.0965338, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:40.641826: step 3511, loss 0.157909, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:41.093173: step 3512, loss 0.187685, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:03:41.592408: step 3513, loss 0.0516311, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:42.072918: step 3514, loss 0.165113, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:03:42.604944: step 3515, loss 0.12286, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:03:43.136414: step 3516, loss 0.0445509, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:43.625394: step 3517, loss 0.0799736, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:44.138395: step 3518, loss 0.0781732, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:44.686972: step 3519, loss 0.0940489, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:45.235296: step 3520, loss 0.114765, acc 0.96875, learning_rate 0.000100003

Evaluation:
2017-10-10T13:03:46.371414: step 3520, loss 0.222627, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3520

2017-10-10T13:03:47.863573: step 3521, loss 0.203849, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:03:48.421688: step 3522, loss 0.17608, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:49.017253: step 3523, loss 0.101269, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:49.434765: step 3524, loss 0.142709, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:03:49.928894: step 3525, loss 0.185246, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:03:50.460894: step 3526, loss 0.0402124, acc 1, learning_rate 0.000100003
2017-10-10T13:03:50.976114: step 3527, loss 0.0863126, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:51.385008: step 3528, loss 0.0217257, acc 0.980392, learning_rate 0.000100003
2017-10-10T13:03:51.885066: step 3529, loss 0.162013, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:52.353040: step 3530, loss 0.147351, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:52.848968: step 3531, loss 0.11473, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:53.369097: step 3532, loss 0.0594035, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:53.944909: step 3533, loss 0.105487, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:54.481084: step 3534, loss 0.133271, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:54.977158: step 3535, loss 0.0542833, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:55.487392: step 3536, loss 0.0665327, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:56.013378: step 3537, loss 0.108773, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:56.534453: step 3538, loss 0.170987, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:56.998987: step 3539, loss 0.219688, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:03:57.542294: step 3540, loss 0.0768099, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:58.011694: step 3541, loss 0.156744, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:03:58.505305: step 3542, loss 0.126587, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:03:59.009281: step 3543, loss 0.0604971, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:03:59.476838: step 3544, loss 0.175223, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:00.014981: step 3545, loss 0.0710396, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:00.572951: step 3546, loss 0.169823, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:01.125574: step 3547, loss 0.167435, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:01.708878: step 3548, loss 0.0882674, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:02.338500: step 3549, loss 0.041987, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:02.648782: step 3550, loss 0.107583, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:02.921426: step 3551, loss 0.0946891, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:03.288744: step 3552, loss 0.0993596, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:03.633902: step 3553, loss 0.0849672, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:03.966968: step 3554, loss 0.0730784, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:04.540796: step 3555, loss 0.0542772, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:05.097727: step 3556, loss 0.0835954, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:05.592837: step 3557, loss 0.208629, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:06.139709: step 3558, loss 0.0882792, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:06.669407: step 3559, loss 0.138419, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:07.185349: step 3560, loss 0.0949882, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T13:04:08.368242: step 3560, loss 0.223213, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3560

2017-10-10T13:04:09.943109: step 3561, loss 0.0728253, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:10.444821: step 3562, loss 0.112782, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:11.008816: step 3563, loss 0.199979, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:11.628837: step 3564, loss 0.133366, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:12.191115: step 3565, loss 0.0453648, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:12.650795: step 3566, loss 0.112061, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:13.133683: step 3567, loss 0.0858102, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:13.652996: step 3568, loss 0.11106, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:14.197195: step 3569, loss 0.129287, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:14.681025: step 3570, loss 0.123045, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:15.204970: step 3571, loss 0.245733, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:15.777447: step 3572, loss 0.0913426, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:16.256880: step 3573, loss 0.0891341, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:16.756945: step 3574, loss 0.175126, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:17.299222: step 3575, loss 0.0646997, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:17.743886: step 3576, loss 0.16151, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:18.237012: step 3577, loss 0.142647, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:18.836874: step 3578, loss 0.10234, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:19.344938: step 3579, loss 0.0968539, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:19.876246: step 3580, loss 0.0732176, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:20.404875: step 3581, loss 0.172718, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:20.988658: step 3582, loss 0.0808108, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:21.509107: step 3583, loss 0.0960614, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:22.050026: step 3584, loss 0.0644326, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:22.607792: step 3585, loss 0.0346845, acc 1, learning_rate 0.000100002
2017-10-10T13:04:23.134239: step 3586, loss 0.216762, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:23.654283: step 3587, loss 0.0380855, acc 1, learning_rate 0.000100002
2017-10-10T13:04:24.107602: step 3588, loss 0.140273, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:24.584982: step 3589, loss 0.10311, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:25.100880: step 3590, loss 0.076192, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:25.640868: step 3591, loss 0.0372943, acc 1, learning_rate 0.000100002
2017-10-10T13:04:26.288888: step 3592, loss 0.14913, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:26.636074: step 3593, loss 0.159423, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:26.969939: step 3594, loss 0.120126, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:27.484963: step 3595, loss 0.0843723, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:28.047795: step 3596, loss 0.165956, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:28.585058: step 3597, loss 0.122466, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:29.132858: step 3598, loss 0.0788679, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:29.670845: step 3599, loss 0.0755452, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:30.208928: step 3600, loss 0.151223, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T13:04:31.347552: step 3600, loss 0.220786, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3600

2017-10-10T13:04:33.152810: step 3601, loss 0.125252, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:33.760778: step 3602, loss 0.115061, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:34.292992: step 3603, loss 0.0869456, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:34.688903: step 3604, loss 0.0783621, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:35.165351: step 3605, loss 0.0761249, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:35.712866: step 3606, loss 0.0865887, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:36.266708: step 3607, loss 0.193844, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:36.734784: step 3608, loss 0.153475, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:37.308840: step 3609, loss 0.0966237, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:37.865414: step 3610, loss 0.088374, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:38.420717: step 3611, loss 0.0874235, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:38.985742: step 3612, loss 0.0419349, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:39.423928: step 3613, loss 0.196476, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:39.857605: step 3614, loss 0.161808, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:04:40.419986: step 3615, loss 0.181904, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:40.999462: step 3616, loss 0.0914879, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:41.528892: step 3617, loss 0.100028, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:42.136990: step 3618, loss 0.141823, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:04:42.685072: step 3619, loss 0.0941885, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:43.257167: step 3620, loss 0.167163, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:04:43.844838: step 3621, loss 0.104927, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:44.309081: step 3622, loss 0.249032, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:44.844120: step 3623, loss 0.11396, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:45.421470: step 3624, loss 0.181414, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:45.898774: step 3625, loss 0.154696, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:46.336517: step 3626, loss 0.319115, acc 0.862745, learning_rate 0.000100002
2017-10-10T13:04:46.769087: step 3627, loss 0.216129, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:47.375887: step 3628, loss 0.0689959, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:47.909235: step 3629, loss 0.0730956, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:48.439850: step 3630, loss 0.246624, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:49.069085: step 3631, loss 0.0573046, acc 1, learning_rate 0.000100002
2017-10-10T13:04:49.364988: step 3632, loss 0.107882, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:49.685602: step 3633, loss 0.0784321, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:50.072605: step 3634, loss 0.133677, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:50.608292: step 3635, loss 0.100639, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:51.153152: step 3636, loss 0.241735, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:04:51.687168: step 3637, loss 0.0714265, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:52.252217: step 3638, loss 0.132668, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:04:52.816894: step 3639, loss 0.0396754, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:53.406469: step 3640, loss 0.0573129, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T13:04:54.555725: step 3640, loss 0.22267, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3640

2017-10-10T13:04:56.031146: step 3641, loss 0.245817, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:56.578177: step 3642, loss 0.2133, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:04:57.036864: step 3643, loss 0.184339, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:04:57.436700: step 3644, loss 0.0842544, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:04:57.880056: step 3645, loss 0.083007, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:58.449703: step 3646, loss 0.0653833, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:58.984868: step 3647, loss 0.0819322, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:04:59.532940: step 3648, loss 0.048474, acc 1, learning_rate 0.000100002
2017-10-10T13:05:00.072837: step 3649, loss 0.125944, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:05:00.612600: step 3650, loss 0.0912356, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:05:01.208175: step 3651, loss 0.0959141, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:05:01.753092: step 3652, loss 0.102006, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:05:02.288920: step 3653, loss 0.103191, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:05:02.804758: step 3654, loss 0.176675, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:05:03.353006: step 3655, loss 0.107968, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:05:03.829104: step 3656, loss 0.0637307, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:05:04.354019: step 3657, loss 0.0905904, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:05:04.956979: step 3658, loss 0.0850552, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:05:05.428927: step 3659, loss 0.0844376, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:05:05.969885: step 3660, loss 0.110096, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:05:06.452925: step 3661, loss 0.162039, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:05:06.996088: step 3662, loss 0.12395, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:05:07.564949: step 3663, loss 0.0750442, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:05:08.092420: step 3664, loss 0.0894324, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:05:08.628996: step 3665, loss 0.181344, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:05:09.160822: step 3666, loss 0.15676, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:05:09.626826: step 3667, loss 0.0644044, acc 1, learning_rate 0.000100002
2017-10-10T13:05:10.138813: step 3668, loss 0.0659446, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:05:10.649322: step 3669, loss 0.0201533, acc 1, learning_rate 0.000100001
2017-10-10T13:05:11.201373: step 3670, loss 0.0895772, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:11.785118: step 3671, loss 0.0861898, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:12.294120: step 3672, loss 0.0540208, acc 1, learning_rate 0.000100001
2017-10-10T13:05:12.616811: step 3673, loss 0.168373, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:12.903135: step 3674, loss 0.0899689, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:13.180973: step 3675, loss 0.0597204, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:13.777428: step 3676, loss 0.0674402, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:14.289092: step 3677, loss 0.104029, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:14.801030: step 3678, loss 0.219273, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:15.317194: step 3679, loss 0.065445, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:15.849370: step 3680, loss 0.0463127, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T13:05:16.984939: step 3680, loss 0.222802, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3680

2017-10-10T13:05:18.589036: step 3681, loss 0.0562253, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:19.065716: step 3682, loss 0.0660497, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:19.689084: step 3683, loss 0.109676, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:20.253460: step 3684, loss 0.0900725, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:20.728965: step 3685, loss 0.236211, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:21.176850: step 3686, loss 0.0899515, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:21.745215: step 3687, loss 0.310454, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:05:22.368910: step 3688, loss 0.133864, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:22.861301: step 3689, loss 0.150505, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:05:23.395276: step 3690, loss 0.106005, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:23.959399: step 3691, loss 0.0795876, acc 1, learning_rate 0.000100001
2017-10-10T13:05:24.441001: step 3692, loss 0.165084, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:24.933150: step 3693, loss 0.195745, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:25.457299: step 3694, loss 0.2043, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:05:26.045025: step 3695, loss 0.0979878, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:26.547263: step 3696, loss 0.0884583, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:27.019157: step 3697, loss 0.241376, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:27.572981: step 3698, loss 0.125426, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:28.121620: step 3699, loss 0.156757, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:28.636854: step 3700, loss 0.0739059, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:29.209296: step 3701, loss 0.158233, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:29.716983: step 3702, loss 0.154406, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:30.205052: step 3703, loss 0.120049, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:30.598321: step 3704, loss 0.132578, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:31.111220: step 3705, loss 0.104158, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:31.614174: step 3706, loss 0.0935788, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:32.105176: step 3707, loss 0.0830956, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:32.613239: step 3708, loss 0.0819894, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:33.164553: step 3709, loss 0.112561, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:33.693010: step 3710, loss 0.0857615, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:34.247931: step 3711, loss 0.213219, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:05:34.796824: step 3712, loss 0.159156, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:05:35.316924: step 3713, loss 0.0418922, acc 1, learning_rate 0.000100001
2017-10-10T13:05:35.692917: step 3714, loss 0.105149, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:36.093049: step 3715, loss 0.116207, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:36.521076: step 3716, loss 0.106917, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:37.061184: step 3717, loss 0.158527, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:37.689013: step 3718, loss 0.074124, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:38.240404: step 3719, loss 0.0726922, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:38.829255: step 3720, loss 0.201672, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:05:39.932578: step 3720, loss 0.220569, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3720

2017-10-10T13:05:41.818658: step 3721, loss 0.0693281, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:42.365428: step 3722, loss 0.16672, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:43.084884: step 3723, loss 0.0596261, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:43.593315: step 3724, loss 0.173734, acc 0.921569, learning_rate 0.000100001
2017-10-10T13:05:44.027910: step 3725, loss 0.101031, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:44.426507: step 3726, loss 0.105892, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:44.981161: step 3727, loss 0.0688786, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:45.482152: step 3728, loss 0.100596, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:46.045068: step 3729, loss 0.0575725, acc 1, learning_rate 0.000100001
2017-10-10T13:05:46.594242: step 3730, loss 0.146683, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:47.124446: step 3731, loss 0.137839, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:47.633208: step 3732, loss 0.178316, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:05:48.216207: step 3733, loss 0.109799, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:48.708875: step 3734, loss 0.0606617, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:49.246567: step 3735, loss 0.0841334, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:49.765461: step 3736, loss 0.0417465, acc 1, learning_rate 0.000100001
2017-10-10T13:05:50.324898: step 3737, loss 0.0735954, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:50.873227: step 3738, loss 0.133124, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:05:51.377077: step 3739, loss 0.102126, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:51.872396: step 3740, loss 0.0677553, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:52.360448: step 3741, loss 0.133563, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:52.906298: step 3742, loss 0.122089, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:53.460638: step 3743, loss 0.0740994, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:53.971838: step 3744, loss 0.106175, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:54.483754: step 3745, loss 0.139269, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:54.984865: step 3746, loss 0.140249, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:55.532877: step 3747, loss 0.162846, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:05:56.055518: step 3748, loss 0.129432, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:05:56.568914: step 3749, loss 0.0723999, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:57.157310: step 3750, loss 0.113256, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:57.658723: step 3751, loss 0.111498, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:58.148731: step 3752, loss 0.117229, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:05:58.560959: step 3753, loss 0.103387, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:59.019858: step 3754, loss 0.114775, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:05:59.409262: step 3755, loss 0.0603302, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:05:59.944912: step 3756, loss 0.0562981, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:00.448909: step 3757, loss 0.0947412, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:00.933429: step 3758, loss 0.10164, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:01.451873: step 3759, loss 0.171934, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:06:01.976852: step 3760, loss 0.0833186, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:06:03.108151: step 3760, loss 0.22386, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3760

2017-10-10T13:06:04.512894: step 3761, loss 0.131864, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:06:05.031788: step 3762, loss 0.0549142, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:05.597634: step 3763, loss 0.0675785, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:06.244876: step 3764, loss 0.159594, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:06.709567: step 3765, loss 0.0651051, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:07.140533: step 3766, loss 0.0827947, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:07.597587: step 3767, loss 0.143132, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:08.099122: step 3768, loss 0.0625832, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:08.620449: step 3769, loss 0.116341, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:09.101043: step 3770, loss 0.151565, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:09.585154: step 3771, loss 0.0854764, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:10.136989: step 3772, loss 0.073441, acc 1, learning_rate 0.000100001
2017-10-10T13:06:10.696873: step 3773, loss 0.0557626, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:11.105361: step 3774, loss 0.201926, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:06:11.672987: step 3775, loss 0.0729392, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:12.185618: step 3776, loss 0.120804, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:12.761044: step 3777, loss 0.0630959, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:13.297034: step 3778, loss 0.11223, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:13.793928: step 3779, loss 0.0534816, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:14.205287: step 3780, loss 0.080453, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:14.756805: step 3781, loss 0.0723566, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:15.222701: step 3782, loss 0.111567, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:15.732832: step 3783, loss 0.117566, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:16.284038: step 3784, loss 0.0334894, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:16.764865: step 3785, loss 0.107624, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:17.354307: step 3786, loss 0.0636311, acc 1, learning_rate 0.000100001
2017-10-10T13:06:17.814378: step 3787, loss 0.0788894, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:18.412834: step 3788, loss 0.103476, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:18.923837: step 3789, loss 0.130643, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:19.485129: step 3790, loss 0.0644549, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:20.006386: step 3791, loss 0.0925873, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:20.640907: step 3792, loss 0.104635, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:21.160820: step 3793, loss 0.146965, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:21.601088: step 3794, loss 0.0864207, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:21.920435: step 3795, loss 0.0473603, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:22.340833: step 3796, loss 0.167551, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:06:22.900857: step 3797, loss 0.17504, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:23.436895: step 3798, loss 0.0331367, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:23.989202: step 3799, loss 0.0882244, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:24.537551: step 3800, loss 0.0499517, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T13:06:25.722309: step 3800, loss 0.22156, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3800

2017-10-10T13:06:27.321971: step 3801, loss 0.0721064, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:27.833816: step 3802, loss 0.19153, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:06:28.461634: step 3803, loss 0.0804476, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:29.052872: step 3804, loss 0.176643, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:29.418848: step 3805, loss 0.0724356, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:29.826010: step 3806, loss 0.131785, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:30.311179: step 3807, loss 0.130251, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:30.836919: step 3808, loss 0.102739, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:31.348873: step 3809, loss 0.162482, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:31.862998: step 3810, loss 0.16129, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:32.465370: step 3811, loss 0.083277, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:32.980976: step 3812, loss 0.135048, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:33.508950: step 3813, loss 0.164849, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:34.024982: step 3814, loss 0.0808896, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:34.565066: step 3815, loss 0.106454, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:35.044859: step 3816, loss 0.0852315, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:35.664271: step 3817, loss 0.0661356, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:36.187607: step 3818, loss 0.188739, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:36.741130: step 3819, loss 0.119704, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:37.242852: step 3820, loss 0.0600185, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:37.856826: step 3821, loss 0.15043, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:38.340844: step 3822, loss 0.106662, acc 0.960784, learning_rate 0.000100001
2017-10-10T13:06:38.911439: step 3823, loss 0.128264, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:39.457095: step 3824, loss 0.231998, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:39.960887: step 3825, loss 0.0776257, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:40.446253: step 3826, loss 0.0587985, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:40.977523: step 3827, loss 0.168891, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:41.469170: step 3828, loss 0.0400811, acc 1, learning_rate 0.000100001
2017-10-10T13:06:42.008737: step 3829, loss 0.0607915, acc 1, learning_rate 0.000100001
2017-10-10T13:06:42.579652: step 3830, loss 0.039724, acc 1, learning_rate 0.000100001
2017-10-10T13:06:43.192884: step 3831, loss 0.0482308, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:43.633951: step 3832, loss 0.0505965, acc 1, learning_rate 0.000100001
2017-10-10T13:06:44.180955: step 3833, loss 0.0467209, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:44.645300: step 3834, loss 0.118566, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:45.048941: step 3835, loss 0.101291, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:45.473481: step 3836, loss 0.0954778, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:45.904815: step 3837, loss 0.079981, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:46.452863: step 3838, loss 0.113944, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:47.035606: step 3839, loss 0.0613097, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:47.608970: step 3840, loss 0.0644953, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T13:06:48.701729: step 3840, loss 0.222943, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3840

2017-10-10T13:06:50.465792: step 3841, loss 0.063273, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:51.015441: step 3842, loss 0.274426, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:06:51.551647: step 3843, loss 0.168131, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:06:52.102929: step 3844, loss 0.0711233, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:52.456005: step 3845, loss 0.0849336, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:06:52.800742: step 3846, loss 0.121127, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:53.326652: step 3847, loss 0.152879, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:53.873288: step 3848, loss 0.0977954, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:54.372920: step 3849, loss 0.113414, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:54.904693: step 3850, loss 0.105367, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:55.444907: step 3851, loss 0.12307, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:55.984987: step 3852, loss 0.0901954, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:56.472870: step 3853, loss 0.159072, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:56.952927: step 3854, loss 0.141838, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:57.441280: step 3855, loss 0.100907, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:06:57.872956: step 3856, loss 0.164725, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:06:58.446985: step 3857, loss 0.106158, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:58.972948: step 3858, loss 0.116374, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:06:59.496851: step 3859, loss 0.108328, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:00.055779: step 3860, loss 0.035517, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:00.588892: step 3861, loss 0.168641, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:01.080868: step 3862, loss 0.060656, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:01.548865: step 3863, loss 0.120988, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:02.061039: step 3864, loss 0.220032, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:07:02.564861: step 3865, loss 0.0739466, acc 1, learning_rate 0.000100001
2017-10-10T13:07:03.053399: step 3866, loss 0.0517621, acc 1, learning_rate 0.000100001
2017-10-10T13:07:03.607451: step 3867, loss 0.175695, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:07:04.056952: step 3868, loss 0.126051, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:04.592849: step 3869, loss 0.068101, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:05.117114: step 3870, loss 0.0933065, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:05.845789: step 3871, loss 0.118604, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:06.252891: step 3872, loss 0.20277, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:06.670590: step 3873, loss 0.0938951, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:07.140933: step 3874, loss 0.130217, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:07.793500: step 3875, loss 0.107538, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:08.225466: step 3876, loss 0.101212, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:08.696956: step 3877, loss 0.0657264, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:09.232886: step 3878, loss 0.0651143, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:09.690609: step 3879, loss 0.178466, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:10.186824: step 3880, loss 0.0782855, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T13:07:11.276254: step 3880, loss 0.224065, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3880

2017-10-10T13:07:12.840908: step 3881, loss 0.0905524, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:13.411934: step 3882, loss 0.1135, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:13.967875: step 3883, loss 0.209866, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:07:14.561034: step 3884, loss 0.132341, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:15.123968: step 3885, loss 0.199547, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:15.579876: step 3886, loss 0.0889289, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:16.034616: step 3887, loss 0.0582869, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:16.509098: step 3888, loss 0.0649782, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:17.060947: step 3889, loss 0.0888506, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:17.620933: step 3890, loss 0.0546301, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:18.125329: step 3891, loss 0.0548386, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:18.600270: step 3892, loss 0.141244, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:19.096470: step 3893, loss 0.0399313, acc 1, learning_rate 0.000100001
2017-10-10T13:07:19.568891: step 3894, loss 0.0934975, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:20.066688: step 3895, loss 0.115649, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:20.966279: step 3896, loss 0.152027, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:21.505602: step 3897, loss 0.157166, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:07:22.004917: step 3898, loss 0.220831, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:07:22.476664: step 3899, loss 0.0983915, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:23.011643: step 3900, loss 0.190439, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:23.596070: step 3901, loss 0.122508, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:24.068382: step 3902, loss 0.106188, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:24.583731: step 3903, loss 0.127076, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:25.136904: step 3904, loss 0.0903087, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:25.636844: step 3905, loss 0.0830664, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:26.201839: step 3906, loss 0.126008, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:26.678436: step 3907, loss 0.203719, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:07:27.144883: step 3908, loss 0.164412, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:27.672961: step 3909, loss 0.0280531, acc 1, learning_rate 0.000100001
2017-10-10T13:07:28.232840: step 3910, loss 0.0651253, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:28.872261: step 3911, loss 0.205369, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:07:29.355625: step 3912, loss 0.0739354, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:29.789655: step 3913, loss 0.0827039, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:30.236890: step 3914, loss 0.124636, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:30.857393: step 3915, loss 0.208594, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:07:31.286191: step 3916, loss 0.062972, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:31.718184: step 3917, loss 0.0675941, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:32.237405: step 3918, loss 0.0752515, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:32.724169: step 3919, loss 0.0983758, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:33.188286: step 3920, loss 0.0635819, acc 0.980392, learning_rate 0.000100001

Evaluation:
2017-10-10T13:07:34.330845: step 3920, loss 0.220126, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3920

2017-10-10T13:07:35.944880: step 3921, loss 0.094985, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:36.434819: step 3922, loss 0.09388, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:36.992889: step 3923, loss 0.0791061, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:37.496844: step 3924, loss 0.134093, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:38.088854: step 3925, loss 0.113727, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:38.673833: step 3926, loss 0.141061, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:39.125097: step 3927, loss 0.18813, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:07:39.580909: step 3928, loss 0.144547, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:39.996825: step 3929, loss 0.104188, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:40.560951: step 3930, loss 0.127859, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:41.101724: step 3931, loss 0.0684089, acc 1, learning_rate 0.000100001
2017-10-10T13:07:41.668651: step 3932, loss 0.112183, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:42.217788: step 3933, loss 0.0491838, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:07:42.772025: step 3934, loss 0.0828329, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:07:43.337125: step 3935, loss 0.186676, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:07:43.864880: step 3936, loss 0.150306, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:07:44.350696: step 3937, loss 0.0666797, acc 0.96875, learning_rate 0.0001
2017-10-10T13:07:44.880131: step 3938, loss 0.125245, acc 0.921875, learning_rate 0.0001
2017-10-10T13:07:45.402706: step 3939, loss 0.174181, acc 0.9375, learning_rate 0.0001
2017-10-10T13:07:45.938110: step 3940, loss 0.0738536, acc 0.953125, learning_rate 0.0001
2017-10-10T13:07:46.494966: step 3941, loss 0.0670199, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:47.080893: step 3942, loss 0.231179, acc 0.921875, learning_rate 0.0001
2017-10-10T13:07:47.621020: step 3943, loss 0.105184, acc 0.96875, learning_rate 0.0001
2017-10-10T13:07:48.174107: step 3944, loss 0.0760364, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:48.688943: step 3945, loss 0.0848347, acc 0.953125, learning_rate 0.0001
2017-10-10T13:07:49.205696: step 3946, loss 0.0663735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:49.780212: step 3947, loss 0.0887383, acc 0.96875, learning_rate 0.0001
2017-10-10T13:07:50.337158: step 3948, loss 0.1122, acc 0.953125, learning_rate 0.0001
2017-10-10T13:07:50.720127: step 3949, loss 0.0776131, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:51.264463: step 3950, loss 0.0759978, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:51.765592: step 3951, loss 0.119085, acc 0.96875, learning_rate 0.0001
2017-10-10T13:07:52.195131: step 3952, loss 0.0928975, acc 0.96875, learning_rate 0.0001
2017-10-10T13:07:52.630246: step 3953, loss 0.0312245, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:53.175952: step 3954, loss 0.102647, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:53.717229: step 3955, loss 0.15449, acc 0.9375, learning_rate 0.0001
2017-10-10T13:07:54.288868: step 3956, loss 0.0506702, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:54.788854: step 3957, loss 0.130725, acc 0.953125, learning_rate 0.0001
2017-10-10T13:07:55.199473: step 3958, loss 0.0702726, acc 0.984375, learning_rate 0.0001
2017-10-10T13:07:55.628849: step 3959, loss 0.0970328, acc 0.953125, learning_rate 0.0001
2017-10-10T13:07:56.128861: step 3960, loss 0.069185, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:07:57.145002: step 3960, loss 0.219817, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-3960

2017-10-10T13:07:58.911791: step 3961, loss 0.222191, acc 0.9375, learning_rate 0.0001
2017-10-10T13:07:59.425307: step 3962, loss 0.0433576, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:00.032909: step 3963, loss 0.0711426, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:00.618858: step 3964, loss 0.146797, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:01.183800: step 3965, loss 0.121488, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:01.724177: step 3966, loss 0.149821, acc 0.921875, learning_rate 0.0001
2017-10-10T13:08:02.361536: step 3967, loss 0.0822274, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:02.813075: step 3968, loss 0.0461853, acc 1, learning_rate 0.0001
2017-10-10T13:08:03.236860: step 3969, loss 0.0906638, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:03.745069: step 3970, loss 0.0944112, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:04.317158: step 3971, loss 0.0972566, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:04.825870: step 3972, loss 0.157666, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:05.395800: step 3973, loss 0.0794749, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:05.964414: step 3974, loss 0.140679, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:06.479342: step 3975, loss 0.112837, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:06.980928: step 3976, loss 0.0715827, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:07.480859: step 3977, loss 0.0857286, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:07.999214: step 3978, loss 0.0619962, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:08.449066: step 3979, loss 0.0544452, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:08.939265: step 3980, loss 0.14181, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:09.412297: step 3981, loss 0.081501, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:09.886941: step 3982, loss 0.0501765, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:10.394373: step 3983, loss 0.115098, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:10.920082: step 3984, loss 0.132359, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:11.459991: step 3985, loss 0.102919, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:12.021456: step 3986, loss 0.116332, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:12.572870: step 3987, loss 0.0540875, acc 1, learning_rate 0.0001
2017-10-10T13:08:13.119608: step 3988, loss 0.158712, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:13.587055: step 3989, loss 0.198951, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:14.148279: step 3990, loss 0.104555, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:14.618224: step 3991, loss 0.111483, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:15.140907: step 3992, loss 0.09192, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:15.588482: step 3993, loss 0.158932, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:16.147002: step 3994, loss 0.159212, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:16.716839: step 3995, loss 0.108028, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:17.334985: step 3996, loss 0.108252, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:17.798874: step 3997, loss 0.126313, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:18.254701: step 3998, loss 0.0765061, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:18.786887: step 3999, loss 0.08825, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:19.336532: step 4000, loss 0.0458799, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:08:20.507066: step 4000, loss 0.223181, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4000

2017-10-10T13:08:21.988412: step 4001, loss 0.090396, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:22.552844: step 4002, loss 0.072494, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:23.102988: step 4003, loss 0.0557436, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:23.627139: step 4004, loss 0.0970746, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:24.156951: step 4005, loss 0.113576, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:24.676836: step 4006, loss 0.0984967, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:25.208853: step 4007, loss 0.179157, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:25.804889: step 4008, loss 0.138957, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:26.290038: step 4009, loss 0.0463638, acc 1, learning_rate 0.0001
2017-10-10T13:08:26.721974: step 4010, loss 0.154515, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:27.155596: step 4011, loss 0.111173, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:27.561115: step 4012, loss 0.122448, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:28.208873: step 4013, loss 0.0945535, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:28.738171: step 4014, loss 0.0865161, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:29.289049: step 4015, loss 0.226349, acc 0.921875, learning_rate 0.0001
2017-10-10T13:08:29.820852: step 4016, loss 0.0871407, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:30.416970: step 4017, loss 0.0911094, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:30.892960: step 4018, loss 0.0538069, acc 1, learning_rate 0.0001
2017-10-10T13:08:31.460750: step 4019, loss 0.0334045, acc 1, learning_rate 0.0001
2017-10-10T13:08:32.001445: step 4020, loss 0.0885785, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:32.512164: step 4021, loss 0.168939, acc 0.921875, learning_rate 0.0001
2017-10-10T13:08:33.040851: step 4022, loss 0.331593, acc 0.921875, learning_rate 0.0001
2017-10-10T13:08:33.990380: step 4023, loss 0.0231581, acc 1, learning_rate 0.0001
2017-10-10T13:08:34.500819: step 4024, loss 0.0608989, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:34.988891: step 4025, loss 0.0786658, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:35.521767: step 4026, loss 0.118207, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:35.849124: step 4027, loss 0.152096, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:36.315788: step 4028, loss 0.0751161, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:36.880308: step 4029, loss 0.0341069, acc 1, learning_rate 0.0001
2017-10-10T13:08:37.460874: step 4030, loss 0.179413, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:38.045509: step 4031, loss 0.123171, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:38.492838: step 4032, loss 0.0697978, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:38.928362: step 4033, loss 0.0458744, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:39.473364: step 4034, loss 0.0696446, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:40.044987: step 4035, loss 0.082156, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:40.652970: step 4036, loss 0.125176, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:41.196488: step 4037, loss 0.157407, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:41.604832: step 4038, loss 0.0277364, acc 1, learning_rate 0.0001
2017-10-10T13:08:42.053272: step 4039, loss 0.101856, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:42.530634: step 4040, loss 0.164054, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:08:43.674084: step 4040, loss 0.22202, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4040

2017-10-10T13:08:45.201038: step 4041, loss 0.0455777, acc 1, learning_rate 0.0001
2017-10-10T13:08:45.748872: step 4042, loss 0.0550874, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:46.308911: step 4043, loss 0.0913402, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:46.788899: step 4044, loss 0.0611814, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:47.300824: step 4045, loss 0.0979838, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:47.812942: step 4046, loss 0.162974, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:48.340890: step 4047, loss 0.174868, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:48.795680: step 4048, loss 0.113483, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:49.393122: step 4049, loss 0.0530835, acc 1, learning_rate 0.0001
2017-10-10T13:08:49.844670: step 4050, loss 0.194331, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:50.306723: step 4051, loss 0.104647, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:50.706788: step 4052, loss 0.095192, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:51.200172: step 4053, loss 0.0485828, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:51.717460: step 4054, loss 0.146155, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:52.218020: step 4055, loss 0.132693, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:52.727898: step 4056, loss 0.0972892, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:53.273041: step 4057, loss 0.0983731, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:53.780954: step 4058, loss 0.170093, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:54.340819: step 4059, loss 0.138927, acc 0.9375, learning_rate 0.0001
2017-10-10T13:08:54.866222: step 4060, loss 0.0930585, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:55.400201: step 4061, loss 0.19947, acc 0.90625, learning_rate 0.0001
2017-10-10T13:08:55.907627: step 4062, loss 0.137191, acc 0.921875, learning_rate 0.0001
2017-10-10T13:08:56.418478: step 4063, loss 0.0405854, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:56.917511: step 4064, loss 0.084711, acc 0.96875, learning_rate 0.0001
2017-10-10T13:08:57.437279: step 4065, loss 0.0294039, acc 1, learning_rate 0.0001
2017-10-10T13:08:57.984959: step 4066, loss 0.0669835, acc 1, learning_rate 0.0001
2017-10-10T13:08:58.520952: step 4067, loss 0.218132, acc 0.953125, learning_rate 0.0001
2017-10-10T13:08:58.985059: step 4068, loss 0.0601303, acc 0.984375, learning_rate 0.0001
2017-10-10T13:08:59.398856: step 4069, loss 0.0437626, acc 1, learning_rate 0.0001
2017-10-10T13:08:59.988966: step 4070, loss 0.0591361, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:00.592870: step 4071, loss 0.269247, acc 0.90625, learning_rate 0.0001
2017-10-10T13:09:01.007412: step 4072, loss 0.151027, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:01.404856: step 4073, loss 0.0818983, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:01.811305: step 4074, loss 0.150331, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:02.305224: step 4075, loss 0.164944, acc 0.921875, learning_rate 0.0001
2017-10-10T13:09:02.769093: step 4076, loss 0.0912875, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:03.241074: step 4077, loss 0.0251875, acc 1, learning_rate 0.0001
2017-10-10T13:09:03.816693: step 4078, loss 0.123558, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:04.384902: step 4079, loss 0.112982, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:04.820863: step 4080, loss 0.119784, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:09:05.768909: step 4080, loss 0.221916, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4080

2017-10-10T13:09:07.480187: step 4081, loss 0.15714, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:07.984952: step 4082, loss 0.049028, acc 1, learning_rate 0.0001
2017-10-10T13:09:08.476536: step 4083, loss 0.0587305, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:09.150172: step 4084, loss 0.0680848, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:09.608092: step 4085, loss 0.117688, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:10.096988: step 4086, loss 0.0485446, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:10.638074: step 4087, loss 0.0439708, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:11.123515: step 4088, loss 0.0584366, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:11.584571: step 4089, loss 0.148675, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:12.145090: step 4090, loss 0.0737427, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:12.721235: step 4091, loss 0.184589, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:13.229666: step 4092, loss 0.0979958, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:13.669019: step 4093, loss 0.104978, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:14.094361: step 4094, loss 0.0698152, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:14.603858: step 4095, loss 0.167809, acc 0.921875, learning_rate 0.0001
2017-10-10T13:09:15.104849: step 4096, loss 0.10806, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:15.628437: step 4097, loss 0.0646809, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:16.180933: step 4098, loss 0.179135, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:16.764978: step 4099, loss 0.0535231, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:17.282458: step 4100, loss 0.0387726, acc 1, learning_rate 0.0001
2017-10-10T13:09:17.847927: step 4101, loss 0.156797, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:18.416989: step 4102, loss 0.0826902, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:18.946194: step 4103, loss 0.066349, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:19.490694: step 4104, loss 0.0964837, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:20.061085: step 4105, loss 0.173785, acc 0.921875, learning_rate 0.0001
2017-10-10T13:09:20.603273: step 4106, loss 0.0992267, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:21.174560: step 4107, loss 0.0599523, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:21.709172: step 4108, loss 0.189209, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:22.191982: step 4109, loss 0.0835964, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:22.726766: step 4110, loss 0.0857626, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:23.268996: step 4111, loss 0.125413, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:23.748832: step 4112, loss 0.12241, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:24.184877: step 4113, loss 0.0609323, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:24.664908: step 4114, loss 0.101561, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:25.196912: step 4115, loss 0.138132, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:25.605046: step 4116, loss 0.128581, acc 0.960784, learning_rate 0.0001
2017-10-10T13:09:26.132929: step 4117, loss 0.0459465, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:26.745003: step 4118, loss 0.0575905, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:27.372864: step 4119, loss 0.0443439, acc 1, learning_rate 0.0001
2017-10-10T13:09:27.829605: step 4120, loss 0.085103, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:09:28.938721: step 4120, loss 0.218658, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4120

2017-10-10T13:09:30.568838: step 4121, loss 0.115121, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:31.127468: step 4122, loss 0.0902817, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:31.638288: step 4123, loss 0.12921, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:32.252870: step 4124, loss 0.0853501, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:32.784877: step 4125, loss 0.126104, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:33.376994: step 4126, loss 0.0634901, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:33.896986: step 4127, loss 0.12165, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:34.440316: step 4128, loss 0.258348, acc 0.921875, learning_rate 0.0001
2017-10-10T13:09:35.030075: step 4129, loss 0.109082, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:35.676989: step 4130, loss 0.129427, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:36.144845: step 4131, loss 0.106209, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:36.556903: step 4132, loss 0.123047, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:36.983508: step 4133, loss 0.0674339, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:37.505119: step 4134, loss 0.0603642, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:38.048914: step 4135, loss 0.0521235, acc 1, learning_rate 0.0001
2017-10-10T13:09:38.544876: step 4136, loss 0.0580257, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:39.011720: step 4137, loss 0.115547, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:39.576834: step 4138, loss 0.116598, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:40.118627: step 4139, loss 0.118025, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:40.648898: step 4140, loss 0.158594, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:41.138025: step 4141, loss 0.0518963, acc 1, learning_rate 0.0001
2017-10-10T13:09:41.604949: step 4142, loss 0.0979242, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:42.122785: step 4143, loss 0.076836, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:42.648838: step 4144, loss 0.0503863, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:43.155724: step 4145, loss 0.0864979, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:43.662524: step 4146, loss 0.106468, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:44.172888: step 4147, loss 0.0733138, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:44.721756: step 4148, loss 0.115646, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:45.261249: step 4149, loss 0.066905, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:45.873121: step 4150, loss 0.057919, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:46.323299: step 4151, loss 0.0935507, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:46.792827: step 4152, loss 0.108503, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:47.350733: step 4153, loss 0.106583, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:47.960992: step 4154, loss 0.0842084, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:48.425143: step 4155, loss 0.116779, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:49.056961: step 4156, loss 0.0314917, acc 1, learning_rate 0.0001
2017-10-10T13:09:49.585895: step 4157, loss 0.0803777, acc 0.9375, learning_rate 0.0001
2017-10-10T13:09:50.126779: step 4158, loss 0.109297, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:50.592864: step 4159, loss 0.158755, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:51.055730: step 4160, loss 0.165475, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:09:52.213434: step 4160, loss 0.220533, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4160

2017-10-10T13:09:53.682585: step 4161, loss 0.142821, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:54.235455: step 4162, loss 0.0989233, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:54.784861: step 4163, loss 0.122023, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:55.329908: step 4164, loss 0.0677836, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:55.867449: step 4165, loss 0.0631787, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:56.404445: step 4166, loss 0.087589, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:56.981130: step 4167, loss 0.135406, acc 0.96875, learning_rate 0.0001
2017-10-10T13:09:57.540797: step 4168, loss 0.0854096, acc 0.953125, learning_rate 0.0001
2017-10-10T13:09:58.079625: step 4169, loss 0.233247, acc 0.921875, learning_rate 0.0001
2017-10-10T13:09:58.667013: step 4170, loss 0.0356842, acc 1, learning_rate 0.0001
2017-10-10T13:09:59.267570: step 4171, loss 0.0793208, acc 0.984375, learning_rate 0.0001
2017-10-10T13:09:59.742572: step 4172, loss 0.106252, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:00.208919: step 4173, loss 0.140248, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:00.756854: step 4174, loss 0.151523, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:01.304849: step 4175, loss 0.089801, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:01.880492: step 4176, loss 0.153424, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:02.422766: step 4177, loss 0.0408432, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:02.939681: step 4178, loss 0.0454227, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:03.424859: step 4179, loss 0.144171, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:03.912819: step 4180, loss 0.171212, acc 0.9375, learning_rate 0.0001
2017-10-10T13:10:04.464982: step 4181, loss 0.0998495, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:04.984881: step 4182, loss 0.13952, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:05.508886: step 4183, loss 0.0973695, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:05.921090: step 4184, loss 0.0495198, acc 1, learning_rate 0.0001
2017-10-10T13:10:06.378591: step 4185, loss 0.125983, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:06.910234: step 4186, loss 0.0289058, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:07.504900: step 4187, loss 0.0924873, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:08.034667: step 4188, loss 0.085947, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:08.537183: step 4189, loss 0.081007, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:09.140837: step 4190, loss 0.237577, acc 0.90625, learning_rate 0.0001
2017-10-10T13:10:09.648282: step 4191, loss 0.0931413, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:10.109001: step 4192, loss 0.170205, acc 0.890625, learning_rate 0.0001
2017-10-10T13:10:10.569527: step 4193, loss 0.163225, acc 0.921875, learning_rate 0.0001
2017-10-10T13:10:11.131762: step 4194, loss 0.104474, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:11.680436: step 4195, loss 0.135987, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:12.211427: step 4196, loss 0.0583933, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:12.743514: step 4197, loss 0.127916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:13.380972: step 4198, loss 0.0644454, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:13.871134: step 4199, loss 0.115767, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:14.341045: step 4200, loss 0.182778, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:10:15.480966: step 4200, loss 0.219247, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4200

2017-10-10T13:10:17.033744: step 4201, loss 0.0720574, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:17.470376: step 4202, loss 0.102664, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:17.936719: step 4203, loss 0.0546047, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:18.457916: step 4204, loss 0.0506912, acc 1, learning_rate 0.0001
2017-10-10T13:10:19.004851: step 4205, loss 0.26505, acc 0.921875, learning_rate 0.0001
2017-10-10T13:10:19.544900: step 4206, loss 0.101692, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:20.071779: step 4207, loss 0.0764446, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:20.602498: step 4208, loss 0.0610725, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:21.143965: step 4209, loss 0.113271, acc 0.9375, learning_rate 0.0001
2017-10-10T13:10:21.698467: step 4210, loss 0.133266, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:22.193824: step 4211, loss 0.0584729, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:22.705035: step 4212, loss 0.0257089, acc 1, learning_rate 0.0001
2017-10-10T13:10:23.128421: step 4213, loss 0.0278767, acc 1, learning_rate 0.0001
2017-10-10T13:10:23.452824: step 4214, loss 0.105692, acc 0.960784, learning_rate 0.0001
2017-10-10T13:10:23.908868: step 4215, loss 0.0813097, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:24.437102: step 4216, loss 0.0449153, acc 1, learning_rate 0.0001
2017-10-10T13:10:25.026465: step 4217, loss 0.103147, acc 0.9375, learning_rate 0.0001
2017-10-10T13:10:25.586905: step 4218, loss 0.186539, acc 0.921875, learning_rate 0.0001
2017-10-10T13:10:26.129224: step 4219, loss 0.19284, acc 0.921875, learning_rate 0.0001
2017-10-10T13:10:26.683501: step 4220, loss 0.0499323, acc 1, learning_rate 0.0001
2017-10-10T13:10:27.232897: step 4221, loss 0.166304, acc 0.90625, learning_rate 0.0001
2017-10-10T13:10:27.744981: step 4222, loss 0.116047, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:28.345143: step 4223, loss 0.027321, acc 1, learning_rate 0.0001
2017-10-10T13:10:28.887317: step 4224, loss 0.0856664, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:29.432902: step 4225, loss 0.0863482, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:29.967511: step 4226, loss 0.0812372, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:30.551194: step 4227, loss 0.136268, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:31.130678: step 4228, loss 0.15116, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:31.643674: step 4229, loss 0.040233, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:32.181087: step 4230, loss 0.0677277, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:32.769034: step 4231, loss 0.092942, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:33.213393: step 4232, loss 0.0808162, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:33.660579: step 4233, loss 0.0558873, acc 1, learning_rate 0.0001
2017-10-10T13:10:34.139167: step 4234, loss 0.0347124, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:34.688858: step 4235, loss 0.0259529, acc 1, learning_rate 0.0001
2017-10-10T13:10:35.279206: step 4236, loss 0.106718, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:35.771964: step 4237, loss 0.113132, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:36.374357: step 4238, loss 0.182086, acc 0.921875, learning_rate 0.0001
2017-10-10T13:10:36.946792: step 4239, loss 0.0890104, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:37.398636: step 4240, loss 0.119549, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:10:38.484523: step 4240, loss 0.219333, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4240

2017-10-10T13:10:40.214396: step 4241, loss 0.0806787, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:40.676832: step 4242, loss 0.0912138, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:41.182829: step 4243, loss 0.0802034, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:41.652916: step 4244, loss 0.142196, acc 0.9375, learning_rate 0.0001
2017-10-10T13:10:42.125106: step 4245, loss 0.0661372, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:42.644048: step 4246, loss 0.156244, acc 0.921875, learning_rate 0.0001
2017-10-10T13:10:43.119869: step 4247, loss 0.0541404, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:43.571074: step 4248, loss 0.0589102, acc 1, learning_rate 0.0001
2017-10-10T13:10:44.140913: step 4249, loss 0.100393, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:44.681042: step 4250, loss 0.050468, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:45.189117: step 4251, loss 0.171533, acc 0.9375, learning_rate 0.0001
2017-10-10T13:10:45.726526: step 4252, loss 0.0841484, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:46.250030: step 4253, loss 0.0602028, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:46.710776: step 4254, loss 0.181431, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:47.145986: step 4255, loss 0.0610593, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:47.574840: step 4256, loss 0.0847902, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:48.140957: step 4257, loss 0.0856306, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:48.679752: step 4258, loss 0.0463414, acc 1, learning_rate 0.0001
2017-10-10T13:10:49.234813: step 4259, loss 0.066466, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:49.812983: step 4260, loss 0.130653, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:50.333482: step 4261, loss 0.0491914, acc 1, learning_rate 0.0001
2017-10-10T13:10:50.844861: step 4262, loss 0.0810219, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:51.345675: step 4263, loss 0.0924595, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:51.836569: step 4264, loss 0.146423, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:52.388872: step 4265, loss 0.0277828, acc 1, learning_rate 0.0001
2017-10-10T13:10:52.948911: step 4266, loss 0.0935866, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:53.454202: step 4267, loss 0.115276, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:53.964857: step 4268, loss 0.12791, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:54.540850: step 4269, loss 0.165186, acc 0.9375, learning_rate 0.0001
2017-10-10T13:10:55.161771: step 4270, loss 0.0878566, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:55.760373: step 4271, loss 0.093608, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:56.184757: step 4272, loss 0.10402, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:56.604845: step 4273, loss 0.150556, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:57.067387: step 4274, loss 0.104086, acc 0.953125, learning_rate 0.0001
2017-10-10T13:10:57.621122: step 4275, loss 0.0847391, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:58.153093: step 4276, loss 0.0604436, acc 0.984375, learning_rate 0.0001
2017-10-10T13:10:58.604739: step 4277, loss 0.200385, acc 0.9375, learning_rate 0.0001
2017-10-10T13:10:59.205891: step 4278, loss 0.0971135, acc 0.96875, learning_rate 0.0001
2017-10-10T13:10:59.777105: step 4279, loss 0.0881862, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:00.214153: step 4280, loss 0.170718, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:11:01.381750: step 4280, loss 0.218752, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4280

2017-10-10T13:11:02.876849: step 4281, loss 0.0985497, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:03.448878: step 4282, loss 0.059382, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:03.988950: step 4283, loss 0.125139, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:04.595063: step 4284, loss 0.113801, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:05.124928: step 4285, loss 0.0821591, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:05.629202: step 4286, loss 0.098996, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:06.185292: step 4287, loss 0.25346, acc 0.90625, learning_rate 0.0001
2017-10-10T13:11:06.663835: step 4288, loss 0.070257, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:07.104618: step 4289, loss 0.0527261, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:07.604581: step 4290, loss 0.0454052, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:08.127316: step 4291, loss 0.100891, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:08.609697: step 4292, loss 0.0918126, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:09.160412: step 4293, loss 0.0474166, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:09.784873: step 4294, loss 0.0647814, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:10.193322: step 4295, loss 0.140104, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:10.647891: step 4296, loss 0.0564434, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:11.101084: step 4297, loss 0.053435, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:11.638019: step 4298, loss 0.0484554, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:12.085365: step 4299, loss 0.13243, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:12.593049: step 4300, loss 0.190397, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:13.082650: step 4301, loss 0.0539435, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:13.640921: step 4302, loss 0.0685902, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:14.114448: step 4303, loss 0.0193835, acc 1, learning_rate 0.0001
2017-10-10T13:11:14.584196: step 4304, loss 0.146759, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:15.120939: step 4305, loss 0.0284632, acc 1, learning_rate 0.0001
2017-10-10T13:11:15.624713: step 4306, loss 0.0687618, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:16.153015: step 4307, loss 0.08938, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:16.712502: step 4308, loss 0.105592, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:17.264628: step 4309, loss 0.128613, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:17.768885: step 4310, loss 0.0795632, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:18.324973: step 4311, loss 0.0842383, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:18.835883: step 4312, loss 0.146859, acc 0.960784, learning_rate 0.0001
2017-10-10T13:11:19.297155: step 4313, loss 0.0510487, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:19.792760: step 4314, loss 0.0946085, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:20.281010: step 4315, loss 0.114585, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:20.880966: step 4316, loss 0.0321846, acc 1, learning_rate 0.0001
2017-10-10T13:11:21.397074: step 4317, loss 0.0676535, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:21.879844: step 4318, loss 0.104604, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:22.315880: step 4319, loss 0.188268, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:22.785105: step 4320, loss 0.081982, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:11:23.916913: step 4320, loss 0.221668, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4320

2017-10-10T13:11:25.517917: step 4321, loss 0.219426, acc 0.921875, learning_rate 0.0001
2017-10-10T13:11:26.016895: step 4322, loss 0.0838012, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:26.441177: step 4323, loss 0.134187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:26.989598: step 4324, loss 0.128561, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:27.509236: step 4325, loss 0.145241, acc 0.921875, learning_rate 0.0001
2017-10-10T13:11:28.057229: step 4326, loss 0.0910375, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:28.609237: step 4327, loss 0.0513059, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:29.068919: step 4328, loss 0.0793368, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:29.595856: step 4329, loss 0.0579421, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:30.205143: step 4330, loss 0.109342, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:30.680220: step 4331, loss 0.0829453, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:31.220261: step 4332, loss 0.0721436, acc 1, learning_rate 0.0001
2017-10-10T13:11:31.651969: step 4333, loss 0.0757445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:32.257017: step 4334, loss 0.0584356, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:32.873160: step 4335, loss 0.121427, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:33.317194: step 4336, loss 0.147207, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:33.797484: step 4337, loss 0.104553, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:34.199553: step 4338, loss 0.142161, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:34.702698: step 4339, loss 0.0171085, acc 1, learning_rate 0.0001
2017-10-10T13:11:35.214932: step 4340, loss 0.02607, acc 1, learning_rate 0.0001
2017-10-10T13:11:35.693034: step 4341, loss 0.17483, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:36.172855: step 4342, loss 0.0603163, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:36.707617: step 4343, loss 0.0840365, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:37.258815: step 4344, loss 0.116199, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:37.833012: step 4345, loss 0.0259861, acc 1, learning_rate 0.0001
2017-10-10T13:11:38.398840: step 4346, loss 0.0480839, acc 1, learning_rate 0.0001
2017-10-10T13:11:38.917083: step 4347, loss 0.100888, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:39.412989: step 4348, loss 0.185213, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:39.924887: step 4349, loss 0.23006, acc 0.921875, learning_rate 0.0001
2017-10-10T13:11:40.454868: step 4350, loss 0.102255, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:41.038680: step 4351, loss 0.0640387, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:41.666201: step 4352, loss 0.066942, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:42.240877: step 4353, loss 0.117096, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:42.700878: step 4354, loss 0.0259269, acc 1, learning_rate 0.0001
2017-10-10T13:11:43.098836: step 4355, loss 0.120934, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:43.644855: step 4356, loss 0.14795, acc 0.921875, learning_rate 0.0001
2017-10-10T13:11:44.194425: step 4357, loss 0.181093, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:44.724870: step 4358, loss 0.138997, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:45.277069: step 4359, loss 0.126361, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:45.736863: step 4360, loss 0.0938259, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:11:46.932858: step 4360, loss 0.220798, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4360

2017-10-10T13:11:48.708266: step 4361, loss 0.0841612, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:49.229796: step 4362, loss 0.0626862, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:49.776814: step 4363, loss 0.0900914, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:50.336347: step 4364, loss 0.0365517, acc 1, learning_rate 0.0001
2017-10-10T13:11:50.893142: step 4365, loss 0.145866, acc 0.90625, learning_rate 0.0001
2017-10-10T13:11:51.412864: step 4366, loss 0.159672, acc 0.921875, learning_rate 0.0001
2017-10-10T13:11:51.924887: step 4367, loss 0.0738259, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:52.499312: step 4368, loss 0.158303, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:53.020938: step 4369, loss 0.0712401, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:53.566987: step 4370, loss 0.0980616, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:54.091701: step 4371, loss 0.0424819, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:54.616944: step 4372, loss 0.0441475, acc 1, learning_rate 0.0001
2017-10-10T13:11:55.112955: step 4373, loss 0.107841, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:55.745904: step 4374, loss 0.128968, acc 0.9375, learning_rate 0.0001
2017-10-10T13:11:56.225482: step 4375, loss 0.0820526, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:56.652823: step 4376, loss 0.0901127, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:57.108886: step 4377, loss 0.0660799, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:57.552838: step 4378, loss 0.0581454, acc 0.984375, learning_rate 0.0001
2017-10-10T13:11:58.116942: step 4379, loss 0.104296, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:58.517671: step 4380, loss 0.0892905, acc 0.96875, learning_rate 0.0001
2017-10-10T13:11:59.052846: step 4381, loss 0.104991, acc 0.953125, learning_rate 0.0001
2017-10-10T13:11:59.591530: step 4382, loss 0.160746, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:00.164883: step 4383, loss 0.0864491, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:00.657980: step 4384, loss 0.0419479, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:01.161093: step 4385, loss 0.123018, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:01.676499: step 4386, loss 0.051262, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:02.282960: step 4387, loss 0.162324, acc 0.9375, learning_rate 0.0001
2017-10-10T13:12:02.819334: step 4388, loss 0.124112, acc 0.9375, learning_rate 0.0001
2017-10-10T13:12:03.396973: step 4389, loss 0.0528017, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:03.945102: step 4390, loss 0.0393583, acc 1, learning_rate 0.0001
2017-10-10T13:12:04.495660: step 4391, loss 0.136019, acc 0.921875, learning_rate 0.0001
2017-10-10T13:12:05.068865: step 4392, loss 0.0801327, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:05.480471: step 4393, loss 0.0749096, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:05.907931: step 4394, loss 0.0734314, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:06.436821: step 4395, loss 0.130917, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:06.996885: step 4396, loss 0.155896, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:07.552831: step 4397, loss 0.0988366, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:08.162723: step 4398, loss 0.0997256, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:08.608147: step 4399, loss 0.117609, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:09.068973: step 4400, loss 0.089633, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:12:10.278660: step 4400, loss 0.220312, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4400

2017-10-10T13:12:11.721446: step 4401, loss 0.0924736, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:12.242504: step 4402, loss 0.0918019, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:12.816966: step 4403, loss 0.117471, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:13.337755: step 4404, loss 0.145011, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:13.853598: step 4405, loss 0.0830949, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:14.425082: step 4406, loss 0.0655763, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:14.946149: step 4407, loss 0.0497781, acc 1, learning_rate 0.0001
2017-10-10T13:12:15.444539: step 4408, loss 0.037755, acc 1, learning_rate 0.0001
2017-10-10T13:12:15.972862: step 4409, loss 0.113078, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:16.477060: step 4410, loss 0.212866, acc 0.921569, learning_rate 0.0001
2017-10-10T13:12:17.064847: step 4411, loss 0.141449, acc 0.9375, learning_rate 0.0001
2017-10-10T13:12:17.562805: step 4412, loss 0.0641975, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:18.086867: step 4413, loss 0.0716287, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:18.633342: step 4414, loss 0.118808, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:19.212394: step 4415, loss 0.0852789, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:19.683851: step 4416, loss 0.0758601, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:20.116046: step 4417, loss 0.0374193, acc 1, learning_rate 0.0001
2017-10-10T13:12:20.584983: step 4418, loss 0.174884, acc 0.921875, learning_rate 0.0001
2017-10-10T13:12:21.076969: step 4419, loss 0.0760064, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:21.577114: step 4420, loss 0.0273847, acc 1, learning_rate 0.0001
2017-10-10T13:12:22.063911: step 4421, loss 0.055647, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:22.505143: step 4422, loss 0.0445953, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:23.046986: step 4423, loss 0.115257, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:23.484121: step 4424, loss 0.05189, acc 1, learning_rate 0.0001
2017-10-10T13:12:23.988855: step 4425, loss 0.0245961, acc 1, learning_rate 0.0001
2017-10-10T13:12:24.489263: step 4426, loss 0.0679042, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:25.045062: step 4427, loss 0.0472492, acc 1, learning_rate 0.0001
2017-10-10T13:12:25.549771: step 4428, loss 0.0737709, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:26.027606: step 4429, loss 0.074354, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:26.560350: step 4430, loss 0.1826, acc 0.921875, learning_rate 0.0001
2017-10-10T13:12:27.072869: step 4431, loss 0.154919, acc 0.921875, learning_rate 0.0001
2017-10-10T13:12:27.664688: step 4432, loss 0.119086, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:28.224979: step 4433, loss 0.0596252, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:28.599019: step 4434, loss 0.0749426, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:29.032954: step 4435, loss 0.0972689, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:29.481192: step 4436, loss 0.12042, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:29.993052: step 4437, loss 0.0641376, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:30.449704: step 4438, loss 0.0321016, acc 1, learning_rate 0.0001
2017-10-10T13:12:31.056931: step 4439, loss 0.100659, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:31.624869: step 4440, loss 0.0715885, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:12:32.794947: step 4440, loss 0.223145, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4440

2017-10-10T13:12:34.244954: step 4441, loss 0.0346984, acc 1, learning_rate 0.0001
2017-10-10T13:12:34.638503: step 4442, loss 0.105872, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:35.084816: step 4443, loss 0.026193, acc 1, learning_rate 0.0001
2017-10-10T13:12:35.616795: step 4444, loss 0.130462, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:36.206924: step 4445, loss 0.150702, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:36.722279: step 4446, loss 0.0580146, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:37.217557: step 4447, loss 0.0950439, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:37.745368: step 4448, loss 0.122011, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:38.295003: step 4449, loss 0.0781227, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:38.821091: step 4450, loss 0.0828888, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:39.402092: step 4451, loss 0.0873704, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:40.007309: step 4452, loss 0.148889, acc 0.9375, learning_rate 0.0001
2017-10-10T13:12:40.482493: step 4453, loss 0.0368288, acc 1, learning_rate 0.0001
2017-10-10T13:12:41.010488: step 4454, loss 0.148627, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:41.508848: step 4455, loss 0.0389025, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:42.055887: step 4456, loss 0.135127, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:42.634491: step 4457, loss 0.229309, acc 0.921875, learning_rate 0.0001
2017-10-10T13:12:43.148828: step 4458, loss 0.0696469, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:43.614079: step 4459, loss 0.0850239, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:44.033668: step 4460, loss 0.0337143, acc 1, learning_rate 0.0001
2017-10-10T13:12:44.583644: step 4461, loss 0.130817, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:45.150813: step 4462, loss 0.0490111, acc 1, learning_rate 0.0001
2017-10-10T13:12:45.723419: step 4463, loss 0.0818055, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:46.278739: step 4464, loss 0.0382526, acc 1, learning_rate 0.0001
2017-10-10T13:12:46.840612: step 4465, loss 0.0567063, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:47.374838: step 4466, loss 0.0556333, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:47.910243: step 4467, loss 0.0234764, acc 1, learning_rate 0.0001
2017-10-10T13:12:48.472429: step 4468, loss 0.0522682, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:49.040521: step 4469, loss 0.0747995, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:49.569920: step 4470, loss 0.087041, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:50.116856: step 4471, loss 0.0448141, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:50.641077: step 4472, loss 0.159057, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:51.294791: step 4473, loss 0.175273, acc 0.9375, learning_rate 0.0001
2017-10-10T13:12:51.716516: step 4474, loss 0.0357186, acc 1, learning_rate 0.0001
2017-10-10T13:12:52.165965: step 4475, loss 0.0313784, acc 1, learning_rate 0.0001
2017-10-10T13:12:52.629014: step 4476, loss 0.157516, acc 0.953125, learning_rate 0.0001
2017-10-10T13:12:53.205511: step 4477, loss 0.089126, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:53.709815: step 4478, loss 0.163077, acc 0.9375, learning_rate 0.0001
2017-10-10T13:12:54.308843: step 4479, loss 0.0619756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:54.873173: step 4480, loss 0.0777917, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:12:55.908786: step 4480, loss 0.221534, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4480

2017-10-10T13:12:57.443935: step 4481, loss 0.118325, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:57.949379: step 4482, loss 0.06874, acc 1, learning_rate 0.0001
2017-10-10T13:12:58.400894: step 4483, loss 0.0798455, acc 0.96875, learning_rate 0.0001
2017-10-10T13:12:58.932929: step 4484, loss 0.0662198, acc 0.984375, learning_rate 0.0001
2017-10-10T13:12:59.583827: step 4485, loss 0.132625, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:00.109019: step 4486, loss 0.0501276, acc 1, learning_rate 0.0001
2017-10-10T13:13:00.571965: step 4487, loss 0.141297, acc 0.921875, learning_rate 0.0001
2017-10-10T13:13:01.077187: step 4488, loss 0.100267, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:01.572853: step 4489, loss 0.0800079, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:02.094977: step 4490, loss 0.0846026, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:02.642968: step 4491, loss 0.117836, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:03.173346: step 4492, loss 0.127048, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:03.701748: step 4493, loss 0.0706683, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:04.270622: step 4494, loss 0.114225, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:04.820920: step 4495, loss 0.140722, acc 0.921875, learning_rate 0.0001
2017-10-10T13:13:05.364826: step 4496, loss 0.134748, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:05.956923: step 4497, loss 0.0866906, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:06.526576: step 4498, loss 0.0348254, acc 1, learning_rate 0.0001
2017-10-10T13:13:06.984768: step 4499, loss 0.0908938, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:07.441125: step 4500, loss 0.0402075, acc 1, learning_rate 0.0001
2017-10-10T13:13:07.937656: step 4501, loss 0.11735, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:08.460165: step 4502, loss 0.201779, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:08.967405: step 4503, loss 0.155959, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:09.477018: step 4504, loss 0.0692476, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:10.037033: step 4505, loss 0.0285312, acc 1, learning_rate 0.0001
2017-10-10T13:13:10.529521: step 4506, loss 0.14269, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:11.047710: step 4507, loss 0.104277, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:11.441356: step 4508, loss 0.0320423, acc 1, learning_rate 0.0001
2017-10-10T13:13:11.899783: step 4509, loss 0.0890545, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:12.465021: step 4510, loss 0.0786831, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:12.971231: step 4511, loss 0.0421045, acc 1, learning_rate 0.0001
2017-10-10T13:13:13.485224: step 4512, loss 0.0452283, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:14.027743: step 4513, loss 0.0808286, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:14.633118: step 4514, loss 0.0701298, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:15.069977: step 4515, loss 0.0244986, acc 1, learning_rate 0.0001
2017-10-10T13:13:15.484725: step 4516, loss 0.0558555, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:16.002131: step 4517, loss 0.137709, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:16.456911: step 4518, loss 0.146234, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:16.997096: step 4519, loss 0.0789322, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:17.504884: step 4520, loss 0.0872947, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:13:19.003156: step 4520, loss 0.222236, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4520

2017-10-10T13:13:20.780976: step 4521, loss 0.0582037, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:21.314563: step 4522, loss 0.163827, acc 0.921875, learning_rate 0.0001
2017-10-10T13:13:21.853771: step 4523, loss 0.123944, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:22.468821: step 4524, loss 0.0255754, acc 1, learning_rate 0.0001
2017-10-10T13:13:23.018401: step 4525, loss 0.0785909, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:23.525131: step 4526, loss 0.116628, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:24.028614: step 4527, loss 0.102395, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:24.565033: step 4528, loss 0.0358888, acc 1, learning_rate 0.0001
2017-10-10T13:13:25.108026: step 4529, loss 0.0799217, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:25.592812: step 4530, loss 0.0819414, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:26.112973: step 4531, loss 0.122891, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:26.616756: step 4532, loss 0.0872518, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:27.083122: step 4533, loss 0.163267, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:27.521282: step 4534, loss 0.0912257, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:28.033050: step 4535, loss 0.108127, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:28.611815: step 4536, loss 0.0706742, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:29.231229: step 4537, loss 0.0692348, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:29.704837: step 4538, loss 0.0644537, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:30.160988: step 4539, loss 0.0326467, acc 1, learning_rate 0.0001
2017-10-10T13:13:30.705153: step 4540, loss 0.149322, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:31.230808: step 4541, loss 0.0791332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:31.768985: step 4542, loss 0.0636181, acc 1, learning_rate 0.0001
2017-10-10T13:13:32.309001: step 4543, loss 0.0374658, acc 1, learning_rate 0.0001
2017-10-10T13:13:32.849438: step 4544, loss 0.0885648, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:33.268551: step 4545, loss 0.0810023, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:33.809067: step 4546, loss 0.0529963, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:34.336914: step 4547, loss 0.134535, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:34.908926: step 4548, loss 0.048978, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:35.413072: step 4549, loss 0.0401532, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:35.907784: step 4550, loss 0.0375619, acc 1, learning_rate 0.0001
2017-10-10T13:13:36.552937: step 4551, loss 0.0545262, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:37.152917: step 4552, loss 0.255743, acc 0.90625, learning_rate 0.0001
2017-10-10T13:13:37.584870: step 4553, loss 0.0836008, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:37.992849: step 4554, loss 0.0718489, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:38.365663: step 4555, loss 0.0436053, acc 1, learning_rate 0.0001
2017-10-10T13:13:38.884533: step 4556, loss 0.0541163, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:39.328712: step 4557, loss 0.210136, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:39.841037: step 4558, loss 0.0571496, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:40.396161: step 4559, loss 0.167217, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:40.965263: step 4560, loss 0.126303, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:13:42.080866: step 4560, loss 0.220551, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4560

2017-10-10T13:13:43.546738: step 4561, loss 0.102679, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:44.089261: step 4562, loss 0.0947734, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:44.561078: step 4563, loss 0.110866, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:45.142725: step 4564, loss 0.10249, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:45.673134: step 4565, loss 0.172024, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:46.197711: step 4566, loss 0.0607361, acc 1, learning_rate 0.0001
2017-10-10T13:13:46.686004: step 4567, loss 0.0446767, acc 1, learning_rate 0.0001
2017-10-10T13:13:47.281109: step 4568, loss 0.101593, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:47.797881: step 4569, loss 0.135775, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:48.264959: step 4570, loss 0.0224209, acc 1, learning_rate 0.0001
2017-10-10T13:13:48.813077: step 4571, loss 0.135108, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:49.336927: step 4572, loss 0.0797889, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:49.861201: step 4573, loss 0.0853918, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:50.360991: step 4574, loss 0.0871762, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:50.925123: step 4575, loss 0.112811, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:51.414568: step 4576, loss 0.070691, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:51.989141: step 4577, loss 0.19198, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:52.463177: step 4578, loss 0.119461, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:52.935144: step 4579, loss 0.13764, acc 0.921875, learning_rate 0.0001
2017-10-10T13:13:53.382974: step 4580, loss 0.0917747, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:53.913697: step 4581, loss 0.118002, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:54.439020: step 4582, loss 0.119888, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:54.928852: step 4583, loss 0.110449, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:55.504873: step 4584, loss 0.0503, acc 1, learning_rate 0.0001
2017-10-10T13:13:56.052876: step 4585, loss 0.139234, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:56.575417: step 4586, loss 0.117512, acc 0.9375, learning_rate 0.0001
2017-10-10T13:13:57.158517: step 4587, loss 0.100262, acc 0.953125, learning_rate 0.0001
2017-10-10T13:13:57.700971: step 4588, loss 0.0818422, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:58.213050: step 4589, loss 0.0565753, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:58.714152: step 4590, loss 0.0530782, acc 0.984375, learning_rate 0.0001
2017-10-10T13:13:59.242367: step 4591, loss 0.10125, acc 0.96875, learning_rate 0.0001
2017-10-10T13:13:59.770822: step 4592, loss 0.0351735, acc 1, learning_rate 0.0001
2017-10-10T13:14:00.290215: step 4593, loss 0.0546623, acc 1, learning_rate 0.0001
2017-10-10T13:14:00.686145: step 4594, loss 0.0535752, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:01.122487: step 4595, loss 0.201604, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:01.584532: step 4596, loss 0.0908059, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:02.147708: step 4597, loss 0.122861, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:02.638348: step 4598, loss 0.0740038, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:03.161062: step 4599, loss 0.116526, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:03.696492: step 4600, loss 0.1624, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:14:04.766242: step 4600, loss 0.220726, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4600

2017-10-10T13:14:06.289137: step 4601, loss 0.119906, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:06.778378: step 4602, loss 0.0944392, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:07.265230: step 4603, loss 0.0688521, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:07.795007: step 4604, loss 0.116398, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:08.409243: step 4605, loss 0.0267803, acc 1, learning_rate 0.0001
2017-10-10T13:14:08.845944: step 4606, loss 0.0443983, acc 1, learning_rate 0.0001
2017-10-10T13:14:09.357981: step 4607, loss 0.0979461, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:09.968791: step 4608, loss 0.0433022, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:10.453113: step 4609, loss 0.11033, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:10.961193: step 4610, loss 0.0965899, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:11.599748: step 4611, loss 0.117574, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:12.135861: step 4612, loss 0.0639249, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:12.639402: step 4613, loss 0.028145, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:13.153035: step 4614, loss 0.125967, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:13.696583: step 4615, loss 0.0865876, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:14.121044: step 4616, loss 0.228378, acc 0.90625, learning_rate 0.0001
2017-10-10T13:14:14.698876: step 4617, loss 0.0586245, acc 1, learning_rate 0.0001
2017-10-10T13:14:15.185136: step 4618, loss 0.0859498, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:15.596881: step 4619, loss 0.0932464, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:16.013176: step 4620, loss 0.192749, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:16.558679: step 4621, loss 0.17928, acc 0.921875, learning_rate 0.0001
2017-10-10T13:14:17.099740: step 4622, loss 0.0705685, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:17.582461: step 4623, loss 0.0507644, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:18.070544: step 4624, loss 0.169028, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:18.597176: step 4625, loss 0.0673853, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:19.138896: step 4626, loss 0.192559, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:19.658147: step 4627, loss 0.134274, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:20.146212: step 4628, loss 0.0951771, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:20.608844: step 4629, loss 0.0977744, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:21.161007: step 4630, loss 0.12806, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:21.709107: step 4631, loss 0.0902316, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:22.222816: step 4632, loss 0.0989167, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:22.725096: step 4633, loss 0.16434, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:23.216900: step 4634, loss 0.0937482, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:23.669093: step 4635, loss 0.110958, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:24.028892: step 4636, loss 0.100832, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:24.621106: step 4637, loss 0.0320961, acc 1, learning_rate 0.0001
2017-10-10T13:14:25.136526: step 4638, loss 0.12173, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:25.696197: step 4639, loss 0.0505174, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:26.216843: step 4640, loss 0.102561, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:14:27.511870: step 4640, loss 0.21613, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4640

2017-10-10T13:14:29.229336: step 4641, loss 0.129054, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:29.777187: step 4642, loss 0.0655298, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:30.279674: step 4643, loss 0.0815755, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:30.780953: step 4644, loss 0.0302697, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:31.320856: step 4645, loss 0.155596, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:31.836895: step 4646, loss 0.0675865, acc 1, learning_rate 0.0001
2017-10-10T13:14:32.377131: step 4647, loss 0.121904, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:33.068857: step 4648, loss 0.0336932, acc 1, learning_rate 0.0001
2017-10-10T13:14:33.585737: step 4649, loss 0.0216226, acc 1, learning_rate 0.0001
2017-10-10T13:14:34.120067: step 4650, loss 0.156139, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:34.685115: step 4651, loss 0.137434, acc 0.890625, learning_rate 0.0001
2017-10-10T13:14:35.249748: step 4652, loss 0.112458, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:35.669082: step 4653, loss 0.0553641, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:36.212937: step 4654, loss 0.0326029, acc 1, learning_rate 0.0001
2017-10-10T13:14:36.687572: step 4655, loss 0.0854054, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:37.256976: step 4656, loss 0.119429, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:37.856943: step 4657, loss 0.0814228, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:38.313170: step 4658, loss 0.0611456, acc 1, learning_rate 0.0001
2017-10-10T13:14:38.783531: step 4659, loss 0.111655, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:39.285241: step 4660, loss 0.0904985, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:39.764851: step 4661, loss 0.0815191, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:40.296888: step 4662, loss 0.0964462, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:40.804863: step 4663, loss 0.205616, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:41.296922: step 4664, loss 0.176479, acc 0.9375, learning_rate 0.0001
2017-10-10T13:14:41.825260: step 4665, loss 0.0591835, acc 1, learning_rate 0.0001
2017-10-10T13:14:42.259531: step 4666, loss 0.0833463, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:42.754682: step 4667, loss 0.150044, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:43.182033: step 4668, loss 0.119682, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:43.706189: step 4669, loss 0.0603997, acc 1, learning_rate 0.0001
2017-10-10T13:14:44.241329: step 4670, loss 0.0871841, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:44.768858: step 4671, loss 0.0955956, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:45.410192: step 4672, loss 0.125296, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:46.001114: step 4673, loss 0.268696, acc 0.90625, learning_rate 0.0001
2017-10-10T13:14:46.464857: step 4674, loss 0.113792, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:46.905345: step 4675, loss 0.102272, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:47.472866: step 4676, loss 0.0387037, acc 1, learning_rate 0.0001
2017-10-10T13:14:47.997010: step 4677, loss 0.0764988, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:48.568882: step 4678, loss 0.123518, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:49.151747: step 4679, loss 0.131043, acc 0.921875, learning_rate 0.0001
2017-10-10T13:14:49.756910: step 4680, loss 0.109844, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:14:50.913933: step 4680, loss 0.221193, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4680

2017-10-10T13:14:52.336135: step 4681, loss 0.0586953, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:52.877096: step 4682, loss 0.13971, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:53.421966: step 4683, loss 0.0243165, acc 1, learning_rate 0.0001
2017-10-10T13:14:53.992849: step 4684, loss 0.105937, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:54.609169: step 4685, loss 0.171811, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:55.076859: step 4686, loss 0.0575999, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:55.596214: step 4687, loss 0.0770132, acc 0.953125, learning_rate 0.0001
2017-10-10T13:14:56.153652: step 4688, loss 0.040052, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:56.653877: step 4689, loss 0.0652836, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:57.194874: step 4690, loss 0.0581764, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:57.697833: step 4691, loss 0.0489025, acc 0.984375, learning_rate 0.0001
2017-10-10T13:14:58.216986: step 4692, loss 0.0434683, acc 1, learning_rate 0.0001
2017-10-10T13:14:58.707966: step 4693, loss 0.149762, acc 0.921875, learning_rate 0.0001
2017-10-10T13:14:59.261027: step 4694, loss 0.0788282, acc 0.96875, learning_rate 0.0001
2017-10-10T13:14:59.833538: step 4695, loss 0.0865727, acc 1, learning_rate 0.0001
2017-10-10T13:15:00.343804: step 4696, loss 0.0647319, acc 1, learning_rate 0.0001
2017-10-10T13:15:00.916890: step 4697, loss 0.0952349, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:01.474141: step 4698, loss 0.190733, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:01.940860: step 4699, loss 0.0815051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:02.367291: step 4700, loss 0.133872, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:02.776087: step 4701, loss 0.0713898, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:03.381442: step 4702, loss 0.0628854, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:03.872049: step 4703, loss 0.0234141, acc 1, learning_rate 0.0001
2017-10-10T13:15:04.317145: step 4704, loss 0.0488395, acc 0.980392, learning_rate 0.0001
2017-10-10T13:15:04.828266: step 4705, loss 0.172386, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:05.401001: step 4706, loss 0.0446255, acc 1, learning_rate 0.0001
2017-10-10T13:15:05.961863: step 4707, loss 0.0623168, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:06.487921: step 4708, loss 0.0579629, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:06.969488: step 4709, loss 0.13401, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:07.500922: step 4710, loss 0.0595509, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:08.071081: step 4711, loss 0.047701, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:08.525034: step 4712, loss 0.0596822, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:09.163335: step 4713, loss 0.0715339, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:09.676743: step 4714, loss 0.0660735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:10.131879: step 4715, loss 0.102142, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:10.585167: step 4716, loss 0.0606537, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:11.133646: step 4717, loss 0.133097, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:11.715847: step 4718, loss 0.0545763, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:12.227500: step 4719, loss 0.196901, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:12.644868: step 4720, loss 0.0741355, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:15:13.803150: step 4720, loss 0.21993, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4720

2017-10-10T13:15:15.273286: step 4721, loss 0.0686566, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:15.826175: step 4722, loss 0.108843, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:16.324752: step 4723, loss 0.0670491, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:16.809758: step 4724, loss 0.0874411, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:17.350093: step 4725, loss 0.074531, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:17.956555: step 4726, loss 0.0801343, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:18.475485: step 4727, loss 0.147331, acc 0.921875, learning_rate 0.0001
2017-10-10T13:15:18.999416: step 4728, loss 0.0430667, acc 1, learning_rate 0.0001
2017-10-10T13:15:19.564383: step 4729, loss 0.06319, acc 1, learning_rate 0.0001
2017-10-10T13:15:20.058833: step 4730, loss 0.176536, acc 0.90625, learning_rate 0.0001
2017-10-10T13:15:20.568244: step 4731, loss 0.111724, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:21.028864: step 4732, loss 0.0621734, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:21.520506: step 4733, loss 0.101904, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:22.034069: step 4734, loss 0.0165105, acc 1, learning_rate 0.0001
2017-10-10T13:15:22.452851: step 4735, loss 0.218237, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:22.993162: step 4736, loss 0.0947966, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:23.521969: step 4737, loss 0.12577, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:24.012448: step 4738, loss 0.0866308, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:24.564838: step 4739, loss 0.0636262, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:25.097241: step 4740, loss 0.0464281, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:25.516910: step 4741, loss 0.0640544, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:25.988961: step 4742, loss 0.0745864, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:26.364972: step 4743, loss 0.0900128, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:26.933061: step 4744, loss 0.0872598, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:27.386470: step 4745, loss 0.083375, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:27.881013: step 4746, loss 0.0973572, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:28.412988: step 4747, loss 0.0837752, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:28.932947: step 4748, loss 0.0271348, acc 1, learning_rate 0.0001
2017-10-10T13:15:29.415520: step 4749, loss 0.111781, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:30.036939: step 4750, loss 0.0819667, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:30.563642: step 4751, loss 0.117338, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:31.092889: step 4752, loss 0.0824513, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:31.524973: step 4753, loss 0.0496232, acc 1, learning_rate 0.0001
2017-10-10T13:15:32.120866: step 4754, loss 0.158738, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:32.745099: step 4755, loss 0.0754968, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:33.233194: step 4756, loss 0.194427, acc 0.921875, learning_rate 0.0001
2017-10-10T13:15:33.708998: step 4757, loss 0.137114, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:34.286365: step 4758, loss 0.0846708, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:34.944625: step 4759, loss 0.1699, acc 0.921875, learning_rate 0.0001
2017-10-10T13:15:35.352476: step 4760, loss 0.25657, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:15:36.666052: step 4760, loss 0.221324, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4760

2017-10-10T13:15:38.867560: step 4761, loss 0.0704144, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:39.404777: step 4762, loss 0.080883, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:39.938416: step 4763, loss 0.125628, acc 0.921875, learning_rate 0.0001
2017-10-10T13:15:40.475688: step 4764, loss 0.0539063, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:41.013307: step 4765, loss 0.0573653, acc 1, learning_rate 0.0001
2017-10-10T13:15:41.568914: step 4766, loss 0.073332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:42.128877: step 4767, loss 0.085795, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:42.610618: step 4768, loss 0.118539, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:43.129486: step 4769, loss 0.135677, acc 0.921875, learning_rate 0.0001
2017-10-10T13:15:43.653316: step 4770, loss 0.144888, acc 0.9375, learning_rate 0.0001
2017-10-10T13:15:44.160563: step 4771, loss 0.111702, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:44.690214: step 4772, loss 0.102827, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:45.260404: step 4773, loss 0.0876013, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:45.804287: step 4774, loss 0.0469908, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:46.366193: step 4775, loss 0.0552372, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:47.014310: step 4776, loss 0.0385294, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:47.456337: step 4777, loss 0.0728288, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:48.052941: step 4778, loss 0.0865864, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:48.607181: step 4779, loss 0.0301023, acc 1, learning_rate 0.0001
2017-10-10T13:15:49.064809: step 4780, loss 0.0573635, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:49.537070: step 4781, loss 0.0461598, acc 1, learning_rate 0.0001
2017-10-10T13:15:50.100997: step 4782, loss 0.12255, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:50.609145: step 4783, loss 0.0809699, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:51.202716: step 4784, loss 0.110344, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:51.692891: step 4785, loss 0.14109, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:52.216879: step 4786, loss 0.0912822, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:52.748847: step 4787, loss 0.0646794, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:53.305036: step 4788, loss 0.0825742, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:53.822529: step 4789, loss 0.0582557, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:54.336931: step 4790, loss 0.0825291, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:54.933159: step 4791, loss 0.0480746, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:55.393849: step 4792, loss 0.134135, acc 0.921875, learning_rate 0.0001
2017-10-10T13:15:55.820847: step 4793, loss 0.0790897, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:56.372437: step 4794, loss 0.12055, acc 0.96875, learning_rate 0.0001
2017-10-10T13:15:56.921103: step 4795, loss 0.147372, acc 0.953125, learning_rate 0.0001
2017-10-10T13:15:57.489175: step 4796, loss 0.0556943, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:58.144904: step 4797, loss 0.0994139, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:58.735535: step 4798, loss 0.0628892, acc 0.984375, learning_rate 0.0001
2017-10-10T13:15:59.218924: step 4799, loss 0.0405217, acc 1, learning_rate 0.0001
2017-10-10T13:15:59.705364: step 4800, loss 0.0583187, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:16:01.025215: step 4800, loss 0.214593, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4800

2017-10-10T13:16:02.385020: step 4801, loss 0.100499, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:02.814373: step 4802, loss 0.122895, acc 0.941176, learning_rate 0.0001
2017-10-10T13:16:03.380901: step 4803, loss 0.0518558, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:03.900554: step 4804, loss 0.285744, acc 0.90625, learning_rate 0.0001
2017-10-10T13:16:04.461096: step 4805, loss 0.230562, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:04.949694: step 4806, loss 0.0793192, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:05.501793: step 4807, loss 0.0760272, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:06.061801: step 4808, loss 0.0545878, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:06.606379: step 4809, loss 0.208115, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:07.203542: step 4810, loss 0.0933072, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:07.728951: step 4811, loss 0.0631474, acc 1, learning_rate 0.0001
2017-10-10T13:16:08.235326: step 4812, loss 0.0977693, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:08.776864: step 4813, loss 0.128889, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:09.336840: step 4814, loss 0.0238582, acc 1, learning_rate 0.0001
2017-10-10T13:16:09.872319: step 4815, loss 0.103135, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:10.434421: step 4816, loss 0.146029, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:11.030539: step 4817, loss 0.0665993, acc 1, learning_rate 0.0001
2017-10-10T13:16:11.588772: step 4818, loss 0.166771, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:12.029622: step 4819, loss 0.134979, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:12.447451: step 4820, loss 0.137548, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:12.887591: step 4821, loss 0.0307906, acc 1, learning_rate 0.0001
2017-10-10T13:16:13.389122: step 4822, loss 0.0824379, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:13.838824: step 4823, loss 0.1137, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:14.321011: step 4824, loss 0.107467, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:14.941828: step 4825, loss 0.09048, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:15.387991: step 4826, loss 0.0986168, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:15.920903: step 4827, loss 0.157941, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:16.417003: step 4828, loss 0.09487, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:16.926825: step 4829, loss 0.0517689, acc 1, learning_rate 0.0001
2017-10-10T13:16:17.472239: step 4830, loss 0.0929297, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:18.029621: step 4831, loss 0.111911, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:18.474712: step 4832, loss 0.0555275, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:18.912430: step 4833, loss 0.192715, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:19.365075: step 4834, loss 0.183651, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:19.932748: step 4835, loss 0.0688952, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:20.484832: step 4836, loss 0.136346, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:21.025159: step 4837, loss 0.133971, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:21.596998: step 4838, loss 0.0896195, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:22.152182: step 4839, loss 0.133979, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:22.556986: step 4840, loss 0.176323, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:16:23.696901: step 4840, loss 0.222136, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4840

2017-10-10T13:16:25.216378: step 4841, loss 0.101476, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:25.684853: step 4842, loss 0.062197, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:26.109179: step 4843, loss 0.0919816, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:26.588032: step 4844, loss 0.0360017, acc 1, learning_rate 0.0001
2017-10-10T13:16:27.033255: step 4845, loss 0.118651, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:27.584274: step 4846, loss 0.0610386, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:28.143260: step 4847, loss 0.121417, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:28.605618: step 4848, loss 0.06534, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:29.146570: step 4849, loss 0.103526, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:29.725182: step 4850, loss 0.19102, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:30.281857: step 4851, loss 0.0924726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:30.764707: step 4852, loss 0.050456, acc 1, learning_rate 0.0001
2017-10-10T13:16:31.256876: step 4853, loss 0.187462, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:31.723639: step 4854, loss 0.0631188, acc 1, learning_rate 0.0001
2017-10-10T13:16:32.316964: step 4855, loss 0.0389089, acc 1, learning_rate 0.0001
2017-10-10T13:16:32.827002: step 4856, loss 0.104128, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:33.350866: step 4857, loss 0.0818256, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:33.876965: step 4858, loss 0.0703781, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:34.478490: step 4859, loss 0.0213624, acc 1, learning_rate 0.0001
2017-10-10T13:16:35.037722: step 4860, loss 0.0850299, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:35.483247: step 4861, loss 0.115201, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:35.864813: step 4862, loss 0.204671, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:36.310904: step 4863, loss 0.0655871, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:36.860867: step 4864, loss 0.0720389, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:37.269099: step 4865, loss 0.0629222, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:37.772865: step 4866, loss 0.122912, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:38.345348: step 4867, loss 0.103705, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:38.844914: step 4868, loss 0.0346016, acc 1, learning_rate 0.0001
2017-10-10T13:16:39.384837: step 4869, loss 0.0296353, acc 1, learning_rate 0.0001
2017-10-10T13:16:39.892817: step 4870, loss 0.0647295, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:40.443383: step 4871, loss 0.113337, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:41.010776: step 4872, loss 0.0593475, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:41.523104: step 4873, loss 0.0573321, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:42.088977: step 4874, loss 0.078546, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:42.617862: step 4875, loss 0.165412, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:43.081721: step 4876, loss 0.234622, acc 0.921875, learning_rate 0.0001
2017-10-10T13:16:43.666413: step 4877, loss 0.0269358, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:44.189024: step 4878, loss 0.0620617, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:44.698931: step 4879, loss 0.0514726, acc 1, learning_rate 0.0001
2017-10-10T13:16:45.329716: step 4880, loss 0.151003, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:16:46.508969: step 4880, loss 0.221517, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4880

2017-10-10T13:16:48.217141: step 4881, loss 0.0322918, acc 1, learning_rate 0.0001
2017-10-10T13:16:48.771240: step 4882, loss 0.12195, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:49.306516: step 4883, loss 0.16716, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:49.845138: step 4884, loss 0.158289, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:50.348954: step 4885, loss 0.149906, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:50.852637: step 4886, loss 0.078873, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:51.364900: step 4887, loss 0.0944568, acc 0.9375, learning_rate 0.0001
2017-10-10T13:16:51.884007: step 4888, loss 0.0780334, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:52.393003: step 4889, loss 0.124182, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:52.893136: step 4890, loss 0.060638, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:53.577180: step 4891, loss 0.071425, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:54.086611: step 4892, loss 0.0827145, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:54.633897: step 4893, loss 0.0541646, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:55.109219: step 4894, loss 0.0743571, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:55.658932: step 4895, loss 0.101963, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:56.204937: step 4896, loss 0.0456707, acc 0.984375, learning_rate 0.0001
2017-10-10T13:16:56.792335: step 4897, loss 0.0860116, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:57.304794: step 4898, loss 0.0290214, acc 1, learning_rate 0.0001
2017-10-10T13:16:57.896996: step 4899, loss 0.145884, acc 0.953125, learning_rate 0.0001
2017-10-10T13:16:58.410836: step 4900, loss 0.0549047, acc 0.980392, learning_rate 0.0001
2017-10-10T13:16:58.871607: step 4901, loss 0.113306, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:59.293756: step 4902, loss 0.0855533, acc 0.96875, learning_rate 0.0001
2017-10-10T13:16:59.804892: step 4903, loss 0.0711801, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:00.305050: step 4904, loss 0.0303385, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:00.874589: step 4905, loss 0.165881, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:01.363286: step 4906, loss 0.0311592, acc 1, learning_rate 0.0001
2017-10-10T13:17:01.939936: step 4907, loss 0.174677, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:02.444859: step 4908, loss 0.0446095, acc 1, learning_rate 0.0001
2017-10-10T13:17:03.015171: step 4909, loss 0.207204, acc 0.921875, learning_rate 0.0001
2017-10-10T13:17:03.524862: step 4910, loss 0.0455358, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:04.093067: step 4911, loss 0.0792892, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:04.609036: step 4912, loss 0.102901, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:05.153887: step 4913, loss 0.111217, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:05.604851: step 4914, loss 0.0970743, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:06.085230: step 4915, loss 0.0764793, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:06.642249: step 4916, loss 0.202326, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:07.147671: step 4917, loss 0.115793, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:07.672826: step 4918, loss 0.0824329, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:08.232955: step 4919, loss 0.0523791, acc 1, learning_rate 0.0001
2017-10-10T13:17:08.783879: step 4920, loss 0.0524613, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:17:09.852927: step 4920, loss 0.216728, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4920

2017-10-10T13:17:11.331359: step 4921, loss 0.0651987, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:11.828846: step 4922, loss 0.0807328, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:12.402883: step 4923, loss 0.0892536, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:12.909004: step 4924, loss 0.0801745, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:13.447981: step 4925, loss 0.0567826, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:13.920873: step 4926, loss 0.0513801, acc 1, learning_rate 0.0001
2017-10-10T13:17:14.466450: step 4927, loss 0.0979754, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:15.018317: step 4928, loss 0.100881, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:15.552313: step 4929, loss 0.152918, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:16.048870: step 4930, loss 0.105438, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:16.525046: step 4931, loss 0.201922, acc 0.890625, learning_rate 0.0001
2017-10-10T13:17:16.993099: step 4932, loss 0.106007, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:17.643925: step 4933, loss 0.0820896, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:18.140050: step 4934, loss 0.0628604, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:18.556658: step 4935, loss 0.228895, acc 0.890625, learning_rate 0.0001
2017-10-10T13:17:19.089710: step 4936, loss 0.0388337, acc 1, learning_rate 0.0001
2017-10-10T13:17:19.576930: step 4937, loss 0.120778, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:20.096904: step 4938, loss 0.108317, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:20.585053: step 4939, loss 0.157828, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:21.255850: step 4940, loss 0.0328245, acc 1, learning_rate 0.0001
2017-10-10T13:17:21.809379: step 4941, loss 0.0819991, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:22.264519: step 4942, loss 0.11915, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:22.716225: step 4943, loss 0.0932847, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:23.156265: step 4944, loss 0.115651, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:23.661793: step 4945, loss 0.179381, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:24.140885: step 4946, loss 0.145623, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:24.621006: step 4947, loss 0.112738, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:25.152473: step 4948, loss 0.108822, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:25.634881: step 4949, loss 0.045, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:26.175976: step 4950, loss 0.0497076, acc 1, learning_rate 0.0001
2017-10-10T13:17:26.694764: step 4951, loss 0.130792, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:27.293089: step 4952, loss 0.0658241, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:27.869029: step 4953, loss 0.073077, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:28.362479: step 4954, loss 0.0749621, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:28.826710: step 4955, loss 0.0343404, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:29.280916: step 4956, loss 0.100419, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:29.848364: step 4957, loss 0.0325847, acc 1, learning_rate 0.0001
2017-10-10T13:17:30.396996: step 4958, loss 0.122516, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:30.909599: step 4959, loss 0.12052, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:31.428890: step 4960, loss 0.0637407, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:17:32.716726: step 4960, loss 0.217733, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-4960

2017-10-10T13:17:34.269543: step 4961, loss 0.107536, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:34.793404: step 4962, loss 0.0594511, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:35.219874: step 4963, loss 0.0225625, acc 1, learning_rate 0.0001
2017-10-10T13:17:35.622936: step 4964, loss 0.0266612, acc 1, learning_rate 0.0001
2017-10-10T13:17:36.119827: step 4965, loss 0.0582185, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:36.689660: step 4966, loss 0.0814018, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:37.189027: step 4967, loss 0.0860659, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:37.770713: step 4968, loss 0.0354436, acc 1, learning_rate 0.0001
2017-10-10T13:17:38.292005: step 4969, loss 0.0508819, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:38.799087: step 4970, loss 0.113356, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:39.290000: step 4971, loss 0.0435698, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:39.815929: step 4972, loss 0.044693, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:40.360575: step 4973, loss 0.0665256, acc 1, learning_rate 0.0001
2017-10-10T13:17:40.936617: step 4974, loss 0.119236, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:41.456825: step 4975, loss 0.0946126, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:42.008129: step 4976, loss 0.0454095, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:42.548844: step 4977, loss 0.185513, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:43.108011: step 4978, loss 0.0570594, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:43.666736: step 4979, loss 0.12729, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:44.285060: step 4980, loss 0.119497, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:44.816744: step 4981, loss 0.112463, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:45.332842: step 4982, loss 0.0554941, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:45.753141: step 4983, loss 0.0386605, acc 1, learning_rate 0.0001
2017-10-10T13:17:46.150308: step 4984, loss 0.0818409, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:46.680896: step 4985, loss 0.148218, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:47.165159: step 4986, loss 0.0629266, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:47.696720: step 4987, loss 0.0482664, acc 1, learning_rate 0.0001
2017-10-10T13:17:48.235170: step 4988, loss 0.060716, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:48.784051: step 4989, loss 0.0629351, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:49.273379: step 4990, loss 0.104931, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:49.708800: step 4991, loss 0.10603, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:50.272734: step 4992, loss 0.188875, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:50.834318: step 4993, loss 0.0956146, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:51.375532: step 4994, loss 0.116399, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:51.772922: step 4995, loss 0.0516566, acc 1, learning_rate 0.0001
2017-10-10T13:17:52.204829: step 4996, loss 0.148163, acc 0.953125, learning_rate 0.0001
2017-10-10T13:17:52.656161: step 4997, loss 0.124625, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:53.088874: step 4998, loss 0.117496, acc 0.941176, learning_rate 0.0001
2017-10-10T13:17:53.563893: step 4999, loss 0.0373055, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:54.078839: step 5000, loss 0.0529593, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:17:55.389683: step 5000, loss 0.217999, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5000

2017-10-10T13:17:57.146614: step 5001, loss 0.177395, acc 0.9375, learning_rate 0.0001
2017-10-10T13:17:57.708862: step 5002, loss 0.0498376, acc 1, learning_rate 0.0001
2017-10-10T13:17:58.168317: step 5003, loss 0.0517728, acc 1, learning_rate 0.0001
2017-10-10T13:17:58.696968: step 5004, loss 0.0984524, acc 0.96875, learning_rate 0.0001
2017-10-10T13:17:59.273979: step 5005, loss 0.0398086, acc 0.984375, learning_rate 0.0001
2017-10-10T13:17:59.788869: step 5006, loss 0.0407081, acc 1, learning_rate 0.0001
2017-10-10T13:18:00.277465: step 5007, loss 0.0808473, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:00.794194: step 5008, loss 0.11715, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:01.237097: step 5009, loss 0.123046, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:01.697331: step 5010, loss 0.0459271, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:02.204953: step 5011, loss 0.10971, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:02.728872: step 5012, loss 0.06467, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:03.258396: step 5013, loss 0.040955, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:03.769499: step 5014, loss 0.131274, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:04.315465: step 5015, loss 0.0863782, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:04.831168: step 5016, loss 0.0255092, acc 1, learning_rate 0.0001
2017-10-10T13:18:05.381358: step 5017, loss 0.14871, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:05.917407: step 5018, loss 0.0948087, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:06.489143: step 5019, loss 0.126197, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:07.008909: step 5020, loss 0.151905, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:07.522202: step 5021, loss 0.259086, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:07.996956: step 5022, loss 0.050648, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:08.485852: step 5023, loss 0.0422187, acc 1, learning_rate 0.0001
2017-10-10T13:18:08.925495: step 5024, loss 0.046059, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:09.401144: step 5025, loss 0.0777822, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:09.911308: step 5026, loss 0.091028, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:10.393197: step 5027, loss 0.152974, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:10.827593: step 5028, loss 0.0367114, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:11.401022: step 5029, loss 0.138508, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:11.881033: step 5030, loss 0.12595, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:12.419455: step 5031, loss 0.0913213, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:12.976852: step 5032, loss 0.0323864, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:13.528941: step 5033, loss 0.117531, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:14.156878: step 5034, loss 0.0985751, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:14.772983: step 5035, loss 0.0697289, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:15.213880: step 5036, loss 0.0930162, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:15.635424: step 5037, loss 0.0766452, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:16.120444: step 5038, loss 0.133503, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:16.585634: step 5039, loss 0.134473, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:17.124799: step 5040, loss 0.0897841, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:18:18.374091: step 5040, loss 0.21927, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5040

2017-10-10T13:18:19.800713: step 5041, loss 0.087167, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:20.232584: step 5042, loss 0.0542246, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:20.751042: step 5043, loss 0.0529794, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:21.286851: step 5044, loss 0.0345123, acc 1, learning_rate 0.0001
2017-10-10T13:18:21.828917: step 5045, loss 0.0587663, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:22.389960: step 5046, loss 0.048278, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:22.972908: step 5047, loss 0.110197, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:23.517716: step 5048, loss 0.0424756, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:24.073847: step 5049, loss 0.08349, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:24.632491: step 5050, loss 0.0455159, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:25.160705: step 5051, loss 0.112347, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:25.780858: step 5052, loss 0.0672096, acc 1, learning_rate 0.0001
2017-10-10T13:18:26.297613: step 5053, loss 0.0580737, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:26.782453: step 5054, loss 0.0442716, acc 1, learning_rate 0.0001
2017-10-10T13:18:27.344520: step 5055, loss 0.0477087, acc 1, learning_rate 0.0001
2017-10-10T13:18:27.820937: step 5056, loss 0.0992828, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:28.428965: step 5057, loss 0.0382313, acc 1, learning_rate 0.0001
2017-10-10T13:18:28.892523: step 5058, loss 0.174167, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:29.385017: step 5059, loss 0.0460815, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:29.908552: step 5060, loss 0.0856206, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:30.525096: step 5061, loss 0.0538593, acc 1, learning_rate 0.0001
2017-10-10T13:18:31.034437: step 5062, loss 0.104427, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:31.449223: step 5063, loss 0.101887, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:31.890308: step 5064, loss 0.175234, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:32.323680: step 5065, loss 0.0741744, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:32.855669: step 5066, loss 0.100485, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:33.402195: step 5067, loss 0.116213, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:33.936756: step 5068, loss 0.0846198, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:34.446963: step 5069, loss 0.0521849, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:34.916884: step 5070, loss 0.123241, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:35.381109: step 5071, loss 0.13578, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:35.883043: step 5072, loss 0.106553, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:36.342538: step 5073, loss 0.0453341, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:36.769280: step 5074, loss 0.0853007, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:37.327601: step 5075, loss 0.222633, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:37.917835: step 5076, loss 0.0946487, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:38.357075: step 5077, loss 0.100533, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:38.775341: step 5078, loss 0.0365736, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:39.150050: step 5079, loss 0.0442975, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:39.665033: step 5080, loss 0.111559, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:18:40.965842: step 5080, loss 0.218048, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5080

2017-10-10T13:18:42.584380: step 5081, loss 0.069554, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:42.995509: step 5082, loss 0.0185258, acc 1, learning_rate 0.0001
2017-10-10T13:18:43.448672: step 5083, loss 0.123721, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:43.989681: step 5084, loss 0.0825878, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:44.548660: step 5085, loss 0.1801, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:45.053081: step 5086, loss 0.0949837, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:45.535446: step 5087, loss 0.12062, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:46.005037: step 5088, loss 0.0564508, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:46.451510: step 5089, loss 0.0587888, acc 1, learning_rate 0.0001
2017-10-10T13:18:46.926459: step 5090, loss 0.132949, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:47.472932: step 5091, loss 0.0559338, acc 1, learning_rate 0.0001
2017-10-10T13:18:48.017989: step 5092, loss 0.0595671, acc 1, learning_rate 0.0001
2017-10-10T13:18:48.539265: step 5093, loss 0.0650205, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:49.123086: step 5094, loss 0.0710008, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:49.632396: step 5095, loss 0.12334, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:50.052459: step 5096, loss 0.148871, acc 0.921569, learning_rate 0.0001
2017-10-10T13:18:50.522481: step 5097, loss 0.0353177, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:51.096887: step 5098, loss 0.098248, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:51.640303: step 5099, loss 0.117049, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:52.161568: step 5100, loss 0.0608486, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:52.707070: step 5101, loss 0.0496041, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:53.216118: step 5102, loss 0.0669085, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:53.720123: step 5103, loss 0.112955, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:54.297395: step 5104, loss 0.0530351, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:54.829121: step 5105, loss 0.0741908, acc 0.984375, learning_rate 0.0001
2017-10-10T13:18:55.265364: step 5106, loss 0.108086, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:55.620902: step 5107, loss 0.182818, acc 0.90625, learning_rate 0.0001
2017-10-10T13:18:56.040972: step 5108, loss 0.151495, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:56.559580: step 5109, loss 0.0914911, acc 0.96875, learning_rate 0.0001
2017-10-10T13:18:57.116899: step 5110, loss 0.125489, acc 0.9375, learning_rate 0.0001
2017-10-10T13:18:57.577144: step 5111, loss 0.137715, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:58.110706: step 5112, loss 0.025209, acc 1, learning_rate 0.0001
2017-10-10T13:18:58.645156: step 5113, loss 0.195014, acc 0.921875, learning_rate 0.0001
2017-10-10T13:18:59.117098: step 5114, loss 0.174688, acc 0.953125, learning_rate 0.0001
2017-10-10T13:18:59.677137: step 5115, loss 0.0354616, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:00.184872: step 5116, loss 0.134363, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:00.760550: step 5117, loss 0.0645195, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:01.204852: step 5118, loss 0.12772, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:01.656919: step 5119, loss 0.0940343, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:02.142380: step 5120, loss 0.0815547, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:19:03.392825: step 5120, loss 0.215926, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5120

2017-10-10T13:19:04.932832: step 5121, loss 0.0559026, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:05.546891: step 5122, loss 0.117872, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:06.048330: step 5123, loss 0.0650264, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:06.512499: step 5124, loss 0.050194, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:06.967252: step 5125, loss 0.101951, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:07.516921: step 5126, loss 0.111702, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:08.065908: step 5127, loss 0.0989406, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:08.580828: step 5128, loss 0.100725, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:09.100857: step 5129, loss 0.045067, acc 1, learning_rate 0.0001
2017-10-10T13:19:09.632094: step 5130, loss 0.0234201, acc 1, learning_rate 0.0001
2017-10-10T13:19:10.097935: step 5131, loss 0.0461514, acc 1, learning_rate 0.0001
2017-10-10T13:19:10.616969: step 5132, loss 0.131894, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:11.121545: step 5133, loss 0.1041, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:11.601090: step 5134, loss 0.0460039, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:12.156743: step 5135, loss 0.0789282, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:12.739063: step 5136, loss 0.10816, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:13.237113: step 5137, loss 0.0603973, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:13.815239: step 5138, loss 0.183972, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:14.328970: step 5139, loss 0.0988562, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:14.804927: step 5140, loss 0.0171249, acc 1, learning_rate 0.0001
2017-10-10T13:19:15.285026: step 5141, loss 0.0196073, acc 1, learning_rate 0.0001
2017-10-10T13:19:15.801511: step 5142, loss 0.056958, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:16.360872: step 5143, loss 0.0675679, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:16.894924: step 5144, loss 0.130705, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:17.466804: step 5145, loss 0.071704, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:18.066570: step 5146, loss 0.125456, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:18.620025: step 5147, loss 0.0846931, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:18.992957: step 5148, loss 0.0764926, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:19.412913: step 5149, loss 0.0556218, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:19.785429: step 5150, loss 0.0792648, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:20.251113: step 5151, loss 0.03138, acc 1, learning_rate 0.0001
2017-10-10T13:19:20.768369: step 5152, loss 0.0942414, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:21.268842: step 5153, loss 0.119224, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:21.836005: step 5154, loss 0.0784542, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:22.364842: step 5155, loss 0.113159, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:22.844894: step 5156, loss 0.0397378, acc 1, learning_rate 0.0001
2017-10-10T13:19:23.368934: step 5157, loss 0.0432642, acc 1, learning_rate 0.0001
2017-10-10T13:19:23.998676: step 5158, loss 0.0292973, acc 1, learning_rate 0.0001
2017-10-10T13:19:24.432896: step 5159, loss 0.0357664, acc 1, learning_rate 0.0001
2017-10-10T13:19:24.900923: step 5160, loss 0.0652439, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:19:26.012943: step 5160, loss 0.219852, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5160

2017-10-10T13:19:27.673786: step 5161, loss 0.0645965, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:28.220903: step 5162, loss 0.0821894, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:28.776988: step 5163, loss 0.0224971, acc 1, learning_rate 0.0001
2017-10-10T13:19:29.272885: step 5164, loss 0.0539803, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:29.752430: step 5165, loss 0.0727922, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:30.304921: step 5166, loss 0.0244847, acc 1, learning_rate 0.0001
2017-10-10T13:19:30.779353: step 5167, loss 0.121721, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:31.223059: step 5168, loss 0.0895164, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:31.732906: step 5169, loss 0.0576737, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:32.329002: step 5170, loss 0.0716439, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:32.828971: step 5171, loss 0.0923427, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:33.357151: step 5172, loss 0.0471799, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:33.910379: step 5173, loss 0.15386, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:34.488997: step 5174, loss 0.090495, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:34.977169: step 5175, loss 0.138147, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:35.471567: step 5176, loss 0.0682326, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:35.979994: step 5177, loss 0.110971, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:36.496872: step 5178, loss 0.034679, acc 1, learning_rate 0.0001
2017-10-10T13:19:36.988911: step 5179, loss 0.175208, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:37.548967: step 5180, loss 0.111517, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:38.092738: step 5181, loss 0.190695, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:38.681193: step 5182, loss 0.126632, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:39.213047: step 5183, loss 0.176916, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:39.793039: step 5184, loss 0.0532308, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:40.320840: step 5185, loss 0.0894011, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:40.805069: step 5186, loss 0.146351, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:41.383423: step 5187, loss 0.131703, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:42.046442: step 5188, loss 0.067066, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:42.432839: step 5189, loss 0.0839892, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:42.880836: step 5190, loss 0.125851, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:43.396036: step 5191, loss 0.0526996, acc 1, learning_rate 0.0001
2017-10-10T13:19:43.920957: step 5192, loss 0.0859159, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:44.451787: step 5193, loss 0.0467473, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:44.907378: step 5194, loss 0.0366231, acc 1, learning_rate 0.0001
2017-10-10T13:19:45.524934: step 5195, loss 0.0658437, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:46.039641: step 5196, loss 0.150701, acc 0.9375, learning_rate 0.0001
2017-10-10T13:19:46.539047: step 5197, loss 0.152872, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:47.136855: step 5198, loss 0.15147, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:47.656108: step 5199, loss 0.134688, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:48.049745: step 5200, loss 0.0484486, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:19:49.171260: step 5200, loss 0.221895, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5200

2017-10-10T13:19:50.533642: step 5201, loss 0.0516295, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:50.985033: step 5202, loss 0.0565444, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:51.552162: step 5203, loss 0.0410012, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:52.132577: step 5204, loss 0.0962951, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:52.536940: step 5205, loss 0.0401162, acc 1, learning_rate 0.0001
2017-10-10T13:19:52.984284: step 5206, loss 0.088335, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:53.413894: step 5207, loss 0.0374928, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:53.916923: step 5208, loss 0.0554669, acc 0.984375, learning_rate 0.0001
2017-10-10T13:19:54.377118: step 5209, loss 0.198724, acc 0.921875, learning_rate 0.0001
2017-10-10T13:19:54.893040: step 5210, loss 0.0518818, acc 1, learning_rate 0.0001
2017-10-10T13:19:55.423733: step 5211, loss 0.0492552, acc 1, learning_rate 0.0001
2017-10-10T13:19:55.962602: step 5212, loss 0.076705, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:56.503859: step 5213, loss 0.0932994, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:57.009400: step 5214, loss 0.10828, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:57.567387: step 5215, loss 0.110498, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:58.008939: step 5216, loss 0.149491, acc 0.953125, learning_rate 0.0001
2017-10-10T13:19:58.463238: step 5217, loss 0.0497575, acc 1, learning_rate 0.0001
2017-10-10T13:19:59.036888: step 5218, loss 0.0902521, acc 0.96875, learning_rate 0.0001
2017-10-10T13:19:59.614452: step 5219, loss 0.096588, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:00.157843: step 5220, loss 0.0658875, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:00.780798: step 5221, loss 0.110766, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:01.312534: step 5222, loss 0.0571824, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:01.812844: step 5223, loss 0.0688293, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:02.324852: step 5224, loss 0.0624555, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:02.873648: step 5225, loss 0.119151, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:03.416463: step 5226, loss 0.132545, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:03.947942: step 5227, loss 0.0635686, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:04.414548: step 5228, loss 0.0544247, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:05.059162: step 5229, loss 0.117846, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:05.620898: step 5230, loss 0.0760826, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:06.060834: step 5231, loss 0.0555934, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:06.519239: step 5232, loss 0.0746377, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:07.088934: step 5233, loss 0.0680795, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:07.655456: step 5234, loss 0.0718669, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:08.145633: step 5235, loss 0.0844155, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:08.681189: step 5236, loss 0.0780756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:09.279279: step 5237, loss 0.108794, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:09.844177: step 5238, loss 0.0559544, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:10.357028: step 5239, loss 0.0453381, acc 1, learning_rate 0.0001
2017-10-10T13:20:10.757063: step 5240, loss 0.0932944, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:20:12.113768: step 5240, loss 0.215533, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5240

2017-10-10T13:20:13.741880: step 5241, loss 0.0600314, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:14.336872: step 5242, loss 0.0760041, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:14.922976: step 5243, loss 0.0956248, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:15.477774: step 5244, loss 0.0462328, acc 1, learning_rate 0.0001
2017-10-10T13:20:15.916245: step 5245, loss 0.0712733, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:16.415814: step 5246, loss 0.117995, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:16.881212: step 5247, loss 0.0642197, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:17.424841: step 5248, loss 0.0689161, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:18.032040: step 5249, loss 0.0984708, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:18.572874: step 5250, loss 0.160025, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:19.125117: step 5251, loss 0.0600245, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:19.556870: step 5252, loss 0.0275746, acc 1, learning_rate 0.0001
2017-10-10T13:20:20.121097: step 5253, loss 0.128795, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:20.656319: step 5254, loss 0.13588, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:21.144504: step 5255, loss 0.131821, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:21.676897: step 5256, loss 0.14532, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:22.245051: step 5257, loss 0.0490176, acc 1, learning_rate 0.0001
2017-10-10T13:20:22.776943: step 5258, loss 0.0154185, acc 1, learning_rate 0.0001
2017-10-10T13:20:23.356528: step 5259, loss 0.0911161, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:23.904311: step 5260, loss 0.0628612, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:24.434169: step 5261, loss 0.0522991, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:24.941636: step 5262, loss 0.0509936, acc 1, learning_rate 0.0001
2017-10-10T13:20:25.484046: step 5263, loss 0.0636136, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:26.015800: step 5264, loss 0.0908525, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:26.580610: step 5265, loss 0.156533, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:27.164853: step 5266, loss 0.131259, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:27.704814: step 5267, loss 0.0361328, acc 1, learning_rate 0.0001
2017-10-10T13:20:28.292372: step 5268, loss 0.0822566, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:28.742917: step 5269, loss 0.154607, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:29.199025: step 5270, loss 0.102233, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:29.716867: step 5271, loss 0.141516, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:30.226475: step 5272, loss 0.156035, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:30.751762: step 5273, loss 0.0834038, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:31.274721: step 5274, loss 0.102365, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:31.797128: step 5275, loss 0.0618131, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:32.393002: step 5276, loss 0.130359, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:32.910727: step 5277, loss 0.154756, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:33.284926: step 5278, loss 0.0964563, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:33.722218: step 5279, loss 0.132314, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:34.259698: step 5280, loss 0.0871271, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:20:35.635539: step 5280, loss 0.223218, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5280

2017-10-10T13:20:37.338687: step 5281, loss 0.0331151, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:37.922547: step 5282, loss 0.0940602, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:38.460827: step 5283, loss 0.0308878, acc 1, learning_rate 0.0001
2017-10-10T13:20:39.033130: step 5284, loss 0.107916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:39.465023: step 5285, loss 0.0610289, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:39.936814: step 5286, loss 0.095525, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:40.540874: step 5287, loss 0.0876693, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:41.105034: step 5288, loss 0.0862724, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:41.626322: step 5289, loss 0.095056, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:42.192418: step 5290, loss 0.0977737, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:42.776863: step 5291, loss 0.018573, acc 1, learning_rate 0.0001
2017-10-10T13:20:43.253202: step 5292, loss 0.109812, acc 0.960784, learning_rate 0.0001
2017-10-10T13:20:43.800805: step 5293, loss 0.222574, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:44.344111: step 5294, loss 0.0266741, acc 1, learning_rate 0.0001
2017-10-10T13:20:44.853271: step 5295, loss 0.0544428, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:45.273157: step 5296, loss 0.0565157, acc 1, learning_rate 0.0001
2017-10-10T13:20:45.777141: step 5297, loss 0.079077, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:46.228185: step 5298, loss 0.0691813, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:46.759564: step 5299, loss 0.0533259, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:47.220466: step 5300, loss 0.0483036, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:47.773878: step 5301, loss 0.0434434, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:48.302431: step 5302, loss 0.0785036, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:48.924843: step 5303, loss 0.124976, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:49.502181: step 5304, loss 0.0734457, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:50.013104: step 5305, loss 0.0815584, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:50.485051: step 5306, loss 0.078891, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:51.065021: step 5307, loss 0.0547302, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:51.627425: step 5308, loss 0.0392416, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:52.112961: step 5309, loss 0.0415587, acc 1, learning_rate 0.0001
2017-10-10T13:20:52.553307: step 5310, loss 0.0920183, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:52.956968: step 5311, loss 0.0747892, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:53.473083: step 5312, loss 0.0526362, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:54.053066: step 5313, loss 0.0989435, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:54.599272: step 5314, loss 0.0453555, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:55.167028: step 5315, loss 0.0428083, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:56.096170: step 5316, loss 0.0240368, acc 1, learning_rate 0.0001
2017-10-10T13:20:56.541290: step 5317, loss 0.132701, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:57.086297: step 5318, loss 0.117787, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:57.623269: step 5319, loss 0.0787121, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:58.241253: step 5320, loss 0.0579245, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:20:59.556944: step 5320, loss 0.22216, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5320

2017-10-10T13:21:00.916173: step 5321, loss 0.0895511, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:01.368911: step 5322, loss 0.154019, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:01.879045: step 5323, loss 0.0749662, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:02.429054: step 5324, loss 0.0379712, acc 1, learning_rate 0.0001
2017-10-10T13:21:03.050080: step 5325, loss 0.122521, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:03.556867: step 5326, loss 0.0355762, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:04.008893: step 5327, loss 0.0937781, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:04.608325: step 5328, loss 0.0361066, acc 1, learning_rate 0.0001
2017-10-10T13:21:05.060284: step 5329, loss 0.115853, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:05.539570: step 5330, loss 0.089653, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:06.047997: step 5331, loss 0.0346798, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:06.557092: step 5332, loss 0.0231227, acc 1, learning_rate 0.0001
2017-10-10T13:21:07.052576: step 5333, loss 0.101162, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:07.492871: step 5334, loss 0.0576805, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:08.029063: step 5335, loss 0.0640592, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:08.669062: step 5336, loss 0.0745387, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:09.223320: step 5337, loss 0.111926, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:09.705105: step 5338, loss 0.0698428, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:10.184949: step 5339, loss 0.0796151, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:10.693125: step 5340, loss 0.107849, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:11.196965: step 5341, loss 0.0810384, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:11.687295: step 5342, loss 0.0498047, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:12.256845: step 5343, loss 0.151152, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:12.768837: step 5344, loss 0.0661682, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:13.359340: step 5345, loss 0.0829753, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:13.882468: step 5346, loss 0.16168, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:14.407235: step 5347, loss 0.0331018, acc 1, learning_rate 0.0001
2017-10-10T13:21:14.959300: step 5348, loss 0.0891738, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:15.479617: step 5349, loss 0.0260854, acc 1, learning_rate 0.0001
2017-10-10T13:21:15.931481: step 5350, loss 0.126307, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:16.398263: step 5351, loss 0.105883, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:16.872868: step 5352, loss 0.128716, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:17.413671: step 5353, loss 0.0353792, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:17.937291: step 5354, loss 0.0868362, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:18.459912: step 5355, loss 0.0583505, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:19.025221: step 5356, loss 0.0832963, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:19.333164: step 5357, loss 0.0594019, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:19.800012: step 5358, loss 0.0677892, acc 1, learning_rate 0.0001
2017-10-10T13:21:20.280976: step 5359, loss 0.13388, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:20.732889: step 5360, loss 0.104392, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:21:22.072299: step 5360, loss 0.216435, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5360

2017-10-10T13:21:23.641076: step 5361, loss 0.072318, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:24.197042: step 5362, loss 0.0552736, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:24.724849: step 5363, loss 0.16201, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:25.308991: step 5364, loss 0.0520603, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:25.853160: step 5365, loss 0.0834571, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:26.384215: step 5366, loss 0.0741615, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:26.868616: step 5367, loss 0.08442, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:27.339422: step 5368, loss 0.15047, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:27.772747: step 5369, loss 0.0678953, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:28.292977: step 5370, loss 0.146147, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:28.808330: step 5371, loss 0.0201213, acc 1, learning_rate 0.0001
2017-10-10T13:21:29.353004: step 5372, loss 0.179101, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:29.957087: step 5373, loss 0.122383, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:30.500957: step 5374, loss 0.0640707, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:31.081172: step 5375, loss 0.0611578, acc 1, learning_rate 0.0001
2017-10-10T13:21:31.629102: step 5376, loss 0.0750311, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:32.202984: step 5377, loss 0.139881, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:32.808889: step 5378, loss 0.0524193, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:33.353280: step 5379, loss 0.0952566, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:33.911197: step 5380, loss 0.046399, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:34.460836: step 5381, loss 0.204682, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:35.007875: step 5382, loss 0.071247, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:35.552771: step 5383, loss 0.0820082, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:36.138705: step 5384, loss 0.145775, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:36.663727: step 5385, loss 0.0596941, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:37.204882: step 5386, loss 0.0513156, acc 1, learning_rate 0.0001
2017-10-10T13:21:37.829045: step 5387, loss 0.0746529, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:38.354296: step 5388, loss 0.027228, acc 1, learning_rate 0.0001
2017-10-10T13:21:38.820257: step 5389, loss 0.0470718, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:39.261296: step 5390, loss 0.151212, acc 0.921569, learning_rate 0.0001
2017-10-10T13:21:39.850115: step 5391, loss 0.0438857, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:40.433547: step 5392, loss 0.146419, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:41.024845: step 5393, loss 0.0226845, acc 1, learning_rate 0.0001
2017-10-10T13:21:41.596882: step 5394, loss 0.117082, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:42.112924: step 5395, loss 0.14167, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:42.507430: step 5396, loss 0.0534768, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:43.001045: step 5397, loss 0.0662455, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:43.600873: step 5398, loss 0.108623, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:44.112951: step 5399, loss 0.059056, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:44.574719: step 5400, loss 0.092295, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:21:45.879264: step 5400, loss 0.218513, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5400

2017-10-10T13:21:47.625076: step 5401, loss 0.0288695, acc 1, learning_rate 0.0001
2017-10-10T13:21:48.168921: step 5402, loss 0.120209, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:48.690879: step 5403, loss 0.187862, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:49.233153: step 5404, loss 0.0966153, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:49.687523: step 5405, loss 0.0794998, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:50.297087: step 5406, loss 0.0222072, acc 1, learning_rate 0.0001
2017-10-10T13:21:50.820604: step 5407, loss 0.091373, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:51.324916: step 5408, loss 0.0824649, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:51.709599: step 5409, loss 0.0440858, acc 1, learning_rate 0.0001
2017-10-10T13:21:52.143463: step 5410, loss 0.119244, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:52.661551: step 5411, loss 0.143623, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:53.124961: step 5412, loss 0.0678926, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:53.701662: step 5413, loss 0.0936212, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:54.229173: step 5414, loss 0.122104, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:54.681125: step 5415, loss 0.0836042, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:55.282854: step 5416, loss 0.0676399, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:55.705392: step 5417, loss 0.0640325, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:56.205295: step 5418, loss 0.0787838, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:56.653117: step 5419, loss 0.0614996, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:57.247191: step 5420, loss 0.0269693, acc 1, learning_rate 0.0001
2017-10-10T13:21:57.748978: step 5421, loss 0.0262685, acc 1, learning_rate 0.0001
2017-10-10T13:21:58.265145: step 5422, loss 0.0799054, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:58.748885: step 5423, loss 0.0948118, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:59.289260: step 5424, loss 0.0561631, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:59.801146: step 5425, loss 0.134499, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:00.305116: step 5426, loss 0.0662824, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:00.855508: step 5427, loss 0.044821, acc 1, learning_rate 0.0001
2017-10-10T13:22:01.423027: step 5428, loss 0.159335, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:01.862695: step 5429, loss 0.11323, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:02.343295: step 5430, loss 0.0240708, acc 1, learning_rate 0.0001
2017-10-10T13:22:02.873082: step 5431, loss 0.155668, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:03.380819: step 5432, loss 0.0517315, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:03.836928: step 5433, loss 0.0438273, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:04.446414: step 5434, loss 0.0528276, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:04.960815: step 5435, loss 0.0907211, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:05.423756: step 5436, loss 0.0934874, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:05.884980: step 5437, loss 0.0805634, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:06.443857: step 5438, loss 0.121001, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:06.997034: step 5439, loss 0.0671154, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:07.452855: step 5440, loss 0.0794922, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:22:08.832810: step 5440, loss 0.217666, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5440

2017-10-10T13:22:10.268349: step 5441, loss 0.0706601, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:10.786592: step 5442, loss 0.0637415, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:11.330899: step 5443, loss 0.129385, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:11.834486: step 5444, loss 0.0980242, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:12.364870: step 5445, loss 0.112931, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:12.884885: step 5446, loss 0.0819336, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:13.469381: step 5447, loss 0.0825656, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:13.987785: step 5448, loss 0.0702594, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:14.376073: step 5449, loss 0.0402373, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:14.724983: step 5450, loss 0.101333, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:15.204760: step 5451, loss 0.0618106, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:15.728392: step 5452, loss 0.140804, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:16.298260: step 5453, loss 0.0889447, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:16.756127: step 5454, loss 0.115726, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:17.292854: step 5455, loss 0.12957, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:17.833118: step 5456, loss 0.139957, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:18.283324: step 5457, loss 0.280565, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:18.836678: step 5458, loss 0.107971, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:19.417615: step 5459, loss 0.110847, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:19.936970: step 5460, loss 0.189134, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:20.465645: step 5461, loss 0.143306, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:20.996878: step 5462, loss 0.104658, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:21.501954: step 5463, loss 0.066251, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:22.017774: step 5464, loss 0.0333003, acc 1, learning_rate 0.0001
2017-10-10T13:22:22.536700: step 5465, loss 0.0427996, acc 1, learning_rate 0.0001
2017-10-10T13:22:23.060822: step 5466, loss 0.061669, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:23.537430: step 5467, loss 0.0769321, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:24.023733: step 5468, loss 0.0799591, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:24.557734: step 5469, loss 0.131828, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:25.053505: step 5470, loss 0.0910156, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:25.456966: step 5471, loss 0.0934024, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:25.916977: step 5472, loss 0.0705237, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:26.460913: step 5473, loss 0.0745578, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:27.010522: step 5474, loss 0.0744505, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:27.584827: step 5475, loss 0.168859, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:28.117408: step 5476, loss 0.0740511, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:29.253505: step 5477, loss 0.11039, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:29.695298: step 5478, loss 0.0467147, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:30.242146: step 5479, loss 0.0406831, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:30.791212: step 5480, loss 0.0500455, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:22:32.100984: step 5480, loss 0.216321, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5480

2017-10-10T13:22:33.701019: step 5481, loss 0.0598401, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:34.216950: step 5482, loss 0.118598, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:34.716375: step 5483, loss 0.0756143, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:35.193064: step 5484, loss 0.127991, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:35.592154: step 5485, loss 0.102805, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:36.177172: step 5486, loss 0.0417835, acc 1, learning_rate 0.0001
2017-10-10T13:22:36.792921: step 5487, loss 0.100604, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:37.281383: step 5488, loss 0.0586934, acc 0.980392, learning_rate 0.0001
2017-10-10T13:22:37.713066: step 5489, loss 0.0507993, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:38.205222: step 5490, loss 0.116824, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:38.590004: step 5491, loss 0.0733458, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:39.032824: step 5492, loss 0.0163625, acc 1, learning_rate 0.0001
2017-10-10T13:22:39.590936: step 5493, loss 0.110072, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:40.127878: step 5494, loss 0.161086, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:40.663454: step 5495, loss 0.102873, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:41.239813: step 5496, loss 0.186393, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:41.790435: step 5497, loss 0.0624953, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:42.370674: step 5498, loss 0.111372, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:42.927146: step 5499, loss 0.0511263, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:43.505424: step 5500, loss 0.0712683, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:43.981063: step 5501, loss 0.116185, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:44.545177: step 5502, loss 0.0436502, acc 1, learning_rate 0.0001
2017-10-10T13:22:45.084840: step 5503, loss 0.125371, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:45.658087: step 5504, loss 0.194109, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:46.193033: step 5505, loss 0.0558477, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:46.752866: step 5506, loss 0.28457, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:47.262768: step 5507, loss 0.0783798, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:47.824875: step 5508, loss 0.0576848, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:48.439851: step 5509, loss 0.0846508, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:49.067429: step 5510, loss 0.0819331, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:49.506573: step 5511, loss 0.122169, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:49.922412: step 5512, loss 0.120069, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:50.400582: step 5513, loss 0.049922, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:50.934581: step 5514, loss 0.0685496, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:51.524188: step 5515, loss 0.0478893, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:51.938631: step 5516, loss 0.116181, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:52.440916: step 5517, loss 0.0711273, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:52.832924: step 5518, loss 0.02625, acc 1, learning_rate 0.0001
2017-10-10T13:22:53.221060: step 5519, loss 0.069842, acc 1, learning_rate 0.0001
2017-10-10T13:22:53.701011: step 5520, loss 0.0973834, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:22:55.124812: step 5520, loss 0.220228, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5520

2017-10-10T13:22:56.708807: step 5521, loss 0.0426482, acc 1, learning_rate 0.0001
2017-10-10T13:22:57.224171: step 5522, loss 0.129399, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:57.769492: step 5523, loss 0.0659122, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:58.280003: step 5524, loss 0.027511, acc 1, learning_rate 0.0001
2017-10-10T13:22:58.830595: step 5525, loss 0.0370734, acc 1, learning_rate 0.0001
2017-10-10T13:22:59.356592: step 5526, loss 0.0743145, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:59.913199: step 5527, loss 0.128021, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:00.435314: step 5528, loss 0.0568076, acc 1, learning_rate 0.0001
2017-10-10T13:23:00.960938: step 5529, loss 0.134317, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:01.540353: step 5530, loss 0.088795, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:01.967790: step 5531, loss 0.0620599, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:02.308541: step 5532, loss 0.0794822, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:02.784114: step 5533, loss 0.118107, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:03.273586: step 5534, loss 0.0519921, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:03.765607: step 5535, loss 0.258191, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:04.316501: step 5536, loss 0.103913, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:04.850686: step 5537, loss 0.102457, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:05.420836: step 5538, loss 0.0816382, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:05.995074: step 5539, loss 0.0274993, acc 1, learning_rate 0.0001
2017-10-10T13:23:06.539427: step 5540, loss 0.0378279, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:07.056949: step 5541, loss 0.0696405, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:07.573406: step 5542, loss 0.0385642, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:08.128923: step 5543, loss 0.0665573, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:08.680583: step 5544, loss 0.0711473, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:09.178141: step 5545, loss 0.122563, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:09.699503: step 5546, loss 0.0595367, acc 1, learning_rate 0.0001
2017-10-10T13:23:10.089075: step 5547, loss 0.0417654, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:10.587205: step 5548, loss 0.0555541, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:11.015058: step 5549, loss 0.0840941, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:11.589513: step 5550, loss 0.0790849, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:12.109175: step 5551, loss 0.212874, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:12.540734: step 5552, loss 0.191607, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:12.987074: step 5553, loss 0.0627444, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:13.496857: step 5554, loss 0.0455172, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:14.004858: step 5555, loss 0.0799866, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:14.528639: step 5556, loss 0.105614, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:15.189014: step 5557, loss 0.052897, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:15.541182: step 5558, loss 0.0434853, acc 1, learning_rate 0.0001
2017-10-10T13:23:15.930833: step 5559, loss 0.108635, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:16.380229: step 5560, loss 0.103915, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:23:17.759686: step 5560, loss 0.220559, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5560

2017-10-10T13:23:19.511587: step 5561, loss 0.079702, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:19.994822: step 5562, loss 0.145349, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:20.564946: step 5563, loss 0.0692925, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:21.068224: step 5564, loss 0.0384499, acc 1, learning_rate 0.0001
2017-10-10T13:23:21.610404: step 5565, loss 0.10896, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:22.124946: step 5566, loss 0.106625, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:22.639495: step 5567, loss 0.163844, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:23.182343: step 5568, loss 0.0956222, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:23.727107: step 5569, loss 0.0693586, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:24.258878: step 5570, loss 0.116891, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:24.819042: step 5571, loss 0.069424, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:25.446732: step 5572, loss 0.0662139, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:25.931219: step 5573, loss 0.0485496, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:26.374490: step 5574, loss 0.059319, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:26.873251: step 5575, loss 0.0531421, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:27.420492: step 5576, loss 0.0801186, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:27.917745: step 5577, loss 0.0302486, acc 1, learning_rate 0.0001
2017-10-10T13:23:28.436631: step 5578, loss 0.229567, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:28.988845: step 5579, loss 0.067841, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:29.490830: step 5580, loss 0.0394862, acc 1, learning_rate 0.0001
2017-10-10T13:23:30.002856: step 5581, loss 0.100764, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:30.493205: step 5582, loss 0.0375988, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:31.080987: step 5583, loss 0.0496511, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:31.601033: step 5584, loss 0.0591133, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:32.116957: step 5585, loss 0.11108, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:32.568843: step 5586, loss 0.113043, acc 0.980392, learning_rate 0.0001
2017-10-10T13:23:33.168396: step 5587, loss 0.108785, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:33.701300: step 5588, loss 0.110836, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:34.211761: step 5589, loss 0.16949, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:34.767655: step 5590, loss 0.0819006, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:35.294652: step 5591, loss 0.0456636, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:35.726031: step 5592, loss 0.0736865, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:36.134471: step 5593, loss 0.0784639, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:36.662934: step 5594, loss 0.0376763, acc 1, learning_rate 0.0001
2017-10-10T13:23:37.188864: step 5595, loss 0.111031, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:37.805040: step 5596, loss 0.0607074, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:38.274888: step 5597, loss 0.0538571, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:38.727298: step 5598, loss 0.068585, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:39.141085: step 5599, loss 0.0367606, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:39.706909: step 5600, loss 0.136749, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:23:41.081855: step 5600, loss 0.216818, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5600

2017-10-10T13:23:42.545357: step 5601, loss 0.0871624, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:43.105709: step 5602, loss 0.0720964, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:43.604900: step 5603, loss 0.108439, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:44.116947: step 5604, loss 0.0333906, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:44.579850: step 5605, loss 0.0360685, acc 1, learning_rate 0.0001
2017-10-10T13:23:45.093624: step 5606, loss 0.185075, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:45.499999: step 5607, loss 0.116646, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:46.045094: step 5608, loss 0.0577964, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:46.632939: step 5609, loss 0.0712698, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:47.228892: step 5610, loss 0.0576562, acc 1, learning_rate 0.0001
2017-10-10T13:23:47.868873: step 5611, loss 0.0597403, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:48.411193: step 5612, loss 0.101712, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:48.861903: step 5613, loss 0.15983, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:49.295533: step 5614, loss 0.079318, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:49.736878: step 5615, loss 0.0461641, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:50.252979: step 5616, loss 0.0435246, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:50.822301: step 5617, loss 0.13118, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:51.345038: step 5618, loss 0.0519417, acc 1, learning_rate 0.0001
2017-10-10T13:23:51.938062: step 5619, loss 0.0565949, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:52.460694: step 5620, loss 0.0895656, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:52.940907: step 5621, loss 0.0739944, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:53.411265: step 5622, loss 0.0166404, acc 1, learning_rate 0.0001
2017-10-10T13:23:53.929088: step 5623, loss 0.131078, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:54.380794: step 5624, loss 0.0931337, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:54.869079: step 5625, loss 0.0594483, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:55.378684: step 5626, loss 0.100844, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:55.917258: step 5627, loss 0.0399494, acc 1, learning_rate 0.0001
2017-10-10T13:23:56.424800: step 5628, loss 0.0720234, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:56.917864: step 5629, loss 0.0982535, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:57.357155: step 5630, loss 0.0387661, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:57.916912: step 5631, loss 0.083655, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:58.559576: step 5632, loss 0.0661407, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:59.093904: step 5633, loss 0.0197532, acc 1, learning_rate 0.0001
2017-10-10T13:23:59.501767: step 5634, loss 0.083985, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:59.940468: step 5635, loss 0.0845578, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:00.417137: step 5636, loss 0.0320395, acc 1, learning_rate 0.0001
2017-10-10T13:24:01.010568: step 5637, loss 0.0757222, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:01.445274: step 5638, loss 0.0421823, acc 1, learning_rate 0.0001
2017-10-10T13:24:01.837789: step 5639, loss 0.0309264, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:02.295123: step 5640, loss 0.142333, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:03.796792: step 5640, loss 0.216709, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5640

2017-10-10T13:24:05.388888: step 5641, loss 0.213649, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:05.944853: step 5642, loss 0.0345773, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:06.504591: step 5643, loss 0.0311467, acc 1, learning_rate 0.0001
2017-10-10T13:24:07.068846: step 5644, loss 0.156838, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:07.568920: step 5645, loss 0.0792972, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:08.124093: step 5646, loss 0.102572, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:08.669544: step 5647, loss 0.213263, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:09.173994: step 5648, loss 0.0896728, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:09.669985: step 5649, loss 0.0661413, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:10.188832: step 5650, loss 0.109713, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:10.765315: step 5651, loss 0.0417914, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:11.286991: step 5652, loss 0.0477671, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:11.736839: step 5653, loss 0.0706785, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:12.116982: step 5654, loss 0.0547304, acc 1, learning_rate 0.0001
2017-10-10T13:24:12.648842: step 5655, loss 0.0430996, acc 1, learning_rate 0.0001
2017-10-10T13:24:13.237247: step 5656, loss 0.11635, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:13.785102: step 5657, loss 0.0519524, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:14.404940: step 5658, loss 0.0533071, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:14.906689: step 5659, loss 0.0761671, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:15.330592: step 5660, loss 0.199892, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:15.902934: step 5661, loss 0.0285257, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:16.498781: step 5662, loss 0.039599, acc 1, learning_rate 0.0001
2017-10-10T13:24:17.044373: step 5663, loss 0.102472, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:17.548871: step 5664, loss 0.0619752, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:18.049977: step 5665, loss 0.0736258, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:18.569515: step 5666, loss 0.14751, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:19.079613: step 5667, loss 0.115428, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:19.629708: step 5668, loss 0.0396399, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:20.203666: step 5669, loss 0.0898022, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:20.777177: step 5670, loss 0.1112, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:21.344501: step 5671, loss 0.0272953, acc 1, learning_rate 0.0001
2017-10-10T13:24:21.921180: step 5672, loss 0.148743, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:22.352886: step 5673, loss 0.0438461, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:22.769058: step 5674, loss 0.047246, acc 1, learning_rate 0.0001
2017-10-10T13:24:23.268303: step 5675, loss 0.0605428, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:23.793765: step 5676, loss 0.0737527, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:24.334000: step 5677, loss 0.0712879, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:24.808864: step 5678, loss 0.11104, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:25.235801: step 5679, loss 0.0729567, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:25.709500: step 5680, loss 0.133124, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:27.097668: step 5680, loss 0.214836, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5680

2017-10-10T13:24:28.845953: step 5681, loss 0.059451, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:29.334272: step 5682, loss 0.0419865, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:29.909089: step 5683, loss 0.106636, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:30.316017: step 5684, loss 0.111113, acc 0.980392, learning_rate 0.0001
2017-10-10T13:24:30.834993: step 5685, loss 0.079044, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:31.316855: step 5686, loss 0.105559, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:31.850580: step 5687, loss 0.0843677, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:32.392963: step 5688, loss 0.1439, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:32.953622: step 5689, loss 0.136261, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:33.476879: step 5690, loss 0.0588796, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:34.045030: step 5691, loss 0.0659617, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:34.533098: step 5692, loss 0.0850625, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:34.953716: step 5693, loss 0.143568, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:35.433175: step 5694, loss 0.0343438, acc 1, learning_rate 0.0001
2017-10-10T13:24:36.044913: step 5695, loss 0.0893432, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:36.627008: step 5696, loss 0.0445129, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:37.192356: step 5697, loss 0.0521546, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:37.732492: step 5698, loss 0.0948279, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:38.289448: step 5699, loss 0.0431121, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:38.865239: step 5700, loss 0.156619, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:39.463329: step 5701, loss 0.10074, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:40.015251: step 5702, loss 0.0525144, acc 1, learning_rate 0.0001
2017-10-10T13:24:40.563376: step 5703, loss 0.154341, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:41.149178: step 5704, loss 0.0330319, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:41.701058: step 5705, loss 0.0655537, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:42.284970: step 5706, loss 0.034724, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:42.800996: step 5707, loss 0.0416134, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:43.325112: step 5708, loss 0.0822451, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:43.821259: step 5709, loss 0.116433, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:44.396981: step 5710, loss 0.0777584, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:45.005102: step 5711, loss 0.0408036, acc 1, learning_rate 0.0001
2017-10-10T13:24:45.448856: step 5712, loss 0.131983, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:45.913867: step 5713, loss 0.104236, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:46.496934: step 5714, loss 0.116956, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:47.001957: step 5715, loss 0.0922962, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:47.494366: step 5716, loss 0.0380306, acc 1, learning_rate 0.0001
2017-10-10T13:24:47.897025: step 5717, loss 0.0240715, acc 1, learning_rate 0.0001
2017-10-10T13:24:48.333031: step 5718, loss 0.0314677, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:48.841988: step 5719, loss 0.0981916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:49.349754: step 5720, loss 0.0657414, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:50.648915: step 5720, loss 0.221129, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5720

2017-10-10T13:24:52.080844: step 5721, loss 0.052888, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:52.606161: step 5722, loss 0.0243625, acc 1, learning_rate 0.0001
2017-10-10T13:24:53.136821: step 5723, loss 0.0660843, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:53.632304: step 5724, loss 0.0285589, acc 1, learning_rate 0.0001
2017-10-10T13:24:54.170596: step 5725, loss 0.148474, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:54.696877: step 5726, loss 0.0483062, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:55.258689: step 5727, loss 0.0524298, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:55.797758: step 5728, loss 0.0744983, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:56.300863: step 5729, loss 0.0894717, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:56.868868: step 5730, loss 0.0417166, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:57.340118: step 5731, loss 0.150335, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:57.723285: step 5732, loss 0.058372, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:58.241357: step 5733, loss 0.102163, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:58.749429: step 5734, loss 0.0315193, acc 1, learning_rate 0.0001
2017-10-10T13:24:59.250007: step 5735, loss 0.0775536, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:59.740627: step 5736, loss 0.0656107, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:00.285048: step 5737, loss 0.0368699, acc 1, learning_rate 0.0001
2017-10-10T13:25:00.729018: step 5738, loss 0.0331793, acc 1, learning_rate 0.0001
2017-10-10T13:25:01.335508: step 5739, loss 0.0863194, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:01.843331: step 5740, loss 0.12832, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:02.349183: step 5741, loss 0.0691873, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:02.808915: step 5742, loss 0.112302, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:03.412890: step 5743, loss 0.0557988, acc 1, learning_rate 0.0001
2017-10-10T13:25:03.892921: step 5744, loss 0.178501, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:04.383317: step 5745, loss 0.0958553, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:04.905015: step 5746, loss 0.0937829, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:05.395285: step 5747, loss 0.080718, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:05.887613: step 5748, loss 0.112646, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:06.444120: step 5749, loss 0.121944, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:06.955768: step 5750, loss 0.0288033, acc 1, learning_rate 0.0001
2017-10-10T13:25:07.414618: step 5751, loss 0.19382, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:07.960981: step 5752, loss 0.0807197, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:08.584965: step 5753, loss 0.0903829, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:09.059215: step 5754, loss 0.0249115, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:09.497501: step 5755, loss 0.0585939, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:10.048997: step 5756, loss 0.107247, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:10.504050: step 5757, loss 0.0682998, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:10.948893: step 5758, loss 0.0504058, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:11.376815: step 5759, loss 0.0819508, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:11.881137: step 5760, loss 0.0150841, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:13.267159: step 5760, loss 0.217712, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5760

2017-10-10T13:25:14.899831: step 5761, loss 0.10633, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:15.380089: step 5762, loss 0.0574038, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:15.872132: step 5763, loss 0.0684891, acc 1, learning_rate 0.0001
2017-10-10T13:25:16.418568: step 5764, loss 0.271357, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:16.891618: step 5765, loss 0.14187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:17.426100: step 5766, loss 0.049953, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:17.894596: step 5767, loss 0.0827636, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:18.413079: step 5768, loss 0.126121, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:18.905714: step 5769, loss 0.0336143, acc 1, learning_rate 0.0001
2017-10-10T13:25:19.525085: step 5770, loss 0.12812, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:20.049223: step 5771, loss 0.045059, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:20.489947: step 5772, loss 0.0422746, acc 1, learning_rate 0.0001
2017-10-10T13:25:20.945128: step 5773, loss 0.0907156, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:21.372779: step 5774, loss 0.0659216, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:21.903139: step 5775, loss 0.0366806, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:22.470932: step 5776, loss 0.0464045, acc 1, learning_rate 0.0001
2017-10-10T13:25:23.013046: step 5777, loss 0.0844985, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:23.535625: step 5778, loss 0.076746, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:24.066374: step 5779, loss 0.0754239, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:24.553004: step 5780, loss 0.0730126, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:24.937661: step 5781, loss 0.104289, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:25.386559: step 5782, loss 0.0499732, acc 1, learning_rate 0.0001
2017-10-10T13:25:25.924880: step 5783, loss 0.144048, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:26.436813: step 5784, loss 0.118218, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:26.976124: step 5785, loss 0.103002, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:27.529532: step 5786, loss 0.139171, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:28.085026: step 5787, loss 0.0667637, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:28.651943: step 5788, loss 0.0321929, acc 1, learning_rate 0.0001
2017-10-10T13:25:29.225199: step 5789, loss 0.188237, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:29.772846: step 5790, loss 0.116466, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:30.292538: step 5791, loss 0.0818738, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:30.808346: step 5792, loss 0.0962924, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:31.426027: step 5793, loss 0.0337065, acc 1, learning_rate 0.0001
2017-10-10T13:25:31.946874: step 5794, loss 0.147231, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:32.381695: step 5795, loss 0.0806858, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:32.916856: step 5796, loss 0.0713275, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:33.465213: step 5797, loss 0.0583859, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:34.024131: step 5798, loss 0.0442874, acc 1, learning_rate 0.0001
2017-10-10T13:25:34.452343: step 5799, loss 0.0932217, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:34.972031: step 5800, loss 0.0356519, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:36.384987: step 5800, loss 0.220268, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5800

2017-10-10T13:25:38.200820: step 5801, loss 0.101393, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:38.760580: step 5802, loss 0.125174, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:39.311684: step 5803, loss 0.199522, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:39.853117: step 5804, loss 0.0421627, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:40.356920: step 5805, loss 0.0265513, acc 1, learning_rate 0.0001
2017-10-10T13:25:40.904852: step 5806, loss 0.0533128, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:41.459639: step 5807, loss 0.165847, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:41.990095: step 5808, loss 0.0875202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:42.468936: step 5809, loss 0.0695115, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:42.920971: step 5810, loss 0.187317, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:43.464968: step 5811, loss 0.0770808, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:43.969458: step 5812, loss 0.0549779, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:44.492950: step 5813, loss 0.0591322, acc 1, learning_rate 0.0001
2017-10-10T13:25:44.936908: step 5814, loss 0.0608297, acc 1, learning_rate 0.0001
2017-10-10T13:25:45.351346: step 5815, loss 0.120896, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:45.860894: step 5816, loss 0.125767, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:46.429196: step 5817, loss 0.046408, acc 1, learning_rate 0.0001
2017-10-10T13:25:46.910321: step 5818, loss 0.12186, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:47.432933: step 5819, loss 0.0705337, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:47.969212: step 5820, loss 0.0691803, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:48.523460: step 5821, loss 0.051814, acc 1, learning_rate 0.0001
2017-10-10T13:25:49.069487: step 5822, loss 0.0330879, acc 1, learning_rate 0.0001
2017-10-10T13:25:49.597035: step 5823, loss 0.0406506, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:50.112939: step 5824, loss 0.0432572, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:50.653060: step 5825, loss 0.122225, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:51.251298: step 5826, loss 0.0277923, acc 1, learning_rate 0.0001
2017-10-10T13:25:51.835313: step 5827, loss 0.0455715, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:52.393869: step 5828, loss 0.150422, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:52.997083: step 5829, loss 0.056332, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:53.524152: step 5830, loss 0.0463304, acc 1, learning_rate 0.0001
2017-10-10T13:25:54.098542: step 5831, loss 0.0982912, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:54.670726: step 5832, loss 0.0923828, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:55.237136: step 5833, loss 0.143689, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:55.776882: step 5834, loss 0.0817578, acc 1, learning_rate 0.0001
2017-10-10T13:25:56.236833: step 5835, loss 0.129596, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:56.793204: step 5836, loss 0.109352, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:57.160843: step 5837, loss 0.0566874, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:57.596134: step 5838, loss 0.0268601, acc 1, learning_rate 0.0001
2017-10-10T13:25:58.048944: step 5839, loss 0.0442077, acc 1, learning_rate 0.0001
2017-10-10T13:25:58.595243: step 5840, loss 0.107955, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:59.989041: step 5840, loss 0.217478, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5840

2017-10-10T13:26:01.412974: step 5841, loss 0.0609612, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:01.891020: step 5842, loss 0.0268001, acc 1, learning_rate 0.0001
2017-10-10T13:26:02.431608: step 5843, loss 0.0561209, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:02.966311: step 5844, loss 0.0811303, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:03.474173: step 5845, loss 0.0497286, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:04.018307: step 5846, loss 0.056729, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:04.563141: step 5847, loss 0.0854258, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:05.072947: step 5848, loss 0.0959623, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:05.625027: step 5849, loss 0.0677372, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:06.132914: step 5850, loss 0.0523095, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:06.656887: step 5851, loss 0.0754897, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:07.257352: step 5852, loss 0.129632, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:07.708786: step 5853, loss 0.0822785, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:08.169269: step 5854, loss 0.0900391, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:08.696587: step 5855, loss 0.0823331, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:09.220887: step 5856, loss 0.0629931, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:09.746504: step 5857, loss 0.082211, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:10.200465: step 5858, loss 0.0631207, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:10.725040: step 5859, loss 0.0247825, acc 1, learning_rate 0.0001
2017-10-10T13:26:11.235704: step 5860, loss 0.120444, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:11.752533: step 5861, loss 0.081973, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:12.285522: step 5862, loss 0.0729648, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:12.731130: step 5863, loss 0.0484184, acc 1, learning_rate 0.0001
2017-10-10T13:26:13.252561: step 5864, loss 0.043625, acc 1, learning_rate 0.0001
2017-10-10T13:26:13.717066: step 5865, loss 0.0557197, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:14.293030: step 5866, loss 0.0732612, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:14.809205: step 5867, loss 0.0976289, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:15.379206: step 5868, loss 0.0800233, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:15.897082: step 5869, loss 0.045331, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:16.424904: step 5870, loss 0.0318697, acc 1, learning_rate 0.0001
2017-10-10T13:26:16.942726: step 5871, loss 0.069522, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:17.569510: step 5872, loss 0.0538419, acc 1, learning_rate 0.0001
2017-10-10T13:26:18.088621: step 5873, loss 0.0967304, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:18.485127: step 5874, loss 0.0775094, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:18.820151: step 5875, loss 0.108905, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:19.324887: step 5876, loss 0.137301, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:19.889516: step 5877, loss 0.0345541, acc 1, learning_rate 0.0001
2017-10-10T13:26:20.316886: step 5878, loss 0.0690093, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:20.759652: step 5879, loss 0.0492482, acc 1, learning_rate 0.0001
2017-10-10T13:26:21.174209: step 5880, loss 0.145933, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:22.562419: step 5880, loss 0.218775, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5880

2017-10-10T13:26:24.150122: step 5881, loss 0.0457516, acc 1, learning_rate 0.0001
2017-10-10T13:26:24.673173: step 5882, loss 0.0463992, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:25.184859: step 5883, loss 0.0934095, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:25.716073: step 5884, loss 0.0750384, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:26.211811: step 5885, loss 0.036105, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:26.725541: step 5886, loss 0.0527297, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:27.248953: step 5887, loss 0.0714062, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:27.755753: step 5888, loss 0.106343, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:28.209118: step 5889, loss 0.0531991, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:28.747485: step 5890, loss 0.104633, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:29.216871: step 5891, loss 0.0567657, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:29.816879: step 5892, loss 0.0662861, acc 1, learning_rate 0.0001
2017-10-10T13:26:30.329588: step 5893, loss 0.0986486, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:30.814377: step 5894, loss 0.0229988, acc 1, learning_rate 0.0001
2017-10-10T13:26:31.196828: step 5895, loss 0.0697945, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:31.725190: step 5896, loss 0.0878838, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:32.242228: step 5897, loss 0.137824, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:32.791859: step 5898, loss 0.0682951, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:33.286460: step 5899, loss 0.0469882, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:33.776223: step 5900, loss 0.0784743, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:34.324862: step 5901, loss 0.112155, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:34.796896: step 5902, loss 0.0707592, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:35.273167: step 5903, loss 0.0828507, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:35.813059: step 5904, loss 0.100705, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:36.299617: step 5905, loss 0.0752493, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:36.758052: step 5906, loss 0.0354918, acc 1, learning_rate 0.0001
2017-10-10T13:26:37.272903: step 5907, loss 0.0435982, acc 1, learning_rate 0.0001
2017-10-10T13:26:37.796701: step 5908, loss 0.0673468, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:38.341058: step 5909, loss 0.0605693, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:38.856883: step 5910, loss 0.0643934, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:39.396573: step 5911, loss 0.0893383, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:39.928852: step 5912, loss 0.119688, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:40.561056: step 5913, loss 0.0301824, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:41.137037: step 5914, loss 0.161741, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:41.594980: step 5915, loss 0.0817416, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:42.065262: step 5916, loss 0.0433486, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:42.600949: step 5917, loss 0.0786789, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:43.135312: step 5918, loss 0.0731311, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:43.527947: step 5919, loss 0.0890086, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:43.948909: step 5920, loss 0.105637, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:45.207620: step 5920, loss 0.219432, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5920

2017-10-10T13:26:46.913165: step 5921, loss 0.0431599, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:47.336404: step 5922, loss 0.0727988, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:47.885375: step 5923, loss 0.0850302, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:48.441006: step 5924, loss 0.0499482, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:49.024065: step 5925, loss 0.0816183, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:49.583318: step 5926, loss 0.111657, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:50.115678: step 5927, loss 0.110956, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:50.647540: step 5928, loss 0.0442357, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:51.205081: step 5929, loss 0.0492399, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:51.738743: step 5930, loss 0.118597, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:52.278965: step 5931, loss 0.0715122, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:52.816890: step 5932, loss 0.1265, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:53.447275: step 5933, loss 0.103137, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:53.953890: step 5934, loss 0.0535687, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:54.378710: step 5935, loss 0.106759, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:54.828821: step 5936, loss 0.0797292, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:55.293295: step 5937, loss 0.0672441, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:55.838672: step 5938, loss 0.0702732, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:56.360472: step 5939, loss 0.0986371, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:56.839753: step 5940, loss 0.0873476, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:57.369009: step 5941, loss 0.086258, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:57.875425: step 5942, loss 0.0539463, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:58.328599: step 5943, loss 0.121783, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:58.864766: step 5944, loss 0.0845104, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:59.397041: step 5945, loss 0.210455, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:00.023923: step 5946, loss 0.0441425, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:00.488848: step 5947, loss 0.0461559, acc 1, learning_rate 0.0001
2017-10-10T13:27:00.988985: step 5948, loss 0.139499, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:01.517057: step 5949, loss 0.126291, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:02.029187: step 5950, loss 0.111734, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:02.560947: step 5951, loss 0.044752, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:03.064855: step 5952, loss 0.031538, acc 1, learning_rate 0.0001
2017-10-10T13:27:03.613386: step 5953, loss 0.0758851, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:04.216832: step 5954, loss 0.0653857, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:04.635915: step 5955, loss 0.0827985, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:05.044838: step 5956, loss 0.109409, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:05.472290: step 5957, loss 0.0912566, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:06.280485: step 5958, loss 0.105495, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:06.738824: step 5959, loss 0.088543, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:07.193080: step 5960, loss 0.0162285, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:08.734059: step 5960, loss 0.217846, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-5960

2017-10-10T13:27:10.228849: step 5961, loss 0.0933293, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:10.764893: step 5962, loss 0.0642775, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:11.296112: step 5963, loss 0.0915677, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:11.841400: step 5964, loss 0.111998, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:12.350933: step 5965, loss 0.127502, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:12.912925: step 5966, loss 0.0192988, acc 1, learning_rate 0.0001
2017-10-10T13:27:13.483756: step 5967, loss 0.127799, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:14.044993: step 5968, loss 0.0949668, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:14.644785: step 5969, loss 0.0361342, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:15.158594: step 5970, loss 0.0548978, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:15.693409: step 5971, loss 0.0294368, acc 1, learning_rate 0.0001
2017-10-10T13:27:16.272855: step 5972, loss 0.0903952, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:16.898749: step 5973, loss 0.0279226, acc 1, learning_rate 0.0001
2017-10-10T13:27:17.355954: step 5974, loss 0.0509302, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:17.820866: step 5975, loss 0.0460899, acc 1, learning_rate 0.0001
2017-10-10T13:27:18.366723: step 5976, loss 0.106621, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:18.891650: step 5977, loss 0.0658413, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:19.379107: step 5978, loss 0.0656392, acc 0.980392, learning_rate 0.0001
2017-10-10T13:27:19.901439: step 5979, loss 0.0874967, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:20.360859: step 5980, loss 0.0383001, acc 1, learning_rate 0.0001
2017-10-10T13:27:20.816878: step 5981, loss 0.0866234, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:21.271588: step 5982, loss 0.0229862, acc 1, learning_rate 0.0001
2017-10-10T13:27:21.828567: step 5983, loss 0.0672311, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:22.365901: step 5984, loss 0.0767339, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:22.804904: step 5985, loss 0.0372405, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:23.370197: step 5986, loss 0.0875345, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:23.853205: step 5987, loss 0.153188, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:24.353707: step 5988, loss 0.0903344, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:24.841406: step 5989, loss 0.128679, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:25.376823: step 5990, loss 0.0450127, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:25.890628: step 5991, loss 0.0522036, acc 1, learning_rate 0.0001
2017-10-10T13:27:26.496850: step 5992, loss 0.0390117, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:27.075181: step 5993, loss 0.0790496, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:27.535839: step 5994, loss 0.0193577, acc 1, learning_rate 0.0001
2017-10-10T13:27:28.016875: step 5995, loss 0.209079, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:28.580081: step 5996, loss 0.097982, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:29.027855: step 5997, loss 0.0889975, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:29.361876: step 5998, loss 0.0511084, acc 1, learning_rate 0.0001
2017-10-10T13:27:29.765528: step 5999, loss 0.147824, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:30.288874: step 6000, loss 0.0190534, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:31.608966: step 6000, loss 0.21747, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6000

2017-10-10T13:27:33.192571: step 6001, loss 0.0674799, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:33.673656: step 6002, loss 0.0744881, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:34.205113: step 6003, loss 0.0230316, acc 1, learning_rate 0.0001
2017-10-10T13:27:34.730882: step 6004, loss 0.215208, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:35.265151: step 6005, loss 0.0437049, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:35.757573: step 6006, loss 0.101693, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:36.252824: step 6007, loss 0.075618, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:36.756373: step 6008, loss 0.127928, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:37.240918: step 6009, loss 0.0691256, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:37.684812: step 6010, loss 0.132444, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:38.108479: step 6011, loss 0.119136, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:38.574841: step 6012, loss 0.0615419, acc 1, learning_rate 0.0001
2017-10-10T13:27:39.169083: step 6013, loss 0.0741448, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:39.864873: step 6014, loss 0.0702363, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:40.424968: step 6015, loss 0.042063, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:40.872856: step 6016, loss 0.0739877, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:41.340877: step 6017, loss 0.0470777, acc 1, learning_rate 0.0001
2017-10-10T13:27:41.901022: step 6018, loss 0.0657108, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:42.486287: step 6019, loss 0.125324, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:42.973294: step 6020, loss 0.0287598, acc 1, learning_rate 0.0001
2017-10-10T13:27:43.477160: step 6021, loss 0.072146, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:43.964397: step 6022, loss 0.0623597, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:44.491260: step 6023, loss 0.0671629, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:44.969335: step 6024, loss 0.0483081, acc 1, learning_rate 0.0001
2017-10-10T13:27:45.480909: step 6025, loss 0.0793778, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:46.025300: step 6026, loss 0.060225, acc 1, learning_rate 0.0001
2017-10-10T13:27:46.572965: step 6027, loss 0.0825941, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:47.117213: step 6028, loss 0.126569, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:47.645496: step 6029, loss 0.0788039, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:48.096844: step 6030, loss 0.111835, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:48.593257: step 6031, loss 0.0929396, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:49.177524: step 6032, loss 0.073668, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:49.737038: step 6033, loss 0.0490717, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:50.213455: step 6034, loss 0.098387, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:50.629869: step 6035, loss 0.0310564, acc 1, learning_rate 0.0001
2017-10-10T13:27:51.132843: step 6036, loss 0.0658085, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:51.704735: step 6037, loss 0.0134678, acc 1, learning_rate 0.0001
2017-10-10T13:27:52.020031: step 6038, loss 0.136372, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:52.383738: step 6039, loss 0.171006, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:52.893552: step 6040, loss 0.144909, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:54.204938: step 6040, loss 0.213775, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6040

2017-10-10T13:27:55.924900: step 6041, loss 0.0430183, acc 1, learning_rate 0.0001
2017-10-10T13:27:56.356968: step 6042, loss 0.0325721, acc 1, learning_rate 0.0001
2017-10-10T13:27:56.890776: step 6043, loss 0.0812712, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:57.372970: step 6044, loss 0.105582, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:57.903856: step 6045, loss 0.0503428, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:58.395911: step 6046, loss 0.0439774, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:58.798255: step 6047, loss 0.0398623, acc 1, learning_rate 0.0001
2017-10-10T13:27:59.269151: step 6048, loss 0.0598894, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:59.795924: step 6049, loss 0.0372663, acc 1, learning_rate 0.0001
2017-10-10T13:28:00.372214: step 6050, loss 0.0761545, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:00.951333: step 6051, loss 0.0338727, acc 1, learning_rate 0.0001
2017-10-10T13:28:01.488973: step 6052, loss 0.0602277, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:02.030032: step 6053, loss 0.155403, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:02.604521: step 6054, loss 0.0734579, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:03.103523: step 6055, loss 0.0855649, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:03.654109: step 6056, loss 0.120503, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:04.224924: step 6057, loss 0.0115423, acc 1, learning_rate 0.0001
2017-10-10T13:28:04.647136: step 6058, loss 0.0675524, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:05.108944: step 6059, loss 0.0479187, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:05.538763: step 6060, loss 0.047411, acc 1, learning_rate 0.0001
2017-10-10T13:28:06.109991: step 6061, loss 0.0569898, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:06.628833: step 6062, loss 0.0898699, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:07.233216: step 6063, loss 0.0606995, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:07.735270: step 6064, loss 0.119491, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:08.273039: step 6065, loss 0.0405732, acc 1, learning_rate 0.0001
2017-10-10T13:28:08.807333: step 6066, loss 0.0598854, acc 1, learning_rate 0.0001
2017-10-10T13:28:09.305496: step 6067, loss 0.0725582, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:09.756083: step 6068, loss 0.122668, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:10.292971: step 6069, loss 0.0221225, acc 1, learning_rate 0.0001
2017-10-10T13:28:10.856906: step 6070, loss 0.0537341, acc 1, learning_rate 0.0001
2017-10-10T13:28:11.385843: step 6071, loss 0.0566889, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:11.899963: step 6072, loss 0.107302, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:12.396871: step 6073, loss 0.0673264, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:12.993374: step 6074, loss 0.0377939, acc 1, learning_rate 0.0001
2017-10-10T13:28:13.448830: step 6075, loss 0.0845694, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:13.868902: step 6076, loss 0.0847846, acc 0.960784, learning_rate 0.0001
2017-10-10T13:28:14.352963: step 6077, loss 0.0609105, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:14.793139: step 6078, loss 0.0408519, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:15.272910: step 6079, loss 0.0405939, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:15.752838: step 6080, loss 0.057037, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:17.088390: step 6080, loss 0.216123, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6080

2017-10-10T13:28:18.545163: step 6081, loss 0.0363578, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:19.085428: step 6082, loss 0.134737, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:19.653083: step 6083, loss 0.07101, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:20.173012: step 6084, loss 0.090444, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:20.702525: step 6085, loss 0.0810929, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:21.100904: step 6086, loss 0.0516367, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:21.658765: step 6087, loss 0.126167, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:22.200863: step 6088, loss 0.0388268, acc 1, learning_rate 0.0001
2017-10-10T13:28:22.757256: step 6089, loss 0.0164383, acc 1, learning_rate 0.0001
2017-10-10T13:28:23.256882: step 6090, loss 0.0486361, acc 1, learning_rate 0.0001
2017-10-10T13:28:23.748954: step 6091, loss 0.032888, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:24.264861: step 6092, loss 0.107202, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:24.852056: step 6093, loss 0.0565711, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:25.478917: step 6094, loss 0.137686, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:25.976323: step 6095, loss 0.0848225, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:26.488875: step 6096, loss 0.053644, acc 1, learning_rate 0.0001
2017-10-10T13:28:27.084833: step 6097, loss 0.120162, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:27.673155: step 6098, loss 0.0926458, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:28.108905: step 6099, loss 0.194034, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:28.596836: step 6100, loss 0.0257966, acc 1, learning_rate 0.0001
2017-10-10T13:28:29.112982: step 6101, loss 0.115487, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:29.631835: step 6102, loss 0.182693, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:30.182230: step 6103, loss 0.0627141, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:30.737189: step 6104, loss 0.0760445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:31.282857: step 6105, loss 0.0624817, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:31.790176: step 6106, loss 0.0850663, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:32.375709: step 6107, loss 0.0611581, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:32.922766: step 6108, loss 0.0346785, acc 1, learning_rate 0.0001
2017-10-10T13:28:33.456777: step 6109, loss 0.076018, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:34.039966: step 6110, loss 0.0965192, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:34.561851: step 6111, loss 0.0541707, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:35.073449: step 6112, loss 0.0556359, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:35.612742: step 6113, loss 0.0772824, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:36.196569: step 6114, loss 0.138762, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:36.669403: step 6115, loss 0.0894576, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:37.161238: step 6116, loss 0.0710494, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:37.764975: step 6117, loss 0.0800919, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:38.255615: step 6118, loss 0.143697, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:38.799193: step 6119, loss 0.0869688, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:39.257015: step 6120, loss 0.0558754, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:40.668827: step 6120, loss 0.216921, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6120

2017-10-10T13:28:42.192856: step 6121, loss 0.0875969, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:42.732497: step 6122, loss 0.0285414, acc 1, learning_rate 0.0001
2017-10-10T13:28:43.269042: step 6123, loss 0.0524502, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:43.804672: step 6124, loss 0.0484544, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:44.361191: step 6125, loss 0.126835, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:44.919372: step 6126, loss 0.163609, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:45.484124: step 6127, loss 0.103763, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:46.064928: step 6128, loss 0.0398815, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:46.551927: step 6129, loss 0.0352669, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:47.092835: step 6130, loss 0.0941191, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:47.618314: step 6131, loss 0.0611014, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:48.169081: step 6132, loss 0.0609054, acc 1, learning_rate 0.0001
2017-10-10T13:28:48.736862: step 6133, loss 0.116185, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:49.271660: step 6134, loss 0.106643, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:49.852842: step 6135, loss 0.0887526, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:50.482575: step 6136, loss 0.0789234, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:51.071217: step 6137, loss 0.136012, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:51.540839: step 6138, loss 0.043209, acc 1, learning_rate 0.0001
2017-10-10T13:28:51.983333: step 6139, loss 0.0537019, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:52.520840: step 6140, loss 0.0668706, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:53.056561: step 6141, loss 0.0478467, acc 1, learning_rate 0.0001
2017-10-10T13:28:53.595139: step 6142, loss 0.134592, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:54.154997: step 6143, loss 0.0283624, acc 1, learning_rate 0.0001
2017-10-10T13:28:54.718312: step 6144, loss 0.142672, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:55.237934: step 6145, loss 0.142162, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:55.814622: step 6146, loss 0.178513, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:56.352879: step 6147, loss 0.0996356, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:56.883728: step 6148, loss 0.0584739, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:57.437471: step 6149, loss 0.075422, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:57.942794: step 6150, loss 0.047265, acc 1, learning_rate 0.0001
2017-10-10T13:28:58.470614: step 6151, loss 0.036068, acc 1, learning_rate 0.0001
2017-10-10T13:28:59.043128: step 6152, loss 0.0671784, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:59.518538: step 6153, loss 0.0401886, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:00.115882: step 6154, loss 0.0974291, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:00.592797: step 6155, loss 0.0805442, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:01.092880: step 6156, loss 0.0699028, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:01.661809: step 6157, loss 0.0396149, acc 1, learning_rate 0.0001
2017-10-10T13:29:02.072834: step 6158, loss 0.0932962, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:02.488847: step 6159, loss 0.0900651, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:02.836926: step 6160, loss 0.0732044, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:04.296806: step 6160, loss 0.21399, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6160

2017-10-10T13:29:05.919178: step 6161, loss 0.0396381, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:06.397666: step 6162, loss 0.0637856, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:06.953321: step 6163, loss 0.0400461, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:07.472990: step 6164, loss 0.10057, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:07.964882: step 6165, loss 0.20819, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:08.463545: step 6166, loss 0.114352, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:09.021964: step 6167, loss 0.0280647, acc 1, learning_rate 0.0001
2017-10-10T13:29:09.526998: step 6168, loss 0.0225432, acc 1, learning_rate 0.0001
2017-10-10T13:29:10.085031: step 6169, loss 0.0593827, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:10.649848: step 6170, loss 0.0795896, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:11.160841: step 6171, loss 0.173941, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:11.762405: step 6172, loss 0.0818554, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:12.284316: step 6173, loss 0.0998059, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:12.740867: step 6174, loss 0.0475424, acc 0.980392, learning_rate 0.0001
2017-10-10T13:29:13.289273: step 6175, loss 0.0781554, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:13.832941: step 6176, loss 0.10702, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:14.353172: step 6177, loss 0.0445246, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:14.908583: step 6178, loss 0.0978573, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:15.345981: step 6179, loss 0.0359465, acc 1, learning_rate 0.0001
2017-10-10T13:29:15.800869: step 6180, loss 0.0822655, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:16.301078: step 6181, loss 0.157327, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:16.872937: step 6182, loss 0.0454249, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:17.356048: step 6183, loss 0.10836, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:17.857874: step 6184, loss 0.108856, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:18.378788: step 6185, loss 0.0409703, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:18.895012: step 6186, loss 0.0359271, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:19.403036: step 6187, loss 0.112815, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:19.902057: step 6188, loss 0.081071, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:20.381861: step 6189, loss 0.0550837, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:20.919050: step 6190, loss 0.0814881, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:21.417036: step 6191, loss 0.039706, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:21.972989: step 6192, loss 0.0450135, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:22.536036: step 6193, loss 0.0694392, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:23.105015: step 6194, loss 0.0691733, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:23.544899: step 6195, loss 0.0364307, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:24.020823: step 6196, loss 0.118187, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:24.452414: step 6197, loss 0.0584346, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:24.824322: step 6198, loss 0.108768, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:25.252834: step 6199, loss 0.0368728, acc 1, learning_rate 0.0001
2017-10-10T13:29:25.670391: step 6200, loss 0.0536694, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:26.906399: step 6200, loss 0.215749, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6200

2017-10-10T13:29:28.791536: step 6201, loss 0.113348, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:29.346087: step 6202, loss 0.0484039, acc 1, learning_rate 0.0001
2017-10-10T13:29:29.894634: step 6203, loss 0.0902598, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:30.428838: step 6204, loss 0.152917, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:30.936870: step 6205, loss 0.0660154, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:31.516804: step 6206, loss 0.12515, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:32.078208: step 6207, loss 0.107597, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:32.645050: step 6208, loss 0.108962, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:33.140520: step 6209, loss 0.0844314, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:33.640889: step 6210, loss 0.0560103, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:34.128990: step 6211, loss 0.0688448, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:34.617009: step 6212, loss 0.020114, acc 1, learning_rate 0.0001
2017-10-10T13:29:35.184874: step 6213, loss 0.112004, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:35.748890: step 6214, loss 0.0611632, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:36.321073: step 6215, loss 0.0603124, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:36.856876: step 6216, loss 0.0571055, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:37.360372: step 6217, loss 0.0949841, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:37.876048: step 6218, loss 0.0502162, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:38.369129: step 6219, loss 0.0688833, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:38.794149: step 6220, loss 0.106763, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:39.246217: step 6221, loss 0.0958834, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:39.796264: step 6222, loss 0.0241585, acc 1, learning_rate 0.0001
2017-10-10T13:29:40.337496: step 6223, loss 0.142856, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:40.845582: step 6224, loss 0.107132, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:41.300809: step 6225, loss 0.0471015, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:41.839594: step 6226, loss 0.0468471, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:42.463220: step 6227, loss 0.109192, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:42.941052: step 6228, loss 0.0961676, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:43.481333: step 6229, loss 0.144215, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:44.001094: step 6230, loss 0.10191, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:44.542722: step 6231, loss 0.0592837, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:45.068860: step 6232, loss 0.0420552, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:45.616905: step 6233, loss 0.106038, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:46.209119: step 6234, loss 0.027863, acc 1, learning_rate 0.0001
2017-10-10T13:29:46.678312: step 6235, loss 0.024264, acc 1, learning_rate 0.0001
2017-10-10T13:29:47.293571: step 6236, loss 0.0985321, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:47.892897: step 6237, loss 0.0431236, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:48.286664: step 6238, loss 0.0878444, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:48.704849: step 6239, loss 0.0187941, acc 1, learning_rate 0.0001
2017-10-10T13:29:49.066293: step 6240, loss 0.0510961, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:50.588959: step 6240, loss 0.214723, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6240

2017-10-10T13:29:52.057787: step 6241, loss 0.116915, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:52.594579: step 6242, loss 0.0962425, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:53.126306: step 6243, loss 0.0360817, acc 1, learning_rate 0.0001
2017-10-10T13:29:53.650747: step 6244, loss 0.0795042, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:54.191954: step 6245, loss 0.0513413, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:54.731652: step 6246, loss 0.0385513, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:55.247900: step 6247, loss 0.0820828, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:55.746300: step 6248, loss 0.175227, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:56.239817: step 6249, loss 0.0237882, acc 1, learning_rate 0.0001
2017-10-10T13:29:56.789720: step 6250, loss 0.0608492, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:57.237028: step 6251, loss 0.0822301, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:57.803214: step 6252, loss 0.0552586, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:58.257073: step 6253, loss 0.0710393, acc 1, learning_rate 0.0001
2017-10-10T13:29:58.733119: step 6254, loss 0.0896385, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:59.287210: step 6255, loss 0.109565, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:59.720053: step 6256, loss 0.0773401, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:00.332906: step 6257, loss 0.105502, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:00.888939: step 6258, loss 0.0329666, acc 1, learning_rate 0.0001
2017-10-10T13:30:01.410605: step 6259, loss 0.0102858, acc 1, learning_rate 0.0001
2017-10-10T13:30:01.839575: step 6260, loss 0.0975919, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:02.313157: step 6261, loss 0.0310061, acc 1, learning_rate 0.0001
2017-10-10T13:30:02.860093: step 6262, loss 0.184396, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:03.398440: step 6263, loss 0.0279339, acc 1, learning_rate 0.0001
2017-10-10T13:30:03.932556: step 6264, loss 0.0736414, acc 1, learning_rate 0.0001
2017-10-10T13:30:04.545020: step 6265, loss 0.0394579, acc 1, learning_rate 0.0001
2017-10-10T13:30:05.080882: step 6266, loss 0.0729825, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:05.632848: step 6267, loss 0.0785318, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:06.119888: step 6268, loss 0.0832629, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:06.624972: step 6269, loss 0.171039, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:07.113026: step 6270, loss 0.0707873, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:07.616951: step 6271, loss 0.0587315, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:08.064806: step 6272, loss 0.154932, acc 0.960784, learning_rate 0.0001
2017-10-10T13:30:08.644874: step 6273, loss 0.0604362, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:09.261627: step 6274, loss 0.150426, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:09.803197: step 6275, loss 0.0582845, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:10.274951: step 6276, loss 0.0856338, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:10.820846: step 6277, loss 0.11022, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:11.363768: step 6278, loss 0.130027, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:11.844290: step 6279, loss 0.0649522, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:12.290019: step 6280, loss 0.0849857, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:13.641133: step 6280, loss 0.214347, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6280

2017-10-10T13:30:15.200497: step 6281, loss 0.0558551, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:15.699532: step 6282, loss 0.0293836, acc 1, learning_rate 0.0001
2017-10-10T13:30:16.228978: step 6283, loss 0.0884957, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:16.661209: step 6284, loss 0.0508485, acc 1, learning_rate 0.0001
2017-10-10T13:30:17.116289: step 6285, loss 0.182376, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:17.595011: step 6286, loss 0.0336613, acc 1, learning_rate 0.0001
2017-10-10T13:30:18.116964: step 6287, loss 0.0369864, acc 1, learning_rate 0.0001
2017-10-10T13:30:18.645090: step 6288, loss 0.179231, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:19.108973: step 6289, loss 0.0797118, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:19.636879: step 6290, loss 0.100449, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:20.157840: step 6291, loss 0.0564892, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:20.704833: step 6292, loss 0.0526706, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:21.258430: step 6293, loss 0.0328656, acc 1, learning_rate 0.0001
2017-10-10T13:30:21.813035: step 6294, loss 0.122843, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:22.321296: step 6295, loss 0.0302379, acc 1, learning_rate 0.0001
2017-10-10T13:30:22.894160: step 6296, loss 0.0775109, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:23.385166: step 6297, loss 0.166933, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:24.048876: step 6298, loss 0.0533955, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:24.621129: step 6299, loss 0.0193928, acc 1, learning_rate 0.0001
2017-10-10T13:30:25.096989: step 6300, loss 0.0383979, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:25.510447: step 6301, loss 0.0917419, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:25.947247: step 6302, loss 0.0507955, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:26.484992: step 6303, loss 0.150238, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:27.008265: step 6304, loss 0.021606, acc 1, learning_rate 0.0001
2017-10-10T13:30:27.556386: step 6305, loss 0.040371, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:28.119212: step 6306, loss 0.0981984, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:28.694011: step 6307, loss 0.044553, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:29.234985: step 6308, loss 0.039837, acc 1, learning_rate 0.0001
2017-10-10T13:30:29.784976: step 6309, loss 0.0408823, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:30.330629: step 6310, loss 0.0570481, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:30.804958: step 6311, loss 0.142899, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:31.243783: step 6312, loss 0.126401, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:31.749026: step 6313, loss 0.00586943, acc 1, learning_rate 0.0001
2017-10-10T13:30:32.203110: step 6314, loss 0.0361127, acc 1, learning_rate 0.0001
2017-10-10T13:30:32.723473: step 6315, loss 0.0785676, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:33.141513: step 6316, loss 0.119118, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:33.668863: step 6317, loss 0.0639124, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:34.203463: step 6318, loss 0.0934178, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:34.679942: step 6319, loss 0.0650073, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:35.134635: step 6320, loss 0.0592913, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:36.514017: step 6320, loss 0.212567, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6320

2017-10-10T13:30:38.383465: step 6321, loss 0.0473159, acc 1, learning_rate 0.0001
2017-10-10T13:30:38.970058: step 6322, loss 0.0303697, acc 1, learning_rate 0.0001
2017-10-10T13:30:39.520803: step 6323, loss 0.0629441, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:40.067795: step 6324, loss 0.056707, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:40.624799: step 6325, loss 0.0303287, acc 1, learning_rate 0.0001
2017-10-10T13:30:41.209267: step 6326, loss 0.0658317, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:41.788925: step 6327, loss 0.139308, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:42.272059: step 6328, loss 0.0343248, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:42.821442: step 6329, loss 0.0231767, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:43.356883: step 6330, loss 0.036908, acc 1, learning_rate 0.0001
2017-10-10T13:30:43.904340: step 6331, loss 0.122123, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:44.422514: step 6332, loss 0.0382291, acc 1, learning_rate 0.0001
2017-10-10T13:30:44.977555: step 6333, loss 0.108563, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:45.547273: step 6334, loss 0.0696076, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:46.103135: step 6335, loss 0.130049, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:46.650090: step 6336, loss 0.0500932, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:47.267435: step 6337, loss 0.100157, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:47.861734: step 6338, loss 0.0160256, acc 1, learning_rate 0.0001
2017-10-10T13:30:48.322401: step 6339, loss 0.0533354, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:48.752979: step 6340, loss 0.0911616, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:49.243549: step 6341, loss 0.0352005, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:49.657150: step 6342, loss 0.0563351, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:50.109247: step 6343, loss 0.117455, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:50.661015: step 6344, loss 0.0569395, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:51.139067: step 6345, loss 0.0612727, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:51.632423: step 6346, loss 0.061267, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:52.118457: step 6347, loss 0.0345122, acc 1, learning_rate 0.0001
2017-10-10T13:30:52.672725: step 6348, loss 0.114018, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:53.221847: step 6349, loss 0.0794154, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:53.788261: step 6350, loss 0.11523, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:54.336750: step 6351, loss 0.0717274, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:54.900956: step 6352, loss 0.0789163, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:55.441227: step 6353, loss 0.122243, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:56.040926: step 6354, loss 0.0914009, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:56.630686: step 6355, loss 0.105258, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:57.109040: step 6356, loss 0.0958878, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:57.589598: step 6357, loss 0.0440999, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:58.009041: step 6358, loss 0.069536, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:58.403438: step 6359, loss 0.0586438, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:58.868900: step 6360, loss 0.0960187, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:00.216968: step 6360, loss 0.215589, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6360

2017-10-10T13:31:01.664622: step 6361, loss 0.121939, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:02.188888: step 6362, loss 0.0489556, acc 1, learning_rate 0.0001
2017-10-10T13:31:02.684886: step 6363, loss 0.096028, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:03.447472: step 6364, loss 0.0488143, acc 1, learning_rate 0.0001
2017-10-10T13:31:04.011607: step 6365, loss 0.122187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:04.524882: step 6366, loss 0.0784139, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:05.070874: step 6367, loss 0.0549044, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:05.612899: step 6368, loss 0.0458405, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:06.174818: step 6369, loss 0.1179, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:06.658716: step 6370, loss 0.0253626, acc 1, learning_rate 0.0001
2017-10-10T13:31:07.172848: step 6371, loss 0.10847, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:07.690643: step 6372, loss 0.0556989, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:08.220035: step 6373, loss 0.0356623, acc 1, learning_rate 0.0001
2017-10-10T13:31:08.708865: step 6374, loss 0.0787823, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:09.228820: step 6375, loss 0.11336, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:09.801637: step 6376, loss 0.120944, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:10.380871: step 6377, loss 0.0414544, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:10.980854: step 6378, loss 0.0790314, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:11.446115: step 6379, loss 0.0320313, acc 1, learning_rate 0.0001
2017-10-10T13:31:11.886166: step 6380, loss 0.0629515, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:12.327676: step 6381, loss 0.0759126, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:12.755257: step 6382, loss 0.0804176, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:13.237050: step 6383, loss 0.0615544, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:13.712926: step 6384, loss 0.0464017, acc 1, learning_rate 0.0001
2017-10-10T13:31:14.261292: step 6385, loss 0.0941124, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:14.728128: step 6386, loss 0.0558924, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:15.217281: step 6387, loss 0.0938941, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:15.790619: step 6388, loss 0.0828005, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:16.349654: step 6389, loss 0.0596984, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:16.879301: step 6390, loss 0.145828, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:17.392033: step 6391, loss 0.0288205, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:17.942819: step 6392, loss 0.0514917, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:18.539313: step 6393, loss 0.053319, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:19.067618: step 6394, loss 0.0234736, acc 1, learning_rate 0.0001
2017-10-10T13:31:19.578000: step 6395, loss 0.0288752, acc 1, learning_rate 0.0001
2017-10-10T13:31:20.140851: step 6396, loss 0.202527, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:20.666068: step 6397, loss 0.0914583, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:21.185269: step 6398, loss 0.0708525, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:21.576625: step 6399, loss 0.0369992, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:22.020926: step 6400, loss 0.0870118, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:23.273823: step 6400, loss 0.21384, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6400

2017-10-10T13:31:24.909014: step 6401, loss 0.0893882, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:25.408563: step 6402, loss 0.0574347, acc 1, learning_rate 0.0001
2017-10-10T13:31:25.933092: step 6403, loss 0.10816, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:26.521185: step 6404, loss 0.156758, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:27.041248: step 6405, loss 0.0544516, acc 1, learning_rate 0.0001
2017-10-10T13:31:27.541581: step 6406, loss 0.0132263, acc 1, learning_rate 0.0001
2017-10-10T13:31:28.116021: step 6407, loss 0.0870946, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:28.636853: step 6408, loss 0.114806, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:29.145700: step 6409, loss 0.0384658, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:29.588961: step 6410, loss 0.117328, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:30.087046: step 6411, loss 0.0609373, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:30.610440: step 6412, loss 0.0455768, acc 1, learning_rate 0.0001
2017-10-10T13:31:31.080922: step 6413, loss 0.0454835, acc 1, learning_rate 0.0001
2017-10-10T13:31:31.637136: step 6414, loss 0.0541307, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:32.104863: step 6415, loss 0.082379, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:32.625780: step 6416, loss 0.0754814, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:33.195913: step 6417, loss 0.0488519, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:33.724863: step 6418, loss 0.171272, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:34.176830: step 6419, loss 0.0402898, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:34.597218: step 6420, loss 0.0643912, acc 1, learning_rate 0.0001
2017-10-10T13:31:35.101820: step 6421, loss 0.0445467, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:35.617208: step 6422, loss 0.0340844, acc 1, learning_rate 0.0001
2017-10-10T13:31:36.173079: step 6423, loss 0.124297, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:36.739170: step 6424, loss 0.0323766, acc 1, learning_rate 0.0001
2017-10-10T13:31:37.246645: step 6425, loss 0.125835, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:37.726699: step 6426, loss 0.0939438, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:38.200956: step 6427, loss 0.0168249, acc 1, learning_rate 0.0001
2017-10-10T13:31:38.776506: step 6428, loss 0.0200865, acc 1, learning_rate 0.0001
2017-10-10T13:31:39.283553: step 6429, loss 0.0440213, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:39.787550: step 6430, loss 0.046656, acc 1, learning_rate 0.0001
2017-10-10T13:31:40.375022: step 6431, loss 0.116444, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:40.992828: step 6432, loss 0.0130553, acc 1, learning_rate 0.0001
2017-10-10T13:31:41.395999: step 6433, loss 0.0537544, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:41.942920: step 6434, loss 0.0346031, acc 1, learning_rate 0.0001
2017-10-10T13:31:42.409052: step 6435, loss 0.0981653, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:42.876852: step 6436, loss 0.110684, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:43.520950: step 6437, loss 0.0691163, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:44.109166: step 6438, loss 0.0251117, acc 1, learning_rate 0.0001
2017-10-10T13:31:44.471748: step 6439, loss 0.0759051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:44.806661: step 6440, loss 0.0604556, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:45.856892: step 6440, loss 0.214299, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6440

2017-10-10T13:31:47.596796: step 6441, loss 0.0521641, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:48.132828: step 6442, loss 0.099911, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:48.664342: step 6443, loss 0.0579764, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:49.164942: step 6444, loss 0.104892, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:49.723643: step 6445, loss 0.124841, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:50.263950: step 6446, loss 0.0559289, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:50.816861: step 6447, loss 0.029134, acc 1, learning_rate 0.0001
2017-10-10T13:31:51.366277: step 6448, loss 0.0722112, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:51.904789: step 6449, loss 0.0856133, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:52.432900: step 6450, loss 0.0588396, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:52.933236: step 6451, loss 0.116566, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:53.409013: step 6452, loss 0.0377735, acc 1, learning_rate 0.0001
2017-10-10T13:31:53.972863: step 6453, loss 0.0448483, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:54.489007: step 6454, loss 0.0823807, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:55.043168: step 6455, loss 0.130925, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:55.613186: step 6456, loss 0.0979258, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:56.182833: step 6457, loss 0.061888, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:56.793025: step 6458, loss 0.0139132, acc 1, learning_rate 0.0001
2017-10-10T13:31:57.285031: step 6459, loss 0.0683703, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:57.737366: step 6460, loss 0.0725899, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:58.194674: step 6461, loss 0.112799, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:58.708951: step 6462, loss 0.185471, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:59.263497: step 6463, loss 0.153581, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:59.809823: step 6464, loss 0.0837756, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:00.333049: step 6465, loss 0.0776049, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:00.824056: step 6466, loss 0.0602855, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:01.289051: step 6467, loss 0.0805528, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:01.659525: step 6468, loss 0.0172869, acc 1, learning_rate 0.0001
2017-10-10T13:32:02.177078: step 6469, loss 0.0759129, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:02.695080: step 6470, loss 0.0877185, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:03.194458: step 6471, loss 0.0418056, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:03.737053: step 6472, loss 0.130493, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:04.326443: step 6473, loss 0.0317168, acc 1, learning_rate 0.0001
2017-10-10T13:32:04.903084: step 6474, loss 0.0970331, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:05.471021: step 6475, loss 0.0982987, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:05.989183: step 6476, loss 0.09215, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:06.656909: step 6477, loss 0.0764695, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:07.321253: step 6478, loss 0.0740856, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:07.623858: step 6479, loss 0.0508799, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:07.945014: step 6480, loss 0.0670854, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:09.022238: step 6480, loss 0.213043, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6480

2017-10-10T13:32:10.434093: step 6481, loss 0.11177, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:10.978299: step 6482, loss 0.0512176, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:11.468532: step 6483, loss 0.0922811, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:11.981015: step 6484, loss 0.0280691, acc 1, learning_rate 0.0001
2017-10-10T13:32:12.538336: step 6485, loss 0.0402391, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:13.091676: step 6486, loss 0.0411319, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:13.646866: step 6487, loss 0.0684079, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:14.241790: step 6488, loss 0.0714155, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:14.723845: step 6489, loss 0.0944526, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:15.215919: step 6490, loss 0.085267, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:15.716975: step 6491, loss 0.113339, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:16.123544: step 6492, loss 0.0697826, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:16.647895: step 6493, loss 0.0946628, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:17.204910: step 6494, loss 0.0398774, acc 1, learning_rate 0.0001
2017-10-10T13:32:17.768908: step 6495, loss 0.109242, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:18.301113: step 6496, loss 0.0871538, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:18.862905: step 6497, loss 0.085392, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:19.516854: step 6498, loss 0.0402037, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:20.142027: step 6499, loss 0.0738857, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:20.585645: step 6500, loss 0.0628064, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:21.164368: step 6501, loss 0.0355869, acc 1, learning_rate 0.0001
2017-10-10T13:32:21.680883: step 6502, loss 0.0261897, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:22.176878: step 6503, loss 0.0438306, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:22.676922: step 6504, loss 0.0370273, acc 1, learning_rate 0.0001
2017-10-10T13:32:23.197033: step 6505, loss 0.0642545, acc 1, learning_rate 0.0001
2017-10-10T13:32:23.736959: step 6506, loss 0.0920367, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:24.221026: step 6507, loss 0.0606335, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:24.820883: step 6508, loss 0.0424363, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:25.353146: step 6509, loss 0.0991163, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:25.874800: step 6510, loss 0.0694419, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:26.364017: step 6511, loss 0.0846149, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:26.869031: step 6512, loss 0.051762, acc 1, learning_rate 0.0001
2017-10-10T13:32:27.383165: step 6513, loss 0.0804019, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:27.891513: step 6514, loss 0.067153, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:28.417908: step 6515, loss 0.0785464, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:28.931300: step 6516, loss 0.0273864, acc 1, learning_rate 0.0001
2017-10-10T13:32:29.508984: step 6517, loss 0.0946267, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:30.105274: step 6518, loss 0.132327, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:30.447458: step 6519, loss 0.034311, acc 1, learning_rate 0.0001
2017-10-10T13:32:30.860129: step 6520, loss 0.15265, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:32.629637: step 6520, loss 0.21526, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6520

2017-10-10T13:32:34.301031: step 6521, loss 0.194922, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:34.600501: step 6522, loss 0.0916017, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:34.981065: step 6523, loss 0.038483, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:35.521259: step 6524, loss 0.113691, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:36.019825: step 6525, loss 0.110552, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:36.575983: step 6526, loss 0.127055, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:37.105065: step 6527, loss 0.0833825, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:37.650983: step 6528, loss 0.086735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:38.194403: step 6529, loss 0.200583, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:38.744687: step 6530, loss 0.0648298, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:39.324172: step 6531, loss 0.0766113, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:39.924912: step 6532, loss 0.11915, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:40.516713: step 6533, loss 0.0524252, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:40.983270: step 6534, loss 0.0401794, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:41.425528: step 6535, loss 0.0487786, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:41.875001: step 6536, loss 0.0980602, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:42.395105: step 6537, loss 0.0331778, acc 1, learning_rate 0.0001
2017-10-10T13:32:42.904835: step 6538, loss 0.0468925, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:43.452863: step 6539, loss 0.0247358, acc 1, learning_rate 0.0001
2017-10-10T13:32:43.960725: step 6540, loss 0.0441106, acc 1, learning_rate 0.0001
2017-10-10T13:32:44.491636: step 6541, loss 0.0578917, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:45.034734: step 6542, loss 0.0259651, acc 1, learning_rate 0.0001
2017-10-10T13:32:45.591778: step 6543, loss 0.0527313, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:46.121680: step 6544, loss 0.0818282, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:46.600835: step 6545, loss 0.0468262, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:47.045083: step 6546, loss 0.125367, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:47.537305: step 6547, loss 0.063748, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:48.060938: step 6548, loss 0.0667739, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:48.572912: step 6549, loss 0.0346224, acc 1, learning_rate 0.0001
2017-10-10T13:32:49.100953: step 6550, loss 0.0955078, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:49.616873: step 6551, loss 0.0228003, acc 1, learning_rate 0.0001
2017-10-10T13:32:50.160861: step 6552, loss 0.0891409, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:50.795150: step 6553, loss 0.120307, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:51.324891: step 6554, loss 0.0695948, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:51.812545: step 6555, loss 0.0842392, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:52.408888: step 6556, loss 0.0414831, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:52.944911: step 6557, loss 0.0571916, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:53.580897: step 6558, loss 0.046107, acc 1, learning_rate 0.0001
2017-10-10T13:32:53.998822: step 6559, loss 0.0333871, acc 1, learning_rate 0.0001
2017-10-10T13:32:54.457876: step 6560, loss 0.0633105, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:55.565004: step 6560, loss 0.213974, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6560

2017-10-10T13:32:57.168924: step 6561, loss 0.0574983, acc 1, learning_rate 0.0001
2017-10-10T13:32:57.758154: step 6562, loss 0.0752573, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:58.212379: step 6563, loss 0.0750649, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:58.676865: step 6564, loss 0.168187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:59.192945: step 6565, loss 0.049003, acc 1, learning_rate 0.0001
2017-10-10T13:32:59.586260: step 6566, loss 0.0279194, acc 1, learning_rate 0.0001
2017-10-10T13:33:00.049034: step 6567, loss 0.0963946, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:00.628802: step 6568, loss 0.0944475, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:01.160965: step 6569, loss 0.0388965, acc 1, learning_rate 0.0001
2017-10-10T13:33:01.677274: step 6570, loss 0.0821885, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:02.145278: step 6571, loss 0.128728, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:02.688980: step 6572, loss 0.0742514, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:03.303332: step 6573, loss 0.0560839, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:03.773001: step 6574, loss 0.0784192, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:04.248872: step 6575, loss 0.0570025, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:04.785328: step 6576, loss 0.0621222, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:05.330769: step 6577, loss 0.190955, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:05.888957: step 6578, loss 0.04694, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:06.432868: step 6579, loss 0.0215527, acc 1, learning_rate 0.0001
2017-10-10T13:33:06.911315: step 6580, loss 0.0738844, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:07.380968: step 6581, loss 0.0857726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:07.885053: step 6582, loss 0.0758085, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:08.404448: step 6583, loss 0.0555866, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:08.957027: step 6584, loss 0.0376998, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:09.461064: step 6585, loss 0.0353268, acc 1, learning_rate 0.0001
2017-10-10T13:33:10.019649: step 6586, loss 0.079141, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:10.555902: step 6587, loss 0.0956816, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:11.081055: step 6588, loss 0.0428871, acc 1, learning_rate 0.0001
2017-10-10T13:33:11.648884: step 6589, loss 0.061174, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:12.156877: step 6590, loss 0.0365253, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:12.660965: step 6591, loss 0.0500651, acc 1, learning_rate 0.0001
2017-10-10T13:33:13.137076: step 6592, loss 0.063359, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:13.680985: step 6593, loss 0.0374073, acc 1, learning_rate 0.0001
2017-10-10T13:33:14.142450: step 6594, loss 0.173401, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:14.637322: step 6595, loss 0.103217, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:15.145788: step 6596, loss 0.107744, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:15.615293: step 6597, loss 0.0695999, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:16.247130: step 6598, loss 0.105042, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:16.744829: step 6599, loss 0.0630919, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:17.174552: step 6600, loss 0.0645933, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:18.274083: step 6600, loss 0.211235, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6600

2017-10-10T13:33:20.123779: step 6601, loss 0.0284593, acc 1, learning_rate 0.0001
2017-10-10T13:33:20.632948: step 6602, loss 0.0448629, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:21.067658: step 6603, loss 0.034745, acc 1, learning_rate 0.0001
2017-10-10T13:33:21.461022: step 6604, loss 0.0968262, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:21.973016: step 6605, loss 0.0639168, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:22.415906: step 6606, loss 0.123307, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:22.964918: step 6607, loss 0.0283057, acc 1, learning_rate 0.0001
2017-10-10T13:33:23.528972: step 6608, loss 0.0783372, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:24.032845: step 6609, loss 0.0547105, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:24.532913: step 6610, loss 0.0427196, acc 1, learning_rate 0.0001
2017-10-10T13:33:25.122899: step 6611, loss 0.0779476, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:25.658748: step 6612, loss 0.0468803, acc 1, learning_rate 0.0001
2017-10-10T13:33:26.152531: step 6613, loss 0.0932037, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:26.766100: step 6614, loss 0.078673, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:27.217173: step 6615, loss 0.0895784, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:27.633065: step 6616, loss 0.0638203, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:28.153827: step 6617, loss 0.121314, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:28.572960: step 6618, loss 0.0287062, acc 1, learning_rate 0.0001
2017-10-10T13:33:29.108189: step 6619, loss 0.0372615, acc 1, learning_rate 0.0001
2017-10-10T13:33:29.613524: step 6620, loss 0.0665135, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:30.139350: step 6621, loss 0.0576087, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:30.683311: step 6622, loss 0.0393294, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:31.250967: step 6623, loss 0.103719, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:31.822693: step 6624, loss 0.0736133, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:32.373220: step 6625, loss 0.050236, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:32.924307: step 6626, loss 0.115445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:33.489373: step 6627, loss 0.0930861, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:34.060958: step 6628, loss 0.0289971, acc 1, learning_rate 0.0001
2017-10-10T13:33:34.608918: step 6629, loss 0.0196239, acc 1, learning_rate 0.0001
2017-10-10T13:33:35.141200: step 6630, loss 0.0929328, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:35.744770: step 6631, loss 0.0526795, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:36.304857: step 6632, loss 0.0810762, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:36.873938: step 6633, loss 0.029407, acc 1, learning_rate 0.0001
2017-10-10T13:33:37.379153: step 6634, loss 0.185035, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:38.020408: step 6635, loss 0.0815632, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:38.579346: step 6636, loss 0.129121, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:39.172917: step 6637, loss 0.0713822, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:39.711732: step 6638, loss 0.0745719, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:40.136184: step 6639, loss 0.0269772, acc 1, learning_rate 0.0001
2017-10-10T13:33:40.632053: step 6640, loss 0.0346527, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:41.748906: step 6640, loss 0.211894, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6640

2017-10-10T13:33:43.069059: step 6641, loss 0.0932327, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:43.623533: step 6642, loss 0.103705, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:44.234654: step 6643, loss 0.106961, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:44.664853: step 6644, loss 0.032209, acc 1, learning_rate 0.0001
2017-10-10T13:33:45.138236: step 6645, loss 0.073579, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:45.652853: step 6646, loss 0.0848604, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:46.188896: step 6647, loss 0.127602, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:46.739668: step 6648, loss 0.0498768, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:47.205524: step 6649, loss 0.0876554, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:47.665059: step 6650, loss 0.0733333, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:48.210362: step 6651, loss 0.0587471, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:48.732880: step 6652, loss 0.0508784, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:49.359417: step 6653, loss 0.0608427, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:49.873526: step 6654, loss 0.113036, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:50.358451: step 6655, loss 0.0953907, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:50.818732: step 6656, loss 0.0661793, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:51.330738: step 6657, loss 0.0367735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:51.864868: step 6658, loss 0.0579532, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:52.431760: step 6659, loss 0.0497126, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:53.024852: step 6660, loss 0.0451355, acc 1, learning_rate 0.0001
2017-10-10T13:33:53.560949: step 6661, loss 0.0827948, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:54.206716: step 6662, loss 0.0389049, acc 1, learning_rate 0.0001
2017-10-10T13:33:54.668997: step 6663, loss 0.0404203, acc 1, learning_rate 0.0001
2017-10-10T13:33:55.124847: step 6664, loss 0.0329296, acc 1, learning_rate 0.0001
2017-10-10T13:33:55.696202: step 6665, loss 0.103123, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:56.252036: step 6666, loss 0.0593434, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:56.752528: step 6667, loss 0.0861859, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:57.245074: step 6668, loss 0.119324, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:57.732955: step 6669, loss 0.0848156, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:58.252614: step 6670, loss 0.134686, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:58.780989: step 6671, loss 0.094963, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:59.248929: step 6672, loss 0.0453347, acc 1, learning_rate 0.0001
2017-10-10T13:33:59.698489: step 6673, loss 0.0399901, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:00.249609: step 6674, loss 0.0788226, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:00.864936: step 6675, loss 0.0361879, acc 1, learning_rate 0.0001
2017-10-10T13:34:01.256951: step 6676, loss 0.0824354, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:01.885087: step 6677, loss 0.0560701, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:02.372886: step 6678, loss 0.117357, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:02.796200: step 6679, loss 0.0851187, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:03.254625: step 6680, loss 0.0428268, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:04.400804: step 6680, loss 0.218541, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6680

2017-10-10T13:34:05.928895: step 6681, loss 0.14527, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:06.545201: step 6682, loss 0.122716, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:07.219046: step 6683, loss 0.101025, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:07.648834: step 6684, loss 0.0877121, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:08.124759: step 6685, loss 0.0801725, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:08.671849: step 6686, loss 0.0709347, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:09.256771: step 6687, loss 0.0417653, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:09.848899: step 6688, loss 0.0623338, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:10.431031: step 6689, loss 0.0780139, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:10.980945: step 6690, loss 0.0567304, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:11.525449: step 6691, loss 0.0426663, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:12.093056: step 6692, loss 0.153104, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:12.667552: step 6693, loss 0.0695703, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:13.080549: step 6694, loss 0.135133, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:13.491005: step 6695, loss 0.017666, acc 1, learning_rate 0.0001
2017-10-10T13:34:13.938473: step 6696, loss 0.0830605, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:14.489060: step 6697, loss 0.0844464, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:15.004888: step 6698, loss 0.0545101, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:15.548319: step 6699, loss 0.035013, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:16.078169: step 6700, loss 0.0668051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:16.637187: step 6701, loss 0.0556559, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:17.178984: step 6702, loss 0.0574199, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:17.692494: step 6703, loss 0.112276, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:18.220299: step 6704, loss 0.0960845, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:18.752127: step 6705, loss 0.080145, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:19.340950: step 6706, loss 0.0348365, acc 1, learning_rate 0.0001
2017-10-10T13:34:19.865535: step 6707, loss 0.0555637, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:20.349047: step 6708, loss 0.0301896, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:20.913356: step 6709, loss 0.112121, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:21.349628: step 6710, loss 0.0274842, acc 1, learning_rate 0.0001
2017-10-10T13:34:21.829015: step 6711, loss 0.0150024, acc 1, learning_rate 0.0001
2017-10-10T13:34:22.428717: step 6712, loss 0.0286494, acc 1, learning_rate 0.0001
2017-10-10T13:34:22.976912: step 6713, loss 0.0328891, acc 1, learning_rate 0.0001
2017-10-10T13:34:23.473096: step 6714, loss 0.0569935, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:23.932926: step 6715, loss 0.0917318, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:24.531644: step 6716, loss 0.0650529, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:25.056284: step 6717, loss 0.0945123, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:25.496863: step 6718, loss 0.0317723, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:26.008895: step 6719, loss 0.022538, acc 1, learning_rate 0.0001
2017-10-10T13:34:26.492254: step 6720, loss 0.0797319, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:27.608022: step 6720, loss 0.215592, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6720

2017-10-10T13:34:29.256118: step 6721, loss 0.0260748, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:29.929691: step 6722, loss 0.0462789, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:30.438556: step 6723, loss 0.0662237, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:30.882390: step 6724, loss 0.144789, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:31.456998: step 6725, loss 0.0213041, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:31.940993: step 6726, loss 0.0702803, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:32.502795: step 6727, loss 0.0454543, acc 1, learning_rate 0.0001
2017-10-10T13:34:33.021547: step 6728, loss 0.0465639, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:33.473106: step 6729, loss 0.0253473, acc 1, learning_rate 0.0001
2017-10-10T13:34:34.005159: step 6730, loss 0.0600201, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:34.509870: step 6731, loss 0.0822356, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:34.974825: step 6732, loss 0.0656863, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:35.495631: step 6733, loss 0.0662952, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:36.092571: step 6734, loss 0.0425654, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:36.601437: step 6735, loss 0.119, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:37.471635: step 6736, loss 0.0874463, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:38.011804: step 6737, loss 0.153551, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:38.548972: step 6738, loss 0.0702922, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:39.097559: step 6739, loss 0.030669, acc 1, learning_rate 0.0001
2017-10-10T13:34:39.534399: step 6740, loss 0.0358673, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:39.991898: step 6741, loss 0.0373207, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:40.524192: step 6742, loss 0.043389, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:41.068917: step 6743, loss 0.0692954, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:41.589176: step 6744, loss 0.120447, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:42.144194: step 6745, loss 0.0375387, acc 1, learning_rate 0.0001
2017-10-10T13:34:42.679369: step 6746, loss 0.0320022, acc 1, learning_rate 0.0001
2017-10-10T13:34:43.167002: step 6747, loss 0.155237, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:43.732380: step 6748, loss 0.0206653, acc 1, learning_rate 0.0001
2017-10-10T13:34:44.231881: step 6749, loss 0.129198, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:44.728421: step 6750, loss 0.0647432, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:45.192092: step 6751, loss 0.0503793, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:45.643366: step 6752, loss 0.0968011, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:46.185084: step 6753, loss 0.0334293, acc 1, learning_rate 0.0001
2017-10-10T13:34:46.652864: step 6754, loss 0.066241, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:47.226287: step 6755, loss 0.0603355, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:47.843483: step 6756, loss 0.0712743, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:48.270817: step 6757, loss 0.0865139, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:48.739634: step 6758, loss 0.0547448, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:49.251228: step 6759, loss 0.114048, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:49.748997: step 6760, loss 0.108186, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:50.967291: step 6760, loss 0.210212, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6760

2017-10-10T13:34:52.452776: step 6761, loss 0.046878, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:53.016046: step 6762, loss 0.110074, acc 0.980392, learning_rate 0.0001
2017-10-10T13:34:53.457146: step 6763, loss 0.0421312, acc 1, learning_rate 0.0001
2017-10-10T13:34:53.925800: step 6764, loss 0.0489844, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:54.389013: step 6765, loss 0.0850873, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:54.865247: step 6766, loss 0.0685661, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:55.357973: step 6767, loss 0.088864, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:55.883078: step 6768, loss 0.0997955, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:56.420094: step 6769, loss 0.083975, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:56.946219: step 6770, loss 0.0524395, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:57.479006: step 6771, loss 0.0124367, acc 1, learning_rate 0.0001
2017-10-10T13:34:58.021912: step 6772, loss 0.0904737, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:58.540864: step 6773, loss 0.0618278, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:59.221734: step 6774, loss 0.0853354, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:59.748836: step 6775, loss 0.0300332, acc 1, learning_rate 0.0001
2017-10-10T13:35:00.152829: step 6776, loss 0.0667515, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:00.609920: step 6777, loss 0.0447885, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:01.001155: step 6778, loss 0.0756206, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:01.461872: step 6779, loss 0.0204426, acc 1, learning_rate 0.0001
2017-10-10T13:35:01.992923: step 6780, loss 0.119064, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:02.488847: step 6781, loss 0.0536462, acc 1, learning_rate 0.0001
2017-10-10T13:35:02.984904: step 6782, loss 0.0523135, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:03.596967: step 6783, loss 0.0848536, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:04.156965: step 6784, loss 0.0162425, acc 1, learning_rate 0.0001
2017-10-10T13:35:04.705002: step 6785, loss 0.0301132, acc 1, learning_rate 0.0001
2017-10-10T13:35:05.237542: step 6786, loss 0.0577017, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:05.816041: step 6787, loss 0.0284082, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:06.248919: step 6788, loss 0.0665292, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:06.763158: step 6789, loss 0.0754974, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:07.297130: step 6790, loss 0.127397, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:07.822339: step 6791, loss 0.073042, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:08.390152: step 6792, loss 0.119422, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:08.877107: step 6793, loss 0.0582007, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:09.316870: step 6794, loss 0.0784572, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:09.872923: step 6795, loss 0.103001, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:10.505390: step 6796, loss 0.0806404, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:10.959399: step 6797, loss 0.0925208, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:11.403746: step 6798, loss 0.0269222, acc 1, learning_rate 0.0001
2017-10-10T13:35:11.905930: step 6799, loss 0.0922252, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:12.482440: step 6800, loss 0.0478464, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:13.654480: step 6800, loss 0.215597, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6800

2017-10-10T13:35:15.328973: step 6801, loss 0.0332121, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:15.982191: step 6802, loss 0.130929, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:16.433381: step 6803, loss 0.066277, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:16.830842: step 6804, loss 0.0860917, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:17.265042: step 6805, loss 0.0383414, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:17.779905: step 6806, loss 0.130584, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:18.320734: step 6807, loss 0.0300883, acc 1, learning_rate 0.0001
2017-10-10T13:35:18.912915: step 6808, loss 0.035173, acc 1, learning_rate 0.0001
2017-10-10T13:35:19.456882: step 6809, loss 0.0554842, acc 1, learning_rate 0.0001
2017-10-10T13:35:19.989236: step 6810, loss 0.0789385, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:20.497138: step 6811, loss 0.0987174, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:21.082410: step 6812, loss 0.131638, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:21.742339: step 6813, loss 0.0184873, acc 1, learning_rate 0.0001
2017-10-10T13:35:22.164830: step 6814, loss 0.0790875, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:22.653588: step 6815, loss 0.0601087, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:23.250601: step 6816, loss 0.037209, acc 1, learning_rate 0.0001
2017-10-10T13:35:23.804919: step 6817, loss 0.0418216, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:24.365004: step 6818, loss 0.0353867, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:24.920931: step 6819, loss 0.0500825, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:25.421346: step 6820, loss 0.0172252, acc 1, learning_rate 0.0001
2017-10-10T13:35:25.953298: step 6821, loss 0.0301931, acc 1, learning_rate 0.0001
2017-10-10T13:35:26.398644: step 6822, loss 0.103586, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:26.951558: step 6823, loss 0.0435551, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:27.540852: step 6824, loss 0.0486039, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:28.095736: step 6825, loss 0.0800272, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:28.665038: step 6826, loss 0.0609133, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:29.209792: step 6827, loss 0.0700719, acc 1, learning_rate 0.0001
2017-10-10T13:35:29.720327: step 6828, loss 0.0312154, acc 1, learning_rate 0.0001
2017-10-10T13:35:30.293984: step 6829, loss 0.0589714, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:30.776977: step 6830, loss 0.0182784, acc 1, learning_rate 0.0001
2017-10-10T13:35:31.328980: step 6831, loss 0.087976, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:31.896850: step 6832, loss 0.0952939, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:32.508010: step 6833, loss 0.13498, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:33.116177: step 6834, loss 0.0539969, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:33.550978: step 6835, loss 0.0525828, acc 1, learning_rate 0.0001
2017-10-10T13:35:33.993003: step 6836, loss 0.0453367, acc 1, learning_rate 0.0001
2017-10-10T13:35:34.529536: step 6837, loss 0.0734494, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:35.085992: step 6838, loss 0.0550456, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:35.619609: step 6839, loss 0.0564199, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:36.152512: step 6840, loss 0.0631046, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:37.340022: step 6840, loss 0.213347, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6840

2017-10-10T13:35:39.240178: step 6841, loss 0.0513781, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:39.728864: step 6842, loss 0.0309486, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:40.166881: step 6843, loss 0.064638, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:40.546344: step 6844, loss 0.0385934, acc 1, learning_rate 0.0001
2017-10-10T13:35:40.991153: step 6845, loss 0.094282, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:41.494932: step 6846, loss 0.0943943, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:42.132891: step 6847, loss 0.0446189, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:42.668945: step 6848, loss 0.0571069, acc 1, learning_rate 0.0001
2017-10-10T13:35:43.197376: step 6849, loss 0.0553766, acc 1, learning_rate 0.0001
2017-10-10T13:35:43.681047: step 6850, loss 0.130761, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:44.151566: step 6851, loss 0.119409, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:44.719969: step 6852, loss 0.0256604, acc 1, learning_rate 0.0001
2017-10-10T13:35:45.284993: step 6853, loss 0.0285787, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:45.795668: step 6854, loss 0.0937445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:46.241098: step 6855, loss 0.149993, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:46.742090: step 6856, loss 0.0669897, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:47.316872: step 6857, loss 0.0505518, acc 1, learning_rate 0.0001
2017-10-10T13:35:47.936839: step 6858, loss 0.0798956, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:48.462138: step 6859, loss 0.0531825, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:48.828783: step 6860, loss 0.0254539, acc 1, learning_rate 0.0001
2017-10-10T13:35:49.360821: step 6861, loss 0.0958563, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:49.872856: step 6862, loss 0.104361, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:50.401585: step 6863, loss 0.0818063, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:50.865198: step 6864, loss 0.06812, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:51.395503: step 6865, loss 0.0369913, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:51.885034: step 6866, loss 0.0182364, acc 1, learning_rate 0.0001
2017-10-10T13:35:52.329916: step 6867, loss 0.118216, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:52.835632: step 6868, loss 0.0655051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:53.434534: step 6869, loss 0.06668, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:53.897836: step 6870, loss 0.0471747, acc 1, learning_rate 0.0001
2017-10-10T13:35:54.359793: step 6871, loss 0.130918, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:54.940939: step 6872, loss 0.0655192, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:55.604965: step 6873, loss 0.0365964, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:56.032893: step 6874, loss 0.0500179, acc 1, learning_rate 0.0001
2017-10-10T13:35:56.445736: step 6875, loss 0.0473285, acc 1, learning_rate 0.0001
2017-10-10T13:35:56.916027: step 6876, loss 0.116717, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:57.369432: step 6877, loss 0.0949123, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:57.889250: step 6878, loss 0.122158, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:58.376037: step 6879, loss 0.101578, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:58.908884: step 6880, loss 0.0362075, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:00.152890: step 6880, loss 0.214564, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6880

2017-10-10T13:36:01.668887: step 6881, loss 0.037554, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:02.296920: step 6882, loss 0.112304, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:02.736550: step 6883, loss 0.103421, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:03.308827: step 6884, loss 0.0318274, acc 1, learning_rate 0.0001
2017-10-10T13:36:03.856836: step 6885, loss 0.0716325, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:04.428803: step 6886, loss 0.112349, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:04.937438: step 6887, loss 0.0859219, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:05.400855: step 6888, loss 0.101829, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:05.937036: step 6889, loss 0.0610754, acc 1, learning_rate 0.0001
2017-10-10T13:36:06.501082: step 6890, loss 0.209847, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:07.165063: step 6891, loss 0.0470398, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:07.717062: step 6892, loss 0.0208865, acc 1, learning_rate 0.0001
2017-10-10T13:36:08.202660: step 6893, loss 0.0296752, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:08.712825: step 6894, loss 0.0973016, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:09.260549: step 6895, loss 0.0739318, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:09.796979: step 6896, loss 0.0314681, acc 1, learning_rate 0.0001
2017-10-10T13:36:10.390472: step 6897, loss 0.111792, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:10.928836: step 6898, loss 0.0450412, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:11.449406: step 6899, loss 0.046535, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:12.007585: step 6900, loss 0.0749904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:12.550538: step 6901, loss 0.0561632, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:13.107611: step 6902, loss 0.21079, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:13.641476: step 6903, loss 0.0598175, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:14.204111: step 6904, loss 0.0191818, acc 1, learning_rate 0.0001
2017-10-10T13:36:14.768880: step 6905, loss 0.112942, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:15.322565: step 6906, loss 0.0738515, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:15.868630: step 6907, loss 0.0363324, acc 1, learning_rate 0.0001
2017-10-10T13:36:16.405236: step 6908, loss 0.0540295, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:16.941098: step 6909, loss 0.0739696, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:17.463827: step 6910, loss 0.0271652, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:18.028109: step 6911, loss 0.0823023, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:18.575375: step 6912, loss 0.0556851, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:19.136509: step 6913, loss 0.0963539, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:19.767798: step 6914, loss 0.044891, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:20.250848: step 6915, loss 0.0474567, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:20.708547: step 6916, loss 0.132419, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:21.139311: step 6917, loss 0.057598, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:21.670767: step 6918, loss 0.0433365, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:22.156973: step 6919, loss 0.0604571, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:22.699058: step 6920, loss 0.0701707, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:23.794688: step 6920, loss 0.214014, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6920

2017-10-10T13:36:25.301125: step 6921, loss 0.0236098, acc 1, learning_rate 0.0001
2017-10-10T13:36:25.735071: step 6922, loss 0.0651456, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:26.217822: step 6923, loss 0.169906, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:26.705353: step 6924, loss 0.0430034, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:27.216298: step 6925, loss 0.0640327, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:27.597061: step 6926, loss 0.100973, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:28.098650: step 6927, loss 0.0822373, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:28.624597: step 6928, loss 0.126619, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:29.205179: step 6929, loss 0.0240186, acc 1, learning_rate 0.0001
2017-10-10T13:36:29.737284: step 6930, loss 0.0170062, acc 1, learning_rate 0.0001
2017-10-10T13:36:30.176885: step 6931, loss 0.0267757, acc 1, learning_rate 0.0001
2017-10-10T13:36:30.744957: step 6932, loss 0.0530343, acc 1, learning_rate 0.0001
2017-10-10T13:36:31.327709: step 6933, loss 0.0600171, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:31.749924: step 6934, loss 0.0580358, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:32.217657: step 6935, loss 0.050626, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:32.871520: step 6936, loss 0.0753029, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:33.341190: step 6937, loss 0.0743193, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:33.848905: step 6938, loss 0.0969184, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:34.403863: step 6939, loss 0.0910455, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:34.960556: step 6940, loss 0.124904, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:35.500828: step 6941, loss 0.0789417, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:36.068623: step 6942, loss 0.128296, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:36.580889: step 6943, loss 0.161368, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:37.138954: step 6944, loss 0.061848, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:37.693325: step 6945, loss 0.125525, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:38.247986: step 6946, loss 0.0532554, acc 1, learning_rate 0.0001
2017-10-10T13:36:38.798488: step 6947, loss 0.0478477, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:39.357466: step 6948, loss 0.0212324, acc 1, learning_rate 0.0001
2017-10-10T13:36:39.908260: step 6949, loss 0.0536215, acc 1, learning_rate 0.0001
2017-10-10T13:36:40.431860: step 6950, loss 0.0754378, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:40.957323: step 6951, loss 0.0760793, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:41.533733: step 6952, loss 0.0531386, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:42.088255: step 6953, loss 0.0848207, acc 1, learning_rate 0.0001
2017-10-10T13:36:42.635413: step 6954, loss 0.0647364, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:43.212601: step 6955, loss 0.0439644, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:43.706721: step 6956, loss 0.0606027, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:44.136451: step 6957, loss 0.0232517, acc 1, learning_rate 0.0001
2017-10-10T13:36:44.546975: step 6958, loss 0.043049, acc 0.980392, learning_rate 0.0001
2017-10-10T13:36:45.053267: step 6959, loss 0.0434363, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:45.508972: step 6960, loss 0.088373, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:46.571890: step 6960, loss 0.216543, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-6960

2017-10-10T13:36:48.425118: step 6961, loss 0.0257947, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:48.871099: step 6962, loss 0.0783648, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:49.302562: step 6963, loss 0.0185424, acc 1, learning_rate 0.0001
2017-10-10T13:36:49.795462: step 6964, loss 0.100577, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:50.361011: step 6965, loss 0.0562183, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:50.888292: step 6966, loss 0.0427504, acc 1, learning_rate 0.0001
2017-10-10T13:36:51.420886: step 6967, loss 0.115137, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:51.963125: step 6968, loss 0.0573903, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:52.478158: step 6969, loss 0.0837804, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:52.932952: step 6970, loss 0.0928147, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:53.429069: step 6971, loss 0.0559366, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:53.936982: step 6972, loss 0.0971659, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:54.500990: step 6973, loss 0.0384984, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:55.033000: step 6974, loss 0.0417224, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:55.476420: step 6975, loss 0.104094, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:55.912976: step 6976, loss 0.0168476, acc 1, learning_rate 0.0001
2017-10-10T13:36:56.433004: step 6977, loss 0.130871, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:56.933371: step 6978, loss 0.0695016, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:57.473713: step 6979, loss 0.183711, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:57.957058: step 6980, loss 0.149538, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:58.473167: step 6981, loss 0.0946262, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:58.905254: step 6982, loss 0.0619514, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:59.444930: step 6983, loss 0.0423691, acc 1, learning_rate 0.0001
2017-10-10T13:37:00.084857: step 6984, loss 0.103359, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:00.596860: step 6985, loss 0.117646, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:01.118700: step 6986, loss 0.0495452, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:01.660868: step 6987, loss 0.0188418, acc 1, learning_rate 0.0001
2017-10-10T13:37:02.192599: step 6988, loss 0.0692876, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:02.756846: step 6989, loss 0.0951602, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:03.240926: step 6990, loss 0.0346834, acc 1, learning_rate 0.0001
2017-10-10T13:37:03.802032: step 6991, loss 0.0246851, acc 1, learning_rate 0.0001
2017-10-10T13:37:04.344851: step 6992, loss 0.0890577, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:04.890463: step 6993, loss 0.00654788, acc 1, learning_rate 0.0001
2017-10-10T13:37:05.412920: step 6994, loss 0.0982565, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:06.114149: step 6995, loss 0.0929262, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:06.616845: step 6996, loss 0.0366802, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:07.048865: step 6997, loss 0.0511436, acc 1, learning_rate 0.0001
2017-10-10T13:37:07.466409: step 6998, loss 0.0242997, acc 1, learning_rate 0.0001
2017-10-10T13:37:08.018208: step 6999, loss 0.111571, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:08.523192: step 7000, loss 0.0779462, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:09.511754: step 7000, loss 0.217392, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7000

2017-10-10T13:37:10.968228: step 7001, loss 0.0280466, acc 1, learning_rate 0.0001
2017-10-10T13:37:11.419433: step 7002, loss 0.0426463, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:11.780220: step 7003, loss 0.0700133, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:12.341026: step 7004, loss 0.085941, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:12.908824: step 7005, loss 0.0462532, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:13.439951: step 7006, loss 0.0376483, acc 1, learning_rate 0.0001
2017-10-10T13:37:13.874800: step 7007, loss 0.0228093, acc 1, learning_rate 0.0001
2017-10-10T13:37:14.277063: step 7008, loss 0.0642527, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:14.837008: step 7009, loss 0.163743, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:15.336998: step 7010, loss 0.0824296, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:15.872847: step 7011, loss 0.14476, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:16.494206: step 7012, loss 0.0578357, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:17.012915: step 7013, loss 0.0677336, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:17.587719: step 7014, loss 0.166122, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:18.165252: step 7015, loss 0.0311524, acc 1, learning_rate 0.0001
2017-10-10T13:37:18.570657: step 7016, loss 0.0640572, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:18.981062: step 7017, loss 0.0689459, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:19.512603: step 7018, loss 0.0865511, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:20.060989: step 7019, loss 0.0687256, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:20.628939: step 7020, loss 0.127645, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:21.129301: step 7021, loss 0.0767444, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:21.568944: step 7022, loss 0.0416008, acc 1, learning_rate 0.0001
2017-10-10T13:37:22.101821: step 7023, loss 0.0232274, acc 1, learning_rate 0.0001
2017-10-10T13:37:22.638792: step 7024, loss 0.0937966, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:23.101084: step 7025, loss 0.0545624, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:23.639575: step 7026, loss 0.0552526, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:24.184977: step 7027, loss 0.089545, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:24.720957: step 7028, loss 0.0472352, acc 1, learning_rate 0.0001
2017-10-10T13:37:25.260984: step 7029, loss 0.0509717, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:25.831961: step 7030, loss 0.0494476, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:26.412622: step 7031, loss 0.0683314, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:26.886977: step 7032, loss 0.0503117, acc 1, learning_rate 0.0001
2017-10-10T13:37:27.332428: step 7033, loss 0.0830668, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:27.872997: step 7034, loss 0.0965472, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:28.439488: step 7035, loss 0.143932, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:29.105030: step 7036, loss 0.0658715, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:29.582033: step 7037, loss 0.0383497, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:30.045128: step 7038, loss 0.0722852, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:30.484950: step 7039, loss 0.0716483, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:31.017138: step 7040, loss 0.058756, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:32.135207: step 7040, loss 0.214518, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7040

2017-10-10T13:37:33.794099: step 7041, loss 0.0696936, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:34.202719: step 7042, loss 0.0775802, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:34.610094: step 7043, loss 0.0762915, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:35.042163: step 7044, loss 0.141811, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:35.577594: step 7045, loss 0.089835, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:36.132863: step 7046, loss 0.0264433, acc 1, learning_rate 0.0001
2017-10-10T13:37:36.684705: step 7047, loss 0.150709, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:37.269010: step 7048, loss 0.055962, acc 1, learning_rate 0.0001
2017-10-10T13:37:37.790967: step 7049, loss 0.176556, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:38.306873: step 7050, loss 0.0392998, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:38.839663: step 7051, loss 0.058637, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:39.349101: step 7052, loss 0.0359493, acc 1, learning_rate 0.0001
2017-10-10T13:37:39.874152: step 7053, loss 0.0793451, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:40.345224: step 7054, loss 0.122445, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:40.921396: step 7055, loss 0.0575341, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:41.349835: step 7056, loss 0.0800567, acc 0.980392, learning_rate 0.0001
2017-10-10T13:37:41.783113: step 7057, loss 0.0154936, acc 1, learning_rate 0.0001
2017-10-10T13:37:42.228877: step 7058, loss 0.0218836, acc 1, learning_rate 0.0001
2017-10-10T13:37:42.811616: step 7059, loss 0.0773887, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:43.347050: step 7060, loss 0.0581704, acc 1, learning_rate 0.0001
2017-10-10T13:37:43.901684: step 7061, loss 0.0150635, acc 1, learning_rate 0.0001
2017-10-10T13:37:44.443696: step 7062, loss 0.0158349, acc 1, learning_rate 0.0001
2017-10-10T13:37:44.967751: step 7063, loss 0.0319072, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:45.520911: step 7064, loss 0.0421597, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:46.058063: step 7065, loss 0.0584769, acc 1, learning_rate 0.0001
2017-10-10T13:37:46.580501: step 7066, loss 0.0465028, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:47.100783: step 7067, loss 0.0588246, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:47.672279: step 7068, loss 0.0359971, acc 1, learning_rate 0.0001
2017-10-10T13:37:48.244334: step 7069, loss 0.0548076, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:48.772412: step 7070, loss 0.0897193, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:49.320923: step 7071, loss 0.0650875, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:49.828571: step 7072, loss 0.0313232, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:50.378288: step 7073, loss 0.101252, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:50.933066: step 7074, loss 0.0630487, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:51.537990: step 7075, loss 0.143572, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:52.100913: step 7076, loss 0.056135, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:52.750399: step 7077, loss 0.0606715, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:53.197963: step 7078, loss 0.0894454, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:53.652871: step 7079, loss 0.0964724, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:54.076891: step 7080, loss 0.0974472, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:55.237856: step 7080, loss 0.21415, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7080

2017-10-10T13:37:57.052317: step 7081, loss 0.0795144, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:57.637026: step 7082, loss 0.0740789, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:58.082592: step 7083, loss 0.051437, acc 1, learning_rate 0.0001
2017-10-10T13:37:58.521422: step 7084, loss 0.0354673, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:59.050100: step 7085, loss 0.0620877, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:59.595299: step 7086, loss 0.0377892, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:00.147235: step 7087, loss 0.0328556, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:00.662363: step 7088, loss 0.124043, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:01.195648: step 7089, loss 0.104719, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:01.728822: step 7090, loss 0.0402225, acc 1, learning_rate 0.0001
2017-10-10T13:38:02.293812: step 7091, loss 0.0288113, acc 1, learning_rate 0.0001
2017-10-10T13:38:02.842854: step 7092, loss 0.0483232, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:03.402952: step 7093, loss 0.0493344, acc 1, learning_rate 0.0001
2017-10-10T13:38:04.046862: step 7094, loss 0.0313619, acc 1, learning_rate 0.0001
2017-10-10T13:38:04.447908: step 7095, loss 0.04482, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:04.915213: step 7096, loss 0.0449217, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:05.316833: step 7097, loss 0.0739004, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:05.836894: step 7098, loss 0.059947, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:06.385463: step 7099, loss 0.0805581, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:06.923429: step 7100, loss 0.0825255, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:07.500083: step 7101, loss 0.0817715, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:08.022548: step 7102, loss 0.0564652, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:08.583283: step 7103, loss 0.0685275, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:09.153176: step 7104, loss 0.0971768, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:09.689627: step 7105, loss 0.0227052, acc 1, learning_rate 0.0001
2017-10-10T13:38:10.235580: step 7106, loss 0.105648, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:10.782365: step 7107, loss 0.0476355, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:11.333189: step 7108, loss 0.0245951, acc 1, learning_rate 0.0001
2017-10-10T13:38:11.891551: step 7109, loss 0.0287948, acc 1, learning_rate 0.0001
2017-10-10T13:38:12.434582: step 7110, loss 0.0654875, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:12.981049: step 7111, loss 0.0670044, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:13.548684: step 7112, loss 0.156957, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:14.078352: step 7113, loss 0.138358, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:14.604275: step 7114, loss 0.122195, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:15.115192: step 7115, loss 0.0581763, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:15.661776: step 7116, loss 0.0532435, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:16.264938: step 7117, loss 0.0544658, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:16.599933: step 7118, loss 0.0639031, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:17.076837: step 7119, loss 0.0944769, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:17.608977: step 7120, loss 0.0703105, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:18.720919: step 7120, loss 0.214358, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7120

2017-10-10T13:38:20.233983: step 7121, loss 0.0468976, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:20.756513: step 7122, loss 0.0750629, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:21.201914: step 7123, loss 0.116226, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:21.646396: step 7124, loss 0.0599722, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:22.161172: step 7125, loss 0.0296145, acc 1, learning_rate 0.0001
2017-10-10T13:38:22.716899: step 7126, loss 0.0760277, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:23.324830: step 7127, loss 0.0503943, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:23.862915: step 7128, loss 0.0402689, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:24.348864: step 7129, loss 0.073645, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:24.876954: step 7130, loss 0.0458365, acc 1, learning_rate 0.0001
2017-10-10T13:38:25.467410: step 7131, loss 0.0440993, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:25.929008: step 7132, loss 0.0823776, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:26.399849: step 7133, loss 0.0632695, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:27.010426: step 7134, loss 0.0992997, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:27.581508: step 7135, loss 0.0309098, acc 1, learning_rate 0.0001
2017-10-10T13:38:28.006057: step 7136, loss 0.0877381, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:28.420888: step 7137, loss 0.049646, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:28.921023: step 7138, loss 0.0494127, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:29.501282: step 7139, loss 0.0724647, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:29.992836: step 7140, loss 0.0619383, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:30.524101: step 7141, loss 0.0544072, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:31.064409: step 7142, loss 0.0443044, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:31.706178: step 7143, loss 0.0467483, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:32.195197: step 7144, loss 0.0794686, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:32.727669: step 7145, loss 0.0775226, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:33.252840: step 7146, loss 0.130902, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:33.812848: step 7147, loss 0.0502875, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:34.379231: step 7148, loss 0.0421735, acc 1, learning_rate 0.0001
2017-10-10T13:38:34.909134: step 7149, loss 0.0605356, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:35.491870: step 7150, loss 0.0548268, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:35.993733: step 7151, loss 0.0846971, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:36.553170: step 7152, loss 0.100909, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:37.105133: step 7153, loss 0.0540152, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:37.532898: step 7154, loss 0.0376911, acc 1, learning_rate 0.0001
2017-10-10T13:38:38.163489: step 7155, loss 0.0872056, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:38.570545: step 7156, loss 0.0796923, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:39.005026: step 7157, loss 0.026896, acc 1, learning_rate 0.0001
2017-10-10T13:38:39.423208: step 7158, loss 0.0760008, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:39.934140: step 7159, loss 0.128904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:40.478775: step 7160, loss 0.0435284, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:41.721117: step 7160, loss 0.213525, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7160

2017-10-10T13:38:43.296902: step 7161, loss 0.0977022, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:43.814502: step 7162, loss 0.0710693, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:44.365201: step 7163, loss 0.0644848, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:44.860857: step 7164, loss 0.050963, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:45.307989: step 7165, loss 0.0664203, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:45.867554: step 7166, loss 0.0785125, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:46.439743: step 7167, loss 0.0214434, acc 1, learning_rate 0.0001
2017-10-10T13:38:47.002453: step 7168, loss 0.0871655, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:47.616438: step 7169, loss 0.0802262, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:48.147475: step 7170, loss 0.0496375, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:48.731400: step 7171, loss 0.0716492, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:49.211968: step 7172, loss 0.0428504, acc 1, learning_rate 0.0001
2017-10-10T13:38:49.781149: step 7173, loss 0.0205381, acc 1, learning_rate 0.0001
2017-10-10T13:38:50.333810: step 7174, loss 0.0422957, acc 1, learning_rate 0.0001
2017-10-10T13:38:50.891395: step 7175, loss 0.0487565, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:51.296887: step 7176, loss 0.0328714, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:51.640835: step 7177, loss 0.0315312, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:52.140874: step 7178, loss 0.0330293, acc 1, learning_rate 0.0001
2017-10-10T13:38:52.655076: step 7179, loss 0.0523724, acc 1, learning_rate 0.0001
2017-10-10T13:38:53.129100: step 7180, loss 0.0659135, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:53.541225: step 7181, loss 0.144533, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:54.042353: step 7182, loss 0.0671545, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:54.516878: step 7183, loss 0.0635415, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:55.035992: step 7184, loss 0.0579128, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:55.556208: step 7185, loss 0.119927, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:56.158955: step 7186, loss 0.0809406, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:56.756355: step 7187, loss 0.0692358, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:57.310060: step 7188, loss 0.0752089, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:57.861205: step 7189, loss 0.0176805, acc 1, learning_rate 0.0001
2017-10-10T13:38:58.383113: step 7190, loss 0.0555159, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:58.900009: step 7191, loss 0.0554842, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:59.425067: step 7192, loss 0.0976919, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:59.884155: step 7193, loss 0.092639, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:00.397296: step 7194, loss 0.0112833, acc 1, learning_rate 0.0001
2017-10-10T13:39:01.009196: step 7195, loss 0.0275814, acc 1, learning_rate 0.0001
2017-10-10T13:39:01.478482: step 7196, loss 0.0458021, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:01.952077: step 7197, loss 0.0261112, acc 1, learning_rate 0.0001
2017-10-10T13:39:02.420815: step 7198, loss 0.0762733, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:02.939242: step 7199, loss 0.046735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:03.516463: step 7200, loss 0.061833, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:04.717073: step 7200, loss 0.213208, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7200

2017-10-10T13:39:06.293036: step 7201, loss 0.0828113, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:06.836284: step 7202, loss 0.100029, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:07.405247: step 7203, loss 0.0582304, acc 1, learning_rate 0.0001
2017-10-10T13:39:07.807475: step 7204, loss 0.0730616, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:08.233430: step 7205, loss 0.0389105, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:08.816970: step 7206, loss 0.0765396, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:09.381286: step 7207, loss 0.102352, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:09.927027: step 7208, loss 0.0508531, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:10.497050: step 7209, loss 0.0133667, acc 1, learning_rate 0.0001
2017-10-10T13:39:10.993127: step 7210, loss 0.0698576, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:11.524940: step 7211, loss 0.121783, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:12.065060: step 7212, loss 0.0227171, acc 1, learning_rate 0.0001
2017-10-10T13:39:12.511267: step 7213, loss 0.135665, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:13.000107: step 7214, loss 0.0651823, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:13.537559: step 7215, loss 0.0606049, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:14.113087: step 7216, loss 0.0639251, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:14.552816: step 7217, loss 0.116198, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:15.008928: step 7218, loss 0.128614, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:15.497209: step 7219, loss 0.0440453, acc 1, learning_rate 0.0001
2017-10-10T13:39:16.049189: step 7220, loss 0.0550621, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:16.581152: step 7221, loss 0.0330105, acc 1, learning_rate 0.0001
2017-10-10T13:39:17.155613: step 7222, loss 0.0654821, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:17.651846: step 7223, loss 0.113721, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:18.119789: step 7224, loss 0.0677237, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:18.633161: step 7225, loss 0.0314184, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:19.127098: step 7226, loss 0.0206319, acc 1, learning_rate 0.0001
2017-10-10T13:39:19.596249: step 7227, loss 0.0589454, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:20.160871: step 7228, loss 0.0726356, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:20.674256: step 7229, loss 0.116912, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:21.212922: step 7230, loss 0.118699, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:21.652927: step 7231, loss 0.0363137, acc 1, learning_rate 0.0001
2017-10-10T13:39:22.166029: step 7232, loss 0.0307958, acc 1, learning_rate 0.0001
2017-10-10T13:39:22.668962: step 7233, loss 0.105065, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:23.157444: step 7234, loss 0.0713862, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:23.773802: step 7235, loss 0.0866171, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:24.304589: step 7236, loss 0.131361, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:24.765089: step 7237, loss 0.126177, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:25.213438: step 7238, loss 0.050137, acc 1, learning_rate 0.0001
2017-10-10T13:39:25.785064: step 7239, loss 0.0592791, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:26.268041: step 7240, loss 0.0199346, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:27.446980: step 7240, loss 0.213773, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7240

2017-10-10T13:39:29.370756: step 7241, loss 0.0299048, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:29.969662: step 7242, loss 0.054258, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:30.404912: step 7243, loss 0.0826567, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:30.859502: step 7244, loss 0.0802632, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:31.264809: step 7245, loss 0.0620737, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:31.841460: step 7246, loss 0.0406667, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:32.349757: step 7247, loss 0.00752417, acc 1, learning_rate 0.0001
2017-10-10T13:39:32.809217: step 7248, loss 0.0668703, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:33.296363: step 7249, loss 0.0365467, acc 1, learning_rate 0.0001
2017-10-10T13:39:33.769138: step 7250, loss 0.0710487, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:34.356994: step 7251, loss 0.0271942, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:34.810459: step 7252, loss 0.151314, acc 0.960784, learning_rate 0.0001
2017-10-10T13:39:35.257109: step 7253, loss 0.0935102, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:35.870599: step 7254, loss 0.0908365, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:36.378201: step 7255, loss 0.0742002, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:36.958145: step 7256, loss 0.0691637, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:37.433974: step 7257, loss 0.101923, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:37.888959: step 7258, loss 0.0252641, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:38.377189: step 7259, loss 0.0284088, acc 1, learning_rate 0.0001
2017-10-10T13:39:38.829090: step 7260, loss 0.0872994, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:39.403677: step 7261, loss 0.0378914, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:39.855779: step 7262, loss 0.04765, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:40.389016: step 7263, loss 0.0974041, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:40.994350: step 7264, loss 0.0377833, acc 1, learning_rate 0.0001
2017-10-10T13:39:41.566809: step 7265, loss 0.0865361, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:42.076555: step 7266, loss 0.0723393, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:42.596182: step 7267, loss 0.0162783, acc 1, learning_rate 0.0001
2017-10-10T13:39:43.129086: step 7268, loss 0.0727127, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:43.824892: step 7269, loss 0.0336811, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:44.340146: step 7270, loss 0.0222452, acc 1, learning_rate 0.0001
2017-10-10T13:39:44.813285: step 7271, loss 0.0461027, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:45.336678: step 7272, loss 0.0300022, acc 1, learning_rate 0.0001
2017-10-10T13:39:45.826887: step 7273, loss 0.0405921, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:46.314027: step 7274, loss 0.126595, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:46.934160: step 7275, loss 0.0479647, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:47.407655: step 7276, loss 0.0885757, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:47.841651: step 7277, loss 0.0915874, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:48.281467: step 7278, loss 0.0441385, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:48.838669: step 7279, loss 0.0227196, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:49.348853: step 7280, loss 0.160104, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:50.472279: step 7280, loss 0.211677, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7280

2017-10-10T13:39:51.836237: step 7281, loss 0.038318, acc 1, learning_rate 0.0001
2017-10-10T13:39:52.280963: step 7282, loss 0.0773961, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:52.798335: step 7283, loss 0.0727393, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:53.428973: step 7284, loss 0.059223, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:53.940854: step 7285, loss 0.0435054, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:54.372128: step 7286, loss 0.217585, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:54.911558: step 7287, loss 0.125735, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:55.435502: step 7288, loss 0.0321956, acc 1, learning_rate 0.0001
2017-10-10T13:39:55.980826: step 7289, loss 0.0875268, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:56.510194: step 7290, loss 0.0678739, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:57.026458: step 7291, loss 0.0479321, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:57.560845: step 7292, loss 0.0939065, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:58.032906: step 7293, loss 0.0738334, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:58.555413: step 7294, loss 0.0212822, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:59.045105: step 7295, loss 0.0913729, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:59.525030: step 7296, loss 0.0343378, acc 1, learning_rate 0.0001
2017-10-10T13:40:00.045136: step 7297, loss 0.034708, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:00.566155: step 7298, loss 0.0713312, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:01.044848: step 7299, loss 0.0598974, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:01.500873: step 7300, loss 0.0858068, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:02.044925: step 7301, loss 0.131885, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:02.572805: step 7302, loss 0.0992336, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:03.124899: step 7303, loss 0.0228683, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:03.636875: step 7304, loss 0.0667967, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:04.053267: step 7305, loss 0.0365203, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:04.456932: step 7306, loss 0.0795743, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:05.016982: step 7307, loss 0.0268509, acc 1, learning_rate 0.0001
2017-10-10T13:40:05.564833: step 7308, loss 0.0520335, acc 1, learning_rate 0.0001
2017-10-10T13:40:06.067768: step 7309, loss 0.0317161, acc 1, learning_rate 0.0001
2017-10-10T13:40:06.609999: step 7310, loss 0.0227618, acc 1, learning_rate 0.0001
2017-10-10T13:40:07.121931: step 7311, loss 0.0620883, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:07.636830: step 7312, loss 0.0689807, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:08.168536: step 7313, loss 0.0389998, acc 1, learning_rate 0.0001
2017-10-10T13:40:08.670472: step 7314, loss 0.0662043, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:09.207778: step 7315, loss 0.125913, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:09.697540: step 7316, loss 0.091292, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:10.292032: step 7317, loss 0.0876886, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:10.820975: step 7318, loss 0.0503194, acc 1, learning_rate 0.0001
2017-10-10T13:40:11.321686: step 7319, loss 0.0498248, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:11.788401: step 7320, loss 0.0661865, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:12.988801: step 7320, loss 0.215021, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7320

2017-10-10T13:40:14.528692: step 7321, loss 0.0385373, acc 1, learning_rate 0.0001
2017-10-10T13:40:15.020454: step 7322, loss 0.013945, acc 1, learning_rate 0.0001
2017-10-10T13:40:15.560055: step 7323, loss 0.0940289, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:16.120834: step 7324, loss 0.0966665, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:16.655847: step 7325, loss 0.0505982, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:17.224841: step 7326, loss 0.0255313, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:17.733941: step 7327, loss 0.0562, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:18.196195: step 7328, loss 0.0218252, acc 1, learning_rate 0.0001
2017-10-10T13:40:18.635344: step 7329, loss 0.0784424, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:19.148136: step 7330, loss 0.0358284, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:19.688079: step 7331, loss 0.0871536, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:20.168833: step 7332, loss 0.150597, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:20.744996: step 7333, loss 0.042767, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:21.272411: step 7334, loss 0.087954, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:21.790612: step 7335, loss 0.0292772, acc 1, learning_rate 0.0001
2017-10-10T13:40:22.261137: step 7336, loss 0.0698372, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:22.878078: step 7337, loss 0.0935736, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:23.317206: step 7338, loss 0.0747048, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:23.769507: step 7339, loss 0.0884952, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:24.262735: step 7340, loss 0.0601593, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:24.752453: step 7341, loss 0.0371909, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:25.272849: step 7342, loss 0.297318, acc 0.84375, learning_rate 0.0001
2017-10-10T13:40:25.743678: step 7343, loss 0.0579204, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:26.387392: step 7344, loss 0.0766225, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:26.932867: step 7345, loss 0.0673709, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:27.462666: step 7346, loss 0.0261522, acc 1, learning_rate 0.0001
2017-10-10T13:40:28.033093: step 7347, loss 0.0875248, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:28.545054: step 7348, loss 0.0243585, acc 1, learning_rate 0.0001
2017-10-10T13:40:29.085047: step 7349, loss 0.0133933, acc 1, learning_rate 0.0001
2017-10-10T13:40:29.567837: step 7350, loss 0.0306844, acc 1, learning_rate 0.0001
2017-10-10T13:40:30.114414: step 7351, loss 0.0592332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:30.766855: step 7352, loss 0.112013, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:31.324837: step 7353, loss 0.0633711, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:31.864283: step 7354, loss 0.121975, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:32.391642: step 7355, loss 0.0720113, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:32.921822: step 7356, loss 0.0369284, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:33.573042: step 7357, loss 0.0603231, acc 1, learning_rate 0.0001
2017-10-10T13:40:34.164859: step 7358, loss 0.0738226, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:34.612347: step 7359, loss 0.0570607, acc 1, learning_rate 0.0001
2017-10-10T13:40:35.128989: step 7360, loss 0.0291657, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:36.504817: step 7360, loss 0.215873, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7360

2017-10-10T13:40:38.185337: step 7361, loss 0.0458634, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:38.645047: step 7362, loss 0.0112887, acc 1, learning_rate 0.0001
2017-10-10T13:40:39.224876: step 7363, loss 0.0588353, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:39.840788: step 7364, loss 0.0699863, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:40.412919: step 7365, loss 0.0731625, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:40.859673: step 7366, loss 0.050179, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:41.217211: step 7367, loss 0.0659441, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:41.614508: step 7368, loss 0.0419547, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:42.160675: step 7369, loss 0.0682098, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:42.736891: step 7370, loss 0.028656, acc 1, learning_rate 0.0001
2017-10-10T13:40:43.246298: step 7371, loss 0.112029, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:43.830670: step 7372, loss 0.0497103, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:44.206018: step 7373, loss 0.0962177, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:44.686248: step 7374, loss 0.0677546, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:45.215956: step 7375, loss 0.0546086, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:45.749481: step 7376, loss 0.0690039, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:46.314760: step 7377, loss 0.0778305, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:46.800089: step 7378, loss 0.0637779, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:47.268863: step 7379, loss 0.0305595, acc 1, learning_rate 0.0001
2017-10-10T13:40:47.803224: step 7380, loss 0.0488538, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:48.381812: step 7381, loss 0.0766352, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:48.925731: step 7382, loss 0.0550035, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:49.429891: step 7383, loss 0.0700076, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:49.909524: step 7384, loss 0.0347422, acc 1, learning_rate 0.0001
2017-10-10T13:40:50.364897: step 7385, loss 0.0105388, acc 1, learning_rate 0.0001
2017-10-10T13:40:50.905213: step 7386, loss 0.0904731, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:51.425271: step 7387, loss 0.0959729, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:52.012990: step 7388, loss 0.0528434, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:52.568875: step 7389, loss 0.0140213, acc 1, learning_rate 0.0001
2017-10-10T13:40:53.145098: step 7390, loss 0.0208239, acc 1, learning_rate 0.0001
2017-10-10T13:40:53.696959: step 7391, loss 0.0785321, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:54.181927: step 7392, loss 0.174108, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:54.697067: step 7393, loss 0.0454957, acc 1, learning_rate 0.0001
2017-10-10T13:40:55.253063: step 7394, loss 0.0751172, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:55.801013: step 7395, loss 0.0506441, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:56.408079: step 7396, loss 0.0671088, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:56.888967: step 7397, loss 0.0915323, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:57.295651: step 7398, loss 0.129237, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:57.723330: step 7399, loss 0.114899, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:58.217278: step 7400, loss 0.0521319, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:59.659209: step 7400, loss 0.213136, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7400

2017-10-10T13:41:01.116903: step 7401, loss 0.0175927, acc 1, learning_rate 0.0001
2017-10-10T13:41:01.599578: step 7402, loss 0.0244185, acc 1, learning_rate 0.0001
2017-10-10T13:41:02.110914: step 7403, loss 0.124083, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:02.623188: step 7404, loss 0.0274966, acc 1, learning_rate 0.0001
2017-10-10T13:41:03.248955: step 7405, loss 0.0644807, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:03.831015: step 7406, loss 0.0769735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:04.286053: step 7407, loss 0.0941263, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:04.789124: step 7408, loss 0.01565, acc 1, learning_rate 0.0001
2017-10-10T13:41:05.324950: step 7409, loss 0.0578598, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:05.823511: step 7410, loss 0.0283378, acc 1, learning_rate 0.0001
2017-10-10T13:41:06.345052: step 7411, loss 0.0625508, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:06.901169: step 7412, loss 0.100749, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:07.413025: step 7413, loss 0.0365035, acc 1, learning_rate 0.0001
2017-10-10T13:41:07.997050: step 7414, loss 0.0928754, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:08.602084: step 7415, loss 0.0377807, acc 1, learning_rate 0.0001
2017-10-10T13:41:09.177029: step 7416, loss 0.0208359, acc 1, learning_rate 0.0001
2017-10-10T13:41:09.679606: step 7417, loss 0.0872035, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:10.067776: step 7418, loss 0.0197387, acc 1, learning_rate 0.0001
2017-10-10T13:41:10.623619: step 7419, loss 0.0928756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:11.152836: step 7420, loss 0.0141904, acc 1, learning_rate 0.0001
2017-10-10T13:41:11.699907: step 7421, loss 0.0781723, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:12.222828: step 7422, loss 0.0827755, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:12.740473: step 7423, loss 0.0824566, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:13.300820: step 7424, loss 0.136644, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:13.808524: step 7425, loss 0.0375848, acc 1, learning_rate 0.0001
2017-10-10T13:41:14.349165: step 7426, loss 0.0299831, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:14.896889: step 7427, loss 0.180939, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:15.398052: step 7428, loss 0.0514487, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:15.906961: step 7429, loss 0.130782, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:16.400877: step 7430, loss 0.0243219, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:16.936774: step 7431, loss 0.0919061, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:17.465496: step 7432, loss 0.031748, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:18.062293: step 7433, loss 0.121069, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:18.516991: step 7434, loss 0.076364, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:19.090380: step 7435, loss 0.0227757, acc 1, learning_rate 0.0001
2017-10-10T13:41:19.592629: step 7436, loss 0.151279, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:20.096869: step 7437, loss 0.0964578, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:20.469024: step 7438, loss 0.0891917, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:20.853110: step 7439, loss 0.0769886, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:21.313582: step 7440, loss 0.0719221, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:22.442631: step 7440, loss 0.212727, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7440

2017-10-10T13:41:24.040890: step 7441, loss 0.0328238, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:24.549474: step 7442, loss 0.0172426, acc 1, learning_rate 0.0001
2017-10-10T13:41:25.077970: step 7443, loss 0.0380031, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:25.628571: step 7444, loss 0.0374353, acc 1, learning_rate 0.0001
2017-10-10T13:41:26.224885: step 7445, loss 0.0302176, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:26.816556: step 7446, loss 0.0140281, acc 1, learning_rate 0.0001
2017-10-10T13:41:27.262013: step 7447, loss 0.0555945, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:27.644951: step 7448, loss 0.0879997, acc 0.960784, learning_rate 0.0001
2017-10-10T13:41:28.085226: step 7449, loss 0.0290169, acc 1, learning_rate 0.0001
2017-10-10T13:41:28.684274: step 7450, loss 0.0342754, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:29.237737: step 7451, loss 0.0510415, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:29.734084: step 7452, loss 0.0621539, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:30.257382: step 7453, loss 0.0507339, acc 1, learning_rate 0.0001
2017-10-10T13:41:30.764602: step 7454, loss 0.0670143, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:31.332854: step 7455, loss 0.134888, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:31.914994: step 7456, loss 0.0386408, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:32.495130: step 7457, loss 0.0442207, acc 1, learning_rate 0.0001
2017-10-10T13:41:32.937935: step 7458, loss 0.0503521, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:33.389188: step 7459, loss 0.0471202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:33.944947: step 7460, loss 0.027417, acc 1, learning_rate 0.0001
2017-10-10T13:41:34.476994: step 7461, loss 0.0630507, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:34.992743: step 7462, loss 0.143681, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:35.525057: step 7463, loss 0.14403, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:36.076606: step 7464, loss 0.0587937, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:36.638342: step 7465, loss 0.0634937, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:37.208991: step 7466, loss 0.0627601, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:37.724694: step 7467, loss 0.0888337, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:38.236446: step 7468, loss 0.0647805, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:38.770706: step 7469, loss 0.0636644, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:39.244894: step 7470, loss 0.00954563, acc 1, learning_rate 0.0001
2017-10-10T13:41:39.757823: step 7471, loss 0.0388876, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:40.206465: step 7472, loss 0.019322, acc 1, learning_rate 0.0001
2017-10-10T13:41:40.736933: step 7473, loss 0.097388, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:41.281037: step 7474, loss 0.0789497, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:41.708895: step 7475, loss 0.0622853, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:42.313328: step 7476, loss 0.035931, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:42.792845: step 7477, loss 0.0321695, acc 1, learning_rate 0.0001
2017-10-10T13:41:43.280902: step 7478, loss 0.101118, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:43.691525: step 7479, loss 0.0393925, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:44.216971: step 7480, loss 0.0978161, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:45.252071: step 7480, loss 0.215104, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7480

2017-10-10T13:41:47.029299: step 7481, loss 0.0688914, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:47.602975: step 7482, loss 0.0300455, acc 1, learning_rate 0.0001
2017-10-10T13:41:48.078434: step 7483, loss 0.0182645, acc 1, learning_rate 0.0001
2017-10-10T13:41:48.585066: step 7484, loss 0.0301481, acc 1, learning_rate 0.0001
2017-10-10T13:41:49.080771: step 7485, loss 0.0259003, acc 1, learning_rate 0.0001
2017-10-10T13:41:49.625120: step 7486, loss 0.0782683, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:50.220985: step 7487, loss 0.146664, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:50.645697: step 7488, loss 0.0690523, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:51.061881: step 7489, loss 0.0261525, acc 1, learning_rate 0.0001
2017-10-10T13:41:51.496051: step 7490, loss 0.0451973, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:51.996938: step 7491, loss 0.0638446, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:52.501921: step 7492, loss 0.179596, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:52.987583: step 7493, loss 0.0185457, acc 1, learning_rate 0.0001
2017-10-10T13:41:53.569164: step 7494, loss 0.138408, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:54.059842: step 7495, loss 0.0569812, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:54.660277: step 7496, loss 0.07681, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:55.204914: step 7497, loss 0.0392939, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:55.548670: step 7498, loss 0.0921707, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:56.020961: step 7499, loss 0.03251, acc 1, learning_rate 0.0001
2017-10-10T13:41:56.488894: step 7500, loss 0.061323, acc 1, learning_rate 0.0001
2017-10-10T13:41:57.025249: step 7501, loss 0.154662, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:57.468870: step 7502, loss 0.0854791, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:58.037088: step 7503, loss 0.143113, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:58.557026: step 7504, loss 0.122792, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:59.028966: step 7505, loss 0.0159175, acc 1, learning_rate 0.0001
2017-10-10T13:41:59.540014: step 7506, loss 0.114438, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:59.985308: step 7507, loss 0.143131, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:00.546635: step 7508, loss 0.157605, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:01.078132: step 7509, loss 0.0588792, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:01.604837: step 7510, loss 0.0436751, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:02.117161: step 7511, loss 0.0936421, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:02.644400: step 7512, loss 0.0672516, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:03.109061: step 7513, loss 0.0535273, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:03.644955: step 7514, loss 0.0365607, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:04.185281: step 7515, loss 0.0411812, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:04.746147: step 7516, loss 0.078436, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:05.300883: step 7517, loss 0.0930022, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:06.044353: step 7518, loss 0.0272147, acc 1, learning_rate 0.0001
2017-10-10T13:42:06.501623: step 7519, loss 0.0249436, acc 1, learning_rate 0.0001
2017-10-10T13:42:07.124176: step 7520, loss 0.0717009, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:08.257182: step 7520, loss 0.212957, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7520

2017-10-10T13:42:09.716824: step 7521, loss 0.0163462, acc 1, learning_rate 0.0001
2017-10-10T13:42:10.229352: step 7522, loss 0.0983989, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:10.651628: step 7523, loss 0.0629838, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:11.146508: step 7524, loss 0.0632324, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:11.696325: step 7525, loss 0.0387921, acc 1, learning_rate 0.0001
2017-10-10T13:42:12.225081: step 7526, loss 0.0607321, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:12.748397: step 7527, loss 0.0280175, acc 1, learning_rate 0.0001
2017-10-10T13:42:13.339635: step 7528, loss 0.0389143, acc 1, learning_rate 0.0001
2017-10-10T13:42:13.846686: step 7529, loss 0.1216, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:14.180951: step 7530, loss 0.0628874, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:14.607930: step 7531, loss 0.0452851, acc 1, learning_rate 0.0001
2017-10-10T13:42:15.123754: step 7532, loss 0.0499152, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:15.688916: step 7533, loss 0.0404195, acc 1, learning_rate 0.0001
2017-10-10T13:42:16.212606: step 7534, loss 0.0566152, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:16.641147: step 7535, loss 0.0611185, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:17.160078: step 7536, loss 0.0179499, acc 1, learning_rate 0.0001
2017-10-10T13:42:17.676993: step 7537, loss 0.0249244, acc 1, learning_rate 0.0001
2017-10-10T13:42:18.271943: step 7538, loss 0.0849031, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:18.911240: step 7539, loss 0.059203, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:19.330623: step 7540, loss 0.060749, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:19.793841: step 7541, loss 0.0678063, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:20.333997: step 7542, loss 0.0697285, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:20.889228: step 7543, loss 0.0860807, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:21.433261: step 7544, loss 0.0819161, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:21.966494: step 7545, loss 0.0421217, acc 1, learning_rate 0.0001
2017-10-10T13:42:22.424995: step 7546, loss 0.0931365, acc 0.960784, learning_rate 0.0001
2017-10-10T13:42:22.922928: step 7547, loss 0.143267, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:23.425120: step 7548, loss 0.0762405, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:24.026258: step 7549, loss 0.0790302, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:24.532862: step 7550, loss 0.0723593, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:25.045129: step 7551, loss 0.0338626, acc 1, learning_rate 0.0001
2017-10-10T13:42:25.536417: step 7552, loss 0.0411391, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:26.024499: step 7553, loss 0.0885484, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:26.568319: step 7554, loss 0.0689801, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:27.140718: step 7555, loss 0.101563, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:27.709116: step 7556, loss 0.0796641, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:28.297594: step 7557, loss 0.163456, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:28.872504: step 7558, loss 0.054952, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:29.336354: step 7559, loss 0.0427973, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:29.804459: step 7560, loss 0.0432149, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:30.895870: step 7560, loss 0.215621, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7560

2017-10-10T13:42:32.477814: step 7561, loss 0.0321167, acc 1, learning_rate 0.0001
2017-10-10T13:42:33.005066: step 7562, loss 0.0751192, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:33.592527: step 7563, loss 0.0473461, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:34.079348: step 7564, loss 0.0673681, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:34.592364: step 7565, loss 0.0566202, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:35.100982: step 7566, loss 0.0899602, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:35.625161: step 7567, loss 0.106542, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:36.132000: step 7568, loss 0.023152, acc 1, learning_rate 0.0001
2017-10-10T13:42:36.723201: step 7569, loss 0.0299464, acc 1, learning_rate 0.0001
2017-10-10T13:42:37.214267: step 7570, loss 0.0304318, acc 1, learning_rate 0.0001
2017-10-10T13:42:37.666487: step 7571, loss 0.0289762, acc 1, learning_rate 0.0001
2017-10-10T13:42:38.195517: step 7572, loss 0.0331275, acc 1, learning_rate 0.0001
2017-10-10T13:42:38.716115: step 7573, loss 0.0164641, acc 1, learning_rate 0.0001
2017-10-10T13:42:39.248856: step 7574, loss 0.0558523, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:39.760441: step 7575, loss 0.0286938, acc 1, learning_rate 0.0001
2017-10-10T13:42:40.312858: step 7576, loss 0.0765567, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:40.855898: step 7577, loss 0.0347794, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:41.426331: step 7578, loss 0.0324779, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:41.845090: step 7579, loss 0.0209666, acc 1, learning_rate 0.0001
2017-10-10T13:42:42.280937: step 7580, loss 0.0282234, acc 1, learning_rate 0.0001
2017-10-10T13:42:42.661208: step 7581, loss 0.043734, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:43.126175: step 7582, loss 0.0143648, acc 1, learning_rate 0.0001
2017-10-10T13:42:43.593028: step 7583, loss 0.0401382, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:44.147290: step 7584, loss 0.0757087, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:44.653148: step 7585, loss 0.0820699, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:45.137150: step 7586, loss 0.0836337, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:45.657288: step 7587, loss 0.0245928, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:46.204900: step 7588, loss 0.0833993, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:46.725081: step 7589, loss 0.0505862, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:47.248077: step 7590, loss 0.0385614, acc 1, learning_rate 0.0001
2017-10-10T13:42:47.764166: step 7591, loss 0.0500589, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:48.240830: step 7592, loss 0.0517168, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:48.824852: step 7593, loss 0.057033, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:49.349139: step 7594, loss 0.0436668, acc 1, learning_rate 0.0001
2017-10-10T13:42:49.808891: step 7595, loss 0.086868, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:50.387834: step 7596, loss 0.0941051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:50.987455: step 7597, loss 0.0682407, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:51.549112: step 7598, loss 0.0881151, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:52.116192: step 7599, loss 0.0513936, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:52.584889: step 7600, loss 0.043555, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:53.716407: step 7600, loss 0.214366, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7600

2017-10-10T13:42:55.268996: step 7601, loss 0.0368752, acc 1, learning_rate 0.0001
2017-10-10T13:42:55.693044: step 7602, loss 0.0860532, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:56.181778: step 7603, loss 0.0715848, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:56.708846: step 7604, loss 0.155155, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:57.157868: step 7605, loss 0.0875806, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:57.728867: step 7606, loss 0.0518659, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:58.253194: step 7607, loss 0.0515041, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:58.772843: step 7608, loss 0.0306684, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:59.366091: step 7609, loss 0.0481035, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:59.937134: step 7610, loss 0.0849149, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:00.409061: step 7611, loss 0.0476473, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:00.832849: step 7612, loss 0.0278769, acc 1, learning_rate 0.0001
2017-10-10T13:43:01.312974: step 7613, loss 0.117465, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:01.865060: step 7614, loss 0.0378557, acc 1, learning_rate 0.0001
2017-10-10T13:43:02.313089: step 7615, loss 0.0358448, acc 1, learning_rate 0.0001
2017-10-10T13:43:02.813225: step 7616, loss 0.0778485, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:03.379993: step 7617, loss 0.0279996, acc 1, learning_rate 0.0001
2017-10-10T13:43:03.995324: step 7618, loss 0.0563616, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:04.398443: step 7619, loss 0.0335116, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:04.776851: step 7620, loss 0.049149, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:05.227756: step 7621, loss 0.0324068, acc 1, learning_rate 0.0001
2017-10-10T13:43:05.784056: step 7622, loss 0.157873, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:06.345001: step 7623, loss 0.131028, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:06.887175: step 7624, loss 0.0696643, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:07.508937: step 7625, loss 0.0284913, acc 1, learning_rate 0.0001
2017-10-10T13:43:08.064322: step 7626, loss 0.0890673, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:08.636882: step 7627, loss 0.121352, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:09.149746: step 7628, loss 0.030096, acc 1, learning_rate 0.0001
2017-10-10T13:43:09.676846: step 7629, loss 0.0598932, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:10.201965: step 7630, loss 0.0500735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:10.752027: step 7631, loss 0.0809215, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:11.245146: step 7632, loss 0.0292525, acc 1, learning_rate 0.0001
2017-10-10T13:43:11.700855: step 7633, loss 0.111471, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:12.234572: step 7634, loss 0.0653877, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:12.720948: step 7635, loss 0.0803798, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:13.221093: step 7636, loss 0.145105, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:13.686405: step 7637, loss 0.0809178, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:14.151320: step 7638, loss 0.0662435, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:14.700906: step 7639, loss 0.0623423, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:15.231148: step 7640, loss 0.0638853, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:16.312973: step 7640, loss 0.21695, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7640

2017-10-10T13:43:18.065701: step 7641, loss 0.0573029, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:18.501434: step 7642, loss 0.00960082, acc 1, learning_rate 0.0001
2017-10-10T13:43:19.005371: step 7643, loss 0.107357, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:19.388488: step 7644, loss 0.0831927, acc 0.960784, learning_rate 0.0001
2017-10-10T13:43:19.917017: step 7645, loss 0.0671021, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:20.462066: step 7646, loss 0.0810004, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:21.042094: step 7647, loss 0.087232, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:21.509018: step 7648, loss 0.029836, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:22.143993: step 7649, loss 0.0566495, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:22.723427: step 7650, loss 0.0881786, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:23.273311: step 7651, loss 0.072629, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:23.704824: step 7652, loss 0.0304163, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:24.112843: step 7653, loss 0.0378425, acc 1, learning_rate 0.0001
2017-10-10T13:43:24.600905: step 7654, loss 0.0968684, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:25.180884: step 7655, loss 0.0166094, acc 1, learning_rate 0.0001
2017-10-10T13:43:25.676948: step 7656, loss 0.0475852, acc 1, learning_rate 0.0001
2017-10-10T13:43:26.182522: step 7657, loss 0.115856, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:26.677368: step 7658, loss 0.0752208, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:27.237092: step 7659, loss 0.0276108, acc 1, learning_rate 0.0001
2017-10-10T13:43:27.740990: step 7660, loss 0.0109898, acc 1, learning_rate 0.0001
2017-10-10T13:43:28.178046: step 7661, loss 0.102954, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:28.664579: step 7662, loss 0.0721451, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:29.211251: step 7663, loss 0.108742, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:29.710946: step 7664, loss 0.0609955, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:30.161060: step 7665, loss 0.136503, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:30.673077: step 7666, loss 0.0260789, acc 1, learning_rate 0.0001
2017-10-10T13:43:31.172879: step 7667, loss 0.0153375, acc 1, learning_rate 0.0001
2017-10-10T13:43:31.732957: step 7668, loss 0.0392424, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:32.290250: step 7669, loss 0.0470035, acc 1, learning_rate 0.0001
2017-10-10T13:43:32.800887: step 7670, loss 0.0387335, acc 1, learning_rate 0.0001
2017-10-10T13:43:33.312849: step 7671, loss 0.0609736, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:33.816156: step 7672, loss 0.121466, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:34.324431: step 7673, loss 0.132998, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:34.809210: step 7674, loss 0.0890432, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:35.384857: step 7675, loss 0.0394291, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:35.916876: step 7676, loss 0.0613, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:36.460817: step 7677, loss 0.0523417, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:36.996865: step 7678, loss 0.0470773, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:37.605111: step 7679, loss 0.0905103, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:38.130758: step 7680, loss 0.0485203, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:39.191598: step 7680, loss 0.215282, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7680

2017-10-10T13:43:40.614294: step 7681, loss 0.0290291, acc 1, learning_rate 0.0001
2017-10-10T13:43:41.241304: step 7682, loss 0.122509, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:41.680610: step 7683, loss 0.0628433, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:42.195488: step 7684, loss 0.0833814, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:42.699222: step 7685, loss 0.0174295, acc 1, learning_rate 0.0001
2017-10-10T13:43:43.192980: step 7686, loss 0.106142, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:43.716924: step 7687, loss 0.0466272, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:44.248119: step 7688, loss 0.0240047, acc 1, learning_rate 0.0001
2017-10-10T13:43:44.800504: step 7689, loss 0.0865477, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:45.348853: step 7690, loss 0.0871293, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:45.881103: step 7691, loss 0.0626374, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:46.374443: step 7692, loss 0.0299516, acc 1, learning_rate 0.0001
2017-10-10T13:43:46.938739: step 7693, loss 0.092118, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:47.388251: step 7694, loss 0.171767, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:47.844819: step 7695, loss 0.141075, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:48.273478: step 7696, loss 0.0554732, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:48.812882: step 7697, loss 0.0516905, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:49.394920: step 7698, loss 0.0884517, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:49.989227: step 7699, loss 0.126554, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:50.430485: step 7700, loss 0.198131, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:50.802418: step 7701, loss 0.0440625, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:51.238170: step 7702, loss 0.0690343, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:51.756834: step 7703, loss 0.111865, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:52.307698: step 7704, loss 0.0592556, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:52.840070: step 7705, loss 0.168408, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:53.344607: step 7706, loss 0.046733, acc 1, learning_rate 0.0001
2017-10-10T13:43:53.842466: step 7707, loss 0.0395866, acc 1, learning_rate 0.0001
2017-10-10T13:43:54.296173: step 7708, loss 0.052463, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:54.707950: step 7709, loss 0.088627, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:55.240006: step 7710, loss 0.144801, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:55.736951: step 7711, loss 0.0162102, acc 1, learning_rate 0.0001
2017-10-10T13:43:56.255539: step 7712, loss 0.09618, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:56.785627: step 7713, loss 0.022217, acc 1, learning_rate 0.0001
2017-10-10T13:43:57.260839: step 7714, loss 0.0570667, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:57.780323: step 7715, loss 0.0808795, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:58.348951: step 7716, loss 0.0150245, acc 1, learning_rate 0.0001
2017-10-10T13:43:58.912786: step 7717, loss 0.0134083, acc 1, learning_rate 0.0001
2017-10-10T13:43:59.512901: step 7718, loss 0.0185083, acc 1, learning_rate 0.0001
2017-10-10T13:44:00.031182: step 7719, loss 0.0294767, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:00.601002: step 7720, loss 0.0532915, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:44:01.683750: step 7720, loss 0.212436, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7720

2017-10-10T13:44:03.161425: step 7721, loss 0.0739376, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:03.655774: step 7722, loss 0.103504, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:04.128978: step 7723, loss 0.0223857, acc 1, learning_rate 0.0001
2017-10-10T13:44:04.600911: step 7724, loss 0.0999183, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:05.141104: step 7725, loss 0.0417242, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:05.704474: step 7726, loss 0.0520413, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:06.246424: step 7727, loss 0.11894, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:06.742716: step 7728, loss 0.0435969, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:07.256885: step 7729, loss 0.0595013, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:07.781013: step 7730, loss 0.0174095, acc 1, learning_rate 0.0001
2017-10-10T13:44:08.324876: step 7731, loss 0.081153, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:08.840918: step 7732, loss 0.0342791, acc 1, learning_rate 0.0001
2017-10-10T13:44:09.353870: step 7733, loss 0.0896494, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:09.951047: step 7734, loss 0.111896, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:10.484573: step 7735, loss 0.00753939, acc 1, learning_rate 0.0001
2017-10-10T13:44:10.921018: step 7736, loss 0.134604, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:11.349733: step 7737, loss 0.0237218, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:11.828928: step 7738, loss 0.0306386, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:12.283945: step 7739, loss 0.015911, acc 1, learning_rate 0.0001
2017-10-10T13:44:12.809645: step 7740, loss 0.0836087, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:13.377371: step 7741, loss 0.0437098, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:13.752074: step 7742, loss 0.0360655, acc 0.980392, learning_rate 0.0001
2017-10-10T13:44:14.188933: step 7743, loss 0.0626949, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:14.682603: step 7744, loss 0.0258447, acc 1, learning_rate 0.0001
2017-10-10T13:44:15.276257: step 7745, loss 0.0654226, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:15.780833: step 7746, loss 0.0830757, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:16.296415: step 7747, loss 0.0536925, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:16.800978: step 7748, loss 0.0686608, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:17.349271: step 7749, loss 0.0682069, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:17.896067: step 7750, loss 0.0294628, acc 1, learning_rate 0.0001
2017-10-10T13:44:18.376890: step 7751, loss 0.0449233, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:18.884897: step 7752, loss 0.044946, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:19.408865: step 7753, loss 0.0863877, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:19.938864: step 7754, loss 0.0479895, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:20.504856: step 7755, loss 0.0708656, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:21.044839: step 7756, loss 0.0734052, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:21.528279: step 7757, loss 0.119499, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:22.068837: step 7758, loss 0.0561666, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:22.592271: step 7759, loss 0.0372811, acc 1, learning_rate 0.0001
2017-10-10T13:44:23.096910: step 7760, loss 0.095271, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:44:41.060079: step 7760, loss 0.212872, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7760

2017-10-10T13:44:59.537998: step 7761, loss 0.081294, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:00.453618: step 7762, loss 0.0530662, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:01.408869: step 7763, loss 0.0213795, acc 1, learning_rate 0.0001
2017-10-10T13:45:01.733647: step 7764, loss 0.0262857, acc 1, learning_rate 0.0001
2017-10-10T13:45:02.277167: step 7765, loss 0.058687, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:02.567850: step 7766, loss 0.0443399, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:02.879216: step 7767, loss 0.091919, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:03.863530: step 7768, loss 0.0308204, acc 1, learning_rate 0.0001
2017-10-10T13:45:04.576110: step 7769, loss 0.0618681, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:05.537931: step 7770, loss 0.0462402, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:06.238930: step 7771, loss 0.080233, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:06.402209: step 7772, loss 0.0174633, acc 1, learning_rate 0.0001
2017-10-10T13:45:06.562576: step 7773, loss 0.0397939, acc 1, learning_rate 0.0001
2017-10-10T13:45:06.847358: step 7774, loss 0.0904411, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:07.325230: step 7775, loss 0.0682284, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:07.545090: step 7776, loss 0.0867227, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:07.760010: step 7777, loss 0.0771498, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:08.062660: step 7778, loss 0.0821183, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:09.807739: step 7779, loss 0.108914, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:10.281217: step 7780, loss 0.105238, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:10.464357: step 7781, loss 0.0314375, acc 1, learning_rate 0.0001
2017-10-10T13:45:10.738874: step 7782, loss 0.0591325, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:11.062008: step 7783, loss 0.0484193, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:11.304153: step 7784, loss 0.0680137, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:11.604406: step 7785, loss 0.0163844, acc 1, learning_rate 0.0001
2017-10-10T13:45:11.940839: step 7786, loss 0.0665763, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:12.728722: step 7787, loss 0.0494322, acc 1, learning_rate 0.0001
2017-10-10T13:45:12.893356: step 7788, loss 0.0615043, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:13.052618: step 7789, loss 0.0142744, acc 1, learning_rate 0.0001
2017-10-10T13:45:13.271414: step 7790, loss 0.0505851, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:13.433782: step 7791, loss 0.0637129, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:13.594773: step 7792, loss 0.0272837, acc 1, learning_rate 0.0001
2017-10-10T13:45:13.753714: step 7793, loss 0.0409091, acc 1, learning_rate 0.0001
2017-10-10T13:45:13.913973: step 7794, loss 0.156636, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:14.255095: step 7795, loss 0.0347121, acc 1, learning_rate 0.0001
2017-10-10T13:45:14.435427: step 7796, loss 0.0409596, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:14.952743: step 7797, loss 0.0567317, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:15.124951: step 7798, loss 0.0867535, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:15.504042: step 7799, loss 0.057484, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:16.484682: step 7800, loss 0.11225, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:45:16.895906: step 7800, loss 0.214666, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7800

2017-10-10T13:45:17.477062: step 7801, loss 0.0170535, acc 1, learning_rate 0.0001
2017-10-10T13:45:17.640968: step 7802, loss 0.0551143, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:17.857568: step 7803, loss 0.0383225, acc 1, learning_rate 0.0001
2017-10-10T13:45:18.027544: step 7804, loss 0.0387278, acc 1, learning_rate 0.0001
2017-10-10T13:45:18.188692: step 7805, loss 0.0314555, acc 1, learning_rate 0.0001
2017-10-10T13:45:18.351265: step 7806, loss 0.0490283, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:18.530373: step 7807, loss 0.0320364, acc 1, learning_rate 0.0001
2017-10-10T13:45:19.080986: step 7808, loss 0.0587508, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:19.262799: step 7809, loss 0.0577519, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:19.428844: step 7810, loss 0.0234832, acc 1, learning_rate 0.0001
2017-10-10T13:45:19.597079: step 7811, loss 0.10767, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:19.761630: step 7812, loss 0.0426701, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:19.923151: step 7813, loss 0.0279059, acc 1, learning_rate 0.0001
2017-10-10T13:45:20.087876: step 7814, loss 0.0349681, acc 1, learning_rate 0.0001
2017-10-10T13:45:20.249953: step 7815, loss 0.0437864, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:20.414593: step 7816, loss 0.0205623, acc 1, learning_rate 0.0001
2017-10-10T13:45:20.579728: step 7817, loss 0.0428564, acc 1, learning_rate 0.0001
2017-10-10T13:45:20.739674: step 7818, loss 0.018516, acc 1, learning_rate 0.0001
2017-10-10T13:45:20.933848: step 7819, loss 0.0679439, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:21.099407: step 7820, loss 0.0573636, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:21.348021: step 7821, loss 0.0206931, acc 1, learning_rate 0.0001
2017-10-10T13:45:21.863192: step 7822, loss 0.114342, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:22.054638: step 7823, loss 0.0296141, acc 1, learning_rate 0.0001
2017-10-10T13:45:22.217704: step 7824, loss 0.12716, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:22.381470: step 7825, loss 0.0169063, acc 1, learning_rate 0.0001
2017-10-10T13:45:22.545949: step 7826, loss 0.082654, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:22.707633: step 7827, loss 0.040821, acc 1, learning_rate 0.0001
2017-10-10T13:45:22.871265: step 7828, loss 0.0506924, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:23.102002: step 7829, loss 0.0173047, acc 1, learning_rate 0.0001
2017-10-10T13:45:23.264243: step 7830, loss 0.0254856, acc 1, learning_rate 0.0001
2017-10-10T13:45:23.430914: step 7831, loss 0.0604746, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:23.594008: step 7832, loss 0.0234492, acc 1, learning_rate 0.0001
2017-10-10T13:45:23.756596: step 7833, loss 0.0334063, acc 1, learning_rate 0.0001
2017-10-10T13:45:23.917843: step 7834, loss 0.128033, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:24.079018: step 7835, loss 0.120694, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:24.239825: step 7836, loss 0.0965819, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:24.404719: step 7837, loss 0.0666951, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:24.566134: step 7838, loss 0.121181, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:24.728552: step 7839, loss 0.0545186, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:24.862575: step 7840, loss 0.197049, acc 0.921569, learning_rate 0.0001

Evaluation:
2017-10-10T13:45:25.277278: step 7840, loss 0.213418, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657072/checkpoints/model-7840

