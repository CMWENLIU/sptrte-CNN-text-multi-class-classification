
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6954
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
Vocabulary Size: 1936201
Train/Dev split: 6259/695
Writing to /home/sheep/bigdata/runs/1507000132

Load glove file /home/sheep/bigdata/d7000/myglove.glove
glove file has been loaded

2017-10-02T22:12:14.712750: step 1, loss 2.57119, acc 0.171875, learning_rate 0.005
2017-10-02T22:12:16.761368: step 2, loss 2.77573, acc 0.421875, learning_rate 0.00498
2017-10-02T22:12:17.947118: step 3, loss 3.09161, acc 0.265625, learning_rate 0.00496008
2017-10-02T22:12:19.122186: step 4, loss 1.59172, acc 0.390625, learning_rate 0.00494024
2017-10-02T22:12:20.306962: step 5, loss 2.41056, acc 0.125, learning_rate 0.00492049
2017-10-02T22:12:21.482802: step 6, loss 2.15833, acc 0.0625, learning_rate 0.00490081
2017-10-02T22:12:22.643138: step 7, loss 2.29179, acc 0.15625, learning_rate 0.00488121
2017-10-02T22:12:23.807706: step 8, loss 2.18616, acc 0.140625, learning_rate 0.0048617
2017-10-02T22:12:24.980570: step 9, loss 1.8854, acc 0.234375, learning_rate 0.00484226
2017-10-02T22:12:26.157021: step 10, loss 1.75113, acc 0.265625, learning_rate 0.00482291
2017-10-02T22:12:27.329848: step 11, loss 1.95353, acc 0.25, learning_rate 0.00480363
2017-10-02T22:12:28.525010: step 12, loss 1.89628, acc 0.296875, learning_rate 0.00478443
2017-10-02T22:12:29.679710: step 13, loss 1.91483, acc 0.34375, learning_rate 0.00476531
2017-10-02T22:12:30.864775: step 14, loss 1.92762, acc 0.34375, learning_rate 0.00474627
2017-10-02T22:12:32.039802: step 15, loss 1.79703, acc 0.328125, learning_rate 0.0047273
2017-10-02T22:12:33.215108: step 16, loss 1.75635, acc 0.265625, learning_rate 0.00470841
2017-10-02T22:12:34.394601: step 17, loss 1.6057, acc 0.25, learning_rate 0.0046896
2017-10-02T22:12:35.555278: step 18, loss 1.60161, acc 0.265625, learning_rate 0.00467087
2017-10-02T22:12:36.744935: step 19, loss 1.58452, acc 0.265625, learning_rate 0.00465221
2017-10-02T22:12:37.904440: step 20, loss 1.80887, acc 0.15625, learning_rate 0.00463363
2017-10-02T22:12:39.081598: step 21, loss 1.78054, acc 0.140625, learning_rate 0.00461513
2017-10-02T22:12:40.258209: step 22, loss 1.5372, acc 0.265625, learning_rate 0.0045967
2017-10-02T22:12:41.454759: step 23, loss 1.53746, acc 0.390625, learning_rate 0.00457834
2017-10-02T22:12:42.641396: step 24, loss 1.61178, acc 0.359375, learning_rate 0.00456006
2017-10-02T22:12:43.830497: step 25, loss 1.57581, acc 0.28125, learning_rate 0.00454186
2017-10-02T22:12:45.007093: step 26, loss 1.51514, acc 0.296875, learning_rate 0.00452373
2017-10-02T22:12:46.171433: step 27, loss 1.58351, acc 0.34375, learning_rate 0.00450567
2017-10-02T22:12:47.331158: step 28, loss 1.75089, acc 0.28125, learning_rate 0.00448769
2017-10-02T22:12:48.513075: step 29, loss 1.50947, acc 0.3125, learning_rate 0.00446978
2017-10-02T22:12:49.669844: step 30, loss 1.50168, acc 0.34375, learning_rate 0.00445194
2017-10-02T22:12:50.858879: step 31, loss 1.72492, acc 0.265625, learning_rate 0.00443418
2017-10-02T22:12:52.014579: step 32, loss 1.57094, acc 0.296875, learning_rate 0.00441649
2017-10-02T22:12:53.190884: step 33, loss 1.40653, acc 0.40625, learning_rate 0.00439887
2017-10-02T22:12:54.364980: step 34, loss 1.65739, acc 0.234375, learning_rate 0.00438132
2017-10-02T22:12:55.529565: step 35, loss 1.51014, acc 0.328125, learning_rate 0.00436385
2017-10-02T22:12:56.711852: step 36, loss 1.47818, acc 0.296875, learning_rate 0.00434644
2017-10-02T22:12:57.893595: step 37, loss 1.56187, acc 0.328125, learning_rate 0.00432911
2017-10-02T22:12:59.063574: step 38, loss 1.52925, acc 0.40625, learning_rate 0.00431185
2017-10-02T22:13:00.262978: step 39, loss 1.48946, acc 0.3125, learning_rate 0.00429465
2017-10-02T22:13:01.440013: step 40, loss 1.60699, acc 0.234375, learning_rate 0.00427753

Evaluation:
2017-10-02T22:13:02.146497: step 40, loss 1.50564, acc 0.315108

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-40

2017-10-02T22:13:17.670403: step 41, loss 1.46487, acc 0.296875, learning_rate 0.00426048
2017-10-02T22:13:19.362872: step 42, loss 1.57501, acc 0.296875, learning_rate 0.0042435
2017-10-02T22:13:20.561913: step 43, loss 1.47818, acc 0.359375, learning_rate 0.00422659
2017-10-02T22:13:21.734415: step 44, loss 1.47997, acc 0.328125, learning_rate 0.00420974
2017-10-02T22:13:22.930672: step 45, loss 1.65835, acc 0.296875, learning_rate 0.00419297
2017-10-02T22:13:24.099151: step 46, loss 1.52433, acc 0.34375, learning_rate 0.00417626
2017-10-02T22:13:25.298167: step 47, loss 1.53934, acc 0.328125, learning_rate 0.00415962
2017-10-02T22:13:26.476700: step 48, loss 1.49713, acc 0.328125, learning_rate 0.00414305
2017-10-02T22:13:27.657504: step 49, loss 1.54981, acc 0.296875, learning_rate 0.00412655
2017-10-02T22:13:28.848789: step 50, loss 1.56874, acc 0.296875, learning_rate 0.00411011
2017-10-02T22:13:30.016410: step 51, loss 1.49959, acc 0.390625, learning_rate 0.00409375
2017-10-02T22:13:31.187052: step 52, loss 1.47813, acc 0.3125, learning_rate 0.00407744
2017-10-02T22:13:32.360402: step 53, loss 1.55072, acc 0.3125, learning_rate 0.00406121
2017-10-02T22:13:33.538295: step 54, loss 1.41458, acc 0.4375, learning_rate 0.00404504
2017-10-02T22:13:34.705117: step 55, loss 1.43919, acc 0.40625, learning_rate 0.00402894
2017-10-02T22:13:35.875912: step 56, loss 1.50887, acc 0.25, learning_rate 0.0040129
2017-10-02T22:13:37.045619: step 57, loss 1.55158, acc 0.234375, learning_rate 0.00399693
2017-10-02T22:13:38.228806: step 58, loss 1.45829, acc 0.4375, learning_rate 0.00398102
2017-10-02T22:13:39.412663: step 59, loss 1.48478, acc 0.34375, learning_rate 0.00396518
2017-10-02T22:13:40.601891: step 60, loss 1.50204, acc 0.359375, learning_rate 0.00394941
2017-10-02T22:13:41.852981: step 61, loss 1.50955, acc 0.34375, learning_rate 0.00393369
2017-10-02T22:13:43.010109: step 62, loss 1.40703, acc 0.40625, learning_rate 0.00391804
2017-10-02T22:13:44.128413: step 63, loss 1.59852, acc 0.3125, learning_rate 0.00390246
2017-10-02T22:13:45.315312: step 64, loss 1.4865, acc 0.3125, learning_rate 0.00388694
2017-10-02T22:13:46.491545: step 65, loss 1.60609, acc 0.25, learning_rate 0.00387148
2017-10-02T22:13:47.653104: step 66, loss 1.55266, acc 0.328125, learning_rate 0.00385609
2017-10-02T22:13:48.822468: step 67, loss 1.53849, acc 0.3125, learning_rate 0.00384076
2017-10-02T22:13:49.996817: step 68, loss 1.48197, acc 0.328125, learning_rate 0.00382549
2017-10-02T22:13:51.172613: step 69, loss 1.45297, acc 0.40625, learning_rate 0.00381028
2017-10-02T22:13:52.348516: step 70, loss 1.47082, acc 0.375, learning_rate 0.00379514
2017-10-02T22:13:53.469213: step 71, loss 1.43719, acc 0.390625, learning_rate 0.00378005
2017-10-02T22:13:54.641901: step 72, loss 1.47516, acc 0.4375, learning_rate 0.00376503
2017-10-02T22:13:55.838369: step 73, loss 1.54476, acc 0.375, learning_rate 0.00375007
2017-10-02T22:13:57.012349: step 74, loss 1.66808, acc 0.21875, learning_rate 0.00373517
2017-10-02T22:13:58.185832: step 75, loss 1.55654, acc 0.28125, learning_rate 0.00372034
2017-10-02T22:13:59.348532: step 76, loss 1.48663, acc 0.375, learning_rate 0.00370556
2017-10-02T22:14:00.509960: step 77, loss 1.59534, acc 0.171875, learning_rate 0.00369084
2017-10-02T22:14:01.674404: step 78, loss 1.48867, acc 0.390625, learning_rate 0.00367619
2017-10-02T22:14:02.852013: step 79, loss 1.52465, acc 0.3125, learning_rate 0.00366159
2017-10-02T22:14:04.090730: step 80, loss 1.66881, acc 0.15625, learning_rate 0.00364705

Evaluation:
2017-10-02T22:14:04.389183: step 80, loss 1.49146, acc 0.421583

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-80

2017-10-02T22:14:11.965059: step 81, loss 1.4219, acc 0.328125, learning_rate 0.00363257
2017-10-02T22:14:13.195057: step 82, loss 1.47961, acc 0.390625, learning_rate 0.00361815
2017-10-02T22:14:14.359251: step 83, loss 1.46475, acc 0.234375, learning_rate 0.00360379
2017-10-02T22:14:15.531700: step 84, loss 1.55405, acc 0.265625, learning_rate 0.00358949
2017-10-02T22:14:16.687798: step 85, loss 1.6169, acc 0.265625, learning_rate 0.00357525
2017-10-02T22:14:17.853783: step 86, loss 1.61272, acc 0.15625, learning_rate 0.00356106
2017-10-02T22:14:19.201570: step 87, loss 1.43589, acc 0.40625, learning_rate 0.00354694
2017-10-02T22:14:20.382395: step 88, loss 1.54488, acc 0.3125, learning_rate 0.00353287
2017-10-02T22:14:21.568794: step 89, loss 1.40249, acc 0.34375, learning_rate 0.00351885
2017-10-02T22:14:22.748383: step 90, loss 1.43798, acc 0.40625, learning_rate 0.0035049
2017-10-02T22:14:23.944582: step 91, loss 1.51394, acc 0.28125, learning_rate 0.003491
2017-10-02T22:14:25.108110: step 92, loss 1.43389, acc 0.40625, learning_rate 0.00347716
2017-10-02T22:14:26.277899: step 93, loss 1.51391, acc 0.328125, learning_rate 0.00346338
2017-10-02T22:14:27.444030: step 94, loss 1.5208, acc 0.359375, learning_rate 0.00344965
2017-10-02T22:14:28.592068: step 95, loss 1.51021, acc 0.328125, learning_rate 0.00343597
2017-10-02T22:14:29.762478: step 96, loss 1.42669, acc 0.40625, learning_rate 0.00342236
2017-10-02T22:14:30.930019: step 97, loss 1.42576, acc 0.390625, learning_rate 0.0034088
2017-10-02T22:14:32.079206: step 98, loss 1.64415, acc 0.27451, learning_rate 0.00339529
2017-10-02T22:14:33.325825: step 99, loss 1.54085, acc 0.3125, learning_rate 0.00338184
2017-10-02T22:14:34.486920: step 100, loss 1.32286, acc 0.484375, learning_rate 0.00336844
2017-10-02T22:14:35.654288: step 101, loss 1.3243, acc 0.453125, learning_rate 0.0033551
2017-10-02T22:14:36.815109: step 102, loss 1.40895, acc 0.421875, learning_rate 0.00334182
2017-10-02T22:14:37.976582: step 103, loss 1.40793, acc 0.453125, learning_rate 0.00332858
2017-10-02T22:14:39.155842: step 104, loss 1.42, acc 0.515625, learning_rate 0.00331541
2017-10-02T22:14:40.336484: step 105, loss 1.33729, acc 0.53125, learning_rate 0.00330228
2017-10-02T22:14:41.514740: step 106, loss 1.40139, acc 0.546875, learning_rate 0.00328921
2017-10-02T22:14:42.694299: step 107, loss 1.33001, acc 0.46875, learning_rate 0.00327619
2017-10-02T22:14:43.862582: step 108, loss 1.39411, acc 0.46875, learning_rate 0.00326323
2017-10-02T22:14:45.028583: step 109, loss 1.41078, acc 0.390625, learning_rate 0.00325032
2017-10-02T22:14:46.197235: step 110, loss 1.25189, acc 0.5625, learning_rate 0.00323746
2017-10-02T22:14:47.376791: step 111, loss 1.32786, acc 0.5, learning_rate 0.00322465
2017-10-02T22:14:48.552890: step 112, loss 1.32076, acc 0.5, learning_rate 0.0032119
2017-10-02T22:14:49.716729: step 113, loss 1.40893, acc 0.40625, learning_rate 0.0031992
2017-10-02T22:14:50.896205: step 114, loss 1.35843, acc 0.453125, learning_rate 0.00318655
2017-10-02T22:14:52.068919: step 115, loss 1.29178, acc 0.515625, learning_rate 0.00317395
2017-10-02T22:14:53.242654: step 116, loss 1.3068, acc 0.53125, learning_rate 0.0031614
2017-10-02T22:14:54.413784: step 117, loss 1.42188, acc 0.296875, learning_rate 0.0031489
2017-10-02T22:14:55.584580: step 118, loss 1.40828, acc 0.40625, learning_rate 0.00313646
2017-10-02T22:14:56.748710: step 119, loss 1.37331, acc 0.40625, learning_rate 0.00312407
2017-10-02T22:14:57.929832: step 120, loss 1.4248, acc 0.40625, learning_rate 0.00311172

Evaluation:
2017-10-02T22:14:58.234405: step 120, loss 1.47709, acc 0.427338

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-120

2017-10-02T22:15:05.612337: step 121, loss 1.3306, acc 0.484375, learning_rate 0.00309943
2017-10-02T22:15:07.047646: step 122, loss 1.25675, acc 0.59375, learning_rate 0.00308719
2017-10-02T22:15:08.213913: step 123, loss 1.32841, acc 0.484375, learning_rate 0.00307499
2017-10-02T22:15:09.373283: step 124, loss 1.26899, acc 0.46875, learning_rate 0.00306285
2017-10-02T22:15:10.541516: step 125, loss 1.25629, acc 0.53125, learning_rate 0.00305076
2017-10-02T22:15:11.699137: step 126, loss 1.32059, acc 0.421875, learning_rate 0.00303871
2017-10-02T22:15:12.866034: step 127, loss 1.41878, acc 0.359375, learning_rate 0.00302672
2017-10-02T22:15:14.038673: step 128, loss 1.44105, acc 0.34375, learning_rate 0.00301477
2017-10-02T22:15:15.210029: step 129, loss 1.37345, acc 0.484375, learning_rate 0.00300287
2017-10-02T22:15:16.376702: step 130, loss 1.33635, acc 0.46875, learning_rate 0.00299102
2017-10-02T22:15:17.539872: step 131, loss 1.30802, acc 0.453125, learning_rate 0.00297922
2017-10-02T22:15:18.728477: step 132, loss 1.34713, acc 0.390625, learning_rate 0.00296747
2017-10-02T22:15:19.876155: step 133, loss 1.33875, acc 0.5, learning_rate 0.00295577
2017-10-02T22:15:21.108863: step 134, loss 1.32277, acc 0.484375, learning_rate 0.00294411
2017-10-02T22:15:22.283992: step 135, loss 1.46467, acc 0.40625, learning_rate 0.0029325
2017-10-02T22:15:23.450808: step 136, loss 1.41288, acc 0.40625, learning_rate 0.00292094
2017-10-02T22:15:24.607361: step 137, loss 1.35621, acc 0.53125, learning_rate 0.00290943
2017-10-02T22:15:25.771184: step 138, loss 1.33686, acc 0.484375, learning_rate 0.00289796
2017-10-02T22:15:26.934976: step 139, loss 1.36023, acc 0.453125, learning_rate 0.00288654
2017-10-02T22:15:28.096624: step 140, loss 1.35601, acc 0.546875, learning_rate 0.00287516
2017-10-02T22:15:29.257423: step 141, loss 1.34183, acc 0.484375, learning_rate 0.00286384
2017-10-02T22:15:30.422648: step 142, loss 1.29784, acc 0.453125, learning_rate 0.00285256
2017-10-02T22:15:31.581608: step 143, loss 1.3434, acc 0.4375, learning_rate 0.00284132
2017-10-02T22:15:32.743786: step 144, loss 1.34549, acc 0.421875, learning_rate 0.00283013
2017-10-02T22:15:33.901860: step 145, loss 1.2887, acc 0.484375, learning_rate 0.00281899
2017-10-02T22:15:35.069927: step 146, loss 1.296, acc 0.5, learning_rate 0.00280789
2017-10-02T22:15:36.217916: step 147, loss 1.27794, acc 0.5, learning_rate 0.00279684
2017-10-02T22:15:37.379467: step 148, loss 1.32188, acc 0.5625, learning_rate 0.00278583
2017-10-02T22:15:38.552470: step 149, loss 1.29059, acc 0.546875, learning_rate 0.00277486
2017-10-02T22:15:39.717393: step 150, loss 1.42791, acc 0.40625, learning_rate 0.00276395
2017-10-02T22:15:40.883953: step 151, loss 1.24416, acc 0.53125, learning_rate 0.00275307
2017-10-02T22:15:42.044050: step 152, loss 1.27274, acc 0.5625, learning_rate 0.00274224
2017-10-02T22:15:43.207087: step 153, loss 1.33739, acc 0.421875, learning_rate 0.00273146
2017-10-02T22:15:44.387110: step 154, loss 1.20204, acc 0.53125, learning_rate 0.00272072
2017-10-02T22:15:45.542908: step 155, loss 1.31469, acc 0.546875, learning_rate 0.00271002
2017-10-02T22:15:46.693897: step 156, loss 1.43209, acc 0.40625, learning_rate 0.00269937
2017-10-02T22:15:47.846032: step 157, loss 1.327, acc 0.46875, learning_rate 0.00268876
2017-10-02T22:15:49.001443: step 158, loss 1.28981, acc 0.515625, learning_rate 0.00267819
2017-10-02T22:15:50.164171: step 159, loss 1.24149, acc 0.671875, learning_rate 0.00266767
2017-10-02T22:15:51.330794: step 160, loss 1.30587, acc 0.53125, learning_rate 0.00265719

Evaluation:
2017-10-02T22:15:51.657741: step 160, loss 1.46633, acc 0.464748

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-160

2017-10-02T22:15:57.921930: step 161, loss 1.19306, acc 0.59375, learning_rate 0.00264675
2017-10-02T22:15:59.120865: step 162, loss 1.3259, acc 0.546875, learning_rate 0.00263635
2017-10-02T22:16:00.286939: step 163, loss 1.32625, acc 0.484375, learning_rate 0.002626
2017-10-02T22:16:01.437020: step 164, loss 1.24899, acc 0.515625, learning_rate 0.00261569
2017-10-02T22:16:02.597852: step 165, loss 1.15955, acc 0.625, learning_rate 0.00260542
2017-10-02T22:16:03.761933: step 166, loss 1.27832, acc 0.5625, learning_rate 0.0025952
2017-10-02T22:16:04.927672: step 167, loss 1.21348, acc 0.53125, learning_rate 0.00258501
2017-10-02T22:16:06.092288: step 168, loss 1.34741, acc 0.484375, learning_rate 0.00257487
2017-10-02T22:16:07.248192: step 169, loss 1.26398, acc 0.484375, learning_rate 0.00256477
2017-10-02T22:16:08.406156: step 170, loss 1.25713, acc 0.5625, learning_rate 0.0025547
2017-10-02T22:16:09.580571: step 171, loss 1.31642, acc 0.421875, learning_rate 0.00254469
2017-10-02T22:16:10.743776: step 172, loss 1.33729, acc 0.390625, learning_rate 0.00253471
2017-10-02T22:16:11.914421: step 173, loss 1.36676, acc 0.4375, learning_rate 0.00252477
2017-10-02T22:16:13.075623: step 174, loss 1.32061, acc 0.46875, learning_rate 0.00251487
2017-10-02T22:16:14.233868: step 175, loss 1.33332, acc 0.46875, learning_rate 0.00250501
2017-10-02T22:16:15.415523: step 176, loss 1.3857, acc 0.53125, learning_rate 0.0024952
2017-10-02T22:16:16.572645: step 177, loss 1.23613, acc 0.5625, learning_rate 0.00248542
2017-10-02T22:16:17.732456: step 178, loss 1.26125, acc 0.53125, learning_rate 0.00247568
2017-10-02T22:16:18.895429: step 179, loss 1.25857, acc 0.59375, learning_rate 0.00246599
2017-10-02T22:16:20.066927: step 180, loss 1.2092, acc 0.546875, learning_rate 0.00245633
2017-10-02T22:16:21.229601: step 181, loss 1.3018, acc 0.5, learning_rate 0.00244671
2017-10-02T22:16:22.382338: step 182, loss 1.4197, acc 0.375, learning_rate 0.00243713
2017-10-02T22:16:23.536949: step 183, loss 1.30362, acc 0.484375, learning_rate 0.00242759
2017-10-02T22:16:24.700167: step 184, loss 1.20605, acc 0.640625, learning_rate 0.00241809
2017-10-02T22:16:25.850862: step 185, loss 1.21561, acc 0.609375, learning_rate 0.00240863
2017-10-02T22:16:26.994591: step 186, loss 1.20935, acc 0.53125, learning_rate 0.00239921
2017-10-02T22:16:28.152916: step 187, loss 1.27724, acc 0.484375, learning_rate 0.00238982
2017-10-02T22:16:29.323567: step 188, loss 1.23587, acc 0.515625, learning_rate 0.00238048
2017-10-02T22:16:30.481782: step 189, loss 1.0907, acc 0.671875, learning_rate 0.00237117
2017-10-02T22:16:31.674039: step 190, loss 1.30438, acc 0.4375, learning_rate 0.0023619
2017-10-02T22:16:32.843351: step 191, loss 1.25591, acc 0.5625, learning_rate 0.00235267
2017-10-02T22:16:34.028495: step 192, loss 1.19647, acc 0.53125, learning_rate 0.00234347
2017-10-02T22:16:35.195292: step 193, loss 1.34794, acc 0.484375, learning_rate 0.00233431
2017-10-02T22:16:36.358513: step 194, loss 1.25751, acc 0.46875, learning_rate 0.00232519
2017-10-02T22:16:37.516378: step 195, loss 1.20138, acc 0.5625, learning_rate 0.00231611
2017-10-02T22:16:38.663089: step 196, loss 1.28862, acc 0.509804, learning_rate 0.00230707
2017-10-02T22:16:39.819140: step 197, loss 1.00144, acc 0.65625, learning_rate 0.00229806
2017-10-02T22:16:40.980338: step 198, loss 1.008, acc 0.6875, learning_rate 0.00228908
2017-10-02T22:16:42.118493: step 199, loss 0.952597, acc 0.78125, learning_rate 0.00228015
2017-10-02T22:16:43.289648: step 200, loss 0.97412, acc 0.8125, learning_rate 0.00227125

Evaluation:
2017-10-02T22:16:43.585662: step 200, loss 1.47944, acc 0.353957

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-200

2017-10-02T22:16:50.985469: step 201, loss 0.907101, acc 0.8125, learning_rate 0.00226239
2017-10-02T22:16:52.147698: step 202, loss 0.98668, acc 0.75, learning_rate 0.00225356
2017-10-02T22:16:53.311740: step 203, loss 0.879967, acc 0.828125, learning_rate 0.00224477
2017-10-02T22:16:54.484940: step 204, loss 0.940453, acc 0.734375, learning_rate 0.00223602
2017-10-02T22:16:55.641459: step 205, loss 0.976257, acc 0.671875, learning_rate 0.0022273
2017-10-02T22:16:56.781822: step 206, loss 0.843566, acc 0.703125, learning_rate 0.00221862
2017-10-02T22:16:57.952441: step 207, loss 1.01242, acc 0.65625, learning_rate 0.00220997
2017-10-02T22:16:59.120809: step 208, loss 0.900115, acc 0.734375, learning_rate 0.00220136
2017-10-02T22:17:00.290327: step 209, loss 1.08744, acc 0.5625, learning_rate 0.00219278
2017-10-02T22:17:01.443933: step 210, loss 0.977021, acc 0.765625, learning_rate 0.00218424
2017-10-02T22:17:02.613537: step 211, loss 0.946118, acc 0.765625, learning_rate 0.00217573
2017-10-02T22:17:03.769867: step 212, loss 0.905317, acc 0.703125, learning_rate 0.00216726
2017-10-02T22:17:04.940507: step 213, loss 0.976531, acc 0.71875, learning_rate 0.00215882
2017-10-02T22:17:06.093904: step 214, loss 0.862483, acc 0.84375, learning_rate 0.00215041
2017-10-02T22:17:07.266007: step 215, loss 0.905561, acc 0.84375, learning_rate 0.00214204
2017-10-02T22:17:08.438242: step 216, loss 0.917541, acc 0.890625, learning_rate 0.00213371
2017-10-02T22:17:09.611768: step 217, loss 0.933101, acc 0.8125, learning_rate 0.00212541
2017-10-02T22:17:10.787592: step 218, loss 0.872207, acc 0.8125, learning_rate 0.00211714
2017-10-02T22:17:11.938776: step 219, loss 0.871545, acc 0.8125, learning_rate 0.00210891
2017-10-02T22:17:13.104670: step 220, loss 0.876211, acc 0.734375, learning_rate 0.00210071
2017-10-02T22:17:14.273754: step 221, loss 0.836594, acc 0.75, learning_rate 0.00209254
2017-10-02T22:17:15.434842: step 222, loss 0.846838, acc 0.75, learning_rate 0.00208441
2017-10-02T22:17:16.597135: step 223, loss 0.725414, acc 0.828125, learning_rate 0.00207631
2017-10-02T22:17:17.755935: step 224, loss 0.916072, acc 0.65625, learning_rate 0.00206824
2017-10-02T22:17:18.945230: step 225, loss 0.795912, acc 0.75, learning_rate 0.00206021
2017-10-02T22:17:20.116203: step 226, loss 0.914401, acc 0.65625, learning_rate 0.00205221
2017-10-02T22:17:21.276595: step 227, loss 0.734625, acc 0.796875, learning_rate 0.00204424
2017-10-02T22:17:22.446460: step 228, loss 0.841689, acc 0.78125, learning_rate 0.0020363
2017-10-02T22:17:23.617178: step 229, loss 0.786162, acc 0.78125, learning_rate 0.0020284
2017-10-02T22:17:24.785598: step 230, loss 0.975171, acc 0.71875, learning_rate 0.00202053
2017-10-02T22:17:25.949559: step 231, loss 0.781868, acc 0.859375, learning_rate 0.00201269
2017-10-02T22:17:27.099261: step 232, loss 0.897169, acc 0.84375, learning_rate 0.00200488
2017-10-02T22:17:28.250188: step 233, loss 0.887751, acc 0.796875, learning_rate 0.00199711
2017-10-02T22:17:29.414930: step 234, loss 0.748411, acc 0.890625, learning_rate 0.00198936
2017-10-02T22:17:30.573262: step 235, loss 0.850245, acc 0.765625, learning_rate 0.00198165
2017-10-02T22:17:31.744810: step 236, loss 0.809138, acc 0.78125, learning_rate 0.00197397
2017-10-02T22:17:32.886260: step 237, loss 0.689606, acc 0.78125, learning_rate 0.00196632
2017-10-02T22:17:34.046235: step 238, loss 0.814768, acc 0.71875, learning_rate 0.0019587
2017-10-02T22:17:35.216729: step 239, loss 0.766765, acc 0.8125, learning_rate 0.00195112
2017-10-02T22:17:36.389544: step 240, loss 0.673584, acc 0.859375, learning_rate 0.00194356

Evaluation:
2017-10-02T22:17:36.711915: step 240, loss 1.42895, acc 0.456115

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-240

2017-10-02T22:17:45.536941: step 241, loss 0.890005, acc 0.6875, learning_rate 0.00193604
2017-10-02T22:17:46.747892: step 242, loss 0.755864, acc 0.828125, learning_rate 0.00192854
2017-10-02T22:17:47.912110: step 243, loss 0.926658, acc 0.640625, learning_rate 0.00192108
2017-10-02T22:17:49.067261: step 244, loss 0.759959, acc 0.78125, learning_rate 0.00191364
2017-10-02T22:17:50.233988: step 245, loss 0.668641, acc 0.90625, learning_rate 0.00190624
2017-10-02T22:17:51.381099: step 246, loss 0.695946, acc 0.859375, learning_rate 0.00189887
2017-10-02T22:17:52.542870: step 247, loss 0.822139, acc 0.796875, learning_rate 0.00189153
2017-10-02T22:17:53.712826: step 248, loss 0.732489, acc 0.84375, learning_rate 0.00188421
2017-10-02T22:17:54.866873: step 249, loss 0.736369, acc 0.78125, learning_rate 0.00187693
2017-10-02T22:17:56.028117: step 250, loss 0.782959, acc 0.71875, learning_rate 0.00186968
2017-10-02T22:17:57.186776: step 251, loss 0.773611, acc 0.8125, learning_rate 0.00186245
2017-10-02T22:17:58.357094: step 252, loss 0.713908, acc 0.828125, learning_rate 0.00185526
2017-10-02T22:17:59.525971: step 253, loss 0.731919, acc 0.828125, learning_rate 0.0018481
2017-10-02T22:18:00.689032: step 254, loss 0.690731, acc 0.828125, learning_rate 0.00184096
2017-10-02T22:18:01.849556: step 255, loss 0.786648, acc 0.8125, learning_rate 0.00183385
2017-10-02T22:18:03.006367: step 256, loss 0.618791, acc 0.90625, learning_rate 0.00182678
2017-10-02T22:18:04.168117: step 257, loss 0.731821, acc 0.84375, learning_rate 0.00181973
2017-10-02T22:18:05.379994: step 258, loss 0.81281, acc 0.84375, learning_rate 0.00181271
2017-10-02T22:18:06.545387: step 259, loss 0.731872, acc 0.859375, learning_rate 0.00180572
2017-10-02T22:18:07.713543: step 260, loss 0.666176, acc 0.859375, learning_rate 0.00179876
2017-10-02T22:18:08.881257: step 261, loss 0.685624, acc 0.859375, learning_rate 0.00179182
2017-10-02T22:18:10.049308: step 262, loss 0.67504, acc 0.90625, learning_rate 0.00178492
2017-10-02T22:18:11.218886: step 263, loss 0.730214, acc 0.78125, learning_rate 0.00177804
2017-10-02T22:18:12.394404: step 264, loss 0.781895, acc 0.84375, learning_rate 0.00177119
2017-10-02T22:18:13.561998: step 265, loss 0.75167, acc 0.8125, learning_rate 0.00176437
2017-10-02T22:18:14.718778: step 266, loss 0.632151, acc 0.921875, learning_rate 0.00175758
2017-10-02T22:18:15.912436: step 267, loss 0.697002, acc 0.828125, learning_rate 0.00175081
2017-10-02T22:18:17.064766: step 268, loss 0.674521, acc 0.90625, learning_rate 0.00174407
2017-10-02T22:18:18.231236: step 269, loss 0.756974, acc 0.890625, learning_rate 0.00173736
2017-10-02T22:18:19.405528: step 270, loss 0.758823, acc 0.78125, learning_rate 0.00173068
2017-10-02T22:18:20.585332: step 271, loss 0.674657, acc 0.90625, learning_rate 0.00172402
2017-10-02T22:18:21.758942: step 272, loss 0.71777, acc 0.875, learning_rate 0.00171739
2017-10-02T22:18:22.926881: step 273, loss 0.632868, acc 0.90625, learning_rate 0.00171079
2017-10-02T22:18:24.101842: step 274, loss 0.767384, acc 0.734375, learning_rate 0.00170422
2017-10-02T22:18:25.297836: step 275, loss 0.581772, acc 0.890625, learning_rate 0.00169767
2017-10-02T22:18:26.476001: step 276, loss 0.707684, acc 0.796875, learning_rate 0.00169115
2017-10-02T22:18:27.656578: step 277, loss 0.638028, acc 0.890625, learning_rate 0.00168465
2017-10-02T22:18:28.850566: step 278, loss 0.673392, acc 0.859375, learning_rate 0.00167818
2017-10-02T22:18:30.019969: step 279, loss 0.614822, acc 0.875, learning_rate 0.00167174
2017-10-02T22:18:31.215509: step 280, loss 0.752397, acc 0.765625, learning_rate 0.00166533

Evaluation:
2017-10-02T22:18:31.511243: step 280, loss 1.42572, acc 0.454676

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-280

2017-10-02T22:18:41.527348: step 281, loss 0.662825, acc 0.890625, learning_rate 0.00165894
2017-10-02T22:18:42.847039: step 282, loss 0.685273, acc 0.859375, learning_rate 0.00165257
2017-10-02T22:18:44.006699: step 283, loss 0.706798, acc 0.890625, learning_rate 0.00164624
2017-10-02T22:18:45.166548: step 284, loss 0.712949, acc 0.828125, learning_rate 0.00163993
2017-10-02T22:18:46.320062: step 285, loss 0.738535, acc 0.875, learning_rate 0.00163364
2017-10-02T22:18:47.478710: step 286, loss 0.681347, acc 0.890625, learning_rate 0.00162738
2017-10-02T22:18:48.625870: step 287, loss 0.560469, acc 0.90625, learning_rate 0.00162115
2017-10-02T22:18:49.785075: step 288, loss 0.683862, acc 0.90625, learning_rate 0.00161494
2017-10-02T22:18:50.941914: step 289, loss 0.675823, acc 0.875, learning_rate 0.00160875
2017-10-02T22:18:52.097963: step 290, loss 0.741752, acc 0.875, learning_rate 0.00160259
2017-10-02T22:18:53.257654: step 291, loss 0.722259, acc 0.859375, learning_rate 0.00159646
2017-10-02T22:18:54.431408: step 292, loss 0.623068, acc 0.859375, learning_rate 0.00159035
2017-10-02T22:18:55.589667: step 293, loss 0.561665, acc 0.921875, learning_rate 0.00158427
2017-10-02T22:18:56.743184: step 294, loss 0.663938, acc 0.901961, learning_rate 0.00157821
2017-10-02T22:18:57.909489: step 295, loss 0.414665, acc 0.953125, learning_rate 0.00157218
2017-10-02T22:18:59.065837: step 296, loss 0.382516, acc 0.953125, learning_rate 0.00156617
2017-10-02T22:19:00.224099: step 297, loss 0.333794, acc 0.984375, learning_rate 0.00156018
2017-10-02T22:19:01.383771: step 298, loss 0.347525, acc 0.96875, learning_rate 0.00155422
2017-10-02T22:19:02.577122: step 299, loss 0.419747, acc 0.96875, learning_rate 0.00154829
2017-10-02T22:19:03.755186: step 300, loss 0.327009, acc 0.96875, learning_rate 0.00154238
2017-10-02T22:19:04.911466: step 301, loss 0.336129, acc 0.984375, learning_rate 0.00153649
2017-10-02T22:19:06.075925: step 302, loss 0.400723, acc 0.953125, learning_rate 0.00153063
2017-10-02T22:19:07.231694: step 303, loss 0.396426, acc 1, learning_rate 0.00152479
2017-10-02T22:19:08.391380: step 304, loss 0.3993, acc 0.96875, learning_rate 0.00151897
2017-10-02T22:19:09.566878: step 305, loss 0.403761, acc 0.9375, learning_rate 0.00151318
2017-10-02T22:19:10.717198: step 306, loss 0.389203, acc 0.96875, learning_rate 0.00150741
2017-10-02T22:19:11.883074: step 307, loss 0.371693, acc 1, learning_rate 0.00150167
2017-10-02T22:19:13.038108: step 308, loss 0.379028, acc 0.96875, learning_rate 0.00149594
2017-10-02T22:19:14.194057: step 309, loss 0.379744, acc 0.984375, learning_rate 0.00149025
2017-10-02T22:19:15.356701: step 310, loss 0.454617, acc 0.984375, learning_rate 0.00148457
2017-10-02T22:19:16.519559: step 311, loss 0.342058, acc 1, learning_rate 0.00147892
2017-10-02T22:19:17.706812: step 312, loss 0.390113, acc 0.96875, learning_rate 0.00147329
2017-10-02T22:19:18.876691: step 313, loss 0.426078, acc 0.96875, learning_rate 0.00146769
2017-10-02T22:19:20.043876: step 314, loss 0.383426, acc 0.9375, learning_rate 0.0014621
2017-10-02T22:19:21.203789: step 315, loss 0.42862, acc 0.984375, learning_rate 0.00145654
2017-10-02T22:19:22.374763: step 316, loss 0.381386, acc 0.953125, learning_rate 0.00145101
2017-10-02T22:19:23.942960: step 317, loss 0.408336, acc 0.96875, learning_rate 0.00144549
2017-10-02T22:19:25.104478: step 318, loss 0.321805, acc 1, learning_rate 0.00144
2017-10-02T22:19:26.269694: step 319, loss 0.380019, acc 1, learning_rate 0.00143453
2017-10-02T22:19:27.436462: step 320, loss 0.351351, acc 0.984375, learning_rate 0.00142908

Evaluation:
2017-10-02T22:19:27.734354: step 320, loss 1.488, acc 0.300719

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-320

2017-10-02T22:19:35.955797: step 321, loss 0.389962, acc 0.953125, learning_rate 0.00142366
2017-10-02T22:19:37.135314: step 322, loss 0.364525, acc 0.984375, learning_rate 0.00141826
2017-10-02T22:19:38.307112: step 323, loss 0.359519, acc 0.96875, learning_rate 0.00141288
2017-10-02T22:19:39.475529: step 324, loss 0.329928, acc 0.96875, learning_rate 0.00140752
2017-10-02T22:19:40.637123: step 325, loss 0.302142, acc 0.984375, learning_rate 0.00140218
2017-10-02T22:19:41.798477: step 326, loss 0.374495, acc 0.984375, learning_rate 0.00139686
2017-10-02T22:19:42.956163: step 327, loss 0.263662, acc 0.96875, learning_rate 0.00139157
2017-10-02T22:19:44.110221: step 328, loss 0.288534, acc 0.984375, learning_rate 0.0013863
2017-10-02T22:19:45.280696: step 329, loss 0.309738, acc 0.984375, learning_rate 0.00138105
2017-10-02T22:19:46.436248: step 330, loss 0.352244, acc 0.921875, learning_rate 0.00137582
2017-10-02T22:19:47.600036: step 331, loss 0.346613, acc 0.96875, learning_rate 0.00137061
2017-10-02T22:19:48.812532: step 332, loss 0.361076, acc 0.953125, learning_rate 0.00136543
2017-10-02T22:19:49.969986: step 333, loss 0.29394, acc 0.984375, learning_rate 0.00136026
2017-10-02T22:19:51.134690: step 334, loss 0.346246, acc 1, learning_rate 0.00135512
2017-10-02T22:19:52.301932: step 335, loss 0.317587, acc 0.9375, learning_rate 0.00134999
2017-10-02T22:19:53.469707: step 336, loss 0.330981, acc 0.984375, learning_rate 0.00134489
2017-10-02T22:19:54.624406: step 337, loss 0.311432, acc 0.96875, learning_rate 0.00133981
2017-10-02T22:19:55.789657: step 338, loss 0.294904, acc 0.96875, learning_rate 0.00133475
2017-10-02T22:19:56.944700: step 339, loss 0.350632, acc 0.96875, learning_rate 0.00132971
2017-10-02T22:19:58.104337: step 340, loss 0.394377, acc 0.953125, learning_rate 0.00132469
2017-10-02T22:19:59.259885: step 341, loss 0.297534, acc 1, learning_rate 0.00131969
2017-10-02T22:20:00.420225: step 342, loss 0.30012, acc 0.984375, learning_rate 0.00131471
2017-10-02T22:20:01.584560: step 343, loss 0.312933, acc 0.96875, learning_rate 0.00130975
2017-10-02T22:20:02.829010: step 344, loss 0.399988, acc 0.984375, learning_rate 0.00130482
2017-10-02T22:20:04.017254: step 345, loss 0.35427, acc 0.984375, learning_rate 0.0012999
2017-10-02T22:20:05.174508: step 346, loss 0.325866, acc 0.9375, learning_rate 0.001295
2017-10-02T22:20:06.341859: step 347, loss 0.322756, acc 0.984375, learning_rate 0.00129012
2017-10-02T22:20:07.498627: step 348, loss 0.254211, acc 0.984375, learning_rate 0.00128527
2017-10-02T22:20:08.662721: step 349, loss 0.323087, acc 0.953125, learning_rate 0.00128043
2017-10-02T22:20:09.814464: step 350, loss 0.369758, acc 0.984375, learning_rate 0.00127561
2017-10-02T22:20:10.959637: step 351, loss 0.253638, acc 1, learning_rate 0.00127081
2017-10-02T22:20:12.107490: step 352, loss 0.310385, acc 0.9375, learning_rate 0.00126603
2017-10-02T22:20:13.284440: step 353, loss 0.324094, acc 0.984375, learning_rate 0.00126127
2017-10-02T22:20:14.446569: step 354, loss 0.274967, acc 0.984375, learning_rate 0.00125653
2017-10-02T22:20:15.601066: step 355, loss 0.316248, acc 0.96875, learning_rate 0.00125181
2017-10-02T22:20:16.764855: step 356, loss 0.257717, acc 0.953125, learning_rate 0.00124711
2017-10-02T22:20:17.927370: step 357, loss 0.338191, acc 0.9375, learning_rate 0.00124243
2017-10-02T22:20:19.085525: step 358, loss 0.253207, acc 1, learning_rate 0.00123777
2017-10-02T22:20:20.227042: step 359, loss 0.270215, acc 0.984375, learning_rate 0.00123312
2017-10-02T22:20:21.394658: step 360, loss 0.281817, acc 0.96875, learning_rate 0.0012285

Evaluation:
2017-10-02T22:20:21.686530: step 360, loss 1.41274, acc 0.513669

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-360

2017-10-02T22:20:30.030355: step 361, loss 0.310818, acc 0.984375, learning_rate 0.00122389
2017-10-02T22:20:31.194871: step 362, loss 0.23899, acc 0.984375, learning_rate 0.0012193
2017-10-02T22:20:32.393417: step 363, loss 0.225066, acc 1, learning_rate 0.00121473
2017-10-02T22:20:33.568394: step 364, loss 0.299047, acc 0.984375, learning_rate 0.00121018
2017-10-02T22:20:34.740646: step 365, loss 0.266374, acc 0.984375, learning_rate 0.00120565
2017-10-02T22:20:35.900615: step 366, loss 0.321262, acc 0.984375, learning_rate 0.00120114
2017-10-02T22:20:37.057136: step 367, loss 0.290123, acc 0.984375, learning_rate 0.00119664
2017-10-02T22:20:38.222611: step 368, loss 0.301391, acc 1, learning_rate 0.00119217
2017-10-02T22:20:39.412298: step 369, loss 0.295964, acc 0.984375, learning_rate 0.00118771
2017-10-02T22:20:40.591281: step 370, loss 0.266291, acc 1, learning_rate 0.00118327
2017-10-02T22:20:41.796490: step 371, loss 0.266067, acc 0.953125, learning_rate 0.00117885
2017-10-02T22:20:42.989779: step 372, loss 0.278735, acc 1, learning_rate 0.00117445
2017-10-02T22:20:44.163824: step 373, loss 0.247921, acc 0.984375, learning_rate 0.00117006
2017-10-02T22:20:45.373011: step 374, loss 0.202449, acc 0.984375, learning_rate 0.00116569
2017-10-02T22:20:46.610148: step 375, loss 0.256231, acc 0.984375, learning_rate 0.00116134
2017-10-02T22:20:47.813552: step 376, loss 0.280344, acc 0.953125, learning_rate 0.00115701
2017-10-02T22:20:49.044610: step 377, loss 0.282292, acc 1, learning_rate 0.0011527
2017-10-02T22:20:50.223963: step 378, loss 0.225069, acc 1, learning_rate 0.0011484
2017-10-02T22:20:51.419840: step 379, loss 0.301097, acc 0.96875, learning_rate 0.00114412
2017-10-02T22:20:52.593845: step 380, loss 0.293777, acc 0.953125, learning_rate 0.00113986
2017-10-02T22:20:53.765932: step 381, loss 0.271936, acc 0.984375, learning_rate 0.00113561
2017-10-02T22:20:54.940517: step 382, loss 0.25963, acc 0.984375, learning_rate 0.00113139
2017-10-02T22:20:56.114659: step 383, loss 0.279505, acc 0.953125, learning_rate 0.00112718
2017-10-02T22:20:57.264236: step 384, loss 0.315749, acc 0.9375, learning_rate 0.00112298
2017-10-02T22:20:58.421361: step 385, loss 0.218814, acc 1, learning_rate 0.00111881
2017-10-02T22:20:59.575264: step 386, loss 0.315558, acc 0.984375, learning_rate 0.00111465
2017-10-02T22:21:00.734378: step 387, loss 0.194908, acc 1, learning_rate 0.00111051
2017-10-02T22:21:01.889034: step 388, loss 0.383037, acc 0.953125, learning_rate 0.00110638
2017-10-02T22:21:03.052590: step 389, loss 0.245355, acc 0.984375, learning_rate 0.00110228
2017-10-02T22:21:04.202354: step 390, loss 0.259246, acc 1, learning_rate 0.00109818
2017-10-02T22:21:05.342951: step 391, loss 0.2632, acc 1, learning_rate 0.00109411
2017-10-02T22:21:06.483750: step 392, loss 0.240783, acc 1, learning_rate 0.00109005
2017-10-02T22:21:07.648726: step 393, loss 0.176107, acc 1, learning_rate 0.00108601
2017-10-02T22:21:08.821241: step 394, loss 0.175022, acc 1, learning_rate 0.00108199
2017-10-02T22:21:09.975949: step 395, loss 0.172831, acc 1, learning_rate 0.00107798
2017-10-02T22:21:11.131301: step 396, loss 0.168711, acc 1, learning_rate 0.00107399
2017-10-02T22:21:12.342294: step 397, loss 0.199148, acc 1, learning_rate 0.00107001
2017-10-02T22:21:13.493876: step 398, loss 0.169437, acc 0.984375, learning_rate 0.00106605
2017-10-02T22:21:14.668863: step 399, loss 0.193149, acc 1, learning_rate 0.00106211
2017-10-02T22:21:15.847484: step 400, loss 0.178582, acc 0.984375, learning_rate 0.00105818

Evaluation:
2017-10-02T22:21:16.166105: step 400, loss 1.43793, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-400

2017-10-02T22:21:24.099758: step 401, loss 0.125827, acc 1, learning_rate 0.00105427
2017-10-02T22:21:25.259171: step 402, loss 0.162647, acc 1, learning_rate 0.00105037
2017-10-02T22:21:26.408332: step 403, loss 0.152363, acc 0.984375, learning_rate 0.0010465
2017-10-02T22:21:27.561872: step 404, loss 0.134115, acc 1, learning_rate 0.00104263
2017-10-02T22:21:28.714182: step 405, loss 0.14304, acc 0.984375, learning_rate 0.00103878
2017-10-02T22:21:29.881074: step 406, loss 0.146712, acc 1, learning_rate 0.00103495
2017-10-02T22:21:31.049439: step 407, loss 0.188586, acc 0.984375, learning_rate 0.00103114
2017-10-02T22:21:32.195160: step 408, loss 0.171349, acc 1, learning_rate 0.00102734
2017-10-02T22:21:33.371446: step 409, loss 0.145928, acc 0.984375, learning_rate 0.00102355
2017-10-02T22:21:34.516985: step 410, loss 0.169527, acc 1, learning_rate 0.00101978
2017-10-02T22:21:35.672184: step 411, loss 0.171282, acc 0.984375, learning_rate 0.00101603
2017-10-02T22:21:36.837351: step 412, loss 0.137558, acc 1, learning_rate 0.00101229
2017-10-02T22:21:37.996593: step 413, loss 0.110622, acc 1, learning_rate 0.00100856
2017-10-02T22:21:39.166391: step 414, loss 0.144503, acc 1, learning_rate 0.00100486
2017-10-02T22:21:40.331976: step 415, loss 0.173653, acc 1, learning_rate 0.00100116
2017-10-02T22:21:41.477817: step 416, loss 0.111392, acc 1, learning_rate 0.000997483
2017-10-02T22:21:42.625431: step 417, loss 0.160511, acc 0.984375, learning_rate 0.00099382
2017-10-02T22:21:43.795599: step 418, loss 0.167962, acc 0.984375, learning_rate 0.000990172
2017-10-02T22:21:44.953074: step 419, loss 0.152416, acc 0.984375, learning_rate 0.000986538
2017-10-02T22:21:46.095243: step 420, loss 0.123661, acc 1, learning_rate 0.00098292
2017-10-02T22:21:47.248521: step 421, loss 0.124204, acc 0.984375, learning_rate 0.000979316
2017-10-02T22:21:48.413410: step 422, loss 0.149516, acc 1, learning_rate 0.000975727
2017-10-02T22:21:49.574027: step 423, loss 0.138656, acc 1, learning_rate 0.000972152
2017-10-02T22:21:50.746513: step 424, loss 0.175839, acc 1, learning_rate 0.000968592
2017-10-02T22:21:51.896295: step 425, loss 0.162684, acc 1, learning_rate 0.000965047
2017-10-02T22:21:53.047368: step 426, loss 0.201968, acc 0.984375, learning_rate 0.000961516
2017-10-02T22:21:54.203696: step 427, loss 0.115396, acc 1, learning_rate 0.000958
2017-10-02T22:21:55.358637: step 428, loss 0.106606, acc 1, learning_rate 0.000954497
2017-10-02T22:21:56.524276: step 429, loss 0.175957, acc 0.984375, learning_rate 0.00095101
2017-10-02T22:21:57.673574: step 430, loss 0.14976, acc 0.984375, learning_rate 0.000947536
2017-10-02T22:21:58.846735: step 431, loss 0.157611, acc 0.984375, learning_rate 0.000944076
2017-10-02T22:22:00.033763: step 432, loss 0.157036, acc 1, learning_rate 0.000940631
2017-10-02T22:22:01.190263: step 433, loss 0.141213, acc 1, learning_rate 0.0009372
2017-10-02T22:22:02.374211: step 434, loss 0.159004, acc 0.984375, learning_rate 0.000933783
2017-10-02T22:22:03.541641: step 435, loss 0.117215, acc 1, learning_rate 0.000930379
2017-10-02T22:22:04.699015: step 436, loss 0.14502, acc 0.984375, learning_rate 0.00092699
2017-10-02T22:22:05.855326: step 437, loss 0.144345, acc 1, learning_rate 0.000923614
2017-10-02T22:22:07.028325: step 438, loss 0.211589, acc 0.96875, learning_rate 0.000920253
2017-10-02T22:22:08.170136: step 439, loss 0.155734, acc 0.984375, learning_rate 0.000916905
2017-10-02T22:22:09.332076: step 440, loss 0.167814, acc 0.984375, learning_rate 0.00091357

Evaluation:
2017-10-02T22:22:09.646056: step 440, loss 1.42455, acc 0.448921

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-440

2017-10-02T22:22:17.463981: step 441, loss 0.132088, acc 1, learning_rate 0.000910249
2017-10-02T22:22:18.612158: step 442, loss 0.168931, acc 0.984375, learning_rate 0.000906942
2017-10-02T22:22:19.770967: step 443, loss 0.139801, acc 0.96875, learning_rate 0.000903648
2017-10-02T22:22:20.918085: step 444, loss 0.124659, acc 0.984375, learning_rate 0.000900368
2017-10-02T22:22:22.064714: step 445, loss 0.109568, acc 1, learning_rate 0.000897101
2017-10-02T22:22:23.215108: step 446, loss 0.153241, acc 1, learning_rate 0.000893848
2017-10-02T22:22:24.383506: step 447, loss 0.178028, acc 0.984375, learning_rate 0.000890607
2017-10-02T22:22:25.533180: step 448, loss 0.169328, acc 1, learning_rate 0.00088738
2017-10-02T22:22:26.691600: step 449, loss 0.120439, acc 1, learning_rate 0.000884166
2017-10-02T22:22:27.854055: step 450, loss 0.141846, acc 1, learning_rate 0.000880966
2017-10-02T22:22:29.009737: step 451, loss 0.13008, acc 0.984375, learning_rate 0.000877778
2017-10-02T22:22:30.166680: step 452, loss 0.163099, acc 1, learning_rate 0.000874603
2017-10-02T22:22:31.326363: step 453, loss 0.152227, acc 1, learning_rate 0.000871441
2017-10-02T22:22:32.495771: step 454, loss 0.168785, acc 0.96875, learning_rate 0.000868293
2017-10-02T22:22:33.650729: step 455, loss 0.154389, acc 1, learning_rate 0.000865157
2017-10-02T22:22:34.798207: step 456, loss 0.132592, acc 1, learning_rate 0.000862033
2017-10-02T22:22:35.954328: step 457, loss 0.125651, acc 0.984375, learning_rate 0.000858923
2017-10-02T22:22:37.112362: step 458, loss 0.14373, acc 1, learning_rate 0.000855825
2017-10-02T22:22:38.286407: step 459, loss 0.124365, acc 1, learning_rate 0.00085274
2017-10-02T22:22:39.449340: step 460, loss 0.124728, acc 1, learning_rate 0.000849668
2017-10-02T22:22:40.610532: step 461, loss 0.194449, acc 0.984375, learning_rate 0.000846608
2017-10-02T22:22:41.761836: step 462, loss 0.143915, acc 0.984375, learning_rate 0.00084356
2017-10-02T22:22:42.935521: step 463, loss 0.130288, acc 1, learning_rate 0.000840525
2017-10-02T22:22:44.088008: step 464, loss 0.140807, acc 1, learning_rate 0.000837502
2017-10-02T22:22:45.238272: step 465, loss 0.141032, acc 0.984375, learning_rate 0.000834492
2017-10-02T22:22:46.405657: step 466, loss 0.16284, acc 0.984375, learning_rate 0.000831494
2017-10-02T22:22:47.553913: step 467, loss 0.116207, acc 1, learning_rate 0.000828508
2017-10-02T22:22:48.711380: step 468, loss 0.151415, acc 0.984375, learning_rate 0.000825535
2017-10-02T22:22:49.856813: step 469, loss 0.176776, acc 1, learning_rate 0.000822573
2017-10-02T22:22:51.011603: step 470, loss 0.0905445, acc 1, learning_rate 0.000819624
2017-10-02T22:22:52.159027: step 471, loss 0.11881, acc 1, learning_rate 0.000816687
2017-10-02T22:22:53.318940: step 472, loss 0.117039, acc 1, learning_rate 0.000813761
2017-10-02T22:22:54.477134: step 473, loss 0.1245, acc 0.984375, learning_rate 0.000810848
2017-10-02T22:22:55.632213: step 474, loss 0.14543, acc 1, learning_rate 0.000807946
2017-10-02T22:22:56.777016: step 475, loss 0.130193, acc 1, learning_rate 0.000805057
2017-10-02T22:22:57.929519: step 476, loss 0.11933, acc 1, learning_rate 0.000802179
2017-10-02T22:22:59.087662: step 477, loss 0.125684, acc 1, learning_rate 0.000799313
2017-10-02T22:23:00.254783: step 478, loss 0.0859604, acc 1, learning_rate 0.000796458
2017-10-02T22:23:01.413963: step 479, loss 0.132823, acc 1, learning_rate 0.000793616
2017-10-02T22:23:02.555147: step 480, loss 0.150683, acc 1, learning_rate 0.000790784

Evaluation:
2017-10-02T22:23:02.874431: step 480, loss 1.44547, acc 0.457554

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-480

2017-10-02T22:23:11.021105: step 481, loss 0.106135, acc 1, learning_rate 0.000787965
2017-10-02T22:23:12.182925: step 482, loss 0.13936, acc 0.984375, learning_rate 0.000785157
2017-10-02T22:23:13.349022: step 483, loss 0.129725, acc 0.96875, learning_rate 0.00078236
2017-10-02T22:23:14.503075: step 484, loss 0.168134, acc 0.96875, learning_rate 0.000779575
2017-10-02T22:23:15.664195: step 485, loss 0.105326, acc 1, learning_rate 0.000776801
2017-10-02T22:23:16.821167: step 486, loss 0.11646, acc 1, learning_rate 0.000774038
2017-10-02T22:23:17.997499: step 487, loss 0.158855, acc 1, learning_rate 0.000771287
2017-10-02T22:23:19.164662: step 488, loss 0.13817, acc 0.984375, learning_rate 0.000768547
2017-10-02T22:23:20.341858: step 489, loss 0.164291, acc 0.984375, learning_rate 0.000765818
2017-10-02T22:23:21.471414: step 490, loss 0.141238, acc 1, learning_rate 0.000763101
2017-10-02T22:23:22.644240: step 491, loss 0.139213, acc 1, learning_rate 0.000760394
2017-10-02T22:23:23.786561: step 492, loss 0.105728, acc 0.984375, learning_rate 0.000757698
2017-10-02T22:23:24.942020: step 493, loss 0.103314, acc 0.984375, learning_rate 0.000755014
2017-10-02T22:23:26.101366: step 494, loss 0.102443, acc 1, learning_rate 0.00075234
2017-10-02T22:23:27.256043: step 495, loss 0.105101, acc 1, learning_rate 0.000749677
2017-10-02T22:23:28.409244: step 496, loss 0.110912, acc 0.984375, learning_rate 0.000747026
2017-10-02T22:23:29.558257: step 497, loss 0.0931613, acc 1, learning_rate 0.000744385
2017-10-02T22:23:30.723266: step 498, loss 0.0786653, acc 1, learning_rate 0.000741754
2017-10-02T22:23:31.878533: step 499, loss 0.098427, acc 0.984375, learning_rate 0.000739135
2017-10-02T22:23:33.033859: step 500, loss 0.0819667, acc 1, learning_rate 0.000736526
2017-10-02T22:23:34.198037: step 501, loss 0.132312, acc 0.96875, learning_rate 0.000733928
2017-10-02T22:23:35.356338: step 502, loss 0.0646577, acc 1, learning_rate 0.00073134
2017-10-02T22:23:36.507614: step 503, loss 0.0759767, acc 1, learning_rate 0.000728763
2017-10-02T22:23:37.662881: step 504, loss 0.0900149, acc 1, learning_rate 0.000726197
2017-10-02T22:23:39.040031: step 505, loss 0.0995822, acc 0.984375, learning_rate 0.000723641
2017-10-02T22:23:40.191535: step 506, loss 0.0528829, acc 1, learning_rate 0.000721095
2017-10-02T22:23:41.348402: step 507, loss 0.0833696, acc 1, learning_rate 0.00071856
2017-10-02T22:23:42.497440: step 508, loss 0.0927609, acc 1, learning_rate 0.000716036
2017-10-02T22:23:43.667603: step 509, loss 0.115539, acc 1, learning_rate 0.000713521
2017-10-02T22:23:44.827806: step 510, loss 0.0965551, acc 1, learning_rate 0.000711017
2017-10-02T22:23:45.973458: step 511, loss 0.067389, acc 1, learning_rate 0.000708523
2017-10-02T22:23:47.132892: step 512, loss 0.0980695, acc 1, learning_rate 0.000706039
2017-10-02T22:23:48.291585: step 513, loss 0.113641, acc 0.984375, learning_rate 0.000703565
2017-10-02T22:23:49.443801: step 514, loss 0.0727942, acc 1, learning_rate 0.000701102
2017-10-02T22:23:50.605916: step 515, loss 0.0839185, acc 1, learning_rate 0.000698648
2017-10-02T22:23:51.768392: step 516, loss 0.0758441, acc 1, learning_rate 0.000696204
2017-10-02T22:23:52.929509: step 517, loss 0.126546, acc 0.984375, learning_rate 0.000693771
2017-10-02T22:23:54.092327: step 518, loss 0.0957818, acc 0.984375, learning_rate 0.000691347
2017-10-02T22:23:55.256543: step 519, loss 0.0641598, acc 1, learning_rate 0.000688934
2017-10-02T22:23:56.416778: step 520, loss 0.100338, acc 1, learning_rate 0.00068653

Evaluation:
2017-10-02T22:23:56.766762: step 520, loss 1.46141, acc 0.446043

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-520

2017-10-02T22:24:04.393706: step 521, loss 0.0996914, acc 0.984375, learning_rate 0.000684136
2017-10-02T22:24:05.609048: step 522, loss 0.0889256, acc 1, learning_rate 0.000681751
2017-10-02T22:24:06.847193: step 523, loss 0.0843303, acc 1, learning_rate 0.000679377
2017-10-02T22:24:08.014501: step 524, loss 0.120411, acc 0.984375, learning_rate 0.000677012
2017-10-02T22:24:09.179925: step 525, loss 0.0924123, acc 1, learning_rate 0.000674657
2017-10-02T22:24:10.326318: step 526, loss 0.0679352, acc 1, learning_rate 0.000672311
2017-10-02T22:24:11.471617: step 527, loss 0.0799021, acc 1, learning_rate 0.000669975
2017-10-02T22:24:12.640233: step 528, loss 0.0698594, acc 1, learning_rate 0.000667648
2017-10-02T22:24:13.806449: step 529, loss 0.0909576, acc 1, learning_rate 0.000665331
2017-10-02T22:24:14.968626: step 530, loss 0.0806948, acc 1, learning_rate 0.000663024
2017-10-02T22:24:16.133401: step 531, loss 0.0801047, acc 1, learning_rate 0.000660726
2017-10-02T22:24:17.306592: step 532, loss 0.0854341, acc 0.984375, learning_rate 0.000658437
2017-10-02T22:24:18.482927: step 533, loss 0.0908266, acc 0.984375, learning_rate 0.000656158
2017-10-02T22:24:19.630402: step 534, loss 0.0972393, acc 0.984375, learning_rate 0.000653888
2017-10-02T22:24:20.783562: step 535, loss 0.0608483, acc 1, learning_rate 0.000651627
2017-10-02T22:24:21.929411: step 536, loss 0.0793234, acc 1, learning_rate 0.000649375
2017-10-02T22:24:23.102117: step 537, loss 0.0786374, acc 1, learning_rate 0.000647133
2017-10-02T22:24:24.267157: step 538, loss 0.12154, acc 0.984375, learning_rate 0.000644899
2017-10-02T22:24:25.418415: step 539, loss 0.0878762, acc 1, learning_rate 0.000642675
2017-10-02T22:24:26.569944: step 540, loss 0.158556, acc 0.96875, learning_rate 0.00064046
2017-10-02T22:24:27.740537: step 541, loss 0.113593, acc 0.984375, learning_rate 0.000638254
2017-10-02T22:24:28.910117: step 542, loss 0.0968604, acc 1, learning_rate 0.000636057
2017-10-02T22:24:30.065321: step 543, loss 0.0899725, acc 0.984375, learning_rate 0.000633869
2017-10-02T22:24:31.218313: step 544, loss 0.0808834, acc 1, learning_rate 0.00063169
2017-10-02T22:24:32.381193: step 545, loss 0.0857438, acc 1, learning_rate 0.00062952
2017-10-02T22:24:33.553194: step 546, loss 0.0800928, acc 1, learning_rate 0.000627358
2017-10-02T22:24:34.733813: step 547, loss 0.0988795, acc 1, learning_rate 0.000625206
2017-10-02T22:24:35.927813: step 548, loss 0.0857194, acc 1, learning_rate 0.000623062
2017-10-02T22:24:37.108552: step 549, loss 0.0878846, acc 1, learning_rate 0.000620927
2017-10-02T22:24:38.261660: step 550, loss 0.0753427, acc 1, learning_rate 0.000618801
2017-10-02T22:24:39.431056: step 551, loss 0.0720447, acc 1, learning_rate 0.000616683
2017-10-02T22:24:40.576914: step 552, loss 0.133267, acc 0.984375, learning_rate 0.000614574
2017-10-02T22:24:41.732349: step 553, loss 0.0787263, acc 1, learning_rate 0.000612474
2017-10-02T22:24:42.892895: step 554, loss 0.0743516, acc 1, learning_rate 0.000610382
2017-10-02T22:24:44.043088: step 555, loss 0.0871066, acc 1, learning_rate 0.000608299
2017-10-02T22:24:45.190221: step 556, loss 0.109442, acc 0.984375, learning_rate 0.000606224
2017-10-02T22:24:46.341015: step 557, loss 0.0682328, acc 1, learning_rate 0.000604158
2017-10-02T22:24:47.584651: step 558, loss 0.0881803, acc 1, learning_rate 0.0006021
2017-10-02T22:24:48.742537: step 559, loss 0.0905262, acc 0.96875, learning_rate 0.00060005
2017-10-02T22:24:49.914955: step 560, loss 0.0477283, acc 1, learning_rate 0.000598009

Evaluation:
2017-10-02T22:24:50.223285: step 560, loss 1.45902, acc 0.443165

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-560

2017-10-02T22:24:57.863057: step 561, loss 0.129203, acc 0.96875, learning_rate 0.000595977
2017-10-02T22:24:59.035581: step 562, loss 0.0620277, acc 1, learning_rate 0.000593952
2017-10-02T22:25:00.207007: step 563, loss 0.0751711, acc 1, learning_rate 0.000591936
2017-10-02T22:25:01.358983: step 564, loss 0.0856342, acc 1, learning_rate 0.000589928
2017-10-02T22:25:02.511845: step 565, loss 0.0559388, acc 1, learning_rate 0.000587928
2017-10-02T22:25:03.669867: step 566, loss 0.116467, acc 1, learning_rate 0.000585937
2017-10-02T22:25:04.825361: step 567, loss 0.0767097, acc 1, learning_rate 0.000583953
2017-10-02T22:25:05.991336: step 568, loss 0.0922594, acc 0.984375, learning_rate 0.000581978
2017-10-02T22:25:07.148881: step 569, loss 0.100928, acc 0.984375, learning_rate 0.00058001
2017-10-02T22:25:08.317093: step 570, loss 0.0798067, acc 1, learning_rate 0.000578051
2017-10-02T22:25:09.467494: step 571, loss 0.0567444, acc 1, learning_rate 0.0005761
2017-10-02T22:25:10.647882: step 572, loss 0.0896599, acc 1, learning_rate 0.000574157
2017-10-02T22:25:11.804907: step 573, loss 0.0844318, acc 1, learning_rate 0.000572221
2017-10-02T22:25:12.957050: step 574, loss 0.0948321, acc 0.984375, learning_rate 0.000570294
2017-10-02T22:25:14.147169: step 575, loss 0.0781898, acc 1, learning_rate 0.000568374
2017-10-02T22:25:15.310204: step 576, loss 0.0697047, acc 1, learning_rate 0.000566462
2017-10-02T22:25:16.466218: step 577, loss 0.104937, acc 0.96875, learning_rate 0.000564558
2017-10-02T22:25:17.616641: step 578, loss 0.0524062, acc 1, learning_rate 0.000562662
2017-10-02T22:25:18.763892: step 579, loss 0.0765608, acc 1, learning_rate 0.000560774
2017-10-02T22:25:19.929521: step 580, loss 0.093329, acc 0.984375, learning_rate 0.000558893
2017-10-02T22:25:21.086639: step 581, loss 0.112301, acc 0.984375, learning_rate 0.00055702
2017-10-02T22:25:22.289353: step 582, loss 0.06995, acc 0.984375, learning_rate 0.000555154
2017-10-02T22:25:23.449260: step 583, loss 0.10668, acc 0.984375, learning_rate 0.000553296
2017-10-02T22:25:24.608568: step 584, loss 0.0978719, acc 0.984375, learning_rate 0.000551446
2017-10-02T22:25:25.765914: step 585, loss 0.0578223, acc 1, learning_rate 0.000549604
2017-10-02T22:25:26.921059: step 586, loss 0.0956692, acc 0.984375, learning_rate 0.000547768
2017-10-02T22:25:28.063982: step 587, loss 0.0777006, acc 1, learning_rate 0.000545941
2017-10-02T22:25:29.222917: step 588, loss 0.0776962, acc 1, learning_rate 0.00054412
2017-10-02T22:25:30.383938: step 589, loss 0.118219, acc 0.984375, learning_rate 0.000542308
2017-10-02T22:25:31.539818: step 590, loss 0.0899208, acc 0.984375, learning_rate 0.000540502
2017-10-02T22:25:32.697040: step 591, loss 0.0583885, acc 1, learning_rate 0.000538704
2017-10-02T22:25:33.858738: step 592, loss 0.0945445, acc 0.984375, learning_rate 0.000536914
2017-10-02T22:25:35.012940: step 593, loss 0.0758703, acc 0.984375, learning_rate 0.00053513
2017-10-02T22:25:36.182531: step 594, loss 0.0919214, acc 0.984375, learning_rate 0.000533354
2017-10-02T22:25:37.342917: step 595, loss 0.0399293, acc 1, learning_rate 0.000531585
2017-10-02T22:25:38.510103: step 596, loss 0.0833752, acc 0.984375, learning_rate 0.000529824
2017-10-02T22:25:39.668031: step 597, loss 0.0574166, acc 1, learning_rate 0.000528069
2017-10-02T22:25:40.829306: step 598, loss 0.0566463, acc 1, learning_rate 0.000526322
2017-10-02T22:25:41.987102: step 599, loss 0.062892, acc 1, learning_rate 0.000524582
2017-10-02T22:25:43.133980: step 600, loss 0.0642512, acc 1, learning_rate 0.000522849

Evaluation:
2017-10-02T22:25:43.502190: step 600, loss 1.46349, acc 0.421583

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-600

2017-10-02T22:25:51.733805: step 601, loss 0.0812494, acc 1, learning_rate 0.000521123
2017-10-02T22:25:52.923792: step 602, loss 0.0573434, acc 1, learning_rate 0.000519404
2017-10-02T22:25:54.083024: step 603, loss 0.0455581, acc 1, learning_rate 0.000517692
2017-10-02T22:25:55.212414: step 604, loss 0.0744808, acc 0.984375, learning_rate 0.000515987
2017-10-02T22:25:56.368885: step 605, loss 0.0628655, acc 1, learning_rate 0.000514289
2017-10-02T22:25:57.525824: step 606, loss 0.0848986, acc 0.984375, learning_rate 0.000512598
2017-10-02T22:25:58.694928: step 607, loss 0.0748567, acc 1, learning_rate 0.000510914
2017-10-02T22:25:59.879018: step 608, loss 0.0632893, acc 1, learning_rate 0.000509237
2017-10-02T22:26:01.048442: step 609, loss 0.102836, acc 0.984375, learning_rate 0.000507566
2017-10-02T22:26:02.203358: step 610, loss 0.0650077, acc 1, learning_rate 0.000505903
2017-10-02T22:26:03.363838: step 611, loss 0.0874521, acc 0.984375, learning_rate 0.000504246
2017-10-02T22:26:04.544601: step 612, loss 0.0952146, acc 0.984375, learning_rate 0.000502596
2017-10-02T22:26:05.711387: step 613, loss 0.0535921, acc 1, learning_rate 0.000500953
2017-10-02T22:26:06.868733: step 614, loss 0.0612323, acc 1, learning_rate 0.000499316
2017-10-02T22:26:08.015582: step 615, loss 0.0514589, acc 1, learning_rate 0.000497686
2017-10-02T22:26:09.169481: step 616, loss 0.0438066, acc 1, learning_rate 0.000496063
2017-10-02T22:26:10.317475: step 617, loss 0.0791063, acc 1, learning_rate 0.000494446
2017-10-02T22:26:11.467349: step 618, loss 0.16609, acc 0.953125, learning_rate 0.000492836
2017-10-02T22:26:12.614958: step 619, loss 0.0981168, acc 0.984375, learning_rate 0.000491233
2017-10-02T22:26:13.774506: step 620, loss 0.0798019, acc 0.984375, learning_rate 0.000489636
2017-10-02T22:26:15.172895: step 621, loss 0.0616509, acc 1, learning_rate 0.000488045
2017-10-02T22:26:16.325736: step 622, loss 0.0807367, acc 1, learning_rate 0.000486461
2017-10-02T22:26:17.477335: step 623, loss 0.0614724, acc 1, learning_rate 0.000484884
2017-10-02T22:26:18.636950: step 624, loss 0.0692262, acc 0.984375, learning_rate 0.000483313
2017-10-02T22:26:19.787631: step 625, loss 0.0746657, acc 1, learning_rate 0.000481748
2017-10-02T22:26:20.950962: step 626, loss 0.0395194, acc 1, learning_rate 0.00048019
2017-10-02T22:26:22.099975: step 627, loss 0.0512412, acc 1, learning_rate 0.000478638
2017-10-02T22:26:23.263307: step 628, loss 0.101278, acc 0.984375, learning_rate 0.000477093
2017-10-02T22:26:24.435372: step 629, loss 0.0424647, acc 1, learning_rate 0.000475554
2017-10-02T22:26:25.599042: step 630, loss 0.0540081, acc 1, learning_rate 0.000474021
2017-10-02T22:26:26.963445: step 631, loss 0.072226, acc 1, learning_rate 0.000472494
2017-10-02T22:26:28.126046: step 632, loss 0.0581947, acc 1, learning_rate 0.000470974
2017-10-02T22:26:29.297239: step 633, loss 0.043235, acc 1, learning_rate 0.000469459
2017-10-02T22:26:30.454111: step 634, loss 0.055591, acc 1, learning_rate 0.000467951
2017-10-02T22:26:31.606075: step 635, loss 0.0521345, acc 1, learning_rate 0.000466449
2017-10-02T22:26:32.864206: step 636, loss 0.0846805, acc 1, learning_rate 0.000464954
2017-10-02T22:26:34.013314: step 637, loss 0.0575463, acc 1, learning_rate 0.000463464
2017-10-02T22:26:35.178063: step 638, loss 0.0859439, acc 1, learning_rate 0.00046198
2017-10-02T22:26:36.343814: step 639, loss 0.0536634, acc 1, learning_rate 0.000460503
2017-10-02T22:26:37.510642: step 640, loss 0.0720608, acc 1, learning_rate 0.000459031

Evaluation:
2017-10-02T22:26:37.821254: step 640, loss 1.43501, acc 0.482014

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-640

2017-10-02T22:26:45.952871: step 641, loss 0.0455576, acc 1, learning_rate 0.000457566
2017-10-02T22:26:47.153730: step 642, loss 0.062423, acc 1, learning_rate 0.000456106
2017-10-02T22:26:48.309609: step 643, loss 0.0546, acc 1, learning_rate 0.000454653
2017-10-02T22:26:49.455052: step 644, loss 0.0548952, acc 1, learning_rate 0.000453205
2017-10-02T22:26:50.619528: step 645, loss 0.14842, acc 0.96875, learning_rate 0.000451764
2017-10-02T22:26:51.758666: step 646, loss 0.0572851, acc 1, learning_rate 0.000450328
2017-10-02T22:26:52.932752: step 647, loss 0.0624752, acc 1, learning_rate 0.000448898
2017-10-02T22:26:54.104949: step 648, loss 0.129285, acc 0.984375, learning_rate 0.000447474
2017-10-02T22:26:55.272539: step 649, loss 0.0534309, acc 0.984375, learning_rate 0.000446055
2017-10-02T22:26:56.424000: step 650, loss 0.0637639, acc 1, learning_rate 0.000444643
2017-10-02T22:26:57.579904: step 651, loss 0.0594039, acc 1, learning_rate 0.000443236
2017-10-02T22:26:58.735959: step 652, loss 0.0731465, acc 1, learning_rate 0.000441835
2017-10-02T22:26:59.903508: step 653, loss 0.0632421, acc 1, learning_rate 0.00044044
2017-10-02T22:27:01.064810: step 654, loss 0.0750577, acc 0.984375, learning_rate 0.00043905
2017-10-02T22:27:02.221597: step 655, loss 0.0709573, acc 1, learning_rate 0.000437666
2017-10-02T22:27:03.378115: step 656, loss 0.0457468, acc 1, learning_rate 0.000436288
2017-10-02T22:27:04.523009: step 657, loss 0.0830221, acc 0.984375, learning_rate 0.000434915
2017-10-02T22:27:05.679604: step 658, loss 0.0660641, acc 1, learning_rate 0.000433548
2017-10-02T22:27:06.821305: step 659, loss 0.0637777, acc 1, learning_rate 0.000432187
2017-10-02T22:27:07.972110: step 660, loss 0.114772, acc 0.984375, learning_rate 0.000430831
2017-10-02T22:27:09.119515: step 661, loss 0.0734975, acc 1, learning_rate 0.000429481
2017-10-02T22:27:10.276176: step 662, loss 0.073805, acc 0.984375, learning_rate 0.000428136
2017-10-02T22:27:11.439028: step 663, loss 0.0357173, acc 1, learning_rate 0.000426796
2017-10-02T22:27:12.600460: step 664, loss 0.0574315, acc 1, learning_rate 0.000425463
2017-10-02T22:27:13.744246: step 665, loss 0.0499749, acc 1, learning_rate 0.000424134
2017-10-02T22:27:14.899701: step 666, loss 0.0404448, acc 1, learning_rate 0.000422811
2017-10-02T22:27:16.037809: step 667, loss 0.0734612, acc 1, learning_rate 0.000421493
2017-10-02T22:27:17.193566: step 668, loss 0.0576442, acc 1, learning_rate 0.000420181
2017-10-02T22:27:18.364503: step 669, loss 0.0654185, acc 1, learning_rate 0.000418874
2017-10-02T22:27:19.521082: step 670, loss 0.0540079, acc 1, learning_rate 0.000417573
2017-10-02T22:27:20.672297: step 671, loss 0.0681377, acc 1, learning_rate 0.000416276
2017-10-02T22:27:21.818674: step 672, loss 0.117541, acc 0.96875, learning_rate 0.000414985
2017-10-02T22:27:23.200804: step 673, loss 0.0895538, acc 0.96875, learning_rate 0.0004137
2017-10-02T22:27:24.373950: step 674, loss 0.0779087, acc 1, learning_rate 0.000412419
2017-10-02T22:27:25.532784: step 675, loss 0.067946, acc 1, learning_rate 0.000411144
2017-10-02T22:27:26.678640: step 676, loss 0.0334865, acc 1, learning_rate 0.000409874
2017-10-02T22:27:27.832912: step 677, loss 0.0781953, acc 1, learning_rate 0.000408609
2017-10-02T22:27:28.982286: step 678, loss 0.0827267, acc 0.984375, learning_rate 0.00040735
2017-10-02T22:27:30.147028: step 679, loss 0.0786949, acc 0.984375, learning_rate 0.000406095
2017-10-02T22:27:31.323329: step 680, loss 0.0431596, acc 1, learning_rate 0.000404846

Evaluation:
2017-10-02T22:27:31.649584: step 680, loss 1.43302, acc 0.492086

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-680

2017-10-02T22:27:39.637819: step 681, loss 0.0624867, acc 1, learning_rate 0.000403601
2017-10-02T22:27:40.822260: step 682, loss 0.0599166, acc 1, learning_rate 0.000402362
2017-10-02T22:27:41.978607: step 683, loss 0.0868663, acc 0.984375, learning_rate 0.000401128
2017-10-02T22:27:43.127714: step 684, loss 0.0738592, acc 0.984375, learning_rate 0.000399899
2017-10-02T22:27:44.297376: step 685, loss 0.0741741, acc 1, learning_rate 0.000398675
2017-10-02T22:27:45.460676: step 686, loss 0.0373355, acc 1, learning_rate 0.000397456
2017-10-02T22:27:46.621040: step 687, loss 0.0770379, acc 1, learning_rate 0.000396241
2017-10-02T22:27:47.762387: step 688, loss 0.0765007, acc 0.96875, learning_rate 0.000395032
2017-10-02T22:27:48.916133: step 689, loss 0.0447726, acc 1, learning_rate 0.000393828
2017-10-02T22:27:50.058474: step 690, loss 0.0417542, acc 1, learning_rate 0.000392629
2017-10-02T22:27:51.210516: step 691, loss 0.0752705, acc 0.984375, learning_rate 0.000391434
2017-10-02T22:27:52.383795: step 692, loss 0.0581629, acc 1, learning_rate 0.000390245
2017-10-02T22:27:53.541592: step 693, loss 0.0680538, acc 1, learning_rate 0.00038906
2017-10-02T22:27:54.877453: step 694, loss 0.0795014, acc 0.984375, learning_rate 0.00038788
2017-10-02T22:27:56.024122: step 695, loss 0.0484042, acc 1, learning_rate 0.000386705
2017-10-02T22:27:57.185226: step 696, loss 0.0430416, acc 1, learning_rate 0.000385535
2017-10-02T22:27:58.343144: step 697, loss 0.0460627, acc 1, learning_rate 0.000384369
2017-10-02T22:27:59.497856: step 698, loss 0.0677537, acc 0.984375, learning_rate 0.000383209
2017-10-02T22:28:00.692387: step 699, loss 0.0450705, acc 1, learning_rate 0.000382053
2017-10-02T22:28:01.894116: step 700, loss 0.0908964, acc 0.96875, learning_rate 0.000380901
2017-10-02T22:28:03.064582: step 701, loss 0.0397729, acc 1, learning_rate 0.000379755
2017-10-02T22:28:04.227245: step 702, loss 0.0811722, acc 0.984375, learning_rate 0.000378613
2017-10-02T22:28:05.400492: step 703, loss 0.0522821, acc 1, learning_rate 0.000377476
2017-10-02T22:28:06.552284: step 704, loss 0.0422211, acc 1, learning_rate 0.000376343
2017-10-02T22:28:07.711248: step 705, loss 0.0413267, acc 1, learning_rate 0.000375215
2017-10-02T22:28:08.873001: step 706, loss 0.0540777, acc 1, learning_rate 0.000374092
2017-10-02T22:28:10.044839: step 707, loss 0.0533674, acc 1, learning_rate 0.000372973
2017-10-02T22:28:11.180853: step 708, loss 0.0808735, acc 1, learning_rate 0.000371859
2017-10-02T22:28:12.332951: step 709, loss 0.0571206, acc 1, learning_rate 0.000370749
2017-10-02T22:28:13.489249: step 710, loss 0.044799, acc 1, learning_rate 0.000369644
2017-10-02T22:28:14.644413: step 711, loss 0.0353409, acc 1, learning_rate 0.000368543
2017-10-02T22:28:15.806368: step 712, loss 0.0565696, acc 1, learning_rate 0.000367447
2017-10-02T22:28:16.957648: step 713, loss 0.0641377, acc 1, learning_rate 0.000366356
2017-10-02T22:28:18.109643: step 714, loss 0.0479686, acc 1, learning_rate 0.000365268
2017-10-02T22:28:19.252298: step 715, loss 0.0715231, acc 0.984375, learning_rate 0.000364186
2017-10-02T22:28:20.425196: step 716, loss 0.0488728, acc 1, learning_rate 0.000363107
2017-10-02T22:28:21.573743: step 717, loss 0.0549719, acc 1, learning_rate 0.000362033
2017-10-02T22:28:22.729943: step 718, loss 0.105051, acc 0.953125, learning_rate 0.000360964
2017-10-02T22:28:23.885318: step 719, loss 0.0490447, acc 1, learning_rate 0.000359899
2017-10-02T22:28:25.040433: step 720, loss 0.0584269, acc 1, learning_rate 0.000358838

Evaluation:
2017-10-02T22:28:25.364160: step 720, loss 1.4302, acc 0.46187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-720

2017-10-02T22:28:33.138834: step 721, loss 0.0576074, acc 0.984375, learning_rate 0.000357781
2017-10-02T22:28:34.343057: step 722, loss 0.052353, acc 1, learning_rate 0.000356729
2017-10-02T22:28:35.503184: step 723, loss 0.051768, acc 0.984375, learning_rate 0.000355681
2017-10-02T22:28:36.648365: step 724, loss 0.0422602, acc 1, learning_rate 0.000354637
2017-10-02T22:28:37.866078: step 725, loss 0.0414854, acc 1, learning_rate 0.000353598
2017-10-02T22:28:39.022709: step 726, loss 0.0441619, acc 1, learning_rate 0.000352563
2017-10-02T22:28:40.173766: step 727, loss 0.0882066, acc 0.984375, learning_rate 0.000351532
2017-10-02T22:28:41.330029: step 728, loss 0.0566123, acc 1, learning_rate 0.000350505
2017-10-02T22:28:42.488963: step 729, loss 0.0379812, acc 1, learning_rate 0.000349483
2017-10-02T22:28:43.655245: step 730, loss 0.061014, acc 0.984375, learning_rate 0.000348465
2017-10-02T22:28:44.816759: step 731, loss 0.0984526, acc 0.984375, learning_rate 0.00034745
2017-10-02T22:28:45.974996: step 732, loss 0.0712681, acc 0.984375, learning_rate 0.00034644
2017-10-02T22:28:47.141623: step 733, loss 0.0595819, acc 1, learning_rate 0.000345434
2017-10-02T22:28:48.296965: step 734, loss 0.0558672, acc 1, learning_rate 0.000344433
2017-10-02T22:28:49.463911: step 735, loss 0.0609534, acc 1, learning_rate 0.000343435
2017-10-02T22:28:50.612960: step 736, loss 0.0392565, acc 1, learning_rate 0.000342441
2017-10-02T22:28:51.784028: step 737, loss 0.0471501, acc 1, learning_rate 0.000341452
2017-10-02T22:28:52.943165: step 738, loss 0.0657779, acc 0.984375, learning_rate 0.000340466
2017-10-02T22:28:54.101750: step 739, loss 0.0505171, acc 1, learning_rate 0.000339485
2017-10-02T22:28:55.251885: step 740, loss 0.0439406, acc 1, learning_rate 0.000338507
2017-10-02T22:28:56.418438: step 741, loss 0.038807, acc 1, learning_rate 0.000337534
2017-10-02T22:28:57.567469: step 742, loss 0.0315675, acc 1, learning_rate 0.000336564
2017-10-02T22:28:58.719175: step 743, loss 0.0605037, acc 1, learning_rate 0.000335598
2017-10-02T22:28:59.904337: step 744, loss 0.0650398, acc 0.984375, learning_rate 0.000334637
2017-10-02T22:29:01.057104: step 745, loss 0.032843, acc 1, learning_rate 0.000333679
2017-10-02T22:29:02.204251: step 746, loss 0.109093, acc 0.96875, learning_rate 0.000332725
2017-10-02T22:29:03.350871: step 747, loss 0.075121, acc 0.984375, learning_rate 0.000331775
2017-10-02T22:29:04.512967: step 748, loss 0.0382274, acc 1, learning_rate 0.000330829
2017-10-02T22:29:05.675641: step 749, loss 0.0340855, acc 1, learning_rate 0.000329887
2017-10-02T22:29:06.830934: step 750, loss 0.0836526, acc 1, learning_rate 0.000328949
2017-10-02T22:29:07.972672: step 751, loss 0.0330658, acc 1, learning_rate 0.000328014
2017-10-02T22:29:09.132478: step 752, loss 0.0533229, acc 1, learning_rate 0.000327083
2017-10-02T22:29:10.282237: step 753, loss 0.0778829, acc 0.96875, learning_rate 0.000326157
2017-10-02T22:29:11.445032: step 754, loss 0.0740823, acc 1, learning_rate 0.000325233
2017-10-02T22:29:12.600262: step 755, loss 0.0456699, acc 1, learning_rate 0.000324314
2017-10-02T22:29:13.756520: step 756, loss 0.050073, acc 1, learning_rate 0.000323399
2017-10-02T22:29:14.902787: step 757, loss 0.0528248, acc 1, learning_rate 0.000322487
2017-10-02T22:29:16.059726: step 758, loss 0.0540796, acc 1, learning_rate 0.000321579
2017-10-02T22:29:17.213900: step 759, loss 0.0626736, acc 1, learning_rate 0.000320674
2017-10-02T22:29:18.387687: step 760, loss 0.0601913, acc 1, learning_rate 0.000319773

Evaluation:
2017-10-02T22:29:18.728149: step 760, loss 1.43959, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-760

2017-10-02T22:29:26.606395: step 761, loss 0.0720017, acc 0.984375, learning_rate 0.000318876
2017-10-02T22:29:27.764853: step 762, loss 0.0716555, acc 1, learning_rate 0.000317983
2017-10-02T22:29:28.920007: step 763, loss 0.0624482, acc 1, learning_rate 0.000317093
2017-10-02T22:29:30.057595: step 764, loss 0.0520859, acc 1, learning_rate 0.000316207
2017-10-02T22:29:31.208339: step 765, loss 0.0792583, acc 0.984375, learning_rate 0.000315325
2017-10-02T22:29:32.380878: step 766, loss 0.0709706, acc 0.984375, learning_rate 0.000314446
2017-10-02T22:29:33.540007: step 767, loss 0.0486674, acc 1, learning_rate 0.00031357
2017-10-02T22:29:34.691003: step 768, loss 0.0328129, acc 1, learning_rate 0.000312699
2017-10-02T22:29:35.844824: step 769, loss 0.0769759, acc 0.984375, learning_rate 0.00031183
2017-10-02T22:29:36.996453: step 770, loss 0.0431228, acc 1, learning_rate 0.000310966
2017-10-02T22:29:38.150801: step 771, loss 0.097972, acc 0.96875, learning_rate 0.000310105
2017-10-02T22:29:39.314527: step 772, loss 0.034915, acc 1, learning_rate 0.000309247
2017-10-02T22:29:40.472721: step 773, loss 0.0642942, acc 1, learning_rate 0.000308393
2017-10-02T22:29:41.630294: step 774, loss 0.0721319, acc 0.984375, learning_rate 0.000307542
2017-10-02T22:29:42.798148: step 775, loss 0.0599376, acc 0.984375, learning_rate 0.000306695
2017-10-02T22:29:43.953638: step 776, loss 0.0724194, acc 0.984375, learning_rate 0.000305852
2017-10-02T22:29:45.108351: step 777, loss 0.0469431, acc 1, learning_rate 0.000305011
2017-10-02T22:29:46.257394: step 778, loss 0.0758268, acc 0.984375, learning_rate 0.000304174
2017-10-02T22:29:47.404179: step 779, loss 0.0612067, acc 1, learning_rate 0.000303341
2017-10-02T22:29:48.560144: step 780, loss 0.0485218, acc 1, learning_rate 0.000302511
2017-10-02T22:29:49.719828: step 781, loss 0.0413704, acc 1, learning_rate 0.000301684
2017-10-02T22:29:50.870782: step 782, loss 0.0491269, acc 1, learning_rate 0.000300861
2017-10-02T22:29:52.018685: step 783, loss 0.0354703, acc 1, learning_rate 0.000300041
2017-10-02T22:29:53.140590: step 784, loss 0.0455952, acc 1, learning_rate 0.000299225
2017-10-02T22:29:54.293128: step 785, loss 0.0235871, acc 1, learning_rate 0.000298412
2017-10-02T22:29:55.457153: step 786, loss 0.0282831, acc 1, learning_rate 0.000297602
2017-10-02T22:29:56.616640: step 787, loss 0.0423422, acc 1, learning_rate 0.000296795
2017-10-02T22:29:57.770446: step 788, loss 0.0487527, acc 1, learning_rate 0.000295992
2017-10-02T22:29:59.009113: step 789, loss 0.031379, acc 1, learning_rate 0.000295192
2017-10-02T22:30:00.172796: step 790, loss 0.0299414, acc 1, learning_rate 0.000294395
2017-10-02T22:30:01.344634: step 791, loss 0.0744558, acc 0.984375, learning_rate 0.000293602
2017-10-02T22:30:02.504186: step 792, loss 0.0566891, acc 1, learning_rate 0.000292812
2017-10-02T22:30:03.656199: step 793, loss 0.0368129, acc 1, learning_rate 0.000292025
2017-10-02T22:30:04.805535: step 794, loss 0.051689, acc 1, learning_rate 0.000291241
2017-10-02T22:30:05.958399: step 795, loss 0.038437, acc 1, learning_rate 0.00029046
2017-10-02T22:30:07.113382: step 796, loss 0.058366, acc 0.984375, learning_rate 0.000289683
2017-10-02T22:30:08.270096: step 797, loss 0.0315293, acc 1, learning_rate 0.000288908
2017-10-02T22:30:09.421875: step 798, loss 0.0345075, acc 1, learning_rate 0.000288137
2017-10-02T22:30:10.579744: step 799, loss 0.0595533, acc 1, learning_rate 0.000287369
2017-10-02T22:30:11.738212: step 800, loss 0.0398313, acc 1, learning_rate 0.000286605

Evaluation:
2017-10-02T22:30:12.070756: step 800, loss 1.44666, acc 0.477698

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-800

2017-10-02T22:30:20.225665: step 801, loss 0.0340717, acc 1, learning_rate 0.000285843
2017-10-02T22:30:21.397232: step 802, loss 0.0464488, acc 1, learning_rate 0.000285084
2017-10-02T22:30:22.554742: step 803, loss 0.0455235, acc 1, learning_rate 0.000284329
2017-10-02T22:30:23.708831: step 804, loss 0.0403979, acc 1, learning_rate 0.000283577
2017-10-02T22:30:24.858026: step 805, loss 0.047823, acc 1, learning_rate 0.000282827
2017-10-02T22:30:26.024933: step 806, loss 0.0438632, acc 1, learning_rate 0.000282081
2017-10-02T22:30:27.193357: step 807, loss 0.0532251, acc 0.984375, learning_rate 0.000281338
2017-10-02T22:30:28.332494: step 808, loss 0.056699, acc 0.984375, learning_rate 0.000280598
2017-10-02T22:30:29.482900: step 809, loss 0.04955, acc 1, learning_rate 0.00027986
2017-10-02T22:30:30.630284: step 810, loss 0.0435962, acc 1, learning_rate 0.000279126
2017-10-02T22:30:31.779066: step 811, loss 0.0660409, acc 0.984375, learning_rate 0.000278395
2017-10-02T22:30:32.925075: step 812, loss 0.0268286, acc 1, learning_rate 0.000277667
2017-10-02T22:30:34.070426: step 813, loss 0.0246907, acc 1, learning_rate 0.000276942
2017-10-02T22:30:35.222058: step 814, loss 0.0585389, acc 1, learning_rate 0.00027622
2017-10-02T22:30:36.375665: step 815, loss 0.0748287, acc 0.984375, learning_rate 0.0002755
2017-10-02T22:30:37.524754: step 816, loss 0.0523195, acc 1, learning_rate 0.000274784
2017-10-02T22:30:38.680762: step 817, loss 0.0509696, acc 1, learning_rate 0.000274071
2017-10-02T22:30:39.838214: step 818, loss 0.0375144, acc 1, learning_rate 0.00027336
2017-10-02T22:30:40.987355: step 819, loss 0.03683, acc 1, learning_rate 0.000272652
2017-10-02T22:30:42.147164: step 820, loss 0.0533798, acc 1, learning_rate 0.000271948
2017-10-02T22:30:43.302140: step 821, loss 0.0607847, acc 0.984375, learning_rate 0.000271246
2017-10-02T22:30:44.460115: step 822, loss 0.0467442, acc 1, learning_rate 0.000270547
2017-10-02T22:30:45.612600: step 823, loss 0.0428695, acc 1, learning_rate 0.000269851
2017-10-02T22:30:46.776467: step 824, loss 0.0382992, acc 1, learning_rate 0.000269157
2017-10-02T22:30:47.924932: step 825, loss 0.0528786, acc 1, learning_rate 0.000268467
2017-10-02T22:30:49.080036: step 826, loss 0.0429565, acc 1, learning_rate 0.000267779
2017-10-02T22:30:50.247252: step 827, loss 0.0779112, acc 0.984375, learning_rate 0.000267094
2017-10-02T22:30:51.402412: step 828, loss 0.0706972, acc 0.984375, learning_rate 0.000266412
2017-10-02T22:30:52.556604: step 829, loss 0.0341628, acc 1, learning_rate 0.000265733
2017-10-02T22:30:53.786837: step 830, loss 0.0400669, acc 1, learning_rate 0.000265057
2017-10-02T22:30:54.938881: step 831, loss 0.0645477, acc 0.984375, learning_rate 0.000264383
2017-10-02T22:30:56.069441: step 832, loss 0.0376575, acc 1, learning_rate 0.000263712
2017-10-02T22:30:57.216313: step 833, loss 0.03772, acc 1, learning_rate 0.000263044
2017-10-02T22:30:58.382679: step 834, loss 0.0502217, acc 0.984375, learning_rate 0.000262378
2017-10-02T22:30:59.534068: step 835, loss 0.0464792, acc 1, learning_rate 0.000261715
2017-10-02T22:31:00.696295: step 836, loss 0.066402, acc 0.984375, learning_rate 0.000261055
2017-10-02T22:31:01.855641: step 837, loss 0.108323, acc 0.96875, learning_rate 0.000260398
2017-10-02T22:31:02.998802: step 838, loss 0.0415549, acc 1, learning_rate 0.000259743
2017-10-02T22:31:04.144787: step 839, loss 0.0380383, acc 1, learning_rate 0.000259091
2017-10-02T22:31:05.292731: step 840, loss 0.113528, acc 0.96875, learning_rate 0.000258442

Evaluation:
2017-10-02T22:31:05.616763: step 840, loss 1.44394, acc 0.479137

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-840

2017-10-02T22:31:13.638242: step 841, loss 0.0382157, acc 1, learning_rate 0.000257795
2017-10-02T22:31:14.807392: step 842, loss 0.037211, acc 1, learning_rate 0.000257151
2017-10-02T22:31:15.951750: step 843, loss 0.0727028, acc 1, learning_rate 0.00025651
2017-10-02T22:31:17.112224: step 844, loss 0.0514941, acc 1, learning_rate 0.000255871
2017-10-02T22:31:18.269362: step 845, loss 0.0735257, acc 0.984375, learning_rate 0.000255235
2017-10-02T22:31:19.426443: step 846, loss 0.0311478, acc 1, learning_rate 0.000254601
2017-10-02T22:31:20.584760: step 847, loss 0.0521025, acc 1, learning_rate 0.00025397
2017-10-02T22:31:21.736809: step 848, loss 0.0378088, acc 1, learning_rate 0.000253341
2017-10-02T22:31:23.020063: step 849, loss 0.035096, acc 1, learning_rate 0.000252716
2017-10-02T22:31:24.169762: step 850, loss 0.0528432, acc 1, learning_rate 0.000252092
2017-10-02T22:31:25.330846: step 851, loss 0.047363, acc 0.984375, learning_rate 0.000251471
2017-10-02T22:31:26.485196: step 852, loss 0.051135, acc 1, learning_rate 0.000250853
2017-10-02T22:31:27.628497: step 853, loss 0.0539271, acc 1, learning_rate 0.000250237
2017-10-02T22:31:28.799349: step 854, loss 0.0599803, acc 1, learning_rate 0.000249624
2017-10-02T22:31:29.942764: step 855, loss 0.0485692, acc 0.984375, learning_rate 0.000249013
2017-10-02T22:31:31.085835: step 856, loss 0.0526245, acc 1, learning_rate 0.000248405
2017-10-02T22:31:32.226101: step 857, loss 0.0378873, acc 1, learning_rate 0.000247799
2017-10-02T22:31:33.376917: step 858, loss 0.0817115, acc 0.984375, learning_rate 0.000247196
2017-10-02T22:31:34.609219: step 859, loss 0.0715398, acc 0.984375, learning_rate 0.000246595
2017-10-02T22:31:35.765420: step 860, loss 0.0666729, acc 0.984375, learning_rate 0.000245997
2017-10-02T22:31:36.917426: step 861, loss 0.0530581, acc 0.984375, learning_rate 0.000245401
2017-10-02T22:31:38.058809: step 862, loss 0.0377955, acc 1, learning_rate 0.000244808
2017-10-02T22:31:39.206934: step 863, loss 0.055095, acc 1, learning_rate 0.000244216
2017-10-02T22:31:40.381730: step 864, loss 0.0719949, acc 0.984375, learning_rate 0.000243628
2017-10-02T22:31:41.542336: step 865, loss 0.0443229, acc 1, learning_rate 0.000243042
2017-10-02T22:31:42.693485: step 866, loss 0.0357408, acc 1, learning_rate 0.000242458
2017-10-02T22:31:43.831901: step 867, loss 0.0664794, acc 0.984375, learning_rate 0.000241876
2017-10-02T22:31:44.984715: step 868, loss 0.0438214, acc 0.984375, learning_rate 0.000241297
2017-10-02T22:31:46.135507: step 869, loss 0.0459467, acc 1, learning_rate 0.00024072
2017-10-02T22:31:47.284853: step 870, loss 0.0337249, acc 1, learning_rate 0.000240146
2017-10-02T22:31:48.432777: step 871, loss 0.0613766, acc 1, learning_rate 0.000239574
2017-10-02T22:31:49.597619: step 872, loss 0.0349904, acc 1, learning_rate 0.000239004
2017-10-02T22:31:50.743592: step 873, loss 0.0366384, acc 1, learning_rate 0.000238437
2017-10-02T22:31:51.891663: step 874, loss 0.0370527, acc 1, learning_rate 0.000237872
2017-10-02T22:31:53.034671: step 875, loss 0.0515464, acc 0.984375, learning_rate 0.000237309
2017-10-02T22:31:54.188892: step 876, loss 0.063879, acc 0.984375, learning_rate 0.000236749
2017-10-02T22:31:55.331999: step 877, loss 0.0324903, acc 1, learning_rate 0.00023619
2017-10-02T22:31:56.512483: step 878, loss 0.038268, acc 1, learning_rate 0.000235635
2017-10-02T22:31:57.676024: step 879, loss 0.0522035, acc 1, learning_rate 0.000235081
2017-10-02T22:31:58.838152: step 880, loss 0.0673657, acc 0.984375, learning_rate 0.00023453

Evaluation:
2017-10-02T22:31:59.166697: step 880, loss 1.43277, acc 0.483453

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-880

2017-10-02T22:32:06.867867: step 881, loss 0.0486755, acc 1, learning_rate 0.00023398
2017-10-02T22:32:08.024278: step 882, loss 0.0463875, acc 1, learning_rate 0.000233434
2017-10-02T22:32:09.167806: step 883, loss 0.0608951, acc 1, learning_rate 0.000232889
2017-10-02T22:32:10.317058: step 884, loss 0.0362014, acc 1, learning_rate 0.000232346
2017-10-02T22:32:11.461876: step 885, loss 0.0374433, acc 1, learning_rate 0.000231806
2017-10-02T22:32:12.613627: step 886, loss 0.0440293, acc 1, learning_rate 0.000231268
2017-10-02T22:32:13.753843: step 887, loss 0.0416205, acc 1, learning_rate 0.000230732
2017-10-02T22:32:14.911151: step 888, loss 0.0319036, acc 1, learning_rate 0.000230199
2017-10-02T22:32:16.065745: step 889, loss 0.0366705, acc 1, learning_rate 0.000229667
2017-10-02T22:32:17.225557: step 890, loss 0.0588196, acc 0.984375, learning_rate 0.000229138
2017-10-02T22:32:18.376641: step 891, loss 0.0479751, acc 0.984375, learning_rate 0.000228611
2017-10-02T22:32:19.526325: step 892, loss 0.0331716, acc 1, learning_rate 0.000228086
2017-10-02T22:32:20.674805: step 893, loss 0.0661954, acc 0.984375, learning_rate 0.000227563
2017-10-02T22:32:21.819712: step 894, loss 0.0410103, acc 1, learning_rate 0.000227043
2017-10-02T22:32:22.970822: step 895, loss 0.105557, acc 0.984375, learning_rate 0.000226524
2017-10-02T22:32:24.138290: step 896, loss 0.0341377, acc 1, learning_rate 0.000226008
2017-10-02T22:32:25.285081: step 897, loss 0.0327062, acc 1, learning_rate 0.000225493
2017-10-02T22:32:26.443268: step 898, loss 0.0423477, acc 1, learning_rate 0.000224981
2017-10-02T22:32:27.603884: step 899, loss 0.0319145, acc 1, learning_rate 0.000224471
2017-10-02T22:32:28.823748: step 900, loss 0.0324486, acc 1, learning_rate 0.000223963
2017-10-02T22:32:29.984384: step 901, loss 0.0565595, acc 1, learning_rate 0.000223457
2017-10-02T22:32:31.143481: step 902, loss 0.0355333, acc 1, learning_rate 0.000222953
2017-10-02T22:32:32.292872: step 903, loss 0.0303948, acc 1, learning_rate 0.000222451
2017-10-02T22:32:33.443061: step 904, loss 0.0363412, acc 1, learning_rate 0.000221951
2017-10-02T22:32:34.598493: step 905, loss 0.0303125, acc 1, learning_rate 0.000221453
2017-10-02T22:32:35.751414: step 906, loss 0.0424421, acc 1, learning_rate 0.000220958
2017-10-02T22:32:36.897805: step 907, loss 0.0290783, acc 1, learning_rate 0.000220464
2017-10-02T22:32:38.061091: step 908, loss 0.041339, acc 1, learning_rate 0.000219972
2017-10-02T22:32:39.216711: step 909, loss 0.034019, acc 1, learning_rate 0.000219483
2017-10-02T22:32:40.363045: step 910, loss 0.0623614, acc 0.984375, learning_rate 0.000218995
2017-10-02T22:32:41.515226: step 911, loss 0.0397297, acc 1, learning_rate 0.000218509
2017-10-02T22:32:42.664699: step 912, loss 0.0381569, acc 1, learning_rate 0.000218025
2017-10-02T22:32:43.817296: step 913, loss 0.0253467, acc 1, learning_rate 0.000217544
2017-10-02T22:32:44.954546: step 914, loss 0.0307487, acc 1, learning_rate 0.000217064
2017-10-02T22:32:46.101754: step 915, loss 0.0663589, acc 0.984375, learning_rate 0.000216586
2017-10-02T22:32:47.265407: step 916, loss 0.0358096, acc 1, learning_rate 0.00021611
2017-10-02T22:32:48.432116: step 917, loss 0.0351003, acc 1, learning_rate 0.000215636
2017-10-02T22:32:49.580405: step 918, loss 0.0424679, acc 1, learning_rate 0.000215164
2017-10-02T22:32:50.796302: step 919, loss 0.0475131, acc 1, learning_rate 0.000214694
2017-10-02T22:32:51.957003: step 920, loss 0.11056, acc 0.96875, learning_rate 0.000214226

Evaluation:
2017-10-02T22:32:52.288081: step 920, loss 1.46095, acc 0.420144

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-920

2017-10-02T22:33:00.392683: step 921, loss 0.0887851, acc 0.984375, learning_rate 0.00021376
2017-10-02T22:33:01.564526: step 922, loss 0.030225, acc 1, learning_rate 0.000213295
2017-10-02T22:33:02.721643: step 923, loss 0.0333416, acc 1, learning_rate 0.000212833
2017-10-02T22:33:03.871160: step 924, loss 0.0794128, acc 0.984375, learning_rate 0.000212372
2017-10-02T22:33:05.026351: step 925, loss 0.093785, acc 0.96875, learning_rate 0.000211914
2017-10-02T22:33:06.182940: step 926, loss 0.0499037, acc 1, learning_rate 0.000211457
2017-10-02T22:33:07.328364: step 927, loss 0.0755425, acc 0.984375, learning_rate 0.000211002
2017-10-02T22:33:08.468859: step 928, loss 0.0525313, acc 1, learning_rate 0.000210549
2017-10-02T22:33:09.623301: step 929, loss 0.0433403, acc 0.984375, learning_rate 0.000210098
2017-10-02T22:33:10.762407: step 930, loss 0.0435144, acc 1, learning_rate 0.000209648
2017-10-02T22:33:11.908584: step 931, loss 0.0497582, acc 1, learning_rate 0.000209201
2017-10-02T22:33:13.071221: step 932, loss 0.0256481, acc 1, learning_rate 0.000208755
2017-10-02T22:33:14.222770: step 933, loss 0.0382884, acc 1, learning_rate 0.000208311
2017-10-02T22:33:15.388528: step 934, loss 0.0317275, acc 1, learning_rate 0.000207869
2017-10-02T22:33:16.553498: step 935, loss 0.0310017, acc 1, learning_rate 0.000207429
2017-10-02T22:33:17.711411: step 936, loss 0.0549772, acc 0.984375, learning_rate 0.00020699
2017-10-02T22:33:18.874171: step 937, loss 0.117962, acc 0.953125, learning_rate 0.000206554
2017-10-02T22:33:20.029735: step 938, loss 0.0380649, acc 1, learning_rate 0.000206119
2017-10-02T22:33:21.175031: step 939, loss 0.0498796, acc 1, learning_rate 0.000205685
2017-10-02T22:33:22.315440: step 940, loss 0.0342436, acc 1, learning_rate 0.000205254
2017-10-02T22:33:23.475745: step 941, loss 0.0270448, acc 1, learning_rate 0.000204824
2017-10-02T22:33:24.631542: step 942, loss 0.047024, acc 1, learning_rate 0.000204397
2017-10-02T22:33:25.803246: step 943, loss 0.0394256, acc 1, learning_rate 0.00020397
2017-10-02T22:33:27.177623: step 944, loss 0.0552235, acc 0.984375, learning_rate 0.000203546
2017-10-02T22:33:28.318740: step 945, loss 0.0540054, acc 0.984375, learning_rate 0.000203123
2017-10-02T22:33:29.471771: step 946, loss 0.0357426, acc 1, learning_rate 0.000202702
2017-10-02T22:33:30.617629: step 947, loss 0.0454047, acc 1, learning_rate 0.000202283
2017-10-02T22:33:31.767130: step 948, loss 0.0542916, acc 0.984375, learning_rate 0.000201866
2017-10-02T22:33:32.939072: step 949, loss 0.0320383, acc 1, learning_rate 0.00020145
2017-10-02T22:33:34.097114: step 950, loss 0.0646033, acc 1, learning_rate 0.000201036
2017-10-02T22:33:35.265267: step 951, loss 0.0245254, acc 1, learning_rate 0.000200623
2017-10-02T22:33:36.412550: step 952, loss 0.0397772, acc 1, learning_rate 0.000200213
2017-10-02T22:33:37.554644: step 953, loss 0.0400525, acc 1, learning_rate 0.000199804
2017-10-02T22:33:38.705045: step 954, loss 0.0551454, acc 0.984375, learning_rate 0.000199396
2017-10-02T22:33:39.858914: step 955, loss 0.0603764, acc 0.984375, learning_rate 0.000198991
2017-10-02T22:33:41.003761: step 956, loss 0.0666335, acc 0.984375, learning_rate 0.000198587
2017-10-02T22:33:42.162152: step 957, loss 0.0345546, acc 1, learning_rate 0.000198184
2017-10-02T22:33:43.315033: step 958, loss 0.0358879, acc 1, learning_rate 0.000197783
2017-10-02T22:33:44.461998: step 959, loss 0.052985, acc 0.984375, learning_rate 0.000197384
2017-10-02T22:33:45.619793: step 960, loss 0.0589562, acc 0.984375, learning_rate 0.000196987

Evaluation:
2017-10-02T22:33:45.947562: step 960, loss 1.43963, acc 0.482014

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-960

2017-10-02T22:33:53.237636: step 961, loss 0.028978, acc 1, learning_rate 0.000196591
2017-10-02T22:33:54.403035: step 962, loss 0.0332141, acc 1, learning_rate 0.000196197
2017-10-02T22:33:55.558960: step 963, loss 0.0367063, acc 1, learning_rate 0.000195804
2017-10-02T22:33:56.714386: step 964, loss 0.0380412, acc 1, learning_rate 0.000195413
2017-10-02T22:33:57.892307: step 965, loss 0.0410934, acc 1, learning_rate 0.000195023
2017-10-02T22:33:59.073521: step 966, loss 0.029614, acc 1, learning_rate 0.000194636
2017-10-02T22:34:00.226916: step 967, loss 0.0560624, acc 0.984375, learning_rate 0.000194249
2017-10-02T22:34:01.386832: step 968, loss 0.0207352, acc 1, learning_rate 0.000193865
2017-10-02T22:34:02.529508: step 969, loss 0.0747301, acc 0.96875, learning_rate 0.000193482
2017-10-02T22:34:03.683897: step 970, loss 0.0490664, acc 1, learning_rate 0.0001931
2017-10-02T22:34:04.839163: step 971, loss 0.0430018, acc 1, learning_rate 0.00019272
2017-10-02T22:34:05.986581: step 972, loss 0.0259273, acc 1, learning_rate 0.000192341
2017-10-02T22:34:07.142826: step 973, loss 0.0636641, acc 1, learning_rate 0.000191965
2017-10-02T22:34:08.298792: step 974, loss 0.0375591, acc 1, learning_rate 0.000191589
2017-10-02T22:34:09.446998: step 975, loss 0.0352181, acc 1, learning_rate 0.000191215
2017-10-02T22:34:10.606788: step 976, loss 0.0407461, acc 1, learning_rate 0.000190843
2017-10-02T22:34:11.758721: step 977, loss 0.0224707, acc 1, learning_rate 0.000190472
2017-10-02T22:34:12.900755: step 978, loss 0.0333597, acc 1, learning_rate 0.000190103
2017-10-02T22:34:14.053546: step 979, loss 0.0557487, acc 0.984375, learning_rate 0.000189735
2017-10-02T22:34:15.184385: step 980, loss 0.0763012, acc 0.980392, learning_rate 0.000189369
2017-10-02T22:34:16.331790: step 981, loss 0.0276158, acc 1, learning_rate 0.000189004
2017-10-02T22:34:17.483926: step 982, loss 0.0343254, acc 1, learning_rate 0.000188641
2017-10-02T22:34:18.623143: step 983, loss 0.0339515, acc 1, learning_rate 0.000188279
2017-10-02T22:34:19.774212: step 984, loss 0.0593697, acc 0.984375, learning_rate 0.000187919
2017-10-02T22:34:20.916281: step 985, loss 0.0489488, acc 1, learning_rate 0.00018756
2017-10-02T22:34:22.064717: step 986, loss 0.037003, acc 1, learning_rate 0.000187202
2017-10-02T22:34:23.220590: step 987, loss 0.0426805, acc 1, learning_rate 0.000186846
2017-10-02T22:34:24.370811: step 988, loss 0.0277365, acc 1, learning_rate 0.000186492
2017-10-02T22:34:25.520878: step 989, loss 0.0392329, acc 1, learning_rate 0.000186139
2017-10-02T22:34:26.671158: step 990, loss 0.0739023, acc 0.96875, learning_rate 0.000185787
2017-10-02T22:34:27.811921: step 991, loss 0.0517573, acc 1, learning_rate 0.000185437
2017-10-02T22:34:28.979025: step 992, loss 0.0324147, acc 1, learning_rate 0.000185088
2017-10-02T22:34:30.126483: step 993, loss 0.0280293, acc 1, learning_rate 0.000184741
2017-10-02T22:34:31.277846: step 994, loss 0.0233486, acc 1, learning_rate 0.000184395
2017-10-02T22:34:32.425555: step 995, loss 0.0400749, acc 1, learning_rate 0.000184051
2017-10-02T22:34:33.584512: step 996, loss 0.0426726, acc 1, learning_rate 0.000183708
2017-10-02T22:34:34.735687: step 997, loss 0.0316934, acc 1, learning_rate 0.000183366
2017-10-02T22:34:35.885333: step 998, loss 0.0654358, acc 1, learning_rate 0.000183026
2017-10-02T22:34:37.027114: step 999, loss 0.0328779, acc 1, learning_rate 0.000182687
2017-10-02T22:34:38.166918: step 1000, loss 0.0516338, acc 0.984375, learning_rate 0.000182349

Evaluation:
2017-10-02T22:34:38.502836: step 1000, loss 1.44625, acc 0.463309

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1000

2017-10-02T22:34:46.148429: step 1001, loss 0.0496647, acc 1, learning_rate 0.000182013
2017-10-02T22:34:47.295256: step 1002, loss 0.021755, acc 1, learning_rate 0.000181678
2017-10-02T22:34:48.452161: step 1003, loss 0.0616132, acc 0.984375, learning_rate 0.000181345
2017-10-02T22:34:49.606291: step 1004, loss 0.0313079, acc 1, learning_rate 0.000181013
2017-10-02T22:34:50.963297: step 1005, loss 0.0438769, acc 1, learning_rate 0.000180682
2017-10-02T22:34:52.098998: step 1006, loss 0.0452107, acc 1, learning_rate 0.000180353
2017-10-02T22:34:53.249081: step 1007, loss 0.0401054, acc 1, learning_rate 0.000180025
2017-10-02T22:34:54.396571: step 1008, loss 0.0333047, acc 0.984375, learning_rate 0.000179698
2017-10-02T22:34:55.550998: step 1009, loss 0.0695675, acc 0.984375, learning_rate 0.000179373
2017-10-02T22:34:56.702980: step 1010, loss 0.0581391, acc 0.984375, learning_rate 0.000179049
2017-10-02T22:34:57.858668: step 1011, loss 0.0486333, acc 0.984375, learning_rate 0.000178726
2017-10-02T22:34:59.011073: step 1012, loss 0.0329735, acc 1, learning_rate 0.000178405
2017-10-02T22:35:00.166168: step 1013, loss 0.0382329, acc 1, learning_rate 0.000178085
2017-10-02T22:35:01.315486: step 1014, loss 0.0486749, acc 0.984375, learning_rate 0.000177766
2017-10-02T22:35:02.476917: step 1015, loss 0.0224209, acc 1, learning_rate 0.000177449
2017-10-02T22:35:03.632301: step 1016, loss 0.0295672, acc 1, learning_rate 0.000177133
2017-10-02T22:35:04.782448: step 1017, loss 0.0299196, acc 1, learning_rate 0.000176818
2017-10-02T22:35:05.947302: step 1018, loss 0.0337746, acc 1, learning_rate 0.000176504
2017-10-02T22:35:07.094431: step 1019, loss 0.0538892, acc 0.984375, learning_rate 0.000176192
2017-10-02T22:35:08.248904: step 1020, loss 0.0408612, acc 1, learning_rate 0.000175881
2017-10-02T22:35:09.399488: step 1021, loss 0.022963, acc 1, learning_rate 0.000175571
2017-10-02T22:35:10.567496: step 1022, loss 0.026568, acc 1, learning_rate 0.000175263
2017-10-02T22:35:11.725026: step 1023, loss 0.022099, acc 1, learning_rate 0.000174956
2017-10-02T22:35:12.874102: step 1024, loss 0.0419281, acc 1, learning_rate 0.00017465
2017-10-02T22:35:14.022667: step 1025, loss 0.0365489, acc 1, learning_rate 0.000174345
2017-10-02T22:35:15.175089: step 1026, loss 0.0401631, acc 1, learning_rate 0.000174042
2017-10-02T22:35:16.328819: step 1027, loss 0.0505001, acc 0.984375, learning_rate 0.000173739
2017-10-02T22:35:17.486780: step 1028, loss 0.046984, acc 1, learning_rate 0.000173438
2017-10-02T22:35:18.650033: step 1029, loss 0.0312776, acc 1, learning_rate 0.000173139
2017-10-02T22:35:19.798265: step 1030, loss 0.0861512, acc 0.96875, learning_rate 0.00017284
2017-10-02T22:35:20.946910: step 1031, loss 0.0515992, acc 0.984375, learning_rate 0.000172543
2017-10-02T22:35:22.106957: step 1032, loss 0.0546475, acc 0.96875, learning_rate 0.000172247
2017-10-02T22:35:23.261401: step 1033, loss 0.0351743, acc 1, learning_rate 0.000171952
2017-10-02T22:35:24.428133: step 1034, loss 0.0212525, acc 1, learning_rate 0.000171658
2017-10-02T22:35:25.578477: step 1035, loss 0.0410127, acc 0.984375, learning_rate 0.000171366
2017-10-02T22:35:26.765969: step 1036, loss 0.0241555, acc 1, learning_rate 0.000171074
2017-10-02T22:35:27.908808: step 1037, loss 0.0282575, acc 1, learning_rate 0.000170784
2017-10-02T22:35:29.059947: step 1038, loss 0.0897263, acc 0.984375, learning_rate 0.000170495
2017-10-02T22:35:30.211268: step 1039, loss 0.0597944, acc 0.984375, learning_rate 0.000170208
2017-10-02T22:35:31.356801: step 1040, loss 0.0417288, acc 1, learning_rate 0.000169921

Evaluation:
2017-10-02T22:35:31.683946: step 1040, loss 1.43335, acc 0.483453

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1040

2017-10-02T22:35:39.345643: step 1041, loss 0.039773, acc 1, learning_rate 0.000169636
2017-10-02T22:35:40.519047: step 1042, loss 0.0516567, acc 1, learning_rate 0.000169351
2017-10-02T22:35:41.663635: step 1043, loss 0.0438299, acc 1, learning_rate 0.000169068
2017-10-02T22:35:42.810890: step 1044, loss 0.0337223, acc 1, learning_rate 0.000168786
2017-10-02T22:35:43.957134: step 1045, loss 0.0293186, acc 1, learning_rate 0.000168506
2017-10-02T22:35:45.098177: step 1046, loss 0.0575942, acc 1, learning_rate 0.000168226
2017-10-02T22:35:46.257577: step 1047, loss 0.0502319, acc 1, learning_rate 0.000167947
2017-10-02T22:35:47.409536: step 1048, loss 0.0283676, acc 1, learning_rate 0.00016767
2017-10-02T22:35:48.569952: step 1049, loss 0.0296036, acc 1, learning_rate 0.000167394
2017-10-02T22:35:49.720477: step 1050, loss 0.035389, acc 1, learning_rate 0.000167119
2017-10-02T22:35:50.875494: step 1051, loss 0.0276457, acc 1, learning_rate 0.000166845
2017-10-02T22:35:52.020439: step 1052, loss 0.0250642, acc 1, learning_rate 0.000166572
2017-10-02T22:35:53.155318: step 1053, loss 0.0585511, acc 0.984375, learning_rate 0.0001663
2017-10-02T22:35:54.334533: step 1054, loss 0.0652506, acc 0.984375, learning_rate 0.00016603
2017-10-02T22:35:55.559165: step 1055, loss 0.019176, acc 1, learning_rate 0.00016576
2017-10-02T22:35:56.702840: step 1056, loss 0.0610564, acc 0.984375, learning_rate 0.000165492
2017-10-02T22:35:57.866389: step 1057, loss 0.0577526, acc 0.984375, learning_rate 0.000165224
2017-10-02T22:35:59.022771: step 1058, loss 0.0249455, acc 1, learning_rate 0.000164958
2017-10-02T22:36:00.184660: step 1059, loss 0.0397463, acc 1, learning_rate 0.000164693
2017-10-02T22:36:01.341218: step 1060, loss 0.0310871, acc 1, learning_rate 0.000164429
2017-10-02T22:36:02.502045: step 1061, loss 0.023497, acc 1, learning_rate 0.000164166
2017-10-02T22:36:03.821460: step 1062, loss 0.0602359, acc 1, learning_rate 0.000163904
2017-10-02T22:36:04.970187: step 1063, loss 0.0388591, acc 0.984375, learning_rate 0.000163643
2017-10-02T22:36:06.109225: step 1064, loss 0.0300205, acc 1, learning_rate 0.000163383
2017-10-02T22:36:07.271265: step 1065, loss 0.0595566, acc 0.984375, learning_rate 0.000163125
2017-10-02T22:36:08.426911: step 1066, loss 0.0707813, acc 0.984375, learning_rate 0.000162867
2017-10-02T22:36:09.581813: step 1067, loss 0.0228395, acc 1, learning_rate 0.00016261
2017-10-02T22:36:10.735089: step 1068, loss 0.0332318, acc 1, learning_rate 0.000162355
2017-10-02T22:36:11.897328: step 1069, loss 0.0272801, acc 1, learning_rate 0.0001621
2017-10-02T22:36:13.048261: step 1070, loss 0.0301467, acc 1, learning_rate 0.000161847
2017-10-02T22:36:14.207665: step 1071, loss 0.0411036, acc 1, learning_rate 0.000161594
2017-10-02T22:36:15.372444: step 1072, loss 0.0485158, acc 0.984375, learning_rate 0.000161343
2017-10-02T22:36:16.529368: step 1073, loss 0.0626311, acc 0.984375, learning_rate 0.000161093
2017-10-02T22:36:17.692959: step 1074, loss 0.0382832, acc 1, learning_rate 0.000160843
2017-10-02T22:36:18.842538: step 1075, loss 0.0400535, acc 1, learning_rate 0.000160595
2017-10-02T22:36:19.992368: step 1076, loss 0.0353625, acc 1, learning_rate 0.000160348
2017-10-02T22:36:21.141569: step 1077, loss 0.0496897, acc 1, learning_rate 0.000160101
2017-10-02T22:36:22.279784: step 1078, loss 0.0333817, acc 1, learning_rate 0.000159856
2017-10-02T22:36:23.432060: step 1079, loss 0.0303244, acc 1, learning_rate 0.000159612
2017-10-02T22:36:24.578684: step 1080, loss 0.0274021, acc 1, learning_rate 0.000159368

Evaluation:
2017-10-02T22:36:24.932616: step 1080, loss 1.44868, acc 0.435971

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1080

2017-10-02T22:36:32.622600: step 1081, loss 0.0198252, acc 1, learning_rate 0.000159126
2017-10-02T22:36:33.800976: step 1082, loss 0.0373305, acc 1, learning_rate 0.000158885
2017-10-02T22:36:34.941932: step 1083, loss 0.034658, acc 1, learning_rate 0.000158644
2017-10-02T22:36:36.084472: step 1084, loss 0.0517615, acc 1, learning_rate 0.000158405
2017-10-02T22:36:37.222643: step 1085, loss 0.0592144, acc 0.984375, learning_rate 0.000158167
2017-10-02T22:36:38.374558: step 1086, loss 0.047247, acc 0.984375, learning_rate 0.000157929
2017-10-02T22:36:39.533220: step 1087, loss 0.0326567, acc 1, learning_rate 0.000157693
2017-10-02T22:36:40.696015: step 1088, loss 0.0377545, acc 1, learning_rate 0.000157457
2017-10-02T22:36:41.894370: step 1089, loss 0.0230959, acc 1, learning_rate 0.000157223
2017-10-02T22:36:43.043594: step 1090, loss 0.0426395, acc 0.984375, learning_rate 0.000156989
2017-10-02T22:36:44.188182: step 1091, loss 0.02748, acc 1, learning_rate 0.000156757
2017-10-02T22:36:45.337753: step 1092, loss 0.0565273, acc 0.984375, learning_rate 0.000156525
2017-10-02T22:36:46.503941: step 1093, loss 0.0377449, acc 1, learning_rate 0.000156294
2017-10-02T22:36:47.654143: step 1094, loss 0.0276243, acc 1, learning_rate 0.000156064
2017-10-02T22:36:48.794588: step 1095, loss 0.0366863, acc 1, learning_rate 0.000155836
2017-10-02T22:36:49.946062: step 1096, loss 0.0300169, acc 1, learning_rate 0.000155608
2017-10-02T22:36:51.089434: step 1097, loss 0.0479791, acc 0.984375, learning_rate 0.000155381
2017-10-02T22:36:52.245196: step 1098, loss 0.0266013, acc 1, learning_rate 0.000155155
2017-10-02T22:36:53.390479: step 1099, loss 0.0391234, acc 0.984375, learning_rate 0.000154929
2017-10-02T22:36:54.543395: step 1100, loss 0.0368221, acc 1, learning_rate 0.000154705
2017-10-02T22:36:55.683405: step 1101, loss 0.049838, acc 1, learning_rate 0.000154482
2017-10-02T22:36:56.831870: step 1102, loss 0.0295246, acc 1, learning_rate 0.00015426
2017-10-02T22:36:57.975938: step 1103, loss 0.0205507, acc 1, learning_rate 0.000154038
2017-10-02T22:36:59.122416: step 1104, loss 0.0326493, acc 1, learning_rate 0.000153818
2017-10-02T22:37:00.285397: step 1105, loss 0.0400021, acc 1, learning_rate 0.000153598
2017-10-02T22:37:01.436261: step 1106, loss 0.0223436, acc 1, learning_rate 0.000153379
2017-10-02T22:37:02.583634: step 1107, loss 0.0335555, acc 1, learning_rate 0.000153161
2017-10-02T22:37:03.731277: step 1108, loss 0.0273521, acc 1, learning_rate 0.000152944
2017-10-02T22:37:04.926743: step 1109, loss 0.0189317, acc 1, learning_rate 0.000152728
2017-10-02T22:37:06.062875: step 1110, loss 0.0267468, acc 1, learning_rate 0.000152513
2017-10-02T22:37:07.208703: step 1111, loss 0.0289686, acc 1, learning_rate 0.000152299
2017-10-02T22:37:08.366208: step 1112, loss 0.026167, acc 1, learning_rate 0.000152085
2017-10-02T22:37:09.529178: step 1113, loss 0.0358605, acc 1, learning_rate 0.000151872
2017-10-02T22:37:10.668490: step 1114, loss 0.0263805, acc 1, learning_rate 0.000151661
2017-10-02T22:37:11.816321: step 1115, loss 0.0246167, acc 1, learning_rate 0.00015145
2017-10-02T22:37:13.020813: step 1116, loss 0.0238866, acc 1, learning_rate 0.00015124
2017-10-02T22:37:14.164948: step 1117, loss 0.0306779, acc 1, learning_rate 0.000151031
2017-10-02T22:37:15.319135: step 1118, loss 0.0416462, acc 1, learning_rate 0.000150822
2017-10-02T22:37:16.466534: step 1119, loss 0.0377671, acc 1, learning_rate 0.000150615
2017-10-02T22:37:17.618614: step 1120, loss 0.0345115, acc 1, learning_rate 0.000150408

Evaluation:
2017-10-02T22:37:17.962570: step 1120, loss 1.4377, acc 0.476259

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1120

2017-10-02T22:37:25.673898: step 1121, loss 0.0242262, acc 1, learning_rate 0.000150203
2017-10-02T22:37:26.842734: step 1122, loss 0.0228121, acc 1, learning_rate 0.000149998
2017-10-02T22:37:27.989207: step 1123, loss 0.0652785, acc 0.984375, learning_rate 0.000149794
2017-10-02T22:37:29.141545: step 1124, loss 0.050922, acc 0.984375, learning_rate 0.00014959
2017-10-02T22:37:30.303789: step 1125, loss 0.0234083, acc 1, learning_rate 0.000149388
2017-10-02T22:37:31.450308: step 1126, loss 0.0555215, acc 1, learning_rate 0.000149186
2017-10-02T22:37:32.609520: step 1127, loss 0.0747481, acc 0.984375, learning_rate 0.000148986
2017-10-02T22:37:33.788097: step 1128, loss 0.0277298, acc 1, learning_rate 0.000148786
2017-10-02T22:37:34.953203: step 1129, loss 0.0322506, acc 1, learning_rate 0.000148587
2017-10-02T22:37:36.103154: step 1130, loss 0.0395316, acc 1, learning_rate 0.000148388
2017-10-02T22:37:37.274355: step 1131, loss 0.055767, acc 0.984375, learning_rate 0.000148191
2017-10-02T22:37:38.416561: step 1132, loss 0.0366151, acc 1, learning_rate 0.000147994
2017-10-02T22:37:39.566222: step 1133, loss 0.0294841, acc 1, learning_rate 0.000147798
2017-10-02T22:37:40.710428: step 1134, loss 0.0401271, acc 1, learning_rate 0.000147603
2017-10-02T22:37:41.863414: step 1135, loss 0.0267138, acc 1, learning_rate 0.000147409
2017-10-02T22:37:43.004260: step 1136, loss 0.0353877, acc 1, learning_rate 0.000147215
2017-10-02T22:37:44.157456: step 1137, loss 0.0235353, acc 1, learning_rate 0.000147022
2017-10-02T22:37:45.318266: step 1138, loss 0.0517641, acc 1, learning_rate 0.000146831
2017-10-02T22:37:46.472647: step 1139, loss 0.0492697, acc 1, learning_rate 0.000146639
2017-10-02T22:37:47.630515: step 1140, loss 0.0446307, acc 1, learning_rate 0.000146449
2017-10-02T22:37:48.781804: step 1141, loss 0.0195402, acc 1, learning_rate 0.000146259
2017-10-02T22:37:49.930845: step 1142, loss 0.0521757, acc 0.984375, learning_rate 0.000146071
2017-10-02T22:37:51.104685: step 1143, loss 0.0993798, acc 0.984375, learning_rate 0.000145883
2017-10-02T22:37:52.279277: step 1144, loss 0.0385103, acc 0.984375, learning_rate 0.000145695
2017-10-02T22:37:53.417052: step 1145, loss 0.0249633, acc 1, learning_rate 0.000145509
2017-10-02T22:37:54.570467: step 1146, loss 0.0386301, acc 1, learning_rate 0.000145323
2017-10-02T22:37:55.789713: step 1147, loss 0.103511, acc 0.96875, learning_rate 0.000145138
2017-10-02T22:37:56.945627: step 1148, loss 0.0261724, acc 1, learning_rate 0.000144954
2017-10-02T22:37:58.094518: step 1149, loss 0.0504387, acc 1, learning_rate 0.00014477
2017-10-02T22:37:59.243233: step 1150, loss 0.0203674, acc 1, learning_rate 0.000144588
2017-10-02T22:38:00.418437: step 1151, loss 0.0309948, acc 1, learning_rate 0.000144406
2017-10-02T22:38:01.572404: step 1152, loss 0.0318322, acc 1, learning_rate 0.000144224
2017-10-02T22:38:02.724909: step 1153, loss 0.0269174, acc 1, learning_rate 0.000144044
2017-10-02T22:38:03.873847: step 1154, loss 0.0335876, acc 1, learning_rate 0.000143864
2017-10-02T22:38:05.028380: step 1155, loss 0.0301467, acc 1, learning_rate 0.000143685
2017-10-02T22:38:06.170774: step 1156, loss 0.050938, acc 1, learning_rate 0.000143507
2017-10-02T22:38:07.339441: step 1157, loss 0.0635176, acc 0.96875, learning_rate 0.000143329
2017-10-02T22:38:08.483856: step 1158, loss 0.0605825, acc 0.984375, learning_rate 0.000143152
2017-10-02T22:38:09.632423: step 1159, loss 0.0274043, acc 1, learning_rate 0.000142976
2017-10-02T22:38:10.931718: step 1160, loss 0.0778256, acc 0.984375, learning_rate 0.000142801

Evaluation:
2017-10-02T22:38:11.250675: step 1160, loss 1.43753, acc 0.471942

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1160

2017-10-02T22:38:19.161108: step 1161, loss 0.0634112, acc 0.984375, learning_rate 0.000142626
2017-10-02T22:38:20.315999: step 1162, loss 0.0626151, acc 0.984375, learning_rate 0.000142452
2017-10-02T22:38:21.460823: step 1163, loss 0.0215633, acc 1, learning_rate 0.000142279
2017-10-02T22:38:22.616846: step 1164, loss 0.104405, acc 0.984375, learning_rate 0.000142106
2017-10-02T22:38:23.778428: step 1165, loss 0.0308325, acc 1, learning_rate 0.000141934
2017-10-02T22:38:24.921866: step 1166, loss 0.0189524, acc 1, learning_rate 0.000141763
2017-10-02T22:38:26.056744: step 1167, loss 0.0747753, acc 0.96875, learning_rate 0.000141593
2017-10-02T22:38:27.223064: step 1168, loss 0.0588566, acc 0.984375, learning_rate 0.000141423
2017-10-02T22:38:28.377473: step 1169, loss 0.0195482, acc 1, learning_rate 0.000141254
2017-10-02T22:38:29.527665: step 1170, loss 0.0441129, acc 0.984375, learning_rate 0.000141085
2017-10-02T22:38:30.675237: step 1171, loss 0.0336818, acc 1, learning_rate 0.000140918
2017-10-02T22:38:31.828728: step 1172, loss 0.0243674, acc 1, learning_rate 0.000140751
2017-10-02T22:38:32.984019: step 1173, loss 0.0325846, acc 1, learning_rate 0.000140584
2017-10-02T22:38:34.129291: step 1174, loss 0.0555108, acc 0.984375, learning_rate 0.000140419
2017-10-02T22:38:35.278146: step 1175, loss 0.0378245, acc 1, learning_rate 0.000140254
2017-10-02T22:38:36.411981: step 1176, loss 0.0481282, acc 0.980392, learning_rate 0.000140089
2017-10-02T22:38:37.575552: step 1177, loss 0.0316595, acc 1, learning_rate 0.000139926
2017-10-02T22:38:38.718353: step 1178, loss 0.0397362, acc 1, learning_rate 0.000139763
2017-10-02T22:38:39.860093: step 1179, loss 0.0236441, acc 1, learning_rate 0.0001396
2017-10-02T22:38:41.011423: step 1180, loss 0.029941, acc 1, learning_rate 0.000139439
2017-10-02T22:38:42.173156: step 1181, loss 0.0374099, acc 1, learning_rate 0.000139278
2017-10-02T22:38:43.333422: step 1182, loss 0.016472, acc 1, learning_rate 0.000139118
2017-10-02T22:38:44.493206: step 1183, loss 0.0595817, acc 0.984375, learning_rate 0.000138958
2017-10-02T22:38:45.646895: step 1184, loss 0.0222808, acc 1, learning_rate 0.000138799
2017-10-02T22:38:46.800177: step 1185, loss 0.0295496, acc 1, learning_rate 0.00013864
2017-10-02T22:38:47.944186: step 1186, loss 0.0471718, acc 0.984375, learning_rate 0.000138483
2017-10-02T22:38:49.093748: step 1187, loss 0.036102, acc 1, learning_rate 0.000138326
2017-10-02T22:38:50.244636: step 1188, loss 0.0491196, acc 0.984375, learning_rate 0.000138169
2017-10-02T22:38:51.412781: step 1189, loss 0.0590715, acc 0.984375, learning_rate 0.000138013
2017-10-02T22:38:52.563408: step 1190, loss 0.0319387, acc 1, learning_rate 0.000137858
2017-10-02T22:38:53.707394: step 1191, loss 0.0447698, acc 1, learning_rate 0.000137704
2017-10-02T22:38:54.871414: step 1192, loss 0.0226626, acc 1, learning_rate 0.00013755
2017-10-02T22:38:56.032861: step 1193, loss 0.0243017, acc 1, learning_rate 0.000137397
2017-10-02T22:38:57.174895: step 1194, loss 0.0403802, acc 1, learning_rate 0.000137244
2017-10-02T22:38:58.319917: step 1195, loss 0.0408033, acc 1, learning_rate 0.000137092
2017-10-02T22:38:59.461005: step 1196, loss 0.0351874, acc 1, learning_rate 0.000136941
2017-10-02T22:39:00.631735: step 1197, loss 0.0495113, acc 0.984375, learning_rate 0.00013679
2017-10-02T22:39:01.782540: step 1198, loss 0.0390117, acc 1, learning_rate 0.00013664
2017-10-02T22:39:03.161409: step 1199, loss 0.0272125, acc 1, learning_rate 0.00013649
2017-10-02T22:39:04.309933: step 1200, loss 0.0300137, acc 1, learning_rate 0.000136341

Evaluation:
2017-10-02T22:39:04.644850: step 1200, loss 1.44289, acc 0.47482

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1200

2017-10-02T22:39:12.284726: step 1201, loss 0.0398851, acc 1, learning_rate 0.000136193
2017-10-02T22:39:13.451209: step 1202, loss 0.0365655, acc 1, learning_rate 0.000136045
2017-10-02T22:39:14.594170: step 1203, loss 0.0243072, acc 1, learning_rate 0.000135898
2017-10-02T22:39:15.742871: step 1204, loss 0.0472146, acc 0.984375, learning_rate 0.000135751
2017-10-02T22:39:16.889852: step 1205, loss 0.0470404, acc 1, learning_rate 0.000135605
2017-10-02T22:39:18.031682: step 1206, loss 0.036703, acc 1, learning_rate 0.00013546
2017-10-02T22:39:19.180314: step 1207, loss 0.0217301, acc 1, learning_rate 0.000135315
2017-10-02T22:39:20.338319: step 1208, loss 0.0201278, acc 1, learning_rate 0.000135171
2017-10-02T22:39:21.495977: step 1209, loss 0.0299146, acc 1, learning_rate 0.000135028
2017-10-02T22:39:22.653234: step 1210, loss 0.026604, acc 1, learning_rate 0.000134885
2017-10-02T22:39:23.804465: step 1211, loss 0.0341794, acc 1, learning_rate 0.000134742
2017-10-02T22:39:24.952861: step 1212, loss 0.02573, acc 1, learning_rate 0.0001346
2017-10-02T22:39:26.110658: step 1213, loss 0.050575, acc 0.984375, learning_rate 0.000134459
2017-10-02T22:39:27.286336: step 1214, loss 0.0685529, acc 0.984375, learning_rate 0.000134319
2017-10-02T22:39:28.436375: step 1215, loss 0.0491316, acc 0.984375, learning_rate 0.000134178
2017-10-02T22:39:29.594705: step 1216, loss 0.0312503, acc 1, learning_rate 0.000134039
2017-10-02T22:39:30.749401: step 1217, loss 0.0303022, acc 1, learning_rate 0.0001339
2017-10-02T22:39:31.904071: step 1218, loss 0.0491122, acc 0.984375, learning_rate 0.000133762
2017-10-02T22:39:33.055003: step 1219, loss 0.0395795, acc 1, learning_rate 0.000133624
2017-10-02T22:39:34.207322: step 1220, loss 0.0292531, acc 1, learning_rate 0.000133487
2017-10-02T22:39:35.366704: step 1221, loss 0.0592204, acc 0.984375, learning_rate 0.00013335
2017-10-02T22:39:36.529592: step 1222, loss 0.025047, acc 1, learning_rate 0.000133214
2017-10-02T22:39:37.680350: step 1223, loss 0.0322929, acc 1, learning_rate 0.000133078
2017-10-02T22:39:38.890496: step 1224, loss 0.0416869, acc 1, learning_rate 0.000132943
2017-10-02T22:39:40.033383: step 1225, loss 0.045132, acc 1, learning_rate 0.000132809
2017-10-02T22:39:41.186944: step 1226, loss 0.0746241, acc 0.984375, learning_rate 0.000132675
2017-10-02T22:39:42.340278: step 1227, loss 0.0719709, acc 0.96875, learning_rate 0.000132541
2017-10-02T22:39:43.501822: step 1228, loss 0.0525782, acc 0.984375, learning_rate 0.000132409
2017-10-02T22:39:44.644838: step 1229, loss 0.0482187, acc 0.984375, learning_rate 0.000132276
2017-10-02T22:39:45.799415: step 1230, loss 0.0279183, acc 1, learning_rate 0.000132145
2017-10-02T22:39:46.970226: step 1231, loss 0.0265616, acc 1, learning_rate 0.000132013
2017-10-02T22:39:48.113586: step 1232, loss 0.069842, acc 0.984375, learning_rate 0.000131883
2017-10-02T22:39:49.258194: step 1233, loss 0.0698757, acc 0.984375, learning_rate 0.000131753
2017-10-02T22:39:50.413450: step 1234, loss 0.0390518, acc 1, learning_rate 0.000131623
2017-10-02T22:39:51.562799: step 1235, loss 0.0384771, acc 1, learning_rate 0.000131494
2017-10-02T22:39:52.705118: step 1236, loss 0.0303102, acc 1, learning_rate 0.000131365
2017-10-02T22:39:53.919262: step 1237, loss 0.0248688, acc 1, learning_rate 0.000131237
2017-10-02T22:39:55.082445: step 1238, loss 0.0440972, acc 1, learning_rate 0.00013111
2017-10-02T22:39:56.252815: step 1239, loss 0.0286874, acc 1, learning_rate 0.000130983
2017-10-02T22:39:57.413961: step 1240, loss 0.0200555, acc 1, learning_rate 0.000130856

Evaluation:
2017-10-02T22:39:57.735532: step 1240, loss 1.45132, acc 0.440288

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1240

2017-10-02T22:40:05.434112: step 1241, loss 0.0417003, acc 0.984375, learning_rate 0.00013073
2017-10-02T22:40:06.600900: step 1242, loss 0.0281146, acc 1, learning_rate 0.000130605
2017-10-02T22:40:07.748356: step 1243, loss 0.0403651, acc 1, learning_rate 0.00013048
2017-10-02T22:40:08.890585: step 1244, loss 0.0542647, acc 1, learning_rate 0.000130356
2017-10-02T22:40:10.045928: step 1245, loss 0.0300333, acc 1, learning_rate 0.000130232
2017-10-02T22:40:11.191634: step 1246, loss 0.0338088, acc 1, learning_rate 0.000130108
2017-10-02T22:40:12.357896: step 1247, loss 0.0318967, acc 1, learning_rate 0.000129985
2017-10-02T22:40:13.519871: step 1248, loss 0.0374755, acc 1, learning_rate 0.000129863
2017-10-02T22:40:14.674974: step 1249, loss 0.0551567, acc 1, learning_rate 0.000129741
2017-10-02T22:40:15.822860: step 1250, loss 0.0257939, acc 1, learning_rate 0.00012962
2017-10-02T22:40:16.974900: step 1251, loss 0.0249027, acc 1, learning_rate 0.000129499
2017-10-02T22:40:18.132220: step 1252, loss 0.0285418, acc 1, learning_rate 0.000129378
2017-10-02T22:40:19.301499: step 1253, loss 0.0390244, acc 0.984375, learning_rate 0.000129259
2017-10-02T22:40:20.469604: step 1254, loss 0.0403142, acc 1, learning_rate 0.000129139
2017-10-02T22:40:21.644314: step 1255, loss 0.0357024, acc 1, learning_rate 0.00012902
2017-10-02T22:40:22.790272: step 1256, loss 0.026976, acc 1, learning_rate 0.000128902
2017-10-02T22:40:23.937906: step 1257, loss 0.0183646, acc 1, learning_rate 0.000128784
2017-10-02T22:40:25.092778: step 1258, loss 0.0272785, acc 1, learning_rate 0.000128666
2017-10-02T22:40:26.235344: step 1259, loss 0.0334338, acc 1, learning_rate 0.000128549
2017-10-02T22:40:27.381843: step 1260, loss 0.0425008, acc 1, learning_rate 0.000128433
2017-10-02T22:40:28.526676: step 1261, loss 0.0554964, acc 0.984375, learning_rate 0.000128317
2017-10-02T22:40:29.676441: step 1262, loss 0.0241227, acc 1, learning_rate 0.000128201
2017-10-02T22:40:30.821979: step 1263, loss 0.0291826, acc 1, learning_rate 0.000128086
2017-10-02T22:40:31.964520: step 1264, loss 0.0315743, acc 1, learning_rate 0.000127971
2017-10-02T22:40:33.128021: step 1265, loss 0.0309191, acc 1, learning_rate 0.000127857
2017-10-02T22:40:34.276248: step 1266, loss 0.0215945, acc 1, learning_rate 0.000127743
2017-10-02T22:40:35.427621: step 1267, loss 0.0222832, acc 1, learning_rate 0.00012763
2017-10-02T22:40:36.584270: step 1268, loss 0.0695771, acc 0.984375, learning_rate 0.000127517
2017-10-02T22:40:37.727491: step 1269, loss 0.122246, acc 0.9375, learning_rate 0.000127405
2017-10-02T22:40:38.934358: step 1270, loss 0.0332489, acc 1, learning_rate 0.000127293
2017-10-02T22:40:40.087144: step 1271, loss 0.0352654, acc 1, learning_rate 0.000127182
2017-10-02T22:40:41.254802: step 1272, loss 0.0221404, acc 1, learning_rate 0.000127071
2017-10-02T22:40:42.408460: step 1273, loss 0.0227999, acc 1, learning_rate 0.00012696
2017-10-02T22:40:43.536202: step 1274, loss 0.0182055, acc 1, learning_rate 0.00012685
2017-10-02T22:40:44.691824: step 1275, loss 0.0200842, acc 1, learning_rate 0.000126741
2017-10-02T22:40:45.842836: step 1276, loss 0.0561263, acc 0.984375, learning_rate 0.000126632
2017-10-02T22:40:47.221312: step 1277, loss 0.0274717, acc 1, learning_rate 0.000126523
2017-10-02T22:40:48.367413: step 1278, loss 0.0345919, acc 1, learning_rate 0.000126415
2017-10-02T22:40:49.524012: step 1279, loss 0.0214358, acc 1, learning_rate 0.000126307
2017-10-02T22:40:50.687392: step 1280, loss 0.0393058, acc 1, learning_rate 0.000126199

Evaluation:
2017-10-02T22:40:51.013568: step 1280, loss 1.43334, acc 0.486331

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1280

2017-10-02T22:40:58.616571: step 1281, loss 0.0334665, acc 1, learning_rate 0.000126093
2017-10-02T22:40:59.843862: step 1282, loss 0.0287447, acc 1, learning_rate 0.000125986
2017-10-02T22:41:00.988754: step 1283, loss 0.0245598, acc 1, learning_rate 0.00012588
2017-10-02T22:41:02.137030: step 1284, loss 0.0132227, acc 1, learning_rate 0.000125774
2017-10-02T22:41:03.294980: step 1285, loss 0.0410249, acc 1, learning_rate 0.000125669
2017-10-02T22:41:04.444043: step 1286, loss 0.0788975, acc 0.96875, learning_rate 0.000125564
2017-10-02T22:41:05.599000: step 1287, loss 0.0422989, acc 1, learning_rate 0.00012546
2017-10-02T22:41:06.746171: step 1288, loss 0.0206618, acc 1, learning_rate 0.000125356
2017-10-02T22:41:07.897783: step 1289, loss 0.0266416, acc 1, learning_rate 0.000125253
2017-10-02T22:41:09.046349: step 1290, loss 0.0267451, acc 1, learning_rate 0.00012515
2017-10-02T22:41:10.202159: step 1291, loss 0.0540746, acc 0.984375, learning_rate 0.000125047
2017-10-02T22:41:11.349503: step 1292, loss 0.0344433, acc 1, learning_rate 0.000124945
2017-10-02T22:41:12.512363: step 1293, loss 0.0253942, acc 1, learning_rate 0.000124843
2017-10-02T22:41:13.676904: step 1294, loss 0.103964, acc 0.953125, learning_rate 0.000124741
2017-10-02T22:41:14.830303: step 1295, loss 0.0192085, acc 1, learning_rate 0.00012464
2017-10-02T22:41:15.980491: step 1296, loss 0.0411301, acc 0.984375, learning_rate 0.00012454
2017-10-02T22:41:17.126699: step 1297, loss 0.0337276, acc 1, learning_rate 0.00012444
2017-10-02T22:41:18.286447: step 1298, loss 0.0704032, acc 0.984375, learning_rate 0.00012434
2017-10-02T22:41:19.448269: step 1299, loss 0.0315941, acc 1, learning_rate 0.000124241
2017-10-02T22:41:20.601163: step 1300, loss 0.0259411, acc 1, learning_rate 0.000124142
2017-10-02T22:41:21.763084: step 1301, loss 0.0254826, acc 1, learning_rate 0.000124043
2017-10-02T22:41:22.954236: step 1302, loss 0.0583169, acc 0.96875, learning_rate 0.000123945
2017-10-02T22:41:24.096809: step 1303, loss 0.0371844, acc 1, learning_rate 0.000123847
2017-10-02T22:41:25.236372: step 1304, loss 0.0446337, acc 1, learning_rate 0.00012375
2017-10-02T22:41:26.383100: step 1305, loss 0.0742937, acc 0.96875, learning_rate 0.000123653
2017-10-02T22:41:27.534967: step 1306, loss 0.0248252, acc 1, learning_rate 0.000123556
2017-10-02T22:41:28.682401: step 1307, loss 0.0255102, acc 1, learning_rate 0.00012346
2017-10-02T22:41:29.824803: step 1308, loss 0.024195, acc 1, learning_rate 0.000123364
2017-10-02T22:41:30.962261: step 1309, loss 0.0201587, acc 1, learning_rate 0.000123269
2017-10-02T22:41:32.108985: step 1310, loss 0.021514, acc 1, learning_rate 0.000123174
2017-10-02T22:41:33.257388: step 1311, loss 0.0323405, acc 1, learning_rate 0.00012308
2017-10-02T22:41:34.409858: step 1312, loss 0.023766, acc 1, learning_rate 0.000122985
2017-10-02T22:41:35.580570: step 1313, loss 0.0238678, acc 1, learning_rate 0.000122892
2017-10-02T22:41:36.747445: step 1314, loss 0.0500697, acc 0.984375, learning_rate 0.000122798
2017-10-02T22:41:37.899054: step 1315, loss 0.0460655, acc 0.984375, learning_rate 0.000122705
2017-10-02T22:41:39.042101: step 1316, loss 0.0652536, acc 0.984375, learning_rate 0.000122612
2017-10-02T22:41:40.200111: step 1317, loss 0.0291636, acc 1, learning_rate 0.00012252
2017-10-02T22:41:41.380149: step 1318, loss 0.0393574, acc 0.984375, learning_rate 0.000122428
2017-10-02T22:41:42.545638: step 1319, loss 0.0283831, acc 1, learning_rate 0.000122337
2017-10-02T22:41:43.721416: step 1320, loss 0.0257454, acc 1, learning_rate 0.000122245

Evaluation:
2017-10-02T22:41:44.051942: step 1320, loss 1.43988, acc 0.466187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1320

2017-10-02T22:41:51.749739: step 1321, loss 0.0301736, acc 1, learning_rate 0.000122155
2017-10-02T22:41:52.916862: step 1322, loss 0.0210793, acc 1, learning_rate 0.000122064
2017-10-02T22:41:54.069005: step 1323, loss 0.0312965, acc 1, learning_rate 0.000121974
2017-10-02T22:41:55.208431: step 1324, loss 0.0279273, acc 1, learning_rate 0.000121884
2017-10-02T22:41:56.358291: step 1325, loss 0.0272129, acc 1, learning_rate 0.000121795
2017-10-02T22:41:57.520695: step 1326, loss 0.0246717, acc 1, learning_rate 0.000121706
2017-10-02T22:41:58.793323: step 1327, loss 0.0432758, acc 1, learning_rate 0.000121618
2017-10-02T22:41:59.941926: step 1328, loss 0.0474601, acc 0.984375, learning_rate 0.000121529
2017-10-02T22:42:01.081897: step 1329, loss 0.059361, acc 1, learning_rate 0.000121441
2017-10-02T22:42:02.242414: step 1330, loss 0.030351, acc 1, learning_rate 0.000121354
2017-10-02T22:42:03.397166: step 1331, loss 0.0243358, acc 1, learning_rate 0.000121267
2017-10-02T22:42:04.542114: step 1332, loss 0.0442459, acc 1, learning_rate 0.00012118
2017-10-02T22:42:05.711564: step 1333, loss 0.0304636, acc 1, learning_rate 0.000121093
2017-10-02T22:42:06.861560: step 1334, loss 0.0268059, acc 1, learning_rate 0.000121007
2017-10-02T22:42:08.104832: step 1335, loss 0.0490452, acc 1, learning_rate 0.000120922
2017-10-02T22:42:09.247679: step 1336, loss 0.0356247, acc 1, learning_rate 0.000120836
2017-10-02T22:42:10.396750: step 1337, loss 0.0598933, acc 0.984375, learning_rate 0.000120751
2017-10-02T22:42:11.546057: step 1338, loss 0.0339772, acc 1, learning_rate 0.000120666
2017-10-02T22:42:12.687689: step 1339, loss 0.0584195, acc 0.984375, learning_rate 0.000120582
2017-10-02T22:42:13.856421: step 1340, loss 0.0135269, acc 1, learning_rate 0.000120498
2017-10-02T22:42:15.006582: step 1341, loss 0.0716729, acc 0.984375, learning_rate 0.000120414
2017-10-02T22:42:16.154707: step 1342, loss 0.077495, acc 0.984375, learning_rate 0.000120331
2017-10-02T22:42:17.300943: step 1343, loss 0.0227755, acc 1, learning_rate 0.000120248
2017-10-02T22:42:18.454994: step 1344, loss 0.0207687, acc 1, learning_rate 0.000120165
2017-10-02T22:42:19.600820: step 1345, loss 0.0183297, acc 1, learning_rate 0.000120083
2017-10-02T22:42:20.788647: step 1346, loss 0.0222921, acc 1, learning_rate 0.000120001
2017-10-02T22:42:21.927515: step 1347, loss 0.0520617, acc 0.984375, learning_rate 0.00011992
2017-10-02T22:42:23.080234: step 1348, loss 0.0207549, acc 1, learning_rate 0.000119838
2017-10-02T22:42:24.236354: step 1349, loss 0.0272147, acc 1, learning_rate 0.000119757
2017-10-02T22:42:25.389528: step 1350, loss 0.0458398, acc 1, learning_rate 0.000119677
2017-10-02T22:42:26.536790: step 1351, loss 0.0304765, acc 1, learning_rate 0.000119596
2017-10-02T22:42:27.687365: step 1352, loss 0.0338518, acc 0.984375, learning_rate 0.000119516
2017-10-02T22:42:28.902512: step 1353, loss 0.0311645, acc 1, learning_rate 0.000119437
2017-10-02T22:42:30.058780: step 1354, loss 0.02563, acc 1, learning_rate 0.000119357
2017-10-02T22:42:31.208697: step 1355, loss 0.0231313, acc 1, learning_rate 0.000119278
2017-10-02T22:42:32.352384: step 1356, loss 0.0275311, acc 1, learning_rate 0.0001192
2017-10-02T22:42:33.511323: step 1357, loss 0.027208, acc 1, learning_rate 0.000119121
2017-10-02T22:42:34.646135: step 1358, loss 0.0232405, acc 1, learning_rate 0.000119043
2017-10-02T22:42:35.796044: step 1359, loss 0.0174886, acc 1, learning_rate 0.000118965
2017-10-02T22:42:36.944669: step 1360, loss 0.0532322, acc 0.984375, learning_rate 0.000118888

Evaluation:
2017-10-02T22:42:37.275610: step 1360, loss 1.43662, acc 0.480576

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1360

2017-10-02T22:42:43.960768: step 1361, loss 0.0350231, acc 1, learning_rate 0.000118811
2017-10-02T22:42:45.112577: step 1362, loss 0.0482058, acc 0.984375, learning_rate 0.000118734
2017-10-02T22:42:46.271080: step 1363, loss 0.0280403, acc 1, learning_rate 0.000118658
2017-10-02T22:42:47.437247: step 1364, loss 0.0898335, acc 0.984375, learning_rate 0.000118582
2017-10-02T22:42:48.602801: step 1365, loss 0.0399869, acc 1, learning_rate 0.000118506
2017-10-02T22:42:49.758344: step 1366, loss 0.0287292, acc 1, learning_rate 0.00011843
2017-10-02T22:42:50.908022: step 1367, loss 0.0457247, acc 1, learning_rate 0.000118355
2017-10-02T22:42:52.056340: step 1368, loss 0.0167291, acc 1, learning_rate 0.00011828
2017-10-02T22:42:53.203371: step 1369, loss 0.0895073, acc 0.96875, learning_rate 0.000118205
2017-10-02T22:42:54.361941: step 1370, loss 0.0285723, acc 1, learning_rate 0.000118131
2017-10-02T22:42:55.507337: step 1371, loss 0.0230944, acc 1, learning_rate 0.000118057
2017-10-02T22:42:56.642249: step 1372, loss 0.0204319, acc 1, learning_rate 0.000117983
2017-10-02T22:42:57.773545: step 1373, loss 0.0249404, acc 1, learning_rate 0.00011791
2017-10-02T22:42:59.157017: step 1374, loss 0.0248668, acc 1, learning_rate 0.000117837
2017-10-02T22:43:00.307450: step 1375, loss 0.0238684, acc 1, learning_rate 0.000117764
2017-10-02T22:43:01.466296: step 1376, loss 0.0515056, acc 0.984375, learning_rate 0.000117692
2017-10-02T22:43:02.615473: step 1377, loss 0.0236294, acc 1, learning_rate 0.000117619
2017-10-02T22:43:03.779124: step 1378, loss 0.0277086, acc 1, learning_rate 0.000117547
2017-10-02T22:43:04.932035: step 1379, loss 0.0259938, acc 1, learning_rate 0.000117476
2017-10-02T22:43:06.080048: step 1380, loss 0.031143, acc 1, learning_rate 0.000117404
2017-10-02T22:43:07.223419: step 1381, loss 0.0243753, acc 1, learning_rate 0.000117333
2017-10-02T22:43:08.379759: step 1382, loss 0.0391472, acc 0.984375, learning_rate 0.000117263
2017-10-02T22:43:09.532206: step 1383, loss 0.0371519, acc 1, learning_rate 0.000117192
2017-10-02T22:43:10.682114: step 1384, loss 0.0214242, acc 1, learning_rate 0.000117122
2017-10-02T22:43:11.836350: step 1385, loss 0.0427523, acc 0.984375, learning_rate 0.000117052
2017-10-02T22:43:12.982790: step 1386, loss 0.0241129, acc 1, learning_rate 0.000116983
2017-10-02T22:43:14.136511: step 1387, loss 0.0370254, acc 1, learning_rate 0.000116913
2017-10-02T22:43:15.301396: step 1388, loss 0.0480539, acc 0.984375, learning_rate 0.000116844
2017-10-02T22:43:16.450498: step 1389, loss 0.0388321, acc 1, learning_rate 0.000116775
2017-10-02T22:43:17.634153: step 1390, loss 0.0291458, acc 1, learning_rate 0.000116707
2017-10-02T22:43:19.013394: step 1391, loss 0.0392965, acc 0.984375, learning_rate 0.000116639
2017-10-02T22:43:20.150656: step 1392, loss 0.0338762, acc 0.984375, learning_rate 0.000116571
2017-10-02T22:43:21.310544: step 1393, loss 0.0908488, acc 0.984375, learning_rate 0.000116503
2017-10-02T22:43:22.464416: step 1394, loss 0.0287117, acc 1, learning_rate 0.000116436
2017-10-02T22:43:23.608771: step 1395, loss 0.0268906, acc 1, learning_rate 0.000116369
2017-10-02T22:43:24.752011: step 1396, loss 0.022926, acc 1, learning_rate 0.000116302
2017-10-02T22:43:25.908541: step 1397, loss 0.0190218, acc 1, learning_rate 0.000116235
2017-10-02T22:43:27.063029: step 1398, loss 0.0209512, acc 1, learning_rate 0.000116169
2017-10-02T22:43:28.220388: step 1399, loss 0.0312986, acc 1, learning_rate 0.000116103
2017-10-02T22:43:29.378764: step 1400, loss 0.0564836, acc 0.984375, learning_rate 0.000116037

Evaluation:
2017-10-02T22:43:29.712743: step 1400, loss 1.43444, acc 0.490647

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1400

2017-10-02T22:43:37.564130: step 1401, loss 0.0319939, acc 1, learning_rate 0.000115972
2017-10-02T22:43:38.917320: step 1402, loss 0.0386544, acc 1, learning_rate 0.000115907
2017-10-02T22:43:40.082708: step 1403, loss 0.0410266, acc 0.984375, learning_rate 0.000115842
2017-10-02T22:43:41.229749: step 1404, loss 0.0201444, acc 1, learning_rate 0.000115777
2017-10-02T22:43:42.378249: step 1405, loss 0.0225655, acc 1, learning_rate 0.000115713
2017-10-02T22:43:43.539093: step 1406, loss 0.0595746, acc 0.984375, learning_rate 0.000115649
2017-10-02T22:43:44.716753: step 1407, loss 0.0218814, acc 1, learning_rate 0.000115585
2017-10-02T22:43:45.885702: step 1408, loss 0.0416602, acc 1, learning_rate 0.000115521
2017-10-02T22:43:47.043800: step 1409, loss 0.0297888, acc 1, learning_rate 0.000115458
2017-10-02T22:43:48.186487: step 1410, loss 0.0399535, acc 1, learning_rate 0.000115395
2017-10-02T22:43:49.339119: step 1411, loss 0.0647417, acc 0.984375, learning_rate 0.000115332
2017-10-02T22:43:50.502362: step 1412, loss 0.0216462, acc 1, learning_rate 0.000115269
2017-10-02T22:43:51.645396: step 1413, loss 0.0250625, acc 1, learning_rate 0.000115207
2017-10-02T22:43:52.798935: step 1414, loss 0.0205008, acc 1, learning_rate 0.000115145
2017-10-02T22:43:53.951740: step 1415, loss 0.0409483, acc 1, learning_rate 0.000115083
2017-10-02T22:43:55.100346: step 1416, loss 0.0407897, acc 1, learning_rate 0.000115022
2017-10-02T22:43:56.247153: step 1417, loss 0.0285959, acc 1, learning_rate 0.00011496
2017-10-02T22:43:57.400329: step 1418, loss 0.0242764, acc 1, learning_rate 0.000114899
2017-10-02T22:43:58.559657: step 1419, loss 0.0234324, acc 1, learning_rate 0.000114838
2017-10-02T22:43:59.706532: step 1420, loss 0.0261039, acc 1, learning_rate 0.000114778
2017-10-02T22:44:00.850748: step 1421, loss 0.0346808, acc 1, learning_rate 0.000114717
2017-10-02T22:44:02.015859: step 1422, loss 0.027801, acc 1, learning_rate 0.000114657
2017-10-02T22:44:03.165664: step 1423, loss 0.0561703, acc 0.96875, learning_rate 0.000114598
2017-10-02T22:44:04.300581: step 1424, loss 0.0533265, acc 0.984375, learning_rate 0.000114538
2017-10-02T22:44:05.457005: step 1425, loss 0.0291201, acc 1, learning_rate 0.000114479
2017-10-02T22:44:06.726775: step 1426, loss 0.0424138, acc 0.984375, learning_rate 0.00011442
2017-10-02T22:44:07.886274: step 1427, loss 0.0272048, acc 1, learning_rate 0.000114361
2017-10-02T22:44:09.026907: step 1428, loss 0.0258386, acc 1, learning_rate 0.000114302
2017-10-02T22:44:10.167697: step 1429, loss 0.0246584, acc 1, learning_rate 0.000114244
2017-10-02T22:44:11.326179: step 1430, loss 0.0335231, acc 1, learning_rate 0.000114186
2017-10-02T22:44:12.519715: step 1431, loss 0.0283323, acc 1, learning_rate 0.000114128
2017-10-02T22:44:13.681051: step 1432, loss 0.043207, acc 0.984375, learning_rate 0.00011407
2017-10-02T22:44:15.066624: step 1433, loss 0.0375578, acc 1, learning_rate 0.000114013
2017-10-02T22:44:16.226079: step 1434, loss 0.0265921, acc 1, learning_rate 0.000113955
2017-10-02T22:44:17.371827: step 1435, loss 0.0334047, acc 1, learning_rate 0.000113898
2017-10-02T22:44:18.610242: step 1436, loss 0.0357609, acc 1, learning_rate 0.000113842
2017-10-02T22:44:19.771882: step 1437, loss 0.0160645, acc 1, learning_rate 0.000113785
2017-10-02T22:44:20.912180: step 1438, loss 0.0264705, acc 0.984375, learning_rate 0.000113729
2017-10-02T22:44:22.060107: step 1439, loss 0.0297449, acc 1, learning_rate 0.000113673
2017-10-02T22:44:23.204602: step 1440, loss 0.0419927, acc 0.984375, learning_rate 0.000113617

Evaluation:
2017-10-02T22:44:23.535896: step 1440, loss 1.45905, acc 0.446043

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1440

2017-10-02T22:44:30.482864: step 1441, loss 0.0269732, acc 1, learning_rate 0.000113561
2017-10-02T22:44:31.641431: step 1442, loss 0.0218778, acc 1, learning_rate 0.000113506
2017-10-02T22:44:32.781479: step 1443, loss 0.0288793, acc 1, learning_rate 0.000113451
2017-10-02T22:44:33.924865: step 1444, loss 0.0269461, acc 1, learning_rate 0.000113396
2017-10-02T22:44:35.070381: step 1445, loss 0.0199008, acc 1, learning_rate 0.000113341
2017-10-02T22:44:36.226822: step 1446, loss 0.0248743, acc 1, learning_rate 0.000113287
2017-10-02T22:44:37.380630: step 1447, loss 0.034867, acc 1, learning_rate 0.000113233
2017-10-02T22:44:38.526220: step 1448, loss 0.0396554, acc 0.984375, learning_rate 0.000113179
2017-10-02T22:44:39.674124: step 1449, loss 0.0557131, acc 0.984375, learning_rate 0.000113125
2017-10-02T22:44:40.819154: step 1450, loss 0.0156388, acc 1, learning_rate 0.000113071
2017-10-02T22:44:41.968310: step 1451, loss 0.022362, acc 1, learning_rate 0.000113018
2017-10-02T22:44:43.119287: step 1452, loss 0.0451341, acc 1, learning_rate 0.000112965
2017-10-02T22:44:44.279727: step 1453, loss 0.0375698, acc 1, learning_rate 0.000112912
2017-10-02T22:44:45.432123: step 1454, loss 0.0519248, acc 0.984375, learning_rate 0.000112859
2017-10-02T22:44:46.600439: step 1455, loss 0.0458239, acc 1, learning_rate 0.000112807
2017-10-02T22:44:47.756402: step 1456, loss 0.0342032, acc 1, learning_rate 0.000112754
2017-10-02T22:44:49.025301: step 1457, loss 0.0110623, acc 1, learning_rate 0.000112702
2017-10-02T22:44:50.192966: step 1458, loss 0.0213886, acc 1, learning_rate 0.000112651
2017-10-02T22:44:51.345538: step 1459, loss 0.0427473, acc 0.984375, learning_rate 0.000112599
2017-10-02T22:44:52.501066: step 1460, loss 0.0355864, acc 1, learning_rate 0.000112547
2017-10-02T22:44:53.649306: step 1461, loss 0.0239929, acc 1, learning_rate 0.000112496
2017-10-02T22:44:54.804134: step 1462, loss 0.0621724, acc 0.984375, learning_rate 0.000112445
2017-10-02T22:44:55.956945: step 1463, loss 0.0213667, acc 1, learning_rate 0.000112394
2017-10-02T22:44:57.111599: step 1464, loss 0.0331763, acc 1, learning_rate 0.000112344
2017-10-02T22:44:58.263241: step 1465, loss 0.0421832, acc 0.984375, learning_rate 0.000112293
2017-10-02T22:44:59.422142: step 1466, loss 0.0287458, acc 1, learning_rate 0.000112243
2017-10-02T22:45:00.576854: step 1467, loss 0.029728, acc 1, learning_rate 0.000112193
2017-10-02T22:45:01.739338: step 1468, loss 0.0396276, acc 0.984375, learning_rate 0.000112144
2017-10-02T22:45:02.897510: step 1469, loss 0.0246925, acc 1, learning_rate 0.000112094
2017-10-02T22:45:04.035202: step 1470, loss 0.0191639, acc 1, learning_rate 0.000112045
2017-10-02T22:45:05.182591: step 1471, loss 0.0366184, acc 1, learning_rate 0.000111995
2017-10-02T22:45:06.333244: step 1472, loss 0.117898, acc 0.953125, learning_rate 0.000111946
2017-10-02T22:45:07.492509: step 1473, loss 0.0316796, acc 1, learning_rate 0.000111898
2017-10-02T22:45:08.642256: step 1474, loss 0.0561883, acc 0.984375, learning_rate 0.000111849
2017-10-02T22:45:09.801114: step 1475, loss 0.0346295, acc 1, learning_rate 0.000111801
2017-10-02T22:45:11.015167: step 1476, loss 0.0234607, acc 1, learning_rate 0.000111753
2017-10-02T22:45:12.171624: step 1477, loss 0.0198112, acc 1, learning_rate 0.000111705
2017-10-02T22:45:13.352100: step 1478, loss 0.0233844, acc 1, learning_rate 0.000111657
2017-10-02T22:45:14.546056: step 1479, loss 0.0353324, acc 1, learning_rate 0.000111609
2017-10-02T22:45:15.725070: step 1480, loss 0.0215106, acc 1, learning_rate 0.000111562

Evaluation:
2017-10-02T22:45:16.067599: step 1480, loss 1.44615, acc 0.45036

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1480

2017-10-02T22:45:23.645003: step 1481, loss 0.0264465, acc 1, learning_rate 0.000111515
2017-10-02T22:45:24.810005: step 1482, loss 0.0369866, acc 1, learning_rate 0.000111468
2017-10-02T22:45:25.959539: step 1483, loss 0.0257906, acc 1, learning_rate 0.000111421
2017-10-02T22:45:27.112094: step 1484, loss 0.0206607, acc 1, learning_rate 0.000111374
2017-10-02T22:45:28.258988: step 1485, loss 0.0258424, acc 1, learning_rate 0.000111328
2017-10-02T22:45:29.413167: step 1486, loss 0.0209256, acc 1, learning_rate 0.000111282
2017-10-02T22:45:30.558430: step 1487, loss 0.0255457, acc 1, learning_rate 0.000111236
2017-10-02T22:45:31.724633: step 1488, loss 0.0475486, acc 0.984375, learning_rate 0.00011119
2017-10-02T22:45:32.874338: step 1489, loss 0.0347367, acc 1, learning_rate 0.000111144
2017-10-02T22:45:34.030520: step 1490, loss 0.0454784, acc 0.984375, learning_rate 0.000111099
2017-10-02T22:45:35.175000: step 1491, loss 0.0267716, acc 1, learning_rate 0.000111053
2017-10-02T22:45:36.335210: step 1492, loss 0.030833, acc 1, learning_rate 0.000111008
2017-10-02T22:45:37.399383: step 1493, loss 0.0255563, acc 1, learning_rate 0.000110963
2017-10-02T22:45:38.642343: step 1494, loss 0.0158737, acc 1, learning_rate 0.000110918
2017-10-02T22:45:39.794617: step 1495, loss 0.0239483, acc 1, learning_rate 0.000110874
2017-10-02T22:45:40.949413: step 1496, loss 0.0266773, acc 1, learning_rate 0.00011083
2017-10-02T22:45:42.108070: step 1497, loss 0.0500116, acc 0.984375, learning_rate 0.000110785
2017-10-02T22:45:43.244161: step 1498, loss 0.0266803, acc 1, learning_rate 0.000110741
2017-10-02T22:45:44.383743: step 1499, loss 0.0555709, acc 0.96875, learning_rate 0.000110697
2017-10-02T22:45:45.535107: step 1500, loss 0.0223342, acc 1, learning_rate 0.000110654
2017-10-02T22:45:46.688605: step 1501, loss 0.0586779, acc 0.984375, learning_rate 0.00011061
2017-10-02T22:45:47.844944: step 1502, loss 0.0173714, acc 1, learning_rate 0.000110567
2017-10-02T22:45:48.995531: step 1503, loss 0.0446834, acc 0.984375, learning_rate 0.000110524
2017-10-02T22:45:50.156673: step 1504, loss 0.0350477, acc 1, learning_rate 0.000110481
2017-10-02T22:45:51.308027: step 1505, loss 0.0229314, acc 1, learning_rate 0.000110438
2017-10-02T22:45:52.473906: step 1506, loss 0.0272357, acc 1, learning_rate 0.000110396
2017-10-02T22:45:53.624716: step 1507, loss 0.0436102, acc 0.984375, learning_rate 0.000110353
2017-10-02T22:45:54.777570: step 1508, loss 0.0186707, acc 1, learning_rate 0.000110311
2017-10-02T22:45:55.933488: step 1509, loss 0.0294726, acc 1, learning_rate 0.000110269
2017-10-02T22:45:57.084002: step 1510, loss 0.0299771, acc 1, learning_rate 0.000110227
2017-10-02T22:45:58.233317: step 1511, loss 0.0381729, acc 0.984375, learning_rate 0.000110185
2017-10-02T22:45:59.391556: step 1512, loss 0.0259705, acc 1, learning_rate 0.000110144
2017-10-02T22:46:00.559847: step 1513, loss 0.0393122, acc 0.984375, learning_rate 0.000110102
2017-10-02T22:46:01.709642: step 1514, loss 0.0265904, acc 1, learning_rate 0.000110061
2017-10-02T22:46:02.858907: step 1515, loss 0.0281149, acc 0.984375, learning_rate 0.00011002
2017-10-02T22:46:03.997840: step 1516, loss 0.0182838, acc 1, learning_rate 0.000109979
2017-10-02T22:46:05.146769: step 1517, loss 0.0269551, acc 1, learning_rate 0.000109938
2017-10-02T22:46:06.305614: step 1518, loss 0.0480066, acc 1, learning_rate 0.000109898
2017-10-02T22:46:07.477289: step 1519, loss 0.0417902, acc 1, learning_rate 0.000109857
2017-10-02T22:46:08.635979: step 1520, loss 0.0211805, acc 1, learning_rate 0.000109817

Evaluation:
2017-10-02T22:46:08.969635: step 1520, loss 1.44482, acc 0.464748

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1520

2017-10-02T22:46:16.839220: step 1521, loss 0.0541536, acc 0.984375, learning_rate 0.000109777
2017-10-02T22:46:18.003511: step 1522, loss 0.0347719, acc 1, learning_rate 0.000109737
2017-10-02T22:46:19.160750: step 1523, loss 0.0358661, acc 1, learning_rate 0.000109697
2017-10-02T22:46:20.314829: step 1524, loss 0.0382615, acc 0.984375, learning_rate 0.000109658
2017-10-02T22:46:21.461689: step 1525, loss 0.0272096, acc 1, learning_rate 0.000109618
2017-10-02T22:46:22.626753: step 1526, loss 0.0205828, acc 1, learning_rate 0.000109579
2017-10-02T22:46:23.774307: step 1527, loss 0.0269716, acc 1, learning_rate 0.00010954
2017-10-02T22:46:24.922683: step 1528, loss 0.0499926, acc 0.96875, learning_rate 0.000109501
2017-10-02T22:46:26.077292: step 1529, loss 0.0249746, acc 1, learning_rate 0.000109462
2017-10-02T22:46:27.224782: step 1530, loss 0.0360032, acc 1, learning_rate 0.000109424
2017-10-02T22:46:28.387038: step 1531, loss 0.0369503, acc 0.984375, learning_rate 0.000109385
2017-10-02T22:46:29.544141: step 1532, loss 0.0340621, acc 1, learning_rate 0.000109347
2017-10-02T22:46:30.702975: step 1533, loss 0.0199579, acc 1, learning_rate 0.000109309
2017-10-02T22:46:31.858184: step 1534, loss 0.0237267, acc 1, learning_rate 0.000109271
2017-10-02T22:46:33.002487: step 1535, loss 0.0249666, acc 1, learning_rate 0.000109233
2017-10-02T22:46:34.141283: step 1536, loss 0.0314895, acc 1, learning_rate 0.000109195
2017-10-02T22:46:35.299608: step 1537, loss 0.0239622, acc 1, learning_rate 0.000109158
2017-10-02T22:46:36.447090: step 1538, loss 0.0199572, acc 1, learning_rate 0.00010912
2017-10-02T22:46:37.601592: step 1539, loss 0.0753822, acc 0.96875, learning_rate 0.000109083
2017-10-02T22:46:38.968613: step 1540, loss 0.0198304, acc 1, learning_rate 0.000109046
2017-10-02T22:46:40.120186: step 1541, loss 0.0230536, acc 1, learning_rate 0.000109009
2017-10-02T22:46:41.270762: step 1542, loss 0.0573037, acc 0.984375, learning_rate 0.000108972
2017-10-02T22:46:42.433656: step 1543, loss 0.0274106, acc 1, learning_rate 0.000108936
2017-10-02T22:46:43.581945: step 1544, loss 0.0239957, acc 1, learning_rate 0.000108899
2017-10-02T22:46:44.727930: step 1545, loss 0.0222132, acc 1, learning_rate 0.000108863
2017-10-02T22:46:45.872632: step 1546, loss 0.0364428, acc 0.984375, learning_rate 0.000108827
2017-10-02T22:46:47.040014: step 1547, loss 0.0454693, acc 0.984375, learning_rate 0.000108791
2017-10-02T22:46:48.201566: step 1548, loss 0.02123, acc 1, learning_rate 0.000108755
2017-10-02T22:46:49.348377: step 1549, loss 0.0549289, acc 0.984375, learning_rate 0.000108719
2017-10-02T22:46:50.589302: step 1550, loss 0.0162837, acc 1, learning_rate 0.000108683
2017-10-02T22:46:51.753127: step 1551, loss 0.0295841, acc 1, learning_rate 0.000108648
2017-10-02T22:46:52.908475: step 1552, loss 0.0245397, acc 1, learning_rate 0.000108613
2017-10-02T22:46:54.058861: step 1553, loss 0.0368796, acc 1, learning_rate 0.000108577
2017-10-02T22:46:55.206371: step 1554, loss 0.0298371, acc 1, learning_rate 0.000108542
2017-10-02T22:46:56.353641: step 1555, loss 0.0552365, acc 0.984375, learning_rate 0.000108508
2017-10-02T22:46:57.501036: step 1556, loss 0.0438674, acc 1, learning_rate 0.000108473
2017-10-02T22:46:58.646054: step 1557, loss 0.0187593, acc 1, learning_rate 0.000108438
2017-10-02T22:46:59.797654: step 1558, loss 0.0683746, acc 0.984375, learning_rate 0.000108404
2017-10-02T22:47:00.951700: step 1559, loss 0.024021, acc 1, learning_rate 0.00010837
2017-10-02T22:47:02.095538: step 1560, loss 0.030878, acc 1, learning_rate 0.000108335

Evaluation:
2017-10-02T22:47:02.428137: step 1560, loss 1.42931, acc 0.489209

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1560

2017-10-02T22:47:10.086833: step 1561, loss 0.0487282, acc 0.984375, learning_rate 0.000108301
2017-10-02T22:47:11.231801: step 1562, loss 0.0385013, acc 0.984375, learning_rate 0.000108267
2017-10-02T22:47:12.403145: step 1563, loss 0.0248117, acc 1, learning_rate 0.000108234
2017-10-02T22:47:13.558319: step 1564, loss 0.0206776, acc 1, learning_rate 0.0001082
2017-10-02T22:47:14.885900: step 1565, loss 0.0658907, acc 0.984375, learning_rate 0.000108167
2017-10-02T22:47:16.024502: step 1566, loss 0.0503987, acc 0.984375, learning_rate 0.000108133
2017-10-02T22:47:17.173340: step 1567, loss 0.028292, acc 1, learning_rate 0.0001081
2017-10-02T22:47:18.329064: step 1568, loss 0.0255985, acc 1, learning_rate 0.000108067
2017-10-02T22:47:19.488458: step 1569, loss 0.0447048, acc 0.984375, learning_rate 0.000108034
2017-10-02T22:47:20.647396: step 1570, loss 0.0287481, acc 1, learning_rate 0.000108001
2017-10-02T22:47:21.794868: step 1571, loss 0.0153463, acc 1, learning_rate 0.000107969
2017-10-02T22:47:22.939108: step 1572, loss 0.0148944, acc 1, learning_rate 0.000107936
2017-10-02T22:47:24.085646: step 1573, loss 0.0231363, acc 1, learning_rate 0.000107904
2017-10-02T22:47:25.229241: step 1574, loss 0.016065, acc 1, learning_rate 0.000107871
2017-10-02T22:47:26.377289: step 1575, loss 0.0429609, acc 1, learning_rate 0.000107839
2017-10-02T22:47:27.530150: step 1576, loss 0.0604487, acc 0.984375, learning_rate 0.000107807
2017-10-02T22:47:28.679023: step 1577, loss 0.0215146, acc 1, learning_rate 0.000107775
2017-10-02T22:47:29.814472: step 1578, loss 0.0347611, acc 1, learning_rate 0.000107744
2017-10-02T22:47:30.959528: step 1579, loss 0.0363587, acc 1, learning_rate 0.000107712
2017-10-02T22:47:32.114543: step 1580, loss 0.0305003, acc 1, learning_rate 0.000107681
2017-10-02T22:47:33.275899: step 1581, loss 0.0313022, acc 1, learning_rate 0.000107649
2017-10-02T22:47:34.424479: step 1582, loss 0.0592823, acc 0.984375, learning_rate 0.000107618
2017-10-02T22:47:35.582681: step 1583, loss 0.0334701, acc 1, learning_rate 0.000107587
2017-10-02T22:47:36.750459: step 1584, loss 0.0898887, acc 0.953125, learning_rate 0.000107556
2017-10-02T22:47:37.924314: step 1585, loss 0.0415944, acc 1, learning_rate 0.000107525
2017-10-02T22:47:39.072793: step 1586, loss 0.0179087, acc 1, learning_rate 0.000107494
2017-10-02T22:47:40.230333: step 1587, loss 0.0413546, acc 0.984375, learning_rate 0.000107464
2017-10-02T22:47:41.379997: step 1588, loss 0.0253721, acc 1, learning_rate 0.000107433
2017-10-02T22:47:42.525629: step 1589, loss 0.039576, acc 0.984375, learning_rate 0.000107403
2017-10-02T22:47:43.681970: step 1590, loss 0.0456356, acc 1, learning_rate 0.000107373
2017-10-02T22:47:44.834661: step 1591, loss 0.0543497, acc 0.984375, learning_rate 0.000107343
2017-10-02T22:47:45.982666: step 1592, loss 0.0218257, acc 1, learning_rate 0.000107313
2017-10-02T22:47:47.144744: step 1593, loss 0.0430081, acc 0.984375, learning_rate 0.000107283
2017-10-02T22:47:48.300721: step 1594, loss 0.0199536, acc 1, learning_rate 0.000107253
2017-10-02T22:47:49.450019: step 1595, loss 0.025892, acc 1, learning_rate 0.000107224
2017-10-02T22:47:50.604056: step 1596, loss 0.0271611, acc 1, learning_rate 0.000107194
2017-10-02T22:47:51.770688: step 1597, loss 0.0350428, acc 1, learning_rate 0.000107165
2017-10-02T22:47:52.918801: step 1598, loss 0.0223206, acc 1, learning_rate 0.000107136
2017-10-02T22:47:54.070359: step 1599, loss 0.0167812, acc 1, learning_rate 0.000107106
2017-10-02T22:47:55.223343: step 1600, loss 0.0296041, acc 1, learning_rate 0.000107077

Evaluation:
2017-10-02T22:47:55.572718: step 1600, loss 1.4389, acc 0.47482

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1600

2017-10-02T22:48:02.680342: step 1601, loss 0.0207793, acc 1, learning_rate 0.000107048
2017-10-02T22:48:03.847985: step 1602, loss 0.0251522, acc 1, learning_rate 0.00010702
2017-10-02T22:48:04.995894: step 1603, loss 0.0202936, acc 1, learning_rate 0.000106991
2017-10-02T22:48:06.153835: step 1604, loss 0.0536785, acc 0.984375, learning_rate 0.000106963
2017-10-02T22:48:07.331334: step 1605, loss 0.0213741, acc 1, learning_rate 0.000106934
2017-10-02T22:48:08.477792: step 1606, loss 0.0210751, acc 1, learning_rate 0.000106906
2017-10-02T22:48:09.628421: step 1607, loss 0.0211561, acc 1, learning_rate 0.000106878
2017-10-02T22:48:10.762540: step 1608, loss 0.0358106, acc 1, learning_rate 0.00010685
2017-10-02T22:48:11.952138: step 1609, loss 0.0478067, acc 0.984375, learning_rate 0.000106822
2017-10-02T22:48:13.124348: step 1610, loss 0.0227877, acc 1, learning_rate 0.000106794
2017-10-02T22:48:14.275159: step 1611, loss 0.0301668, acc 1, learning_rate 0.000106766
2017-10-02T22:48:15.446484: step 1612, loss 0.030237, acc 1, learning_rate 0.000106738
2017-10-02T22:48:16.618852: step 1613, loss 0.0468019, acc 0.984375, learning_rate 0.000106711
2017-10-02T22:48:17.774923: step 1614, loss 0.0485503, acc 0.984375, learning_rate 0.000106684
2017-10-02T22:48:18.934151: step 1615, loss 0.0309966, acc 1, learning_rate 0.000106656
2017-10-02T22:48:20.092920: step 1616, loss 0.0380713, acc 0.984375, learning_rate 0.000106629
2017-10-02T22:48:21.238264: step 1617, loss 0.023265, acc 1, learning_rate 0.000106602
2017-10-02T22:48:22.390980: step 1618, loss 0.0193748, acc 1, learning_rate 0.000106575
2017-10-02T22:48:23.546036: step 1619, loss 0.0275698, acc 0.984375, learning_rate 0.000106548
2017-10-02T22:48:24.698146: step 1620, loss 0.0486176, acc 1, learning_rate 0.000106521
2017-10-02T22:48:25.834248: step 1621, loss 0.0224122, acc 1, learning_rate 0.000106495
2017-10-02T22:48:26.993052: step 1622, loss 0.0165751, acc 1, learning_rate 0.000106468
2017-10-02T22:48:28.146741: step 1623, loss 0.0136909, acc 1, learning_rate 0.000106442
2017-10-02T22:48:29.303495: step 1624, loss 0.0306702, acc 1, learning_rate 0.000106416
2017-10-02T22:48:30.445120: step 1625, loss 0.0289073, acc 0.984375, learning_rate 0.000106389
2017-10-02T22:48:31.605645: step 1626, loss 0.0198655, acc 1, learning_rate 0.000106363
2017-10-02T22:48:32.750732: step 1627, loss 0.0220588, acc 1, learning_rate 0.000106337
2017-10-02T22:48:33.889514: step 1628, loss 0.0213555, acc 1, learning_rate 0.000106312
2017-10-02T22:48:35.037422: step 1629, loss 0.0177598, acc 1, learning_rate 0.000106286
2017-10-02T22:48:36.187030: step 1630, loss 0.0500486, acc 0.984375, learning_rate 0.00010626
2017-10-02T22:48:37.338189: step 1631, loss 0.0345182, acc 0.984375, learning_rate 0.000106235
2017-10-02T22:48:38.499153: step 1632, loss 0.0254197, acc 1, learning_rate 0.000106209
2017-10-02T22:48:39.668976: step 1633, loss 0.0508033, acc 0.984375, learning_rate 0.000106184
2017-10-02T22:48:40.810241: step 1634, loss 0.030195, acc 1, learning_rate 0.000106159
2017-10-02T22:48:41.960042: step 1635, loss 0.0216267, acc 1, learning_rate 0.000106133
2017-10-02T22:48:43.113410: step 1636, loss 0.0137566, acc 1, learning_rate 0.000106108
2017-10-02T22:48:44.263040: step 1637, loss 0.0651029, acc 0.984375, learning_rate 0.000106083
2017-10-02T22:48:45.511502: step 1638, loss 0.0222003, acc 1, learning_rate 0.000106059
2017-10-02T22:48:46.681130: step 1639, loss 0.0250784, acc 1, learning_rate 0.000106034
2017-10-02T22:48:47.839266: step 1640, loss 0.0302763, acc 1, learning_rate 0.000106009

Evaluation:
2017-10-02T22:48:48.155970: step 1640, loss 1.43928, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1640

2017-10-02T22:48:56.202909: step 1641, loss 0.0221836, acc 1, learning_rate 0.000105985
2017-10-02T22:48:57.408980: step 1642, loss 0.0521653, acc 0.984375, learning_rate 0.00010596
2017-10-02T22:48:58.664043: step 1643, loss 0.0374431, acc 0.984375, learning_rate 0.000105936
2017-10-02T22:48:59.807355: step 1644, loss 0.0355655, acc 1, learning_rate 0.000105912
2017-10-02T22:49:00.949824: step 1645, loss 0.0389216, acc 1, learning_rate 0.000105888
2017-10-02T22:49:02.094517: step 1646, loss 0.0433355, acc 0.984375, learning_rate 0.000105864
2017-10-02T22:49:03.246235: step 1647, loss 0.0172819, acc 1, learning_rate 0.00010584
2017-10-02T22:49:04.421408: step 1648, loss 0.0151716, acc 1, learning_rate 0.000105816
2017-10-02T22:49:05.586297: step 1649, loss 0.0439209, acc 0.984375, learning_rate 0.000105792
2017-10-02T22:49:06.728429: step 1650, loss 0.0215434, acc 1, learning_rate 0.000105768
2017-10-02T22:49:07.890988: step 1651, loss 0.0320213, acc 1, learning_rate 0.000105745
2017-10-02T22:49:09.033017: step 1652, loss 0.0553354, acc 0.96875, learning_rate 0.000105721
2017-10-02T22:49:10.190072: step 1653, loss 0.0183668, acc 1, learning_rate 0.000105698
2017-10-02T22:49:11.340592: step 1654, loss 0.0165994, acc 1, learning_rate 0.000105675
2017-10-02T22:49:12.493558: step 1655, loss 0.0221338, acc 1, learning_rate 0.000105652
2017-10-02T22:49:13.642366: step 1656, loss 0.0340941, acc 0.984375, learning_rate 0.000105629
2017-10-02T22:49:15.018415: step 1657, loss 0.0197878, acc 1, learning_rate 0.000105606
2017-10-02T22:49:16.163083: step 1658, loss 0.0432051, acc 0.984375, learning_rate 0.000105583
2017-10-02T22:49:17.325698: step 1659, loss 0.0301055, acc 1, learning_rate 0.00010556
2017-10-02T22:49:18.498731: step 1660, loss 0.0235101, acc 1, learning_rate 0.000105537
2017-10-02T22:49:19.659238: step 1661, loss 0.0240782, acc 1, learning_rate 0.000105515
2017-10-02T22:49:20.806739: step 1662, loss 0.0201239, acc 1, learning_rate 0.000105492
2017-10-02T22:49:21.969442: step 1663, loss 0.0321248, acc 1, learning_rate 0.00010547
2017-10-02T22:49:23.115145: step 1664, loss 0.0601356, acc 0.96875, learning_rate 0.000105447
2017-10-02T22:49:24.255806: step 1665, loss 0.0172076, acc 1, learning_rate 0.000105425
2017-10-02T22:49:25.393550: step 1666, loss 0.0601197, acc 0.980392, learning_rate 0.000105403
2017-10-02T22:49:26.559638: step 1667, loss 0.0169543, acc 1, learning_rate 0.000105381
2017-10-02T22:49:27.715942: step 1668, loss 0.0309203, acc 1, learning_rate 0.000105359
2017-10-02T22:49:28.878414: step 1669, loss 0.0298435, acc 0.984375, learning_rate 0.000105337
2017-10-02T22:49:30.041649: step 1670, loss 0.0200884, acc 1, learning_rate 0.000105315
2017-10-02T22:49:31.211088: step 1671, loss 0.0362481, acc 1, learning_rate 0.000105294
2017-10-02T22:49:32.371334: step 1672, loss 0.0233678, acc 1, learning_rate 0.000105272
2017-10-02T22:49:33.540652: step 1673, loss 0.0546066, acc 0.984375, learning_rate 0.000105251
2017-10-02T22:49:34.680498: step 1674, loss 0.0295726, acc 1, learning_rate 0.000105229
2017-10-02T22:49:35.820836: step 1675, loss 0.0362423, acc 0.984375, learning_rate 0.000105208
2017-10-02T22:49:36.975934: step 1676, loss 0.0279153, acc 1, learning_rate 0.000105186
2017-10-02T22:49:38.127536: step 1677, loss 0.029719, acc 1, learning_rate 0.000105165
2017-10-02T22:49:39.287010: step 1678, loss 0.0206042, acc 1, learning_rate 0.000105144
2017-10-02T22:49:40.436556: step 1679, loss 0.0279785, acc 1, learning_rate 0.000105123
2017-10-02T22:49:41.597179: step 1680, loss 0.0512647, acc 0.984375, learning_rate 0.000105102

Evaluation:
2017-10-02T22:49:41.944678: step 1680, loss 1.45629, acc 0.446043

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1680

2017-10-02T22:49:49.513808: step 1681, loss 0.0383347, acc 1, learning_rate 0.000105081
2017-10-02T22:49:50.683279: step 1682, loss 0.0192108, acc 1, learning_rate 0.000105061
2017-10-02T22:49:51.824415: step 1683, loss 0.0279366, acc 1, learning_rate 0.00010504
2017-10-02T22:49:52.980950: step 1684, loss 0.0289214, acc 1, learning_rate 0.00010502
2017-10-02T22:49:54.148563: step 1685, loss 0.0172348, acc 1, learning_rate 0.000104999
2017-10-02T22:49:55.309383: step 1686, loss 0.0328303, acc 1, learning_rate 0.000104979
2017-10-02T22:49:56.446511: step 1687, loss 0.0171968, acc 1, learning_rate 0.000104958
2017-10-02T22:49:57.604793: step 1688, loss 0.016228, acc 1, learning_rate 0.000104938
2017-10-02T22:49:58.749517: step 1689, loss 0.0193986, acc 1, learning_rate 0.000104918
2017-10-02T22:49:59.901665: step 1690, loss 0.0249044, acc 1, learning_rate 0.000104898
2017-10-02T22:50:01.035506: step 1691, loss 0.0370652, acc 0.984375, learning_rate 0.000104878
2017-10-02T22:50:02.187999: step 1692, loss 0.0244335, acc 1, learning_rate 0.000104858
2017-10-02T22:50:03.334612: step 1693, loss 0.0141757, acc 1, learning_rate 0.000104838
2017-10-02T22:50:04.544365: step 1694, loss 0.0190623, acc 1, learning_rate 0.000104818
2017-10-02T22:50:05.693317: step 1695, loss 0.0698029, acc 0.984375, learning_rate 0.000104799
2017-10-02T22:50:06.845945: step 1696, loss 0.0208197, acc 1, learning_rate 0.000104779
2017-10-02T22:50:07.988394: step 1697, loss 0.0182215, acc 1, learning_rate 0.00010476
2017-10-02T22:50:09.130513: step 1698, loss 0.0304411, acc 1, learning_rate 0.00010474
2017-10-02T22:50:10.268827: step 1699, loss 0.0461661, acc 0.984375, learning_rate 0.000104721
2017-10-02T22:50:11.425952: step 1700, loss 0.0160917, acc 1, learning_rate 0.000104702
2017-10-02T22:50:12.565922: step 1701, loss 0.0179291, acc 1, learning_rate 0.000104682
2017-10-02T22:50:13.712461: step 1702, loss 0.0253946, acc 1, learning_rate 0.000104663
2017-10-02T22:50:14.855583: step 1703, loss 0.0163223, acc 1, learning_rate 0.000104644
2017-10-02T22:50:15.999166: step 1704, loss 0.02008, acc 1, learning_rate 0.000104625
2017-10-02T22:50:17.145077: step 1705, loss 0.0399667, acc 1, learning_rate 0.000104606
2017-10-02T22:50:18.304929: step 1706, loss 0.100003, acc 0.96875, learning_rate 0.000104588
2017-10-02T22:50:19.454753: step 1707, loss 0.0513273, acc 0.96875, learning_rate 0.000104569
2017-10-02T22:50:20.613851: step 1708, loss 0.0338087, acc 1, learning_rate 0.00010455
2017-10-02T22:50:21.763994: step 1709, loss 0.0413221, acc 0.984375, learning_rate 0.000104532
2017-10-02T22:50:22.909886: step 1710, loss 0.0221639, acc 1, learning_rate 0.000104513
2017-10-02T22:50:24.057340: step 1711, loss 0.0319721, acc 0.984375, learning_rate 0.000104495
2017-10-02T22:50:25.205434: step 1712, loss 0.031141, acc 1, learning_rate 0.000104476
2017-10-02T22:50:26.364200: step 1713, loss 0.0222702, acc 1, learning_rate 0.000104458
2017-10-02T22:50:27.517315: step 1714, loss 0.0378084, acc 1, learning_rate 0.00010444
2017-10-02T22:50:28.658935: step 1715, loss 0.0183262, acc 1, learning_rate 0.000104422
2017-10-02T22:50:29.802838: step 1716, loss 0.0664114, acc 0.984375, learning_rate 0.000104404
2017-10-02T22:50:30.962061: step 1717, loss 0.030387, acc 1, learning_rate 0.000104386
2017-10-02T22:50:32.107080: step 1718, loss 0.0324295, acc 0.984375, learning_rate 0.000104368
2017-10-02T22:50:33.241074: step 1719, loss 0.0336985, acc 1, learning_rate 0.00010435
2017-10-02T22:50:34.415764: step 1720, loss 0.0273948, acc 0.984375, learning_rate 0.000104332

Evaluation:
2017-10-02T22:50:34.753598: step 1720, loss 1.43613, acc 0.471942

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1720

2017-10-02T22:50:42.724256: step 1721, loss 0.0345661, acc 1, learning_rate 0.000104315
2017-10-02T22:50:43.889109: step 1722, loss 0.0116853, acc 1, learning_rate 0.000104297
2017-10-02T22:50:45.046871: step 1723, loss 0.0220794, acc 1, learning_rate 0.000104279
2017-10-02T22:50:46.197278: step 1724, loss 0.0238447, acc 1, learning_rate 0.000104262
2017-10-02T22:50:47.360753: step 1725, loss 0.0371366, acc 0.984375, learning_rate 0.000104245
2017-10-02T22:50:48.518450: step 1726, loss 0.0462551, acc 0.984375, learning_rate 0.000104227
2017-10-02T22:50:49.669079: step 1727, loss 0.0422115, acc 0.984375, learning_rate 0.00010421
2017-10-02T22:50:50.816541: step 1728, loss 0.023102, acc 1, learning_rate 0.000104193
2017-10-02T22:50:51.967807: step 1729, loss 0.0250409, acc 1, learning_rate 0.000104176
2017-10-02T22:50:53.117621: step 1730, loss 0.023869, acc 1, learning_rate 0.000104159
2017-10-02T22:50:54.261842: step 1731, loss 0.0283861, acc 1, learning_rate 0.000104142
2017-10-02T22:50:55.408007: step 1732, loss 0.031435, acc 1, learning_rate 0.000104125
2017-10-02T22:50:56.557171: step 1733, loss 0.0333252, acc 1, learning_rate 0.000104108
2017-10-02T22:50:57.723162: step 1734, loss 0.0308323, acc 1, learning_rate 0.000104091
2017-10-02T22:50:58.876265: step 1735, loss 0.0117839, acc 1, learning_rate 0.000104074
2017-10-02T22:51:00.032630: step 1736, loss 0.0291649, acc 1, learning_rate 0.000104058
2017-10-02T22:51:01.196932: step 1737, loss 0.0749005, acc 0.984375, learning_rate 0.000104041
2017-10-02T22:51:02.373906: step 1738, loss 0.0228354, acc 1, learning_rate 0.000104025
2017-10-02T22:51:03.532353: step 1739, loss 0.0243398, acc 1, learning_rate 0.000104008
2017-10-02T22:51:04.678405: step 1740, loss 0.0178172, acc 1, learning_rate 0.000103992
2017-10-02T22:51:05.815513: step 1741, loss 0.0288941, acc 1, learning_rate 0.000103976
2017-10-02T22:51:06.976229: step 1742, loss 0.0228232, acc 1, learning_rate 0.000103959
2017-10-02T22:51:08.147276: step 1743, loss 0.0219616, acc 1, learning_rate 0.000103943
2017-10-02T22:51:09.327695: step 1744, loss 0.0475697, acc 0.984375, learning_rate 0.000103927
2017-10-02T22:51:10.517293: step 1745, loss 0.0299361, acc 1, learning_rate 0.000103911
2017-10-02T22:51:11.703972: step 1746, loss 0.0255133, acc 1, learning_rate 0.000103895
2017-10-02T22:51:12.885504: step 1747, loss 0.0220353, acc 1, learning_rate 0.000103879
2017-10-02T22:51:14.055385: step 1748, loss 0.0613816, acc 0.984375, learning_rate 0.000103863
2017-10-02T22:51:15.245884: step 1749, loss 0.0286548, acc 1, learning_rate 0.000103848
2017-10-02T22:51:16.478130: step 1750, loss 0.0207767, acc 1, learning_rate 0.000103832
2017-10-02T22:51:17.674061: step 1751, loss 0.0198492, acc 1, learning_rate 0.000103816
2017-10-02T22:51:18.854927: step 1752, loss 0.0247048, acc 1, learning_rate 0.000103801
2017-10-02T22:51:20.036613: step 1753, loss 0.0170853, acc 1, learning_rate 0.000103785
2017-10-02T22:51:21.205058: step 1754, loss 0.0217612, acc 1, learning_rate 0.00010377
2017-10-02T22:51:22.362850: step 1755, loss 0.023742, acc 1, learning_rate 0.000103754
2017-10-02T22:51:23.525023: step 1756, loss 0.0172988, acc 1, learning_rate 0.000103739
2017-10-02T22:51:24.685621: step 1757, loss 0.0610951, acc 0.984375, learning_rate 0.000103724
2017-10-02T22:51:25.836121: step 1758, loss 0.0325991, acc 1, learning_rate 0.000103709
2017-10-02T22:51:26.986568: step 1759, loss 0.019075, acc 1, learning_rate 0.000103694
2017-10-02T22:51:28.213589: step 1760, loss 0.0510688, acc 0.984375, learning_rate 0.000103678

Evaluation:
2017-10-02T22:51:28.554832: step 1760, loss 1.43528, acc 0.479137

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1760

2017-10-02T22:51:36.405954: step 1761, loss 0.0285767, acc 1, learning_rate 0.000103663
2017-10-02T22:51:37.600242: step 1762, loss 0.0221974, acc 1, learning_rate 0.000103648
2017-10-02T22:51:38.759272: step 1763, loss 0.0653169, acc 0.984375, learning_rate 0.000103634
2017-10-02T22:51:39.900047: step 1764, loss 0.0549592, acc 0.980392, learning_rate 0.000103619
2017-10-02T22:51:41.051482: step 1765, loss 0.0358221, acc 0.984375, learning_rate 0.000103604
2017-10-02T22:51:42.221345: step 1766, loss 0.0196417, acc 1, learning_rate 0.000103589
2017-10-02T22:51:43.391824: step 1767, loss 0.0237936, acc 1, learning_rate 0.000103575
2017-10-02T22:51:44.559042: step 1768, loss 0.0267364, acc 1, learning_rate 0.00010356
2017-10-02T22:51:45.714264: step 1769, loss 0.0659166, acc 0.96875, learning_rate 0.000103545
2017-10-02T22:51:46.873338: step 1770, loss 0.0198627, acc 1, learning_rate 0.000103531
2017-10-02T22:51:48.053082: step 1771, loss 0.0303672, acc 0.984375, learning_rate 0.000103517
2017-10-02T22:51:49.205778: step 1772, loss 0.0479846, acc 0.984375, learning_rate 0.000103502
2017-10-02T22:51:50.351918: step 1773, loss 0.0196763, acc 1, learning_rate 0.000103488
2017-10-02T22:51:51.498456: step 1774, loss 0.022993, acc 1, learning_rate 0.000103474
2017-10-02T22:51:52.658252: step 1775, loss 0.021366, acc 1, learning_rate 0.00010346
2017-10-02T22:51:53.818209: step 1776, loss 0.02084, acc 1, learning_rate 0.000103445
2017-10-02T22:51:54.966924: step 1777, loss 0.0596446, acc 0.984375, learning_rate 0.000103431
2017-10-02T22:51:56.140676: step 1778, loss 0.0200808, acc 1, learning_rate 0.000103417
2017-10-02T22:51:57.289223: step 1779, loss 0.0169029, acc 1, learning_rate 0.000103403
2017-10-02T22:51:58.496673: step 1780, loss 0.0148233, acc 1, learning_rate 0.00010339
2017-10-02T22:51:59.652419: step 1781, loss 0.0328699, acc 0.984375, learning_rate 0.000103376
2017-10-02T22:52:00.805565: step 1782, loss 0.0797341, acc 0.984375, learning_rate 0.000103362
2017-10-02T22:52:01.954267: step 1783, loss 0.073015, acc 0.984375, learning_rate 0.000103348
2017-10-02T22:52:03.119751: step 1784, loss 0.0267047, acc 1, learning_rate 0.000103335
2017-10-02T22:52:04.271029: step 1785, loss 0.020266, acc 1, learning_rate 0.000103321
2017-10-02T22:52:05.423477: step 1786, loss 0.0212947, acc 1, learning_rate 0.000103307
2017-10-02T22:52:06.576501: step 1787, loss 0.0228546, acc 1, learning_rate 0.000103294
2017-10-02T22:52:07.729070: step 1788, loss 0.0203562, acc 1, learning_rate 0.00010328
2017-10-02T22:52:08.891748: step 1789, loss 0.021469, acc 1, learning_rate 0.000103267
2017-10-02T22:52:10.037476: step 1790, loss 0.0227837, acc 1, learning_rate 0.000103254
2017-10-02T22:52:11.194362: step 1791, loss 0.0169354, acc 1, learning_rate 0.00010324
2017-10-02T22:52:12.333801: step 1792, loss 0.0198658, acc 1, learning_rate 0.000103227
2017-10-02T22:52:13.498526: step 1793, loss 0.0304653, acc 1, learning_rate 0.000103214
2017-10-02T22:52:14.652425: step 1794, loss 0.0487676, acc 0.96875, learning_rate 0.000103201
2017-10-02T22:52:15.793554: step 1795, loss 0.0228663, acc 1, learning_rate 0.000103188
2017-10-02T22:52:16.929416: step 1796, loss 0.0228851, acc 1, learning_rate 0.000103175
2017-10-02T22:52:18.077692: step 1797, loss 0.0135809, acc 1, learning_rate 0.000103162
2017-10-02T22:52:19.233880: step 1798, loss 0.0474134, acc 0.984375, learning_rate 0.000103149
2017-10-02T22:52:20.410348: step 1799, loss 0.0427934, acc 0.984375, learning_rate 0.000103136
2017-10-02T22:52:21.549625: step 1800, loss 0.0139569, acc 1, learning_rate 0.000103123

Evaluation:
2017-10-02T22:52:21.894842: step 1800, loss 1.44114, acc 0.453237

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1800

2017-10-02T22:52:29.454890: step 1801, loss 0.0239921, acc 1, learning_rate 0.000103111
2017-10-02T22:52:30.624627: step 1802, loss 0.0550736, acc 0.984375, learning_rate 0.000103098
2017-10-02T22:52:31.788501: step 1803, loss 0.0322375, acc 1, learning_rate 0.000103085
2017-10-02T22:52:32.942150: step 1804, loss 0.0146013, acc 1, learning_rate 0.000103073
2017-10-02T22:52:34.091188: step 1805, loss 0.03225, acc 1, learning_rate 0.00010306
2017-10-02T22:52:35.245771: step 1806, loss 0.0247164, acc 1, learning_rate 0.000103048
2017-10-02T22:52:36.412773: step 1807, loss 0.0158816, acc 1, learning_rate 0.000103035
2017-10-02T22:52:37.587755: step 1808, loss 0.0191515, acc 1, learning_rate 0.000103023
2017-10-02T22:52:38.778468: step 1809, loss 0.0677069, acc 0.984375, learning_rate 0.00010301
2017-10-02T22:52:39.938792: step 1810, loss 0.0420701, acc 1, learning_rate 0.000102998
2017-10-02T22:52:41.079820: step 1811, loss 0.0198154, acc 1, learning_rate 0.000102986
2017-10-02T22:52:42.311295: step 1812, loss 0.0216674, acc 1, learning_rate 0.000102974
2017-10-02T22:52:43.471246: step 1813, loss 0.0200125, acc 1, learning_rate 0.000102962
2017-10-02T22:52:44.627866: step 1814, loss 0.0411732, acc 1, learning_rate 0.000102949
2017-10-02T22:52:45.786820: step 1815, loss 0.04599, acc 0.984375, learning_rate 0.000102937
2017-10-02T22:52:46.943302: step 1816, loss 0.0299562, acc 1, learning_rate 0.000102925
2017-10-02T22:52:48.109891: step 1817, loss 0.0229736, acc 1, learning_rate 0.000102913
2017-10-02T22:52:49.259309: step 1818, loss 0.0388039, acc 1, learning_rate 0.000102902
2017-10-02T22:52:50.419463: step 1819, loss 0.0129962, acc 1, learning_rate 0.00010289
2017-10-02T22:52:51.582623: step 1820, loss 0.017764, acc 1, learning_rate 0.000102878
2017-10-02T22:52:52.730977: step 1821, loss 0.0846257, acc 0.96875, learning_rate 0.000102866
2017-10-02T22:52:53.884766: step 1822, loss 0.0556408, acc 0.984375, learning_rate 0.000102855
2017-10-02T22:52:55.033826: step 1823, loss 0.0167077, acc 1, learning_rate 0.000102843
2017-10-02T22:52:56.182062: step 1824, loss 0.0281136, acc 0.984375, learning_rate 0.000102831
2017-10-02T22:52:57.395040: step 1825, loss 0.0121607, acc 1, learning_rate 0.00010282
2017-10-02T22:52:58.544528: step 1826, loss 0.0227605, acc 1, learning_rate 0.000102808
2017-10-02T22:52:59.689233: step 1827, loss 0.0356962, acc 1, learning_rate 0.000102797
2017-10-02T22:53:00.863446: step 1828, loss 0.0177369, acc 1, learning_rate 0.000102785
2017-10-02T22:53:02.019959: step 1829, loss 0.0558659, acc 0.984375, learning_rate 0.000102774
2017-10-02T22:53:03.169626: step 1830, loss 0.0290028, acc 1, learning_rate 0.000102763
2017-10-02T22:53:04.323763: step 1831, loss 0.0296658, acc 1, learning_rate 0.000102751
2017-10-02T22:53:05.476817: step 1832, loss 0.0342107, acc 1, learning_rate 0.00010274
2017-10-02T22:53:06.633183: step 1833, loss 0.0554875, acc 0.984375, learning_rate 0.000102729
2017-10-02T22:53:07.781021: step 1834, loss 0.0176901, acc 1, learning_rate 0.000102718
2017-10-02T22:53:08.932377: step 1835, loss 0.0181729, acc 1, learning_rate 0.000102707
2017-10-02T22:53:10.097253: step 1836, loss 0.0975134, acc 0.984375, learning_rate 0.000102696
2017-10-02T22:53:11.253773: step 1837, loss 0.0128787, acc 1, learning_rate 0.000102685
2017-10-02T22:53:12.406198: step 1838, loss 0.0257155, acc 1, learning_rate 0.000102674
2017-10-02T22:53:13.555990: step 1839, loss 0.0244929, acc 1, learning_rate 0.000102663
2017-10-02T22:53:14.711709: step 1840, loss 0.0254772, acc 1, learning_rate 0.000102652

Evaluation:
2017-10-02T22:53:15.046171: step 1840, loss 1.4424, acc 0.448921

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1840

2017-10-02T22:53:23.156236: step 1841, loss 0.0637748, acc 0.96875, learning_rate 0.000102641
2017-10-02T22:53:24.309556: step 1842, loss 0.0164497, acc 1, learning_rate 0.00010263
2017-10-02T22:53:25.473777: step 1843, loss 0.024081, acc 1, learning_rate 0.00010262
2017-10-02T22:53:26.623722: step 1844, loss 0.0317322, acc 1, learning_rate 0.000102609
2017-10-02T22:53:27.802901: step 1845, loss 0.0517213, acc 0.984375, learning_rate 0.000102598
2017-10-02T22:53:28.997642: step 1846, loss 0.0368118, acc 0.984375, learning_rate 0.000102588
2017-10-02T22:53:30.145110: step 1847, loss 0.0391401, acc 0.984375, learning_rate 0.000102577
2017-10-02T22:53:31.301462: step 1848, loss 0.0123423, acc 1, learning_rate 0.000102567
2017-10-02T22:53:32.450066: step 1849, loss 0.0402312, acc 0.984375, learning_rate 0.000102556
2017-10-02T22:53:33.603784: step 1850, loss 0.0279928, acc 1, learning_rate 0.000102546
2017-10-02T22:53:34.760436: step 1851, loss 0.0143227, acc 1, learning_rate 0.000102535
2017-10-02T22:53:35.913572: step 1852, loss 0.015169, acc 1, learning_rate 0.000102525
2017-10-02T22:53:37.081439: step 1853, loss 0.0210125, acc 1, learning_rate 0.000102515
2017-10-02T22:53:38.231137: step 1854, loss 0.014279, acc 1, learning_rate 0.000102504
2017-10-02T22:53:39.384399: step 1855, loss 0.0328978, acc 1, learning_rate 0.000102494
2017-10-02T22:53:40.546378: step 1856, loss 0.0188052, acc 1, learning_rate 0.000102484
2017-10-02T22:53:41.725118: step 1857, loss 0.017891, acc 1, learning_rate 0.000102474
2017-10-02T22:53:42.874717: step 1858, loss 0.0240463, acc 1, learning_rate 0.000102464
2017-10-02T22:53:44.033435: step 1859, loss 0.0184411, acc 1, learning_rate 0.000102454
2017-10-02T22:53:45.181363: step 1860, loss 0.0471549, acc 0.984375, learning_rate 0.000102444
2017-10-02T22:53:46.345047: step 1861, loss 0.0217668, acc 1, learning_rate 0.000102434
2017-10-02T22:53:47.481144: step 1862, loss 0.0187187, acc 1, learning_rate 0.000102424
2017-10-02T22:53:48.639084: step 1863, loss 0.0272237, acc 1, learning_rate 0.000102414
2017-10-02T22:53:49.780550: step 1864, loss 0.0227393, acc 1, learning_rate 0.000102404
2017-10-02T22:53:50.938751: step 1865, loss 0.0193532, acc 1, learning_rate 0.000102394
2017-10-02T22:53:52.089189: step 1866, loss 0.015369, acc 1, learning_rate 0.000102384
2017-10-02T22:53:53.245056: step 1867, loss 0.0484286, acc 0.984375, learning_rate 0.000102375
2017-10-02T22:53:54.403602: step 1868, loss 0.0200126, acc 1, learning_rate 0.000102365
2017-10-02T22:53:55.546276: step 1869, loss 0.056826, acc 0.96875, learning_rate 0.000102355
2017-10-02T22:53:56.704907: step 1870, loss 0.0134659, acc 1, learning_rate 0.000102346
2017-10-02T22:53:57.869530: step 1871, loss 0.0183396, acc 1, learning_rate 0.000102336
2017-10-02T22:53:59.018177: step 1872, loss 0.0203312, acc 1, learning_rate 0.000102327
2017-10-02T22:54:00.182848: step 1873, loss 0.0691038, acc 0.96875, learning_rate 0.000102317
2017-10-02T22:54:01.326378: step 1874, loss 0.0180954, acc 1, learning_rate 0.000102308
2017-10-02T22:54:02.488929: step 1875, loss 0.0178914, acc 1, learning_rate 0.000102298
2017-10-02T22:54:03.651848: step 1876, loss 0.0369126, acc 0.984375, learning_rate 0.000102289
2017-10-02T22:54:04.810193: step 1877, loss 0.0424574, acc 1, learning_rate 0.000102279
2017-10-02T22:54:05.977953: step 1878, loss 0.0204918, acc 1, learning_rate 0.00010227
2017-10-02T22:54:07.130059: step 1879, loss 0.030723, acc 1, learning_rate 0.000102261
2017-10-02T22:54:08.281895: step 1880, loss 0.0261265, acc 1, learning_rate 0.000102252

Evaluation:
2017-10-02T22:54:08.628445: step 1880, loss 1.42887, acc 0.479137

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1880

2017-10-02T22:54:17.244154: step 1881, loss 0.0270983, acc 1, learning_rate 0.000102242
2017-10-02T22:54:18.412530: step 1882, loss 0.0207888, acc 1, learning_rate 0.000102233
2017-10-02T22:54:19.567244: step 1883, loss 0.0247671, acc 1, learning_rate 0.000102224
2017-10-02T22:54:20.729920: step 1884, loss 0.0322172, acc 1, learning_rate 0.000102215
2017-10-02T22:54:21.878733: step 1885, loss 0.0155363, acc 1, learning_rate 0.000102206
2017-10-02T22:54:23.042806: step 1886, loss 0.0484664, acc 0.984375, learning_rate 0.000102197
2017-10-02T22:54:24.209749: step 1887, loss 0.0233686, acc 1, learning_rate 0.000102188
2017-10-02T22:54:25.377635: step 1888, loss 0.0673411, acc 0.96875, learning_rate 0.000102179
2017-10-02T22:54:26.541941: step 1889, loss 0.0395986, acc 0.984375, learning_rate 0.00010217
2017-10-02T22:54:27.699874: step 1890, loss 0.0179466, acc 1, learning_rate 0.000102161
2017-10-02T22:54:28.841852: step 1891, loss 0.0114131, acc 1, learning_rate 0.000102153
2017-10-02T22:54:29.994132: step 1892, loss 0.0214437, acc 1, learning_rate 0.000102144
2017-10-02T22:54:31.144842: step 1893, loss 0.0228848, acc 1, learning_rate 0.000102135
2017-10-02T22:54:32.307893: step 1894, loss 0.0172206, acc 1, learning_rate 0.000102126
2017-10-02T22:54:33.466148: step 1895, loss 0.0265798, acc 1, learning_rate 0.000102118
2017-10-02T22:54:34.620047: step 1896, loss 0.0593825, acc 0.96875, learning_rate 0.000102109
2017-10-02T22:54:35.785874: step 1897, loss 0.0565546, acc 0.984375, learning_rate 0.0001021
2017-10-02T22:54:36.965989: step 1898, loss 0.0316818, acc 1, learning_rate 0.000102092
2017-10-02T22:54:38.117243: step 1899, loss 0.030976, acc 1, learning_rate 0.000102083
2017-10-02T22:54:39.289782: step 1900, loss 0.0258806, acc 0.984375, learning_rate 0.000102075
2017-10-02T22:54:40.442386: step 1901, loss 0.0163859, acc 1, learning_rate 0.000102066
2017-10-02T22:54:41.596213: step 1902, loss 0.0188647, acc 1, learning_rate 0.000102058
2017-10-02T22:54:42.744483: step 1903, loss 0.0219329, acc 1, learning_rate 0.00010205
2017-10-02T22:54:43.890928: step 1904, loss 0.0267684, acc 1, learning_rate 0.000102041
2017-10-02T22:54:45.049834: step 1905, loss 0.0168014, acc 1, learning_rate 0.000102033
2017-10-02T22:54:46.271448: step 1906, loss 0.0427556, acc 0.984375, learning_rate 0.000102025
2017-10-02T22:54:47.431786: step 1907, loss 0.0303293, acc 1, learning_rate 0.000102016
2017-10-02T22:54:48.578521: step 1908, loss 0.0422041, acc 0.984375, learning_rate 0.000102008
2017-10-02T22:54:49.737762: step 1909, loss 0.0206776, acc 1, learning_rate 0.000102
2017-10-02T22:54:50.892701: step 1910, loss 0.0344128, acc 1, learning_rate 0.000101992
2017-10-02T22:54:52.052760: step 1911, loss 0.0275348, acc 1, learning_rate 0.000101984
2017-10-02T22:54:53.201498: step 1912, loss 0.0194154, acc 1, learning_rate 0.000101975
2017-10-02T22:54:54.365425: step 1913, loss 0.0137799, acc 1, learning_rate 0.000101967
2017-10-02T22:54:55.518113: step 1914, loss 0.012011, acc 1, learning_rate 0.000101959
2017-10-02T22:54:56.681797: step 1915, loss 0.0180491, acc 1, learning_rate 0.000101951
2017-10-02T22:54:57.825920: step 1916, loss 0.0142673, acc 1, learning_rate 0.000101943
2017-10-02T22:54:58.995161: step 1917, loss 0.0186703, acc 1, learning_rate 0.000101935
2017-10-02T22:55:00.145846: step 1918, loss 0.0284699, acc 1, learning_rate 0.000101928
2017-10-02T22:55:01.285125: step 1919, loss 0.0264464, acc 1, learning_rate 0.00010192
2017-10-02T22:55:02.438157: step 1920, loss 0.0323894, acc 1, learning_rate 0.000101912

Evaluation:
2017-10-02T22:55:02.780856: step 1920, loss 1.45552, acc 0.43741

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1920

2017-10-02T22:55:11.134502: step 1921, loss 0.0416702, acc 0.984375, learning_rate 0.000101904
2017-10-02T22:55:12.311538: step 1922, loss 0.0285712, acc 1, learning_rate 0.000101896
2017-10-02T22:55:13.469443: step 1923, loss 0.0228473, acc 1, learning_rate 0.000101889
2017-10-02T22:55:14.620229: step 1924, loss 0.0135499, acc 1, learning_rate 0.000101881
2017-10-02T22:55:15.774345: step 1925, loss 0.024287, acc 1, learning_rate 0.000101873
2017-10-02T22:55:16.922952: step 1926, loss 0.0159706, acc 1, learning_rate 0.000101865
2017-10-02T22:55:18.068762: step 1927, loss 0.0120746, acc 1, learning_rate 0.000101858
2017-10-02T22:55:19.224861: step 1928, loss 0.0217566, acc 1, learning_rate 0.00010185
2017-10-02T22:55:20.376437: step 1929, loss 0.0186374, acc 1, learning_rate 0.000101843
2017-10-02T22:55:21.526119: step 1930, loss 0.0200918, acc 1, learning_rate 0.000101835
2017-10-02T22:55:22.663348: step 1931, loss 0.038209, acc 1, learning_rate 0.000101828
2017-10-02T22:55:23.830472: step 1932, loss 0.0250802, acc 1, learning_rate 0.00010182
2017-10-02T22:55:24.983195: step 1933, loss 0.0355317, acc 0.984375, learning_rate 0.000101813
2017-10-02T22:55:26.138678: step 1934, loss 0.0199471, acc 1, learning_rate 0.000101805
2017-10-02T22:55:27.320529: step 1935, loss 0.0370549, acc 0.984375, learning_rate 0.000101798
2017-10-02T22:55:28.466632: step 1936, loss 0.0180139, acc 1, learning_rate 0.000101791
2017-10-02T22:55:29.622281: step 1937, loss 0.0177214, acc 1, learning_rate 0.000101783
2017-10-02T22:55:30.769150: step 1938, loss 0.0556458, acc 0.984375, learning_rate 0.000101776
2017-10-02T22:55:31.910857: step 1939, loss 0.0441856, acc 0.984375, learning_rate 0.000101769
2017-10-02T22:55:33.072765: step 1940, loss 0.0371979, acc 1, learning_rate 0.000101762
2017-10-02T22:55:34.239918: step 1941, loss 0.0312768, acc 1, learning_rate 0.000101754
2017-10-02T22:55:35.391760: step 1942, loss 0.054753, acc 0.984375, learning_rate 0.000101747
2017-10-02T22:55:36.544150: step 1943, loss 0.017686, acc 1, learning_rate 0.00010174
2017-10-02T22:55:37.706602: step 1944, loss 0.0313101, acc 0.984375, learning_rate 0.000101733
2017-10-02T22:55:38.869875: step 1945, loss 0.0478175, acc 0.984375, learning_rate 0.000101726
2017-10-02T22:55:40.022573: step 1946, loss 0.0336736, acc 1, learning_rate 0.000101719
2017-10-02T22:55:41.184428: step 1947, loss 0.0288441, acc 1, learning_rate 0.000101712
2017-10-02T22:55:42.340440: step 1948, loss 0.0482539, acc 0.984375, learning_rate 0.000101705
2017-10-02T22:55:43.495900: step 1949, loss 0.0157991, acc 1, learning_rate 0.000101698
2017-10-02T22:55:44.648804: step 1950, loss 0.0418665, acc 0.96875, learning_rate 0.000101691
2017-10-02T22:55:45.807503: step 1951, loss 0.0385508, acc 0.984375, learning_rate 0.000101684
2017-10-02T22:55:46.966269: step 1952, loss 0.0172906, acc 1, learning_rate 0.000101677
2017-10-02T22:55:48.117106: step 1953, loss 0.0555106, acc 0.984375, learning_rate 0.00010167
2017-10-02T22:55:49.268067: step 1954, loss 0.0473077, acc 0.984375, learning_rate 0.000101664
2017-10-02T22:55:50.450039: step 1955, loss 0.0698759, acc 0.984375, learning_rate 0.000101657
2017-10-02T22:55:51.608016: step 1956, loss 0.0254221, acc 1, learning_rate 0.00010165
2017-10-02T22:55:52.818376: step 1957, loss 0.0336237, acc 0.984375, learning_rate 0.000101643
2017-10-02T22:55:53.973882: step 1958, loss 0.0202387, acc 1, learning_rate 0.000101637
2017-10-02T22:55:55.150433: step 1959, loss 0.0258401, acc 1, learning_rate 0.00010163
2017-10-02T22:55:56.294653: step 1960, loss 0.0324641, acc 1, learning_rate 0.000101623

Evaluation:
2017-10-02T22:55:56.627326: step 1960, loss 1.44007, acc 0.476259

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-1960

2017-10-02T22:56:04.271677: step 1961, loss 0.0535119, acc 0.984375, learning_rate 0.000101617
2017-10-02T22:56:05.446582: step 1962, loss 0.058395, acc 0.984375, learning_rate 0.00010161
2017-10-02T22:56:06.615503: step 1963, loss 0.0773482, acc 0.984375, learning_rate 0.000101604
2017-10-02T22:56:07.764467: step 1964, loss 0.0171625, acc 1, learning_rate 0.000101597
2017-10-02T22:56:08.920079: step 1965, loss 0.0255907, acc 1, learning_rate 0.00010159
2017-10-02T22:56:10.090943: step 1966, loss 0.0412958, acc 1, learning_rate 0.000101584
2017-10-02T22:56:11.271575: step 1967, loss 0.0377737, acc 0.984375, learning_rate 0.000101577
2017-10-02T22:56:12.433362: step 1968, loss 0.0346105, acc 1, learning_rate 0.000101571
2017-10-02T22:56:13.592714: step 1969, loss 0.0138716, acc 1, learning_rate 0.000101565
2017-10-02T22:56:14.732098: step 1970, loss 0.0378443, acc 1, learning_rate 0.000101558
2017-10-02T22:56:15.892159: step 1971, loss 0.0378075, acc 0.984375, learning_rate 0.000101552
2017-10-02T22:56:17.045888: step 1972, loss 0.034216, acc 1, learning_rate 0.000101546
2017-10-02T22:56:18.215637: step 1973, loss 0.035281, acc 1, learning_rate 0.000101539
2017-10-02T22:56:19.391015: step 1974, loss 0.0251219, acc 1, learning_rate 0.000101533
2017-10-02T22:56:20.540953: step 1975, loss 0.0171194, acc 1, learning_rate 0.000101527
2017-10-02T22:56:21.700805: step 1976, loss 0.0868564, acc 0.984375, learning_rate 0.00010152
2017-10-02T22:56:22.855854: step 1977, loss 0.0157487, acc 1, learning_rate 0.000101514
2017-10-02T22:56:24.037688: step 1978, loss 0.0249041, acc 1, learning_rate 0.000101508
2017-10-02T22:56:25.192066: step 1979, loss 0.0331712, acc 0.984375, learning_rate 0.000101502
2017-10-02T22:56:26.357378: step 1980, loss 0.0183472, acc 1, learning_rate 0.000101496
2017-10-02T22:56:27.514377: step 1981, loss 0.0184538, acc 1, learning_rate 0.00010149
2017-10-02T22:56:28.762632: step 1982, loss 0.046961, acc 0.984375, learning_rate 0.000101484
2017-10-02T22:56:29.910444: step 1983, loss 0.0206024, acc 1, learning_rate 0.000101478
2017-10-02T22:56:31.047993: step 1984, loss 0.00840935, acc 1, learning_rate 0.000101472
2017-10-02T22:56:32.197870: step 1985, loss 0.0577313, acc 0.984375, learning_rate 0.000101466
2017-10-02T22:56:33.352711: step 1986, loss 0.021218, acc 1, learning_rate 0.00010146
2017-10-02T22:56:34.511915: step 1987, loss 0.0269602, acc 1, learning_rate 0.000101454
2017-10-02T22:56:35.661840: step 1988, loss 0.0153632, acc 1, learning_rate 0.000101448
2017-10-02T22:56:36.917207: step 1989, loss 0.0392534, acc 0.984375, learning_rate 0.000101442
2017-10-02T22:56:38.159391: step 1990, loss 0.0114002, acc 1, learning_rate 0.000101436
2017-10-02T22:56:39.311287: step 1991, loss 0.018191, acc 1, learning_rate 0.00010143
2017-10-02T22:56:40.451284: step 1992, loss 0.0174409, acc 1, learning_rate 0.000101424
2017-10-02T22:56:41.611187: step 1993, loss 0.103134, acc 0.96875, learning_rate 0.000101418
2017-10-02T22:56:42.787141: step 1994, loss 0.0201183, acc 1, learning_rate 0.000101413
2017-10-02T22:56:43.938587: step 1995, loss 0.0150218, acc 1, learning_rate 0.000101407
2017-10-02T22:56:45.086446: step 1996, loss 0.0167113, acc 1, learning_rate 0.000101401
2017-10-02T22:56:46.232900: step 1997, loss 0.0211814, acc 1, learning_rate 0.000101395
2017-10-02T22:56:47.403443: step 1998, loss 0.0115619, acc 1, learning_rate 0.00010139
2017-10-02T22:56:48.559712: step 1999, loss 0.0144083, acc 1, learning_rate 0.000101384
2017-10-02T22:56:49.711721: step 2000, loss 0.0318284, acc 1, learning_rate 0.000101378

Evaluation:
2017-10-02T22:56:50.054654: step 2000, loss 1.42211, acc 0.493525

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2000

2017-10-02T22:56:57.957849: step 2001, loss 0.0279571, acc 1, learning_rate 0.000101373
2017-10-02T22:56:59.117285: step 2002, loss 0.0166587, acc 1, learning_rate 0.000101367
2017-10-02T22:57:00.274904: step 2003, loss 0.0266442, acc 1, learning_rate 0.000101362
2017-10-02T22:57:01.434314: step 2004, loss 0.023034, acc 1, learning_rate 0.000101356
2017-10-02T22:57:02.603676: step 2005, loss 0.00817677, acc 1, learning_rate 0.00010135
2017-10-02T22:57:03.771746: step 2006, loss 0.013963, acc 1, learning_rate 0.000101345
2017-10-02T22:57:04.922219: step 2007, loss 0.0211769, acc 1, learning_rate 0.000101339
2017-10-02T22:57:06.064645: step 2008, loss 0.0244589, acc 1, learning_rate 0.000101334
2017-10-02T22:57:07.225078: step 2009, loss 0.0184446, acc 1, learning_rate 0.000101328
2017-10-02T22:57:08.366312: step 2010, loss 0.0149517, acc 1, learning_rate 0.000101323
2017-10-02T22:57:09.514056: step 2011, loss 0.0182098, acc 1, learning_rate 0.000101318
2017-10-02T22:57:10.800332: step 2012, loss 0.0207474, acc 1, learning_rate 0.000101312
2017-10-02T22:57:11.960193: step 2013, loss 0.0161804, acc 1, learning_rate 0.000101307
2017-10-02T22:57:13.107371: step 2014, loss 0.0258033, acc 1, learning_rate 0.000101302
2017-10-02T22:57:14.260038: step 2015, loss 0.0445739, acc 0.984375, learning_rate 0.000101296
2017-10-02T22:57:15.413781: step 2016, loss 0.088261, acc 0.96875, learning_rate 0.000101291
2017-10-02T22:57:16.573336: step 2017, loss 0.0184908, acc 1, learning_rate 0.000101286
2017-10-02T22:57:17.721798: step 2018, loss 0.0759814, acc 0.96875, learning_rate 0.00010128
2017-10-02T22:57:18.887695: step 2019, loss 0.0159832, acc 1, learning_rate 0.000101275
2017-10-02T22:57:20.048851: step 2020, loss 0.0340335, acc 0.984375, learning_rate 0.00010127
2017-10-02T22:57:21.203932: step 2021, loss 0.0339066, acc 0.984375, learning_rate 0.000101265
2017-10-02T22:57:22.355452: step 2022, loss 0.0219847, acc 1, learning_rate 0.00010126
2017-10-02T22:57:23.526818: step 2023, loss 0.0374072, acc 0.984375, learning_rate 0.000101255
2017-10-02T22:57:24.670283: step 2024, loss 0.0158332, acc 1, learning_rate 0.000101249
2017-10-02T22:57:25.813380: step 2025, loss 0.026046, acc 1, learning_rate 0.000101244
2017-10-02T22:57:26.960217: step 2026, loss 0.0357011, acc 1, learning_rate 0.000101239
2017-10-02T22:57:28.112538: step 2027, loss 0.0291967, acc 1, learning_rate 0.000101234
2017-10-02T22:57:29.258642: step 2028, loss 0.0369516, acc 0.984375, learning_rate 0.000101229
2017-10-02T22:57:30.406854: step 2029, loss 0.0374026, acc 0.984375, learning_rate 0.000101224
2017-10-02T22:57:31.552725: step 2030, loss 0.0298496, acc 0.984375, learning_rate 0.000101219
2017-10-02T22:57:32.701970: step 2031, loss 0.0162798, acc 1, learning_rate 0.000101214
2017-10-02T22:57:33.852216: step 2032, loss 0.0161318, acc 1, learning_rate 0.000101209
2017-10-02T22:57:34.996024: step 2033, loss 0.0506285, acc 0.984375, learning_rate 0.000101204
2017-10-02T22:57:36.140894: step 2034, loss 0.0226985, acc 1, learning_rate 0.000101199
2017-10-02T22:57:37.304456: step 2035, loss 0.0103855, acc 1, learning_rate 0.000101194
2017-10-02T22:57:38.466545: step 2036, loss 0.0189176, acc 1, learning_rate 0.00010119
2017-10-02T22:57:39.623470: step 2037, loss 0.0328251, acc 0.984375, learning_rate 0.000101185
2017-10-02T22:57:40.779627: step 2038, loss 0.0192822, acc 1, learning_rate 0.00010118
2017-10-02T22:57:41.925863: step 2039, loss 0.0298308, acc 1, learning_rate 0.000101175
2017-10-02T22:57:43.078159: step 2040, loss 0.0440075, acc 0.984375, learning_rate 0.00010117

Evaluation:
2017-10-02T22:57:43.412784: step 2040, loss 1.44072, acc 0.460432

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2040

2017-10-02T22:57:50.973515: step 2041, loss 0.0196797, acc 1, learning_rate 0.000101166
2017-10-02T22:57:52.136637: step 2042, loss 0.0292914, acc 1, learning_rate 0.000101161
2017-10-02T22:57:53.293730: step 2043, loss 0.0522893, acc 0.984375, learning_rate 0.000101156
2017-10-02T22:57:54.472963: step 2044, loss 0.0198412, acc 1, learning_rate 0.000101151
2017-10-02T22:57:55.614903: step 2045, loss 0.0534405, acc 0.984375, learning_rate 0.000101147
2017-10-02T22:57:56.776452: step 2046, loss 0.0329846, acc 1, learning_rate 0.000101142
2017-10-02T22:57:57.920703: step 2047, loss 0.0211278, acc 1, learning_rate 0.000101137
2017-10-02T22:57:59.067030: step 2048, loss 0.0208838, acc 1, learning_rate 0.000101133
2017-10-02T22:58:00.227445: step 2049, loss 0.022775, acc 1, learning_rate 0.000101128
2017-10-02T22:58:01.384670: step 2050, loss 0.0161341, acc 1, learning_rate 0.000101123
2017-10-02T22:58:02.540384: step 2051, loss 0.0304788, acc 1, learning_rate 0.000101119
2017-10-02T22:58:03.708213: step 2052, loss 0.0256123, acc 1, learning_rate 0.000101114
2017-10-02T22:58:04.876763: step 2053, loss 0.0610344, acc 0.96875, learning_rate 0.00010111
2017-10-02T22:58:06.027586: step 2054, loss 0.0292177, acc 1, learning_rate 0.000101105
2017-10-02T22:58:07.176926: step 2055, loss 0.0506853, acc 0.984375, learning_rate 0.000101101
2017-10-02T22:58:08.341562: step 2056, loss 0.0188903, acc 1, learning_rate 0.000101096
2017-10-02T22:58:09.506798: step 2057, loss 0.0323715, acc 1, learning_rate 0.000101092
2017-10-02T22:58:10.657117: step 2058, loss 0.0260713, acc 1, learning_rate 0.000101087
2017-10-02T22:58:11.828612: step 2059, loss 0.021321, acc 1, learning_rate 0.000101083
2017-10-02T22:58:12.977263: step 2060, loss 0.0529447, acc 0.984375, learning_rate 0.000101078
2017-10-02T22:58:14.154727: step 2061, loss 0.0175503, acc 1, learning_rate 0.000101074
2017-10-02T22:58:15.325975: step 2062, loss 0.01863, acc 1, learning_rate 0.00010107
2017-10-02T22:58:16.484198: step 2063, loss 0.0203225, acc 1, learning_rate 0.000101065
2017-10-02T22:58:17.632243: step 2064, loss 0.0233568, acc 1, learning_rate 0.000101061
2017-10-02T22:58:18.771328: step 2065, loss 0.036179, acc 0.984375, learning_rate 0.000101057
2017-10-02T22:58:19.918689: step 2066, loss 0.0134925, acc 1, learning_rate 0.000101052
2017-10-02T22:58:21.062939: step 2067, loss 0.0596854, acc 0.984375, learning_rate 0.000101048
2017-10-02T22:58:22.212958: step 2068, loss 0.0227329, acc 1, learning_rate 0.000101044
2017-10-02T22:58:23.375293: step 2069, loss 0.0123785, acc 1, learning_rate 0.000101039
2017-10-02T22:58:24.529192: step 2070, loss 0.0183238, acc 1, learning_rate 0.000101035
2017-10-02T22:58:25.682157: step 2071, loss 0.0315793, acc 1, learning_rate 0.000101031
2017-10-02T22:58:26.823699: step 2072, loss 0.111171, acc 0.96875, learning_rate 0.000101027
2017-10-02T22:58:27.974047: step 2073, loss 0.0227772, acc 1, learning_rate 0.000101023
2017-10-02T22:58:29.131526: step 2074, loss 0.0359267, acc 0.984375, learning_rate 0.000101018
2017-10-02T22:58:30.301316: step 2075, loss 0.0673694, acc 0.984375, learning_rate 0.000101014
2017-10-02T22:58:31.460424: step 2076, loss 0.0163472, acc 1, learning_rate 0.00010101
2017-10-02T22:58:32.611171: step 2077, loss 0.106135, acc 0.953125, learning_rate 0.000101006
2017-10-02T22:58:33.765838: step 2078, loss 0.0533093, acc 0.984375, learning_rate 0.000101002
2017-10-02T22:58:34.918773: step 2079, loss 0.0148125, acc 1, learning_rate 0.000100998
2017-10-02T22:58:36.069530: step 2080, loss 0.0216068, acc 1, learning_rate 0.000100994

Evaluation:
2017-10-02T22:58:36.403128: step 2080, loss 1.44997, acc 0.454676

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2080

2017-10-02T22:58:44.165891: step 2081, loss 0.0283067, acc 0.984375, learning_rate 0.00010099
2017-10-02T22:58:45.331145: step 2082, loss 0.0276221, acc 1, learning_rate 0.000100986
2017-10-02T22:58:46.562581: step 2083, loss 0.0140397, acc 1, learning_rate 0.000100982
2017-10-02T22:58:47.713661: step 2084, loss 0.0166581, acc 1, learning_rate 0.000100978
2017-10-02T22:58:48.868094: step 2085, loss 0.0181659, acc 1, learning_rate 0.000100974
2017-10-02T22:58:50.017429: step 2086, loss 0.0714522, acc 0.96875, learning_rate 0.00010097
2017-10-02T22:58:51.168364: step 2087, loss 0.0459392, acc 0.984375, learning_rate 0.000100966
2017-10-02T22:58:52.321368: step 2088, loss 0.0389784, acc 0.984375, learning_rate 0.000100962
2017-10-02T22:58:53.479559: step 2089, loss 0.022699, acc 1, learning_rate 0.000100958
2017-10-02T22:58:54.628104: step 2090, loss 0.0471467, acc 0.984375, learning_rate 0.000100954
2017-10-02T22:58:55.787639: step 2091, loss 0.0145091, acc 1, learning_rate 0.00010095
2017-10-02T22:58:56.957308: step 2092, loss 0.0100385, acc 1, learning_rate 0.000100946
2017-10-02T22:58:58.105819: step 2093, loss 0.0265134, acc 1, learning_rate 0.000100942
2017-10-02T22:58:59.277958: step 2094, loss 0.0213682, acc 1, learning_rate 0.000100938
2017-10-02T22:59:00.440495: step 2095, loss 0.0272855, acc 1, learning_rate 0.000100935
2017-10-02T22:59:01.586162: step 2096, loss 0.0211413, acc 1, learning_rate 0.000100931
2017-10-02T22:59:02.929886: step 2097, loss 0.0144254, acc 1, learning_rate 0.000100927
2017-10-02T22:59:04.079999: step 2098, loss 0.026937, acc 1, learning_rate 0.000100923
2017-10-02T22:59:05.234021: step 2099, loss 0.0315007, acc 1, learning_rate 0.000100919
2017-10-02T22:59:06.393769: step 2100, loss 0.019756, acc 1, learning_rate 0.000100916
2017-10-02T22:59:07.544554: step 2101, loss 0.0221511, acc 1, learning_rate 0.000100912
2017-10-02T22:59:08.696095: step 2102, loss 0.0340402, acc 0.984375, learning_rate 0.000100908
2017-10-02T22:59:09.842458: step 2103, loss 0.0523446, acc 0.984375, learning_rate 0.000100904
2017-10-02T22:59:10.993756: step 2104, loss 0.0357602, acc 1, learning_rate 0.000100901
2017-10-02T22:59:12.147524: step 2105, loss 0.0461557, acc 1, learning_rate 0.000100897
2017-10-02T22:59:13.304659: step 2106, loss 0.0333168, acc 0.984375, learning_rate 0.000100893
2017-10-02T22:59:14.446398: step 2107, loss 0.0237335, acc 1, learning_rate 0.00010089
2017-10-02T22:59:15.599705: step 2108, loss 0.0146325, acc 1, learning_rate 0.000100886
2017-10-02T22:59:16.755367: step 2109, loss 0.0165963, acc 1, learning_rate 0.000100883
2017-10-02T22:59:17.902719: step 2110, loss 0.0114557, acc 1, learning_rate 0.000100879
2017-10-02T22:59:19.092767: step 2111, loss 0.0155269, acc 1, learning_rate 0.000100875
2017-10-02T22:59:20.268461: step 2112, loss 0.0160183, acc 1, learning_rate 0.000100872
2017-10-02T22:59:21.438648: step 2113, loss 0.0209146, acc 1, learning_rate 0.000100868
2017-10-02T22:59:22.595470: step 2114, loss 0.0197375, acc 1, learning_rate 0.000100865
2017-10-02T22:59:23.741003: step 2115, loss 0.0155283, acc 1, learning_rate 0.000100861
2017-10-02T22:59:24.885990: step 2116, loss 0.0237894, acc 1, learning_rate 0.000100858
2017-10-02T22:59:26.038703: step 2117, loss 0.030428, acc 1, learning_rate 0.000100854
2017-10-02T22:59:27.194791: step 2118, loss 0.0216203, acc 1, learning_rate 0.000100851
2017-10-02T22:59:28.362838: step 2119, loss 0.0204367, acc 1, learning_rate 0.000100847
2017-10-02T22:59:29.522247: step 2120, loss 0.0343995, acc 1, learning_rate 0.000100844

Evaluation:
2017-10-02T22:59:29.851644: step 2120, loss 1.43013, acc 0.477698

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2120

2017-10-02T22:59:37.522866: step 2121, loss 0.0164335, acc 1, learning_rate 0.00010084
2017-10-02T22:59:38.758345: step 2122, loss 0.0626039, acc 0.984375, learning_rate 0.000100837
2017-10-02T22:59:39.933182: step 2123, loss 0.0213947, acc 1, learning_rate 0.000100833
2017-10-02T22:59:41.114965: step 2124, loss 0.0362879, acc 0.984375, learning_rate 0.00010083
2017-10-02T22:59:42.286587: step 2125, loss 0.0172798, acc 1, learning_rate 0.000100827
2017-10-02T22:59:43.437492: step 2126, loss 0.0289343, acc 1, learning_rate 0.000100823
2017-10-02T22:59:44.595938: step 2127, loss 0.0256557, acc 1, learning_rate 0.00010082
2017-10-02T22:59:45.751019: step 2128, loss 0.0184156, acc 1, learning_rate 0.000100817
2017-10-02T22:59:46.905394: step 2129, loss 0.0179585, acc 1, learning_rate 0.000100813
2017-10-02T22:59:48.081173: step 2130, loss 0.0349122, acc 1, learning_rate 0.00010081
2017-10-02T22:59:49.244599: step 2131, loss 0.0150322, acc 1, learning_rate 0.000100807
2017-10-02T22:59:50.402374: step 2132, loss 0.0470999, acc 0.984375, learning_rate 0.000100803
2017-10-02T22:59:51.555222: step 2133, loss 0.051812, acc 0.984375, learning_rate 0.0001008
2017-10-02T22:59:52.706058: step 2134, loss 0.0187082, acc 1, learning_rate 0.000100797
2017-10-02T22:59:53.862691: step 2135, loss 0.010087, acc 1, learning_rate 0.000100793
2017-10-02T22:59:55.014911: step 2136, loss 0.0365793, acc 0.984375, learning_rate 0.00010079
2017-10-02T22:59:56.155877: step 2137, loss 0.0192078, acc 1, learning_rate 0.000100787
2017-10-02T22:59:57.315970: step 2138, loss 0.0219546, acc 1, learning_rate 0.000100784
2017-10-02T22:59:58.461037: step 2139, loss 0.038048, acc 1, learning_rate 0.000100781
2017-10-02T22:59:59.613528: step 2140, loss 0.0228809, acc 1, learning_rate 0.000100777
2017-10-02T23:00:00.773772: step 2141, loss 0.0398027, acc 0.984375, learning_rate 0.000100774
2017-10-02T23:00:01.939520: step 2142, loss 0.0254634, acc 1, learning_rate 0.000100771
2017-10-02T23:00:03.100055: step 2143, loss 0.0431606, acc 0.984375, learning_rate 0.000100768
2017-10-02T23:00:04.262003: step 2144, loss 0.0151738, acc 1, learning_rate 0.000100765
2017-10-02T23:00:05.424003: step 2145, loss 0.0392094, acc 0.984375, learning_rate 0.000100762
2017-10-02T23:00:06.581190: step 2146, loss 0.0375189, acc 0.984375, learning_rate 0.000100759
2017-10-02T23:00:07.791418: step 2147, loss 0.0262117, acc 1, learning_rate 0.000100755
2017-10-02T23:00:08.937884: step 2148, loss 0.0196722, acc 1, learning_rate 0.000100752
2017-10-02T23:00:10.098931: step 2149, loss 0.0445566, acc 0.984375, learning_rate 0.000100749
2017-10-02T23:00:11.257637: step 2150, loss 0.067115, acc 0.984375, learning_rate 0.000100746
2017-10-02T23:00:12.416495: step 2151, loss 0.0167415, acc 1, learning_rate 0.000100743
2017-10-02T23:00:13.582200: step 2152, loss 0.0155888, acc 1, learning_rate 0.00010074
2017-10-02T23:00:14.736527: step 2153, loss 0.0388981, acc 0.984375, learning_rate 0.000100737
2017-10-02T23:00:15.914018: step 2154, loss 0.0206718, acc 1, learning_rate 0.000100734
2017-10-02T23:00:17.062730: step 2155, loss 0.03893, acc 1, learning_rate 0.000100731
2017-10-02T23:00:18.283177: step 2156, loss 0.0102732, acc 1, learning_rate 0.000100728
2017-10-02T23:00:19.456431: step 2157, loss 0.0176934, acc 1, learning_rate 0.000100725
2017-10-02T23:00:20.608448: step 2158, loss 0.0182627, acc 1, learning_rate 0.000100722
2017-10-02T23:00:21.773226: step 2159, loss 0.103552, acc 0.96875, learning_rate 0.000100719
2017-10-02T23:00:22.950969: step 2160, loss 0.0377049, acc 0.984375, learning_rate 0.000100716

Evaluation:
2017-10-02T23:00:23.277600: step 2160, loss 1.43701, acc 0.473381

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2160

2017-10-02T23:00:31.250368: step 2161, loss 0.0374261, acc 0.984375, learning_rate 0.000100713
2017-10-02T23:00:32.413505: step 2162, loss 0.016255, acc 1, learning_rate 0.000100711
2017-10-02T23:00:33.559702: step 2163, loss 0.0510258, acc 0.984375, learning_rate 0.000100708
2017-10-02T23:00:34.708561: step 2164, loss 0.0466039, acc 1, learning_rate 0.000100705
2017-10-02T23:00:35.851313: step 2165, loss 0.0362174, acc 0.984375, learning_rate 0.000100702
2017-10-02T23:00:37.023690: step 2166, loss 0.0688707, acc 0.953125, learning_rate 0.000100699
2017-10-02T23:00:38.178525: step 2167, loss 0.0253458, acc 1, learning_rate 0.000100696
2017-10-02T23:00:39.330399: step 2168, loss 0.0215062, acc 1, learning_rate 0.000100693
2017-10-02T23:00:40.477896: step 2169, loss 0.0141918, acc 1, learning_rate 0.00010069
2017-10-02T23:00:41.635940: step 2170, loss 0.0172745, acc 1, learning_rate 0.000100688
2017-10-02T23:00:42.792900: step 2171, loss 0.0180329, acc 1, learning_rate 0.000100685
2017-10-02T23:00:43.939259: step 2172, loss 0.0232403, acc 1, learning_rate 0.000100682
2017-10-02T23:00:45.075499: step 2173, loss 0.0126783, acc 1, learning_rate 0.000100679
2017-10-02T23:00:46.232483: step 2174, loss 0.0192991, acc 1, learning_rate 0.000100677
2017-10-02T23:00:47.370427: step 2175, loss 0.0188213, acc 1, learning_rate 0.000100674
2017-10-02T23:00:48.520932: step 2176, loss 0.0398159, acc 0.984375, learning_rate 0.000100671
2017-10-02T23:00:49.681268: step 2177, loss 0.0116714, acc 1, learning_rate 0.000100668
2017-10-02T23:00:50.830718: step 2178, loss 0.0139911, acc 1, learning_rate 0.000100666
2017-10-02T23:00:51.990401: step 2179, loss 0.0307592, acc 1, learning_rate 0.000100663
2017-10-02T23:00:53.139830: step 2180, loss 0.0380809, acc 0.984375, learning_rate 0.00010066
2017-10-02T23:00:54.297393: step 2181, loss 0.0393024, acc 0.984375, learning_rate 0.000100657
2017-10-02T23:00:55.475052: step 2182, loss 0.026652, acc 1, learning_rate 0.000100655
2017-10-02T23:00:56.631345: step 2183, loss 0.0181076, acc 1, learning_rate 0.000100652
2017-10-02T23:00:57.791382: step 2184, loss 0.073311, acc 0.96875, learning_rate 0.000100649
2017-10-02T23:00:58.951848: step 2185, loss 0.0186203, acc 1, learning_rate 0.000100647
2017-10-02T23:01:00.113341: step 2186, loss 0.0565961, acc 0.96875, learning_rate 0.000100644
2017-10-02T23:01:01.270065: step 2187, loss 0.0172794, acc 1, learning_rate 0.000100641
2017-10-02T23:01:02.413965: step 2188, loss 0.0334127, acc 0.984375, learning_rate 0.000100639
2017-10-02T23:01:03.559537: step 2189, loss 0.0191975, acc 0.984375, learning_rate 0.000100636
2017-10-02T23:01:04.720441: step 2190, loss 0.0154367, acc 1, learning_rate 0.000100634
2017-10-02T23:01:05.913110: step 2191, loss 0.0143721, acc 1, learning_rate 0.000100631
2017-10-02T23:01:07.065844: step 2192, loss 0.0197507, acc 1, learning_rate 0.000100628
2017-10-02T23:01:08.207920: step 2193, loss 0.0235281, acc 1, learning_rate 0.000100626
2017-10-02T23:01:09.363483: step 2194, loss 0.0168312, acc 1, learning_rate 0.000100623
2017-10-02T23:01:10.512689: step 2195, loss 0.0251921, acc 1, learning_rate 0.000100621
2017-10-02T23:01:11.662134: step 2196, loss 0.0181377, acc 1, learning_rate 0.000100618
2017-10-02T23:01:12.809936: step 2197, loss 0.0177742, acc 1, learning_rate 0.000100616
2017-10-02T23:01:13.979731: step 2198, loss 0.066127, acc 0.96875, learning_rate 0.000100613
2017-10-02T23:01:15.166180: step 2199, loss 0.0173949, acc 1, learning_rate 0.000100611
2017-10-02T23:01:16.351015: step 2200, loss 0.0376107, acc 0.984375, learning_rate 0.000100608

Evaluation:
2017-10-02T23:01:16.674391: step 2200, loss 1.43464, acc 0.477698

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2200

2017-10-02T23:01:24.839361: step 2201, loss 0.0276999, acc 1, learning_rate 0.000100606
2017-10-02T23:01:25.998482: step 2202, loss 0.0164813, acc 1, learning_rate 0.000100603
2017-10-02T23:01:27.153398: step 2203, loss 0.017057, acc 1, learning_rate 0.000100601
2017-10-02T23:01:28.314532: step 2204, loss 0.0539824, acc 0.984375, learning_rate 0.000100598
2017-10-02T23:01:29.472490: step 2205, loss 0.0150161, acc 1, learning_rate 0.000100596
2017-10-02T23:01:30.627921: step 2206, loss 0.0108201, acc 1, learning_rate 0.000100594
2017-10-02T23:01:31.782232: step 2207, loss 0.0220761, acc 1, learning_rate 0.000100591
2017-10-02T23:01:32.942943: step 2208, loss 0.026822, acc 1, learning_rate 0.000100589
2017-10-02T23:01:34.108894: step 2209, loss 0.0315334, acc 1, learning_rate 0.000100586
2017-10-02T23:01:35.255304: step 2210, loss 0.0150395, acc 1, learning_rate 0.000100584
2017-10-02T23:01:36.433034: step 2211, loss 0.0204894, acc 1, learning_rate 0.000100581
2017-10-02T23:01:37.598088: step 2212, loss 0.020742, acc 1, learning_rate 0.000100579
2017-10-02T23:01:38.750453: step 2213, loss 0.0228807, acc 1, learning_rate 0.000100577
2017-10-02T23:01:39.896159: step 2214, loss 0.0232315, acc 1, learning_rate 0.000100574
2017-10-02T23:01:41.051467: step 2215, loss 0.0301882, acc 1, learning_rate 0.000100572
2017-10-02T23:01:42.220725: step 2216, loss 0.0217981, acc 1, learning_rate 0.00010057
2017-10-02T23:01:43.405071: step 2217, loss 0.0136496, acc 1, learning_rate 0.000100567
2017-10-02T23:01:44.553505: step 2218, loss 0.011522, acc 1, learning_rate 0.000100565
2017-10-02T23:01:45.700183: step 2219, loss 0.0192771, acc 1, learning_rate 0.000100563
2017-10-02T23:01:46.845086: step 2220, loss 0.018113, acc 1, learning_rate 0.00010056
2017-10-02T23:01:48.001347: step 2221, loss 0.0273113, acc 1, learning_rate 0.000100558
2017-10-02T23:01:49.152665: step 2222, loss 0.0477767, acc 0.984375, learning_rate 0.000100556
2017-10-02T23:01:50.326570: step 2223, loss 0.00928196, acc 1, learning_rate 0.000100554
2017-10-02T23:01:51.481242: step 2224, loss 0.0210612, acc 1, learning_rate 0.000100551
2017-10-02T23:01:52.649840: step 2225, loss 0.0203636, acc 1, learning_rate 0.000100549
2017-10-02T23:01:53.821898: step 2226, loss 0.0220844, acc 1, learning_rate 0.000100547
2017-10-02T23:01:54.974362: step 2227, loss 0.0257914, acc 1, learning_rate 0.000100545
2017-10-02T23:01:56.125246: step 2228, loss 0.0601641, acc 0.984375, learning_rate 0.000100542
2017-10-02T23:01:57.284836: step 2229, loss 0.0194189, acc 1, learning_rate 0.00010054
2017-10-02T23:01:58.488371: step 2230, loss 0.0318774, acc 1, learning_rate 0.000100538
2017-10-02T23:01:59.656635: step 2231, loss 0.0356033, acc 0.984375, learning_rate 0.000100536
2017-10-02T23:02:00.801985: step 2232, loss 0.00995294, acc 1, learning_rate 0.000100534
2017-10-02T23:02:01.959713: step 2233, loss 0.0104156, acc 1, learning_rate 0.000100531
2017-10-02T23:02:03.119331: step 2234, loss 0.0353987, acc 0.984375, learning_rate 0.000100529
2017-10-02T23:02:04.289535: step 2235, loss 0.0104626, acc 1, learning_rate 0.000100527
2017-10-02T23:02:05.438430: step 2236, loss 0.019127, acc 1, learning_rate 0.000100525
2017-10-02T23:02:06.591024: step 2237, loss 0.0155651, acc 1, learning_rate 0.000100523
2017-10-02T23:02:07.753909: step 2238, loss 0.0292729, acc 1, learning_rate 0.000100521
2017-10-02T23:02:08.908812: step 2239, loss 0.0238764, acc 1, learning_rate 0.000100519
2017-10-02T23:02:10.072197: step 2240, loss 0.0238483, acc 1, learning_rate 0.000100516

Evaluation:
2017-10-02T23:02:10.410195: step 2240, loss 1.43738, acc 0.470504

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2240

2017-10-02T23:02:18.504908: step 2241, loss 0.0136436, acc 1, learning_rate 0.000100514
2017-10-02T23:02:19.679335: step 2242, loss 0.0124298, acc 1, learning_rate 0.000100512
2017-10-02T23:02:20.835355: step 2243, loss 0.0217965, acc 1, learning_rate 0.00010051
2017-10-02T23:02:21.980010: step 2244, loss 0.0184688, acc 1, learning_rate 0.000100508
2017-10-02T23:02:23.133885: step 2245, loss 0.0284929, acc 0.984375, learning_rate 0.000100506
2017-10-02T23:02:24.284368: step 2246, loss 0.0130306, acc 1, learning_rate 0.000100504
2017-10-02T23:02:25.436363: step 2247, loss 0.0170828, acc 1, learning_rate 0.000100502
2017-10-02T23:02:26.580272: step 2248, loss 0.0213696, acc 1, learning_rate 0.0001005
2017-10-02T23:02:27.751260: step 2249, loss 0.0166553, acc 1, learning_rate 0.000100498
2017-10-02T23:02:28.909091: step 2250, loss 0.0565888, acc 0.984375, learning_rate 0.000100496
2017-10-02T23:02:30.060520: step 2251, loss 0.0202782, acc 1, learning_rate 0.000100494
2017-10-02T23:02:31.212684: step 2252, loss 0.0305824, acc 1, learning_rate 0.000100492
2017-10-02T23:02:32.365727: step 2253, loss 0.0277772, acc 1, learning_rate 0.00010049
2017-10-02T23:02:33.506587: step 2254, loss 0.0126773, acc 1, learning_rate 0.000100488
2017-10-02T23:02:34.661093: step 2255, loss 0.0319898, acc 1, learning_rate 0.000100486
2017-10-02T23:02:35.804334: step 2256, loss 0.0138869, acc 1, learning_rate 0.000100484
2017-10-02T23:02:36.957281: step 2257, loss 0.0180574, acc 1, learning_rate 0.000100482
2017-10-02T23:02:38.107874: step 2258, loss 0.014666, acc 1, learning_rate 0.00010048
2017-10-02T23:02:39.257180: step 2259, loss 0.0268584, acc 1, learning_rate 0.000100478
2017-10-02T23:02:40.422110: step 2260, loss 0.0215726, acc 1, learning_rate 0.000100476
2017-10-02T23:02:41.577890: step 2261, loss 0.0642706, acc 0.96875, learning_rate 0.000100474
2017-10-02T23:02:42.726040: step 2262, loss 0.0310039, acc 1, learning_rate 0.000100472
2017-10-02T23:02:43.872900: step 2263, loss 0.017356, acc 1, learning_rate 0.00010047
2017-10-02T23:02:45.045020: step 2264, loss 0.0184355, acc 1, learning_rate 0.000100468
2017-10-02T23:02:46.212903: step 2265, loss 0.0152322, acc 1, learning_rate 0.000100466
2017-10-02T23:02:47.367911: step 2266, loss 0.0260088, acc 0.984375, learning_rate 0.000100464
2017-10-02T23:02:48.513735: step 2267, loss 0.0119739, acc 1, learning_rate 0.000100462
2017-10-02T23:02:49.670959: step 2268, loss 0.0426565, acc 0.984375, learning_rate 0.000100461
2017-10-02T23:02:50.839711: step 2269, loss 0.0259316, acc 1, learning_rate 0.000100459
2017-10-02T23:02:51.993002: step 2270, loss 0.0099179, acc 1, learning_rate 0.000100457
2017-10-02T23:02:53.152101: step 2271, loss 0.0119166, acc 1, learning_rate 0.000100455
2017-10-02T23:02:54.307170: step 2272, loss 0.0129376, acc 1, learning_rate 0.000100453
2017-10-02T23:02:55.458581: step 2273, loss 0.0334023, acc 1, learning_rate 0.000100451
2017-10-02T23:02:56.616568: step 2274, loss 0.0264779, acc 1, learning_rate 0.000100449
2017-10-02T23:02:57.760714: step 2275, loss 0.0270941, acc 0.984375, learning_rate 0.000100448
2017-10-02T23:02:58.912187: step 2276, loss 0.0188072, acc 1, learning_rate 0.000100446
2017-10-02T23:03:00.070403: step 2277, loss 0.0258748, acc 1, learning_rate 0.000100444
2017-10-02T23:03:01.216666: step 2278, loss 0.0120313, acc 1, learning_rate 0.000100442
2017-10-02T23:03:02.360126: step 2279, loss 0.0204803, acc 1, learning_rate 0.00010044
2017-10-02T23:03:03.512751: step 2280, loss 0.0184842, acc 1, learning_rate 0.000100439

Evaluation:
2017-10-02T23:03:03.858495: step 2280, loss 1.42853, acc 0.48777

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2280

2017-10-02T23:03:11.239697: step 2281, loss 0.0101668, acc 1, learning_rate 0.000100437
2017-10-02T23:03:12.414656: step 2282, loss 0.0211132, acc 1, learning_rate 0.000100435
2017-10-02T23:03:13.579050: step 2283, loss 0.0417984, acc 0.984375, learning_rate 0.000100433
2017-10-02T23:03:14.927403: step 2284, loss 0.00757856, acc 1, learning_rate 0.000100431
2017-10-02T23:03:16.072372: step 2285, loss 0.0463772, acc 0.984375, learning_rate 0.00010043
2017-10-02T23:03:17.258088: step 2286, loss 0.0303969, acc 1, learning_rate 0.000100428
2017-10-02T23:03:18.420862: step 2287, loss 0.0261829, acc 1, learning_rate 0.000100426
2017-10-02T23:03:19.564151: step 2288, loss 0.0119677, acc 1, learning_rate 0.000100424
2017-10-02T23:03:20.713384: step 2289, loss 0.0174453, acc 1, learning_rate 0.000100423
2017-10-02T23:03:21.875116: step 2290, loss 0.0141081, acc 1, learning_rate 0.000100421
2017-10-02T23:03:23.025447: step 2291, loss 0.023516, acc 1, learning_rate 0.000100419
2017-10-02T23:03:24.181892: step 2292, loss 0.0109111, acc 1, learning_rate 0.000100418
2017-10-02T23:03:25.330430: step 2293, loss 0.0153066, acc 1, learning_rate 0.000100416
2017-10-02T23:03:26.482582: step 2294, loss 0.0137633, acc 1, learning_rate 0.000100414
2017-10-02T23:03:27.631482: step 2295, loss 0.0259249, acc 0.984375, learning_rate 0.000100412
2017-10-02T23:03:28.806929: step 2296, loss 0.0112268, acc 1, learning_rate 0.000100411
2017-10-02T23:03:29.953159: step 2297, loss 0.0109343, acc 1, learning_rate 0.000100409
2017-10-02T23:03:31.106683: step 2298, loss 0.0166929, acc 1, learning_rate 0.000100407
2017-10-02T23:03:32.260823: step 2299, loss 0.0130203, acc 1, learning_rate 0.000100406
2017-10-02T23:03:33.418663: step 2300, loss 0.0306678, acc 1, learning_rate 0.000100404
2017-10-02T23:03:34.582602: step 2301, loss 0.0261917, acc 1, learning_rate 0.000100402
2017-10-02T23:03:35.721949: step 2302, loss 0.0326847, acc 1, learning_rate 0.000100401
2017-10-02T23:03:36.862255: step 2303, loss 0.0163608, acc 1, learning_rate 0.000100399
2017-10-02T23:03:38.012145: step 2304, loss 0.0226574, acc 1, learning_rate 0.000100398
2017-10-02T23:03:39.167117: step 2305, loss 0.0255729, acc 1, learning_rate 0.000100396
2017-10-02T23:03:40.309388: step 2306, loss 0.0260437, acc 0.984375, learning_rate 0.000100394
2017-10-02T23:03:41.467530: step 2307, loss 0.0291119, acc 1, learning_rate 0.000100393
2017-10-02T23:03:42.625335: step 2308, loss 0.0183889, acc 1, learning_rate 0.000100391
2017-10-02T23:03:43.799735: step 2309, loss 0.0229717, acc 1, learning_rate 0.000100389
2017-10-02T23:03:44.942753: step 2310, loss 0.0306942, acc 0.984375, learning_rate 0.000100388
2017-10-02T23:03:46.107146: step 2311, loss 0.0180656, acc 1, learning_rate 0.000100386
2017-10-02T23:03:47.269872: step 2312, loss 0.0334704, acc 0.984375, learning_rate 0.000100385
2017-10-02T23:03:48.429466: step 2313, loss 0.0258563, acc 0.984375, learning_rate 0.000100383
2017-10-02T23:03:49.599705: step 2314, loss 0.0119454, acc 1, learning_rate 0.000100382
2017-10-02T23:03:50.753510: step 2315, loss 0.0222032, acc 1, learning_rate 0.00010038
2017-10-02T23:03:51.914973: step 2316, loss 0.0351545, acc 0.984375, learning_rate 0.000100378
2017-10-02T23:03:53.063049: step 2317, loss 0.0232083, acc 1, learning_rate 0.000100377
2017-10-02T23:03:54.205903: step 2318, loss 0.013267, acc 1, learning_rate 0.000100375
2017-10-02T23:03:55.355961: step 2319, loss 0.0340353, acc 1, learning_rate 0.000100374
2017-10-02T23:03:56.522207: step 2320, loss 0.0179189, acc 1, learning_rate 0.000100372

Evaluation:
2017-10-02T23:03:56.848540: step 2320, loss 1.43159, acc 0.479137

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2320

2017-10-02T23:04:04.443199: step 2321, loss 0.0142978, acc 1, learning_rate 0.000100371
2017-10-02T23:04:05.607415: step 2322, loss 0.0313147, acc 0.984375, learning_rate 0.000100369
2017-10-02T23:04:06.773520: step 2323, loss 0.0297411, acc 0.984375, learning_rate 0.000100368
2017-10-02T23:04:07.918835: step 2324, loss 0.0248902, acc 0.984375, learning_rate 0.000100366
2017-10-02T23:04:09.060967: step 2325, loss 0.0133933, acc 1, learning_rate 0.000100365
2017-10-02T23:04:10.217274: step 2326, loss 0.00870219, acc 1, learning_rate 0.000100363
2017-10-02T23:04:11.359973: step 2327, loss 0.0147748, acc 1, learning_rate 0.000100362
2017-10-02T23:04:12.516704: step 2328, loss 0.0484247, acc 0.984375, learning_rate 0.00010036
2017-10-02T23:04:13.675943: step 2329, loss 0.0242613, acc 1, learning_rate 0.000100359
2017-10-02T23:04:14.832854: step 2330, loss 0.0479833, acc 0.984375, learning_rate 0.000100357
2017-10-02T23:04:15.971886: step 2331, loss 0.0248816, acc 1, learning_rate 0.000100356
2017-10-02T23:04:17.122611: step 2332, loss 0.020113, acc 1, learning_rate 0.000100354
2017-10-02T23:04:18.292276: step 2333, loss 0.0390676, acc 1, learning_rate 0.000100353
2017-10-02T23:04:19.460772: step 2334, loss 0.0152511, acc 1, learning_rate 0.000100352
2017-10-02T23:04:20.614150: step 2335, loss 0.0174487, acc 1, learning_rate 0.00010035
2017-10-02T23:04:21.765216: step 2336, loss 0.0403304, acc 0.984375, learning_rate 0.000100349
2017-10-02T23:04:22.923027: step 2337, loss 0.0327189, acc 0.984375, learning_rate 0.000100347
2017-10-02T23:04:24.077214: step 2338, loss 0.0101162, acc 1, learning_rate 0.000100346
2017-10-02T23:04:25.235065: step 2339, loss 0.0162117, acc 1, learning_rate 0.000100344
2017-10-02T23:04:26.401987: step 2340, loss 0.0374026, acc 0.984375, learning_rate 0.000100343
2017-10-02T23:04:27.564260: step 2341, loss 0.0140024, acc 1, learning_rate 0.000100342
2017-10-02T23:04:28.725510: step 2342, loss 0.033544, acc 0.984375, learning_rate 0.00010034
2017-10-02T23:04:29.906457: step 2343, loss 0.0326923, acc 0.984375, learning_rate 0.000100339
2017-10-02T23:04:31.065354: step 2344, loss 0.0157455, acc 1, learning_rate 0.000100338
2017-10-02T23:04:32.216861: step 2345, loss 0.0166924, acc 1, learning_rate 0.000100336
2017-10-02T23:04:33.364254: step 2346, loss 0.0129695, acc 1, learning_rate 0.000100335
2017-10-02T23:04:34.523706: step 2347, loss 0.0180211, acc 1, learning_rate 0.000100333
2017-10-02T23:04:35.673070: step 2348, loss 0.0244156, acc 1, learning_rate 0.000100332
2017-10-02T23:04:36.823798: step 2349, loss 0.0488274, acc 0.984375, learning_rate 0.000100331
2017-10-02T23:04:37.987547: step 2350, loss 0.0177485, acc 1, learning_rate 0.000100329
2017-10-02T23:04:39.142212: step 2351, loss 0.0358508, acc 1, learning_rate 0.000100328
2017-10-02T23:04:40.277024: step 2352, loss 0.0120267, acc 1, learning_rate 0.000100327
2017-10-02T23:04:41.426571: step 2353, loss 0.0289904, acc 1, learning_rate 0.000100325
2017-10-02T23:04:42.575436: step 2354, loss 0.0171358, acc 1, learning_rate 0.000100324
2017-10-02T23:04:43.721985: step 2355, loss 0.0148306, acc 1, learning_rate 0.000100323
2017-10-02T23:04:44.863815: step 2356, loss 0.0145324, acc 1, learning_rate 0.000100321
2017-10-02T23:04:46.000730: step 2357, loss 0.0417614, acc 0.984375, learning_rate 0.00010032
2017-10-02T23:04:47.141332: step 2358, loss 0.0269992, acc 1, learning_rate 0.000100319
2017-10-02T23:04:48.298208: step 2359, loss 0.0136607, acc 1, learning_rate 0.000100317
2017-10-02T23:04:49.451340: step 2360, loss 0.0166102, acc 1, learning_rate 0.000100316

Evaluation:
2017-10-02T23:04:49.808553: step 2360, loss 1.45364, acc 0.438849

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2360

2017-10-02T23:04:57.455389: step 2361, loss 0.0622343, acc 0.984375, learning_rate 0.000100315
2017-10-02T23:04:58.629010: step 2362, loss 0.0163126, acc 1, learning_rate 0.000100314
2017-10-02T23:04:59.776934: step 2363, loss 0.0451698, acc 0.984375, learning_rate 0.000100312
2017-10-02T23:05:00.945521: step 2364, loss 0.0219912, acc 1, learning_rate 0.000100311
2017-10-02T23:05:02.096316: step 2365, loss 0.0388569, acc 0.984375, learning_rate 0.00010031
2017-10-02T23:05:03.249648: step 2366, loss 0.0538935, acc 0.96875, learning_rate 0.000100308
2017-10-02T23:05:04.400967: step 2367, loss 0.0103853, acc 1, learning_rate 0.000100307
2017-10-02T23:05:05.555313: step 2368, loss 0.0463002, acc 0.984375, learning_rate 0.000100306
2017-10-02T23:05:06.707818: step 2369, loss 0.0284918, acc 1, learning_rate 0.000100305
2017-10-02T23:05:07.876152: step 2370, loss 0.103778, acc 0.96875, learning_rate 0.000100303
2017-10-02T23:05:09.028662: step 2371, loss 0.0192594, acc 1, learning_rate 0.000100302
2017-10-02T23:05:10.186600: step 2372, loss 0.0585007, acc 0.984375, learning_rate 0.000100301
2017-10-02T23:05:11.347450: step 2373, loss 0.0309813, acc 0.984375, learning_rate 0.0001003
2017-10-02T23:05:12.508186: step 2374, loss 0.0187163, acc 1, learning_rate 0.000100299
2017-10-02T23:05:13.675889: step 2375, loss 0.0104306, acc 1, learning_rate 0.000100297
2017-10-02T23:05:14.814790: step 2376, loss 0.0179439, acc 1, learning_rate 0.000100296
2017-10-02T23:05:15.956910: step 2377, loss 0.0723774, acc 0.984375, learning_rate 0.000100295
2017-10-02T23:05:17.167808: step 2378, loss 0.0132173, acc 1, learning_rate 0.000100294
2017-10-02T23:05:18.330472: step 2379, loss 0.0194317, acc 1, learning_rate 0.000100292
2017-10-02T23:05:19.482580: step 2380, loss 0.0171186, acc 1, learning_rate 0.000100291
2017-10-02T23:05:20.640470: step 2381, loss 0.0161618, acc 1, learning_rate 0.00010029
2017-10-02T23:05:21.779429: step 2382, loss 0.0302907, acc 0.984375, learning_rate 0.000100289
2017-10-02T23:05:22.922945: step 2383, loss 0.0174385, acc 1, learning_rate 0.000100288
2017-10-02T23:05:24.072644: step 2384, loss 0.0214031, acc 1, learning_rate 0.000100287
2017-10-02T23:05:25.222801: step 2385, loss 0.0264938, acc 1, learning_rate 0.000100285
2017-10-02T23:05:26.368542: step 2386, loss 0.0445241, acc 0.984375, learning_rate 0.000100284
2017-10-02T23:05:27.533301: step 2387, loss 0.0405572, acc 0.984375, learning_rate 0.000100283
2017-10-02T23:05:28.716122: step 2388, loss 0.0138303, acc 1, learning_rate 0.000100282
2017-10-02T23:05:29.869577: step 2389, loss 0.0158253, acc 1, learning_rate 0.000100281
2017-10-02T23:05:31.024760: step 2390, loss 0.0219719, acc 1, learning_rate 0.00010028
2017-10-02T23:05:32.179673: step 2391, loss 0.0191742, acc 1, learning_rate 0.000100278
2017-10-02T23:05:33.342779: step 2392, loss 0.0397272, acc 0.984375, learning_rate 0.000100277
2017-10-02T23:05:34.494764: step 2393, loss 0.0158756, acc 1, learning_rate 0.000100276
2017-10-02T23:05:35.660386: step 2394, loss 0.0294181, acc 0.984375, learning_rate 0.000100275
2017-10-02T23:05:36.822824: step 2395, loss 0.0275272, acc 1, learning_rate 0.000100274
2017-10-02T23:05:37.984558: step 2396, loss 0.00930461, acc 1, learning_rate 0.000100273
2017-10-02T23:05:39.133297: step 2397, loss 0.028267, acc 1, learning_rate 0.000100272
2017-10-02T23:05:40.282139: step 2398, loss 0.0149811, acc 1, learning_rate 0.000100271
2017-10-02T23:05:41.427430: step 2399, loss 0.0194377, acc 1, learning_rate 0.00010027
2017-10-02T23:05:42.583501: step 2400, loss 0.0271185, acc 1, learning_rate 0.000100268

Evaluation:
2017-10-02T23:05:42.913319: step 2400, loss 1.43169, acc 0.492086

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2400

2017-10-02T23:05:51.236683: step 2401, loss 0.0737445, acc 0.984375, learning_rate 0.000100267
2017-10-02T23:05:52.400291: step 2402, loss 0.0163536, acc 1, learning_rate 0.000100266
2017-10-02T23:05:53.551103: step 2403, loss 0.0173623, acc 1, learning_rate 0.000100265
2017-10-02T23:05:54.712844: step 2404, loss 0.0175683, acc 1, learning_rate 0.000100264
2017-10-02T23:05:55.946292: step 2405, loss 0.0130661, acc 1, learning_rate 0.000100263
2017-10-02T23:05:57.105556: step 2406, loss 0.0165134, acc 1, learning_rate 0.000100262
2017-10-02T23:05:58.276710: step 2407, loss 0.0221358, acc 1, learning_rate 0.000100261
2017-10-02T23:05:59.432534: step 2408, loss 0.0152745, acc 1, learning_rate 0.00010026
2017-10-02T23:06:00.579718: step 2409, loss 0.0115354, acc 1, learning_rate 0.000100259
2017-10-02T23:06:01.737194: step 2410, loss 0.0123797, acc 1, learning_rate 0.000100258
2017-10-02T23:06:02.883816: step 2411, loss 0.0304875, acc 1, learning_rate 0.000100257
2017-10-02T23:06:04.033750: step 2412, loss 0.0506732, acc 0.984375, learning_rate 0.000100256
2017-10-02T23:06:05.187516: step 2413, loss 0.0107614, acc 1, learning_rate 0.000100255
2017-10-02T23:06:06.335993: step 2414, loss 0.0148818, acc 1, learning_rate 0.000100253
2017-10-02T23:06:07.485214: step 2415, loss 0.0160568, acc 1, learning_rate 0.000100252
2017-10-02T23:06:08.646728: step 2416, loss 0.0134991, acc 1, learning_rate 0.000100251
2017-10-02T23:06:09.807360: step 2417, loss 0.0160988, acc 1, learning_rate 0.00010025
2017-10-02T23:06:10.969425: step 2418, loss 0.0152064, acc 1, learning_rate 0.000100249
2017-10-02T23:06:12.142236: step 2419, loss 0.0295861, acc 1, learning_rate 0.000100248
2017-10-02T23:06:13.299292: step 2420, loss 0.0362591, acc 0.984375, learning_rate 0.000100247
2017-10-02T23:06:14.461209: step 2421, loss 0.0473453, acc 0.984375, learning_rate 0.000100246
2017-10-02T23:06:15.624361: step 2422, loss 0.0388309, acc 0.984375, learning_rate 0.000100245
2017-10-02T23:06:16.782993: step 2423, loss 0.0342186, acc 1, learning_rate 0.000100244
2017-10-02T23:06:17.943261: step 2424, loss 0.0199766, acc 1, learning_rate 0.000100243
2017-10-02T23:06:19.085819: step 2425, loss 0.0166233, acc 1, learning_rate 0.000100242
2017-10-02T23:06:20.226608: step 2426, loss 0.0338234, acc 1, learning_rate 0.000100241
2017-10-02T23:06:21.396250: step 2427, loss 0.0301977, acc 0.984375, learning_rate 0.00010024
2017-10-02T23:06:22.558528: step 2428, loss 0.0583305, acc 0.984375, learning_rate 0.000100239
2017-10-02T23:06:23.711536: step 2429, loss 0.038623, acc 0.984375, learning_rate 0.000100238
2017-10-02T23:06:24.853605: step 2430, loss 0.0179099, acc 1, learning_rate 0.000100237
2017-10-02T23:06:26.005165: step 2431, loss 0.0170582, acc 1, learning_rate 0.000100236
2017-10-02T23:06:27.163492: step 2432, loss 0.0108674, acc 1, learning_rate 0.000100235
2017-10-02T23:06:28.321961: step 2433, loss 0.0155543, acc 1, learning_rate 0.000100235
2017-10-02T23:06:29.485497: step 2434, loss 0.0449959, acc 0.984375, learning_rate 0.000100234
2017-10-02T23:06:30.645722: step 2435, loss 0.0344316, acc 1, learning_rate 0.000100233
2017-10-02T23:06:31.791146: step 2436, loss 0.0587915, acc 0.984375, learning_rate 0.000100232
2017-10-02T23:06:32.936821: step 2437, loss 0.0135521, acc 1, learning_rate 0.000100231
2017-10-02T23:06:34.079585: step 2438, loss 0.0327258, acc 0.984375, learning_rate 0.00010023
2017-10-02T23:06:35.233938: step 2439, loss 0.0161943, acc 1, learning_rate 0.000100229
2017-10-02T23:06:36.397219: step 2440, loss 0.0121667, acc 1, learning_rate 0.000100228

Evaluation:
2017-10-02T23:06:36.734905: step 2440, loss 1.44996, acc 0.46187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2440

2017-10-02T23:06:43.409083: step 2441, loss 0.0147329, acc 1, learning_rate 0.000100227
2017-10-02T23:06:44.550486: step 2442, loss 0.0279402, acc 1, learning_rate 0.000100226
2017-10-02T23:06:45.712566: step 2443, loss 0.0387929, acc 0.984375, learning_rate 0.000100225
2017-10-02T23:06:46.872354: step 2444, loss 0.0262897, acc 1, learning_rate 0.000100224
2017-10-02T23:06:48.010804: step 2445, loss 0.012641, acc 1, learning_rate 0.000100223
2017-10-02T23:06:49.184946: step 2446, loss 0.0118337, acc 1, learning_rate 0.000100222
2017-10-02T23:06:50.358056: step 2447, loss 0.0094348, acc 1, learning_rate 0.000100221
2017-10-02T23:06:51.498661: step 2448, loss 0.0213967, acc 1, learning_rate 0.000100221
2017-10-02T23:06:52.639045: step 2449, loss 0.0171683, acc 1, learning_rate 0.00010022
2017-10-02T23:06:53.776760: step 2450, loss 0.015069, acc 1, learning_rate 0.000100219
2017-10-02T23:06:54.943362: step 2451, loss 0.0306908, acc 1, learning_rate 0.000100218
2017-10-02T23:06:56.090683: step 2452, loss 0.0288965, acc 0.984375, learning_rate 0.000100217
2017-10-02T23:06:57.243546: step 2453, loss 0.0184455, acc 1, learning_rate 0.000100216
2017-10-02T23:06:58.388996: step 2454, loss 0.0303132, acc 1, learning_rate 0.000100215
2017-10-02T23:06:59.541080: step 2455, loss 0.01572, acc 1, learning_rate 0.000100214
2017-10-02T23:07:00.708414: step 2456, loss 0.0176014, acc 1, learning_rate 0.000100213
2017-10-02T23:07:01.878076: step 2457, loss 0.013063, acc 1, learning_rate 0.000100213
2017-10-02T23:07:03.020115: step 2458, loss 0.0252153, acc 1, learning_rate 0.000100212
2017-10-02T23:07:04.164283: step 2459, loss 0.042621, acc 0.984375, learning_rate 0.000100211
2017-10-02T23:07:05.317942: step 2460, loss 0.0316106, acc 1, learning_rate 0.00010021
2017-10-02T23:07:06.471235: step 2461, loss 0.0199027, acc 1, learning_rate 0.000100209
2017-10-02T23:07:07.670187: step 2462, loss 0.0315943, acc 1, learning_rate 0.000100208
2017-10-02T23:07:08.822105: step 2463, loss 0.0200018, acc 1, learning_rate 0.000100207
2017-10-02T23:07:09.974769: step 2464, loss 0.0668562, acc 0.984375, learning_rate 0.000100207
2017-10-02T23:07:11.111080: step 2465, loss 0.0144694, acc 1, learning_rate 0.000100206
2017-10-02T23:07:12.257862: step 2466, loss 0.0739163, acc 0.984375, learning_rate 0.000100205
2017-10-02T23:07:13.426806: step 2467, loss 0.0161534, acc 1, learning_rate 0.000100204
2017-10-02T23:07:14.576849: step 2468, loss 0.0442191, acc 0.984375, learning_rate 0.000100203
2017-10-02T23:07:15.723173: step 2469, loss 0.0691337, acc 0.984375, learning_rate 0.000100202
2017-10-02T23:07:16.896150: step 2470, loss 0.0132992, acc 1, learning_rate 0.000100202
2017-10-02T23:07:18.052393: step 2471, loss 0.00951033, acc 1, learning_rate 0.000100201
2017-10-02T23:07:19.207791: step 2472, loss 0.0168781, acc 1, learning_rate 0.0001002
2017-10-02T23:07:20.376020: step 2473, loss 0.0166405, acc 1, learning_rate 0.000100199
2017-10-02T23:07:21.527409: step 2474, loss 0.0132641, acc 1, learning_rate 0.000100198
2017-10-02T23:07:22.681211: step 2475, loss 0.0139704, acc 1, learning_rate 0.000100198
2017-10-02T23:07:23.826962: step 2476, loss 0.0176609, acc 1, learning_rate 0.000100197
2017-10-02T23:07:24.970839: step 2477, loss 0.0173587, acc 1, learning_rate 0.000100196
2017-10-02T23:07:26.120317: step 2478, loss 0.0373478, acc 0.984375, learning_rate 0.000100195
2017-10-02T23:07:27.285089: step 2479, loss 0.0519528, acc 0.984375, learning_rate 0.000100194
2017-10-02T23:07:28.451843: step 2480, loss 0.0225782, acc 1, learning_rate 0.000100194

Evaluation:
2017-10-02T23:07:28.780683: step 2480, loss 1.4408, acc 0.473381

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2480

2017-10-02T23:07:35.838131: step 2481, loss 0.0415401, acc 0.984375, learning_rate 0.000100193
2017-10-02T23:07:37.015936: step 2482, loss 0.0217438, acc 0.984375, learning_rate 0.000100192
2017-10-02T23:07:38.184488: step 2483, loss 0.0510862, acc 0.984375, learning_rate 0.000100191
2017-10-02T23:07:39.334017: step 2484, loss 0.0242167, acc 0.984375, learning_rate 0.00010019
2017-10-02T23:07:40.499582: step 2485, loss 0.0598244, acc 0.984375, learning_rate 0.00010019
2017-10-02T23:07:41.672352: step 2486, loss 0.0136574, acc 1, learning_rate 0.000100189
2017-10-02T23:07:42.820013: step 2487, loss 0.0136328, acc 1, learning_rate 0.000100188
2017-10-02T23:07:43.972566: step 2488, loss 0.0166805, acc 1, learning_rate 0.000100187
2017-10-02T23:07:45.132766: step 2489, loss 0.0149777, acc 1, learning_rate 0.000100187
2017-10-02T23:07:46.281389: step 2490, loss 0.0245441, acc 1, learning_rate 0.000100186
2017-10-02T23:07:47.448162: step 2491, loss 0.0195563, acc 1, learning_rate 0.000100185
2017-10-02T23:07:48.596181: step 2492, loss 0.0239344, acc 1, learning_rate 0.000100184
2017-10-02T23:07:49.832074: step 2493, loss 0.0221346, acc 1, learning_rate 0.000100183
2017-10-02T23:07:50.986777: step 2494, loss 0.0262709, acc 1, learning_rate 0.000100183
2017-10-02T23:07:52.130490: step 2495, loss 0.0231122, acc 1, learning_rate 0.000100182
2017-10-02T23:07:53.270780: step 2496, loss 0.0137916, acc 1, learning_rate 0.000100181
2017-10-02T23:07:54.437858: step 2497, loss 0.0076622, acc 1, learning_rate 0.000100181
2017-10-02T23:07:55.592652: step 2498, loss 0.0131547, acc 1, learning_rate 0.00010018
2017-10-02T23:07:56.799282: step 2499, loss 0.0268865, acc 0.984375, learning_rate 0.000100179
2017-10-02T23:07:57.961116: step 2500, loss 0.0123287, acc 1, learning_rate 0.000100178
2017-10-02T23:07:59.115388: step 2501, loss 0.0315732, acc 0.984375, learning_rate 0.000100178
2017-10-02T23:08:00.294855: step 2502, loss 0.0256978, acc 1, learning_rate 0.000100177
2017-10-02T23:08:01.450462: step 2503, loss 0.0183249, acc 1, learning_rate 0.000100176
2017-10-02T23:08:02.619475: step 2504, loss 0.0340018, acc 0.984375, learning_rate 0.000100175
2017-10-02T23:08:03.776800: step 2505, loss 0.017501, acc 1, learning_rate 0.000100175
2017-10-02T23:08:04.924323: step 2506, loss 0.0176602, acc 1, learning_rate 0.000100174
2017-10-02T23:08:06.090621: step 2507, loss 0.0618773, acc 0.984375, learning_rate 0.000100173
2017-10-02T23:08:07.244921: step 2508, loss 0.0307374, acc 0.984375, learning_rate 0.000100173
2017-10-02T23:08:08.421749: step 2509, loss 0.0178448, acc 1, learning_rate 0.000100172
2017-10-02T23:08:09.575001: step 2510, loss 0.029892, acc 0.984375, learning_rate 0.000100171
2017-10-02T23:08:10.909301: step 2511, loss 0.0335249, acc 0.984375, learning_rate 0.00010017
2017-10-02T23:08:12.053632: step 2512, loss 0.0139089, acc 1, learning_rate 0.00010017
2017-10-02T23:08:13.233263: step 2513, loss 0.0499597, acc 0.984375, learning_rate 0.000100169
2017-10-02T23:08:14.416177: step 2514, loss 0.0418067, acc 1, learning_rate 0.000100168
2017-10-02T23:08:15.576582: step 2515, loss 0.0423547, acc 0.984375, learning_rate 0.000100168
2017-10-02T23:08:16.724144: step 2516, loss 0.0224377, acc 1, learning_rate 0.000100167
2017-10-02T23:08:17.864134: step 2517, loss 0.0158237, acc 1, learning_rate 0.000100166
2017-10-02T23:08:19.019488: step 2518, loss 0.0364339, acc 0.984375, learning_rate 0.000100166
2017-10-02T23:08:20.168545: step 2519, loss 0.0540204, acc 0.984375, learning_rate 0.000100165
2017-10-02T23:08:21.336229: step 2520, loss 0.0190562, acc 1, learning_rate 0.000100164

Evaluation:
2017-10-02T23:08:21.704258: step 2520, loss 1.43207, acc 0.467626

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2520

2017-10-02T23:08:29.656780: step 2521, loss 0.0144954, acc 1, learning_rate 0.000100164
2017-10-02T23:08:30.844265: step 2522, loss 0.0206546, acc 1, learning_rate 0.000100163
2017-10-02T23:08:31.986344: step 2523, loss 0.0204451, acc 1, learning_rate 0.000100162
2017-10-02T23:08:33.129734: step 2524, loss 0.0328625, acc 0.984375, learning_rate 0.000100162
2017-10-02T23:08:34.287415: step 2525, loss 0.0374216, acc 0.984375, learning_rate 0.000100161
2017-10-02T23:08:35.440574: step 2526, loss 0.0481649, acc 0.96875, learning_rate 0.00010016
2017-10-02T23:08:36.595278: step 2527, loss 0.070182, acc 0.984375, learning_rate 0.00010016
2017-10-02T23:08:37.764502: step 2528, loss 0.0154662, acc 1, learning_rate 0.000100159
2017-10-02T23:08:38.940012: step 2529, loss 0.027387, acc 1, learning_rate 0.000100158
2017-10-02T23:08:40.086760: step 2530, loss 0.00711127, acc 1, learning_rate 0.000100158
2017-10-02T23:08:41.233922: step 2531, loss 0.011264, acc 1, learning_rate 0.000100157
2017-10-02T23:08:42.383955: step 2532, loss 0.00991104, acc 1, learning_rate 0.000100156
2017-10-02T23:08:43.526581: step 2533, loss 0.0185077, acc 1, learning_rate 0.000100156
2017-10-02T23:08:44.692911: step 2534, loss 0.0141983, acc 1, learning_rate 0.000100155
2017-10-02T23:08:45.840193: step 2535, loss 0.00974362, acc 1, learning_rate 0.000100155
2017-10-02T23:08:47.009290: step 2536, loss 0.0122189, acc 1, learning_rate 0.000100154
2017-10-02T23:08:48.171471: step 2537, loss 0.0242005, acc 1, learning_rate 0.000100153
2017-10-02T23:08:49.340955: step 2538, loss 0.0234459, acc 1, learning_rate 0.000100153
2017-10-02T23:08:50.519496: step 2539, loss 0.0112159, acc 1, learning_rate 0.000100152
2017-10-02T23:08:51.673450: step 2540, loss 0.0230901, acc 1, learning_rate 0.000100151
2017-10-02T23:08:52.828507: step 2541, loss 0.0498078, acc 0.984375, learning_rate 0.000100151
2017-10-02T23:08:53.989293: step 2542, loss 0.0135612, acc 1, learning_rate 0.00010015
2017-10-02T23:08:55.139670: step 2543, loss 0.0444375, acc 0.984375, learning_rate 0.00010015
2017-10-02T23:08:56.294177: step 2544, loss 0.0185581, acc 1, learning_rate 0.000100149
2017-10-02T23:08:57.455384: step 2545, loss 0.0118836, acc 1, learning_rate 0.000100148
2017-10-02T23:08:58.613720: step 2546, loss 0.0351881, acc 0.984375, learning_rate 0.000100148
2017-10-02T23:08:59.764234: step 2547, loss 0.0187905, acc 1, learning_rate 0.000100147
2017-10-02T23:09:00.902629: step 2548, loss 0.0301668, acc 1, learning_rate 0.000100147
2017-10-02T23:09:02.077307: step 2549, loss 0.0120075, acc 1, learning_rate 0.000100146
2017-10-02T23:09:03.239940: step 2550, loss 0.0121507, acc 1, learning_rate 0.000100145
2017-10-02T23:09:04.393022: step 2551, loss 0.0130466, acc 1, learning_rate 0.000100145
2017-10-02T23:09:05.557658: step 2552, loss 0.0405629, acc 0.984375, learning_rate 0.000100144
2017-10-02T23:09:06.719376: step 2553, loss 0.0150118, acc 1, learning_rate 0.000100144
2017-10-02T23:09:07.871744: step 2554, loss 0.0128042, acc 1, learning_rate 0.000100143
2017-10-02T23:09:09.023941: step 2555, loss 0.0274051, acc 1, learning_rate 0.000100142
2017-10-02T23:09:10.176415: step 2556, loss 0.0530922, acc 0.984375, learning_rate 0.000100142
2017-10-02T23:09:11.344907: step 2557, loss 0.0455704, acc 0.984375, learning_rate 0.000100141
2017-10-02T23:09:12.515983: step 2558, loss 0.0534739, acc 0.984375, learning_rate 0.000100141
2017-10-02T23:09:13.671201: step 2559, loss 0.0234409, acc 1, learning_rate 0.00010014
2017-10-02T23:09:14.832801: step 2560, loss 0.0130862, acc 1, learning_rate 0.00010014

Evaluation:
2017-10-02T23:09:15.168721: step 2560, loss 1.43548, acc 0.457554

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2560

2017-10-02T23:09:23.291997: step 2561, loss 0.0219255, acc 1, learning_rate 0.000100139
2017-10-02T23:09:24.478192: step 2562, loss 0.0134404, acc 1, learning_rate 0.000100138
2017-10-02T23:09:25.641052: step 2563, loss 0.0136965, acc 1, learning_rate 0.000100138
2017-10-02T23:09:26.809538: step 2564, loss 0.0125179, acc 1, learning_rate 0.000100137
2017-10-02T23:09:27.964760: step 2565, loss 0.0106591, acc 1, learning_rate 0.000100137
2017-10-02T23:09:29.133304: step 2566, loss 0.0323654, acc 0.984375, learning_rate 0.000100136
2017-10-02T23:09:30.304430: step 2567, loss 0.0134622, acc 1, learning_rate 0.000100136
2017-10-02T23:09:31.447967: step 2568, loss 0.00951413, acc 1, learning_rate 0.000100135
2017-10-02T23:09:32.604956: step 2569, loss 0.016969, acc 1, learning_rate 0.000100134
2017-10-02T23:09:33.793962: step 2570, loss 0.0254682, acc 1, learning_rate 0.000100134
2017-10-02T23:09:34.956163: step 2571, loss 0.0124392, acc 1, learning_rate 0.000100133
2017-10-02T23:09:36.100258: step 2572, loss 0.0402907, acc 0.984375, learning_rate 0.000100133
2017-10-02T23:09:37.252216: step 2573, loss 0.0646631, acc 0.953125, learning_rate 0.000100132
2017-10-02T23:09:38.420197: step 2574, loss 0.0180008, acc 1, learning_rate 0.000100132
2017-10-02T23:09:39.574971: step 2575, loss 0.020323, acc 1, learning_rate 0.000100131
2017-10-02T23:09:40.729966: step 2576, loss 0.0220803, acc 1, learning_rate 0.000100131
2017-10-02T23:09:41.883244: step 2577, loss 0.0314733, acc 0.984375, learning_rate 0.00010013
2017-10-02T23:09:43.034577: step 2578, loss 0.00907507, acc 1, learning_rate 0.00010013
2017-10-02T23:09:44.187413: step 2579, loss 0.0146424, acc 1, learning_rate 0.000100129
2017-10-02T23:09:45.346769: step 2580, loss 0.011117, acc 1, learning_rate 0.000100129
2017-10-02T23:09:46.520210: step 2581, loss 0.0153212, acc 1, learning_rate 0.000100128
2017-10-02T23:09:47.668359: step 2582, loss 0.0165924, acc 1, learning_rate 0.000100128
2017-10-02T23:09:48.816462: step 2583, loss 0.015224, acc 1, learning_rate 0.000100127
2017-10-02T23:09:49.968108: step 2584, loss 0.0521555, acc 0.984375, learning_rate 0.000100126
2017-10-02T23:09:51.126933: step 2585, loss 0.025729, acc 0.984375, learning_rate 0.000100126
2017-10-02T23:09:52.355903: step 2586, loss 0.0183818, acc 1, learning_rate 0.000100125
2017-10-02T23:09:53.519473: step 2587, loss 0.0171756, acc 1, learning_rate 0.000100125
2017-10-02T23:09:54.673101: step 2588, loss 0.0761917, acc 0.984375, learning_rate 0.000100124
2017-10-02T23:09:55.823099: step 2589, loss 0.0137294, acc 1, learning_rate 0.000100124
2017-10-02T23:09:56.979254: step 2590, loss 0.0183554, acc 1, learning_rate 0.000100123
2017-10-02T23:09:58.140293: step 2591, loss 0.029698, acc 1, learning_rate 0.000100123
2017-10-02T23:09:59.294261: step 2592, loss 0.010671, acc 1, learning_rate 0.000100122
2017-10-02T23:10:00.461558: step 2593, loss 0.0169134, acc 1, learning_rate 0.000100122
2017-10-02T23:10:01.618555: step 2594, loss 0.0736622, acc 0.96875, learning_rate 0.000100121
2017-10-02T23:10:02.842310: step 2595, loss 0.0192408, acc 1, learning_rate 0.000100121
2017-10-02T23:10:03.992348: step 2596, loss 0.0181617, acc 1, learning_rate 0.00010012
2017-10-02T23:10:05.148277: step 2597, loss 0.014182, acc 1, learning_rate 0.00010012
2017-10-02T23:10:06.306217: step 2598, loss 0.0165375, acc 1, learning_rate 0.000100119
2017-10-02T23:10:07.464557: step 2599, loss 0.00887947, acc 1, learning_rate 0.000100119
2017-10-02T23:10:08.626783: step 2600, loss 0.0229877, acc 1, learning_rate 0.000100118

Evaluation:
2017-10-02T23:10:08.955803: step 2600, loss 1.45298, acc 0.456115

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2600

2017-10-02T23:10:16.380039: step 2601, loss 0.0413751, acc 0.96875, learning_rate 0.000100118
2017-10-02T23:10:17.532554: step 2602, loss 0.0584247, acc 0.984375, learning_rate 0.000100117
2017-10-02T23:10:18.690225: step 2603, loss 0.00990735, acc 1, learning_rate 0.000100117
2017-10-02T23:10:19.838265: step 2604, loss 0.0198067, acc 1, learning_rate 0.000100117
2017-10-02T23:10:20.991607: step 2605, loss 0.0337892, acc 0.984375, learning_rate 0.000100116
2017-10-02T23:10:22.143491: step 2606, loss 0.010761, acc 1, learning_rate 0.000100116
2017-10-02T23:10:23.296665: step 2607, loss 0.0534485, acc 0.96875, learning_rate 0.000100115
2017-10-02T23:10:24.443713: step 2608, loss 0.0165808, acc 1, learning_rate 0.000100115
2017-10-02T23:10:25.588037: step 2609, loss 0.0137477, acc 1, learning_rate 0.000100114
2017-10-02T23:10:26.755490: step 2610, loss 0.0122109, acc 1, learning_rate 0.000100114
2017-10-02T23:10:27.911925: step 2611, loss 0.0502553, acc 1, learning_rate 0.000100113
2017-10-02T23:10:29.064531: step 2612, loss 0.0159517, acc 1, learning_rate 0.000100113
2017-10-02T23:10:30.219937: step 2613, loss 0.0176336, acc 1, learning_rate 0.000100112
2017-10-02T23:10:31.370956: step 2614, loss 0.055305, acc 0.984375, learning_rate 0.000100112
2017-10-02T23:10:32.515846: step 2615, loss 0.0169818, acc 1, learning_rate 0.000100111
2017-10-02T23:10:33.668241: step 2616, loss 0.0161252, acc 1, learning_rate 0.000100111
2017-10-02T23:10:34.815514: step 2617, loss 0.0166829, acc 1, learning_rate 0.000100111
2017-10-02T23:10:35.962325: step 2618, loss 0.0167806, acc 1, learning_rate 0.00010011
2017-10-02T23:10:37.112182: step 2619, loss 0.0135543, acc 1, learning_rate 0.00010011
2017-10-02T23:10:38.273464: step 2620, loss 0.0220901, acc 1, learning_rate 0.000100109
2017-10-02T23:10:39.433396: step 2621, loss 0.0302129, acc 0.984375, learning_rate 0.000100109
2017-10-02T23:10:40.594529: step 2622, loss 0.0167888, acc 1, learning_rate 0.000100108
2017-10-02T23:10:41.738146: step 2623, loss 0.0156661, acc 1, learning_rate 0.000100108
2017-10-02T23:10:42.896099: step 2624, loss 0.0302427, acc 0.984375, learning_rate 0.000100107
2017-10-02T23:10:44.054115: step 2625, loss 0.0140108, acc 1, learning_rate 0.000100107
2017-10-02T23:10:45.214051: step 2626, loss 0.0117017, acc 1, learning_rate 0.000100107
2017-10-02T23:10:46.375466: step 2627, loss 0.0295514, acc 0.984375, learning_rate 0.000100106
2017-10-02T23:10:47.542264: step 2628, loss 0.0113401, acc 1, learning_rate 0.000100106
2017-10-02T23:10:48.702826: step 2629, loss 0.0169649, acc 1, learning_rate 0.000100105
2017-10-02T23:10:49.847620: step 2630, loss 0.0161609, acc 1, learning_rate 0.000100105
2017-10-02T23:10:50.997962: step 2631, loss 0.0111184, acc 1, learning_rate 0.000100104
2017-10-02T23:10:52.167000: step 2632, loss 0.0239735, acc 1, learning_rate 0.000100104
2017-10-02T23:10:53.321034: step 2633, loss 0.0375245, acc 0.984375, learning_rate 0.000100104
2017-10-02T23:10:54.470357: step 2634, loss 0.0147309, acc 1, learning_rate 0.000100103
2017-10-02T23:10:55.622068: step 2635, loss 0.0134292, acc 1, learning_rate 0.000100103
2017-10-02T23:10:56.771086: step 2636, loss 0.0139074, acc 1, learning_rate 0.000100102
2017-10-02T23:10:57.925592: step 2637, loss 0.0125223, acc 1, learning_rate 0.000100102
2017-10-02T23:10:59.096914: step 2638, loss 0.021834, acc 1, learning_rate 0.000100101
2017-10-02T23:11:00.245852: step 2639, loss 0.0358107, acc 1, learning_rate 0.000100101
2017-10-02T23:11:01.413190: step 2640, loss 0.0339202, acc 0.984375, learning_rate 0.000100101

Evaluation:
2017-10-02T23:11:01.745137: step 2640, loss 1.43257, acc 0.493525

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2640

2017-10-02T23:11:09.675663: step 2641, loss 0.0317208, acc 0.984375, learning_rate 0.0001001
2017-10-02T23:11:10.863750: step 2642, loss 0.0130794, acc 1, learning_rate 0.0001001
2017-10-02T23:11:12.025699: step 2643, loss 0.0154834, acc 1, learning_rate 0.000100099
2017-10-02T23:11:13.181136: step 2644, loss 0.0163935, acc 1, learning_rate 0.000100099
2017-10-02T23:11:14.333590: step 2645, loss 0.0178985, acc 1, learning_rate 0.000100099
2017-10-02T23:11:15.468245: step 2646, loss 0.0121, acc 1, learning_rate 0.000100098
2017-10-02T23:11:16.621977: step 2647, loss 0.028055, acc 1, learning_rate 0.000100098
2017-10-02T23:11:17.782487: step 2648, loss 0.00767296, acc 1, learning_rate 0.000100097
2017-10-02T23:11:18.938831: step 2649, loss 0.023291, acc 1, learning_rate 0.000100097
2017-10-02T23:11:20.101614: step 2650, loss 0.0207448, acc 1, learning_rate 0.000100097
2017-10-02T23:11:21.241870: step 2651, loss 0.0151696, acc 1, learning_rate 0.000100096
2017-10-02T23:11:22.392210: step 2652, loss 0.0303857, acc 0.984375, learning_rate 0.000100096
2017-10-02T23:11:23.538589: step 2653, loss 0.0406491, acc 0.984375, learning_rate 0.000100095
2017-10-02T23:11:24.705874: step 2654, loss 0.0096734, acc 1, learning_rate 0.000100095
2017-10-02T23:11:25.850233: step 2655, loss 0.0336494, acc 0.984375, learning_rate 0.000100095
2017-10-02T23:11:27.006214: step 2656, loss 0.0110045, acc 1, learning_rate 0.000100094
2017-10-02T23:11:28.147186: step 2657, loss 0.0354067, acc 0.984375, learning_rate 0.000100094
2017-10-02T23:11:29.310632: step 2658, loss 0.0235185, acc 0.984375, learning_rate 0.000100093
2017-10-02T23:11:30.528061: step 2659, loss 0.0271285, acc 0.984375, learning_rate 0.000100093
2017-10-02T23:11:31.690551: step 2660, loss 0.0121262, acc 1, learning_rate 0.000100093
2017-10-02T23:11:32.897391: step 2661, loss 0.0253663, acc 0.984375, learning_rate 0.000100092
2017-10-02T23:11:34.057516: step 2662, loss 0.0156493, acc 1, learning_rate 0.000100092
2017-10-02T23:11:35.209320: step 2663, loss 0.0117532, acc 1, learning_rate 0.000100092
2017-10-02T23:11:36.365659: step 2664, loss 0.0114001, acc 1, learning_rate 0.000100091
2017-10-02T23:11:37.519275: step 2665, loss 0.0453465, acc 0.984375, learning_rate 0.000100091
2017-10-02T23:11:38.670305: step 2666, loss 0.0106749, acc 1, learning_rate 0.00010009
2017-10-02T23:11:39.816713: step 2667, loss 0.023838, acc 1, learning_rate 0.00010009
2017-10-02T23:11:40.967278: step 2668, loss 0.0409416, acc 0.984375, learning_rate 0.00010009
2017-10-02T23:11:42.128841: step 2669, loss 0.0220092, acc 1, learning_rate 0.000100089
2017-10-02T23:11:43.291381: step 2670, loss 0.0211911, acc 1, learning_rate 0.000100089
2017-10-02T23:11:44.442142: step 2671, loss 0.00773922, acc 1, learning_rate 0.000100089
2017-10-02T23:11:45.605100: step 2672, loss 0.039815, acc 0.984375, learning_rate 0.000100088
2017-10-02T23:11:46.744470: step 2673, loss 0.021251, acc 1, learning_rate 0.000100088
2017-10-02T23:11:47.893857: step 2674, loss 0.0197417, acc 1, learning_rate 0.000100088
2017-10-02T23:11:49.033051: step 2675, loss 0.0434501, acc 0.96875, learning_rate 0.000100087
2017-10-02T23:11:50.185654: step 2676, loss 0.0456233, acc 0.984375, learning_rate 0.000100087
2017-10-02T23:11:51.330362: step 2677, loss 0.071724, acc 0.984375, learning_rate 0.000100086
2017-10-02T23:11:52.487848: step 2678, loss 0.0125633, acc 1, learning_rate 0.000100086
2017-10-02T23:11:53.650528: step 2679, loss 0.0261644, acc 1, learning_rate 0.000100086
2017-10-02T23:11:54.802348: step 2680, loss 0.0101362, acc 1, learning_rate 0.000100085

Evaluation:
2017-10-02T23:11:55.124853: step 2680, loss 1.4402, acc 0.476259

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2680

2017-10-02T23:12:03.471295: step 2681, loss 0.017477, acc 1, learning_rate 0.000100085
2017-10-02T23:12:04.640431: step 2682, loss 0.0333248, acc 0.984375, learning_rate 0.000100085
2017-10-02T23:12:05.795029: step 2683, loss 0.00876853, acc 1, learning_rate 0.000100084
2017-10-02T23:12:06.953721: step 2684, loss 0.0136615, acc 1, learning_rate 0.000100084
2017-10-02T23:12:08.104003: step 2685, loss 0.0134069, acc 1, learning_rate 0.000100084
2017-10-02T23:12:09.249101: step 2686, loss 0.0157256, acc 1, learning_rate 0.000100083
2017-10-02T23:12:10.403108: step 2687, loss 0.0187312, acc 1, learning_rate 0.000100083
2017-10-02T23:12:11.562492: step 2688, loss 0.0219337, acc 0.984375, learning_rate 0.000100083
2017-10-02T23:12:12.715522: step 2689, loss 0.0116961, acc 1, learning_rate 0.000100082
2017-10-02T23:12:13.872935: step 2690, loss 0.0150757, acc 1, learning_rate 0.000100082
2017-10-02T23:12:15.024479: step 2691, loss 0.0258238, acc 1, learning_rate 0.000100082
2017-10-02T23:12:16.211255: step 2692, loss 0.0519075, acc 0.96875, learning_rate 0.000100081
2017-10-02T23:12:17.393534: step 2693, loss 0.0112488, acc 1, learning_rate 0.000100081
2017-10-02T23:12:18.560381: step 2694, loss 0.00803117, acc 1, learning_rate 0.000100081
2017-10-02T23:12:19.697060: step 2695, loss 0.0688702, acc 0.984375, learning_rate 0.00010008
2017-10-02T23:12:20.853602: step 2696, loss 0.0107527, acc 1, learning_rate 0.00010008
2017-10-02T23:12:22.013033: step 2697, loss 0.0186159, acc 1, learning_rate 0.00010008
2017-10-02T23:12:23.159078: step 2698, loss 0.006302, acc 1, learning_rate 0.000100079
2017-10-02T23:12:24.325147: step 2699, loss 0.0564959, acc 0.984375, learning_rate 0.000100079
2017-10-02T23:12:25.497646: step 2700, loss 0.0272029, acc 0.984375, learning_rate 0.000100079
2017-10-02T23:12:26.647739: step 2701, loss 0.0358986, acc 1, learning_rate 0.000100078
2017-10-02T23:12:27.817087: step 2702, loss 0.0165574, acc 1, learning_rate 0.000100078
2017-10-02T23:12:28.983084: step 2703, loss 0.0261907, acc 1, learning_rate 0.000100078
2017-10-02T23:12:30.158535: step 2704, loss 0.0179744, acc 1, learning_rate 0.000100077
2017-10-02T23:12:31.310512: step 2705, loss 0.0137456, acc 1, learning_rate 0.000100077
2017-10-02T23:12:32.456784: step 2706, loss 0.0125164, acc 1, learning_rate 0.000100077
2017-10-02T23:12:33.617392: step 2707, loss 0.017499, acc 1, learning_rate 0.000100076
2017-10-02T23:12:34.772965: step 2708, loss 0.0159299, acc 1, learning_rate 0.000100076
2017-10-02T23:12:35.943083: step 2709, loss 0.00746955, acc 1, learning_rate 0.000100076
2017-10-02T23:12:37.093750: step 2710, loss 0.0135566, acc 1, learning_rate 0.000100076
2017-10-02T23:12:38.262932: step 2711, loss 0.00927933, acc 1, learning_rate 0.000100075
2017-10-02T23:12:39.409252: step 2712, loss 0.0143448, acc 1, learning_rate 0.000100075
2017-10-02T23:12:40.564224: step 2713, loss 0.0271574, acc 0.984375, learning_rate 0.000100075
2017-10-02T23:12:41.712260: step 2714, loss 0.016353, acc 1, learning_rate 0.000100074
2017-10-02T23:12:42.867086: step 2715, loss 0.00868625, acc 1, learning_rate 0.000100074
2017-10-02T23:12:44.026494: step 2716, loss 0.00948722, acc 1, learning_rate 0.000100074
2017-10-02T23:12:45.172371: step 2717, loss 0.0092664, acc 1, learning_rate 0.000100073
2017-10-02T23:12:46.330804: step 2718, loss 0.01805, acc 1, learning_rate 0.000100073
2017-10-02T23:12:47.483802: step 2719, loss 0.020713, acc 1, learning_rate 0.000100073
2017-10-02T23:12:48.636600: step 2720, loss 0.0145777, acc 1, learning_rate 0.000100073

Evaluation:
2017-10-02T23:12:48.988729: step 2720, loss 1.4366, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2720

2017-10-02T23:12:56.687877: step 2721, loss 0.0202899, acc 1, learning_rate 0.000100072
2017-10-02T23:12:57.845157: step 2722, loss 0.0275478, acc 1, learning_rate 0.000100072
2017-10-02T23:12:59.003292: step 2723, loss 0.0725206, acc 0.96875, learning_rate 0.000100072
2017-10-02T23:13:00.187310: step 2724, loss 0.0444985, acc 0.984375, learning_rate 0.000100071
2017-10-02T23:13:01.342998: step 2725, loss 0.0127874, acc 1, learning_rate 0.000100071
2017-10-02T23:13:02.495324: step 2726, loss 0.0135953, acc 1, learning_rate 0.000100071
2017-10-02T23:13:03.648585: step 2727, loss 0.0195616, acc 1, learning_rate 0.00010007
2017-10-02T23:13:04.796568: step 2728, loss 0.0078661, acc 1, learning_rate 0.00010007
2017-10-02T23:13:05.940542: step 2729, loss 0.0425808, acc 0.984375, learning_rate 0.00010007
2017-10-02T23:13:07.105589: step 2730, loss 0.0253361, acc 0.984375, learning_rate 0.00010007
2017-10-02T23:13:08.252560: step 2731, loss 0.0173555, acc 1, learning_rate 0.000100069
2017-10-02T23:13:09.423977: step 2732, loss 0.0249055, acc 1, learning_rate 0.000100069
2017-10-02T23:13:10.673968: step 2733, loss 0.0239082, acc 0.984375, learning_rate 0.000100069
2017-10-02T23:13:12.041378: step 2734, loss 0.0164479, acc 1, learning_rate 0.000100068
2017-10-02T23:13:13.191955: step 2735, loss 0.0295634, acc 0.984375, learning_rate 0.000100068
2017-10-02T23:13:14.355709: step 2736, loss 0.0314764, acc 0.984375, learning_rate 0.000100068
2017-10-02T23:13:15.507227: step 2737, loss 0.0120774, acc 1, learning_rate 0.000100068
2017-10-02T23:13:16.652005: step 2738, loss 0.0140025, acc 1, learning_rate 0.000100067
2017-10-02T23:13:17.867557: step 2739, loss 0.0147027, acc 1, learning_rate 0.000100067
2017-10-02T23:13:19.022648: step 2740, loss 0.0182648, acc 1, learning_rate 0.000100067
2017-10-02T23:13:20.181915: step 2741, loss 0.0110237, acc 1, learning_rate 0.000100067
2017-10-02T23:13:21.330165: step 2742, loss 0.0116583, acc 1, learning_rate 0.000100066
2017-10-02T23:13:22.478016: step 2743, loss 0.0190178, acc 1, learning_rate 0.000100066
2017-10-02T23:13:23.760753: step 2744, loss 0.0167158, acc 1, learning_rate 0.000100066
2017-10-02T23:13:24.916287: step 2745, loss 0.0167642, acc 1, learning_rate 0.000100065
2017-10-02T23:13:26.082440: step 2746, loss 0.0175986, acc 1, learning_rate 0.000100065
2017-10-02T23:13:27.234550: step 2747, loss 0.0553036, acc 0.984375, learning_rate 0.000100065
2017-10-02T23:13:28.399507: step 2748, loss 0.00894907, acc 1, learning_rate 0.000100065
2017-10-02T23:13:29.552800: step 2749, loss 0.0120111, acc 1, learning_rate 0.000100064
2017-10-02T23:13:30.723075: step 2750, loss 0.0124245, acc 1, learning_rate 0.000100064
2017-10-02T23:13:31.873590: step 2751, loss 0.0124017, acc 1, learning_rate 0.000100064
2017-10-02T23:13:33.024358: step 2752, loss 0.0307031, acc 1, learning_rate 0.000100064
2017-10-02T23:13:34.178546: step 2753, loss 0.0241235, acc 0.984375, learning_rate 0.000100063
2017-10-02T23:13:35.327784: step 2754, loss 0.029883, acc 0.984375, learning_rate 0.000100063
2017-10-02T23:13:36.474936: step 2755, loss 0.0377983, acc 0.984375, learning_rate 0.000100063
2017-10-02T23:13:37.639421: step 2756, loss 0.0260878, acc 1, learning_rate 0.000100063
2017-10-02T23:13:38.798741: step 2757, loss 0.0278391, acc 0.984375, learning_rate 0.000100062
2017-10-02T23:13:39.939264: step 2758, loss 0.014514, acc 1, learning_rate 0.000100062
2017-10-02T23:13:41.087433: step 2759, loss 0.00610104, acc 1, learning_rate 0.000100062
2017-10-02T23:13:42.239096: step 2760, loss 0.0244064, acc 1, learning_rate 0.000100062

Evaluation:
2017-10-02T23:13:42.581053: step 2760, loss 1.43209, acc 0.484892

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2760

2017-10-02T23:13:50.355392: step 2761, loss 0.00955599, acc 1, learning_rate 0.000100061
2017-10-02T23:13:51.548661: step 2762, loss 0.048257, acc 0.96875, learning_rate 0.000100061
2017-10-02T23:13:52.697573: step 2763, loss 0.0138123, acc 1, learning_rate 0.000100061
2017-10-02T23:13:53.849812: step 2764, loss 0.012533, acc 1, learning_rate 0.000100061
2017-10-02T23:13:55.010828: step 2765, loss 0.0127984, acc 1, learning_rate 0.00010006
2017-10-02T23:13:56.169066: step 2766, loss 0.0238558, acc 1, learning_rate 0.00010006
2017-10-02T23:13:57.311243: step 2767, loss 0.0193018, acc 1, learning_rate 0.00010006
2017-10-02T23:13:58.458566: step 2768, loss 0.0503846, acc 0.984375, learning_rate 0.00010006
2017-10-02T23:13:59.605969: step 2769, loss 0.0201373, acc 1, learning_rate 0.000100059
2017-10-02T23:14:00.758377: step 2770, loss 0.0110696, acc 1, learning_rate 0.000100059
2017-10-02T23:14:01.905412: step 2771, loss 0.0110383, acc 1, learning_rate 0.000100059
2017-10-02T23:14:03.053755: step 2772, loss 0.0160586, acc 1, learning_rate 0.000100059
2017-10-02T23:14:04.219383: step 2773, loss 0.0208111, acc 1, learning_rate 0.000100058
2017-10-02T23:14:05.378993: step 2774, loss 0.0328677, acc 0.984375, learning_rate 0.000100058
2017-10-02T23:14:06.629596: step 2775, loss 0.0161955, acc 1, learning_rate 0.000100058
2017-10-02T23:14:07.785427: step 2776, loss 0.0229862, acc 1, learning_rate 0.000100058
2017-10-02T23:14:08.932381: step 2777, loss 0.0354132, acc 0.984375, learning_rate 0.000100057
2017-10-02T23:14:10.088097: step 2778, loss 0.00951204, acc 1, learning_rate 0.000100057
2017-10-02T23:14:11.241764: step 2779, loss 0.0417346, acc 0.984375, learning_rate 0.000100057
2017-10-02T23:14:12.397439: step 2780, loss 0.0256657, acc 1, learning_rate 0.000100057
2017-10-02T23:14:13.560214: step 2781, loss 0.00943759, acc 1, learning_rate 0.000100056
2017-10-02T23:14:14.705297: step 2782, loss 0.0373791, acc 0.984375, learning_rate 0.000100056
2017-10-02T23:14:15.850931: step 2783, loss 0.0213519, acc 1, learning_rate 0.000100056
2017-10-02T23:14:17.004263: step 2784, loss 0.0279827, acc 1, learning_rate 0.000100056
2017-10-02T23:14:18.155624: step 2785, loss 0.0104026, acc 1, learning_rate 0.000100056
2017-10-02T23:14:19.339949: step 2786, loss 0.0156465, acc 1, learning_rate 0.000100055
2017-10-02T23:14:20.498662: step 2787, loss 0.013953, acc 1, learning_rate 0.000100055
2017-10-02T23:14:21.655263: step 2788, loss 0.053411, acc 0.984375, learning_rate 0.000100055
2017-10-02T23:14:22.805504: step 2789, loss 0.0234062, acc 1, learning_rate 0.000100055
2017-10-02T23:14:23.946528: step 2790, loss 0.0368596, acc 0.984375, learning_rate 0.000100054
2017-10-02T23:14:25.117579: step 2791, loss 0.0274589, acc 1, learning_rate 0.000100054
2017-10-02T23:14:26.277667: step 2792, loss 0.0154484, acc 1, learning_rate 0.000100054
2017-10-02T23:14:27.435059: step 2793, loss 0.0197363, acc 1, learning_rate 0.000100054
2017-10-02T23:14:28.588962: step 2794, loss 0.00995891, acc 1, learning_rate 0.000100054
2017-10-02T23:14:29.755751: step 2795, loss 0.00910464, acc 1, learning_rate 0.000100053
2017-10-02T23:14:31.139186: step 2796, loss 0.0209735, acc 1, learning_rate 0.000100053
2017-10-02T23:14:32.326788: step 2797, loss 0.0406647, acc 0.984375, learning_rate 0.000100053
2017-10-02T23:14:33.489349: step 2798, loss 0.0211417, acc 1, learning_rate 0.000100053
2017-10-02T23:14:34.646996: step 2799, loss 0.00833107, acc 1, learning_rate 0.000100052
2017-10-02T23:14:35.805156: step 2800, loss 0.011385, acc 1, learning_rate 0.000100052

Evaluation:
2017-10-02T23:14:36.140711: step 2800, loss 1.45882, acc 0.433094

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2800

2017-10-02T23:14:42.890136: step 2801, loss 0.0115514, acc 1, learning_rate 0.000100052
2017-10-02T23:14:44.039743: step 2802, loss 0.0127744, acc 1, learning_rate 0.000100052
2017-10-02T23:14:45.188593: step 2803, loss 0.0405472, acc 0.984375, learning_rate 0.000100052
2017-10-02T23:14:46.333403: step 2804, loss 0.0250242, acc 0.984375, learning_rate 0.000100051
2017-10-02T23:14:47.483015: step 2805, loss 0.0106535, acc 1, learning_rate 0.000100051
2017-10-02T23:14:48.640041: step 2806, loss 0.0140534, acc 1, learning_rate 0.000100051
2017-10-02T23:14:49.787545: step 2807, loss 0.00956512, acc 1, learning_rate 0.000100051
2017-10-02T23:14:50.939543: step 2808, loss 0.0305361, acc 1, learning_rate 0.000100051
2017-10-02T23:14:52.091637: step 2809, loss 0.0227109, acc 1, learning_rate 0.00010005
2017-10-02T23:14:53.260906: step 2810, loss 0.0204201, acc 1, learning_rate 0.00010005
2017-10-02T23:14:54.415212: step 2811, loss 0.0102799, acc 1, learning_rate 0.00010005
2017-10-02T23:14:55.595779: step 2812, loss 0.0160483, acc 1, learning_rate 0.00010005
2017-10-02T23:14:56.756977: step 2813, loss 0.0168687, acc 1, learning_rate 0.00010005
2017-10-02T23:14:57.905991: step 2814, loss 0.0156093, acc 1, learning_rate 0.000100049
2017-10-02T23:14:59.057353: step 2815, loss 0.0111964, acc 1, learning_rate 0.000100049
2017-10-02T23:15:00.216566: step 2816, loss 0.00574462, acc 1, learning_rate 0.000100049
2017-10-02T23:15:01.365415: step 2817, loss 0.0106013, acc 1, learning_rate 0.000100049
2017-10-02T23:15:02.520370: step 2818, loss 0.00922945, acc 1, learning_rate 0.000100049
2017-10-02T23:15:03.663706: step 2819, loss 0.0176345, acc 1, learning_rate 0.000100048
2017-10-02T23:15:04.808641: step 2820, loss 0.0258764, acc 0.984375, learning_rate 0.000100048
2017-10-02T23:15:05.955154: step 2821, loss 0.0331902, acc 0.984375, learning_rate 0.000100048
2017-10-02T23:15:07.107742: step 2822, loss 0.0139189, acc 1, learning_rate 0.000100048
2017-10-02T23:15:08.284843: step 2823, loss 0.0384932, acc 0.984375, learning_rate 0.000100048
2017-10-02T23:15:09.437147: step 2824, loss 0.0691492, acc 0.96875, learning_rate 0.000100047
2017-10-02T23:15:10.594493: step 2825, loss 0.0312934, acc 0.984375, learning_rate 0.000100047
2017-10-02T23:15:11.747563: step 2826, loss 0.0334706, acc 0.984375, learning_rate 0.000100047
2017-10-02T23:15:12.905627: step 2827, loss 0.0183665, acc 1, learning_rate 0.000100047
2017-10-02T23:15:14.049090: step 2828, loss 0.0268997, acc 0.984375, learning_rate 0.000100047
2017-10-02T23:15:15.198109: step 2829, loss 0.0499711, acc 0.984375, learning_rate 0.000100046
2017-10-02T23:15:16.343634: step 2830, loss 0.0116415, acc 1, learning_rate 0.000100046
2017-10-02T23:15:17.488762: step 2831, loss 0.042464, acc 0.984375, learning_rate 0.000100046
2017-10-02T23:15:18.640473: step 2832, loss 0.0607104, acc 0.984375, learning_rate 0.000100046
2017-10-02T23:15:19.792062: step 2833, loss 0.0117469, acc 1, learning_rate 0.000100046
2017-10-02T23:15:20.951268: step 2834, loss 0.0139507, acc 1, learning_rate 0.000100045
2017-10-02T23:15:22.105041: step 2835, loss 0.0425119, acc 0.984375, learning_rate 0.000100045
2017-10-02T23:15:23.252241: step 2836, loss 0.0296713, acc 0.984375, learning_rate 0.000100045
2017-10-02T23:15:24.411822: step 2837, loss 0.0128864, acc 1, learning_rate 0.000100045
2017-10-02T23:15:25.581205: step 2838, loss 0.0177729, acc 1, learning_rate 0.000100045
2017-10-02T23:15:26.734405: step 2839, loss 0.0155246, acc 1, learning_rate 0.000100045
2017-10-02T23:15:27.885197: step 2840, loss 0.0293332, acc 1, learning_rate 0.000100044

Evaluation:
2017-10-02T23:15:28.214163: step 2840, loss 1.44529, acc 0.47482

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2840

2017-10-02T23:15:36.172370: step 2841, loss 0.0143368, acc 1, learning_rate 0.000100044
2017-10-02T23:15:37.405573: step 2842, loss 0.0328965, acc 1, learning_rate 0.000100044
2017-10-02T23:15:38.650857: step 2843, loss 0.029654, acc 0.984375, learning_rate 0.000100044
2017-10-02T23:15:39.889557: step 2844, loss 0.0247192, acc 0.984375, learning_rate 0.000100044
2017-10-02T23:15:41.055217: step 2845, loss 0.0436831, acc 0.984375, learning_rate 0.000100043
2017-10-02T23:15:42.297375: step 2846, loss 0.0405134, acc 0.96875, learning_rate 0.000100043
2017-10-02T23:15:43.454147: step 2847, loss 0.0129544, acc 1, learning_rate 0.000100043
2017-10-02T23:15:44.606122: step 2848, loss 0.0780764, acc 0.96875, learning_rate 0.000100043
2017-10-02T23:15:45.855165: step 2849, loss 0.011109, acc 1, learning_rate 0.000100043
2017-10-02T23:15:47.094229: step 2850, loss 0.0210445, acc 1, learning_rate 0.000100043
2017-10-02T23:15:48.247476: step 2851, loss 0.011429, acc 1, learning_rate 0.000100042
2017-10-02T23:15:49.394173: step 2852, loss 0.0168332, acc 1, learning_rate 0.000100042
2017-10-02T23:15:50.544911: step 2853, loss 0.0120142, acc 1, learning_rate 0.000100042
2017-10-02T23:15:51.700216: step 2854, loss 0.0119619, acc 1, learning_rate 0.000100042
2017-10-02T23:15:52.854599: step 2855, loss 0.0133014, acc 1, learning_rate 0.000100042
2017-10-02T23:15:54.002766: step 2856, loss 0.0125499, acc 1, learning_rate 0.000100042
2017-10-02T23:15:55.156805: step 2857, loss 0.0228506, acc 1, learning_rate 0.000100041
2017-10-02T23:15:56.316432: step 2858, loss 0.0200779, acc 1, learning_rate 0.000100041
2017-10-02T23:15:57.475521: step 2859, loss 0.00996875, acc 1, learning_rate 0.000100041
2017-10-02T23:15:58.635060: step 2860, loss 0.0157046, acc 1, learning_rate 0.000100041
2017-10-02T23:15:59.786800: step 2861, loss 0.0271888, acc 1, learning_rate 0.000100041
2017-10-02T23:16:00.939171: step 2862, loss 0.0309393, acc 0.984375, learning_rate 0.000100041
2017-10-02T23:16:02.084708: step 2863, loss 0.0233089, acc 1, learning_rate 0.00010004
2017-10-02T23:16:03.234864: step 2864, loss 0.035562, acc 0.984375, learning_rate 0.00010004
2017-10-02T23:16:04.388494: step 2865, loss 0.00995188, acc 1, learning_rate 0.00010004
2017-10-02T23:16:05.539500: step 2866, loss 0.0100049, acc 1, learning_rate 0.00010004
2017-10-02T23:16:06.691652: step 2867, loss 0.0521405, acc 0.984375, learning_rate 0.00010004
2017-10-02T23:16:07.831154: step 2868, loss 0.0183023, acc 1, learning_rate 0.00010004
2017-10-02T23:16:08.980950: step 2869, loss 0.0110463, acc 1, learning_rate 0.000100039
2017-10-02T23:16:10.129295: step 2870, loss 0.019457, acc 1, learning_rate 0.000100039
2017-10-02T23:16:11.289717: step 2871, loss 0.0155688, acc 1, learning_rate 0.000100039
2017-10-02T23:16:12.424225: step 2872, loss 0.0386728, acc 0.984375, learning_rate 0.000100039
2017-10-02T23:16:13.587889: step 2873, loss 0.0486147, acc 0.984375, learning_rate 0.000100039
2017-10-02T23:16:14.741450: step 2874, loss 0.0206068, acc 1, learning_rate 0.000100039
2017-10-02T23:16:15.883701: step 2875, loss 0.0182206, acc 1, learning_rate 0.000100038
2017-10-02T23:16:17.037562: step 2876, loss 0.0168359, acc 1, learning_rate 0.000100038
2017-10-02T23:16:18.208189: step 2877, loss 0.02123, acc 1, learning_rate 0.000100038
2017-10-02T23:16:19.358563: step 2878, loss 0.0193565, acc 1, learning_rate 0.000100038
2017-10-02T23:16:20.505423: step 2879, loss 0.0107255, acc 1, learning_rate 0.000100038
2017-10-02T23:16:21.651680: step 2880, loss 0.0103679, acc 1, learning_rate 0.000100038

Evaluation:
2017-10-02T23:16:21.982025: step 2880, loss 1.43299, acc 0.476259

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2880

2017-10-02T23:16:30.315837: step 2881, loss 0.0220616, acc 1, learning_rate 0.000100038
2017-10-02T23:16:31.500197: step 2882, loss 0.0126863, acc 1, learning_rate 0.000100037
2017-10-02T23:16:32.642084: step 2883, loss 0.010902, acc 1, learning_rate 0.000100037
2017-10-02T23:16:33.810391: step 2884, loss 0.0158532, acc 1, learning_rate 0.000100037
2017-10-02T23:16:34.963169: step 2885, loss 0.0131747, acc 1, learning_rate 0.000100037
2017-10-02T23:16:36.116310: step 2886, loss 0.0698599, acc 0.984375, learning_rate 0.000100037
2017-10-02T23:16:37.276613: step 2887, loss 0.0124527, acc 1, learning_rate 0.000100037
2017-10-02T23:16:38.424022: step 2888, loss 0.0112939, acc 1, learning_rate 0.000100036
2017-10-02T23:16:39.588667: step 2889, loss 0.010969, acc 1, learning_rate 0.000100036
2017-10-02T23:16:40.791611: step 2890, loss 0.0191395, acc 1, learning_rate 0.000100036
2017-10-02T23:16:41.959301: step 2891, loss 0.0359294, acc 1, learning_rate 0.000100036
2017-10-02T23:16:43.117222: step 2892, loss 0.0149377, acc 1, learning_rate 0.000100036
2017-10-02T23:16:44.288466: step 2893, loss 0.0142404, acc 1, learning_rate 0.000100036
2017-10-02T23:16:45.445223: step 2894, loss 0.00820326, acc 1, learning_rate 0.000100036
2017-10-02T23:16:46.609842: step 2895, loss 0.0230543, acc 1, learning_rate 0.000100035
2017-10-02T23:16:47.763509: step 2896, loss 0.0549515, acc 0.984375, learning_rate 0.000100035
2017-10-02T23:16:48.921148: step 2897, loss 0.0490718, acc 0.984375, learning_rate 0.000100035
2017-10-02T23:16:50.076726: step 2898, loss 0.011169, acc 1, learning_rate 0.000100035
2017-10-02T23:16:51.261538: step 2899, loss 0.0441465, acc 0.984375, learning_rate 0.000100035
2017-10-02T23:16:52.469139: step 2900, loss 0.0124461, acc 1, learning_rate 0.000100035
2017-10-02T23:16:53.638868: step 2901, loss 0.0124441, acc 1, learning_rate 0.000100035
2017-10-02T23:16:54.775034: step 2902, loss 0.0149525, acc 1, learning_rate 0.000100034
2017-10-02T23:16:55.929962: step 2903, loss 0.026893, acc 1, learning_rate 0.000100034
2017-10-02T23:16:57.075455: step 2904, loss 0.014489, acc 1, learning_rate 0.000100034
2017-10-02T23:16:58.237293: step 2905, loss 0.0593844, acc 0.96875, learning_rate 0.000100034
2017-10-02T23:16:59.406320: step 2906, loss 0.0080981, acc 1, learning_rate 0.000100034
2017-10-02T23:17:00.566768: step 2907, loss 0.00950802, acc 1, learning_rate 0.000100034
2017-10-02T23:17:01.744600: step 2908, loss 0.0269479, acc 0.984375, learning_rate 0.000100034
2017-10-02T23:17:02.895390: step 2909, loss 0.0147671, acc 1, learning_rate 0.000100033
2017-10-02T23:17:04.047373: step 2910, loss 0.0175213, acc 1, learning_rate 0.000100033
2017-10-02T23:17:05.197941: step 2911, loss 0.0155705, acc 1, learning_rate 0.000100033
2017-10-02T23:17:06.358893: step 2912, loss 0.014323, acc 1, learning_rate 0.000100033
2017-10-02T23:17:07.529879: step 2913, loss 0.0138707, acc 1, learning_rate 0.000100033
2017-10-02T23:17:08.696377: step 2914, loss 0.017189, acc 1, learning_rate 0.000100033
2017-10-02T23:17:09.843349: step 2915, loss 0.0183525, acc 1, learning_rate 0.000100033
2017-10-02T23:17:10.988660: step 2916, loss 0.0085522, acc 1, learning_rate 0.000100033
2017-10-02T23:17:12.151083: step 2917, loss 0.0132299, acc 1, learning_rate 0.000100032
2017-10-02T23:17:13.305274: step 2918, loss 0.0146441, acc 1, learning_rate 0.000100032
2017-10-02T23:17:14.475696: step 2919, loss 0.0274175, acc 1, learning_rate 0.000100032
2017-10-02T23:17:15.630671: step 2920, loss 0.00858161, acc 1, learning_rate 0.000100032

Evaluation:
2017-10-02T23:17:15.962721: step 2920, loss 1.43902, acc 0.47482

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2920

2017-10-02T23:17:23.680399: step 2921, loss 0.0128992, acc 1, learning_rate 0.000100032
2017-10-02T23:17:24.836671: step 2922, loss 0.0116356, acc 1, learning_rate 0.000100032
2017-10-02T23:17:25.994678: step 2923, loss 0.0145785, acc 1, learning_rate 0.000100032
2017-10-02T23:17:27.133775: step 2924, loss 0.00892097, acc 1, learning_rate 0.000100031
2017-10-02T23:17:28.284321: step 2925, loss 0.0140117, acc 1, learning_rate 0.000100031
2017-10-02T23:17:29.435570: step 2926, loss 0.0115933, acc 1, learning_rate 0.000100031
2017-10-02T23:17:30.691514: step 2927, loss 0.0110019, acc 1, learning_rate 0.000100031
2017-10-02T23:17:31.834992: step 2928, loss 0.0246161, acc 1, learning_rate 0.000100031
2017-10-02T23:17:32.986876: step 2929, loss 0.0646613, acc 0.953125, learning_rate 0.000100031
2017-10-02T23:17:34.162509: step 2930, loss 0.0309434, acc 0.984375, learning_rate 0.000100031
2017-10-02T23:17:35.318450: step 2931, loss 0.0143328, acc 1, learning_rate 0.000100031
2017-10-02T23:17:36.488056: step 2932, loss 0.0198321, acc 0.984375, learning_rate 0.00010003
2017-10-02T23:17:37.639728: step 2933, loss 0.00921431, acc 1, learning_rate 0.00010003
2017-10-02T23:17:38.810257: step 2934, loss 0.0158211, acc 1, learning_rate 0.00010003
2017-10-02T23:17:39.990014: step 2935, loss 0.0129784, acc 1, learning_rate 0.00010003
2017-10-02T23:17:41.136985: step 2936, loss 0.0111007, acc 1, learning_rate 0.00010003
2017-10-02T23:17:42.278072: step 2937, loss 0.0128127, acc 1, learning_rate 0.00010003
2017-10-02T23:17:43.426644: step 2938, loss 0.00579073, acc 1, learning_rate 0.00010003
2017-10-02T23:17:44.575708: step 2939, loss 0.0566073, acc 0.96875, learning_rate 0.00010003
2017-10-02T23:17:45.716456: step 2940, loss 0.00904184, acc 1, learning_rate 0.000100029
2017-10-02T23:17:46.882916: step 2941, loss 0.009941, acc 1, learning_rate 0.000100029
2017-10-02T23:17:48.038910: step 2942, loss 0.0148969, acc 1, learning_rate 0.000100029
2017-10-02T23:17:49.191828: step 2943, loss 0.0630884, acc 0.96875, learning_rate 0.000100029
2017-10-02T23:17:50.351897: step 2944, loss 0.0154772, acc 1, learning_rate 0.000100029
2017-10-02T23:17:51.495467: step 2945, loss 0.0122586, acc 1, learning_rate 0.000100029
2017-10-02T23:17:52.672451: step 2946, loss 0.0377742, acc 0.984375, learning_rate 0.000100029
2017-10-02T23:17:53.824627: step 2947, loss 0.0426743, acc 0.984375, learning_rate 0.000100029
2017-10-02T23:17:54.981946: step 2948, loss 0.0124763, acc 1, learning_rate 0.000100029
2017-10-02T23:17:56.123392: step 2949, loss 0.0394938, acc 0.984375, learning_rate 0.000100028
2017-10-02T23:17:57.301242: step 2950, loss 0.00905022, acc 1, learning_rate 0.000100028
2017-10-02T23:17:58.831603: step 2951, loss 0.00821826, acc 1, learning_rate 0.000100028
2017-10-02T23:17:59.986830: step 2952, loss 0.0109457, acc 1, learning_rate 0.000100028
2017-10-02T23:18:01.136992: step 2953, loss 0.0130417, acc 1, learning_rate 0.000100028
2017-10-02T23:18:02.295727: step 2954, loss 0.0154434, acc 1, learning_rate 0.000100028
2017-10-02T23:18:03.460645: step 2955, loss 0.0205812, acc 0.984375, learning_rate 0.000100028
2017-10-02T23:18:04.600016: step 2956, loss 0.013242, acc 1, learning_rate 0.000100028
2017-10-02T23:18:05.745736: step 2957, loss 0.0200324, acc 1, learning_rate 0.000100028
2017-10-02T23:18:06.906207: step 2958, loss 0.0119034, acc 1, learning_rate 0.000100027
2017-10-02T23:18:08.335270: step 2959, loss 0.0207838, acc 1, learning_rate 0.000100027
2017-10-02T23:18:09.485489: step 2960, loss 0.0331434, acc 0.984375, learning_rate 0.000100027

Evaluation:
2017-10-02T23:18:09.817766: step 2960, loss 1.44204, acc 0.460432

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-2960

2017-10-02T23:18:17.252912: step 2961, loss 0.00933763, acc 1, learning_rate 0.000100027
2017-10-02T23:18:18.440069: step 2962, loss 0.00789725, acc 1, learning_rate 0.000100027
2017-10-02T23:18:19.596562: step 2963, loss 0.0229417, acc 1, learning_rate 0.000100027
2017-10-02T23:18:20.747631: step 2964, loss 0.0254673, acc 1, learning_rate 0.000100027
2017-10-02T23:18:21.906175: step 2965, loss 0.0120699, acc 1, learning_rate 0.000100027
2017-10-02T23:18:23.045855: step 2966, loss 0.0344775, acc 0.984375, learning_rate 0.000100027
2017-10-02T23:18:24.201663: step 2967, loss 0.0108785, acc 1, learning_rate 0.000100026
2017-10-02T23:18:25.354095: step 2968, loss 0.0108269, acc 1, learning_rate 0.000100026
2017-10-02T23:18:26.582517: step 2969, loss 0.0195734, acc 1, learning_rate 0.000100026
2017-10-02T23:18:27.737565: step 2970, loss 0.0292104, acc 0.984375, learning_rate 0.000100026
2017-10-02T23:18:28.883825: step 2971, loss 0.0110366, acc 1, learning_rate 0.000100026
2017-10-02T23:18:30.021454: step 2972, loss 0.0917773, acc 0.953125, learning_rate 0.000100026
2017-10-02T23:18:31.175009: step 2973, loss 0.015877, acc 1, learning_rate 0.000100026
2017-10-02T23:18:32.319901: step 2974, loss 0.0125475, acc 1, learning_rate 0.000100026
2017-10-02T23:18:33.480230: step 2975, loss 0.0215388, acc 1, learning_rate 0.000100026
2017-10-02T23:18:34.642952: step 2976, loss 0.00857418, acc 1, learning_rate 0.000100025
2017-10-02T23:18:35.787460: step 2977, loss 0.0107797, acc 1, learning_rate 0.000100025
2017-10-02T23:18:36.941950: step 2978, loss 0.00662336, acc 1, learning_rate 0.000100025
2017-10-02T23:18:38.087423: step 2979, loss 0.0112258, acc 1, learning_rate 0.000100025
2017-10-02T23:18:39.239180: step 2980, loss 0.00748803, acc 1, learning_rate 0.000100025
2017-10-02T23:18:40.384618: step 2981, loss 0.0150821, acc 1, learning_rate 0.000100025
2017-10-02T23:18:41.535363: step 2982, loss 0.02412, acc 0.984375, learning_rate 0.000100025
2017-10-02T23:18:42.681843: step 2983, loss 0.0308247, acc 0.984375, learning_rate 0.000100025
2017-10-02T23:18:43.832200: step 2984, loss 0.0170435, acc 1, learning_rate 0.000100025
2017-10-02T23:18:44.991468: step 2985, loss 0.0122386, acc 1, learning_rate 0.000100025
2017-10-02T23:18:46.137796: step 2986, loss 0.0237494, acc 0.984375, learning_rate 0.000100024
2017-10-02T23:18:47.321719: step 2987, loss 0.0269697, acc 0.984375, learning_rate 0.000100024
2017-10-02T23:18:48.508447: step 2988, loss 0.0114767, acc 1, learning_rate 0.000100024
2017-10-02T23:18:49.651029: step 2989, loss 0.0102815, acc 1, learning_rate 0.000100024
2017-10-02T23:18:50.796737: step 2990, loss 0.0156664, acc 1, learning_rate 0.000100024
2017-10-02T23:18:51.957885: step 2991, loss 0.0302188, acc 0.984375, learning_rate 0.000100024
2017-10-02T23:18:53.105909: step 2992, loss 0.0136285, acc 1, learning_rate 0.000100024
2017-10-02T23:18:54.248465: step 2993, loss 0.0291114, acc 0.984375, learning_rate 0.000100024
2017-10-02T23:18:55.409352: step 2994, loss 0.0223753, acc 1, learning_rate 0.000100024
2017-10-02T23:18:56.558603: step 2995, loss 0.0230661, acc 1, learning_rate 0.000100024
2017-10-02T23:18:57.713229: step 2996, loss 0.0123384, acc 1, learning_rate 0.000100023
2017-10-02T23:18:58.862368: step 2997, loss 0.0335244, acc 0.984375, learning_rate 0.000100023
2017-10-02T23:19:00.021020: step 2998, loss 0.0278856, acc 0.984375, learning_rate 0.000100023
2017-10-02T23:19:01.185229: step 2999, loss 0.036715, acc 0.984375, learning_rate 0.000100023
2017-10-02T23:19:02.339546: step 3000, loss 0.020667, acc 1, learning_rate 0.000100023

Evaluation:
2017-10-02T23:19:02.661605: step 3000, loss 1.44888, acc 0.464748

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3000

2017-10-02T23:19:09.670392: step 3001, loss 0.0141395, acc 1, learning_rate 0.000100023
2017-10-02T23:19:10.855581: step 3002, loss 0.0380368, acc 0.984375, learning_rate 0.000100023
2017-10-02T23:19:12.004248: step 3003, loss 0.0139291, acc 1, learning_rate 0.000100023
2017-10-02T23:19:13.153071: step 3004, loss 0.0158355, acc 1, learning_rate 0.000100023
2017-10-02T23:19:14.307270: step 3005, loss 0.0135249, acc 1, learning_rate 0.000100023
2017-10-02T23:19:15.448230: step 3006, loss 0.0176068, acc 1, learning_rate 0.000100023
2017-10-02T23:19:16.596997: step 3007, loss 0.0100941, acc 1, learning_rate 0.000100022
2017-10-02T23:19:17.827312: step 3008, loss 0.0406286, acc 0.984375, learning_rate 0.000100022
2017-10-02T23:19:18.984182: step 3009, loss 0.0077634, acc 1, learning_rate 0.000100022
2017-10-02T23:19:20.130667: step 3010, loss 0.0120809, acc 1, learning_rate 0.000100022
2017-10-02T23:19:21.280951: step 3011, loss 0.00804604, acc 1, learning_rate 0.000100022
2017-10-02T23:19:22.445029: step 3012, loss 0.0105075, acc 1, learning_rate 0.000100022
2017-10-02T23:19:23.599016: step 3013, loss 0.0333317, acc 0.984375, learning_rate 0.000100022
2017-10-02T23:19:24.749054: step 3014, loss 0.00958583, acc 1, learning_rate 0.000100022
2017-10-02T23:19:25.896168: step 3015, loss 0.00882099, acc 1, learning_rate 0.000100022
2017-10-02T23:19:27.040932: step 3016, loss 0.0196455, acc 1, learning_rate 0.000100022
2017-10-02T23:19:28.185800: step 3017, loss 0.0138415, acc 1, learning_rate 0.000100022
2017-10-02T23:19:29.343858: step 3018, loss 0.0408676, acc 0.984375, learning_rate 0.000100021
2017-10-02T23:19:30.486163: step 3019, loss 0.0296764, acc 0.984375, learning_rate 0.000100021
2017-10-02T23:19:31.641432: step 3020, loss 0.00688358, acc 1, learning_rate 0.000100021
2017-10-02T23:19:32.784823: step 3021, loss 0.018757, acc 1, learning_rate 0.000100021
2017-10-02T23:19:33.942493: step 3022, loss 0.016439, acc 1, learning_rate 0.000100021
2017-10-02T23:19:35.112613: step 3023, loss 0.0118767, acc 1, learning_rate 0.000100021
2017-10-02T23:19:36.256948: step 3024, loss 0.0293326, acc 0.984375, learning_rate 0.000100021
2017-10-02T23:19:37.413278: step 3025, loss 0.016196, acc 1, learning_rate 0.000100021
2017-10-02T23:19:38.567014: step 3026, loss 0.0101854, acc 1, learning_rate 0.000100021
2017-10-02T23:19:39.719460: step 3027, loss 0.0133132, acc 1, learning_rate 0.000100021
2017-10-02T23:19:40.887730: step 3028, loss 0.0586834, acc 0.96875, learning_rate 0.000100021
2017-10-02T23:19:42.032128: step 3029, loss 0.025701, acc 1, learning_rate 0.00010002
2017-10-02T23:19:43.179643: step 3030, loss 0.0153727, acc 1, learning_rate 0.00010002
2017-10-02T23:19:44.354932: step 3031, loss 0.0110026, acc 1, learning_rate 0.00010002
2017-10-02T23:19:45.502122: step 3032, loss 0.030587, acc 0.984375, learning_rate 0.00010002
2017-10-02T23:19:49.205965: step 3033, loss 0.0108827, acc 1, learning_rate 0.00010002
2017-10-02T23:19:50.356918: step 3034, loss 0.0377396, acc 1, learning_rate 0.00010002
2017-10-02T23:19:51.514486: step 3035, loss 0.0252092, acc 1, learning_rate 0.00010002
2017-10-02T23:19:53.315714: step 3036, loss 0.010849, acc 1, learning_rate 0.00010002
2017-10-02T23:19:54.533404: step 3037, loss 0.00970391, acc 1, learning_rate 0.00010002
2017-10-02T23:19:55.670750: step 3038, loss 0.0189088, acc 1, learning_rate 0.00010002
2017-10-02T23:19:56.824401: step 3039, loss 0.013852, acc 1, learning_rate 0.00010002
2017-10-02T23:19:57.983812: step 3040, loss 0.0465379, acc 0.984375, learning_rate 0.00010002

Evaluation:
2017-10-02T23:19:58.331655: step 3040, loss 1.46404, acc 0.410072

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3040

2017-10-02T23:20:06.429485: step 3041, loss 0.00708097, acc 1, learning_rate 0.00010002
2017-10-02T23:20:07.587193: step 3042, loss 0.0379857, acc 0.984375, learning_rate 0.000100019
2017-10-02T23:20:08.738165: step 3043, loss 0.0112051, acc 1, learning_rate 0.000100019
2017-10-02T23:20:09.892426: step 3044, loss 0.0241954, acc 0.984375, learning_rate 0.000100019
2017-10-02T23:20:11.038567: step 3045, loss 0.0136484, acc 1, learning_rate 0.000100019
2017-10-02T23:20:12.169193: step 3046, loss 0.0245663, acc 0.984375, learning_rate 0.000100019
2017-10-02T23:20:13.334554: step 3047, loss 0.0790315, acc 0.96875, learning_rate 0.000100019
2017-10-02T23:20:14.477970: step 3048, loss 0.0133783, acc 1, learning_rate 0.000100019
2017-10-02T23:20:15.641115: step 3049, loss 0.0113628, acc 1, learning_rate 0.000100019
2017-10-02T23:20:16.784344: step 3050, loss 0.00990046, acc 1, learning_rate 0.000100019
2017-10-02T23:20:17.931730: step 3051, loss 0.00857725, acc 1, learning_rate 0.000100019
2017-10-02T23:20:19.084401: step 3052, loss 0.0172526, acc 1, learning_rate 0.000100019
2017-10-02T23:20:20.223066: step 3053, loss 0.0222384, acc 1, learning_rate 0.000100019
2017-10-02T23:20:21.381837: step 3054, loss 0.0164662, acc 1, learning_rate 0.000100018
2017-10-02T23:20:22.525455: step 3055, loss 0.0153782, acc 1, learning_rate 0.000100018
2017-10-02T23:20:23.684562: step 3056, loss 0.0364386, acc 0.984375, learning_rate 0.000100018
2017-10-02T23:20:24.833748: step 3057, loss 0.013904, acc 1, learning_rate 0.000100018
2017-10-02T23:20:26.009929: step 3058, loss 0.0402114, acc 0.984375, learning_rate 0.000100018
2017-10-02T23:20:27.174379: step 3059, loss 0.00932723, acc 1, learning_rate 0.000100018
2017-10-02T23:20:28.327301: step 3060, loss 0.0169295, acc 1, learning_rate 0.000100018
2017-10-02T23:20:29.475711: step 3061, loss 0.0231322, acc 0.984375, learning_rate 0.000100018
2017-10-02T23:20:30.621592: step 3062, loss 0.0100318, acc 1, learning_rate 0.000100018
2017-10-02T23:20:31.772449: step 3063, loss 0.0543237, acc 0.984375, learning_rate 0.000100018
2017-10-02T23:20:32.918815: step 3064, loss 0.0436773, acc 0.984375, learning_rate 0.000100018
2017-10-02T23:20:34.078165: step 3065, loss 0.0110868, acc 1, learning_rate 0.000100018
2017-10-02T23:20:35.239412: step 3066, loss 0.0121306, acc 1, learning_rate 0.000100018
2017-10-02T23:20:36.383676: step 3067, loss 0.0299975, acc 0.984375, learning_rate 0.000100018
2017-10-02T23:20:37.529139: step 3068, loss 0.0106479, acc 1, learning_rate 0.000100017
2017-10-02T23:20:38.723779: step 3069, loss 0.0113953, acc 1, learning_rate 0.000100017
2017-10-02T23:20:39.891121: step 3070, loss 0.0157035, acc 1, learning_rate 0.000100017
2017-10-02T23:20:41.033657: step 3071, loss 0.00950353, acc 1, learning_rate 0.000100017
2017-10-02T23:20:42.179433: step 3072, loss 0.00992408, acc 1, learning_rate 0.000100017
2017-10-02T23:20:43.351010: step 3073, loss 0.0187283, acc 1, learning_rate 0.000100017
2017-10-02T23:20:44.503665: step 3074, loss 0.0277846, acc 0.984375, learning_rate 0.000100017
2017-10-02T23:20:45.659464: step 3075, loss 0.00854086, acc 1, learning_rate 0.000100017
2017-10-02T23:20:46.832290: step 3076, loss 0.00724854, acc 1, learning_rate 0.000100017
2017-10-02T23:20:47.993648: step 3077, loss 0.0548816, acc 0.984375, learning_rate 0.000100017
2017-10-02T23:20:49.146352: step 3078, loss 0.0125888, acc 1, learning_rate 0.000100017
2017-10-02T23:20:50.294992: step 3079, loss 0.0341569, acc 0.984375, learning_rate 0.000100017
2017-10-02T23:20:51.441905: step 3080, loss 0.0130811, acc 1, learning_rate 0.000100017

Evaluation:
2017-10-02T23:20:51.778848: step 3080, loss 1.44617, acc 0.464748

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3080

2017-10-02T23:20:59.918647: step 3081, loss 0.00811731, acc 1, learning_rate 0.000100017
2017-10-02T23:21:01.079614: step 3082, loss 0.0114557, acc 1, learning_rate 0.000100016
2017-10-02T23:21:02.223165: step 3083, loss 0.0249694, acc 0.984375, learning_rate 0.000100016
2017-10-02T23:21:03.371164: step 3084, loss 0.0132281, acc 1, learning_rate 0.000100016
2017-10-02T23:21:04.516421: step 3085, loss 0.0683458, acc 0.984375, learning_rate 0.000100016
2017-10-02T23:21:05.662111: step 3086, loss 0.00814504, acc 1, learning_rate 0.000100016
2017-10-02T23:21:06.817134: step 3087, loss 0.0178939, acc 1, learning_rate 0.000100016
2017-10-02T23:21:07.965893: step 3088, loss 0.0503537, acc 0.96875, learning_rate 0.000100016
2017-10-02T23:21:09.117759: step 3089, loss 0.0131182, acc 1, learning_rate 0.000100016
2017-10-02T23:21:10.284448: step 3090, loss 0.016096, acc 1, learning_rate 0.000100016
2017-10-02T23:21:11.437386: step 3091, loss 0.079738, acc 0.984375, learning_rate 0.000100016
2017-10-02T23:21:12.599087: step 3092, loss 0.0466033, acc 0.984375, learning_rate 0.000100016
2017-10-02T23:21:13.774144: step 3093, loss 0.00921408, acc 1, learning_rate 0.000100016
2017-10-02T23:21:14.957234: step 3094, loss 0.0173598, acc 1, learning_rate 0.000100016
2017-10-02T23:21:16.131191: step 3095, loss 0.00960327, acc 1, learning_rate 0.000100016
2017-10-02T23:21:17.296814: step 3096, loss 0.00698757, acc 1, learning_rate 0.000100016
2017-10-02T23:21:18.445944: step 3097, loss 0.00686864, acc 1, learning_rate 0.000100016
2017-10-02T23:21:19.600432: step 3098, loss 0.00933271, acc 1, learning_rate 0.000100015
2017-10-02T23:21:20.768236: step 3099, loss 0.00888843, acc 1, learning_rate 0.000100015
2017-10-02T23:21:21.920871: step 3100, loss 0.0532834, acc 0.984375, learning_rate 0.000100015
2017-10-02T23:21:23.072449: step 3101, loss 0.0533974, acc 0.984375, learning_rate 0.000100015
2017-10-02T23:21:24.229535: step 3102, loss 0.0122125, acc 1, learning_rate 0.000100015
2017-10-02T23:21:25.381959: step 3103, loss 0.0200361, acc 0.984375, learning_rate 0.000100015
2017-10-02T23:21:26.643310: step 3104, loss 0.0160664, acc 1, learning_rate 0.000100015
2017-10-02T23:21:27.792436: step 3105, loss 0.0103028, acc 1, learning_rate 0.000100015
2017-10-02T23:21:28.937092: step 3106, loss 0.0149524, acc 1, learning_rate 0.000100015
2017-10-02T23:21:30.085420: step 3107, loss 0.0199995, acc 1, learning_rate 0.000100015
2017-10-02T23:21:31.246482: step 3108, loss 0.00809711, acc 1, learning_rate 0.000100015
2017-10-02T23:21:32.388195: step 3109, loss 0.016387, acc 1, learning_rate 0.000100015
2017-10-02T23:21:33.662688: step 3110, loss 0.0115231, acc 1, learning_rate 0.000100015
2017-10-02T23:21:34.805006: step 3111, loss 0.048656, acc 0.984375, learning_rate 0.000100015
2017-10-02T23:21:35.969694: step 3112, loss 0.00957447, acc 1, learning_rate 0.000100015
2017-10-02T23:21:37.126114: step 3113, loss 0.00907936, acc 1, learning_rate 0.000100015
2017-10-02T23:21:38.255284: step 3114, loss 0.0117456, acc 1, learning_rate 0.000100014
2017-10-02T23:21:39.418024: step 3115, loss 0.0270609, acc 0.984375, learning_rate 0.000100014
2017-10-02T23:21:40.573787: step 3116, loss 0.0185001, acc 1, learning_rate 0.000100014
2017-10-02T23:21:41.729295: step 3117, loss 0.0546007, acc 0.984375, learning_rate 0.000100014
2017-10-02T23:21:42.930992: step 3118, loss 0.00691973, acc 1, learning_rate 0.000100014
2017-10-02T23:21:44.082784: step 3119, loss 0.0242947, acc 0.984375, learning_rate 0.000100014
2017-10-02T23:21:45.247074: step 3120, loss 0.0209224, acc 1, learning_rate 0.000100014

Evaluation:
2017-10-02T23:21:45.578536: step 3120, loss 1.44045, acc 0.476259

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3120

2017-10-02T23:21:53.417924: step 3121, loss 0.0420648, acc 0.984375, learning_rate 0.000100014
2017-10-02T23:21:54.720337: step 3122, loss 0.0315523, acc 0.984375, learning_rate 0.000100014
2017-10-02T23:21:55.865454: step 3123, loss 0.027199, acc 1, learning_rate 0.000100014
2017-10-02T23:21:57.021913: step 3124, loss 0.0212883, acc 1, learning_rate 0.000100014
2017-10-02T23:21:58.165751: step 3125, loss 0.031185, acc 1, learning_rate 0.000100014
2017-10-02T23:21:59.317227: step 3126, loss 0.0191307, acc 1, learning_rate 0.000100014
2017-10-02T23:22:00.467403: step 3127, loss 0.0368501, acc 0.984375, learning_rate 0.000100014
2017-10-02T23:22:01.641991: step 3128, loss 0.00724865, acc 1, learning_rate 0.000100014
2017-10-02T23:22:02.797891: step 3129, loss 0.021065, acc 1, learning_rate 0.000100014
2017-10-02T23:22:03.946314: step 3130, loss 0.00653708, acc 1, learning_rate 0.000100014
2017-10-02T23:22:05.098960: step 3131, loss 0.0248322, acc 1, learning_rate 0.000100014
2017-10-02T23:22:06.264876: step 3132, loss 0.0243469, acc 1, learning_rate 0.000100013
2017-10-02T23:22:07.708784: step 3133, loss 0.0198109, acc 1, learning_rate 0.000100013
2017-10-02T23:22:08.857616: step 3134, loss 0.0230003, acc 1, learning_rate 0.000100013
2017-10-02T23:22:10.021875: step 3135, loss 0.0156795, acc 1, learning_rate 0.000100013
2017-10-02T23:22:11.137222: step 3136, loss 0.0082622, acc 1, learning_rate 0.000100013
2017-10-02T23:22:12.286555: step 3137, loss 0.00856229, acc 1, learning_rate 0.000100013
2017-10-02T23:22:13.455978: step 3138, loss 0.00871205, acc 1, learning_rate 0.000100013
2017-10-02T23:22:14.731011: step 3139, loss 0.0452149, acc 0.984375, learning_rate 0.000100013
2017-10-02T23:22:15.880136: step 3140, loss 0.0153129, acc 1, learning_rate 0.000100013
2017-10-02T23:22:17.022773: step 3141, loss 0.0152104, acc 1, learning_rate 0.000100013
2017-10-02T23:22:18.169884: step 3142, loss 0.0225693, acc 1, learning_rate 0.000100013
2017-10-02T23:22:19.315024: step 3143, loss 0.0325483, acc 0.984375, learning_rate 0.000100013
2017-10-02T23:22:20.465485: step 3144, loss 0.0159113, acc 1, learning_rate 0.000100013
2017-10-02T23:22:21.702554: step 3145, loss 0.0148154, acc 1, learning_rate 0.000100013
2017-10-02T23:22:22.866992: step 3146, loss 0.0104111, acc 1, learning_rate 0.000100013
2017-10-02T23:22:24.010092: step 3147, loss 0.0123264, acc 1, learning_rate 0.000100013
2017-10-02T23:22:25.156565: step 3148, loss 0.00659521, acc 1, learning_rate 0.000100013
2017-10-02T23:22:26.312044: step 3149, loss 0.0135173, acc 1, learning_rate 0.000100013
2017-10-02T23:22:27.463016: step 3150, loss 0.0083601, acc 1, learning_rate 0.000100012
2017-10-02T23:22:28.623635: step 3151, loss 0.00993175, acc 1, learning_rate 0.000100012
2017-10-02T23:22:29.768968: step 3152, loss 0.0266359, acc 0.984375, learning_rate 0.000100012
2017-10-02T23:22:30.921879: step 3153, loss 0.0153152, acc 1, learning_rate 0.000100012
2017-10-02T23:22:32.066588: step 3154, loss 0.0324682, acc 1, learning_rate 0.000100012
2017-10-02T23:22:33.215025: step 3155, loss 0.0163564, acc 1, learning_rate 0.000100012
2017-10-02T23:22:34.377454: step 3156, loss 0.0126498, acc 1, learning_rate 0.000100012
2017-10-02T23:22:35.533221: step 3157, loss 0.0150151, acc 1, learning_rate 0.000100012
2017-10-02T23:22:36.693009: step 3158, loss 0.0521526, acc 0.984375, learning_rate 0.000100012
2017-10-02T23:22:37.863031: step 3159, loss 0.0249474, acc 1, learning_rate 0.000100012
2017-10-02T23:22:39.850445: step 3160, loss 0.0118312, acc 1, learning_rate 0.000100012

Evaluation:
2017-10-02T23:22:40.180022: step 3160, loss 1.43231, acc 0.489209

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3160

2017-10-02T23:22:46.916489: step 3161, loss 0.0112143, acc 1, learning_rate 0.000100012
2017-10-02T23:22:48.084506: step 3162, loss 0.00969244, acc 1, learning_rate 0.000100012
2017-10-02T23:22:49.237527: step 3163, loss 0.00843372, acc 1, learning_rate 0.000100012
2017-10-02T23:22:50.393007: step 3164, loss 0.0085831, acc 1, learning_rate 0.000100012
2017-10-02T23:22:51.545518: step 3165, loss 0.011333, acc 1, learning_rate 0.000100012
2017-10-02T23:22:52.717600: step 3166, loss 0.00952028, acc 1, learning_rate 0.000100012
2017-10-02T23:22:53.862481: step 3167, loss 0.038088, acc 0.984375, learning_rate 0.000100012
2017-10-02T23:22:55.019721: step 3168, loss 0.0171463, acc 1, learning_rate 0.000100012
2017-10-02T23:22:56.178580: step 3169, loss 0.00716004, acc 1, learning_rate 0.000100012
2017-10-02T23:22:57.340106: step 3170, loss 0.0623555, acc 0.96875, learning_rate 0.000100012
2017-10-02T23:22:58.491411: step 3171, loss 0.0114044, acc 1, learning_rate 0.000100011
2017-10-02T23:22:59.651357: step 3172, loss 0.00924018, acc 1, learning_rate 0.000100011
2017-10-02T23:23:00.794546: step 3173, loss 0.0153474, acc 1, learning_rate 0.000100011
2017-10-02T23:23:01.946868: step 3174, loss 0.0294968, acc 0.984375, learning_rate 0.000100011
2017-10-02T23:23:03.099676: step 3175, loss 0.0126364, acc 1, learning_rate 0.000100011
2017-10-02T23:23:04.251414: step 3176, loss 0.0110137, acc 1, learning_rate 0.000100011
2017-10-02T23:23:05.418498: step 3177, loss 0.0364158, acc 0.984375, learning_rate 0.000100011
2017-10-02T23:23:06.587854: step 3178, loss 0.0207764, acc 0.984375, learning_rate 0.000100011
2017-10-02T23:23:07.749944: step 3179, loss 0.0515108, acc 0.984375, learning_rate 0.000100011
2017-10-02T23:23:08.964663: step 3180, loss 0.0436323, acc 0.984375, learning_rate 0.000100011
2017-10-02T23:23:10.119415: step 3181, loss 0.013948, acc 1, learning_rate 0.000100011
2017-10-02T23:23:11.280846: step 3182, loss 0.00904183, acc 1, learning_rate 0.000100011
2017-10-02T23:23:12.427391: step 3183, loss 0.0120543, acc 1, learning_rate 0.000100011
2017-10-02T23:23:13.575896: step 3184, loss 0.00827693, acc 1, learning_rate 0.000100011
2017-10-02T23:23:14.757399: step 3185, loss 0.0389673, acc 0.984375, learning_rate 0.000100011
2017-10-02T23:23:15.918087: step 3186, loss 0.00799429, acc 1, learning_rate 0.000100011
2017-10-02T23:23:17.077148: step 3187, loss 0.0312774, acc 0.984375, learning_rate 0.000100011
2017-10-02T23:23:18.237787: step 3188, loss 0.0131268, acc 1, learning_rate 0.000100011
2017-10-02T23:23:19.385489: step 3189, loss 0.0480332, acc 0.96875, learning_rate 0.000100011
2017-10-02T23:23:20.544099: step 3190, loss 0.00728042, acc 1, learning_rate 0.000100011
2017-10-02T23:23:21.693532: step 3191, loss 0.0104112, acc 1, learning_rate 0.000100011
2017-10-02T23:23:22.843116: step 3192, loss 0.00698464, acc 1, learning_rate 0.000100011
2017-10-02T23:23:23.997155: step 3193, loss 0.0109264, acc 1, learning_rate 0.00010001
2017-10-02T23:23:25.145986: step 3194, loss 0.0115146, acc 1, learning_rate 0.00010001
2017-10-02T23:23:26.306833: step 3195, loss 0.0258743, acc 1, learning_rate 0.00010001
2017-10-02T23:23:27.487751: step 3196, loss 0.00961212, acc 1, learning_rate 0.00010001
2017-10-02T23:23:28.637317: step 3197, loss 0.00849732, acc 1, learning_rate 0.00010001
2017-10-02T23:23:29.784494: step 3198, loss 0.0151836, acc 1, learning_rate 0.00010001
2017-10-02T23:23:30.937727: step 3199, loss 0.0123708, acc 1, learning_rate 0.00010001
2017-10-02T23:23:32.089166: step 3200, loss 0.0150604, acc 1, learning_rate 0.00010001

Evaluation:
2017-10-02T23:23:32.413185: step 3200, loss 1.44898, acc 0.46187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3200

2017-10-02T23:23:40.123722: step 3201, loss 0.00959107, acc 1, learning_rate 0.00010001
2017-10-02T23:23:41.285969: step 3202, loss 0.0393321, acc 0.984375, learning_rate 0.00010001
2017-10-02T23:23:42.475763: step 3203, loss 0.0160509, acc 1, learning_rate 0.00010001
2017-10-02T23:23:43.629799: step 3204, loss 0.0116202, acc 1, learning_rate 0.00010001
2017-10-02T23:23:44.800423: step 3205, loss 0.011895, acc 1, learning_rate 0.00010001
2017-10-02T23:23:45.955966: step 3206, loss 0.00711049, acc 1, learning_rate 0.00010001
2017-10-02T23:23:47.113597: step 3207, loss 0.0124739, acc 1, learning_rate 0.00010001
2017-10-02T23:23:48.275084: step 3208, loss 0.0407364, acc 0.984375, learning_rate 0.00010001
2017-10-02T23:23:49.429522: step 3209, loss 0.0252233, acc 1, learning_rate 0.00010001
2017-10-02T23:23:50.583551: step 3210, loss 0.0105801, acc 1, learning_rate 0.00010001
2017-10-02T23:23:51.730664: step 3211, loss 0.0118926, acc 1, learning_rate 0.00010001
2017-10-02T23:23:52.886126: step 3212, loss 0.0353531, acc 0.984375, learning_rate 0.00010001
2017-10-02T23:23:54.047474: step 3213, loss 0.0488926, acc 0.984375, learning_rate 0.00010001
2017-10-02T23:23:55.208094: step 3214, loss 0.0612496, acc 0.96875, learning_rate 0.00010001
2017-10-02T23:23:56.372326: step 3215, loss 0.0161598, acc 1, learning_rate 0.00010001
2017-10-02T23:23:57.618781: step 3216, loss 0.0102025, acc 1, learning_rate 0.00010001
2017-10-02T23:23:58.830457: step 3217, loss 0.0673906, acc 0.96875, learning_rate 0.000100009
2017-10-02T23:24:00.214810: step 3218, loss 0.0324821, acc 0.984375, learning_rate 0.000100009
2017-10-02T23:24:01.369125: step 3219, loss 0.0116896, acc 1, learning_rate 0.000100009
2017-10-02T23:24:02.527003: step 3220, loss 0.00757492, acc 1, learning_rate 0.000100009
2017-10-02T23:24:03.678614: step 3221, loss 0.0152412, acc 1, learning_rate 0.000100009
2017-10-02T23:24:04.859787: step 3222, loss 0.035069, acc 0.984375, learning_rate 0.000100009
2017-10-02T23:24:06.019626: step 3223, loss 0.00804231, acc 1, learning_rate 0.000100009
2017-10-02T23:24:07.183155: step 3224, loss 0.00537874, acc 1, learning_rate 0.000100009
2017-10-02T23:24:08.346938: step 3225, loss 0.0130935, acc 1, learning_rate 0.000100009
2017-10-02T23:24:09.497032: step 3226, loss 0.00664074, acc 1, learning_rate 0.000100009
2017-10-02T23:24:10.643080: step 3227, loss 0.0126691, acc 1, learning_rate 0.000100009
2017-10-02T23:24:11.791929: step 3228, loss 0.0100984, acc 1, learning_rate 0.000100009
2017-10-02T23:24:12.952109: step 3229, loss 0.0284066, acc 0.984375, learning_rate 0.000100009
2017-10-02T23:24:14.118945: step 3230, loss 0.0265143, acc 0.984375, learning_rate 0.000100009
2017-10-02T23:24:15.277103: step 3231, loss 0.0340444, acc 0.984375, learning_rate 0.000100009
2017-10-02T23:24:16.442716: step 3232, loss 0.00699065, acc 1, learning_rate 0.000100009
2017-10-02T23:24:17.610323: step 3233, loss 0.0185703, acc 1, learning_rate 0.000100009
2017-10-02T23:24:18.955517: step 3234, loss 0.017196, acc 1, learning_rate 0.000100009
2017-10-02T23:24:20.122985: step 3235, loss 0.0250953, acc 1, learning_rate 0.000100009
2017-10-02T23:24:21.273405: step 3236, loss 0.0396536, acc 0.984375, learning_rate 0.000100009
2017-10-02T23:24:22.478058: step 3237, loss 0.00579355, acc 1, learning_rate 0.000100009
2017-10-02T23:24:23.633076: step 3238, loss 0.0224204, acc 0.984375, learning_rate 0.000100009
2017-10-02T23:24:24.790417: step 3239, loss 0.0707226, acc 0.96875, learning_rate 0.000100009
2017-10-02T23:24:25.947044: step 3240, loss 0.0442521, acc 0.984375, learning_rate 0.000100009

Evaluation:
2017-10-02T23:24:26.290426: step 3240, loss 1.44184, acc 0.464748

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3240

2017-10-02T23:24:34.485927: step 3241, loss 0.0126056, acc 1, learning_rate 0.000100009
2017-10-02T23:24:35.701517: step 3242, loss 0.00705947, acc 1, learning_rate 0.000100009
2017-10-02T23:24:36.840141: step 3243, loss 0.0117136, acc 1, learning_rate 0.000100009
2017-10-02T23:24:37.996882: step 3244, loss 0.0464783, acc 0.96875, learning_rate 0.000100009
2017-10-02T23:24:39.155162: step 3245, loss 0.0140002, acc 1, learning_rate 0.000100008
2017-10-02T23:24:40.308216: step 3246, loss 0.0620266, acc 0.96875, learning_rate 0.000100008
2017-10-02T23:24:41.469116: step 3247, loss 0.0357087, acc 0.984375, learning_rate 0.000100008
2017-10-02T23:24:42.630569: step 3248, loss 0.0188613, acc 1, learning_rate 0.000100008
2017-10-02T23:24:43.779311: step 3249, loss 0.0111063, acc 1, learning_rate 0.000100008
2017-10-02T23:24:44.973033: step 3250, loss 0.0204154, acc 1, learning_rate 0.000100008
2017-10-02T23:24:46.133787: step 3251, loss 0.0693652, acc 0.96875, learning_rate 0.000100008
2017-10-02T23:24:47.281308: step 3252, loss 0.0113127, acc 1, learning_rate 0.000100008
2017-10-02T23:24:48.446057: step 3253, loss 0.0114223, acc 1, learning_rate 0.000100008
2017-10-02T23:24:49.585051: step 3254, loss 0.00672886, acc 1, learning_rate 0.000100008
2017-10-02T23:24:50.742196: step 3255, loss 0.00733481, acc 1, learning_rate 0.000100008
2017-10-02T23:24:51.898340: step 3256, loss 0.0155123, acc 1, learning_rate 0.000100008
2017-10-02T23:24:53.055092: step 3257, loss 0.0116217, acc 1, learning_rate 0.000100008
2017-10-02T23:24:54.201375: step 3258, loss 0.0351192, acc 0.984375, learning_rate 0.000100008
2017-10-02T23:24:55.363752: step 3259, loss 0.0229905, acc 0.984375, learning_rate 0.000100008
2017-10-02T23:24:56.521632: step 3260, loss 0.0157864, acc 1, learning_rate 0.000100008
2017-10-02T23:24:57.666825: step 3261, loss 0.00719687, acc 1, learning_rate 0.000100008
2017-10-02T23:24:58.816536: step 3262, loss 0.0068011, acc 1, learning_rate 0.000100008
2017-10-02T23:24:59.977126: step 3263, loss 0.0113349, acc 1, learning_rate 0.000100008
2017-10-02T23:25:01.125760: step 3264, loss 0.0409622, acc 0.984375, learning_rate 0.000100008
2017-10-02T23:25:02.289390: step 3265, loss 0.00755215, acc 1, learning_rate 0.000100008
2017-10-02T23:25:03.464473: step 3266, loss 0.0165557, acc 1, learning_rate 0.000100008
2017-10-02T23:25:04.620885: step 3267, loss 0.0104073, acc 1, learning_rate 0.000100008
2017-10-02T23:25:05.779427: step 3268, loss 0.0190941, acc 1, learning_rate 0.000100008
2017-10-02T23:25:06.937846: step 3269, loss 0.0312684, acc 0.984375, learning_rate 0.000100008
2017-10-02T23:25:08.091124: step 3270, loss 0.0324277, acc 1, learning_rate 0.000100008
2017-10-02T23:25:09.252680: step 3271, loss 0.0119117, acc 1, learning_rate 0.000100008
2017-10-02T23:25:10.419048: step 3272, loss 0.0102693, acc 1, learning_rate 0.000100008
2017-10-02T23:25:11.572492: step 3273, loss 0.010772, acc 1, learning_rate 0.000100008
2017-10-02T23:25:12.740165: step 3274, loss 0.00997443, acc 1, learning_rate 0.000100008
2017-10-02T23:25:13.892809: step 3275, loss 0.00682417, acc 1, learning_rate 0.000100007
2017-10-02T23:25:15.046367: step 3276, loss 0.0290395, acc 0.984375, learning_rate 0.000100007
2017-10-02T23:25:16.192185: step 3277, loss 0.0114732, acc 1, learning_rate 0.000100007
2017-10-02T23:25:17.347124: step 3278, loss 0.0106682, acc 1, learning_rate 0.000100007
2017-10-02T23:25:18.508324: step 3279, loss 0.0670547, acc 0.96875, learning_rate 0.000100007
2017-10-02T23:25:19.664003: step 3280, loss 0.0120405, acc 1, learning_rate 0.000100007

Evaluation:
2017-10-02T23:25:19.986494: step 3280, loss 1.42804, acc 0.489209

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3280

2017-10-02T23:25:27.789546: step 3281, loss 0.0127922, acc 1, learning_rate 0.000100007
2017-10-02T23:25:28.982123: step 3282, loss 0.00600559, acc 1, learning_rate 0.000100007
2017-10-02T23:25:30.125346: step 3283, loss 0.00826912, acc 1, learning_rate 0.000100007
2017-10-02T23:25:31.276604: step 3284, loss 0.0107565, acc 1, learning_rate 0.000100007
2017-10-02T23:25:32.424637: step 3285, loss 0.0506826, acc 0.984375, learning_rate 0.000100007
2017-10-02T23:25:33.586078: step 3286, loss 0.0319798, acc 0.984375, learning_rate 0.000100007
2017-10-02T23:25:34.735098: step 3287, loss 0.0457691, acc 0.984375, learning_rate 0.000100007
2017-10-02T23:25:35.883602: step 3288, loss 0.0221541, acc 1, learning_rate 0.000100007
2017-10-02T23:25:37.039601: step 3289, loss 0.0184211, acc 1, learning_rate 0.000100007
2017-10-02T23:25:38.207281: step 3290, loss 0.00872975, acc 1, learning_rate 0.000100007
2017-10-02T23:25:39.361732: step 3291, loss 0.0216213, acc 1, learning_rate 0.000100007
2017-10-02T23:25:40.518968: step 3292, loss 0.0341422, acc 0.984375, learning_rate 0.000100007
2017-10-02T23:25:41.680745: step 3293, loss 0.00839103, acc 1, learning_rate 0.000100007
2017-10-02T23:25:42.824112: step 3294, loss 0.0557076, acc 0.984375, learning_rate 0.000100007
2017-10-02T23:25:43.960010: step 3295, loss 0.0395258, acc 0.984375, learning_rate 0.000100007
2017-10-02T23:25:45.123405: step 3296, loss 0.0113402, acc 1, learning_rate 0.000100007
2017-10-02T23:25:46.278896: step 3297, loss 0.0185856, acc 1, learning_rate 0.000100007
2017-10-02T23:25:47.436567: step 3298, loss 0.0110765, acc 1, learning_rate 0.000100007
2017-10-02T23:25:48.588778: step 3299, loss 0.0144871, acc 1, learning_rate 0.000100007
2017-10-02T23:25:49.737781: step 3300, loss 0.0274931, acc 1, learning_rate 0.000100007
2017-10-02T23:25:50.939592: step 3301, loss 0.0106719, acc 1, learning_rate 0.000100007
2017-10-02T23:25:52.089488: step 3302, loss 0.0421036, acc 0.984375, learning_rate 0.000100007
2017-10-02T23:25:53.247528: step 3303, loss 0.0147264, acc 1, learning_rate 0.000100007
2017-10-02T23:25:54.405985: step 3304, loss 0.00799795, acc 1, learning_rate 0.000100007
2017-10-02T23:25:55.565729: step 3305, loss 0.0107893, acc 1, learning_rate 0.000100007
2017-10-02T23:25:56.733686: step 3306, loss 0.00783255, acc 1, learning_rate 0.000100007
2017-10-02T23:25:57.877542: step 3307, loss 0.00809741, acc 1, learning_rate 0.000100007
2017-10-02T23:25:59.026860: step 3308, loss 0.0115969, acc 1, learning_rate 0.000100007
2017-10-02T23:26:00.175340: step 3309, loss 0.00830874, acc 1, learning_rate 0.000100007
2017-10-02T23:26:01.326950: step 3310, loss 0.0146854, acc 1, learning_rate 0.000100006
2017-10-02T23:26:02.563697: step 3311, loss 0.0154722, acc 1, learning_rate 0.000100006
2017-10-02T23:26:03.709516: step 3312, loss 0.00966009, acc 1, learning_rate 0.000100006
2017-10-02T23:26:04.852639: step 3313, loss 0.00986173, acc 1, learning_rate 0.000100006
2017-10-02T23:26:05.998260: step 3314, loss 0.0125197, acc 1, learning_rate 0.000100006
2017-10-02T23:26:07.158046: step 3315, loss 0.0113648, acc 1, learning_rate 0.000100006
2017-10-02T23:26:08.314646: step 3316, loss 0.00947282, acc 1, learning_rate 0.000100006
2017-10-02T23:26:09.559632: step 3317, loss 0.0134538, acc 1, learning_rate 0.000100006
2017-10-02T23:26:10.730066: step 3318, loss 0.010554, acc 1, learning_rate 0.000100006
2017-10-02T23:26:11.874628: step 3319, loss 0.0273439, acc 0.984375, learning_rate 0.000100006
2017-10-02T23:26:13.033162: step 3320, loss 0.0259814, acc 0.984375, learning_rate 0.000100006

Evaluation:
2017-10-02T23:26:13.381880: step 3320, loss 1.45246, acc 0.46187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3320

2017-10-02T23:26:21.095799: step 3321, loss 0.0471494, acc 0.984375, learning_rate 0.000100006
2017-10-02T23:26:22.259282: step 3322, loss 0.017027, acc 1, learning_rate 0.000100006
2017-10-02T23:26:23.418954: step 3323, loss 0.00907674, acc 1, learning_rate 0.000100006
2017-10-02T23:26:24.584013: step 3324, loss 0.0138077, acc 1, learning_rate 0.000100006
2017-10-02T23:26:25.730623: step 3325, loss 0.0394815, acc 0.984375, learning_rate 0.000100006
2017-10-02T23:26:26.885936: step 3326, loss 0.0419285, acc 0.984375, learning_rate 0.000100006
2017-10-02T23:26:28.035517: step 3327, loss 0.0175833, acc 1, learning_rate 0.000100006
2017-10-02T23:26:29.190393: step 3328, loss 0.0144106, acc 1, learning_rate 0.000100006
2017-10-02T23:26:30.340521: step 3329, loss 0.00971589, acc 1, learning_rate 0.000100006
2017-10-02T23:26:31.484506: step 3330, loss 0.0302938, acc 0.984375, learning_rate 0.000100006
2017-10-02T23:26:32.656832: step 3331, loss 0.0087712, acc 1, learning_rate 0.000100006
2017-10-02T23:26:33.791386: step 3332, loss 0.0468378, acc 0.980392, learning_rate 0.000100006
2017-10-02T23:26:34.952323: step 3333, loss 0.00783031, acc 1, learning_rate 0.000100006
2017-10-02T23:26:36.086126: step 3334, loss 0.0115115, acc 1, learning_rate 0.000100006
2017-10-02T23:26:37.243828: step 3335, loss 0.0131643, acc 1, learning_rate 0.000100006
2017-10-02T23:26:38.404850: step 3336, loss 0.011283, acc 1, learning_rate 0.000100006
2017-10-02T23:26:39.563609: step 3337, loss 0.0196446, acc 0.984375, learning_rate 0.000100006
2017-10-02T23:26:40.772699: step 3338, loss 0.0125031, acc 1, learning_rate 0.000100006
2017-10-02T23:26:41.936481: step 3339, loss 0.0137899, acc 1, learning_rate 0.000100006
2017-10-02T23:26:43.095202: step 3340, loss 0.00831651, acc 1, learning_rate 0.000100006
2017-10-02T23:26:44.247451: step 3341, loss 0.0157846, acc 1, learning_rate 0.000100006
2017-10-02T23:26:45.407113: step 3342, loss 0.0127456, acc 1, learning_rate 0.000100006
2017-10-02T23:26:46.567429: step 3343, loss 0.0163126, acc 1, learning_rate 0.000100006
2017-10-02T23:26:47.737360: step 3344, loss 0.0084102, acc 1, learning_rate 0.000100006
2017-10-02T23:26:48.889211: step 3345, loss 0.0167955, acc 1, learning_rate 0.000100006
2017-10-02T23:26:50.022370: step 3346, loss 0.0386258, acc 0.984375, learning_rate 0.000100006
2017-10-02T23:26:51.168575: step 3347, loss 0.0120847, acc 1, learning_rate 0.000100006
2017-10-02T23:26:52.308713: step 3348, loss 0.00519311, acc 1, learning_rate 0.000100006
2017-10-02T23:26:53.468014: step 3349, loss 0.0102863, acc 1, learning_rate 0.000100006
2017-10-02T23:26:54.624688: step 3350, loss 0.0348628, acc 1, learning_rate 0.000100006
2017-10-02T23:26:55.768167: step 3351, loss 0.0109442, acc 1, learning_rate 0.000100005
2017-10-02T23:26:56.919086: step 3352, loss 0.024271, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:26:58.072049: step 3353, loss 0.00579018, acc 1, learning_rate 0.000100005
2017-10-02T23:26:59.224715: step 3354, loss 0.0226268, acc 1, learning_rate 0.000100005
2017-10-02T23:27:00.376754: step 3355, loss 0.0168082, acc 1, learning_rate 0.000100005
2017-10-02T23:27:01.596445: step 3356, loss 0.0285617, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:02.752018: step 3357, loss 0.00863336, acc 1, learning_rate 0.000100005
2017-10-02T23:27:03.904002: step 3358, loss 0.0129774, acc 1, learning_rate 0.000100005
2017-10-02T23:27:05.054407: step 3359, loss 0.0111385, acc 1, learning_rate 0.000100005
2017-10-02T23:27:06.205321: step 3360, loss 0.00650908, acc 1, learning_rate 0.000100005

Evaluation:
2017-10-02T23:27:06.531568: step 3360, loss 1.43548, acc 0.483453

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3360

2017-10-02T23:27:14.246481: step 3361, loss 0.0083218, acc 1, learning_rate 0.000100005
2017-10-02T23:27:15.396470: step 3362, loss 0.0225045, acc 1, learning_rate 0.000100005
2017-10-02T23:27:16.549728: step 3363, loss 0.0123369, acc 1, learning_rate 0.000100005
2017-10-02T23:27:17.696735: step 3364, loss 0.00775538, acc 1, learning_rate 0.000100005
2017-10-02T23:27:19.021159: step 3365, loss 0.0109729, acc 1, learning_rate 0.000100005
2017-10-02T23:27:20.186453: step 3366, loss 0.0122065, acc 1, learning_rate 0.000100005
2017-10-02T23:27:21.337382: step 3367, loss 0.0293406, acc 1, learning_rate 0.000100005
2017-10-02T23:27:22.557176: step 3368, loss 0.010274, acc 1, learning_rate 0.000100005
2017-10-02T23:27:23.709958: step 3369, loss 0.00875734, acc 1, learning_rate 0.000100005
2017-10-02T23:27:24.863456: step 3370, loss 0.0225022, acc 1, learning_rate 0.000100005
2017-10-02T23:27:26.023035: step 3371, loss 0.0260593, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:27.183543: step 3372, loss 0.00585805, acc 1, learning_rate 0.000100005
2017-10-02T23:27:28.342214: step 3373, loss 0.0156608, acc 1, learning_rate 0.000100005
2017-10-02T23:27:29.486905: step 3374, loss 0.00909847, acc 1, learning_rate 0.000100005
2017-10-02T23:27:30.624821: step 3375, loss 0.0387405, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:31.773356: step 3376, loss 0.013756, acc 1, learning_rate 0.000100005
2017-10-02T23:27:32.964459: step 3377, loss 0.013245, acc 1, learning_rate 0.000100005
2017-10-02T23:27:34.109511: step 3378, loss 0.010324, acc 1, learning_rate 0.000100005
2017-10-02T23:27:35.267965: step 3379, loss 0.0286572, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:36.422858: step 3380, loss 0.0329482, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:37.578449: step 3381, loss 0.0078028, acc 1, learning_rate 0.000100005
2017-10-02T23:27:38.725893: step 3382, loss 0.0132457, acc 1, learning_rate 0.000100005
2017-10-02T23:27:39.870766: step 3383, loss 0.0130209, acc 1, learning_rate 0.000100005
2017-10-02T23:27:41.016667: step 3384, loss 0.00714111, acc 1, learning_rate 0.000100005
2017-10-02T23:27:42.169883: step 3385, loss 0.0509163, acc 0.96875, learning_rate 0.000100005
2017-10-02T23:27:43.312474: step 3386, loss 0.0113933, acc 1, learning_rate 0.000100005
2017-10-02T23:27:44.476082: step 3387, loss 0.0166495, acc 1, learning_rate 0.000100005
2017-10-02T23:27:45.632322: step 3388, loss 0.0219271, acc 1, learning_rate 0.000100005
2017-10-02T23:27:46.788519: step 3389, loss 0.00747819, acc 1, learning_rate 0.000100005
2017-10-02T23:27:47.935903: step 3390, loss 0.0558146, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:49.082178: step 3391, loss 0.0067367, acc 1, learning_rate 0.000100005
2017-10-02T23:27:50.254849: step 3392, loss 0.0139751, acc 1, learning_rate 0.000100005
2017-10-02T23:27:51.397289: step 3393, loss 0.0330396, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:52.557189: step 3394, loss 0.00744096, acc 1, learning_rate 0.000100005
2017-10-02T23:27:53.723156: step 3395, loss 0.0118896, acc 1, learning_rate 0.000100005
2017-10-02T23:27:54.873802: step 3396, loss 0.0339411, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:56.035505: step 3397, loss 0.0302302, acc 0.984375, learning_rate 0.000100005
2017-10-02T23:27:57.184767: step 3398, loss 0.0230714, acc 1, learning_rate 0.000100005
2017-10-02T23:27:58.347356: step 3399, loss 0.00583939, acc 1, learning_rate 0.000100005
2017-10-02T23:27:59.500179: step 3400, loss 0.0271541, acc 0.984375, learning_rate 0.000100004

Evaluation:
2017-10-02T23:27:59.820714: step 3400, loss 1.46174, acc 0.443165

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3400

2017-10-02T23:28:07.305628: step 3401, loss 0.0261854, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:28:08.464597: step 3402, loss 0.00838343, acc 1, learning_rate 0.000100004
2017-10-02T23:28:09.626152: step 3403, loss 0.0306324, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:28:10.779877: step 3404, loss 0.0146345, acc 1, learning_rate 0.000100004
2017-10-02T23:28:11.936354: step 3405, loss 0.0111484, acc 1, learning_rate 0.000100004
2017-10-02T23:28:13.092476: step 3406, loss 0.0141878, acc 1, learning_rate 0.000100004
2017-10-02T23:28:14.235768: step 3407, loss 0.0257123, acc 1, learning_rate 0.000100004
2017-10-02T23:28:15.391429: step 3408, loss 0.0432083, acc 0.96875, learning_rate 0.000100004
2017-10-02T23:28:16.556369: step 3409, loss 0.00972566, acc 1, learning_rate 0.000100004
2017-10-02T23:28:17.711047: step 3410, loss 0.00980799, acc 1, learning_rate 0.000100004
2017-10-02T23:28:18.854010: step 3411, loss 0.0242647, acc 1, learning_rate 0.000100004
2017-10-02T23:28:20.004501: step 3412, loss 0.0141038, acc 1, learning_rate 0.000100004
2017-10-02T23:28:21.161247: step 3413, loss 0.0235933, acc 1, learning_rate 0.000100004
2017-10-02T23:28:22.312034: step 3414, loss 0.0323656, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:28:23.466518: step 3415, loss 0.0155978, acc 1, learning_rate 0.000100004
2017-10-02T23:28:24.626575: step 3416, loss 0.0157099, acc 1, learning_rate 0.000100004
2017-10-02T23:28:25.781060: step 3417, loss 0.0221005, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:28:26.954940: step 3418, loss 0.0110782, acc 1, learning_rate 0.000100004
2017-10-02T23:28:28.105163: step 3419, loss 0.0125111, acc 1, learning_rate 0.000100004
2017-10-02T23:28:29.249605: step 3420, loss 0.0134884, acc 1, learning_rate 0.000100004
2017-10-02T23:28:30.404902: step 3421, loss 0.00688747, acc 1, learning_rate 0.000100004
2017-10-02T23:28:31.550636: step 3422, loss 0.0108506, acc 1, learning_rate 0.000100004
2017-10-02T23:28:32.696057: step 3423, loss 0.00827077, acc 1, learning_rate 0.000100004
2017-10-02T23:28:33.840693: step 3424, loss 0.0107198, acc 1, learning_rate 0.000100004
2017-10-02T23:28:34.988888: step 3425, loss 0.00727424, acc 1, learning_rate 0.000100004
2017-10-02T23:28:36.165936: step 3426, loss 0.0552798, acc 0.96875, learning_rate 0.000100004
2017-10-02T23:28:37.317416: step 3427, loss 0.0252072, acc 1, learning_rate 0.000100004
2017-10-02T23:28:38.504225: step 3428, loss 0.0120718, acc 1, learning_rate 0.000100004
2017-10-02T23:28:39.661390: step 3429, loss 0.0329374, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:28:40.809915: step 3430, loss 0.00609461, acc 1, learning_rate 0.000100004
2017-10-02T23:28:41.966687: step 3431, loss 0.00662376, acc 1, learning_rate 0.000100004
2017-10-02T23:28:43.117848: step 3432, loss 0.034851, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:28:44.264308: step 3433, loss 0.0153734, acc 1, learning_rate 0.000100004
2017-10-02T23:28:45.438762: step 3434, loss 0.0431544, acc 0.96875, learning_rate 0.000100004
2017-10-02T23:28:46.601415: step 3435, loss 0.0125925, acc 1, learning_rate 0.000100004
2017-10-02T23:28:47.743395: step 3436, loss 0.0237318, acc 1, learning_rate 0.000100004
2017-10-02T23:28:48.894037: step 3437, loss 0.0155798, acc 1, learning_rate 0.000100004
2017-10-02T23:28:50.051612: step 3438, loss 0.0187244, acc 1, learning_rate 0.000100004
2017-10-02T23:28:51.205322: step 3439, loss 0.0381978, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:28:52.366910: step 3440, loss 0.0284662, acc 0.984375, learning_rate 0.000100004

Evaluation:
2017-10-02T23:28:52.687343: step 3440, loss 1.4405, acc 0.467626

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3440

2017-10-02T23:29:00.688804: step 3441, loss 0.015126, acc 1, learning_rate 0.000100004
2017-10-02T23:29:01.865119: step 3442, loss 0.0894533, acc 0.96875, learning_rate 0.000100004
2017-10-02T23:29:03.040624: step 3443, loss 0.0188854, acc 1, learning_rate 0.000100004
2017-10-02T23:29:04.187538: step 3444, loss 0.0129323, acc 1, learning_rate 0.000100004
2017-10-02T23:29:05.344984: step 3445, loss 0.0163408, acc 1, learning_rate 0.000100004
2017-10-02T23:29:06.497017: step 3446, loss 0.00898467, acc 1, learning_rate 0.000100004
2017-10-02T23:29:07.653576: step 3447, loss 0.0125781, acc 1, learning_rate 0.000100004
2017-10-02T23:29:08.798422: step 3448, loss 0.023591, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:29:09.944681: step 3449, loss 0.00661331, acc 1, learning_rate 0.000100004
2017-10-02T23:29:11.088992: step 3450, loss 0.0104542, acc 1, learning_rate 0.000100004
2017-10-02T23:29:12.272106: step 3451, loss 0.010797, acc 1, learning_rate 0.000100004
2017-10-02T23:29:13.422745: step 3452, loss 0.0413752, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:29:14.588943: step 3453, loss 0.0115746, acc 1, learning_rate 0.000100004
2017-10-02T23:29:15.735522: step 3454, loss 0.00851581, acc 1, learning_rate 0.000100004
2017-10-02T23:29:16.884931: step 3455, loss 0.00853237, acc 1, learning_rate 0.000100004
2017-10-02T23:29:18.035675: step 3456, loss 0.013203, acc 1, learning_rate 0.000100004
2017-10-02T23:29:19.182778: step 3457, loss 0.00960225, acc 1, learning_rate 0.000100004
2017-10-02T23:29:20.350982: step 3458, loss 0.00813976, acc 1, learning_rate 0.000100004
2017-10-02T23:29:21.487120: step 3459, loss 0.00701457, acc 1, learning_rate 0.000100004
2017-10-02T23:29:22.771118: step 3460, loss 0.0146589, acc 1, learning_rate 0.000100004
2017-10-02T23:29:23.921806: step 3461, loss 0.0356134, acc 0.984375, learning_rate 0.000100004
2017-10-02T23:29:25.074955: step 3462, loss 0.00773714, acc 1, learning_rate 0.000100003
2017-10-02T23:29:26.222212: step 3463, loss 0.0211568, acc 1, learning_rate 0.000100003
2017-10-02T23:29:27.374535: step 3464, loss 0.00860688, acc 1, learning_rate 0.000100003
2017-10-02T23:29:28.512388: step 3465, loss 0.0146274, acc 1, learning_rate 0.000100003
2017-10-02T23:29:29.660378: step 3466, loss 0.00761377, acc 1, learning_rate 0.000100003
2017-10-02T23:29:30.797060: step 3467, loss 0.011841, acc 1, learning_rate 0.000100003
2017-10-02T23:29:31.943376: step 3468, loss 0.0110992, acc 1, learning_rate 0.000100003
2017-10-02T23:29:33.103947: step 3469, loss 0.0154596, acc 1, learning_rate 0.000100003
2017-10-02T23:29:34.263208: step 3470, loss 0.00849835, acc 1, learning_rate 0.000100003
2017-10-02T23:29:35.412084: step 3471, loss 0.0103403, acc 1, learning_rate 0.000100003
2017-10-02T23:29:36.568299: step 3472, loss 0.00573973, acc 1, learning_rate 0.000100003
2017-10-02T23:29:37.724171: step 3473, loss 0.0286457, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:29:39.125798: step 3474, loss 0.0166453, acc 1, learning_rate 0.000100003
2017-10-02T23:29:40.279380: step 3475, loss 0.0300867, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:29:41.446335: step 3476, loss 0.0164126, acc 1, learning_rate 0.000100003
2017-10-02T23:29:42.596741: step 3477, loss 0.0176509, acc 1, learning_rate 0.000100003
2017-10-02T23:29:43.763227: step 3478, loss 0.00516147, acc 1, learning_rate 0.000100003
2017-10-02T23:29:44.908428: step 3479, loss 0.020981, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:29:46.073216: step 3480, loss 0.0163162, acc 1, learning_rate 0.000100003

Evaluation:
2017-10-02T23:29:46.402695: step 3480, loss 1.43945, acc 0.470504

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3480

2017-10-02T23:29:54.286604: step 3481, loss 0.0131573, acc 1, learning_rate 0.000100003
2017-10-02T23:29:55.549985: step 3482, loss 0.0059222, acc 1, learning_rate 0.000100003
2017-10-02T23:29:56.699322: step 3483, loss 0.00749101, acc 1, learning_rate 0.000100003
2017-10-02T23:29:57.852143: step 3484, loss 0.010308, acc 1, learning_rate 0.000100003
2017-10-02T23:29:59.040382: step 3485, loss 0.0109772, acc 1, learning_rate 0.000100003
2017-10-02T23:30:00.196426: step 3486, loss 0.0130544, acc 1, learning_rate 0.000100003
2017-10-02T23:30:01.346620: step 3487, loss 0.00709051, acc 1, learning_rate 0.000100003
2017-10-02T23:30:02.509292: step 3488, loss 0.0167018, acc 1, learning_rate 0.000100003
2017-10-02T23:30:03.682958: step 3489, loss 0.00571825, acc 1, learning_rate 0.000100003
2017-10-02T23:30:04.831150: step 3490, loss 0.0407972, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:05.980120: step 3491, loss 0.00692832, acc 1, learning_rate 0.000100003
2017-10-02T23:30:07.126495: step 3492, loss 0.00950665, acc 1, learning_rate 0.000100003
2017-10-02T23:30:08.280251: step 3493, loss 0.044541, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:09.432792: step 3494, loss 0.0663445, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:10.611506: step 3495, loss 0.057551, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:11.755241: step 3496, loss 0.0115586, acc 1, learning_rate 0.000100003
2017-10-02T23:30:12.928528: step 3497, loss 0.0265594, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:14.068442: step 3498, loss 0.0298518, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:15.210309: step 3499, loss 0.00889403, acc 1, learning_rate 0.000100003
2017-10-02T23:30:16.354460: step 3500, loss 0.01126, acc 1, learning_rate 0.000100003
2017-10-02T23:30:17.503953: step 3501, loss 0.013663, acc 1, learning_rate 0.000100003
2017-10-02T23:30:18.802900: step 3502, loss 0.00952853, acc 1, learning_rate 0.000100003
2017-10-02T23:30:19.951976: step 3503, loss 0.0514237, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:21.113155: step 3504, loss 0.0147237, acc 1, learning_rate 0.000100003
2017-10-02T23:30:22.259946: step 3505, loss 0.0118059, acc 1, learning_rate 0.000100003
2017-10-02T23:30:23.413588: step 3506, loss 0.00685829, acc 1, learning_rate 0.000100003
2017-10-02T23:30:24.563038: step 3507, loss 0.0330323, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:25.714169: step 3508, loss 0.0153785, acc 1, learning_rate 0.000100003
2017-10-02T23:30:26.943385: step 3509, loss 0.0332714, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:28.092901: step 3510, loss 0.00787261, acc 1, learning_rate 0.000100003
2017-10-02T23:30:29.235914: step 3511, loss 0.0125142, acc 1, learning_rate 0.000100003
2017-10-02T23:30:30.398176: step 3512, loss 0.0321893, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:31.554245: step 3513, loss 0.00700694, acc 1, learning_rate 0.000100003
2017-10-02T23:30:32.725580: step 3514, loss 0.0521529, acc 0.96875, learning_rate 0.000100003
2017-10-02T23:30:33.863494: step 3515, loss 0.00924798, acc 1, learning_rate 0.000100003
2017-10-02T23:30:35.005056: step 3516, loss 0.0120651, acc 1, learning_rate 0.000100003
2017-10-02T23:30:36.150447: step 3517, loss 0.0463604, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:37.305346: step 3518, loss 0.00431527, acc 1, learning_rate 0.000100003
2017-10-02T23:30:38.457770: step 3519, loss 0.00739872, acc 1, learning_rate 0.000100003
2017-10-02T23:30:39.619037: step 3520, loss 0.0436142, acc 0.984375, learning_rate 0.000100003

Evaluation:
2017-10-02T23:30:39.969545: step 3520, loss 1.45068, acc 0.457554

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3520

2017-10-02T23:30:47.558188: step 3521, loss 0.0294404, acc 1, learning_rate 0.000100003
2017-10-02T23:30:48.715102: step 3522, loss 0.00891308, acc 1, learning_rate 0.000100003
2017-10-02T23:30:49.862275: step 3523, loss 0.00586478, acc 1, learning_rate 0.000100003
2017-10-02T23:30:51.017440: step 3524, loss 0.011576, acc 1, learning_rate 0.000100003
2017-10-02T23:30:52.184396: step 3525, loss 0.00825917, acc 1, learning_rate 0.000100003
2017-10-02T23:30:53.327595: step 3526, loss 0.00737175, acc 1, learning_rate 0.000100003
2017-10-02T23:30:54.474382: step 3527, loss 0.0140506, acc 1, learning_rate 0.000100003
2017-10-02T23:30:55.598697: step 3528, loss 0.0354757, acc 1, learning_rate 0.000100003
2017-10-02T23:30:56.762332: step 3529, loss 0.0221086, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:57.918404: step 3530, loss 0.0322512, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:30:59.083320: step 3531, loss 0.0104564, acc 1, learning_rate 0.000100003
2017-10-02T23:31:00.236164: step 3532, loss 0.011124, acc 1, learning_rate 0.000100003
2017-10-02T23:31:01.387464: step 3533, loss 0.0154592, acc 1, learning_rate 0.000100003
2017-10-02T23:31:02.619687: step 3534, loss 0.0255074, acc 1, learning_rate 0.000100003
2017-10-02T23:31:03.776435: step 3535, loss 0.0047236, acc 1, learning_rate 0.000100003
2017-10-02T23:31:04.921153: step 3536, loss 0.0223448, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:31:06.067179: step 3537, loss 0.0113949, acc 1, learning_rate 0.000100003
2017-10-02T23:31:07.228464: step 3538, loss 0.00786069, acc 1, learning_rate 0.000100003
2017-10-02T23:31:08.383679: step 3539, loss 0.0444652, acc 0.984375, learning_rate 0.000100003
2017-10-02T23:31:09.552321: step 3540, loss 0.0111152, acc 1, learning_rate 0.000100003
2017-10-02T23:31:10.693220: step 3541, loss 0.00605957, acc 1, learning_rate 0.000100003
2017-10-02T23:31:11.835942: step 3542, loss 0.0105173, acc 1, learning_rate 0.000100003
2017-10-02T23:31:12.980684: step 3543, loss 0.0242303, acc 1, learning_rate 0.000100003
2017-10-02T23:31:14.129844: step 3544, loss 0.0132948, acc 1, learning_rate 0.000100002
2017-10-02T23:31:15.272068: step 3545, loss 0.0054515, acc 1, learning_rate 0.000100002
2017-10-02T23:31:16.428952: step 3546, loss 0.0212384, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:31:17.591959: step 3547, loss 0.0119141, acc 1, learning_rate 0.000100002
2017-10-02T23:31:18.742324: step 3548, loss 0.00705684, acc 1, learning_rate 0.000100002
2017-10-02T23:31:19.937446: step 3549, loss 0.0110244, acc 1, learning_rate 0.000100002
2017-10-02T23:31:21.093343: step 3550, loss 0.00877004, acc 1, learning_rate 0.000100002
2017-10-02T23:31:22.255734: step 3551, loss 0.0074193, acc 1, learning_rate 0.000100002
2017-10-02T23:31:23.404474: step 3552, loss 0.0256875, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:31:24.552761: step 3553, loss 0.0271933, acc 1, learning_rate 0.000100002
2017-10-02T23:31:25.712215: step 3554, loss 0.0111888, acc 1, learning_rate 0.000100002
2017-10-02T23:31:26.938111: step 3555, loss 0.00891518, acc 1, learning_rate 0.000100002
2017-10-02T23:31:28.116245: step 3556, loss 0.0224069, acc 1, learning_rate 0.000100002
2017-10-02T23:31:29.270674: step 3557, loss 0.00762263, acc 1, learning_rate 0.000100002
2017-10-02T23:31:30.405395: step 3558, loss 0.0204013, acc 1, learning_rate 0.000100002
2017-10-02T23:31:31.568794: step 3559, loss 0.044526, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:31:32.713358: step 3560, loss 0.00950023, acc 1, learning_rate 0.000100002

Evaluation:
2017-10-02T23:31:33.058612: step 3560, loss 1.44337, acc 0.460432

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3560

2017-10-02T23:31:41.290966: step 3561, loss 0.0128192, acc 1, learning_rate 0.000100002
2017-10-02T23:31:42.443446: step 3562, loss 0.0088734, acc 1, learning_rate 0.000100002
2017-10-02T23:31:43.588761: step 3563, loss 0.00856996, acc 1, learning_rate 0.000100002
2017-10-02T23:31:44.754331: step 3564, loss 0.0153309, acc 1, learning_rate 0.000100002
2017-10-02T23:31:45.898460: step 3565, loss 0.0198643, acc 1, learning_rate 0.000100002
2017-10-02T23:31:47.050110: step 3566, loss 0.00988127, acc 1, learning_rate 0.000100002
2017-10-02T23:31:48.217500: step 3567, loss 0.0457393, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:31:49.379608: step 3568, loss 0.0189685, acc 1, learning_rate 0.000100002
2017-10-02T23:31:50.534923: step 3569, loss 0.0183093, acc 1, learning_rate 0.000100002
2017-10-02T23:31:51.695311: step 3570, loss 0.0149725, acc 1, learning_rate 0.000100002
2017-10-02T23:31:52.836800: step 3571, loss 0.0162831, acc 1, learning_rate 0.000100002
2017-10-02T23:31:54.007566: step 3572, loss 0.0100172, acc 1, learning_rate 0.000100002
2017-10-02T23:31:55.156208: step 3573, loss 0.00848858, acc 1, learning_rate 0.000100002
2017-10-02T23:31:56.332723: step 3574, loss 0.00812995, acc 1, learning_rate 0.000100002
2017-10-02T23:31:57.470779: step 3575, loss 0.0113817, acc 1, learning_rate 0.000100002
2017-10-02T23:31:58.620019: step 3576, loss 0.00925793, acc 1, learning_rate 0.000100002
2017-10-02T23:31:59.772528: step 3577, loss 0.0183863, acc 1, learning_rate 0.000100002
2017-10-02T23:32:00.941299: step 3578, loss 0.0164586, acc 1, learning_rate 0.000100002
2017-10-02T23:32:02.110612: step 3579, loss 0.0307786, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:03.273250: step 3580, loss 0.00833975, acc 1, learning_rate 0.000100002
2017-10-02T23:32:04.420751: step 3581, loss 0.0333494, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:05.572175: step 3582, loss 0.0218731, acc 1, learning_rate 0.000100002
2017-10-02T23:32:06.747364: step 3583, loss 0.0112342, acc 1, learning_rate 0.000100002
2017-10-02T23:32:07.905848: step 3584, loss 0.0156944, acc 1, learning_rate 0.000100002
2017-10-02T23:32:09.036973: step 3585, loss 0.00835154, acc 1, learning_rate 0.000100002
2017-10-02T23:32:10.197219: step 3586, loss 0.00534657, acc 1, learning_rate 0.000100002
2017-10-02T23:32:11.346001: step 3587, loss 0.00837503, acc 1, learning_rate 0.000100002
2017-10-02T23:32:12.500166: step 3588, loss 0.011033, acc 1, learning_rate 0.000100002
2017-10-02T23:32:13.658360: step 3589, loss 0.0208851, acc 1, learning_rate 0.000100002
2017-10-02T23:32:14.819905: step 3590, loss 0.00851252, acc 1, learning_rate 0.000100002
2017-10-02T23:32:15.971100: step 3591, loss 0.00649788, acc 1, learning_rate 0.000100002
2017-10-02T23:32:17.129839: step 3592, loss 0.0360249, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:18.285983: step 3593, loss 0.0176466, acc 1, learning_rate 0.000100002
2017-10-02T23:32:19.436213: step 3594, loss 0.0910022, acc 0.96875, learning_rate 0.000100002
2017-10-02T23:32:20.582055: step 3595, loss 0.0455782, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:21.753146: step 3596, loss 0.0467309, acc 0.96875, learning_rate 0.000100002
2017-10-02T23:32:22.904642: step 3597, loss 0.00898731, acc 1, learning_rate 0.000100002
2017-10-02T23:32:24.052981: step 3598, loss 0.0289423, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:25.201214: step 3599, loss 0.0403529, acc 0.96875, learning_rate 0.000100002
2017-10-02T23:32:26.357249: step 3600, loss 0.0260167, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-02T23:32:26.681798: step 3600, loss 1.45567, acc 0.454676

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3600

2017-10-02T23:32:34.111813: step 3601, loss 0.00925397, acc 1, learning_rate 0.000100002
2017-10-02T23:32:35.267805: step 3602, loss 0.0422929, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:36.418157: step 3603, loss 0.0107559, acc 1, learning_rate 0.000100002
2017-10-02T23:32:37.578230: step 3604, loss 0.0104575, acc 1, learning_rate 0.000100002
2017-10-02T23:32:38.721750: step 3605, loss 0.0109553, acc 1, learning_rate 0.000100002
2017-10-02T23:32:39.862559: step 3606, loss 0.0196848, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:40.997483: step 3607, loss 0.00588283, acc 1, learning_rate 0.000100002
2017-10-02T23:32:42.147094: step 3608, loss 0.0102284, acc 1, learning_rate 0.000100002
2017-10-02T23:32:43.309534: step 3609, loss 0.00926084, acc 1, learning_rate 0.000100002
2017-10-02T23:32:44.449889: step 3610, loss 0.0428754, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:45.597160: step 3611, loss 0.00507696, acc 1, learning_rate 0.000100002
2017-10-02T23:32:46.936160: step 3612, loss 0.0238081, acc 1, learning_rate 0.000100002
2017-10-02T23:32:48.080028: step 3613, loss 0.00930718, acc 1, learning_rate 0.000100002
2017-10-02T23:32:49.227067: step 3614, loss 0.00590096, acc 1, learning_rate 0.000100002
2017-10-02T23:32:50.393256: step 3615, loss 0.0086486, acc 1, learning_rate 0.000100002
2017-10-02T23:32:51.557419: step 3616, loss 0.00784305, acc 1, learning_rate 0.000100002
2017-10-02T23:32:52.733760: step 3617, loss 0.010121, acc 1, learning_rate 0.000100002
2017-10-02T23:32:53.891345: step 3618, loss 0.012386, acc 1, learning_rate 0.000100002
2017-10-02T23:32:55.037150: step 3619, loss 0.0238726, acc 1, learning_rate 0.000100002
2017-10-02T23:32:56.207738: step 3620, loss 0.012358, acc 1, learning_rate 0.000100002
2017-10-02T23:32:57.355809: step 3621, loss 0.0242102, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:58.497325: step 3622, loss 0.0385224, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:32:59.640310: step 3623, loss 0.0299272, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:00.816126: step 3624, loss 0.0118933, acc 1, learning_rate 0.000100002
2017-10-02T23:33:01.976004: step 3625, loss 0.00984496, acc 1, learning_rate 0.000100002
2017-10-02T23:33:03.112408: step 3626, loss 0.00525713, acc 1, learning_rate 0.000100002
2017-10-02T23:33:04.268994: step 3627, loss 0.0465967, acc 0.96875, learning_rate 0.000100002
2017-10-02T23:33:05.415466: step 3628, loss 0.00424426, acc 1, learning_rate 0.000100002
2017-10-02T23:33:06.559981: step 3629, loss 0.00773481, acc 1, learning_rate 0.000100002
2017-10-02T23:33:07.767129: step 3630, loss 0.00971685, acc 1, learning_rate 0.000100002
2017-10-02T23:33:08.938014: step 3631, loss 0.0171317, acc 1, learning_rate 0.000100002
2017-10-02T23:33:10.105385: step 3632, loss 0.0322722, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:11.260024: step 3633, loss 0.0330499, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:12.417440: step 3634, loss 0.0149877, acc 1, learning_rate 0.000100002
2017-10-02T23:33:13.578411: step 3635, loss 0.01723, acc 1, learning_rate 0.000100002
2017-10-02T23:33:14.737313: step 3636, loss 0.00733992, acc 1, learning_rate 0.000100002
2017-10-02T23:33:15.884398: step 3637, loss 0.0247371, acc 1, learning_rate 0.000100002
2017-10-02T23:33:17.045051: step 3638, loss 0.0155523, acc 1, learning_rate 0.000100002
2017-10-02T23:33:18.191744: step 3639, loss 0.0109367, acc 1, learning_rate 0.000100002
2017-10-02T23:33:19.344060: step 3640, loss 0.0101572, acc 1, learning_rate 0.000100002

Evaluation:
2017-10-02T23:33:19.692746: step 3640, loss 1.45809, acc 0.443165

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3640

2017-10-02T23:33:27.393756: step 3641, loss 0.0229842, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:28.550899: step 3642, loss 0.034651, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:29.704137: step 3643, loss 0.0122382, acc 1, learning_rate 0.000100002
2017-10-02T23:33:30.843638: step 3644, loss 0.0118735, acc 1, learning_rate 0.000100002
2017-10-02T23:33:31.995419: step 3645, loss 0.0114423, acc 1, learning_rate 0.000100002
2017-10-02T23:33:33.154399: step 3646, loss 0.0174394, acc 1, learning_rate 0.000100002
2017-10-02T23:33:34.313390: step 3647, loss 0.00942461, acc 1, learning_rate 0.000100002
2017-10-02T23:33:35.476697: step 3648, loss 0.0410191, acc 0.96875, learning_rate 0.000100002
2017-10-02T23:33:36.779290: step 3649, loss 0.0067116, acc 1, learning_rate 0.000100002
2017-10-02T23:33:38.062736: step 3650, loss 0.0142147, acc 1, learning_rate 0.000100002
2017-10-02T23:33:39.530762: step 3651, loss 0.00602501, acc 1, learning_rate 0.000100002
2017-10-02T23:33:40.678555: step 3652, loss 0.0120673, acc 1, learning_rate 0.000100002
2017-10-02T23:33:41.846967: step 3653, loss 0.00844137, acc 1, learning_rate 0.000100002
2017-10-02T23:33:42.998708: step 3654, loss 0.0122017, acc 1, learning_rate 0.000100002
2017-10-02T23:33:44.148254: step 3655, loss 0.00925394, acc 1, learning_rate 0.000100002
2017-10-02T23:33:45.324433: step 3656, loss 0.0104906, acc 1, learning_rate 0.000100002
2017-10-02T23:33:46.479075: step 3657, loss 0.0118383, acc 1, learning_rate 0.000100002
2017-10-02T23:33:47.628860: step 3658, loss 0.00584192, acc 1, learning_rate 0.000100002
2017-10-02T23:33:48.778384: step 3659, loss 0.0189221, acc 1, learning_rate 0.000100002
2017-10-02T23:33:49.936019: step 3660, loss 0.00860005, acc 1, learning_rate 0.000100002
2017-10-02T23:33:51.093141: step 3661, loss 0.0249313, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:52.260498: step 3662, loss 0.033863, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:53.421293: step 3663, loss 0.00611054, acc 1, learning_rate 0.000100002
2017-10-02T23:33:54.569639: step 3664, loss 0.032073, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:55.723206: step 3665, loss 0.027179, acc 0.984375, learning_rate 0.000100002
2017-10-02T23:33:56.951536: step 3666, loss 0.0107079, acc 1, learning_rate 0.000100002
2017-10-02T23:33:58.115742: step 3667, loss 0.0065399, acc 1, learning_rate 0.000100002
2017-10-02T23:33:59.255166: step 3668, loss 0.0157041, acc 1, learning_rate 0.000100002
2017-10-02T23:34:00.404884: step 3669, loss 0.0406518, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:01.574668: step 3670, loss 0.00674299, acc 1, learning_rate 0.000100001
2017-10-02T23:34:02.725759: step 3671, loss 0.00958472, acc 1, learning_rate 0.000100001
2017-10-02T23:34:03.880871: step 3672, loss 0.011551, acc 1, learning_rate 0.000100001
2017-10-02T23:34:05.035097: step 3673, loss 0.00849591, acc 1, learning_rate 0.000100001
2017-10-02T23:34:06.183722: step 3674, loss 0.013008, acc 1, learning_rate 0.000100001
2017-10-02T23:34:07.336727: step 3675, loss 0.00623738, acc 1, learning_rate 0.000100001
2017-10-02T23:34:08.490422: step 3676, loss 0.00851839, acc 1, learning_rate 0.000100001
2017-10-02T23:34:09.638149: step 3677, loss 0.0064881, acc 1, learning_rate 0.000100001
2017-10-02T23:34:10.778600: step 3678, loss 0.0196704, acc 1, learning_rate 0.000100001
2017-10-02T23:34:11.975542: step 3679, loss 0.0181794, acc 1, learning_rate 0.000100001
2017-10-02T23:34:13.131582: step 3680, loss 0.0346914, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-02T23:34:13.489207: step 3680, loss 1.47816, acc 0.392806

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3680

2017-10-02T23:34:20.919264: step 3681, loss 0.0362104, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:22.091233: step 3682, loss 0.00523035, acc 1, learning_rate 0.000100001
2017-10-02T23:34:23.248329: step 3683, loss 0.0194343, acc 1, learning_rate 0.000100001
2017-10-02T23:34:24.409947: step 3684, loss 0.0100041, acc 1, learning_rate 0.000100001
2017-10-02T23:34:25.564790: step 3685, loss 0.0189959, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:26.715236: step 3686, loss 0.0284997, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:27.873380: step 3687, loss 0.00925092, acc 1, learning_rate 0.000100001
2017-10-02T23:34:29.033116: step 3688, loss 0.032263, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:30.185290: step 3689, loss 0.00594409, acc 1, learning_rate 0.000100001
2017-10-02T23:34:31.331057: step 3690, loss 0.0151226, acc 1, learning_rate 0.000100001
2017-10-02T23:34:32.484484: step 3691, loss 0.0311807, acc 1, learning_rate 0.000100001
2017-10-02T23:34:33.637229: step 3692, loss 0.00771796, acc 1, learning_rate 0.000100001
2017-10-02T23:34:34.822999: step 3693, loss 0.00922858, acc 1, learning_rate 0.000100001
2017-10-02T23:34:35.986400: step 3694, loss 0.0057123, acc 1, learning_rate 0.000100001
2017-10-02T23:34:37.131174: step 3695, loss 0.0255437, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:38.296651: step 3696, loss 0.00888334, acc 1, learning_rate 0.000100001
2017-10-02T23:34:39.450386: step 3697, loss 0.0123501, acc 1, learning_rate 0.000100001
2017-10-02T23:34:40.618559: step 3698, loss 0.0462381, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:41.776397: step 3699, loss 0.00796687, acc 1, learning_rate 0.000100001
2017-10-02T23:34:43.053598: step 3700, loss 0.0050515, acc 1, learning_rate 0.000100001
2017-10-02T23:34:44.196175: step 3701, loss 0.0651261, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:34:45.348338: step 3702, loss 0.0480487, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:46.495291: step 3703, loss 0.0273528, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:47.669190: step 3704, loss 0.0109016, acc 1, learning_rate 0.000100001
2017-10-02T23:34:48.813893: step 3705, loss 0.0259696, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:49.970870: step 3706, loss 0.0488896, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:34:51.121525: step 3707, loss 0.00876485, acc 1, learning_rate 0.000100001
2017-10-02T23:34:52.268226: step 3708, loss 0.0288186, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:34:53.423836: step 3709, loss 0.00753965, acc 1, learning_rate 0.000100001
2017-10-02T23:34:54.593651: step 3710, loss 0.0231159, acc 1, learning_rate 0.000100001
2017-10-02T23:34:55.753868: step 3711, loss 0.00904578, acc 1, learning_rate 0.000100001
2017-10-02T23:34:56.930876: step 3712, loss 0.0134584, acc 1, learning_rate 0.000100001
2017-10-02T23:34:58.081312: step 3713, loss 0.0101401, acc 1, learning_rate 0.000100001
2017-10-02T23:34:59.233134: step 3714, loss 0.0133087, acc 1, learning_rate 0.000100001
2017-10-02T23:35:00.382587: step 3715, loss 0.00948463, acc 1, learning_rate 0.000100001
2017-10-02T23:35:01.531976: step 3716, loss 0.0371271, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:35:02.682236: step 3717, loss 0.0781473, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:35:03.844755: step 3718, loss 0.0066612, acc 1, learning_rate 0.000100001
2017-10-02T23:35:04.993347: step 3719, loss 0.00818867, acc 1, learning_rate 0.000100001
2017-10-02T23:35:06.146258: step 3720, loss 0.0249144, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-02T23:35:06.488213: step 3720, loss 1.45203, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3720

2017-10-02T23:35:14.357912: step 3721, loss 0.00828758, acc 1, learning_rate 0.000100001
2017-10-02T23:35:15.527969: step 3722, loss 0.00730251, acc 1, learning_rate 0.000100001
2017-10-02T23:35:16.667452: step 3723, loss 0.00778456, acc 1, learning_rate 0.000100001
2017-10-02T23:35:17.832254: step 3724, loss 0.049577, acc 0.980392, learning_rate 0.000100001
2017-10-02T23:35:19.000404: step 3725, loss 0.0130198, acc 1, learning_rate 0.000100001
2017-10-02T23:35:20.152058: step 3726, loss 0.0152685, acc 1, learning_rate 0.000100001
2017-10-02T23:35:21.321378: step 3727, loss 0.00614644, acc 1, learning_rate 0.000100001
2017-10-02T23:35:22.514720: step 3728, loss 0.00891416, acc 1, learning_rate 0.000100001
2017-10-02T23:35:23.668271: step 3729, loss 0.00764983, acc 1, learning_rate 0.000100001
2017-10-02T23:35:24.812544: step 3730, loss 0.00918998, acc 1, learning_rate 0.000100001
2017-10-02T23:35:25.958180: step 3731, loss 0.00775268, acc 1, learning_rate 0.000100001
2017-10-02T23:35:27.106200: step 3732, loss 0.00448321, acc 1, learning_rate 0.000100001
2017-10-02T23:35:28.249239: step 3733, loss 0.0142547, acc 1, learning_rate 0.000100001
2017-10-02T23:35:29.394605: step 3734, loss 0.00495064, acc 1, learning_rate 0.000100001
2017-10-02T23:35:30.561590: step 3735, loss 0.00457596, acc 1, learning_rate 0.000100001
2017-10-02T23:35:31.714839: step 3736, loss 0.00615043, acc 1, learning_rate 0.000100001
2017-10-02T23:35:32.867178: step 3737, loss 0.00629936, acc 1, learning_rate 0.000100001
2017-10-02T23:35:34.015997: step 3738, loss 0.00906134, acc 1, learning_rate 0.000100001
2017-10-02T23:35:35.164349: step 3739, loss 0.03765, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:35:36.314126: step 3740, loss 0.00586142, acc 1, learning_rate 0.000100001
2017-10-02T23:35:37.468236: step 3741, loss 0.0175816, acc 1, learning_rate 0.000100001
2017-10-02T23:35:38.633886: step 3742, loss 0.0513035, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:35:39.783450: step 3743, loss 0.0107903, acc 1, learning_rate 0.000100001
2017-10-02T23:35:40.933911: step 3744, loss 0.0138055, acc 1, learning_rate 0.000100001
2017-10-02T23:35:42.081371: step 3745, loss 0.0114586, acc 1, learning_rate 0.000100001
2017-10-02T23:35:43.227532: step 3746, loss 0.00953181, acc 1, learning_rate 0.000100001
2017-10-02T23:35:44.378387: step 3747, loss 0.00673882, acc 1, learning_rate 0.000100001
2017-10-02T23:35:45.535171: step 3748, loss 0.0060324, acc 1, learning_rate 0.000100001
2017-10-02T23:35:46.692306: step 3749, loss 0.00684923, acc 1, learning_rate 0.000100001
2017-10-02T23:35:47.839202: step 3750, loss 0.0209697, acc 1, learning_rate 0.000100001
2017-10-02T23:35:49.000035: step 3751, loss 0.0220375, acc 1, learning_rate 0.000100001
2017-10-02T23:35:50.165730: step 3752, loss 0.0306155, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:35:51.309714: step 3753, loss 0.079308, acc 0.953125, learning_rate 0.000100001
2017-10-02T23:35:52.468749: step 3754, loss 0.0217997, acc 1, learning_rate 0.000100001
2017-10-02T23:35:53.633379: step 3755, loss 0.0063599, acc 1, learning_rate 0.000100001
2017-10-02T23:35:54.791668: step 3756, loss 0.0100494, acc 1, learning_rate 0.000100001
2017-10-02T23:35:55.951113: step 3757, loss 0.00418847, acc 1, learning_rate 0.000100001
2017-10-02T23:35:57.099307: step 3758, loss 0.00736627, acc 1, learning_rate 0.000100001
2017-10-02T23:35:58.246014: step 3759, loss 0.00760668, acc 1, learning_rate 0.000100001
2017-10-02T23:35:59.408574: step 3760, loss 0.0414375, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-02T23:35:59.739195: step 3760, loss 1.44822, acc 0.471942

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3760

2017-10-02T23:36:06.985371: step 3761, loss 0.0109323, acc 1, learning_rate 0.000100001
2017-10-02T23:36:08.262198: step 3762, loss 0.0121164, acc 1, learning_rate 0.000100001
2017-10-02T23:36:09.410985: step 3763, loss 0.0276013, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:10.560690: step 3764, loss 0.0472592, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:11.721060: step 3765, loss 0.0193568, acc 1, learning_rate 0.000100001
2017-10-02T23:36:12.877641: step 3766, loss 0.00847101, acc 1, learning_rate 0.000100001
2017-10-02T23:36:14.023059: step 3767, loss 0.0395393, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:15.176382: step 3768, loss 0.00797034, acc 1, learning_rate 0.000100001
2017-10-02T23:36:16.323619: step 3769, loss 0.00711342, acc 1, learning_rate 0.000100001
2017-10-02T23:36:17.461753: step 3770, loss 0.00876487, acc 1, learning_rate 0.000100001
2017-10-02T23:36:18.609994: step 3771, loss 0.0137906, acc 1, learning_rate 0.000100001
2017-10-02T23:36:19.770182: step 3772, loss 0.00871893, acc 1, learning_rate 0.000100001
2017-10-02T23:36:20.900655: step 3773, loss 0.00787049, acc 1, learning_rate 0.000100001
2017-10-02T23:36:22.053087: step 3774, loss 0.00672133, acc 1, learning_rate 0.000100001
2017-10-02T23:36:23.209919: step 3775, loss 0.0140489, acc 1, learning_rate 0.000100001
2017-10-02T23:36:24.367709: step 3776, loss 0.0172617, acc 1, learning_rate 0.000100001
2017-10-02T23:36:25.532878: step 3777, loss 0.016698, acc 1, learning_rate 0.000100001
2017-10-02T23:36:26.684270: step 3778, loss 0.00819064, acc 1, learning_rate 0.000100001
2017-10-02T23:36:27.846605: step 3779, loss 0.0267696, acc 1, learning_rate 0.000100001
2017-10-02T23:36:28.990975: step 3780, loss 0.0182639, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:30.143284: step 3781, loss 0.0304495, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:31.288325: step 3782, loss 0.00836835, acc 1, learning_rate 0.000100001
2017-10-02T23:36:32.437178: step 3783, loss 0.00496599, acc 1, learning_rate 0.000100001
2017-10-02T23:36:33.603661: step 3784, loss 0.0248767, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:34.746756: step 3785, loss 0.0577055, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:35.933403: step 3786, loss 0.0261018, acc 1, learning_rate 0.000100001
2017-10-02T23:36:37.089498: step 3787, loss 0.0317827, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:38.254594: step 3788, loss 0.00391419, acc 1, learning_rate 0.000100001
2017-10-02T23:36:39.411806: step 3789, loss 0.0076675, acc 1, learning_rate 0.000100001
2017-10-02T23:36:40.579061: step 3790, loss 0.0104447, acc 1, learning_rate 0.000100001
2017-10-02T23:36:41.729530: step 3791, loss 0.0335776, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:42.878861: step 3792, loss 0.031316, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:44.046721: step 3793, loss 0.0227625, acc 1, learning_rate 0.000100001
2017-10-02T23:36:45.188390: step 3794, loss 0.0209779, acc 1, learning_rate 0.000100001
2017-10-02T23:36:46.345798: step 3795, loss 0.0300414, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:47.490155: step 3796, loss 0.0261706, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:36:48.645880: step 3797, loss 0.00491087, acc 1, learning_rate 0.000100001
2017-10-02T23:36:49.806606: step 3798, loss 0.00790732, acc 1, learning_rate 0.000100001
2017-10-02T23:36:51.012076: step 3799, loss 0.0124383, acc 1, learning_rate 0.000100001
2017-10-02T23:36:52.154202: step 3800, loss 0.00959687, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-02T23:36:52.486239: step 3800, loss 1.44642, acc 0.457554

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3800

2017-10-02T23:36:59.398154: step 3801, loss 0.0254258, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:00.559043: step 3802, loss 0.0102946, acc 1, learning_rate 0.000100001
2017-10-02T23:37:01.698482: step 3803, loss 0.00755642, acc 1, learning_rate 0.000100001
2017-10-02T23:37:02.848963: step 3804, loss 0.0538986, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:03.994641: step 3805, loss 0.0063269, acc 1, learning_rate 0.000100001
2017-10-02T23:37:05.141030: step 3806, loss 0.00552884, acc 1, learning_rate 0.000100001
2017-10-02T23:37:06.298987: step 3807, loss 0.012969, acc 1, learning_rate 0.000100001
2017-10-02T23:37:07.450305: step 3808, loss 0.0347156, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:08.593513: step 3809, loss 0.0139756, acc 1, learning_rate 0.000100001
2017-10-02T23:37:09.753853: step 3810, loss 0.0155802, acc 1, learning_rate 0.000100001
2017-10-02T23:37:10.945344: step 3811, loss 0.0206129, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:12.103662: step 3812, loss 0.0082516, acc 1, learning_rate 0.000100001
2017-10-02T23:37:13.246778: step 3813, loss 0.0100212, acc 1, learning_rate 0.000100001
2017-10-02T23:37:14.384672: step 3814, loss 0.0173213, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:15.544349: step 3815, loss 0.0733023, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:37:16.704457: step 3816, loss 0.0119382, acc 1, learning_rate 0.000100001
2017-10-02T23:37:17.842400: step 3817, loss 0.0119552, acc 1, learning_rate 0.000100001
2017-10-02T23:37:18.997208: step 3818, loss 0.0126797, acc 1, learning_rate 0.000100001
2017-10-02T23:37:20.131071: step 3819, loss 0.0066531, acc 1, learning_rate 0.000100001
2017-10-02T23:37:21.280346: step 3820, loss 0.0200193, acc 1, learning_rate 0.000100001
2017-10-02T23:37:22.463324: step 3821, loss 0.037895, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:23.593308: step 3822, loss 0.0119607, acc 1, learning_rate 0.000100001
2017-10-02T23:37:24.739322: step 3823, loss 0.0272911, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:25.889960: step 3824, loss 0.0561303, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:37:27.033903: step 3825, loss 0.00970373, acc 1, learning_rate 0.000100001
2017-10-02T23:37:28.173318: step 3826, loss 0.00434427, acc 1, learning_rate 0.000100001
2017-10-02T23:37:29.310694: step 3827, loss 0.00587863, acc 1, learning_rate 0.000100001
2017-10-02T23:37:30.466847: step 3828, loss 0.00918291, acc 1, learning_rate 0.000100001
2017-10-02T23:37:31.617401: step 3829, loss 0.00801624, acc 1, learning_rate 0.000100001
2017-10-02T23:37:32.773071: step 3830, loss 0.0136437, acc 1, learning_rate 0.000100001
2017-10-02T23:37:33.920658: step 3831, loss 0.0126182, acc 1, learning_rate 0.000100001
2017-10-02T23:37:35.071991: step 3832, loss 0.0101488, acc 1, learning_rate 0.000100001
2017-10-02T23:37:36.227985: step 3833, loss 0.00817978, acc 1, learning_rate 0.000100001
2017-10-02T23:37:37.412751: step 3834, loss 0.0576644, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:37:38.576520: step 3835, loss 0.00962981, acc 1, learning_rate 0.000100001
2017-10-02T23:37:39.711325: step 3836, loss 0.00726822, acc 1, learning_rate 0.000100001
2017-10-02T23:37:40.859638: step 3837, loss 0.0634106, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:42.004692: step 3838, loss 0.0116047, acc 1, learning_rate 0.000100001
2017-10-02T23:37:43.155004: step 3839, loss 0.019045, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:44.311242: step 3840, loss 0.0080371, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-02T23:37:44.656323: step 3840, loss 1.44587, acc 0.457554

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3840

2017-10-02T23:37:51.881150: step 3841, loss 0.00803841, acc 1, learning_rate 0.000100001
2017-10-02T23:37:53.030922: step 3842, loss 0.0104994, acc 1, learning_rate 0.000100001
2017-10-02T23:37:54.177496: step 3843, loss 0.0415766, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:55.341710: step 3844, loss 0.0217405, acc 1, learning_rate 0.000100001
2017-10-02T23:37:56.483176: step 3845, loss 0.00518374, acc 1, learning_rate 0.000100001
2017-10-02T23:37:57.624737: step 3846, loss 0.0459265, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:58.775758: step 3847, loss 0.0240566, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:37:59.920253: step 3848, loss 0.00804745, acc 1, learning_rate 0.000100001
2017-10-02T23:38:01.074258: step 3849, loss 0.0480231, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:02.222885: step 3850, loss 0.0257355, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:03.373161: step 3851, loss 0.00717615, acc 1, learning_rate 0.000100001
2017-10-02T23:38:04.540169: step 3852, loss 0.029939, acc 1, learning_rate 0.000100001
2017-10-02T23:38:05.694223: step 3853, loss 0.0172175, acc 1, learning_rate 0.000100001
2017-10-02T23:38:06.845829: step 3854, loss 0.0109486, acc 1, learning_rate 0.000100001
2017-10-02T23:38:08.011616: step 3855, loss 0.00668337, acc 1, learning_rate 0.000100001
2017-10-02T23:38:09.159283: step 3856, loss 0.00888412, acc 1, learning_rate 0.000100001
2017-10-02T23:38:10.312955: step 3857, loss 0.011255, acc 1, learning_rate 0.000100001
2017-10-02T23:38:11.477078: step 3858, loss 0.0139652, acc 1, learning_rate 0.000100001
2017-10-02T23:38:12.622164: step 3859, loss 0.0132537, acc 1, learning_rate 0.000100001
2017-10-02T23:38:13.758244: step 3860, loss 0.0460532, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:14.907982: step 3861, loss 0.0149543, acc 1, learning_rate 0.000100001
2017-10-02T23:38:16.072816: step 3862, loss 0.00797667, acc 1, learning_rate 0.000100001
2017-10-02T23:38:17.229213: step 3863, loss 0.00542733, acc 1, learning_rate 0.000100001
2017-10-02T23:38:18.419501: step 3864, loss 0.0088604, acc 1, learning_rate 0.000100001
2017-10-02T23:38:19.568582: step 3865, loss 0.0169678, acc 1, learning_rate 0.000100001
2017-10-02T23:38:20.716573: step 3866, loss 0.0144134, acc 1, learning_rate 0.000100001
2017-10-02T23:38:21.859732: step 3867, loss 0.0109416, acc 1, learning_rate 0.000100001
2017-10-02T23:38:23.002842: step 3868, loss 0.021856, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:24.161655: step 3869, loss 0.0217077, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:25.333644: step 3870, loss 0.00550801, acc 1, learning_rate 0.000100001
2017-10-02T23:38:26.494439: step 3871, loss 0.0112091, acc 1, learning_rate 0.000100001
2017-10-02T23:38:27.647775: step 3872, loss 0.0277098, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:28.796732: step 3873, loss 0.00801135, acc 1, learning_rate 0.000100001
2017-10-02T23:38:29.955765: step 3874, loss 0.00966251, acc 1, learning_rate 0.000100001
2017-10-02T23:38:31.110792: step 3875, loss 0.00571608, acc 1, learning_rate 0.000100001
2017-10-02T23:38:32.262126: step 3876, loss 0.0101784, acc 1, learning_rate 0.000100001
2017-10-02T23:38:33.414458: step 3877, loss 0.0103241, acc 1, learning_rate 0.000100001
2017-10-02T23:38:34.574113: step 3878, loss 0.0273125, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:35.810286: step 3879, loss 0.00633783, acc 1, learning_rate 0.000100001
2017-10-02T23:38:36.949202: step 3880, loss 0.0140548, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-02T23:38:37.275510: step 3880, loss 1.45996, acc 0.43741

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3880

2017-10-02T23:38:45.259878: step 3881, loss 0.0073221, acc 1, learning_rate 0.000100001
2017-10-02T23:38:46.423731: step 3882, loss 0.0120043, acc 1, learning_rate 0.000100001
2017-10-02T23:38:47.583861: step 3883, loss 0.0155694, acc 1, learning_rate 0.000100001
2017-10-02T23:38:48.740144: step 3884, loss 0.00822283, acc 1, learning_rate 0.000100001
2017-10-02T23:38:49.881502: step 3885, loss 0.0253476, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:51.031447: step 3886, loss 0.00725994, acc 1, learning_rate 0.000100001
2017-10-02T23:38:52.174061: step 3887, loss 0.017283, acc 1, learning_rate 0.000100001
2017-10-02T23:38:53.314546: step 3888, loss 0.0109482, acc 1, learning_rate 0.000100001
2017-10-02T23:38:54.457295: step 3889, loss 0.0279114, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:38:55.732817: step 3890, loss 0.00615435, acc 1, learning_rate 0.000100001
2017-10-02T23:38:56.869218: step 3891, loss 0.0079552, acc 1, learning_rate 0.000100001
2017-10-02T23:38:58.014292: step 3892, loss 0.0109197, acc 1, learning_rate 0.000100001
2017-10-02T23:38:59.186350: step 3893, loss 0.0155248, acc 1, learning_rate 0.000100001
2017-10-02T23:39:00.369178: step 3894, loss 0.0148673, acc 1, learning_rate 0.000100001
2017-10-02T23:39:01.520483: step 3895, loss 0.00645518, acc 1, learning_rate 0.000100001
2017-10-02T23:39:02.684684: step 3896, loss 0.00494065, acc 1, learning_rate 0.000100001
2017-10-02T23:39:03.829869: step 3897, loss 0.0697961, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:04.983642: step 3898, loss 0.00756982, acc 1, learning_rate 0.000100001
2017-10-02T23:39:06.133203: step 3899, loss 0.00561679, acc 1, learning_rate 0.000100001
2017-10-02T23:39:07.296540: step 3900, loss 0.020581, acc 1, learning_rate 0.000100001
2017-10-02T23:39:08.442795: step 3901, loss 0.0272408, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:09.585092: step 3902, loss 0.00747795, acc 1, learning_rate 0.000100001
2017-10-02T23:39:10.731981: step 3903, loss 0.00783895, acc 1, learning_rate 0.000100001
2017-10-02T23:39:11.895961: step 3904, loss 0.00809966, acc 1, learning_rate 0.000100001
2017-10-02T23:39:13.040750: step 3905, loss 0.0277151, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:14.185078: step 3906, loss 0.00784514, acc 1, learning_rate 0.000100001
2017-10-02T23:39:15.325606: step 3907, loss 0.0167014, acc 1, learning_rate 0.000100001
2017-10-02T23:39:16.472940: step 3908, loss 0.00623361, acc 1, learning_rate 0.000100001
2017-10-02T23:39:17.621228: step 3909, loss 0.0127613, acc 1, learning_rate 0.000100001
2017-10-02T23:39:18.775094: step 3910, loss 0.00632836, acc 1, learning_rate 0.000100001
2017-10-02T23:39:19.912761: step 3911, loss 0.00539277, acc 1, learning_rate 0.000100001
2017-10-02T23:39:21.056559: step 3912, loss 0.0336261, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:22.197884: step 3913, loss 0.0147772, acc 1, learning_rate 0.000100001
2017-10-02T23:39:23.361050: step 3914, loss 0.00853598, acc 1, learning_rate 0.000100001
2017-10-02T23:39:24.514722: step 3915, loss 0.0230232, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:25.664524: step 3916, loss 0.0105931, acc 1, learning_rate 0.000100001
2017-10-02T23:39:26.813704: step 3917, loss 0.0402355, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:27.960311: step 3918, loss 0.081806, acc 0.953125, learning_rate 0.000100001
2017-10-02T23:39:29.132029: step 3919, loss 0.0377851, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:39:30.266247: step 3920, loss 0.026383, acc 0.980392, learning_rate 0.000100001

Evaluation:
2017-10-02T23:39:30.587616: step 3920, loss 1.45375, acc 0.45036

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3920

2017-10-02T23:39:38.434643: step 3921, loss 0.00695314, acc 1, learning_rate 0.000100001
2017-10-02T23:39:39.594415: step 3922, loss 0.00908665, acc 1, learning_rate 0.000100001
2017-10-02T23:39:40.761777: step 3923, loss 0.0132277, acc 1, learning_rate 0.000100001
2017-10-02T23:39:41.925541: step 3924, loss 0.027381, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:43.081558: step 3925, loss 0.0282947, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:44.234559: step 3926, loss 0.0175707, acc 1, learning_rate 0.000100001
2017-10-02T23:39:45.391195: step 3927, loss 0.0194419, acc 1, learning_rate 0.000100001
2017-10-02T23:39:46.535312: step 3928, loss 0.00685325, acc 1, learning_rate 0.000100001
2017-10-02T23:39:47.680765: step 3929, loss 0.00899352, acc 1, learning_rate 0.000100001
2017-10-02T23:39:48.829170: step 3930, loss 0.00658253, acc 1, learning_rate 0.000100001
2017-10-02T23:39:49.970966: step 3931, loss 0.016056, acc 1, learning_rate 0.000100001
2017-10-02T23:39:51.124110: step 3932, loss 0.0370585, acc 0.984375, learning_rate 0.000100001
2017-10-02T23:39:52.269906: step 3933, loss 0.00615183, acc 1, learning_rate 0.000100001
2017-10-02T23:39:53.430813: step 3934, loss 0.0421369, acc 0.96875, learning_rate 0.000100001
2017-10-02T23:39:54.588098: step 3935, loss 0.00639832, acc 1, learning_rate 0.000100001
2017-10-02T23:39:55.751540: step 3936, loss 0.0128452, acc 1, learning_rate 0.000100001
2017-10-02T23:39:56.888725: step 3937, loss 0.00823702, acc 1, learning_rate 0.0001
2017-10-02T23:39:58.036380: step 3938, loss 0.00652099, acc 1, learning_rate 0.0001
2017-10-02T23:39:59.206125: step 3939, loss 0.021659, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:00.367582: step 3940, loss 0.00998802, acc 1, learning_rate 0.0001
2017-10-02T23:40:01.516129: step 3941, loss 0.0122232, acc 1, learning_rate 0.0001
2017-10-02T23:40:02.658512: step 3942, loss 0.0091523, acc 1, learning_rate 0.0001
2017-10-02T23:40:03.827761: step 3943, loss 0.00892752, acc 1, learning_rate 0.0001
2017-10-02T23:40:04.998959: step 3944, loss 0.0244033, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:06.152658: step 3945, loss 0.0524596, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:07.314327: step 3946, loss 0.0180329, acc 1, learning_rate 0.0001
2017-10-02T23:40:08.467222: step 3947, loss 0.0122828, acc 1, learning_rate 0.0001
2017-10-02T23:40:09.622343: step 3948, loss 0.00558525, acc 1, learning_rate 0.0001
2017-10-02T23:40:10.782606: step 3949, loss 0.0068985, acc 1, learning_rate 0.0001
2017-10-02T23:40:11.931490: step 3950, loss 0.00533393, acc 1, learning_rate 0.0001
2017-10-02T23:40:13.087952: step 3951, loss 0.00611773, acc 1, learning_rate 0.0001
2017-10-02T23:40:14.240412: step 3952, loss 0.00992529, acc 1, learning_rate 0.0001
2017-10-02T23:40:15.424647: step 3953, loss 0.0105399, acc 1, learning_rate 0.0001
2017-10-02T23:40:16.585259: step 3954, loss 0.009569, acc 1, learning_rate 0.0001
2017-10-02T23:40:17.724340: step 3955, loss 0.00529345, acc 1, learning_rate 0.0001
2017-10-02T23:40:18.878844: step 3956, loss 0.0346821, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:20.025943: step 3957, loss 0.00771833, acc 1, learning_rate 0.0001
2017-10-02T23:40:21.184781: step 3958, loss 0.0564496, acc 0.96875, learning_rate 0.0001
2017-10-02T23:40:22.337804: step 3959, loss 0.00813459, acc 1, learning_rate 0.0001
2017-10-02T23:40:23.497912: step 3960, loss 0.00846609, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:40:23.829420: step 3960, loss 1.45852, acc 0.448921

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-3960

2017-10-02T23:40:32.080283: step 3961, loss 0.0230672, acc 1, learning_rate 0.0001
2017-10-02T23:40:33.247470: step 3962, loss 0.0141934, acc 1, learning_rate 0.0001
2017-10-02T23:40:34.401524: step 3963, loss 0.0171457, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:35.556772: step 3964, loss 0.037593, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:36.709829: step 3965, loss 0.00696944, acc 1, learning_rate 0.0001
2017-10-02T23:40:37.866857: step 3966, loss 0.014162, acc 1, learning_rate 0.0001
2017-10-02T23:40:39.011571: step 3967, loss 0.0210922, acc 1, learning_rate 0.0001
2017-10-02T23:40:40.170985: step 3968, loss 0.0208646, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:41.315741: step 3969, loss 0.00876222, acc 1, learning_rate 0.0001
2017-10-02T23:40:42.454292: step 3970, loss 0.00816248, acc 1, learning_rate 0.0001
2017-10-02T23:40:43.607335: step 3971, loss 0.0674769, acc 0.96875, learning_rate 0.0001
2017-10-02T23:40:44.759264: step 3972, loss 0.016627, acc 1, learning_rate 0.0001
2017-10-02T23:40:45.912736: step 3973, loss 0.0108537, acc 1, learning_rate 0.0001
2017-10-02T23:40:47.062223: step 3974, loss 0.0076527, acc 1, learning_rate 0.0001
2017-10-02T23:40:48.213290: step 3975, loss 0.00606189, acc 1, learning_rate 0.0001
2017-10-02T23:40:49.359165: step 3976, loss 0.0137144, acc 1, learning_rate 0.0001
2017-10-02T23:40:50.495790: step 3977, loss 0.00848271, acc 1, learning_rate 0.0001
2017-10-02T23:40:51.639361: step 3978, loss 0.00650112, acc 1, learning_rate 0.0001
2017-10-02T23:40:52.789989: step 3979, loss 0.0401722, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:53.938757: step 3980, loss 0.00713949, acc 1, learning_rate 0.0001
2017-10-02T23:40:55.088001: step 3981, loss 0.00754656, acc 1, learning_rate 0.0001
2017-10-02T23:40:56.233235: step 3982, loss 0.023518, acc 0.984375, learning_rate 0.0001
2017-10-02T23:40:57.391111: step 3983, loss 0.00752073, acc 1, learning_rate 0.0001
2017-10-02T23:40:58.547325: step 3984, loss 0.0119102, acc 1, learning_rate 0.0001
2017-10-02T23:40:59.692135: step 3985, loss 0.0087577, acc 1, learning_rate 0.0001
2017-10-02T23:41:00.856253: step 3986, loss 0.0342631, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:02.006278: step 3987, loss 0.0490716, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:03.156878: step 3988, loss 0.00625083, acc 1, learning_rate 0.0001
2017-10-02T23:41:04.315787: step 3989, loss 0.00972625, acc 1, learning_rate 0.0001
2017-10-02T23:41:05.472415: step 3990, loss 0.00571147, acc 1, learning_rate 0.0001
2017-10-02T23:41:06.623514: step 3991, loss 0.00499482, acc 1, learning_rate 0.0001
2017-10-02T23:41:07.777650: step 3992, loss 0.00799223, acc 1, learning_rate 0.0001
2017-10-02T23:41:08.921418: step 3993, loss 0.0409219, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:10.076435: step 3994, loss 0.0287951, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:11.219376: step 3995, loss 0.00604927, acc 1, learning_rate 0.0001
2017-10-02T23:41:12.383749: step 3996, loss 0.0106464, acc 1, learning_rate 0.0001
2017-10-02T23:41:13.531409: step 3997, loss 0.0363291, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:14.702680: step 3998, loss 0.0304924, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:15.857523: step 3999, loss 0.0106354, acc 1, learning_rate 0.0001
2017-10-02T23:41:17.016665: step 4000, loss 0.00879529, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:41:17.344204: step 4000, loss 1.47571, acc 0.41295

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4000

2017-10-02T23:41:25.015650: step 4001, loss 0.010229, acc 1, learning_rate 0.0001
2017-10-02T23:41:26.161770: step 4002, loss 0.00555178, acc 1, learning_rate 0.0001
2017-10-02T23:41:27.322678: step 4003, loss 0.0270151, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:28.474783: step 4004, loss 0.00920828, acc 1, learning_rate 0.0001
2017-10-02T23:41:29.632890: step 4005, loss 0.0198556, acc 1, learning_rate 0.0001
2017-10-02T23:41:30.788670: step 4006, loss 0.00880033, acc 1, learning_rate 0.0001
2017-10-02T23:41:31.939419: step 4007, loss 0.0181326, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:33.079800: step 4008, loss 0.00472494, acc 1, learning_rate 0.0001
2017-10-02T23:41:34.232944: step 4009, loss 0.0066959, acc 1, learning_rate 0.0001
2017-10-02T23:41:35.386514: step 4010, loss 0.0359167, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:36.578856: step 4011, loss 0.00673846, acc 1, learning_rate 0.0001
2017-10-02T23:41:37.731823: step 4012, loss 0.0089604, acc 1, learning_rate 0.0001
2017-10-02T23:41:38.890421: step 4013, loss 0.0429759, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:40.035258: step 4014, loss 0.0063495, acc 1, learning_rate 0.0001
2017-10-02T23:41:41.178634: step 4015, loss 0.0337972, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:42.342197: step 4016, loss 0.012164, acc 1, learning_rate 0.0001
2017-10-02T23:41:43.495817: step 4017, loss 0.0378437, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:44.632431: step 4018, loss 0.0301426, acc 0.980392, learning_rate 0.0001
2017-10-02T23:41:45.792828: step 4019, loss 0.0046492, acc 1, learning_rate 0.0001
2017-10-02T23:41:46.951390: step 4020, loss 0.00593245, acc 1, learning_rate 0.0001
2017-10-02T23:41:48.120123: step 4021, loss 0.00714447, acc 1, learning_rate 0.0001
2017-10-02T23:41:49.263012: step 4022, loss 0.0141495, acc 1, learning_rate 0.0001
2017-10-02T23:41:50.504918: step 4023, loss 0.00589334, acc 1, learning_rate 0.0001
2017-10-02T23:41:51.663659: step 4024, loss 0.0370114, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:52.821350: step 4025, loss 0.0045144, acc 1, learning_rate 0.0001
2017-10-02T23:41:53.986099: step 4026, loss 0.00558026, acc 1, learning_rate 0.0001
2017-10-02T23:41:55.138599: step 4027, loss 0.00837244, acc 1, learning_rate 0.0001
2017-10-02T23:41:56.298298: step 4028, loss 0.00897146, acc 1, learning_rate 0.0001
2017-10-02T23:41:57.443937: step 4029, loss 0.0266856, acc 0.984375, learning_rate 0.0001
2017-10-02T23:41:58.714951: step 4030, loss 0.0185721, acc 1, learning_rate 0.0001
2017-10-02T23:41:59.869882: step 4031, loss 0.0346235, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:01.014141: step 4032, loss 0.0171498, acc 1, learning_rate 0.0001
2017-10-02T23:42:02.160696: step 4033, loss 0.0458795, acc 0.96875, learning_rate 0.0001
2017-10-02T23:42:03.318217: step 4034, loss 0.0065431, acc 1, learning_rate 0.0001
2017-10-02T23:42:04.496811: step 4035, loss 0.00927771, acc 1, learning_rate 0.0001
2017-10-02T23:42:05.650132: step 4036, loss 0.00644127, acc 1, learning_rate 0.0001
2017-10-02T23:42:06.813056: step 4037, loss 0.00324188, acc 1, learning_rate 0.0001
2017-10-02T23:42:07.959186: step 4038, loss 0.00779968, acc 1, learning_rate 0.0001
2017-10-02T23:42:09.127505: step 4039, loss 0.0142199, acc 1, learning_rate 0.0001
2017-10-02T23:42:10.290371: step 4040, loss 0.00732252, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:42:10.617220: step 4040, loss 1.47919, acc 0.414388

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4040

2017-10-02T23:42:18.392200: step 4041, loss 0.00740026, acc 1, learning_rate 0.0001
2017-10-02T23:42:19.563030: step 4042, loss 0.0151787, acc 1, learning_rate 0.0001
2017-10-02T23:42:20.800151: step 4043, loss 0.00425385, acc 1, learning_rate 0.0001
2017-10-02T23:42:21.951016: step 4044, loss 0.00677549, acc 1, learning_rate 0.0001
2017-10-02T23:42:23.101488: step 4045, loss 0.0278227, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:24.243896: step 4046, loss 0.0228242, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:25.389452: step 4047, loss 0.0050068, acc 1, learning_rate 0.0001
2017-10-02T23:42:26.540293: step 4048, loss 0.0141211, acc 1, learning_rate 0.0001
2017-10-02T23:42:27.692557: step 4049, loss 0.00859425, acc 1, learning_rate 0.0001
2017-10-02T23:42:28.843729: step 4050, loss 0.0159959, acc 1, learning_rate 0.0001
2017-10-02T23:42:30.001456: step 4051, loss 0.0078691, acc 1, learning_rate 0.0001
2017-10-02T23:42:31.146996: step 4052, loss 0.0133161, acc 1, learning_rate 0.0001
2017-10-02T23:42:32.289442: step 4053, loss 0.0177001, acc 1, learning_rate 0.0001
2017-10-02T23:42:33.446706: step 4054, loss 0.0182772, acc 1, learning_rate 0.0001
2017-10-02T23:42:34.608551: step 4055, loss 0.00625584, acc 1, learning_rate 0.0001
2017-10-02T23:42:35.766284: step 4056, loss 0.0246459, acc 1, learning_rate 0.0001
2017-10-02T23:42:36.907746: step 4057, loss 0.0460604, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:38.060671: step 4058, loss 0.00833173, acc 1, learning_rate 0.0001
2017-10-02T23:42:39.214612: step 4059, loss 0.0301162, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:40.366009: step 4060, loss 0.0226216, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:41.510117: step 4061, loss 0.00684191, acc 1, learning_rate 0.0001
2017-10-02T23:42:42.676769: step 4062, loss 0.0116232, acc 1, learning_rate 0.0001
2017-10-02T23:42:43.822974: step 4063, loss 0.0491447, acc 0.96875, learning_rate 0.0001
2017-10-02T23:42:44.974585: step 4064, loss 0.0116626, acc 1, learning_rate 0.0001
2017-10-02T23:42:46.133699: step 4065, loss 0.00597245, acc 1, learning_rate 0.0001
2017-10-02T23:42:47.314861: step 4066, loss 0.00475885, acc 1, learning_rate 0.0001
2017-10-02T23:42:48.459074: step 4067, loss 0.00681467, acc 1, learning_rate 0.0001
2017-10-02T23:42:49.609641: step 4068, loss 0.0104977, acc 1, learning_rate 0.0001
2017-10-02T23:42:50.769587: step 4069, loss 0.014405, acc 1, learning_rate 0.0001
2017-10-02T23:42:51.929887: step 4070, loss 0.0220614, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:53.080360: step 4071, loss 0.0206796, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:54.252823: step 4072, loss 0.0108024, acc 1, learning_rate 0.0001
2017-10-02T23:42:55.400978: step 4073, loss 0.0312341, acc 0.984375, learning_rate 0.0001
2017-10-02T23:42:56.552970: step 4074, loss 0.00975617, acc 1, learning_rate 0.0001
2017-10-02T23:42:57.713028: step 4075, loss 0.0129935, acc 1, learning_rate 0.0001
2017-10-02T23:42:58.864881: step 4076, loss 0.0133567, acc 1, learning_rate 0.0001
2017-10-02T23:43:00.003411: step 4077, loss 0.0183242, acc 1, learning_rate 0.0001
2017-10-02T23:43:01.155983: step 4078, loss 0.0091945, acc 1, learning_rate 0.0001
2017-10-02T23:43:02.303429: step 4079, loss 0.0238272, acc 1, learning_rate 0.0001
2017-10-02T23:43:03.459242: step 4080, loss 0.00641703, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:43:03.785780: step 4080, loss 1.46486, acc 0.430216

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4080

2017-10-02T23:43:11.631304: step 4081, loss 0.0136585, acc 1, learning_rate 0.0001
2017-10-02T23:43:12.772784: step 4082, loss 0.00706909, acc 1, learning_rate 0.0001
2017-10-02T23:43:13.932094: step 4083, loss 0.0171392, acc 1, learning_rate 0.0001
2017-10-02T23:43:15.093065: step 4084, loss 0.00828433, acc 1, learning_rate 0.0001
2017-10-02T23:43:16.254605: step 4085, loss 0.0177988, acc 1, learning_rate 0.0001
2017-10-02T23:43:17.407372: step 4086, loss 0.0207024, acc 0.984375, learning_rate 0.0001
2017-10-02T23:43:18.567551: step 4087, loss 0.0201793, acc 0.984375, learning_rate 0.0001
2017-10-02T23:43:19.724139: step 4088, loss 0.00685952, acc 1, learning_rate 0.0001
2017-10-02T23:43:20.868581: step 4089, loss 0.00624395, acc 1, learning_rate 0.0001
2017-10-02T23:43:22.012478: step 4090, loss 0.0235108, acc 0.984375, learning_rate 0.0001
2017-10-02T23:43:23.173527: step 4091, loss 0.0607127, acc 0.984375, learning_rate 0.0001
2017-10-02T23:43:24.334470: step 4092, loss 0.009946, acc 1, learning_rate 0.0001
2017-10-02T23:43:25.485145: step 4093, loss 0.00548479, acc 1, learning_rate 0.0001
2017-10-02T23:43:26.642057: step 4094, loss 0.0223757, acc 1, learning_rate 0.0001
2017-10-02T23:43:27.795326: step 4095, loss 0.00556084, acc 1, learning_rate 0.0001
2017-10-02T23:43:28.945766: step 4096, loss 0.0226712, acc 1, learning_rate 0.0001
2017-10-02T23:43:30.091639: step 4097, loss 0.00510884, acc 1, learning_rate 0.0001
2017-10-02T23:43:31.244115: step 4098, loss 0.00692662, acc 1, learning_rate 0.0001
2017-10-02T23:43:32.390687: step 4099, loss 0.0352761, acc 0.984375, learning_rate 0.0001
2017-10-02T23:43:33.547636: step 4100, loss 0.0223138, acc 0.984375, learning_rate 0.0001
2017-10-02T23:43:34.699630: step 4101, loss 0.00590786, acc 1, learning_rate 0.0001
2017-10-02T23:43:35.876104: step 4102, loss 0.0535876, acc 0.984375, learning_rate 0.0001
2017-10-02T23:43:37.053271: step 4103, loss 0.0106449, acc 1, learning_rate 0.0001
2017-10-02T23:43:38.204875: step 4104, loss 0.047778, acc 0.96875, learning_rate 0.0001
2017-10-02T23:43:39.367816: step 4105, loss 0.00742402, acc 1, learning_rate 0.0001
2017-10-02T23:43:40.521567: step 4106, loss 0.0139094, acc 1, learning_rate 0.0001
2017-10-02T23:43:41.679959: step 4107, loss 0.0061382, acc 1, learning_rate 0.0001
2017-10-02T23:43:43.078512: step 4108, loss 0.0104988, acc 1, learning_rate 0.0001
2017-10-02T23:43:44.237045: step 4109, loss 0.0125403, acc 1, learning_rate 0.0001
2017-10-02T23:43:45.397311: step 4110, loss 0.0354897, acc 0.984375, learning_rate 0.0001
2017-10-02T23:43:46.549492: step 4111, loss 0.00291488, acc 1, learning_rate 0.0001
2017-10-02T23:43:47.697972: step 4112, loss 0.00754828, acc 1, learning_rate 0.0001
2017-10-02T23:43:48.845759: step 4113, loss 0.00253877, acc 1, learning_rate 0.0001
2017-10-02T23:43:50.003613: step 4114, loss 0.0102498, acc 1, learning_rate 0.0001
2017-10-02T23:43:51.163899: step 4115, loss 0.00734027, acc 1, learning_rate 0.0001
2017-10-02T23:43:52.307133: step 4116, loss 0.0128582, acc 1, learning_rate 0.0001
2017-10-02T23:43:53.466056: step 4117, loss 0.0103977, acc 1, learning_rate 0.0001
2017-10-02T23:43:54.617919: step 4118, loss 0.00640513, acc 1, learning_rate 0.0001
2017-10-02T23:43:55.783607: step 4119, loss 0.019778, acc 1, learning_rate 0.0001
2017-10-02T23:43:56.934698: step 4120, loss 0.00948548, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:43:57.254536: step 4120, loss 1.43759, acc 0.483453

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4120

2017-10-02T23:44:05.098485: step 4121, loss 0.0316552, acc 0.984375, learning_rate 0.0001
2017-10-02T23:44:06.257560: step 4122, loss 0.00325249, acc 1, learning_rate 0.0001
2017-10-02T23:44:07.398901: step 4123, loss 0.00648275, acc 1, learning_rate 0.0001
2017-10-02T23:44:08.552602: step 4124, loss 0.0070952, acc 1, learning_rate 0.0001
2017-10-02T23:44:09.690725: step 4125, loss 0.0271797, acc 1, learning_rate 0.0001
2017-10-02T23:44:10.894507: step 4126, loss 0.00829085, acc 1, learning_rate 0.0001
2017-10-02T23:44:12.041468: step 4127, loss 0.0069551, acc 1, learning_rate 0.0001
2017-10-02T23:44:13.195866: step 4128, loss 0.042771, acc 0.984375, learning_rate 0.0001
2017-10-02T23:44:14.343285: step 4129, loss 0.00872357, acc 1, learning_rate 0.0001
2017-10-02T23:44:15.490212: step 4130, loss 0.00536219, acc 1, learning_rate 0.0001
2017-10-02T23:44:16.634662: step 4131, loss 0.0146144, acc 1, learning_rate 0.0001
2017-10-02T23:44:17.780863: step 4132, loss 0.0193964, acc 0.984375, learning_rate 0.0001
2017-10-02T23:44:18.927649: step 4133, loss 0.0843991, acc 0.984375, learning_rate 0.0001
2017-10-02T23:44:20.070218: step 4134, loss 0.00968956, acc 1, learning_rate 0.0001
2017-10-02T23:44:21.205861: step 4135, loss 0.004723, acc 1, learning_rate 0.0001
2017-10-02T23:44:22.351096: step 4136, loss 0.0160553, acc 1, learning_rate 0.0001
2017-10-02T23:44:23.532849: step 4137, loss 0.00647829, acc 1, learning_rate 0.0001
2017-10-02T23:44:24.700744: step 4138, loss 0.0056282, acc 1, learning_rate 0.0001
2017-10-02T23:44:25.838530: step 4139, loss 0.00606198, acc 1, learning_rate 0.0001
2017-10-02T23:44:26.987560: step 4140, loss 0.00696888, acc 1, learning_rate 0.0001
2017-10-02T23:44:28.305702: step 4141, loss 0.00831469, acc 1, learning_rate 0.0001
2017-10-02T23:44:29.453357: step 4142, loss 0.00816189, acc 1, learning_rate 0.0001
2017-10-02T23:44:30.619950: step 4143, loss 0.0335177, acc 0.984375, learning_rate 0.0001
2017-10-02T23:44:31.762224: step 4144, loss 0.0050482, acc 1, learning_rate 0.0001
2017-10-02T23:44:32.906195: step 4145, loss 0.029306, acc 0.984375, learning_rate 0.0001
2017-10-02T23:44:34.050853: step 4146, loss 0.00780426, acc 1, learning_rate 0.0001
2017-10-02T23:44:35.196538: step 4147, loss 0.0307763, acc 0.984375, learning_rate 0.0001
2017-10-02T23:44:36.353378: step 4148, loss 0.00525022, acc 1, learning_rate 0.0001
2017-10-02T23:44:37.501173: step 4149, loss 0.00573474, acc 1, learning_rate 0.0001
2017-10-02T23:44:38.647901: step 4150, loss 0.012418, acc 1, learning_rate 0.0001
2017-10-02T23:44:39.798192: step 4151, loss 0.00600479, acc 1, learning_rate 0.0001
2017-10-02T23:44:40.938863: step 4152, loss 0.00865244, acc 1, learning_rate 0.0001
2017-10-02T23:44:42.099025: step 4153, loss 0.0103146, acc 1, learning_rate 0.0001
2017-10-02T23:44:43.245910: step 4154, loss 0.0135569, acc 1, learning_rate 0.0001
2017-10-02T23:44:44.417096: step 4155, loss 0.00863518, acc 1, learning_rate 0.0001
2017-10-02T23:44:45.568252: step 4156, loss 0.0170118, acc 0.984375, learning_rate 0.0001
2017-10-02T23:44:46.709335: step 4157, loss 0.0721081, acc 0.96875, learning_rate 0.0001
2017-10-02T23:44:47.859487: step 4158, loss 0.0705183, acc 0.96875, learning_rate 0.0001
2017-10-02T23:44:49.002813: step 4159, loss 0.00609156, acc 1, learning_rate 0.0001
2017-10-02T23:44:50.139470: step 4160, loss 0.00708951, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:44:50.491875: step 4160, loss 1.44958, acc 0.471942

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4160

2017-10-02T23:44:58.053767: step 4161, loss 0.015936, acc 1, learning_rate 0.0001
2017-10-02T23:44:59.226547: step 4162, loss 0.00705222, acc 1, learning_rate 0.0001
2017-10-02T23:45:00.371044: step 4163, loss 0.0147578, acc 1, learning_rate 0.0001
2017-10-02T23:45:01.527117: step 4164, loss 0.0297571, acc 0.96875, learning_rate 0.0001
2017-10-02T23:45:02.688133: step 4165, loss 0.00878807, acc 1, learning_rate 0.0001
2017-10-02T23:45:03.840881: step 4166, loss 0.00979827, acc 1, learning_rate 0.0001
2017-10-02T23:45:04.978992: step 4167, loss 0.00543623, acc 1, learning_rate 0.0001
2017-10-02T23:45:06.124294: step 4168, loss 0.00779238, acc 1, learning_rate 0.0001
2017-10-02T23:45:07.267905: step 4169, loss 0.00807846, acc 1, learning_rate 0.0001
2017-10-02T23:45:08.406560: step 4170, loss 0.00705883, acc 1, learning_rate 0.0001
2017-10-02T23:45:09.557461: step 4171, loss 0.0462304, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:10.710191: step 4172, loss 0.0263274, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:11.848865: step 4173, loss 0.0111575, acc 1, learning_rate 0.0001
2017-10-02T23:45:12.996382: step 4174, loss 0.00423831, acc 1, learning_rate 0.0001
2017-10-02T23:45:14.144834: step 4175, loss 0.00868996, acc 1, learning_rate 0.0001
2017-10-02T23:45:15.293697: step 4176, loss 0.00444265, acc 1, learning_rate 0.0001
2017-10-02T23:45:16.461579: step 4177, loss 0.0101282, acc 1, learning_rate 0.0001
2017-10-02T23:45:17.614464: step 4178, loss 0.0107867, acc 1, learning_rate 0.0001
2017-10-02T23:45:18.763888: step 4179, loss 0.021657, acc 1, learning_rate 0.0001
2017-10-02T23:45:19.947964: step 4180, loss 0.0202222, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:21.112238: step 4181, loss 0.0516508, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:22.251122: step 4182, loss 0.0235433, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:23.399157: step 4183, loss 0.0226212, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:24.543535: step 4184, loss 0.018902, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:25.688426: step 4185, loss 0.0272052, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:26.841125: step 4186, loss 0.00568561, acc 1, learning_rate 0.0001
2017-10-02T23:45:27.982970: step 4187, loss 0.0188872, acc 1, learning_rate 0.0001
2017-10-02T23:45:29.126998: step 4188, loss 0.00613428, acc 1, learning_rate 0.0001
2017-10-02T23:45:30.274449: step 4189, loss 0.0644476, acc 0.96875, learning_rate 0.0001
2017-10-02T23:45:31.500876: step 4190, loss 0.0256444, acc 1, learning_rate 0.0001
2017-10-02T23:45:32.642595: step 4191, loss 0.0141665, acc 1, learning_rate 0.0001
2017-10-02T23:45:33.809192: step 4192, loss 0.0104868, acc 1, learning_rate 0.0001
2017-10-02T23:45:34.977665: step 4193, loss 0.00653967, acc 1, learning_rate 0.0001
2017-10-02T23:45:36.136968: step 4194, loss 0.00499495, acc 1, learning_rate 0.0001
2017-10-02T23:45:37.290657: step 4195, loss 0.00690933, acc 1, learning_rate 0.0001
2017-10-02T23:45:38.451034: step 4196, loss 0.0733601, acc 0.96875, learning_rate 0.0001
2017-10-02T23:45:39.607296: step 4197, loss 0.0060055, acc 1, learning_rate 0.0001
2017-10-02T23:45:40.750106: step 4198, loss 0.0127215, acc 1, learning_rate 0.0001
2017-10-02T23:45:41.903167: step 4199, loss 0.0195485, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:43.034844: step 4200, loss 0.0125022, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:45:43.365266: step 4200, loss 1.44556, acc 0.470504

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4200

2017-10-02T23:45:51.224307: step 4201, loss 0.0324185, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:52.386961: step 4202, loss 0.00953721, acc 1, learning_rate 0.0001
2017-10-02T23:45:53.541148: step 4203, loss 0.00819585, acc 1, learning_rate 0.0001
2017-10-02T23:45:54.696967: step 4204, loss 0.0387303, acc 0.984375, learning_rate 0.0001
2017-10-02T23:45:55.855147: step 4205, loss 0.00701884, acc 1, learning_rate 0.0001
2017-10-02T23:45:56.994293: step 4206, loss 0.00953773, acc 1, learning_rate 0.0001
2017-10-02T23:45:58.152922: step 4207, loss 0.00481961, acc 1, learning_rate 0.0001
2017-10-02T23:45:59.303140: step 4208, loss 0.00788102, acc 1, learning_rate 0.0001
2017-10-02T23:46:00.466247: step 4209, loss 0.0153425, acc 1, learning_rate 0.0001
2017-10-02T23:46:01.622057: step 4210, loss 0.018681, acc 1, learning_rate 0.0001
2017-10-02T23:46:02.760128: step 4211, loss 0.0189676, acc 1, learning_rate 0.0001
2017-10-02T23:46:03.910817: step 4212, loss 0.0547067, acc 0.96875, learning_rate 0.0001
2017-10-02T23:46:05.062172: step 4213, loss 0.00704939, acc 1, learning_rate 0.0001
2017-10-02T23:46:06.208363: step 4214, loss 0.00555227, acc 1, learning_rate 0.0001
2017-10-02T23:46:07.361184: step 4215, loss 0.0051204, acc 1, learning_rate 0.0001
2017-10-02T23:46:08.510460: step 4216, loss 0.0133089, acc 1, learning_rate 0.0001
2017-10-02T23:46:09.653658: step 4217, loss 0.0302839, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:10.813260: step 4218, loss 0.00481689, acc 1, learning_rate 0.0001
2017-10-02T23:46:11.964523: step 4219, loss 0.00524606, acc 1, learning_rate 0.0001
2017-10-02T23:46:13.115917: step 4220, loss 0.040322, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:14.263571: step 4221, loss 0.0504207, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:15.426071: step 4222, loss 0.0162952, acc 1, learning_rate 0.0001
2017-10-02T23:46:16.581748: step 4223, loss 0.00986302, acc 1, learning_rate 0.0001
2017-10-02T23:46:17.751599: step 4224, loss 0.0210475, acc 1, learning_rate 0.0001
2017-10-02T23:46:18.914259: step 4225, loss 0.0194297, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:20.072764: step 4226, loss 0.0234516, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:21.246269: step 4227, loss 0.0328299, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:22.385466: step 4228, loss 0.0454355, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:23.540926: step 4229, loss 0.00289029, acc 1, learning_rate 0.0001
2017-10-02T23:46:24.729716: step 4230, loss 0.00609175, acc 1, learning_rate 0.0001
2017-10-02T23:46:25.885568: step 4231, loss 0.00798667, acc 1, learning_rate 0.0001
2017-10-02T23:46:27.024138: step 4232, loss 0.00774215, acc 1, learning_rate 0.0001
2017-10-02T23:46:28.166722: step 4233, loss 0.00496543, acc 1, learning_rate 0.0001
2017-10-02T23:46:29.342880: step 4234, loss 0.00833689, acc 1, learning_rate 0.0001
2017-10-02T23:46:30.508757: step 4235, loss 0.00533151, acc 1, learning_rate 0.0001
2017-10-02T23:46:31.664558: step 4236, loss 0.006369, acc 1, learning_rate 0.0001
2017-10-02T23:46:32.812115: step 4237, loss 0.00837656, acc 1, learning_rate 0.0001
2017-10-02T23:46:33.963000: step 4238, loss 0.00417882, acc 1, learning_rate 0.0001
2017-10-02T23:46:35.129061: step 4239, loss 0.0297785, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:36.288814: step 4240, loss 0.0139679, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:46:36.638709: step 4240, loss 1.48067, acc 0.408633

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4240

2017-10-02T23:46:44.944906: step 4241, loss 0.0140477, acc 1, learning_rate 0.0001
2017-10-02T23:46:46.125486: step 4242, loss 0.00720223, acc 1, learning_rate 0.0001
2017-10-02T23:46:47.287057: step 4243, loss 0.00925414, acc 1, learning_rate 0.0001
2017-10-02T23:46:48.439655: step 4244, loss 0.0248273, acc 1, learning_rate 0.0001
2017-10-02T23:46:49.799226: step 4245, loss 0.0612312, acc 0.96875, learning_rate 0.0001
2017-10-02T23:46:50.941140: step 4246, loss 0.00997682, acc 1, learning_rate 0.0001
2017-10-02T23:46:52.087435: step 4247, loss 0.0214812, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:53.235584: step 4248, loss 0.0256205, acc 0.984375, learning_rate 0.0001
2017-10-02T23:46:54.395003: step 4249, loss 0.00643846, acc 1, learning_rate 0.0001
2017-10-02T23:46:55.554369: step 4250, loss 0.00614629, acc 1, learning_rate 0.0001
2017-10-02T23:46:56.712340: step 4251, loss 0.00903369, acc 1, learning_rate 0.0001
2017-10-02T23:46:57.872943: step 4252, loss 0.00844384, acc 1, learning_rate 0.0001
2017-10-02T23:46:59.022435: step 4253, loss 0.0315387, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:00.177998: step 4254, loss 0.0372786, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:01.325345: step 4255, loss 0.00692031, acc 1, learning_rate 0.0001
2017-10-02T23:47:02.478674: step 4256, loss 0.058458, acc 0.96875, learning_rate 0.0001
2017-10-02T23:47:03.633265: step 4257, loss 0.00392071, acc 1, learning_rate 0.0001
2017-10-02T23:47:04.785371: step 4258, loss 0.0426462, acc 1, learning_rate 0.0001
2017-10-02T23:47:05.927751: step 4259, loss 0.00393605, acc 1, learning_rate 0.0001
2017-10-02T23:47:07.079309: step 4260, loss 0.0181769, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:08.243184: step 4261, loss 0.00957238, acc 1, learning_rate 0.0001
2017-10-02T23:47:09.399224: step 4262, loss 0.0664824, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:10.551139: step 4263, loss 0.00761732, acc 1, learning_rate 0.0001
2017-10-02T23:47:11.707463: step 4264, loss 0.0509211, acc 0.96875, learning_rate 0.0001
2017-10-02T23:47:12.859428: step 4265, loss 0.00942052, acc 1, learning_rate 0.0001
2017-10-02T23:47:14.023702: step 4266, loss 0.0177331, acc 1, learning_rate 0.0001
2017-10-02T23:47:15.173083: step 4267, loss 0.00645008, acc 1, learning_rate 0.0001
2017-10-02T23:47:16.334157: step 4268, loss 0.00344575, acc 1, learning_rate 0.0001
2017-10-02T23:47:17.484614: step 4269, loss 0.0649897, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:18.633333: step 4270, loss 0.00695454, acc 1, learning_rate 0.0001
2017-10-02T23:47:19.787912: step 4271, loss 0.00900969, acc 1, learning_rate 0.0001
2017-10-02T23:47:20.946137: step 4272, loss 0.0061063, acc 1, learning_rate 0.0001
2017-10-02T23:47:22.102994: step 4273, loss 0.00996875, acc 1, learning_rate 0.0001
2017-10-02T23:47:23.249527: step 4274, loss 0.0385555, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:24.401716: step 4275, loss 0.0199338, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:25.558121: step 4276, loss 0.00547504, acc 1, learning_rate 0.0001
2017-10-02T23:47:26.779476: step 4277, loss 0.0038699, acc 1, learning_rate 0.0001
2017-10-02T23:47:27.954612: step 4278, loss 0.00957087, acc 1, learning_rate 0.0001
2017-10-02T23:47:29.120856: step 4279, loss 0.00585487, acc 1, learning_rate 0.0001
2017-10-02T23:47:30.295723: step 4280, loss 0.0117067, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:47:30.616704: step 4280, loss 1.4406, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4280

2017-10-02T23:47:38.277812: step 4281, loss 0.0303361, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:39.452324: step 4282, loss 0.0319196, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:40.604590: step 4283, loss 0.0232173, acc 1, learning_rate 0.0001
2017-10-02T23:47:41.773063: step 4284, loss 0.0107265, acc 1, learning_rate 0.0001
2017-10-02T23:47:42.922731: step 4285, loss 0.0099382, acc 1, learning_rate 0.0001
2017-10-02T23:47:44.079726: step 4286, loss 0.0129193, acc 1, learning_rate 0.0001
2017-10-02T23:47:45.226217: step 4287, loss 0.0106396, acc 1, learning_rate 0.0001
2017-10-02T23:47:46.387471: step 4288, loss 0.00809216, acc 1, learning_rate 0.0001
2017-10-02T23:47:47.541334: step 4289, loss 0.0205284, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:49.853691: step 4290, loss 0.00536265, acc 1, learning_rate 0.0001
2017-10-02T23:47:51.019854: step 4291, loss 0.0275743, acc 0.984375, learning_rate 0.0001
2017-10-02T23:47:53.875882: step 4292, loss 0.0311209, acc 1, learning_rate 0.0001
2017-10-02T23:47:55.023517: step 4293, loss 0.00710447, acc 1, learning_rate 0.0001
2017-10-02T23:47:56.196106: step 4294, loss 0.00562778, acc 1, learning_rate 0.0001
2017-10-02T23:47:57.361310: step 4295, loss 0.00806008, acc 1, learning_rate 0.0001
2017-10-02T23:47:58.525590: step 4296, loss 0.00674132, acc 1, learning_rate 0.0001
2017-10-02T23:47:59.686247: step 4297, loss 0.00836675, acc 1, learning_rate 0.0001
2017-10-02T23:48:00.827074: step 4298, loss 0.0251856, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:01.982872: step 4299, loss 0.0278552, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:03.143143: step 4300, loss 0.00580784, acc 1, learning_rate 0.0001
2017-10-02T23:48:04.302919: step 4301, loss 0.0166776, acc 1, learning_rate 0.0001
2017-10-02T23:48:05.472753: step 4302, loss 0.00796898, acc 1, learning_rate 0.0001
2017-10-02T23:48:06.626998: step 4303, loss 0.0667289, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:07.786610: step 4304, loss 0.00837861, acc 1, learning_rate 0.0001
2017-10-02T23:48:08.933130: step 4305, loss 0.0135437, acc 1, learning_rate 0.0001
2017-10-02T23:48:10.081987: step 4306, loss 0.00386686, acc 1, learning_rate 0.0001
2017-10-02T23:48:11.225347: step 4307, loss 0.00829406, acc 1, learning_rate 0.0001
2017-10-02T23:48:12.388304: step 4308, loss 0.00669879, acc 1, learning_rate 0.0001
2017-10-02T23:48:13.551062: step 4309, loss 0.0685288, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:14.708886: step 4310, loss 0.00653869, acc 1, learning_rate 0.0001
2017-10-02T23:48:15.877437: step 4311, loss 0.00683333, acc 1, learning_rate 0.0001
2017-10-02T23:48:17.015155: step 4312, loss 0.0102762, acc 1, learning_rate 0.0001
2017-10-02T23:48:18.172307: step 4313, loss 0.00447063, acc 1, learning_rate 0.0001
2017-10-02T23:48:19.337075: step 4314, loss 0.0110337, acc 1, learning_rate 0.0001
2017-10-02T23:48:20.501071: step 4315, loss 0.00634738, acc 1, learning_rate 0.0001
2017-10-02T23:48:21.656869: step 4316, loss 0.0101535, acc 1, learning_rate 0.0001
2017-10-02T23:48:22.806963: step 4317, loss 0.0290366, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:23.974071: step 4318, loss 0.0056472, acc 1, learning_rate 0.0001
2017-10-02T23:48:25.119911: step 4319, loss 0.00591291, acc 1, learning_rate 0.0001
2017-10-02T23:48:26.260211: step 4320, loss 0.0101061, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:48:26.611793: step 4320, loss 1.4611, acc 0.458993

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4320

2017-10-02T23:48:34.952833: step 4321, loss 0.00651838, acc 1, learning_rate 0.0001
2017-10-02T23:48:36.128010: step 4322, loss 0.00413503, acc 1, learning_rate 0.0001
2017-10-02T23:48:37.274116: step 4323, loss 0.0258891, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:38.430224: step 4324, loss 0.00750199, acc 1, learning_rate 0.0001
2017-10-02T23:48:39.581763: step 4325, loss 0.00474485, acc 1, learning_rate 0.0001
2017-10-02T23:48:40.735866: step 4326, loss 0.0126986, acc 1, learning_rate 0.0001
2017-10-02T23:48:41.899683: step 4327, loss 0.00798333, acc 1, learning_rate 0.0001
2017-10-02T23:48:43.069593: step 4328, loss 0.00908983, acc 1, learning_rate 0.0001
2017-10-02T23:48:44.216845: step 4329, loss 0.00894332, acc 1, learning_rate 0.0001
2017-10-02T23:48:45.376270: step 4330, loss 0.00509132, acc 1, learning_rate 0.0001
2017-10-02T23:48:46.531154: step 4331, loss 0.00957852, acc 1, learning_rate 0.0001
2017-10-02T23:48:47.682729: step 4332, loss 0.0441549, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:48.835339: step 4333, loss 0.00415705, acc 1, learning_rate 0.0001
2017-10-02T23:48:49.984228: step 4334, loss 0.00703104, acc 1, learning_rate 0.0001
2017-10-02T23:48:51.155307: step 4335, loss 0.0417943, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:52.307947: step 4336, loss 0.00359388, acc 1, learning_rate 0.0001
2017-10-02T23:48:53.460119: step 4337, loss 0.0477435, acc 0.984375, learning_rate 0.0001
2017-10-02T23:48:54.603853: step 4338, loss 0.00524518, acc 1, learning_rate 0.0001
2017-10-02T23:48:55.760491: step 4339, loss 0.0206979, acc 1, learning_rate 0.0001
2017-10-02T23:48:56.910491: step 4340, loss 0.0100463, acc 1, learning_rate 0.0001
2017-10-02T23:48:58.063401: step 4341, loss 0.00812696, acc 1, learning_rate 0.0001
2017-10-02T23:48:59.232064: step 4342, loss 0.0104203, acc 1, learning_rate 0.0001
2017-10-02T23:49:00.390004: step 4343, loss 0.0276801, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:01.537574: step 4344, loss 0.013466, acc 1, learning_rate 0.0001
2017-10-02T23:49:02.692161: step 4345, loss 0.00861191, acc 1, learning_rate 0.0001
2017-10-02T23:49:03.857058: step 4346, loss 0.0100628, acc 1, learning_rate 0.0001
2017-10-02T23:49:05.019616: step 4347, loss 0.0108271, acc 1, learning_rate 0.0001
2017-10-02T23:49:06.171350: step 4348, loss 0.00496092, acc 1, learning_rate 0.0001
2017-10-02T23:49:07.327020: step 4349, loss 0.0308428, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:08.494321: step 4350, loss 0.00951427, acc 1, learning_rate 0.0001
2017-10-02T23:49:09.661011: step 4351, loss 0.0267012, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:10.881746: step 4352, loss 0.0186383, acc 1, learning_rate 0.0001
2017-10-02T23:49:12.035512: step 4353, loss 0.00767884, acc 1, learning_rate 0.0001
2017-10-02T23:49:13.195637: step 4354, loss 0.0277591, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:14.350723: step 4355, loss 0.00691393, acc 1, learning_rate 0.0001
2017-10-02T23:49:15.494485: step 4356, loss 0.0183995, acc 1, learning_rate 0.0001
2017-10-02T23:49:16.652751: step 4357, loss 0.00561626, acc 1, learning_rate 0.0001
2017-10-02T23:49:17.801352: step 4358, loss 0.0182137, acc 1, learning_rate 0.0001
2017-10-02T23:49:18.955899: step 4359, loss 0.019261, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:20.117927: step 4360, loss 0.0423363, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-02T23:49:20.446426: step 4360, loss 1.45397, acc 0.45036

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4360

2017-10-02T23:49:27.876263: step 4361, loss 0.0191428, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:29.051843: step 4362, loss 0.00414758, acc 1, learning_rate 0.0001
2017-10-02T23:49:30.212937: step 4363, loss 0.00753635, acc 1, learning_rate 0.0001
2017-10-02T23:49:31.347905: step 4364, loss 0.00564853, acc 1, learning_rate 0.0001
2017-10-02T23:49:32.505151: step 4365, loss 0.0258181, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:33.660194: step 4366, loss 0.00787524, acc 1, learning_rate 0.0001
2017-10-02T23:49:34.808927: step 4367, loss 0.0102437, acc 1, learning_rate 0.0001
2017-10-02T23:49:35.960672: step 4368, loss 0.00675529, acc 1, learning_rate 0.0001
2017-10-02T23:49:37.101742: step 4369, loss 0.0137879, acc 1, learning_rate 0.0001
2017-10-02T23:49:38.259699: step 4370, loss 0.00484494, acc 1, learning_rate 0.0001
2017-10-02T23:49:39.408821: step 4371, loss 0.00722906, acc 1, learning_rate 0.0001
2017-10-02T23:49:40.560764: step 4372, loss 0.00441533, acc 1, learning_rate 0.0001
2017-10-02T23:49:41.707445: step 4373, loss 0.0204914, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:42.866532: step 4374, loss 0.00945686, acc 1, learning_rate 0.0001
2017-10-02T23:49:44.012653: step 4375, loss 0.00724819, acc 1, learning_rate 0.0001
2017-10-02T23:49:45.156468: step 4376, loss 0.0112196, acc 1, learning_rate 0.0001
2017-10-02T23:49:46.294785: step 4377, loss 0.00666786, acc 1, learning_rate 0.0001
2017-10-02T23:49:47.457657: step 4378, loss 0.0113106, acc 1, learning_rate 0.0001
2017-10-02T23:49:48.610265: step 4379, loss 0.0163859, acc 1, learning_rate 0.0001
2017-10-02T23:49:49.772589: step 4380, loss 0.00569909, acc 1, learning_rate 0.0001
2017-10-02T23:49:51.017912: step 4381, loss 0.0293104, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:52.178494: step 4382, loss 0.00981829, acc 1, learning_rate 0.0001
2017-10-02T23:49:53.325258: step 4383, loss 0.0743864, acc 0.96875, learning_rate 0.0001
2017-10-02T23:49:54.473953: step 4384, loss 0.0367419, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:55.633031: step 4385, loss 0.00861552, acc 1, learning_rate 0.0001
2017-10-02T23:49:56.788237: step 4386, loss 0.0426034, acc 0.984375, learning_rate 0.0001
2017-10-02T23:49:57.957210: step 4387, loss 0.00504832, acc 1, learning_rate 0.0001
2017-10-02T23:49:59.117900: step 4388, loss 0.00623443, acc 1, learning_rate 0.0001
2017-10-02T23:50:00.287439: step 4389, loss 0.0446876, acc 0.984375, learning_rate 0.0001
2017-10-02T23:50:01.439219: step 4390, loss 0.0358262, acc 0.984375, learning_rate 0.0001
2017-10-02T23:50:02.583015: step 4391, loss 0.00881958, acc 1, learning_rate 0.0001
2017-10-02T23:50:03.720313: step 4392, loss 0.00487867, acc 1, learning_rate 0.0001
2017-10-02T23:50:04.854308: step 4393, loss 0.0377337, acc 0.984375, learning_rate 0.0001
2017-10-02T23:50:06.006931: step 4394, loss 0.0532833, acc 0.96875, learning_rate 0.0001
2017-10-02T23:50:07.158523: step 4395, loss 0.0501002, acc 0.96875, learning_rate 0.0001
2017-10-02T23:50:08.301116: step 4396, loss 0.00716877, acc 1, learning_rate 0.0001
2017-10-02T23:50:09.440209: step 4397, loss 0.00555452, acc 1, learning_rate 0.0001
2017-10-02T23:50:10.588044: step 4398, loss 0.0166064, acc 0.984375, learning_rate 0.0001
2017-10-02T23:50:11.737910: step 4399, loss 0.00481621, acc 1, learning_rate 0.0001
2017-10-02T23:50:12.882022: step 4400, loss 0.00803135, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:50:13.207705: step 4400, loss 1.45241, acc 0.464748

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4400

2017-10-02T23:50:21.004817: step 4401, loss 0.00625456, acc 1, learning_rate 0.0001
2017-10-02T23:50:22.164703: step 4402, loss 0.00984236, acc 1, learning_rate 0.0001
2017-10-02T23:50:23.326867: step 4403, loss 0.00578853, acc 1, learning_rate 0.0001
2017-10-02T23:50:24.498450: step 4404, loss 0.00397546, acc 1, learning_rate 0.0001
2017-10-02T23:50:25.661824: step 4405, loss 0.00665437, acc 1, learning_rate 0.0001
2017-10-02T23:50:26.821322: step 4406, loss 0.00322268, acc 1, learning_rate 0.0001
2017-10-02T23:50:27.983350: step 4407, loss 0.0345729, acc 0.984375, learning_rate 0.0001
2017-10-02T23:50:29.142920: step 4408, loss 0.0104454, acc 1, learning_rate 0.0001
2017-10-02T23:50:30.298203: step 4409, loss 0.0055308, acc 1, learning_rate 0.0001
2017-10-02T23:50:31.436266: step 4410, loss 0.00797181, acc 1, learning_rate 0.0001
2017-10-02T23:50:32.605435: step 4411, loss 0.0114209, acc 1, learning_rate 0.0001
2017-10-02T23:50:33.795481: step 4412, loss 0.0128329, acc 1, learning_rate 0.0001
2017-10-02T23:50:35.106457: step 4413, loss 0.0431322, acc 0.96875, learning_rate 0.0001
2017-10-02T23:50:36.269944: step 4414, loss 0.0202542, acc 0.984375, learning_rate 0.0001
2017-10-02T23:50:37.431138: step 4415, loss 0.0177555, acc 0.984375, learning_rate 0.0001
2017-10-02T23:50:38.578058: step 4416, loss 0.00901025, acc 1, learning_rate 0.0001
2017-10-02T23:50:39.729426: step 4417, loss 0.00412323, acc 1, learning_rate 0.0001
2017-10-02T23:50:40.888977: step 4418, loss 0.00462485, acc 1, learning_rate 0.0001
2017-10-02T23:50:42.050995: step 4419, loss 0.00587404, acc 1, learning_rate 0.0001
2017-10-02T23:50:43.196117: step 4420, loss 0.00698027, acc 1, learning_rate 0.0001
2017-10-02T23:50:44.356362: step 4421, loss 0.00415744, acc 1, learning_rate 0.0001
2017-10-02T23:50:45.505889: step 4422, loss 0.00666361, acc 1, learning_rate 0.0001
2017-10-02T23:50:46.660246: step 4423, loss 0.00848045, acc 1, learning_rate 0.0001
2017-10-02T23:50:47.827805: step 4424, loss 0.00528786, acc 1, learning_rate 0.0001
2017-10-02T23:50:48.971371: step 4425, loss 0.0387295, acc 0.984375, learning_rate 0.0001
2017-10-02T23:50:50.112030: step 4426, loss 0.0714421, acc 0.953125, learning_rate 0.0001
2017-10-02T23:50:51.261945: step 4427, loss 0.0110669, acc 1, learning_rate 0.0001
2017-10-02T23:50:52.416476: step 4428, loss 0.00604687, acc 1, learning_rate 0.0001
2017-10-02T23:50:53.566906: step 4429, loss 0.00856085, acc 1, learning_rate 0.0001
2017-10-02T23:50:54.781081: step 4430, loss 0.00697, acc 1, learning_rate 0.0001
2017-10-02T23:50:55.943272: step 4431, loss 0.00673286, acc 1, learning_rate 0.0001
2017-10-02T23:50:57.097154: step 4432, loss 0.00591359, acc 1, learning_rate 0.0001
2017-10-02T23:50:58.242256: step 4433, loss 0.00684034, acc 1, learning_rate 0.0001
2017-10-02T23:50:59.390066: step 4434, loss 0.0231391, acc 1, learning_rate 0.0001
2017-10-02T23:51:00.548470: step 4435, loss 0.0119478, acc 1, learning_rate 0.0001
2017-10-02T23:51:01.717562: step 4436, loss 0.00552475, acc 1, learning_rate 0.0001
2017-10-02T23:51:02.863626: step 4437, loss 0.0297882, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:04.027141: step 4438, loss 0.0168896, acc 1, learning_rate 0.0001
2017-10-02T23:51:05.173365: step 4439, loss 0.0161519, acc 1, learning_rate 0.0001
2017-10-02T23:51:06.342287: step 4440, loss 0.0066054, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:51:06.705046: step 4440, loss 1.43187, acc 0.484892

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4440

2017-10-02T23:51:13.626966: step 4441, loss 0.007449, acc 1, learning_rate 0.0001
2017-10-02T23:51:14.866959: step 4442, loss 0.0186165, acc 1, learning_rate 0.0001
2017-10-02T23:51:16.023167: step 4443, loss 0.00567663, acc 1, learning_rate 0.0001
2017-10-02T23:51:17.174008: step 4444, loss 0.00899997, acc 1, learning_rate 0.0001
2017-10-02T23:51:18.335033: step 4445, loss 0.0042375, acc 1, learning_rate 0.0001
2017-10-02T23:51:19.490112: step 4446, loss 0.0461676, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:20.631373: step 4447, loss 0.00533599, acc 1, learning_rate 0.0001
2017-10-02T23:51:21.790818: step 4448, loss 0.00611451, acc 1, learning_rate 0.0001
2017-10-02T23:51:22.941018: step 4449, loss 0.0334972, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:24.106195: step 4450, loss 0.0225003, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:25.267734: step 4451, loss 0.027597, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:26.416493: step 4452, loss 0.0212732, acc 1, learning_rate 0.0001
2017-10-02T23:51:27.567328: step 4453, loss 0.0870999, acc 0.96875, learning_rate 0.0001
2017-10-02T23:51:28.734210: step 4454, loss 0.0124058, acc 1, learning_rate 0.0001
2017-10-02T23:51:29.933007: step 4455, loss 0.00380747, acc 1, learning_rate 0.0001
2017-10-02T23:51:31.089415: step 4456, loss 0.0171086, acc 1, learning_rate 0.0001
2017-10-02T23:51:32.229456: step 4457, loss 0.00505408, acc 1, learning_rate 0.0001
2017-10-02T23:51:33.378962: step 4458, loss 0.0243499, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:34.522449: step 4459, loss 0.0232578, acc 1, learning_rate 0.0001
2017-10-02T23:51:35.664041: step 4460, loss 0.0266187, acc 1, learning_rate 0.0001
2017-10-02T23:51:36.816700: step 4461, loss 0.00409918, acc 1, learning_rate 0.0001
2017-10-02T23:51:37.970025: step 4462, loss 0.0227324, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:39.118905: step 4463, loss 0.00428564, acc 1, learning_rate 0.0001
2017-10-02T23:51:40.281509: step 4464, loss 0.006699, acc 1, learning_rate 0.0001
2017-10-02T23:51:41.435749: step 4465, loss 0.0247837, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:42.578411: step 4466, loss 0.00932734, acc 1, learning_rate 0.0001
2017-10-02T23:51:43.728034: step 4467, loss 0.00438265, acc 1, learning_rate 0.0001
2017-10-02T23:51:44.884741: step 4468, loss 0.00586749, acc 1, learning_rate 0.0001
2017-10-02T23:51:46.047763: step 4469, loss 0.00949861, acc 1, learning_rate 0.0001
2017-10-02T23:51:47.196083: step 4470, loss 0.02034, acc 1, learning_rate 0.0001
2017-10-02T23:51:48.347289: step 4471, loss 0.00606974, acc 1, learning_rate 0.0001
2017-10-02T23:51:49.484637: step 4472, loss 0.0232423, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:50.642313: step 4473, loss 0.008194, acc 1, learning_rate 0.0001
2017-10-02T23:51:51.790780: step 4474, loss 0.0117054, acc 1, learning_rate 0.0001
2017-10-02T23:51:52.940708: step 4475, loss 0.0435279, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:54.085952: step 4476, loss 0.00931766, acc 1, learning_rate 0.0001
2017-10-02T23:51:55.238390: step 4477, loss 0.0132963, acc 1, learning_rate 0.0001
2017-10-02T23:51:56.390613: step 4478, loss 0.0391432, acc 0.984375, learning_rate 0.0001
2017-10-02T23:51:57.544426: step 4479, loss 0.00506304, acc 1, learning_rate 0.0001
2017-10-02T23:51:58.707678: step 4480, loss 0.00940485, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:51:59.039348: step 4480, loss 1.46707, acc 0.438849

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4480

2017-10-02T23:52:07.142999: step 4481, loss 0.00514708, acc 1, learning_rate 0.0001
2017-10-02T23:52:08.293377: step 4482, loss 0.00526193, acc 1, learning_rate 0.0001
2017-10-02T23:52:09.428622: step 4483, loss 0.00567998, acc 1, learning_rate 0.0001
2017-10-02T23:52:10.571711: step 4484, loss 0.00918398, acc 1, learning_rate 0.0001
2017-10-02T23:52:11.719988: step 4485, loss 0.00539329, acc 1, learning_rate 0.0001
2017-10-02T23:52:12.886968: step 4486, loss 0.0138109, acc 1, learning_rate 0.0001
2017-10-02T23:52:14.041897: step 4487, loss 0.0133747, acc 1, learning_rate 0.0001
2017-10-02T23:52:15.187259: step 4488, loss 0.0180352, acc 0.984375, learning_rate 0.0001
2017-10-02T23:52:16.338910: step 4489, loss 0.00560941, acc 1, learning_rate 0.0001
2017-10-02T23:52:17.493825: step 4490, loss 0.00490829, acc 1, learning_rate 0.0001
2017-10-02T23:52:18.636971: step 4491, loss 0.015121, acc 1, learning_rate 0.0001
2017-10-02T23:52:19.794448: step 4492, loss 0.00558889, acc 1, learning_rate 0.0001
2017-10-02T23:52:20.939583: step 4493, loss 0.071964, acc 0.96875, learning_rate 0.0001
2017-10-02T23:52:22.093754: step 4494, loss 0.0102083, acc 1, learning_rate 0.0001
2017-10-02T23:52:23.241042: step 4495, loss 0.00471305, acc 1, learning_rate 0.0001
2017-10-02T23:52:24.393662: step 4496, loss 0.00690963, acc 1, learning_rate 0.0001
2017-10-02T23:52:25.535282: step 4497, loss 0.00594219, acc 1, learning_rate 0.0001
2017-10-02T23:52:26.683843: step 4498, loss 0.0101324, acc 1, learning_rate 0.0001
2017-10-02T23:52:27.830576: step 4499, loss 0.00419753, acc 1, learning_rate 0.0001
2017-10-02T23:52:28.977306: step 4500, loss 0.00845913, acc 1, learning_rate 0.0001
2017-10-02T23:52:30.121085: step 4501, loss 0.0149894, acc 0.984375, learning_rate 0.0001
2017-10-02T23:52:31.267230: step 4502, loss 0.00603019, acc 1, learning_rate 0.0001
2017-10-02T23:52:32.414240: step 4503, loss 0.0294738, acc 0.984375, learning_rate 0.0001
2017-10-02T23:52:33.556639: step 4504, loss 0.00579018, acc 1, learning_rate 0.0001
2017-10-02T23:52:34.690422: step 4505, loss 0.00849011, acc 1, learning_rate 0.0001
2017-10-02T23:52:35.845727: step 4506, loss 0.0181117, acc 1, learning_rate 0.0001
2017-10-02T23:52:37.021394: step 4507, loss 0.0104318, acc 1, learning_rate 0.0001
2017-10-02T23:52:38.160121: step 4508, loss 0.00656242, acc 1, learning_rate 0.0001
2017-10-02T23:52:39.312188: step 4509, loss 0.0240225, acc 0.984375, learning_rate 0.0001
2017-10-02T23:52:40.464222: step 4510, loss 0.0586951, acc 0.984375, learning_rate 0.0001
2017-10-02T23:52:41.623712: step 4511, loss 0.0161488, acc 1, learning_rate 0.0001
2017-10-02T23:52:42.773175: step 4512, loss 0.0209955, acc 0.984375, learning_rate 0.0001
2017-10-02T23:52:43.941878: step 4513, loss 0.00722866, acc 1, learning_rate 0.0001
2017-10-02T23:52:45.079124: step 4514, loss 0.0138484, acc 1, learning_rate 0.0001
2017-10-02T23:52:46.225929: step 4515, loss 0.0125625, acc 1, learning_rate 0.0001
2017-10-02T23:52:47.372491: step 4516, loss 0.00491132, acc 1, learning_rate 0.0001
2017-10-02T23:52:48.513071: step 4517, loss 0.00731998, acc 1, learning_rate 0.0001
2017-10-02T23:52:49.677879: step 4518, loss 0.00383508, acc 1, learning_rate 0.0001
2017-10-02T23:52:50.812980: step 4519, loss 0.0207969, acc 0.984375, learning_rate 0.0001
2017-10-02T23:52:51.980975: step 4520, loss 0.00618264, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:52:52.315042: step 4520, loss 1.48946, acc 0.404317

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4520

2017-10-02T23:52:59.982244: step 4521, loss 0.00679173, acc 1, learning_rate 0.0001
2017-10-02T23:53:01.148529: step 4522, loss 0.00351951, acc 1, learning_rate 0.0001
2017-10-02T23:53:02.295331: step 4523, loss 0.00578714, acc 1, learning_rate 0.0001
2017-10-02T23:53:03.451141: step 4524, loss 0.0469468, acc 0.96875, learning_rate 0.0001
2017-10-02T23:53:04.612423: step 4525, loss 0.00505661, acc 1, learning_rate 0.0001
2017-10-02T23:53:05.767562: step 4526, loss 0.00402484, acc 1, learning_rate 0.0001
2017-10-02T23:53:06.935126: step 4527, loss 0.0150812, acc 1, learning_rate 0.0001
2017-10-02T23:53:08.089968: step 4528, loss 0.00734854, acc 1, learning_rate 0.0001
2017-10-02T23:53:09.237887: step 4529, loss 0.00779985, acc 1, learning_rate 0.0001
2017-10-02T23:53:10.391248: step 4530, loss 0.00521269, acc 1, learning_rate 0.0001
2017-10-02T23:53:11.535968: step 4531, loss 0.00770394, acc 1, learning_rate 0.0001
2017-10-02T23:53:12.683659: step 4532, loss 0.00784527, acc 1, learning_rate 0.0001
2017-10-02T23:53:13.938750: step 4533, loss 0.00759997, acc 1, learning_rate 0.0001
2017-10-02T23:53:15.089532: step 4534, loss 0.0076618, acc 1, learning_rate 0.0001
2017-10-02T23:53:16.244163: step 4535, loss 0.00637017, acc 1, learning_rate 0.0001
2017-10-02T23:53:17.399926: step 4536, loss 0.00937127, acc 1, learning_rate 0.0001
2017-10-02T23:53:18.567722: step 4537, loss 0.00589036, acc 1, learning_rate 0.0001
2017-10-02T23:53:19.715222: step 4538, loss 0.0642408, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:20.865033: step 4539, loss 0.0244336, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:22.023518: step 4540, loss 0.0050473, acc 1, learning_rate 0.0001
2017-10-02T23:53:23.171605: step 4541, loss 0.0654593, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:24.324806: step 4542, loss 0.0316473, acc 1, learning_rate 0.0001
2017-10-02T23:53:25.488041: step 4543, loss 0.0047061, acc 1, learning_rate 0.0001
2017-10-02T23:53:26.628958: step 4544, loss 0.0220426, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:27.783806: step 4545, loss 0.0482762, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:28.928303: step 4546, loss 0.00525851, acc 1, learning_rate 0.0001
2017-10-02T23:53:30.082383: step 4547, loss 0.00768143, acc 1, learning_rate 0.0001
2017-10-02T23:53:31.246894: step 4548, loss 0.0384765, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:32.396140: step 4549, loss 0.00916595, acc 1, learning_rate 0.0001
2017-10-02T23:53:33.541076: step 4550, loss 0.0454484, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:34.690527: step 4551, loss 0.00494869, acc 1, learning_rate 0.0001
2017-10-02T23:53:35.863125: step 4552, loss 0.0109563, acc 1, learning_rate 0.0001
2017-10-02T23:53:37.003102: step 4553, loss 0.0243581, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:38.246971: step 4554, loss 0.00446687, acc 1, learning_rate 0.0001
2017-10-02T23:53:39.406613: step 4555, loss 0.0633505, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:40.556351: step 4556, loss 0.0927124, acc 0.96875, learning_rate 0.0001
2017-10-02T23:53:41.726450: step 4557, loss 0.00814647, acc 1, learning_rate 0.0001
2017-10-02T23:53:42.868046: step 4558, loss 0.00555529, acc 1, learning_rate 0.0001
2017-10-02T23:53:44.040997: step 4559, loss 0.00712694, acc 1, learning_rate 0.0001
2017-10-02T23:53:45.193401: step 4560, loss 0.00370595, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:53:45.533611: step 4560, loss 1.44707, acc 0.467626

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4560

2017-10-02T23:53:52.412435: step 4561, loss 0.00435744, acc 1, learning_rate 0.0001
2017-10-02T23:53:53.571131: step 4562, loss 0.00577764, acc 1, learning_rate 0.0001
2017-10-02T23:53:54.733056: step 4563, loss 0.00368277, acc 1, learning_rate 0.0001
2017-10-02T23:53:55.886753: step 4564, loss 0.036207, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:57.038105: step 4565, loss 0.0126463, acc 1, learning_rate 0.0001
2017-10-02T23:53:58.194896: step 4566, loss 0.0211627, acc 0.984375, learning_rate 0.0001
2017-10-02T23:53:59.350933: step 4567, loss 0.0316721, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:00.500724: step 4568, loss 0.00520016, acc 1, learning_rate 0.0001
2017-10-02T23:54:01.669208: step 4569, loss 0.0139753, acc 1, learning_rate 0.0001
2017-10-02T23:54:02.809059: step 4570, loss 0.00954767, acc 1, learning_rate 0.0001
2017-10-02T23:54:03.970602: step 4571, loss 0.00488729, acc 1, learning_rate 0.0001
2017-10-02T23:54:05.125912: step 4572, loss 0.00582526, acc 1, learning_rate 0.0001
2017-10-02T23:54:06.271739: step 4573, loss 0.00685339, acc 1, learning_rate 0.0001
2017-10-02T23:54:07.418723: step 4574, loss 0.00796561, acc 1, learning_rate 0.0001
2017-10-02T23:54:08.564661: step 4575, loss 0.0214895, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:09.718267: step 4576, loss 0.0230073, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:10.870928: step 4577, loss 0.00602099, acc 1, learning_rate 0.0001
2017-10-02T23:54:12.023026: step 4578, loss 0.0353381, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:13.187328: step 4579, loss 0.00985047, acc 1, learning_rate 0.0001
2017-10-02T23:54:14.338358: step 4580, loss 0.00307865, acc 1, learning_rate 0.0001
2017-10-02T23:54:15.487311: step 4581, loss 0.00552125, acc 1, learning_rate 0.0001
2017-10-02T23:54:16.639623: step 4582, loss 0.0568736, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:17.839771: step 4583, loss 0.0111842, acc 1, learning_rate 0.0001
2017-10-02T23:54:18.988783: step 4584, loss 0.0231673, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:20.140122: step 4585, loss 0.00561937, acc 1, learning_rate 0.0001
2017-10-02T23:54:21.293849: step 4586, loss 0.0194914, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:22.442910: step 4587, loss 0.0263836, acc 1, learning_rate 0.0001
2017-10-02T23:54:23.603439: step 4588, loss 0.0103341, acc 1, learning_rate 0.0001
2017-10-02T23:54:24.755317: step 4589, loss 0.00369575, acc 1, learning_rate 0.0001
2017-10-02T23:54:25.915173: step 4590, loss 0.0040192, acc 1, learning_rate 0.0001
2017-10-02T23:54:27.082904: step 4591, loss 0.00386953, acc 1, learning_rate 0.0001
2017-10-02T23:54:28.229271: step 4592, loss 0.012051, acc 1, learning_rate 0.0001
2017-10-02T23:54:29.385503: step 4593, loss 0.0115917, acc 1, learning_rate 0.0001
2017-10-02T23:54:30.609852: step 4594, loss 0.00870244, acc 1, learning_rate 0.0001
2017-10-02T23:54:31.752894: step 4595, loss 0.00903902, acc 1, learning_rate 0.0001
2017-10-02T23:54:32.904941: step 4596, loss 0.00538307, acc 1, learning_rate 0.0001
2017-10-02T23:54:34.054283: step 4597, loss 0.0161827, acc 1, learning_rate 0.0001
2017-10-02T23:54:35.209709: step 4598, loss 0.0139188, acc 1, learning_rate 0.0001
2017-10-02T23:54:36.368947: step 4599, loss 0.0264697, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:37.566715: step 4600, loss 0.0038794, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:54:37.888526: step 4600, loss 1.45361, acc 0.466187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4600

2017-10-02T23:54:46.181488: step 4601, loss 0.0556573, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:47.372091: step 4602, loss 0.0572192, acc 0.96875, learning_rate 0.0001
2017-10-02T23:54:48.526494: step 4603, loss 0.00483305, acc 1, learning_rate 0.0001
2017-10-02T23:54:49.674175: step 4604, loss 0.0490234, acc 0.984375, learning_rate 0.0001
2017-10-02T23:54:51.053890: step 4605, loss 0.004292, acc 1, learning_rate 0.0001
2017-10-02T23:54:52.209398: step 4606, loss 0.00598293, acc 1, learning_rate 0.0001
2017-10-02T23:54:53.360771: step 4607, loss 0.0060311, acc 1, learning_rate 0.0001
2017-10-02T23:54:54.506085: step 4608, loss 0.00513269, acc 1, learning_rate 0.0001
2017-10-02T23:54:55.657238: step 4609, loss 0.00639919, acc 1, learning_rate 0.0001
2017-10-02T23:54:56.821338: step 4610, loss 0.0107408, acc 1, learning_rate 0.0001
2017-10-02T23:54:57.989215: step 4611, loss 0.0104428, acc 1, learning_rate 0.0001
2017-10-02T23:54:59.151889: step 4612, loss 0.0135629, acc 1, learning_rate 0.0001
2017-10-02T23:55:00.311825: step 4613, loss 0.00512832, acc 1, learning_rate 0.0001
2017-10-02T23:55:01.462845: step 4614, loss 0.0066499, acc 1, learning_rate 0.0001
2017-10-02T23:55:02.609938: step 4615, loss 0.00547277, acc 1, learning_rate 0.0001
2017-10-02T23:55:03.746534: step 4616, loss 0.00857601, acc 1, learning_rate 0.0001
2017-10-02T23:55:04.894700: step 4617, loss 0.0271013, acc 0.984375, learning_rate 0.0001
2017-10-02T23:55:06.045384: step 4618, loss 0.00367721, acc 1, learning_rate 0.0001
2017-10-02T23:55:07.217301: step 4619, loss 0.0102773, acc 1, learning_rate 0.0001
2017-10-02T23:55:08.371571: step 4620, loss 0.00584831, acc 1, learning_rate 0.0001
2017-10-02T23:55:09.529203: step 4621, loss 0.00562459, acc 1, learning_rate 0.0001
2017-10-02T23:55:10.696578: step 4622, loss 0.00586185, acc 1, learning_rate 0.0001
2017-10-02T23:55:11.847052: step 4623, loss 0.00443283, acc 1, learning_rate 0.0001
2017-10-02T23:55:12.992158: step 4624, loss 0.0060033, acc 1, learning_rate 0.0001
2017-10-02T23:55:14.148166: step 4625, loss 0.00553073, acc 1, learning_rate 0.0001
2017-10-02T23:55:15.302327: step 4626, loss 0.00576842, acc 1, learning_rate 0.0001
2017-10-02T23:55:16.459116: step 4627, loss 0.0323895, acc 0.96875, learning_rate 0.0001
2017-10-02T23:55:17.601909: step 4628, loss 0.0543337, acc 0.984375, learning_rate 0.0001
2017-10-02T23:55:18.746167: step 4629, loss 0.00509084, acc 1, learning_rate 0.0001
2017-10-02T23:55:19.885823: step 4630, loss 0.0183074, acc 1, learning_rate 0.0001
2017-10-02T23:55:21.032715: step 4631, loss 0.00468538, acc 1, learning_rate 0.0001
2017-10-02T23:55:22.177003: step 4632, loss 0.00513917, acc 1, learning_rate 0.0001
2017-10-02T23:55:23.314474: step 4633, loss 0.0158273, acc 1, learning_rate 0.0001
2017-10-02T23:55:24.474698: step 4634, loss 0.0328866, acc 0.96875, learning_rate 0.0001
2017-10-02T23:55:25.627303: step 4635, loss 0.00509282, acc 1, learning_rate 0.0001
2017-10-02T23:55:26.778234: step 4636, loss 0.00824133, acc 1, learning_rate 0.0001
2017-10-02T23:55:27.934281: step 4637, loss 0.0108672, acc 1, learning_rate 0.0001
2017-10-02T23:55:29.084714: step 4638, loss 0.011539, acc 1, learning_rate 0.0001
2017-10-02T23:55:30.230702: step 4639, loss 0.0120293, acc 1, learning_rate 0.0001
2017-10-02T23:55:31.388508: step 4640, loss 0.0159395, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:55:31.710520: step 4640, loss 1.46318, acc 0.431655

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4640

2017-10-02T23:55:39.470987: step 4641, loss 0.0564499, acc 0.984375, learning_rate 0.0001
2017-10-02T23:55:40.742564: step 4642, loss 0.0110633, acc 1, learning_rate 0.0001
2017-10-02T23:55:41.987143: step 4643, loss 0.00564943, acc 1, learning_rate 0.0001
2017-10-02T23:55:43.148078: step 4644, loss 0.0049699, acc 1, learning_rate 0.0001
2017-10-02T23:55:44.305279: step 4645, loss 0.0090642, acc 1, learning_rate 0.0001
2017-10-02T23:55:45.455250: step 4646, loss 0.00462505, acc 1, learning_rate 0.0001
2017-10-02T23:55:46.606913: step 4647, loss 0.0323071, acc 0.984375, learning_rate 0.0001
2017-10-02T23:55:47.766227: step 4648, loss 0.00726295, acc 1, learning_rate 0.0001
2017-10-02T23:55:48.927518: step 4649, loss 0.00577513, acc 1, learning_rate 0.0001
2017-10-02T23:55:50.091801: step 4650, loss 0.0327954, acc 0.96875, learning_rate 0.0001
2017-10-02T23:55:51.249462: step 4651, loss 0.00520391, acc 1, learning_rate 0.0001
2017-10-02T23:55:52.401472: step 4652, loss 0.00383227, acc 1, learning_rate 0.0001
2017-10-02T23:55:53.557711: step 4653, loss 0.0042528, acc 1, learning_rate 0.0001
2017-10-02T23:55:54.713872: step 4654, loss 0.00699767, acc 1, learning_rate 0.0001
2017-10-02T23:55:55.871828: step 4655, loss 0.0038912, acc 1, learning_rate 0.0001
2017-10-02T23:55:57.019870: step 4656, loss 0.00549504, acc 1, learning_rate 0.0001
2017-10-02T23:55:58.163709: step 4657, loss 0.0058042, acc 1, learning_rate 0.0001
2017-10-02T23:55:59.330860: step 4658, loss 0.0632794, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:00.496195: step 4659, loss 0.00728833, acc 1, learning_rate 0.0001
2017-10-02T23:56:01.665172: step 4660, loss 0.00471175, acc 1, learning_rate 0.0001
2017-10-02T23:56:02.799015: step 4661, loss 0.00717357, acc 1, learning_rate 0.0001
2017-10-02T23:56:03.946862: step 4662, loss 0.0392142, acc 0.96875, learning_rate 0.0001
2017-10-02T23:56:05.093567: step 4663, loss 0.0452451, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:06.261531: step 4664, loss 0.00770869, acc 1, learning_rate 0.0001
2017-10-02T23:56:07.414922: step 4665, loss 0.0232012, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:08.562500: step 4666, loss 0.0067356, acc 1, learning_rate 0.0001
2017-10-02T23:56:09.779054: step 4667, loss 0.0326487, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:10.922900: step 4668, loss 0.00444705, acc 1, learning_rate 0.0001
2017-10-02T23:56:12.084879: step 4669, loss 0.0261982, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:13.231336: step 4670, loss 0.0102157, acc 1, learning_rate 0.0001
2017-10-02T23:56:14.385122: step 4671, loss 0.00853387, acc 1, learning_rate 0.0001
2017-10-02T23:56:15.554768: step 4672, loss 0.0203737, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:16.695442: step 4673, loss 0.013453, acc 1, learning_rate 0.0001
2017-10-02T23:56:17.872381: step 4674, loss 0.0192632, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:19.026622: step 4675, loss 0.00554478, acc 1, learning_rate 0.0001
2017-10-02T23:56:20.181812: step 4676, loss 0.00751446, acc 1, learning_rate 0.0001
2017-10-02T23:56:21.330776: step 4677, loss 0.00666874, acc 1, learning_rate 0.0001
2017-10-02T23:56:22.493370: step 4678, loss 0.0227569, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:23.653740: step 4679, loss 0.00598629, acc 1, learning_rate 0.0001
2017-10-02T23:56:24.880993: step 4680, loss 0.054911, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-02T23:56:25.228814: step 4680, loss 1.43722, acc 0.484892

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4680

2017-10-02T23:56:33.154929: step 4681, loss 0.0388715, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:34.327355: step 4682, loss 0.00485919, acc 1, learning_rate 0.0001
2017-10-02T23:56:35.484114: step 4683, loss 0.00882466, acc 1, learning_rate 0.0001
2017-10-02T23:56:36.636279: step 4684, loss 0.0172738, acc 1, learning_rate 0.0001
2017-10-02T23:56:38.005370: step 4685, loss 0.0264554, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:39.161852: step 4686, loss 0.00432308, acc 1, learning_rate 0.0001
2017-10-02T23:56:40.327606: step 4687, loss 0.0317079, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:41.475151: step 4688, loss 0.0138884, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:42.763140: step 4689, loss 0.0164974, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:43.921128: step 4690, loss 0.0339364, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:45.101468: step 4691, loss 0.00573128, acc 1, learning_rate 0.0001
2017-10-02T23:56:46.258974: step 4692, loss 0.0367232, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:47.404049: step 4693, loss 0.00540648, acc 1, learning_rate 0.0001
2017-10-02T23:56:48.562064: step 4694, loss 0.00851438, acc 1, learning_rate 0.0001
2017-10-02T23:56:49.768006: step 4695, loss 0.00434747, acc 1, learning_rate 0.0001
2017-10-02T23:56:50.916065: step 4696, loss 0.00520988, acc 1, learning_rate 0.0001
2017-10-02T23:56:52.076257: step 4697, loss 0.0128327, acc 1, learning_rate 0.0001
2017-10-02T23:56:53.215457: step 4698, loss 0.0219043, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:54.357625: step 4699, loss 0.0266032, acc 0.984375, learning_rate 0.0001
2017-10-02T23:56:55.510929: step 4700, loss 0.00411267, acc 1, learning_rate 0.0001
2017-10-02T23:56:56.660621: step 4701, loss 0.0080555, acc 1, learning_rate 0.0001
2017-10-02T23:56:57.831420: step 4702, loss 0.00780431, acc 1, learning_rate 0.0001
2017-10-02T23:56:59.207793: step 4703, loss 0.00561218, acc 1, learning_rate 0.0001
2017-10-02T23:57:00.356642: step 4704, loss 0.00960164, acc 1, learning_rate 0.0001
2017-10-02T23:57:01.510556: step 4705, loss 0.0188864, acc 1, learning_rate 0.0001
2017-10-02T23:57:02.821572: step 4706, loss 0.0442615, acc 0.984375, learning_rate 0.0001
2017-10-02T23:57:03.975349: step 4707, loss 0.00502378, acc 1, learning_rate 0.0001
2017-10-02T23:57:05.124378: step 4708, loss 0.00931377, acc 1, learning_rate 0.0001
2017-10-02T23:57:06.275407: step 4709, loss 0.00493075, acc 1, learning_rate 0.0001
2017-10-02T23:57:07.442724: step 4710, loss 0.00807001, acc 1, learning_rate 0.0001
2017-10-02T23:57:08.591637: step 4711, loss 0.00408464, acc 1, learning_rate 0.0001
2017-10-02T23:57:09.803675: step 4712, loss 0.00456173, acc 1, learning_rate 0.0001
2017-10-02T23:57:11.193105: step 4713, loss 0.00498094, acc 1, learning_rate 0.0001
2017-10-02T23:57:12.361146: step 4714, loss 0.0273565, acc 0.984375, learning_rate 0.0001
2017-10-02T23:57:13.516869: step 4715, loss 0.00411464, acc 1, learning_rate 0.0001
2017-10-02T23:57:14.671403: step 4716, loss 0.0400787, acc 1, learning_rate 0.0001
2017-10-02T23:57:15.828809: step 4717, loss 0.00516215, acc 1, learning_rate 0.0001
2017-10-02T23:57:16.982418: step 4718, loss 0.0248031, acc 0.984375, learning_rate 0.0001
2017-10-02T23:57:18.148620: step 4719, loss 0.00458043, acc 1, learning_rate 0.0001
2017-10-02T23:57:19.387547: step 4720, loss 0.0605162, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-02T23:57:19.736326: step 4720, loss 1.48314, acc 0.423022

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4720

2017-10-02T23:57:27.140234: step 4721, loss 0.00916046, acc 1, learning_rate 0.0001
2017-10-02T23:57:28.308849: step 4722, loss 0.029528, acc 0.984375, learning_rate 0.0001
2017-10-02T23:57:29.454292: step 4723, loss 0.00419045, acc 1, learning_rate 0.0001
2017-10-02T23:57:30.608353: step 4724, loss 0.00754323, acc 1, learning_rate 0.0001
2017-10-02T23:57:31.844652: step 4725, loss 0.0188886, acc 1, learning_rate 0.0001
2017-10-02T23:57:32.993577: step 4726, loss 0.0186791, acc 1, learning_rate 0.0001
2017-10-02T23:57:34.130216: step 4727, loss 0.00420855, acc 1, learning_rate 0.0001
2017-10-02T23:57:35.271990: step 4728, loss 0.00909627, acc 1, learning_rate 0.0001
2017-10-02T23:57:36.424902: step 4729, loss 0.00660662, acc 1, learning_rate 0.0001
2017-10-02T23:57:37.577924: step 4730, loss 0.00545023, acc 1, learning_rate 0.0001
2017-10-02T23:57:38.915176: step 4731, loss 0.0418031, acc 0.984375, learning_rate 0.0001
2017-10-02T23:57:40.073279: step 4732, loss 0.00598849, acc 1, learning_rate 0.0001
2017-10-02T23:57:41.223980: step 4733, loss 0.00545902, acc 1, learning_rate 0.0001
2017-10-02T23:57:42.382588: step 4734, loss 0.00520592, acc 1, learning_rate 0.0001
2017-10-02T23:57:43.512334: step 4735, loss 0.0747969, acc 0.96875, learning_rate 0.0001
2017-10-02T23:57:44.668088: step 4736, loss 0.0222434, acc 0.984375, learning_rate 0.0001
2017-10-02T23:57:45.808968: step 4737, loss 0.00408413, acc 1, learning_rate 0.0001
2017-10-02T23:57:46.971118: step 4738, loss 0.00537823, acc 1, learning_rate 0.0001
2017-10-02T23:57:48.121427: step 4739, loss 0.00732563, acc 1, learning_rate 0.0001
2017-10-02T23:57:49.275044: step 4740, loss 0.0107524, acc 1, learning_rate 0.0001
2017-10-02T23:57:50.444938: step 4741, loss 0.00904111, acc 1, learning_rate 0.0001
2017-10-02T23:57:51.606107: step 4742, loss 0.0077264, acc 1, learning_rate 0.0001
2017-10-02T23:57:52.750874: step 4743, loss 0.0372073, acc 0.984375, learning_rate 0.0001
2017-10-02T23:57:53.916166: step 4744, loss 0.0603237, acc 0.96875, learning_rate 0.0001
2017-10-02T23:57:55.071532: step 4745, loss 0.0306544, acc 0.96875, learning_rate 0.0001
2017-10-02T23:57:56.220367: step 4746, loss 0.0068249, acc 1, learning_rate 0.0001
2017-10-02T23:57:57.379005: step 4747, loss 0.00528537, acc 1, learning_rate 0.0001
2017-10-02T23:57:58.526240: step 4748, loss 0.00688506, acc 1, learning_rate 0.0001
2017-10-02T23:57:59.679400: step 4749, loss 0.0306397, acc 0.984375, learning_rate 0.0001
2017-10-02T23:58:00.830740: step 4750, loss 0.0125589, acc 1, learning_rate 0.0001
2017-10-02T23:58:01.979956: step 4751, loss 0.00542468, acc 1, learning_rate 0.0001
2017-10-02T23:58:03.140704: step 4752, loss 0.00596814, acc 1, learning_rate 0.0001
2017-10-02T23:58:04.288523: step 4753, loss 0.00780305, acc 1, learning_rate 0.0001
2017-10-02T23:58:05.431514: step 4754, loss 0.00518777, acc 1, learning_rate 0.0001
2017-10-02T23:58:06.582678: step 4755, loss 0.00467238, acc 1, learning_rate 0.0001
2017-10-02T23:58:07.726151: step 4756, loss 0.00858922, acc 1, learning_rate 0.0001
2017-10-02T23:58:08.867564: step 4757, loss 0.00520992, acc 1, learning_rate 0.0001
2017-10-02T23:58:10.017612: step 4758, loss 0.0107686, acc 1, learning_rate 0.0001
2017-10-02T23:58:11.170300: step 4759, loss 0.0249779, acc 1, learning_rate 0.0001
2017-10-02T23:58:12.324205: step 4760, loss 0.00456088, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:58:12.644246: step 4760, loss 1.44462, acc 0.477698

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4760

2017-10-02T23:58:20.223699: step 4761, loss 0.0241805, acc 0.984375, learning_rate 0.0001
2017-10-02T23:58:21.421278: step 4762, loss 0.0201301, acc 0.984375, learning_rate 0.0001
2017-10-02T23:58:22.551566: step 4763, loss 0.0531687, acc 0.96875, learning_rate 0.0001
2017-10-02T23:58:23.701661: step 4764, loss 0.00464932, acc 1, learning_rate 0.0001
2017-10-02T23:58:24.855013: step 4765, loss 0.00472671, acc 1, learning_rate 0.0001
2017-10-02T23:58:25.996642: step 4766, loss 0.00813057, acc 1, learning_rate 0.0001
2017-10-02T23:58:27.159354: step 4767, loss 0.00458092, acc 1, learning_rate 0.0001
2017-10-02T23:58:28.310771: step 4768, loss 0.00390156, acc 1, learning_rate 0.0001
2017-10-02T23:58:29.464322: step 4769, loss 0.00572274, acc 1, learning_rate 0.0001
2017-10-02T23:58:30.613039: step 4770, loss 0.00488457, acc 1, learning_rate 0.0001
2017-10-02T23:58:31.774298: step 4771, loss 0.00973225, acc 1, learning_rate 0.0001
2017-10-02T23:58:32.970269: step 4772, loss 0.00842793, acc 1, learning_rate 0.0001
2017-10-02T23:58:34.122403: step 4773, loss 0.00759487, acc 1, learning_rate 0.0001
2017-10-02T23:58:35.284474: step 4774, loss 0.00963722, acc 1, learning_rate 0.0001
2017-10-02T23:58:36.459192: step 4775, loss 0.00444683, acc 1, learning_rate 0.0001
2017-10-02T23:58:37.615735: step 4776, loss 0.0176521, acc 0.984375, learning_rate 0.0001
2017-10-02T23:58:38.777785: step 4777, loss 0.0209069, acc 1, learning_rate 0.0001
2017-10-02T23:58:39.933555: step 4778, loss 0.0553695, acc 0.96875, learning_rate 0.0001
2017-10-02T23:58:41.091838: step 4779, loss 0.00839869, acc 1, learning_rate 0.0001
2017-10-02T23:58:42.242409: step 4780, loss 0.0227752, acc 0.984375, learning_rate 0.0001
2017-10-02T23:58:43.393581: step 4781, loss 0.00421967, acc 1, learning_rate 0.0001
2017-10-02T23:58:44.547914: step 4782, loss 0.00399831, acc 1, learning_rate 0.0001
2017-10-02T23:58:45.694253: step 4783, loss 0.0119594, acc 1, learning_rate 0.0001
2017-10-02T23:58:46.850729: step 4784, loss 0.0126219, acc 1, learning_rate 0.0001
2017-10-02T23:58:48.015461: step 4785, loss 0.00536478, acc 1, learning_rate 0.0001
2017-10-02T23:58:49.153409: step 4786, loss 0.00302091, acc 1, learning_rate 0.0001
2017-10-02T23:58:50.300896: step 4787, loss 0.00407593, acc 1, learning_rate 0.0001
2017-10-02T23:58:51.452466: step 4788, loss 0.00355425, acc 1, learning_rate 0.0001
2017-10-02T23:58:52.595565: step 4789, loss 0.039771, acc 0.984375, learning_rate 0.0001
2017-10-02T23:58:53.744264: step 4790, loss 0.00927863, acc 1, learning_rate 0.0001
2017-10-02T23:58:54.896341: step 4791, loss 0.00815557, acc 1, learning_rate 0.0001
2017-10-02T23:58:56.046003: step 4792, loss 0.00635154, acc 1, learning_rate 0.0001
2017-10-02T23:58:57.210751: step 4793, loss 0.0185624, acc 0.984375, learning_rate 0.0001
2017-10-02T23:58:58.360602: step 4794, loss 0.0292812, acc 0.984375, learning_rate 0.0001
2017-10-02T23:58:59.505130: step 4795, loss 0.00803378, acc 1, learning_rate 0.0001
2017-10-02T23:59:00.658982: step 4796, loss 0.00500442, acc 1, learning_rate 0.0001
2017-10-02T23:59:01.805109: step 4797, loss 0.0178685, acc 1, learning_rate 0.0001
2017-10-02T23:59:02.962288: step 4798, loss 0.0211596, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:04.102724: step 4799, loss 0.0369195, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:05.248904: step 4800, loss 0.00309603, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:59:05.596874: step 4800, loss 1.43248, acc 0.493525

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4800

2017-10-02T23:59:12.932061: step 4801, loss 0.0282848, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:14.096194: step 4802, loss 0.00713815, acc 1, learning_rate 0.0001
2017-10-02T23:59:15.223676: step 4803, loss 0.0049512, acc 1, learning_rate 0.0001
2017-10-02T23:59:16.374953: step 4804, loss 0.00548235, acc 1, learning_rate 0.0001
2017-10-02T23:59:17.515410: step 4805, loss 0.00702047, acc 1, learning_rate 0.0001
2017-10-02T23:59:18.666515: step 4806, loss 0.00817909, acc 1, learning_rate 0.0001
2017-10-02T23:59:19.871128: step 4807, loss 0.00580554, acc 1, learning_rate 0.0001
2017-10-02T23:59:21.022948: step 4808, loss 0.0114477, acc 1, learning_rate 0.0001
2017-10-02T23:59:22.168021: step 4809, loss 0.00418066, acc 1, learning_rate 0.0001
2017-10-02T23:59:23.310752: step 4810, loss 0.0123017, acc 1, learning_rate 0.0001
2017-10-02T23:59:24.463152: step 4811, loss 0.00428075, acc 1, learning_rate 0.0001
2017-10-02T23:59:25.604196: step 4812, loss 0.00522619, acc 1, learning_rate 0.0001
2017-10-02T23:59:26.767301: step 4813, loss 0.00461108, acc 1, learning_rate 0.0001
2017-10-02T23:59:27.925438: step 4814, loss 0.00574045, acc 1, learning_rate 0.0001
2017-10-02T23:59:29.090430: step 4815, loss 0.0371356, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:30.260101: step 4816, loss 0.0376446, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:31.439248: step 4817, loss 0.00468325, acc 1, learning_rate 0.0001
2017-10-02T23:59:32.599391: step 4818, loss 0.0317934, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:33.761956: step 4819, loss 0.0218187, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:34.910020: step 4820, loss 0.00857682, acc 1, learning_rate 0.0001
2017-10-02T23:59:36.075700: step 4821, loss 0.0336372, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:37.234653: step 4822, loss 0.00758618, acc 1, learning_rate 0.0001
2017-10-02T23:59:38.418191: step 4823, loss 0.00402335, acc 1, learning_rate 0.0001
2017-10-02T23:59:39.565976: step 4824, loss 0.0126489, acc 1, learning_rate 0.0001
2017-10-02T23:59:40.728204: step 4825, loss 0.00584075, acc 1, learning_rate 0.0001
2017-10-02T23:59:41.864488: step 4826, loss 0.0285742, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:43.009667: step 4827, loss 0.0116208, acc 1, learning_rate 0.0001
2017-10-02T23:59:44.163403: step 4828, loss 0.023285, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:45.306037: step 4829, loss 0.00541988, acc 1, learning_rate 0.0001
2017-10-02T23:59:46.461314: step 4830, loss 0.0196964, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:47.616604: step 4831, loss 0.00827088, acc 1, learning_rate 0.0001
2017-10-02T23:59:48.783356: step 4832, loss 0.00341101, acc 1, learning_rate 0.0001
2017-10-02T23:59:49.948796: step 4833, loss 0.00539996, acc 1, learning_rate 0.0001
2017-10-02T23:59:51.088039: step 4834, loss 0.033104, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:52.240450: step 4835, loss 0.0254309, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:53.413097: step 4836, loss 0.0222048, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:54.568630: step 4837, loss 0.00968305, acc 1, learning_rate 0.0001
2017-10-02T23:59:55.728356: step 4838, loss 0.0208424, acc 1, learning_rate 0.0001
2017-10-02T23:59:56.925872: step 4839, loss 0.0147011, acc 0.984375, learning_rate 0.0001
2017-10-02T23:59:58.086914: step 4840, loss 0.0165427, acc 1, learning_rate 0.0001

Evaluation:
2017-10-02T23:59:58.433334: step 4840, loss 1.46145, acc 0.454676

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4840

2017-10-03T00:00:05.397894: step 4841, loss 0.00582608, acc 1, learning_rate 0.0001
2017-10-03T00:00:06.567067: step 4842, loss 0.00580754, acc 1, learning_rate 0.0001
2017-10-03T00:00:07.724700: step 4843, loss 0.0142484, acc 1, learning_rate 0.0001
2017-10-03T00:00:08.869497: step 4844, loss 0.00715254, acc 1, learning_rate 0.0001
2017-10-03T00:00:10.030811: step 4845, loss 0.00542929, acc 1, learning_rate 0.0001
2017-10-03T00:00:11.191686: step 4846, loss 0.00515925, acc 1, learning_rate 0.0001
2017-10-03T00:00:12.357456: step 4847, loss 0.00891845, acc 1, learning_rate 0.0001
2017-10-03T00:00:13.513044: step 4848, loss 0.00572342, acc 1, learning_rate 0.0001
2017-10-03T00:00:14.656501: step 4849, loss 0.00389633, acc 1, learning_rate 0.0001
2017-10-03T00:00:15.797078: step 4850, loss 0.0132364, acc 1, learning_rate 0.0001
2017-10-03T00:00:16.956158: step 4851, loss 0.00612268, acc 1, learning_rate 0.0001
2017-10-03T00:00:18.107191: step 4852, loss 0.00314035, acc 1, learning_rate 0.0001
2017-10-03T00:00:19.255081: step 4853, loss 0.0228088, acc 1, learning_rate 0.0001
2017-10-03T00:00:20.402697: step 4854, loss 0.00448849, acc 1, learning_rate 0.0001
2017-10-03T00:00:21.543127: step 4855, loss 0.00415638, acc 1, learning_rate 0.0001
2017-10-03T00:00:22.686559: step 4856, loss 0.00459039, acc 1, learning_rate 0.0001
2017-10-03T00:00:23.838166: step 4857, loss 0.057595, acc 0.984375, learning_rate 0.0001
2017-10-03T00:00:24.991069: step 4858, loss 0.00465272, acc 1, learning_rate 0.0001
2017-10-03T00:00:26.150065: step 4859, loss 0.00342708, acc 1, learning_rate 0.0001
2017-10-03T00:00:27.290183: step 4860, loss 0.0154525, acc 1, learning_rate 0.0001
2017-10-03T00:00:28.439238: step 4861, loss 0.00429212, acc 1, learning_rate 0.0001
2017-10-03T00:00:29.592349: step 4862, loss 0.00513602, acc 1, learning_rate 0.0001
2017-10-03T00:00:30.759871: step 4863, loss 0.0102888, acc 1, learning_rate 0.0001
2017-10-03T00:00:31.910034: step 4864, loss 0.0310289, acc 0.984375, learning_rate 0.0001
2017-10-03T00:00:33.065206: step 4865, loss 0.0100286, acc 1, learning_rate 0.0001
2017-10-03T00:00:34.209841: step 4866, loss 0.012763, acc 1, learning_rate 0.0001
2017-10-03T00:00:35.355132: step 4867, loss 0.00511741, acc 1, learning_rate 0.0001
2017-10-03T00:00:36.519632: step 4868, loss 0.00378305, acc 1, learning_rate 0.0001
2017-10-03T00:00:37.663835: step 4869, loss 0.0062442, acc 1, learning_rate 0.0001
2017-10-03T00:00:38.809922: step 4870, loss 0.00740916, acc 1, learning_rate 0.0001
2017-10-03T00:00:39.962666: step 4871, loss 0.0446698, acc 0.984375, learning_rate 0.0001
2017-10-03T00:00:41.116424: step 4872, loss 0.0140094, acc 1, learning_rate 0.0001
2017-10-03T00:00:42.285712: step 4873, loss 0.0287198, acc 0.984375, learning_rate 0.0001
2017-10-03T00:00:43.436662: step 4874, loss 0.00316093, acc 1, learning_rate 0.0001
2017-10-03T00:00:44.581256: step 4875, loss 0.0385141, acc 0.984375, learning_rate 0.0001
2017-10-03T00:00:45.743985: step 4876, loss 0.00224318, acc 1, learning_rate 0.0001
2017-10-03T00:00:46.916231: step 4877, loss 0.00488648, acc 1, learning_rate 0.0001
2017-10-03T00:00:48.086963: step 4878, loss 0.037515, acc 0.96875, learning_rate 0.0001
2017-10-03T00:00:49.238031: step 4879, loss 0.028563, acc 0.984375, learning_rate 0.0001
2017-10-03T00:00:50.408869: step 4880, loss 0.00337699, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:00:50.734921: step 4880, loss 1.47264, acc 0.423022

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4880

2017-10-03T00:00:58.104771: step 4881, loss 0.00274411, acc 1, learning_rate 0.0001
2017-10-03T00:00:59.273309: step 4882, loss 0.0580353, acc 0.96875, learning_rate 0.0001
2017-10-03T00:01:00.464049: step 4883, loss 0.0042888, acc 1, learning_rate 0.0001
2017-10-03T00:01:01.617228: step 4884, loss 0.0116991, acc 1, learning_rate 0.0001
2017-10-03T00:01:02.776824: step 4885, loss 0.00729403, acc 1, learning_rate 0.0001
2017-10-03T00:01:03.925169: step 4886, loss 0.018658, acc 1, learning_rate 0.0001
2017-10-03T00:01:05.090772: step 4887, loss 0.0106136, acc 1, learning_rate 0.0001
2017-10-03T00:01:06.260603: step 4888, loss 0.038916, acc 0.96875, learning_rate 0.0001
2017-10-03T00:01:07.419168: step 4889, loss 0.00603616, acc 1, learning_rate 0.0001
2017-10-03T00:01:08.562179: step 4890, loss 0.0584329, acc 0.96875, learning_rate 0.0001
2017-10-03T00:01:09.718659: step 4891, loss 0.0200131, acc 1, learning_rate 0.0001
2017-10-03T00:01:10.870222: step 4892, loss 0.0481879, acc 0.96875, learning_rate 0.0001
2017-10-03T00:01:12.023662: step 4893, loss 0.0124845, acc 1, learning_rate 0.0001
2017-10-03T00:01:13.187646: step 4894, loss 0.0044385, acc 1, learning_rate 0.0001
2017-10-03T00:01:14.348364: step 4895, loss 0.00409233, acc 1, learning_rate 0.0001
2017-10-03T00:01:15.507030: step 4896, loss 0.00413845, acc 1, learning_rate 0.0001
2017-10-03T00:01:16.653464: step 4897, loss 0.0382761, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:17.802684: step 4898, loss 0.00570805, acc 1, learning_rate 0.0001
2017-10-03T00:01:18.965721: step 4899, loss 0.0053363, acc 1, learning_rate 0.0001
2017-10-03T00:01:20.102550: step 4900, loss 0.0599368, acc 0.980392, learning_rate 0.0001
2017-10-03T00:01:21.263910: step 4901, loss 0.023469, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:22.420025: step 4902, loss 0.0299037, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:23.559790: step 4903, loss 0.0283078, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:24.707072: step 4904, loss 0.00449466, acc 1, learning_rate 0.0001
2017-10-03T00:01:25.851166: step 4905, loss 0.0169656, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:27.021018: step 4906, loss 0.0088866, acc 1, learning_rate 0.0001
2017-10-03T00:01:28.170756: step 4907, loss 0.0379105, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:29.316334: step 4908, loss 0.00695257, acc 1, learning_rate 0.0001
2017-10-03T00:01:30.449328: step 4909, loss 0.0194469, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:31.595790: step 4910, loss 0.0227382, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:32.747129: step 4911, loss 0.00478927, acc 1, learning_rate 0.0001
2017-10-03T00:01:33.907355: step 4912, loss 0.0185244, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:35.046513: step 4913, loss 0.00600656, acc 1, learning_rate 0.0001
2017-10-03T00:01:36.192947: step 4914, loss 0.0111486, acc 1, learning_rate 0.0001
2017-10-03T00:01:37.359420: step 4915, loss 0.0285244, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:38.517139: step 4916, loss 0.00650947, acc 1, learning_rate 0.0001
2017-10-03T00:01:39.677264: step 4917, loss 0.0158788, acc 1, learning_rate 0.0001
2017-10-03T00:01:40.824140: step 4918, loss 0.00510639, acc 1, learning_rate 0.0001
2017-10-03T00:01:41.975130: step 4919, loss 0.00392919, acc 1, learning_rate 0.0001
2017-10-03T00:01:43.124136: step 4920, loss 0.0262155, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:01:43.458759: step 4920, loss 1.45817, acc 0.453237

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4920

2017-10-03T00:01:51.133706: step 4921, loss 0.0127542, acc 1, learning_rate 0.0001
2017-10-03T00:01:52.344983: step 4922, loss 0.00826386, acc 1, learning_rate 0.0001
2017-10-03T00:01:53.495055: step 4923, loss 0.00664308, acc 1, learning_rate 0.0001
2017-10-03T00:01:54.649980: step 4924, loss 0.0259643, acc 0.984375, learning_rate 0.0001
2017-10-03T00:01:55.803636: step 4925, loss 0.00622488, acc 1, learning_rate 0.0001
2017-10-03T00:01:56.974750: step 4926, loss 0.00350201, acc 1, learning_rate 0.0001
2017-10-03T00:01:58.130702: step 4927, loss 0.00725167, acc 1, learning_rate 0.0001
2017-10-03T00:01:59.294698: step 4928, loss 0.00674822, acc 1, learning_rate 0.0001
2017-10-03T00:02:00.443839: step 4929, loss 0.00617379, acc 1, learning_rate 0.0001
2017-10-03T00:02:01.586830: step 4930, loss 0.00291961, acc 1, learning_rate 0.0001
2017-10-03T00:02:02.745609: step 4931, loss 0.0610014, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:03.890370: step 4932, loss 0.00304557, acc 1, learning_rate 0.0001
2017-10-03T00:02:05.045401: step 4933, loss 0.0660095, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:06.192008: step 4934, loss 0.00357748, acc 1, learning_rate 0.0001
2017-10-03T00:02:07.340723: step 4935, loss 0.0184273, acc 1, learning_rate 0.0001
2017-10-03T00:02:08.497721: step 4936, loss 0.0206569, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:09.656994: step 4937, loss 0.00691646, acc 1, learning_rate 0.0001
2017-10-03T00:02:10.807046: step 4938, loss 0.00451694, acc 1, learning_rate 0.0001
2017-10-03T00:02:11.972807: step 4939, loss 0.00367727, acc 1, learning_rate 0.0001
2017-10-03T00:02:13.139281: step 4940, loss 0.0384516, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:14.297820: step 4941, loss 0.0644944, acc 0.96875, learning_rate 0.0001
2017-10-03T00:02:15.467886: step 4942, loss 0.00257432, acc 1, learning_rate 0.0001
2017-10-03T00:02:16.644895: step 4943, loss 0.00771405, acc 1, learning_rate 0.0001
2017-10-03T00:02:17.862439: step 4944, loss 0.0598884, acc 0.96875, learning_rate 0.0001
2017-10-03T00:02:19.017907: step 4945, loss 0.0287186, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:20.172646: step 4946, loss 0.00551675, acc 1, learning_rate 0.0001
2017-10-03T00:02:21.337292: step 4947, loss 0.0173092, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:22.499620: step 4948, loss 0.00402262, acc 1, learning_rate 0.0001
2017-10-03T00:02:23.669144: step 4949, loss 0.0103925, acc 1, learning_rate 0.0001
2017-10-03T00:02:24.822296: step 4950, loss 0.00279785, acc 1, learning_rate 0.0001
2017-10-03T00:02:25.987870: step 4951, loss 0.0237133, acc 1, learning_rate 0.0001
2017-10-03T00:02:27.160783: step 4952, loss 0.00340737, acc 1, learning_rate 0.0001
2017-10-03T00:02:28.319889: step 4953, loss 0.00495068, acc 1, learning_rate 0.0001
2017-10-03T00:02:29.473282: step 4954, loss 0.00559941, acc 1, learning_rate 0.0001
2017-10-03T00:02:30.630303: step 4955, loss 0.00837864, acc 1, learning_rate 0.0001
2017-10-03T00:02:31.830096: step 4956, loss 0.0370403, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:32.984592: step 4957, loss 0.0116375, acc 1, learning_rate 0.0001
2017-10-03T00:02:34.147747: step 4958, loss 0.00300386, acc 1, learning_rate 0.0001
2017-10-03T00:02:35.299110: step 4959, loss 0.0392697, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:36.451104: step 4960, loss 0.0157384, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:02:36.819989: step 4960, loss 1.46009, acc 0.460432

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-4960

2017-10-03T00:02:44.532466: step 4961, loss 0.0375858, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:45.700961: step 4962, loss 0.0141655, acc 1, learning_rate 0.0001
2017-10-03T00:02:46.845742: step 4963, loss 0.00336722, acc 1, learning_rate 0.0001
2017-10-03T00:02:47.987435: step 4964, loss 0.00915321, acc 1, learning_rate 0.0001
2017-10-03T00:02:49.155767: step 4965, loss 0.00546571, acc 1, learning_rate 0.0001
2017-10-03T00:02:50.301595: step 4966, loss 0.0258784, acc 1, learning_rate 0.0001
2017-10-03T00:02:51.480401: step 4967, loss 0.0344267, acc 0.984375, learning_rate 0.0001
2017-10-03T00:02:52.659335: step 4968, loss 0.00759751, acc 1, learning_rate 0.0001
2017-10-03T00:02:53.807214: step 4969, loss 0.00398232, acc 1, learning_rate 0.0001
2017-10-03T00:02:54.951839: step 4970, loss 0.00530187, acc 1, learning_rate 0.0001
2017-10-03T00:02:56.103901: step 4971, loss 0.00363339, acc 1, learning_rate 0.0001
2017-10-03T00:02:57.285400: step 4972, loss 0.00449215, acc 1, learning_rate 0.0001
2017-10-03T00:02:58.436010: step 4973, loss 0.00475309, acc 1, learning_rate 0.0001
2017-10-03T00:02:59.599128: step 4974, loss 0.00721628, acc 1, learning_rate 0.0001
2017-10-03T00:03:00.751265: step 4975, loss 0.0182488, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:01.899173: step 4976, loss 0.00354167, acc 1, learning_rate 0.0001
2017-10-03T00:03:03.044912: step 4977, loss 0.00624894, acc 1, learning_rate 0.0001
2017-10-03T00:03:04.187045: step 4978, loss 0.0382257, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:05.344229: step 4979, loss 0.0409649, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:06.504905: step 4980, loss 0.00585549, acc 1, learning_rate 0.0001
2017-10-03T00:03:07.680428: step 4981, loss 0.077276, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:08.833642: step 4982, loss 0.00750669, acc 1, learning_rate 0.0001
2017-10-03T00:03:09.997597: step 4983, loss 0.00431448, acc 1, learning_rate 0.0001
2017-10-03T00:03:11.142471: step 4984, loss 0.00357862, acc 1, learning_rate 0.0001
2017-10-03T00:03:12.302264: step 4985, loss 0.0422563, acc 0.96875, learning_rate 0.0001
2017-10-03T00:03:13.484554: step 4986, loss 0.00491764, acc 1, learning_rate 0.0001
2017-10-03T00:03:14.630413: step 4987, loss 0.00538397, acc 1, learning_rate 0.0001
2017-10-03T00:03:15.781192: step 4988, loss 0.00571282, acc 1, learning_rate 0.0001
2017-10-03T00:03:16.932199: step 4989, loss 0.0417649, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:18.076231: step 4990, loss 0.0027944, acc 1, learning_rate 0.0001
2017-10-03T00:03:19.222210: step 4991, loss 0.00698381, acc 1, learning_rate 0.0001
2017-10-03T00:03:20.375838: step 4992, loss 0.00744484, acc 1, learning_rate 0.0001
2017-10-03T00:03:21.522276: step 4993, loss 0.0195235, acc 1, learning_rate 0.0001
2017-10-03T00:03:22.671817: step 4994, loss 0.00407438, acc 1, learning_rate 0.0001
2017-10-03T00:03:23.839773: step 4995, loss 0.00835124, acc 1, learning_rate 0.0001
2017-10-03T00:03:24.992027: step 4996, loss 0.00547645, acc 1, learning_rate 0.0001
2017-10-03T00:03:26.163602: step 4997, loss 0.00395241, acc 1, learning_rate 0.0001
2017-10-03T00:03:27.300942: step 4998, loss 0.00917899, acc 1, learning_rate 0.0001
2017-10-03T00:03:28.452292: step 4999, loss 0.0284645, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:29.605078: step 5000, loss 0.0055605, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:03:29.931546: step 5000, loss 1.44765, acc 0.466187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5000

2017-10-03T00:03:37.786508: step 5001, loss 0.0566968, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:38.949938: step 5002, loss 0.0245354, acc 1, learning_rate 0.0001
2017-10-03T00:03:40.106511: step 5003, loss 0.00785593, acc 1, learning_rate 0.0001
2017-10-03T00:03:41.260407: step 5004, loss 0.00549019, acc 1, learning_rate 0.0001
2017-10-03T00:03:42.415735: step 5005, loss 0.0117178, acc 1, learning_rate 0.0001
2017-10-03T00:03:43.567046: step 5006, loss 0.00412319, acc 1, learning_rate 0.0001
2017-10-03T00:03:44.726888: step 5007, loss 0.023073, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:45.886772: step 5008, loss 0.0276536, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:47.095794: step 5009, loss 0.00517978, acc 1, learning_rate 0.0001
2017-10-03T00:03:48.279758: step 5010, loss 0.00874637, acc 1, learning_rate 0.0001
2017-10-03T00:03:49.432765: step 5011, loss 0.0229682, acc 0.984375, learning_rate 0.0001
2017-10-03T00:03:50.586348: step 5012, loss 0.00441679, acc 1, learning_rate 0.0001
2017-10-03T00:03:51.724815: step 5013, loss 0.0225621, acc 1, learning_rate 0.0001
2017-10-03T00:03:52.876802: step 5014, loss 0.0573551, acc 0.96875, learning_rate 0.0001
2017-10-03T00:03:54.015761: step 5015, loss 0.00288117, acc 1, learning_rate 0.0001
2017-10-03T00:03:55.155155: step 5016, loss 0.00384841, acc 1, learning_rate 0.0001
2017-10-03T00:03:56.306730: step 5017, loss 0.00773949, acc 1, learning_rate 0.0001
2017-10-03T00:03:57.468421: step 5018, loss 0.00737943, acc 1, learning_rate 0.0001
2017-10-03T00:03:58.615400: step 5019, loss 0.00467434, acc 1, learning_rate 0.0001
2017-10-03T00:03:59.778233: step 5020, loss 0.00245678, acc 1, learning_rate 0.0001
2017-10-03T00:04:00.923541: step 5021, loss 0.00332539, acc 1, learning_rate 0.0001
2017-10-03T00:04:02.095137: step 5022, loss 0.0187779, acc 0.984375, learning_rate 0.0001
2017-10-03T00:04:03.256578: step 5023, loss 0.00382437, acc 1, learning_rate 0.0001
2017-10-03T00:04:04.411428: step 5024, loss 0.00345964, acc 1, learning_rate 0.0001
2017-10-03T00:04:05.565321: step 5025, loss 0.0037983, acc 1, learning_rate 0.0001
2017-10-03T00:04:06.729421: step 5026, loss 0.0199051, acc 0.984375, learning_rate 0.0001
2017-10-03T00:04:07.887340: step 5027, loss 0.00406235, acc 1, learning_rate 0.0001
2017-10-03T00:04:09.025884: step 5028, loss 0.00861956, acc 1, learning_rate 0.0001
2017-10-03T00:04:10.174743: step 5029, loss 0.0144311, acc 1, learning_rate 0.0001
2017-10-03T00:04:11.325615: step 5030, loss 0.0225886, acc 0.984375, learning_rate 0.0001
2017-10-03T00:04:12.482511: step 5031, loss 0.0208727, acc 1, learning_rate 0.0001
2017-10-03T00:04:13.639130: step 5032, loss 0.00232176, acc 1, learning_rate 0.0001
2017-10-03T00:04:14.798471: step 5033, loss 0.0084901, acc 1, learning_rate 0.0001
2017-10-03T00:04:15.949474: step 5034, loss 0.00451919, acc 1, learning_rate 0.0001
2017-10-03T00:04:17.090675: step 5035, loss 0.0060047, acc 1, learning_rate 0.0001
2017-10-03T00:04:18.230813: step 5036, loss 0.00487877, acc 1, learning_rate 0.0001
2017-10-03T00:04:19.384357: step 5037, loss 0.00497051, acc 1, learning_rate 0.0001
2017-10-03T00:04:20.550536: step 5038, loss 0.02729, acc 0.984375, learning_rate 0.0001
2017-10-03T00:04:21.711241: step 5039, loss 0.004505, acc 1, learning_rate 0.0001
2017-10-03T00:04:22.866695: step 5040, loss 0.00419104, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:04:23.206670: step 5040, loss 1.46108, acc 0.463309

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5040

2017-10-03T00:04:31.160371: step 5041, loss 0.0189223, acc 0.984375, learning_rate 0.0001
2017-10-03T00:04:32.328853: step 5042, loss 0.00575355, acc 1, learning_rate 0.0001
2017-10-03T00:04:33.478069: step 5043, loss 0.00485997, acc 1, learning_rate 0.0001
2017-10-03T00:04:34.616050: step 5044, loss 0.00447873, acc 1, learning_rate 0.0001
2017-10-03T00:04:35.770526: step 5045, loss 0.0440775, acc 0.96875, learning_rate 0.0001
2017-10-03T00:04:36.920485: step 5046, loss 0.00482388, acc 1, learning_rate 0.0001
2017-10-03T00:04:38.084320: step 5047, loss 0.00440607, acc 1, learning_rate 0.0001
2017-10-03T00:04:39.243552: step 5048, loss 0.0148799, acc 1, learning_rate 0.0001
2017-10-03T00:04:40.401792: step 5049, loss 0.0138296, acc 1, learning_rate 0.0001
2017-10-03T00:04:41.565740: step 5050, loss 0.0136595, acc 1, learning_rate 0.0001
2017-10-03T00:04:42.730888: step 5051, loss 0.00350535, acc 1, learning_rate 0.0001
2017-10-03T00:04:43.877261: step 5052, loss 0.0201911, acc 0.984375, learning_rate 0.0001
2017-10-03T00:04:45.045882: step 5053, loss 0.00693829, acc 1, learning_rate 0.0001
2017-10-03T00:04:46.193662: step 5054, loss 0.0217727, acc 0.984375, learning_rate 0.0001
2017-10-03T00:04:47.349142: step 5055, loss 0.00298043, acc 1, learning_rate 0.0001
2017-10-03T00:04:48.509529: step 5056, loss 0.00451269, acc 1, learning_rate 0.0001
2017-10-03T00:04:49.657899: step 5057, loss 0.0413229, acc 0.984375, learning_rate 0.0001
2017-10-03T00:04:50.815029: step 5058, loss 0.00396138, acc 1, learning_rate 0.0001
2017-10-03T00:04:51.958082: step 5059, loss 0.0198842, acc 1, learning_rate 0.0001
2017-10-03T00:04:53.152475: step 5060, loss 0.00379835, acc 1, learning_rate 0.0001
2017-10-03T00:04:54.308625: step 5061, loss 0.00450626, acc 1, learning_rate 0.0001
2017-10-03T00:04:55.469081: step 5062, loss 0.0161098, acc 1, learning_rate 0.0001
2017-10-03T00:04:56.637411: step 5063, loss 0.00354947, acc 1, learning_rate 0.0001
2017-10-03T00:04:57.842352: step 5064, loss 0.00322326, acc 1, learning_rate 0.0001
2017-10-03T00:04:58.996424: step 5065, loss 0.00311753, acc 1, learning_rate 0.0001
2017-10-03T00:05:00.150523: step 5066, loss 0.00396119, acc 1, learning_rate 0.0001
2017-10-03T00:05:01.294731: step 5067, loss 0.0173888, acc 1, learning_rate 0.0001
2017-10-03T00:05:02.440588: step 5068, loss 0.0137412, acc 1, learning_rate 0.0001
2017-10-03T00:05:03.605915: step 5069, loss 0.00342105, acc 1, learning_rate 0.0001
2017-10-03T00:05:04.769396: step 5070, loss 0.00472818, acc 1, learning_rate 0.0001
2017-10-03T00:05:05.954381: step 5071, loss 0.00441483, acc 1, learning_rate 0.0001
2017-10-03T00:05:07.114284: step 5072, loss 0.0188072, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:08.264116: step 5073, loss 0.00324437, acc 1, learning_rate 0.0001
2017-10-03T00:05:09.434983: step 5074, loss 0.0130181, acc 1, learning_rate 0.0001
2017-10-03T00:05:10.588100: step 5075, loss 0.00333755, acc 1, learning_rate 0.0001
2017-10-03T00:05:11.735159: step 5076, loss 0.0112366, acc 1, learning_rate 0.0001
2017-10-03T00:05:12.887952: step 5077, loss 0.0190261, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:14.039327: step 5078, loss 0.0236425, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:15.186096: step 5079, loss 0.011181, acc 1, learning_rate 0.0001
2017-10-03T00:05:16.341436: step 5080, loss 0.00363739, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:05:16.690182: step 5080, loss 1.44093, acc 0.482014

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5080

2017-10-03T00:05:24.449855: step 5081, loss 0.00467385, acc 1, learning_rate 0.0001
2017-10-03T00:05:25.599871: step 5082, loss 0.00586552, acc 1, learning_rate 0.0001
2017-10-03T00:05:26.756288: step 5083, loss 0.00269061, acc 1, learning_rate 0.0001
2017-10-03T00:05:27.919927: step 5084, loss 0.00707936, acc 1, learning_rate 0.0001
2017-10-03T00:05:29.084024: step 5085, loss 0.00662284, acc 1, learning_rate 0.0001
2017-10-03T00:05:30.234949: step 5086, loss 0.0363934, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:31.384988: step 5087, loss 0.00573192, acc 1, learning_rate 0.0001
2017-10-03T00:05:32.548523: step 5088, loss 0.0430606, acc 0.96875, learning_rate 0.0001
2017-10-03T00:05:33.682457: step 5089, loss 0.0274138, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:34.829882: step 5090, loss 0.0259797, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:35.974533: step 5091, loss 0.032925, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:37.123139: step 5092, loss 0.00417298, acc 1, learning_rate 0.0001
2017-10-03T00:05:38.271742: step 5093, loss 0.00647968, acc 1, learning_rate 0.0001
2017-10-03T00:05:39.429754: step 5094, loss 0.00412086, acc 1, learning_rate 0.0001
2017-10-03T00:05:40.601716: step 5095, loss 0.00670465, acc 1, learning_rate 0.0001
2017-10-03T00:05:41.753532: step 5096, loss 0.0592134, acc 0.980392, learning_rate 0.0001
2017-10-03T00:05:42.916549: step 5097, loss 0.0112563, acc 1, learning_rate 0.0001
2017-10-03T00:05:44.060094: step 5098, loss 0.00871537, acc 1, learning_rate 0.0001
2017-10-03T00:05:45.194454: step 5099, loss 0.0412646, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:46.339367: step 5100, loss 0.0033783, acc 1, learning_rate 0.0001
2017-10-03T00:05:47.484562: step 5101, loss 0.0040802, acc 1, learning_rate 0.0001
2017-10-03T00:05:48.624062: step 5102, loss 0.00915393, acc 1, learning_rate 0.0001
2017-10-03T00:05:49.769904: step 5103, loss 0.0141199, acc 1, learning_rate 0.0001
2017-10-03T00:05:50.930273: step 5104, loss 0.0217273, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:52.079929: step 5105, loss 0.0111039, acc 1, learning_rate 0.0001
2017-10-03T00:05:53.246859: step 5106, loss 0.00289576, acc 1, learning_rate 0.0001
2017-10-03T00:05:54.409095: step 5107, loss 0.00529019, acc 1, learning_rate 0.0001
2017-10-03T00:05:55.562274: step 5108, loss 0.0196614, acc 0.984375, learning_rate 0.0001
2017-10-03T00:05:56.713209: step 5109, loss 0.00590138, acc 1, learning_rate 0.0001
2017-10-03T00:05:57.859929: step 5110, loss 0.0517691, acc 0.953125, learning_rate 0.0001
2017-10-03T00:05:59.019605: step 5111, loss 0.0343994, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:00.165180: step 5112, loss 0.00382562, acc 1, learning_rate 0.0001
2017-10-03T00:06:01.312932: step 5113, loss 0.00549187, acc 1, learning_rate 0.0001
2017-10-03T00:06:02.526414: step 5114, loss 0.00836231, acc 1, learning_rate 0.0001
2017-10-03T00:06:03.753917: step 5115, loss 0.0300943, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:04.922095: step 5116, loss 0.0274844, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:06.102258: step 5117, loss 0.0160081, acc 1, learning_rate 0.0001
2017-10-03T00:06:07.246961: step 5118, loss 0.00505189, acc 1, learning_rate 0.0001
2017-10-03T00:06:08.389589: step 5119, loss 0.00714509, acc 1, learning_rate 0.0001
2017-10-03T00:06:09.541026: step 5120, loss 0.00395248, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:06:09.881440: step 5120, loss 1.48723, acc 0.394245

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5120

2017-10-03T00:06:17.583877: step 5121, loss 0.00490141, acc 1, learning_rate 0.0001
2017-10-03T00:06:18.769950: step 5122, loss 0.0109729, acc 1, learning_rate 0.0001
2017-10-03T00:06:19.923456: step 5123, loss 0.0427179, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:21.079955: step 5124, loss 0.00995587, acc 1, learning_rate 0.0001
2017-10-03T00:06:22.232017: step 5125, loss 0.00724225, acc 1, learning_rate 0.0001
2017-10-03T00:06:23.383412: step 5126, loss 0.0061453, acc 1, learning_rate 0.0001
2017-10-03T00:06:24.537380: step 5127, loss 0.00755285, acc 1, learning_rate 0.0001
2017-10-03T00:06:25.695271: step 5128, loss 0.0613576, acc 0.96875, learning_rate 0.0001
2017-10-03T00:06:26.836187: step 5129, loss 0.0036141, acc 1, learning_rate 0.0001
2017-10-03T00:06:27.976701: step 5130, loss 0.0035965, acc 1, learning_rate 0.0001
2017-10-03T00:06:29.120764: step 5131, loss 0.0143337, acc 1, learning_rate 0.0001
2017-10-03T00:06:30.284296: step 5132, loss 0.00557686, acc 1, learning_rate 0.0001
2017-10-03T00:06:31.442430: step 5133, loss 0.00464, acc 1, learning_rate 0.0001
2017-10-03T00:06:32.588989: step 5134, loss 0.00769369, acc 1, learning_rate 0.0001
2017-10-03T00:06:33.738191: step 5135, loss 0.0361031, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:34.876296: step 5136, loss 0.0739957, acc 0.96875, learning_rate 0.0001
2017-10-03T00:06:36.046238: step 5137, loss 0.00442779, acc 1, learning_rate 0.0001
2017-10-03T00:06:37.187582: step 5138, loss 0.00726905, acc 1, learning_rate 0.0001
2017-10-03T00:06:38.342496: step 5139, loss 0.0245576, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:39.488444: step 5140, loss 0.00518345, acc 1, learning_rate 0.0001
2017-10-03T00:06:40.634594: step 5141, loss 0.018681, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:41.794470: step 5142, loss 0.0497541, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:42.945757: step 5143, loss 0.0372475, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:44.117072: step 5144, loss 0.00345325, acc 1, learning_rate 0.0001
2017-10-03T00:06:45.268549: step 5145, loss 0.0244101, acc 0.984375, learning_rate 0.0001
2017-10-03T00:06:46.416247: step 5146, loss 0.00478854, acc 1, learning_rate 0.0001
2017-10-03T00:06:47.553821: step 5147, loss 0.0188761, acc 1, learning_rate 0.0001
2017-10-03T00:06:48.705927: step 5148, loss 0.00568371, acc 1, learning_rate 0.0001
2017-10-03T00:06:49.866672: step 5149, loss 0.00725932, acc 1, learning_rate 0.0001
2017-10-03T00:06:51.023438: step 5150, loss 0.00625997, acc 1, learning_rate 0.0001
2017-10-03T00:06:52.170522: step 5151, loss 0.00422876, acc 1, learning_rate 0.0001
2017-10-03T00:06:53.321624: step 5152, loss 0.00592992, acc 1, learning_rate 0.0001
2017-10-03T00:06:54.499377: step 5153, loss 0.0366076, acc 1, learning_rate 0.0001
2017-10-03T00:06:55.664656: step 5154, loss 0.00412505, acc 1, learning_rate 0.0001
2017-10-03T00:06:56.819845: step 5155, loss 0.00625551, acc 1, learning_rate 0.0001
2017-10-03T00:06:57.961463: step 5156, loss 0.0451752, acc 0.96875, learning_rate 0.0001
2017-10-03T00:06:59.102304: step 5157, loss 0.00933253, acc 1, learning_rate 0.0001
2017-10-03T00:07:00.250060: step 5158, loss 0.00321143, acc 1, learning_rate 0.0001
2017-10-03T00:07:01.410668: step 5159, loss 0.00505773, acc 1, learning_rate 0.0001
2017-10-03T00:07:02.566167: step 5160, loss 0.00507773, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:07:02.908235: step 5160, loss 1.467, acc 0.45036

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5160

2017-10-03T00:07:10.433865: step 5161, loss 0.00448417, acc 1, learning_rate 0.0001
2017-10-03T00:07:11.586702: step 5162, loss 0.00398328, acc 1, learning_rate 0.0001
2017-10-03T00:07:12.726757: step 5163, loss 0.00345843, acc 1, learning_rate 0.0001
2017-10-03T00:07:13.872898: step 5164, loss 0.0078186, acc 1, learning_rate 0.0001
2017-10-03T00:07:15.034378: step 5165, loss 0.00391019, acc 1, learning_rate 0.0001
2017-10-03T00:07:16.178393: step 5166, loss 0.00382864, acc 1, learning_rate 0.0001
2017-10-03T00:07:17.324051: step 5167, loss 0.00197883, acc 1, learning_rate 0.0001
2017-10-03T00:07:18.480731: step 5168, loss 0.00428833, acc 1, learning_rate 0.0001
2017-10-03T00:07:19.623400: step 5169, loss 0.00669328, acc 1, learning_rate 0.0001
2017-10-03T00:07:20.774149: step 5170, loss 0.0468131, acc 0.984375, learning_rate 0.0001
2017-10-03T00:07:21.918041: step 5171, loss 0.00614528, acc 1, learning_rate 0.0001
2017-10-03T00:07:23.056638: step 5172, loss 0.00275531, acc 1, learning_rate 0.0001
2017-10-03T00:07:24.213147: step 5173, loss 0.00587581, acc 1, learning_rate 0.0001
2017-10-03T00:07:25.378116: step 5174, loss 0.00405041, acc 1, learning_rate 0.0001
2017-10-03T00:07:26.525975: step 5175, loss 0.029578, acc 0.984375, learning_rate 0.0001
2017-10-03T00:07:27.680674: step 5176, loss 0.00368218, acc 1, learning_rate 0.0001
2017-10-03T00:07:28.823232: step 5177, loss 0.0102128, acc 1, learning_rate 0.0001
2017-10-03T00:07:29.973734: step 5178, loss 0.00402524, acc 1, learning_rate 0.0001
2017-10-03T00:07:31.117471: step 5179, loss 0.0107859, acc 1, learning_rate 0.0001
2017-10-03T00:07:32.290820: step 5180, loss 0.00300436, acc 1, learning_rate 0.0001
2017-10-03T00:07:33.453465: step 5181, loss 0.00841378, acc 1, learning_rate 0.0001
2017-10-03T00:07:34.601792: step 5182, loss 0.0146375, acc 1, learning_rate 0.0001
2017-10-03T00:07:35.758590: step 5183, loss 0.0413235, acc 0.984375, learning_rate 0.0001
2017-10-03T00:07:36.907819: step 5184, loss 0.00640716, acc 1, learning_rate 0.0001
2017-10-03T00:07:38.063696: step 5185, loss 0.00638315, acc 1, learning_rate 0.0001
2017-10-03T00:07:39.223964: step 5186, loss 0.00842813, acc 1, learning_rate 0.0001
2017-10-03T00:07:40.377602: step 5187, loss 0.00612257, acc 1, learning_rate 0.0001
2017-10-03T00:07:41.530183: step 5188, loss 0.0467979, acc 1, learning_rate 0.0001
2017-10-03T00:07:42.776616: step 5189, loss 0.00258335, acc 1, learning_rate 0.0001
2017-10-03T00:07:43.948363: step 5190, loss 0.010451, acc 1, learning_rate 0.0001
2017-10-03T00:07:45.088656: step 5191, loss 0.00907518, acc 1, learning_rate 0.0001
2017-10-03T00:07:46.229564: step 5192, loss 0.0188446, acc 0.984375, learning_rate 0.0001
2017-10-03T00:07:47.374445: step 5193, loss 0.00385625, acc 1, learning_rate 0.0001
2017-10-03T00:07:48.499599: step 5194, loss 0.0541815, acc 0.960784, learning_rate 0.0001
2017-10-03T00:07:49.673130: step 5195, loss 0.00529429, acc 1, learning_rate 0.0001
2017-10-03T00:07:50.834156: step 5196, loss 0.00315335, acc 1, learning_rate 0.0001
2017-10-03T00:07:51.978527: step 5197, loss 0.00573575, acc 1, learning_rate 0.0001
2017-10-03T00:07:53.122137: step 5198, loss 0.0593839, acc 0.984375, learning_rate 0.0001
2017-10-03T00:07:54.301526: step 5199, loss 0.01094, acc 1, learning_rate 0.0001
2017-10-03T00:07:55.453547: step 5200, loss 0.0240037, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:07:55.813115: step 5200, loss 1.47539, acc 0.425899

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5200

2017-10-03T00:08:02.737273: step 5201, loss 0.00412655, acc 1, learning_rate 0.0001
2017-10-03T00:08:03.900777: step 5202, loss 0.0532717, acc 0.96875, learning_rate 0.0001
2017-10-03T00:08:05.037141: step 5203, loss 0.0162934, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:06.192823: step 5204, loss 0.00260884, acc 1, learning_rate 0.0001
2017-10-03T00:08:07.332963: step 5205, loss 0.0138737, acc 1, learning_rate 0.0001
2017-10-03T00:08:08.489504: step 5206, loss 0.0324179, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:09.659772: step 5207, loss 0.019234, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:10.804081: step 5208, loss 0.00486375, acc 1, learning_rate 0.0001
2017-10-03T00:08:11.953644: step 5209, loss 0.00620246, acc 1, learning_rate 0.0001
2017-10-03T00:08:13.105571: step 5210, loss 0.00294398, acc 1, learning_rate 0.0001
2017-10-03T00:08:14.259526: step 5211, loss 0.00348007, acc 1, learning_rate 0.0001
2017-10-03T00:08:15.399519: step 5212, loss 0.0209846, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:16.553165: step 5213, loss 0.00497536, acc 1, learning_rate 0.0001
2017-10-03T00:08:17.699627: step 5214, loss 0.00440157, acc 1, learning_rate 0.0001
2017-10-03T00:08:18.865843: step 5215, loss 0.00719256, acc 1, learning_rate 0.0001
2017-10-03T00:08:20.003730: step 5216, loss 0.0258382, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:21.171002: step 5217, loss 0.00394169, acc 1, learning_rate 0.0001
2017-10-03T00:08:22.322387: step 5218, loss 0.042742, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:23.473540: step 5219, loss 0.0191233, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:24.638971: step 5220, loss 0.0148375, acc 1, learning_rate 0.0001
2017-10-03T00:08:25.791841: step 5221, loss 0.00342982, acc 1, learning_rate 0.0001
2017-10-03T00:08:26.934418: step 5222, loss 0.0192552, acc 1, learning_rate 0.0001
2017-10-03T00:08:28.086258: step 5223, loss 0.0460432, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:29.242695: step 5224, loss 0.00645325, acc 1, learning_rate 0.0001
2017-10-03T00:08:30.391872: step 5225, loss 0.0066386, acc 1, learning_rate 0.0001
2017-10-03T00:08:31.539836: step 5226, loss 0.00308137, acc 1, learning_rate 0.0001
2017-10-03T00:08:32.691636: step 5227, loss 0.0185281, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:33.849579: step 5228, loss 0.00475285, acc 1, learning_rate 0.0001
2017-10-03T00:08:34.998566: step 5229, loss 0.00368192, acc 1, learning_rate 0.0001
2017-10-03T00:08:36.144158: step 5230, loss 0.00387517, acc 1, learning_rate 0.0001
2017-10-03T00:08:37.295658: step 5231, loss 0.00406225, acc 1, learning_rate 0.0001
2017-10-03T00:08:38.493052: step 5232, loss 0.00721571, acc 1, learning_rate 0.0001
2017-10-03T00:08:39.638289: step 5233, loss 0.0240193, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:40.816484: step 5234, loss 0.0254604, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:41.972481: step 5235, loss 0.0116565, acc 1, learning_rate 0.0001
2017-10-03T00:08:43.123962: step 5236, loss 0.0145239, acc 1, learning_rate 0.0001
2017-10-03T00:08:44.274377: step 5237, loss 0.0257555, acc 0.984375, learning_rate 0.0001
2017-10-03T00:08:45.422559: step 5238, loss 0.00332044, acc 1, learning_rate 0.0001
2017-10-03T00:08:46.681827: step 5239, loss 0.0583265, acc 0.96875, learning_rate 0.0001
2017-10-03T00:08:47.818277: step 5240, loss 0.00378982, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:08:48.160439: step 5240, loss 1.45831, acc 0.467626

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5240

2017-10-03T00:08:56.037074: step 5241, loss 0.0115606, acc 1, learning_rate 0.0001
2017-10-03T00:08:57.215761: step 5242, loss 0.00469075, acc 1, learning_rate 0.0001
2017-10-03T00:08:58.372904: step 5243, loss 0.00836812, acc 1, learning_rate 0.0001
2017-10-03T00:08:59.550059: step 5244, loss 0.0344383, acc 0.984375, learning_rate 0.0001
2017-10-03T00:09:00.720975: step 5245, loss 0.00321591, acc 1, learning_rate 0.0001
2017-10-03T00:09:01.860417: step 5246, loss 0.00417414, acc 1, learning_rate 0.0001
2017-10-03T00:09:03.005479: step 5247, loss 0.00317374, acc 1, learning_rate 0.0001
2017-10-03T00:09:04.161584: step 5248, loss 0.0134227, acc 1, learning_rate 0.0001
2017-10-03T00:09:05.321392: step 5249, loss 0.00292974, acc 1, learning_rate 0.0001
2017-10-03T00:09:06.470594: step 5250, loss 0.00244507, acc 1, learning_rate 0.0001
2017-10-03T00:09:07.616955: step 5251, loss 0.00585071, acc 1, learning_rate 0.0001
2017-10-03T00:09:08.750261: step 5252, loss 0.00428938, acc 1, learning_rate 0.0001
2017-10-03T00:09:09.926323: step 5253, loss 0.0150214, acc 1, learning_rate 0.0001
2017-10-03T00:09:11.075148: step 5254, loss 0.0349283, acc 0.984375, learning_rate 0.0001
2017-10-03T00:09:12.228311: step 5255, loss 0.0612183, acc 0.96875, learning_rate 0.0001
2017-10-03T00:09:13.376131: step 5256, loss 0.00540295, acc 1, learning_rate 0.0001
2017-10-03T00:09:14.527905: step 5257, loss 0.00568824, acc 1, learning_rate 0.0001
2017-10-03T00:09:15.700394: step 5258, loss 0.0339476, acc 0.984375, learning_rate 0.0001
2017-10-03T00:09:16.858427: step 5259, loss 0.00288005, acc 1, learning_rate 0.0001
2017-10-03T00:09:18.009870: step 5260, loss 0.00363199, acc 1, learning_rate 0.0001
2017-10-03T00:09:19.168595: step 5261, loss 0.00740563, acc 1, learning_rate 0.0001
2017-10-03T00:09:20.323303: step 5262, loss 0.00611526, acc 1, learning_rate 0.0001
2017-10-03T00:09:21.474696: step 5263, loss 0.00358013, acc 1, learning_rate 0.0001
2017-10-03T00:09:22.768971: step 5264, loss 0.0318876, acc 0.984375, learning_rate 0.0001
2017-10-03T00:09:23.913318: step 5265, loss 0.018671, acc 1, learning_rate 0.0001
2017-10-03T00:09:25.067176: step 5266, loss 0.00413155, acc 1, learning_rate 0.0001
2017-10-03T00:09:26.223541: step 5267, loss 0.00327651, acc 1, learning_rate 0.0001
2017-10-03T00:09:27.382073: step 5268, loss 0.0458356, acc 0.984375, learning_rate 0.0001
2017-10-03T00:09:28.535738: step 5269, loss 0.037808, acc 0.984375, learning_rate 0.0001
2017-10-03T00:09:29.683561: step 5270, loss 0.0111634, acc 1, learning_rate 0.0001
2017-10-03T00:09:30.841924: step 5271, loss 0.00849922, acc 1, learning_rate 0.0001
2017-10-03T00:09:31.985244: step 5272, loss 0.0129533, acc 1, learning_rate 0.0001
2017-10-03T00:09:33.145555: step 5273, loss 0.0185343, acc 0.984375, learning_rate 0.0001
2017-10-03T00:09:34.313565: step 5274, loss 0.00508111, acc 1, learning_rate 0.0001
2017-10-03T00:09:35.464923: step 5275, loss 0.00617588, acc 1, learning_rate 0.0001
2017-10-03T00:09:36.628259: step 5276, loss 0.0116919, acc 1, learning_rate 0.0001
2017-10-03T00:09:37.811728: step 5277, loss 0.029601, acc 0.984375, learning_rate 0.0001
2017-10-03T00:09:38.962393: step 5278, loss 0.00743657, acc 1, learning_rate 0.0001
2017-10-03T00:09:40.118925: step 5279, loss 0.00618453, acc 1, learning_rate 0.0001
2017-10-03T00:09:41.256743: step 5280, loss 0.00389934, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:09:41.589149: step 5280, loss 1.42345, acc 0.493525

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5280

2017-10-03T00:09:49.275962: step 5281, loss 0.0243141, acc 1, learning_rate 0.0001
2017-10-03T00:09:50.433168: step 5282, loss 0.00531093, acc 1, learning_rate 0.0001
2017-10-03T00:09:51.570954: step 5283, loss 0.00949534, acc 1, learning_rate 0.0001
2017-10-03T00:09:52.711626: step 5284, loss 0.00309046, acc 1, learning_rate 0.0001
2017-10-03T00:09:53.863012: step 5285, loss 0.00389346, acc 1, learning_rate 0.0001
2017-10-03T00:09:55.016148: step 5286, loss 0.00860443, acc 1, learning_rate 0.0001
2017-10-03T00:09:56.182108: step 5287, loss 0.0138849, acc 1, learning_rate 0.0001
2017-10-03T00:09:57.338156: step 5288, loss 0.0105927, acc 1, learning_rate 0.0001
2017-10-03T00:09:58.482951: step 5289, loss 0.0183627, acc 1, learning_rate 0.0001
2017-10-03T00:09:59.633747: step 5290, loss 0.00453569, acc 1, learning_rate 0.0001
2017-10-03T00:10:00.770221: step 5291, loss 0.00401922, acc 1, learning_rate 0.0001
2017-10-03T00:10:01.903105: step 5292, loss 0.00849692, acc 1, learning_rate 0.0001
2017-10-03T00:10:03.046198: step 5293, loss 0.0067378, acc 1, learning_rate 0.0001
2017-10-03T00:10:04.197507: step 5294, loss 0.0231482, acc 1, learning_rate 0.0001
2017-10-03T00:10:05.352037: step 5295, loss 0.00293661, acc 1, learning_rate 0.0001
2017-10-03T00:10:06.496422: step 5296, loss 0.00637224, acc 1, learning_rate 0.0001
2017-10-03T00:10:07.642793: step 5297, loss 0.00576179, acc 1, learning_rate 0.0001
2017-10-03T00:10:08.787372: step 5298, loss 0.0170524, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:09.939626: step 5299, loss 0.0203447, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:11.091795: step 5300, loss 0.00824838, acc 1, learning_rate 0.0001
2017-10-03T00:10:12.238005: step 5301, loss 0.0400554, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:13.387044: step 5302, loss 0.0148682, acc 1, learning_rate 0.0001
2017-10-03T00:10:14.536587: step 5303, loss 0.00318612, acc 1, learning_rate 0.0001
2017-10-03T00:10:15.689394: step 5304, loss 0.0400177, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:16.842216: step 5305, loss 0.0258596, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:17.992878: step 5306, loss 0.0563428, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:19.146814: step 5307, loss 0.00560813, acc 1, learning_rate 0.0001
2017-10-03T00:10:20.295905: step 5308, loss 0.018144, acc 1, learning_rate 0.0001
2017-10-03T00:10:21.455728: step 5309, loss 0.00580457, acc 1, learning_rate 0.0001
2017-10-03T00:10:22.609067: step 5310, loss 0.00927222, acc 1, learning_rate 0.0001
2017-10-03T00:10:23.750449: step 5311, loss 0.00762544, acc 1, learning_rate 0.0001
2017-10-03T00:10:24.886699: step 5312, loss 0.00472302, acc 1, learning_rate 0.0001
2017-10-03T00:10:26.044910: step 5313, loss 0.00312032, acc 1, learning_rate 0.0001
2017-10-03T00:10:27.187197: step 5314, loss 0.0248269, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:28.347084: step 5315, loss 0.046651, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:29.511522: step 5316, loss 0.00617009, acc 1, learning_rate 0.0001
2017-10-03T00:10:30.657270: step 5317, loss 0.00950215, acc 1, learning_rate 0.0001
2017-10-03T00:10:31.795354: step 5318, loss 0.00314367, acc 1, learning_rate 0.0001
2017-10-03T00:10:32.947049: step 5319, loss 0.00797347, acc 1, learning_rate 0.0001
2017-10-03T00:10:34.103490: step 5320, loss 0.0038013, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:10:34.434399: step 5320, loss 1.45075, acc 0.466187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5320

2017-10-03T00:10:42.576928: step 5321, loss 0.0346272, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:43.745120: step 5322, loss 0.0139033, acc 1, learning_rate 0.0001
2017-10-03T00:10:44.890386: step 5323, loss 0.0114925, acc 1, learning_rate 0.0001
2017-10-03T00:10:46.115449: step 5324, loss 0.00367551, acc 1, learning_rate 0.0001
2017-10-03T00:10:47.274479: step 5325, loss 0.0340438, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:48.443837: step 5326, loss 0.0248542, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:49.600361: step 5327, loss 0.00429383, acc 1, learning_rate 0.0001
2017-10-03T00:10:50.752370: step 5328, loss 0.0177459, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:52.120487: step 5329, loss 0.00300834, acc 1, learning_rate 0.0001
2017-10-03T00:10:53.276423: step 5330, loss 0.031976, acc 0.984375, learning_rate 0.0001
2017-10-03T00:10:54.447214: step 5331, loss 0.00548479, acc 1, learning_rate 0.0001
2017-10-03T00:10:55.596789: step 5332, loss 0.015309, acc 1, learning_rate 0.0001
2017-10-03T00:10:56.760843: step 5333, loss 0.00471369, acc 1, learning_rate 0.0001
2017-10-03T00:10:57.907211: step 5334, loss 0.00478986, acc 1, learning_rate 0.0001
2017-10-03T00:10:59.063692: step 5335, loss 0.0330386, acc 0.984375, learning_rate 0.0001
2017-10-03T00:11:00.214943: step 5336, loss 0.00509962, acc 1, learning_rate 0.0001
2017-10-03T00:11:01.389026: step 5337, loss 0.0392544, acc 0.984375, learning_rate 0.0001
2017-10-03T00:11:02.534071: step 5338, loss 0.0042943, acc 1, learning_rate 0.0001
2017-10-03T00:11:03.684591: step 5339, loss 0.0195199, acc 1, learning_rate 0.0001
2017-10-03T00:11:04.825298: step 5340, loss 0.0027626, acc 1, learning_rate 0.0001
2017-10-03T00:11:05.973114: step 5341, loss 0.00553426, acc 1, learning_rate 0.0001
2017-10-03T00:11:07.175906: step 5342, loss 0.00286546, acc 1, learning_rate 0.0001
2017-10-03T00:11:08.329744: step 5343, loss 0.0072808, acc 1, learning_rate 0.0001
2017-10-03T00:11:09.474577: step 5344, loss 0.00362701, acc 1, learning_rate 0.0001
2017-10-03T00:11:10.636945: step 5345, loss 0.0355712, acc 0.984375, learning_rate 0.0001
2017-10-03T00:11:11.798912: step 5346, loss 0.00436261, acc 1, learning_rate 0.0001
2017-10-03T00:11:12.951554: step 5347, loss 0.0047535, acc 1, learning_rate 0.0001
2017-10-03T00:11:14.085776: step 5348, loss 0.00766884, acc 1, learning_rate 0.0001
2017-10-03T00:11:15.218424: step 5349, loss 0.0206568, acc 0.984375, learning_rate 0.0001
2017-10-03T00:11:16.374179: step 5350, loss 0.00496253, acc 1, learning_rate 0.0001
2017-10-03T00:11:17.523443: step 5351, loss 0.00281374, acc 1, learning_rate 0.0001
2017-10-03T00:11:18.656219: step 5352, loss 0.00918236, acc 1, learning_rate 0.0001
2017-10-03T00:11:19.805064: step 5353, loss 0.00439629, acc 1, learning_rate 0.0001
2017-10-03T00:11:20.960768: step 5354, loss 0.00405717, acc 1, learning_rate 0.0001
2017-10-03T00:11:22.101472: step 5355, loss 0.00444557, acc 1, learning_rate 0.0001
2017-10-03T00:11:23.250923: step 5356, loss 0.00696143, acc 1, learning_rate 0.0001
2017-10-03T00:11:24.403043: step 5357, loss 0.00348846, acc 1, learning_rate 0.0001
2017-10-03T00:11:25.559861: step 5358, loss 0.00202595, acc 1, learning_rate 0.0001
2017-10-03T00:11:26.727103: step 5359, loss 0.00384701, acc 1, learning_rate 0.0001
2017-10-03T00:11:27.889259: step 5360, loss 0.00720961, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:11:28.234371: step 5360, loss 1.49393, acc 0.407194

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5360

2017-10-03T00:11:36.508295: step 5361, loss 0.0355948, acc 0.96875, learning_rate 0.0001
2017-10-03T00:11:37.698833: step 5362, loss 0.0037021, acc 1, learning_rate 0.0001
2017-10-03T00:11:38.852004: step 5363, loss 0.00445929, acc 1, learning_rate 0.0001
2017-10-03T00:11:39.992835: step 5364, loss 0.00215749, acc 1, learning_rate 0.0001
2017-10-03T00:11:41.141759: step 5365, loss 0.00765273, acc 1, learning_rate 0.0001
2017-10-03T00:11:42.297118: step 5366, loss 0.0127577, acc 1, learning_rate 0.0001
2017-10-03T00:11:43.450333: step 5367, loss 0.00334811, acc 1, learning_rate 0.0001
2017-10-03T00:11:44.594708: step 5368, loss 0.0318012, acc 0.984375, learning_rate 0.0001
2017-10-03T00:11:45.763084: step 5369, loss 0.0150872, acc 1, learning_rate 0.0001
2017-10-03T00:11:46.917504: step 5370, loss 0.0401663, acc 0.984375, learning_rate 0.0001
2017-10-03T00:11:48.075560: step 5371, loss 0.00627974, acc 1, learning_rate 0.0001
2017-10-03T00:11:49.229695: step 5372, loss 0.00423915, acc 1, learning_rate 0.0001
2017-10-03T00:11:50.377159: step 5373, loss 0.00418282, acc 1, learning_rate 0.0001
2017-10-03T00:11:51.529131: step 5374, loss 0.00718371, acc 1, learning_rate 0.0001
2017-10-03T00:11:52.675306: step 5375, loss 0.00291232, acc 1, learning_rate 0.0001
2017-10-03T00:11:53.837851: step 5376, loss 0.00646254, acc 1, learning_rate 0.0001
2017-10-03T00:11:54.976490: step 5377, loss 0.0305454, acc 0.984375, learning_rate 0.0001
2017-10-03T00:11:56.126729: step 5378, loss 0.0198841, acc 1, learning_rate 0.0001
2017-10-03T00:11:57.282675: step 5379, loss 0.0394114, acc 0.984375, learning_rate 0.0001
2017-10-03T00:11:58.429753: step 5380, loss 0.00588833, acc 1, learning_rate 0.0001
2017-10-03T00:11:59.581471: step 5381, loss 0.00297234, acc 1, learning_rate 0.0001
2017-10-03T00:12:00.720633: step 5382, loss 0.0693171, acc 0.96875, learning_rate 0.0001
2017-10-03T00:12:01.876274: step 5383, loss 0.00974243, acc 1, learning_rate 0.0001
2017-10-03T00:12:03.017118: step 5384, loss 0.0531051, acc 0.96875, learning_rate 0.0001
2017-10-03T00:12:04.182684: step 5385, loss 0.0234287, acc 0.984375, learning_rate 0.0001
2017-10-03T00:12:05.350785: step 5386, loss 0.00399706, acc 1, learning_rate 0.0001
2017-10-03T00:12:06.596555: step 5387, loss 0.00510528, acc 1, learning_rate 0.0001
2017-10-03T00:12:07.737582: step 5388, loss 0.00485468, acc 1, learning_rate 0.0001
2017-10-03T00:12:08.879154: step 5389, loss 0.0233243, acc 1, learning_rate 0.0001
2017-10-03T00:12:10.021790: step 5390, loss 0.0034498, acc 1, learning_rate 0.0001
2017-10-03T00:12:11.164331: step 5391, loss 0.0220121, acc 1, learning_rate 0.0001
2017-10-03T00:12:12.318716: step 5392, loss 0.00576968, acc 1, learning_rate 0.0001
2017-10-03T00:12:13.464843: step 5393, loss 0.00663468, acc 1, learning_rate 0.0001
2017-10-03T00:12:14.617811: step 5394, loss 0.0106685, acc 1, learning_rate 0.0001
2017-10-03T00:12:15.780480: step 5395, loss 0.0077327, acc 1, learning_rate 0.0001
2017-10-03T00:12:16.939346: step 5396, loss 0.0144647, acc 1, learning_rate 0.0001
2017-10-03T00:12:18.092943: step 5397, loss 0.0371868, acc 0.96875, learning_rate 0.0001
2017-10-03T00:12:19.254364: step 5398, loss 0.0073768, acc 1, learning_rate 0.0001
2017-10-03T00:12:20.397838: step 5399, loss 0.00514611, acc 1, learning_rate 0.0001
2017-10-03T00:12:21.545909: step 5400, loss 0.00761882, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:12:21.902578: step 5400, loss 1.47775, acc 0.441727

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5400

2017-10-03T00:12:29.940588: step 5401, loss 0.00638271, acc 1, learning_rate 0.0001
2017-10-03T00:12:31.106463: step 5402, loss 0.00403687, acc 1, learning_rate 0.0001
2017-10-03T00:12:32.263710: step 5403, loss 0.0042358, acc 1, learning_rate 0.0001
2017-10-03T00:12:33.410488: step 5404, loss 0.00485241, acc 1, learning_rate 0.0001
2017-10-03T00:12:34.564318: step 5405, loss 0.00507103, acc 1, learning_rate 0.0001
2017-10-03T00:12:35.717275: step 5406, loss 0.0355483, acc 0.96875, learning_rate 0.0001
2017-10-03T00:12:36.877083: step 5407, loss 0.00719567, acc 1, learning_rate 0.0001
2017-10-03T00:12:38.042817: step 5408, loss 0.00437468, acc 1, learning_rate 0.0001
2017-10-03T00:12:39.192131: step 5409, loss 0.00533343, acc 1, learning_rate 0.0001
2017-10-03T00:12:40.349961: step 5410, loss 0.0088007, acc 1, learning_rate 0.0001
2017-10-03T00:12:41.525479: step 5411, loss 0.0809222, acc 0.96875, learning_rate 0.0001
2017-10-03T00:12:42.694483: step 5412, loss 0.0290096, acc 0.984375, learning_rate 0.0001
2017-10-03T00:12:43.845574: step 5413, loss 0.00394448, acc 1, learning_rate 0.0001
2017-10-03T00:12:45.001391: step 5414, loss 0.00515243, acc 1, learning_rate 0.0001
2017-10-03T00:12:46.153562: step 5415, loss 0.00386668, acc 1, learning_rate 0.0001
2017-10-03T00:12:47.330633: step 5416, loss 0.00383094, acc 1, learning_rate 0.0001
2017-10-03T00:12:48.495619: step 5417, loss 0.00687601, acc 1, learning_rate 0.0001
2017-10-03T00:12:49.655582: step 5418, loss 0.00581093, acc 1, learning_rate 0.0001
2017-10-03T00:12:50.808440: step 5419, loss 0.0155497, acc 1, learning_rate 0.0001
2017-10-03T00:12:51.955677: step 5420, loss 0.0164424, acc 1, learning_rate 0.0001
2017-10-03T00:12:53.103520: step 5421, loss 0.00563497, acc 1, learning_rate 0.0001
2017-10-03T00:12:54.246899: step 5422, loss 0.0375741, acc 0.96875, learning_rate 0.0001
2017-10-03T00:12:55.405886: step 5423, loss 0.00953336, acc 1, learning_rate 0.0001
2017-10-03T00:12:56.555031: step 5424, loss 0.00314631, acc 1, learning_rate 0.0001
2017-10-03T00:12:57.702943: step 5425, loss 0.0102704, acc 1, learning_rate 0.0001
2017-10-03T00:12:58.869587: step 5426, loss 0.0403859, acc 0.984375, learning_rate 0.0001
2017-10-03T00:13:00.023022: step 5427, loss 0.00354414, acc 1, learning_rate 0.0001
2017-10-03T00:13:01.161258: step 5428, loss 0.00618952, acc 1, learning_rate 0.0001
2017-10-03T00:13:02.316063: step 5429, loss 0.0131391, acc 1, learning_rate 0.0001
2017-10-03T00:13:03.495125: step 5430, loss 0.00669397, acc 1, learning_rate 0.0001
2017-10-03T00:13:04.637468: step 5431, loss 0.00377654, acc 1, learning_rate 0.0001
2017-10-03T00:13:05.788559: step 5432, loss 0.00478771, acc 1, learning_rate 0.0001
2017-10-03T00:13:06.931555: step 5433, loss 0.00514015, acc 1, learning_rate 0.0001
2017-10-03T00:13:08.095543: step 5434, loss 0.0463186, acc 0.984375, learning_rate 0.0001
2017-10-03T00:13:09.248981: step 5435, loss 0.00389949, acc 1, learning_rate 0.0001
2017-10-03T00:13:10.414318: step 5436, loss 0.00366407, acc 1, learning_rate 0.0001
2017-10-03T00:13:11.572327: step 5437, loss 0.0471335, acc 0.984375, learning_rate 0.0001
2017-10-03T00:13:12.726386: step 5438, loss 0.0534757, acc 0.96875, learning_rate 0.0001
2017-10-03T00:13:13.878033: step 5439, loss 0.00992102, acc 1, learning_rate 0.0001
2017-10-03T00:13:15.039166: step 5440, loss 0.0895493, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-03T00:13:15.371640: step 5440, loss 1.47103, acc 0.438849

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5440

2017-10-03T00:13:23.532782: step 5441, loss 0.00445016, acc 1, learning_rate 0.0001
2017-10-03T00:13:24.732491: step 5442, loss 0.00260323, acc 1, learning_rate 0.0001
2017-10-03T00:13:25.872574: step 5443, loss 0.00457404, acc 1, learning_rate 0.0001
2017-10-03T00:13:27.026773: step 5444, loss 0.00485476, acc 1, learning_rate 0.0001
2017-10-03T00:13:28.180132: step 5445, loss 0.00319311, acc 1, learning_rate 0.0001
2017-10-03T00:13:29.343641: step 5446, loss 0.00401629, acc 1, learning_rate 0.0001
2017-10-03T00:13:30.483012: step 5447, loss 0.00230265, acc 1, learning_rate 0.0001
2017-10-03T00:13:31.632491: step 5448, loss 0.00577664, acc 1, learning_rate 0.0001
2017-10-03T00:13:32.788463: step 5449, loss 0.0047513, acc 1, learning_rate 0.0001
2017-10-03T00:13:33.939983: step 5450, loss 0.0227785, acc 0.984375, learning_rate 0.0001
2017-10-03T00:13:35.084960: step 5451, loss 0.0444408, acc 0.984375, learning_rate 0.0001
2017-10-03T00:13:36.234021: step 5452, loss 0.00308685, acc 1, learning_rate 0.0001
2017-10-03T00:13:37.403907: step 5453, loss 0.00520379, acc 1, learning_rate 0.0001
2017-10-03T00:13:38.562880: step 5454, loss 0.0153531, acc 1, learning_rate 0.0001
2017-10-03T00:13:39.703719: step 5455, loss 0.00505353, acc 1, learning_rate 0.0001
2017-10-03T00:13:40.851811: step 5456, loss 0.00736255, acc 1, learning_rate 0.0001
2017-10-03T00:13:42.014758: step 5457, loss 0.00497152, acc 1, learning_rate 0.0001
2017-10-03T00:13:43.168808: step 5458, loss 0.0115977, acc 1, learning_rate 0.0001
2017-10-03T00:13:44.324854: step 5459, loss 0.023165, acc 0.984375, learning_rate 0.0001
2017-10-03T00:13:45.489871: step 5460, loss 0.00236215, acc 1, learning_rate 0.0001
2017-10-03T00:13:46.639336: step 5461, loss 0.011803, acc 1, learning_rate 0.0001
2017-10-03T00:13:47.782543: step 5462, loss 0.0351718, acc 0.984375, learning_rate 0.0001
2017-10-03T00:13:48.918715: step 5463, loss 0.00257557, acc 1, learning_rate 0.0001
2017-10-03T00:13:50.064421: step 5464, loss 0.0109445, acc 1, learning_rate 0.0001
2017-10-03T00:13:51.221353: step 5465, loss 0.00321759, acc 1, learning_rate 0.0001
2017-10-03T00:13:52.372405: step 5466, loss 0.00287095, acc 1, learning_rate 0.0001
2017-10-03T00:13:53.526444: step 5467, loss 0.0205747, acc 0.984375, learning_rate 0.0001
2017-10-03T00:13:54.692041: step 5468, loss 0.00367141, acc 1, learning_rate 0.0001
2017-10-03T00:13:55.835333: step 5469, loss 0.00570627, acc 1, learning_rate 0.0001
2017-10-03T00:13:56.974066: step 5470, loss 0.00786029, acc 1, learning_rate 0.0001
2017-10-03T00:13:58.138493: step 5471, loss 0.0146789, acc 1, learning_rate 0.0001
2017-10-03T00:13:59.380016: step 5472, loss 0.0136854, acc 1, learning_rate 0.0001
2017-10-03T00:14:00.532124: step 5473, loss 0.00947621, acc 1, learning_rate 0.0001
2017-10-03T00:14:01.688584: step 5474, loss 0.0128475, acc 1, learning_rate 0.0001
2017-10-03T00:14:02.840412: step 5475, loss 0.0431512, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:03.994325: step 5476, loss 0.00382877, acc 1, learning_rate 0.0001
2017-10-03T00:14:05.146953: step 5477, loss 0.0380849, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:06.297540: step 5478, loss 0.0104836, acc 1, learning_rate 0.0001
2017-10-03T00:14:07.447042: step 5479, loss 0.0528892, acc 0.96875, learning_rate 0.0001
2017-10-03T00:14:08.602324: step 5480, loss 0.00347257, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:14:08.942782: step 5480, loss 1.44702, acc 0.466187

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5480

2017-10-03T00:14:16.795423: step 5481, loss 0.0252406, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:17.944405: step 5482, loss 0.0050524, acc 1, learning_rate 0.0001
2017-10-03T00:14:19.099491: step 5483, loss 0.0452705, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:20.258525: step 5484, loss 0.00626155, acc 1, learning_rate 0.0001
2017-10-03T00:14:21.404568: step 5485, loss 0.0276007, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:22.571432: step 5486, loss 0.00453956, acc 1, learning_rate 0.0001
2017-10-03T00:14:23.711340: step 5487, loss 0.0160526, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:24.837312: step 5488, loss 0.0788958, acc 0.980392, learning_rate 0.0001
2017-10-03T00:14:25.986630: step 5489, loss 0.0265286, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:27.142038: step 5490, loss 0.0116821, acc 1, learning_rate 0.0001
2017-10-03T00:14:28.294130: step 5491, loss 0.0223024, acc 1, learning_rate 0.0001
2017-10-03T00:14:29.437649: step 5492, loss 0.00915363, acc 1, learning_rate 0.0001
2017-10-03T00:14:30.581478: step 5493, loss 0.00441084, acc 1, learning_rate 0.0001
2017-10-03T00:14:31.731541: step 5494, loss 0.00317306, acc 1, learning_rate 0.0001
2017-10-03T00:14:32.881787: step 5495, loss 0.00290103, acc 1, learning_rate 0.0001
2017-10-03T00:14:34.031670: step 5496, loss 0.00398573, acc 1, learning_rate 0.0001
2017-10-03T00:14:35.198531: step 5497, loss 0.00935078, acc 1, learning_rate 0.0001
2017-10-03T00:14:36.344448: step 5498, loss 0.00461925, acc 1, learning_rate 0.0001
2017-10-03T00:14:37.487107: step 5499, loss 0.00972658, acc 1, learning_rate 0.0001
2017-10-03T00:14:38.627616: step 5500, loss 0.00483403, acc 1, learning_rate 0.0001
2017-10-03T00:14:39.782552: step 5501, loss 0.00327443, acc 1, learning_rate 0.0001
2017-10-03T00:14:40.938757: step 5502, loss 0.0162597, acc 1, learning_rate 0.0001
2017-10-03T00:14:42.080873: step 5503, loss 0.00560297, acc 1, learning_rate 0.0001
2017-10-03T00:14:43.213762: step 5504, loss 0.00458726, acc 1, learning_rate 0.0001
2017-10-03T00:14:44.366240: step 5505, loss 0.002679, acc 1, learning_rate 0.0001
2017-10-03T00:14:45.507297: step 5506, loss 0.00338036, acc 1, learning_rate 0.0001
2017-10-03T00:14:46.652357: step 5507, loss 0.0159515, acc 1, learning_rate 0.0001
2017-10-03T00:14:47.813166: step 5508, loss 0.00919053, acc 1, learning_rate 0.0001
2017-10-03T00:14:48.960329: step 5509, loss 0.052681, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:50.116271: step 5510, loss 0.00366943, acc 1, learning_rate 0.0001
2017-10-03T00:14:51.291079: step 5511, loss 0.00391411, acc 1, learning_rate 0.0001
2017-10-03T00:14:52.444098: step 5512, loss 0.00480768, acc 1, learning_rate 0.0001
2017-10-03T00:14:53.591167: step 5513, loss 0.0197029, acc 0.984375, learning_rate 0.0001
2017-10-03T00:14:54.741519: step 5514, loss 0.00670462, acc 1, learning_rate 0.0001
2017-10-03T00:14:55.894735: step 5515, loss 0.00834229, acc 1, learning_rate 0.0001
2017-10-03T00:14:57.035723: step 5516, loss 0.0128738, acc 1, learning_rate 0.0001
2017-10-03T00:14:58.168383: step 5517, loss 0.0160971, acc 1, learning_rate 0.0001
2017-10-03T00:14:59.321981: step 5518, loss 0.00409051, acc 1, learning_rate 0.0001
2017-10-03T00:15:00.517562: step 5519, loss 0.00420348, acc 1, learning_rate 0.0001
2017-10-03T00:15:01.666780: step 5520, loss 0.00375383, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:15:02.011702: step 5520, loss 1.46725, acc 0.444604

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5520

2017-10-03T00:15:10.428770: step 5521, loss 0.00432031, acc 1, learning_rate 0.0001
2017-10-03T00:15:11.617827: step 5522, loss 0.0243121, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:12.771804: step 5523, loss 0.00301537, acc 1, learning_rate 0.0001
2017-10-03T00:15:13.921910: step 5524, loss 0.0495596, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:15.078968: step 5525, loss 0.00221728, acc 1, learning_rate 0.0001
2017-10-03T00:15:16.245419: step 5526, loss 0.00564441, acc 1, learning_rate 0.0001
2017-10-03T00:15:17.401458: step 5527, loss 0.00508085, acc 1, learning_rate 0.0001
2017-10-03T00:15:18.565710: step 5528, loss 0.00385545, acc 1, learning_rate 0.0001
2017-10-03T00:15:19.718685: step 5529, loss 0.00380013, acc 1, learning_rate 0.0001
2017-10-03T00:15:20.920115: step 5530, loss 0.00986157, acc 1, learning_rate 0.0001
2017-10-03T00:15:22.087363: step 5531, loss 0.0484573, acc 0.96875, learning_rate 0.0001
2017-10-03T00:15:23.248078: step 5532, loss 0.00364097, acc 1, learning_rate 0.0001
2017-10-03T00:15:24.398494: step 5533, loss 0.00613847, acc 1, learning_rate 0.0001
2017-10-03T00:15:25.541980: step 5534, loss 0.00579032, acc 1, learning_rate 0.0001
2017-10-03T00:15:26.697754: step 5535, loss 0.0683514, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:27.853471: step 5536, loss 0.00306901, acc 1, learning_rate 0.0001
2017-10-03T00:15:28.996825: step 5537, loss 0.00358613, acc 1, learning_rate 0.0001
2017-10-03T00:15:30.146486: step 5538, loss 0.00546981, acc 1, learning_rate 0.0001
2017-10-03T00:15:31.302530: step 5539, loss 0.00466056, acc 1, learning_rate 0.0001
2017-10-03T00:15:32.451419: step 5540, loss 0.0225635, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:33.617066: step 5541, loss 0.0114434, acc 1, learning_rate 0.0001
2017-10-03T00:15:34.807621: step 5542, loss 0.00858637, acc 1, learning_rate 0.0001
2017-10-03T00:15:35.949829: step 5543, loss 0.00440174, acc 1, learning_rate 0.0001
2017-10-03T00:15:37.098639: step 5544, loss 0.00559642, acc 1, learning_rate 0.0001
2017-10-03T00:15:38.236590: step 5545, loss 0.0106254, acc 1, learning_rate 0.0001
2017-10-03T00:15:39.414405: step 5546, loss 0.0675232, acc 0.96875, learning_rate 0.0001
2017-10-03T00:15:40.576271: step 5547, loss 0.00908332, acc 1, learning_rate 0.0001
2017-10-03T00:15:41.715428: step 5548, loss 0.0240631, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:42.875082: step 5549, loss 0.0306437, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:44.042144: step 5550, loss 0.00597105, acc 1, learning_rate 0.0001
2017-10-03T00:15:45.189650: step 5551, loss 0.0153259, acc 1, learning_rate 0.0001
2017-10-03T00:15:46.363875: step 5552, loss 0.0057342, acc 1, learning_rate 0.0001
2017-10-03T00:15:47.531473: step 5553, loss 0.0455138, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:48.683520: step 5554, loss 0.00473215, acc 1, learning_rate 0.0001
2017-10-03T00:15:49.840096: step 5555, loss 0.0312165, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:51.218131: step 5556, loss 0.00318861, acc 1, learning_rate 0.0001
2017-10-03T00:15:52.375436: step 5557, loss 0.0141984, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:53.534104: step 5558, loss 0.0393495, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:54.710600: step 5559, loss 0.0213606, acc 0.984375, learning_rate 0.0001
2017-10-03T00:15:55.877391: step 5560, loss 0.0274665, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:15:56.215138: step 5560, loss 1.44342, acc 0.483453

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5560

2017-10-03T00:16:04.156657: step 5561, loss 0.00687919, acc 1, learning_rate 0.0001
2017-10-03T00:16:05.327138: step 5562, loss 0.00563669, acc 1, learning_rate 0.0001
2017-10-03T00:16:06.486206: step 5563, loss 0.017763, acc 0.984375, learning_rate 0.0001
2017-10-03T00:16:07.631684: step 5564, loss 0.00560301, acc 1, learning_rate 0.0001
2017-10-03T00:16:08.783504: step 5565, loss 0.00487714, acc 1, learning_rate 0.0001
2017-10-03T00:16:09.954139: step 5566, loss 0.0211393, acc 0.984375, learning_rate 0.0001
2017-10-03T00:16:11.110096: step 5567, loss 0.0286474, acc 0.984375, learning_rate 0.0001
2017-10-03T00:16:12.259263: step 5568, loss 0.00543305, acc 1, learning_rate 0.0001
2017-10-03T00:16:13.416265: step 5569, loss 0.00434184, acc 1, learning_rate 0.0001
2017-10-03T00:16:14.566539: step 5570, loss 0.016312, acc 1, learning_rate 0.0001
2017-10-03T00:16:15.725187: step 5571, loss 0.00424463, acc 1, learning_rate 0.0001
2017-10-03T00:16:16.878306: step 5572, loss 0.00285295, acc 1, learning_rate 0.0001
2017-10-03T00:16:18.032536: step 5573, loss 0.0085823, acc 1, learning_rate 0.0001
2017-10-03T00:16:19.179948: step 5574, loss 0.00349878, acc 1, learning_rate 0.0001
2017-10-03T00:16:20.339300: step 5575, loss 0.00445269, acc 1, learning_rate 0.0001
2017-10-03T00:16:21.501896: step 5576, loss 0.00466867, acc 1, learning_rate 0.0001
2017-10-03T00:16:22.647216: step 5577, loss 0.00480677, acc 1, learning_rate 0.0001
2017-10-03T00:16:23.794659: step 5578, loss 0.0406019, acc 0.984375, learning_rate 0.0001
2017-10-03T00:16:24.975374: step 5579, loss 0.00545475, acc 1, learning_rate 0.0001
2017-10-03T00:16:26.123873: step 5580, loss 0.00526925, acc 1, learning_rate 0.0001
2017-10-03T00:16:27.284440: step 5581, loss 0.00478855, acc 1, learning_rate 0.0001
2017-10-03T00:16:28.428734: step 5582, loss 0.0182283, acc 1, learning_rate 0.0001
2017-10-03T00:16:29.590287: step 5583, loss 0.00319736, acc 1, learning_rate 0.0001
2017-10-03T00:16:30.747996: step 5584, loss 0.00322656, acc 1, learning_rate 0.0001
2017-10-03T00:16:31.895374: step 5585, loss 0.00617406, acc 1, learning_rate 0.0001
2017-10-03T00:16:33.029701: step 5586, loss 0.00457447, acc 1, learning_rate 0.0001
2017-10-03T00:16:34.184838: step 5587, loss 0.0053473, acc 1, learning_rate 0.0001
2017-10-03T00:16:35.346642: step 5588, loss 0.00479863, acc 1, learning_rate 0.0001
2017-10-03T00:16:36.495477: step 5589, loss 0.0029027, acc 1, learning_rate 0.0001
2017-10-03T00:16:37.647145: step 5590, loss 0.0028209, acc 1, learning_rate 0.0001
2017-10-03T00:16:38.780710: step 5591, loss 0.00536203, acc 1, learning_rate 0.0001
2017-10-03T00:16:40.018345: step 5592, loss 0.00427841, acc 1, learning_rate 0.0001
2017-10-03T00:16:41.173400: step 5593, loss 0.00488528, acc 1, learning_rate 0.0001
2017-10-03T00:16:42.330520: step 5594, loss 0.00610936, acc 1, learning_rate 0.0001
2017-10-03T00:16:43.480275: step 5595, loss 0.024628, acc 0.984375, learning_rate 0.0001
2017-10-03T00:16:44.635653: step 5596, loss 0.00357708, acc 1, learning_rate 0.0001
2017-10-03T00:16:45.772906: step 5597, loss 0.00516564, acc 1, learning_rate 0.0001
2017-10-03T00:16:46.922729: step 5598, loss 0.0257266, acc 0.984375, learning_rate 0.0001
2017-10-03T00:16:48.067568: step 5599, loss 0.00491694, acc 1, learning_rate 0.0001
2017-10-03T00:16:49.285269: step 5600, loss 0.0308178, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:16:49.622269: step 5600, loss 1.42187, acc 0.496403

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5600

2017-10-03T00:16:57.653248: step 5601, loss 0.0058522, acc 1, learning_rate 0.0001
2017-10-03T00:16:58.846187: step 5602, loss 0.00366339, acc 1, learning_rate 0.0001
2017-10-03T00:16:59.982855: step 5603, loss 0.0370159, acc 0.96875, learning_rate 0.0001
2017-10-03T00:17:01.149512: step 5604, loss 0.00524743, acc 1, learning_rate 0.0001
2017-10-03T00:17:02.299820: step 5605, loss 0.00429271, acc 1, learning_rate 0.0001
2017-10-03T00:17:03.447817: step 5606, loss 0.00571883, acc 1, learning_rate 0.0001
2017-10-03T00:17:04.608871: step 5607, loss 0.00736731, acc 1, learning_rate 0.0001
2017-10-03T00:17:05.760337: step 5608, loss 0.00271763, acc 1, learning_rate 0.0001
2017-10-03T00:17:06.907419: step 5609, loss 0.0452364, acc 0.984375, learning_rate 0.0001
2017-10-03T00:17:08.061547: step 5610, loss 0.00458583, acc 1, learning_rate 0.0001
2017-10-03T00:17:09.215717: step 5611, loss 0.00601851, acc 1, learning_rate 0.0001
2017-10-03T00:17:10.381632: step 5612, loss 0.00526449, acc 1, learning_rate 0.0001
2017-10-03T00:17:11.548346: step 5613, loss 0.00372287, acc 1, learning_rate 0.0001
2017-10-03T00:17:12.726834: step 5614, loss 0.00663358, acc 1, learning_rate 0.0001
2017-10-03T00:17:13.945878: step 5615, loss 0.0197498, acc 0.984375, learning_rate 0.0001
2017-10-03T00:17:15.098825: step 5616, loss 0.00415806, acc 1, learning_rate 0.0001
2017-10-03T00:17:16.250366: step 5617, loss 0.00180013, acc 1, learning_rate 0.0001
2017-10-03T00:17:17.395394: step 5618, loss 0.00503645, acc 1, learning_rate 0.0001
2017-10-03T00:17:18.553346: step 5619, loss 0.00312001, acc 1, learning_rate 0.0001
2017-10-03T00:17:19.705107: step 5620, loss 0.0101045, acc 1, learning_rate 0.0001
2017-10-03T00:17:20.890774: step 5621, loss 0.00949064, acc 1, learning_rate 0.0001
2017-10-03T00:17:22.051542: step 5622, loss 0.0471068, acc 0.984375, learning_rate 0.0001
2017-10-03T00:17:23.206817: step 5623, loss 0.0124888, acc 1, learning_rate 0.0001
2017-10-03T00:17:24.368356: step 5624, loss 0.0116425, acc 1, learning_rate 0.0001
2017-10-03T00:17:25.526342: step 5625, loss 0.00203278, acc 1, learning_rate 0.0001
2017-10-03T00:17:26.679159: step 5626, loss 0.063789, acc 0.984375, learning_rate 0.0001
2017-10-03T00:17:27.850545: step 5627, loss 0.046381, acc 0.96875, learning_rate 0.0001
2017-10-03T00:17:28.997826: step 5628, loss 0.00499213, acc 1, learning_rate 0.0001
2017-10-03T00:17:30.139929: step 5629, loss 0.00603124, acc 1, learning_rate 0.0001
2017-10-03T00:17:31.291820: step 5630, loss 0.00217811, acc 1, learning_rate 0.0001
2017-10-03T00:17:32.433007: step 5631, loss 0.0036548, acc 1, learning_rate 0.0001
2017-10-03T00:17:33.581434: step 5632, loss 0.00571307, acc 1, learning_rate 0.0001
2017-10-03T00:17:34.727310: step 5633, loss 0.0364109, acc 0.96875, learning_rate 0.0001
2017-10-03T00:17:35.888751: step 5634, loss 0.00243544, acc 1, learning_rate 0.0001
2017-10-03T00:17:37.039472: step 5635, loss 0.0025186, acc 1, learning_rate 0.0001
2017-10-03T00:17:38.196413: step 5636, loss 0.00383979, acc 1, learning_rate 0.0001
2017-10-03T00:17:39.359154: step 5637, loss 0.0105175, acc 1, learning_rate 0.0001
2017-10-03T00:17:40.499358: step 5638, loss 0.0101268, acc 1, learning_rate 0.0001
2017-10-03T00:17:41.644845: step 5639, loss 0.0202709, acc 0.984375, learning_rate 0.0001
2017-10-03T00:17:42.796661: step 5640, loss 0.0718712, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:17:43.137881: step 5640, loss 1.42916, acc 0.503597

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5640

2017-10-03T00:17:50.057825: step 5641, loss 0.00623794, acc 1, learning_rate 0.0001
2017-10-03T00:17:51.216189: step 5642, loss 0.00769027, acc 1, learning_rate 0.0001
2017-10-03T00:17:52.382160: step 5643, loss 0.0272974, acc 0.984375, learning_rate 0.0001
2017-10-03T00:17:53.532671: step 5644, loss 0.00472141, acc 1, learning_rate 0.0001
2017-10-03T00:17:54.672709: step 5645, loss 0.0074167, acc 1, learning_rate 0.0001
2017-10-03T00:17:55.824127: step 5646, loss 0.00696825, acc 1, learning_rate 0.0001
2017-10-03T00:17:56.965594: step 5647, loss 0.0192675, acc 0.984375, learning_rate 0.0001
2017-10-03T00:17:58.125650: step 5648, loss 0.0360644, acc 0.984375, learning_rate 0.0001
2017-10-03T00:17:59.284536: step 5649, loss 0.017574, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:00.461887: step 5650, loss 0.0170104, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:01.618546: step 5651, loss 0.00571357, acc 1, learning_rate 0.0001
2017-10-03T00:18:02.771477: step 5652, loss 0.00324498, acc 1, learning_rate 0.0001
2017-10-03T00:18:03.921912: step 5653, loss 0.00403984, acc 1, learning_rate 0.0001
2017-10-03T00:18:05.093905: step 5654, loss 0.0028435, acc 1, learning_rate 0.0001
2017-10-03T00:18:06.242267: step 5655, loss 0.020315, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:07.400724: step 5656, loss 0.0034613, acc 1, learning_rate 0.0001
2017-10-03T00:18:08.549932: step 5657, loss 0.0574231, acc 0.96875, learning_rate 0.0001
2017-10-03T00:18:09.703616: step 5658, loss 0.00550935, acc 1, learning_rate 0.0001
2017-10-03T00:18:10.862436: step 5659, loss 0.0197172, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:12.009933: step 5660, loss 0.00710147, acc 1, learning_rate 0.0001
2017-10-03T00:18:13.160511: step 5661, loss 0.0115024, acc 1, learning_rate 0.0001
2017-10-03T00:18:14.320906: step 5662, loss 0.00305603, acc 1, learning_rate 0.0001
2017-10-03T00:18:15.467833: step 5663, loss 0.00710397, acc 1, learning_rate 0.0001
2017-10-03T00:18:16.621892: step 5664, loss 0.00690719, acc 1, learning_rate 0.0001
2017-10-03T00:18:17.759877: step 5665, loss 0.0313216, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:18.919135: step 5666, loss 0.033923, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:20.073820: step 5667, loss 0.0178872, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:21.227796: step 5668, loss 0.0057589, acc 1, learning_rate 0.0001
2017-10-03T00:18:22.374350: step 5669, loss 0.00440182, acc 1, learning_rate 0.0001
2017-10-03T00:18:23.520088: step 5670, loss 0.0918914, acc 0.96875, learning_rate 0.0001
2017-10-03T00:18:24.675252: step 5671, loss 0.0390776, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:25.829984: step 5672, loss 0.0839562, acc 0.96875, learning_rate 0.0001
2017-10-03T00:18:26.989279: step 5673, loss 0.00849312, acc 1, learning_rate 0.0001
2017-10-03T00:18:28.142999: step 5674, loss 0.026849, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:29.297604: step 5675, loss 0.0121163, acc 1, learning_rate 0.0001
2017-10-03T00:18:30.447444: step 5676, loss 0.00704959, acc 1, learning_rate 0.0001
2017-10-03T00:18:31.588169: step 5677, loss 0.00498865, acc 1, learning_rate 0.0001
2017-10-03T00:18:32.748404: step 5678, loss 0.0039406, acc 1, learning_rate 0.0001
2017-10-03T00:18:33.899691: step 5679, loss 0.00324258, acc 1, learning_rate 0.0001
2017-10-03T00:18:35.045893: step 5680, loss 0.00370618, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:18:35.368029: step 5680, loss 1.45899, acc 0.467626

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5680

2017-10-03T00:18:43.305366: step 5681, loss 0.00358289, acc 1, learning_rate 0.0001
2017-10-03T00:18:44.517792: step 5682, loss 0.0526199, acc 0.96875, learning_rate 0.0001
2017-10-03T00:18:45.679336: step 5683, loss 0.0060945, acc 1, learning_rate 0.0001
2017-10-03T00:18:46.951913: step 5684, loss 0.0185232, acc 1, learning_rate 0.0001
2017-10-03T00:18:48.122059: step 5685, loss 0.0114986, acc 1, learning_rate 0.0001
2017-10-03T00:18:49.280780: step 5686, loss 0.00427413, acc 1, learning_rate 0.0001
2017-10-03T00:18:50.429644: step 5687, loss 0.0449612, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:51.600842: step 5688, loss 0.00336582, acc 1, learning_rate 0.0001
2017-10-03T00:18:52.755429: step 5689, loss 0.0048192, acc 1, learning_rate 0.0001
2017-10-03T00:18:53.905383: step 5690, loss 0.0239774, acc 0.984375, learning_rate 0.0001
2017-10-03T00:18:55.089565: step 5691, loss 0.00378705, acc 1, learning_rate 0.0001
2017-10-03T00:18:56.245952: step 5692, loss 0.0110374, acc 1, learning_rate 0.0001
2017-10-03T00:18:57.397173: step 5693, loss 0.0131345, acc 1, learning_rate 0.0001
2017-10-03T00:18:58.539139: step 5694, loss 0.00269953, acc 1, learning_rate 0.0001
2017-10-03T00:18:59.684763: step 5695, loss 0.00407617, acc 1, learning_rate 0.0001
2017-10-03T00:19:00.840668: step 5696, loss 0.00550003, acc 1, learning_rate 0.0001
2017-10-03T00:19:01.985065: step 5697, loss 0.00696993, acc 1, learning_rate 0.0001
2017-10-03T00:19:03.130978: step 5698, loss 0.00443333, acc 1, learning_rate 0.0001
2017-10-03T00:19:04.299895: step 5699, loss 0.00717889, acc 1, learning_rate 0.0001
2017-10-03T00:19:05.431054: step 5700, loss 0.00289315, acc 1, learning_rate 0.0001
2017-10-03T00:19:06.591086: step 5701, loss 0.00383684, acc 1, learning_rate 0.0001
2017-10-03T00:19:07.776244: step 5702, loss 0.00292051, acc 1, learning_rate 0.0001
2017-10-03T00:19:08.925840: step 5703, loss 0.0112147, acc 1, learning_rate 0.0001
2017-10-03T00:19:10.077943: step 5704, loss 0.00462413, acc 1, learning_rate 0.0001
2017-10-03T00:19:11.225388: step 5705, loss 0.00374275, acc 1, learning_rate 0.0001
2017-10-03T00:19:12.390754: step 5706, loss 0.0360729, acc 0.96875, learning_rate 0.0001
2017-10-03T00:19:13.566529: step 5707, loss 0.00585377, acc 1, learning_rate 0.0001
2017-10-03T00:19:14.721413: step 5708, loss 0.00362309, acc 1, learning_rate 0.0001
2017-10-03T00:19:15.879660: step 5709, loss 0.0127045, acc 1, learning_rate 0.0001
2017-10-03T00:19:17.023801: step 5710, loss 0.0276954, acc 0.984375, learning_rate 0.0001
2017-10-03T00:19:18.179486: step 5711, loss 0.0101529, acc 1, learning_rate 0.0001
2017-10-03T00:19:19.349592: step 5712, loss 0.00313338, acc 1, learning_rate 0.0001
2017-10-03T00:19:20.504890: step 5713, loss 0.0046481, acc 1, learning_rate 0.0001
2017-10-03T00:19:21.660141: step 5714, loss 0.00451728, acc 1, learning_rate 0.0001
2017-10-03T00:19:22.811620: step 5715, loss 0.0569574, acc 0.96875, learning_rate 0.0001
2017-10-03T00:19:23.970520: step 5716, loss 0.0109142, acc 1, learning_rate 0.0001
2017-10-03T00:19:25.141944: step 5717, loss 0.0151231, acc 1, learning_rate 0.0001
2017-10-03T00:19:26.308888: step 5718, loss 0.0274255, acc 0.984375, learning_rate 0.0001
2017-10-03T00:19:27.482127: step 5719, loss 0.00664879, acc 1, learning_rate 0.0001
2017-10-03T00:19:28.648232: step 5720, loss 0.00252009, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:19:28.976198: step 5720, loss 1.43197, acc 0.502158

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5720

2017-10-03T00:19:36.792552: step 5721, loss 0.0246288, acc 1, learning_rate 0.0001
2017-10-03T00:19:37.943618: step 5722, loss 0.00886597, acc 1, learning_rate 0.0001
2017-10-03T00:19:39.106631: step 5723, loss 0.004712, acc 1, learning_rate 0.0001
2017-10-03T00:19:40.280665: step 5724, loss 0.00822877, acc 1, learning_rate 0.0001
2017-10-03T00:19:41.452068: step 5725, loss 0.0528973, acc 0.984375, learning_rate 0.0001
2017-10-03T00:19:42.598659: step 5726, loss 0.00369447, acc 1, learning_rate 0.0001
2017-10-03T00:19:43.725258: step 5727, loss 0.0177293, acc 1, learning_rate 0.0001
2017-10-03T00:19:44.889098: step 5728, loss 0.00335869, acc 1, learning_rate 0.0001
2017-10-03T00:19:46.019650: step 5729, loss 0.00509943, acc 1, learning_rate 0.0001
2017-10-03T00:19:47.252794: step 5730, loss 0.00573176, acc 1, learning_rate 0.0001
2017-10-03T00:19:48.419650: step 5731, loss 0.00387284, acc 1, learning_rate 0.0001
2017-10-03T00:19:49.578479: step 5732, loss 0.015356, acc 1, learning_rate 0.0001
2017-10-03T00:19:50.745756: step 5733, loss 0.0042498, acc 1, learning_rate 0.0001
2017-10-03T00:19:51.898403: step 5734, loss 0.00292453, acc 1, learning_rate 0.0001
2017-10-03T00:19:53.037201: step 5735, loss 0.00568642, acc 1, learning_rate 0.0001
2017-10-03T00:19:54.184007: step 5736, loss 0.00270923, acc 1, learning_rate 0.0001
2017-10-03T00:19:55.336718: step 5737, loss 0.0170642, acc 1, learning_rate 0.0001
2017-10-03T00:19:56.549928: step 5738, loss 0.0171482, acc 1, learning_rate 0.0001
2017-10-03T00:19:57.708040: step 5739, loss 0.0132723, acc 1, learning_rate 0.0001
2017-10-03T00:19:58.875018: step 5740, loss 0.0131145, acc 1, learning_rate 0.0001
2017-10-03T00:20:00.037809: step 5741, loss 0.00382752, acc 1, learning_rate 0.0001
2017-10-03T00:20:01.197214: step 5742, loss 0.00309014, acc 1, learning_rate 0.0001
2017-10-03T00:20:02.359715: step 5743, loss 0.030969, acc 1, learning_rate 0.0001
2017-10-03T00:20:03.506700: step 5744, loss 0.029799, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:04.686079: step 5745, loss 0.025508, acc 1, learning_rate 0.0001
2017-10-03T00:20:05.846126: step 5746, loss 0.00470727, acc 1, learning_rate 0.0001
2017-10-03T00:20:07.007563: step 5747, loss 0.0180426, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:08.149780: step 5748, loss 0.00470147, acc 1, learning_rate 0.0001
2017-10-03T00:20:09.303650: step 5749, loss 0.0220922, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:10.506942: step 5750, loss 0.00251086, acc 1, learning_rate 0.0001
2017-10-03T00:20:11.662486: step 5751, loss 0.0379931, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:12.808877: step 5752, loss 0.036827, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:13.975968: step 5753, loss 0.00435885, acc 1, learning_rate 0.0001
2017-10-03T00:20:15.130368: step 5754, loss 0.00253383, acc 1, learning_rate 0.0001
2017-10-03T00:20:16.290446: step 5755, loss 0.00498019, acc 1, learning_rate 0.0001
2017-10-03T00:20:17.444671: step 5756, loss 0.00431551, acc 1, learning_rate 0.0001
2017-10-03T00:20:18.597046: step 5757, loss 0.00426588, acc 1, learning_rate 0.0001
2017-10-03T00:20:19.745966: step 5758, loss 0.00413573, acc 1, learning_rate 0.0001
2017-10-03T00:20:20.901882: step 5759, loss 0.00416174, acc 1, learning_rate 0.0001
2017-10-03T00:20:22.054358: step 5760, loss 0.00591759, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:20:22.398432: step 5760, loss 1.44973, acc 0.47482

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5760

2017-10-03T00:20:30.303593: step 5761, loss 0.0385849, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:31.470381: step 5762, loss 0.0170997, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:32.620358: step 5763, loss 0.00221244, acc 1, learning_rate 0.0001
2017-10-03T00:20:33.776382: step 5764, loss 0.0240206, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:34.937349: step 5765, loss 0.0252634, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:36.093650: step 5766, loss 0.0335323, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:37.253550: step 5767, loss 0.00904562, acc 1, learning_rate 0.0001
2017-10-03T00:20:38.572356: step 5768, loss 0.0049331, acc 1, learning_rate 0.0001
2017-10-03T00:20:39.733155: step 5769, loss 0.0131316, acc 1, learning_rate 0.0001
2017-10-03T00:20:40.890216: step 5770, loss 0.00576372, acc 1, learning_rate 0.0001
2017-10-03T00:20:42.034390: step 5771, loss 0.00478417, acc 1, learning_rate 0.0001
2017-10-03T00:20:43.182647: step 5772, loss 0.00464942, acc 1, learning_rate 0.0001
2017-10-03T00:20:44.362213: step 5773, loss 0.0640639, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:45.518227: step 5774, loss 0.00375557, acc 1, learning_rate 0.0001
2017-10-03T00:20:46.666463: step 5775, loss 0.00891242, acc 1, learning_rate 0.0001
2017-10-03T00:20:47.889877: step 5776, loss 0.00370924, acc 1, learning_rate 0.0001
2017-10-03T00:20:49.034399: step 5777, loss 0.0148729, acc 1, learning_rate 0.0001
2017-10-03T00:20:50.187417: step 5778, loss 0.00569367, acc 1, learning_rate 0.0001
2017-10-03T00:20:51.332327: step 5779, loss 0.00952328, acc 1, learning_rate 0.0001
2017-10-03T00:20:52.489818: step 5780, loss 0.0234193, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:53.664640: step 5781, loss 0.00489546, acc 1, learning_rate 0.0001
2017-10-03T00:20:55.038089: step 5782, loss 0.0061796, acc 1, learning_rate 0.0001
2017-10-03T00:20:56.196772: step 5783, loss 0.00326928, acc 1, learning_rate 0.0001
2017-10-03T00:20:57.346324: step 5784, loss 0.0248201, acc 0.984375, learning_rate 0.0001
2017-10-03T00:20:58.518372: step 5785, loss 0.0182674, acc 1, learning_rate 0.0001
2017-10-03T00:20:59.672053: step 5786, loss 0.00334836, acc 1, learning_rate 0.0001
2017-10-03T00:21:00.816880: step 5787, loss 0.00208383, acc 1, learning_rate 0.0001
2017-10-03T00:21:01.974521: step 5788, loss 0.00325006, acc 1, learning_rate 0.0001
2017-10-03T00:21:03.116279: step 5789, loss 0.0185563, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:04.274694: step 5790, loss 0.00277891, acc 1, learning_rate 0.0001
2017-10-03T00:21:05.411899: step 5791, loss 0.00553655, acc 1, learning_rate 0.0001
2017-10-03T00:21:06.559542: step 5792, loss 0.00216486, acc 1, learning_rate 0.0001
2017-10-03T00:21:07.786121: step 5793, loss 0.0185131, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:08.924623: step 5794, loss 0.00391219, acc 1, learning_rate 0.0001
2017-10-03T00:21:10.085370: step 5795, loss 0.00322465, acc 1, learning_rate 0.0001
2017-10-03T00:21:11.248168: step 5796, loss 0.00291636, acc 1, learning_rate 0.0001
2017-10-03T00:21:12.407945: step 5797, loss 0.0180344, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:13.558787: step 5798, loss 0.00438348, acc 1, learning_rate 0.0001
2017-10-03T00:21:14.720817: step 5799, loss 0.0125617, acc 1, learning_rate 0.0001
2017-10-03T00:21:15.893533: step 5800, loss 0.0275777, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:21:16.246172: step 5800, loss 1.44696, acc 0.47482

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5800

2017-10-03T00:21:23.289752: step 5801, loss 0.026239, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:24.436526: step 5802, loss 0.0160924, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:25.584600: step 5803, loss 0.00793074, acc 1, learning_rate 0.0001
2017-10-03T00:21:26.736742: step 5804, loss 0.002494, acc 1, learning_rate 0.0001
2017-10-03T00:21:27.891458: step 5805, loss 0.00453765, acc 1, learning_rate 0.0001
2017-10-03T00:21:29.049856: step 5806, loss 0.0315547, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:30.204879: step 5807, loss 0.060866, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:31.368660: step 5808, loss 0.0388265, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:32.526162: step 5809, loss 0.00260272, acc 1, learning_rate 0.0001
2017-10-03T00:21:33.675352: step 5810, loss 0.00671427, acc 1, learning_rate 0.0001
2017-10-03T00:21:34.820913: step 5811, loss 0.0312564, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:35.962547: step 5812, loss 0.00717243, acc 1, learning_rate 0.0001
2017-10-03T00:21:37.126173: step 5813, loss 0.0060386, acc 1, learning_rate 0.0001
2017-10-03T00:21:38.292833: step 5814, loss 0.00342797, acc 1, learning_rate 0.0001
2017-10-03T00:21:39.437233: step 5815, loss 0.0409918, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:40.584234: step 5816, loss 0.0177241, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:41.741900: step 5817, loss 0.00208919, acc 1, learning_rate 0.0001
2017-10-03T00:21:42.901524: step 5818, loss 0.00339109, acc 1, learning_rate 0.0001
2017-10-03T00:21:44.057157: step 5819, loss 0.0442131, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:45.211062: step 5820, loss 0.00232644, acc 1, learning_rate 0.0001
2017-10-03T00:21:46.371754: step 5821, loss 0.0203633, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:47.524885: step 5822, loss 0.00341588, acc 1, learning_rate 0.0001
2017-10-03T00:21:48.682841: step 5823, loss 0.00413439, acc 1, learning_rate 0.0001
2017-10-03T00:21:49.813648: step 5824, loss 0.0115731, acc 1, learning_rate 0.0001
2017-10-03T00:21:50.957975: step 5825, loss 0.0127118, acc 1, learning_rate 0.0001
2017-10-03T00:21:52.101466: step 5826, loss 0.0280291, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:53.260808: step 5827, loss 0.0204265, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:54.425868: step 5828, loss 0.00285971, acc 1, learning_rate 0.0001
2017-10-03T00:21:55.574027: step 5829, loss 0.00340974, acc 1, learning_rate 0.0001
2017-10-03T00:21:56.723692: step 5830, loss 0.0040839, acc 1, learning_rate 0.0001
2017-10-03T00:21:57.893273: step 5831, loss 0.0187704, acc 0.984375, learning_rate 0.0001
2017-10-03T00:21:59.041813: step 5832, loss 0.0109418, acc 1, learning_rate 0.0001
2017-10-03T00:22:00.195612: step 5833, loss 0.0013668, acc 1, learning_rate 0.0001
2017-10-03T00:22:01.339169: step 5834, loss 0.0025984, acc 1, learning_rate 0.0001
2017-10-03T00:22:02.490122: step 5835, loss 0.00426475, acc 1, learning_rate 0.0001
2017-10-03T00:22:03.637860: step 5836, loss 0.00365066, acc 1, learning_rate 0.0001
2017-10-03T00:22:04.812995: step 5837, loss 0.0155453, acc 1, learning_rate 0.0001
2017-10-03T00:22:05.963394: step 5838, loss 0.00284315, acc 1, learning_rate 0.0001
2017-10-03T00:22:07.113264: step 5839, loss 0.00374319, acc 1, learning_rate 0.0001
2017-10-03T00:22:08.265315: step 5840, loss 0.00585783, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:22:08.604777: step 5840, loss 1.44475, acc 0.484892

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5840

2017-10-03T00:22:16.568499: step 5841, loss 0.00431304, acc 1, learning_rate 0.0001
2017-10-03T00:22:17.789032: step 5842, loss 0.00281014, acc 1, learning_rate 0.0001
2017-10-03T00:22:18.935745: step 5843, loss 0.00325181, acc 1, learning_rate 0.0001
2017-10-03T00:22:20.088858: step 5844, loss 0.0150266, acc 0.984375, learning_rate 0.0001
2017-10-03T00:22:21.231133: step 5845, loss 0.00253425, acc 1, learning_rate 0.0001
2017-10-03T00:22:22.387520: step 5846, loss 0.0621178, acc 0.96875, learning_rate 0.0001
2017-10-03T00:22:23.536636: step 5847, loss 0.0137861, acc 1, learning_rate 0.0001
2017-10-03T00:22:24.708039: step 5848, loss 0.00436872, acc 1, learning_rate 0.0001
2017-10-03T00:22:25.870881: step 5849, loss 0.0148016, acc 1, learning_rate 0.0001
2017-10-03T00:22:27.014541: step 5850, loss 0.0177124, acc 0.984375, learning_rate 0.0001
2017-10-03T00:22:28.172618: step 5851, loss 0.0204404, acc 0.984375, learning_rate 0.0001
2017-10-03T00:22:29.344714: step 5852, loss 0.0962609, acc 0.96875, learning_rate 0.0001
2017-10-03T00:22:30.565667: step 5853, loss 0.00525578, acc 1, learning_rate 0.0001
2017-10-03T00:22:31.722131: step 5854, loss 0.0062004, acc 1, learning_rate 0.0001
2017-10-03T00:22:32.866985: step 5855, loss 0.0355861, acc 0.984375, learning_rate 0.0001
2017-10-03T00:22:34.037846: step 5856, loss 0.00377992, acc 1, learning_rate 0.0001
2017-10-03T00:22:35.201542: step 5857, loss 0.00286639, acc 1, learning_rate 0.0001
2017-10-03T00:22:36.347488: step 5858, loss 0.00247073, acc 1, learning_rate 0.0001
2017-10-03T00:22:37.515082: step 5859, loss 0.0040268, acc 1, learning_rate 0.0001
2017-10-03T00:22:38.659426: step 5860, loss 0.00912456, acc 1, learning_rate 0.0001
2017-10-03T00:22:39.831329: step 5861, loss 0.00231427, acc 1, learning_rate 0.0001
2017-10-03T00:22:40.992850: step 5862, loss 0.0252107, acc 0.984375, learning_rate 0.0001
2017-10-03T00:22:42.140987: step 5863, loss 0.00506742, acc 1, learning_rate 0.0001
2017-10-03T00:22:43.299284: step 5864, loss 0.00955914, acc 1, learning_rate 0.0001
2017-10-03T00:22:44.457929: step 5865, loss 0.0140471, acc 1, learning_rate 0.0001
2017-10-03T00:22:45.604881: step 5866, loss 0.00872332, acc 1, learning_rate 0.0001
2017-10-03T00:22:46.769256: step 5867, loss 0.00225952, acc 1, learning_rate 0.0001
2017-10-03T00:22:47.937925: step 5868, loss 0.0513096, acc 0.96875, learning_rate 0.0001
2017-10-03T00:22:49.088119: step 5869, loss 0.00291205, acc 1, learning_rate 0.0001
2017-10-03T00:22:50.238755: step 5870, loss 0.00352974, acc 1, learning_rate 0.0001
2017-10-03T00:22:51.395006: step 5871, loss 0.00305879, acc 1, learning_rate 0.0001
2017-10-03T00:22:52.532089: step 5872, loss 0.00842333, acc 1, learning_rate 0.0001
2017-10-03T00:22:53.692139: step 5873, loss 0.00496827, acc 1, learning_rate 0.0001
2017-10-03T00:22:54.843495: step 5874, loss 0.00206518, acc 1, learning_rate 0.0001
2017-10-03T00:22:56.005995: step 5875, loss 0.0152292, acc 0.984375, learning_rate 0.0001
2017-10-03T00:22:57.175375: step 5876, loss 0.0265994, acc 0.984375, learning_rate 0.0001
2017-10-03T00:22:58.322315: step 5877, loss 0.00462374, acc 1, learning_rate 0.0001
2017-10-03T00:22:59.473936: step 5878, loss 0.00347117, acc 1, learning_rate 0.0001
2017-10-03T00:23:00.624060: step 5879, loss 0.00637699, acc 1, learning_rate 0.0001
2017-10-03T00:23:01.747368: step 5880, loss 0.0479768, acc 0.980392, learning_rate 0.0001

Evaluation:
2017-10-03T00:23:02.080590: step 5880, loss 1.50467, acc 0.376978

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5880

2017-10-03T00:23:09.760518: step 5881, loss 0.00392929, acc 1, learning_rate 0.0001
2017-10-03T00:23:10.936260: step 5882, loss 0.00299802, acc 1, learning_rate 0.0001
2017-10-03T00:23:12.087750: step 5883, loss 0.00354493, acc 1, learning_rate 0.0001
2017-10-03T00:23:13.247927: step 5884, loss 0.00923641, acc 1, learning_rate 0.0001
2017-10-03T00:23:14.403496: step 5885, loss 0.0299499, acc 0.96875, learning_rate 0.0001
2017-10-03T00:23:15.561055: step 5886, loss 0.00263305, acc 1, learning_rate 0.0001
2017-10-03T00:23:16.710506: step 5887, loss 0.018096, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:17.864969: step 5888, loss 0.0208027, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:19.017530: step 5889, loss 0.0205704, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:20.187361: step 5890, loss 0.0391588, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:21.375296: step 5891, loss 0.0087051, acc 1, learning_rate 0.0001
2017-10-03T00:23:22.528256: step 5892, loss 0.00311886, acc 1, learning_rate 0.0001
2017-10-03T00:23:23.686521: step 5893, loss 0.00345448, acc 1, learning_rate 0.0001
2017-10-03T00:23:24.826858: step 5894, loss 0.00539168, acc 1, learning_rate 0.0001
2017-10-03T00:23:25.971777: step 5895, loss 0.00371056, acc 1, learning_rate 0.0001
2017-10-03T00:23:27.119575: step 5896, loss 0.027402, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:28.280521: step 5897, loss 0.033659, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:29.444338: step 5898, loss 0.0022457, acc 1, learning_rate 0.0001
2017-10-03T00:23:30.611452: step 5899, loss 0.00993137, acc 1, learning_rate 0.0001
2017-10-03T00:23:31.766388: step 5900, loss 0.00421265, acc 1, learning_rate 0.0001
2017-10-03T00:23:32.921657: step 5901, loss 0.0187389, acc 1, learning_rate 0.0001
2017-10-03T00:23:34.071514: step 5902, loss 0.0384476, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:35.238308: step 5903, loss 0.013298, acc 1, learning_rate 0.0001
2017-10-03T00:23:36.388635: step 5904, loss 0.0120352, acc 1, learning_rate 0.0001
2017-10-03T00:23:37.552658: step 5905, loss 0.00472138, acc 1, learning_rate 0.0001
2017-10-03T00:23:38.713905: step 5906, loss 0.00377814, acc 1, learning_rate 0.0001
2017-10-03T00:23:39.871241: step 5907, loss 0.00183696, acc 1, learning_rate 0.0001
2017-10-03T00:23:41.031455: step 5908, loss 0.00342151, acc 1, learning_rate 0.0001
2017-10-03T00:23:42.188433: step 5909, loss 0.0520833, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:43.355376: step 5910, loss 0.00335507, acc 1, learning_rate 0.0001
2017-10-03T00:23:44.510593: step 5911, loss 0.00513198, acc 1, learning_rate 0.0001
2017-10-03T00:23:45.650436: step 5912, loss 0.00274802, acc 1, learning_rate 0.0001
2017-10-03T00:23:46.803021: step 5913, loss 0.00288438, acc 1, learning_rate 0.0001
2017-10-03T00:23:47.959449: step 5914, loss 0.028134, acc 1, learning_rate 0.0001
2017-10-03T00:23:49.120441: step 5915, loss 0.00482509, acc 1, learning_rate 0.0001
2017-10-03T00:23:50.281060: step 5916, loss 0.0349045, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:51.430515: step 5917, loss 0.00382018, acc 1, learning_rate 0.0001
2017-10-03T00:23:52.614235: step 5918, loss 0.00334394, acc 1, learning_rate 0.0001
2017-10-03T00:23:53.759588: step 5919, loss 0.0336761, acc 0.984375, learning_rate 0.0001
2017-10-03T00:23:54.925011: step 5920, loss 0.0154626, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:23:55.248482: step 5920, loss 1.4381, acc 0.48777

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5920

2017-10-03T00:24:03.094154: step 5921, loss 0.0777861, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:04.273547: step 5922, loss 0.0103022, acc 1, learning_rate 0.0001
2017-10-03T00:24:05.436917: step 5923, loss 0.0445446, acc 0.96875, learning_rate 0.0001
2017-10-03T00:24:06.583724: step 5924, loss 0.0299783, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:07.730328: step 5925, loss 0.00579596, acc 1, learning_rate 0.0001
2017-10-03T00:24:08.879379: step 5926, loss 0.0285172, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:10.043043: step 5927, loss 0.00309809, acc 1, learning_rate 0.0001
2017-10-03T00:24:11.181528: step 5928, loss 0.00684811, acc 1, learning_rate 0.0001
2017-10-03T00:24:12.333324: step 5929, loss 0.0276757, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:13.505121: step 5930, loss 0.00423785, acc 1, learning_rate 0.0001
2017-10-03T00:24:14.656625: step 5931, loss 0.00272613, acc 1, learning_rate 0.0001
2017-10-03T00:24:15.807909: step 5932, loss 0.00249379, acc 1, learning_rate 0.0001
2017-10-03T00:24:16.950674: step 5933, loss 0.0021162, acc 1, learning_rate 0.0001
2017-10-03T00:24:18.107245: step 5934, loss 0.0172364, acc 1, learning_rate 0.0001
2017-10-03T00:24:19.274259: step 5935, loss 0.0488726, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:20.431824: step 5936, loss 0.00548059, acc 1, learning_rate 0.0001
2017-10-03T00:24:21.593665: step 5937, loss 0.00692603, acc 1, learning_rate 0.0001
2017-10-03T00:24:22.754761: step 5938, loss 0.00254151, acc 1, learning_rate 0.0001
2017-10-03T00:24:23.896378: step 5939, loss 0.015172, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:25.060655: step 5940, loss 0.00502876, acc 1, learning_rate 0.0001
2017-10-03T00:24:26.225339: step 5941, loss 0.00377654, acc 1, learning_rate 0.0001
2017-10-03T00:24:27.379104: step 5942, loss 0.0108208, acc 1, learning_rate 0.0001
2017-10-03T00:24:28.540223: step 5943, loss 0.0132626, acc 1, learning_rate 0.0001
2017-10-03T00:24:29.704031: step 5944, loss 0.0424973, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:30.857180: step 5945, loss 0.0111633, acc 1, learning_rate 0.0001
2017-10-03T00:24:32.010270: step 5946, loss 0.00227608, acc 1, learning_rate 0.0001
2017-10-03T00:24:33.191644: step 5947, loss 0.00298932, acc 1, learning_rate 0.0001
2017-10-03T00:24:34.338738: step 5948, loss 0.00669641, acc 1, learning_rate 0.0001
2017-10-03T00:24:35.496498: step 5949, loss 0.0023026, acc 1, learning_rate 0.0001
2017-10-03T00:24:36.643878: step 5950, loss 0.034773, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:37.803530: step 5951, loss 0.00423149, acc 1, learning_rate 0.0001
2017-10-03T00:24:38.971125: step 5952, loss 0.00440648, acc 1, learning_rate 0.0001
2017-10-03T00:24:40.120621: step 5953, loss 0.0419289, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:41.291878: step 5954, loss 0.00434079, acc 1, learning_rate 0.0001
2017-10-03T00:24:42.442023: step 5955, loss 0.00593573, acc 1, learning_rate 0.0001
2017-10-03T00:24:43.590920: step 5956, loss 0.00812903, acc 1, learning_rate 0.0001
2017-10-03T00:24:44.745922: step 5957, loss 0.0173332, acc 1, learning_rate 0.0001
2017-10-03T00:24:45.907383: step 5958, loss 0.00330667, acc 1, learning_rate 0.0001
2017-10-03T00:24:47.060732: step 5959, loss 0.0238641, acc 0.984375, learning_rate 0.0001
2017-10-03T00:24:48.206241: step 5960, loss 0.00579769, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:24:48.546311: step 5960, loss 1.50418, acc 0.397122

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-5960

2017-10-03T00:24:56.437919: step 5961, loss 0.00336475, acc 1, learning_rate 0.0001
2017-10-03T00:24:57.607504: step 5962, loss 0.00311837, acc 1, learning_rate 0.0001
2017-10-03T00:24:58.760534: step 5963, loss 0.00320765, acc 1, learning_rate 0.0001
2017-10-03T00:24:59.926715: step 5964, loss 0.0191462, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:01.070903: step 5965, loss 0.0345556, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:02.210014: step 5966, loss 0.00220197, acc 1, learning_rate 0.0001
2017-10-03T00:25:03.365046: step 5967, loss 0.0141328, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:04.521894: step 5968, loss 0.0289195, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:05.690475: step 5969, loss 0.0131189, acc 1, learning_rate 0.0001
2017-10-03T00:25:06.907638: step 5970, loss 0.0058213, acc 1, learning_rate 0.0001
2017-10-03T00:25:08.046577: step 5971, loss 0.00324597, acc 1, learning_rate 0.0001
2017-10-03T00:25:09.186348: step 5972, loss 0.00457417, acc 1, learning_rate 0.0001
2017-10-03T00:25:10.355055: step 5973, loss 0.0026865, acc 1, learning_rate 0.0001
2017-10-03T00:25:11.497832: step 5974, loss 0.00273019, acc 1, learning_rate 0.0001
2017-10-03T00:25:12.644249: step 5975, loss 0.0401252, acc 0.96875, learning_rate 0.0001
2017-10-03T00:25:13.809781: step 5976, loss 0.00279799, acc 1, learning_rate 0.0001
2017-10-03T00:25:14.973933: step 5977, loss 0.0026381, acc 1, learning_rate 0.0001
2017-10-03T00:25:16.100953: step 5978, loss 0.00315044, acc 1, learning_rate 0.0001
2017-10-03T00:25:17.269103: step 5979, loss 0.00803256, acc 1, learning_rate 0.0001
2017-10-03T00:25:18.428605: step 5980, loss 0.0061698, acc 1, learning_rate 0.0001
2017-10-03T00:25:19.585925: step 5981, loss 0.0112099, acc 1, learning_rate 0.0001
2017-10-03T00:25:20.737575: step 5982, loss 0.00290773, acc 1, learning_rate 0.0001
2017-10-03T00:25:21.943089: step 5983, loss 0.00269784, acc 1, learning_rate 0.0001
2017-10-03T00:25:23.091723: step 5984, loss 0.00554365, acc 1, learning_rate 0.0001
2017-10-03T00:25:24.242575: step 5985, loss 0.0027723, acc 1, learning_rate 0.0001
2017-10-03T00:25:25.403989: step 5986, loss 0.00768775, acc 1, learning_rate 0.0001
2017-10-03T00:25:26.548500: step 5987, loss 0.00317594, acc 1, learning_rate 0.0001
2017-10-03T00:25:27.704138: step 5988, loss 0.0150446, acc 1, learning_rate 0.0001
2017-10-03T00:25:28.847028: step 5989, loss 0.0131988, acc 1, learning_rate 0.0001
2017-10-03T00:25:30.005245: step 5990, loss 0.0300593, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:31.150594: step 5991, loss 0.00466724, acc 1, learning_rate 0.0001
2017-10-03T00:25:32.310623: step 5992, loss 0.00563976, acc 1, learning_rate 0.0001
2017-10-03T00:25:33.459153: step 5993, loss 0.00693434, acc 1, learning_rate 0.0001
2017-10-03T00:25:34.611964: step 5994, loss 0.0209165, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:35.757968: step 5995, loss 0.0392631, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:36.906740: step 5996, loss 0.0123648, acc 1, learning_rate 0.0001
2017-10-03T00:25:38.071190: step 5997, loss 0.00219833, acc 1, learning_rate 0.0001
2017-10-03T00:25:39.225744: step 5998, loss 0.00216927, acc 1, learning_rate 0.0001
2017-10-03T00:25:40.380681: step 5999, loss 0.00354948, acc 1, learning_rate 0.0001
2017-10-03T00:25:41.536413: step 6000, loss 0.0194827, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:25:41.879607: step 6000, loss 1.51159, acc 0.381295

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6000

2017-10-03T00:25:50.093943: step 6001, loss 0.00183276, acc 1, learning_rate 0.0001
2017-10-03T00:25:51.267617: step 6002, loss 0.00236336, acc 1, learning_rate 0.0001
2017-10-03T00:25:52.418521: step 6003, loss 0.0351409, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:53.585813: step 6004, loss 0.00367841, acc 1, learning_rate 0.0001
2017-10-03T00:25:54.739151: step 6005, loss 0.0463481, acc 0.984375, learning_rate 0.0001
2017-10-03T00:25:55.892423: step 6006, loss 0.0104938, acc 1, learning_rate 0.0001
2017-10-03T00:25:57.048320: step 6007, loss 0.00434309, acc 1, learning_rate 0.0001
2017-10-03T00:25:58.193595: step 6008, loss 0.0092892, acc 1, learning_rate 0.0001
2017-10-03T00:25:59.343421: step 6009, loss 0.00528135, acc 1, learning_rate 0.0001
2017-10-03T00:26:00.499582: step 6010, loss 0.0105653, acc 1, learning_rate 0.0001
2017-10-03T00:26:01.663757: step 6011, loss 0.0056844, acc 1, learning_rate 0.0001
2017-10-03T00:26:02.820470: step 6012, loss 0.0418725, acc 0.984375, learning_rate 0.0001
2017-10-03T00:26:04.218449: step 6013, loss 0.00412271, acc 1, learning_rate 0.0001
2017-10-03T00:26:05.362206: step 6014, loss 0.0054353, acc 1, learning_rate 0.0001
2017-10-03T00:26:06.517277: step 6015, loss 0.0318049, acc 0.984375, learning_rate 0.0001
2017-10-03T00:26:07.673826: step 6016, loss 0.0123425, acc 1, learning_rate 0.0001
2017-10-03T00:26:08.820090: step 6017, loss 0.00576618, acc 1, learning_rate 0.0001
2017-10-03T00:26:09.972484: step 6018, loss 0.0124074, acc 1, learning_rate 0.0001
2017-10-03T00:26:11.125443: step 6019, loss 0.0133311, acc 1, learning_rate 0.0001
2017-10-03T00:26:12.283660: step 6020, loss 0.00368735, acc 1, learning_rate 0.0001
2017-10-03T00:26:13.442313: step 6021, loss 0.00630301, acc 1, learning_rate 0.0001
2017-10-03T00:26:14.579321: step 6022, loss 0.00256557, acc 1, learning_rate 0.0001
2017-10-03T00:26:15.735453: step 6023, loss 0.0020312, acc 1, learning_rate 0.0001
2017-10-03T00:26:16.887983: step 6024, loss 0.00227948, acc 1, learning_rate 0.0001
2017-10-03T00:26:18.030492: step 6025, loss 0.00553541, acc 1, learning_rate 0.0001
2017-10-03T00:26:19.174456: step 6026, loss 0.00522787, acc 1, learning_rate 0.0001
2017-10-03T00:26:20.324878: step 6027, loss 0.00219188, acc 1, learning_rate 0.0001
2017-10-03T00:26:21.483525: step 6028, loss 0.00519261, acc 1, learning_rate 0.0001
2017-10-03T00:26:22.629106: step 6029, loss 0.00642456, acc 1, learning_rate 0.0001
2017-10-03T00:26:23.794520: step 6030, loss 0.00898269, acc 1, learning_rate 0.0001
2017-10-03T00:26:24.954000: step 6031, loss 0.0070371, acc 1, learning_rate 0.0001
2017-10-03T00:26:26.105412: step 6032, loss 0.00307367, acc 1, learning_rate 0.0001
2017-10-03T00:26:27.248523: step 6033, loss 0.0309231, acc 0.984375, learning_rate 0.0001
2017-10-03T00:26:28.422646: step 6034, loss 0.00297606, acc 1, learning_rate 0.0001
2017-10-03T00:26:29.567774: step 6035, loss 0.00718175, acc 1, learning_rate 0.0001
2017-10-03T00:26:30.715363: step 6036, loss 0.00568437, acc 1, learning_rate 0.0001
2017-10-03T00:26:31.874344: step 6037, loss 0.0179319, acc 0.984375, learning_rate 0.0001
2017-10-03T00:26:33.034403: step 6038, loss 0.00493996, acc 1, learning_rate 0.0001
2017-10-03T00:26:34.192645: step 6039, loss 0.00431915, acc 1, learning_rate 0.0001
2017-10-03T00:26:35.341733: step 6040, loss 0.00387491, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:26:35.688458: step 6040, loss 1.46883, acc 0.446043

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6040

2017-10-03T00:26:43.630710: step 6041, loss 0.00375403, acc 1, learning_rate 0.0001
2017-10-03T00:26:44.812282: step 6042, loss 0.00280144, acc 1, learning_rate 0.0001
2017-10-03T00:26:45.959138: step 6043, loss 0.00342691, acc 1, learning_rate 0.0001
2017-10-03T00:26:47.116573: step 6044, loss 0.00673408, acc 1, learning_rate 0.0001
2017-10-03T00:26:48.273575: step 6045, loss 0.0027759, acc 1, learning_rate 0.0001
2017-10-03T00:26:49.425810: step 6046, loss 0.00898821, acc 1, learning_rate 0.0001
2017-10-03T00:26:50.567538: step 6047, loss 0.00654706, acc 1, learning_rate 0.0001
2017-10-03T00:26:51.720351: step 6048, loss 0.00383957, acc 1, learning_rate 0.0001
2017-10-03T00:26:52.864734: step 6049, loss 0.0169288, acc 1, learning_rate 0.0001
2017-10-03T00:26:54.005139: step 6050, loss 0.0511615, acc 0.96875, learning_rate 0.0001
2017-10-03T00:26:55.154977: step 6051, loss 0.00386421, acc 1, learning_rate 0.0001
2017-10-03T00:26:56.328148: step 6052, loss 0.00230674, acc 1, learning_rate 0.0001
2017-10-03T00:26:57.477537: step 6053, loss 0.00430369, acc 1, learning_rate 0.0001
2017-10-03T00:26:58.630280: step 6054, loss 0.0296306, acc 0.984375, learning_rate 0.0001
2017-10-03T00:26:59.791354: step 6055, loss 0.0314818, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:00.968998: step 6056, loss 0.0151565, acc 1, learning_rate 0.0001
2017-10-03T00:27:02.121029: step 6057, loss 0.0346309, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:03.288193: step 6058, loss 0.0140652, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:04.455599: step 6059, loss 0.00407228, acc 1, learning_rate 0.0001
2017-10-03T00:27:05.616637: step 6060, loss 0.00194649, acc 1, learning_rate 0.0001
2017-10-03T00:27:06.766851: step 6061, loss 0.00395961, acc 1, learning_rate 0.0001
2017-10-03T00:27:07.922717: step 6062, loss 0.00420534, acc 1, learning_rate 0.0001
2017-10-03T00:27:09.088675: step 6063, loss 0.00311943, acc 1, learning_rate 0.0001
2017-10-03T00:27:10.232107: step 6064, loss 0.00305814, acc 1, learning_rate 0.0001
2017-10-03T00:27:11.384830: step 6065, loss 0.00203168, acc 1, learning_rate 0.0001
2017-10-03T00:27:12.530916: step 6066, loss 0.0264515, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:13.710865: step 6067, loss 0.0021547, acc 1, learning_rate 0.0001
2017-10-03T00:27:14.859448: step 6068, loss 0.0147711, acc 1, learning_rate 0.0001
2017-10-03T00:27:16.006630: step 6069, loss 0.0130951, acc 1, learning_rate 0.0001
2017-10-03T00:27:17.152932: step 6070, loss 0.0157725, acc 1, learning_rate 0.0001
2017-10-03T00:27:18.330672: step 6071, loss 0.00171502, acc 1, learning_rate 0.0001
2017-10-03T00:27:19.499336: step 6072, loss 0.0149859, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:20.666481: step 6073, loss 0.00548142, acc 1, learning_rate 0.0001
2017-10-03T00:27:21.852503: step 6074, loss 0.00304379, acc 1, learning_rate 0.0001
2017-10-03T00:27:22.991461: step 6075, loss 0.0422459, acc 0.96875, learning_rate 0.0001
2017-10-03T00:27:24.130505: step 6076, loss 0.00839081, acc 1, learning_rate 0.0001
2017-10-03T00:27:25.273480: step 6077, loss 0.00265581, acc 1, learning_rate 0.0001
2017-10-03T00:27:26.436198: step 6078, loss 0.00420177, acc 1, learning_rate 0.0001
2017-10-03T00:27:27.593178: step 6079, loss 0.0045517, acc 1, learning_rate 0.0001
2017-10-03T00:27:28.751735: step 6080, loss 0.0261371, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:27:29.091373: step 6080, loss 1.47504, acc 0.433094

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6080

2017-10-03T00:27:36.121666: step 6081, loss 0.015606, acc 1, learning_rate 0.0001
2017-10-03T00:27:37.328783: step 6082, loss 0.006909, acc 1, learning_rate 0.0001
2017-10-03T00:27:38.489490: step 6083, loss 0.0254144, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:39.641902: step 6084, loss 0.00661908, acc 1, learning_rate 0.0001
2017-10-03T00:27:40.797707: step 6085, loss 0.0185478, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:41.956667: step 6086, loss 0.00742234, acc 1, learning_rate 0.0001
2017-10-03T00:27:43.127348: step 6087, loss 0.00384015, acc 1, learning_rate 0.0001
2017-10-03T00:27:44.304496: step 6088, loss 0.0102401, acc 1, learning_rate 0.0001
2017-10-03T00:27:45.455472: step 6089, loss 0.0226073, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:46.610624: step 6090, loss 0.0172559, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:47.770545: step 6091, loss 0.00228912, acc 1, learning_rate 0.0001
2017-10-03T00:27:48.924479: step 6092, loss 0.00397476, acc 1, learning_rate 0.0001
2017-10-03T00:27:50.107915: step 6093, loss 0.0035788, acc 1, learning_rate 0.0001
2017-10-03T00:27:51.253785: step 6094, loss 0.00480328, acc 1, learning_rate 0.0001
2017-10-03T00:27:52.419744: step 6095, loss 0.0141122, acc 1, learning_rate 0.0001
2017-10-03T00:27:53.585876: step 6096, loss 0.00722863, acc 1, learning_rate 0.0001
2017-10-03T00:27:54.941607: step 6097, loss 0.0373133, acc 0.984375, learning_rate 0.0001
2017-10-03T00:27:56.105907: step 6098, loss 0.00554426, acc 1, learning_rate 0.0001
2017-10-03T00:27:57.258740: step 6099, loss 0.00594129, acc 1, learning_rate 0.0001
2017-10-03T00:27:58.409030: step 6100, loss 0.00535909, acc 1, learning_rate 0.0001
2017-10-03T00:27:59.552047: step 6101, loss 0.050502, acc 0.96875, learning_rate 0.0001
2017-10-03T00:28:00.702645: step 6102, loss 0.00582485, acc 1, learning_rate 0.0001
2017-10-03T00:28:01.843071: step 6103, loss 0.00232467, acc 1, learning_rate 0.0001
2017-10-03T00:28:02.982995: step 6104, loss 0.00447936, acc 1, learning_rate 0.0001
2017-10-03T00:28:04.143190: step 6105, loss 0.0288107, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:05.386699: step 6106, loss 0.00797882, acc 1, learning_rate 0.0001
2017-10-03T00:28:06.556409: step 6107, loss 0.0293736, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:07.715092: step 6108, loss 0.00320347, acc 1, learning_rate 0.0001
2017-10-03T00:28:08.872733: step 6109, loss 0.0380987, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:10.026485: step 6110, loss 0.00586818, acc 1, learning_rate 0.0001
2017-10-03T00:28:11.161915: step 6111, loss 0.0280279, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:12.328283: step 6112, loss 0.00426199, acc 1, learning_rate 0.0001
2017-10-03T00:28:13.523992: step 6113, loss 0.00588054, acc 1, learning_rate 0.0001
2017-10-03T00:28:14.772088: step 6114, loss 0.0265429, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:15.937856: step 6115, loss 0.0316616, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:17.097099: step 6116, loss 0.00827971, acc 1, learning_rate 0.0001
2017-10-03T00:28:18.260409: step 6117, loss 0.0432138, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:19.414425: step 6118, loss 0.044315, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:20.558457: step 6119, loss 0.0138429, acc 1, learning_rate 0.0001
2017-10-03T00:28:21.718902: step 6120, loss 0.0024823, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:28:22.062941: step 6120, loss 1.45948, acc 0.477698

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6120

2017-10-03T00:28:30.734111: step 6121, loss 0.00295256, acc 1, learning_rate 0.0001
2017-10-03T00:28:31.894753: step 6122, loss 0.0562866, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:33.043148: step 6123, loss 0.029062, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:34.196247: step 6124, loss 0.0192862, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:35.368270: step 6125, loss 0.00593442, acc 1, learning_rate 0.0001
2017-10-03T00:28:36.525345: step 6126, loss 0.00358446, acc 1, learning_rate 0.0001
2017-10-03T00:28:37.688142: step 6127, loss 0.0163963, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:39.060754: step 6128, loss 0.00283183, acc 1, learning_rate 0.0001
2017-10-03T00:28:40.213812: step 6129, loss 0.00422351, acc 1, learning_rate 0.0001
2017-10-03T00:28:41.363185: step 6130, loss 0.00468873, acc 1, learning_rate 0.0001
2017-10-03T00:28:42.518671: step 6131, loss 0.0222267, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:43.675792: step 6132, loss 0.00548939, acc 1, learning_rate 0.0001
2017-10-03T00:28:44.827906: step 6133, loss 0.0428492, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:45.987232: step 6134, loss 0.001207, acc 1, learning_rate 0.0001
2017-10-03T00:28:47.137651: step 6135, loss 0.0197694, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:48.293983: step 6136, loss 0.00335341, acc 1, learning_rate 0.0001
2017-10-03T00:28:49.443305: step 6137, loss 0.0082655, acc 1, learning_rate 0.0001
2017-10-03T00:28:50.606652: step 6138, loss 0.00413999, acc 1, learning_rate 0.0001
2017-10-03T00:28:51.778967: step 6139, loss 0.0256801, acc 0.984375, learning_rate 0.0001
2017-10-03T00:28:52.920716: step 6140, loss 0.00279135, acc 1, learning_rate 0.0001
2017-10-03T00:28:54.068021: step 6141, loss 0.002668, acc 1, learning_rate 0.0001
2017-10-03T00:28:55.219471: step 6142, loss 0.00209793, acc 1, learning_rate 0.0001
2017-10-03T00:28:56.374619: step 6143, loss 0.00495203, acc 1, learning_rate 0.0001
2017-10-03T00:28:57.527805: step 6144, loss 0.00233582, acc 1, learning_rate 0.0001
2017-10-03T00:28:58.672760: step 6145, loss 0.00193765, acc 1, learning_rate 0.0001
2017-10-03T00:28:59.820287: step 6146, loss 0.00294622, acc 1, learning_rate 0.0001
2017-10-03T00:29:00.990910: step 6147, loss 0.00179637, acc 1, learning_rate 0.0001
2017-10-03T00:29:02.130823: step 6148, loss 0.00471106, acc 1, learning_rate 0.0001
2017-10-03T00:29:03.299237: step 6149, loss 0.0034982, acc 1, learning_rate 0.0001
2017-10-03T00:29:04.465174: step 6150, loss 0.00275709, acc 1, learning_rate 0.0001
2017-10-03T00:29:05.620531: step 6151, loss 0.00426927, acc 1, learning_rate 0.0001
2017-10-03T00:29:06.773915: step 6152, loss 0.0212352, acc 0.984375, learning_rate 0.0001
2017-10-03T00:29:07.937252: step 6153, loss 0.00524582, acc 1, learning_rate 0.0001
2017-10-03T00:29:09.101134: step 6154, loss 0.00376638, acc 1, learning_rate 0.0001
2017-10-03T00:29:10.256050: step 6155, loss 0.00236392, acc 1, learning_rate 0.0001
2017-10-03T00:29:11.397206: step 6156, loss 0.00428718, acc 1, learning_rate 0.0001
2017-10-03T00:29:12.555761: step 6157, loss 0.00666517, acc 1, learning_rate 0.0001
2017-10-03T00:29:13.719484: step 6158, loss 0.0217599, acc 0.984375, learning_rate 0.0001
2017-10-03T00:29:14.881545: step 6159, loss 0.0533131, acc 0.96875, learning_rate 0.0001
2017-10-03T00:29:16.043439: step 6160, loss 0.00225378, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:29:16.377139: step 6160, loss 1.44459, acc 0.490647

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6160

2017-10-03T00:29:24.057050: step 6161, loss 0.00668178, acc 1, learning_rate 0.0001
2017-10-03T00:29:25.226589: step 6162, loss 0.00414012, acc 1, learning_rate 0.0001
2017-10-03T00:29:26.382942: step 6163, loss 0.00672037, acc 1, learning_rate 0.0001
2017-10-03T00:29:27.545002: step 6164, loss 0.0050352, acc 1, learning_rate 0.0001
2017-10-03T00:29:28.678869: step 6165, loss 0.0222827, acc 0.984375, learning_rate 0.0001
2017-10-03T00:29:29.851839: step 6166, loss 0.00255836, acc 1, learning_rate 0.0001
2017-10-03T00:29:31.000494: step 6167, loss 0.00394102, acc 1, learning_rate 0.0001
2017-10-03T00:29:32.155743: step 6168, loss 0.0127629, acc 1, learning_rate 0.0001
2017-10-03T00:29:33.309106: step 6169, loss 0.00275085, acc 1, learning_rate 0.0001
2017-10-03T00:29:34.456365: step 6170, loss 0.064331, acc 0.984375, learning_rate 0.0001
2017-10-03T00:29:35.621315: step 6171, loss 0.00207621, acc 1, learning_rate 0.0001
2017-10-03T00:29:36.768570: step 6172, loss 0.00496187, acc 1, learning_rate 0.0001
2017-10-03T00:29:37.919097: step 6173, loss 0.00103578, acc 1, learning_rate 0.0001
2017-10-03T00:29:39.047821: step 6174, loss 0.0209876, acc 0.980392, learning_rate 0.0001
2017-10-03T00:29:40.210550: step 6175, loss 0.0541381, acc 0.96875, learning_rate 0.0001
2017-10-03T00:29:41.373106: step 6176, loss 0.00302067, acc 1, learning_rate 0.0001
2017-10-03T00:29:42.602529: step 6177, loss 0.00579873, acc 1, learning_rate 0.0001
2017-10-03T00:29:43.748203: step 6178, loss 0.00353817, acc 1, learning_rate 0.0001
2017-10-03T00:29:44.889710: step 6179, loss 0.00199864, acc 1, learning_rate 0.0001
2017-10-03T00:29:46.031924: step 6180, loss 0.0163027, acc 0.984375, learning_rate 0.0001
2017-10-03T00:29:47.175408: step 6181, loss 0.00290815, acc 1, learning_rate 0.0001
2017-10-03T00:29:48.325096: step 6182, loss 0.00470738, acc 1, learning_rate 0.0001
2017-10-03T00:29:49.518218: step 6183, loss 0.00619845, acc 1, learning_rate 0.0001
2017-10-03T00:29:50.670641: step 6184, loss 0.0367872, acc 0.984375, learning_rate 0.0001
2017-10-03T00:29:51.825391: step 6185, loss 0.00408067, acc 1, learning_rate 0.0001
2017-10-03T00:29:52.989573: step 6186, loss 0.0518758, acc 0.96875, learning_rate 0.0001
2017-10-03T00:29:54.156589: step 6187, loss 0.0158559, acc 0.984375, learning_rate 0.0001
2017-10-03T00:29:55.302636: step 6188, loss 0.0127675, acc 1, learning_rate 0.0001
2017-10-03T00:29:56.457726: step 6189, loss 0.0264132, acc 0.984375, learning_rate 0.0001
2017-10-03T00:29:57.627895: step 6190, loss 0.00960144, acc 1, learning_rate 0.0001
2017-10-03T00:29:58.848022: step 6191, loss 0.00287192, acc 1, learning_rate 0.0001
2017-10-03T00:29:59.996191: step 6192, loss 0.00814322, acc 1, learning_rate 0.0001
2017-10-03T00:30:01.146800: step 6193, loss 0.0121545, acc 1, learning_rate 0.0001
2017-10-03T00:30:02.307811: step 6194, loss 0.00495833, acc 1, learning_rate 0.0001
2017-10-03T00:30:03.450795: step 6195, loss 0.00333672, acc 1, learning_rate 0.0001
2017-10-03T00:30:04.616068: step 6196, loss 0.0462661, acc 0.96875, learning_rate 0.0001
2017-10-03T00:30:05.791155: step 6197, loss 0.00868751, acc 1, learning_rate 0.0001
2017-10-03T00:30:06.938298: step 6198, loss 0.00379752, acc 1, learning_rate 0.0001
2017-10-03T00:30:08.078482: step 6199, loss 0.0197027, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:09.221006: step 6200, loss 0.0019543, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:30:09.545188: step 6200, loss 1.49051, acc 0.417266

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6200

2017-10-03T00:30:17.970849: step 6201, loss 0.034733, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:19.155517: step 6202, loss 0.00455805, acc 1, learning_rate 0.0001
2017-10-03T00:30:20.332499: step 6203, loss 0.0374486, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:21.498809: step 6204, loss 0.00347206, acc 1, learning_rate 0.0001
2017-10-03T00:30:22.638491: step 6205, loss 0.0112306, acc 1, learning_rate 0.0001
2017-10-03T00:30:23.787295: step 6206, loss 0.0131168, acc 1, learning_rate 0.0001
2017-10-03T00:30:24.951077: step 6207, loss 0.00396272, acc 1, learning_rate 0.0001
2017-10-03T00:30:26.131742: step 6208, loss 0.00695298, acc 1, learning_rate 0.0001
2017-10-03T00:30:27.279876: step 6209, loss 0.0353224, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:28.416791: step 6210, loss 0.0046016, acc 1, learning_rate 0.0001
2017-10-03T00:30:29.587557: step 6211, loss 0.00921044, acc 1, learning_rate 0.0001
2017-10-03T00:30:30.746663: step 6212, loss 0.00242183, acc 1, learning_rate 0.0001
2017-10-03T00:30:31.893431: step 6213, loss 0.0228842, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:33.068212: step 6214, loss 0.0226221, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:34.222439: step 6215, loss 0.0228433, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:35.382358: step 6216, loss 0.0227337, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:36.555604: step 6217, loss 0.00640085, acc 1, learning_rate 0.0001
2017-10-03T00:30:37.704670: step 6218, loss 0.00355866, acc 1, learning_rate 0.0001
2017-10-03T00:30:38.851502: step 6219, loss 0.0166033, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:39.998427: step 6220, loss 0.00204995, acc 1, learning_rate 0.0001
2017-10-03T00:30:41.143683: step 6221, loss 0.00398177, acc 1, learning_rate 0.0001
2017-10-03T00:30:42.305897: step 6222, loss 0.00405815, acc 1, learning_rate 0.0001
2017-10-03T00:30:43.481432: step 6223, loss 0.00316505, acc 1, learning_rate 0.0001
2017-10-03T00:30:44.640910: step 6224, loss 0.00350081, acc 1, learning_rate 0.0001
2017-10-03T00:30:45.789931: step 6225, loss 0.00280787, acc 1, learning_rate 0.0001
2017-10-03T00:30:46.974551: step 6226, loss 0.0017929, acc 1, learning_rate 0.0001
2017-10-03T00:30:48.123850: step 6227, loss 0.00190711, acc 1, learning_rate 0.0001
2017-10-03T00:30:49.301059: step 6228, loss 0.00419609, acc 1, learning_rate 0.0001
2017-10-03T00:30:50.474151: step 6229, loss 0.00648787, acc 1, learning_rate 0.0001
2017-10-03T00:30:51.633005: step 6230, loss 0.00907563, acc 1, learning_rate 0.0001
2017-10-03T00:30:52.812051: step 6231, loss 0.00312908, acc 1, learning_rate 0.0001
2017-10-03T00:30:53.971527: step 6232, loss 0.0103784, acc 1, learning_rate 0.0001
2017-10-03T00:30:55.144599: step 6233, loss 0.00373218, acc 1, learning_rate 0.0001
2017-10-03T00:30:56.287900: step 6234, loss 0.0087172, acc 1, learning_rate 0.0001
2017-10-03T00:30:57.447893: step 6235, loss 0.0255295, acc 0.984375, learning_rate 0.0001
2017-10-03T00:30:58.602539: step 6236, loss 0.00157931, acc 1, learning_rate 0.0001
2017-10-03T00:30:59.757183: step 6237, loss 0.0485128, acc 0.96875, learning_rate 0.0001
2017-10-03T00:31:00.920632: step 6238, loss 0.00324739, acc 1, learning_rate 0.0001
2017-10-03T00:31:02.076513: step 6239, loss 0.00289679, acc 1, learning_rate 0.0001
2017-10-03T00:31:03.225491: step 6240, loss 0.00382504, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:31:03.568446: step 6240, loss 1.491, acc 0.392806

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6240

2017-10-03T00:31:11.331684: step 6241, loss 0.00521358, acc 1, learning_rate 0.0001
2017-10-03T00:31:12.476799: step 6242, loss 0.00273458, acc 1, learning_rate 0.0001
2017-10-03T00:31:13.625334: step 6243, loss 0.0194412, acc 0.984375, learning_rate 0.0001
2017-10-03T00:31:14.755257: step 6244, loss 0.00318347, acc 1, learning_rate 0.0001
2017-10-03T00:31:15.917961: step 6245, loss 0.00162475, acc 1, learning_rate 0.0001
2017-10-03T00:31:17.063484: step 6246, loss 0.00979815, acc 1, learning_rate 0.0001
2017-10-03T00:31:18.214001: step 6247, loss 0.0338847, acc 0.984375, learning_rate 0.0001
2017-10-03T00:31:19.372627: step 6248, loss 0.00438533, acc 1, learning_rate 0.0001
2017-10-03T00:31:20.531019: step 6249, loss 0.0185275, acc 0.984375, learning_rate 0.0001
2017-10-03T00:31:21.675136: step 6250, loss 0.0107214, acc 1, learning_rate 0.0001
2017-10-03T00:31:22.837045: step 6251, loss 0.00463128, acc 1, learning_rate 0.0001
2017-10-03T00:31:24.007750: step 6252, loss 0.00806947, acc 1, learning_rate 0.0001
2017-10-03T00:31:25.154854: step 6253, loss 0.0744844, acc 0.96875, learning_rate 0.0001
2017-10-03T00:31:26.310683: step 6254, loss 0.0123986, acc 1, learning_rate 0.0001
2017-10-03T00:31:27.465610: step 6255, loss 0.00441292, acc 1, learning_rate 0.0001
2017-10-03T00:31:28.625634: step 6256, loss 0.00478141, acc 1, learning_rate 0.0001
2017-10-03T00:31:29.766867: step 6257, loss 0.00372565, acc 1, learning_rate 0.0001
2017-10-03T00:31:30.925733: step 6258, loss 0.0106625, acc 1, learning_rate 0.0001
2017-10-03T00:31:32.079595: step 6259, loss 0.0211873, acc 0.984375, learning_rate 0.0001
2017-10-03T00:31:33.246027: step 6260, loss 0.00489268, acc 1, learning_rate 0.0001
2017-10-03T00:31:34.396695: step 6261, loss 0.00508658, acc 1, learning_rate 0.0001
2017-10-03T00:31:35.550693: step 6262, loss 0.0024908, acc 1, learning_rate 0.0001
2017-10-03T00:31:36.698702: step 6263, loss 0.00317986, acc 1, learning_rate 0.0001
2017-10-03T00:31:37.848889: step 6264, loss 0.00240766, acc 1, learning_rate 0.0001
2017-10-03T00:31:39.031270: step 6265, loss 0.00144145, acc 1, learning_rate 0.0001
2017-10-03T00:31:40.174723: step 6266, loss 0.0348829, acc 0.984375, learning_rate 0.0001
2017-10-03T00:31:41.335402: step 6267, loss 0.00343345, acc 1, learning_rate 0.0001
2017-10-03T00:31:42.475141: step 6268, loss 0.00409966, acc 1, learning_rate 0.0001
2017-10-03T00:31:43.635799: step 6269, loss 0.00753386, acc 1, learning_rate 0.0001
2017-10-03T00:31:44.808240: step 6270, loss 0.0107624, acc 1, learning_rate 0.0001
2017-10-03T00:31:45.977864: step 6271, loss 0.00192036, acc 1, learning_rate 0.0001
2017-10-03T00:31:47.110699: step 6272, loss 0.0117106, acc 1, learning_rate 0.0001
2017-10-03T00:31:48.275754: step 6273, loss 0.00228706, acc 1, learning_rate 0.0001
2017-10-03T00:31:49.429439: step 6274, loss 0.00418232, acc 1, learning_rate 0.0001
2017-10-03T00:31:50.575803: step 6275, loss 0.00295542, acc 1, learning_rate 0.0001
2017-10-03T00:31:51.729496: step 6276, loss 0.025575, acc 0.984375, learning_rate 0.0001
2017-10-03T00:31:52.894304: step 6277, loss 0.0126001, acc 1, learning_rate 0.0001
2017-10-03T00:31:54.046630: step 6278, loss 0.00245922, acc 1, learning_rate 0.0001
2017-10-03T00:31:55.211481: step 6279, loss 0.0100265, acc 1, learning_rate 0.0001
2017-10-03T00:31:56.386947: step 6280, loss 0.00456635, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:31:56.726117: step 6280, loss 1.44727, acc 0.480576

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6280

2017-10-03T00:32:04.505506: step 6281, loss 0.00659878, acc 1, learning_rate 0.0001
2017-10-03T00:32:05.692332: step 6282, loss 0.05731, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:06.846303: step 6283, loss 0.0052426, acc 1, learning_rate 0.0001
2017-10-03T00:32:08.012463: step 6284, loss 0.0271121, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:09.167257: step 6285, loss 0.00302101, acc 1, learning_rate 0.0001
2017-10-03T00:32:10.409258: step 6286, loss 0.00407854, acc 1, learning_rate 0.0001
2017-10-03T00:32:11.554777: step 6287, loss 0.00459051, acc 1, learning_rate 0.0001
2017-10-03T00:32:12.721662: step 6288, loss 0.00252899, acc 1, learning_rate 0.0001
2017-10-03T00:32:13.862469: step 6289, loss 0.00468172, acc 1, learning_rate 0.0001
2017-10-03T00:32:14.998577: step 6290, loss 0.0177851, acc 1, learning_rate 0.0001
2017-10-03T00:32:16.163928: step 6291, loss 0.0320877, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:17.317501: step 6292, loss 0.0344263, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:18.554305: step 6293, loss 0.0242488, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:19.703009: step 6294, loss 0.0126987, acc 1, learning_rate 0.0001
2017-10-03T00:32:20.849213: step 6295, loss 0.051842, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:22.002754: step 6296, loss 0.0040206, acc 1, learning_rate 0.0001
2017-10-03T00:32:23.149856: step 6297, loss 0.00410698, acc 1, learning_rate 0.0001
2017-10-03T00:32:24.303242: step 6298, loss 0.0028208, acc 1, learning_rate 0.0001
2017-10-03T00:32:25.454867: step 6299, loss 0.024542, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:26.615063: step 6300, loss 0.0182899, acc 1, learning_rate 0.0001
2017-10-03T00:32:27.776396: step 6301, loss 0.00251465, acc 1, learning_rate 0.0001
2017-10-03T00:32:28.943243: step 6302, loss 0.0025928, acc 1, learning_rate 0.0001
2017-10-03T00:32:30.114406: step 6303, loss 0.0040111, acc 1, learning_rate 0.0001
2017-10-03T00:32:31.284512: step 6304, loss 0.00684875, acc 1, learning_rate 0.0001
2017-10-03T00:32:32.449115: step 6305, loss 0.0160717, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:33.612802: step 6306, loss 0.0037186, acc 1, learning_rate 0.0001
2017-10-03T00:32:34.781761: step 6307, loss 0.0398171, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:35.933834: step 6308, loss 0.0169112, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:37.076166: step 6309, loss 0.00281828, acc 1, learning_rate 0.0001
2017-10-03T00:32:38.224979: step 6310, loss 0.00206231, acc 1, learning_rate 0.0001
2017-10-03T00:32:39.371848: step 6311, loss 0.0696939, acc 0.96875, learning_rate 0.0001
2017-10-03T00:32:40.528704: step 6312, loss 0.0132926, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:41.690315: step 6313, loss 0.00231663, acc 1, learning_rate 0.0001
2017-10-03T00:32:42.852309: step 6314, loss 0.00316361, acc 1, learning_rate 0.0001
2017-10-03T00:32:44.209188: step 6315, loss 0.00177781, acc 1, learning_rate 0.0001
2017-10-03T00:32:45.364015: step 6316, loss 0.0108459, acc 1, learning_rate 0.0001
2017-10-03T00:32:46.559627: step 6317, loss 0.0194524, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:47.709058: step 6318, loss 0.0181186, acc 0.984375, learning_rate 0.0001
2017-10-03T00:32:48.858797: step 6319, loss 0.00272002, acc 1, learning_rate 0.0001
2017-10-03T00:32:50.008667: step 6320, loss 0.00293554, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:32:50.332995: step 6320, loss 1.45536, acc 0.470504

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6320

2017-10-03T00:32:58.754356: step 6321, loss 0.00357686, acc 1, learning_rate 0.0001
2017-10-03T00:32:59.915426: step 6322, loss 0.00361214, acc 1, learning_rate 0.0001
2017-10-03T00:33:01.071867: step 6323, loss 0.00330631, acc 1, learning_rate 0.0001
2017-10-03T00:33:02.223937: step 6324, loss 0.00164499, acc 1, learning_rate 0.0001
2017-10-03T00:33:03.388181: step 6325, loss 0.00363488, acc 1, learning_rate 0.0001
2017-10-03T00:33:04.530542: step 6326, loss 0.00250596, acc 1, learning_rate 0.0001
2017-10-03T00:33:05.686328: step 6327, loss 0.00399017, acc 1, learning_rate 0.0001
2017-10-03T00:33:06.826701: step 6328, loss 0.0031484, acc 1, learning_rate 0.0001
2017-10-03T00:33:07.971644: step 6329, loss 0.0112944, acc 1, learning_rate 0.0001
2017-10-03T00:33:09.124605: step 6330, loss 0.00422924, acc 1, learning_rate 0.0001
2017-10-03T00:33:10.297154: step 6331, loss 0.0359888, acc 0.984375, learning_rate 0.0001
2017-10-03T00:33:11.447332: step 6332, loss 0.00405043, acc 1, learning_rate 0.0001
2017-10-03T00:33:12.609033: step 6333, loss 0.00286707, acc 1, learning_rate 0.0001
2017-10-03T00:33:13.751315: step 6334, loss 0.0242884, acc 0.984375, learning_rate 0.0001
2017-10-03T00:33:14.918787: step 6335, loss 0.00251633, acc 1, learning_rate 0.0001
2017-10-03T00:33:16.067293: step 6336, loss 0.00185387, acc 1, learning_rate 0.0001
2017-10-03T00:33:17.222202: step 6337, loss 0.00163909, acc 1, learning_rate 0.0001
2017-10-03T00:33:18.391963: step 6338, loss 0.00324383, acc 1, learning_rate 0.0001
2017-10-03T00:33:20.253630: step 6339, loss 0.00245979, acc 1, learning_rate 0.0001
2017-10-03T00:33:21.402868: step 6340, loss 0.00155149, acc 1, learning_rate 0.0001
2017-10-03T00:33:22.623616: step 6341, loss 0.00410689, acc 1, learning_rate 0.0001
2017-10-03T00:33:23.772187: step 6342, loss 0.00393008, acc 1, learning_rate 0.0001
2017-10-03T00:33:24.959772: step 6343, loss 0.028302, acc 0.984375, learning_rate 0.0001
2017-10-03T00:33:26.112982: step 6344, loss 0.0167847, acc 0.984375, learning_rate 0.0001
2017-10-03T00:33:27.270009: step 6345, loss 0.00200577, acc 1, learning_rate 0.0001
2017-10-03T00:33:28.404088: step 6346, loss 0.0458301, acc 0.984375, learning_rate 0.0001
2017-10-03T00:33:29.548272: step 6347, loss 0.0091985, acc 1, learning_rate 0.0001
2017-10-03T00:33:30.707913: step 6348, loss 0.00404096, acc 1, learning_rate 0.0001
2017-10-03T00:33:31.859326: step 6349, loss 0.00294675, acc 1, learning_rate 0.0001
2017-10-03T00:33:33.008089: step 6350, loss 0.0138841, acc 1, learning_rate 0.0001
2017-10-03T00:33:34.161267: step 6351, loss 0.016956, acc 1, learning_rate 0.0001
2017-10-03T00:33:35.474293: step 6352, loss 0.0473127, acc 0.984375, learning_rate 0.0001
2017-10-03T00:33:36.643426: step 6353, loss 0.00280676, acc 1, learning_rate 0.0001
2017-10-03T00:33:37.804105: step 6354, loss 0.00351864, acc 1, learning_rate 0.0001
2017-10-03T00:33:38.967185: step 6355, loss 0.026648, acc 0.984375, learning_rate 0.0001
2017-10-03T00:33:40.120772: step 6356, loss 0.013666, acc 1, learning_rate 0.0001
2017-10-03T00:33:41.282314: step 6357, loss 0.00226975, acc 1, learning_rate 0.0001
2017-10-03T00:33:42.448429: step 6358, loss 0.0413969, acc 0.96875, learning_rate 0.0001
2017-10-03T00:33:43.615161: step 6359, loss 0.00353406, acc 1, learning_rate 0.0001
2017-10-03T00:33:44.758737: step 6360, loss 0.0247586, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:33:45.107375: step 6360, loss 1.44522, acc 0.479137

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6360

2017-10-03T00:33:53.667287: step 6361, loss 0.00166613, acc 1, learning_rate 0.0001
2017-10-03T00:33:54.837252: step 6362, loss 0.00879531, acc 1, learning_rate 0.0001
2017-10-03T00:33:55.979585: step 6363, loss 0.00202987, acc 1, learning_rate 0.0001
2017-10-03T00:33:57.146797: step 6364, loss 0.00240721, acc 1, learning_rate 0.0001
2017-10-03T00:33:58.303864: step 6365, loss 0.0118457, acc 1, learning_rate 0.0001
2017-10-03T00:33:59.463576: step 6366, loss 0.00313232, acc 1, learning_rate 0.0001
2017-10-03T00:34:00.634815: step 6367, loss 0.00292028, acc 1, learning_rate 0.0001
2017-10-03T00:34:01.775422: step 6368, loss 0.00329362, acc 1, learning_rate 0.0001
2017-10-03T00:34:02.917285: step 6369, loss 0.0333708, acc 0.984375, learning_rate 0.0001
2017-10-03T00:34:04.074366: step 6370, loss 0.00278852, acc 1, learning_rate 0.0001
2017-10-03T00:34:05.215953: step 6371, loss 0.0357635, acc 0.984375, learning_rate 0.0001
2017-10-03T00:34:06.378340: step 6372, loss 0.0486228, acc 0.984375, learning_rate 0.0001
2017-10-03T00:34:07.549255: step 6373, loss 0.0188811, acc 1, learning_rate 0.0001
2017-10-03T00:34:08.702663: step 6374, loss 0.00206498, acc 1, learning_rate 0.0001
2017-10-03T00:34:09.914596: step 6375, loss 0.0162368, acc 0.984375, learning_rate 0.0001
2017-10-03T00:34:11.052512: step 6376, loss 0.00129799, acc 1, learning_rate 0.0001
2017-10-03T00:34:12.204660: step 6377, loss 0.00737997, acc 1, learning_rate 0.0001
2017-10-03T00:34:13.365465: step 6378, loss 0.00451279, acc 1, learning_rate 0.0001
2017-10-03T00:34:14.545284: step 6379, loss 0.00188986, acc 1, learning_rate 0.0001
2017-10-03T00:34:15.695125: step 6380, loss 0.0085533, acc 1, learning_rate 0.0001
2017-10-03T00:34:16.856722: step 6381, loss 0.00279205, acc 1, learning_rate 0.0001
2017-10-03T00:34:18.018188: step 6382, loss 0.00265503, acc 1, learning_rate 0.0001
2017-10-03T00:34:19.165118: step 6383, loss 0.0025505, acc 1, learning_rate 0.0001
2017-10-03T00:34:20.333130: step 6384, loss 0.00214064, acc 1, learning_rate 0.0001
2017-10-03T00:34:21.495852: step 6385, loss 0.0266232, acc 0.96875, learning_rate 0.0001
2017-10-03T00:34:22.649148: step 6386, loss 0.00676077, acc 1, learning_rate 0.0001
2017-10-03T00:34:23.804042: step 6387, loss 0.0124671, acc 1, learning_rate 0.0001
2017-10-03T00:34:24.958138: step 6388, loss 0.00535595, acc 1, learning_rate 0.0001
2017-10-03T00:34:26.116843: step 6389, loss 0.00254191, acc 1, learning_rate 0.0001
2017-10-03T00:34:27.260826: step 6390, loss 0.0120783, acc 1, learning_rate 0.0001
2017-10-03T00:34:28.423324: step 6391, loss 0.0367583, acc 0.984375, learning_rate 0.0001
2017-10-03T00:34:29.603141: step 6392, loss 0.0303086, acc 0.984375, learning_rate 0.0001
2017-10-03T00:34:30.764326: step 6393, loss 0.00233364, acc 1, learning_rate 0.0001
2017-10-03T00:34:31.918534: step 6394, loss 0.0147403, acc 1, learning_rate 0.0001
2017-10-03T00:34:33.077847: step 6395, loss 0.0034651, acc 1, learning_rate 0.0001
2017-10-03T00:34:34.227009: step 6396, loss 0.00271215, acc 1, learning_rate 0.0001
2017-10-03T00:34:35.382987: step 6397, loss 0.00298961, acc 1, learning_rate 0.0001
2017-10-03T00:34:36.546491: step 6398, loss 0.0149912, acc 1, learning_rate 0.0001
2017-10-03T00:34:37.706435: step 6399, loss 0.00366923, acc 1, learning_rate 0.0001
2017-10-03T00:34:39.106230: step 6400, loss 0.0496168, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:34:39.451433: step 6400, loss 1.45301, acc 0.470504

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6400

2017-10-03T00:34:47.524473: step 6401, loss 0.00826474, acc 1, learning_rate 0.0001
2017-10-03T00:34:48.689822: step 6402, loss 0.00499868, acc 1, learning_rate 0.0001
2017-10-03T00:34:49.856950: step 6403, loss 0.00159621, acc 1, learning_rate 0.0001
2017-10-03T00:34:51.005883: step 6404, loss 0.00130338, acc 1, learning_rate 0.0001
2017-10-03T00:34:52.149932: step 6405, loss 0.00146261, acc 1, learning_rate 0.0001
2017-10-03T00:34:53.298633: step 6406, loss 0.00126526, acc 1, learning_rate 0.0001
2017-10-03T00:34:54.506091: step 6407, loss 0.0337664, acc 0.984375, learning_rate 0.0001
2017-10-03T00:34:55.674598: step 6408, loss 0.0349502, acc 0.984375, learning_rate 0.0001
2017-10-03T00:34:56.834943: step 6409, loss 0.00445966, acc 1, learning_rate 0.0001
2017-10-03T00:34:57.995919: step 6410, loss 0.00694656, acc 1, learning_rate 0.0001
2017-10-03T00:34:59.159326: step 6411, loss 0.0118458, acc 1, learning_rate 0.0001
2017-10-03T00:35:00.310341: step 6412, loss 0.0113072, acc 1, learning_rate 0.0001
2017-10-03T00:35:08.432058: step 6413, loss 0.00368474, acc 1, learning_rate 0.0001
2017-10-03T00:35:09.586403: step 6414, loss 0.000942673, acc 1, learning_rate 0.0001
2017-10-03T00:35:10.736862: step 6415, loss 0.012579, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:11.929844: step 6416, loss 0.00579709, acc 1, learning_rate 0.0001
2017-10-03T00:35:13.099966: step 6417, loss 0.00449671, acc 1, learning_rate 0.0001
2017-10-03T00:35:14.283257: step 6418, loss 0.0377306, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:15.449693: step 6419, loss 0.0141854, acc 1, learning_rate 0.0001
2017-10-03T00:35:16.598989: step 6420, loss 0.00256346, acc 1, learning_rate 0.0001
2017-10-03T00:35:17.760972: step 6421, loss 0.0443986, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:18.921557: step 6422, loss 0.00194456, acc 1, learning_rate 0.0001
2017-10-03T00:35:20.103446: step 6423, loss 0.00497008, acc 1, learning_rate 0.0001
2017-10-03T00:35:21.272465: step 6424, loss 0.00369499, acc 1, learning_rate 0.0001
2017-10-03T00:35:22.529047: step 6425, loss 0.00144057, acc 1, learning_rate 0.0001
2017-10-03T00:35:23.685510: step 6426, loss 0.00371921, acc 1, learning_rate 0.0001
2017-10-03T00:35:24.847750: step 6427, loss 0.00265, acc 1, learning_rate 0.0001
2017-10-03T00:35:26.007804: step 6428, loss 0.0129185, acc 1, learning_rate 0.0001
2017-10-03T00:35:27.179674: step 6429, loss 0.0207119, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:28.363800: step 6430, loss 0.00634213, acc 1, learning_rate 0.0001
2017-10-03T00:35:29.527641: step 6431, loss 0.00449067, acc 1, learning_rate 0.0001
2017-10-03T00:35:30.678629: step 6432, loss 0.0257302, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:31.839583: step 6433, loss 0.00277685, acc 1, learning_rate 0.0001
2017-10-03T00:35:32.999259: step 6434, loss 0.0273412, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:34.157782: step 6435, loss 0.00232145, acc 1, learning_rate 0.0001
2017-10-03T00:35:35.323787: step 6436, loss 0.0151133, acc 1, learning_rate 0.0001
2017-10-03T00:35:36.484428: step 6437, loss 0.00396219, acc 1, learning_rate 0.0001
2017-10-03T00:35:37.651247: step 6438, loss 0.00378381, acc 1, learning_rate 0.0001
2017-10-03T00:35:38.814370: step 6439, loss 0.0336773, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:39.975249: step 6440, loss 0.00240749, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:35:40.304352: step 6440, loss 1.45576, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6440

2017-10-03T00:35:47.940647: step 6441, loss 0.0126287, acc 1, learning_rate 0.0001
2017-10-03T00:35:49.083087: step 6442, loss 0.00452092, acc 1, learning_rate 0.0001
2017-10-03T00:35:50.250449: step 6443, loss 0.0198272, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:51.513290: step 6444, loss 0.00449089, acc 1, learning_rate 0.0001
2017-10-03T00:35:52.690766: step 6445, loss 0.0983539, acc 0.96875, learning_rate 0.0001
2017-10-03T00:35:53.843654: step 6446, loss 0.00182998, acc 1, learning_rate 0.0001
2017-10-03T00:35:55.017803: step 6447, loss 0.00361323, acc 1, learning_rate 0.0001
2017-10-03T00:35:56.178531: step 6448, loss 0.0480583, acc 0.984375, learning_rate 0.0001
2017-10-03T00:35:57.357227: step 6449, loss 0.00534474, acc 1, learning_rate 0.0001
2017-10-03T00:35:58.536857: step 6450, loss 0.0119853, acc 1, learning_rate 0.0001
2017-10-03T00:35:59.700290: step 6451, loss 0.00525674, acc 1, learning_rate 0.0001
2017-10-03T00:36:00.860081: step 6452, loss 0.0317577, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:02.016537: step 6453, loss 0.00427363, acc 1, learning_rate 0.0001
2017-10-03T00:36:03.169530: step 6454, loss 0.00391135, acc 1, learning_rate 0.0001
2017-10-03T00:36:04.324182: step 6455, loss 0.00395873, acc 1, learning_rate 0.0001
2017-10-03T00:36:05.485190: step 6456, loss 0.00347748, acc 1, learning_rate 0.0001
2017-10-03T00:36:06.643101: step 6457, loss 0.00224895, acc 1, learning_rate 0.0001
2017-10-03T00:36:07.813262: step 6458, loss 0.00310419, acc 1, learning_rate 0.0001
2017-10-03T00:36:08.990632: step 6459, loss 0.00309794, acc 1, learning_rate 0.0001
2017-10-03T00:36:10.143408: step 6460, loss 0.017814, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:11.324300: step 6461, loss 0.00288841, acc 1, learning_rate 0.0001
2017-10-03T00:36:12.493586: step 6462, loss 0.0210576, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:13.670625: step 6463, loss 0.00340054, acc 1, learning_rate 0.0001
2017-10-03T00:36:14.826586: step 6464, loss 0.00109738, acc 1, learning_rate 0.0001
2017-10-03T00:36:15.985593: step 6465, loss 0.0028189, acc 1, learning_rate 0.0001
2017-10-03T00:36:17.145116: step 6466, loss 0.00572397, acc 1, learning_rate 0.0001
2017-10-03T00:36:18.320829: step 6467, loss 0.0234793, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:19.468023: step 6468, loss 0.00555874, acc 1, learning_rate 0.0001
2017-10-03T00:36:20.627505: step 6469, loss 0.00603885, acc 1, learning_rate 0.0001
2017-10-03T00:36:21.791762: step 6470, loss 0.00392689, acc 1, learning_rate 0.0001
2017-10-03T00:36:22.949425: step 6471, loss 0.02425, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:24.115645: step 6472, loss 0.0425964, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:25.279473: step 6473, loss 0.00183411, acc 1, learning_rate 0.0001
2017-10-03T00:36:26.444459: step 6474, loss 0.00763964, acc 1, learning_rate 0.0001
2017-10-03T00:36:27.626464: step 6475, loss 0.00300087, acc 1, learning_rate 0.0001
2017-10-03T00:36:28.788156: step 6476, loss 0.00169214, acc 1, learning_rate 0.0001
2017-10-03T00:36:29.932600: step 6477, loss 0.0453701, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:31.094465: step 6478, loss 0.0209267, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:32.267677: step 6479, loss 0.0203079, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:33.456789: step 6480, loss 0.00202372, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:36:33.798259: step 6480, loss 1.45601, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6480

2017-10-03T00:36:41.895887: step 6481, loss 0.0112141, acc 1, learning_rate 0.0001
2017-10-03T00:36:43.062526: step 6482, loss 0.0262561, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:44.235966: step 6483, loss 0.00493331, acc 1, learning_rate 0.0001
2017-10-03T00:36:45.407863: step 6484, loss 0.00243916, acc 1, learning_rate 0.0001
2017-10-03T00:36:46.553827: step 6485, loss 0.00138419, acc 1, learning_rate 0.0001
2017-10-03T00:36:47.698113: step 6486, loss 0.00284655, acc 1, learning_rate 0.0001
2017-10-03T00:36:48.850887: step 6487, loss 0.0313339, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:50.003386: step 6488, loss 0.0027036, acc 1, learning_rate 0.0001
2017-10-03T00:36:51.155728: step 6489, loss 0.00951993, acc 1, learning_rate 0.0001
2017-10-03T00:36:52.338417: step 6490, loss 0.00201135, acc 1, learning_rate 0.0001
2017-10-03T00:36:53.509925: step 6491, loss 0.00364678, acc 1, learning_rate 0.0001
2017-10-03T00:36:54.825539: step 6492, loss 0.0177136, acc 0.984375, learning_rate 0.0001
2017-10-03T00:36:55.983106: step 6493, loss 0.00204982, acc 1, learning_rate 0.0001
2017-10-03T00:36:57.148144: step 6494, loss 0.00373484, acc 1, learning_rate 0.0001
2017-10-03T00:36:58.306435: step 6495, loss 0.00499262, acc 1, learning_rate 0.0001
2017-10-03T00:36:59.511512: step 6496, loss 0.0277966, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:00.693446: step 6497, loss 0.0169365, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:01.891445: step 6498, loss 0.00257144, acc 1, learning_rate 0.0001
2017-10-03T00:37:03.068146: step 6499, loss 0.028993, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:04.227743: step 6500, loss 0.0227838, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:05.398013: step 6501, loss 0.0183873, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:06.548747: step 6502, loss 0.0385988, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:07.717273: step 6503, loss 0.0584505, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:08.901330: step 6504, loss 0.0296819, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:10.056219: step 6505, loss 0.00255101, acc 1, learning_rate 0.0001
2017-10-03T00:37:11.205207: step 6506, loss 0.00727909, acc 1, learning_rate 0.0001
2017-10-03T00:37:12.376873: step 6507, loss 0.0524716, acc 0.96875, learning_rate 0.0001
2017-10-03T00:37:13.539403: step 6508, loss 0.00411372, acc 1, learning_rate 0.0001
2017-10-03T00:37:14.688858: step 6509, loss 0.0220994, acc 1, learning_rate 0.0001
2017-10-03T00:37:15.845056: step 6510, loss 0.0361217, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:16.987244: step 6511, loss 0.00802957, acc 1, learning_rate 0.0001
2017-10-03T00:37:18.144450: step 6512, loss 0.0125602, acc 1, learning_rate 0.0001
2017-10-03T00:37:19.319516: step 6513, loss 0.0127761, acc 1, learning_rate 0.0001
2017-10-03T00:37:20.464655: step 6514, loss 0.00275289, acc 1, learning_rate 0.0001
2017-10-03T00:37:21.656765: step 6515, loss 0.0318651, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:22.801357: step 6516, loss 0.00148843, acc 1, learning_rate 0.0001
2017-10-03T00:37:23.949354: step 6517, loss 0.0202647, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:25.107998: step 6518, loss 0.0129653, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:26.278718: step 6519, loss 0.00277185, acc 1, learning_rate 0.0001
2017-10-03T00:37:27.442722: step 6520, loss 0.0315247, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:37:27.768898: step 6520, loss 1.46026, acc 0.458993

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6520

2017-10-03T00:37:36.059801: step 6521, loss 0.0054303, acc 1, learning_rate 0.0001
2017-10-03T00:37:37.240503: step 6522, loss 0.00712102, acc 1, learning_rate 0.0001
2017-10-03T00:37:38.399986: step 6523, loss 0.00295029, acc 1, learning_rate 0.0001
2017-10-03T00:37:39.564467: step 6524, loss 0.00312502, acc 1, learning_rate 0.0001
2017-10-03T00:37:40.720877: step 6525, loss 0.0019383, acc 1, learning_rate 0.0001
2017-10-03T00:37:41.880851: step 6526, loss 0.0256609, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:43.045153: step 6527, loss 0.00446503, acc 1, learning_rate 0.0001
2017-10-03T00:37:44.207537: step 6528, loss 0.0216359, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:45.358921: step 6529, loss 0.0415537, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:46.513728: step 6530, loss 0.00747215, acc 1, learning_rate 0.0001
2017-10-03T00:37:47.670356: step 6531, loss 0.0026116, acc 1, learning_rate 0.0001
2017-10-03T00:37:48.821909: step 6532, loss 0.00409955, acc 1, learning_rate 0.0001
2017-10-03T00:37:49.984123: step 6533, loss 0.00436793, acc 1, learning_rate 0.0001
2017-10-03T00:37:51.147401: step 6534, loss 0.00414129, acc 1, learning_rate 0.0001
2017-10-03T00:37:52.317638: step 6535, loss 0.0182965, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:53.488399: step 6536, loss 0.0285277, acc 0.984375, learning_rate 0.0001
2017-10-03T00:37:54.640315: step 6537, loss 0.00169561, acc 1, learning_rate 0.0001
2017-10-03T00:37:55.793995: step 6538, loss 0.002326, acc 1, learning_rate 0.0001
2017-10-03T00:37:56.949702: step 6539, loss 0.00677919, acc 1, learning_rate 0.0001
2017-10-03T00:37:58.109812: step 6540, loss 0.0145052, acc 1, learning_rate 0.0001
2017-10-03T00:37:59.263719: step 6541, loss 0.00355449, acc 1, learning_rate 0.0001
2017-10-03T00:38:00.417965: step 6542, loss 0.0127837, acc 1, learning_rate 0.0001
2017-10-03T00:38:01.578565: step 6543, loss 0.00297295, acc 1, learning_rate 0.0001
2017-10-03T00:38:02.740779: step 6544, loss 0.00157488, acc 1, learning_rate 0.0001
2017-10-03T00:38:03.889874: step 6545, loss 0.00202018, acc 1, learning_rate 0.0001
2017-10-03T00:38:05.043758: step 6546, loss 0.00332524, acc 1, learning_rate 0.0001
2017-10-03T00:38:06.207881: step 6547, loss 0.0246272, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:07.359508: step 6548, loss 0.00217274, acc 1, learning_rate 0.0001
2017-10-03T00:38:08.514660: step 6549, loss 0.00412993, acc 1, learning_rate 0.0001
2017-10-03T00:38:09.677746: step 6550, loss 0.00251861, acc 1, learning_rate 0.0001
2017-10-03T00:38:10.891602: step 6551, loss 0.00134489, acc 1, learning_rate 0.0001
2017-10-03T00:38:12.047721: step 6552, loss 0.0318851, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:13.220167: step 6553, loss 0.0160337, acc 1, learning_rate 0.0001
2017-10-03T00:38:14.374210: step 6554, loss 0.00452807, acc 1, learning_rate 0.0001
2017-10-03T00:38:15.527132: step 6555, loss 0.035436, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:16.680083: step 6556, loss 0.0120896, acc 1, learning_rate 0.0001
2017-10-03T00:38:17.830795: step 6557, loss 0.00445454, acc 1, learning_rate 0.0001
2017-10-03T00:38:18.998784: step 6558, loss 0.00195393, acc 1, learning_rate 0.0001
2017-10-03T00:38:20.153469: step 6559, loss 0.0187504, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:21.309135: step 6560, loss 0.00380246, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:38:21.626362: step 6560, loss 1.47428, acc 0.428777

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6560

2017-10-03T00:38:29.598254: step 6561, loss 0.0027543, acc 1, learning_rate 0.0001
2017-10-03T00:38:30.755117: step 6562, loss 0.00457132, acc 1, learning_rate 0.0001
2017-10-03T00:38:31.900006: step 6563, loss 0.00401375, acc 1, learning_rate 0.0001
2017-10-03T00:38:33.047470: step 6564, loss 0.0040795, acc 1, learning_rate 0.0001
2017-10-03T00:38:34.194604: step 6565, loss 0.00489436, acc 1, learning_rate 0.0001
2017-10-03T00:38:35.339868: step 6566, loss 0.00328023, acc 1, learning_rate 0.0001
2017-10-03T00:38:36.495163: step 6567, loss 0.00962942, acc 1, learning_rate 0.0001
2017-10-03T00:38:37.641009: step 6568, loss 0.00984622, acc 1, learning_rate 0.0001
2017-10-03T00:38:39.026013: step 6569, loss 0.00100975, acc 1, learning_rate 0.0001
2017-10-03T00:38:40.182492: step 6570, loss 0.00242351, acc 1, learning_rate 0.0001
2017-10-03T00:38:41.333349: step 6571, loss 0.0328177, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:42.477224: step 6572, loss 0.00671754, acc 1, learning_rate 0.0001
2017-10-03T00:38:43.639134: step 6573, loss 0.00415111, acc 1, learning_rate 0.0001
2017-10-03T00:38:44.798735: step 6574, loss 0.00534647, acc 1, learning_rate 0.0001
2017-10-03T00:38:45.941773: step 6575, loss 0.0185162, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:47.109257: step 6576, loss 0.0155841, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:48.266731: step 6577, loss 0.00303746, acc 1, learning_rate 0.0001
2017-10-03T00:38:49.418940: step 6578, loss 0.00906541, acc 1, learning_rate 0.0001
2017-10-03T00:38:50.581413: step 6579, loss 0.0200206, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:51.744094: step 6580, loss 0.00281694, acc 1, learning_rate 0.0001
2017-10-03T00:38:52.904509: step 6581, loss 0.0246618, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:54.039875: step 6582, loss 0.0103483, acc 1, learning_rate 0.0001
2017-10-03T00:38:55.212698: step 6583, loss 0.043331, acc 0.96875, learning_rate 0.0001
2017-10-03T00:38:56.370578: step 6584, loss 0.00196681, acc 1, learning_rate 0.0001
2017-10-03T00:38:57.525608: step 6585, loss 0.0663844, acc 0.984375, learning_rate 0.0001
2017-10-03T00:38:58.689244: step 6586, loss 0.00271206, acc 1, learning_rate 0.0001
2017-10-03T00:38:59.853343: step 6587, loss 0.00608627, acc 1, learning_rate 0.0001
2017-10-03T00:39:01.003206: step 6588, loss 0.00341773, acc 1, learning_rate 0.0001
2017-10-03T00:39:02.157134: step 6589, loss 0.00526661, acc 1, learning_rate 0.0001
2017-10-03T00:39:03.314340: step 6590, loss 0.00514168, acc 1, learning_rate 0.0001
2017-10-03T00:39:04.472255: step 6591, loss 0.0101339, acc 1, learning_rate 0.0001
2017-10-03T00:39:05.626038: step 6592, loss 0.00254391, acc 1, learning_rate 0.0001
2017-10-03T00:39:06.785666: step 6593, loss 0.00161845, acc 1, learning_rate 0.0001
2017-10-03T00:39:07.950583: step 6594, loss 0.00802883, acc 1, learning_rate 0.0001
2017-10-03T00:39:09.116261: step 6595, loss 0.00310099, acc 1, learning_rate 0.0001
2017-10-03T00:39:10.289930: step 6596, loss 0.0013017, acc 1, learning_rate 0.0001
2017-10-03T00:39:11.448526: step 6597, loss 0.0277192, acc 0.984375, learning_rate 0.0001
2017-10-03T00:39:12.601871: step 6598, loss 0.00491883, acc 1, learning_rate 0.0001
2017-10-03T00:39:13.771868: step 6599, loss 0.0033245, acc 1, learning_rate 0.0001
2017-10-03T00:39:14.924349: step 6600, loss 0.00491957, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:39:15.246764: step 6600, loss 1.49373, acc 0.41295

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6600

2017-10-03T00:39:23.193900: step 6601, loss 0.00512782, acc 1, learning_rate 0.0001
2017-10-03T00:39:24.361436: step 6602, loss 0.00717732, acc 1, learning_rate 0.0001
2017-10-03T00:39:25.513847: step 6603, loss 0.0040314, acc 1, learning_rate 0.0001
2017-10-03T00:39:26.659268: step 6604, loss 0.0122123, acc 1, learning_rate 0.0001
2017-10-03T00:39:27.822558: step 6605, loss 0.0921186, acc 0.953125, learning_rate 0.0001
2017-10-03T00:39:28.972698: step 6606, loss 0.00350726, acc 1, learning_rate 0.0001
2017-10-03T00:39:30.133001: step 6607, loss 0.060205, acc 0.96875, learning_rate 0.0001
2017-10-03T00:39:31.415377: step 6608, loss 0.00389921, acc 1, learning_rate 0.0001
2017-10-03T00:39:32.580013: step 6609, loss 0.00514593, acc 1, learning_rate 0.0001
2017-10-03T00:39:33.835970: step 6610, loss 0.00172341, acc 1, learning_rate 0.0001
2017-10-03T00:39:34.984188: step 6611, loss 0.022035, acc 0.984375, learning_rate 0.0001
2017-10-03T00:39:36.139552: step 6612, loss 0.00868736, acc 1, learning_rate 0.0001
2017-10-03T00:39:37.290246: step 6613, loss 0.00581748, acc 1, learning_rate 0.0001
2017-10-03T00:39:38.460909: step 6614, loss 0.00194552, acc 1, learning_rate 0.0001
2017-10-03T00:39:39.604740: step 6615, loss 0.00344019, acc 1, learning_rate 0.0001
2017-10-03T00:39:40.753535: step 6616, loss 0.00386918, acc 1, learning_rate 0.0001
2017-10-03T00:39:41.903452: step 6617, loss 0.0495819, acc 0.984375, learning_rate 0.0001
2017-10-03T00:39:43.066377: step 6618, loss 0.00245242, acc 1, learning_rate 0.0001
2017-10-03T00:39:44.213429: step 6619, loss 0.00554403, acc 1, learning_rate 0.0001
2017-10-03T00:39:45.371079: step 6620, loss 0.00208521, acc 1, learning_rate 0.0001
2017-10-03T00:39:46.521815: step 6621, loss 0.00583868, acc 1, learning_rate 0.0001
2017-10-03T00:39:47.680147: step 6622, loss 0.0046237, acc 1, learning_rate 0.0001
2017-10-03T00:39:48.837483: step 6623, loss 0.00972769, acc 1, learning_rate 0.0001
2017-10-03T00:39:49.983887: step 6624, loss 0.0124901, acc 1, learning_rate 0.0001
2017-10-03T00:39:51.136051: step 6625, loss 0.00564714, acc 1, learning_rate 0.0001
2017-10-03T00:39:52.303970: step 6626, loss 0.0211948, acc 0.984375, learning_rate 0.0001
2017-10-03T00:39:53.472328: step 6627, loss 0.018585, acc 0.984375, learning_rate 0.0001
2017-10-03T00:39:54.648257: step 6628, loss 0.0319387, acc 0.984375, learning_rate 0.0001
2017-10-03T00:39:55.820966: step 6629, loss 0.00287858, acc 1, learning_rate 0.0001
2017-10-03T00:39:56.967279: step 6630, loss 0.00235241, acc 1, learning_rate 0.0001
2017-10-03T00:39:58.116841: step 6631, loss 0.0145098, acc 0.984375, learning_rate 0.0001
2017-10-03T00:39:59.266054: step 6632, loss 0.00417089, acc 1, learning_rate 0.0001
2017-10-03T00:40:00.426612: step 6633, loss 0.0170703, acc 1, learning_rate 0.0001
2017-10-03T00:40:01.579221: step 6634, loss 0.0151563, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:02.735411: step 6635, loss 0.0249998, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:03.906581: step 6636, loss 0.00204215, acc 1, learning_rate 0.0001
2017-10-03T00:40:05.038632: step 6637, loss 0.00365066, acc 1, learning_rate 0.0001
2017-10-03T00:40:06.208458: step 6638, loss 0.00262836, acc 1, learning_rate 0.0001
2017-10-03T00:40:07.378659: step 6639, loss 0.00458969, acc 1, learning_rate 0.0001
2017-10-03T00:40:08.528237: step 6640, loss 0.00214154, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:40:08.843141: step 6640, loss 1.48626, acc 0.417266

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6640

2017-10-03T00:40:16.269893: step 6641, loss 0.00182202, acc 1, learning_rate 0.0001
2017-10-03T00:40:17.447529: step 6642, loss 0.0348802, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:18.602334: step 6643, loss 0.0026521, acc 1, learning_rate 0.0001
2017-10-03T00:40:19.752060: step 6644, loss 0.0313628, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:20.906172: step 6645, loss 0.00248317, acc 1, learning_rate 0.0001
2017-10-03T00:40:22.056945: step 6646, loss 0.0138674, acc 1, learning_rate 0.0001
2017-10-03T00:40:23.202868: step 6647, loss 0.00283148, acc 1, learning_rate 0.0001
2017-10-03T00:40:24.369255: step 6648, loss 0.00424953, acc 1, learning_rate 0.0001
2017-10-03T00:40:25.531363: step 6649, loss 0.00486692, acc 1, learning_rate 0.0001
2017-10-03T00:40:26.695757: step 6650, loss 0.0221044, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:27.858389: step 6651, loss 0.0141323, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:29.013199: step 6652, loss 0.00356571, acc 1, learning_rate 0.0001
2017-10-03T00:40:30.164581: step 6653, loss 0.00307442, acc 1, learning_rate 0.0001
2017-10-03T00:40:31.337891: step 6654, loss 0.0037572, acc 1, learning_rate 0.0001
2017-10-03T00:40:32.497388: step 6655, loss 0.0320593, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:33.645549: step 6656, loss 0.00457711, acc 1, learning_rate 0.0001
2017-10-03T00:40:35.059041: step 6657, loss 0.00126559, acc 1, learning_rate 0.0001
2017-10-03T00:40:36.207046: step 6658, loss 0.00262733, acc 1, learning_rate 0.0001
2017-10-03T00:40:37.368911: step 6659, loss 0.00613517, acc 1, learning_rate 0.0001
2017-10-03T00:40:38.516550: step 6660, loss 0.0352433, acc 0.96875, learning_rate 0.0001
2017-10-03T00:40:39.670896: step 6661, loss 0.00419639, acc 1, learning_rate 0.0001
2017-10-03T00:40:40.825725: step 6662, loss 0.00250147, acc 1, learning_rate 0.0001
2017-10-03T00:40:41.987789: step 6663, loss 0.0304817, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:43.127665: step 6664, loss 0.00910391, acc 1, learning_rate 0.0001
2017-10-03T00:40:44.299145: step 6665, loss 0.0184373, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:45.446433: step 6666, loss 0.0368482, acc 0.96875, learning_rate 0.0001
2017-10-03T00:40:46.624143: step 6667, loss 0.0344101, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:47.779594: step 6668, loss 0.00195487, acc 1, learning_rate 0.0001
2017-10-03T00:40:48.929044: step 6669, loss 0.00404011, acc 1, learning_rate 0.0001
2017-10-03T00:40:50.086104: step 6670, loss 0.00338493, acc 1, learning_rate 0.0001
2017-10-03T00:40:51.237073: step 6671, loss 0.053877, acc 0.96875, learning_rate 0.0001
2017-10-03T00:40:52.418587: step 6672, loss 0.00766936, acc 1, learning_rate 0.0001
2017-10-03T00:40:53.582543: step 6673, loss 0.00449915, acc 1, learning_rate 0.0001
2017-10-03T00:40:54.725933: step 6674, loss 0.0205772, acc 0.984375, learning_rate 0.0001
2017-10-03T00:40:55.889603: step 6675, loss 0.00122994, acc 1, learning_rate 0.0001
2017-10-03T00:40:57.044624: step 6676, loss 0.0191041, acc 1, learning_rate 0.0001
2017-10-03T00:40:58.213531: step 6677, loss 0.00583739, acc 1, learning_rate 0.0001
2017-10-03T00:40:59.364793: step 6678, loss 0.00267758, acc 1, learning_rate 0.0001
2017-10-03T00:41:00.511371: step 6679, loss 0.00169555, acc 1, learning_rate 0.0001
2017-10-03T00:41:01.665184: step 6680, loss 0.00177381, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:41:02.019060: step 6680, loss 1.46166, acc 0.464748

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6680

2017-10-03T00:41:09.847560: step 6681, loss 0.0146472, acc 1, learning_rate 0.0001
2017-10-03T00:41:10.993900: step 6682, loss 0.00234665, acc 1, learning_rate 0.0001
2017-10-03T00:41:12.135966: step 6683, loss 0.00720068, acc 1, learning_rate 0.0001
2017-10-03T00:41:13.294937: step 6684, loss 0.0474155, acc 0.96875, learning_rate 0.0001
2017-10-03T00:41:14.457229: step 6685, loss 0.0119273, acc 1, learning_rate 0.0001
2017-10-03T00:41:15.621873: step 6686, loss 0.00175724, acc 1, learning_rate 0.0001
2017-10-03T00:41:16.762899: step 6687, loss 0.00122326, acc 1, learning_rate 0.0001
2017-10-03T00:41:17.921966: step 6688, loss 0.0209467, acc 1, learning_rate 0.0001
2017-10-03T00:41:19.079834: step 6689, loss 0.0242963, acc 0.984375, learning_rate 0.0001
2017-10-03T00:41:20.229612: step 6690, loss 0.00246096, acc 1, learning_rate 0.0001
2017-10-03T00:41:21.546900: step 6691, loss 0.024857, acc 0.984375, learning_rate 0.0001
2017-10-03T00:41:22.721058: step 6692, loss 0.0217729, acc 0.984375, learning_rate 0.0001
2017-10-03T00:41:23.881479: step 6693, loss 0.00267206, acc 1, learning_rate 0.0001
2017-10-03T00:41:25.043434: step 6694, loss 0.00331471, acc 1, learning_rate 0.0001
2017-10-03T00:41:26.201137: step 6695, loss 0.00175924, acc 1, learning_rate 0.0001
2017-10-03T00:41:27.355640: step 6696, loss 0.00187299, acc 1, learning_rate 0.0001
2017-10-03T00:41:28.508331: step 6697, loss 0.00334354, acc 1, learning_rate 0.0001
2017-10-03T00:41:29.658175: step 6698, loss 0.00650155, acc 1, learning_rate 0.0001
2017-10-03T00:41:30.827304: step 6699, loss 0.013806, acc 1, learning_rate 0.0001
2017-10-03T00:41:31.980544: step 6700, loss 0.0250721, acc 0.984375, learning_rate 0.0001
2017-10-03T00:41:33.268878: step 6701, loss 0.0298414, acc 0.984375, learning_rate 0.0001
2017-10-03T00:41:34.434802: step 6702, loss 0.0463934, acc 0.984375, learning_rate 0.0001
2017-10-03T00:41:35.592497: step 6703, loss 0.00436723, acc 1, learning_rate 0.0001
2017-10-03T00:41:36.742895: step 6704, loss 0.0026306, acc 1, learning_rate 0.0001
2017-10-03T00:41:37.902704: step 6705, loss 0.00453527, acc 1, learning_rate 0.0001
2017-10-03T00:41:39.068652: step 6706, loss 0.00408183, acc 1, learning_rate 0.0001
2017-10-03T00:41:40.225592: step 6707, loss 0.00227392, acc 1, learning_rate 0.0001
2017-10-03T00:41:41.384813: step 6708, loss 0.00297808, acc 1, learning_rate 0.0001
2017-10-03T00:41:42.565068: step 6709, loss 0.00300934, acc 1, learning_rate 0.0001
2017-10-03T00:41:43.724051: step 6710, loss 0.00464342, acc 1, learning_rate 0.0001
2017-10-03T00:41:44.878179: step 6711, loss 0.0181522, acc 0.984375, learning_rate 0.0001
2017-10-03T00:41:46.022373: step 6712, loss 0.00314858, acc 1, learning_rate 0.0001
2017-10-03T00:41:47.176829: step 6713, loss 0.00535755, acc 1, learning_rate 0.0001
2017-10-03T00:41:48.331982: step 6714, loss 0.00200687, acc 1, learning_rate 0.0001
2017-10-03T00:41:49.493151: step 6715, loss 0.0012889, acc 1, learning_rate 0.0001
2017-10-03T00:41:50.642533: step 6716, loss 0.0128391, acc 1, learning_rate 0.0001
2017-10-03T00:41:51.790739: step 6717, loss 0.00192221, acc 1, learning_rate 0.0001
2017-10-03T00:41:52.940607: step 6718, loss 0.0191971, acc 1, learning_rate 0.0001
2017-10-03T00:41:54.095784: step 6719, loss 0.00315116, acc 1, learning_rate 0.0001
2017-10-03T00:41:55.261843: step 6720, loss 0.00431146, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:41:55.625791: step 6720, loss 1.50405, acc 0.407194

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6720

2017-10-03T00:42:03.451209: step 6721, loss 0.00341087, acc 1, learning_rate 0.0001
2017-10-03T00:42:04.606256: step 6722, loss 0.00301722, acc 1, learning_rate 0.0001
2017-10-03T00:42:05.763012: step 6723, loss 0.0641185, acc 0.96875, learning_rate 0.0001
2017-10-03T00:42:06.901218: step 6724, loss 0.0717251, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:08.062700: step 6725, loss 0.00288451, acc 1, learning_rate 0.0001
2017-10-03T00:42:09.213972: step 6726, loss 0.00171557, acc 1, learning_rate 0.0001
2017-10-03T00:42:10.366133: step 6727, loss 0.00351066, acc 1, learning_rate 0.0001
2017-10-03T00:42:11.509389: step 6728, loss 0.027265, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:12.664420: step 6729, loss 0.00260455, acc 1, learning_rate 0.0001
2017-10-03T00:42:13.831630: step 6730, loss 0.0220552, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:15.005119: step 6731, loss 0.00241791, acc 1, learning_rate 0.0001
2017-10-03T00:42:16.166392: step 6732, loss 0.00693418, acc 1, learning_rate 0.0001
2017-10-03T00:42:17.318652: step 6733, loss 0.0232617, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:18.475587: step 6734, loss 0.00604767, acc 1, learning_rate 0.0001
2017-10-03T00:42:19.630348: step 6735, loss 0.0263647, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:20.834697: step 6736, loss 0.00257856, acc 1, learning_rate 0.0001
2017-10-03T00:42:21.981807: step 6737, loss 0.00911603, acc 1, learning_rate 0.0001
2017-10-03T00:42:23.133250: step 6738, loss 0.0315608, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:24.309219: step 6739, loss 0.00214781, acc 1, learning_rate 0.0001
2017-10-03T00:42:25.495459: step 6740, loss 0.00569965, acc 1, learning_rate 0.0001
2017-10-03T00:42:26.651417: step 6741, loss 0.00527738, acc 1, learning_rate 0.0001
2017-10-03T00:42:27.798069: step 6742, loss 0.0112986, acc 1, learning_rate 0.0001
2017-10-03T00:42:28.962617: step 6743, loss 0.00363186, acc 1, learning_rate 0.0001
2017-10-03T00:42:30.107074: step 6744, loss 0.0202299, acc 1, learning_rate 0.0001
2017-10-03T00:42:31.266225: step 6745, loss 0.00183773, acc 1, learning_rate 0.0001
2017-10-03T00:42:32.415302: step 6746, loss 0.00313234, acc 1, learning_rate 0.0001
2017-10-03T00:42:33.571327: step 6747, loss 0.00528319, acc 1, learning_rate 0.0001
2017-10-03T00:42:34.726438: step 6748, loss 0.00289836, acc 1, learning_rate 0.0001
2017-10-03T00:42:35.896343: step 6749, loss 0.0335863, acc 0.96875, learning_rate 0.0001
2017-10-03T00:42:37.067200: step 6750, loss 0.00360371, acc 1, learning_rate 0.0001
2017-10-03T00:42:38.222113: step 6751, loss 0.0348851, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:39.386513: step 6752, loss 0.00434746, acc 1, learning_rate 0.0001
2017-10-03T00:42:40.543213: step 6753, loss 0.00290001, acc 1, learning_rate 0.0001
2017-10-03T00:42:41.689863: step 6754, loss 0.0013235, acc 1, learning_rate 0.0001
2017-10-03T00:42:42.846556: step 6755, loss 0.00264736, acc 1, learning_rate 0.0001
2017-10-03T00:42:43.991827: step 6756, loss 0.0153595, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:45.138282: step 6757, loss 0.0147978, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:46.280464: step 6758, loss 0.00367728, acc 1, learning_rate 0.0001
2017-10-03T00:42:47.436625: step 6759, loss 0.00243468, acc 1, learning_rate 0.0001
2017-10-03T00:42:48.582260: step 6760, loss 0.0245328, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:42:48.915230: step 6760, loss 1.49508, acc 0.407194

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6760

2017-10-03T00:42:56.609992: step 6761, loss 0.0277228, acc 0.984375, learning_rate 0.0001
2017-10-03T00:42:57.768598: step 6762, loss 0.00129297, acc 1, learning_rate 0.0001
2017-10-03T00:42:58.941377: step 6763, loss 0.0253647, acc 0.984375, learning_rate 0.0001
2017-10-03T00:43:00.104359: step 6764, loss 0.0060008, acc 1, learning_rate 0.0001
2017-10-03T00:43:01.252069: step 6765, loss 0.0177892, acc 1, learning_rate 0.0001
2017-10-03T00:43:02.423089: step 6766, loss 0.0173094, acc 0.984375, learning_rate 0.0001
2017-10-03T00:43:03.580615: step 6767, loss 0.0035006, acc 1, learning_rate 0.0001
2017-10-03T00:43:04.721562: step 6768, loss 0.00862245, acc 1, learning_rate 0.0001
2017-10-03T00:43:05.883710: step 6769, loss 0.0121762, acc 1, learning_rate 0.0001
2017-10-03T00:43:07.035782: step 6770, loss 0.00335728, acc 1, learning_rate 0.0001
2017-10-03T00:43:08.187405: step 6771, loss 0.00282653, acc 1, learning_rate 0.0001
2017-10-03T00:43:09.341140: step 6772, loss 0.0034505, acc 1, learning_rate 0.0001
2017-10-03T00:43:10.518226: step 6773, loss 0.00247132, acc 1, learning_rate 0.0001
2017-10-03T00:43:11.669888: step 6774, loss 0.000971084, acc 1, learning_rate 0.0001
2017-10-03T00:43:12.819290: step 6775, loss 0.0442722, acc 0.984375, learning_rate 0.0001
2017-10-03T00:43:13.978878: step 6776, loss 0.00226171, acc 1, learning_rate 0.0001
2017-10-03T00:43:15.209528: step 6777, loss 0.00690533, acc 1, learning_rate 0.0001
2017-10-03T00:43:16.365692: step 6778, loss 0.00314932, acc 1, learning_rate 0.0001
2017-10-03T00:43:17.538483: step 6779, loss 0.0065233, acc 1, learning_rate 0.0001
2017-10-03T00:43:18.761584: step 6780, loss 0.00216123, acc 1, learning_rate 0.0001
2017-10-03T00:43:19.906445: step 6781, loss 0.00411503, acc 1, learning_rate 0.0001
2017-10-03T00:43:21.062299: step 6782, loss 0.0221317, acc 0.984375, learning_rate 0.0001
2017-10-03T00:43:22.211024: step 6783, loss 0.00450208, acc 1, learning_rate 0.0001
2017-10-03T00:43:23.450894: step 6784, loss 0.00263805, acc 1, learning_rate 0.0001
2017-10-03T00:43:24.602596: step 6785, loss 0.00259281, acc 1, learning_rate 0.0001
2017-10-03T00:43:25.762075: step 6786, loss 0.00404936, acc 1, learning_rate 0.0001
2017-10-03T00:43:26.986360: step 6787, loss 0.0216519, acc 1, learning_rate 0.0001
2017-10-03T00:43:28.148990: step 6788, loss 0.0186372, acc 0.984375, learning_rate 0.0001
2017-10-03T00:43:29.308730: step 6789, loss 0.0255566, acc 0.984375, learning_rate 0.0001
2017-10-03T00:43:30.480633: step 6790, loss 0.00134009, acc 1, learning_rate 0.0001
2017-10-03T00:43:31.632342: step 6791, loss 0.00163551, acc 1, learning_rate 0.0001
2017-10-03T00:43:32.788084: step 6792, loss 0.00375188, acc 1, learning_rate 0.0001
2017-10-03T00:43:33.945820: step 6793, loss 0.0576712, acc 0.96875, learning_rate 0.0001
2017-10-03T00:43:35.102767: step 6794, loss 0.0130725, acc 1, learning_rate 0.0001
2017-10-03T00:43:36.273719: step 6795, loss 0.0021361, acc 1, learning_rate 0.0001
2017-10-03T00:43:37.436222: step 6796, loss 0.00286231, acc 1, learning_rate 0.0001
2017-10-03T00:43:38.590025: step 6797, loss 0.0017523, acc 1, learning_rate 0.0001
2017-10-03T00:43:39.959672: step 6798, loss 0.00220589, acc 1, learning_rate 0.0001
2017-10-03T00:43:41.112304: step 6799, loss 0.00463847, acc 1, learning_rate 0.0001
2017-10-03T00:43:42.294656: step 6800, loss 0.00152357, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:43:42.656730: step 6800, loss 1.46671, acc 0.448921

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6800

2017-10-03T00:43:50.555209: step 6801, loss 0.0112852, acc 1, learning_rate 0.0001
2017-10-03T00:43:51.786554: step 6802, loss 0.0107926, acc 1, learning_rate 0.0001
2017-10-03T00:43:52.945904: step 6803, loss 0.00577323, acc 1, learning_rate 0.0001
2017-10-03T00:43:54.089282: step 6804, loss 0.00328706, acc 1, learning_rate 0.0001
2017-10-03T00:43:55.244528: step 6805, loss 0.00953401, acc 1, learning_rate 0.0001
2017-10-03T00:43:56.393955: step 6806, loss 0.030663, acc 0.984375, learning_rate 0.0001
2017-10-03T00:43:57.543464: step 6807, loss 0.00221778, acc 1, learning_rate 0.0001
2017-10-03T00:43:58.698303: step 6808, loss 0.00385618, acc 1, learning_rate 0.0001
2017-10-03T00:43:59.854904: step 6809, loss 0.0319094, acc 0.984375, learning_rate 0.0001
2017-10-03T00:44:01.004430: step 6810, loss 0.0043786, acc 1, learning_rate 0.0001
2017-10-03T00:44:02.156375: step 6811, loss 0.0295601, acc 1, learning_rate 0.0001
2017-10-03T00:44:03.334436: step 6812, loss 0.0183241, acc 1, learning_rate 0.0001
2017-10-03T00:44:04.502159: step 6813, loss 0.00269832, acc 1, learning_rate 0.0001
2017-10-03T00:44:05.657769: step 6814, loss 0.00263384, acc 1, learning_rate 0.0001
2017-10-03T00:44:06.823148: step 6815, loss 0.00285437, acc 1, learning_rate 0.0001
2017-10-03T00:44:07.966912: step 6816, loss 0.0258284, acc 0.984375, learning_rate 0.0001
2017-10-03T00:44:09.114941: step 6817, loss 0.00343471, acc 1, learning_rate 0.0001
2017-10-03T00:44:10.274772: step 6818, loss 0.00203114, acc 1, learning_rate 0.0001
2017-10-03T00:44:11.427353: step 6819, loss 0.00455098, acc 1, learning_rate 0.0001
2017-10-03T00:44:12.580899: step 6820, loss 0.00215847, acc 1, learning_rate 0.0001
2017-10-03T00:44:13.726212: step 6821, loss 0.00155378, acc 1, learning_rate 0.0001
2017-10-03T00:44:14.876522: step 6822, loss 0.00163129, acc 1, learning_rate 0.0001
2017-10-03T00:44:16.039231: step 6823, loss 0.00192705, acc 1, learning_rate 0.0001
2017-10-03T00:44:17.188664: step 6824, loss 0.057914, acc 0.96875, learning_rate 0.0001
2017-10-03T00:44:18.334908: step 6825, loss 0.00181382, acc 1, learning_rate 0.0001
2017-10-03T00:44:19.492933: step 6826, loss 0.00313648, acc 1, learning_rate 0.0001
2017-10-03T00:44:20.632129: step 6827, loss 0.0028428, acc 1, learning_rate 0.0001
2017-10-03T00:44:21.781802: step 6828, loss 0.00197633, acc 1, learning_rate 0.0001
2017-10-03T00:44:22.928483: step 6829, loss 0.0198203, acc 0.984375, learning_rate 0.0001
2017-10-03T00:44:24.075520: step 6830, loss 0.00437391, acc 1, learning_rate 0.0001
2017-10-03T00:44:25.283978: step 6831, loss 0.00367104, acc 1, learning_rate 0.0001
2017-10-03T00:44:26.462610: step 6832, loss 0.0203491, acc 0.984375, learning_rate 0.0001
2017-10-03T00:44:27.615503: step 6833, loss 0.00167897, acc 1, learning_rate 0.0001
2017-10-03T00:44:28.768009: step 6834, loss 0.0634444, acc 0.96875, learning_rate 0.0001
2017-10-03T00:44:29.914490: step 6835, loss 0.0123593, acc 1, learning_rate 0.0001
2017-10-03T00:44:31.065138: step 6836, loss 0.0279399, acc 0.984375, learning_rate 0.0001
2017-10-03T00:44:32.219996: step 6837, loss 0.0123383, acc 1, learning_rate 0.0001
2017-10-03T00:44:33.380240: step 6838, loss 0.00184379, acc 1, learning_rate 0.0001
2017-10-03T00:44:34.526575: step 6839, loss 0.00200551, acc 1, learning_rate 0.0001
2017-10-03T00:44:35.747937: step 6840, loss 0.0127536, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:44:36.100795: step 6840, loss 1.47889, acc 0.43741

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6840

2017-10-03T00:44:43.856441: step 6841, loss 0.00256638, acc 1, learning_rate 0.0001
2017-10-03T00:44:45.006708: step 6842, loss 0.0554295, acc 0.953125, learning_rate 0.0001
2017-10-03T00:44:46.156365: step 6843, loss 0.0265595, acc 0.984375, learning_rate 0.0001
2017-10-03T00:44:47.301603: step 6844, loss 0.00475938, acc 1, learning_rate 0.0001
2017-10-03T00:44:48.466703: step 6845, loss 0.00196321, acc 1, learning_rate 0.0001
2017-10-03T00:44:49.624172: step 6846, loss 0.0150872, acc 1, learning_rate 0.0001
2017-10-03T00:44:50.784768: step 6847, loss 0.0176104, acc 0.984375, learning_rate 0.0001
2017-10-03T00:44:51.943659: step 6848, loss 0.00158548, acc 1, learning_rate 0.0001
2017-10-03T00:44:53.090188: step 6849, loss 0.00165007, acc 1, learning_rate 0.0001
2017-10-03T00:44:54.251573: step 6850, loss 0.00193917, acc 1, learning_rate 0.0001
2017-10-03T00:44:55.400393: step 6851, loss 0.00214245, acc 1, learning_rate 0.0001
2017-10-03T00:44:56.568955: step 6852, loss 0.00259898, acc 1, learning_rate 0.0001
2017-10-03T00:44:57.718999: step 6853, loss 0.00254448, acc 1, learning_rate 0.0001
2017-10-03T00:44:58.871315: step 6854, loss 0.0031637, acc 1, learning_rate 0.0001
2017-10-03T00:45:00.034651: step 6855, loss 0.0111272, acc 1, learning_rate 0.0001
2017-10-03T00:45:01.191963: step 6856, loss 0.00204199, acc 1, learning_rate 0.0001
2017-10-03T00:45:02.345251: step 6857, loss 0.0198107, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:03.491309: step 6858, loss 0.0328417, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:04.646976: step 6859, loss 0.0154644, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:05.795909: step 6860, loss 0.00371471, acc 1, learning_rate 0.0001
2017-10-03T00:45:06.937421: step 6861, loss 0.0397211, acc 0.96875, learning_rate 0.0001
2017-10-03T00:45:08.127327: step 6862, loss 0.00436161, acc 1, learning_rate 0.0001
2017-10-03T00:45:09.290699: step 6863, loss 0.0016932, acc 1, learning_rate 0.0001
2017-10-03T00:45:10.448361: step 6864, loss 0.00453582, acc 1, learning_rate 0.0001
2017-10-03T00:45:11.619470: step 6865, loss 0.0035217, acc 1, learning_rate 0.0001
2017-10-03T00:45:12.761267: step 6866, loss 0.00249749, acc 1, learning_rate 0.0001
2017-10-03T00:45:13.910668: step 6867, loss 0.00480403, acc 1, learning_rate 0.0001
2017-10-03T00:45:15.079267: step 6868, loss 0.0028324, acc 1, learning_rate 0.0001
2017-10-03T00:45:16.237009: step 6869, loss 0.00214154, acc 1, learning_rate 0.0001
2017-10-03T00:45:17.402668: step 6870, loss 0.00199473, acc 1, learning_rate 0.0001
2017-10-03T00:45:18.650843: step 6871, loss 0.00209479, acc 1, learning_rate 0.0001
2017-10-03T00:45:19.790951: step 6872, loss 0.015347, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:20.954819: step 6873, loss 0.00307494, acc 1, learning_rate 0.0001
2017-10-03T00:45:22.120090: step 6874, loss 0.00223562, acc 1, learning_rate 0.0001
2017-10-03T00:45:23.288830: step 6875, loss 0.00317171, acc 1, learning_rate 0.0001
2017-10-03T00:45:24.429390: step 6876, loss 0.00294617, acc 1, learning_rate 0.0001
2017-10-03T00:45:25.579838: step 6877, loss 0.0215948, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:26.834723: step 6878, loss 0.00558278, acc 1, learning_rate 0.0001
2017-10-03T00:45:27.988143: step 6879, loss 0.0283494, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:29.128265: step 6880, loss 0.0188865, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:45:29.488136: step 6880, loss 1.45903, acc 0.47482

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6880

2017-10-03T00:45:37.030407: step 6881, loss 0.0205504, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:38.206522: step 6882, loss 0.0445198, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:39.370177: step 6883, loss 0.0652676, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:40.526614: step 6884, loss 0.00225803, acc 1, learning_rate 0.0001
2017-10-03T00:45:41.677470: step 6885, loss 0.00426702, acc 1, learning_rate 0.0001
2017-10-03T00:45:42.934977: step 6886, loss 0.00301622, acc 1, learning_rate 0.0001
2017-10-03T00:45:44.072284: step 6887, loss 0.00621814, acc 1, learning_rate 0.0001
2017-10-03T00:45:45.243803: step 6888, loss 0.0353376, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:46.419380: step 6889, loss 0.0314246, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:47.573172: step 6890, loss 0.00238051, acc 1, learning_rate 0.0001
2017-10-03T00:45:48.734702: step 6891, loss 0.00236838, acc 1, learning_rate 0.0001
2017-10-03T00:45:49.888278: step 6892, loss 0.0798626, acc 0.96875, learning_rate 0.0001
2017-10-03T00:45:51.037416: step 6893, loss 0.0145469, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:52.195752: step 6894, loss 0.0321457, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:53.349296: step 6895, loss 0.00233769, acc 1, learning_rate 0.0001
2017-10-03T00:45:54.513181: step 6896, loss 0.0376101, acc 0.984375, learning_rate 0.0001
2017-10-03T00:45:55.668965: step 6897, loss 0.00272534, acc 1, learning_rate 0.0001
2017-10-03T00:45:56.834580: step 6898, loss 0.0977762, acc 0.953125, learning_rate 0.0001
2017-10-03T00:45:57.985415: step 6899, loss 0.00638147, acc 1, learning_rate 0.0001
2017-10-03T00:45:59.136113: step 6900, loss 0.00180965, acc 1, learning_rate 0.0001
2017-10-03T00:46:00.292754: step 6901, loss 0.0195293, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:01.440328: step 6902, loss 0.00842919, acc 1, learning_rate 0.0001
2017-10-03T00:46:02.603678: step 6903, loss 0.091041, acc 0.9375, learning_rate 0.0001
2017-10-03T00:46:03.780246: step 6904, loss 0.00343113, acc 1, learning_rate 0.0001
2017-10-03T00:46:04.924680: step 6905, loss 0.00222903, acc 1, learning_rate 0.0001
2017-10-03T00:46:06.065712: step 6906, loss 0.00463167, acc 1, learning_rate 0.0001
2017-10-03T00:46:07.219867: step 6907, loss 0.00272854, acc 1, learning_rate 0.0001
2017-10-03T00:46:08.371067: step 6908, loss 0.0117643, acc 1, learning_rate 0.0001
2017-10-03T00:46:09.533766: step 6909, loss 0.0034562, acc 1, learning_rate 0.0001
2017-10-03T00:46:10.691917: step 6910, loss 0.00210921, acc 1, learning_rate 0.0001
2017-10-03T00:46:11.845051: step 6911, loss 0.00301915, acc 1, learning_rate 0.0001
2017-10-03T00:46:13.003537: step 6912, loss 0.00370676, acc 1, learning_rate 0.0001
2017-10-03T00:46:14.149529: step 6913, loss 0.0228209, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:15.307247: step 6914, loss 0.00216205, acc 1, learning_rate 0.0001
2017-10-03T00:46:16.469914: step 6915, loss 0.0052046, acc 1, learning_rate 0.0001
2017-10-03T00:46:17.630683: step 6916, loss 0.00297245, acc 1, learning_rate 0.0001
2017-10-03T00:46:18.831999: step 6917, loss 0.0161714, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:19.980444: step 6918, loss 0.00352005, acc 1, learning_rate 0.0001
2017-10-03T00:46:21.134374: step 6919, loss 0.00153613, acc 1, learning_rate 0.0001
2017-10-03T00:46:22.280215: step 6920, loss 0.00167832, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:46:22.602321: step 6920, loss 1.47179, acc 0.441727

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6920

2017-10-03T00:46:30.780579: step 6921, loss 0.00533139, acc 1, learning_rate 0.0001
2017-10-03T00:46:31.954192: step 6922, loss 0.00303534, acc 1, learning_rate 0.0001
2017-10-03T00:46:33.105188: step 6923, loss 0.00492384, acc 1, learning_rate 0.0001
2017-10-03T00:46:34.256306: step 6924, loss 0.0240102, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:35.436569: step 6925, loss 0.00445829, acc 1, learning_rate 0.0001
2017-10-03T00:46:36.583821: step 6926, loss 0.0142019, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:37.747354: step 6927, loss 0.0249391, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:38.902103: step 6928, loss 0.00273686, acc 1, learning_rate 0.0001
2017-10-03T00:46:40.051877: step 6929, loss 0.00395679, acc 1, learning_rate 0.0001
2017-10-03T00:46:41.211996: step 6930, loss 0.00595272, acc 1, learning_rate 0.0001
2017-10-03T00:46:42.369767: step 6931, loss 0.00939982, acc 1, learning_rate 0.0001
2017-10-03T00:46:43.513436: step 6932, loss 0.0544357, acc 0.953125, learning_rate 0.0001
2017-10-03T00:46:44.667451: step 6933, loss 0.016222, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:45.813918: step 6934, loss 0.0182287, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:46.974235: step 6935, loss 0.0012434, acc 1, learning_rate 0.0001
2017-10-03T00:46:48.130416: step 6936, loss 0.00277552, acc 1, learning_rate 0.0001
2017-10-03T00:46:49.305890: step 6937, loss 0.0045791, acc 1, learning_rate 0.0001
2017-10-03T00:46:50.467146: step 6938, loss 0.00293991, acc 1, learning_rate 0.0001
2017-10-03T00:46:51.625952: step 6939, loss 0.00229027, acc 1, learning_rate 0.0001
2017-10-03T00:46:52.780943: step 6940, loss 0.029018, acc 0.984375, learning_rate 0.0001
2017-10-03T00:46:53.952250: step 6941, loss 0.0026529, acc 1, learning_rate 0.0001
2017-10-03T00:46:55.106311: step 6942, loss 0.00245828, acc 1, learning_rate 0.0001
2017-10-03T00:46:56.244695: step 6943, loss 0.0157021, acc 1, learning_rate 0.0001
2017-10-03T00:46:57.408478: step 6944, loss 0.00303438, acc 1, learning_rate 0.0001
2017-10-03T00:46:58.575737: step 6945, loss 0.00160857, acc 1, learning_rate 0.0001
2017-10-03T00:46:59.729166: step 6946, loss 0.0409635, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:00.930268: step 6947, loss 0.0435917, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:02.109648: step 6948, loss 0.00321474, acc 1, learning_rate 0.0001
2017-10-03T00:47:03.268501: step 6949, loss 0.0343546, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:04.420295: step 6950, loss 0.00859172, acc 1, learning_rate 0.0001
2017-10-03T00:47:05.578959: step 6951, loss 0.0190177, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:06.751401: step 6952, loss 0.00216974, acc 1, learning_rate 0.0001
2017-10-03T00:47:07.916027: step 6953, loss 0.00269076, acc 1, learning_rate 0.0001
2017-10-03T00:47:09.072780: step 6954, loss 0.0244598, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:10.233983: step 6955, loss 0.0039924, acc 1, learning_rate 0.0001
2017-10-03T00:47:11.374503: step 6956, loss 0.00276243, acc 1, learning_rate 0.0001
2017-10-03T00:47:12.537938: step 6957, loss 0.0325133, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:13.688325: step 6958, loss 0.00309289, acc 1, learning_rate 0.0001
2017-10-03T00:47:14.852111: step 6959, loss 0.00228499, acc 1, learning_rate 0.0001
2017-10-03T00:47:16.003615: step 6960, loss 0.0111154, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:47:16.330433: step 6960, loss 1.4809, acc 0.431655

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-6960

2017-10-03T00:47:24.385225: step 6961, loss 0.0166556, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:25.565414: step 6962, loss 0.0166989, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:26.727077: step 6963, loss 0.0485077, acc 0.96875, learning_rate 0.0001
2017-10-03T00:47:27.869156: step 6964, loss 0.0215932, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:29.022183: step 6965, loss 0.00159837, acc 1, learning_rate 0.0001
2017-10-03T00:47:30.179136: step 6966, loss 0.00360724, acc 1, learning_rate 0.0001
2017-10-03T00:47:31.339748: step 6967, loss 0.00262949, acc 1, learning_rate 0.0001
2017-10-03T00:47:32.555696: step 6968, loss 0.019898, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:33.709720: step 6969, loss 0.0151602, acc 1, learning_rate 0.0001
2017-10-03T00:47:34.853199: step 6970, loss 0.001655, acc 1, learning_rate 0.0001
2017-10-03T00:47:36.002107: step 6971, loss 0.00218695, acc 1, learning_rate 0.0001
2017-10-03T00:47:37.158806: step 6972, loss 0.0082047, acc 1, learning_rate 0.0001
2017-10-03T00:47:38.309135: step 6973, loss 0.00393618, acc 1, learning_rate 0.0001
2017-10-03T00:47:39.489539: step 6974, loss 0.0063664, acc 1, learning_rate 0.0001
2017-10-03T00:47:40.650232: step 6975, loss 0.0164953, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:41.795474: step 6976, loss 0.002544, acc 1, learning_rate 0.0001
2017-10-03T00:47:42.950362: step 6977, loss 0.00456447, acc 1, learning_rate 0.0001
2017-10-03T00:47:44.101429: step 6978, loss 0.0162112, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:45.257609: step 6979, loss 0.0176174, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:46.416550: step 6980, loss 0.00235738, acc 1, learning_rate 0.0001
2017-10-03T00:47:47.565642: step 6981, loss 0.027284, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:48.728492: step 6982, loss 0.00260406, acc 1, learning_rate 0.0001
2017-10-03T00:47:49.889057: step 6983, loss 0.00134722, acc 1, learning_rate 0.0001
2017-10-03T00:47:51.045808: step 6984, loss 0.00154672, acc 1, learning_rate 0.0001
2017-10-03T00:47:52.197371: step 6985, loss 0.0602188, acc 0.96875, learning_rate 0.0001
2017-10-03T00:47:53.348332: step 6986, loss 0.0327858, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:54.583952: step 6987, loss 0.00254884, acc 1, learning_rate 0.0001
2017-10-03T00:47:55.749144: step 6988, loss 0.00820894, acc 1, learning_rate 0.0001
2017-10-03T00:47:56.897708: step 6989, loss 0.00209364, acc 1, learning_rate 0.0001
2017-10-03T00:47:58.048726: step 6990, loss 0.0174479, acc 0.984375, learning_rate 0.0001
2017-10-03T00:47:59.190495: step 6991, loss 0.00361635, acc 1, learning_rate 0.0001
2017-10-03T00:48:00.349877: step 6992, loss 0.00214246, acc 1, learning_rate 0.0001
2017-10-03T00:48:01.488162: step 6993, loss 0.0237035, acc 0.984375, learning_rate 0.0001
2017-10-03T00:48:02.628260: step 6994, loss 0.00865542, acc 1, learning_rate 0.0001
2017-10-03T00:48:03.782125: step 6995, loss 0.0314001, acc 0.984375, learning_rate 0.0001
2017-10-03T00:48:04.932674: step 6996, loss 0.00136846, acc 1, learning_rate 0.0001
2017-10-03T00:48:06.078715: step 6997, loss 0.00418631, acc 1, learning_rate 0.0001
2017-10-03T00:48:07.229928: step 6998, loss 0.0023217, acc 1, learning_rate 0.0001
2017-10-03T00:48:08.389333: step 6999, loss 0.00777026, acc 1, learning_rate 0.0001
2017-10-03T00:48:09.547578: step 7000, loss 0.00376721, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:48:09.886199: step 7000, loss 1.47977, acc 0.427338

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7000

2017-10-03T00:48:17.870129: step 7001, loss 0.00279327, acc 1, learning_rate 0.0001
2017-10-03T00:48:19.042847: step 7002, loss 0.00277918, acc 1, learning_rate 0.0001
2017-10-03T00:48:20.211709: step 7003, loss 0.00473991, acc 1, learning_rate 0.0001
2017-10-03T00:48:21.366991: step 7004, loss 0.0272422, acc 0.984375, learning_rate 0.0001
2017-10-03T00:48:22.525576: step 7005, loss 0.00138594, acc 1, learning_rate 0.0001
2017-10-03T00:48:23.671531: step 7006, loss 0.00188001, acc 1, learning_rate 0.0001
2017-10-03T00:48:24.816756: step 7007, loss 0.00114215, acc 1, learning_rate 0.0001
2017-10-03T00:48:25.964695: step 7008, loss 0.00579198, acc 1, learning_rate 0.0001
2017-10-03T00:48:27.136107: step 7009, loss 0.00557617, acc 1, learning_rate 0.0001
2017-10-03T00:48:28.310791: step 7010, loss 0.00440405, acc 1, learning_rate 0.0001
2017-10-03T00:48:29.461201: step 7011, loss 0.00351918, acc 1, learning_rate 0.0001
2017-10-03T00:48:30.613629: step 7012, loss 0.00244638, acc 1, learning_rate 0.0001
2017-10-03T00:48:31.756109: step 7013, loss 0.00286824, acc 1, learning_rate 0.0001
2017-10-03T00:48:32.919415: step 7014, loss 0.0017016, acc 1, learning_rate 0.0001
2017-10-03T00:48:34.066454: step 7015, loss 0.00146201, acc 1, learning_rate 0.0001
2017-10-03T00:48:35.234200: step 7016, loss 0.00206706, acc 1, learning_rate 0.0001
2017-10-03T00:48:36.376479: step 7017, loss 0.0020975, acc 1, learning_rate 0.0001
2017-10-03T00:48:37.541422: step 7018, loss 0.00181825, acc 1, learning_rate 0.0001
2017-10-03T00:48:38.685359: step 7019, loss 0.00180519, acc 1, learning_rate 0.0001
2017-10-03T00:48:39.836747: step 7020, loss 0.00168371, acc 1, learning_rate 0.0001
2017-10-03T00:48:41.007227: step 7021, loss 0.00221124, acc 1, learning_rate 0.0001
2017-10-03T00:48:42.152954: step 7022, loss 0.00212188, acc 1, learning_rate 0.0001
2017-10-03T00:48:43.307572: step 7023, loss 0.00214412, acc 1, learning_rate 0.0001
2017-10-03T00:48:44.452854: step 7024, loss 0.00342161, acc 1, learning_rate 0.0001
2017-10-03T00:48:45.618667: step 7025, loss 0.0260044, acc 0.984375, learning_rate 0.0001
2017-10-03T00:48:46.774127: step 7026, loss 0.0188871, acc 0.984375, learning_rate 0.0001
2017-10-03T00:48:47.931076: step 7027, loss 0.00442667, acc 1, learning_rate 0.0001
2017-10-03T00:48:49.074543: step 7028, loss 0.0260637, acc 0.984375, learning_rate 0.0001
2017-10-03T00:48:50.226789: step 7029, loss 0.003046, acc 1, learning_rate 0.0001
2017-10-03T00:48:51.391685: step 7030, loss 0.0291527, acc 1, learning_rate 0.0001
2017-10-03T00:48:52.559547: step 7031, loss 0.000979168, acc 1, learning_rate 0.0001
2017-10-03T00:48:53.709584: step 7032, loss 0.000987247, acc 1, learning_rate 0.0001
2017-10-03T00:48:54.861793: step 7033, loss 0.0378687, acc 0.96875, learning_rate 0.0001
2017-10-03T00:48:56.013131: step 7034, loss 0.0483694, acc 0.984375, learning_rate 0.0001
2017-10-03T00:48:57.167578: step 7035, loss 0.00262972, acc 1, learning_rate 0.0001
2017-10-03T00:48:58.330494: step 7036, loss 0.0424735, acc 0.984375, learning_rate 0.0001
2017-10-03T00:48:59.485353: step 7037, loss 0.00554284, acc 1, learning_rate 0.0001
2017-10-03T00:49:00.640540: step 7038, loss 0.0187418, acc 0.984375, learning_rate 0.0001
2017-10-03T00:49:01.793451: step 7039, loss 0.00176477, acc 1, learning_rate 0.0001
2017-10-03T00:49:02.950862: step 7040, loss 0.00119555, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:49:03.277499: step 7040, loss 1.46143, acc 0.469065

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7040

2017-10-03T00:49:11.144084: step 7041, loss 0.00319219, acc 1, learning_rate 0.0001
2017-10-03T00:49:12.297733: step 7042, loss 0.00793949, acc 1, learning_rate 0.0001
2017-10-03T00:49:13.442686: step 7043, loss 0.00182044, acc 1, learning_rate 0.0001
2017-10-03T00:49:14.719088: step 7044, loss 0.0235697, acc 0.984375, learning_rate 0.0001
2017-10-03T00:49:15.872749: step 7045, loss 0.00377851, acc 1, learning_rate 0.0001
2017-10-03T00:49:17.024394: step 7046, loss 0.00236499, acc 1, learning_rate 0.0001
2017-10-03T00:49:18.176408: step 7047, loss 0.00523581, acc 1, learning_rate 0.0001
2017-10-03T00:49:19.331804: step 7048, loss 0.017387, acc 0.984375, learning_rate 0.0001
2017-10-03T00:49:20.491557: step 7049, loss 0.0026471, acc 1, learning_rate 0.0001
2017-10-03T00:49:21.633067: step 7050, loss 0.0053006, acc 1, learning_rate 0.0001
2017-10-03T00:49:22.798375: step 7051, loss 0.00317712, acc 1, learning_rate 0.0001
2017-10-03T00:49:23.953202: step 7052, loss 0.0106618, acc 1, learning_rate 0.0001
2017-10-03T00:49:25.109625: step 7053, loss 0.00254544, acc 1, learning_rate 0.0001
2017-10-03T00:49:26.255887: step 7054, loss 0.00302777, acc 1, learning_rate 0.0001
2017-10-03T00:49:27.411468: step 7055, loss 0.00225329, acc 1, learning_rate 0.0001
2017-10-03T00:49:28.552148: step 7056, loss 0.00142381, acc 1, learning_rate 0.0001
2017-10-03T00:49:29.688453: step 7057, loss 0.0123002, acc 1, learning_rate 0.0001
2017-10-03T00:49:30.848625: step 7058, loss 0.0156997, acc 1, learning_rate 0.0001
2017-10-03T00:49:31.995276: step 7059, loss 0.00183869, acc 1, learning_rate 0.0001
2017-10-03T00:49:33.153325: step 7060, loss 0.00150218, acc 1, learning_rate 0.0001
2017-10-03T00:49:34.301209: step 7061, loss 0.00140447, acc 1, learning_rate 0.0001
2017-10-03T00:49:35.439332: step 7062, loss 0.016761, acc 1, learning_rate 0.0001
2017-10-03T00:49:36.600921: step 7063, loss 0.00126185, acc 1, learning_rate 0.0001
2017-10-03T00:49:37.772213: step 7064, loss 0.00183543, acc 1, learning_rate 0.0001
2017-10-03T00:49:38.919893: step 7065, loss 0.00464614, acc 1, learning_rate 0.0001
2017-10-03T00:49:40.071000: step 7066, loss 0.00269785, acc 1, learning_rate 0.0001
2017-10-03T00:49:41.231826: step 7067, loss 0.00276774, acc 1, learning_rate 0.0001
2017-10-03T00:49:42.387458: step 7068, loss 0.0561422, acc 0.96875, learning_rate 0.0001
2017-10-03T00:49:43.538013: step 7069, loss 0.00997758, acc 1, learning_rate 0.0001
2017-10-03T00:49:44.693385: step 7070, loss 0.0452037, acc 0.96875, learning_rate 0.0001
2017-10-03T00:49:45.842459: step 7071, loss 0.00655323, acc 1, learning_rate 0.0001
2017-10-03T00:49:47.224975: step 7072, loss 0.00218595, acc 1, learning_rate 0.0001
2017-10-03T00:49:48.364047: step 7073, loss 0.0257464, acc 0.984375, learning_rate 0.0001
2017-10-03T00:49:49.527422: step 7074, loss 0.00300065, acc 1, learning_rate 0.0001
2017-10-03T00:49:50.690661: step 7075, loss 0.00247182, acc 1, learning_rate 0.0001
2017-10-03T00:49:51.833065: step 7076, loss 0.00118044, acc 1, learning_rate 0.0001
2017-10-03T00:49:53.001459: step 7077, loss 0.000719987, acc 1, learning_rate 0.0001
2017-10-03T00:49:54.149211: step 7078, loss 0.0170845, acc 0.984375, learning_rate 0.0001
2017-10-03T00:49:55.313885: step 7079, loss 0.00289015, acc 1, learning_rate 0.0001
2017-10-03T00:49:56.462254: step 7080, loss 0.00994441, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:49:56.787313: step 7080, loss 1.44159, acc 0.483453

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7080

2017-10-03T00:50:04.873977: step 7081, loss 0.00281142, acc 1, learning_rate 0.0001
2017-10-03T00:50:06.033262: step 7082, loss 0.00605178, acc 1, learning_rate 0.0001
2017-10-03T00:50:07.198852: step 7083, loss 0.0189089, acc 1, learning_rate 0.0001
2017-10-03T00:50:08.369297: step 7084, loss 0.00329056, acc 1, learning_rate 0.0001
2017-10-03T00:50:09.511329: step 7085, loss 0.00197558, acc 1, learning_rate 0.0001
2017-10-03T00:50:10.667686: step 7086, loss 0.024074, acc 0.984375, learning_rate 0.0001
2017-10-03T00:50:11.851305: step 7087, loss 0.0298978, acc 0.984375, learning_rate 0.0001
2017-10-03T00:50:13.008111: step 7088, loss 0.00206813, acc 1, learning_rate 0.0001
2017-10-03T00:50:14.165124: step 7089, loss 0.003693, acc 1, learning_rate 0.0001
2017-10-03T00:50:15.317859: step 7090, loss 0.00238991, acc 1, learning_rate 0.0001
2017-10-03T00:50:16.476170: step 7091, loss 0.00238664, acc 1, learning_rate 0.0001
2017-10-03T00:50:17.644798: step 7092, loss 0.0118224, acc 1, learning_rate 0.0001
2017-10-03T00:50:18.782962: step 7093, loss 0.00170174, acc 1, learning_rate 0.0001
2017-10-03T00:50:19.937048: step 7094, loss 0.0112538, acc 1, learning_rate 0.0001
2017-10-03T00:50:21.094618: step 7095, loss 0.0422812, acc 0.984375, learning_rate 0.0001
2017-10-03T00:50:22.234246: step 7096, loss 0.00225587, acc 1, learning_rate 0.0001
2017-10-03T00:50:23.824925: step 7097, loss 0.0162535, acc 1, learning_rate 0.0001
2017-10-03T00:50:25.237166: step 7098, loss 0.0322073, acc 0.96875, learning_rate 0.0001
2017-10-03T00:50:26.434997: step 7099, loss 0.0018559, acc 1, learning_rate 0.0001
2017-10-03T00:50:27.579543: step 7100, loss 0.00238281, acc 1, learning_rate 0.0001
2017-10-03T00:50:28.768585: step 7101, loss 0.00382342, acc 1, learning_rate 0.0001
2017-10-03T00:50:29.917241: step 7102, loss 0.0025329, acc 1, learning_rate 0.0001
2017-10-03T00:50:31.072586: step 7103, loss 0.00144612, acc 1, learning_rate 0.0001
2017-10-03T00:50:32.221941: step 7104, loss 0.0234815, acc 0.984375, learning_rate 0.0001
2017-10-03T00:50:33.376228: step 7105, loss 0.00199783, acc 1, learning_rate 0.0001
2017-10-03T00:50:34.530815: step 7106, loss 0.00662924, acc 1, learning_rate 0.0001
2017-10-03T00:50:35.676594: step 7107, loss 0.00302225, acc 1, learning_rate 0.0001
2017-10-03T00:50:36.861831: step 7108, loss 0.00127201, acc 1, learning_rate 0.0001
2017-10-03T00:50:38.013180: step 7109, loss 0.00394079, acc 1, learning_rate 0.0001
2017-10-03T00:50:39.161306: step 7110, loss 0.0139517, acc 1, learning_rate 0.0001
2017-10-03T00:50:40.323382: step 7111, loss 0.00232651, acc 1, learning_rate 0.0001
2017-10-03T00:50:41.472862: step 7112, loss 0.0380693, acc 0.96875, learning_rate 0.0001
2017-10-03T00:50:42.668817: step 7113, loss 0.0187965, acc 0.984375, learning_rate 0.0001
2017-10-03T00:50:43.821222: step 7114, loss 0.00497495, acc 1, learning_rate 0.0001
2017-10-03T00:50:44.975239: step 7115, loss 0.00431024, acc 1, learning_rate 0.0001
2017-10-03T00:50:46.128567: step 7116, loss 0.040447, acc 0.984375, learning_rate 0.0001
2017-10-03T00:50:47.296700: step 7117, loss 0.013094, acc 1, learning_rate 0.0001
2017-10-03T00:50:48.457228: step 7118, loss 0.00218428, acc 1, learning_rate 0.0001
2017-10-03T00:50:49.634379: step 7119, loss 0.00163787, acc 1, learning_rate 0.0001
2017-10-03T00:50:50.793459: step 7120, loss 0.0026621, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:50:51.123526: step 7120, loss 1.45827, acc 0.467626

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7120

2017-10-03T00:50:59.271029: step 7121, loss 0.0128337, acc 1, learning_rate 0.0001
2017-10-03T00:51:00.471758: step 7122, loss 0.003568, acc 1, learning_rate 0.0001
2017-10-03T00:51:01.632459: step 7123, loss 0.00204329, acc 1, learning_rate 0.0001
2017-10-03T00:51:02.783836: step 7124, loss 0.00214576, acc 1, learning_rate 0.0001
2017-10-03T00:51:03.929151: step 7125, loss 0.00134035, acc 1, learning_rate 0.0001
2017-10-03T00:51:05.076691: step 7126, loss 0.0136946, acc 0.984375, learning_rate 0.0001
2017-10-03T00:51:06.316256: step 7127, loss 0.0119836, acc 1, learning_rate 0.0001
2017-10-03T00:51:07.478587: step 7128, loss 0.0016609, acc 1, learning_rate 0.0001
2017-10-03T00:51:08.631403: step 7129, loss 0.00232834, acc 1, learning_rate 0.0001
2017-10-03T00:51:09.787225: step 7130, loss 0.00284962, acc 1, learning_rate 0.0001
2017-10-03T00:51:10.931533: step 7131, loss 0.00141539, acc 1, learning_rate 0.0001
2017-10-03T00:51:12.075492: step 7132, loss 0.00612399, acc 1, learning_rate 0.0001
2017-10-03T00:51:13.228761: step 7133, loss 0.00585299, acc 1, learning_rate 0.0001
2017-10-03T00:51:14.381499: step 7134, loss 0.00162741, acc 1, learning_rate 0.0001
2017-10-03T00:51:15.533119: step 7135, loss 0.00156163, acc 1, learning_rate 0.0001
2017-10-03T00:51:16.705898: step 7136, loss 0.00347336, acc 1, learning_rate 0.0001
2017-10-03T00:51:17.868033: step 7137, loss 0.00629645, acc 1, learning_rate 0.0001
2017-10-03T00:51:19.031800: step 7138, loss 0.0521, acc 0.984375, learning_rate 0.0001
2017-10-03T00:51:20.193073: step 7139, loss 0.00156718, acc 1, learning_rate 0.0001
2017-10-03T00:51:21.343917: step 7140, loss 0.00317175, acc 1, learning_rate 0.0001
2017-10-03T00:51:22.492290: step 7141, loss 0.00208522, acc 1, learning_rate 0.0001
2017-10-03T00:51:23.644782: step 7142, loss 0.00216881, acc 1, learning_rate 0.0001
2017-10-03T00:51:24.815598: step 7143, loss 0.0425765, acc 0.984375, learning_rate 0.0001
2017-10-03T00:51:25.967583: step 7144, loss 0.00849639, acc 1, learning_rate 0.0001
2017-10-03T00:51:27.125715: step 7145, loss 0.0022711, acc 1, learning_rate 0.0001
2017-10-03T00:51:28.279799: step 7146, loss 0.00391144, acc 1, learning_rate 0.0001
2017-10-03T00:51:29.451824: step 7147, loss 0.0289084, acc 0.984375, learning_rate 0.0001
2017-10-03T00:51:30.611340: step 7148, loss 0.00532959, acc 1, learning_rate 0.0001
2017-10-03T00:51:31.775509: step 7149, loss 0.00572536, acc 1, learning_rate 0.0001
2017-10-03T00:51:32.914884: step 7150, loss 0.0159684, acc 0.984375, learning_rate 0.0001
2017-10-03T00:51:34.061777: step 7151, loss 0.056951, acc 0.96875, learning_rate 0.0001
2017-10-03T00:51:35.219111: step 7152, loss 0.00496168, acc 1, learning_rate 0.0001
2017-10-03T00:51:36.371669: step 7153, loss 0.00196291, acc 1, learning_rate 0.0001
2017-10-03T00:51:37.507203: step 7154, loss 0.00155778, acc 1, learning_rate 0.0001
2017-10-03T00:51:38.660904: step 7155, loss 0.00188735, acc 1, learning_rate 0.0001
2017-10-03T00:51:39.807463: step 7156, loss 0.0466879, acc 0.984375, learning_rate 0.0001
2017-10-03T00:51:40.964800: step 7157, loss 0.00358633, acc 1, learning_rate 0.0001
2017-10-03T00:51:42.118689: step 7158, loss 0.0374514, acc 0.984375, learning_rate 0.0001
2017-10-03T00:51:43.286256: step 7159, loss 0.00185754, acc 1, learning_rate 0.0001
2017-10-03T00:51:44.444474: step 7160, loss 0.00174252, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:51:44.779567: step 7160, loss 1.50217, acc 0.388489

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7160

2017-10-03T00:51:52.572882: step 7161, loss 0.00286939, acc 1, learning_rate 0.0001
2017-10-03T00:51:53.734329: step 7162, loss 0.00148858, acc 1, learning_rate 0.0001
2017-10-03T00:51:54.882843: step 7163, loss 0.0143943, acc 1, learning_rate 0.0001
2017-10-03T00:51:56.042204: step 7164, loss 0.00206621, acc 1, learning_rate 0.0001
2017-10-03T00:51:57.191246: step 7165, loss 0.0026142, acc 1, learning_rate 0.0001
2017-10-03T00:51:58.334538: step 7166, loss 0.0177686, acc 0.984375, learning_rate 0.0001
2017-10-03T00:51:59.495708: step 7167, loss 0.00363305, acc 1, learning_rate 0.0001
2017-10-03T00:52:00.665521: step 7168, loss 0.0028128, acc 1, learning_rate 0.0001
2017-10-03T00:52:01.818602: step 7169, loss 0.0713459, acc 0.96875, learning_rate 0.0001
2017-10-03T00:52:02.963714: step 7170, loss 0.00140049, acc 1, learning_rate 0.0001
2017-10-03T00:52:04.110351: step 7171, loss 0.0027935, acc 1, learning_rate 0.0001
2017-10-03T00:52:05.266566: step 7172, loss 0.00365006, acc 1, learning_rate 0.0001
2017-10-03T00:52:06.442846: step 7173, loss 0.00293579, acc 1, learning_rate 0.0001
2017-10-03T00:52:07.590018: step 7174, loss 0.00227394, acc 1, learning_rate 0.0001
2017-10-03T00:52:08.744681: step 7175, loss 0.00246602, acc 1, learning_rate 0.0001
2017-10-03T00:52:09.895030: step 7176, loss 0.013016, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:11.054341: step 7177, loss 0.0357547, acc 0.96875, learning_rate 0.0001
2017-10-03T00:52:12.208058: step 7178, loss 0.01811, acc 1, learning_rate 0.0001
2017-10-03T00:52:13.359840: step 7179, loss 0.0227568, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:14.529575: step 7180, loss 0.0349157, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:15.702357: step 7181, loss 0.0557274, acc 0.96875, learning_rate 0.0001
2017-10-03T00:52:16.845940: step 7182, loss 0.00192028, acc 1, learning_rate 0.0001
2017-10-03T00:52:17.987410: step 7183, loss 0.00426257, acc 1, learning_rate 0.0001
2017-10-03T00:52:19.131411: step 7184, loss 0.00147523, acc 1, learning_rate 0.0001
2017-10-03T00:52:20.288019: step 7185, loss 0.00514272, acc 1, learning_rate 0.0001
2017-10-03T00:52:21.436087: step 7186, loss 0.0283258, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:22.571430: step 7187, loss 0.00262199, acc 1, learning_rate 0.0001
2017-10-03T00:52:23.733758: step 7188, loss 0.0162321, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:24.896695: step 7189, loss 0.00270793, acc 1, learning_rate 0.0001
2017-10-03T00:52:26.049460: step 7190, loss 0.00461494, acc 1, learning_rate 0.0001
2017-10-03T00:52:27.223064: step 7191, loss 0.017735, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:28.388191: step 7192, loss 0.00278932, acc 1, learning_rate 0.0001
2017-10-03T00:52:29.542080: step 7193, loss 0.00212841, acc 1, learning_rate 0.0001
2017-10-03T00:52:30.699152: step 7194, loss 0.0159272, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:31.847375: step 7195, loss 0.0206622, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:33.003611: step 7196, loss 0.0155553, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:34.156179: step 7197, loss 0.00164324, acc 1, learning_rate 0.0001
2017-10-03T00:52:35.316038: step 7198, loss 0.00435842, acc 1, learning_rate 0.0001
2017-10-03T00:52:36.456458: step 7199, loss 0.0223573, acc 1, learning_rate 0.0001
2017-10-03T00:52:37.612859: step 7200, loss 0.00317539, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:52:37.970562: step 7200, loss 1.47204, acc 0.43741

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7200

2017-10-03T00:52:46.197580: step 7201, loss 0.00125192, acc 1, learning_rate 0.0001
2017-10-03T00:52:47.367185: step 7202, loss 0.0033442, acc 1, learning_rate 0.0001
2017-10-03T00:52:48.522921: step 7203, loss 0.00366802, acc 1, learning_rate 0.0001
2017-10-03T00:52:49.665224: step 7204, loss 0.00168054, acc 1, learning_rate 0.0001
2017-10-03T00:52:50.805888: step 7205, loss 0.00220058, acc 1, learning_rate 0.0001
2017-10-03T00:52:51.987572: step 7206, loss 0.00804044, acc 1, learning_rate 0.0001
2017-10-03T00:52:53.134039: step 7207, loss 0.00197229, acc 1, learning_rate 0.0001
2017-10-03T00:52:54.278960: step 7208, loss 0.00120107, acc 1, learning_rate 0.0001
2017-10-03T00:52:55.433437: step 7209, loss 0.00128502, acc 1, learning_rate 0.0001
2017-10-03T00:52:56.587059: step 7210, loss 0.0147521, acc 0.984375, learning_rate 0.0001
2017-10-03T00:52:57.737877: step 7211, loss 0.0892367, acc 0.96875, learning_rate 0.0001
2017-10-03T00:52:59.119595: step 7212, loss 0.00411074, acc 1, learning_rate 0.0001
2017-10-03T00:53:00.274980: step 7213, loss 0.00255389, acc 1, learning_rate 0.0001
2017-10-03T00:53:01.421237: step 7214, loss 0.0118062, acc 1, learning_rate 0.0001
2017-10-03T00:53:02.585999: step 7215, loss 0.00122323, acc 1, learning_rate 0.0001
2017-10-03T00:53:03.752271: step 7216, loss 0.0027046, acc 1, learning_rate 0.0001
2017-10-03T00:53:05.035664: step 7217, loss 0.0458543, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:06.199081: step 7218, loss 0.0113545, acc 1, learning_rate 0.0001
2017-10-03T00:53:07.363729: step 7219, loss 0.0103936, acc 1, learning_rate 0.0001
2017-10-03T00:53:08.527313: step 7220, loss 0.0215531, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:09.695450: step 7221, loss 0.0223363, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:10.850977: step 7222, loss 0.0338944, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:12.003450: step 7223, loss 0.00154019, acc 1, learning_rate 0.0001
2017-10-03T00:53:13.154791: step 7224, loss 0.006603, acc 1, learning_rate 0.0001
2017-10-03T00:53:14.327861: step 7225, loss 0.00227545, acc 1, learning_rate 0.0001
2017-10-03T00:53:15.484090: step 7226, loss 0.0278178, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:16.649860: step 7227, loss 0.00253618, acc 1, learning_rate 0.0001
2017-10-03T00:53:17.803807: step 7228, loss 0.00353538, acc 1, learning_rate 0.0001
2017-10-03T00:53:18.953715: step 7229, loss 0.0015479, acc 1, learning_rate 0.0001
2017-10-03T00:53:20.108249: step 7230, loss 0.00288467, acc 1, learning_rate 0.0001
2017-10-03T00:53:21.267915: step 7231, loss 0.000930056, acc 1, learning_rate 0.0001
2017-10-03T00:53:22.414475: step 7232, loss 0.00297954, acc 1, learning_rate 0.0001
2017-10-03T00:53:23.555030: step 7233, loss 0.0266298, acc 0.96875, learning_rate 0.0001
2017-10-03T00:53:24.712328: step 7234, loss 0.00197973, acc 1, learning_rate 0.0001
2017-10-03T00:53:25.869122: step 7235, loss 0.00229565, acc 1, learning_rate 0.0001
2017-10-03T00:53:27.021487: step 7236, loss 0.0241507, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:28.173845: step 7237, loss 0.00396496, acc 1, learning_rate 0.0001
2017-10-03T00:53:29.341504: step 7238, loss 0.00488791, acc 1, learning_rate 0.0001
2017-10-03T00:53:30.496962: step 7239, loss 0.00358265, acc 1, learning_rate 0.0001
2017-10-03T00:53:31.655354: step 7240, loss 0.00161346, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:53:31.991995: step 7240, loss 1.47544, acc 0.448921

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7240

2017-10-03T00:53:40.479918: step 7241, loss 0.0415495, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:41.674293: step 7242, loss 0.00330256, acc 1, learning_rate 0.0001
2017-10-03T00:53:42.882412: step 7243, loss 0.00917728, acc 1, learning_rate 0.0001
2017-10-03T00:53:44.028126: step 7244, loss 0.016525, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:45.194333: step 7245, loss 0.00400681, acc 1, learning_rate 0.0001
2017-10-03T00:53:46.365155: step 7246, loss 0.00383541, acc 1, learning_rate 0.0001
2017-10-03T00:53:47.528777: step 7247, loss 0.00159085, acc 1, learning_rate 0.0001
2017-10-03T00:53:48.679394: step 7248, loss 0.00407061, acc 1, learning_rate 0.0001
2017-10-03T00:53:49.889266: step 7249, loss 0.0418177, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:51.046447: step 7250, loss 0.0115426, acc 1, learning_rate 0.0001
2017-10-03T00:53:52.188350: step 7251, loss 0.0148962, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:53.335086: step 7252, loss 0.0152826, acc 0.980392, learning_rate 0.0001
2017-10-03T00:53:54.492147: step 7253, loss 0.00132324, acc 1, learning_rate 0.0001
2017-10-03T00:53:55.635867: step 7254, loss 0.00276684, acc 1, learning_rate 0.0001
2017-10-03T00:53:56.793666: step 7255, loss 0.0278127, acc 0.96875, learning_rate 0.0001
2017-10-03T00:53:57.962119: step 7256, loss 0.0309428, acc 0.984375, learning_rate 0.0001
2017-10-03T00:53:59.114358: step 7257, loss 0.00200788, acc 1, learning_rate 0.0001
2017-10-03T00:54:00.271019: step 7258, loss 0.0108754, acc 1, learning_rate 0.0001
2017-10-03T00:54:01.424954: step 7259, loss 0.00259569, acc 1, learning_rate 0.0001
2017-10-03T00:54:02.580285: step 7260, loss 0.00205438, acc 1, learning_rate 0.0001
2017-10-03T00:54:03.724591: step 7261, loss 0.0433947, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:04.883951: step 7262, loss 0.0154718, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:06.040221: step 7263, loss 0.00154667, acc 1, learning_rate 0.0001
2017-10-03T00:54:07.180703: step 7264, loss 0.00505644, acc 1, learning_rate 0.0001
2017-10-03T00:54:08.339992: step 7265, loss 0.00185435, acc 1, learning_rate 0.0001
2017-10-03T00:54:09.492827: step 7266, loss 0.0116993, acc 1, learning_rate 0.0001
2017-10-03T00:54:10.642432: step 7267, loss 0.0156485, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:11.795063: step 7268, loss 0.00846866, acc 1, learning_rate 0.0001
2017-10-03T00:54:12.981015: step 7269, loss 0.0228448, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:14.134342: step 7270, loss 0.00254142, acc 1, learning_rate 0.0001
2017-10-03T00:54:15.276564: step 7271, loss 0.0129103, acc 1, learning_rate 0.0001
2017-10-03T00:54:16.452399: step 7272, loss 0.00267119, acc 1, learning_rate 0.0001
2017-10-03T00:54:17.594966: step 7273, loss 0.00390909, acc 1, learning_rate 0.0001
2017-10-03T00:54:18.793564: step 7274, loss 0.00636399, acc 1, learning_rate 0.0001
2017-10-03T00:54:19.972237: step 7275, loss 0.00231638, acc 1, learning_rate 0.0001
2017-10-03T00:54:21.153932: step 7276, loss 0.00191295, acc 1, learning_rate 0.0001
2017-10-03T00:54:22.315234: step 7277, loss 0.00360765, acc 1, learning_rate 0.0001
2017-10-03T00:54:23.463527: step 7278, loss 0.00320401, acc 1, learning_rate 0.0001
2017-10-03T00:54:24.616714: step 7279, loss 0.00423498, acc 1, learning_rate 0.0001
2017-10-03T00:54:25.778627: step 7280, loss 0.00414219, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:54:26.099615: step 7280, loss 1.47343, acc 0.453237

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7280

2017-10-03T00:54:34.543115: step 7281, loss 0.0015337, acc 1, learning_rate 0.0001
2017-10-03T00:54:35.714558: step 7282, loss 0.0029216, acc 1, learning_rate 0.0001
2017-10-03T00:54:36.877137: step 7283, loss 0.0210001, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:38.041243: step 7284, loss 0.0330334, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:39.182301: step 7285, loss 0.0145587, acc 1, learning_rate 0.0001
2017-10-03T00:54:40.334751: step 7286, loss 0.00435297, acc 1, learning_rate 0.0001
2017-10-03T00:54:41.481896: step 7287, loss 0.0186885, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:42.660029: step 7288, loss 0.00668089, acc 1, learning_rate 0.0001
2017-10-03T00:54:43.857786: step 7289, loss 0.00281121, acc 1, learning_rate 0.0001
2017-10-03T00:54:44.999940: step 7290, loss 0.00207955, acc 1, learning_rate 0.0001
2017-10-03T00:54:46.161206: step 7291, loss 0.00495361, acc 1, learning_rate 0.0001
2017-10-03T00:54:47.333555: step 7292, loss 0.00217556, acc 1, learning_rate 0.0001
2017-10-03T00:54:48.507794: step 7293, loss 0.00193135, acc 1, learning_rate 0.0001
2017-10-03T00:54:49.664263: step 7294, loss 0.0445971, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:50.816421: step 7295, loss 0.0406589, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:51.975420: step 7296, loss 0.00266151, acc 1, learning_rate 0.0001
2017-10-03T00:54:53.144659: step 7297, loss 0.00291827, acc 1, learning_rate 0.0001
2017-10-03T00:54:54.312848: step 7298, loss 0.00194765, acc 1, learning_rate 0.0001
2017-10-03T00:54:55.477754: step 7299, loss 0.00460364, acc 1, learning_rate 0.0001
2017-10-03T00:54:56.630651: step 7300, loss 0.00619357, acc 1, learning_rate 0.0001
2017-10-03T00:54:57.791303: step 7301, loss 0.0384356, acc 0.984375, learning_rate 0.0001
2017-10-03T00:54:58.952637: step 7302, loss 0.00766938, acc 1, learning_rate 0.0001
2017-10-03T00:55:00.107108: step 7303, loss 0.015506, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:01.253050: step 7304, loss 0.00735927, acc 1, learning_rate 0.0001
2017-10-03T00:55:02.406200: step 7305, loss 0.0193617, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:03.562454: step 7306, loss 0.00114088, acc 1, learning_rate 0.0001
2017-10-03T00:55:04.712521: step 7307, loss 0.0269805, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:05.876136: step 7308, loss 0.0126911, acc 1, learning_rate 0.0001
2017-10-03T00:55:07.019726: step 7309, loss 0.00191177, acc 1, learning_rate 0.0001
2017-10-03T00:55:08.155822: step 7310, loss 0.0228781, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:09.317932: step 7311, loss 0.00153326, acc 1, learning_rate 0.0001
2017-10-03T00:55:10.460327: step 7312, loss 0.00266088, acc 1, learning_rate 0.0001
2017-10-03T00:55:11.614910: step 7313, loss 0.0404474, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:12.770935: step 7314, loss 0.00298032, acc 1, learning_rate 0.0001
2017-10-03T00:55:13.920504: step 7315, loss 0.0293696, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:15.058611: step 7316, loss 0.00911261, acc 1, learning_rate 0.0001
2017-10-03T00:55:16.202553: step 7317, loss 0.00208721, acc 1, learning_rate 0.0001
2017-10-03T00:55:17.354626: step 7318, loss 0.0490111, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:18.497797: step 7319, loss 0.00263038, acc 1, learning_rate 0.0001
2017-10-03T00:55:19.664467: step 7320, loss 0.0015543, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:55:20.000666: step 7320, loss 1.44854, acc 0.460432

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7320

2017-10-03T00:55:27.823286: step 7321, loss 0.0461494, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:28.986694: step 7322, loss 0.00470887, acc 1, learning_rate 0.0001
2017-10-03T00:55:30.130944: step 7323, loss 0.00369882, acc 1, learning_rate 0.0001
2017-10-03T00:55:31.281411: step 7324, loss 0.00108841, acc 1, learning_rate 0.0001
2017-10-03T00:55:32.432519: step 7325, loss 0.00667305, acc 1, learning_rate 0.0001
2017-10-03T00:55:33.574857: step 7326, loss 0.0034033, acc 1, learning_rate 0.0001
2017-10-03T00:55:34.723957: step 7327, loss 0.00271699, acc 1, learning_rate 0.0001
2017-10-03T00:55:35.863128: step 7328, loss 0.0453738, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:37.010971: step 7329, loss 0.00334029, acc 1, learning_rate 0.0001
2017-10-03T00:55:38.152989: step 7330, loss 0.00201375, acc 1, learning_rate 0.0001
2017-10-03T00:55:39.305780: step 7331, loss 0.010483, acc 1, learning_rate 0.0001
2017-10-03T00:55:40.454651: step 7332, loss 0.00244068, acc 1, learning_rate 0.0001
2017-10-03T00:55:41.604550: step 7333, loss 0.0856279, acc 0.953125, learning_rate 0.0001
2017-10-03T00:55:42.763739: step 7334, loss 0.0118646, acc 1, learning_rate 0.0001
2017-10-03T00:55:43.929308: step 7335, loss 0.00257629, acc 1, learning_rate 0.0001
2017-10-03T00:55:45.080965: step 7336, loss 0.00119424, acc 1, learning_rate 0.0001
2017-10-03T00:55:46.226425: step 7337, loss 0.00886519, acc 1, learning_rate 0.0001
2017-10-03T00:55:47.384436: step 7338, loss 0.00442851, acc 1, learning_rate 0.0001
2017-10-03T00:55:48.538488: step 7339, loss 0.00458656, acc 1, learning_rate 0.0001
2017-10-03T00:55:49.689820: step 7340, loss 0.0265609, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:50.852720: step 7341, loss 0.00172798, acc 1, learning_rate 0.0001
2017-10-03T00:55:52.024125: step 7342, loss 0.0011541, acc 1, learning_rate 0.0001
2017-10-03T00:55:53.190786: step 7343, loss 0.00419878, acc 1, learning_rate 0.0001
2017-10-03T00:55:54.337067: step 7344, loss 0.0237038, acc 0.984375, learning_rate 0.0001
2017-10-03T00:55:55.529597: step 7345, loss 0.00340656, acc 1, learning_rate 0.0001
2017-10-03T00:55:56.678015: step 7346, loss 0.00193515, acc 1, learning_rate 0.0001
2017-10-03T00:55:57.817173: step 7347, loss 0.00129978, acc 1, learning_rate 0.0001
2017-10-03T00:55:58.969249: step 7348, loss 0.00149528, acc 1, learning_rate 0.0001
2017-10-03T00:56:00.121869: step 7349, loss 0.0156814, acc 1, learning_rate 0.0001
2017-10-03T00:56:01.245381: step 7350, loss 0.0024136, acc 1, learning_rate 0.0001
2017-10-03T00:56:02.404052: step 7351, loss 0.00436448, acc 1, learning_rate 0.0001
2017-10-03T00:56:03.557931: step 7352, loss 0.0155674, acc 1, learning_rate 0.0001
2017-10-03T00:56:04.707281: step 7353, loss 0.00406149, acc 1, learning_rate 0.0001
2017-10-03T00:56:05.857549: step 7354, loss 0.0276706, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:07.003736: step 7355, loss 0.0103764, acc 1, learning_rate 0.0001
2017-10-03T00:56:08.143523: step 7356, loss 0.00250601, acc 1, learning_rate 0.0001
2017-10-03T00:56:09.299909: step 7357, loss 0.00361292, acc 1, learning_rate 0.0001
2017-10-03T00:56:10.481650: step 7358, loss 0.00158643, acc 1, learning_rate 0.0001
2017-10-03T00:56:11.623471: step 7359, loss 0.0032384, acc 1, learning_rate 0.0001
2017-10-03T00:56:12.771511: step 7360, loss 0.0141721, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:56:13.106905: step 7360, loss 1.44739, acc 0.460432

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7360

2017-10-03T00:56:21.029287: step 7361, loss 0.00309169, acc 1, learning_rate 0.0001
2017-10-03T00:56:22.197811: step 7362, loss 0.00377363, acc 1, learning_rate 0.0001
2017-10-03T00:56:23.366231: step 7363, loss 0.0085144, acc 1, learning_rate 0.0001
2017-10-03T00:56:24.523033: step 7364, loss 0.00492039, acc 1, learning_rate 0.0001
2017-10-03T00:56:25.670058: step 7365, loss 0.00224966, acc 1, learning_rate 0.0001
2017-10-03T00:56:26.838623: step 7366, loss 0.0026431, acc 1, learning_rate 0.0001
2017-10-03T00:56:27.994655: step 7367, loss 0.00702971, acc 1, learning_rate 0.0001
2017-10-03T00:56:29.139007: step 7368, loss 0.00161677, acc 1, learning_rate 0.0001
2017-10-03T00:56:30.293615: step 7369, loss 0.00208733, acc 1, learning_rate 0.0001
2017-10-03T00:56:31.445290: step 7370, loss 0.00240507, acc 1, learning_rate 0.0001
2017-10-03T00:56:32.609478: step 7371, loss 0.0191065, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:33.773554: step 7372, loss 0.00222892, acc 1, learning_rate 0.0001
2017-10-03T00:56:34.930661: step 7373, loss 0.00460588, acc 1, learning_rate 0.0001
2017-10-03T00:56:36.109225: step 7374, loss 0.00626868, acc 1, learning_rate 0.0001
2017-10-03T00:56:37.283372: step 7375, loss 0.0409661, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:38.431602: step 7376, loss 0.0139304, acc 1, learning_rate 0.0001
2017-10-03T00:56:39.589655: step 7377, loss 0.01054, acc 1, learning_rate 0.0001
2017-10-03T00:56:40.741014: step 7378, loss 0.00617277, acc 1, learning_rate 0.0001
2017-10-03T00:56:41.949973: step 7379, loss 0.00174702, acc 1, learning_rate 0.0001
2017-10-03T00:56:43.091248: step 7380, loss 0.0116031, acc 1, learning_rate 0.0001
2017-10-03T00:56:44.261159: step 7381, loss 0.0173785, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:45.412214: step 7382, loss 0.0259569, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:46.574569: step 7383, loss 0.00599073, acc 1, learning_rate 0.0001
2017-10-03T00:56:47.723050: step 7384, loss 0.0026212, acc 1, learning_rate 0.0001
2017-10-03T00:56:48.875886: step 7385, loss 0.0237497, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:50.043725: step 7386, loss 0.0022264, acc 1, learning_rate 0.0001
2017-10-03T00:56:51.214031: step 7387, loss 0.00728197, acc 1, learning_rate 0.0001
2017-10-03T00:56:52.385088: step 7388, loss 0.0231018, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:53.536077: step 7389, loss 0.00272742, acc 1, learning_rate 0.0001
2017-10-03T00:56:54.692544: step 7390, loss 0.0324073, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:55.846280: step 7391, loss 0.0181957, acc 0.984375, learning_rate 0.0001
2017-10-03T00:56:57.000346: step 7392, loss 0.00162117, acc 1, learning_rate 0.0001
2017-10-03T00:56:58.159080: step 7393, loss 0.00266641, acc 1, learning_rate 0.0001
2017-10-03T00:56:59.327091: step 7394, loss 0.00340022, acc 1, learning_rate 0.0001
2017-10-03T00:57:00.490627: step 7395, loss 0.0128977, acc 1, learning_rate 0.0001
2017-10-03T00:57:01.675823: step 7396, loss 0.0390604, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:02.836107: step 7397, loss 0.00122147, acc 1, learning_rate 0.0001
2017-10-03T00:57:04.009626: step 7398, loss 0.011924, acc 1, learning_rate 0.0001
2017-10-03T00:57:05.158893: step 7399, loss 0.00597566, acc 1, learning_rate 0.0001
2017-10-03T00:57:06.317580: step 7400, loss 0.0217516, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T00:57:06.678270: step 7400, loss 1.47129, acc 0.45036

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7400

2017-10-03T00:57:14.081780: step 7401, loss 0.0021033, acc 1, learning_rate 0.0001
2017-10-03T00:57:15.216345: step 7402, loss 0.00195306, acc 1, learning_rate 0.0001
2017-10-03T00:57:16.369868: step 7403, loss 0.00200318, acc 1, learning_rate 0.0001
2017-10-03T00:57:17.528530: step 7404, loss 0.00141224, acc 1, learning_rate 0.0001
2017-10-03T00:57:18.681350: step 7405, loss 0.0192247, acc 1, learning_rate 0.0001
2017-10-03T00:57:19.839470: step 7406, loss 0.000622422, acc 1, learning_rate 0.0001
2017-10-03T00:57:21.028052: step 7407, loss 0.0279793, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:22.175543: step 7408, loss 0.00284404, acc 1, learning_rate 0.0001
2017-10-03T00:57:23.332985: step 7409, loss 0.00590804, acc 1, learning_rate 0.0001
2017-10-03T00:57:24.477070: step 7410, loss 0.0024264, acc 1, learning_rate 0.0001
2017-10-03T00:57:25.628972: step 7411, loss 0.00245091, acc 1, learning_rate 0.0001
2017-10-03T00:57:26.790276: step 7412, loss 0.030298, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:27.947368: step 7413, loss 0.0290924, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:29.093527: step 7414, loss 0.00249395, acc 1, learning_rate 0.0001
2017-10-03T00:57:30.254609: step 7415, loss 0.00315391, acc 1, learning_rate 0.0001
2017-10-03T00:57:31.420870: step 7416, loss 0.00170406, acc 1, learning_rate 0.0001
2017-10-03T00:57:32.572434: step 7417, loss 0.00226259, acc 1, learning_rate 0.0001
2017-10-03T00:57:33.719889: step 7418, loss 0.00249689, acc 1, learning_rate 0.0001
2017-10-03T00:57:34.874222: step 7419, loss 0.00417471, acc 1, learning_rate 0.0001
2017-10-03T00:57:36.052598: step 7420, loss 0.00638374, acc 1, learning_rate 0.0001
2017-10-03T00:57:37.206422: step 7421, loss 0.0170911, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:38.363485: step 7422, loss 0.0551675, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:39.528950: step 7423, loss 0.00210476, acc 1, learning_rate 0.0001
2017-10-03T00:57:40.682095: step 7424, loss 0.00253088, acc 1, learning_rate 0.0001
2017-10-03T00:57:41.831942: step 7425, loss 0.0015908, acc 1, learning_rate 0.0001
2017-10-03T00:57:42.996900: step 7426, loss 0.0094102, acc 1, learning_rate 0.0001
2017-10-03T00:57:44.153116: step 7427, loss 0.00560811, acc 1, learning_rate 0.0001
2017-10-03T00:57:45.305516: step 7428, loss 0.0467221, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:46.475069: step 7429, loss 0.0361723, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:47.635200: step 7430, loss 0.00284826, acc 1, learning_rate 0.0001
2017-10-03T00:57:48.781533: step 7431, loss 0.00884471, acc 1, learning_rate 0.0001
2017-10-03T00:57:49.944968: step 7432, loss 0.0367363, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:51.115819: step 7433, loss 0.029415, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:52.267973: step 7434, loss 0.0323744, acc 0.984375, learning_rate 0.0001
2017-10-03T00:57:53.424195: step 7435, loss 0.0116546, acc 1, learning_rate 0.0001
2017-10-03T00:57:54.581574: step 7436, loss 0.00131916, acc 1, learning_rate 0.0001
2017-10-03T00:57:55.734516: step 7437, loss 0.00304882, acc 1, learning_rate 0.0001
2017-10-03T00:57:56.933056: step 7438, loss 0.00164294, acc 1, learning_rate 0.0001
2017-10-03T00:57:58.091814: step 7439, loss 0.00693348, acc 1, learning_rate 0.0001
2017-10-03T00:57:59.243142: step 7440, loss 0.00432897, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:57:59.598236: step 7440, loss 1.50243, acc 0.388489

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7440

2017-10-03T00:58:07.271553: step 7441, loss 0.00268026, acc 1, learning_rate 0.0001
2017-10-03T00:58:08.462734: step 7442, loss 0.0356836, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:09.629798: step 7443, loss 0.00146348, acc 1, learning_rate 0.0001
2017-10-03T00:58:10.802917: step 7444, loss 0.029143, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:11.948905: step 7445, loss 0.00152517, acc 1, learning_rate 0.0001
2017-10-03T00:58:13.108396: step 7446, loss 0.00814986, acc 1, learning_rate 0.0001
2017-10-03T00:58:14.250966: step 7447, loss 0.00330973, acc 1, learning_rate 0.0001
2017-10-03T00:58:15.386277: step 7448, loss 0.0031759, acc 1, learning_rate 0.0001
2017-10-03T00:58:16.552669: step 7449, loss 0.0189912, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:17.713429: step 7450, loss 0.00172504, acc 1, learning_rate 0.0001
2017-10-03T00:58:18.857401: step 7451, loss 0.0112245, acc 1, learning_rate 0.0001
2017-10-03T00:58:20.005873: step 7452, loss 0.00185489, acc 1, learning_rate 0.0001
2017-10-03T00:58:21.153199: step 7453, loss 0.00221639, acc 1, learning_rate 0.0001
2017-10-03T00:58:22.318627: step 7454, loss 0.0181781, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:23.476063: step 7455, loss 0.00211234, acc 1, learning_rate 0.0001
2017-10-03T00:58:24.654248: step 7456, loss 0.0288903, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:25.804515: step 7457, loss 0.00199705, acc 1, learning_rate 0.0001
2017-10-03T00:58:26.945228: step 7458, loss 0.00418077, acc 1, learning_rate 0.0001
2017-10-03T00:58:28.108288: step 7459, loss 0.0405726, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:29.280118: step 7460, loss 0.000958654, acc 1, learning_rate 0.0001
2017-10-03T00:58:30.455302: step 7461, loss 0.00226579, acc 1, learning_rate 0.0001
2017-10-03T00:58:31.632097: step 7462, loss 0.00218698, acc 1, learning_rate 0.0001
2017-10-03T00:58:32.797149: step 7463, loss 0.00148471, acc 1, learning_rate 0.0001
2017-10-03T00:58:33.965102: step 7464, loss 0.0178865, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:35.193553: step 7465, loss 0.00846352, acc 1, learning_rate 0.0001
2017-10-03T00:58:36.368015: step 7466, loss 0.0197592, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:37.540650: step 7467, loss 0.00263806, acc 1, learning_rate 0.0001
2017-10-03T00:58:38.696984: step 7468, loss 0.00345215, acc 1, learning_rate 0.0001
2017-10-03T00:58:39.847297: step 7469, loss 0.00963445, acc 1, learning_rate 0.0001
2017-10-03T00:58:40.998917: step 7470, loss 0.00292322, acc 1, learning_rate 0.0001
2017-10-03T00:58:42.176124: step 7471, loss 0.0187785, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:43.325451: step 7472, loss 0.0162521, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:44.480641: step 7473, loss 0.0117786, acc 1, learning_rate 0.0001
2017-10-03T00:58:45.644995: step 7474, loss 0.0353915, acc 0.96875, learning_rate 0.0001
2017-10-03T00:58:46.799628: step 7475, loss 0.0418456, acc 0.984375, learning_rate 0.0001
2017-10-03T00:58:47.950560: step 7476, loss 0.00162305, acc 1, learning_rate 0.0001
2017-10-03T00:58:49.109074: step 7477, loss 0.0057678, acc 1, learning_rate 0.0001
2017-10-03T00:58:50.263695: step 7478, loss 0.0115941, acc 1, learning_rate 0.0001
2017-10-03T00:58:51.415273: step 7479, loss 0.00202359, acc 1, learning_rate 0.0001
2017-10-03T00:58:52.566860: step 7480, loss 0.00228337, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:58:52.905897: step 7480, loss 1.45343, acc 0.476259

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7480

2017-10-03T00:58:59.941374: step 7481, loss 0.00266209, acc 1, learning_rate 0.0001
2017-10-03T00:59:01.115777: step 7482, loss 0.0403493, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:02.279399: step 7483, loss 0.00242717, acc 1, learning_rate 0.0001
2017-10-03T00:59:03.421841: step 7484, loss 0.00202252, acc 1, learning_rate 0.0001
2017-10-03T00:59:04.572385: step 7485, loss 0.00928136, acc 1, learning_rate 0.0001
2017-10-03T00:59:05.745131: step 7486, loss 0.00367218, acc 1, learning_rate 0.0001
2017-10-03T00:59:06.896960: step 7487, loss 0.00194101, acc 1, learning_rate 0.0001
2017-10-03T00:59:08.074032: step 7488, loss 0.00195706, acc 1, learning_rate 0.0001
2017-10-03T00:59:09.210794: step 7489, loss 0.00352923, acc 1, learning_rate 0.0001
2017-10-03T00:59:10.366440: step 7490, loss 0.0268679, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:11.518780: step 7491, loss 0.0380898, acc 0.96875, learning_rate 0.0001
2017-10-03T00:59:12.678864: step 7492, loss 0.00153527, acc 1, learning_rate 0.0001
2017-10-03T00:59:13.847646: step 7493, loss 0.00231829, acc 1, learning_rate 0.0001
2017-10-03T00:59:14.997063: step 7494, loss 0.00448879, acc 1, learning_rate 0.0001
2017-10-03T00:59:16.160341: step 7495, loss 0.00208934, acc 1, learning_rate 0.0001
2017-10-03T00:59:17.332013: step 7496, loss 0.0435467, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:18.486047: step 7497, loss 0.00180703, acc 1, learning_rate 0.0001
2017-10-03T00:59:19.639489: step 7498, loss 0.0223198, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:20.797712: step 7499, loss 0.035183, acc 0.96875, learning_rate 0.0001
2017-10-03T00:59:21.955896: step 7500, loss 0.00183605, acc 1, learning_rate 0.0001
2017-10-03T00:59:23.109751: step 7501, loss 0.00267484, acc 1, learning_rate 0.0001
2017-10-03T00:59:24.266849: step 7502, loss 0.0124534, acc 1, learning_rate 0.0001
2017-10-03T00:59:25.416039: step 7503, loss 0.0196701, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:26.577866: step 7504, loss 0.00190255, acc 1, learning_rate 0.0001
2017-10-03T00:59:27.786778: step 7505, loss 0.00225935, acc 1, learning_rate 0.0001
2017-10-03T00:59:28.947489: step 7506, loss 0.0101422, acc 1, learning_rate 0.0001
2017-10-03T00:59:30.112109: step 7507, loss 0.00142367, acc 1, learning_rate 0.0001
2017-10-03T00:59:31.281862: step 7508, loss 0.00347751, acc 1, learning_rate 0.0001
2017-10-03T00:59:32.429332: step 7509, loss 0.0164988, acc 1, learning_rate 0.0001
2017-10-03T00:59:33.578528: step 7510, loss 0.00159424, acc 1, learning_rate 0.0001
2017-10-03T00:59:34.720945: step 7511, loss 0.0102022, acc 1, learning_rate 0.0001
2017-10-03T00:59:35.889826: step 7512, loss 0.0498925, acc 0.96875, learning_rate 0.0001
2017-10-03T00:59:37.036472: step 7513, loss 0.0031666, acc 1, learning_rate 0.0001
2017-10-03T00:59:38.180847: step 7514, loss 0.00370492, acc 1, learning_rate 0.0001
2017-10-03T00:59:39.396062: step 7515, loss 0.0648164, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:40.568087: step 7516, loss 0.0111837, acc 1, learning_rate 0.0001
2017-10-03T00:59:41.736295: step 7517, loss 0.00125908, acc 1, learning_rate 0.0001
2017-10-03T00:59:42.922816: step 7518, loss 0.0159296, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:44.082076: step 7519, loss 0.0014216, acc 1, learning_rate 0.0001
2017-10-03T00:59:45.237786: step 7520, loss 0.00264129, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T00:59:45.577244: step 7520, loss 1.484, acc 0.42446

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7520

2017-10-03T00:59:53.034904: step 7521, loss 0.0139838, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:54.196701: step 7522, loss 0.00140543, acc 1, learning_rate 0.0001
2017-10-03T00:59:55.357444: step 7523, loss 0.0138863, acc 0.984375, learning_rate 0.0001
2017-10-03T00:59:56.516621: step 7524, loss 0.00186061, acc 1, learning_rate 0.0001
2017-10-03T00:59:57.679632: step 7525, loss 0.00371661, acc 1, learning_rate 0.0001
2017-10-03T00:59:58.842259: step 7526, loss 0.00247766, acc 1, learning_rate 0.0001
2017-10-03T01:00:00.021218: step 7527, loss 0.0464793, acc 0.96875, learning_rate 0.0001
2017-10-03T01:00:01.195651: step 7528, loss 0.00237523, acc 1, learning_rate 0.0001
2017-10-03T01:00:02.405796: step 7529, loss 0.00168783, acc 1, learning_rate 0.0001
2017-10-03T01:00:03.553974: step 7530, loss 0.00168974, acc 1, learning_rate 0.0001
2017-10-03T01:00:04.712779: step 7531, loss 0.00213447, acc 1, learning_rate 0.0001
2017-10-03T01:00:05.865391: step 7532, loss 0.00205747, acc 1, learning_rate 0.0001
2017-10-03T01:00:06.997006: step 7533, loss 0.0156125, acc 0.984375, learning_rate 0.0001
2017-10-03T01:00:08.151085: step 7534, loss 0.00390855, acc 1, learning_rate 0.0001
2017-10-03T01:00:09.308193: step 7535, loss 0.0019241, acc 1, learning_rate 0.0001
2017-10-03T01:00:10.470327: step 7536, loss 0.0459243, acc 0.96875, learning_rate 0.0001
2017-10-03T01:00:11.643078: step 7537, loss 0.00839398, acc 1, learning_rate 0.0001
2017-10-03T01:00:12.794618: step 7538, loss 0.0034528, acc 1, learning_rate 0.0001
2017-10-03T01:00:13.944899: step 7539, loss 0.00264666, acc 1, learning_rate 0.0001
2017-10-03T01:00:15.099465: step 7540, loss 0.00182221, acc 1, learning_rate 0.0001
2017-10-03T01:00:16.237097: step 7541, loss 0.0109408, acc 1, learning_rate 0.0001
2017-10-03T01:00:17.392728: step 7542, loss 0.00349001, acc 1, learning_rate 0.0001
2017-10-03T01:00:18.544941: step 7543, loss 0.0043452, acc 1, learning_rate 0.0001
2017-10-03T01:00:19.696451: step 7544, loss 0.00129675, acc 1, learning_rate 0.0001
2017-10-03T01:00:20.848564: step 7545, loss 0.00608757, acc 1, learning_rate 0.0001
2017-10-03T01:00:21.987650: step 7546, loss 0.0010653, acc 1, learning_rate 0.0001
2017-10-03T01:00:23.143428: step 7547, loss 0.00143284, acc 1, learning_rate 0.0001
2017-10-03T01:00:24.311497: step 7548, loss 0.00156841, acc 1, learning_rate 0.0001
2017-10-03T01:00:25.485665: step 7549, loss 0.00212506, acc 1, learning_rate 0.0001
2017-10-03T01:00:26.651415: step 7550, loss 0.00128516, acc 1, learning_rate 0.0001
2017-10-03T01:00:27.818364: step 7551, loss 0.0022039, acc 1, learning_rate 0.0001
2017-10-03T01:00:28.967922: step 7552, loss 0.024167, acc 0.984375, learning_rate 0.0001
2017-10-03T01:00:30.124502: step 7553, loss 0.00182859, acc 1, learning_rate 0.0001
2017-10-03T01:00:31.268157: step 7554, loss 0.00143768, acc 1, learning_rate 0.0001
2017-10-03T01:00:32.410682: step 7555, loss 0.00354776, acc 1, learning_rate 0.0001
2017-10-03T01:00:33.571013: step 7556, loss 0.00304681, acc 1, learning_rate 0.0001
2017-10-03T01:00:34.733031: step 7557, loss 0.00230641, acc 1, learning_rate 0.0001
2017-10-03T01:00:35.879486: step 7558, loss 0.00164642, acc 1, learning_rate 0.0001
2017-10-03T01:00:37.039585: step 7559, loss 0.0158332, acc 0.984375, learning_rate 0.0001
2017-10-03T01:00:38.186856: step 7560, loss 0.0192753, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T01:00:38.522592: step 7560, loss 1.48477, acc 0.43741

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7560

2017-10-03T01:00:45.878218: step 7561, loss 0.0233025, acc 0.984375, learning_rate 0.0001
2017-10-03T01:00:47.038647: step 7562, loss 0.0210356, acc 0.984375, learning_rate 0.0001
2017-10-03T01:00:48.183818: step 7563, loss 0.0016694, acc 1, learning_rate 0.0001
2017-10-03T01:00:49.336865: step 7564, loss 0.00774266, acc 1, learning_rate 0.0001
2017-10-03T01:00:50.512687: step 7565, loss 0.00281646, acc 1, learning_rate 0.0001
2017-10-03T01:00:51.670765: step 7566, loss 0.00169658, acc 1, learning_rate 0.0001
2017-10-03T01:00:52.821438: step 7567, loss 0.00351137, acc 1, learning_rate 0.0001
2017-10-03T01:00:53.981633: step 7568, loss 0.00223844, acc 1, learning_rate 0.0001
2017-10-03T01:00:55.137262: step 7569, loss 0.021114, acc 0.984375, learning_rate 0.0001
2017-10-03T01:00:56.302043: step 7570, loss 0.00243456, acc 1, learning_rate 0.0001
2017-10-03T01:00:57.455979: step 7571, loss 0.0403758, acc 0.984375, learning_rate 0.0001
2017-10-03T01:00:58.617721: step 7572, loss 0.00221401, acc 1, learning_rate 0.0001
2017-10-03T01:00:59.789616: step 7573, loss 0.00906472, acc 1, learning_rate 0.0001
2017-10-03T01:01:00.938370: step 7574, loss 0.00157036, acc 1, learning_rate 0.0001
2017-10-03T01:01:02.086888: step 7575, loss 0.00376045, acc 1, learning_rate 0.0001
2017-10-03T01:01:03.248096: step 7576, loss 0.00218196, acc 1, learning_rate 0.0001
2017-10-03T01:01:04.406643: step 7577, loss 0.0679276, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:05.569598: step 7578, loss 0.0400014, acc 0.96875, learning_rate 0.0001
2017-10-03T01:01:06.718399: step 7579, loss 0.00376285, acc 1, learning_rate 0.0001
2017-10-03T01:01:07.880371: step 7580, loss 0.0018014, acc 1, learning_rate 0.0001
2017-10-03T01:01:09.027433: step 7581, loss 0.00296106, acc 1, learning_rate 0.0001
2017-10-03T01:01:10.183118: step 7582, loss 0.00226487, acc 1, learning_rate 0.0001
2017-10-03T01:01:11.361357: step 7583, loss 0.010312, acc 1, learning_rate 0.0001
2017-10-03T01:01:12.513772: step 7584, loss 0.00124042, acc 1, learning_rate 0.0001
2017-10-03T01:01:13.679645: step 7585, loss 0.0208298, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:14.844917: step 7586, loss 0.0319612, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:15.996309: step 7587, loss 0.00122524, acc 1, learning_rate 0.0001
2017-10-03T01:01:17.153909: step 7588, loss 0.0025918, acc 1, learning_rate 0.0001
2017-10-03T01:01:18.304588: step 7589, loss 0.00146186, acc 1, learning_rate 0.0001
2017-10-03T01:01:19.478887: step 7590, loss 0.00261893, acc 1, learning_rate 0.0001
2017-10-03T01:01:20.630777: step 7591, loss 0.0101531, acc 1, learning_rate 0.0001
2017-10-03T01:01:21.793817: step 7592, loss 0.00256178, acc 1, learning_rate 0.0001
2017-10-03T01:01:22.962786: step 7593, loss 0.00704182, acc 1, learning_rate 0.0001
2017-10-03T01:01:24.107234: step 7594, loss 0.00110801, acc 1, learning_rate 0.0001
2017-10-03T01:01:25.268130: step 7595, loss 0.00130907, acc 1, learning_rate 0.0001
2017-10-03T01:01:26.427148: step 7596, loss 0.00963934, acc 1, learning_rate 0.0001
2017-10-03T01:01:27.590918: step 7597, loss 0.0147283, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:28.761909: step 7598, loss 0.00187246, acc 1, learning_rate 0.0001
2017-10-03T01:01:29.950437: step 7599, loss 0.0219248, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:31.103036: step 7600, loss 0.0135008, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T01:01:31.450212: step 7600, loss 1.53291, acc 0.356835

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7600

2017-10-03T01:01:38.277566: step 7601, loss 0.0243396, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:39.469315: step 7602, loss 0.01087, acc 1, learning_rate 0.0001
2017-10-03T01:01:40.619929: step 7603, loss 0.00350005, acc 1, learning_rate 0.0001
2017-10-03T01:01:41.788312: step 7604, loss 0.00217773, acc 1, learning_rate 0.0001
2017-10-03T01:01:42.924762: step 7605, loss 0.02726, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:44.077049: step 7606, loss 0.00165607, acc 1, learning_rate 0.0001
2017-10-03T01:01:45.233781: step 7607, loss 0.00221768, acc 1, learning_rate 0.0001
2017-10-03T01:01:46.385412: step 7608, loss 0.00305341, acc 1, learning_rate 0.0001
2017-10-03T01:01:47.534483: step 7609, loss 0.00265614, acc 1, learning_rate 0.0001
2017-10-03T01:01:48.799218: step 7610, loss 0.00138615, acc 1, learning_rate 0.0001
2017-10-03T01:01:49.944812: step 7611, loss 0.000909074, acc 1, learning_rate 0.0001
2017-10-03T01:01:51.095513: step 7612, loss 0.00126121, acc 1, learning_rate 0.0001
2017-10-03T01:01:52.244487: step 7613, loss 0.00389649, acc 1, learning_rate 0.0001
2017-10-03T01:01:53.393759: step 7614, loss 0.0168041, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:54.643715: step 7615, loss 0.00278246, acc 1, learning_rate 0.0001
2017-10-03T01:01:55.792923: step 7616, loss 0.0303368, acc 0.984375, learning_rate 0.0001
2017-10-03T01:01:56.960073: step 7617, loss 0.046896, acc 0.96875, learning_rate 0.0001
2017-10-03T01:01:58.134460: step 7618, loss 0.00184995, acc 1, learning_rate 0.0001
2017-10-03T01:01:59.307450: step 7619, loss 0.00172218, acc 1, learning_rate 0.0001
2017-10-03T01:02:00.464071: step 7620, loss 0.00104488, acc 1, learning_rate 0.0001
2017-10-03T01:02:01.624257: step 7621, loss 0.0029126, acc 1, learning_rate 0.0001
2017-10-03T01:02:02.788622: step 7622, loss 0.00300547, acc 1, learning_rate 0.0001
2017-10-03T01:02:03.922031: step 7623, loss 0.00491338, acc 1, learning_rate 0.0001
2017-10-03T01:02:05.060870: step 7624, loss 0.0037523, acc 1, learning_rate 0.0001
2017-10-03T01:02:06.218195: step 7625, loss 0.00258537, acc 1, learning_rate 0.0001
2017-10-03T01:02:07.364772: step 7626, loss 0.019725, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:08.535732: step 7627, loss 0.00200274, acc 1, learning_rate 0.0001
2017-10-03T01:02:09.683212: step 7628, loss 0.0125228, acc 1, learning_rate 0.0001
2017-10-03T01:02:10.827037: step 7629, loss 0.00174217, acc 1, learning_rate 0.0001
2017-10-03T01:02:11.975060: step 7630, loss 0.00281548, acc 1, learning_rate 0.0001
2017-10-03T01:02:13.139526: step 7631, loss 0.00201861, acc 1, learning_rate 0.0001
2017-10-03T01:02:14.309780: step 7632, loss 0.00476624, acc 1, learning_rate 0.0001
2017-10-03T01:02:15.472455: step 7633, loss 0.0494203, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:16.617726: step 7634, loss 0.00245726, acc 1, learning_rate 0.0001
2017-10-03T01:02:17.779991: step 7635, loss 0.0176232, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:18.934734: step 7636, loss 0.00912673, acc 1, learning_rate 0.0001
2017-10-03T01:02:20.097343: step 7637, loss 0.0365731, acc 0.96875, learning_rate 0.0001
2017-10-03T01:02:21.266848: step 7638, loss 0.0132741, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:22.431067: step 7639, loss 0.0383277, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:23.594400: step 7640, loss 0.00169751, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T01:02:23.965903: step 7640, loss 1.48155, acc 0.435971

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7640

2017-10-03T01:02:32.110804: step 7641, loss 0.0144891, acc 1, learning_rate 0.0001
2017-10-03T01:02:33.266694: step 7642, loss 0.00896346, acc 1, learning_rate 0.0001
2017-10-03T01:02:34.417467: step 7643, loss 0.00256884, acc 1, learning_rate 0.0001
2017-10-03T01:02:35.548205: step 7644, loss 0.012947, acc 1, learning_rate 0.0001
2017-10-03T01:02:36.702507: step 7645, loss 0.00209043, acc 1, learning_rate 0.0001
2017-10-03T01:02:37.866325: step 7646, loss 0.0449884, acc 0.96875, learning_rate 0.0001
2017-10-03T01:02:39.041780: step 7647, loss 0.00677615, acc 1, learning_rate 0.0001
2017-10-03T01:02:40.193909: step 7648, loss 0.00297975, acc 1, learning_rate 0.0001
2017-10-03T01:02:41.365931: step 7649, loss 0.00129884, acc 1, learning_rate 0.0001
2017-10-03T01:02:42.540485: step 7650, loss 0.00215267, acc 1, learning_rate 0.0001
2017-10-03T01:02:43.699066: step 7651, loss 0.00249945, acc 1, learning_rate 0.0001
2017-10-03T01:02:44.857441: step 7652, loss 0.0103287, acc 1, learning_rate 0.0001
2017-10-03T01:02:46.058521: step 7653, loss 0.0109575, acc 1, learning_rate 0.0001
2017-10-03T01:02:47.214024: step 7654, loss 0.00410374, acc 1, learning_rate 0.0001
2017-10-03T01:02:48.368867: step 7655, loss 0.00133976, acc 1, learning_rate 0.0001
2017-10-03T01:02:49.514591: step 7656, loss 0.00150554, acc 1, learning_rate 0.0001
2017-10-03T01:02:50.662831: step 7657, loss 0.00475756, acc 1, learning_rate 0.0001
2017-10-03T01:02:51.828287: step 7658, loss 0.0187893, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:52.983873: step 7659, loss 0.00333044, acc 1, learning_rate 0.0001
2017-10-03T01:02:54.139040: step 7660, loss 0.0258948, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:55.298686: step 7661, loss 0.00270523, acc 1, learning_rate 0.0001
2017-10-03T01:02:56.443910: step 7662, loss 0.0546651, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:57.618610: step 7663, loss 0.00212615, acc 1, learning_rate 0.0001
2017-10-03T01:02:58.775428: step 7664, loss 0.023259, acc 0.984375, learning_rate 0.0001
2017-10-03T01:02:59.933009: step 7665, loss 0.00253588, acc 1, learning_rate 0.0001
2017-10-03T01:03:01.087707: step 7666, loss 0.000788704, acc 1, learning_rate 0.0001
2017-10-03T01:03:02.244623: step 7667, loss 0.000771376, acc 1, learning_rate 0.0001
2017-10-03T01:03:03.427102: step 7668, loss 0.00786519, acc 1, learning_rate 0.0001
2017-10-03T01:03:04.586924: step 7669, loss 0.0397655, acc 0.96875, learning_rate 0.0001
2017-10-03T01:03:05.739921: step 7670, loss 0.0123003, acc 1, learning_rate 0.0001
2017-10-03T01:03:06.875813: step 7671, loss 0.00285893, acc 1, learning_rate 0.0001
2017-10-03T01:03:08.033621: step 7672, loss 0.00127188, acc 1, learning_rate 0.0001
2017-10-03T01:03:09.200576: step 7673, loss 0.00157306, acc 1, learning_rate 0.0001
2017-10-03T01:03:10.358111: step 7674, loss 0.00142963, acc 1, learning_rate 0.0001
2017-10-03T01:03:11.493632: step 7675, loss 0.0111601, acc 1, learning_rate 0.0001
2017-10-03T01:03:12.649362: step 7676, loss 0.034531, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:13.810440: step 7677, loss 0.0215741, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:14.970374: step 7678, loss 0.00185043, acc 1, learning_rate 0.0001
2017-10-03T01:03:16.144551: step 7679, loss 0.00294221, acc 1, learning_rate 0.0001
2017-10-03T01:03:17.302095: step 7680, loss 0.00345643, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T01:03:17.623614: step 7680, loss 1.46732, acc 0.45036

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7680

2017-10-03T01:03:24.837483: step 7681, loss 0.00686457, acc 1, learning_rate 0.0001
2017-10-03T01:03:26.019997: step 7682, loss 0.00141681, acc 1, learning_rate 0.0001
2017-10-03T01:03:27.172136: step 7683, loss 0.00214193, acc 1, learning_rate 0.0001
2017-10-03T01:03:28.322842: step 7684, loss 0.00357157, acc 1, learning_rate 0.0001
2017-10-03T01:03:29.482658: step 7685, loss 0.0015269, acc 1, learning_rate 0.0001
2017-10-03T01:03:30.660132: step 7686, loss 0.00323457, acc 1, learning_rate 0.0001
2017-10-03T01:03:31.799142: step 7687, loss 0.0387254, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:32.941906: step 7688, loss 0.0323044, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:34.095449: step 7689, loss 0.00128455, acc 1, learning_rate 0.0001
2017-10-03T01:03:35.251189: step 7690, loss 0.00931983, acc 1, learning_rate 0.0001
2017-10-03T01:03:36.418334: step 7691, loss 0.00220244, acc 1, learning_rate 0.0001
2017-10-03T01:03:37.575265: step 7692, loss 0.0608023, acc 0.96875, learning_rate 0.0001
2017-10-03T01:03:38.729689: step 7693, loss 0.00546377, acc 1, learning_rate 0.0001
2017-10-03T01:03:39.880586: step 7694, loss 0.0275971, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:41.029143: step 7695, loss 0.0224014, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:42.187746: step 7696, loss 0.00301941, acc 1, learning_rate 0.0001
2017-10-03T01:03:43.350786: step 7697, loss 0.0311172, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:44.498751: step 7698, loss 0.0340703, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:45.665949: step 7699, loss 0.00280207, acc 1, learning_rate 0.0001
2017-10-03T01:03:46.841861: step 7700, loss 0.00181603, acc 1, learning_rate 0.0001
2017-10-03T01:03:47.993711: step 7701, loss 0.00169629, acc 1, learning_rate 0.0001
2017-10-03T01:03:49.140180: step 7702, loss 0.00231238, acc 1, learning_rate 0.0001
2017-10-03T01:03:50.284475: step 7703, loss 0.00208508, acc 1, learning_rate 0.0001
2017-10-03T01:03:51.447926: step 7704, loss 0.00120002, acc 1, learning_rate 0.0001
2017-10-03T01:03:52.622217: step 7705, loss 0.0118713, acc 1, learning_rate 0.0001
2017-10-03T01:03:53.770483: step 7706, loss 0.00163616, acc 1, learning_rate 0.0001
2017-10-03T01:03:54.918152: step 7707, loss 0.0479247, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:56.093859: step 7708, loss 0.0047712, acc 1, learning_rate 0.0001
2017-10-03T01:03:57.249880: step 7709, loss 0.0167665, acc 0.984375, learning_rate 0.0001
2017-10-03T01:03:58.398780: step 7710, loss 0.00143156, acc 1, learning_rate 0.0001
2017-10-03T01:03:59.550175: step 7711, loss 0.00295147, acc 1, learning_rate 0.0001
2017-10-03T01:04:00.709369: step 7712, loss 0.0138098, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:01.877037: step 7713, loss 0.00310678, acc 1, learning_rate 0.0001
2017-10-03T01:04:03.034025: step 7714, loss 0.00656643, acc 1, learning_rate 0.0001
2017-10-03T01:04:04.200202: step 7715, loss 0.0165533, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:05.355663: step 7716, loss 0.00153156, acc 1, learning_rate 0.0001
2017-10-03T01:04:06.537217: step 7717, loss 0.0396656, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:07.707705: step 7718, loss 0.00148298, acc 1, learning_rate 0.0001
2017-10-03T01:04:08.856801: step 7719, loss 0.0477326, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:10.016319: step 7720, loss 0.0146242, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T01:04:10.361820: step 7720, loss 1.47134, acc 0.458993

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7720

2017-10-03T01:04:17.966056: step 7721, loss 0.00229033, acc 1, learning_rate 0.0001
2017-10-03T01:04:19.136035: step 7722, loss 0.0013303, acc 1, learning_rate 0.0001
2017-10-03T01:04:20.286971: step 7723, loss 0.00503372, acc 1, learning_rate 0.0001
2017-10-03T01:04:21.448313: step 7724, loss 0.0013304, acc 1, learning_rate 0.0001
2017-10-03T01:04:22.603204: step 7725, loss 0.0126703, acc 1, learning_rate 0.0001
2017-10-03T01:04:23.753845: step 7726, loss 0.0510244, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:24.915799: step 7727, loss 0.0156951, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:26.074245: step 7728, loss 0.041807, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:27.243908: step 7729, loss 0.00125297, acc 1, learning_rate 0.0001
2017-10-03T01:04:28.414644: step 7730, loss 0.00938657, acc 1, learning_rate 0.0001
2017-10-03T01:04:29.570915: step 7731, loss 0.0183577, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:30.751933: step 7732, loss 0.0176417, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:31.921863: step 7733, loss 0.00974844, acc 1, learning_rate 0.0001
2017-10-03T01:04:33.075856: step 7734, loss 0.0105706, acc 1, learning_rate 0.0001
2017-10-03T01:04:34.224416: step 7735, loss 0.00572114, acc 1, learning_rate 0.0001
2017-10-03T01:04:35.381802: step 7736, loss 0.00203403, acc 1, learning_rate 0.0001
2017-10-03T01:04:36.537487: step 7737, loss 0.00143017, acc 1, learning_rate 0.0001
2017-10-03T01:04:37.693087: step 7738, loss 0.000921446, acc 1, learning_rate 0.0001
2017-10-03T01:04:39.080997: step 7739, loss 0.00178825, acc 1, learning_rate 0.0001
2017-10-03T01:04:40.230802: step 7740, loss 0.00198311, acc 1, learning_rate 0.0001
2017-10-03T01:04:41.397225: step 7741, loss 0.0240737, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:42.541997: step 7742, loss 0.00256115, acc 1, learning_rate 0.0001
2017-10-03T01:04:43.698146: step 7743, loss 0.040533, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:44.855885: step 7744, loss 0.0017325, acc 1, learning_rate 0.0001
2017-10-03T01:04:45.988274: step 7745, loss 0.00898987, acc 1, learning_rate 0.0001
2017-10-03T01:04:47.135036: step 7746, loss 0.0143514, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:48.285240: step 7747, loss 0.0140799, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:49.444414: step 7748, loss 0.00197675, acc 1, learning_rate 0.0001
2017-10-03T01:04:50.617094: step 7749, loss 0.0279446, acc 0.984375, learning_rate 0.0001
2017-10-03T01:04:51.796128: step 7750, loss 0.00402111, acc 1, learning_rate 0.0001
2017-10-03T01:04:52.945611: step 7751, loss 0.00262028, acc 1, learning_rate 0.0001
2017-10-03T01:04:54.092943: step 7752, loss 0.0101624, acc 1, learning_rate 0.0001
2017-10-03T01:04:55.249896: step 7753, loss 0.00254524, acc 1, learning_rate 0.0001
2017-10-03T01:04:56.410951: step 7754, loss 0.00258656, acc 1, learning_rate 0.0001
2017-10-03T01:04:57.564648: step 7755, loss 0.00170229, acc 1, learning_rate 0.0001
2017-10-03T01:04:58.713003: step 7756, loss 0.00308094, acc 1, learning_rate 0.0001
2017-10-03T01:04:59.863901: step 7757, loss 0.0185324, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:01.020882: step 7758, loss 0.0354215, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:02.325645: step 7759, loss 0.00246877, acc 1, learning_rate 0.0001
2017-10-03T01:05:03.549593: step 7760, loss 0.00312662, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T01:05:03.880768: step 7760, loss 1.57712, acc 0.32518

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7760

2017-10-03T01:05:11.362233: step 7761, loss 0.00285705, acc 1, learning_rate 0.0001
2017-10-03T01:05:12.552874: step 7762, loss 0.00221877, acc 1, learning_rate 0.0001
2017-10-03T01:05:13.717099: step 7763, loss 0.042472, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:14.890696: step 7764, loss 0.0336369, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:16.037230: step 7765, loss 0.0337334, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:17.183826: step 7766, loss 0.00131887, acc 1, learning_rate 0.0001
2017-10-03T01:05:18.349781: step 7767, loss 0.0034797, acc 1, learning_rate 0.0001
2017-10-03T01:05:19.511203: step 7768, loss 0.0144984, acc 1, learning_rate 0.0001
2017-10-03T01:05:20.661560: step 7769, loss 0.00236948, acc 1, learning_rate 0.0001
2017-10-03T01:05:21.829793: step 7770, loss 0.0378236, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:22.973022: step 7771, loss 0.0338649, acc 0.96875, learning_rate 0.0001
2017-10-03T01:05:24.123595: step 7772, loss 0.00113409, acc 1, learning_rate 0.0001
2017-10-03T01:05:25.277931: step 7773, loss 0.00322059, acc 1, learning_rate 0.0001
2017-10-03T01:05:26.431397: step 7774, loss 0.016064, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:27.582467: step 7775, loss 0.0021826, acc 1, learning_rate 0.0001
2017-10-03T01:05:28.737432: step 7776, loss 0.0396241, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:29.912610: step 7777, loss 0.00204628, acc 1, learning_rate 0.0001
2017-10-03T01:05:31.075533: step 7778, loss 0.0019891, acc 1, learning_rate 0.0001
2017-10-03T01:05:32.252437: step 7779, loss 0.0245817, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:33.421484: step 7780, loss 0.02185, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:34.583993: step 7781, loss 0.0013997, acc 1, learning_rate 0.0001
2017-10-03T01:05:35.745273: step 7782, loss 0.00172848, acc 1, learning_rate 0.0001
2017-10-03T01:05:36.924975: step 7783, loss 0.00161002, acc 1, learning_rate 0.0001
2017-10-03T01:05:38.117217: step 7784, loss 0.00456007, acc 1, learning_rate 0.0001
2017-10-03T01:05:39.297897: step 7785, loss 0.00158785, acc 1, learning_rate 0.0001
2017-10-03T01:05:40.462032: step 7786, loss 0.0136842, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:41.617742: step 7787, loss 0.00378975, acc 1, learning_rate 0.0001
2017-10-03T01:05:42.756909: step 7788, loss 0.0171911, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:43.913124: step 7789, loss 0.00155402, acc 1, learning_rate 0.0001
2017-10-03T01:05:45.052839: step 7790, loss 0.00569197, acc 1, learning_rate 0.0001
2017-10-03T01:05:46.204757: step 7791, loss 0.00151003, acc 1, learning_rate 0.0001
2017-10-03T01:05:47.348625: step 7792, loss 0.00252918, acc 1, learning_rate 0.0001
2017-10-03T01:05:48.498758: step 7793, loss 0.00277769, acc 1, learning_rate 0.0001
2017-10-03T01:05:49.652074: step 7794, loss 0.0114879, acc 1, learning_rate 0.0001
2017-10-03T01:05:50.806138: step 7795, loss 0.032596, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:51.964062: step 7796, loss 0.00941647, acc 1, learning_rate 0.0001
2017-10-03T01:05:53.144202: step 7797, loss 0.00275894, acc 1, learning_rate 0.0001
2017-10-03T01:05:54.309635: step 7798, loss 0.00928385, acc 1, learning_rate 0.0001
2017-10-03T01:05:55.458943: step 7799, loss 0.0132939, acc 0.984375, learning_rate 0.0001
2017-10-03T01:05:56.650466: step 7800, loss 0.0361945, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-03T01:05:56.994407: step 7800, loss 1.51851, acc 0.371223

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7800

2017-10-03T01:06:04.751757: step 7801, loss 0.00539382, acc 1, learning_rate 0.0001
2017-10-03T01:06:05.942025: step 7802, loss 0.00102081, acc 1, learning_rate 0.0001
2017-10-03T01:06:07.096023: step 7803, loss 0.00371601, acc 1, learning_rate 0.0001
2017-10-03T01:06:08.261020: step 7804, loss 0.00197983, acc 1, learning_rate 0.0001
2017-10-03T01:06:09.425914: step 7805, loss 0.00514663, acc 1, learning_rate 0.0001
2017-10-03T01:06:10.596248: step 7806, loss 0.00159075, acc 1, learning_rate 0.0001
2017-10-03T01:06:11.799289: step 7807, loss 0.00441849, acc 1, learning_rate 0.0001
2017-10-03T01:06:12.964585: step 7808, loss 0.0407628, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:14.140952: step 7809, loss 0.00282817, acc 1, learning_rate 0.0001
2017-10-03T01:06:15.308626: step 7810, loss 0.00165268, acc 1, learning_rate 0.0001
2017-10-03T01:06:16.468079: step 7811, loss 0.0127185, acc 1, learning_rate 0.0001
2017-10-03T01:06:17.626209: step 7812, loss 0.0019923, acc 1, learning_rate 0.0001
2017-10-03T01:06:18.993961: step 7813, loss 0.0382229, acc 0.96875, learning_rate 0.0001
2017-10-03T01:06:20.172357: step 7814, loss 0.0206785, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:21.330363: step 7815, loss 0.0022188, acc 1, learning_rate 0.0001
2017-10-03T01:06:22.519488: step 7816, loss 0.0538328, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:23.706350: step 7817, loss 0.00532293, acc 1, learning_rate 0.0001
2017-10-03T01:06:24.866958: step 7818, loss 0.0329151, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:26.032900: step 7819, loss 0.00373651, acc 1, learning_rate 0.0001
2017-10-03T01:06:27.196352: step 7820, loss 0.0399827, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:28.354984: step 7821, loss 0.00154274, acc 1, learning_rate 0.0001
2017-10-03T01:06:29.508828: step 7822, loss 0.00126613, acc 1, learning_rate 0.0001
2017-10-03T01:06:30.671979: step 7823, loss 0.00145507, acc 1, learning_rate 0.0001
2017-10-03T01:06:31.868537: step 7824, loss 0.00166541, acc 1, learning_rate 0.0001
2017-10-03T01:06:33.026616: step 7825, loss 0.00163816, acc 1, learning_rate 0.0001
2017-10-03T01:06:34.191826: step 7826, loss 0.00230574, acc 1, learning_rate 0.0001
2017-10-03T01:06:35.345547: step 7827, loss 0.00399882, acc 1, learning_rate 0.0001
2017-10-03T01:06:36.495923: step 7828, loss 0.00230307, acc 1, learning_rate 0.0001
2017-10-03T01:06:37.650731: step 7829, loss 0.0427753, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:38.809991: step 7830, loss 0.000991043, acc 1, learning_rate 0.0001
2017-10-03T01:06:39.953770: step 7831, loss 0.0450116, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:41.165914: step 7832, loss 0.0237317, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:42.325605: step 7833, loss 0.00262613, acc 1, learning_rate 0.0001
2017-10-03T01:06:43.494970: step 7834, loss 0.0110067, acc 1, learning_rate 0.0001
2017-10-03T01:06:44.652666: step 7835, loss 0.0369381, acc 0.96875, learning_rate 0.0001
2017-10-03T01:06:45.815823: step 7836, loss 0.0258378, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:46.959189: step 7837, loss 0.0645311, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:48.111120: step 7838, loss 0.025057, acc 0.984375, learning_rate 0.0001
2017-10-03T01:06:49.272966: step 7839, loss 0.00179365, acc 1, learning_rate 0.0001
2017-10-03T01:06:50.396883: step 7840, loss 0.00211227, acc 1, learning_rate 0.0001

Evaluation:
2017-10-03T01:06:50.747410: step 7840, loss 1.50856, acc 0.404317

Saved model checkpoint to /home/sheep/bigdata/runs/1507000132/checkpoints/model-7840

