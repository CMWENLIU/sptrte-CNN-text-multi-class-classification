
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=64

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507653779

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T11:43:04.896878: step 1, loss 9.80536, acc 0.125, learning_rate 0.005
2017-10-10T11:43:05.104444: step 2, loss 6.08959, acc 0.203125, learning_rate 0.00498
2017-10-10T11:43:05.284526: step 3, loss 4.99677, acc 0.296875, learning_rate 0.00496008
2017-10-10T11:43:05.485436: step 4, loss 5.3489, acc 0.4375, learning_rate 0.00494024
2017-10-10T11:43:05.690305: step 5, loss 6.87075, acc 0.296875, learning_rate 0.00492049
2017-10-10T11:43:05.884764: step 6, loss 4.75602, acc 0.40625, learning_rate 0.00490081
2017-10-10T11:43:06.081019: step 7, loss 4.49838, acc 0.40625, learning_rate 0.00488121
2017-10-10T11:43:06.262570: step 8, loss 4.59932, acc 0.390625, learning_rate 0.0048617
2017-10-10T11:43:06.438816: step 9, loss 4.96615, acc 0.453125, learning_rate 0.00484226
2017-10-10T11:43:06.617227: step 10, loss 3.29452, acc 0.4375, learning_rate 0.00482291
2017-10-10T11:43:06.793088: step 11, loss 3.00783, acc 0.515625, learning_rate 0.00480363
2017-10-10T11:43:06.973935: step 12, loss 3.03785, acc 0.421875, learning_rate 0.00478443
2017-10-10T11:43:07.164044: step 13, loss 3.34573, acc 0.390625, learning_rate 0.00476531
2017-10-10T11:43:07.346550: step 14, loss 2.91587, acc 0.453125, learning_rate 0.00474627
2017-10-10T11:43:07.505004: step 15, loss 3.03235, acc 0.4375, learning_rate 0.0047273
2017-10-10T11:43:07.665502: step 16, loss 2.32854, acc 0.5, learning_rate 0.00470841
2017-10-10T11:43:07.831966: step 17, loss 2.18998, acc 0.609375, learning_rate 0.0046896
2017-10-10T11:43:08.012852: step 18, loss 2.50738, acc 0.46875, learning_rate 0.00467087
2017-10-10T11:43:08.195456: step 19, loss 1.80962, acc 0.5, learning_rate 0.00465221
2017-10-10T11:43:08.368863: step 20, loss 2.21413, acc 0.5625, learning_rate 0.00463363
2017-10-10T11:43:08.532299: step 21, loss 2.60556, acc 0.609375, learning_rate 0.00461513
2017-10-10T11:43:08.694293: step 22, loss 2.05726, acc 0.625, learning_rate 0.0045967
2017-10-10T11:43:08.864823: step 23, loss 1.11197, acc 0.75, learning_rate 0.00457834
2017-10-10T11:43:09.072406: step 24, loss 2.62471, acc 0.5, learning_rate 0.00456006
2017-10-10T11:43:09.248850: step 25, loss 2.25808, acc 0.578125, learning_rate 0.00454186
2017-10-10T11:43:09.408719: step 26, loss 1.42739, acc 0.71875, learning_rate 0.00452373
2017-10-10T11:43:09.568396: step 27, loss 1.1184, acc 0.625, learning_rate 0.00450567
2017-10-10T11:43:09.718998: step 28, loss 1.74201, acc 0.546875, learning_rate 0.00448769
2017-10-10T11:43:09.884866: step 29, loss 1.58015, acc 0.640625, learning_rate 0.00446978
2017-10-10T11:43:10.050762: step 30, loss 1.82776, acc 0.5625, learning_rate 0.00445194
2017-10-10T11:43:10.232983: step 31, loss 0.916812, acc 0.734375, learning_rate 0.00443418
2017-10-10T11:43:10.441374: step 32, loss 1.55481, acc 0.65625, learning_rate 0.00441649
2017-10-10T11:43:10.635935: step 33, loss 1.56541, acc 0.671875, learning_rate 0.00439887
2017-10-10T11:43:10.828948: step 34, loss 1.49876, acc 0.671875, learning_rate 0.00438132
2017-10-10T11:43:11.030492: step 35, loss 1.45028, acc 0.71875, learning_rate 0.00436385
2017-10-10T11:43:11.209073: step 36, loss 0.94546, acc 0.734375, learning_rate 0.00434644
2017-10-10T11:43:11.393730: step 37, loss 1.65304, acc 0.6875, learning_rate 0.00432911
2017-10-10T11:43:11.572401: step 38, loss 1.48338, acc 0.609375, learning_rate 0.00431185
2017-10-10T11:43:11.751190: step 39, loss 1.36088, acc 0.6875, learning_rate 0.00429465
2017-10-10T11:43:11.942389: step 40, loss 1.55491, acc 0.640625, learning_rate 0.00427753

Evaluation:
2017-10-10T11:43:12.850811: step 40, loss 0.395472, acc 0.856115

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-40

2017-10-10T11:43:14.393024: step 41, loss 1.19542, acc 0.796875, learning_rate 0.00426048
2017-10-10T11:43:14.595293: step 42, loss 1.03271, acc 0.6875, learning_rate 0.0042435
2017-10-10T11:43:14.787814: step 43, loss 1.0175, acc 0.75, learning_rate 0.00422659
2017-10-10T11:43:14.981018: step 44, loss 1.33547, acc 0.703125, learning_rate 0.00420974
2017-10-10T11:43:15.180535: step 45, loss 1.04244, acc 0.734375, learning_rate 0.00419297
2017-10-10T11:43:15.362875: step 46, loss 1.55111, acc 0.671875, learning_rate 0.00417626
2017-10-10T11:43:15.525245: step 47, loss 1.35521, acc 0.671875, learning_rate 0.00415962
2017-10-10T11:43:15.696932: step 48, loss 1.16556, acc 0.703125, learning_rate 0.00414305
2017-10-10T11:43:15.884849: step 49, loss 0.767949, acc 0.75, learning_rate 0.00412655
2017-10-10T11:43:16.063413: step 50, loss 1.33869, acc 0.671875, learning_rate 0.00411011
2017-10-10T11:43:16.244830: step 51, loss 0.972563, acc 0.734375, learning_rate 0.00409375
2017-10-10T11:43:16.384689: step 52, loss 1.06943, acc 0.671875, learning_rate 0.00407744
2017-10-10T11:43:16.582285: step 53, loss 0.738561, acc 0.75, learning_rate 0.00406121
2017-10-10T11:43:16.752874: step 54, loss 0.931172, acc 0.734375, learning_rate 0.00404504
2017-10-10T11:43:16.931800: step 55, loss 0.781572, acc 0.75, learning_rate 0.00402894
2017-10-10T11:43:17.104838: step 56, loss 0.812169, acc 0.84375, learning_rate 0.0040129
2017-10-10T11:43:17.266688: step 57, loss 0.87726, acc 0.765625, learning_rate 0.00399693
2017-10-10T11:43:17.407921: step 58, loss 1.24954, acc 0.671875, learning_rate 0.00398102
2017-10-10T11:43:17.576861: step 59, loss 0.756835, acc 0.796875, learning_rate 0.00396518
2017-10-10T11:43:17.760860: step 60, loss 1.11577, acc 0.765625, learning_rate 0.00394941
2017-10-10T11:43:17.933853: step 61, loss 1.23572, acc 0.6875, learning_rate 0.00393369
2017-10-10T11:43:18.115070: step 62, loss 0.96547, acc 0.75, learning_rate 0.00391804
2017-10-10T11:43:18.315914: step 63, loss 0.948614, acc 0.78125, learning_rate 0.00390246
2017-10-10T11:43:18.507848: step 64, loss 0.759755, acc 0.796875, learning_rate 0.00388694
2017-10-10T11:43:18.677432: step 65, loss 0.848954, acc 0.75, learning_rate 0.00387148
2017-10-10T11:43:18.861655: step 66, loss 0.890938, acc 0.765625, learning_rate 0.00385609
2017-10-10T11:43:19.024087: step 67, loss 0.779585, acc 0.8125, learning_rate 0.00384076
2017-10-10T11:43:19.196846: step 68, loss 1.18344, acc 0.75, learning_rate 0.00382549
2017-10-10T11:43:19.357754: step 69, loss 0.888431, acc 0.765625, learning_rate 0.00381028
2017-10-10T11:43:19.494567: step 70, loss 1.14562, acc 0.671875, learning_rate 0.00379514
2017-10-10T11:43:19.668881: step 71, loss 0.673491, acc 0.8125, learning_rate 0.00378005
2017-10-10T11:43:19.833669: step 72, loss 0.433603, acc 0.8125, learning_rate 0.00376503
2017-10-10T11:43:20.013676: step 73, loss 1.13204, acc 0.734375, learning_rate 0.00375007
2017-10-10T11:43:20.200338: step 74, loss 0.319503, acc 0.90625, learning_rate 0.00373517
2017-10-10T11:43:20.388570: step 75, loss 0.403231, acc 0.90625, learning_rate 0.00372034
2017-10-10T11:43:20.560562: step 76, loss 0.599172, acc 0.8125, learning_rate 0.00370556
2017-10-10T11:43:20.732980: step 77, loss 0.681581, acc 0.8125, learning_rate 0.00369084
2017-10-10T11:43:20.920681: step 78, loss 0.700331, acc 0.78125, learning_rate 0.00367619
2017-10-10T11:43:21.102749: step 79, loss 0.55712, acc 0.8125, learning_rate 0.00366159
2017-10-10T11:43:21.274791: step 80, loss 0.692576, acc 0.75, learning_rate 0.00364705

Evaluation:
2017-10-10T11:43:21.724347: step 80, loss 0.339551, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-80

2017-10-10T11:43:22.541293: step 81, loss 0.846209, acc 0.734375, learning_rate 0.00363257
2017-10-10T11:43:22.660114: step 82, loss 0.551824, acc 0.859375, learning_rate 0.00361815
2017-10-10T11:43:22.775567: step 83, loss 0.5538, acc 0.828125, learning_rate 0.00360379
2017-10-10T11:43:22.959121: step 84, loss 0.685111, acc 0.828125, learning_rate 0.00358949
2017-10-10T11:43:23.138145: step 85, loss 0.838078, acc 0.78125, learning_rate 0.00357525
2017-10-10T11:43:23.300889: step 86, loss 0.602432, acc 0.828125, learning_rate 0.00356106
2017-10-10T11:43:23.454684: step 87, loss 0.688235, acc 0.734375, learning_rate 0.00354694
2017-10-10T11:43:23.629565: step 88, loss 0.569475, acc 0.875, learning_rate 0.00353287
2017-10-10T11:43:23.797168: step 89, loss 0.605414, acc 0.828125, learning_rate 0.00351885
2017-10-10T11:43:23.952468: step 90, loss 0.635112, acc 0.859375, learning_rate 0.0035049
2017-10-10T11:43:24.144874: step 91, loss 0.799403, acc 0.734375, learning_rate 0.003491
2017-10-10T11:43:24.347307: step 92, loss 0.554735, acc 0.890625, learning_rate 0.00347716
2017-10-10T11:43:24.535342: step 93, loss 0.663091, acc 0.859375, learning_rate 0.00346338
2017-10-10T11:43:24.735757: step 94, loss 0.749563, acc 0.765625, learning_rate 0.00344965
2017-10-10T11:43:24.936008: step 95, loss 0.71624, acc 0.765625, learning_rate 0.00343597
2017-10-10T11:43:25.108825: step 96, loss 0.704613, acc 0.84375, learning_rate 0.00342236
2017-10-10T11:43:25.290059: step 97, loss 1.1103, acc 0.71875, learning_rate 0.0034088
2017-10-10T11:43:25.420861: step 98, loss 0.500108, acc 0.882353, learning_rate 0.00339529
2017-10-10T11:43:25.601570: step 99, loss 0.52552, acc 0.828125, learning_rate 0.00338184
2017-10-10T11:43:25.764523: step 100, loss 0.560304, acc 0.8125, learning_rate 0.00336844
2017-10-10T11:43:25.938722: step 101, loss 0.738838, acc 0.796875, learning_rate 0.0033551
2017-10-10T11:43:26.101272: step 102, loss 0.748449, acc 0.78125, learning_rate 0.00334182
2017-10-10T11:43:26.268737: step 103, loss 0.660351, acc 0.75, learning_rate 0.00332858
2017-10-10T11:43:26.472527: step 104, loss 1.01721, acc 0.75, learning_rate 0.00331541
2017-10-10T11:43:26.676257: step 105, loss 1.01179, acc 0.796875, learning_rate 0.00330228
2017-10-10T11:43:26.886540: step 106, loss 0.418654, acc 0.84375, learning_rate 0.00328921
2017-10-10T11:43:27.077529: step 107, loss 0.438657, acc 0.8125, learning_rate 0.00327619
2017-10-10T11:43:27.265358: step 108, loss 0.772813, acc 0.796875, learning_rate 0.00326323
2017-10-10T11:43:27.438631: step 109, loss 0.376094, acc 0.84375, learning_rate 0.00325032
2017-10-10T11:43:27.626437: step 110, loss 0.638773, acc 0.796875, learning_rate 0.00323746
2017-10-10T11:43:27.800522: step 111, loss 0.595708, acc 0.84375, learning_rate 0.00322465
2017-10-10T11:43:28.000844: step 112, loss 0.497652, acc 0.828125, learning_rate 0.0032119
2017-10-10T11:43:28.184636: step 113, loss 0.684643, acc 0.8125, learning_rate 0.0031992
2017-10-10T11:43:28.352860: step 114, loss 0.761323, acc 0.796875, learning_rate 0.00318655
2017-10-10T11:43:28.532831: step 115, loss 0.594948, acc 0.796875, learning_rate 0.00317395
2017-10-10T11:43:28.688474: step 116, loss 0.876041, acc 0.734375, learning_rate 0.0031614
2017-10-10T11:43:28.856909: step 117, loss 0.46295, acc 0.875, learning_rate 0.0031489
2017-10-10T11:43:29.020920: step 118, loss 0.50539, acc 0.84375, learning_rate 0.00313646
2017-10-10T11:43:29.223833: step 119, loss 0.545915, acc 0.875, learning_rate 0.00312407
2017-10-10T11:43:29.380870: step 120, loss 0.545645, acc 0.859375, learning_rate 0.00311172

Evaluation:
2017-10-10T11:43:29.756577: step 120, loss 0.306794, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-120

2017-10-10T11:43:30.775087: step 121, loss 0.315802, acc 0.859375, learning_rate 0.00309943
2017-10-10T11:43:30.906670: step 122, loss 0.573837, acc 0.78125, learning_rate 0.00308719
2017-10-10T11:43:31.031427: step 123, loss 0.820249, acc 0.8125, learning_rate 0.00307499
2017-10-10T11:43:31.145785: step 124, loss 0.72383, acc 0.796875, learning_rate 0.00306285
2017-10-10T11:43:31.258786: step 125, loss 0.454969, acc 0.890625, learning_rate 0.00305076
2017-10-10T11:43:31.371127: step 126, loss 0.320518, acc 0.90625, learning_rate 0.00303871
2017-10-10T11:43:31.520937: step 127, loss 0.693453, acc 0.796875, learning_rate 0.00302672
2017-10-10T11:43:31.705611: step 128, loss 0.262427, acc 0.890625, learning_rate 0.00301477
2017-10-10T11:43:31.852267: step 129, loss 0.461148, acc 0.84375, learning_rate 0.00300287
2017-10-10T11:43:31.964606: step 130, loss 0.331322, acc 0.859375, learning_rate 0.00299102
2017-10-10T11:43:32.073967: step 131, loss 0.44532, acc 0.828125, learning_rate 0.00297922
2017-10-10T11:43:32.185352: step 132, loss 0.691812, acc 0.78125, learning_rate 0.00296747
2017-10-10T11:43:32.390570: step 133, loss 0.450456, acc 0.859375, learning_rate 0.00295577
2017-10-10T11:43:32.588032: step 134, loss 0.366802, acc 0.84375, learning_rate 0.00294411
2017-10-10T11:43:32.778007: step 135, loss 0.301247, acc 0.890625, learning_rate 0.0029325
2017-10-10T11:43:32.966407: step 136, loss 0.232457, acc 0.890625, learning_rate 0.00292094
2017-10-10T11:43:33.158107: step 137, loss 0.495028, acc 0.84375, learning_rate 0.00290943
2017-10-10T11:43:33.342687: step 138, loss 0.682622, acc 0.8125, learning_rate 0.00289796
2017-10-10T11:43:33.536264: step 139, loss 0.383167, acc 0.84375, learning_rate 0.00288654
2017-10-10T11:43:33.722631: step 140, loss 0.3066, acc 0.90625, learning_rate 0.00287516
2017-10-10T11:43:33.917114: step 141, loss 0.403055, acc 0.796875, learning_rate 0.00286384
2017-10-10T11:43:34.084826: step 142, loss 0.400162, acc 0.859375, learning_rate 0.00285256
2017-10-10T11:43:34.278787: step 143, loss 0.69779, acc 0.78125, learning_rate 0.00284132
2017-10-10T11:43:34.433599: step 144, loss 0.23985, acc 0.921875, learning_rate 0.00283013
2017-10-10T11:43:34.580049: step 145, loss 0.174723, acc 0.921875, learning_rate 0.00281899
2017-10-10T11:43:34.732424: step 146, loss 0.617512, acc 0.828125, learning_rate 0.00280789
2017-10-10T11:43:34.908867: step 147, loss 0.360979, acc 0.84375, learning_rate 0.00279684
2017-10-10T11:43:35.086382: step 148, loss 0.313103, acc 0.90625, learning_rate 0.00278583
2017-10-10T11:43:35.257789: step 149, loss 0.33867, acc 0.890625, learning_rate 0.00277486
2017-10-10T11:43:35.430920: step 150, loss 0.380537, acc 0.90625, learning_rate 0.00276395
2017-10-10T11:43:35.596742: step 151, loss 0.466853, acc 0.859375, learning_rate 0.00275307
2017-10-10T11:43:35.764844: step 152, loss 0.251884, acc 0.90625, learning_rate 0.00274224
2017-10-10T11:43:35.917675: step 153, loss 0.450398, acc 0.875, learning_rate 0.00273146
2017-10-10T11:43:36.072607: step 154, loss 0.43563, acc 0.828125, learning_rate 0.00272072
2017-10-10T11:43:36.258306: step 155, loss 0.231073, acc 0.90625, learning_rate 0.00271002
2017-10-10T11:43:36.432977: step 156, loss 0.46053, acc 0.84375, learning_rate 0.00269937
2017-10-10T11:43:36.624699: step 157, loss 0.226005, acc 0.90625, learning_rate 0.00268876
2017-10-10T11:43:36.796514: step 158, loss 0.526721, acc 0.84375, learning_rate 0.00267819
2017-10-10T11:43:36.964046: step 159, loss 0.501172, acc 0.828125, learning_rate 0.00266767
2017-10-10T11:43:37.116226: step 160, loss 0.465669, acc 0.828125, learning_rate 0.00265719

Evaluation:
2017-10-10T11:43:37.480659: step 160, loss 0.302084, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-160

2017-10-10T11:43:38.468300: step 161, loss 0.243499, acc 0.90625, learning_rate 0.00264675
2017-10-10T11:43:38.654962: step 162, loss 0.494888, acc 0.859375, learning_rate 0.00263635
2017-10-10T11:43:38.846299: step 163, loss 0.695112, acc 0.734375, learning_rate 0.002626
2017-10-10T11:43:39.036228: step 164, loss 0.491959, acc 0.84375, learning_rate 0.00261569
2017-10-10T11:43:39.216235: step 165, loss 0.465418, acc 0.890625, learning_rate 0.00260542
2017-10-10T11:43:39.386768: step 166, loss 0.573667, acc 0.8125, learning_rate 0.0025952
2017-10-10T11:43:39.667928: step 167, loss 0.130547, acc 0.984375, learning_rate 0.00258501
2017-10-10T11:43:39.810210: step 168, loss 0.485547, acc 0.859375, learning_rate 0.00257487
2017-10-10T11:43:39.928639: step 169, loss 0.289952, acc 0.84375, learning_rate 0.00256477
2017-10-10T11:43:40.042162: step 170, loss 0.393181, acc 0.84375, learning_rate 0.0025547
2017-10-10T11:43:40.151418: step 171, loss 0.592464, acc 0.859375, learning_rate 0.00254469
2017-10-10T11:43:40.265900: step 172, loss 0.323698, acc 0.90625, learning_rate 0.00253471
2017-10-10T11:43:40.380748: step 173, loss 0.520234, acc 0.859375, learning_rate 0.00252477
2017-10-10T11:43:40.494614: step 174, loss 0.343025, acc 0.90625, learning_rate 0.00251487
2017-10-10T11:43:40.611602: step 175, loss 0.37506, acc 0.890625, learning_rate 0.00250501
2017-10-10T11:43:40.782724: step 176, loss 0.167966, acc 0.9375, learning_rate 0.0024952
2017-10-10T11:43:40.982457: step 177, loss 0.607858, acc 0.859375, learning_rate 0.00248542
2017-10-10T11:43:41.169141: step 178, loss 0.337821, acc 0.890625, learning_rate 0.00247568
2017-10-10T11:43:41.339292: step 179, loss 0.591696, acc 0.84375, learning_rate 0.00246599
2017-10-10T11:43:41.518464: step 180, loss 0.421397, acc 0.859375, learning_rate 0.00245633
2017-10-10T11:43:41.677482: step 181, loss 0.717071, acc 0.796875, learning_rate 0.00244671
2017-10-10T11:43:41.866894: step 182, loss 0.663755, acc 0.796875, learning_rate 0.00243713
2017-10-10T11:43:42.051977: step 183, loss 0.312929, acc 0.890625, learning_rate 0.00242759
2017-10-10T11:43:42.239613: step 184, loss 0.62007, acc 0.84375, learning_rate 0.00241809
2017-10-10T11:43:42.424234: step 185, loss 0.504574, acc 0.8125, learning_rate 0.00240863
2017-10-10T11:43:42.605977: step 186, loss 0.37998, acc 0.90625, learning_rate 0.00239921
2017-10-10T11:43:42.761146: step 187, loss 0.295369, acc 0.890625, learning_rate 0.00238982
2017-10-10T11:43:42.917061: step 188, loss 0.272931, acc 0.921875, learning_rate 0.00238048
2017-10-10T11:43:43.129210: step 189, loss 0.210819, acc 0.9375, learning_rate 0.00237117
2017-10-10T11:43:43.304109: step 190, loss 0.371866, acc 0.859375, learning_rate 0.0023619
2017-10-10T11:43:43.471954: step 191, loss 0.509449, acc 0.828125, learning_rate 0.00235267
2017-10-10T11:43:43.658794: step 192, loss 0.43435, acc 0.875, learning_rate 0.00234347
2017-10-10T11:43:43.821969: step 193, loss 0.300961, acc 0.875, learning_rate 0.00233431
2017-10-10T11:43:43.971737: step 194, loss 0.231441, acc 0.90625, learning_rate 0.00232519
2017-10-10T11:43:44.131625: step 195, loss 0.324332, acc 0.875, learning_rate 0.00231611
2017-10-10T11:43:44.286972: step 196, loss 0.464926, acc 0.862745, learning_rate 0.00230707
2017-10-10T11:43:44.461958: step 197, loss 0.172385, acc 0.9375, learning_rate 0.00229806
2017-10-10T11:43:44.639304: step 198, loss 0.327473, acc 0.90625, learning_rate 0.00228908
2017-10-10T11:43:44.821635: step 199, loss 0.320243, acc 0.921875, learning_rate 0.00228015
2017-10-10T11:43:45.015223: step 200, loss 0.670704, acc 0.84375, learning_rate 0.00227125

Evaluation:
2017-10-10T11:43:45.412823: step 200, loss 0.294064, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-200

2017-10-10T11:43:46.404135: step 201, loss 0.57643, acc 0.8125, learning_rate 0.00226239
2017-10-10T11:43:46.577923: step 202, loss 0.326104, acc 0.84375, learning_rate 0.00225356
2017-10-10T11:43:46.762847: step 203, loss 0.330128, acc 0.90625, learning_rate 0.00224477
2017-10-10T11:43:46.954943: step 204, loss 0.247309, acc 0.890625, learning_rate 0.00223602
2017-10-10T11:43:47.136946: step 205, loss 0.26153, acc 0.921875, learning_rate 0.0022273
2017-10-10T11:43:47.315977: step 206, loss 0.252307, acc 0.9375, learning_rate 0.00221862
2017-10-10T11:43:47.494752: step 207, loss 0.420984, acc 0.890625, learning_rate 0.00220997
2017-10-10T11:43:47.659618: step 208, loss 0.240006, acc 0.890625, learning_rate 0.00220136
2017-10-10T11:43:47.856885: step 209, loss 0.189324, acc 0.921875, learning_rate 0.00219278
2017-10-10T11:43:48.097439: step 210, loss 0.181497, acc 0.9375, learning_rate 0.00218424
2017-10-10T11:43:48.227122: step 211, loss 0.302978, acc 0.875, learning_rate 0.00217573
2017-10-10T11:43:48.345337: step 212, loss 0.286697, acc 0.875, learning_rate 0.00216726
2017-10-10T11:43:48.459951: step 213, loss 0.282646, acc 0.890625, learning_rate 0.00215882
2017-10-10T11:43:48.577170: step 214, loss 0.284384, acc 0.90625, learning_rate 0.00215041
2017-10-10T11:43:48.694917: step 215, loss 0.285332, acc 0.90625, learning_rate 0.00214204
2017-10-10T11:43:48.807268: step 216, loss 0.173722, acc 0.9375, learning_rate 0.00213371
2017-10-10T11:43:48.923998: step 217, loss 0.286085, acc 0.875, learning_rate 0.00212541
2017-10-10T11:43:49.052400: step 218, loss 0.338325, acc 0.875, learning_rate 0.00211714
2017-10-10T11:43:49.217058: step 219, loss 0.321534, acc 0.9375, learning_rate 0.00210891
2017-10-10T11:43:49.394739: step 220, loss 0.299517, acc 0.921875, learning_rate 0.00210071
2017-10-10T11:43:49.587525: step 221, loss 0.202248, acc 0.90625, learning_rate 0.00209254
2017-10-10T11:43:49.787827: step 222, loss 0.269035, acc 0.90625, learning_rate 0.00208441
2017-10-10T11:43:49.978472: step 223, loss 0.308554, acc 0.90625, learning_rate 0.00207631
2017-10-10T11:43:50.143015: step 224, loss 0.286905, acc 0.90625, learning_rate 0.00206824
2017-10-10T11:43:50.294620: step 225, loss 0.272398, acc 0.90625, learning_rate 0.00206021
2017-10-10T11:43:50.450608: step 226, loss 0.306831, acc 0.890625, learning_rate 0.00205221
2017-10-10T11:43:50.600844: step 227, loss 0.417977, acc 0.859375, learning_rate 0.00204424
2017-10-10T11:43:50.774267: step 228, loss 0.352793, acc 0.84375, learning_rate 0.0020363
2017-10-10T11:43:50.972054: step 229, loss 0.298376, acc 0.90625, learning_rate 0.0020284
2017-10-10T11:43:51.145856: step 230, loss 0.340001, acc 0.890625, learning_rate 0.00202053
2017-10-10T11:43:51.319358: step 231, loss 0.480911, acc 0.859375, learning_rate 0.00201269
2017-10-10T11:43:51.503570: step 232, loss 0.333236, acc 0.9375, learning_rate 0.00200488
2017-10-10T11:43:51.679650: step 233, loss 0.235533, acc 0.90625, learning_rate 0.00199711
2017-10-10T11:43:51.863178: step 234, loss 0.320008, acc 0.875, learning_rate 0.00198936
2017-10-10T11:43:52.047357: step 235, loss 0.271334, acc 0.90625, learning_rate 0.00198165
2017-10-10T11:43:52.234199: step 236, loss 0.234428, acc 0.890625, learning_rate 0.00197397
2017-10-10T11:43:52.420935: step 237, loss 0.49541, acc 0.875, learning_rate 0.00196632
2017-10-10T11:43:52.579265: step 238, loss 0.335292, acc 0.921875, learning_rate 0.0019587
2017-10-10T11:43:52.746861: step 239, loss 0.344599, acc 0.859375, learning_rate 0.00195112
2017-10-10T11:43:52.934194: step 240, loss 0.185842, acc 0.921875, learning_rate 0.00194356

Evaluation:
2017-10-10T11:43:53.321834: step 240, loss 0.267285, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-240

2017-10-10T11:43:54.504890: step 241, loss 0.145509, acc 0.9375, learning_rate 0.00193604
2017-10-10T11:43:54.677429: step 242, loss 0.254787, acc 0.9375, learning_rate 0.00192854
2017-10-10T11:43:54.853959: step 243, loss 0.399728, acc 0.890625, learning_rate 0.00192108
2017-10-10T11:43:55.044604: step 244, loss 0.327239, acc 0.875, learning_rate 0.00191364
2017-10-10T11:43:55.217707: step 245, loss 0.474601, acc 0.84375, learning_rate 0.00190624
2017-10-10T11:43:55.418450: step 246, loss 0.357093, acc 0.9375, learning_rate 0.00189887
2017-10-10T11:43:55.610153: step 247, loss 0.361328, acc 0.875, learning_rate 0.00189153
2017-10-10T11:43:55.786405: step 248, loss 0.357942, acc 0.921875, learning_rate 0.00188421
2017-10-10T11:43:55.996128: step 249, loss 0.227065, acc 0.90625, learning_rate 0.00187693
2017-10-10T11:43:56.189198: step 250, loss 0.287963, acc 0.90625, learning_rate 0.00186968
2017-10-10T11:43:56.412912: step 251, loss 0.31636, acc 0.875, learning_rate 0.00186245
2017-10-10T11:43:56.649427: step 252, loss 0.115939, acc 0.96875, learning_rate 0.00185526
2017-10-10T11:43:56.766564: step 253, loss 0.22147, acc 0.890625, learning_rate 0.0018481
2017-10-10T11:43:56.889298: step 254, loss 0.310742, acc 0.828125, learning_rate 0.00184096
2017-10-10T11:43:57.012130: step 255, loss 0.144187, acc 0.921875, learning_rate 0.00183385
2017-10-10T11:43:57.137625: step 256, loss 0.359381, acc 0.828125, learning_rate 0.00182678
2017-10-10T11:43:57.255873: step 257, loss 0.230227, acc 0.921875, learning_rate 0.00181973
2017-10-10T11:43:57.386778: step 258, loss 0.243513, acc 0.890625, learning_rate 0.00181271
2017-10-10T11:43:57.503707: step 259, loss 0.232096, acc 0.90625, learning_rate 0.00180572
2017-10-10T11:43:57.621535: step 260, loss 0.186131, acc 0.9375, learning_rate 0.00179876
2017-10-10T11:43:57.806426: step 261, loss 0.535491, acc 0.875, learning_rate 0.00179182
2017-10-10T11:43:58.000180: step 262, loss 0.250048, acc 0.90625, learning_rate 0.00178492
2017-10-10T11:43:58.189597: step 263, loss 0.308921, acc 0.890625, learning_rate 0.00177804
2017-10-10T11:43:58.376347: step 264, loss 0.415446, acc 0.890625, learning_rate 0.00177119
2017-10-10T11:43:58.561904: step 265, loss 0.320333, acc 0.890625, learning_rate 0.00176437
2017-10-10T11:43:58.732216: step 266, loss 0.261622, acc 0.90625, learning_rate 0.00175758
2017-10-10T11:43:58.916263: step 267, loss 0.363744, acc 0.875, learning_rate 0.00175081
2017-10-10T11:43:59.087155: step 268, loss 0.257767, acc 0.890625, learning_rate 0.00174407
2017-10-10T11:43:59.260969: step 269, loss 0.614439, acc 0.78125, learning_rate 0.00173736
2017-10-10T11:43:59.449774: step 270, loss 0.222041, acc 0.921875, learning_rate 0.00173068
2017-10-10T11:43:59.634325: step 271, loss 0.320857, acc 0.9375, learning_rate 0.00172402
2017-10-10T11:43:59.833004: step 272, loss 0.223885, acc 0.921875, learning_rate 0.00171739
2017-10-10T11:44:00.029060: step 273, loss 0.160499, acc 0.96875, learning_rate 0.00171079
2017-10-10T11:44:00.228170: step 274, loss 0.310868, acc 0.921875, learning_rate 0.00170422
2017-10-10T11:44:00.430888: step 275, loss 0.353747, acc 0.875, learning_rate 0.00169767
2017-10-10T11:44:00.631971: step 276, loss 0.264645, acc 0.90625, learning_rate 0.00169115
2017-10-10T11:44:00.819069: step 277, loss 0.392208, acc 0.890625, learning_rate 0.00168465
2017-10-10T11:44:00.991149: step 278, loss 0.355727, acc 0.90625, learning_rate 0.00167818
2017-10-10T11:44:01.167287: step 279, loss 0.374413, acc 0.875, learning_rate 0.00167174
2017-10-10T11:44:01.344023: step 280, loss 0.0895714, acc 0.984375, learning_rate 0.00166533

Evaluation:
2017-10-10T11:44:01.764972: step 280, loss 0.262508, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-280

2017-10-10T11:44:02.824908: step 281, loss 0.325511, acc 0.921875, learning_rate 0.00165894
2017-10-10T11:44:03.016297: step 282, loss 0.47326, acc 0.875, learning_rate 0.00165257
2017-10-10T11:44:03.192542: step 283, loss 0.249426, acc 0.953125, learning_rate 0.00164624
2017-10-10T11:44:03.373930: step 284, loss 0.487234, acc 0.828125, learning_rate 0.00163993
2017-10-10T11:44:03.553071: step 285, loss 0.207547, acc 0.921875, learning_rate 0.00163364
2017-10-10T11:44:03.734484: step 286, loss 0.211964, acc 0.90625, learning_rate 0.00162738
2017-10-10T11:44:03.920791: step 287, loss 0.200752, acc 0.953125, learning_rate 0.00162115
2017-10-10T11:44:04.119693: step 288, loss 0.502109, acc 0.828125, learning_rate 0.00161494
2017-10-10T11:44:04.305128: step 289, loss 0.435635, acc 0.859375, learning_rate 0.00160875
2017-10-10T11:44:04.492807: step 290, loss 0.53179, acc 0.859375, learning_rate 0.00160259
2017-10-10T11:44:04.672901: step 291, loss 0.282683, acc 0.921875, learning_rate 0.00159646
2017-10-10T11:44:04.824846: step 292, loss 0.274284, acc 0.921875, learning_rate 0.00159035
2017-10-10T11:44:04.992741: step 293, loss 0.435751, acc 0.828125, learning_rate 0.00158427
2017-10-10T11:44:05.237109: step 294, loss 0.30335, acc 0.882353, learning_rate 0.00157821
2017-10-10T11:44:05.396300: step 295, loss 0.241426, acc 0.90625, learning_rate 0.00157218
2017-10-10T11:44:05.508799: step 296, loss 0.400545, acc 0.890625, learning_rate 0.00156617
2017-10-10T11:44:05.620492: step 297, loss 0.174853, acc 0.9375, learning_rate 0.00156018
2017-10-10T11:44:05.728586: step 298, loss 0.232584, acc 0.921875, learning_rate 0.00155422
2017-10-10T11:44:05.846370: step 299, loss 0.2648, acc 0.890625, learning_rate 0.00154829
2017-10-10T11:44:05.954450: step 300, loss 0.104353, acc 0.984375, learning_rate 0.00154238
2017-10-10T11:44:06.068008: step 301, loss 0.572892, acc 0.84375, learning_rate 0.00153649
2017-10-10T11:44:06.246349: step 302, loss 0.27291, acc 0.90625, learning_rate 0.00153063
2017-10-10T11:44:06.427491: step 303, loss 0.288471, acc 0.90625, learning_rate 0.00152479
2017-10-10T11:44:06.625954: step 304, loss 0.272354, acc 0.90625, learning_rate 0.00151897
2017-10-10T11:44:06.819602: step 305, loss 0.124744, acc 0.984375, learning_rate 0.00151318
2017-10-10T11:44:07.007240: step 306, loss 0.330245, acc 0.875, learning_rate 0.00150741
2017-10-10T11:44:07.202310: step 307, loss 0.338243, acc 0.84375, learning_rate 0.00150167
2017-10-10T11:44:07.386611: step 308, loss 0.345917, acc 0.9375, learning_rate 0.00149594
2017-10-10T11:44:07.576758: step 309, loss 0.226357, acc 0.921875, learning_rate 0.00149025
2017-10-10T11:44:07.759820: step 310, loss 0.180777, acc 0.921875, learning_rate 0.00148457
2017-10-10T11:44:07.954281: step 311, loss 0.165886, acc 0.96875, learning_rate 0.00147892
2017-10-10T11:44:08.142873: step 312, loss 0.165553, acc 0.96875, learning_rate 0.00147329
2017-10-10T11:44:08.334668: step 313, loss 0.11429, acc 0.984375, learning_rate 0.00146769
2017-10-10T11:44:08.521901: step 314, loss 0.269876, acc 0.890625, learning_rate 0.0014621
2017-10-10T11:44:08.703575: step 315, loss 0.407088, acc 0.859375, learning_rate 0.00145654
2017-10-10T11:44:08.879910: step 316, loss 0.383666, acc 0.90625, learning_rate 0.00145101
2017-10-10T11:44:09.055870: step 317, loss 0.446211, acc 0.890625, learning_rate 0.00144549
2017-10-10T11:44:09.232845: step 318, loss 0.516205, acc 0.8125, learning_rate 0.00144
2017-10-10T11:44:09.421478: step 319, loss 0.380738, acc 0.890625, learning_rate 0.00143453
2017-10-10T11:44:09.606882: step 320, loss 0.290791, acc 0.890625, learning_rate 0.00142908

Evaluation:
2017-10-10T11:44:10.019006: step 320, loss 0.259143, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-320

2017-10-10T11:44:11.025019: step 321, loss 0.446909, acc 0.84375, learning_rate 0.00142366
2017-10-10T11:44:11.209358: step 322, loss 0.111773, acc 0.953125, learning_rate 0.00141826
2017-10-10T11:44:11.360895: step 323, loss 0.253901, acc 0.921875, learning_rate 0.00141288
2017-10-10T11:44:11.520863: step 324, loss 0.309458, acc 0.875, learning_rate 0.00140752
2017-10-10T11:44:11.700804: step 325, loss 0.236959, acc 0.90625, learning_rate 0.00140218
2017-10-10T11:44:11.879235: step 326, loss 0.210323, acc 0.90625, learning_rate 0.00139686
2017-10-10T11:44:12.055873: step 327, loss 0.230234, acc 0.953125, learning_rate 0.00139157
2017-10-10T11:44:12.231374: step 328, loss 0.302908, acc 0.875, learning_rate 0.0013863
2017-10-10T11:44:12.410166: step 329, loss 0.261664, acc 0.921875, learning_rate 0.00138105
2017-10-10T11:44:12.584855: step 330, loss 0.187847, acc 0.9375, learning_rate 0.00137582
2017-10-10T11:44:12.754618: step 331, loss 0.279295, acc 0.90625, learning_rate 0.00137061
2017-10-10T11:44:12.946078: step 332, loss 0.288347, acc 0.90625, learning_rate 0.00136543
2017-10-10T11:44:13.149718: step 333, loss 0.238043, acc 0.90625, learning_rate 0.00136026
2017-10-10T11:44:13.335760: step 334, loss 0.301117, acc 0.90625, learning_rate 0.00135512
2017-10-10T11:44:13.607856: step 335, loss 0.0861589, acc 0.96875, learning_rate 0.00134999
2017-10-10T11:44:13.822858: step 336, loss 0.306419, acc 0.859375, learning_rate 0.00134489
2017-10-10T11:44:13.950448: step 337, loss 0.345421, acc 0.859375, learning_rate 0.00133981
2017-10-10T11:44:14.068329: step 338, loss 0.243217, acc 0.890625, learning_rate 0.00133475
2017-10-10T11:44:14.181516: step 339, loss 0.23587, acc 0.9375, learning_rate 0.00132971
2017-10-10T11:44:14.296762: step 340, loss 0.259615, acc 0.953125, learning_rate 0.00132469
2017-10-10T11:44:14.411806: step 341, loss 0.173792, acc 0.953125, learning_rate 0.00131969
2017-10-10T11:44:14.524680: step 342, loss 0.123705, acc 0.984375, learning_rate 0.00131471
2017-10-10T11:44:14.640040: step 343, loss 0.203706, acc 0.9375, learning_rate 0.00130975
2017-10-10T11:44:14.821159: step 344, loss 0.246586, acc 0.953125, learning_rate 0.00130482
2017-10-10T11:44:15.010167: step 345, loss 0.27032, acc 0.921875, learning_rate 0.0012999
2017-10-10T11:44:15.203483: step 346, loss 0.259874, acc 0.90625, learning_rate 0.001295
2017-10-10T11:44:15.402092: step 347, loss 0.338186, acc 0.90625, learning_rate 0.00129012
2017-10-10T11:44:15.609836: step 348, loss 0.341446, acc 0.921875, learning_rate 0.00128527
2017-10-10T11:44:15.801683: step 349, loss 0.238366, acc 0.921875, learning_rate 0.00128043
2017-10-10T11:44:15.990765: step 350, loss 0.180841, acc 0.953125, learning_rate 0.00127561
2017-10-10T11:44:16.164838: step 351, loss 0.22952, acc 0.90625, learning_rate 0.00127081
2017-10-10T11:44:16.336844: step 352, loss 0.147639, acc 0.9375, learning_rate 0.00126603
2017-10-10T11:44:16.536866: step 353, loss 0.421664, acc 0.859375, learning_rate 0.00126127
2017-10-10T11:44:16.707151: step 354, loss 0.365708, acc 0.890625, learning_rate 0.00125653
2017-10-10T11:44:16.880870: step 355, loss 0.137416, acc 0.9375, learning_rate 0.00125181
2017-10-10T11:44:17.056845: step 356, loss 0.200085, acc 0.921875, learning_rate 0.00124711
2017-10-10T11:44:17.256461: step 357, loss 0.171271, acc 0.953125, learning_rate 0.00124243
2017-10-10T11:44:17.429471: step 358, loss 0.171948, acc 0.921875, learning_rate 0.00123777
2017-10-10T11:44:17.610500: step 359, loss 0.112251, acc 0.984375, learning_rate 0.00123312
2017-10-10T11:44:17.788818: step 360, loss 0.306893, acc 0.9375, learning_rate 0.0012285

Evaluation:
2017-10-10T11:44:18.236833: step 360, loss 0.251791, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-360

2017-10-10T11:44:19.256837: step 361, loss 0.178513, acc 0.953125, learning_rate 0.00122389
2017-10-10T11:44:19.432266: step 362, loss 0.22382, acc 0.90625, learning_rate 0.0012193
2017-10-10T11:44:19.606198: step 363, loss 0.159484, acc 0.953125, learning_rate 0.00121473
2017-10-10T11:44:19.775822: step 364, loss 0.291252, acc 0.84375, learning_rate 0.00121018
2017-10-10T11:44:19.971474: step 365, loss 0.309774, acc 0.9375, learning_rate 0.00120565
2017-10-10T11:44:20.157981: step 366, loss 0.368606, acc 0.9375, learning_rate 0.00120114
2017-10-10T11:44:20.335730: step 367, loss 0.203291, acc 0.953125, learning_rate 0.00119664
2017-10-10T11:44:20.496828: step 368, loss 0.133589, acc 0.9375, learning_rate 0.00119217
2017-10-10T11:44:20.657620: step 369, loss 0.179199, acc 0.953125, learning_rate 0.00118771
2017-10-10T11:44:20.833904: step 370, loss 0.403966, acc 0.921875, learning_rate 0.00118327
2017-10-10T11:44:20.973619: step 371, loss 0.23306, acc 0.9375, learning_rate 0.00117885
2017-10-10T11:44:21.146459: step 372, loss 0.169253, acc 0.9375, learning_rate 0.00117445
2017-10-10T11:44:21.335424: step 373, loss 0.196183, acc 0.90625, learning_rate 0.00117006
2017-10-10T11:44:21.525108: step 374, loss 0.229063, acc 0.890625, learning_rate 0.00116569
2017-10-10T11:44:21.723930: step 375, loss 0.25192, acc 0.921875, learning_rate 0.00116134
2017-10-10T11:44:21.907117: step 376, loss 0.0919604, acc 0.984375, learning_rate 0.00115701
2017-10-10T11:44:22.164874: step 377, loss 0.280652, acc 0.875, learning_rate 0.0011527
2017-10-10T11:44:22.379647: step 378, loss 0.142904, acc 0.953125, learning_rate 0.0011484
2017-10-10T11:44:22.490448: step 379, loss 0.196974, acc 0.953125, learning_rate 0.00114412
2017-10-10T11:44:22.609535: step 380, loss 0.126398, acc 0.953125, learning_rate 0.00113986
2017-10-10T11:44:22.724795: step 381, loss 0.293944, acc 0.875, learning_rate 0.00113561
2017-10-10T11:44:22.840699: step 382, loss 0.264187, acc 0.890625, learning_rate 0.00113139
2017-10-10T11:44:22.957693: step 383, loss 0.25872, acc 0.875, learning_rate 0.00112718
2017-10-10T11:44:23.066183: step 384, loss 0.119952, acc 0.96875, learning_rate 0.00112298
2017-10-10T11:44:23.183235: step 385, loss 0.12085, acc 0.953125, learning_rate 0.00111881
2017-10-10T11:44:23.294220: step 386, loss 0.164974, acc 0.9375, learning_rate 0.00111465
2017-10-10T11:44:23.490605: step 387, loss 0.15137, acc 0.953125, learning_rate 0.00111051
2017-10-10T11:44:23.672228: step 388, loss 0.339807, acc 0.890625, learning_rate 0.00110638
2017-10-10T11:44:23.873138: step 389, loss 0.308845, acc 0.890625, learning_rate 0.00110228
2017-10-10T11:44:24.055659: step 390, loss 0.160302, acc 0.890625, learning_rate 0.00109818
2017-10-10T11:44:24.222068: step 391, loss 0.232872, acc 0.921875, learning_rate 0.00109411
2017-10-10T11:44:24.384834: step 392, loss 0.201918, acc 0.941176, learning_rate 0.00109005
2017-10-10T11:44:24.574440: step 393, loss 0.199287, acc 0.921875, learning_rate 0.00108601
2017-10-10T11:44:24.744854: step 394, loss 0.232846, acc 0.921875, learning_rate 0.00108199
2017-10-10T11:44:24.926790: step 395, loss 0.255135, acc 0.90625, learning_rate 0.00107798
2017-10-10T11:44:25.122804: step 396, loss 0.201684, acc 0.9375, learning_rate 0.00107399
2017-10-10T11:44:25.280857: step 397, loss 0.248349, acc 0.921875, learning_rate 0.00107001
2017-10-10T11:44:25.452824: step 398, loss 0.122907, acc 0.953125, learning_rate 0.00106605
2017-10-10T11:44:25.623968: step 399, loss 0.145621, acc 0.96875, learning_rate 0.00106211
2017-10-10T11:44:25.752264: step 400, loss 0.24189, acc 0.890625, learning_rate 0.00105818

Evaluation:
2017-10-10T11:44:26.147350: step 400, loss 0.254463, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-400

2017-10-10T11:44:27.049674: step 401, loss 0.163025, acc 0.953125, learning_rate 0.00105427
2017-10-10T11:44:27.234644: step 402, loss 0.324916, acc 0.890625, learning_rate 0.00105037
2017-10-10T11:44:27.414808: step 403, loss 0.248599, acc 0.90625, learning_rate 0.0010465
2017-10-10T11:44:27.593603: step 404, loss 0.299511, acc 0.890625, learning_rate 0.00104263
2017-10-10T11:44:27.763667: step 405, loss 0.206281, acc 0.921875, learning_rate 0.00103878
2017-10-10T11:44:27.947029: step 406, loss 0.260884, acc 0.90625, learning_rate 0.00103495
2017-10-10T11:44:28.129026: step 407, loss 0.373687, acc 0.890625, learning_rate 0.00103114
2017-10-10T11:44:28.304687: step 408, loss 0.103509, acc 0.96875, learning_rate 0.00102734
2017-10-10T11:44:28.490572: step 409, loss 0.304559, acc 0.9375, learning_rate 0.00102355
2017-10-10T11:44:28.677482: step 410, loss 0.232934, acc 0.90625, learning_rate 0.00101978
2017-10-10T11:44:28.825390: step 411, loss 0.241462, acc 0.875, learning_rate 0.00101603
2017-10-10T11:44:28.998301: step 412, loss 0.0999415, acc 0.984375, learning_rate 0.00101229
2017-10-10T11:44:29.171583: step 413, loss 0.238815, acc 0.9375, learning_rate 0.00100856
2017-10-10T11:44:29.310957: step 414, loss 0.226852, acc 0.921875, learning_rate 0.00100486
2017-10-10T11:44:29.480496: step 415, loss 0.215497, acc 0.953125, learning_rate 0.00100116
2017-10-10T11:44:29.654421: step 416, loss 0.212888, acc 0.90625, learning_rate 0.000997483
2017-10-10T11:44:29.846830: step 417, loss 0.189043, acc 0.9375, learning_rate 0.00099382
2017-10-10T11:44:30.045952: step 418, loss 0.238156, acc 0.9375, learning_rate 0.000990172
2017-10-10T11:44:30.238720: step 419, loss 0.169046, acc 0.921875, learning_rate 0.000986538
2017-10-10T11:44:30.414742: step 420, loss 0.202653, acc 0.9375, learning_rate 0.00098292
2017-10-10T11:44:30.600434: step 421, loss 0.111966, acc 0.96875, learning_rate 0.000979316
2017-10-10T11:44:30.798945: step 422, loss 0.270701, acc 0.890625, learning_rate 0.000975727
2017-10-10T11:44:31.055450: step 423, loss 0.391278, acc 0.859375, learning_rate 0.000972152
2017-10-10T11:44:31.175011: step 424, loss 0.177005, acc 0.90625, learning_rate 0.000968592
2017-10-10T11:44:31.287801: step 425, loss 0.375975, acc 0.890625, learning_rate 0.000965047
2017-10-10T11:44:31.407299: step 426, loss 0.192157, acc 0.9375, learning_rate 0.000961516
2017-10-10T11:44:31.520283: step 427, loss 0.107536, acc 0.953125, learning_rate 0.000958
2017-10-10T11:44:31.636734: step 428, loss 0.197895, acc 0.9375, learning_rate 0.000954497
2017-10-10T11:44:31.751805: step 429, loss 0.235262, acc 0.890625, learning_rate 0.00095101
2017-10-10T11:44:31.931749: step 430, loss 0.188628, acc 0.96875, learning_rate 0.000947536
2017-10-10T11:44:32.087708: step 431, loss 0.159613, acc 0.9375, learning_rate 0.000944076
2017-10-10T11:44:32.230600: step 432, loss 0.119462, acc 0.96875, learning_rate 0.000940631
2017-10-10T11:44:32.383351: step 433, loss 0.0943144, acc 0.96875, learning_rate 0.0009372
2017-10-10T11:44:32.561428: step 434, loss 0.177126, acc 0.9375, learning_rate 0.000933783
2017-10-10T11:44:32.769935: step 435, loss 0.212916, acc 0.90625, learning_rate 0.000930379
2017-10-10T11:44:32.967003: step 436, loss 0.167079, acc 0.96875, learning_rate 0.00092699
2017-10-10T11:44:33.133987: step 437, loss 0.219933, acc 0.90625, learning_rate 0.000923614
2017-10-10T11:44:33.269076: step 438, loss 0.109877, acc 0.984375, learning_rate 0.000920253
2017-10-10T11:44:33.406340: step 439, loss 0.229679, acc 0.921875, learning_rate 0.000916905
2017-10-10T11:44:33.582635: step 440, loss 0.314557, acc 0.890625, learning_rate 0.00091357

Evaluation:
2017-10-10T11:44:33.969926: step 440, loss 0.254704, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-440

2017-10-10T11:44:34.938967: step 441, loss 0.172293, acc 0.921875, learning_rate 0.000910249
2017-10-10T11:44:35.107041: step 442, loss 0.119955, acc 0.953125, learning_rate 0.000906942
2017-10-10T11:44:35.274573: step 443, loss 0.149416, acc 0.96875, learning_rate 0.000903648
2017-10-10T11:44:35.415542: step 444, loss 0.274667, acc 0.921875, learning_rate 0.000900368
2017-10-10T11:44:35.597101: step 445, loss 0.400211, acc 0.84375, learning_rate 0.000897101
2017-10-10T11:44:35.796882: step 446, loss 0.155769, acc 0.90625, learning_rate 0.000893848
2017-10-10T11:44:35.993566: step 447, loss 0.144218, acc 0.953125, learning_rate 0.000890607
2017-10-10T11:44:36.173300: step 448, loss 0.240137, acc 0.921875, learning_rate 0.00088738
2017-10-10T11:44:36.356603: step 449, loss 0.234715, acc 0.921875, learning_rate 0.000884166
2017-10-10T11:44:36.528006: step 450, loss 0.243615, acc 0.921875, learning_rate 0.000880966
2017-10-10T11:44:36.719333: step 451, loss 0.129023, acc 0.96875, learning_rate 0.000877778
2017-10-10T11:44:36.886238: step 452, loss 0.187729, acc 0.9375, learning_rate 0.000874603
2017-10-10T11:44:37.048942: step 453, loss 0.314311, acc 0.921875, learning_rate 0.000871441
2017-10-10T11:44:37.199603: step 454, loss 0.135166, acc 0.984375, learning_rate 0.000868293
2017-10-10T11:44:37.355949: step 455, loss 0.326183, acc 0.890625, learning_rate 0.000865157
2017-10-10T11:44:37.522837: step 456, loss 0.145189, acc 0.9375, learning_rate 0.000862033
2017-10-10T11:44:37.709217: step 457, loss 0.21687, acc 0.9375, learning_rate 0.000858923
2017-10-10T11:44:37.896890: step 458, loss 0.289249, acc 0.859375, learning_rate 0.000855825
2017-10-10T11:44:38.075248: step 459, loss 0.125066, acc 0.953125, learning_rate 0.00085274
2017-10-10T11:44:38.261496: step 460, loss 0.120439, acc 0.953125, learning_rate 0.000849668
2017-10-10T11:44:38.437234: step 461, loss 0.161098, acc 0.953125, learning_rate 0.000846608
2017-10-10T11:44:38.618644: step 462, loss 0.190183, acc 0.953125, learning_rate 0.00084356
2017-10-10T11:44:38.802682: step 463, loss 0.366709, acc 0.828125, learning_rate 0.000840525
2017-10-10T11:44:39.004411: step 464, loss 0.126585, acc 0.96875, learning_rate 0.000837502
2017-10-10T11:44:39.246345: step 465, loss 0.228416, acc 0.890625, learning_rate 0.000834492
2017-10-10T11:44:39.453149: step 466, loss 0.243827, acc 0.9375, learning_rate 0.000831494
2017-10-10T11:44:39.577531: step 467, loss 0.289109, acc 0.890625, learning_rate 0.000828508
2017-10-10T11:44:39.691854: step 468, loss 0.244226, acc 0.90625, learning_rate 0.000825535
2017-10-10T11:44:39.807896: step 469, loss 0.227065, acc 0.921875, learning_rate 0.000822573
2017-10-10T11:44:39.922050: step 470, loss 0.168921, acc 0.921875, learning_rate 0.000819624
2017-10-10T11:44:40.034326: step 471, loss 0.239898, acc 0.875, learning_rate 0.000816687
2017-10-10T11:44:40.145765: step 472, loss 0.188973, acc 0.9375, learning_rate 0.000813761
2017-10-10T11:44:40.263048: step 473, loss 0.131581, acc 0.953125, learning_rate 0.000810848
2017-10-10T11:44:40.393282: step 474, loss 0.126778, acc 0.96875, learning_rate 0.000807946
2017-10-10T11:44:40.567185: step 475, loss 0.376434, acc 0.90625, learning_rate 0.000805057
2017-10-10T11:44:40.740883: step 476, loss 0.126986, acc 0.921875, learning_rate 0.000802179
2017-10-10T11:44:40.912873: step 477, loss 0.264148, acc 0.921875, learning_rate 0.000799313
2017-10-10T11:44:41.084756: step 478, loss 0.151001, acc 0.90625, learning_rate 0.000796458
2017-10-10T11:44:41.256473: step 479, loss 0.158209, acc 0.9375, learning_rate 0.000793616
2017-10-10T11:44:41.438856: step 480, loss 0.286285, acc 0.90625, learning_rate 0.000790784

Evaluation:
2017-10-10T11:44:41.856828: step 480, loss 0.247007, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-480

2017-10-10T11:44:42.866579: step 481, loss 0.154859, acc 0.921875, learning_rate 0.000787965
2017-10-10T11:44:43.045286: step 482, loss 0.250237, acc 0.90625, learning_rate 0.000785157
2017-10-10T11:44:43.220414: step 483, loss 0.139843, acc 0.953125, learning_rate 0.00078236
2017-10-10T11:44:43.415403: step 484, loss 0.219622, acc 0.921875, learning_rate 0.000779575
2017-10-10T11:44:43.600867: step 485, loss 0.0861821, acc 0.96875, learning_rate 0.000776801
2017-10-10T11:44:43.795443: step 486, loss 0.264208, acc 0.875, learning_rate 0.000774038
2017-10-10T11:44:43.966323: step 487, loss 0.1659, acc 0.953125, learning_rate 0.000771287
2017-10-10T11:44:44.146607: step 488, loss 0.242169, acc 0.9375, learning_rate 0.000768547
2017-10-10T11:44:44.312862: step 489, loss 0.222789, acc 0.921875, learning_rate 0.000765818
2017-10-10T11:44:44.456749: step 490, loss 0.248793, acc 0.921569, learning_rate 0.000763101
2017-10-10T11:44:44.600877: step 491, loss 0.227287, acc 0.890625, learning_rate 0.000760394
2017-10-10T11:44:44.778202: step 492, loss 0.252589, acc 0.9375, learning_rate 0.000757698
2017-10-10T11:44:44.952218: step 493, loss 0.209643, acc 0.96875, learning_rate 0.000755014
2017-10-10T11:44:45.142369: step 494, loss 0.253705, acc 0.90625, learning_rate 0.00075234
2017-10-10T11:44:45.337041: step 495, loss 0.219027, acc 0.90625, learning_rate 0.000749677
2017-10-10T11:44:45.532860: step 496, loss 0.136211, acc 0.953125, learning_rate 0.000747026
2017-10-10T11:44:45.730425: step 497, loss 0.27493, acc 0.90625, learning_rate 0.000744385
2017-10-10T11:44:45.907502: step 498, loss 0.218434, acc 0.90625, learning_rate 0.000741754
2017-10-10T11:44:46.087003: step 499, loss 0.124591, acc 0.953125, learning_rate 0.000739135
2017-10-10T11:44:46.243386: step 500, loss 0.0824661, acc 0.984375, learning_rate 0.000736526
2017-10-10T11:44:46.403606: step 501, loss 0.0936873, acc 0.96875, learning_rate 0.000733928
2017-10-10T11:44:46.569053: step 502, loss 0.203201, acc 0.890625, learning_rate 0.00073134
2017-10-10T11:44:46.737127: step 503, loss 0.259737, acc 0.921875, learning_rate 0.000728763
2017-10-10T11:44:46.894557: step 504, loss 0.169554, acc 0.953125, learning_rate 0.000726197
2017-10-10T11:44:47.080457: step 505, loss 0.131062, acc 0.953125, learning_rate 0.000723641
2017-10-10T11:44:47.265256: step 506, loss 0.175565, acc 0.9375, learning_rate 0.000721095
2017-10-10T11:44:47.446070: step 507, loss 0.159279, acc 0.9375, learning_rate 0.00071856
2017-10-10T11:44:47.636250: step 508, loss 0.174633, acc 0.96875, learning_rate 0.000716036
2017-10-10T11:44:47.911228: step 509, loss 0.230916, acc 0.9375, learning_rate 0.000713521
2017-10-10T11:44:48.103789: step 510, loss 0.174368, acc 0.96875, learning_rate 0.000711017
2017-10-10T11:44:48.221531: step 511, loss 0.206115, acc 0.9375, learning_rate 0.000708523
2017-10-10T11:44:48.338376: step 512, loss 0.232137, acc 0.9375, learning_rate 0.000706039
2017-10-10T11:44:48.460276: step 513, loss 0.288661, acc 0.90625, learning_rate 0.000703565
2017-10-10T11:44:48.576562: step 514, loss 0.0662875, acc 0.96875, learning_rate 0.000701102
2017-10-10T11:44:48.697613: step 515, loss 0.364954, acc 0.90625, learning_rate 0.000698648
2017-10-10T11:44:48.816098: step 516, loss 0.273372, acc 0.921875, learning_rate 0.000696204
2017-10-10T11:44:48.932729: step 517, loss 0.16529, acc 0.9375, learning_rate 0.000693771
2017-10-10T11:44:49.049595: step 518, loss 0.225969, acc 0.90625, learning_rate 0.000691347
2017-10-10T11:44:49.228856: step 519, loss 0.157982, acc 0.921875, learning_rate 0.000688934
2017-10-10T11:44:49.428220: step 520, loss 0.221864, acc 0.953125, learning_rate 0.00068653

Evaluation:
2017-10-10T11:44:49.801742: step 520, loss 0.243638, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-520

2017-10-10T11:44:50.689038: step 521, loss 0.176283, acc 0.9375, learning_rate 0.000684136
2017-10-10T11:44:50.887851: step 522, loss 0.166259, acc 0.9375, learning_rate 0.000681751
2017-10-10T11:44:51.087151: step 523, loss 0.246054, acc 0.90625, learning_rate 0.000679377
2017-10-10T11:44:51.287857: step 524, loss 0.134956, acc 0.96875, learning_rate 0.000677012
2017-10-10T11:44:51.465455: step 525, loss 0.207498, acc 0.953125, learning_rate 0.000674657
2017-10-10T11:44:51.644838: step 526, loss 0.326017, acc 0.875, learning_rate 0.000672311
2017-10-10T11:44:51.836017: step 527, loss 0.124989, acc 0.96875, learning_rate 0.000669975
2017-10-10T11:44:52.004688: step 528, loss 0.357668, acc 0.875, learning_rate 0.000667648
2017-10-10T11:44:52.173136: step 529, loss 0.22727, acc 0.90625, learning_rate 0.000665331
2017-10-10T11:44:52.323721: step 530, loss 0.119222, acc 0.953125, learning_rate 0.000663024
2017-10-10T11:44:52.496759: step 531, loss 0.182679, acc 0.90625, learning_rate 0.000660726
2017-10-10T11:44:52.704879: step 532, loss 0.370828, acc 0.890625, learning_rate 0.000658437
2017-10-10T11:44:52.912262: step 533, loss 0.259076, acc 0.90625, learning_rate 0.000656158
2017-10-10T11:44:53.122349: step 534, loss 0.307046, acc 0.890625, learning_rate 0.000653888
2017-10-10T11:44:53.322061: step 535, loss 0.25531, acc 0.890625, learning_rate 0.000651627
2017-10-10T11:44:53.524982: step 536, loss 0.231857, acc 0.90625, learning_rate 0.000649375
2017-10-10T11:44:53.730035: step 537, loss 0.205195, acc 0.921875, learning_rate 0.000647133
2017-10-10T11:44:53.957679: step 538, loss 0.197306, acc 0.921875, learning_rate 0.000644899
2017-10-10T11:44:54.173150: step 539, loss 0.299737, acc 0.890625, learning_rate 0.000642675
2017-10-10T11:44:54.386102: step 540, loss 0.131469, acc 0.953125, learning_rate 0.00064046
2017-10-10T11:44:54.598046: step 541, loss 0.214428, acc 0.921875, learning_rate 0.000638254
2017-10-10T11:44:54.785006: step 542, loss 0.144615, acc 0.921875, learning_rate 0.000636057
2017-10-10T11:44:54.981701: step 543, loss 0.0554124, acc 1, learning_rate 0.000633869
2017-10-10T11:44:55.148063: step 544, loss 0.24481, acc 0.90625, learning_rate 0.00063169
2017-10-10T11:44:55.310916: step 545, loss 0.160066, acc 0.953125, learning_rate 0.00062952
2017-10-10T11:44:55.496868: step 546, loss 0.179568, acc 0.96875, learning_rate 0.000627358
2017-10-10T11:44:55.674496: step 547, loss 0.253323, acc 0.90625, learning_rate 0.000625206
2017-10-10T11:44:55.863857: step 548, loss 0.21706, acc 0.90625, learning_rate 0.000623062
2017-10-10T11:44:56.066089: step 549, loss 0.234181, acc 0.90625, learning_rate 0.000620927
2017-10-10T11:44:56.273881: step 550, loss 0.107019, acc 0.96875, learning_rate 0.000618801
2017-10-10T11:44:56.477224: step 551, loss 0.183442, acc 0.921875, learning_rate 0.000616683
2017-10-10T11:44:56.685490: step 552, loss 0.180895, acc 0.9375, learning_rate 0.000614574
2017-10-10T11:44:56.897733: step 553, loss 0.19422, acc 0.9375, learning_rate 0.000612474
2017-10-10T11:44:57.162671: step 554, loss 0.227244, acc 0.90625, learning_rate 0.000610382
2017-10-10T11:44:57.295825: step 555, loss 0.0835765, acc 0.953125, learning_rate 0.000608299
2017-10-10T11:44:57.407682: step 556, loss 0.168601, acc 0.953125, learning_rate 0.000606224
2017-10-10T11:44:57.564999: step 557, loss 0.101975, acc 0.96875, learning_rate 0.000604158
2017-10-10T11:44:57.739935: step 558, loss 0.102118, acc 0.984375, learning_rate 0.0006021
2017-10-10T11:44:57.868162: step 559, loss 0.144204, acc 0.921875, learning_rate 0.00060005
2017-10-10T11:44:57.976325: step 560, loss 0.0954762, acc 0.953125, learning_rate 0.000598009

Evaluation:
2017-10-10T11:44:58.257690: step 560, loss 0.250043, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-560

2017-10-10T11:44:58.906884: step 561, loss 0.161589, acc 0.953125, learning_rate 0.000595977
2017-10-10T11:44:59.108832: step 562, loss 0.290085, acc 0.859375, learning_rate 0.000593952
2017-10-10T11:44:59.315973: step 563, loss 0.143513, acc 0.953125, learning_rate 0.000591936
2017-10-10T11:44:59.514093: step 564, loss 0.1184, acc 0.984375, learning_rate 0.000589928
2017-10-10T11:44:59.720228: step 565, loss 0.188165, acc 0.953125, learning_rate 0.000587928
2017-10-10T11:44:59.927670: step 566, loss 0.279188, acc 0.90625, learning_rate 0.000585937
2017-10-10T11:45:00.122113: step 567, loss 0.174626, acc 0.9375, learning_rate 0.000583953
2017-10-10T11:45:00.299632: step 568, loss 0.150886, acc 0.9375, learning_rate 0.000581978
2017-10-10T11:45:00.496817: step 569, loss 0.1181, acc 0.96875, learning_rate 0.00058001
2017-10-10T11:45:00.699617: step 570, loss 0.29366, acc 0.875, learning_rate 0.000578051
2017-10-10T11:45:00.896826: step 571, loss 0.0758946, acc 0.984375, learning_rate 0.0005761
2017-10-10T11:45:01.103529: step 572, loss 0.370176, acc 0.84375, learning_rate 0.000574157
2017-10-10T11:45:01.287147: step 573, loss 0.21381, acc 0.90625, learning_rate 0.000572221
2017-10-10T11:45:01.478856: step 574, loss 0.207555, acc 0.96875, learning_rate 0.000570294
2017-10-10T11:45:01.661165: step 575, loss 0.138028, acc 0.9375, learning_rate 0.000568374
2017-10-10T11:45:01.858898: step 576, loss 0.204627, acc 0.921875, learning_rate 0.000566462
2017-10-10T11:45:02.090995: step 577, loss 0.276898, acc 0.890625, learning_rate 0.000564558
2017-10-10T11:45:02.267778: step 578, loss 0.217652, acc 0.921875, learning_rate 0.000562662
2017-10-10T11:45:02.450051: step 579, loss 0.111515, acc 0.96875, learning_rate 0.000560774
2017-10-10T11:45:02.636805: step 580, loss 0.179013, acc 0.9375, learning_rate 0.000558893
2017-10-10T11:45:02.804950: step 581, loss 0.432096, acc 0.90625, learning_rate 0.00055702
2017-10-10T11:45:02.980834: step 582, loss 0.119382, acc 0.953125, learning_rate 0.000555154
2017-10-10T11:45:03.171091: step 583, loss 0.263532, acc 0.921875, learning_rate 0.000553296
2017-10-10T11:45:03.368559: step 584, loss 0.324138, acc 0.890625, learning_rate 0.000551446
2017-10-10T11:45:03.558533: step 585, loss 0.246812, acc 0.90625, learning_rate 0.000549604
2017-10-10T11:45:03.734150: step 586, loss 0.263364, acc 0.859375, learning_rate 0.000547768
2017-10-10T11:45:03.910530: step 587, loss 0.113807, acc 0.953125, learning_rate 0.000545941
2017-10-10T11:45:04.026156: step 588, loss 0.34009, acc 0.823529, learning_rate 0.00054412
2017-10-10T11:45:04.200476: step 589, loss 0.269692, acc 0.921875, learning_rate 0.000542308
2017-10-10T11:45:04.380188: step 590, loss 0.136562, acc 0.96875, learning_rate 0.000540502
2017-10-10T11:45:04.573255: step 591, loss 0.132733, acc 0.96875, learning_rate 0.000538704
2017-10-10T11:45:04.740857: step 592, loss 0.0855094, acc 0.984375, learning_rate 0.000536914
2017-10-10T11:45:04.932919: step 593, loss 0.27079, acc 0.90625, learning_rate 0.00053513
2017-10-10T11:45:05.096221: step 594, loss 0.202156, acc 0.9375, learning_rate 0.000533354
2017-10-10T11:45:05.267878: step 595, loss 0.0925289, acc 0.96875, learning_rate 0.000531585
2017-10-10T11:45:05.439579: step 596, loss 0.164072, acc 0.9375, learning_rate 0.000529824
2017-10-10T11:45:05.631062: step 597, loss 0.162197, acc 0.9375, learning_rate 0.000528069
2017-10-10T11:45:05.809423: step 598, loss 0.151115, acc 0.9375, learning_rate 0.000526322
2017-10-10T11:45:05.990075: step 599, loss 0.182051, acc 0.953125, learning_rate 0.000524582
2017-10-10T11:45:06.142420: step 600, loss 0.231859, acc 0.921875, learning_rate 0.000522849

Evaluation:
2017-10-10T11:45:06.497498: step 600, loss 0.239526, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-600

2017-10-10T11:45:07.365313: step 601, loss 0.140352, acc 0.953125, learning_rate 0.000521123
2017-10-10T11:45:07.484840: step 602, loss 0.235855, acc 0.890625, learning_rate 0.000519404
2017-10-10T11:45:07.653671: step 603, loss 0.176929, acc 0.9375, learning_rate 0.000517692
2017-10-10T11:45:07.825312: step 604, loss 0.297928, acc 0.921875, learning_rate 0.000515987
2017-10-10T11:45:08.009137: step 605, loss 0.268467, acc 0.9375, learning_rate 0.000514289
2017-10-10T11:45:08.194213: step 606, loss 0.282598, acc 0.90625, learning_rate 0.000512598
2017-10-10T11:45:08.371006: step 607, loss 0.104909, acc 0.984375, learning_rate 0.000510914
2017-10-10T11:45:08.548068: step 608, loss 0.12994, acc 0.96875, learning_rate 0.000509237
2017-10-10T11:45:08.729512: step 609, loss 0.207993, acc 0.921875, learning_rate 0.000507566
2017-10-10T11:45:08.880264: step 610, loss 0.152552, acc 0.9375, learning_rate 0.000505903
2017-10-10T11:45:09.062072: step 611, loss 0.104084, acc 0.953125, learning_rate 0.000504246
2017-10-10T11:45:09.263599: step 612, loss 0.317869, acc 0.890625, learning_rate 0.000502596
2017-10-10T11:45:09.448860: step 613, loss 0.250086, acc 0.90625, learning_rate 0.000500953
2017-10-10T11:45:09.653391: step 614, loss 0.390234, acc 0.828125, learning_rate 0.000499316
2017-10-10T11:45:09.838400: step 615, loss 0.274934, acc 0.921875, learning_rate 0.000497686
2017-10-10T11:45:10.036833: step 616, loss 0.193236, acc 0.9375, learning_rate 0.000496063
2017-10-10T11:45:10.226069: step 617, loss 0.353342, acc 0.890625, learning_rate 0.000494446
2017-10-10T11:45:10.412825: step 618, loss 0.173627, acc 0.953125, learning_rate 0.000492836
2017-10-10T11:45:10.612678: step 619, loss 0.236258, acc 0.90625, learning_rate 0.000491233
2017-10-10T11:45:10.813005: step 620, loss 0.248035, acc 0.9375, learning_rate 0.000489636
2017-10-10T11:45:11.020834: step 621, loss 0.112735, acc 0.953125, learning_rate 0.000488045
2017-10-10T11:45:11.196798: step 622, loss 0.181447, acc 0.921875, learning_rate 0.000486461
2017-10-10T11:45:11.396332: step 623, loss 0.158294, acc 0.90625, learning_rate 0.000484884
2017-10-10T11:45:11.593925: step 624, loss 0.127236, acc 0.953125, learning_rate 0.000483313
2017-10-10T11:45:11.742633: step 625, loss 0.172216, acc 0.96875, learning_rate 0.000481748
2017-10-10T11:45:11.904344: step 626, loss 0.234384, acc 0.9375, learning_rate 0.00048019
2017-10-10T11:45:12.088258: step 627, loss 0.197573, acc 0.9375, learning_rate 0.000478638
2017-10-10T11:45:12.303905: step 628, loss 0.27137, acc 0.9375, learning_rate 0.000477093
2017-10-10T11:45:12.523901: step 629, loss 0.113968, acc 0.96875, learning_rate 0.000475554
2017-10-10T11:45:12.716948: step 630, loss 0.0712861, acc 1, learning_rate 0.000474021
2017-10-10T11:45:12.897430: step 631, loss 0.227307, acc 0.953125, learning_rate 0.000472494
2017-10-10T11:45:13.077301: step 632, loss 0.19748, acc 0.953125, learning_rate 0.000470974
2017-10-10T11:45:13.248404: step 633, loss 0.255639, acc 0.9375, learning_rate 0.000469459
2017-10-10T11:45:13.409167: step 634, loss 0.22875, acc 0.921875, learning_rate 0.000467951
2017-10-10T11:45:13.615831: step 635, loss 0.175981, acc 0.9375, learning_rate 0.000466449
2017-10-10T11:45:13.827931: step 636, loss 0.0783375, acc 0.984375, learning_rate 0.000464954
2017-10-10T11:45:14.037218: step 637, loss 0.230346, acc 0.9375, learning_rate 0.000463464
2017-10-10T11:45:14.249526: step 638, loss 0.158399, acc 0.9375, learning_rate 0.00046198
2017-10-10T11:45:14.441887: step 639, loss 0.319961, acc 0.90625, learning_rate 0.000460503
2017-10-10T11:45:14.634575: step 640, loss 0.186511, acc 0.921875, learning_rate 0.000459031

Evaluation:
2017-10-10T11:45:15.005219: step 640, loss 0.242101, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-640

2017-10-10T11:45:15.988861: step 641, loss 0.175171, acc 0.921875, learning_rate 0.000457566
2017-10-10T11:45:16.114185: step 642, loss 0.100842, acc 0.96875, learning_rate 0.000456106
2017-10-10T11:45:16.245719: step 643, loss 0.306374, acc 0.875, learning_rate 0.000454653
2017-10-10T11:45:16.375746: step 644, loss 0.448108, acc 0.875, learning_rate 0.000453205
2017-10-10T11:45:16.510314: step 645, loss 0.379336, acc 0.859375, learning_rate 0.000451764
2017-10-10T11:45:16.630684: step 646, loss 0.294253, acc 0.890625, learning_rate 0.000450328
2017-10-10T11:45:16.796830: step 647, loss 0.0433841, acc 1, learning_rate 0.000448898
2017-10-10T11:45:16.984814: step 648, loss 0.327817, acc 0.875, learning_rate 0.000447474
2017-10-10T11:45:17.137717: step 649, loss 0.200223, acc 0.9375, learning_rate 0.000446055
2017-10-10T11:45:17.329454: step 650, loss 0.186323, acc 0.9375, learning_rate 0.000444643
2017-10-10T11:45:17.500262: step 651, loss 0.383919, acc 0.921875, learning_rate 0.000443236
2017-10-10T11:45:17.676801: step 652, loss 0.105799, acc 0.96875, learning_rate 0.000441835
2017-10-10T11:45:17.889078: step 653, loss 0.133939, acc 0.953125, learning_rate 0.00044044
2017-10-10T11:45:18.083242: step 654, loss 0.182076, acc 0.953125, learning_rate 0.00043905
2017-10-10T11:45:18.289548: step 655, loss 0.170476, acc 0.953125, learning_rate 0.000437666
2017-10-10T11:45:18.496639: step 656, loss 0.123931, acc 0.953125, learning_rate 0.000436288
2017-10-10T11:45:18.692510: step 657, loss 0.234321, acc 0.9375, learning_rate 0.000434915
2017-10-10T11:45:18.872839: step 658, loss 0.125639, acc 0.953125, learning_rate 0.000433548
2017-10-10T11:45:19.083033: step 659, loss 0.110008, acc 0.953125, learning_rate 0.000432187
2017-10-10T11:45:19.266852: step 660, loss 0.23004, acc 0.9375, learning_rate 0.000430831
2017-10-10T11:45:19.452832: step 661, loss 0.174918, acc 0.953125, learning_rate 0.000429481
2017-10-10T11:45:19.655778: step 662, loss 0.215295, acc 0.90625, learning_rate 0.000428136
2017-10-10T11:45:19.856755: step 663, loss 0.170864, acc 0.96875, learning_rate 0.000426796
2017-10-10T11:45:20.063006: step 664, loss 0.186907, acc 0.921875, learning_rate 0.000425463
2017-10-10T11:45:20.258321: step 665, loss 0.0498709, acc 0.984375, learning_rate 0.000424134
2017-10-10T11:45:20.453478: step 666, loss 0.183854, acc 0.9375, learning_rate 0.000422811
2017-10-10T11:45:20.640839: step 667, loss 0.155354, acc 0.96875, learning_rate 0.000421493
2017-10-10T11:45:20.793610: step 668, loss 0.140312, acc 0.953125, learning_rate 0.000420181
2017-10-10T11:45:20.985947: step 669, loss 0.176577, acc 0.90625, learning_rate 0.000418874
2017-10-10T11:45:21.160440: step 670, loss 0.186724, acc 0.9375, learning_rate 0.000417573
2017-10-10T11:45:21.341093: step 671, loss 0.157296, acc 0.953125, learning_rate 0.000416276
2017-10-10T11:45:21.529363: step 672, loss 0.221003, acc 0.90625, learning_rate 0.000414985
2017-10-10T11:45:21.749294: step 673, loss 0.135642, acc 0.953125, learning_rate 0.0004137
2017-10-10T11:45:21.952061: step 674, loss 0.230339, acc 0.9375, learning_rate 0.000412419
2017-10-10T11:45:22.150387: step 675, loss 0.131265, acc 0.953125, learning_rate 0.000411144
2017-10-10T11:45:22.338686: step 676, loss 0.105603, acc 0.984375, learning_rate 0.000409874
2017-10-10T11:45:22.545163: step 677, loss 0.213521, acc 0.90625, learning_rate 0.000408609
2017-10-10T11:45:22.755844: step 678, loss 0.265035, acc 0.9375, learning_rate 0.00040735
2017-10-10T11:45:22.975114: step 679, loss 0.271968, acc 0.90625, learning_rate 0.000406095
2017-10-10T11:45:23.174923: step 680, loss 0.136978, acc 0.953125, learning_rate 0.000404846

Evaluation:
2017-10-10T11:45:23.738771: step 680, loss 0.239175, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-680

2017-10-10T11:45:25.167045: step 681, loss 0.242911, acc 0.875, learning_rate 0.000403601
2017-10-10T11:45:25.407381: step 682, loss 0.108962, acc 0.96875, learning_rate 0.000402362
2017-10-10T11:45:25.639347: step 683, loss 0.117787, acc 0.96875, learning_rate 0.000401128
2017-10-10T11:45:25.815775: step 684, loss 0.0826428, acc 0.984375, learning_rate 0.000399899
2017-10-10T11:45:26.012486: step 685, loss 0.162197, acc 0.953125, learning_rate 0.000398675
2017-10-10T11:45:26.160206: step 686, loss 0.319147, acc 0.843137, learning_rate 0.000397456
2017-10-10T11:45:26.336204: step 687, loss 0.234094, acc 0.921875, learning_rate 0.000396241
2017-10-10T11:45:26.585162: step 688, loss 0.280345, acc 0.890625, learning_rate 0.000395032
2017-10-10T11:45:26.820635: step 689, loss 0.1706, acc 0.953125, learning_rate 0.000393828
2017-10-10T11:45:27.051990: step 690, loss 0.0827585, acc 0.984375, learning_rate 0.000392629
2017-10-10T11:45:27.297083: step 691, loss 0.145813, acc 0.984375, learning_rate 0.000391434
2017-10-10T11:45:27.548723: step 692, loss 0.205644, acc 0.9375, learning_rate 0.000390245
2017-10-10T11:45:27.782231: step 693, loss 0.221128, acc 0.9375, learning_rate 0.00038906
2017-10-10T11:45:28.069317: step 694, loss 0.173623, acc 0.9375, learning_rate 0.00038788
2017-10-10T11:45:28.337148: step 695, loss 0.199254, acc 0.921875, learning_rate 0.000386705
2017-10-10T11:45:28.591830: step 696, loss 0.196536, acc 0.90625, learning_rate 0.000385535
2017-10-10T11:45:28.822007: step 697, loss 0.0898928, acc 0.984375, learning_rate 0.000384369
2017-10-10T11:45:29.064835: step 698, loss 0.143088, acc 0.96875, learning_rate 0.000383209
2017-10-10T11:45:29.320966: step 699, loss 0.158162, acc 0.953125, learning_rate 0.000382053
2017-10-10T11:45:29.560862: step 700, loss 0.175219, acc 0.9375, learning_rate 0.000380901
2017-10-10T11:45:29.818632: step 701, loss 0.274688, acc 0.90625, learning_rate 0.000379755
2017-10-10T11:45:30.060509: step 702, loss 0.126315, acc 0.96875, learning_rate 0.000378613
2017-10-10T11:45:30.356836: step 703, loss 0.0877103, acc 1, learning_rate 0.000377476
2017-10-10T11:45:30.581817: step 704, loss 0.145117, acc 0.953125, learning_rate 0.000376343
2017-10-10T11:45:30.764586: step 705, loss 0.162297, acc 0.953125, learning_rate 0.000375215
2017-10-10T11:45:30.964673: step 706, loss 0.118483, acc 0.96875, learning_rate 0.000374092
2017-10-10T11:45:31.173244: step 707, loss 0.16791, acc 0.953125, learning_rate 0.000372973
2017-10-10T11:45:31.351484: step 708, loss 0.183574, acc 0.9375, learning_rate 0.000371859
2017-10-10T11:45:31.597768: step 709, loss 0.129618, acc 0.96875, learning_rate 0.000370749
2017-10-10T11:45:31.842688: step 710, loss 0.155026, acc 0.953125, learning_rate 0.000369644
2017-10-10T11:45:32.080851: step 711, loss 0.139651, acc 0.9375, learning_rate 0.000368543
2017-10-10T11:45:32.288283: step 712, loss 0.123707, acc 0.953125, learning_rate 0.000367447
2017-10-10T11:45:32.504861: step 713, loss 0.200846, acc 0.921875, learning_rate 0.000366356
2017-10-10T11:45:32.716425: step 714, loss 0.30088, acc 0.890625, learning_rate 0.000365268
2017-10-10T11:45:32.963145: step 715, loss 0.15747, acc 0.953125, learning_rate 0.000364186
2017-10-10T11:45:33.214545: step 716, loss 0.309894, acc 0.890625, learning_rate 0.000363107
2017-10-10T11:45:33.462115: step 717, loss 0.120135, acc 0.984375, learning_rate 0.000362033
2017-10-10T11:45:33.668430: step 718, loss 0.203774, acc 0.921875, learning_rate 0.000360964
2017-10-10T11:45:33.916580: step 719, loss 0.164686, acc 0.9375, learning_rate 0.000359899
2017-10-10T11:45:34.164282: step 720, loss 0.159896, acc 0.9375, learning_rate 0.000358838

Evaluation:
2017-10-10T11:45:34.740548: step 720, loss 0.241913, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-720

2017-10-10T11:45:36.313028: step 721, loss 0.306979, acc 0.90625, learning_rate 0.000357781
2017-10-10T11:45:36.553203: step 722, loss 0.135312, acc 0.921875, learning_rate 0.000356729
2017-10-10T11:45:36.734251: step 723, loss 0.191191, acc 0.9375, learning_rate 0.000355681
2017-10-10T11:45:36.941168: step 724, loss 0.0920924, acc 0.96875, learning_rate 0.000354637
2017-10-10T11:45:37.104784: step 725, loss 0.154132, acc 0.9375, learning_rate 0.000353598
2017-10-10T11:45:37.256150: step 726, loss 0.175485, acc 0.9375, learning_rate 0.000352563
2017-10-10T11:45:37.422588: step 727, loss 0.219315, acc 0.9375, learning_rate 0.000351532
2017-10-10T11:45:37.640454: step 728, loss 0.371212, acc 0.875, learning_rate 0.000350505
2017-10-10T11:45:37.861050: step 729, loss 0.235117, acc 0.890625, learning_rate 0.000349483
2017-10-10T11:45:38.117396: step 730, loss 0.124458, acc 0.9375, learning_rate 0.000348465
2017-10-10T11:45:38.384928: step 731, loss 0.22044, acc 0.921875, learning_rate 0.00034745
2017-10-10T11:45:38.672635: step 732, loss 0.113788, acc 0.953125, learning_rate 0.00034644
2017-10-10T11:45:38.879551: step 733, loss 0.12128, acc 0.953125, learning_rate 0.000345434
2017-10-10T11:45:39.101853: step 734, loss 0.175982, acc 0.9375, learning_rate 0.000344433
2017-10-10T11:45:39.314982: step 735, loss 0.168317, acc 0.96875, learning_rate 0.000343435
2017-10-10T11:45:39.518491: step 736, loss 0.284842, acc 0.9375, learning_rate 0.000342441
2017-10-10T11:45:39.769408: step 737, loss 0.0945625, acc 0.96875, learning_rate 0.000341452
2017-10-10T11:45:40.019200: step 738, loss 0.171221, acc 0.9375, learning_rate 0.000340466
2017-10-10T11:45:40.263324: step 739, loss 0.183287, acc 0.96875, learning_rate 0.000339485
2017-10-10T11:45:40.506227: step 740, loss 0.124337, acc 0.953125, learning_rate 0.000338507
2017-10-10T11:45:40.775905: step 741, loss 0.224615, acc 0.9375, learning_rate 0.000337534
2017-10-10T11:45:41.005714: step 742, loss 0.182921, acc 0.9375, learning_rate 0.000336564
2017-10-10T11:45:41.268845: step 743, loss 0.231632, acc 0.90625, learning_rate 0.000335598
2017-10-10T11:45:41.490101: step 744, loss 0.105464, acc 0.953125, learning_rate 0.000334637
2017-10-10T11:45:41.727077: step 745, loss 0.254348, acc 0.890625, learning_rate 0.000333679
2017-10-10T11:45:41.972853: step 746, loss 0.126333, acc 0.9375, learning_rate 0.000332725
2017-10-10T11:45:42.211503: step 747, loss 0.0997003, acc 0.96875, learning_rate 0.000331775
2017-10-10T11:45:42.447521: step 748, loss 0.198124, acc 0.9375, learning_rate 0.000330829
2017-10-10T11:45:42.641147: step 749, loss 0.157403, acc 0.953125, learning_rate 0.000329887
2017-10-10T11:45:42.848718: step 750, loss 0.0692535, acc 1, learning_rate 0.000328949
2017-10-10T11:45:43.075321: step 751, loss 0.131328, acc 0.953125, learning_rate 0.000328014
2017-10-10T11:45:43.295640: step 752, loss 0.173782, acc 0.9375, learning_rate 0.000327083
2017-10-10T11:45:43.544941: step 753, loss 0.200406, acc 0.921875, learning_rate 0.000326157
2017-10-10T11:45:43.778565: step 754, loss 0.416176, acc 0.875, learning_rate 0.000325233
2017-10-10T11:45:44.039035: step 755, loss 0.110632, acc 0.96875, learning_rate 0.000324314
2017-10-10T11:45:44.288047: step 756, loss 0.172011, acc 0.96875, learning_rate 0.000323399
2017-10-10T11:45:44.503433: step 757, loss 0.132807, acc 0.9375, learning_rate 0.000322487
2017-10-10T11:45:44.736808: step 758, loss 0.266387, acc 0.90625, learning_rate 0.000321579
2017-10-10T11:45:44.981522: step 759, loss 0.196397, acc 0.9375, learning_rate 0.000320674
2017-10-10T11:45:45.232852: step 760, loss 0.138338, acc 0.9375, learning_rate 0.000319773

Evaluation:
2017-10-10T11:45:45.731625: step 760, loss 0.239296, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-760

2017-10-10T11:45:46.849064: step 761, loss 0.0996957, acc 0.96875, learning_rate 0.000318876
2017-10-10T11:45:47.052815: step 762, loss 0.0745896, acc 0.984375, learning_rate 0.000317983
2017-10-10T11:45:47.256863: step 763, loss 0.0775656, acc 1, learning_rate 0.000317093
2017-10-10T11:45:47.505622: step 764, loss 0.15757, acc 0.9375, learning_rate 0.000316207
2017-10-10T11:45:48.532335: step 765, loss 0.110677, acc 0.953125, learning_rate 0.000315325
2017-10-10T11:45:48.638771: step 766, loss 0.212211, acc 0.90625, learning_rate 0.000314446
2017-10-10T11:45:48.798690: step 767, loss 0.0512098, acc 1, learning_rate 0.00031357
2017-10-10T11:45:49.750760: step 768, loss 0.173642, acc 0.9375, learning_rate 0.000312699
2017-10-10T11:45:49.900572: step 769, loss 0.244916, acc 0.90625, learning_rate 0.00031183
2017-10-10T11:45:50.055924: step 770, loss 0.261727, acc 0.890625, learning_rate 0.000310966
2017-10-10T11:45:50.241854: step 771, loss 0.245666, acc 0.890625, learning_rate 0.000310105
2017-10-10T11:45:50.399949: step 772, loss 0.22742, acc 0.921875, learning_rate 0.000309247
2017-10-10T11:45:50.739868: step 773, loss 0.235537, acc 0.921875, learning_rate 0.000308393
2017-10-10T11:45:50.916036: step 774, loss 0.192281, acc 0.921875, learning_rate 0.000307542
2017-10-10T11:45:51.081593: step 775, loss 0.287894, acc 0.90625, learning_rate 0.000306695
2017-10-10T11:45:51.323376: step 776, loss 0.0628037, acc 0.984375, learning_rate 0.000305852
2017-10-10T11:45:51.561695: step 777, loss 0.103102, acc 0.984375, learning_rate 0.000305011
2017-10-10T11:45:51.785269: step 778, loss 0.168099, acc 0.9375, learning_rate 0.000304174
2017-10-10T11:45:52.021966: step 779, loss 0.188977, acc 0.90625, learning_rate 0.000303341
2017-10-10T11:45:52.250207: step 780, loss 0.170248, acc 0.96875, learning_rate 0.000302511
2017-10-10T11:45:52.449882: step 781, loss 0.159758, acc 0.953125, learning_rate 0.000301684
2017-10-10T11:45:52.689540: step 782, loss 0.372244, acc 0.859375, learning_rate 0.000300861
2017-10-10T11:45:52.959121: step 783, loss 0.101325, acc 0.96875, learning_rate 0.000300041
2017-10-10T11:45:53.116860: step 784, loss 0.217989, acc 0.882353, learning_rate 0.000299225
2017-10-10T11:45:53.303918: step 785, loss 0.143836, acc 0.953125, learning_rate 0.000298412
2017-10-10T11:45:53.500529: step 786, loss 0.287435, acc 0.921875, learning_rate 0.000297602
2017-10-10T11:45:53.699252: step 787, loss 0.132471, acc 0.9375, learning_rate 0.000296795
2017-10-10T11:45:53.898631: step 788, loss 0.178086, acc 0.921875, learning_rate 0.000295992
2017-10-10T11:45:54.148611: step 789, loss 0.13646, acc 0.9375, learning_rate 0.000295192
2017-10-10T11:45:54.405895: step 790, loss 0.0804666, acc 0.984375, learning_rate 0.000294395
2017-10-10T11:45:54.688583: step 791, loss 0.213732, acc 0.9375, learning_rate 0.000293602
2017-10-10T11:45:54.962896: step 792, loss 0.194772, acc 0.921875, learning_rate 0.000292812
2017-10-10T11:45:55.208505: step 793, loss 0.236293, acc 0.9375, learning_rate 0.000292025
2017-10-10T11:45:55.428663: step 794, loss 0.165234, acc 0.953125, learning_rate 0.000291241
2017-10-10T11:45:55.625064: step 795, loss 0.241347, acc 0.921875, learning_rate 0.00029046
2017-10-10T11:45:55.830719: step 796, loss 0.141676, acc 0.953125, learning_rate 0.000289683
2017-10-10T11:45:56.054004: step 797, loss 0.180037, acc 0.9375, learning_rate 0.000288908
2017-10-10T11:45:56.255632: step 798, loss 0.165433, acc 0.9375, learning_rate 0.000288137
2017-10-10T11:45:56.489388: step 799, loss 0.19035, acc 0.96875, learning_rate 0.000287369
2017-10-10T11:45:56.676891: step 800, loss 0.122613, acc 0.96875, learning_rate 0.000286605

Evaluation:
2017-10-10T11:45:57.199875: step 800, loss 0.239535, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-800

2017-10-10T11:45:58.350256: step 801, loss 0.150264, acc 0.96875, learning_rate 0.000285843
2017-10-10T11:45:58.610155: step 802, loss 0.152272, acc 0.96875, learning_rate 0.000285084
2017-10-10T11:45:58.835333: step 803, loss 0.133167, acc 0.9375, learning_rate 0.000284329
2017-10-10T11:45:59.052162: step 804, loss 0.188684, acc 0.921875, learning_rate 0.000283577
2017-10-10T11:45:59.312898: step 805, loss 0.308628, acc 0.90625, learning_rate 0.000282827
2017-10-10T11:45:59.563949: step 806, loss 0.0953165, acc 0.96875, learning_rate 0.000282081
2017-10-10T11:45:59.783024: step 807, loss 0.129483, acc 0.96875, learning_rate 0.000281338
2017-10-10T11:46:00.037559: step 808, loss 0.18922, acc 0.921875, learning_rate 0.000280598
2017-10-10T11:46:00.260955: step 809, loss 0.15652, acc 0.921875, learning_rate 0.00027986
2017-10-10T11:46:00.498348: step 810, loss 0.22801, acc 0.890625, learning_rate 0.000279126
2017-10-10T11:46:00.708059: step 811, loss 0.156001, acc 0.953125, learning_rate 0.000278395
2017-10-10T11:46:00.988217: step 812, loss 0.13537, acc 0.96875, learning_rate 0.000277667
2017-10-10T11:46:01.349115: step 813, loss 0.229884, acc 0.921875, learning_rate 0.000276942
2017-10-10T11:46:01.558515: step 814, loss 0.124854, acc 0.984375, learning_rate 0.00027622
2017-10-10T11:46:01.683088: step 815, loss 0.134753, acc 0.953125, learning_rate 0.0002755
2017-10-10T11:46:01.805908: step 816, loss 0.158071, acc 0.953125, learning_rate 0.000274784
2017-10-10T11:46:01.935085: step 817, loss 0.102865, acc 0.96875, learning_rate 0.000274071
2017-10-10T11:46:02.066404: step 818, loss 0.193366, acc 0.9375, learning_rate 0.00027336
2017-10-10T11:46:02.200106: step 819, loss 0.238161, acc 0.890625, learning_rate 0.000272652
2017-10-10T11:46:02.415382: step 820, loss 0.208972, acc 0.890625, learning_rate 0.000271948
2017-10-10T11:46:02.640385: step 821, loss 0.160113, acc 0.953125, learning_rate 0.000271246
2017-10-10T11:46:02.884504: step 822, loss 0.211222, acc 0.9375, learning_rate 0.000270547
2017-10-10T11:46:03.156762: step 823, loss 0.190961, acc 0.921875, learning_rate 0.000269851
2017-10-10T11:46:03.392769: step 824, loss 0.223134, acc 0.921875, learning_rate 0.000269157
2017-10-10T11:46:03.653025: step 825, loss 0.146804, acc 0.9375, learning_rate 0.000268467
2017-10-10T11:46:03.923988: step 826, loss 0.219096, acc 0.921875, learning_rate 0.000267779
2017-10-10T11:46:04.154363: step 827, loss 0.142513, acc 0.96875, learning_rate 0.000267094
2017-10-10T11:46:04.422908: step 828, loss 0.0925498, acc 0.953125, learning_rate 0.000266412
2017-10-10T11:46:04.666396: step 829, loss 0.069145, acc 1, learning_rate 0.000265733
2017-10-10T11:46:04.930874: step 830, loss 0.156983, acc 0.953125, learning_rate 0.000265057
2017-10-10T11:46:05.155482: step 831, loss 0.244055, acc 0.921875, learning_rate 0.000264383
2017-10-10T11:46:05.399294: step 832, loss 0.221936, acc 0.921875, learning_rate 0.000263712
2017-10-10T11:46:05.636861: step 833, loss 0.154748, acc 0.90625, learning_rate 0.000263044
2017-10-10T11:46:05.873295: step 834, loss 0.197868, acc 0.9375, learning_rate 0.000262378
2017-10-10T11:46:06.135247: step 835, loss 0.135468, acc 0.953125, learning_rate 0.000261715
2017-10-10T11:46:06.392263: step 836, loss 0.150291, acc 0.984375, learning_rate 0.000261055
2017-10-10T11:46:06.650319: step 837, loss 0.300089, acc 0.90625, learning_rate 0.000260398
2017-10-10T11:46:06.900847: step 838, loss 0.215222, acc 0.921875, learning_rate 0.000259743
2017-10-10T11:46:07.140467: step 839, loss 0.0908459, acc 0.96875, learning_rate 0.000259091
2017-10-10T11:46:07.386546: step 840, loss 0.16146, acc 0.9375, learning_rate 0.000258442

Evaluation:
2017-10-10T11:46:07.922268: step 840, loss 0.237641, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-840

2017-10-10T11:46:09.236872: step 841, loss 0.157822, acc 0.9375, learning_rate 0.000257795
2017-10-10T11:46:09.718416: step 842, loss 0.264175, acc 0.90625, learning_rate 0.000257151
2017-10-10T11:46:09.926882: step 843, loss 0.179197, acc 0.90625, learning_rate 0.00025651
2017-10-10T11:46:10.136597: step 844, loss 0.107966, acc 0.96875, learning_rate 0.000255871
2017-10-10T11:46:10.335850: step 845, loss 0.0886132, acc 0.984375, learning_rate 0.000255235
2017-10-10T11:46:10.541134: step 846, loss 0.203249, acc 0.9375, learning_rate 0.000254601
2017-10-10T11:46:10.748343: step 847, loss 0.199089, acc 0.953125, learning_rate 0.00025397
2017-10-10T11:46:10.993557: step 848, loss 0.202944, acc 0.90625, learning_rate 0.000253341
2017-10-10T11:46:11.220866: step 849, loss 0.113245, acc 0.96875, learning_rate 0.000252716
2017-10-10T11:46:11.408892: step 850, loss 0.177892, acc 0.90625, learning_rate 0.000252092
2017-10-10T11:46:11.631509: step 851, loss 0.149374, acc 0.96875, learning_rate 0.000251471
2017-10-10T11:46:11.852900: step 852, loss 0.239559, acc 0.90625, learning_rate 0.000250853
2017-10-10T11:46:12.092888: step 853, loss 0.141359, acc 0.953125, learning_rate 0.000250237
2017-10-10T11:46:12.409731: step 854, loss 0.1035, acc 0.984375, learning_rate 0.000249624
2017-10-10T11:46:12.604889: step 855, loss 0.130963, acc 0.9375, learning_rate 0.000249013
2017-10-10T11:46:12.768577: step 856, loss 0.0816681, acc 0.984375, learning_rate 0.000248405
2017-10-10T11:46:12.938960: step 857, loss 0.166625, acc 0.96875, learning_rate 0.000247799
2017-10-10T11:46:13.098773: step 858, loss 0.136914, acc 0.96875, learning_rate 0.000247196
2017-10-10T11:46:13.375366: step 859, loss 0.108722, acc 0.9375, learning_rate 0.000246595
2017-10-10T11:46:13.568876: step 860, loss 0.220696, acc 0.921875, learning_rate 0.000245997
2017-10-10T11:46:13.788908: step 861, loss 0.0850118, acc 0.96875, learning_rate 0.000245401
2017-10-10T11:46:14.045009: step 862, loss 0.301551, acc 0.90625, learning_rate 0.000244808
2017-10-10T11:46:14.267779: step 863, loss 0.174673, acc 0.953125, learning_rate 0.000244216
2017-10-10T11:46:14.486650: step 864, loss 0.189034, acc 0.921875, learning_rate 0.000243628
2017-10-10T11:46:14.687018: step 865, loss 0.151723, acc 0.953125, learning_rate 0.000243042
2017-10-10T11:46:14.949435: step 866, loss 0.159944, acc 0.9375, learning_rate 0.000242458
2017-10-10T11:46:15.177409: step 867, loss 0.103893, acc 0.984375, learning_rate 0.000241876
2017-10-10T11:46:15.398821: step 868, loss 0.0810536, acc 0.984375, learning_rate 0.000241297
2017-10-10T11:46:15.667156: step 869, loss 0.0736146, acc 1, learning_rate 0.00024072
2017-10-10T11:46:15.902565: step 870, loss 0.253297, acc 0.9375, learning_rate 0.000240146
2017-10-10T11:46:16.147450: step 871, loss 0.162211, acc 0.953125, learning_rate 0.000239574
2017-10-10T11:46:16.363793: step 872, loss 0.133809, acc 0.96875, learning_rate 0.000239004
2017-10-10T11:46:16.610399: step 873, loss 0.253654, acc 0.921875, learning_rate 0.000238437
2017-10-10T11:46:16.844862: step 874, loss 0.0592493, acc 0.984375, learning_rate 0.000237872
2017-10-10T11:46:17.077244: step 875, loss 0.213686, acc 0.90625, learning_rate 0.000237309
2017-10-10T11:46:17.287393: step 876, loss 0.131994, acc 0.953125, learning_rate 0.000236749
2017-10-10T11:46:17.508942: step 877, loss 0.170007, acc 0.921875, learning_rate 0.00023619
2017-10-10T11:46:17.765017: step 878, loss 0.144731, acc 0.953125, learning_rate 0.000235635
2017-10-10T11:46:18.024883: step 879, loss 0.247828, acc 0.890625, learning_rate 0.000235081
2017-10-10T11:46:18.338411: step 880, loss 0.30146, acc 0.890625, learning_rate 0.00023453

Evaluation:
2017-10-10T11:46:18.745161: step 880, loss 0.237597, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-880

2017-10-10T11:46:19.801746: step 881, loss 0.271225, acc 0.90625, learning_rate 0.00023398
2017-10-10T11:46:19.992810: step 882, loss 0.120388, acc 0.980392, learning_rate 0.000233434
2017-10-10T11:46:20.239155: step 883, loss 0.145018, acc 0.921875, learning_rate 0.000232889
2017-10-10T11:46:20.474635: step 884, loss 0.308882, acc 0.890625, learning_rate 0.000232346
2017-10-10T11:46:20.717968: step 885, loss 0.174382, acc 0.9375, learning_rate 0.000231806
2017-10-10T11:46:20.967896: step 886, loss 0.126989, acc 0.953125, learning_rate 0.000231268
2017-10-10T11:46:21.241316: step 887, loss 0.0917739, acc 0.96875, learning_rate 0.000230732
2017-10-10T11:46:21.464856: step 888, loss 0.249121, acc 0.921875, learning_rate 0.000230199
2017-10-10T11:46:21.720947: step 889, loss 0.266357, acc 0.890625, learning_rate 0.000229667
2017-10-10T11:46:21.935275: step 890, loss 0.168989, acc 0.953125, learning_rate 0.000229138
2017-10-10T11:46:22.184295: step 891, loss 0.192262, acc 0.921875, learning_rate 0.000228611
2017-10-10T11:46:22.428249: step 892, loss 0.250212, acc 0.890625, learning_rate 0.000228086
2017-10-10T11:46:22.689365: step 893, loss 0.11326, acc 0.984375, learning_rate 0.000227563
2017-10-10T11:46:22.931783: step 894, loss 0.13732, acc 0.9375, learning_rate 0.000227043
2017-10-10T11:46:23.161168: step 895, loss 0.215124, acc 0.921875, learning_rate 0.000226524
2017-10-10T11:46:23.532825: step 896, loss 0.0819628, acc 0.96875, learning_rate 0.000226008
2017-10-10T11:46:23.687202: step 897, loss 0.176383, acc 0.921875, learning_rate 0.000225493
2017-10-10T11:46:23.867138: step 898, loss 0.139546, acc 0.921875, learning_rate 0.000224981
2017-10-10T11:46:24.052017: step 899, loss 0.205492, acc 0.921875, learning_rate 0.000224471
2017-10-10T11:46:24.244950: step 900, loss 0.218172, acc 0.921875, learning_rate 0.000223963
2017-10-10T11:46:24.417330: step 901, loss 0.0910733, acc 0.96875, learning_rate 0.000223457
2017-10-10T11:46:24.668842: step 902, loss 0.164878, acc 0.921875, learning_rate 0.000222953
2017-10-10T11:46:24.937985: step 903, loss 0.131402, acc 0.921875, learning_rate 0.000222451
2017-10-10T11:46:25.198614: step 904, loss 0.291336, acc 0.921875, learning_rate 0.000221951
2017-10-10T11:46:25.432826: step 905, loss 0.214194, acc 0.921875, learning_rate 0.000221453
2017-10-10T11:46:25.660843: step 906, loss 0.203242, acc 0.953125, learning_rate 0.000220958
2017-10-10T11:46:25.905089: step 907, loss 0.18314, acc 0.921875, learning_rate 0.000220464
2017-10-10T11:46:26.128935: step 908, loss 0.362302, acc 0.859375, learning_rate 0.000219972
2017-10-10T11:46:26.408984: step 909, loss 0.0910403, acc 0.96875, learning_rate 0.000219483
2017-10-10T11:46:26.586381: step 910, loss 0.295476, acc 0.921875, learning_rate 0.000218995
2017-10-10T11:46:26.760943: step 911, loss 0.116941, acc 0.953125, learning_rate 0.000218509
2017-10-10T11:46:26.957106: step 912, loss 0.196576, acc 0.9375, learning_rate 0.000218025
2017-10-10T11:46:27.121509: step 913, loss 0.156003, acc 0.953125, learning_rate 0.000217544
2017-10-10T11:46:27.319172: step 914, loss 0.217576, acc 0.90625, learning_rate 0.000217064
2017-10-10T11:46:27.561511: step 915, loss 0.158636, acc 0.921875, learning_rate 0.000216586
2017-10-10T11:46:27.816761: step 916, loss 0.158381, acc 0.9375, learning_rate 0.00021611
2017-10-10T11:46:28.074516: step 917, loss 0.170226, acc 0.984375, learning_rate 0.000215636
2017-10-10T11:46:28.315477: step 918, loss 0.0629606, acc 1, learning_rate 0.000215164
2017-10-10T11:46:28.532862: step 919, loss 0.139695, acc 0.96875, learning_rate 0.000214694
2017-10-10T11:46:28.784054: step 920, loss 0.164663, acc 0.9375, learning_rate 0.000214226

Evaluation:
2017-10-10T11:46:29.260832: step 920, loss 0.23295, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-920

2017-10-10T11:46:30.424877: step 921, loss 0.10615, acc 0.96875, learning_rate 0.00021376
2017-10-10T11:46:30.653860: step 922, loss 0.276204, acc 0.921875, learning_rate 0.000213295
2017-10-10T11:46:30.897222: step 923, loss 0.125973, acc 0.9375, learning_rate 0.000212833
2017-10-10T11:46:31.140040: step 924, loss 0.10703, acc 0.96875, learning_rate 0.000212372
2017-10-10T11:46:31.387100: step 925, loss 0.122338, acc 0.96875, learning_rate 0.000211914
2017-10-10T11:46:31.613116: step 926, loss 0.0955856, acc 0.953125, learning_rate 0.000211457
2017-10-10T11:46:31.865135: step 927, loss 0.0989318, acc 0.984375, learning_rate 0.000211002
2017-10-10T11:46:32.103064: step 928, loss 0.241383, acc 0.890625, learning_rate 0.000210549
2017-10-10T11:46:32.336799: step 929, loss 0.0531612, acc 1, learning_rate 0.000210098
2017-10-10T11:46:32.602704: step 930, loss 0.144548, acc 0.953125, learning_rate 0.000209648
2017-10-10T11:46:32.829958: step 931, loss 0.264114, acc 0.890625, learning_rate 0.000209201
2017-10-10T11:46:33.077745: step 932, loss 0.119018, acc 0.96875, learning_rate 0.000208755
2017-10-10T11:46:33.333015: step 933, loss 0.0917214, acc 0.984375, learning_rate 0.000208311
2017-10-10T11:46:33.558247: step 934, loss 0.200745, acc 0.9375, learning_rate 0.000207869
2017-10-10T11:46:33.803944: step 935, loss 0.221638, acc 0.96875, learning_rate 0.000207429
2017-10-10T11:46:34.008849: step 936, loss 0.201536, acc 0.921875, learning_rate 0.00020699
2017-10-10T11:46:34.272893: step 937, loss 0.0580953, acc 0.984375, learning_rate 0.000206554
2017-10-10T11:46:34.557254: step 938, loss 0.0896237, acc 0.984375, learning_rate 0.000206119
2017-10-10T11:46:34.776740: step 939, loss 0.225856, acc 0.890625, learning_rate 0.000205685
2017-10-10T11:46:35.007184: step 940, loss 0.233398, acc 0.921875, learning_rate 0.000205254
2017-10-10T11:46:35.135014: step 941, loss 0.195071, acc 0.875, learning_rate 0.000204824
2017-10-10T11:46:35.272320: step 942, loss 0.225814, acc 0.9375, learning_rate 0.000204397
2017-10-10T11:46:35.412062: step 943, loss 0.079083, acc 0.984375, learning_rate 0.00020397
2017-10-10T11:46:35.536636: step 944, loss 0.128759, acc 0.953125, learning_rate 0.000203546
2017-10-10T11:46:35.671736: step 945, loss 0.130472, acc 0.953125, learning_rate 0.000203123
2017-10-10T11:46:35.859227: step 946, loss 0.246336, acc 0.890625, learning_rate 0.000202702
2017-10-10T11:46:36.114029: step 947, loss 0.281264, acc 0.890625, learning_rate 0.000202283
2017-10-10T11:46:36.348966: step 948, loss 0.150041, acc 0.9375, learning_rate 0.000201866
2017-10-10T11:46:36.596853: step 949, loss 0.111746, acc 0.9375, learning_rate 0.00020145
2017-10-10T11:46:36.853671: step 950, loss 0.193442, acc 0.953125, learning_rate 0.000201036
2017-10-10T11:46:37.103975: step 951, loss 0.138716, acc 0.953125, learning_rate 0.000200623
2017-10-10T11:46:37.338728: step 952, loss 0.0723044, acc 0.984375, learning_rate 0.000200213
2017-10-10T11:46:37.541058: step 953, loss 0.206577, acc 0.90625, learning_rate 0.000199804
2017-10-10T11:46:37.784843: step 954, loss 0.186772, acc 0.9375, learning_rate 0.000199396
2017-10-10T11:46:38.021224: step 955, loss 0.274151, acc 0.875, learning_rate 0.000198991
2017-10-10T11:46:38.213654: step 956, loss 0.145793, acc 0.9375, learning_rate 0.000198587
2017-10-10T11:46:38.498261: step 957, loss 0.149814, acc 0.90625, learning_rate 0.000198184
2017-10-10T11:46:38.706132: step 958, loss 0.0791841, acc 0.984375, learning_rate 0.000197783
2017-10-10T11:46:38.936122: step 959, loss 0.141648, acc 0.984375, learning_rate 0.000197384
2017-10-10T11:46:39.217251: step 960, loss 0.316146, acc 0.921875, learning_rate 0.000196987

Evaluation:
2017-10-10T11:46:39.740903: step 960, loss 0.236438, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-960

2017-10-10T11:46:40.928732: step 961, loss 0.286236, acc 0.90625, learning_rate 0.000196591
2017-10-10T11:46:41.162516: step 962, loss 0.193454, acc 0.96875, learning_rate 0.000196197
2017-10-10T11:46:41.398419: step 963, loss 0.215098, acc 0.9375, learning_rate 0.000195804
2017-10-10T11:46:41.607348: step 964, loss 0.149836, acc 0.953125, learning_rate 0.000195413
2017-10-10T11:46:41.852132: step 965, loss 0.154293, acc 0.9375, learning_rate 0.000195023
2017-10-10T11:46:42.100714: step 966, loss 0.172776, acc 0.953125, learning_rate 0.000194636
2017-10-10T11:46:42.327667: step 967, loss 0.217652, acc 0.921875, learning_rate 0.000194249
2017-10-10T11:46:42.565311: step 968, loss 0.229635, acc 0.90625, learning_rate 0.000193865
2017-10-10T11:46:42.800877: step 969, loss 0.17028, acc 0.953125, learning_rate 0.000193482
2017-10-10T11:46:43.085197: step 970, loss 0.173603, acc 0.96875, learning_rate 0.0001931
2017-10-10T11:46:43.344493: step 971, loss 0.164518, acc 0.9375, learning_rate 0.00019272
2017-10-10T11:46:43.541664: step 972, loss 0.16642, acc 0.9375, learning_rate 0.000192341
2017-10-10T11:46:43.725753: step 973, loss 0.143991, acc 0.9375, learning_rate 0.000191965
2017-10-10T11:46:43.912849: step 974, loss 0.163907, acc 0.921875, learning_rate 0.000191589
2017-10-10T11:46:44.096830: step 975, loss 0.122139, acc 0.96875, learning_rate 0.000191215
2017-10-10T11:46:44.326475: step 976, loss 0.0902165, acc 0.96875, learning_rate 0.000190843
2017-10-10T11:46:44.576889: step 977, loss 0.12631, acc 0.96875, learning_rate 0.000190472
2017-10-10T11:46:44.808382: step 978, loss 0.197356, acc 0.90625, learning_rate 0.000190103
2017-10-10T11:46:45.027873: step 979, loss 0.140619, acc 0.96875, learning_rate 0.000189735
2017-10-10T11:46:45.208522: step 980, loss 0.259912, acc 0.882353, learning_rate 0.000189369
2017-10-10T11:46:45.536882: step 981, loss 0.153749, acc 0.96875, learning_rate 0.000189004
2017-10-10T11:46:45.801327: step 982, loss 0.278915, acc 0.90625, learning_rate 0.000188641
2017-10-10T11:46:45.968685: step 983, loss 0.137891, acc 0.953125, learning_rate 0.000188279
2017-10-10T11:46:46.144825: step 984, loss 0.112472, acc 0.984375, learning_rate 0.000187919
2017-10-10T11:46:46.303127: step 985, loss 0.109048, acc 0.96875, learning_rate 0.00018756
2017-10-10T11:46:46.474221: step 986, loss 0.156944, acc 0.9375, learning_rate 0.000187202
2017-10-10T11:46:46.692851: step 987, loss 0.230618, acc 0.921875, learning_rate 0.000186846
2017-10-10T11:46:46.978366: step 988, loss 0.0867752, acc 0.984375, learning_rate 0.000186492
2017-10-10T11:46:47.206012: step 989, loss 0.0956264, acc 0.984375, learning_rate 0.000186139
2017-10-10T11:46:47.442416: step 990, loss 0.191229, acc 0.953125, learning_rate 0.000185787
2017-10-10T11:46:47.668877: step 991, loss 0.235505, acc 0.9375, learning_rate 0.000185437
2017-10-10T11:46:47.901146: step 992, loss 0.0967136, acc 0.953125, learning_rate 0.000185088
2017-10-10T11:46:48.111221: step 993, loss 0.127978, acc 0.9375, learning_rate 0.000184741
2017-10-10T11:46:48.316687: step 994, loss 0.106541, acc 0.953125, learning_rate 0.000184395
2017-10-10T11:46:48.520837: step 995, loss 0.156115, acc 0.953125, learning_rate 0.000184051
2017-10-10T11:46:48.760909: step 996, loss 0.11996, acc 0.953125, learning_rate 0.000183708
2017-10-10T11:46:48.988875: step 997, loss 0.199686, acc 0.921875, learning_rate 0.000183366
2017-10-10T11:46:49.222194: step 998, loss 0.217116, acc 0.9375, learning_rate 0.000183026
2017-10-10T11:46:49.454611: step 999, loss 0.206547, acc 0.9375, learning_rate 0.000182687
2017-10-10T11:46:49.730997: step 1000, loss 0.135547, acc 0.96875, learning_rate 0.000182349

Evaluation:
2017-10-10T11:46:50.264898: step 1000, loss 0.234895, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1000

2017-10-10T11:46:52.049452: step 1001, loss 0.152848, acc 0.953125, learning_rate 0.000182013
2017-10-10T11:46:52.236917: step 1002, loss 0.0950003, acc 0.984375, learning_rate 0.000181678
2017-10-10T11:46:52.410721: step 1003, loss 0.150823, acc 0.953125, learning_rate 0.000181345
2017-10-10T11:46:52.613399: step 1004, loss 0.184213, acc 0.921875, learning_rate 0.000181013
2017-10-10T11:46:52.865448: step 1005, loss 0.0993941, acc 0.96875, learning_rate 0.000180682
2017-10-10T11:46:53.124866: step 1006, loss 0.162828, acc 0.921875, learning_rate 0.000180353
2017-10-10T11:46:53.366219: step 1007, loss 0.335704, acc 0.921875, learning_rate 0.000180025
2017-10-10T11:46:53.605635: step 1008, loss 0.1003, acc 1, learning_rate 0.000179698
2017-10-10T11:46:53.856750: step 1009, loss 0.0977449, acc 0.96875, learning_rate 0.000179373
2017-10-10T11:46:54.108406: step 1010, loss 0.174904, acc 0.9375, learning_rate 0.000179049
2017-10-10T11:46:54.363018: step 1011, loss 0.127779, acc 0.96875, learning_rate 0.000178726
2017-10-10T11:46:54.592147: step 1012, loss 0.0446881, acc 1, learning_rate 0.000178405
2017-10-10T11:46:54.833624: step 1013, loss 0.191701, acc 0.9375, learning_rate 0.000178085
2017-10-10T11:46:55.103905: step 1014, loss 0.169151, acc 0.9375, learning_rate 0.000177766
2017-10-10T11:46:55.360874: step 1015, loss 0.201989, acc 0.9375, learning_rate 0.000177449
2017-10-10T11:46:55.628721: step 1016, loss 0.153846, acc 0.9375, learning_rate 0.000177133
2017-10-10T11:46:55.848215: step 1017, loss 0.160861, acc 0.90625, learning_rate 0.000176818
2017-10-10T11:46:56.108468: step 1018, loss 0.170256, acc 0.9375, learning_rate 0.000176504
2017-10-10T11:46:56.415193: step 1019, loss 0.153776, acc 0.9375, learning_rate 0.000176192
2017-10-10T11:46:56.609090: step 1020, loss 0.0992752, acc 0.953125, learning_rate 0.000175881
2017-10-10T11:46:56.791743: step 1021, loss 0.227366, acc 0.9375, learning_rate 0.000175571
2017-10-10T11:46:56.982011: step 1022, loss 0.233086, acc 0.890625, learning_rate 0.000175263
2017-10-10T11:46:57.144998: step 1023, loss 0.1038, acc 0.953125, learning_rate 0.000174956
2017-10-10T11:46:57.324893: step 1024, loss 0.132429, acc 0.921875, learning_rate 0.00017465
2017-10-10T11:46:57.518601: step 1025, loss 0.245974, acc 0.9375, learning_rate 0.000174345
2017-10-10T11:46:57.741233: step 1026, loss 0.120157, acc 0.9375, learning_rate 0.000174042
2017-10-10T11:46:57.952270: step 1027, loss 0.0945151, acc 0.984375, learning_rate 0.000173739
2017-10-10T11:46:58.188258: step 1028, loss 0.121371, acc 0.953125, learning_rate 0.000173438
2017-10-10T11:46:58.427241: step 1029, loss 0.0801425, acc 0.96875, learning_rate 0.000173139
2017-10-10T11:46:58.680245: step 1030, loss 0.188275, acc 0.921875, learning_rate 0.00017284
2017-10-10T11:46:58.912126: step 1031, loss 0.13369, acc 0.9375, learning_rate 0.000172543
2017-10-10T11:46:59.141759: step 1032, loss 0.326538, acc 0.90625, learning_rate 0.000172247
2017-10-10T11:46:59.443390: step 1033, loss 0.209287, acc 0.921875, learning_rate 0.000171952
2017-10-10T11:46:59.628873: step 1034, loss 0.183268, acc 0.9375, learning_rate 0.000171658
2017-10-10T11:46:59.808906: step 1035, loss 0.322366, acc 0.859375, learning_rate 0.000171366
2017-10-10T11:46:59.973338: step 1036, loss 0.22971, acc 0.953125, learning_rate 0.000171074
2017-10-10T11:47:00.153966: step 1037, loss 0.250144, acc 0.953125, learning_rate 0.000170784
2017-10-10T11:47:00.326274: step 1038, loss 0.163186, acc 0.953125, learning_rate 0.000170495
2017-10-10T11:47:00.525205: step 1039, loss 0.191856, acc 0.921875, learning_rate 0.000170208
2017-10-10T11:47:00.767372: step 1040, loss 0.185849, acc 0.9375, learning_rate 0.000169921

Evaluation:
2017-10-10T11:47:01.332844: step 1040, loss 0.233455, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1040

2017-10-10T11:47:02.347275: step 1041, loss 0.237431, acc 0.921875, learning_rate 0.000169636
2017-10-10T11:47:02.585680: step 1042, loss 0.111967, acc 0.9375, learning_rate 0.000169351
2017-10-10T11:47:02.818766: step 1043, loss 0.222776, acc 0.921875, learning_rate 0.000169068
2017-10-10T11:47:03.010288: step 1044, loss 0.135376, acc 0.953125, learning_rate 0.000168786
2017-10-10T11:47:03.248118: step 1045, loss 0.100082, acc 0.953125, learning_rate 0.000168506
2017-10-10T11:47:03.479138: step 1046, loss 0.177979, acc 0.90625, learning_rate 0.000168226
2017-10-10T11:47:03.680131: step 1047, loss 0.215454, acc 0.921875, learning_rate 0.000167947
2017-10-10T11:47:03.883113: step 1048, loss 0.0979348, acc 0.984375, learning_rate 0.00016767
2017-10-10T11:47:04.145028: step 1049, loss 0.281391, acc 0.90625, learning_rate 0.000167394
2017-10-10T11:47:04.363843: step 1050, loss 0.125423, acc 0.96875, learning_rate 0.000167119
2017-10-10T11:47:04.611110: step 1051, loss 0.0978696, acc 0.984375, learning_rate 0.000166845
2017-10-10T11:47:04.894127: step 1052, loss 0.133183, acc 0.9375, learning_rate 0.000166572
2017-10-10T11:47:05.144015: step 1053, loss 0.0786038, acc 0.984375, learning_rate 0.0001663
2017-10-10T11:47:05.389902: step 1054, loss 0.110167, acc 0.953125, learning_rate 0.00016603
2017-10-10T11:47:05.629984: step 1055, loss 0.167552, acc 0.90625, learning_rate 0.00016576
2017-10-10T11:47:05.872675: step 1056, loss 0.201783, acc 0.9375, learning_rate 0.000165492
2017-10-10T11:47:06.104891: step 1057, loss 0.160614, acc 0.921875, learning_rate 0.000165224
2017-10-10T11:47:06.369133: step 1058, loss 0.174644, acc 0.921875, learning_rate 0.000164958
2017-10-10T11:47:06.625812: step 1059, loss 0.110883, acc 0.984375, learning_rate 0.000164693
2017-10-10T11:47:06.881951: step 1060, loss 0.135285, acc 0.953125, learning_rate 0.000164429
2017-10-10T11:47:07.129758: step 1061, loss 0.166147, acc 0.953125, learning_rate 0.000164166
2017-10-10T11:47:07.379212: step 1062, loss 0.0580677, acc 1, learning_rate 0.000163904
2017-10-10T11:47:07.669722: step 1063, loss 0.151916, acc 0.984375, learning_rate 0.000163643
2017-10-10T11:47:07.868636: step 1064, loss 0.151766, acc 0.96875, learning_rate 0.000163383
2017-10-10T11:47:08.150482: step 1065, loss 0.0742672, acc 0.984375, learning_rate 0.000163125
2017-10-10T11:47:08.283414: step 1066, loss 0.180298, acc 0.90625, learning_rate 0.000162867
2017-10-10T11:47:08.407404: step 1067, loss 0.158368, acc 0.953125, learning_rate 0.00016261
2017-10-10T11:47:08.529943: step 1068, loss 0.103146, acc 0.96875, learning_rate 0.000162355
2017-10-10T11:47:08.661419: step 1069, loss 0.0895249, acc 0.96875, learning_rate 0.0001621
2017-10-10T11:47:08.854828: step 1070, loss 0.18671, acc 0.953125, learning_rate 0.000161847
2017-10-10T11:47:09.092918: step 1071, loss 0.214394, acc 0.9375, learning_rate 0.000161594
2017-10-10T11:47:09.324811: step 1072, loss 0.114675, acc 0.9375, learning_rate 0.000161343
2017-10-10T11:47:09.558532: step 1073, loss 0.125526, acc 0.953125, learning_rate 0.000161093
2017-10-10T11:47:09.804917: step 1074, loss 0.206553, acc 0.921875, learning_rate 0.000160843
2017-10-10T11:47:10.040770: step 1075, loss 0.211016, acc 0.90625, learning_rate 0.000160595
2017-10-10T11:47:10.292901: step 1076, loss 0.19761, acc 0.921875, learning_rate 0.000160348
2017-10-10T11:47:10.546349: step 1077, loss 0.0845043, acc 0.96875, learning_rate 0.000160101
2017-10-10T11:47:10.714975: step 1078, loss 0.10004, acc 0.980392, learning_rate 0.000159856
2017-10-10T11:47:10.936894: step 1079, loss 0.202251, acc 0.9375, learning_rate 0.000159612
2017-10-10T11:47:11.176878: step 1080, loss 0.116124, acc 0.9375, learning_rate 0.000159368

Evaluation:
2017-10-10T11:47:11.644970: step 1080, loss 0.233366, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1080

2017-10-10T11:47:12.866821: step 1081, loss 0.21215, acc 0.9375, learning_rate 0.000159126
2017-10-10T11:47:13.106857: step 1082, loss 0.139112, acc 0.953125, learning_rate 0.000158885
2017-10-10T11:47:13.354323: step 1083, loss 0.128121, acc 0.953125, learning_rate 0.000158644
2017-10-10T11:47:13.603923: step 1084, loss 0.152703, acc 0.921875, learning_rate 0.000158405
2017-10-10T11:47:13.832479: step 1085, loss 0.147109, acc 0.9375, learning_rate 0.000158167
2017-10-10T11:47:14.074823: step 1086, loss 0.063001, acc 1, learning_rate 0.000157929
2017-10-10T11:47:14.332923: step 1087, loss 0.186368, acc 0.9375, learning_rate 0.000157693
2017-10-10T11:47:14.571754: step 1088, loss 0.176783, acc 0.953125, learning_rate 0.000157457
2017-10-10T11:47:14.813844: step 1089, loss 0.179938, acc 0.9375, learning_rate 0.000157223
2017-10-10T11:47:15.000851: step 1090, loss 0.194488, acc 0.90625, learning_rate 0.000156989
2017-10-10T11:47:15.267765: step 1091, loss 0.165868, acc 0.9375, learning_rate 0.000156757
2017-10-10T11:47:15.488861: step 1092, loss 0.306044, acc 0.90625, learning_rate 0.000156525
2017-10-10T11:47:15.712983: step 1093, loss 0.0824936, acc 0.984375, learning_rate 0.000156294
2017-10-10T11:47:15.948963: step 1094, loss 0.221896, acc 0.921875, learning_rate 0.000156064
2017-10-10T11:47:16.148364: step 1095, loss 0.165851, acc 0.953125, learning_rate 0.000155836
2017-10-10T11:47:16.431459: step 1096, loss 0.329844, acc 0.84375, learning_rate 0.000155608
2017-10-10T11:47:16.596388: step 1097, loss 0.115889, acc 0.96875, learning_rate 0.000155381
2017-10-10T11:47:16.791768: step 1098, loss 0.148538, acc 0.921875, learning_rate 0.000155155
2017-10-10T11:47:17.005037: step 1099, loss 0.157707, acc 0.953125, learning_rate 0.000154929
2017-10-10T11:47:17.210435: step 1100, loss 0.228547, acc 0.921875, learning_rate 0.000154705
2017-10-10T11:47:17.414005: step 1101, loss 0.220992, acc 0.9375, learning_rate 0.000154482
2017-10-10T11:47:17.623088: step 1102, loss 0.117955, acc 0.96875, learning_rate 0.00015426
2017-10-10T11:47:17.853116: step 1103, loss 0.312335, acc 0.921875, learning_rate 0.000154038
2017-10-10T11:47:18.092845: step 1104, loss 0.179956, acc 0.90625, learning_rate 0.000153818
2017-10-10T11:47:18.296695: step 1105, loss 0.206621, acc 0.9375, learning_rate 0.000153598
2017-10-10T11:47:18.594904: step 1106, loss 0.24536, acc 0.9375, learning_rate 0.000153379
2017-10-10T11:47:18.814706: step 1107, loss 0.165878, acc 0.953125, learning_rate 0.000153161
2017-10-10T11:47:18.972105: step 1108, loss 0.183291, acc 0.921875, learning_rate 0.000152944
2017-10-10T11:47:19.151195: step 1109, loss 0.124029, acc 0.984375, learning_rate 0.000152728
2017-10-10T11:47:19.321351: step 1110, loss 0.135901, acc 0.953125, learning_rate 0.000152513
2017-10-10T11:47:19.486436: step 1111, loss 0.0949577, acc 0.984375, learning_rate 0.000152299
2017-10-10T11:47:19.678222: step 1112, loss 0.159666, acc 0.953125, learning_rate 0.000152085
2017-10-10T11:47:19.909442: step 1113, loss 0.149645, acc 0.921875, learning_rate 0.000151872
2017-10-10T11:47:20.140503: step 1114, loss 0.18699, acc 0.9375, learning_rate 0.000151661
2017-10-10T11:47:20.376828: step 1115, loss 0.100943, acc 0.984375, learning_rate 0.00015145
2017-10-10T11:47:20.632718: step 1116, loss 0.11585, acc 0.96875, learning_rate 0.00015124
2017-10-10T11:47:20.835735: step 1117, loss 0.16514, acc 0.953125, learning_rate 0.000151031
2017-10-10T11:47:21.061246: step 1118, loss 0.166029, acc 0.953125, learning_rate 0.000150822
2017-10-10T11:47:21.321145: step 1119, loss 0.186921, acc 0.921875, learning_rate 0.000150615
2017-10-10T11:47:21.565752: step 1120, loss 0.151475, acc 0.9375, learning_rate 0.000150408

Evaluation:
2017-10-10T11:47:22.125573: step 1120, loss 0.232119, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1120

2017-10-10T11:47:23.513666: step 1121, loss 0.143778, acc 0.953125, learning_rate 0.000150203
2017-10-10T11:47:23.794275: step 1122, loss 0.13353, acc 0.953125, learning_rate 0.000149998
2017-10-10T11:47:24.045679: step 1123, loss 0.139145, acc 0.921875, learning_rate 0.000149794
2017-10-10T11:47:24.291606: step 1124, loss 0.169713, acc 0.921875, learning_rate 0.00014959
2017-10-10T11:47:24.524830: step 1125, loss 0.140216, acc 0.9375, learning_rate 0.000149388
2017-10-10T11:47:24.744927: step 1126, loss 0.163321, acc 0.953125, learning_rate 0.000149186
2017-10-10T11:47:25.028861: step 1127, loss 0.188618, acc 0.921875, learning_rate 0.000148986
2017-10-10T11:47:25.196840: step 1128, loss 0.0742192, acc 1, learning_rate 0.000148786
2017-10-10T11:47:25.368988: step 1129, loss 0.138026, acc 0.9375, learning_rate 0.000148587
2017-10-10T11:47:25.575012: step 1130, loss 0.156374, acc 0.953125, learning_rate 0.000148388
2017-10-10T11:47:25.738913: step 1131, loss 0.22216, acc 0.921875, learning_rate 0.000148191
2017-10-10T11:47:25.925302: step 1132, loss 0.211936, acc 0.9375, learning_rate 0.000147994
2017-10-10T11:47:26.088860: step 1133, loss 0.0912488, acc 0.96875, learning_rate 0.000147798
2017-10-10T11:47:26.274827: step 1134, loss 0.219852, acc 0.921875, learning_rate 0.000147603
2017-10-10T11:47:26.462086: step 1135, loss 0.146725, acc 0.96875, learning_rate 0.000147409
2017-10-10T11:47:26.699487: step 1136, loss 0.143496, acc 0.953125, learning_rate 0.000147215
2017-10-10T11:47:26.919149: step 1137, loss 0.23195, acc 0.953125, learning_rate 0.000147022
2017-10-10T11:47:27.152838: step 1138, loss 0.049158, acc 1, learning_rate 0.000146831
2017-10-10T11:47:27.388957: step 1139, loss 0.200392, acc 0.9375, learning_rate 0.000146639
2017-10-10T11:47:27.664597: step 1140, loss 0.220106, acc 0.90625, learning_rate 0.000146449
2017-10-10T11:47:27.887491: step 1141, loss 0.127794, acc 0.96875, learning_rate 0.000146259
2017-10-10T11:47:28.168977: step 1142, loss 0.0493787, acc 1, learning_rate 0.000146071
2017-10-10T11:47:28.403750: step 1143, loss 0.112649, acc 0.9375, learning_rate 0.000145883
2017-10-10T11:47:28.629706: step 1144, loss 0.118756, acc 0.953125, learning_rate 0.000145695
2017-10-10T11:47:28.887993: step 1145, loss 0.353218, acc 0.890625, learning_rate 0.000145509
2017-10-10T11:47:29.212145: step 1146, loss 0.143424, acc 0.953125, learning_rate 0.000145323
2017-10-10T11:47:29.467927: step 1147, loss 0.121296, acc 0.953125, learning_rate 0.000145138
2017-10-10T11:47:29.626192: step 1148, loss 0.121645, acc 0.921875, learning_rate 0.000144954
2017-10-10T11:47:29.807733: step 1149, loss 0.0987465, acc 0.96875, learning_rate 0.00014477
2017-10-10T11:47:29.986643: step 1150, loss 0.138443, acc 0.9375, learning_rate 0.000144588
2017-10-10T11:47:30.154993: step 1151, loss 0.108759, acc 0.953125, learning_rate 0.000144406
2017-10-10T11:47:30.321715: step 1152, loss 0.159638, acc 0.9375, learning_rate 0.000144224
2017-10-10T11:47:30.485324: step 1153, loss 0.145558, acc 0.984375, learning_rate 0.000144044
2017-10-10T11:47:30.722781: step 1154, loss 0.135298, acc 0.953125, learning_rate 0.000143864
2017-10-10T11:47:30.976027: step 1155, loss 0.137324, acc 0.96875, learning_rate 0.000143685
2017-10-10T11:47:31.236151: step 1156, loss 0.21501, acc 0.90625, learning_rate 0.000143507
2017-10-10T11:47:31.498489: step 1157, loss 0.113086, acc 0.953125, learning_rate 0.000143329
2017-10-10T11:47:31.762576: step 1158, loss 0.0988938, acc 0.96875, learning_rate 0.000143152
2017-10-10T11:47:32.020912: step 1159, loss 0.139771, acc 0.953125, learning_rate 0.000142976
2017-10-10T11:47:32.252274: step 1160, loss 0.0817029, acc 0.984375, learning_rate 0.000142801

Evaluation:
2017-10-10T11:47:32.803420: step 1160, loss 0.231316, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1160

2017-10-10T11:47:33.867825: step 1161, loss 0.0977904, acc 0.984375, learning_rate 0.000142626
2017-10-10T11:47:34.067436: step 1162, loss 0.126111, acc 0.953125, learning_rate 0.000142452
2017-10-10T11:47:34.255423: step 1163, loss 0.0895471, acc 0.984375, learning_rate 0.000142279
2017-10-10T11:47:34.436854: step 1164, loss 0.0854715, acc 0.96875, learning_rate 0.000142106
2017-10-10T11:47:34.618965: step 1165, loss 0.107154, acc 0.96875, learning_rate 0.000141934
2017-10-10T11:47:34.828840: step 1166, loss 0.109992, acc 0.953125, learning_rate 0.000141763
2017-10-10T11:47:35.034450: step 1167, loss 0.167502, acc 0.921875, learning_rate 0.000141593
2017-10-10T11:47:35.256458: step 1168, loss 0.122888, acc 0.9375, learning_rate 0.000141423
2017-10-10T11:47:35.495660: step 1169, loss 0.109906, acc 0.953125, learning_rate 0.000141254
2017-10-10T11:47:35.743445: step 1170, loss 0.158339, acc 0.953125, learning_rate 0.000141085
2017-10-10T11:47:36.001491: step 1171, loss 0.0755549, acc 0.96875, learning_rate 0.000140918
2017-10-10T11:47:36.232312: step 1172, loss 0.0949055, acc 0.96875, learning_rate 0.000140751
2017-10-10T11:47:36.497206: step 1173, loss 0.10221, acc 0.984375, learning_rate 0.000140584
2017-10-10T11:47:36.743796: step 1174, loss 0.251748, acc 0.890625, learning_rate 0.000140419
2017-10-10T11:47:37.008861: step 1175, loss 0.13224, acc 0.953125, learning_rate 0.000140254
2017-10-10T11:47:37.214472: step 1176, loss 0.0826589, acc 0.980392, learning_rate 0.000140089
2017-10-10T11:47:37.443704: step 1177, loss 0.138575, acc 0.9375, learning_rate 0.000139926
2017-10-10T11:47:37.680251: step 1178, loss 0.135884, acc 0.96875, learning_rate 0.000139763
2017-10-10T11:47:37.872194: step 1179, loss 0.0975413, acc 0.96875, learning_rate 0.0001396
2017-10-10T11:47:38.120853: step 1180, loss 0.159845, acc 0.96875, learning_rate 0.000139439
2017-10-10T11:47:38.311658: step 1181, loss 0.0810861, acc 1, learning_rate 0.000139278
2017-10-10T11:47:38.552707: step 1182, loss 0.170049, acc 0.953125, learning_rate 0.000139118
2017-10-10T11:47:38.760874: step 1183, loss 0.0993263, acc 0.984375, learning_rate 0.000138958
2017-10-10T11:47:38.998206: step 1184, loss 0.0524686, acc 1, learning_rate 0.000138799
2017-10-10T11:47:39.251049: step 1185, loss 0.158364, acc 0.953125, learning_rate 0.00013864
2017-10-10T11:47:39.465140: step 1186, loss 0.0883313, acc 0.984375, learning_rate 0.000138483
2017-10-10T11:47:39.718627: step 1187, loss 0.2522, acc 0.90625, learning_rate 0.000138326
2017-10-10T11:47:39.916856: step 1188, loss 0.292747, acc 0.890625, learning_rate 0.000138169
2017-10-10T11:47:40.141504: step 1189, loss 0.149768, acc 0.953125, learning_rate 0.000138013
2017-10-10T11:47:40.340855: step 1190, loss 0.0852132, acc 0.984375, learning_rate 0.000137858
2017-10-10T11:47:40.684887: step 1191, loss 0.137436, acc 0.9375, learning_rate 0.000137704
2017-10-10T11:47:40.941695: step 1192, loss 0.210143, acc 0.953125, learning_rate 0.00013755
2017-10-10T11:47:41.113362: step 1193, loss 0.210645, acc 0.953125, learning_rate 0.000137397
2017-10-10T11:47:41.289879: step 1194, loss 0.206429, acc 0.921875, learning_rate 0.000137244
2017-10-10T11:47:41.471654: step 1195, loss 0.212086, acc 0.90625, learning_rate 0.000137092
2017-10-10T11:47:41.650708: step 1196, loss 0.117679, acc 0.953125, learning_rate 0.000136941
2017-10-10T11:47:41.804739: step 1197, loss 0.119805, acc 0.953125, learning_rate 0.00013679
2017-10-10T11:47:42.120315: step 1198, loss 0.116133, acc 0.953125, learning_rate 0.00013664
2017-10-10T11:47:42.276210: step 1199, loss 0.147755, acc 0.9375, learning_rate 0.00013649
2017-10-10T11:47:42.456154: step 1200, loss 0.128896, acc 0.96875, learning_rate 0.000136341

Evaluation:
2017-10-10T11:47:42.911380: step 1200, loss 0.230842, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1200

2017-10-10T11:47:44.008433: step 1201, loss 0.126093, acc 0.96875, learning_rate 0.000136193
2017-10-10T11:47:44.212149: step 1202, loss 0.157223, acc 0.9375, learning_rate 0.000136045
2017-10-10T11:47:44.463633: step 1203, loss 0.116004, acc 0.953125, learning_rate 0.000135898
2017-10-10T11:47:44.716883: step 1204, loss 0.166135, acc 0.953125, learning_rate 0.000135751
2017-10-10T11:47:44.948851: step 1205, loss 0.217687, acc 0.921875, learning_rate 0.000135605
2017-10-10T11:47:45.193094: step 1206, loss 0.17, acc 0.9375, learning_rate 0.00013546
2017-10-10T11:47:45.425032: step 1207, loss 0.14924, acc 0.953125, learning_rate 0.000135315
2017-10-10T11:47:45.641849: step 1208, loss 0.175904, acc 0.953125, learning_rate 0.000135171
2017-10-10T11:47:45.859899: step 1209, loss 0.245626, acc 0.875, learning_rate 0.000135028
2017-10-10T11:47:46.049102: step 1210, loss 0.124805, acc 0.9375, learning_rate 0.000134885
2017-10-10T11:47:46.274040: step 1211, loss 0.139287, acc 0.96875, learning_rate 0.000134742
2017-10-10T11:47:46.458509: step 1212, loss 0.0899615, acc 0.984375, learning_rate 0.0001346
2017-10-10T11:47:46.692125: step 1213, loss 0.0908256, acc 0.984375, learning_rate 0.000134459
2017-10-10T11:47:46.938047: step 1214, loss 0.141689, acc 0.953125, learning_rate 0.000134319
2017-10-10T11:47:47.182154: step 1215, loss 0.188486, acc 0.9375, learning_rate 0.000134178
2017-10-10T11:47:47.388897: step 1216, loss 0.142291, acc 0.96875, learning_rate 0.000134039
2017-10-10T11:47:47.628867: step 1217, loss 0.130349, acc 0.953125, learning_rate 0.0001339
2017-10-10T11:47:47.876249: step 1218, loss 0.186643, acc 0.9375, learning_rate 0.000133762
2017-10-10T11:47:48.108899: step 1219, loss 0.182193, acc 0.9375, learning_rate 0.000133624
2017-10-10T11:47:48.328823: step 1220, loss 0.175904, acc 0.921875, learning_rate 0.000133487
2017-10-10T11:47:48.534774: step 1221, loss 0.0704188, acc 0.984375, learning_rate 0.00013335
2017-10-10T11:47:48.727000: step 1222, loss 0.109825, acc 0.953125, learning_rate 0.000133214
2017-10-10T11:47:48.965541: step 1223, loss 0.118597, acc 0.953125, learning_rate 0.000133078
2017-10-10T11:47:49.229145: step 1224, loss 0.133595, acc 0.96875, learning_rate 0.000132943
2017-10-10T11:47:49.448863: step 1225, loss 0.124632, acc 0.96875, learning_rate 0.000132809
2017-10-10T11:47:49.671967: step 1226, loss 0.113272, acc 0.96875, learning_rate 0.000132675
2017-10-10T11:47:49.925944: step 1227, loss 0.161777, acc 0.953125, learning_rate 0.000132541
2017-10-10T11:47:50.168822: step 1228, loss 0.0601653, acc 0.96875, learning_rate 0.000132409
2017-10-10T11:47:50.468876: step 1229, loss 0.0392573, acc 1, learning_rate 0.000132276
2017-10-10T11:47:50.644877: step 1230, loss 0.182997, acc 0.953125, learning_rate 0.000132145
2017-10-10T11:47:50.848836: step 1231, loss 0.162019, acc 0.9375, learning_rate 0.000132013
2017-10-10T11:47:51.077888: step 1232, loss 0.181022, acc 0.953125, learning_rate 0.000131883
2017-10-10T11:47:51.263135: step 1233, loss 0.336263, acc 0.890625, learning_rate 0.000131753
2017-10-10T11:47:51.433708: step 1234, loss 0.0481227, acc 0.984375, learning_rate 0.000131623
2017-10-10T11:47:51.588918: step 1235, loss 0.152458, acc 0.953125, learning_rate 0.000131494
2017-10-10T11:47:51.986054: step 1236, loss 0.222738, acc 0.90625, learning_rate 0.000131365
2017-10-10T11:47:52.188224: step 1237, loss 0.185873, acc 0.953125, learning_rate 0.000131237
2017-10-10T11:47:52.324911: step 1238, loss 0.115632, acc 0.96875, learning_rate 0.00013111
2017-10-10T11:47:52.477693: step 1239, loss 0.268769, acc 0.9375, learning_rate 0.000130983
2017-10-10T11:47:52.633964: step 1240, loss 0.0343926, acc 1, learning_rate 0.000130856

Evaluation:
2017-10-10T11:47:53.007496: step 1240, loss 0.229609, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1240

2017-10-10T11:47:54.256422: step 1241, loss 0.0989323, acc 0.984375, learning_rate 0.00013073
2017-10-10T11:47:54.484870: step 1242, loss 0.247779, acc 0.90625, learning_rate 0.000130605
2017-10-10T11:47:54.709342: step 1243, loss 0.146837, acc 0.953125, learning_rate 0.00013048
2017-10-10T11:47:54.941411: step 1244, loss 0.0800843, acc 0.96875, learning_rate 0.000130356
2017-10-10T11:47:55.166271: step 1245, loss 0.259998, acc 0.875, learning_rate 0.000130232
2017-10-10T11:47:55.431543: step 1246, loss 0.162895, acc 0.9375, learning_rate 0.000130108
2017-10-10T11:47:55.656488: step 1247, loss 0.0803931, acc 0.984375, learning_rate 0.000129985
2017-10-10T11:47:55.901834: step 1248, loss 0.218591, acc 0.921875, learning_rate 0.000129863
2017-10-10T11:47:56.137129: step 1249, loss 0.172752, acc 0.9375, learning_rate 0.000129741
2017-10-10T11:47:56.348940: step 1250, loss 0.0502157, acc 1, learning_rate 0.00012962
2017-10-10T11:47:56.581605: step 1251, loss 0.194283, acc 0.875, learning_rate 0.000129499
2017-10-10T11:47:56.821040: step 1252, loss 0.148281, acc 0.953125, learning_rate 0.000129378
2017-10-10T11:47:57.057683: step 1253, loss 0.0779401, acc 0.96875, learning_rate 0.000129259
2017-10-10T11:47:57.296107: step 1254, loss 0.109344, acc 0.96875, learning_rate 0.000129139
2017-10-10T11:47:57.516091: step 1255, loss 0.0868249, acc 0.953125, learning_rate 0.00012902
2017-10-10T11:47:57.744937: step 1256, loss 0.197272, acc 0.9375, learning_rate 0.000128902
2017-10-10T11:47:57.972167: step 1257, loss 0.239019, acc 0.90625, learning_rate 0.000128784
2017-10-10T11:47:58.214791: step 1258, loss 0.123851, acc 0.96875, learning_rate 0.000128666
2017-10-10T11:47:58.474734: step 1259, loss 0.271956, acc 0.9375, learning_rate 0.000128549
2017-10-10T11:47:58.778581: step 1260, loss 0.184922, acc 0.96875, learning_rate 0.000128433
2017-10-10T11:47:58.983886: step 1261, loss 0.108569, acc 0.984375, learning_rate 0.000128317
2017-10-10T11:47:59.181585: step 1262, loss 0.176778, acc 0.921875, learning_rate 0.000128201
2017-10-10T11:47:59.377150: step 1263, loss 0.135536, acc 0.953125, learning_rate 0.000128086
2017-10-10T11:47:59.548884: step 1264, loss 0.197944, acc 0.921875, learning_rate 0.000127971
2017-10-10T11:47:59.774489: step 1265, loss 0.0725931, acc 0.984375, learning_rate 0.000127857
2017-10-10T11:47:59.973085: step 1266, loss 0.345439, acc 0.90625, learning_rate 0.000127743
2017-10-10T11:48:00.216992: step 1267, loss 0.0951994, acc 0.984375, learning_rate 0.00012763
2017-10-10T11:48:00.479519: step 1268, loss 0.229712, acc 0.9375, learning_rate 0.000127517
2017-10-10T11:48:00.693851: step 1269, loss 0.0883769, acc 0.953125, learning_rate 0.000127405
2017-10-10T11:48:00.916911: step 1270, loss 0.155946, acc 0.953125, learning_rate 0.000127293
2017-10-10T11:48:01.137270: step 1271, loss 0.173832, acc 0.921875, learning_rate 0.000127182
2017-10-10T11:48:01.349249: step 1272, loss 0.144618, acc 0.953125, learning_rate 0.000127071
2017-10-10T11:48:01.585560: step 1273, loss 0.11427, acc 0.984375, learning_rate 0.00012696
2017-10-10T11:48:01.791433: step 1274, loss 0.157668, acc 0.921569, learning_rate 0.00012685
2017-10-10T11:48:02.036853: step 1275, loss 0.093784, acc 0.984375, learning_rate 0.000126741
2017-10-10T11:48:02.277915: step 1276, loss 0.0764876, acc 0.984375, learning_rate 0.000126632
2017-10-10T11:48:02.513732: step 1277, loss 0.162759, acc 0.9375, learning_rate 0.000126523
2017-10-10T11:48:02.788877: step 1278, loss 0.11227, acc 0.953125, learning_rate 0.000126415
2017-10-10T11:48:03.055804: step 1279, loss 0.117627, acc 0.984375, learning_rate 0.000126307
2017-10-10T11:48:03.266719: step 1280, loss 0.0826265, acc 0.953125, learning_rate 0.000126199

Evaluation:
2017-10-10T11:48:03.658816: step 1280, loss 0.231347, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1280

2017-10-10T11:48:04.614911: step 1281, loss 0.139996, acc 0.921875, learning_rate 0.000126093
2017-10-10T11:48:04.867292: step 1282, loss 0.1241, acc 0.984375, learning_rate 0.000125986
2017-10-10T11:48:05.115772: step 1283, loss 0.161626, acc 0.953125, learning_rate 0.00012588
2017-10-10T11:48:05.351281: step 1284, loss 0.0945198, acc 0.96875, learning_rate 0.000125774
2017-10-10T11:48:05.599268: step 1285, loss 0.0825523, acc 1, learning_rate 0.000125669
2017-10-10T11:48:05.844717: step 1286, loss 0.104375, acc 0.96875, learning_rate 0.000125564
2017-10-10T11:48:06.053536: step 1287, loss 0.0430359, acc 1, learning_rate 0.00012546
2017-10-10T11:48:06.297550: step 1288, loss 0.241841, acc 0.921875, learning_rate 0.000125356
2017-10-10T11:48:06.530514: step 1289, loss 0.120551, acc 0.96875, learning_rate 0.000125253
2017-10-10T11:48:06.805039: step 1290, loss 0.148585, acc 0.9375, learning_rate 0.00012515
2017-10-10T11:48:06.972091: step 1291, loss 0.218208, acc 0.890625, learning_rate 0.000125047
2017-10-10T11:48:07.151885: step 1292, loss 0.209533, acc 0.9375, learning_rate 0.000124945
2017-10-10T11:48:07.332317: step 1293, loss 0.171839, acc 0.9375, learning_rate 0.000124843
2017-10-10T11:48:07.532776: step 1294, loss 0.206175, acc 0.9375, learning_rate 0.000124741
2017-10-10T11:48:07.723575: step 1295, loss 0.162842, acc 0.96875, learning_rate 0.00012464
2017-10-10T11:48:07.944847: step 1296, loss 0.226746, acc 0.921875, learning_rate 0.00012454
2017-10-10T11:48:08.169493: step 1297, loss 0.16779, acc 0.953125, learning_rate 0.00012444
2017-10-10T11:48:08.416849: step 1298, loss 0.0932073, acc 0.984375, learning_rate 0.00012434
2017-10-10T11:48:08.672821: step 1299, loss 0.0973244, acc 0.96875, learning_rate 0.000124241
2017-10-10T11:48:08.907343: step 1300, loss 0.0340455, acc 1, learning_rate 0.000124142
2017-10-10T11:48:09.130338: step 1301, loss 0.04903, acc 1, learning_rate 0.000124043
2017-10-10T11:48:09.362757: step 1302, loss 0.154025, acc 0.953125, learning_rate 0.000123945
2017-10-10T11:48:09.580877: step 1303, loss 0.156881, acc 0.921875, learning_rate 0.000123847
2017-10-10T11:48:09.816612: step 1304, loss 0.181488, acc 0.921875, learning_rate 0.00012375
2017-10-10T11:48:09.987462: step 1305, loss 0.173462, acc 0.9375, learning_rate 0.000123653
2017-10-10T11:48:10.200793: step 1306, loss 0.0744381, acc 0.984375, learning_rate 0.000123556
2017-10-10T11:48:10.424244: step 1307, loss 0.118703, acc 0.953125, learning_rate 0.00012346
2017-10-10T11:48:10.644823: step 1308, loss 0.108172, acc 0.984375, learning_rate 0.000123364
2017-10-10T11:48:10.880496: step 1309, loss 0.0447511, acc 1, learning_rate 0.000123269
2017-10-10T11:48:11.126270: step 1310, loss 0.172227, acc 0.90625, learning_rate 0.000123174
2017-10-10T11:48:11.360026: step 1311, loss 0.137151, acc 0.9375, learning_rate 0.00012308
2017-10-10T11:48:11.576313: step 1312, loss 0.136471, acc 0.953125, learning_rate 0.000122985
2017-10-10T11:48:11.805189: step 1313, loss 0.162816, acc 0.953125, learning_rate 0.000122892
2017-10-10T11:48:12.040863: step 1314, loss 0.188975, acc 0.9375, learning_rate 0.000122798
2017-10-10T11:48:12.284855: step 1315, loss 0.103574, acc 0.96875, learning_rate 0.000122705
2017-10-10T11:48:12.539262: step 1316, loss 0.0812285, acc 0.96875, learning_rate 0.000122612
2017-10-10T11:48:12.780452: step 1317, loss 0.157774, acc 0.96875, learning_rate 0.00012252
2017-10-10T11:48:13.025162: step 1318, loss 0.189641, acc 0.921875, learning_rate 0.000122428
2017-10-10T11:48:13.263619: step 1319, loss 0.375947, acc 0.90625, learning_rate 0.000122337
2017-10-10T11:48:13.493433: step 1320, loss 0.127374, acc 0.953125, learning_rate 0.000122245

Evaluation:
2017-10-10T11:48:37.235714: step 1320, loss 0.230183, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1320

2017-10-10T11:48:45.260411: step 1321, loss 0.233358, acc 0.953125, learning_rate 0.000122155
2017-10-10T11:48:45.538066: step 1322, loss 0.346806, acc 0.90625, learning_rate 0.000122064
2017-10-10T11:48:45.807372: step 1323, loss 0.178793, acc 0.953125, learning_rate 0.000121974
2017-10-10T11:48:46.060867: step 1324, loss 0.319164, acc 0.84375, learning_rate 0.000121884
2017-10-10T11:48:46.316895: step 1325, loss 0.159812, acc 0.953125, learning_rate 0.000121795
2017-10-10T11:48:46.564286: step 1326, loss 0.119603, acc 0.96875, learning_rate 0.000121706
2017-10-10T11:48:46.795236: step 1327, loss 0.0772159, acc 0.984375, learning_rate 0.000121618
2017-10-10T11:48:47.066005: step 1328, loss 0.196163, acc 0.953125, learning_rate 0.000121529
2017-10-10T11:48:47.316993: step 1329, loss 0.166331, acc 0.953125, learning_rate 0.000121441
2017-10-10T11:48:47.574862: step 1330, loss 0.162333, acc 0.96875, learning_rate 0.000121354
2017-10-10T11:48:47.823814: step 1331, loss 0.121198, acc 0.96875, learning_rate 0.000121267
2017-10-10T11:48:48.054305: step 1332, loss 0.136917, acc 0.9375, learning_rate 0.00012118
2017-10-10T11:48:48.295058: step 1333, loss 0.0872592, acc 0.984375, learning_rate 0.000121093
2017-10-10T11:48:48.520233: step 1334, loss 0.15393, acc 0.9375, learning_rate 0.000121007
2017-10-10T11:48:48.737155: step 1335, loss 0.243942, acc 0.921875, learning_rate 0.000120922
2017-10-10T11:48:48.926432: step 1336, loss 0.122276, acc 0.96875, learning_rate 0.000120836
2017-10-10T11:48:49.146637: step 1337, loss 0.0759793, acc 0.984375, learning_rate 0.000120751
2017-10-10T11:48:49.382587: step 1338, loss 0.13291, acc 0.953125, learning_rate 0.000120666
2017-10-10T11:48:49.648762: step 1339, loss 0.133707, acc 0.921875, learning_rate 0.000120582
2017-10-10T11:48:49.896950: step 1340, loss 0.236106, acc 0.890625, learning_rate 0.000120498
2017-10-10T11:48:50.156514: step 1341, loss 0.15656, acc 0.953125, learning_rate 0.000120414
2017-10-10T11:48:50.389610: step 1342, loss 0.123462, acc 0.953125, learning_rate 0.000120331
2017-10-10T11:48:50.630144: step 1343, loss 0.10894, acc 0.984375, learning_rate 0.000120248
2017-10-10T11:48:50.871547: step 1344, loss 0.173179, acc 0.9375, learning_rate 0.000120165
2017-10-10T11:48:51.131843: step 1345, loss 0.0812068, acc 1, learning_rate 0.000120083
2017-10-10T11:48:51.359126: step 1346, loss 0.177328, acc 0.921875, learning_rate 0.000120001
2017-10-10T11:48:51.572850: step 1347, loss 0.128333, acc 0.953125, learning_rate 0.00011992
2017-10-10T11:48:51.778430: step 1348, loss 0.190361, acc 0.90625, learning_rate 0.000119838
2017-10-10T11:48:52.010257: step 1349, loss 0.146879, acc 0.9375, learning_rate 0.000119757
2017-10-10T11:48:52.226164: step 1350, loss 0.132333, acc 0.984375, learning_rate 0.000119677
2017-10-10T11:48:52.512953: step 1351, loss 0.264033, acc 0.890625, learning_rate 0.000119596
2017-10-10T11:48:52.769279: step 1352, loss 0.139822, acc 0.953125, learning_rate 0.000119516
2017-10-10T11:48:52.936029: step 1353, loss 0.0938436, acc 0.953125, learning_rate 0.000119437
2017-10-10T11:48:53.121555: step 1354, loss 0.206299, acc 0.90625, learning_rate 0.000119357
2017-10-10T11:48:53.323673: step 1355, loss 0.194824, acc 0.9375, learning_rate 0.000119278
2017-10-10T11:48:53.532207: step 1356, loss 0.0787535, acc 0.984375, learning_rate 0.0001192
2017-10-10T11:48:53.771701: step 1357, loss 0.161493, acc 0.921875, learning_rate 0.000119121
2017-10-10T11:48:53.993708: step 1358, loss 0.122167, acc 0.953125, learning_rate 0.000119043
2017-10-10T11:48:54.177650: step 1359, loss 0.193985, acc 0.921875, learning_rate 0.000118965
2017-10-10T11:48:54.395323: step 1360, loss 0.121447, acc 0.953125, learning_rate 0.000118888

Evaluation:
2017-10-10T11:48:54.912839: step 1360, loss 0.232025, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1360

2017-10-10T11:48:55.970096: step 1361, loss 0.130572, acc 0.96875, learning_rate 0.000118811
2017-10-10T11:48:56.163624: step 1362, loss 0.128351, acc 0.96875, learning_rate 0.000118734
2017-10-10T11:48:56.327932: step 1363, loss 0.181263, acc 0.921875, learning_rate 0.000118658
2017-10-10T11:48:56.580201: step 1364, loss 0.16772, acc 0.9375, learning_rate 0.000118582
2017-10-10T11:48:56.822677: step 1365, loss 0.219601, acc 0.9375, learning_rate 0.000118506
2017-10-10T11:48:57.085290: step 1366, loss 0.154195, acc 0.9375, learning_rate 0.00011843
2017-10-10T11:48:57.320428: step 1367, loss 0.120742, acc 0.953125, learning_rate 0.000118355
2017-10-10T11:48:57.590438: step 1368, loss 0.1478, acc 0.953125, learning_rate 0.00011828
2017-10-10T11:48:57.868003: step 1369, loss 0.265641, acc 0.90625, learning_rate 0.000118205
2017-10-10T11:48:58.149012: step 1370, loss 0.131204, acc 0.96875, learning_rate 0.000118131
2017-10-10T11:48:58.418999: step 1371, loss 0.0915871, acc 0.96875, learning_rate 0.000118057
2017-10-10T11:48:58.673041: step 1372, loss 0.108304, acc 0.960784, learning_rate 0.000117983
2017-10-10T11:48:58.914632: step 1373, loss 0.217736, acc 0.9375, learning_rate 0.00011791
2017-10-10T11:48:59.163692: step 1374, loss 0.152425, acc 0.921875, learning_rate 0.000117837
2017-10-10T11:48:59.408884: step 1375, loss 0.074003, acc 1, learning_rate 0.000117764
2017-10-10T11:48:59.619503: step 1376, loss 0.12345, acc 0.96875, learning_rate 0.000117692
2017-10-10T11:48:59.824843: step 1377, loss 0.0732351, acc 0.984375, learning_rate 0.000117619
2017-10-10T11:49:00.044861: step 1378, loss 0.210884, acc 0.921875, learning_rate 0.000117547
2017-10-10T11:49:00.244614: step 1379, loss 0.112539, acc 0.96875, learning_rate 0.000117476
2017-10-10T11:49:00.465026: step 1380, loss 0.170284, acc 0.921875, learning_rate 0.000117404
2017-10-10T11:49:00.738523: step 1381, loss 0.110463, acc 0.96875, learning_rate 0.000117333
2017-10-10T11:49:00.954699: step 1382, loss 0.216415, acc 0.921875, learning_rate 0.000117263
2017-10-10T11:49:01.253100: step 1383, loss 0.122051, acc 0.96875, learning_rate 0.000117192
2017-10-10T11:49:01.453800: step 1384, loss 0.0472373, acc 0.984375, learning_rate 0.000117122
2017-10-10T11:49:01.665605: step 1385, loss 0.15705, acc 0.9375, learning_rate 0.000117052
2017-10-10T11:49:01.860953: step 1386, loss 0.107182, acc 0.96875, learning_rate 0.000116983
2017-10-10T11:49:02.068932: step 1387, loss 0.213978, acc 0.890625, learning_rate 0.000116913
2017-10-10T11:49:02.254093: step 1388, loss 0.0508905, acc 0.984375, learning_rate 0.000116844
2017-10-10T11:49:02.482791: step 1389, loss 0.118662, acc 0.9375, learning_rate 0.000116775
2017-10-10T11:49:02.719768: step 1390, loss 0.146192, acc 0.96875, learning_rate 0.000116707
2017-10-10T11:49:02.945905: step 1391, loss 0.130438, acc 0.96875, learning_rate 0.000116639
2017-10-10T11:49:03.183604: step 1392, loss 0.18857, acc 0.9375, learning_rate 0.000116571
2017-10-10T11:49:03.412925: step 1393, loss 0.0938442, acc 0.96875, learning_rate 0.000116503
2017-10-10T11:49:03.642601: step 1394, loss 0.240444, acc 0.90625, learning_rate 0.000116436
2017-10-10T11:49:03.864866: step 1395, loss 0.240799, acc 0.9375, learning_rate 0.000116369
2017-10-10T11:49:04.084753: step 1396, loss 0.13835, acc 0.984375, learning_rate 0.000116302
2017-10-10T11:49:04.316930: step 1397, loss 0.1115, acc 0.953125, learning_rate 0.000116235
2017-10-10T11:49:04.566253: step 1398, loss 0.153806, acc 0.9375, learning_rate 0.000116169
2017-10-10T11:49:04.825000: step 1399, loss 0.175482, acc 0.9375, learning_rate 0.000116103
2017-10-10T11:49:05.033998: step 1400, loss 0.0888669, acc 0.984375, learning_rate 0.000116037

Evaluation:
2017-10-10T11:49:05.506588: step 1400, loss 0.230569, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1400

2017-10-10T11:49:06.701986: step 1401, loss 0.210258, acc 0.953125, learning_rate 0.000115972
2017-10-10T11:49:06.880840: step 1402, loss 0.114916, acc 0.9375, learning_rate 0.000115907
2017-10-10T11:49:07.042754: step 1403, loss 0.109029, acc 0.96875, learning_rate 0.000115842
2017-10-10T11:49:07.228845: step 1404, loss 0.0738324, acc 1, learning_rate 0.000115777
2017-10-10T11:49:07.504832: step 1405, loss 0.284638, acc 0.90625, learning_rate 0.000115713
2017-10-10T11:49:07.755811: step 1406, loss 0.102257, acc 0.96875, learning_rate 0.000115649
2017-10-10T11:49:08.020167: step 1407, loss 0.114426, acc 0.96875, learning_rate 0.000115585
2017-10-10T11:49:08.265655: step 1408, loss 0.186508, acc 0.953125, learning_rate 0.000115521
2017-10-10T11:49:08.485763: step 1409, loss 0.100035, acc 0.953125, learning_rate 0.000115458
2017-10-10T11:49:08.750386: step 1410, loss 0.0905025, acc 0.953125, learning_rate 0.000115395
2017-10-10T11:49:08.963050: step 1411, loss 0.167303, acc 0.953125, learning_rate 0.000115332
2017-10-10T11:49:09.154534: step 1412, loss 0.177093, acc 0.96875, learning_rate 0.000115269
2017-10-10T11:49:09.362353: step 1413, loss 0.0787447, acc 0.984375, learning_rate 0.000115207
2017-10-10T11:49:09.664488: step 1414, loss 0.0613629, acc 1, learning_rate 0.000115145
2017-10-10T11:49:09.876857: step 1415, loss 0.0763369, acc 0.984375, learning_rate 0.000115083
2017-10-10T11:49:10.085530: step 1416, loss 0.122538, acc 0.96875, learning_rate 0.000115022
2017-10-10T11:49:10.275774: step 1417, loss 0.117981, acc 0.953125, learning_rate 0.00011496
2017-10-10T11:49:10.477018: step 1418, loss 0.0705529, acc 1, learning_rate 0.000114899
2017-10-10T11:49:10.696241: step 1419, loss 0.315801, acc 0.890625, learning_rate 0.000114838
2017-10-10T11:49:10.942099: step 1420, loss 0.150314, acc 0.953125, learning_rate 0.000114778
2017-10-10T11:49:11.184983: step 1421, loss 0.197427, acc 0.953125, learning_rate 0.000114717
2017-10-10T11:49:11.434034: step 1422, loss 0.104956, acc 0.96875, learning_rate 0.000114657
2017-10-10T11:49:11.670405: step 1423, loss 0.0680386, acc 0.96875, learning_rate 0.000114598
2017-10-10T11:49:11.913140: step 1424, loss 0.0730207, acc 0.96875, learning_rate 0.000114538
2017-10-10T11:49:12.155453: step 1425, loss 0.289765, acc 0.890625, learning_rate 0.000114479
2017-10-10T11:49:12.391407: step 1426, loss 0.11743, acc 0.953125, learning_rate 0.00011442
2017-10-10T11:49:12.578738: step 1427, loss 0.0749489, acc 0.984375, learning_rate 0.000114361
2017-10-10T11:49:12.794725: step 1428, loss 0.0715047, acc 0.984375, learning_rate 0.000114302
2017-10-10T11:49:13.026233: step 1429, loss 0.149697, acc 0.96875, learning_rate 0.000114244
2017-10-10T11:49:13.271981: step 1430, loss 0.165178, acc 0.953125, learning_rate 0.000114186
2017-10-10T11:49:13.484870: step 1431, loss 0.274987, acc 0.90625, learning_rate 0.000114128
2017-10-10T11:49:13.708869: step 1432, loss 0.0988047, acc 0.9375, learning_rate 0.00011407
2017-10-10T11:49:13.953034: step 1433, loss 0.159857, acc 0.921875, learning_rate 0.000114013
2017-10-10T11:49:14.181112: step 1434, loss 0.173781, acc 0.9375, learning_rate 0.000113955
2017-10-10T11:49:14.432853: step 1435, loss 0.304332, acc 0.875, learning_rate 0.000113898
2017-10-10T11:49:14.688517: step 1436, loss 0.0786768, acc 1, learning_rate 0.000113842
2017-10-10T11:49:14.912293: step 1437, loss 0.164965, acc 0.96875, learning_rate 0.000113785
2017-10-10T11:49:15.147398: step 1438, loss 0.202141, acc 0.953125, learning_rate 0.000113729
2017-10-10T11:49:15.397207: step 1439, loss 0.297377, acc 0.90625, learning_rate 0.000113673
2017-10-10T11:49:15.632312: step 1440, loss 0.175918, acc 0.9375, learning_rate 0.000113617

Evaluation:
2017-10-10T11:49:16.175197: step 1440, loss 0.229865, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1440

2017-10-10T11:49:17.494007: step 1441, loss 0.171048, acc 0.9375, learning_rate 0.000113561
2017-10-10T11:49:17.669029: step 1442, loss 0.149447, acc 0.921875, learning_rate 0.000113506
2017-10-10T11:49:17.921157: step 1443, loss 0.191898, acc 0.953125, learning_rate 0.000113451
2017-10-10T11:49:18.042460: step 1444, loss 0.184077, acc 0.9375, learning_rate 0.000113396
2017-10-10T11:49:18.161703: step 1445, loss 0.153352, acc 0.953125, learning_rate 0.000113341
2017-10-10T11:49:18.291345: step 1446, loss 0.156333, acc 0.921875, learning_rate 0.000113287
2017-10-10T11:49:18.444998: step 1447, loss 0.184102, acc 0.9375, learning_rate 0.000113233
2017-10-10T11:49:18.664189: step 1448, loss 0.133863, acc 0.953125, learning_rate 0.000113179
2017-10-10T11:49:18.848844: step 1449, loss 0.075632, acc 1, learning_rate 0.000113125
2017-10-10T11:49:19.092851: step 1450, loss 0.191479, acc 0.90625, learning_rate 0.000113071
2017-10-10T11:49:19.313368: step 1451, loss 0.0934102, acc 0.984375, learning_rate 0.000113018
2017-10-10T11:49:19.516990: step 1452, loss 0.193203, acc 0.9375, learning_rate 0.000112965
2017-10-10T11:49:19.771399: step 1453, loss 0.136236, acc 0.953125, learning_rate 0.000112912
2017-10-10T11:49:19.976836: step 1454, loss 0.182494, acc 0.953125, learning_rate 0.000112859
2017-10-10T11:49:20.207713: step 1455, loss 0.189353, acc 0.9375, learning_rate 0.000112807
2017-10-10T11:49:20.480433: step 1456, loss 0.096363, acc 0.984375, learning_rate 0.000112754
2017-10-10T11:49:20.732816: step 1457, loss 0.208465, acc 0.921875, learning_rate 0.000112702
2017-10-10T11:49:21.008253: step 1458, loss 0.11778, acc 0.96875, learning_rate 0.000112651
2017-10-10T11:49:21.244840: step 1459, loss 0.220021, acc 0.90625, learning_rate 0.000112599
2017-10-10T11:49:21.519094: step 1460, loss 0.214275, acc 0.890625, learning_rate 0.000112547
2017-10-10T11:49:21.764654: step 1461, loss 0.29636, acc 0.859375, learning_rate 0.000112496
2017-10-10T11:49:22.016896: step 1462, loss 0.0814784, acc 0.96875, learning_rate 0.000112445
2017-10-10T11:49:22.253645: step 1463, loss 0.0805844, acc 1, learning_rate 0.000112394
2017-10-10T11:49:22.480207: step 1464, loss 0.232395, acc 0.859375, learning_rate 0.000112344
2017-10-10T11:49:22.722473: step 1465, loss 0.1927, acc 0.9375, learning_rate 0.000112293
2017-10-10T11:49:22.931613: step 1466, loss 0.14699, acc 0.9375, learning_rate 0.000112243
2017-10-10T11:49:23.211384: step 1467, loss 0.233494, acc 0.9375, learning_rate 0.000112193
2017-10-10T11:49:23.454942: step 1468, loss 0.0446159, acc 1, learning_rate 0.000112144
2017-10-10T11:49:23.691121: step 1469, loss 0.103911, acc 0.96875, learning_rate 0.000112094
2017-10-10T11:49:23.911350: step 1470, loss 0.101394, acc 0.941176, learning_rate 0.000112045
2017-10-10T11:49:24.169416: step 1471, loss 0.18103, acc 0.9375, learning_rate 0.000111995
2017-10-10T11:49:24.428365: step 1472, loss 0.287306, acc 0.921875, learning_rate 0.000111946
2017-10-10T11:49:24.700848: step 1473, loss 0.154541, acc 0.9375, learning_rate 0.000111898
2017-10-10T11:49:24.920268: step 1474, loss 0.22266, acc 0.921875, learning_rate 0.000111849
2017-10-10T11:49:25.176921: step 1475, loss 0.12148, acc 0.96875, learning_rate 0.000111801
2017-10-10T11:49:25.412394: step 1476, loss 0.0756957, acc 0.984375, learning_rate 0.000111753
2017-10-10T11:49:25.641174: step 1477, loss 0.258761, acc 0.875, learning_rate 0.000111705
2017-10-10T11:49:25.899534: step 1478, loss 0.215036, acc 0.921875, learning_rate 0.000111657
2017-10-10T11:49:26.167750: step 1479, loss 0.107049, acc 0.96875, learning_rate 0.000111609
2017-10-10T11:49:26.406388: step 1480, loss 0.156704, acc 0.921875, learning_rate 0.000111562

Evaluation:
2017-10-10T11:49:26.817011: step 1480, loss 0.228087, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1480

2017-10-10T11:49:27.918184: step 1481, loss 0.119363, acc 0.96875, learning_rate 0.000111515
2017-10-10T11:49:28.228877: step 1482, loss 0.133082, acc 0.953125, learning_rate 0.000111468
2017-10-10T11:49:28.457679: step 1483, loss 0.1658, acc 0.953125, learning_rate 0.000111421
2017-10-10T11:49:28.621998: step 1484, loss 0.115867, acc 0.96875, learning_rate 0.000111374
2017-10-10T11:49:28.792317: step 1485, loss 0.190142, acc 0.953125, learning_rate 0.000111328
2017-10-10T11:49:28.955215: step 1486, loss 0.142871, acc 0.953125, learning_rate 0.000111282
2017-10-10T11:49:29.126023: step 1487, loss 0.122551, acc 0.984375, learning_rate 0.000111236
2017-10-10T11:49:29.280707: step 1488, loss 0.105547, acc 0.953125, learning_rate 0.00011119
2017-10-10T11:49:29.456854: step 1489, loss 0.170814, acc 0.921875, learning_rate 0.000111144
2017-10-10T11:49:29.645009: step 1490, loss 0.221909, acc 0.90625, learning_rate 0.000111099
2017-10-10T11:49:29.880906: step 1491, loss 0.191543, acc 0.9375, learning_rate 0.000111053
2017-10-10T11:49:30.099240: step 1492, loss 0.119557, acc 0.96875, learning_rate 0.000111008
2017-10-10T11:49:30.340885: step 1493, loss 0.162031, acc 0.953125, learning_rate 0.000110963
2017-10-10T11:49:30.589474: step 1494, loss 0.188772, acc 0.96875, learning_rate 0.000110918
2017-10-10T11:49:30.825270: step 1495, loss 0.142198, acc 0.953125, learning_rate 0.000110874
2017-10-10T11:49:31.060217: step 1496, loss 0.183613, acc 0.90625, learning_rate 0.00011083
2017-10-10T11:49:31.316427: step 1497, loss 0.146866, acc 0.96875, learning_rate 0.000110785
2017-10-10T11:49:31.566288: step 1498, loss 0.108857, acc 0.953125, learning_rate 0.000110741
2017-10-10T11:49:31.833648: step 1499, loss 0.118511, acc 0.953125, learning_rate 0.000110697
2017-10-10T11:49:32.084885: step 1500, loss 0.130827, acc 0.96875, learning_rate 0.000110654
2017-10-10T11:49:32.359737: step 1501, loss 0.101773, acc 0.984375, learning_rate 0.00011061
2017-10-10T11:49:32.605843: step 1502, loss 0.086686, acc 0.96875, learning_rate 0.000110567
2017-10-10T11:49:32.837496: step 1503, loss 0.127974, acc 0.96875, learning_rate 0.000110524
2017-10-10T11:49:33.072900: step 1504, loss 0.0996181, acc 0.953125, learning_rate 0.000110481
2017-10-10T11:49:33.332150: step 1505, loss 0.0891316, acc 0.953125, learning_rate 0.000110438
2017-10-10T11:49:33.578028: step 1506, loss 0.0948599, acc 1, learning_rate 0.000110396
2017-10-10T11:49:33.831749: step 1507, loss 0.0893021, acc 0.96875, learning_rate 0.000110353
2017-10-10T11:49:34.104415: step 1508, loss 0.13187, acc 0.953125, learning_rate 0.000110311
2017-10-10T11:49:34.365368: step 1509, loss 0.108441, acc 0.96875, learning_rate 0.000110269
2017-10-10T11:49:34.564765: step 1510, loss 0.133497, acc 0.953125, learning_rate 0.000110227
2017-10-10T11:49:34.755228: step 1511, loss 0.110122, acc 0.953125, learning_rate 0.000110185
2017-10-10T11:49:34.955123: step 1512, loss 0.130294, acc 0.9375, learning_rate 0.000110144
2017-10-10T11:49:35.131532: step 1513, loss 0.0791551, acc 1, learning_rate 0.000110102
2017-10-10T11:49:35.333111: step 1514, loss 0.156002, acc 0.9375, learning_rate 0.000110061
2017-10-10T11:49:35.550329: step 1515, loss 0.138297, acc 0.96875, learning_rate 0.00011002
2017-10-10T11:49:35.765440: step 1516, loss 0.188454, acc 0.953125, learning_rate 0.000109979
2017-10-10T11:49:35.988277: step 1517, loss 0.15953, acc 0.953125, learning_rate 0.000109938
2017-10-10T11:49:36.205268: step 1518, loss 0.111533, acc 0.96875, learning_rate 0.000109898
2017-10-10T11:49:36.464591: step 1519, loss 0.217978, acc 0.890625, learning_rate 0.000109857
2017-10-10T11:49:36.700166: step 1520, loss 0.137988, acc 0.953125, learning_rate 0.000109817

Evaluation:
2017-10-10T11:49:37.165758: step 1520, loss 0.228235, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1520

2017-10-10T11:49:38.471273: step 1521, loss 0.218126, acc 0.953125, learning_rate 0.000109777
2017-10-10T11:49:38.734240: step 1522, loss 0.0930783, acc 0.96875, learning_rate 0.000109737
2017-10-10T11:49:38.987114: step 1523, loss 0.12436, acc 0.96875, learning_rate 0.000109697
2017-10-10T11:49:39.234276: step 1524, loss 0.177344, acc 0.96875, learning_rate 0.000109658
2017-10-10T11:49:39.460835: step 1525, loss 0.227941, acc 0.96875, learning_rate 0.000109618
2017-10-10T11:49:39.785269: step 1526, loss 0.0864151, acc 0.984375, learning_rate 0.000109579
2017-10-10T11:49:39.979153: step 1527, loss 0.108963, acc 0.96875, learning_rate 0.00010954
2017-10-10T11:49:40.431443: step 1528, loss 0.110401, acc 0.96875, learning_rate 0.000109501
2017-10-10T11:49:40.689069: step 1529, loss 0.127863, acc 0.953125, learning_rate 0.000109462
2017-10-10T11:49:40.904538: step 1530, loss 0.246683, acc 0.890625, learning_rate 0.000109424
2017-10-10T11:49:41.136779: step 1531, loss 0.135515, acc 0.96875, learning_rate 0.000109385
2017-10-10T11:49:41.373022: step 1532, loss 0.0834532, acc 0.96875, learning_rate 0.000109347
2017-10-10T11:49:41.571240: step 1533, loss 0.197456, acc 0.9375, learning_rate 0.000109309
2017-10-10T11:49:41.765181: step 1534, loss 0.222855, acc 0.921875, learning_rate 0.000109271
2017-10-10T11:49:41.962692: step 1535, loss 0.139071, acc 0.9375, learning_rate 0.000109233
2017-10-10T11:49:42.201175: step 1536, loss 0.180405, acc 0.953125, learning_rate 0.000109195
2017-10-10T11:49:42.453511: step 1537, loss 0.19482, acc 0.921875, learning_rate 0.000109158
2017-10-10T11:49:42.738773: step 1538, loss 0.117231, acc 0.96875, learning_rate 0.00010912
2017-10-10T11:49:42.928874: step 1539, loss 0.171354, acc 0.9375, learning_rate 0.000109083
2017-10-10T11:49:43.102918: step 1540, loss 0.0405388, acc 1, learning_rate 0.000109046
2017-10-10T11:49:43.263598: step 1541, loss 0.204737, acc 0.9375, learning_rate 0.000109009
2017-10-10T11:49:43.451117: step 1542, loss 0.209714, acc 0.921875, learning_rate 0.000108972
2017-10-10T11:49:43.645956: step 1543, loss 0.175435, acc 0.9375, learning_rate 0.000108936
2017-10-10T11:49:43.841228: step 1544, loss 0.135228, acc 0.96875, learning_rate 0.000108899
2017-10-10T11:49:44.086429: step 1545, loss 0.0976609, acc 0.984375, learning_rate 0.000108863
2017-10-10T11:49:44.328985: step 1546, loss 0.189753, acc 0.96875, learning_rate 0.000108827
2017-10-10T11:49:44.576841: step 1547, loss 0.143869, acc 0.953125, learning_rate 0.000108791
2017-10-10T11:49:44.802977: step 1548, loss 0.114265, acc 0.96875, learning_rate 0.000108755
2017-10-10T11:49:45.029224: step 1549, loss 0.0772913, acc 0.984375, learning_rate 0.000108719
2017-10-10T11:49:45.256869: step 1550, loss 0.108248, acc 0.984375, learning_rate 0.000108683
2017-10-10T11:49:45.897691: step 1551, loss 0.193, acc 0.9375, learning_rate 0.000108648
2017-10-10T11:49:46.163865: step 1552, loss 0.235743, acc 0.921875, learning_rate 0.000108613
2017-10-10T11:49:46.410267: step 1553, loss 0.122499, acc 0.953125, learning_rate 0.000108577
2017-10-10T11:49:46.655880: step 1554, loss 0.245587, acc 0.90625, learning_rate 0.000108542
2017-10-10T11:49:46.884454: step 1555, loss 0.192873, acc 0.90625, learning_rate 0.000108508
2017-10-10T11:49:47.133094: step 1556, loss 0.0869043, acc 0.984375, learning_rate 0.000108473
2017-10-10T11:49:47.329090: step 1557, loss 0.0664356, acc 0.984375, learning_rate 0.000108438
2017-10-10T11:49:47.540566: step 1558, loss 0.120366, acc 0.9375, learning_rate 0.000108404
2017-10-10T11:49:47.774951: step 1559, loss 0.118562, acc 0.9375, learning_rate 0.00010837
2017-10-10T11:49:47.987656: step 1560, loss 0.0568391, acc 0.984375, learning_rate 0.000108335

Evaluation:
2017-10-10T11:49:48.476887: step 1560, loss 0.230147, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1560

2017-10-10T11:49:49.483066: step 1561, loss 0.0375049, acc 1, learning_rate 0.000108301
2017-10-10T11:49:49.693639: step 1562, loss 0.0974185, acc 0.96875, learning_rate 0.000108267
2017-10-10T11:49:49.895452: step 1563, loss 0.154301, acc 0.96875, learning_rate 0.000108234
2017-10-10T11:49:50.130661: step 1564, loss 0.271542, acc 0.921875, learning_rate 0.0001082
2017-10-10T11:49:50.365938: step 1565, loss 0.0910194, acc 0.96875, learning_rate 0.000108167
2017-10-10T11:49:50.597108: step 1566, loss 0.0887151, acc 0.96875, learning_rate 0.000108133
2017-10-10T11:49:50.961027: step 1567, loss 0.176825, acc 0.953125, learning_rate 0.0001081
2017-10-10T11:49:51.168341: step 1568, loss 0.149625, acc 0.960784, learning_rate 0.000108067
2017-10-10T11:49:51.294536: step 1569, loss 0.0542937, acc 0.984375, learning_rate 0.000108034
2017-10-10T11:49:51.419717: step 1570, loss 0.197254, acc 0.921875, learning_rate 0.000108001
2017-10-10T11:49:51.554635: step 1571, loss 0.211807, acc 0.953125, learning_rate 0.000107969
2017-10-10T11:49:51.686579: step 1572, loss 0.14122, acc 0.96875, learning_rate 0.000107936
2017-10-10T11:49:51.803312: step 1573, loss 0.193124, acc 0.9375, learning_rate 0.000107904
2017-10-10T11:49:52.000659: step 1574, loss 0.100994, acc 0.96875, learning_rate 0.000107871
2017-10-10T11:49:52.219350: step 1575, loss 0.121989, acc 0.953125, learning_rate 0.000107839
2017-10-10T11:49:52.431750: step 1576, loss 0.165805, acc 0.953125, learning_rate 0.000107807
2017-10-10T11:49:52.636525: step 1577, loss 0.189749, acc 0.921875, learning_rate 0.000107775
2017-10-10T11:49:52.888774: step 1578, loss 0.145962, acc 0.96875, learning_rate 0.000107744
2017-10-10T11:49:53.106341: step 1579, loss 0.17705, acc 0.9375, learning_rate 0.000107712
2017-10-10T11:49:53.332838: step 1580, loss 0.151263, acc 0.90625, learning_rate 0.000107681
2017-10-10T11:49:53.569702: step 1581, loss 0.0767806, acc 0.96875, learning_rate 0.000107649
2017-10-10T11:49:53.829121: step 1582, loss 0.117756, acc 0.953125, learning_rate 0.000107618
2017-10-10T11:49:54.049024: step 1583, loss 0.155737, acc 0.953125, learning_rate 0.000107587
2017-10-10T11:49:54.286224: step 1584, loss 0.0546037, acc 1, learning_rate 0.000107556
2017-10-10T11:49:54.546973: step 1585, loss 0.187002, acc 0.921875, learning_rate 0.000107525
2017-10-10T11:49:54.774296: step 1586, loss 0.112797, acc 0.984375, learning_rate 0.000107494
2017-10-10T11:49:55.011588: step 1587, loss 0.139501, acc 0.96875, learning_rate 0.000107464
2017-10-10T11:49:55.270424: step 1588, loss 0.082482, acc 0.984375, learning_rate 0.000107433
2017-10-10T11:49:55.502202: step 1589, loss 0.265976, acc 0.890625, learning_rate 0.000107403
2017-10-10T11:49:55.754002: step 1590, loss 0.0805268, acc 0.96875, learning_rate 0.000107373
2017-10-10T11:49:55.980160: step 1591, loss 0.247841, acc 0.921875, learning_rate 0.000107343
2017-10-10T11:49:56.201986: step 1592, loss 0.278776, acc 0.90625, learning_rate 0.000107313
2017-10-10T11:49:56.399005: step 1593, loss 0.122377, acc 0.96875, learning_rate 0.000107283
2017-10-10T11:49:56.624871: step 1594, loss 0.147801, acc 0.9375, learning_rate 0.000107253
2017-10-10T11:49:56.849602: step 1595, loss 0.0866417, acc 0.96875, learning_rate 0.000107224
2017-10-10T11:49:57.112866: step 1596, loss 0.187897, acc 0.921875, learning_rate 0.000107194
2017-10-10T11:49:57.351774: step 1597, loss 0.0977607, acc 0.984375, learning_rate 0.000107165
2017-10-10T11:49:57.597051: step 1598, loss 0.102982, acc 0.96875, learning_rate 0.000107136
2017-10-10T11:49:57.848845: step 1599, loss 0.252389, acc 0.875, learning_rate 0.000107106
2017-10-10T11:49:58.079104: step 1600, loss 0.193445, acc 0.9375, learning_rate 0.000107077

Evaluation:
2017-10-10T11:49:58.522087: step 1600, loss 0.228303, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1600

2017-10-10T11:49:59.656751: step 1601, loss 0.216079, acc 0.953125, learning_rate 0.000107048
2017-10-10T11:49:59.851312: step 1602, loss 0.188383, acc 0.953125, learning_rate 0.00010702
2017-10-10T11:50:00.054986: step 1603, loss 0.1336, acc 0.921875, learning_rate 0.000106991
2017-10-10T11:50:00.282592: step 1604, loss 0.159468, acc 0.921875, learning_rate 0.000106963
2017-10-10T11:50:00.514896: step 1605, loss 0.107244, acc 0.984375, learning_rate 0.000106934
2017-10-10T11:50:00.749154: step 1606, loss 0.155311, acc 0.921875, learning_rate 0.000106906
2017-10-10T11:50:00.983287: step 1607, loss 0.199558, acc 0.921875, learning_rate 0.000106878
2017-10-10T11:50:01.166611: step 1608, loss 0.118366, acc 0.96875, learning_rate 0.00010685
2017-10-10T11:50:01.376862: step 1609, loss 0.205791, acc 0.90625, learning_rate 0.000106822
2017-10-10T11:50:01.603054: step 1610, loss 0.0971396, acc 0.96875, learning_rate 0.000106794
2017-10-10T11:50:01.928961: step 1611, loss 0.113489, acc 0.984375, learning_rate 0.000106766
2017-10-10T11:50:02.211636: step 1612, loss 0.129196, acc 0.9375, learning_rate 0.000106738
2017-10-10T11:50:02.389144: step 1613, loss 0.0779756, acc 0.96875, learning_rate 0.000106711
2017-10-10T11:50:02.556850: step 1614, loss 0.109842, acc 0.953125, learning_rate 0.000106684
2017-10-10T11:50:02.762519: step 1615, loss 0.121118, acc 0.953125, learning_rate 0.000106656
2017-10-10T11:50:02.942579: step 1616, loss 0.118263, acc 0.96875, learning_rate 0.000106629
2017-10-10T11:50:03.105196: step 1617, loss 0.0710362, acc 0.984375, learning_rate 0.000106602
2017-10-10T11:50:03.312980: step 1618, loss 0.203278, acc 0.90625, learning_rate 0.000106575
2017-10-10T11:50:03.516888: step 1619, loss 0.0937399, acc 0.953125, learning_rate 0.000106548
2017-10-10T11:50:03.740064: step 1620, loss 0.140676, acc 0.953125, learning_rate 0.000106521
2017-10-10T11:50:03.967066: step 1621, loss 0.165493, acc 0.9375, learning_rate 0.000106495
2017-10-10T11:50:04.212477: step 1622, loss 0.243606, acc 0.90625, learning_rate 0.000106468
2017-10-10T11:50:04.449020: step 1623, loss 0.0858782, acc 0.96875, learning_rate 0.000106442
2017-10-10T11:50:04.664806: step 1624, loss 0.244688, acc 0.890625, learning_rate 0.000106416
2017-10-10T11:50:04.904099: step 1625, loss 0.148064, acc 0.9375, learning_rate 0.000106389
2017-10-10T11:50:05.124844: step 1626, loss 0.0769802, acc 0.984375, learning_rate 0.000106363
2017-10-10T11:50:05.356836: step 1627, loss 0.193591, acc 0.921875, learning_rate 0.000106337
2017-10-10T11:50:05.580830: step 1628, loss 0.214296, acc 0.9375, learning_rate 0.000106312
2017-10-10T11:50:05.809114: step 1629, loss 0.197799, acc 0.953125, learning_rate 0.000106286
2017-10-10T11:50:06.028299: step 1630, loss 0.0681158, acc 1, learning_rate 0.00010626
2017-10-10T11:50:06.256876: step 1631, loss 0.194763, acc 0.9375, learning_rate 0.000106235
2017-10-10T11:50:06.497585: step 1632, loss 0.18214, acc 0.921875, learning_rate 0.000106209
2017-10-10T11:50:06.718571: step 1633, loss 0.227164, acc 0.90625, learning_rate 0.000106184
2017-10-10T11:50:06.962295: step 1634, loss 0.111774, acc 0.96875, learning_rate 0.000106159
2017-10-10T11:50:07.190458: step 1635, loss 0.132385, acc 0.984375, learning_rate 0.000106133
2017-10-10T11:50:07.524992: step 1636, loss 0.0707052, acc 0.96875, learning_rate 0.000106108
2017-10-10T11:50:07.707848: step 1637, loss 0.115027, acc 0.9375, learning_rate 0.000106083
2017-10-10T11:50:07.904868: step 1638, loss 0.143874, acc 0.9375, learning_rate 0.000106059
2017-10-10T11:50:08.102605: step 1639, loss 0.126981, acc 0.9375, learning_rate 0.000106034
2017-10-10T11:50:08.312707: step 1640, loss 0.15747, acc 0.953125, learning_rate 0.000106009

Evaluation:
2017-10-10T11:50:08.759733: step 1640, loss 0.227728, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1640

2017-10-10T11:50:10.061288: step 1641, loss 0.126261, acc 0.96875, learning_rate 0.000105985
2017-10-10T11:50:10.302474: step 1642, loss 0.159786, acc 0.96875, learning_rate 0.00010596
2017-10-10T11:50:10.568908: step 1643, loss 0.167124, acc 0.9375, learning_rate 0.000105936
2017-10-10T11:50:10.812907: step 1644, loss 0.106619, acc 0.984375, learning_rate 0.000105912
2017-10-10T11:50:11.076072: step 1645, loss 0.165324, acc 0.90625, learning_rate 0.000105888
2017-10-10T11:50:11.299850: step 1646, loss 0.205469, acc 0.921875, learning_rate 0.000105864
2017-10-10T11:50:11.540608: step 1647, loss 0.0788507, acc 0.96875, learning_rate 0.00010584
2017-10-10T11:50:11.788846: step 1648, loss 0.19248, acc 0.9375, learning_rate 0.000105816
2017-10-10T11:50:12.027818: step 1649, loss 0.152481, acc 0.953125, learning_rate 0.000105792
2017-10-10T11:50:12.255119: step 1650, loss 0.125713, acc 0.953125, learning_rate 0.000105768
2017-10-10T11:50:12.496828: step 1651, loss 0.0977904, acc 0.953125, learning_rate 0.000105745
2017-10-10T11:50:12.720857: step 1652, loss 0.12591, acc 0.953125, learning_rate 0.000105721
2017-10-10T11:50:12.964922: step 1653, loss 0.0858147, acc 0.96875, learning_rate 0.000105698
2017-10-10T11:50:13.263997: step 1654, loss 0.135136, acc 0.96875, learning_rate 0.000105675
2017-10-10T11:50:13.446174: step 1655, loss 0.150646, acc 0.9375, learning_rate 0.000105652
2017-10-10T11:50:13.596761: step 1656, loss 0.10165, acc 0.953125, learning_rate 0.000105629
2017-10-10T11:50:13.752857: step 1657, loss 0.102834, acc 0.96875, learning_rate 0.000105606
2017-10-10T11:50:13.950384: step 1658, loss 0.167709, acc 0.953125, learning_rate 0.000105583
2017-10-10T11:50:14.119544: step 1659, loss 0.101221, acc 0.984375, learning_rate 0.00010556
2017-10-10T11:50:14.369236: step 1660, loss 0.158214, acc 0.921875, learning_rate 0.000105537
2017-10-10T11:50:14.596901: step 1661, loss 0.172582, acc 0.9375, learning_rate 0.000105515
2017-10-10T11:50:14.812814: step 1662, loss 0.0261254, acc 1, learning_rate 0.000105492
2017-10-10T11:50:15.068690: step 1663, loss 0.173047, acc 0.9375, learning_rate 0.00010547
2017-10-10T11:50:15.313098: step 1664, loss 0.177173, acc 0.953125, learning_rate 0.000105447
2017-10-10T11:50:15.544227: step 1665, loss 0.0517822, acc 0.984375, learning_rate 0.000105425
2017-10-10T11:50:15.740949: step 1666, loss 0.15362, acc 0.941176, learning_rate 0.000105403
2017-10-10T11:50:16.035092: step 1667, loss 0.0681834, acc 0.984375, learning_rate 0.000105381
2017-10-10T11:50:16.241987: step 1668, loss 0.244734, acc 0.890625, learning_rate 0.000105359
2017-10-10T11:50:16.442061: step 1669, loss 0.132851, acc 0.96875, learning_rate 0.000105337
2017-10-10T11:50:16.640842: step 1670, loss 0.0857009, acc 0.984375, learning_rate 0.000105315
2017-10-10T11:50:16.864426: step 1671, loss 0.167052, acc 0.96875, learning_rate 0.000105294
2017-10-10T11:50:17.083949: step 1672, loss 0.154189, acc 0.9375, learning_rate 0.000105272
2017-10-10T11:50:17.300878: step 1673, loss 0.145043, acc 0.953125, learning_rate 0.000105251
2017-10-10T11:50:17.556187: step 1674, loss 0.135412, acc 0.96875, learning_rate 0.000105229
2017-10-10T11:50:17.774583: step 1675, loss 0.099325, acc 0.96875, learning_rate 0.000105208
2017-10-10T11:50:17.989357: step 1676, loss 0.101381, acc 0.96875, learning_rate 0.000105186
2017-10-10T11:50:18.174846: step 1677, loss 0.0890558, acc 0.984375, learning_rate 0.000105165
2017-10-10T11:50:18.400453: step 1678, loss 0.0773027, acc 0.96875, learning_rate 0.000105144
2017-10-10T11:50:18.639714: step 1679, loss 0.0667848, acc 0.984375, learning_rate 0.000105123
2017-10-10T11:50:18.881535: step 1680, loss 0.105825, acc 0.984375, learning_rate 0.000105102

Evaluation:
2017-10-10T11:50:19.408883: step 1680, loss 0.230547, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1680

2017-10-10T11:50:20.436816: step 1681, loss 0.160784, acc 0.921875, learning_rate 0.000105081
2017-10-10T11:50:20.648899: step 1682, loss 0.117598, acc 0.96875, learning_rate 0.000105061
2017-10-10T11:50:20.904868: step 1683, loss 0.139394, acc 0.953125, learning_rate 0.00010504
2017-10-10T11:50:21.120750: step 1684, loss 0.156941, acc 0.921875, learning_rate 0.00010502
2017-10-10T11:50:21.327938: step 1685, loss 0.100731, acc 1, learning_rate 0.000104999
2017-10-10T11:50:21.560098: step 1686, loss 0.156349, acc 0.9375, learning_rate 0.000104979
2017-10-10T11:50:21.808631: step 1687, loss 0.1246, acc 0.984375, learning_rate 0.000104958
2017-10-10T11:50:22.014088: step 1688, loss 0.113185, acc 0.96875, learning_rate 0.000104938
2017-10-10T11:50:22.256847: step 1689, loss 0.157852, acc 0.9375, learning_rate 0.000104918
2017-10-10T11:50:22.463344: step 1690, loss 0.131465, acc 0.9375, learning_rate 0.000104898
2017-10-10T11:50:22.741700: step 1691, loss 0.0864549, acc 0.96875, learning_rate 0.000104878
2017-10-10T11:50:22.959628: step 1692, loss 0.111468, acc 0.953125, learning_rate 0.000104858
2017-10-10T11:50:23.192498: step 1693, loss 0.226336, acc 0.953125, learning_rate 0.000104838
2017-10-10T11:50:23.426546: step 1694, loss 0.125942, acc 0.953125, learning_rate 0.000104818
2017-10-10T11:50:23.647903: step 1695, loss 0.144218, acc 0.9375, learning_rate 0.000104799
2017-10-10T11:50:23.920410: step 1696, loss 0.113435, acc 0.953125, learning_rate 0.000104779
2017-10-10T11:50:24.213114: step 1697, loss 0.0884072, acc 0.984375, learning_rate 0.00010476
2017-10-10T11:50:24.445849: step 1698, loss 0.058818, acc 0.984375, learning_rate 0.00010474
2017-10-10T11:50:24.588413: step 1699, loss 0.0178917, acc 1, learning_rate 0.000104721
2017-10-10T11:50:24.721609: step 1700, loss 0.0787278, acc 0.984375, learning_rate 0.000104702
2017-10-10T11:50:24.847669: step 1701, loss 0.149992, acc 0.953125, learning_rate 0.000104682
2017-10-10T11:50:24.980626: step 1702, loss 0.176223, acc 0.953125, learning_rate 0.000104663
2017-10-10T11:50:25.111758: step 1703, loss 0.0519459, acc 1, learning_rate 0.000104644
2017-10-10T11:50:25.272885: step 1704, loss 0.134671, acc 0.9375, learning_rate 0.000104625
2017-10-10T11:50:25.518274: step 1705, loss 0.227257, acc 0.9375, learning_rate 0.000104606
2017-10-10T11:50:25.776825: step 1706, loss 0.134958, acc 0.953125, learning_rate 0.000104588
2017-10-10T11:50:26.022716: step 1707, loss 0.15976, acc 0.9375, learning_rate 0.000104569
2017-10-10T11:50:26.267734: step 1708, loss 0.176954, acc 0.921875, learning_rate 0.00010455
2017-10-10T11:50:26.498207: step 1709, loss 0.0856438, acc 1, learning_rate 0.000104532
2017-10-10T11:50:26.732876: step 1710, loss 0.186585, acc 0.9375, learning_rate 0.000104513
2017-10-10T11:50:26.940854: step 1711, loss 0.123025, acc 0.96875, learning_rate 0.000104495
2017-10-10T11:50:27.141157: step 1712, loss 0.162861, acc 0.9375, learning_rate 0.000104476
2017-10-10T11:50:27.379613: step 1713, loss 0.101861, acc 0.96875, learning_rate 0.000104458
2017-10-10T11:50:27.613049: step 1714, loss 0.0391866, acc 1, learning_rate 0.00010444
2017-10-10T11:50:27.833976: step 1715, loss 0.182668, acc 0.953125, learning_rate 0.000104422
2017-10-10T11:50:28.064160: step 1716, loss 0.107464, acc 0.984375, learning_rate 0.000104404
2017-10-10T11:50:28.334050: step 1717, loss 0.141965, acc 0.953125, learning_rate 0.000104386
2017-10-10T11:50:28.586047: step 1718, loss 0.170853, acc 0.953125, learning_rate 0.000104368
2017-10-10T11:50:28.849192: step 1719, loss 0.216146, acc 0.921875, learning_rate 0.00010435
2017-10-10T11:50:29.093059: step 1720, loss 0.117882, acc 0.96875, learning_rate 0.000104332

Evaluation:
2017-10-10T11:50:29.638195: step 1720, loss 0.232466, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1720

2017-10-10T11:50:30.712929: step 1721, loss 0.2375, acc 0.921875, learning_rate 0.000104315
2017-10-10T11:50:30.944899: step 1722, loss 0.143735, acc 0.96875, learning_rate 0.000104297
2017-10-10T11:50:31.187935: step 1723, loss 0.125587, acc 0.984375, learning_rate 0.000104279
2017-10-10T11:50:31.432861: step 1724, loss 0.102161, acc 0.9375, learning_rate 0.000104262
2017-10-10T11:50:31.660763: step 1725, loss 0.239812, acc 0.9375, learning_rate 0.000104245
2017-10-10T11:50:31.871781: step 1726, loss 0.0699406, acc 0.984375, learning_rate 0.000104227
2017-10-10T11:50:32.127432: step 1727, loss 0.123859, acc 0.953125, learning_rate 0.00010421
2017-10-10T11:50:32.347737: step 1728, loss 0.0856387, acc 0.984375, learning_rate 0.000104193
2017-10-10T11:50:32.641005: step 1729, loss 0.257648, acc 0.90625, learning_rate 0.000104176
2017-10-10T11:50:32.820206: step 1730, loss 0.112987, acc 0.953125, learning_rate 0.000104159
2017-10-10T11:50:32.987202: step 1731, loss 0.0660273, acc 0.984375, learning_rate 0.000104142
2017-10-10T11:50:33.142078: step 1732, loss 0.257251, acc 0.921875, learning_rate 0.000104125
2017-10-10T11:50:33.368833: step 1733, loss 0.0700059, acc 0.984375, learning_rate 0.000104108
2017-10-10T11:50:33.545212: step 1734, loss 0.259316, acc 0.90625, learning_rate 0.000104091
2017-10-10T11:50:33.724824: step 1735, loss 0.223078, acc 0.921875, learning_rate 0.000104074
2017-10-10T11:50:33.946643: step 1736, loss 0.116054, acc 0.953125, learning_rate 0.000104058
2017-10-10T11:50:34.173987: step 1737, loss 0.038958, acc 1, learning_rate 0.000104041
2017-10-10T11:50:34.420395: step 1738, loss 0.320233, acc 0.921875, learning_rate 0.000104025
2017-10-10T11:50:34.667965: step 1739, loss 0.170574, acc 0.953125, learning_rate 0.000104008
2017-10-10T11:50:34.858669: step 1740, loss 0.173109, acc 0.9375, learning_rate 0.000103992
2017-10-10T11:50:35.149122: step 1741, loss 0.0908586, acc 0.96875, learning_rate 0.000103976
2017-10-10T11:50:35.372867: step 1742, loss 0.187655, acc 0.921875, learning_rate 0.000103959
2017-10-10T11:50:35.568605: step 1743, loss 0.105693, acc 0.984375, learning_rate 0.000103943
2017-10-10T11:50:35.741220: step 1744, loss 0.199837, acc 0.921875, learning_rate 0.000103927
2017-10-10T11:50:35.927329: step 1745, loss 0.148047, acc 0.921875, learning_rate 0.000103911
2017-10-10T11:50:36.098723: step 1746, loss 0.0768168, acc 1, learning_rate 0.000103895
2017-10-10T11:50:36.280298: step 1747, loss 0.186096, acc 0.9375, learning_rate 0.000103879
2017-10-10T11:50:36.530410: step 1748, loss 0.0755126, acc 0.984375, learning_rate 0.000103863
2017-10-10T11:50:36.768003: step 1749, loss 0.0831382, acc 0.984375, learning_rate 0.000103848
2017-10-10T11:50:36.991814: step 1750, loss 0.161151, acc 0.953125, learning_rate 0.000103832
2017-10-10T11:50:37.237416: step 1751, loss 0.166252, acc 0.953125, learning_rate 0.000103816
2017-10-10T11:50:37.458673: step 1752, loss 0.10304, acc 0.953125, learning_rate 0.000103801
2017-10-10T11:50:37.665042: step 1753, loss 0.242004, acc 0.921875, learning_rate 0.000103785
2017-10-10T11:50:37.856523: step 1754, loss 0.0876877, acc 0.953125, learning_rate 0.00010377
2017-10-10T11:50:38.105827: step 1755, loss 0.09738, acc 0.984375, learning_rate 0.000103754
2017-10-10T11:50:38.317125: step 1756, loss 0.137799, acc 0.96875, learning_rate 0.000103739
2017-10-10T11:50:38.541204: step 1757, loss 0.14031, acc 0.9375, learning_rate 0.000103724
2017-10-10T11:50:38.779850: step 1758, loss 0.140261, acc 0.90625, learning_rate 0.000103709
2017-10-10T11:50:38.972955: step 1759, loss 0.114178, acc 0.96875, learning_rate 0.000103694
2017-10-10T11:50:39.208893: step 1760, loss 0.0790773, acc 0.984375, learning_rate 0.000103678

Evaluation:
2017-10-10T11:50:39.723800: step 1760, loss 0.227486, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1760

2017-10-10T11:50:40.985126: step 1761, loss 0.154443, acc 0.953125, learning_rate 0.000103663
2017-10-10T11:50:41.308369: step 1762, loss 0.0394187, acc 1, learning_rate 0.000103648
2017-10-10T11:50:41.492914: step 1763, loss 0.157667, acc 0.90625, learning_rate 0.000103634
2017-10-10T11:50:41.653029: step 1764, loss 0.0749942, acc 0.980392, learning_rate 0.000103619
2017-10-10T11:50:41.857513: step 1765, loss 0.197864, acc 0.921875, learning_rate 0.000103604
2017-10-10T11:50:42.019324: step 1766, loss 0.13957, acc 0.9375, learning_rate 0.000103589
2017-10-10T11:50:42.199221: step 1767, loss 0.18889, acc 0.953125, learning_rate 0.000103575
2017-10-10T11:50:42.401116: step 1768, loss 0.0601427, acc 0.984375, learning_rate 0.00010356
2017-10-10T11:50:42.646338: step 1769, loss 0.138298, acc 0.953125, learning_rate 0.000103545
2017-10-10T11:50:42.863760: step 1770, loss 0.0956488, acc 0.953125, learning_rate 0.000103531
2017-10-10T11:50:43.126570: step 1771, loss 0.12411, acc 0.953125, learning_rate 0.000103517
2017-10-10T11:50:43.360852: step 1772, loss 0.131363, acc 0.9375, learning_rate 0.000103502
2017-10-10T11:50:43.605919: step 1773, loss 0.0589403, acc 0.96875, learning_rate 0.000103488
2017-10-10T11:50:43.823719: step 1774, loss 0.11898, acc 0.921875, learning_rate 0.000103474
2017-10-10T11:50:44.057049: step 1775, loss 0.0604031, acc 0.984375, learning_rate 0.00010346
2017-10-10T11:50:44.312408: step 1776, loss 0.153645, acc 0.921875, learning_rate 0.000103445
2017-10-10T11:50:44.570293: step 1777, loss 0.0829374, acc 0.984375, learning_rate 0.000103431
2017-10-10T11:50:44.797891: step 1778, loss 0.191672, acc 0.9375, learning_rate 0.000103417
2017-10-10T11:50:45.051963: step 1779, loss 0.111276, acc 0.953125, learning_rate 0.000103403
2017-10-10T11:50:45.276703: step 1780, loss 0.177414, acc 0.90625, learning_rate 0.00010339
2017-10-10T11:50:45.504837: step 1781, loss 0.100276, acc 0.96875, learning_rate 0.000103376
2017-10-10T11:50:45.772923: step 1782, loss 0.0825359, acc 0.96875, learning_rate 0.000103362
2017-10-10T11:50:46.051244: step 1783, loss 0.121685, acc 0.984375, learning_rate 0.000103348
2017-10-10T11:50:46.254033: step 1784, loss 0.264944, acc 0.890625, learning_rate 0.000103335
2017-10-10T11:50:46.429240: step 1785, loss 0.0603893, acc 1, learning_rate 0.000103321
2017-10-10T11:50:46.580703: step 1786, loss 0.273197, acc 0.9375, learning_rate 0.000103307
2017-10-10T11:50:46.749313: step 1787, loss 0.0824105, acc 0.96875, learning_rate 0.000103294
2017-10-10T11:50:46.925760: step 1788, loss 0.110886, acc 0.96875, learning_rate 0.00010328
2017-10-10T11:50:47.163249: step 1789, loss 0.101162, acc 0.96875, learning_rate 0.000103267
2017-10-10T11:50:47.393196: step 1790, loss 0.135756, acc 0.953125, learning_rate 0.000103254
2017-10-10T11:50:47.636448: step 1791, loss 0.129138, acc 0.9375, learning_rate 0.00010324
2017-10-10T11:50:47.864341: step 1792, loss 0.185934, acc 0.9375, learning_rate 0.000103227
2017-10-10T11:50:48.099049: step 1793, loss 0.150371, acc 0.9375, learning_rate 0.000103214
2017-10-10T11:50:48.296943: step 1794, loss 0.152097, acc 0.953125, learning_rate 0.000103201
2017-10-10T11:50:48.528960: step 1795, loss 0.148901, acc 0.953125, learning_rate 0.000103188
2017-10-10T11:50:48.730213: step 1796, loss 0.175977, acc 0.921875, learning_rate 0.000103175
2017-10-10T11:50:48.964851: step 1797, loss 0.155187, acc 0.921875, learning_rate 0.000103162
2017-10-10T11:50:49.212862: step 1798, loss 0.19208, acc 0.921875, learning_rate 0.000103149
2017-10-10T11:50:49.476901: step 1799, loss 0.16262, acc 0.9375, learning_rate 0.000103136
2017-10-10T11:50:49.732421: step 1800, loss 0.124341, acc 0.96875, learning_rate 0.000103123

Evaluation:
2017-10-10T11:50:50.229251: step 1800, loss 0.227982, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1800

2017-10-10T11:50:51.137381: step 1801, loss 0.116338, acc 0.984375, learning_rate 0.000103111
2017-10-10T11:50:51.355451: step 1802, loss 0.233939, acc 0.9375, learning_rate 0.000103098
2017-10-10T11:50:51.594561: step 1803, loss 0.100173, acc 0.953125, learning_rate 0.000103085
2017-10-10T11:50:51.852502: step 1804, loss 0.115052, acc 0.953125, learning_rate 0.000103073
2017-10-10T11:50:52.093311: step 1805, loss 0.0770162, acc 0.984375, learning_rate 0.00010306
2017-10-10T11:50:52.317989: step 1806, loss 0.11976, acc 0.96875, learning_rate 0.000103048
2017-10-10T11:50:52.547977: step 1807, loss 0.162466, acc 0.921875, learning_rate 0.000103035
2017-10-10T11:50:52.734860: step 1808, loss 0.164072, acc 0.96875, learning_rate 0.000103023
2017-10-10T11:50:52.953651: step 1809, loss 0.0877271, acc 0.96875, learning_rate 0.00010301
2017-10-10T11:50:53.181664: step 1810, loss 0.181586, acc 0.953125, learning_rate 0.000102998
2017-10-10T11:50:53.428357: step 1811, loss 0.100421, acc 0.984375, learning_rate 0.000102986
2017-10-10T11:50:53.619352: step 1812, loss 0.129124, acc 0.984375, learning_rate 0.000102974
2017-10-10T11:50:53.884836: step 1813, loss 0.198387, acc 0.875, learning_rate 0.000102962
2017-10-10T11:50:54.108885: step 1814, loss 0.115156, acc 0.953125, learning_rate 0.000102949
2017-10-10T11:50:54.364883: step 1815, loss 0.205236, acc 0.921875, learning_rate 0.000102937
2017-10-10T11:50:54.605316: step 1816, loss 0.16648, acc 0.953125, learning_rate 0.000102925
2017-10-10T11:50:54.831940: step 1817, loss 0.144152, acc 0.953125, learning_rate 0.000102913
2017-10-10T11:50:55.085599: step 1818, loss 0.224975, acc 0.921875, learning_rate 0.000102902
2017-10-10T11:50:55.328209: step 1819, loss 0.255529, acc 0.90625, learning_rate 0.00010289
2017-10-10T11:50:55.578681: step 1820, loss 0.207324, acc 0.9375, learning_rate 0.000102878
2017-10-10T11:50:55.826201: step 1821, loss 0.174831, acc 0.953125, learning_rate 0.000102866
2017-10-10T11:50:56.055152: step 1822, loss 0.0859992, acc 0.953125, learning_rate 0.000102855
2017-10-10T11:50:56.274281: step 1823, loss 0.0842202, acc 0.953125, learning_rate 0.000102843
2017-10-10T11:50:56.507990: step 1824, loss 0.211523, acc 0.9375, learning_rate 0.000102831
2017-10-10T11:50:56.792789: step 1825, loss 0.233364, acc 0.875, learning_rate 0.00010282
2017-10-10T11:50:57.081176: step 1826, loss 0.19585, acc 0.90625, learning_rate 0.000102808
2017-10-10T11:50:57.268539: step 1827, loss 0.0938177, acc 0.984375, learning_rate 0.000102797
2017-10-10T11:50:57.435652: step 1828, loss 0.113261, acc 0.9375, learning_rate 0.000102785
2017-10-10T11:50:57.618449: step 1829, loss 0.0840182, acc 0.96875, learning_rate 0.000102774
2017-10-10T11:50:57.806871: step 1830, loss 0.114165, acc 0.953125, learning_rate 0.000102763
2017-10-10T11:50:58.038034: step 1831, loss 0.0703222, acc 0.984375, learning_rate 0.000102751
2017-10-10T11:50:58.208747: step 1832, loss 0.155967, acc 0.953125, learning_rate 0.00010274
2017-10-10T11:50:58.384772: step 1833, loss 0.100947, acc 0.96875, learning_rate 0.000102729
2017-10-10T11:50:58.571592: step 1834, loss 0.0825579, acc 0.984375, learning_rate 0.000102718
2017-10-10T11:50:58.736851: step 1835, loss 0.10125, acc 0.96875, learning_rate 0.000102707
2017-10-10T11:50:58.947204: step 1836, loss 0.163272, acc 0.96875, learning_rate 0.000102696
2017-10-10T11:50:59.132865: step 1837, loss 0.213919, acc 0.9375, learning_rate 0.000102685
2017-10-10T11:50:59.343792: step 1838, loss 0.148063, acc 0.953125, learning_rate 0.000102674
2017-10-10T11:50:59.587414: step 1839, loss 0.152299, acc 0.96875, learning_rate 0.000102663
2017-10-10T11:50:59.864853: step 1840, loss 0.151757, acc 0.953125, learning_rate 0.000102652

Evaluation:
2017-10-10T11:51:00.364813: step 1840, loss 0.22679, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1840

2017-10-10T11:51:01.510012: step 1841, loss 0.205298, acc 0.9375, learning_rate 0.000102641
2017-10-10T11:51:01.805561: step 1842, loss 0.107481, acc 0.953125, learning_rate 0.00010263
2017-10-10T11:51:02.028104: step 1843, loss 0.166236, acc 0.953125, learning_rate 0.00010262
2017-10-10T11:51:02.232883: step 1844, loss 0.062525, acc 1, learning_rate 0.000102609
2017-10-10T11:51:02.460902: step 1845, loss 0.220219, acc 0.921875, learning_rate 0.000102598
2017-10-10T11:51:02.695337: step 1846, loss 0.226213, acc 0.921875, learning_rate 0.000102588
2017-10-10T11:51:02.924859: step 1847, loss 0.177194, acc 0.9375, learning_rate 0.000102577
2017-10-10T11:51:03.133169: step 1848, loss 0.077852, acc 0.984375, learning_rate 0.000102567
2017-10-10T11:51:03.325774: step 1849, loss 0.0710312, acc 0.984375, learning_rate 0.000102556
2017-10-10T11:51:03.510412: step 1850, loss 0.139418, acc 0.953125, learning_rate 0.000102546
2017-10-10T11:51:03.772291: step 1851, loss 0.118498, acc 0.9375, learning_rate 0.000102535
2017-10-10T11:51:04.040257: step 1852, loss 0.161483, acc 0.953125, learning_rate 0.000102525
2017-10-10T11:51:04.274975: step 1853, loss 0.144442, acc 0.90625, learning_rate 0.000102515
2017-10-10T11:51:04.508857: step 1854, loss 0.148449, acc 0.96875, learning_rate 0.000102504
2017-10-10T11:51:04.744156: step 1855, loss 0.0812334, acc 0.984375, learning_rate 0.000102494
2017-10-10T11:51:04.964880: step 1856, loss 0.114677, acc 0.953125, learning_rate 0.000102484
2017-10-10T11:51:05.177046: step 1857, loss 0.118883, acc 0.984375, learning_rate 0.000102474
2017-10-10T11:51:05.388185: step 1858, loss 0.126847, acc 0.96875, learning_rate 0.000102464
2017-10-10T11:51:05.585339: step 1859, loss 0.130889, acc 0.953125, learning_rate 0.000102454
2017-10-10T11:51:05.813033: step 1860, loss 0.137439, acc 0.96875, learning_rate 0.000102444
2017-10-10T11:51:06.544856: step 1861, loss 0.0862543, acc 0.96875, learning_rate 0.000102434
2017-10-10T11:51:06.743226: step 1862, loss 0.217673, acc 0.921569, learning_rate 0.000102424
2017-10-10T11:51:06.952890: step 1863, loss 0.0461084, acc 1, learning_rate 0.000102414
2017-10-10T11:51:07.240402: step 1864, loss 0.217085, acc 0.9375, learning_rate 0.000102404
2017-10-10T11:51:07.418545: step 1865, loss 0.118353, acc 0.953125, learning_rate 0.000102394
2017-10-10T11:51:07.738519: step 1866, loss 0.0913434, acc 0.984375, learning_rate 0.000102384
2017-10-10T11:51:07.881793: step 1867, loss 0.196764, acc 0.9375, learning_rate 0.000102375
2017-10-10T11:51:08.015154: step 1868, loss 0.108657, acc 0.984375, learning_rate 0.000102365
2017-10-10T11:51:08.153507: step 1869, loss 0.209104, acc 0.9375, learning_rate 0.000102355
2017-10-10T11:51:08.281835: step 1870, loss 0.121751, acc 0.96875, learning_rate 0.000102346
2017-10-10T11:51:08.455221: step 1871, loss 0.1805, acc 0.90625, learning_rate 0.000102336
2017-10-10T11:51:08.637593: step 1872, loss 0.125437, acc 0.953125, learning_rate 0.000102327
2017-10-10T11:51:08.904863: step 1873, loss 0.14833, acc 0.96875, learning_rate 0.000102317
2017-10-10T11:51:09.147689: step 1874, loss 0.185578, acc 0.9375, learning_rate 0.000102308
2017-10-10T11:51:09.359728: step 1875, loss 0.19506, acc 0.953125, learning_rate 0.000102298
2017-10-10T11:51:09.538098: step 1876, loss 0.183924, acc 0.921875, learning_rate 0.000102289
2017-10-10T11:51:09.770575: step 1877, loss 0.0347316, acc 1, learning_rate 0.000102279
2017-10-10T11:51:09.983828: step 1878, loss 0.137735, acc 0.9375, learning_rate 0.00010227
2017-10-10T11:51:10.241543: step 1879, loss 0.0993153, acc 0.96875, learning_rate 0.000102261
2017-10-10T11:51:10.470172: step 1880, loss 0.0955062, acc 0.96875, learning_rate 0.000102252

Evaluation:
2017-10-10T11:51:10.926070: step 1880, loss 0.228549, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1880

2017-10-10T11:51:12.131637: step 1881, loss 0.104416, acc 0.96875, learning_rate 0.000102242
2017-10-10T11:51:12.382250: step 1882, loss 0.0816436, acc 0.984375, learning_rate 0.000102233
2017-10-10T11:51:12.612030: step 1883, loss 0.122683, acc 0.984375, learning_rate 0.000102224
2017-10-10T11:51:12.859770: step 1884, loss 0.181057, acc 0.921875, learning_rate 0.000102215
2017-10-10T11:51:13.088183: step 1885, loss 0.105695, acc 0.96875, learning_rate 0.000102206
2017-10-10T11:51:13.247065: step 1886, loss 0.0531862, acc 0.984375, learning_rate 0.000102197
2017-10-10T11:51:13.470186: step 1887, loss 0.148041, acc 0.953125, learning_rate 0.000102188
2017-10-10T11:51:13.704859: step 1888, loss 0.204744, acc 0.890625, learning_rate 0.000102179
2017-10-10T11:51:13.946581: step 1889, loss 0.200548, acc 0.921875, learning_rate 0.00010217
2017-10-10T11:51:14.192244: step 1890, loss 0.104769, acc 0.96875, learning_rate 0.000102161
2017-10-10T11:51:14.444863: step 1891, loss 0.0897766, acc 0.984375, learning_rate 0.000102153
2017-10-10T11:51:14.695843: step 1892, loss 0.155408, acc 0.90625, learning_rate 0.000102144
2017-10-10T11:51:14.948903: step 1893, loss 0.185608, acc 0.953125, learning_rate 0.000102135
2017-10-10T11:51:15.225789: step 1894, loss 0.165825, acc 0.953125, learning_rate 0.000102126
2017-10-10T11:51:15.514416: step 1895, loss 0.09996, acc 0.984375, learning_rate 0.000102118
2017-10-10T11:51:15.751387: step 1896, loss 0.116397, acc 0.984375, learning_rate 0.000102109
2017-10-10T11:51:15.934869: step 1897, loss 0.234311, acc 0.90625, learning_rate 0.0001021
2017-10-10T11:51:16.135852: step 1898, loss 0.215517, acc 0.9375, learning_rate 0.000102092
2017-10-10T11:51:16.321537: step 1899, loss 0.165639, acc 0.9375, learning_rate 0.000102083
2017-10-10T11:51:16.516837: step 1900, loss 0.176713, acc 0.9375, learning_rate 0.000102075
2017-10-10T11:51:16.771879: step 1901, loss 0.170239, acc 0.9375, learning_rate 0.000102066
2017-10-10T11:51:16.999872: step 1902, loss 0.151965, acc 0.921875, learning_rate 0.000102058
2017-10-10T11:51:17.257955: step 1903, loss 0.245231, acc 0.890625, learning_rate 0.00010205
2017-10-10T11:51:17.496774: step 1904, loss 0.147283, acc 0.953125, learning_rate 0.000102041
2017-10-10T11:51:17.765675: step 1905, loss 0.167232, acc 0.921875, learning_rate 0.000102033
2017-10-10T11:51:17.994736: step 1906, loss 0.148823, acc 0.9375, learning_rate 0.000102025
2017-10-10T11:51:18.282119: step 1907, loss 0.0756708, acc 0.984375, learning_rate 0.000102016
2017-10-10T11:51:18.517901: step 1908, loss 0.0676187, acc 1, learning_rate 0.000102008
2017-10-10T11:51:18.760931: step 1909, loss 0.0892296, acc 0.984375, learning_rate 0.000102
2017-10-10T11:51:19.096127: step 1910, loss 0.0873607, acc 0.953125, learning_rate 0.000101992
2017-10-10T11:51:19.265386: step 1911, loss 0.219636, acc 0.921875, learning_rate 0.000101984
2017-10-10T11:51:19.452863: step 1912, loss 0.100892, acc 0.96875, learning_rate 0.000101975
2017-10-10T11:51:19.620036: step 1913, loss 0.182181, acc 0.984375, learning_rate 0.000101967
2017-10-10T11:51:19.770409: step 1914, loss 0.0718951, acc 1, learning_rate 0.000101959
2017-10-10T11:51:19.923783: step 1915, loss 0.124132, acc 0.96875, learning_rate 0.000101951
2017-10-10T11:51:20.186046: step 1916, loss 0.231777, acc 0.921875, learning_rate 0.000101943
2017-10-10T11:51:20.422918: step 1917, loss 0.178478, acc 0.9375, learning_rate 0.000101935
2017-10-10T11:51:20.644841: step 1918, loss 0.14098, acc 0.953125, learning_rate 0.000101928
2017-10-10T11:51:20.883709: step 1919, loss 0.116658, acc 0.96875, learning_rate 0.00010192
2017-10-10T11:51:21.071105: step 1920, loss 0.212438, acc 0.875, learning_rate 0.000101912

Evaluation:
2017-10-10T11:51:21.516374: step 1920, loss 0.228003, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1920

2017-10-10T11:51:22.727237: step 1921, loss 0.153445, acc 0.9375, learning_rate 0.000101904
2017-10-10T11:51:22.958719: step 1922, loss 0.192792, acc 0.890625, learning_rate 0.000101896
2017-10-10T11:51:23.180841: step 1923, loss 0.250718, acc 0.921875, learning_rate 0.000101889
2017-10-10T11:51:23.403587: step 1924, loss 0.227222, acc 0.90625, learning_rate 0.000101881
2017-10-10T11:51:23.656940: step 1925, loss 0.142562, acc 0.953125, learning_rate 0.000101873
2017-10-10T11:51:23.912806: step 1926, loss 0.154533, acc 0.9375, learning_rate 0.000101865
2017-10-10T11:51:24.112693: step 1927, loss 0.119845, acc 0.953125, learning_rate 0.000101858
2017-10-10T11:51:24.304744: step 1928, loss 0.111316, acc 0.96875, learning_rate 0.00010185
2017-10-10T11:51:24.501890: step 1929, loss 0.141704, acc 0.953125, learning_rate 0.000101843
2017-10-10T11:51:24.694483: step 1930, loss 0.110062, acc 0.953125, learning_rate 0.000101835
2017-10-10T11:51:24.860902: step 1931, loss 0.0397577, acc 1, learning_rate 0.000101828
2017-10-10T11:51:25.120947: step 1932, loss 0.148028, acc 0.953125, learning_rate 0.00010182
2017-10-10T11:51:25.369365: step 1933, loss 0.151133, acc 0.96875, learning_rate 0.000101813
2017-10-10T11:51:25.610126: step 1934, loss 0.149158, acc 0.96875, learning_rate 0.000101805
2017-10-10T11:51:25.873434: step 1935, loss 0.236083, acc 0.921875, learning_rate 0.000101798
2017-10-10T11:51:26.114087: step 1936, loss 0.125999, acc 0.96875, learning_rate 0.000101791
2017-10-10T11:51:26.348848: step 1937, loss 0.101345, acc 0.953125, learning_rate 0.000101783
2017-10-10T11:51:26.604840: step 1938, loss 0.148217, acc 0.953125, learning_rate 0.000101776
2017-10-10T11:51:26.829759: step 1939, loss 0.169852, acc 0.9375, learning_rate 0.000101769
2017-10-10T11:51:27.059097: step 1940, loss 0.201722, acc 0.9375, learning_rate 0.000101762
2017-10-10T11:51:27.314505: step 1941, loss 0.0564522, acc 0.984375, learning_rate 0.000101754
2017-10-10T11:51:27.528878: step 1942, loss 0.199061, acc 0.9375, learning_rate 0.000101747
2017-10-10T11:51:27.761070: step 1943, loss 0.0777805, acc 0.984375, learning_rate 0.00010174
2017-10-10T11:51:28.000911: step 1944, loss 0.169816, acc 0.9375, learning_rate 0.000101733
2017-10-10T11:51:28.182991: step 1945, loss 0.149922, acc 0.953125, learning_rate 0.000101726
2017-10-10T11:51:28.397599: step 1946, loss 0.147262, acc 0.953125, learning_rate 0.000101719
2017-10-10T11:51:28.637041: step 1947, loss 0.198177, acc 0.953125, learning_rate 0.000101712
2017-10-10T11:51:28.852839: step 1948, loss 0.149552, acc 0.984375, learning_rate 0.000101705
2017-10-10T11:51:29.081490: step 1949, loss 0.0867867, acc 0.96875, learning_rate 0.000101698
2017-10-10T11:51:29.312835: step 1950, loss 0.114962, acc 0.953125, learning_rate 0.000101691
2017-10-10T11:51:29.578800: step 1951, loss 0.141415, acc 0.96875, learning_rate 0.000101684
2017-10-10T11:51:29.900786: step 1952, loss 0.246074, acc 0.890625, learning_rate 0.000101677
2017-10-10T11:51:30.077112: step 1953, loss 0.0907496, acc 0.96875, learning_rate 0.00010167
2017-10-10T11:51:30.248064: step 1954, loss 0.0806462, acc 1, learning_rate 0.000101664
2017-10-10T11:51:30.442743: step 1955, loss 0.221865, acc 0.90625, learning_rate 0.000101657
2017-10-10T11:51:30.613525: step 1956, loss 0.0750161, acc 0.984375, learning_rate 0.00010165
2017-10-10T11:51:30.780713: step 1957, loss 0.104675, acc 0.953125, learning_rate 0.000101643
2017-10-10T11:51:31.044921: step 1958, loss 0.14687, acc 0.9375, learning_rate 0.000101637
2017-10-10T11:51:31.288503: step 1959, loss 0.112401, acc 0.96875, learning_rate 0.00010163
2017-10-10T11:51:31.504809: step 1960, loss 0.136135, acc 0.941176, learning_rate 0.000101623

Evaluation:
2017-10-10T11:51:31.956252: step 1960, loss 0.226926, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-1960

2017-10-10T11:51:33.048828: step 1961, loss 0.0395033, acc 1, learning_rate 0.000101617
2017-10-10T11:51:33.248452: step 1962, loss 0.119174, acc 0.96875, learning_rate 0.00010161
2017-10-10T11:51:33.481031: step 1963, loss 0.169332, acc 0.953125, learning_rate 0.000101604
2017-10-10T11:51:33.722606: step 1964, loss 0.115729, acc 0.953125, learning_rate 0.000101597
2017-10-10T11:51:33.979977: step 1965, loss 0.142193, acc 0.984375, learning_rate 0.00010159
2017-10-10T11:51:34.215553: step 1966, loss 0.172429, acc 0.953125, learning_rate 0.000101584
2017-10-10T11:51:34.429244: step 1967, loss 0.0823741, acc 1, learning_rate 0.000101577
2017-10-10T11:51:34.643034: step 1968, loss 0.15296, acc 0.953125, learning_rate 0.000101571
2017-10-10T11:51:34.856910: step 1969, loss 0.250979, acc 0.90625, learning_rate 0.000101565
2017-10-10T11:51:35.152738: step 1970, loss 0.127622, acc 0.953125, learning_rate 0.000101558
2017-10-10T11:51:35.387050: step 1971, loss 0.174077, acc 0.921875, learning_rate 0.000101552
2017-10-10T11:51:35.608886: step 1972, loss 0.156532, acc 0.96875, learning_rate 0.000101546
2017-10-10T11:51:35.836917: step 1973, loss 0.261748, acc 0.90625, learning_rate 0.000101539
2017-10-10T11:51:36.051744: step 1974, loss 0.131968, acc 0.96875, learning_rate 0.000101533
2017-10-10T11:51:36.300898: step 1975, loss 0.24053, acc 0.890625, learning_rate 0.000101527
2017-10-10T11:51:36.552881: step 1976, loss 0.181619, acc 0.9375, learning_rate 0.00010152
2017-10-10T11:51:36.783404: step 1977, loss 0.119291, acc 0.953125, learning_rate 0.000101514
2017-10-10T11:51:37.025841: step 1978, loss 0.0429221, acc 0.984375, learning_rate 0.000101508
2017-10-10T11:51:37.256741: step 1979, loss 0.175085, acc 0.9375, learning_rate 0.000101502
2017-10-10T11:51:37.453140: step 1980, loss 0.119732, acc 0.953125, learning_rate 0.000101496
2017-10-10T11:51:37.706237: step 1981, loss 0.177621, acc 0.953125, learning_rate 0.00010149
2017-10-10T11:51:37.915081: step 1982, loss 0.152098, acc 0.953125, learning_rate 0.000101484
2017-10-10T11:51:38.111574: step 1983, loss 0.14498, acc 0.953125, learning_rate 0.000101478
2017-10-10T11:51:38.329745: step 1984, loss 0.144102, acc 0.953125, learning_rate 0.000101472
2017-10-10T11:51:38.609119: step 1985, loss 0.160433, acc 0.9375, learning_rate 0.000101466
2017-10-10T11:51:38.836275: step 1986, loss 0.169273, acc 0.9375, learning_rate 0.00010146
2017-10-10T11:51:39.112823: step 1987, loss 0.142357, acc 0.9375, learning_rate 0.000101454
2017-10-10T11:51:39.321947: step 1988, loss 0.236294, acc 0.921875, learning_rate 0.000101448
2017-10-10T11:51:39.581920: step 1989, loss 0.107042, acc 0.96875, learning_rate 0.000101442
2017-10-10T11:51:39.813061: step 1990, loss 0.0962378, acc 0.984375, learning_rate 0.000101436
2017-10-10T11:51:40.100920: step 1991, loss 0.0771369, acc 0.96875, learning_rate 0.00010143
2017-10-10T11:51:40.405182: step 1992, loss 0.128787, acc 0.953125, learning_rate 0.000101424
2017-10-10T11:51:40.588982: step 1993, loss 0.133317, acc 0.9375, learning_rate 0.000101418
2017-10-10T11:51:40.738928: step 1994, loss 0.182255, acc 0.921875, learning_rate 0.000101413
2017-10-10T11:51:40.996291: step 1995, loss 0.0507445, acc 1, learning_rate 0.000101407
2017-10-10T11:51:41.127865: step 1996, loss 0.0848531, acc 0.984375, learning_rate 0.000101401
2017-10-10T11:51:41.261667: step 1997, loss 0.116615, acc 0.953125, learning_rate 0.000101395
2017-10-10T11:51:41.467680: step 1998, loss 0.141092, acc 0.9375, learning_rate 0.00010139
2017-10-10T11:51:41.684974: step 1999, loss 0.0921631, acc 0.984375, learning_rate 0.000101384
2017-10-10T11:51:41.935777: step 2000, loss 0.107739, acc 0.984375, learning_rate 0.000101378

Evaluation:
2017-10-10T11:51:42.427138: step 2000, loss 0.226935, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2000

2017-10-10T11:51:43.847337: step 2001, loss 0.0872772, acc 0.96875, learning_rate 0.000101373
2017-10-10T11:51:44.068881: step 2002, loss 0.167362, acc 0.953125, learning_rate 0.000101367
2017-10-10T11:51:44.310438: step 2003, loss 0.140176, acc 0.953125, learning_rate 0.000101362
2017-10-10T11:51:44.534048: step 2004, loss 0.165934, acc 0.921875, learning_rate 0.000101356
2017-10-10T11:51:44.737834: step 2005, loss 0.158795, acc 0.953125, learning_rate 0.00010135
2017-10-10T11:51:44.950232: step 2006, loss 0.112223, acc 0.96875, learning_rate 0.000101345
2017-10-10T11:51:45.163561: step 2007, loss 0.105452, acc 0.984375, learning_rate 0.000101339
2017-10-10T11:51:45.391676: step 2008, loss 0.165401, acc 0.953125, learning_rate 0.000101334
2017-10-10T11:51:45.601778: step 2009, loss 0.090872, acc 0.96875, learning_rate 0.000101328
2017-10-10T11:51:45.812418: step 2010, loss 0.181568, acc 0.921875, learning_rate 0.000101323
2017-10-10T11:51:46.032946: step 2011, loss 0.103198, acc 0.953125, learning_rate 0.000101318
2017-10-10T11:51:46.261069: step 2012, loss 0.238659, acc 0.90625, learning_rate 0.000101312
2017-10-10T11:51:46.549586: step 2013, loss 0.180368, acc 0.9375, learning_rate 0.000101307
2017-10-10T11:51:46.786385: step 2014, loss 0.0680438, acc 0.96875, learning_rate 0.000101302
2017-10-10T11:51:46.984829: step 2015, loss 0.100341, acc 0.96875, learning_rate 0.000101296
2017-10-10T11:51:47.237723: step 2016, loss 0.0894852, acc 0.96875, learning_rate 0.000101291
2017-10-10T11:51:47.476055: step 2017, loss 0.237994, acc 0.90625, learning_rate 0.000101286
2017-10-10T11:51:47.715972: step 2018, loss 0.186732, acc 0.921875, learning_rate 0.00010128
2017-10-10T11:51:47.959726: step 2019, loss 0.0489826, acc 1, learning_rate 0.000101275
2017-10-10T11:51:48.195088: step 2020, loss 0.121551, acc 0.953125, learning_rate 0.00010127
2017-10-10T11:51:48.416802: step 2021, loss 0.15472, acc 0.9375, learning_rate 0.000101265
2017-10-10T11:51:48.660942: step 2022, loss 0.155216, acc 0.953125, learning_rate 0.00010126
2017-10-10T11:51:48.918022: step 2023, loss 0.0624362, acc 0.984375, learning_rate 0.000101255
2017-10-10T11:51:49.189080: step 2024, loss 0.134959, acc 0.96875, learning_rate 0.000101249
2017-10-10T11:51:49.386509: step 2025, loss 0.0980605, acc 0.984375, learning_rate 0.000101244
2017-10-10T11:51:49.580153: step 2026, loss 0.0599402, acc 1, learning_rate 0.000101239
2017-10-10T11:51:49.805656: step 2027, loss 0.0940918, acc 0.984375, learning_rate 0.000101234
2017-10-10T11:51:49.964839: step 2028, loss 0.129188, acc 0.953125, learning_rate 0.000101229
2017-10-10T11:51:50.129081: step 2029, loss 0.145046, acc 0.953125, learning_rate 0.000101224
2017-10-10T11:51:50.289144: step 2030, loss 0.0837926, acc 0.984375, learning_rate 0.000101219
2017-10-10T11:51:50.521330: step 2031, loss 0.138094, acc 0.953125, learning_rate 0.000101214
2017-10-10T11:51:50.810247: step 2032, loss 0.0853405, acc 0.96875, learning_rate 0.000101209
2017-10-10T11:51:51.082763: step 2033, loss 0.0781612, acc 0.984375, learning_rate 0.000101204
2017-10-10T11:51:51.260701: step 2034, loss 0.160156, acc 0.953125, learning_rate 0.000101199
2017-10-10T11:51:51.431502: step 2035, loss 0.080161, acc 0.984375, learning_rate 0.000101194
2017-10-10T11:51:51.602424: step 2036, loss 0.0654622, acc 0.984375, learning_rate 0.00010119
2017-10-10T11:51:51.782972: step 2037, loss 0.099818, acc 0.984375, learning_rate 0.000101185
2017-10-10T11:51:51.946799: step 2038, loss 0.236011, acc 0.921875, learning_rate 0.00010118
2017-10-10T11:51:52.108439: step 2039, loss 0.148409, acc 0.953125, learning_rate 0.000101175
2017-10-10T11:51:52.382749: step 2040, loss 0.233258, acc 0.921875, learning_rate 0.00010117

Evaluation:
2017-10-10T11:51:52.836962: step 2040, loss 0.226308, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2040

2017-10-10T11:51:54.155896: step 2041, loss 0.144738, acc 0.953125, learning_rate 0.000101166
2017-10-10T11:51:54.386844: step 2042, loss 0.130179, acc 0.96875, learning_rate 0.000101161
2017-10-10T11:51:54.616908: step 2043, loss 0.113033, acc 0.9375, learning_rate 0.000101156
2017-10-10T11:51:54.848809: step 2044, loss 0.0755465, acc 1, learning_rate 0.000101151
2017-10-10T11:51:55.117068: step 2045, loss 0.20842, acc 0.921875, learning_rate 0.000101147
2017-10-10T11:51:55.348037: step 2046, loss 0.114767, acc 0.96875, learning_rate 0.000101142
2017-10-10T11:51:55.600887: step 2047, loss 0.230946, acc 0.90625, learning_rate 0.000101137
2017-10-10T11:51:55.837971: step 2048, loss 0.119219, acc 0.96875, learning_rate 0.000101133
2017-10-10T11:51:56.092942: step 2049, loss 0.134356, acc 0.921875, learning_rate 0.000101128
2017-10-10T11:51:56.330239: step 2050, loss 0.190645, acc 0.9375, learning_rate 0.000101123
2017-10-10T11:51:56.563988: step 2051, loss 0.100652, acc 0.96875, learning_rate 0.000101119
2017-10-10T11:51:56.801440: step 2052, loss 0.0893348, acc 0.96875, learning_rate 0.000101114
2017-10-10T11:51:57.055022: step 2053, loss 0.110931, acc 0.984375, learning_rate 0.00010111
2017-10-10T11:51:57.328878: step 2054, loss 0.118606, acc 0.953125, learning_rate 0.000101105
2017-10-10T11:51:57.568402: step 2055, loss 0.322586, acc 0.921875, learning_rate 0.000101101
2017-10-10T11:51:57.764749: step 2056, loss 0.106048, acc 0.984375, learning_rate 0.000101096
2017-10-10T11:51:57.958809: step 2057, loss 0.11869, acc 0.953125, learning_rate 0.000101092
2017-10-10T11:51:58.104946: step 2058, loss 0.104073, acc 0.980392, learning_rate 0.000101087
2017-10-10T11:51:58.304422: step 2059, loss 0.150277, acc 0.9375, learning_rate 0.000101083
2017-10-10T11:51:58.512509: step 2060, loss 0.0724834, acc 0.96875, learning_rate 0.000101078
2017-10-10T11:51:58.751975: step 2061, loss 0.284143, acc 0.9375, learning_rate 0.000101074
2017-10-10T11:51:59.008953: step 2062, loss 0.169597, acc 0.953125, learning_rate 0.00010107
2017-10-10T11:51:59.220703: step 2063, loss 0.0791515, acc 0.984375, learning_rate 0.000101065
2017-10-10T11:51:59.443965: step 2064, loss 0.0742368, acc 1, learning_rate 0.000101061
2017-10-10T11:51:59.641655: step 2065, loss 0.112291, acc 0.9375, learning_rate 0.000101057
2017-10-10T11:51:59.856073: step 2066, loss 0.172094, acc 0.921875, learning_rate 0.000101052
2017-10-10T11:52:00.099664: step 2067, loss 0.203636, acc 0.875, learning_rate 0.000101048
2017-10-10T11:52:00.305006: step 2068, loss 0.0484204, acc 1, learning_rate 0.000101044
2017-10-10T11:52:00.531723: step 2069, loss 0.0887707, acc 0.953125, learning_rate 0.000101039
2017-10-10T11:52:00.772891: step 2070, loss 0.147765, acc 0.90625, learning_rate 0.000101035
2017-10-10T11:52:01.027064: step 2071, loss 0.193652, acc 0.953125, learning_rate 0.000101031
2017-10-10T11:52:01.278960: step 2072, loss 0.102113, acc 0.96875, learning_rate 0.000101027
2017-10-10T11:52:01.544607: step 2073, loss 0.17879, acc 0.9375, learning_rate 0.000101023
2017-10-10T11:52:01.812108: step 2074, loss 0.222769, acc 0.953125, learning_rate 0.000101018
2017-10-10T11:52:02.108990: step 2075, loss 0.162861, acc 0.9375, learning_rate 0.000101014
2017-10-10T11:52:02.281084: step 2076, loss 0.152766, acc 0.9375, learning_rate 0.00010101
2017-10-10T11:52:02.425390: step 2077, loss 0.108004, acc 0.96875, learning_rate 0.000101006
2017-10-10T11:52:02.587974: step 2078, loss 0.215557, acc 0.921875, learning_rate 0.000101002
2017-10-10T11:52:02.771011: step 2079, loss 0.121542, acc 0.96875, learning_rate 0.000100998
2017-10-10T11:52:02.944853: step 2080, loss 0.196551, acc 0.90625, learning_rate 0.000100994

Evaluation:
2017-10-10T11:52:03.472923: step 2080, loss 0.226308, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2080

2017-10-10T11:52:04.506375: step 2081, loss 0.108892, acc 0.96875, learning_rate 0.00010099
2017-10-10T11:52:04.751145: step 2082, loss 0.170551, acc 0.9375, learning_rate 0.000100986
2017-10-10T11:52:05.037235: step 2083, loss 0.144095, acc 0.96875, learning_rate 0.000100982
2017-10-10T11:52:05.275669: step 2084, loss 0.0541262, acc 1, learning_rate 0.000100978
2017-10-10T11:52:05.565069: step 2085, loss 0.196336, acc 0.921875, learning_rate 0.000100974
2017-10-10T11:52:05.818480: step 2086, loss 0.0945523, acc 0.96875, learning_rate 0.00010097
2017-10-10T11:52:06.000206: step 2087, loss 0.186526, acc 0.96875, learning_rate 0.000100966
2017-10-10T11:52:06.176816: step 2088, loss 0.170019, acc 0.9375, learning_rate 0.000100962
2017-10-10T11:52:06.392176: step 2089, loss 0.0903069, acc 0.96875, learning_rate 0.000100958
2017-10-10T11:52:06.649917: step 2090, loss 0.146588, acc 0.96875, learning_rate 0.000100954
2017-10-10T11:52:06.891280: step 2091, loss 0.122013, acc 0.96875, learning_rate 0.00010095
2017-10-10T11:52:07.099897: step 2092, loss 0.116975, acc 0.96875, learning_rate 0.000100946
2017-10-10T11:52:07.334338: step 2093, loss 0.149566, acc 0.9375, learning_rate 0.000100942
2017-10-10T11:52:07.568820: step 2094, loss 0.11325, acc 0.984375, learning_rate 0.000100938
2017-10-10T11:52:07.819794: step 2095, loss 0.160227, acc 0.9375, learning_rate 0.000100935
2017-10-10T11:52:08.060765: step 2096, loss 0.123329, acc 0.96875, learning_rate 0.000100931
2017-10-10T11:52:08.325754: step 2097, loss 0.133315, acc 0.953125, learning_rate 0.000100927
2017-10-10T11:52:08.577107: step 2098, loss 0.163693, acc 0.9375, learning_rate 0.000100923
2017-10-10T11:52:08.826735: step 2099, loss 0.171755, acc 0.9375, learning_rate 0.000100919
2017-10-10T11:52:09.097560: step 2100, loss 0.130597, acc 0.953125, learning_rate 0.000100916
2017-10-10T11:52:09.365800: step 2101, loss 0.0494532, acc 1, learning_rate 0.000100912
2017-10-10T11:52:09.602715: step 2102, loss 0.11066, acc 0.953125, learning_rate 0.000100908
2017-10-10T11:52:09.848998: step 2103, loss 0.0825811, acc 0.96875, learning_rate 0.000100904
2017-10-10T11:52:10.104168: step 2104, loss 0.145897, acc 0.921875, learning_rate 0.000100901
2017-10-10T11:52:10.355493: step 2105, loss 0.092296, acc 0.984375, learning_rate 0.000100897
2017-10-10T11:52:10.619482: step 2106, loss 0.0855235, acc 0.984375, learning_rate 0.000100893
2017-10-10T11:52:10.864879: step 2107, loss 0.115735, acc 0.953125, learning_rate 0.00010089
2017-10-10T11:52:11.122566: step 2108, loss 0.143168, acc 0.921875, learning_rate 0.000100886
2017-10-10T11:52:11.356689: step 2109, loss 0.174707, acc 0.96875, learning_rate 0.000100883
2017-10-10T11:52:11.583160: step 2110, loss 0.0882426, acc 0.984375, learning_rate 0.000100879
2017-10-10T11:52:11.936912: step 2111, loss 0.127131, acc 0.96875, learning_rate 0.000100875
2017-10-10T11:52:12.141263: step 2112, loss 0.257782, acc 0.9375, learning_rate 0.000100872
2017-10-10T11:52:12.319061: step 2113, loss 0.0800244, acc 0.984375, learning_rate 0.000100868
2017-10-10T11:52:12.548021: step 2114, loss 0.162725, acc 0.921875, learning_rate 0.000100865
2017-10-10T11:52:12.779206: step 2115, loss 0.105405, acc 0.96875, learning_rate 0.000100861
2017-10-10T11:52:13.029178: step 2116, loss 0.0520707, acc 1, learning_rate 0.000100858
2017-10-10T11:52:13.336837: step 2117, loss 0.0668463, acc 0.984375, learning_rate 0.000100854
2017-10-10T11:52:13.607402: step 2118, loss 0.146507, acc 0.953125, learning_rate 0.000100851
2017-10-10T11:52:13.776108: step 2119, loss 0.166471, acc 0.921875, learning_rate 0.000100847
2017-10-10T11:52:13.944567: step 2120, loss 0.128905, acc 0.96875, learning_rate 0.000100844

Evaluation:
2017-10-10T11:52:14.383778: step 2120, loss 0.226819, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2120

2017-10-10T11:52:15.436474: step 2121, loss 0.0767073, acc 0.984375, learning_rate 0.00010084
2017-10-10T11:52:15.675443: step 2122, loss 0.195508, acc 0.9375, learning_rate 0.000100837
2017-10-10T11:52:15.931708: step 2123, loss 0.107833, acc 0.953125, learning_rate 0.000100833
2017-10-10T11:52:16.176284: step 2124, loss 0.101486, acc 0.984375, learning_rate 0.00010083
2017-10-10T11:52:16.408976: step 2125, loss 0.0426901, acc 0.984375, learning_rate 0.000100827
2017-10-10T11:52:16.614340: step 2126, loss 0.186587, acc 0.921875, learning_rate 0.000100823
2017-10-10T11:52:16.855844: step 2127, loss 0.147424, acc 0.90625, learning_rate 0.00010082
2017-10-10T11:52:17.058377: step 2128, loss 0.0699919, acc 0.96875, learning_rate 0.000100817
2017-10-10T11:52:17.320457: step 2129, loss 0.103033, acc 0.96875, learning_rate 0.000100813
2017-10-10T11:52:17.528860: step 2130, loss 0.144013, acc 0.984375, learning_rate 0.00010081
2017-10-10T11:52:17.779508: step 2131, loss 0.159201, acc 0.9375, learning_rate 0.000100807
2017-10-10T11:52:17.992010: step 2132, loss 0.107761, acc 0.984375, learning_rate 0.000100803
2017-10-10T11:52:18.217154: step 2133, loss 0.0977133, acc 0.984375, learning_rate 0.0001008
2017-10-10T11:52:18.447793: step 2134, loss 0.0752291, acc 0.984375, learning_rate 0.000100797
2017-10-10T11:52:18.691158: step 2135, loss 0.142905, acc 0.96875, learning_rate 0.000100793
2017-10-10T11:52:18.916860: step 2136, loss 0.151272, acc 0.90625, learning_rate 0.00010079
2017-10-10T11:52:19.109053: step 2137, loss 0.0975065, acc 0.96875, learning_rate 0.000100787
2017-10-10T11:52:19.361051: step 2138, loss 0.143928, acc 0.953125, learning_rate 0.000100784
2017-10-10T11:52:19.558064: step 2139, loss 0.140256, acc 0.953125, learning_rate 0.000100781
2017-10-10T11:52:19.763162: step 2140, loss 0.116874, acc 0.96875, learning_rate 0.000100777
2017-10-10T11:52:19.987250: step 2141, loss 0.0817136, acc 0.984375, learning_rate 0.000100774
2017-10-10T11:52:20.239706: step 2142, loss 0.0993291, acc 0.96875, learning_rate 0.000100771
2017-10-10T11:52:20.489062: step 2143, loss 0.0887264, acc 0.953125, learning_rate 0.000100768
2017-10-10T11:52:20.732176: step 2144, loss 0.065541, acc 0.984375, learning_rate 0.000100765
2017-10-10T11:52:20.959072: step 2145, loss 0.203263, acc 0.9375, learning_rate 0.000100762
2017-10-10T11:52:21.176837: step 2146, loss 0.115858, acc 0.953125, learning_rate 0.000100759
2017-10-10T11:52:21.422443: step 2147, loss 0.121464, acc 0.96875, learning_rate 0.000100755
2017-10-10T11:52:21.649284: step 2148, loss 0.141033, acc 0.953125, learning_rate 0.000100752
2017-10-10T11:52:21.852264: step 2149, loss 0.138931, acc 0.984375, learning_rate 0.000100749
2017-10-10T11:52:22.069449: step 2150, loss 0.146026, acc 0.9375, learning_rate 0.000100746
2017-10-10T11:52:22.357093: step 2151, loss 0.240728, acc 0.9375, learning_rate 0.000100743
2017-10-10T11:52:22.548866: step 2152, loss 0.201518, acc 0.9375, learning_rate 0.00010074
2017-10-10T11:52:22.827566: step 2153, loss 0.122371, acc 0.953125, learning_rate 0.000100737
2017-10-10T11:52:23.054720: step 2154, loss 0.106443, acc 0.984375, learning_rate 0.000100734
2017-10-10T11:52:23.261623: step 2155, loss 0.139921, acc 0.953125, learning_rate 0.000100731
2017-10-10T11:52:23.416854: step 2156, loss 0.068424, acc 1, learning_rate 0.000100728
2017-10-10T11:52:23.629148: step 2157, loss 0.0760354, acc 0.984375, learning_rate 0.000100725
2017-10-10T11:52:23.808862: step 2158, loss 0.199136, acc 0.921875, learning_rate 0.000100722
2017-10-10T11:52:24.017710: step 2159, loss 0.0925511, acc 0.96875, learning_rate 0.000100719
2017-10-10T11:52:24.319145: step 2160, loss 0.138831, acc 0.96875, learning_rate 0.000100716

Evaluation:
2017-10-10T11:52:24.723016: step 2160, loss 0.224935, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2160

2017-10-10T11:52:25.917472: step 2161, loss 0.0829611, acc 0.984375, learning_rate 0.000100713
2017-10-10T11:52:26.181076: step 2162, loss 0.178288, acc 0.9375, learning_rate 0.000100711
2017-10-10T11:52:26.376812: step 2163, loss 0.0620455, acc 0.984375, learning_rate 0.000100708
2017-10-10T11:52:26.617049: step 2164, loss 0.318612, acc 0.90625, learning_rate 0.000100705
2017-10-10T11:52:26.849216: step 2165, loss 0.125229, acc 0.96875, learning_rate 0.000100702
2017-10-10T11:52:27.085932: step 2166, loss 0.192714, acc 0.921875, learning_rate 0.000100699
2017-10-10T11:52:27.340758: step 2167, loss 0.13156, acc 0.953125, learning_rate 0.000100696
2017-10-10T11:52:27.584221: step 2168, loss 0.0995473, acc 0.96875, learning_rate 0.000100693
2017-10-10T11:52:27.842498: step 2169, loss 0.162064, acc 0.9375, learning_rate 0.00010069
2017-10-10T11:52:28.068886: step 2170, loss 0.155495, acc 0.921875, learning_rate 0.000100688
2017-10-10T11:52:28.304847: step 2171, loss 0.154458, acc 0.921875, learning_rate 0.000100685
2017-10-10T11:52:28.581528: step 2172, loss 0.217762, acc 0.921875, learning_rate 0.000100682
2017-10-10T11:52:28.831783: step 2173, loss 0.17121, acc 0.96875, learning_rate 0.000100679
2017-10-10T11:52:29.098800: step 2174, loss 0.199479, acc 0.9375, learning_rate 0.000100677
2017-10-10T11:52:29.311416: step 2175, loss 0.120862, acc 0.9375, learning_rate 0.000100674
2017-10-10T11:52:29.516513: step 2176, loss 0.133348, acc 0.96875, learning_rate 0.000100671
2017-10-10T11:52:29.741205: step 2177, loss 0.0304893, acc 1, learning_rate 0.000100668
2017-10-10T11:52:30.020861: step 2178, loss 0.16546, acc 0.921875, learning_rate 0.000100666
2017-10-10T11:52:30.249920: step 2179, loss 0.16616, acc 0.953125, learning_rate 0.000100663
2017-10-10T11:52:30.469296: step 2180, loss 0.0959115, acc 0.984375, learning_rate 0.00010066
2017-10-10T11:52:30.700896: step 2181, loss 0.194873, acc 0.96875, learning_rate 0.000100657
2017-10-10T11:52:30.959846: step 2182, loss 0.0734281, acc 0.984375, learning_rate 0.000100655
2017-10-10T11:52:31.141854: step 2183, loss 0.139518, acc 0.96875, learning_rate 0.000100652
2017-10-10T11:52:31.337291: step 2184, loss 0.162468, acc 0.953125, learning_rate 0.000100649
2017-10-10T11:52:31.551510: step 2185, loss 0.0464573, acc 1, learning_rate 0.000100647
2017-10-10T11:52:31.759605: step 2186, loss 0.0931957, acc 0.96875, learning_rate 0.000100644
2017-10-10T11:52:31.963827: step 2187, loss 0.0802471, acc 0.984375, learning_rate 0.000100641
2017-10-10T11:52:32.149087: step 2188, loss 0.310772, acc 0.875, learning_rate 0.000100639
2017-10-10T11:52:32.387918: step 2189, loss 0.0866162, acc 0.96875, learning_rate 0.000100636
2017-10-10T11:52:32.610468: step 2190, loss 0.0857137, acc 0.984375, learning_rate 0.000100634
2017-10-10T11:52:32.889133: step 2191, loss 0.0937725, acc 0.96875, learning_rate 0.000100631
2017-10-10T11:52:33.098016: step 2192, loss 0.113364, acc 0.953125, learning_rate 0.000100628
2017-10-10T11:52:33.334032: step 2193, loss 0.221442, acc 0.921875, learning_rate 0.000100626
2017-10-10T11:52:33.580878: step 2194, loss 0.0695038, acc 0.96875, learning_rate 0.000100623
2017-10-10T11:52:33.817227: step 2195, loss 0.109206, acc 0.96875, learning_rate 0.000100621
2017-10-10T11:52:34.059457: step 2196, loss 0.106696, acc 0.953125, learning_rate 0.000100618
2017-10-10T11:52:34.303366: step 2197, loss 0.0756109, acc 0.984375, learning_rate 0.000100616
2017-10-10T11:52:34.542377: step 2198, loss 0.076674, acc 0.984375, learning_rate 0.000100613
2017-10-10T11:52:34.754690: step 2199, loss 0.0796865, acc 0.96875, learning_rate 0.000100611
2017-10-10T11:52:34.997921: step 2200, loss 0.140671, acc 0.953125, learning_rate 0.000100608

Evaluation:
2017-10-10T11:52:35.476335: step 2200, loss 0.224222, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2200

2017-10-10T11:52:36.676871: step 2201, loss 0.0678907, acc 0.984375, learning_rate 0.000100606
2017-10-10T11:52:36.953381: step 2202, loss 0.197044, acc 0.9375, learning_rate 0.000100603
2017-10-10T11:52:37.189293: step 2203, loss 0.0872019, acc 0.984375, learning_rate 0.000100601
2017-10-10T11:52:37.424804: step 2204, loss 0.0531004, acc 0.984375, learning_rate 0.000100598
2017-10-10T11:52:37.700857: step 2205, loss 0.148231, acc 0.9375, learning_rate 0.000100596
2017-10-10T11:52:37.952831: step 2206, loss 0.0692218, acc 0.984375, learning_rate 0.000100594
2017-10-10T11:52:38.184963: step 2207, loss 0.0707616, acc 0.984375, learning_rate 0.000100591
2017-10-10T11:52:38.397166: step 2208, loss 0.0479499, acc 1, learning_rate 0.000100589
2017-10-10T11:52:38.694245: step 2209, loss 0.123806, acc 0.9375, learning_rate 0.000100586
2017-10-10T11:52:38.934731: step 2210, loss 0.132994, acc 0.953125, learning_rate 0.000100584
2017-10-10T11:52:39.136989: step 2211, loss 0.0529012, acc 0.984375, learning_rate 0.000100581
2017-10-10T11:52:39.326926: step 2212, loss 0.153123, acc 0.96875, learning_rate 0.000100579
2017-10-10T11:52:39.506002: step 2213, loss 0.0810706, acc 0.953125, learning_rate 0.000100577
2017-10-10T11:52:39.693011: step 2214, loss 0.0348346, acc 1, learning_rate 0.000100574
2017-10-10T11:52:39.882314: step 2215, loss 0.160749, acc 0.953125, learning_rate 0.000100572
2017-10-10T11:52:40.114414: step 2216, loss 0.0778901, acc 0.984375, learning_rate 0.00010057
2017-10-10T11:52:40.308603: step 2217, loss 0.133059, acc 0.9375, learning_rate 0.000100567
2017-10-10T11:52:40.544628: step 2218, loss 0.133768, acc 0.96875, learning_rate 0.000100565
2017-10-10T11:52:40.781372: step 2219, loss 0.117599, acc 0.9375, learning_rate 0.000100563
2017-10-10T11:52:41.041669: step 2220, loss 0.100632, acc 0.953125, learning_rate 0.00010056
2017-10-10T11:52:41.286775: step 2221, loss 0.066665, acc 0.984375, learning_rate 0.000100558
2017-10-10T11:52:41.509353: step 2222, loss 0.204047, acc 0.9375, learning_rate 0.000100556
2017-10-10T11:52:41.756958: step 2223, loss 0.226048, acc 0.921875, learning_rate 0.000100554
2017-10-10T11:52:42.002248: step 2224, loss 0.120773, acc 0.9375, learning_rate 0.000100551
2017-10-10T11:52:42.232481: step 2225, loss 0.0925267, acc 0.984375, learning_rate 0.000100549
2017-10-10T11:52:42.501059: step 2226, loss 0.117364, acc 0.96875, learning_rate 0.000100547
2017-10-10T11:52:42.725185: step 2227, loss 0.11676, acc 0.953125, learning_rate 0.000100545
2017-10-10T11:52:42.956887: step 2228, loss 0.139889, acc 0.96875, learning_rate 0.000100542
2017-10-10T11:52:43.196869: step 2229, loss 0.114814, acc 0.96875, learning_rate 0.00010054
2017-10-10T11:52:43.410449: step 2230, loss 0.0831757, acc 0.953125, learning_rate 0.000100538
2017-10-10T11:52:43.582282: step 2231, loss 0.118052, acc 0.953125, learning_rate 0.000100536
2017-10-10T11:52:43.803538: step 2232, loss 0.18661, acc 0.953125, learning_rate 0.000100534
2017-10-10T11:52:44.041412: step 2233, loss 0.153841, acc 0.921875, learning_rate 0.000100531
2017-10-10T11:52:44.277110: step 2234, loss 0.18944, acc 0.921875, learning_rate 0.000100529
2017-10-10T11:52:44.504117: step 2235, loss 0.156306, acc 0.921875, learning_rate 0.000100527
2017-10-10T11:52:44.720885: step 2236, loss 0.149889, acc 0.96875, learning_rate 0.000100525
2017-10-10T11:52:44.985238: step 2237, loss 0.163708, acc 0.953125, learning_rate 0.000100523
2017-10-10T11:52:45.212804: step 2238, loss 0.125276, acc 0.953125, learning_rate 0.000100521
2017-10-10T11:52:45.460881: step 2239, loss 0.126216, acc 0.953125, learning_rate 0.000100519
2017-10-10T11:52:45.694028: step 2240, loss 0.0577336, acc 1, learning_rate 0.000100516

Evaluation:
2017-10-10T11:52:46.192936: step 2240, loss 0.224786, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2240

2017-10-10T11:52:47.261073: step 2241, loss 0.212692, acc 0.921875, learning_rate 0.000100514
2017-10-10T11:52:47.395043: step 2242, loss 0.148316, acc 0.96875, learning_rate 0.000100512
2017-10-10T11:52:47.516021: step 2243, loss 0.22457, acc 0.90625, learning_rate 0.00010051
2017-10-10T11:52:47.640202: step 2244, loss 0.0872134, acc 0.984375, learning_rate 0.000100508
2017-10-10T11:52:47.816845: step 2245, loss 0.12667, acc 0.9375, learning_rate 0.000100506
2017-10-10T11:52:48.030060: step 2246, loss 0.155153, acc 0.9375, learning_rate 0.000100504
2017-10-10T11:52:48.225120: step 2247, loss 0.10326, acc 0.953125, learning_rate 0.000100502
2017-10-10T11:52:48.420988: step 2248, loss 0.110376, acc 0.96875, learning_rate 0.0001005
2017-10-10T11:52:48.612929: step 2249, loss 0.132754, acc 0.953125, learning_rate 0.000100498
2017-10-10T11:52:48.824801: step 2250, loss 0.154558, acc 0.921875, learning_rate 0.000100496
2017-10-10T11:52:49.076959: step 2251, loss 0.15593, acc 0.9375, learning_rate 0.000100494
2017-10-10T11:52:49.315118: step 2252, loss 0.0845032, acc 0.984375, learning_rate 0.000100492
2017-10-10T11:52:49.596680: step 2253, loss 0.0565153, acc 1, learning_rate 0.00010049
2017-10-10T11:52:49.796819: step 2254, loss 0.138361, acc 0.960784, learning_rate 0.000100488
2017-10-10T11:52:50.033620: step 2255, loss 0.0946944, acc 0.953125, learning_rate 0.000100486
2017-10-10T11:52:50.248831: step 2256, loss 0.142183, acc 0.953125, learning_rate 0.000100484
2017-10-10T11:52:50.486588: step 2257, loss 0.0854254, acc 0.984375, learning_rate 0.000100482
2017-10-10T11:52:50.706400: step 2258, loss 0.0632247, acc 0.984375, learning_rate 0.00010048
2017-10-10T11:52:50.945873: step 2259, loss 0.0792881, acc 0.984375, learning_rate 0.000100478
2017-10-10T11:52:51.168837: step 2260, loss 0.0776435, acc 0.984375, learning_rate 0.000100476
2017-10-10T11:52:51.408286: step 2261, loss 0.0861689, acc 0.96875, learning_rate 0.000100474
2017-10-10T11:52:51.656882: step 2262, loss 0.113066, acc 0.953125, learning_rate 0.000100472
2017-10-10T11:52:51.901077: step 2263, loss 0.115021, acc 0.96875, learning_rate 0.00010047
2017-10-10T11:52:52.156895: step 2264, loss 0.122279, acc 0.96875, learning_rate 0.000100468
2017-10-10T11:52:52.414721: step 2265, loss 0.133762, acc 0.953125, learning_rate 0.000100466
2017-10-10T11:52:52.664846: step 2266, loss 0.090915, acc 0.96875, learning_rate 0.000100464
2017-10-10T11:52:52.912478: step 2267, loss 0.131398, acc 0.953125, learning_rate 0.000100462
2017-10-10T11:52:53.138306: step 2268, loss 0.0744736, acc 0.984375, learning_rate 0.000100461
2017-10-10T11:52:53.344844: step 2269, loss 0.113287, acc 0.96875, learning_rate 0.000100459
2017-10-10T11:52:53.576897: step 2270, loss 0.0938566, acc 0.953125, learning_rate 0.000100457
2017-10-10T11:52:53.769170: step 2271, loss 0.178193, acc 0.96875, learning_rate 0.000100455
2017-10-10T11:52:54.003840: step 2272, loss 0.0870252, acc 0.984375, learning_rate 0.000100453
2017-10-10T11:52:54.264831: step 2273, loss 0.259683, acc 0.90625, learning_rate 0.000100451
2017-10-10T11:52:54.496966: step 2274, loss 0.170974, acc 0.96875, learning_rate 0.000100449
2017-10-10T11:52:54.745051: step 2275, loss 0.1514, acc 0.953125, learning_rate 0.000100448
2017-10-10T11:52:54.967019: step 2276, loss 0.108761, acc 0.96875, learning_rate 0.000100446
2017-10-10T11:52:55.187822: step 2277, loss 0.185058, acc 0.953125, learning_rate 0.000100444
2017-10-10T11:52:55.426705: step 2278, loss 0.196311, acc 0.953125, learning_rate 0.000100442
2017-10-10T11:52:55.630407: step 2279, loss 0.115836, acc 0.953125, learning_rate 0.00010044
2017-10-10T11:52:55.929159: step 2280, loss 0.101887, acc 0.96875, learning_rate 0.000100439

Evaluation:
2017-10-10T11:52:56.334565: step 2280, loss 0.224837, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2280

2017-10-10T11:52:57.472948: step 2281, loss 0.197952, acc 0.9375, learning_rate 0.000100437
2017-10-10T11:52:57.649823: step 2282, loss 0.170109, acc 0.9375, learning_rate 0.000100435
2017-10-10T11:52:57.824620: step 2283, loss 0.160616, acc 0.96875, learning_rate 0.000100433
2017-10-10T11:52:57.996862: step 2284, loss 0.053483, acc 1, learning_rate 0.000100431
2017-10-10T11:52:58.174321: step 2285, loss 0.156089, acc 0.953125, learning_rate 0.00010043
2017-10-10T11:52:58.350987: step 2286, loss 0.165171, acc 0.9375, learning_rate 0.000100428
2017-10-10T11:52:58.512828: step 2287, loss 0.129257, acc 0.953125, learning_rate 0.000100426
2017-10-10T11:52:58.777558: step 2288, loss 0.180481, acc 0.9375, learning_rate 0.000100424
2017-10-10T11:52:59.024899: step 2289, loss 0.220207, acc 0.9375, learning_rate 0.000100423
2017-10-10T11:52:59.267945: step 2290, loss 0.0709217, acc 0.984375, learning_rate 0.000100421
2017-10-10T11:52:59.483372: step 2291, loss 0.180827, acc 0.9375, learning_rate 0.000100419
2017-10-10T11:52:59.686380: step 2292, loss 0.118913, acc 0.953125, learning_rate 0.000100418
2017-10-10T11:52:59.896841: step 2293, loss 0.211066, acc 0.953125, learning_rate 0.000100416
2017-10-10T11:53:00.140893: step 2294, loss 0.122269, acc 0.96875, learning_rate 0.000100414
2017-10-10T11:53:00.378640: step 2295, loss 0.128852, acc 0.9375, learning_rate 0.000100412
2017-10-10T11:53:00.607554: step 2296, loss 0.0817875, acc 0.96875, learning_rate 0.000100411
2017-10-10T11:53:00.878643: step 2297, loss 0.0777949, acc 0.984375, learning_rate 0.000100409
2017-10-10T11:53:01.142040: step 2298, loss 0.252419, acc 0.921875, learning_rate 0.000100407
2017-10-10T11:53:01.396368: step 2299, loss 0.138066, acc 0.9375, learning_rate 0.000100406
2017-10-10T11:53:01.612682: step 2300, loss 0.119542, acc 0.96875, learning_rate 0.000100404
2017-10-10T11:53:01.846057: step 2301, loss 0.130526, acc 0.984375, learning_rate 0.000100402
2017-10-10T11:53:02.106937: step 2302, loss 0.217901, acc 0.953125, learning_rate 0.000100401
2017-10-10T11:53:02.324389: step 2303, loss 0.101632, acc 0.953125, learning_rate 0.000100399
2017-10-10T11:53:02.555401: step 2304, loss 0.157316, acc 0.9375, learning_rate 0.000100398
2017-10-10T11:53:02.804885: step 2305, loss 0.0614577, acc 0.984375, learning_rate 0.000100396
2017-10-10T11:53:03.073349: step 2306, loss 0.10371, acc 0.953125, learning_rate 0.000100394
2017-10-10T11:53:03.287657: step 2307, loss 0.103679, acc 0.953125, learning_rate 0.000100393
2017-10-10T11:53:03.541892: step 2308, loss 0.205061, acc 0.9375, learning_rate 0.000100391
2017-10-10T11:53:03.759891: step 2309, loss 0.122159, acc 0.984375, learning_rate 0.000100389
2017-10-10T11:53:03.992361: step 2310, loss 0.205671, acc 0.921875, learning_rate 0.000100388
2017-10-10T11:53:04.296822: step 2311, loss 0.0508494, acc 0.984375, learning_rate 0.000100386
2017-10-10T11:53:04.511498: step 2312, loss 0.052552, acc 1, learning_rate 0.000100385
2017-10-10T11:53:04.706790: step 2313, loss 0.121131, acc 0.96875, learning_rate 0.000100383
2017-10-10T11:53:04.895043: step 2314, loss 0.13134, acc 0.9375, learning_rate 0.000100382
2017-10-10T11:53:05.082874: step 2315, loss 0.150704, acc 0.9375, learning_rate 0.00010038
2017-10-10T11:53:05.274363: step 2316, loss 0.152084, acc 0.9375, learning_rate 0.000100378
2017-10-10T11:53:05.497836: step 2317, loss 0.0532839, acc 1, learning_rate 0.000100377
2017-10-10T11:53:05.711047: step 2318, loss 0.133415, acc 0.984375, learning_rate 0.000100375
2017-10-10T11:53:05.916852: step 2319, loss 0.145896, acc 0.9375, learning_rate 0.000100374
2017-10-10T11:53:06.130839: step 2320, loss 0.134081, acc 0.9375, learning_rate 0.000100372

Evaluation:
2017-10-10T11:53:06.619468: step 2320, loss 0.224661, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2320

2017-10-10T11:53:07.684888: step 2321, loss 0.134154, acc 0.953125, learning_rate 0.000100371
2017-10-10T11:53:07.936712: step 2322, loss 0.0768232, acc 0.953125, learning_rate 0.000100369
2017-10-10T11:53:08.180909: step 2323, loss 0.077988, acc 0.984375, learning_rate 0.000100368
2017-10-10T11:53:08.490081: step 2324, loss 0.125882, acc 0.9375, learning_rate 0.000100366
2017-10-10T11:53:08.753109: step 2325, loss 0.100741, acc 0.984375, learning_rate 0.000100365
2017-10-10T11:53:08.936255: step 2326, loss 0.0812433, acc 0.984375, learning_rate 0.000100363
2017-10-10T11:53:09.144702: step 2327, loss 0.113409, acc 0.953125, learning_rate 0.000100362
2017-10-10T11:53:09.309902: step 2328, loss 0.0787999, acc 0.96875, learning_rate 0.00010036
2017-10-10T11:53:09.483262: step 2329, loss 0.2492, acc 0.90625, learning_rate 0.000100359
2017-10-10T11:53:09.716909: step 2330, loss 0.173734, acc 0.921875, learning_rate 0.000100357
2017-10-10T11:53:09.928318: step 2331, loss 0.138633, acc 0.9375, learning_rate 0.000100356
2017-10-10T11:53:10.165017: step 2332, loss 0.0512929, acc 1, learning_rate 0.000100354
2017-10-10T11:53:10.412934: step 2333, loss 0.222514, acc 0.953125, learning_rate 0.000100353
2017-10-10T11:53:10.588617: step 2334, loss 0.160833, acc 0.9375, learning_rate 0.000100352
2017-10-10T11:53:10.806848: step 2335, loss 0.103128, acc 0.953125, learning_rate 0.00010035
2017-10-10T11:53:11.056420: step 2336, loss 0.300651, acc 0.890625, learning_rate 0.000100349
2017-10-10T11:53:11.274316: step 2337, loss 0.203815, acc 0.921875, learning_rate 0.000100347
2017-10-10T11:53:11.531907: step 2338, loss 0.169011, acc 0.953125, learning_rate 0.000100346
2017-10-10T11:53:11.747680: step 2339, loss 0.245004, acc 0.9375, learning_rate 0.000100344
2017-10-10T11:53:11.992912: step 2340, loss 0.130016, acc 0.96875, learning_rate 0.000100343
2017-10-10T11:53:12.262442: step 2341, loss 0.122238, acc 0.953125, learning_rate 0.000100342
2017-10-10T11:53:12.514782: step 2342, loss 0.0484919, acc 1, learning_rate 0.00010034
2017-10-10T11:53:12.712808: step 2343, loss 0.0763679, acc 0.96875, learning_rate 0.000100339
2017-10-10T11:53:12.896885: step 2344, loss 0.150738, acc 0.9375, learning_rate 0.000100338
2017-10-10T11:53:13.073711: step 2345, loss 0.169765, acc 0.9375, learning_rate 0.000100336
2017-10-10T11:53:13.237516: step 2346, loss 0.237007, acc 0.875, learning_rate 0.000100335
2017-10-10T11:53:13.472893: step 2347, loss 0.0718706, acc 0.96875, learning_rate 0.000100333
2017-10-10T11:53:13.690507: step 2348, loss 0.175202, acc 0.90625, learning_rate 0.000100332
2017-10-10T11:53:13.938115: step 2349, loss 0.162078, acc 0.9375, learning_rate 0.000100331
2017-10-10T11:53:14.166459: step 2350, loss 0.106733, acc 0.953125, learning_rate 0.000100329
2017-10-10T11:53:14.420764: step 2351, loss 0.125112, acc 0.96875, learning_rate 0.000100328
2017-10-10T11:53:14.630163: step 2352, loss 0.101425, acc 0.960784, learning_rate 0.000100327
2017-10-10T11:53:14.873197: step 2353, loss 0.0615863, acc 0.984375, learning_rate 0.000100325
2017-10-10T11:53:15.102898: step 2354, loss 0.0951359, acc 0.96875, learning_rate 0.000100324
2017-10-10T11:53:15.336632: step 2355, loss 0.135369, acc 0.96875, learning_rate 0.000100323
2017-10-10T11:53:15.598691: step 2356, loss 0.215493, acc 0.921875, learning_rate 0.000100321
2017-10-10T11:53:15.824467: step 2357, loss 0.111005, acc 0.921875, learning_rate 0.00010032
2017-10-10T11:53:16.076898: step 2358, loss 0.104938, acc 0.984375, learning_rate 0.000100319
2017-10-10T11:53:16.331695: step 2359, loss 0.0951751, acc 0.96875, learning_rate 0.000100317
2017-10-10T11:53:16.586599: step 2360, loss 0.110098, acc 0.953125, learning_rate 0.000100316

Evaluation:
2017-10-10T11:53:17.073104: step 2360, loss 0.224758, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2360

2017-10-10T11:53:18.196437: step 2361, loss 0.173153, acc 0.921875, learning_rate 0.000100315
2017-10-10T11:53:18.412899: step 2362, loss 0.199186, acc 0.953125, learning_rate 0.000100314
2017-10-10T11:53:18.640175: step 2363, loss 0.253296, acc 0.921875, learning_rate 0.000100312
2017-10-10T11:53:18.864856: step 2364, loss 0.205897, acc 0.921875, learning_rate 0.000100311
2017-10-10T11:53:19.084848: step 2365, loss 0.129321, acc 0.96875, learning_rate 0.00010031
2017-10-10T11:53:19.370498: step 2366, loss 0.100319, acc 0.96875, learning_rate 0.000100308
2017-10-10T11:53:19.688987: step 2367, loss 0.124605, acc 0.9375, learning_rate 0.000100307
2017-10-10T11:53:19.875888: step 2368, loss 0.113705, acc 0.984375, learning_rate 0.000100306
2017-10-10T11:53:20.060899: step 2369, loss 0.101787, acc 0.96875, learning_rate 0.000100305
2017-10-10T11:53:20.224872: step 2370, loss 0.150994, acc 0.953125, learning_rate 0.000100303
2017-10-10T11:53:20.406373: step 2371, loss 0.107736, acc 0.96875, learning_rate 0.000100302
2017-10-10T11:53:20.651518: step 2372, loss 0.0412461, acc 0.984375, learning_rate 0.000100301
2017-10-10T11:53:20.847534: step 2373, loss 0.131794, acc 0.9375, learning_rate 0.0001003
2017-10-10T11:53:21.035889: step 2374, loss 0.0790448, acc 0.96875, learning_rate 0.000100299
2017-10-10T11:53:21.240488: step 2375, loss 0.182267, acc 0.9375, learning_rate 0.000100297
2017-10-10T11:53:21.436732: step 2376, loss 0.124298, acc 0.953125, learning_rate 0.000100296
2017-10-10T11:53:21.635205: step 2377, loss 0.1025, acc 0.953125, learning_rate 0.000100295
2017-10-10T11:53:21.804828: step 2378, loss 0.185052, acc 0.921875, learning_rate 0.000100294
2017-10-10T11:53:22.028927: step 2379, loss 0.136828, acc 0.953125, learning_rate 0.000100292
2017-10-10T11:53:22.242842: step 2380, loss 0.118833, acc 0.96875, learning_rate 0.000100291
2017-10-10T11:53:22.457352: step 2381, loss 0.137028, acc 0.953125, learning_rate 0.00010029
2017-10-10T11:53:22.683159: step 2382, loss 0.093857, acc 0.96875, learning_rate 0.000100289
2017-10-10T11:53:22.951790: step 2383, loss 0.11585, acc 0.984375, learning_rate 0.000100288
2017-10-10T11:53:23.206132: step 2384, loss 0.12533, acc 0.953125, learning_rate 0.000100287
2017-10-10T11:53:23.469988: step 2385, loss 0.0620629, acc 1, learning_rate 0.000100285
2017-10-10T11:53:23.733273: step 2386, loss 0.0552755, acc 0.984375, learning_rate 0.000100284
2017-10-10T11:53:23.938722: step 2387, loss 0.216946, acc 0.953125, learning_rate 0.000100283
2017-10-10T11:53:24.193627: step 2388, loss 0.115954, acc 0.9375, learning_rate 0.000100282
2017-10-10T11:53:24.444909: step 2389, loss 0.0970957, acc 0.96875, learning_rate 0.000100281
2017-10-10T11:53:24.671439: step 2390, loss 0.212511, acc 0.90625, learning_rate 0.00010028
2017-10-10T11:53:24.923536: step 2391, loss 0.11191, acc 0.953125, learning_rate 0.000100278
2017-10-10T11:53:25.132847: step 2392, loss 0.108342, acc 0.96875, learning_rate 0.000100277
2017-10-10T11:53:25.361445: step 2393, loss 0.103108, acc 0.9375, learning_rate 0.000100276
2017-10-10T11:53:25.620721: step 2394, loss 0.0726331, acc 0.984375, learning_rate 0.000100275
2017-10-10T11:53:25.822648: step 2395, loss 0.0744136, acc 0.984375, learning_rate 0.000100274
2017-10-10T11:53:26.046975: step 2396, loss 0.131904, acc 0.921875, learning_rate 0.000100273
2017-10-10T11:53:26.277111: step 2397, loss 0.0901207, acc 1, learning_rate 0.000100272
2017-10-10T11:53:26.474682: step 2398, loss 0.146087, acc 0.9375, learning_rate 0.000100271
2017-10-10T11:53:26.718727: step 2399, loss 0.162642, acc 0.9375, learning_rate 0.00010027
2017-10-10T11:53:26.939883: step 2400, loss 0.101311, acc 0.984375, learning_rate 0.000100268

Evaluation:
2017-10-10T11:53:27.436973: step 2400, loss 0.225189, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2400

2017-10-10T11:53:28.551428: step 2401, loss 0.109419, acc 0.96875, learning_rate 0.000100267
2017-10-10T11:53:28.802122: step 2402, loss 0.0919271, acc 0.96875, learning_rate 0.000100266
2017-10-10T11:53:29.059433: step 2403, loss 0.0745115, acc 0.96875, learning_rate 0.000100265
2017-10-10T11:53:29.361094: step 2404, loss 0.157446, acc 0.9375, learning_rate 0.000100264
2017-10-10T11:53:29.553868: step 2405, loss 0.0625357, acc 0.96875, learning_rate 0.000100263
2017-10-10T11:53:29.754355: step 2406, loss 0.148016, acc 0.953125, learning_rate 0.000100262
2017-10-10T11:53:29.957657: step 2407, loss 0.101709, acc 0.984375, learning_rate 0.000100261
2017-10-10T11:53:30.147244: step 2408, loss 0.155108, acc 0.953125, learning_rate 0.00010026
2017-10-10T11:53:30.408873: step 2409, loss 0.119184, acc 0.953125, learning_rate 0.000100259
2017-10-10T11:53:30.720278: step 2410, loss 0.110338, acc 0.953125, learning_rate 0.000100258
2017-10-10T11:53:30.870513: step 2411, loss 0.193154, acc 0.90625, learning_rate 0.000100257
2017-10-10T11:53:31.057113: step 2412, loss 0.140363, acc 0.953125, learning_rate 0.000100256
2017-10-10T11:53:31.233501: step 2413, loss 0.248826, acc 0.9375, learning_rate 0.000100255
2017-10-10T11:53:31.419637: step 2414, loss 0.0815559, acc 0.984375, learning_rate 0.000100253
2017-10-10T11:53:31.596883: step 2415, loss 0.0657458, acc 0.984375, learning_rate 0.000100252
2017-10-10T11:53:31.836867: step 2416, loss 0.215496, acc 0.921875, learning_rate 0.000100251
2017-10-10T11:53:32.087612: step 2417, loss 0.137059, acc 0.953125, learning_rate 0.00010025
2017-10-10T11:53:32.330219: step 2418, loss 0.179928, acc 0.9375, learning_rate 0.000100249
2017-10-10T11:53:32.547553: step 2419, loss 0.131172, acc 0.9375, learning_rate 0.000100248
2017-10-10T11:53:32.817363: step 2420, loss 0.131234, acc 0.953125, learning_rate 0.000100247
2017-10-10T11:53:33.052481: step 2421, loss 0.0729569, acc 0.984375, learning_rate 0.000100246
2017-10-10T11:53:33.278110: step 2422, loss 0.0963981, acc 0.96875, learning_rate 0.000100245
2017-10-10T11:53:33.488909: step 2423, loss 0.237317, acc 0.921875, learning_rate 0.000100244
2017-10-10T11:53:33.701031: step 2424, loss 0.118529, acc 0.96875, learning_rate 0.000100243
2017-10-10T11:53:33.941309: step 2425, loss 0.188625, acc 0.953125, learning_rate 0.000100242
2017-10-10T11:53:34.186788: step 2426, loss 0.11041, acc 0.953125, learning_rate 0.000100241
2017-10-10T11:53:34.421179: step 2427, loss 0.162809, acc 0.953125, learning_rate 0.00010024
2017-10-10T11:53:34.636686: step 2428, loss 0.136857, acc 0.9375, learning_rate 0.000100239
2017-10-10T11:53:34.874064: step 2429, loss 0.184205, acc 0.921875, learning_rate 0.000100238
2017-10-10T11:53:35.100974: step 2430, loss 0.0965452, acc 0.953125, learning_rate 0.000100237
2017-10-10T11:53:35.308922: step 2431, loss 0.0887081, acc 0.984375, learning_rate 0.000100236
2017-10-10T11:53:35.540872: step 2432, loss 0.0938814, acc 0.984375, learning_rate 0.000100235
2017-10-10T11:53:35.751221: step 2433, loss 0.0725421, acc 0.984375, learning_rate 0.000100235
2017-10-10T11:53:36.000475: step 2434, loss 0.102266, acc 0.96875, learning_rate 0.000100234
2017-10-10T11:53:36.238224: step 2435, loss 0.0975869, acc 0.984375, learning_rate 0.000100233
2017-10-10T11:53:36.488900: step 2436, loss 0.109839, acc 0.96875, learning_rate 0.000100232
2017-10-10T11:53:36.750734: step 2437, loss 0.0995665, acc 0.9375, learning_rate 0.000100231
2017-10-10T11:53:36.996001: step 2438, loss 0.0967755, acc 0.96875, learning_rate 0.00010023
2017-10-10T11:53:37.243789: step 2439, loss 0.1039, acc 0.96875, learning_rate 0.000100229
2017-10-10T11:53:37.453482: step 2440, loss 0.105087, acc 0.984375, learning_rate 0.000100228

Evaluation:
2017-10-10T11:53:37.955545: step 2440, loss 0.227348, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2440

2017-10-10T11:53:39.134455: step 2441, loss 0.129834, acc 0.9375, learning_rate 0.000100227
2017-10-10T11:53:39.394123: step 2442, loss 0.168654, acc 0.953125, learning_rate 0.000100226
2017-10-10T11:53:39.595746: step 2443, loss 0.0591364, acc 1, learning_rate 0.000100225
2017-10-10T11:53:39.838605: step 2444, loss 0.154117, acc 0.953125, learning_rate 0.000100224
2017-10-10T11:53:40.111688: step 2445, loss 0.156031, acc 0.9375, learning_rate 0.000100223
2017-10-10T11:53:40.364814: step 2446, loss 0.122498, acc 0.953125, learning_rate 0.000100222
2017-10-10T11:53:40.629570: step 2447, loss 0.173547, acc 0.921875, learning_rate 0.000100221
2017-10-10T11:53:40.866534: step 2448, loss 0.163394, acc 0.9375, learning_rate 0.000100221
2017-10-10T11:53:41.095249: step 2449, loss 0.177408, acc 0.9375, learning_rate 0.00010022
2017-10-10T11:53:41.281019: step 2450, loss 0.1208, acc 0.980392, learning_rate 0.000100219
2017-10-10T11:53:41.613277: step 2451, loss 0.133559, acc 0.953125, learning_rate 0.000100218
2017-10-10T11:53:41.812650: step 2452, loss 0.145727, acc 0.9375, learning_rate 0.000100217
2017-10-10T11:53:41.970652: step 2453, loss 0.205002, acc 0.921875, learning_rate 0.000100216
2017-10-10T11:53:42.128847: step 2454, loss 0.0606898, acc 1, learning_rate 0.000100215
2017-10-10T11:53:42.298890: step 2455, loss 0.112579, acc 0.96875, learning_rate 0.000100214
2017-10-10T11:53:42.464526: step 2456, loss 0.0907165, acc 0.96875, learning_rate 0.000100213
2017-10-10T11:53:42.622947: step 2457, loss 0.136093, acc 0.96875, learning_rate 0.000100213
2017-10-10T11:53:42.805101: step 2458, loss 0.0776337, acc 0.96875, learning_rate 0.000100212
2017-10-10T11:53:43.032632: step 2459, loss 0.107966, acc 0.96875, learning_rate 0.000100211
2017-10-10T11:53:43.260885: step 2460, loss 0.181747, acc 0.953125, learning_rate 0.00010021
2017-10-10T11:53:43.500722: step 2461, loss 0.155523, acc 0.953125, learning_rate 0.000100209
2017-10-10T11:53:43.734395: step 2462, loss 0.106456, acc 0.984375, learning_rate 0.000100208
2017-10-10T11:53:43.984493: step 2463, loss 0.0716185, acc 0.984375, learning_rate 0.000100207
2017-10-10T11:53:44.249908: step 2464, loss 0.147544, acc 0.9375, learning_rate 0.000100207
2017-10-10T11:53:44.486468: step 2465, loss 0.153428, acc 0.953125, learning_rate 0.000100206
2017-10-10T11:53:44.735180: step 2466, loss 0.154506, acc 0.9375, learning_rate 0.000100205
2017-10-10T11:53:44.968349: step 2467, loss 0.09784, acc 0.953125, learning_rate 0.000100204
2017-10-10T11:53:45.169438: step 2468, loss 0.106332, acc 0.953125, learning_rate 0.000100203
2017-10-10T11:53:45.374287: step 2469, loss 0.058616, acc 0.984375, learning_rate 0.000100202
2017-10-10T11:53:45.607262: step 2470, loss 0.395939, acc 0.84375, learning_rate 0.000100202
2017-10-10T11:53:45.932727: step 2471, loss 0.0885271, acc 0.984375, learning_rate 0.000100201
2017-10-10T11:53:46.089030: step 2472, loss 0.143147, acc 0.953125, learning_rate 0.0001002
2017-10-10T11:53:46.265209: step 2473, loss 0.176848, acc 0.890625, learning_rate 0.000100199
2017-10-10T11:53:46.449685: step 2474, loss 0.102955, acc 0.984375, learning_rate 0.000100198
2017-10-10T11:53:46.640911: step 2475, loss 0.157314, acc 0.953125, learning_rate 0.000100198
2017-10-10T11:53:46.834521: step 2476, loss 0.0769155, acc 0.96875, learning_rate 0.000100197
2017-10-10T11:53:47.030732: step 2477, loss 0.226746, acc 0.90625, learning_rate 0.000100196
2017-10-10T11:53:47.221001: step 2478, loss 0.159696, acc 0.9375, learning_rate 0.000100195
2017-10-10T11:53:47.448911: step 2479, loss 0.116742, acc 0.9375, learning_rate 0.000100194
2017-10-10T11:53:47.641000: step 2480, loss 0.0923517, acc 0.96875, learning_rate 0.000100194

Evaluation:
2017-10-10T11:53:48.130540: step 2480, loss 0.224986, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2480

2017-10-10T11:53:49.174053: step 2481, loss 0.147244, acc 0.9375, learning_rate 0.000100193
2017-10-10T11:53:49.398201: step 2482, loss 0.171447, acc 0.9375, learning_rate 0.000100192
2017-10-10T11:53:49.626385: step 2483, loss 0.206538, acc 0.921875, learning_rate 0.000100191
2017-10-10T11:53:49.885702: step 2484, loss 0.0818911, acc 0.96875, learning_rate 0.00010019
2017-10-10T11:53:50.102028: step 2485, loss 0.0940434, acc 0.984375, learning_rate 0.00010019
2017-10-10T11:53:50.301525: step 2486, loss 0.199831, acc 0.921875, learning_rate 0.000100189
2017-10-10T11:53:50.519954: step 2487, loss 0.139435, acc 0.953125, learning_rate 0.000100188
2017-10-10T11:53:50.709342: step 2488, loss 0.167851, acc 0.953125, learning_rate 0.000100187
2017-10-10T11:53:50.985401: step 2489, loss 0.106916, acc 0.96875, learning_rate 0.000100187
2017-10-10T11:53:51.223135: step 2490, loss 0.200892, acc 0.9375, learning_rate 0.000100186
2017-10-10T11:53:51.485554: step 2491, loss 0.127028, acc 0.96875, learning_rate 0.000100185
2017-10-10T11:53:51.725026: step 2492, loss 0.137977, acc 0.953125, learning_rate 0.000100184
2017-10-10T11:53:51.964827: step 2493, loss 0.0930312, acc 0.984375, learning_rate 0.000100183
2017-10-10T11:53:52.220879: step 2494, loss 0.24258, acc 0.890625, learning_rate 0.000100183
2017-10-10T11:53:52.446711: step 2495, loss 0.127742, acc 0.9375, learning_rate 0.000100182
2017-10-10T11:53:52.784456: step 2496, loss 0.114372, acc 0.9375, learning_rate 0.000100181
2017-10-10T11:53:53.063400: step 2497, loss 0.061985, acc 0.984375, learning_rate 0.000100181
2017-10-10T11:53:53.236871: step 2498, loss 0.252438, acc 0.9375, learning_rate 0.00010018
2017-10-10T11:53:53.418702: step 2499, loss 0.0932184, acc 0.96875, learning_rate 0.000100179
2017-10-10T11:53:53.596495: step 2500, loss 0.285862, acc 0.9375, learning_rate 0.000100178
2017-10-10T11:53:53.765183: step 2501, loss 0.192556, acc 0.9375, learning_rate 0.000100178
2017-10-10T11:53:54.016809: step 2502, loss 0.077657, acc 0.984375, learning_rate 0.000100177
2017-10-10T11:53:54.269272: step 2503, loss 0.122263, acc 0.953125, learning_rate 0.000100176
2017-10-10T11:53:54.568831: step 2504, loss 0.120631, acc 0.96875, learning_rate 0.000100175
2017-10-10T11:53:54.779987: step 2505, loss 0.0853471, acc 0.96875, learning_rate 0.000100175
2017-10-10T11:53:54.969772: step 2506, loss 0.103942, acc 0.9375, learning_rate 0.000100174
2017-10-10T11:53:55.169967: step 2507, loss 0.119923, acc 0.96875, learning_rate 0.000100173
2017-10-10T11:53:55.378542: step 2508, loss 0.163839, acc 0.9375, learning_rate 0.000100173
2017-10-10T11:53:55.595926: step 2509, loss 0.252334, acc 0.90625, learning_rate 0.000100172
2017-10-10T11:53:55.837375: step 2510, loss 0.0444647, acc 1, learning_rate 0.000100171
2017-10-10T11:53:56.098349: step 2511, loss 0.168423, acc 0.921875, learning_rate 0.00010017
2017-10-10T11:53:56.340404: step 2512, loss 0.0667301, acc 0.984375, learning_rate 0.00010017
2017-10-10T11:53:56.555180: step 2513, loss 0.0873763, acc 0.953125, learning_rate 0.000100169
2017-10-10T11:53:56.760100: step 2514, loss 0.122178, acc 0.96875, learning_rate 0.000100168
2017-10-10T11:53:56.952749: step 2515, loss 0.0846766, acc 0.984375, learning_rate 0.000100168
2017-10-10T11:53:57.193982: step 2516, loss 0.139335, acc 0.9375, learning_rate 0.000100167
2017-10-10T11:53:57.384863: step 2517, loss 0.173032, acc 0.984375, learning_rate 0.000100166
2017-10-10T11:53:57.621792: step 2518, loss 0.206531, acc 0.9375, learning_rate 0.000100166
2017-10-10T11:53:57.842143: step 2519, loss 0.107715, acc 0.953125, learning_rate 0.000100165
2017-10-10T11:53:58.075793: step 2520, loss 0.0822253, acc 0.96875, learning_rate 0.000100164

Evaluation:
2017-10-10T11:53:58.569017: step 2520, loss 0.224134, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2520

2017-10-10T11:53:59.693509: step 2521, loss 0.0815059, acc 0.96875, learning_rate 0.000100164
2017-10-10T11:53:59.917627: step 2522, loss 0.148908, acc 0.921875, learning_rate 0.000100163
2017-10-10T11:54:00.184648: step 2523, loss 0.114135, acc 0.9375, learning_rate 0.000100162
2017-10-10T11:54:00.440470: step 2524, loss 0.109571, acc 0.953125, learning_rate 0.000100162
2017-10-10T11:54:00.678006: step 2525, loss 0.134088, acc 0.9375, learning_rate 0.000100161
2017-10-10T11:54:00.880981: step 2526, loss 0.0726791, acc 0.984375, learning_rate 0.00010016
2017-10-10T11:54:01.075803: step 2527, loss 0.133423, acc 0.96875, learning_rate 0.00010016
2017-10-10T11:54:01.276805: step 2528, loss 0.0632173, acc 0.984375, learning_rate 0.000100159
2017-10-10T11:54:01.509018: step 2529, loss 0.0782057, acc 0.984375, learning_rate 0.000100158
2017-10-10T11:54:01.739043: step 2530, loss 0.0802754, acc 0.984375, learning_rate 0.000100158
2017-10-10T11:54:01.981745: step 2531, loss 0.119875, acc 0.96875, learning_rate 0.000100157
2017-10-10T11:54:02.224949: step 2532, loss 0.067653, acc 0.96875, learning_rate 0.000100156
2017-10-10T11:54:02.465838: step 2533, loss 0.104645, acc 0.953125, learning_rate 0.000100156
2017-10-10T11:54:02.779264: step 2534, loss 0.0594928, acc 0.984375, learning_rate 0.000100155
2017-10-10T11:54:02.917152: step 2535, loss 0.184886, acc 0.953125, learning_rate 0.000100155
2017-10-10T11:54:03.094101: step 2536, loss 0.168721, acc 0.953125, learning_rate 0.000100154
2017-10-10T11:54:03.270179: step 2537, loss 0.164936, acc 0.9375, learning_rate 0.000100153
2017-10-10T11:54:03.468378: step 2538, loss 0.0974457, acc 0.953125, learning_rate 0.000100153
2017-10-10T11:54:03.732947: step 2539, loss 0.0615986, acc 0.984375, learning_rate 0.000100152
2017-10-10T11:54:04.014503: step 2540, loss 0.180944, acc 0.9375, learning_rate 0.000100151
2017-10-10T11:54:04.202646: step 2541, loss 0.172004, acc 0.953125, learning_rate 0.000100151
2017-10-10T11:54:04.364281: step 2542, loss 0.0820361, acc 0.984375, learning_rate 0.00010015
2017-10-10T11:54:04.547620: step 2543, loss 0.163038, acc 0.9375, learning_rate 0.00010015
2017-10-10T11:54:04.743364: step 2544, loss 0.118986, acc 0.96875, learning_rate 0.000100149
2017-10-10T11:54:04.930218: step 2545, loss 0.227679, acc 0.921875, learning_rate 0.000100148
2017-10-10T11:54:05.092161: step 2546, loss 0.287779, acc 0.90625, learning_rate 0.000100148
2017-10-10T11:54:05.340888: step 2547, loss 0.102354, acc 0.96875, learning_rate 0.000100147
2017-10-10T11:54:05.505066: step 2548, loss 0.0800476, acc 0.980392, learning_rate 0.000100147
2017-10-10T11:54:05.740603: step 2549, loss 0.167766, acc 0.9375, learning_rate 0.000100146
2017-10-10T11:54:05.949029: step 2550, loss 0.216399, acc 0.921875, learning_rate 0.000100145
2017-10-10T11:54:06.172866: step 2551, loss 0.120434, acc 0.953125, learning_rate 0.000100145
2017-10-10T11:54:06.399607: step 2552, loss 0.0733517, acc 0.984375, learning_rate 0.000100144
2017-10-10T11:54:06.661153: step 2553, loss 0.114436, acc 0.953125, learning_rate 0.000100144
2017-10-10T11:54:06.889482: step 2554, loss 0.193812, acc 0.921875, learning_rate 0.000100143
2017-10-10T11:54:07.121025: step 2555, loss 0.0940308, acc 0.96875, learning_rate 0.000100142
2017-10-10T11:54:07.356932: step 2556, loss 0.14192, acc 0.953125, learning_rate 0.000100142
2017-10-10T11:54:07.610259: step 2557, loss 0.168818, acc 0.90625, learning_rate 0.000100141
2017-10-10T11:54:07.852576: step 2558, loss 0.108129, acc 0.953125, learning_rate 0.000100141
2017-10-10T11:54:08.076188: step 2559, loss 0.045651, acc 1, learning_rate 0.00010014
2017-10-10T11:54:08.319469: step 2560, loss 0.135256, acc 0.984375, learning_rate 0.00010014

Evaluation:
2017-10-10T11:54:08.870789: step 2560, loss 0.225094, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2560

2017-10-10T11:54:10.103376: step 2561, loss 0.156557, acc 0.953125, learning_rate 0.000100139
2017-10-10T11:54:10.336858: step 2562, loss 0.0486529, acc 0.984375, learning_rate 0.000100138
2017-10-10T11:54:10.566302: step 2563, loss 0.104437, acc 0.96875, learning_rate 0.000100138
2017-10-10T11:54:10.868694: step 2564, loss 0.119422, acc 0.96875, learning_rate 0.000100137
2017-10-10T11:54:11.044861: step 2565, loss 0.0368297, acc 1, learning_rate 0.000100137
2017-10-10T11:54:11.220837: step 2566, loss 0.253355, acc 0.921875, learning_rate 0.000100136
2017-10-10T11:54:11.390070: step 2567, loss 0.122096, acc 0.96875, learning_rate 0.000100136
2017-10-10T11:54:11.559944: step 2568, loss 0.179624, acc 0.9375, learning_rate 0.000100135
2017-10-10T11:54:11.766097: step 2569, loss 0.149969, acc 0.921875, learning_rate 0.000100134
2017-10-10T11:54:12.000469: step 2570, loss 0.0725651, acc 1, learning_rate 0.000100134
2017-10-10T11:54:12.226762: step 2571, loss 0.0690328, acc 0.984375, learning_rate 0.000100133
2017-10-10T11:54:12.468882: step 2572, loss 0.136582, acc 0.953125, learning_rate 0.000100133
2017-10-10T11:54:12.744825: step 2573, loss 0.183948, acc 0.9375, learning_rate 0.000100132
2017-10-10T11:54:12.988842: step 2574, loss 0.0924391, acc 0.96875, learning_rate 0.000100132
2017-10-10T11:54:13.193908: step 2575, loss 0.0999603, acc 0.953125, learning_rate 0.000100131
2017-10-10T11:54:13.417212: step 2576, loss 0.12277, acc 0.96875, learning_rate 0.000100131
2017-10-10T11:54:13.621289: step 2577, loss 0.121297, acc 0.96875, learning_rate 0.00010013
2017-10-10T11:54:13.858069: step 2578, loss 0.131963, acc 0.96875, learning_rate 0.00010013
2017-10-10T11:54:14.113257: step 2579, loss 0.145726, acc 0.9375, learning_rate 0.000100129
2017-10-10T11:54:14.358312: step 2580, loss 0.090157, acc 0.984375, learning_rate 0.000100129
2017-10-10T11:54:14.615630: step 2581, loss 0.240156, acc 0.921875, learning_rate 0.000100128
2017-10-10T11:54:14.870065: step 2582, loss 0.136856, acc 0.96875, learning_rate 0.000100128
2017-10-10T11:54:15.167709: step 2583, loss 0.273112, acc 0.90625, learning_rate 0.000100127
2017-10-10T11:54:15.463505: step 2584, loss 0.123208, acc 0.953125, learning_rate 0.000100126
2017-10-10T11:54:15.640331: step 2585, loss 0.0943415, acc 0.96875, learning_rate 0.000100126
2017-10-10T11:54:15.816917: step 2586, loss 0.167042, acc 0.921875, learning_rate 0.000100125
2017-10-10T11:54:16.011843: step 2587, loss 0.0551507, acc 1, learning_rate 0.000100125
2017-10-10T11:54:16.184149: step 2588, loss 0.0923911, acc 0.984375, learning_rate 0.000100124
2017-10-10T11:54:16.360924: step 2589, loss 0.161306, acc 0.953125, learning_rate 0.000100124
2017-10-10T11:54:16.510421: step 2590, loss 0.108577, acc 0.96875, learning_rate 0.000100123
2017-10-10T11:54:16.752951: step 2591, loss 0.170134, acc 0.953125, learning_rate 0.000100123
2017-10-10T11:54:16.968580: step 2592, loss 0.0655283, acc 0.984375, learning_rate 0.000100122
2017-10-10T11:54:17.199748: step 2593, loss 0.108818, acc 0.96875, learning_rate 0.000100122
2017-10-10T11:54:17.427881: step 2594, loss 0.115296, acc 0.96875, learning_rate 0.000100121
2017-10-10T11:54:17.624979: step 2595, loss 0.131265, acc 0.953125, learning_rate 0.000100121
2017-10-10T11:54:17.887394: step 2596, loss 0.129434, acc 0.953125, learning_rate 0.00010012
2017-10-10T11:54:18.116578: step 2597, loss 0.0918416, acc 0.96875, learning_rate 0.00010012
2017-10-10T11:54:18.383080: step 2598, loss 0.0918229, acc 0.984375, learning_rate 0.000100119
2017-10-10T11:54:18.627740: step 2599, loss 0.0947131, acc 0.96875, learning_rate 0.000100119
2017-10-10T11:54:18.863734: step 2600, loss 0.0411514, acc 1, learning_rate 0.000100118

Evaluation:
2017-10-10T11:54:19.424866: step 2600, loss 0.224723, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2600

2017-10-10T11:54:20.320588: step 2601, loss 0.119741, acc 0.9375, learning_rate 0.000100118
2017-10-10T11:54:20.561414: step 2602, loss 0.0579153, acc 1, learning_rate 0.000100117
2017-10-10T11:54:20.800847: step 2603, loss 0.087124, acc 0.984375, learning_rate 0.000100117
2017-10-10T11:54:21.049089: step 2604, loss 0.158798, acc 0.9375, learning_rate 0.000100117
2017-10-10T11:54:21.289070: step 2605, loss 0.115896, acc 0.96875, learning_rate 0.000100116
2017-10-10T11:54:21.534009: step 2606, loss 0.110018, acc 0.96875, learning_rate 0.000100116
2017-10-10T11:54:21.752927: step 2607, loss 0.179533, acc 0.921875, learning_rate 0.000100115
2017-10-10T11:54:22.007303: step 2608, loss 0.19068, acc 0.9375, learning_rate 0.000100115
2017-10-10T11:54:22.256232: step 2609, loss 0.0960616, acc 0.96875, learning_rate 0.000100114
2017-10-10T11:54:22.456850: step 2610, loss 0.0569798, acc 0.96875, learning_rate 0.000100114
2017-10-10T11:54:22.656864: step 2611, loss 0.118872, acc 0.953125, learning_rate 0.000100113
2017-10-10T11:54:22.906408: step 2612, loss 0.116543, acc 0.96875, learning_rate 0.000100113
2017-10-10T11:54:23.201393: step 2613, loss 0.219608, acc 0.90625, learning_rate 0.000100112
2017-10-10T11:54:23.432537: step 2614, loss 0.175622, acc 0.9375, learning_rate 0.000100112
2017-10-10T11:54:23.695715: step 2615, loss 0.197771, acc 0.921875, learning_rate 0.000100111
2017-10-10T11:54:23.939618: step 2616, loss 0.162665, acc 0.9375, learning_rate 0.000100111
2017-10-10T11:54:24.169974: step 2617, loss 0.0869027, acc 0.953125, learning_rate 0.000100111
2017-10-10T11:54:24.404842: step 2618, loss 0.113202, acc 0.9375, learning_rate 0.00010011
2017-10-10T11:54:24.623051: step 2619, loss 0.20605, acc 0.9375, learning_rate 0.00010011
2017-10-10T11:54:24.838102: step 2620, loss 0.089948, acc 0.96875, learning_rate 0.000100109
2017-10-10T11:54:25.112675: step 2621, loss 0.124719, acc 0.96875, learning_rate 0.000100109
2017-10-10T11:54:25.344584: step 2622, loss 0.14205, acc 0.953125, learning_rate 0.000100108
2017-10-10T11:54:25.577017: step 2623, loss 0.197897, acc 0.90625, learning_rate 0.000100108
2017-10-10T11:54:25.828918: step 2624, loss 0.100098, acc 0.984375, learning_rate 0.000100107
2017-10-10T11:54:26.076259: step 2625, loss 0.0644899, acc 0.984375, learning_rate 0.000100107
2017-10-10T11:54:26.348874: step 2626, loss 0.0460799, acc 1, learning_rate 0.000100107
2017-10-10T11:54:26.659901: step 2627, loss 0.0967209, acc 0.984375, learning_rate 0.000100106
2017-10-10T11:54:26.831750: step 2628, loss 0.101414, acc 0.953125, learning_rate 0.000100106
2017-10-10T11:54:26.987579: step 2629, loss 0.159431, acc 0.921875, learning_rate 0.000100105
2017-10-10T11:54:27.168925: step 2630, loss 0.312406, acc 0.890625, learning_rate 0.000100105
2017-10-10T11:54:27.352894: step 2631, loss 0.139844, acc 0.96875, learning_rate 0.000100104
2017-10-10T11:54:27.614463: step 2632, loss 0.17196, acc 0.9375, learning_rate 0.000100104
2017-10-10T11:54:27.795831: step 2633, loss 0.121317, acc 0.953125, learning_rate 0.000100104
2017-10-10T11:54:27.957557: step 2634, loss 0.123586, acc 0.953125, learning_rate 0.000100103
2017-10-10T11:54:28.160369: step 2635, loss 0.0893307, acc 0.96875, learning_rate 0.000100103
2017-10-10T11:54:28.324800: step 2636, loss 0.144501, acc 0.9375, learning_rate 0.000100102
2017-10-10T11:54:28.608950: step 2637, loss 0.0502644, acc 0.984375, learning_rate 0.000100102
2017-10-10T11:54:28.850996: step 2638, loss 0.0656276, acc 0.984375, learning_rate 0.000100101
2017-10-10T11:54:29.065998: step 2639, loss 0.186299, acc 0.953125, learning_rate 0.000100101
2017-10-10T11:54:29.300723: step 2640, loss 0.147077, acc 0.9375, learning_rate 0.000100101

Evaluation:
2017-10-10T11:54:29.827802: step 2640, loss 0.222774, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2640

2017-10-10T11:54:30.991354: step 2641, loss 0.115489, acc 0.96875, learning_rate 0.0001001
2017-10-10T11:54:31.216871: step 2642, loss 0.0768619, acc 0.96875, learning_rate 0.0001001
2017-10-10T11:54:31.448852: step 2643, loss 0.105678, acc 0.9375, learning_rate 0.000100099
2017-10-10T11:54:31.708870: step 2644, loss 0.13824, acc 0.96875, learning_rate 0.000100099
2017-10-10T11:54:31.925062: step 2645, loss 0.0741133, acc 0.96875, learning_rate 0.000100099
2017-10-10T11:54:32.104866: step 2646, loss 0.0619917, acc 0.980392, learning_rate 0.000100098
2017-10-10T11:54:32.342359: step 2647, loss 0.171075, acc 0.890625, learning_rate 0.000100098
2017-10-10T11:54:32.566130: step 2648, loss 0.063889, acc 0.96875, learning_rate 0.000100097
2017-10-10T11:54:32.834739: step 2649, loss 0.160353, acc 0.9375, learning_rate 0.000100097
2017-10-10T11:54:33.068584: step 2650, loss 0.134915, acc 0.96875, learning_rate 0.000100097
2017-10-10T11:54:33.330892: step 2651, loss 0.118512, acc 0.953125, learning_rate 0.000100096
2017-10-10T11:54:33.591157: step 2652, loss 0.258736, acc 0.90625, learning_rate 0.000100096
2017-10-10T11:54:33.819113: step 2653, loss 0.12805, acc 0.96875, learning_rate 0.000100095
2017-10-10T11:54:34.033177: step 2654, loss 0.0669263, acc 0.984375, learning_rate 0.000100095
2017-10-10T11:54:34.270028: step 2655, loss 0.14364, acc 0.953125, learning_rate 0.000100095
2017-10-10T11:54:34.525612: step 2656, loss 0.129672, acc 0.9375, learning_rate 0.000100094
2017-10-10T11:54:34.740826: step 2657, loss 0.0908274, acc 0.96875, learning_rate 0.000100094
2017-10-10T11:54:34.962783: step 2658, loss 0.336459, acc 0.9375, learning_rate 0.000100093
2017-10-10T11:54:35.193795: step 2659, loss 0.05884, acc 0.984375, learning_rate 0.000100093
2017-10-10T11:54:35.464364: step 2660, loss 0.0839283, acc 0.984375, learning_rate 0.000100093
2017-10-10T11:54:35.749132: step 2661, loss 0.142132, acc 0.953125, learning_rate 0.000100092
2017-10-10T11:54:35.991787: step 2662, loss 0.120136, acc 0.953125, learning_rate 0.000100092
2017-10-10T11:54:36.204715: step 2663, loss 0.117436, acc 0.96875, learning_rate 0.000100092
2017-10-10T11:54:36.399176: step 2664, loss 0.242618, acc 0.90625, learning_rate 0.000100091
2017-10-10T11:54:36.600034: step 2665, loss 0.116209, acc 0.90625, learning_rate 0.000100091
2017-10-10T11:54:36.776120: step 2666, loss 0.170459, acc 0.921875, learning_rate 0.00010009
2017-10-10T11:54:37.019861: step 2667, loss 0.0916439, acc 0.984375, learning_rate 0.00010009
2017-10-10T11:54:37.348303: step 2668, loss 0.209812, acc 0.90625, learning_rate 0.00010009
2017-10-10T11:54:37.539149: step 2669, loss 0.107002, acc 0.984375, learning_rate 0.000100089
2017-10-10T11:54:37.714389: step 2670, loss 0.124892, acc 0.953125, learning_rate 0.000100089
2017-10-10T11:54:37.917748: step 2671, loss 0.153753, acc 0.953125, learning_rate 0.000100089
2017-10-10T11:54:38.093908: step 2672, loss 0.095672, acc 0.96875, learning_rate 0.000100088
2017-10-10T11:54:38.253471: step 2673, loss 0.141528, acc 0.9375, learning_rate 0.000100088
2017-10-10T11:54:38.460963: step 2674, loss 0.184452, acc 0.953125, learning_rate 0.000100088
2017-10-10T11:54:38.693137: step 2675, loss 0.0881545, acc 0.96875, learning_rate 0.000100087
2017-10-10T11:54:38.907870: step 2676, loss 0.204259, acc 0.953125, learning_rate 0.000100087
2017-10-10T11:54:39.132907: step 2677, loss 0.162654, acc 0.9375, learning_rate 0.000100086
2017-10-10T11:54:39.376884: step 2678, loss 0.119879, acc 0.953125, learning_rate 0.000100086
2017-10-10T11:54:39.623297: step 2679, loss 0.0679269, acc 0.96875, learning_rate 0.000100086
2017-10-10T11:54:39.856990: step 2680, loss 0.0797703, acc 0.984375, learning_rate 0.000100085

Evaluation:
2017-10-10T11:54:40.358459: step 2680, loss 0.222127, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2680

2017-10-10T11:54:41.646659: step 2681, loss 0.119468, acc 0.984375, learning_rate 0.000100085
2017-10-10T11:54:41.889590: step 2682, loss 0.133479, acc 0.9375, learning_rate 0.000100085
2017-10-10T11:54:42.133513: step 2683, loss 0.139858, acc 0.9375, learning_rate 0.000100084
2017-10-10T11:54:42.359274: step 2684, loss 0.0729431, acc 1, learning_rate 0.000100084
2017-10-10T11:54:42.629988: step 2685, loss 0.22314, acc 0.921875, learning_rate 0.000100084
2017-10-10T11:54:42.863537: step 2686, loss 0.111796, acc 0.984375, learning_rate 0.000100083
2017-10-10T11:54:43.084538: step 2687, loss 0.115713, acc 0.9375, learning_rate 0.000100083
2017-10-10T11:54:43.282709: step 2688, loss 0.0852441, acc 0.96875, learning_rate 0.000100083
2017-10-10T11:54:43.503957: step 2689, loss 0.0766296, acc 0.984375, learning_rate 0.000100082
2017-10-10T11:54:43.786565: step 2690, loss 0.136219, acc 0.953125, learning_rate 0.000100082
2017-10-10T11:54:44.015432: step 2691, loss 0.122594, acc 0.96875, learning_rate 0.000100082
2017-10-10T11:54:44.219533: step 2692, loss 0.116253, acc 0.953125, learning_rate 0.000100081
2017-10-10T11:54:44.404854: step 2693, loss 0.164652, acc 0.953125, learning_rate 0.000100081
2017-10-10T11:54:44.589591: step 2694, loss 0.084877, acc 0.984375, learning_rate 0.000100081
2017-10-10T11:54:44.789696: step 2695, loss 0.22924, acc 0.9375, learning_rate 0.00010008
2017-10-10T11:54:45.001077: step 2696, loss 0.0824779, acc 0.984375, learning_rate 0.00010008
2017-10-10T11:54:45.247709: step 2697, loss 0.128279, acc 0.953125, learning_rate 0.00010008
2017-10-10T11:54:45.499812: step 2698, loss 0.244054, acc 0.921875, learning_rate 0.000100079
2017-10-10T11:54:45.773041: step 2699, loss 0.0912223, acc 0.96875, learning_rate 0.000100079
2017-10-10T11:54:46.014964: step 2700, loss 0.0832428, acc 0.96875, learning_rate 0.000100079
2017-10-10T11:54:46.232237: step 2701, loss 0.129524, acc 0.96875, learning_rate 0.000100078
2017-10-10T11:54:46.489086: step 2702, loss 0.121039, acc 0.96875, learning_rate 0.000100078
2017-10-10T11:54:46.718951: step 2703, loss 0.145052, acc 0.984375, learning_rate 0.000100078
2017-10-10T11:54:46.964897: step 2704, loss 0.114876, acc 0.953125, learning_rate 0.000100077
2017-10-10T11:54:47.225634: step 2705, loss 0.212403, acc 0.921875, learning_rate 0.000100077
2017-10-10T11:54:47.441760: step 2706, loss 0.173623, acc 0.921875, learning_rate 0.000100077
2017-10-10T11:54:47.640947: step 2707, loss 0.107663, acc 0.984375, learning_rate 0.000100076
2017-10-10T11:54:47.904132: step 2708, loss 0.125895, acc 0.96875, learning_rate 0.000100076
2017-10-10T11:54:48.157097: step 2709, loss 0.0951341, acc 0.96875, learning_rate 0.000100076
2017-10-10T11:54:48.483679: step 2710, loss 0.034027, acc 1, learning_rate 0.000100076
2017-10-10T11:54:48.669647: step 2711, loss 0.0909212, acc 0.984375, learning_rate 0.000100075
2017-10-10T11:54:48.849939: step 2712, loss 0.130623, acc 0.953125, learning_rate 0.000100075
2017-10-10T11:54:49.048975: step 2713, loss 0.123917, acc 0.953125, learning_rate 0.000100075
2017-10-10T11:54:49.225108: step 2714, loss 0.0833552, acc 0.96875, learning_rate 0.000100074
2017-10-10T11:54:49.396530: step 2715, loss 0.0766859, acc 0.984375, learning_rate 0.000100074
2017-10-10T11:54:49.568858: step 2716, loss 0.119722, acc 0.953125, learning_rate 0.000100074
2017-10-10T11:54:49.751385: step 2717, loss 0.0582683, acc 0.984375, learning_rate 0.000100073
2017-10-10T11:54:50.014550: step 2718, loss 0.0609217, acc 0.984375, learning_rate 0.000100073
2017-10-10T11:54:50.277172: step 2719, loss 0.182039, acc 0.90625, learning_rate 0.000100073
2017-10-10T11:54:50.503389: step 2720, loss 0.0340556, acc 1, learning_rate 0.000100073

Evaluation:
2017-10-10T11:54:51.044910: step 2720, loss 0.22243, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2720

2017-10-10T11:54:52.130483: step 2721, loss 0.121649, acc 0.96875, learning_rate 0.000100072
2017-10-10T11:54:52.317605: step 2722, loss 0.108129, acc 0.96875, learning_rate 0.000100072
2017-10-10T11:54:52.510509: step 2723, loss 0.0855425, acc 0.96875, learning_rate 0.000100072
2017-10-10T11:54:52.700616: step 2724, loss 0.113991, acc 0.953125, learning_rate 0.000100071
2017-10-10T11:54:52.898143: step 2725, loss 0.074454, acc 0.984375, learning_rate 0.000100071
2017-10-10T11:54:53.133876: step 2726, loss 0.172863, acc 0.9375, learning_rate 0.000100071
2017-10-10T11:54:53.369633: step 2727, loss 0.0680806, acc 0.984375, learning_rate 0.00010007
2017-10-10T11:54:53.584854: step 2728, loss 0.0344475, acc 0.984375, learning_rate 0.00010007
2017-10-10T11:54:53.794999: step 2729, loss 0.0891961, acc 1, learning_rate 0.00010007
2017-10-10T11:54:53.999355: step 2730, loss 0.286311, acc 0.90625, learning_rate 0.00010007
2017-10-10T11:54:54.235077: step 2731, loss 0.126726, acc 0.953125, learning_rate 0.000100069
2017-10-10T11:54:54.440883: step 2732, loss 0.117757, acc 0.96875, learning_rate 0.000100069
2017-10-10T11:54:54.668866: step 2733, loss 0.072131, acc 0.96875, learning_rate 0.000100069
2017-10-10T11:54:54.938967: step 2734, loss 0.187672, acc 0.921875, learning_rate 0.000100068
2017-10-10T11:54:55.158725: step 2735, loss 0.0853085, acc 0.984375, learning_rate 0.000100068
2017-10-10T11:54:55.361855: step 2736, loss 0.117499, acc 0.953125, learning_rate 0.000100068
2017-10-10T11:54:55.590923: step 2737, loss 0.0843545, acc 0.953125, learning_rate 0.000100068
2017-10-10T11:54:55.852870: step 2738, loss 0.212627, acc 0.96875, learning_rate 0.000100067
2017-10-10T11:54:56.102592: step 2739, loss 0.107657, acc 0.96875, learning_rate 0.000100067
2017-10-10T11:54:56.325307: step 2740, loss 0.180754, acc 0.90625, learning_rate 0.000100067
2017-10-10T11:54:56.594384: step 2741, loss 0.0939828, acc 0.984375, learning_rate 0.000100067
2017-10-10T11:54:56.848828: step 2742, loss 0.171824, acc 0.953125, learning_rate 0.000100066
2017-10-10T11:54:57.105447: step 2743, loss 0.161628, acc 0.921875, learning_rate 0.000100066
2017-10-10T11:54:57.350868: step 2744, loss 0.1381, acc 0.980392, learning_rate 0.000100066
2017-10-10T11:54:57.568406: step 2745, loss 0.1282, acc 0.96875, learning_rate 0.000100065
2017-10-10T11:54:57.810881: step 2746, loss 0.123606, acc 0.953125, learning_rate 0.000100065
2017-10-10T11:54:58.039239: step 2747, loss 0.139228, acc 0.953125, learning_rate 0.000100065
2017-10-10T11:54:58.234986: step 2748, loss 0.0823746, acc 0.984375, learning_rate 0.000100065
2017-10-10T11:54:58.441120: step 2749, loss 0.063737, acc 0.984375, learning_rate 0.000100064
2017-10-10T11:54:58.657178: step 2750, loss 0.0950616, acc 0.953125, learning_rate 0.000100064
2017-10-10T11:54:58.877264: step 2751, loss 0.125421, acc 0.96875, learning_rate 0.000100064
2017-10-10T11:54:59.093123: step 2752, loss 0.117728, acc 0.953125, learning_rate 0.000100064
2017-10-10T11:54:59.361303: step 2753, loss 0.135498, acc 0.953125, learning_rate 0.000100063
2017-10-10T11:54:59.665019: step 2754, loss 0.164058, acc 0.96875, learning_rate 0.000100063
2017-10-10T11:54:59.948895: step 2755, loss 0.134507, acc 0.953125, learning_rate 0.000100063
2017-10-10T11:55:00.120507: step 2756, loss 0.208122, acc 0.9375, learning_rate 0.000100063
2017-10-10T11:55:00.282704: step 2757, loss 0.0745539, acc 0.984375, learning_rate 0.000100062
2017-10-10T11:55:00.534765: step 2758, loss 0.0797314, acc 0.984375, learning_rate 0.000100062
2017-10-10T11:55:00.657809: step 2759, loss 0.19796, acc 0.90625, learning_rate 0.000100062
2017-10-10T11:55:00.849080: step 2760, loss 0.119743, acc 0.953125, learning_rate 0.000100062

Evaluation:
2017-10-10T11:55:01.309669: step 2760, loss 0.224137, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2760

2017-10-10T11:55:02.403932: step 2761, loss 0.0532531, acc 0.984375, learning_rate 0.000100061
2017-10-10T11:55:02.631540: step 2762, loss 0.0872991, acc 0.953125, learning_rate 0.000100061
2017-10-10T11:55:02.860846: step 2763, loss 0.143005, acc 0.921875, learning_rate 0.000100061
2017-10-10T11:55:03.066054: step 2764, loss 0.106903, acc 0.953125, learning_rate 0.000100061
2017-10-10T11:55:03.309314: step 2765, loss 0.0955267, acc 0.984375, learning_rate 0.00010006
2017-10-10T11:55:03.549376: step 2766, loss 0.0787232, acc 0.96875, learning_rate 0.00010006
2017-10-10T11:55:03.752817: step 2767, loss 0.172697, acc 0.953125, learning_rate 0.00010006
2017-10-10T11:55:03.953510: step 2768, loss 0.237232, acc 0.90625, learning_rate 0.00010006
2017-10-10T11:55:04.169387: step 2769, loss 0.157876, acc 0.9375, learning_rate 0.000100059
2017-10-10T11:55:04.384858: step 2770, loss 0.0819374, acc 1, learning_rate 0.000100059
2017-10-10T11:55:04.616348: step 2771, loss 0.100854, acc 0.96875, learning_rate 0.000100059
2017-10-10T11:55:04.851420: step 2772, loss 0.0327599, acc 1, learning_rate 0.000100059
2017-10-10T11:55:05.078939: step 2773, loss 0.120421, acc 0.96875, learning_rate 0.000100058
2017-10-10T11:55:05.309962: step 2774, loss 0.126992, acc 0.9375, learning_rate 0.000100058
2017-10-10T11:55:05.520189: step 2775, loss 0.0715467, acc 0.984375, learning_rate 0.000100058
2017-10-10T11:55:05.787677: step 2776, loss 0.152863, acc 0.9375, learning_rate 0.000100058
2017-10-10T11:55:06.023464: step 2777, loss 0.130216, acc 0.96875, learning_rate 0.000100057
2017-10-10T11:55:06.256887: step 2778, loss 0.08347, acc 0.984375, learning_rate 0.000100057
2017-10-10T11:55:06.487280: step 2779, loss 0.10923, acc 0.9375, learning_rate 0.000100057
2017-10-10T11:55:06.732880: step 2780, loss 0.0945164, acc 0.96875, learning_rate 0.000100057
2017-10-10T11:55:06.967499: step 2781, loss 0.146125, acc 0.96875, learning_rate 0.000100056
2017-10-10T11:55:07.184920: step 2782, loss 0.127089, acc 0.953125, learning_rate 0.000100056
2017-10-10T11:55:07.437978: step 2783, loss 0.109044, acc 0.9375, learning_rate 0.000100056
2017-10-10T11:55:07.649346: step 2784, loss 0.106569, acc 0.984375, learning_rate 0.000100056
2017-10-10T11:55:07.845048: step 2785, loss 0.0499431, acc 1, learning_rate 0.000100056
2017-10-10T11:55:08.089164: step 2786, loss 0.173145, acc 0.921875, learning_rate 0.000100055
2017-10-10T11:55:08.304852: step 2787, loss 0.127128, acc 0.984375, learning_rate 0.000100055
2017-10-10T11:55:08.527510: step 2788, loss 0.138724, acc 0.96875, learning_rate 0.000100055
2017-10-10T11:55:08.820974: step 2789, loss 0.125581, acc 0.96875, learning_rate 0.000100055
2017-10-10T11:55:09.028865: step 2790, loss 0.108502, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:55:09.214841: step 2791, loss 0.127596, acc 0.9375, learning_rate 0.000100054
2017-10-10T11:55:09.416604: step 2792, loss 0.0515483, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:55:09.624132: step 2793, loss 0.138848, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:55:09.830506: step 2794, loss 0.046247, acc 1, learning_rate 0.000100054
2017-10-10T11:55:10.040867: step 2795, loss 0.0762504, acc 0.984375, learning_rate 0.000100053
2017-10-10T11:55:10.353464: step 2796, loss 0.0835893, acc 0.96875, learning_rate 0.000100053
2017-10-10T11:55:10.524850: step 2797, loss 0.0288265, acc 1, learning_rate 0.000100053
2017-10-10T11:55:10.675524: step 2798, loss 0.159315, acc 0.9375, learning_rate 0.000100053
2017-10-10T11:55:10.864830: step 2799, loss 0.09755, acc 0.984375, learning_rate 0.000100052
2017-10-10T11:55:11.040259: step 2800, loss 0.103125, acc 0.953125, learning_rate 0.000100052

Evaluation:
2017-10-10T11:55:11.429633: step 2800, loss 0.224743, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2800

2017-10-10T11:55:12.714245: step 2801, loss 0.058572, acc 0.984375, learning_rate 0.000100052
2017-10-10T11:55:12.940918: step 2802, loss 0.0794964, acc 0.96875, learning_rate 0.000100052
2017-10-10T11:55:13.165951: step 2803, loss 0.0944606, acc 0.96875, learning_rate 0.000100052
2017-10-10T11:55:13.405507: step 2804, loss 0.112106, acc 0.96875, learning_rate 0.000100051
2017-10-10T11:55:13.646647: step 2805, loss 0.0930399, acc 0.984375, learning_rate 0.000100051
2017-10-10T11:55:13.898261: step 2806, loss 0.206143, acc 0.9375, learning_rate 0.000100051
2017-10-10T11:55:14.114932: step 2807, loss 0.108763, acc 0.96875, learning_rate 0.000100051
2017-10-10T11:55:14.367419: step 2808, loss 0.0466503, acc 1, learning_rate 0.000100051
2017-10-10T11:55:14.618358: step 2809, loss 0.0917426, acc 0.984375, learning_rate 0.00010005
2017-10-10T11:55:14.852886: step 2810, loss 0.112201, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:55:15.092863: step 2811, loss 0.0904241, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:55:15.339746: step 2812, loss 0.0776335, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:55:15.542527: step 2813, loss 0.0827444, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:55:15.760076: step 2814, loss 0.234669, acc 0.921875, learning_rate 0.000100049
2017-10-10T11:55:15.945771: step 2815, loss 0.0670781, acc 0.984375, learning_rate 0.000100049
2017-10-10T11:55:16.188906: step 2816, loss 0.124869, acc 0.953125, learning_rate 0.000100049
2017-10-10T11:55:16.442617: step 2817, loss 0.10663, acc 0.953125, learning_rate 0.000100049
2017-10-10T11:55:16.690861: step 2818, loss 0.0784288, acc 0.96875, learning_rate 0.000100049
2017-10-10T11:55:16.970603: step 2819, loss 0.130907, acc 0.953125, learning_rate 0.000100048
2017-10-10T11:55:17.215294: step 2820, loss 0.0453729, acc 1, learning_rate 0.000100048
2017-10-10T11:55:17.430379: step 2821, loss 0.236913, acc 0.9375, learning_rate 0.000100048
2017-10-10T11:55:17.634434: step 2822, loss 0.130178, acc 0.96875, learning_rate 0.000100048
2017-10-10T11:55:17.845582: step 2823, loss 0.0579189, acc 0.984375, learning_rate 0.000100048
2017-10-10T11:55:18.081407: step 2824, loss 0.121697, acc 0.9375, learning_rate 0.000100047
2017-10-10T11:55:18.313813: step 2825, loss 0.125928, acc 0.96875, learning_rate 0.000100047
2017-10-10T11:55:18.557295: step 2826, loss 0.136724, acc 0.953125, learning_rate 0.000100047
2017-10-10T11:55:18.798199: step 2827, loss 0.106649, acc 0.96875, learning_rate 0.000100047
2017-10-10T11:55:19.038620: step 2828, loss 0.106842, acc 0.96875, learning_rate 0.000100047
2017-10-10T11:55:19.274726: step 2829, loss 0.145017, acc 0.953125, learning_rate 0.000100046
2017-10-10T11:55:19.514318: step 2830, loss 0.170694, acc 0.9375, learning_rate 0.000100046
2017-10-10T11:55:19.787357: step 2831, loss 0.0827946, acc 0.984375, learning_rate 0.000100046
2017-10-10T11:55:20.009290: step 2832, loss 0.0770609, acc 0.96875, learning_rate 0.000100046
2017-10-10T11:55:20.253569: step 2833, loss 0.0959626, acc 0.984375, learning_rate 0.000100046
2017-10-10T11:55:20.445055: step 2834, loss 0.161467, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:55:20.632882: step 2835, loss 0.100102, acc 0.96875, learning_rate 0.000100045
2017-10-10T11:55:20.908810: step 2836, loss 0.145932, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:55:21.122221: step 2837, loss 0.151314, acc 0.9375, learning_rate 0.000100045
2017-10-10T11:55:21.422637: step 2838, loss 0.151947, acc 0.9375, learning_rate 0.000100045
2017-10-10T11:55:21.674251: step 2839, loss 0.08154, acc 0.96875, learning_rate 0.000100045
2017-10-10T11:55:21.843780: step 2840, loss 0.0646411, acc 0.984375, learning_rate 0.000100044

Evaluation:
2017-10-10T11:55:22.236547: step 2840, loss 0.222557, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2840

2017-10-10T11:55:23.045936: step 2841, loss 0.117293, acc 0.96875, learning_rate 0.000100044
2017-10-10T11:55:23.240219: step 2842, loss 0.199317, acc 0.921569, learning_rate 0.000100044
2017-10-10T11:55:23.455091: step 2843, loss 0.101673, acc 0.953125, learning_rate 0.000100044
2017-10-10T11:55:23.695581: step 2844, loss 0.107588, acc 0.953125, learning_rate 0.000100044
2017-10-10T11:55:23.956228: step 2845, loss 0.297764, acc 0.875, learning_rate 0.000100043
2017-10-10T11:55:24.226808: step 2846, loss 0.101605, acc 0.96875, learning_rate 0.000100043
2017-10-10T11:55:24.473017: step 2847, loss 0.0677215, acc 0.984375, learning_rate 0.000100043
2017-10-10T11:55:24.708025: step 2848, loss 0.0973063, acc 0.96875, learning_rate 0.000100043
2017-10-10T11:55:25.016311: step 2849, loss 0.203447, acc 0.890625, learning_rate 0.000100043
2017-10-10T11:55:25.180908: step 2850, loss 0.0640782, acc 0.96875, learning_rate 0.000100043
2017-10-10T11:55:25.372868: step 2851, loss 0.0690889, acc 0.984375, learning_rate 0.000100042
2017-10-10T11:55:25.525702: step 2852, loss 0.0620961, acc 1, learning_rate 0.000100042
2017-10-10T11:55:25.694305: step 2853, loss 0.118242, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:55:25.879376: step 2854, loss 0.103415, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:55:26.085056: step 2855, loss 0.0914632, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:55:26.343843: step 2856, loss 0.115784, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:55:26.588820: step 2857, loss 0.133802, acc 0.9375, learning_rate 0.000100041
2017-10-10T11:55:26.855482: step 2858, loss 0.0594079, acc 0.984375, learning_rate 0.000100041
2017-10-10T11:55:27.101523: step 2859, loss 0.0869851, acc 0.984375, learning_rate 0.000100041
2017-10-10T11:55:27.371133: step 2860, loss 0.14881, acc 0.921875, learning_rate 0.000100041
2017-10-10T11:55:27.615549: step 2861, loss 0.10998, acc 0.953125, learning_rate 0.000100041
2017-10-10T11:55:27.861653: step 2862, loss 0.0952465, acc 0.984375, learning_rate 0.000100041
2017-10-10T11:55:28.101902: step 2863, loss 0.129784, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:55:28.305788: step 2864, loss 0.261539, acc 0.9375, learning_rate 0.00010004
2017-10-10T11:55:28.490967: step 2865, loss 0.0479164, acc 0.984375, learning_rate 0.00010004
2017-10-10T11:55:28.732901: step 2866, loss 0.122884, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:55:28.938868: step 2867, loss 0.0562368, acc 1, learning_rate 0.00010004
2017-10-10T11:55:29.222716: step 2868, loss 0.0745861, acc 0.984375, learning_rate 0.00010004
2017-10-10T11:55:29.455149: step 2869, loss 0.0795391, acc 0.953125, learning_rate 0.000100039
2017-10-10T11:55:29.707009: step 2870, loss 0.0581993, acc 1, learning_rate 0.000100039
2017-10-10T11:55:29.934355: step 2871, loss 0.119579, acc 0.96875, learning_rate 0.000100039
2017-10-10T11:55:30.192864: step 2872, loss 0.0445561, acc 1, learning_rate 0.000100039
2017-10-10T11:55:30.404752: step 2873, loss 0.19724, acc 0.921875, learning_rate 0.000100039
2017-10-10T11:55:30.621317: step 2874, loss 0.139611, acc 0.953125, learning_rate 0.000100039
2017-10-10T11:55:30.828696: step 2875, loss 0.0932193, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:55:31.020564: step 2876, loss 0.123832, acc 0.953125, learning_rate 0.000100038
2017-10-10T11:55:31.246470: step 2877, loss 0.067517, acc 1, learning_rate 0.000100038
2017-10-10T11:55:31.496805: step 2878, loss 0.0370193, acc 1, learning_rate 0.000100038
2017-10-10T11:55:31.713619: step 2879, loss 0.110044, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:55:31.954240: step 2880, loss 0.0999772, acc 0.953125, learning_rate 0.000100038

Evaluation:
2017-10-10T11:55:32.502335: step 2880, loss 0.221751, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2880

2017-10-10T11:55:33.524311: step 2881, loss 0.149763, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:55:33.647172: step 2882, loss 0.062475, acc 1, learning_rate 0.000100037
2017-10-10T11:55:33.778913: step 2883, loss 0.165929, acc 0.9375, learning_rate 0.000100037
2017-10-10T11:55:33.909841: step 2884, loss 0.0585582, acc 0.984375, learning_rate 0.000100037
2017-10-10T11:55:34.029724: step 2885, loss 0.0899968, acc 0.953125, learning_rate 0.000100037
2017-10-10T11:55:34.150424: step 2886, loss 0.108811, acc 0.9375, learning_rate 0.000100037
2017-10-10T11:55:34.337018: step 2887, loss 0.117752, acc 0.96875, learning_rate 0.000100037
2017-10-10T11:55:34.541119: step 2888, loss 0.1167, acc 0.953125, learning_rate 0.000100036
2017-10-10T11:55:34.772535: step 2889, loss 0.115157, acc 0.96875, learning_rate 0.000100036
2017-10-10T11:55:34.993532: step 2890, loss 0.136329, acc 0.953125, learning_rate 0.000100036
2017-10-10T11:55:35.221164: step 2891, loss 0.16479, acc 0.90625, learning_rate 0.000100036
2017-10-10T11:55:35.486013: step 2892, loss 0.0455167, acc 1, learning_rate 0.000100036
2017-10-10T11:55:35.726540: step 2893, loss 0.0947794, acc 0.96875, learning_rate 0.000100036
2017-10-10T11:55:35.972402: step 2894, loss 0.142373, acc 0.953125, learning_rate 0.000100036
2017-10-10T11:55:36.229887: step 2895, loss 0.0573346, acc 0.984375, learning_rate 0.000100035
2017-10-10T11:55:36.451809: step 2896, loss 0.164029, acc 0.953125, learning_rate 0.000100035
2017-10-10T11:55:36.688324: step 2897, loss 0.109776, acc 0.96875, learning_rate 0.000100035
2017-10-10T11:55:36.955777: step 2898, loss 0.100604, acc 0.96875, learning_rate 0.000100035
2017-10-10T11:55:37.180085: step 2899, loss 0.105647, acc 0.953125, learning_rate 0.000100035
2017-10-10T11:55:37.419976: step 2900, loss 0.192909, acc 0.890625, learning_rate 0.000100035
2017-10-10T11:55:37.684851: step 2901, loss 0.0977353, acc 0.96875, learning_rate 0.000100035
2017-10-10T11:55:37.927274: step 2902, loss 0.0535921, acc 0.984375, learning_rate 0.000100034
2017-10-10T11:55:38.168919: step 2903, loss 0.0838763, acc 0.984375, learning_rate 0.000100034
2017-10-10T11:55:38.419365: step 2904, loss 0.099665, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:55:38.656420: step 2905, loss 0.136688, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:55:38.886960: step 2906, loss 0.138859, acc 0.9375, learning_rate 0.000100034
2017-10-10T11:55:39.073130: step 2907, loss 0.145292, acc 0.9375, learning_rate 0.000100034
2017-10-10T11:55:39.320237: step 2908, loss 0.0923313, acc 0.96875, learning_rate 0.000100034
2017-10-10T11:55:39.536932: step 2909, loss 0.0814709, acc 0.984375, learning_rate 0.000100033
2017-10-10T11:55:39.768431: step 2910, loss 0.0788549, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:55:40.029028: step 2911, loss 0.110878, acc 0.9375, learning_rate 0.000100033
2017-10-10T11:55:40.285815: step 2912, loss 0.166763, acc 0.953125, learning_rate 0.000100033
2017-10-10T11:55:40.542435: step 2913, loss 0.210554, acc 0.921875, learning_rate 0.000100033
2017-10-10T11:55:40.778496: step 2914, loss 0.130322, acc 0.953125, learning_rate 0.000100033
2017-10-10T11:55:41.053288: step 2915, loss 0.126888, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:55:41.334767: step 2916, loss 0.0837085, acc 0.984375, learning_rate 0.000100033
2017-10-10T11:55:41.579991: step 2917, loss 0.149351, acc 0.9375, learning_rate 0.000100032
2017-10-10T11:55:41.791372: step 2918, loss 0.139365, acc 0.96875, learning_rate 0.000100032
2017-10-10T11:55:42.116851: step 2919, loss 0.142242, acc 0.9375, learning_rate 0.000100032
2017-10-10T11:55:42.305738: step 2920, loss 0.134924, acc 0.953125, learning_rate 0.000100032

Evaluation:
2017-10-10T11:55:42.772731: step 2920, loss 0.221179, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2920

2017-10-10T11:55:44.090295: step 2921, loss 0.0697453, acc 0.984375, learning_rate 0.000100032
2017-10-10T11:55:44.325697: step 2922, loss 0.143472, acc 0.953125, learning_rate 0.000100032
2017-10-10T11:55:44.504786: step 2923, loss 0.0883443, acc 0.984375, learning_rate 0.000100032
2017-10-10T11:55:44.666662: step 2924, loss 0.0804569, acc 0.9375, learning_rate 0.000100031
2017-10-10T11:55:44.845515: step 2925, loss 0.0849203, acc 0.984375, learning_rate 0.000100031
2017-10-10T11:55:45.012040: step 2926, loss 0.266033, acc 0.9375, learning_rate 0.000100031
2017-10-10T11:55:45.176842: step 2927, loss 0.154841, acc 0.921875, learning_rate 0.000100031
2017-10-10T11:55:45.413118: step 2928, loss 0.242865, acc 0.90625, learning_rate 0.000100031
2017-10-10T11:55:45.662984: step 2929, loss 0.138173, acc 0.953125, learning_rate 0.000100031
2017-10-10T11:55:45.911863: step 2930, loss 0.12493, acc 0.953125, learning_rate 0.000100031
2017-10-10T11:55:46.175625: step 2931, loss 0.0824494, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:55:46.412043: step 2932, loss 0.186199, acc 0.921875, learning_rate 0.00010003
2017-10-10T11:55:46.642397: step 2933, loss 0.117915, acc 0.96875, learning_rate 0.00010003
2017-10-10T11:55:46.901060: step 2934, loss 0.12321, acc 0.953125, learning_rate 0.00010003
2017-10-10T11:55:47.129950: step 2935, loss 0.076984, acc 0.96875, learning_rate 0.00010003
2017-10-10T11:55:47.376875: step 2936, loss 0.128901, acc 0.9375, learning_rate 0.00010003
2017-10-10T11:55:47.642963: step 2937, loss 0.0815356, acc 0.984375, learning_rate 0.00010003
2017-10-10T11:55:47.887655: step 2938, loss 0.098422, acc 0.96875, learning_rate 0.00010003
2017-10-10T11:55:48.101000: step 2939, loss 0.189759, acc 0.921875, learning_rate 0.00010003
2017-10-10T11:55:48.262702: step 2940, loss 0.146708, acc 0.941176, learning_rate 0.000100029
2017-10-10T11:55:48.460938: step 2941, loss 0.0811306, acc 0.984375, learning_rate 0.000100029
2017-10-10T11:55:48.686691: step 2942, loss 0.205025, acc 0.890625, learning_rate 0.000100029
2017-10-10T11:55:48.947629: step 2943, loss 0.142257, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:55:49.179234: step 2944, loss 0.0923035, acc 0.96875, learning_rate 0.000100029
2017-10-10T11:55:49.434465: step 2945, loss 0.131039, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:55:49.674854: step 2946, loss 0.0946419, acc 0.984375, learning_rate 0.000100029
2017-10-10T11:55:50.007559: step 2947, loss 0.071652, acc 1, learning_rate 0.000100029
2017-10-10T11:55:50.207427: step 2948, loss 0.103648, acc 0.984375, learning_rate 0.000100029
2017-10-10T11:55:50.392850: step 2949, loss 0.121943, acc 0.9375, learning_rate 0.000100028
2017-10-10T11:55:50.540891: step 2950, loss 0.176241, acc 0.921875, learning_rate 0.000100028
2017-10-10T11:55:50.712840: step 2951, loss 0.14682, acc 0.9375, learning_rate 0.000100028
2017-10-10T11:55:50.904946: step 2952, loss 0.192571, acc 0.9375, learning_rate 0.000100028
2017-10-10T11:55:51.130275: step 2953, loss 0.110732, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:55:51.352821: step 2954, loss 0.0924665, acc 0.984375, learning_rate 0.000100028
2017-10-10T11:55:51.565069: step 2955, loss 0.0659571, acc 1, learning_rate 0.000100028
2017-10-10T11:55:51.811189: step 2956, loss 0.1607, acc 0.953125, learning_rate 0.000100028
2017-10-10T11:55:52.061507: step 2957, loss 0.1011, acc 0.984375, learning_rate 0.000100028
2017-10-10T11:55:52.279904: step 2958, loss 0.101671, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:55:52.484883: step 2959, loss 0.0793543, acc 0.984375, learning_rate 0.000100027
2017-10-10T11:55:52.721045: step 2960, loss 0.076746, acc 0.984375, learning_rate 0.000100027

Evaluation:
2017-10-10T11:55:53.249125: step 2960, loss 0.220664, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-2960

2017-10-10T11:55:54.259410: step 2961, loss 0.147072, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:55:54.501855: step 2962, loss 0.187099, acc 0.9375, learning_rate 0.000100027
2017-10-10T11:55:54.702447: step 2963, loss 0.116639, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:55:54.936892: step 2964, loss 0.0858379, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:55:55.260887: step 2965, loss 0.104128, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:55:55.530992: step 2966, loss 0.11127, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:55:55.707799: step 2967, loss 0.187142, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:55:56.019433: step 2968, loss 0.0417567, acc 1, learning_rate 0.000100026
2017-10-10T11:55:56.197739: step 2969, loss 0.10797, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:55:56.354724: step 2970, loss 0.0821726, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:55:56.584846: step 2971, loss 0.0836708, acc 0.96875, learning_rate 0.000100026
2017-10-10T11:55:56.828081: step 2972, loss 0.26282, acc 0.875, learning_rate 0.000100026
2017-10-10T11:55:57.062148: step 2973, loss 0.0644252, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:55:57.291687: step 2974, loss 0.0469787, acc 1, learning_rate 0.000100026
2017-10-10T11:55:57.523978: step 2975, loss 0.089361, acc 1, learning_rate 0.000100026
2017-10-10T11:55:57.746701: step 2976, loss 0.0682088, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:55:57.989102: step 2977, loss 0.14468, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:55:58.243338: step 2978, loss 0.151745, acc 0.953125, learning_rate 0.000100025
2017-10-10T11:55:58.466809: step 2979, loss 0.0908412, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:55:58.705061: step 2980, loss 0.0892595, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:55:58.893738: step 2981, loss 0.138247, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:55:59.064830: step 2982, loss 0.150048, acc 0.953125, learning_rate 0.000100025
2017-10-10T11:55:59.260859: step 2983, loss 0.157241, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:55:59.452857: step 2984, loss 0.101436, acc 0.953125, learning_rate 0.000100025
2017-10-10T11:55:59.642896: step 2985, loss 0.189164, acc 0.953125, learning_rate 0.000100025
2017-10-10T11:55:59.900877: step 2986, loss 0.0507209, acc 1, learning_rate 0.000100024
2017-10-10T11:56:00.129035: step 2987, loss 0.171509, acc 0.953125, learning_rate 0.000100024
2017-10-10T11:56:00.340854: step 2988, loss 0.210188, acc 0.953125, learning_rate 0.000100024
2017-10-10T11:56:00.610305: step 2989, loss 0.184742, acc 0.9375, learning_rate 0.000100024
2017-10-10T11:56:00.850963: step 2990, loss 0.126136, acc 0.953125, learning_rate 0.000100024
2017-10-10T11:56:01.110512: step 2991, loss 0.0775933, acc 0.96875, learning_rate 0.000100024
2017-10-10T11:56:01.379566: step 2992, loss 0.130296, acc 0.953125, learning_rate 0.000100024
2017-10-10T11:56:01.616724: step 2993, loss 0.0555521, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:56:01.871860: step 2994, loss 0.130754, acc 0.96875, learning_rate 0.000100024
2017-10-10T11:56:02.102718: step 2995, loss 0.0593433, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:56:02.316841: step 2996, loss 0.089032, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:56:02.588242: step 2997, loss 0.0494849, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:56:02.820637: step 2998, loss 0.126692, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:03.052880: step 2999, loss 0.0781773, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:56:03.298300: step 3000, loss 0.0564822, acc 0.96875, learning_rate 0.000100023

Evaluation:
2017-10-10T11:56:03.874754: step 3000, loss 0.220763, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3000

2017-10-10T11:56:05.015622: step 3001, loss 0.173616, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:05.268855: step 3002, loss 0.195545, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:05.541951: step 3003, loss 0.213227, acc 0.90625, learning_rate 0.000100023
2017-10-10T11:56:05.798140: step 3004, loss 0.0573155, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:56:06.064883: step 3005, loss 0.0676666, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:56:06.363855: step 3006, loss 0.149322, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:06.522292: step 3007, loss 0.263038, acc 0.90625, learning_rate 0.000100022
2017-10-10T11:56:06.688202: step 3008, loss 0.191808, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:56:06.859284: step 3009, loss 0.0461343, acc 0.984375, learning_rate 0.000100022
2017-10-10T11:56:07.120014: step 3010, loss 0.149387, acc 0.9375, learning_rate 0.000100022
2017-10-10T11:56:07.263521: step 3011, loss 0.0679254, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:56:07.483386: step 3012, loss 0.0982267, acc 0.953125, learning_rate 0.000100022
2017-10-10T11:56:07.659290: step 3013, loss 0.0712082, acc 1, learning_rate 0.000100022
2017-10-10T11:56:07.828839: step 3014, loss 0.0900872, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:56:08.064070: step 3015, loss 0.0663086, acc 1, learning_rate 0.000100022
2017-10-10T11:56:08.283426: step 3016, loss 0.11019, acc 0.953125, learning_rate 0.000100022
2017-10-10T11:56:08.541198: step 3017, loss 0.118393, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:56:08.750687: step 3018, loss 0.0718886, acc 1, learning_rate 0.000100021
2017-10-10T11:56:08.965080: step 3019, loss 0.122222, acc 0.953125, learning_rate 0.000100021
2017-10-10T11:56:09.212247: step 3020, loss 0.0606933, acc 1, learning_rate 0.000100021
2017-10-10T11:56:09.436867: step 3021, loss 0.0425, acc 1, learning_rate 0.000100021
2017-10-10T11:56:09.676862: step 3022, loss 0.146182, acc 0.9375, learning_rate 0.000100021
2017-10-10T11:56:09.919555: step 3023, loss 0.174109, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:56:10.180882: step 3024, loss 0.0897334, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:56:10.412861: step 3025, loss 0.117994, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:56:10.640854: step 3026, loss 0.182856, acc 0.953125, learning_rate 0.000100021
2017-10-10T11:56:10.861522: step 3027, loss 0.107524, acc 0.953125, learning_rate 0.000100021
2017-10-10T11:56:11.135979: step 3028, loss 0.129016, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:56:11.353331: step 3029, loss 0.0843681, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:56:11.582053: step 3030, loss 0.0951041, acc 0.953125, learning_rate 0.00010002
2017-10-10T11:56:11.834842: step 3031, loss 0.226271, acc 0.921875, learning_rate 0.00010002
2017-10-10T11:56:12.063065: step 3032, loss 0.0676735, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:56:12.319787: step 3033, loss 0.195317, acc 0.921875, learning_rate 0.00010002
2017-10-10T11:56:12.587075: step 3034, loss 0.20952, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:56:12.819176: step 3035, loss 0.102165, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:56:13.068093: step 3036, loss 0.178676, acc 0.921875, learning_rate 0.00010002
2017-10-10T11:56:13.332183: step 3037, loss 0.0244159, acc 1, learning_rate 0.00010002
2017-10-10T11:56:13.532232: step 3038, loss 0.208158, acc 0.921569, learning_rate 0.00010002
2017-10-10T11:56:13.769918: step 3039, loss 0.0958407, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:56:14.019354: step 3040, loss 0.0674981, acc 0.984375, learning_rate 0.00010002

Evaluation:
2017-10-10T11:56:14.544806: step 3040, loss 0.217763, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3040

2017-10-10T11:56:15.657959: step 3041, loss 0.148615, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:56:15.877904: step 3042, loss 0.148683, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:56:16.087283: step 3043, loss 0.0445337, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:56:16.289871: step 3044, loss 0.0976544, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:56:16.528767: step 3045, loss 0.124337, acc 0.953125, learning_rate 0.000100019
2017-10-10T11:56:16.770880: step 3046, loss 0.0522196, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:56:17.053303: step 3047, loss 0.160767, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:56:17.284164: step 3048, loss 0.049297, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:56:17.475110: step 3049, loss 0.210787, acc 0.921875, learning_rate 0.000100019
2017-10-10T11:56:17.644945: step 3050, loss 0.0735717, acc 1, learning_rate 0.000100019
2017-10-10T11:56:17.833091: step 3051, loss 0.146035, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:56:18.027087: step 3052, loss 0.0566668, acc 1, learning_rate 0.000100019
2017-10-10T11:56:18.261289: step 3053, loss 0.0674779, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:56:18.479820: step 3054, loss 0.200959, acc 0.921875, learning_rate 0.000100018
2017-10-10T11:56:18.691629: step 3055, loss 0.0441001, acc 1, learning_rate 0.000100018
2017-10-10T11:56:18.917604: step 3056, loss 0.0910594, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:56:19.173020: step 3057, loss 0.0569641, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:56:19.404221: step 3058, loss 0.0997524, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:56:19.619715: step 3059, loss 0.0799442, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:56:19.872888: step 3060, loss 0.11051, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:56:20.119590: step 3061, loss 0.0693898, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:56:20.347453: step 3062, loss 0.174456, acc 0.9375, learning_rate 0.000100018
2017-10-10T11:56:20.591642: step 3063, loss 0.100377, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:56:20.816234: step 3064, loss 0.0983124, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:56:21.079027: step 3065, loss 0.249668, acc 0.890625, learning_rate 0.000100018
2017-10-10T11:56:21.307268: step 3066, loss 0.0872773, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:56:21.514963: step 3067, loss 0.143943, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:56:21.736838: step 3068, loss 0.0925401, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:56:21.964352: step 3069, loss 0.16614, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:56:22.192538: step 3070, loss 0.0554887, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:56:22.376889: step 3071, loss 0.112793, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:56:22.631526: step 3072, loss 0.0810383, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:56:22.888867: step 3073, loss 0.0919612, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:56:23.128914: step 3074, loss 0.17974, acc 0.90625, learning_rate 0.000100017
2017-10-10T11:56:23.360897: step 3075, loss 0.0953018, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:56:23.681563: step 3076, loss 0.204966, acc 0.9375, learning_rate 0.000100017
2017-10-10T11:56:23.890291: step 3077, loss 0.0926303, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:56:24.096193: step 3078, loss 0.137891, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:56:24.320130: step 3079, loss 0.124636, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:56:24.523707: step 3080, loss 0.114563, acc 0.96875, learning_rate 0.000100017

Evaluation:
2017-10-10T11:56:25.097185: step 3080, loss 0.218555, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3080

2017-10-10T11:56:26.427466: step 3081, loss 0.209744, acc 0.90625, learning_rate 0.000100017
2017-10-10T11:56:26.648824: step 3082, loss 0.138698, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:26.810121: step 3083, loss 0.0463006, acc 1, learning_rate 0.000100016
2017-10-10T11:56:27.063195: step 3084, loss 0.165949, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:27.280208: step 3085, loss 0.164799, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:27.522416: step 3086, loss 0.227401, acc 0.875, learning_rate 0.000100016
2017-10-10T11:56:27.752911: step 3087, loss 0.109344, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:56:27.987220: step 3088, loss 0.160969, acc 0.9375, learning_rate 0.000100016
2017-10-10T11:56:28.298374: step 3089, loss 0.0987363, acc 0.9375, learning_rate 0.000100016
2017-10-10T11:56:28.496880: step 3090, loss 0.0843001, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:28.696928: step 3091, loss 0.25123, acc 0.90625, learning_rate 0.000100016
2017-10-10T11:56:28.883400: step 3092, loss 0.12605, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:56:29.069768: step 3093, loss 0.0564698, acc 1, learning_rate 0.000100016
2017-10-10T11:56:29.274235: step 3094, loss 0.113628, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:29.528872: step 3095, loss 0.138518, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:56:29.795922: step 3096, loss 0.161628, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:30.007101: step 3097, loss 0.112978, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:30.238496: step 3098, loss 0.195692, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:56:30.476952: step 3099, loss 0.173878, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:56:30.728176: step 3100, loss 0.172242, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:56:30.944215: step 3101, loss 0.132705, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:56:31.168947: step 3102, loss 0.0715777, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:56:31.429115: step 3103, loss 0.0890178, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:56:31.724836: step 3104, loss 0.0549228, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:31.998671: step 3105, loss 0.0787703, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:32.219208: step 3106, loss 0.158378, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:56:32.417302: step 3107, loss 0.122013, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:32.616916: step 3108, loss 0.0478508, acc 1, learning_rate 0.000100015
2017-10-10T11:56:32.812025: step 3109, loss 0.137104, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:56:32.986009: step 3110, loss 0.130552, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:56:33.229542: step 3111, loss 0.103688, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:33.456601: step 3112, loss 0.150203, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:56:33.678774: step 3113, loss 0.11805, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:56:33.908892: step 3114, loss 0.0668237, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:34.158885: step 3115, loss 0.119464, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:56:34.400897: step 3116, loss 0.0830285, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:34.613007: step 3117, loss 0.281131, acc 0.921875, learning_rate 0.000100014
2017-10-10T11:56:34.782112: step 3118, loss 0.0542923, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:56:35.001598: step 3119, loss 0.102691, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:56:35.225027: step 3120, loss 0.206132, acc 0.90625, learning_rate 0.000100014

Evaluation:
2017-10-10T11:56:35.755433: step 3120, loss 0.220861, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3120

2017-10-10T11:56:36.777726: step 3121, loss 0.0834117, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:37.000033: step 3122, loss 0.0799963, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:56:37.180192: step 3123, loss 0.11399, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:56:37.432996: step 3124, loss 0.0886456, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:37.640067: step 3125, loss 0.0960117, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:56:37.850001: step 3126, loss 0.117509, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:38.066099: step 3127, loss 0.033894, acc 1, learning_rate 0.000100014
2017-10-10T11:56:38.316963: step 3128, loss 0.0864167, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:38.543078: step 3129, loss 0.128689, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:38.815441: step 3130, loss 0.108664, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:39.100753: step 3131, loss 0.0553687, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:56:39.367276: step 3132, loss 0.0582111, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:39.537193: step 3133, loss 0.0728165, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:56:39.710939: step 3134, loss 0.100305, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:39.895146: step 3135, loss 0.100894, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:40.040162: step 3136, loss 0.0852224, acc 0.960784, learning_rate 0.000100013
2017-10-10T11:56:40.320127: step 3137, loss 0.150386, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:56:40.455164: step 3138, loss 0.0812059, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:56:40.659634: step 3139, loss 0.114705, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:40.840810: step 3140, loss 0.211349, acc 0.90625, learning_rate 0.000100013
2017-10-10T11:56:41.020758: step 3141, loss 0.0967921, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:41.264010: step 3142, loss 0.0886751, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:56:41.504996: step 3143, loss 0.0748804, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:41.776118: step 3144, loss 0.0684069, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:56:41.998738: step 3145, loss 0.0413448, acc 1, learning_rate 0.000100013
2017-10-10T11:56:42.245886: step 3146, loss 0.0892018, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:56:42.480380: step 3147, loss 0.0959799, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:56:42.722200: step 3148, loss 0.143271, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:42.959506: step 3149, loss 0.0821208, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:43.192015: step 3150, loss 0.0890456, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:43.475016: step 3151, loss 0.0605454, acc 1, learning_rate 0.000100012
2017-10-10T11:56:43.720731: step 3152, loss 0.0915908, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:43.982426: step 3153, loss 0.121241, acc 0.921875, learning_rate 0.000100012
2017-10-10T11:56:44.217231: step 3154, loss 0.139209, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:44.466036: step 3155, loss 0.116913, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:56:44.713208: step 3156, loss 0.129811, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:44.949683: step 3157, loss 0.0783396, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:56:45.198251: step 3158, loss 0.131215, acc 0.921875, learning_rate 0.000100012
2017-10-10T11:56:45.445758: step 3159, loss 0.0916804, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:45.666108: step 3160, loss 0.0869067, acc 0.96875, learning_rate 0.000100012

Evaluation:
2017-10-10T11:56:46.249728: step 3160, loss 0.2202, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3160

2017-10-10T11:56:47.764715: step 3161, loss 0.183207, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:47.989188: step 3162, loss 0.144247, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:48.224873: step 3163, loss 0.149579, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:48.494027: step 3164, loss 0.115644, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:48.729894: step 3165, loss 0.128833, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:56:48.880831: step 3166, loss 0.122066, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:49.084002: step 3167, loss 0.268075, acc 0.890625, learning_rate 0.000100012
2017-10-10T11:56:49.277673: step 3168, loss 0.102217, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:49.490667: step 3169, loss 0.158709, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:49.686988: step 3170, loss 0.0869441, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:50.017104: step 3171, loss 0.122918, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:50.256077: step 3172, loss 0.0793377, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:56:50.447866: step 3173, loss 0.159117, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:50.619665: step 3174, loss 0.149387, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:50.797691: step 3175, loss 0.109295, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:50.984977: step 3176, loss 0.153951, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:56:51.214377: step 3177, loss 0.0381559, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:51.413571: step 3178, loss 0.128174, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:51.606038: step 3179, loss 0.106845, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:51.869210: step 3180, loss 0.0844323, acc 1, learning_rate 0.000100011
2017-10-10T11:56:52.137751: step 3181, loss 0.122151, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:56:52.407001: step 3182, loss 0.11132, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:52.617787: step 3183, loss 0.0396201, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:52.862861: step 3184, loss 0.0989015, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:56:53.136858: step 3185, loss 0.0570346, acc 1, learning_rate 0.000100011
2017-10-10T11:56:53.359958: step 3186, loss 0.0999674, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:53.610531: step 3187, loss 0.142569, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:53.857185: step 3188, loss 0.165006, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:54.102067: step 3189, loss 0.0967773, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:54.328947: step 3190, loss 0.089531, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:54.568104: step 3191, loss 0.181992, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:54.800518: step 3192, loss 0.1118, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:55.035148: step 3193, loss 0.0988263, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:56:55.279977: step 3194, loss 0.0657049, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:56:55.496868: step 3195, loss 0.114451, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:56:55.684874: step 3196, loss 0.151728, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:56:55.914077: step 3197, loss 0.138105, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:56:56.137119: step 3198, loss 0.175184, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:56:56.383367: step 3199, loss 0.031102, acc 1, learning_rate 0.00010001
2017-10-10T11:56:56.606970: step 3200, loss 0.083921, acc 0.953125, learning_rate 0.00010001

Evaluation:
2017-10-10T11:56:57.259100: step 3200, loss 0.21898, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3200

2017-10-10T11:56:58.290517: step 3201, loss 0.0666073, acc 1, learning_rate 0.00010001
2017-10-10T11:56:58.508151: step 3202, loss 0.0966133, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:56:58.750061: step 3203, loss 0.042022, acc 1, learning_rate 0.00010001
2017-10-10T11:56:58.962907: step 3204, loss 0.128083, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:56:59.224845: step 3205, loss 0.0809665, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:56:59.448853: step 3206, loss 0.239679, acc 0.921875, learning_rate 0.00010001
2017-10-10T11:56:59.690466: step 3207, loss 0.199503, acc 0.90625, learning_rate 0.00010001
2017-10-10T11:56:59.939275: step 3208, loss 0.0545255, acc 1, learning_rate 0.00010001
2017-10-10T11:57:00.170566: step 3209, loss 0.130596, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:57:00.404531: step 3210, loss 0.127866, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:57:00.662951: step 3211, loss 0.229957, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:57:00.933266: step 3212, loss 0.0942493, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:57:01.161212: step 3213, loss 0.0632749, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:57:01.334582: step 3214, loss 0.31229, acc 0.890625, learning_rate 0.00010001
2017-10-10T11:57:01.503079: step 3215, loss 0.0414279, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:57:01.659900: step 3216, loss 0.0556101, acc 1, learning_rate 0.00010001
2017-10-10T11:57:01.813214: step 3217, loss 0.259016, acc 0.890625, learning_rate 0.000100009
2017-10-10T11:57:02.013602: step 3218, loss 0.188965, acc 0.921875, learning_rate 0.000100009
2017-10-10T11:57:02.205582: step 3219, loss 0.0746059, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:02.444831: step 3220, loss 0.192046, acc 0.921875, learning_rate 0.000100009
2017-10-10T11:57:02.707987: step 3221, loss 0.147951, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:02.944736: step 3222, loss 0.0918766, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:03.178314: step 3223, loss 0.0778232, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:03.404966: step 3224, loss 0.0470842, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:03.640306: step 3225, loss 0.187058, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:03.871022: step 3226, loss 0.0568944, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:04.112854: step 3227, loss 0.260051, acc 0.90625, learning_rate 0.000100009
2017-10-10T11:57:04.345100: step 3228, loss 0.0429353, acc 1, learning_rate 0.000100009
2017-10-10T11:57:04.585088: step 3229, loss 0.0600384, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:04.828831: step 3230, loss 0.185358, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:05.074354: step 3231, loss 0.0978859, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:05.320706: step 3232, loss 0.129824, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:05.563849: step 3233, loss 0.108908, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:05.824866: step 3234, loss 0.146167, acc 0.960784, learning_rate 0.000100009
2017-10-10T11:57:06.045125: step 3235, loss 0.0788714, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:06.228808: step 3236, loss 0.11188, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:06.400868: step 3237, loss 0.103593, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:06.569014: step 3238, loss 0.0295813, acc 1, learning_rate 0.000100009
2017-10-10T11:57:06.747778: step 3239, loss 0.0805464, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:06.981905: step 3240, loss 0.101236, acc 0.96875, learning_rate 0.000100009

Evaluation:
2017-10-10T11:57:07.540735: step 3240, loss 0.218088, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3240

2017-10-10T11:57:08.529637: step 3241, loss 0.131727, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:08.766255: step 3242, loss 0.0629413, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:08.969429: step 3243, loss 0.107561, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:09.160621: step 3244, loss 0.0595378, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:09.385721: step 3245, loss 0.14884, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:57:09.634851: step 3246, loss 0.0671232, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:09.863652: step 3247, loss 0.139305, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:10.102123: step 3248, loss 0.106778, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:10.340903: step 3249, loss 0.105059, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:10.593173: step 3250, loss 0.0788122, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:10.832606: step 3251, loss 0.0816666, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:11.125228: step 3252, loss 0.202868, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:11.356832: step 3253, loss 0.109282, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:11.565111: step 3254, loss 0.124702, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:11.825685: step 3255, loss 0.0724034, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:12.104802: step 3256, loss 0.0949705, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:12.284975: step 3257, loss 0.042308, acc 1, learning_rate 0.000100008
2017-10-10T11:57:12.470231: step 3258, loss 0.0992927, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:12.648016: step 3259, loss 0.208867, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:57:12.813517: step 3260, loss 0.137746, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:12.972861: step 3261, loss 0.1158, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:13.150509: step 3262, loss 0.125692, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:13.355611: step 3263, loss 0.0756605, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:13.651395: step 3264, loss 0.133166, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:13.892918: step 3265, loss 0.0732364, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:14.117333: step 3266, loss 0.0694545, acc 1, learning_rate 0.000100008
2017-10-10T11:57:14.316344: step 3267, loss 0.165993, acc 0.90625, learning_rate 0.000100008
2017-10-10T11:57:14.540828: step 3268, loss 0.0562913, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:14.745113: step 3269, loss 0.0903641, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:14.946211: step 3270, loss 0.0977011, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:15.148270: step 3271, loss 0.1314, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:15.353044: step 3272, loss 0.107141, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:15.569190: step 3273, loss 0.134973, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:57:15.823260: step 3274, loss 0.0970235, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:16.044895: step 3275, loss 0.132515, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:16.285729: step 3276, loss 0.0775301, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:16.531982: step 3277, loss 0.118536, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:16.758280: step 3278, loss 0.135849, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:17.016147: step 3279, loss 0.141922, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:17.224847: step 3280, loss 0.0984575, acc 0.953125, learning_rate 0.000100007

Evaluation:
2017-10-10T11:57:17.784600: step 3280, loss 0.218645, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3280

2017-10-10T11:57:18.864822: step 3281, loss 0.098811, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:19.091990: step 3282, loss 0.0776391, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:19.344737: step 3283, loss 0.0902599, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:19.584889: step 3284, loss 0.0906708, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:19.841124: step 3285, loss 0.115872, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:20.080745: step 3286, loss 0.151571, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:57:20.319900: step 3287, loss 0.0562031, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:20.560196: step 3288, loss 0.055335, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:20.795301: step 3289, loss 0.108633, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:21.027744: step 3290, loss 0.0775004, acc 1, learning_rate 0.000100007
2017-10-10T11:57:21.249550: step 3291, loss 0.116486, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:21.468881: step 3292, loss 0.249766, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:57:21.716846: step 3293, loss 0.0891087, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:21.909181: step 3294, loss 0.114559, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:22.114460: step 3295, loss 0.0883736, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:22.392911: step 3296, loss 0.169126, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:22.614189: step 3297, loss 0.162608, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:57:22.823774: step 3298, loss 0.223284, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:57:23.068879: step 3299, loss 0.137008, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:57:23.341216: step 3300, loss 0.0960825, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:23.473488: step 3301, loss 0.0899855, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:23.647856: step 3302, loss 0.18212, acc 0.90625, learning_rate 0.000100007
2017-10-10T11:57:23.827026: step 3303, loss 0.0625487, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:24.000095: step 3304, loss 0.080371, acc 1, learning_rate 0.000100007
2017-10-10T11:57:24.208944: step 3305, loss 0.147777, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:24.441201: step 3306, loss 0.112569, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:24.686314: step 3307, loss 0.0398526, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:24.891018: step 3308, loss 0.105906, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:25.106103: step 3309, loss 0.226484, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:57:25.347579: step 3310, loss 0.218445, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:25.556842: step 3311, loss 0.0651583, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:25.755356: step 3312, loss 0.0986825, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:25.978933: step 3313, loss 0.152575, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:26.169124: step 3314, loss 0.12098, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:26.405049: step 3315, loss 0.0813418, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:26.631664: step 3316, loss 0.0737176, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:26.877321: step 3317, loss 0.092402, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:27.092809: step 3318, loss 0.0553812, acc 1, learning_rate 0.000100006
2017-10-10T11:57:27.342519: step 3319, loss 0.0671018, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:27.564876: step 3320, loss 0.101217, acc 0.984375, learning_rate 0.000100006

Evaluation:
2017-10-10T11:57:28.100890: step 3320, loss 0.21751, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3320

2017-10-10T11:57:29.384590: step 3321, loss 0.103522, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:29.609089: step 3322, loss 0.133164, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:29.854669: step 3323, loss 0.130867, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:30.144336: step 3324, loss 0.142868, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:30.361900: step 3325, loss 0.083435, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:30.571847: step 3326, loss 0.167319, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:30.893869: step 3327, loss 0.0705426, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:31.064276: step 3328, loss 0.172285, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:31.266565: step 3329, loss 0.193364, acc 0.90625, learning_rate 0.000100006
2017-10-10T11:57:31.480802: step 3330, loss 0.14201, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:31.674396: step 3331, loss 0.211091, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:31.890005: step 3332, loss 0.200077, acc 0.921569, learning_rate 0.000100006
2017-10-10T11:57:32.145301: step 3333, loss 0.150497, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:32.420903: step 3334, loss 0.137023, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:32.667262: step 3335, loss 0.105086, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:32.959973: step 3336, loss 0.099264, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:33.193443: step 3337, loss 0.112069, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:33.424886: step 3338, loss 0.215081, acc 0.921875, learning_rate 0.000100006
2017-10-10T11:57:33.678391: step 3339, loss 0.0358555, acc 1, learning_rate 0.000100006
2017-10-10T11:57:33.948149: step 3340, loss 0.0798884, acc 1, learning_rate 0.000100006
2017-10-10T11:57:34.305087: step 3341, loss 0.0645609, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:34.459504: step 3342, loss 0.0775682, acc 1, learning_rate 0.000100006
2017-10-10T11:57:34.628939: step 3343, loss 0.174805, acc 0.90625, learning_rate 0.000100006
2017-10-10T11:57:34.789826: step 3344, loss 0.0641659, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:34.968854: step 3345, loss 0.0969888, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:35.161928: step 3346, loss 0.189378, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:35.336324: step 3347, loss 0.175096, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:35.577757: step 3348, loss 0.0843337, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:35.768341: step 3349, loss 0.0821393, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:36.001029: step 3350, loss 0.0423142, acc 1, learning_rate 0.000100006
2017-10-10T11:57:36.295273: step 3351, loss 0.116089, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:36.498485: step 3352, loss 0.104817, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:36.685271: step 3353, loss 0.0245544, acc 1, learning_rate 0.000100005
2017-10-10T11:57:36.921341: step 3354, loss 0.0569511, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:37.139541: step 3355, loss 0.129951, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:37.364278: step 3356, loss 0.191722, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:37.621568: step 3357, loss 0.0796139, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:37.905830: step 3358, loss 0.122287, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:38.148085: step 3359, loss 0.0854077, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:38.388792: step 3360, loss 0.0482117, acc 1, learning_rate 0.000100005

Evaluation:
2017-10-10T11:57:38.970877: step 3360, loss 0.217568, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3360

2017-10-10T11:57:39.971006: step 3361, loss 0.10965, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:40.215367: step 3362, loss 0.0865514, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:40.452917: step 3363, loss 0.0682242, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:40.713627: step 3364, loss 0.144872, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:40.966441: step 3365, loss 0.126825, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:41.176918: step 3366, loss 0.134043, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:41.374592: step 3367, loss 0.088853, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:41.561727: step 3368, loss 0.108713, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:41.821041: step 3369, loss 0.078953, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:42.055523: step 3370, loss 0.0677471, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:42.299904: step 3371, loss 0.256003, acc 0.890625, learning_rate 0.000100005
2017-10-10T11:57:42.567343: step 3372, loss 0.0640969, acc 1, learning_rate 0.000100005
2017-10-10T11:57:42.789765: step 3373, loss 0.149512, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:42.981403: step 3374, loss 0.185155, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:43.184905: step 3375, loss 0.122962, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:43.412868: step 3376, loss 0.153073, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:43.635581: step 3377, loss 0.123108, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:43.887961: step 3378, loss 0.231124, acc 0.90625, learning_rate 0.000100005
2017-10-10T11:57:44.111699: step 3379, loss 0.0735071, acc 1, learning_rate 0.000100005
2017-10-10T11:57:44.363334: step 3380, loss 0.0866038, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:44.604633: step 3381, loss 0.1878, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:44.864781: step 3382, loss 0.152907, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:45.164820: step 3383, loss 0.11211, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:45.427209: step 3384, loss 0.194954, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:45.580476: step 3385, loss 0.0746524, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:45.755582: step 3386, loss 0.166832, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:45.936908: step 3387, loss 0.0707608, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:46.137504: step 3388, loss 0.126536, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:46.307025: step 3389, loss 0.0858207, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:46.490213: step 3390, loss 0.125559, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:46.777883: step 3391, loss 0.130139, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:47.024707: step 3392, loss 0.155842, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:47.324842: step 3393, loss 0.0860924, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:47.590354: step 3394, loss 0.0951118, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:47.796305: step 3395, loss 0.0782482, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:47.998321: step 3396, loss 0.143501, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:48.191172: step 3397, loss 0.135657, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:48.394622: step 3398, loss 0.169752, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:48.656117: step 3399, loss 0.0847176, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:48.916914: step 3400, loss 0.126322, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-10T11:57:49.500876: step 3400, loss 0.21836, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3400

2017-10-10T11:57:50.716041: step 3401, loss 0.135195, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:50.962290: step 3402, loss 0.0778786, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:51.222644: step 3403, loss 0.053125, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:51.447855: step 3404, loss 0.11055, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:51.677370: step 3405, loss 0.1203, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:51.918428: step 3406, loss 0.235516, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:52.150329: step 3407, loss 0.182246, acc 0.90625, learning_rate 0.000100004
2017-10-10T11:57:52.388018: step 3408, loss 0.0597935, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:52.637801: step 3409, loss 0.105687, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:52.868852: step 3410, loss 0.297999, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:53.077650: step 3411, loss 0.0901096, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:53.269087: step 3412, loss 0.155501, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:53.493541: step 3413, loss 0.0259728, acc 1, learning_rate 0.000100004
2017-10-10T11:57:53.757054: step 3414, loss 0.119233, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:53.981919: step 3415, loss 0.152486, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:54.233376: step 3416, loss 0.149495, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:54.446998: step 3417, loss 0.105948, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:54.658711: step 3418, loss 0.178778, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:54.914125: step 3419, loss 0.110103, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:55.150870: step 3420, loss 0.109144, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:55.383655: step 3421, loss 0.189564, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:55.717145: step 3422, loss 0.0698566, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:55.959416: step 3423, loss 0.112944, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:56.151355: step 3424, loss 0.0914354, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:56.356855: step 3425, loss 0.170626, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:56.597837: step 3426, loss 0.245333, acc 0.890625, learning_rate 0.000100004
2017-10-10T11:57:56.776949: step 3427, loss 0.13033, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:56.901712: step 3428, loss 0.114672, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:57.058594: step 3429, loss 0.107177, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:57.196830: step 3430, loss 0.0949623, acc 0.960784, learning_rate 0.000100004
2017-10-10T11:57:57.363980: step 3431, loss 0.0503138, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:57.537737: step 3432, loss 0.0546339, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:57.732932: step 3433, loss 0.0977657, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:57.989900: step 3434, loss 0.0712662, acc 1, learning_rate 0.000100004
2017-10-10T11:57:58.197059: step 3435, loss 0.167076, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:58.416516: step 3436, loss 0.103464, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:58.636988: step 3437, loss 0.0740857, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:58.863970: step 3438, loss 0.118415, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:59.116857: step 3439, loss 0.0521944, acc 1, learning_rate 0.000100004
2017-10-10T11:57:59.365218: step 3440, loss 0.134645, acc 0.9375, learning_rate 0.000100004

Evaluation:
2017-10-10T11:57:59.976847: step 3440, loss 0.216987, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3440

2017-10-10T11:58:01.079630: step 3441, loss 0.171136, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:58:01.341930: step 3442, loss 0.0892193, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:58:01.570451: step 3443, loss 0.103692, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:58:01.821081: step 3444, loss 0.103795, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:58:02.099030: step 3445, loss 0.0855954, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:58:02.341300: step 3446, loss 0.200339, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:58:02.581940: step 3447, loss 0.169804, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:58:02.856833: step 3448, loss 0.142975, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:58:03.105360: step 3449, loss 0.0844501, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:58:03.349217: step 3450, loss 0.056743, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:58:03.593046: step 3451, loss 0.0440244, acc 1, learning_rate 0.000100004
2017-10-10T11:58:03.839935: step 3452, loss 0.16467, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:58:04.120912: step 3453, loss 0.0501283, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:58:04.409630: step 3454, loss 0.164544, acc 0.890625, learning_rate 0.000100004
2017-10-10T11:58:04.607282: step 3455, loss 0.0761859, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:58:04.808605: step 3456, loss 0.0966167, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:58:04.997217: step 3457, loss 0.0593753, acc 1, learning_rate 0.000100004
2017-10-10T11:58:05.193391: step 3458, loss 0.13904, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:58:05.420134: step 3459, loss 0.132624, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:58:05.669496: step 3460, loss 0.223152, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:58:05.902355: step 3461, loss 0.107079, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:58:06.148875: step 3462, loss 0.118999, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:06.330427: step 3463, loss 0.146198, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:06.557009: step 3464, loss 0.17137, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:06.765376: step 3465, loss 0.110479, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:07.002338: step 3466, loss 0.0692908, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:07.247868: step 3467, loss 0.160854, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:07.440855: step 3468, loss 0.102227, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:07.734366: step 3469, loss 0.0562918, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:07.992952: step 3470, loss 0.078408, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:08.189447: step 3471, loss 0.105741, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:08.363245: step 3472, loss 0.0652902, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:08.532992: step 3473, loss 0.0840404, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:08.704899: step 3474, loss 0.127108, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:08.892191: step 3475, loss 0.0363285, acc 1, learning_rate 0.000100003
2017-10-10T11:58:09.110332: step 3476, loss 0.0334214, acc 1, learning_rate 0.000100003
2017-10-10T11:58:09.334995: step 3477, loss 0.0752477, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:09.596847: step 3478, loss 0.0577643, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:09.836239: step 3479, loss 0.14022, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:10.061674: step 3480, loss 0.138307, acc 0.984375, learning_rate 0.000100003

Evaluation:
2017-10-10T11:58:10.641676: step 3480, loss 0.216775, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3480

2017-10-10T11:58:11.842379: step 3481, loss 0.265843, acc 0.875, learning_rate 0.000100003
2017-10-10T11:58:12.085853: step 3482, loss 0.0881345, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:12.396906: step 3483, loss 0.0558559, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:12.609895: step 3484, loss 0.128343, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:12.802452: step 3485, loss 0.178427, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:13.009573: step 3486, loss 0.166941, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:58:13.213186: step 3487, loss 0.053618, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:13.416324: step 3488, loss 0.0779651, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:13.664425: step 3489, loss 0.0474998, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:13.912848: step 3490, loss 0.0781181, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:14.159261: step 3491, loss 0.149477, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:14.391442: step 3492, loss 0.0986583, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:14.626165: step 3493, loss 0.0586, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:14.844914: step 3494, loss 0.114026, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:15.076932: step 3495, loss 0.154009, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:15.300816: step 3496, loss 0.0527336, acc 1, learning_rate 0.000100003
2017-10-10T11:58:15.494388: step 3497, loss 0.0687883, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:15.719260: step 3498, loss 0.279416, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:15.957610: step 3499, loss 0.098507, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:16.197282: step 3500, loss 0.133543, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:16.419749: step 3501, loss 0.220391, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:16.664781: step 3502, loss 0.0932961, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:16.905847: step 3503, loss 0.117251, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:17.132121: step 3504, loss 0.215823, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:17.382101: step 3505, loss 0.0877819, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:17.609123: step 3506, loss 0.0807892, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:17.849149: step 3507, loss 0.0850428, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:18.096895: step 3508, loss 0.0491826, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:18.346402: step 3509, loss 0.120848, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:18.625019: step 3510, loss 0.145709, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:18.919812: step 3511, loss 0.17678, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:19.141193: step 3512, loss 0.118919, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:19.320620: step 3513, loss 0.0471993, acc 1, learning_rate 0.000100003
2017-10-10T11:58:19.499999: step 3514, loss 0.124206, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:19.677564: step 3515, loss 0.269965, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:58:19.849288: step 3516, loss 0.0808232, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:20.022593: step 3517, loss 0.077869, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:20.254255: step 3518, loss 0.0767481, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:20.471614: step 3519, loss 0.055884, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:20.749858: step 3520, loss 0.0738259, acc 1, learning_rate 0.000100003

Evaluation:
2017-10-10T11:58:21.272811: step 3520, loss 0.218752, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3520

2017-10-10T11:58:22.200984: step 3521, loss 0.242867, acc 0.890625, learning_rate 0.000100003
2017-10-10T11:58:22.409363: step 3522, loss 0.137983, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:22.639213: step 3523, loss 0.0898497, acc 1, learning_rate 0.000100003
2017-10-10T11:58:22.895295: step 3524, loss 0.133687, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:23.138292: step 3525, loss 0.112364, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:23.404957: step 3526, loss 0.0772826, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:23.624838: step 3527, loss 0.0839908, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:23.826979: step 3528, loss 0.019859, acc 1, learning_rate 0.000100003
2017-10-10T11:58:24.058296: step 3529, loss 0.104962, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:24.286162: step 3530, loss 0.11348, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:24.521403: step 3531, loss 0.154238, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:24.753284: step 3532, loss 0.0222044, acc 1, learning_rate 0.000100003
2017-10-10T11:58:25.007981: step 3533, loss 0.121125, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:25.244498: step 3534, loss 0.115168, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:25.468801: step 3535, loss 0.144906, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:25.697847: step 3536, loss 0.055609, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:25.942578: step 3537, loss 0.077064, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:26.164282: step 3538, loss 0.251648, acc 0.890625, learning_rate 0.000100003
2017-10-10T11:58:26.412884: step 3539, loss 0.128162, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:26.650947: step 3540, loss 0.145384, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:26.899935: step 3541, loss 0.165779, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:27.144862: step 3542, loss 0.148704, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:27.396563: step 3543, loss 0.0985494, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:27.644846: step 3544, loss 0.186446, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:27.875669: step 3545, loss 0.0398169, acc 1, learning_rate 0.000100002
2017-10-10T11:58:28.116757: step 3546, loss 0.205409, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:28.357845: step 3547, loss 0.137421, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:28.581518: step 3548, loss 0.0483141, acc 1, learning_rate 0.000100002
2017-10-10T11:58:28.839695: step 3549, loss 0.0700711, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:29.060092: step 3550, loss 0.108665, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:29.332911: step 3551, loss 0.0680113, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:29.672872: step 3552, loss 0.0820717, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:29.897305: step 3553, loss 0.0964609, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:30.018861: step 3554, loss 0.0665833, acc 1, learning_rate 0.000100002
2017-10-10T11:58:30.145359: step 3555, loss 0.096117, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:30.282641: step 3556, loss 0.0795189, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:30.413051: step 3557, loss 0.105708, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:30.538942: step 3558, loss 0.0555003, acc 1, learning_rate 0.000100002
2017-10-10T11:58:30.712241: step 3559, loss 0.133031, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:30.863284: step 3560, loss 0.0515693, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T11:58:31.408599: step 3560, loss 0.218791, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3560

2017-10-10T11:58:32.494612: step 3561, loss 0.091001, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:32.740841: step 3562, loss 0.0977568, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:32.956003: step 3563, loss 0.151083, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:33.176884: step 3564, loss 0.0849016, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:33.392668: step 3565, loss 0.0899378, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:33.618164: step 3566, loss 0.0830806, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:33.872861: step 3567, loss 0.0888736, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:34.116368: step 3568, loss 0.118703, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:34.331289: step 3569, loss 0.0999257, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:34.548884: step 3570, loss 0.12391, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:34.809037: step 3571, loss 0.159562, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:58:35.040436: step 3572, loss 0.0575961, acc 1, learning_rate 0.000100002
2017-10-10T11:58:35.337803: step 3573, loss 0.137007, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:35.580539: step 3574, loss 0.169875, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:35.807506: step 3575, loss 0.127365, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:36.061949: step 3576, loss 0.109704, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:36.281198: step 3577, loss 0.179511, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:36.535243: step 3578, loss 0.109021, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:36.765942: step 3579, loss 0.0938513, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:36.990228: step 3580, loss 0.0525237, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:37.216882: step 3581, loss 0.217879, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:37.480438: step 3582, loss 0.0408148, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:37.692875: step 3583, loss 0.143915, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:37.977107: step 3584, loss 0.139354, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:38.132995: step 3585, loss 0.090515, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:38.317103: step 3586, loss 0.178499, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:38.491250: step 3587, loss 0.0560944, acc 1, learning_rate 0.000100002
2017-10-10T11:58:38.690265: step 3588, loss 0.119776, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:38.900280: step 3589, loss 0.110397, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:39.153945: step 3590, loss 0.137654, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:39.407653: step 3591, loss 0.116626, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:39.646553: step 3592, loss 0.155332, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:39.882389: step 3593, loss 0.118967, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:40.156435: step 3594, loss 0.111336, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:40.345361: step 3595, loss 0.113732, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:40.548914: step 3596, loss 0.123671, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:40.856870: step 3597, loss 0.0693937, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:41.110825: step 3598, loss 0.0864515, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:41.265434: step 3599, loss 0.0713213, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:41.436792: step 3600, loss 0.0806136, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T11:58:41.906404: step 3600, loss 0.21728, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3600

2017-10-10T11:58:43.275238: step 3601, loss 0.110358, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:43.510396: step 3602, loss 0.0875906, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:43.741106: step 3603, loss 0.103963, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:43.986921: step 3604, loss 0.0591235, acc 1, learning_rate 0.000100002
2017-10-10T11:58:44.208825: step 3605, loss 0.0680508, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:44.450197: step 3606, loss 0.0756236, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:44.691460: step 3607, loss 0.220715, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:44.938623: step 3608, loss 0.141076, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:45.186791: step 3609, loss 0.0940257, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:45.398417: step 3610, loss 0.0622743, acc 1, learning_rate 0.000100002
2017-10-10T11:58:45.608663: step 3611, loss 0.0837148, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:45.823227: step 3612, loss 0.106937, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:46.109117: step 3613, loss 0.0748492, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:46.313019: step 3614, loss 0.0800053, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:46.495796: step 3615, loss 0.15309, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:46.692956: step 3616, loss 0.130406, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:46.899613: step 3617, loss 0.118117, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:47.110006: step 3618, loss 0.132512, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:47.305858: step 3619, loss 0.0959653, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:47.554995: step 3620, loss 0.167662, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:47.818560: step 3621, loss 0.0538193, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:48.051998: step 3622, loss 0.0717702, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:48.294386: step 3623, loss 0.0962116, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:48.568331: step 3624, loss 0.152681, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:48.795933: step 3625, loss 0.148327, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:49.010641: step 3626, loss 0.20654, acc 0.921569, learning_rate 0.000100002
2017-10-10T11:58:49.248851: step 3627, loss 0.193722, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:49.483367: step 3628, loss 0.128178, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:49.722047: step 3629, loss 0.0530414, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:49.970138: step 3630, loss 0.126624, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:50.201364: step 3631, loss 0.118881, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:50.450062: step 3632, loss 0.116315, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:50.679375: step 3633, loss 0.0956774, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:50.923304: step 3634, loss 0.135613, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:51.164060: step 3635, loss 0.0506043, acc 1, learning_rate 0.000100002
2017-10-10T11:58:51.473060: step 3636, loss 0.155488, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:51.774222: step 3637, loss 0.0259225, acc 1, learning_rate 0.000100002
2017-10-10T11:58:51.943545: step 3638, loss 0.0844232, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:52.114891: step 3639, loss 0.0412362, acc 1, learning_rate 0.000100002
2017-10-10T11:58:52.277246: step 3640, loss 0.0843859, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T11:58:52.753941: step 3640, loss 0.217173, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3640

2017-10-10T11:58:53.776228: step 3641, loss 0.148185, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:54.002405: step 3642, loss 0.180363, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:54.255316: step 3643, loss 0.112307, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:54.574934: step 3644, loss 0.089458, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:54.831170: step 3645, loss 0.0718486, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:55.030624: step 3646, loss 0.0858275, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:55.235223: step 3647, loss 0.0574941, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:55.441792: step 3648, loss 0.0702114, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:55.683655: step 3649, loss 0.117818, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:55.936846: step 3650, loss 0.0749373, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:56.189463: step 3651, loss 0.108981, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:56.422570: step 3652, loss 0.135009, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:56.658358: step 3653, loss 0.0414311, acc 1, learning_rate 0.000100002
2017-10-10T11:58:56.848844: step 3654, loss 0.145221, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:57.112347: step 3655, loss 0.0864585, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:57.344889: step 3656, loss 0.126192, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:57.584996: step 3657, loss 0.117391, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:57.803860: step 3658, loss 0.0996504, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:58.010694: step 3659, loss 0.0833163, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:58.216416: step 3660, loss 0.0604684, acc 1, learning_rate 0.000100002
2017-10-10T11:58:58.472144: step 3661, loss 0.170021, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:58.710706: step 3662, loss 0.100207, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:58.926947: step 3663, loss 0.0894827, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:59.176852: step 3664, loss 0.0898526, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:59.416756: step 3665, loss 0.101897, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:59.632680: step 3666, loss 0.171353, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:59.848826: step 3667, loss 0.0721846, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:59:00.097428: step 3668, loss 0.0753956, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:59:00.313053: step 3669, loss 0.058289, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:00.516008: step 3670, loss 0.1175, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:00.744752: step 3671, loss 0.147859, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:00.979874: step 3672, loss 0.114376, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:01.253514: step 3673, loss 0.193471, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:01.494139: step 3674, loss 0.154878, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:01.723946: step 3675, loss 0.044841, acc 1, learning_rate 0.000100001
2017-10-10T11:59:01.990709: step 3676, loss 0.0511905, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:02.180071: step 3677, loss 0.0777056, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:02.404947: step 3678, loss 0.205229, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:02.589029: step 3679, loss 0.0349569, acc 1, learning_rate 0.000100001
2017-10-10T11:59:02.903975: step 3680, loss 0.121789, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:03.486534: step 3680, loss 0.216572, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3680

2017-10-10T11:59:04.396628: step 3681, loss 0.0902791, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:04.632383: step 3682, loss 0.05347, acc 1, learning_rate 0.000100001
2017-10-10T11:59:04.873515: step 3683, loss 0.0909191, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:05.091295: step 3684, loss 0.0969017, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:05.337655: step 3685, loss 0.175449, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:05.617691: step 3686, loss 0.0512031, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:05.859693: step 3687, loss 0.218399, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:06.094049: step 3688, loss 0.177167, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:06.341361: step 3689, loss 0.0850766, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:06.588418: step 3690, loss 0.0654653, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:06.860861: step 3691, loss 0.0687418, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:07.090520: step 3692, loss 0.0929918, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:07.322660: step 3693, loss 0.0634675, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:07.563981: step 3694, loss 0.138855, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:07.804889: step 3695, loss 0.0755905, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:08.048375: step 3696, loss 0.137453, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:08.280195: step 3697, loss 0.19062, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:08.555517: step 3698, loss 0.0806172, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:08.790243: step 3699, loss 0.125692, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:09.027216: step 3700, loss 0.0878692, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:09.257073: step 3701, loss 0.0925938, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:09.493465: step 3702, loss 0.225748, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:09.727231: step 3703, loss 0.0627159, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:09.942703: step 3704, loss 0.102894, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:10.170212: step 3705, loss 0.101043, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:10.394593: step 3706, loss 0.0393777, acc 1, learning_rate 0.000100001
2017-10-10T11:59:10.625722: step 3707, loss 0.0826171, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:10.810312: step 3708, loss 0.0970867, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:11.026313: step 3709, loss 0.0580932, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:11.244148: step 3710, loss 0.0984736, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:11.472878: step 3711, loss 0.102613, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:11.726886: step 3712, loss 0.119306, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:11.951774: step 3713, loss 0.0352946, acc 1, learning_rate 0.000100001
2017-10-10T11:59:12.127502: step 3714, loss 0.10376, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:12.310604: step 3715, loss 0.118355, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:12.518585: step 3716, loss 0.0647383, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:12.714651: step 3717, loss 0.182934, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:12.920523: step 3718, loss 0.0891816, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:13.100328: step 3719, loss 0.0459101, acc 1, learning_rate 0.000100001
2017-10-10T11:59:13.320832: step 3720, loss 0.149008, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:13.892490: step 3720, loss 0.215708, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3720

2017-10-10T11:59:15.110770: step 3721, loss 0.0418317, acc 1, learning_rate 0.000100001
2017-10-10T11:59:15.364962: step 3722, loss 0.136725, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:15.603156: step 3723, loss 0.098373, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:15.801266: step 3724, loss 0.120474, acc 0.921569, learning_rate 0.000100001
2017-10-10T11:59:16.057361: step 3725, loss 0.0464166, acc 1, learning_rate 0.000100001
2017-10-10T11:59:16.289965: step 3726, loss 0.105298, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:16.571260: step 3727, loss 0.0935762, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:16.798803: step 3728, loss 0.0896977, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:17.020205: step 3729, loss 0.0793155, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:17.265814: step 3730, loss 0.128544, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:17.489908: step 3731, loss 0.16088, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:17.721867: step 3732, loss 0.18771, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:17.968855: step 3733, loss 0.120738, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:18.238714: step 3734, loss 0.0785234, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:18.482598: step 3735, loss 0.103711, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:18.756191: step 3736, loss 0.0782564, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:18.977741: step 3737, loss 0.0488599, acc 1, learning_rate 0.000100001
2017-10-10T11:59:19.211643: step 3738, loss 0.11902, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:19.443284: step 3739, loss 0.176764, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:19.672576: step 3740, loss 0.0947687, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:19.838877: step 3741, loss 0.105666, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:20.041456: step 3742, loss 0.0951073, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:20.332877: step 3743, loss 0.0955519, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:20.592790: step 3744, loss 0.132956, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:20.791508: step 3745, loss 0.0437959, acc 1, learning_rate 0.000100001
2017-10-10T11:59:20.986203: step 3746, loss 0.116395, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:21.184725: step 3747, loss 0.135909, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:21.370134: step 3748, loss 0.154756, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:21.600694: step 3749, loss 0.118541, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:21.832885: step 3750, loss 0.0885414, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:22.068871: step 3751, loss 0.0765637, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:22.296659: step 3752, loss 0.1225, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:22.542450: step 3753, loss 0.0505483, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:22.790156: step 3754, loss 0.0752949, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:23.003106: step 3755, loss 0.070498, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:23.210685: step 3756, loss 0.0947389, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:23.422552: step 3757, loss 0.133693, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:23.649281: step 3758, loss 0.0895852, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:23.865493: step 3759, loss 0.246601, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:24.141016: step 3760, loss 0.161262, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:24.763795: step 3760, loss 0.217999, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3760

2017-10-10T11:59:25.700788: step 3761, loss 0.0567579, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:25.891505: step 3762, loss 0.0837172, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:26.060072: step 3763, loss 0.111066, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:26.236862: step 3764, loss 0.174588, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:26.452222: step 3765, loss 0.102334, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:26.700027: step 3766, loss 0.11252, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:26.938448: step 3767, loss 0.0873844, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:27.183622: step 3768, loss 0.0629169, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:27.416872: step 3769, loss 0.0561571, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:27.638244: step 3770, loss 0.118211, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:27.891123: step 3771, loss 0.130513, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:28.141671: step 3772, loss 0.131314, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:28.368871: step 3773, loss 0.0766103, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:28.610445: step 3774, loss 0.108703, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:28.891824: step 3775, loss 0.101249, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:29.078686: step 3776, loss 0.0912222, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:29.264780: step 3777, loss 0.0536472, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:29.452855: step 3778, loss 0.0804546, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:29.615371: step 3779, loss 0.114085, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:29.850878: step 3780, loss 0.148815, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:30.060873: step 3781, loss 0.0784212, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:30.247984: step 3782, loss 0.0957891, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:30.463940: step 3783, loss 0.129327, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:30.675136: step 3784, loss 0.0279512, acc 1, learning_rate 0.000100001
2017-10-10T11:59:30.909548: step 3785, loss 0.0840831, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:31.132877: step 3786, loss 0.125042, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:31.377736: step 3787, loss 0.118317, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:31.643721: step 3788, loss 0.0488308, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:31.870794: step 3789, loss 0.171138, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:32.094536: step 3790, loss 0.0939562, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:32.324880: step 3791, loss 0.0385245, acc 1, learning_rate 0.000100001
2017-10-10T11:59:32.521796: step 3792, loss 0.09666, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:32.718694: step 3793, loss 0.126873, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:32.945423: step 3794, loss 0.119918, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:33.168899: step 3795, loss 0.119671, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:33.412893: step 3796, loss 0.15695, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:33.670013: step 3797, loss 0.210116, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:33.904606: step 3798, loss 0.0617911, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:34.144068: step 3799, loss 0.0837124, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:34.364624: step 3800, loss 0.0891892, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:34.973817: step 3800, loss 0.216484, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3800

2017-10-10T11:59:36.168857: step 3801, loss 0.0380269, acc 1, learning_rate 0.000100001
2017-10-10T11:59:36.433326: step 3802, loss 0.125397, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:36.600961: step 3803, loss 0.122399, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:36.762399: step 3804, loss 0.200618, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:36.927793: step 3805, loss 0.102712, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:37.085577: step 3806, loss 0.0897011, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:37.256869: step 3807, loss 0.0712624, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:37.508495: step 3808, loss 0.0395524, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:37.707712: step 3809, loss 0.161832, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:37.905011: step 3810, loss 0.0509711, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:38.080833: step 3811, loss 0.0504063, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:38.262419: step 3812, loss 0.17316, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:38.406199: step 3813, loss 0.115488, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:38.583867: step 3814, loss 0.0667786, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:38.831001: step 3815, loss 0.05008, acc 1, learning_rate 0.000100001
2017-10-10T11:59:39.122272: step 3816, loss 0.0797782, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:39.357254: step 3817, loss 0.103503, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:39.604200: step 3818, loss 0.121991, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:39.828631: step 3819, loss 0.117254, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:40.071705: step 3820, loss 0.125514, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:40.311952: step 3821, loss 0.135103, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:40.477898: step 3822, loss 0.0843412, acc 0.960784, learning_rate 0.000100001
2017-10-10T11:59:40.708831: step 3823, loss 0.168445, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:40.944356: step 3824, loss 0.0941646, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:41.172750: step 3825, loss 0.0643889, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:41.376540: step 3826, loss 0.0987747, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:41.603128: step 3827, loss 0.0538228, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:41.846950: step 3828, loss 0.0569373, acc 1, learning_rate 0.000100001
2017-10-10T11:59:42.092849: step 3829, loss 0.0744642, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:42.359144: step 3830, loss 0.0857313, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:42.604465: step 3831, loss 0.0259078, acc 1, learning_rate 0.000100001
2017-10-10T11:59:42.838275: step 3832, loss 0.0491824, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:43.041003: step 3833, loss 0.0855887, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:43.249190: step 3834, loss 0.0822291, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:43.479433: step 3835, loss 0.0931789, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:43.688827: step 3836, loss 0.0786709, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:43.944911: step 3837, loss 0.200949, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:44.178964: step 3838, loss 0.0624059, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:44.421042: step 3839, loss 0.0418655, acc 1, learning_rate 0.000100001
2017-10-10T11:59:44.674635: step 3840, loss 0.0345468, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:45.297954: step 3840, loss 0.215756, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3840

2017-10-10T11:59:46.511738: step 3841, loss 0.0367371, acc 1, learning_rate 0.000100001
2017-10-10T11:59:46.710681: step 3842, loss 0.117674, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:46.913989: step 3843, loss 0.106697, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:47.169049: step 3844, loss 0.0866745, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:47.497655: step 3845, loss 0.127879, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:47.705357: step 3846, loss 0.142114, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:47.874361: step 3847, loss 0.141071, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:48.044407: step 3848, loss 0.112417, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:48.215965: step 3849, loss 0.0658739, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:48.377967: step 3850, loss 0.0723799, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:48.620863: step 3851, loss 0.0988203, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:48.851744: step 3852, loss 0.125007, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:49.093723: step 3853, loss 0.0974608, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:49.376677: step 3854, loss 0.0703131, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:49.623728: step 3855, loss 0.0866169, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:49.887016: step 3856, loss 0.160194, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:50.116373: step 3857, loss 0.063846, acc 1, learning_rate 0.000100001
2017-10-10T11:59:50.352887: step 3858, loss 0.121383, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:50.612883: step 3859, loss 0.0715579, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:50.828849: step 3860, loss 0.0293466, acc 1, learning_rate 0.000100001
2017-10-10T11:59:51.093159: step 3861, loss 0.121276, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:51.319989: step 3862, loss 0.144366, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:51.543591: step 3863, loss 0.093393, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:51.741035: step 3864, loss 0.256356, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:52.012851: step 3865, loss 0.0530238, acc 1, learning_rate 0.000100001
2017-10-10T11:59:52.228829: step 3866, loss 0.0829389, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:52.453126: step 3867, loss 0.113866, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:52.706101: step 3868, loss 0.0987484, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:52.977093: step 3869, loss 0.069659, acc 1, learning_rate 0.000100001
2017-10-10T11:59:53.262465: step 3870, loss 0.104087, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:53.516860: step 3871, loss 0.172072, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:53.755502: step 3872, loss 0.0986242, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:54.028869: step 3873, loss 0.0710734, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:54.337924: step 3874, loss 0.116309, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:54.544216: step 3875, loss 0.097282, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:54.735388: step 3876, loss 0.100046, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:54.907481: step 3877, loss 0.143204, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:55.077886: step 3878, loss 0.148445, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:55.229649: step 3879, loss 0.136043, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:55.464364: step 3880, loss 0.0850636, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:56.103450: step 3880, loss 0.215773, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3880

2017-10-10T11:59:57.053273: step 3881, loss 0.0755447, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:57.270246: step 3882, loss 0.175442, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:57.509319: step 3883, loss 0.11748, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:57.757744: step 3884, loss 0.0769525, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:58.019002: step 3885, loss 0.0947755, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:58.264332: step 3886, loss 0.0496422, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:58.538293: step 3887, loss 0.0808514, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:58.757161: step 3888, loss 0.0765573, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:58.913069: step 3889, loss 0.0409443, acc 1, learning_rate 0.000100001
2017-10-10T11:59:59.077940: step 3890, loss 0.0869052, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:59.262178: step 3891, loss 0.183802, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:59.447396: step 3892, loss 0.152552, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:59.623716: step 3893, loss 0.101849, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:59.839518: step 3894, loss 0.092452, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:00.073381: step 3895, loss 0.200938, acc 0.9375, learning_rate 0.000100001
2017-10-10T12:00:00.295996: step 3896, loss 0.12926, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:00.520686: step 3897, loss 0.110877, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:00.717521: step 3898, loss 0.128345, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:00.948889: step 3899, loss 0.169419, acc 0.9375, learning_rate 0.000100001
2017-10-10T12:00:01.172946: step 3900, loss 0.177325, acc 0.9375, learning_rate 0.000100001
2017-10-10T12:00:01.432101: step 3901, loss 0.10607, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:01.660595: step 3902, loss 0.0868793, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:01.892886: step 3903, loss 0.137549, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:02.169121: step 3904, loss 0.12356, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:02.456867: step 3905, loss 0.0871132, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:02.727993: step 3906, loss 0.0763351, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:02.925590: step 3907, loss 0.177519, acc 0.9375, learning_rate 0.000100001
2017-10-10T12:00:03.128630: step 3908, loss 0.167575, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:03.315261: step 3909, loss 0.0613975, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:03.518287: step 3910, loss 0.0657793, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:03.743904: step 3911, loss 0.154924, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:03.981085: step 3912, loss 0.0824043, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:04.216065: step 3913, loss 0.0661978, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:04.443635: step 3914, loss 0.107106, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:04.702359: step 3915, loss 0.119755, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:04.942485: step 3916, loss 0.0646034, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:05.183724: step 3917, loss 0.120144, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:05.416540: step 3918, loss 0.0689682, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:05.653217: step 3919, loss 0.138411, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:05.868791: step 3920, loss 0.0863719, acc 0.980392, learning_rate 0.000100001

Evaluation:
2017-10-10T12:00:06.477619: step 3920, loss 0.214155, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3920

2017-10-10T12:00:07.644956: step 3921, loss 0.16689, acc 0.9375, learning_rate 0.000100001
2017-10-10T12:00:07.877951: step 3922, loss 0.11653, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:08.132904: step 3923, loss 0.0874219, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:08.376842: step 3924, loss 0.122457, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:08.605804: step 3925, loss 0.0821278, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:08.876985: step 3926, loss 0.18138, acc 0.921875, learning_rate 0.000100001
2017-10-10T12:00:09.091233: step 3927, loss 0.117238, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:09.324812: step 3928, loss 0.0820981, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:09.635864: step 3929, loss 0.0668377, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:09.879855: step 3930, loss 0.0932145, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:10.045241: step 3931, loss 0.0824075, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:10.220889: step 3932, loss 0.0808868, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:10.378224: step 3933, loss 0.0872485, acc 0.984375, learning_rate 0.000100001
2017-10-10T12:00:10.528885: step 3934, loss 0.104122, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:10.673195: step 3935, loss 0.105148, acc 0.96875, learning_rate 0.000100001
2017-10-10T12:00:10.981072: step 3936, loss 0.140132, acc 0.953125, learning_rate 0.000100001
2017-10-10T12:00:11.184886: step 3937, loss 0.0873199, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:11.365113: step 3938, loss 0.145513, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:11.569786: step 3939, loss 0.0845096, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:11.772856: step 3940, loss 0.190464, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:11.936132: step 3941, loss 0.102801, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:12.122022: step 3942, loss 0.166133, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:12.283994: step 3943, loss 0.114907, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:12.528242: step 3944, loss 0.0825471, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:12.795609: step 3945, loss 0.0658072, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:13.027546: step 3946, loss 0.0932707, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:13.260861: step 3947, loss 0.14457, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:13.475448: step 3948, loss 0.13057, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:13.708963: step 3949, loss 0.0776667, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:13.945160: step 3950, loss 0.142443, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:14.162457: step 3951, loss 0.143503, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:14.385291: step 3952, loss 0.0830764, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:14.612941: step 3953, loss 0.0788396, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:14.806784: step 3954, loss 0.0652543, acc 1, learning_rate 0.0001
2017-10-10T12:00:15.048923: step 3955, loss 0.124201, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:15.248950: step 3956, loss 0.0478268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:15.461013: step 3957, loss 0.0915038, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:15.713992: step 3958, loss 0.110587, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:15.960584: step 3959, loss 0.085881, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:16.206056: step 3960, loss 0.0990171, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:16.815837: step 3960, loss 0.215159, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-3960

2017-10-10T12:00:18.076768: step 3961, loss 0.189267, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:18.300867: step 3962, loss 0.0664579, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:18.533190: step 3963, loss 0.0404542, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:18.742292: step 3964, loss 0.180231, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:18.968944: step 3965, loss 0.138145, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:19.188922: step 3966, loss 0.0491948, acc 1, learning_rate 0.0001
2017-10-10T12:00:19.433345: step 3967, loss 0.0561711, acc 1, learning_rate 0.0001
2017-10-10T12:00:19.738946: step 3968, loss 0.0373867, acc 1, learning_rate 0.0001
2017-10-10T12:00:19.993284: step 3969, loss 0.0549944, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:20.196246: step 3970, loss 0.161734, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:20.490113: step 3971, loss 0.07421, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:20.668941: step 3972, loss 0.0882268, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:20.892978: step 3973, loss 0.090336, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:21.081328: step 3974, loss 0.147039, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:21.237731: step 3975, loss 0.0469548, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:21.437602: step 3976, loss 0.0867171, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:21.609860: step 3977, loss 0.0310179, acc 1, learning_rate 0.0001
2017-10-10T12:00:21.835626: step 3978, loss 0.0548365, acc 1, learning_rate 0.0001
2017-10-10T12:00:22.101942: step 3979, loss 0.0914554, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:22.350488: step 3980, loss 0.128152, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:22.617611: step 3981, loss 0.0343424, acc 1, learning_rate 0.0001
2017-10-10T12:00:22.868249: step 3982, loss 0.0792843, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:23.130270: step 3983, loss 0.0782455, acc 1, learning_rate 0.0001
2017-10-10T12:00:23.407175: step 3984, loss 0.195604, acc 0.875, learning_rate 0.0001
2017-10-10T12:00:23.676803: step 3985, loss 0.0493723, acc 1, learning_rate 0.0001
2017-10-10T12:00:23.903058: step 3986, loss 0.0765409, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:24.150698: step 3987, loss 0.0419147, acc 1, learning_rate 0.0001
2017-10-10T12:00:24.404807: step 3988, loss 0.0592924, acc 1, learning_rate 0.0001
2017-10-10T12:00:24.662489: step 3989, loss 0.224647, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:24.890198: step 3990, loss 0.131369, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:25.120052: step 3991, loss 0.0964322, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:25.372973: step 3992, loss 0.0619416, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:25.604461: step 3993, loss 0.0766806, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:25.857015: step 3994, loss 0.0924403, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:26.128603: step 3995, loss 0.143438, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:26.372529: step 3996, loss 0.0834559, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:26.609814: step 3997, loss 0.0677462, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:26.873421: step 3998, loss 0.100485, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:27.121447: step 3999, loss 0.112581, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:27.360185: step 4000, loss 0.0659175, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:28.012907: step 4000, loss 0.216015, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4000

2017-10-10T12:00:28.937086: step 4001, loss 0.134376, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:29.148894: step 4002, loss 0.161273, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:29.392878: step 4003, loss 0.0926018, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:29.652537: step 4004, loss 0.0976089, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:29.889155: step 4005, loss 0.0917156, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:30.128747: step 4006, loss 0.0528865, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:30.345046: step 4007, loss 0.0785075, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:30.553045: step 4008, loss 0.112207, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:30.764292: step 4009, loss 0.0853743, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:30.996671: step 4010, loss 0.143887, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:31.244942: step 4011, loss 0.150404, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:31.612403: step 4012, loss 0.157053, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:31.883973: step 4013, loss 0.128864, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:32.065083: step 4014, loss 0.101321, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:32.235671: step 4015, loss 0.193056, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:32.404873: step 4016, loss 0.0508028, acc 1, learning_rate 0.0001
2017-10-10T12:00:32.581501: step 4017, loss 0.068465, acc 1, learning_rate 0.0001
2017-10-10T12:00:32.748179: step 4018, loss 0.116702, acc 0.960784, learning_rate 0.0001
2017-10-10T12:00:33.004276: step 4019, loss 0.041553, acc 1, learning_rate 0.0001
2017-10-10T12:00:33.250954: step 4020, loss 0.0775844, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:33.492616: step 4021, loss 0.0934601, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:33.709477: step 4022, loss 0.206678, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:33.954015: step 4023, loss 0.0379774, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:34.198607: step 4024, loss 0.110513, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:34.432528: step 4025, loss 0.0905692, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:34.647336: step 4026, loss 0.152626, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:34.872453: step 4027, loss 0.138833, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:35.120893: step 4028, loss 0.0791417, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:35.351060: step 4029, loss 0.0979132, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:35.604288: step 4030, loss 0.0895361, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:35.870747: step 4031, loss 0.188022, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:36.116927: step 4032, loss 0.156024, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:36.417843: step 4033, loss 0.0451179, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:36.652902: step 4034, loss 0.0641794, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:36.847195: step 4035, loss 0.127452, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:37.056424: step 4036, loss 0.14298, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:37.268567: step 4037, loss 0.0516685, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:37.488396: step 4038, loss 0.0660782, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:37.713005: step 4039, loss 0.0639208, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:37.933044: step 4040, loss 0.187455, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:38.556475: step 4040, loss 0.214967, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4040

2017-10-10T12:00:39.743165: step 4041, loss 0.0532298, acc 1, learning_rate 0.0001
2017-10-10T12:00:39.969125: step 4042, loss 0.16725, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:40.200844: step 4043, loss 0.0552513, acc 1, learning_rate 0.0001
2017-10-10T12:00:40.424578: step 4044, loss 0.0673313, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:40.674960: step 4045, loss 0.0839957, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:40.890632: step 4046, loss 0.212028, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:41.123875: step 4047, loss 0.11826, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:41.356284: step 4048, loss 0.0875611, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:41.569500: step 4049, loss 0.0433699, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:41.798870: step 4050, loss 0.0567186, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:42.021161: step 4051, loss 0.080309, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:42.296390: step 4052, loss 0.140615, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:42.561335: step 4053, loss 0.108398, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:42.827671: step 4054, loss 0.0606326, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:43.004915: step 4055, loss 0.17589, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:43.172876: step 4056, loss 0.0837707, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:43.346522: step 4057, loss 0.112933, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:43.510612: step 4058, loss 0.14773, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:43.679469: step 4059, loss 0.095716, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:43.845043: step 4060, loss 0.0768195, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:44.090058: step 4061, loss 0.106538, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:44.332975: step 4062, loss 0.120175, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:44.570785: step 4063, loss 0.0544303, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:44.853047: step 4064, loss 0.0998553, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:45.100193: step 4065, loss 0.0733415, acc 1, learning_rate 0.0001
2017-10-10T12:00:45.264818: step 4066, loss 0.07706, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:45.446476: step 4067, loss 0.105454, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:45.628829: step 4068, loss 0.0519077, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:45.793189: step 4069, loss 0.087017, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:46.013315: step 4070, loss 0.101644, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:46.256063: step 4071, loss 0.213787, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:46.505443: step 4072, loss 0.200862, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:46.725165: step 4073, loss 0.175773, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:46.950042: step 4074, loss 0.112853, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:47.166261: step 4075, loss 0.237289, acc 0.890625, learning_rate 0.0001
2017-10-10T12:00:47.427457: step 4076, loss 0.114478, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:47.653708: step 4077, loss 0.0833543, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:47.891051: step 4078, loss 0.0929954, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:48.133351: step 4079, loss 0.18421, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:48.371123: step 4080, loss 0.0694002, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:48.968865: step 4080, loss 0.217616, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4080

2017-10-10T12:00:50.152844: step 4081, loss 0.182282, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:50.372897: step 4082, loss 0.0754597, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:50.588878: step 4083, loss 0.0859858, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:50.809095: step 4084, loss 0.0419559, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:51.020526: step 4085, loss 0.20094, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:51.243607: step 4086, loss 0.0497036, acc 1, learning_rate 0.0001
2017-10-10T12:00:51.505130: step 4087, loss 0.0666131, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:51.760306: step 4088, loss 0.0874813, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:52.021883: step 4089, loss 0.0857752, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:52.283699: step 4090, loss 0.0963383, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:52.539908: step 4091, loss 0.16192, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:52.780339: step 4092, loss 0.0801359, acc 1, learning_rate 0.0001
2017-10-10T12:00:53.072951: step 4093, loss 0.141192, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:53.366585: step 4094, loss 0.115203, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:53.561834: step 4095, loss 0.0795326, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:53.796372: step 4096, loss 0.0794922, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:54.027984: step 4097, loss 0.123541, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:54.219942: step 4098, loss 0.109717, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:54.339011: step 4099, loss 0.154732, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:54.507308: step 4100, loss 0.0565168, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:54.704174: step 4101, loss 0.0927282, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:54.885687: step 4102, loss 0.0637914, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:55.112894: step 4103, loss 0.0809632, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:55.329031: step 4104, loss 0.0507388, acc 1, learning_rate 0.0001
2017-10-10T12:00:55.548913: step 4105, loss 0.119616, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:55.767205: step 4106, loss 0.113109, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:55.994383: step 4107, loss 0.116729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:56.201159: step 4108, loss 0.099214, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:56.446804: step 4109, loss 0.190911, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:56.690601: step 4110, loss 0.0322784, acc 1, learning_rate 0.0001
2017-10-10T12:00:56.929755: step 4111, loss 0.0997504, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:57.152015: step 4112, loss 0.099611, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:57.356998: step 4113, loss 0.0433232, acc 1, learning_rate 0.0001
2017-10-10T12:00:57.624855: step 4114, loss 0.221807, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:57.860876: step 4115, loss 0.115963, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:58.057401: step 4116, loss 0.116044, acc 0.941176, learning_rate 0.0001
2017-10-10T12:00:58.280891: step 4117, loss 0.134594, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:58.484877: step 4118, loss 0.0442713, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:58.724980: step 4119, loss 0.143799, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:58.947131: step 4120, loss 0.0883342, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:59.623962: step 4120, loss 0.214586, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4120

2017-10-10T12:01:00.876243: step 4121, loss 0.126071, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:01.113675: step 4122, loss 0.0923158, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:01.406076: step 4123, loss 0.112639, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:01.649746: step 4124, loss 0.155138, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:01.844694: step 4125, loss 0.0599448, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:02.040081: step 4126, loss 0.0948544, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:02.235779: step 4127, loss 0.124942, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:02.427919: step 4128, loss 0.171672, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:02.599692: step 4129, loss 0.080779, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:02.820853: step 4130, loss 0.0806471, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:03.026977: step 4131, loss 0.0667612, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:03.248459: step 4132, loss 0.103163, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:03.452839: step 4133, loss 0.159687, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:03.713121: step 4134, loss 0.0819381, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:03.943691: step 4135, loss 0.0783561, acc 1, learning_rate 0.0001
2017-10-10T12:01:04.176884: step 4136, loss 0.0899791, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:04.432922: step 4137, loss 0.102905, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:04.697841: step 4138, loss 0.0963391, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:04.933116: step 4139, loss 0.0839894, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:05.181756: step 4140, loss 0.0883467, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:05.361399: step 4141, loss 0.0577268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:05.555394: step 4142, loss 0.144037, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:05.728827: step 4143, loss 0.0343582, acc 1, learning_rate 0.0001
2017-10-10T12:01:05.918790: step 4144, loss 0.0386327, acc 1, learning_rate 0.0001
2017-10-10T12:01:06.099284: step 4145, loss 0.0921315, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:06.325478: step 4146, loss 0.0958428, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:06.570234: step 4147, loss 0.0678447, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:06.811461: step 4148, loss 0.0783782, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:07.034178: step 4149, loss 0.126209, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:07.285544: step 4150, loss 0.0400267, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:07.496741: step 4151, loss 0.0400101, acc 1, learning_rate 0.0001
2017-10-10T12:01:07.744885: step 4152, loss 0.105546, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:07.976854: step 4153, loss 0.0989876, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:08.197041: step 4154, loss 0.103477, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:08.446084: step 4155, loss 0.120585, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:08.668894: step 4156, loss 0.0130446, acc 1, learning_rate 0.0001
2017-10-10T12:01:08.875298: step 4157, loss 0.110564, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:09.093456: step 4158, loss 0.0760258, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:09.313488: step 4159, loss 0.196208, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:09.548780: step 4160, loss 0.0970957, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:10.287219: step 4160, loss 0.216275, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4160

2017-10-10T12:01:11.196111: step 4161, loss 0.0751898, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:11.442186: step 4162, loss 0.135596, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:11.638966: step 4163, loss 0.0615735, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:11.861037: step 4164, loss 0.0711906, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:12.069695: step 4165, loss 0.0354563, acc 1, learning_rate 0.0001
2017-10-10T12:01:12.286270: step 4166, loss 0.0930334, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:12.529168: step 4167, loss 0.126079, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:12.768955: step 4168, loss 0.0672595, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:13.035176: step 4169, loss 0.161287, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:13.268624: step 4170, loss 0.0493031, acc 1, learning_rate 0.0001
2017-10-10T12:01:13.479854: step 4171, loss 0.119762, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:13.720852: step 4172, loss 0.0940987, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:13.981440: step 4173, loss 0.0839971, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:14.236031: step 4174, loss 0.0781899, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:14.510879: step 4175, loss 0.11062, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:14.760918: step 4176, loss 0.149506, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:15.032995: step 4177, loss 0.187763, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:15.252342: step 4178, loss 0.0672504, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:15.501066: step 4179, loss 0.161104, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:15.828861: step 4180, loss 0.131246, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:16.103750: step 4181, loss 0.0961559, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:16.302927: step 4182, loss 0.142424, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:16.469470: step 4183, loss 0.145883, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:16.633126: step 4184, loss 0.064917, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:16.804843: step 4185, loss 0.159928, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:16.962051: step 4186, loss 0.0918412, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:17.127724: step 4187, loss 0.0837867, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:17.350228: step 4188, loss 0.133036, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:17.592008: step 4189, loss 0.0831339, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:17.829448: step 4190, loss 0.169204, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:18.100871: step 4191, loss 0.0757235, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:18.388970: step 4192, loss 0.152712, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:18.575329: step 4193, loss 0.179804, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:18.770267: step 4194, loss 0.157767, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:18.963730: step 4195, loss 0.128382, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:19.161901: step 4196, loss 0.0543235, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:19.348045: step 4197, loss 0.187762, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:19.568238: step 4198, loss 0.0735843, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:19.820802: step 4199, loss 0.161241, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:20.078279: step 4200, loss 0.107821, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:20.772291: step 4200, loss 0.2141, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4200

2017-10-10T12:01:21.908855: step 4201, loss 0.0531494, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:22.117506: step 4202, loss 0.120037, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:22.374388: step 4203, loss 0.105727, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:22.599832: step 4204, loss 0.0719459, acc 1, learning_rate 0.0001
2017-10-10T12:01:22.802697: step 4205, loss 0.178961, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:23.028860: step 4206, loss 0.127216, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:23.270872: step 4207, loss 0.0867328, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:23.485583: step 4208, loss 0.0621117, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:23.716893: step 4209, loss 0.110638, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:23.950451: step 4210, loss 0.0793018, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:24.183289: step 4211, loss 0.0657165, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:24.452830: step 4212, loss 0.096248, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:24.683303: step 4213, loss 0.0314169, acc 1, learning_rate 0.0001
2017-10-10T12:01:24.894246: step 4214, loss 0.0978987, acc 0.980392, learning_rate 0.0001
2017-10-10T12:01:25.121597: step 4215, loss 0.0831268, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:25.378002: step 4216, loss 0.0422407, acc 1, learning_rate 0.0001
2017-10-10T12:01:25.628809: step 4217, loss 0.0771924, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:25.904883: step 4218, loss 0.132629, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:26.143123: step 4219, loss 0.139777, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:26.382888: step 4220, loss 0.0627304, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:26.596872: step 4221, loss 0.0765604, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:26.914153: step 4222, loss 0.0914742, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:27.183375: step 4223, loss 0.104189, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:27.810357: step 4224, loss 0.102786, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:27.953266: step 4225, loss 0.11403, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:28.072388: step 4226, loss 0.0835051, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:28.196617: step 4227, loss 0.118453, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:28.317622: step 4228, loss 0.138153, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:28.447380: step 4229, loss 0.0535435, acc 1, learning_rate 0.0001
2017-10-10T12:01:28.576650: step 4230, loss 0.112978, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:28.836860: step 4231, loss 0.0540424, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:29.070135: step 4232, loss 0.0995692, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:29.298593: step 4233, loss 0.0719344, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:29.506443: step 4234, loss 0.0376794, acc 1, learning_rate 0.0001
2017-10-10T12:01:29.724975: step 4235, loss 0.0938226, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:29.954037: step 4236, loss 0.129451, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:30.166494: step 4237, loss 0.11692, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:30.379929: step 4238, loss 0.192831, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:30.601603: step 4239, loss 0.0521083, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:30.881478: step 4240, loss 0.16437, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:31.476903: step 4240, loss 0.212728, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4240

2017-10-10T12:01:32.770885: step 4241, loss 0.123222, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:32.960525: step 4242, loss 0.0579525, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:33.182334: step 4243, loss 0.082061, acc 1, learning_rate 0.0001
2017-10-10T12:01:33.370126: step 4244, loss 0.130556, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:33.603393: step 4245, loss 0.100609, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:33.885489: step 4246, loss 0.184863, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:34.126293: step 4247, loss 0.0647573, acc 1, learning_rate 0.0001
2017-10-10T12:01:34.401108: step 4248, loss 0.0335785, acc 1, learning_rate 0.0001
2017-10-10T12:01:34.624345: step 4249, loss 0.106548, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:34.863035: step 4250, loss 0.157563, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:35.069746: step 4251, loss 0.0546172, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:35.344358: step 4252, loss 0.0455297, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:35.578005: step 4253, loss 0.09544, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:35.864883: step 4254, loss 0.163878, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:36.084895: step 4255, loss 0.116751, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:36.261181: step 4256, loss 0.0452431, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:36.454929: step 4257, loss 0.078219, acc 1, learning_rate 0.0001
2017-10-10T12:01:36.627510: step 4258, loss 0.0912193, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:36.806338: step 4259, loss 0.127825, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:37.004839: step 4260, loss 0.111553, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:37.181015: step 4261, loss 0.0601191, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:37.436517: step 4262, loss 0.0691148, acc 1, learning_rate 0.0001
2017-10-10T12:01:37.680663: step 4263, loss 0.0660462, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:37.919082: step 4264, loss 0.151561, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:38.171420: step 4265, loss 0.0416273, acc 1, learning_rate 0.0001
2017-10-10T12:01:38.396174: step 4266, loss 0.0922076, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:38.664843: step 4267, loss 0.177833, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:38.925063: step 4268, loss 0.0571743, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:39.134627: step 4269, loss 0.125451, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:39.317838: step 4270, loss 0.0815788, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:39.488438: step 4271, loss 0.0715252, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:39.659029: step 4272, loss 0.104482, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:39.888980: step 4273, loss 0.131502, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:40.084867: step 4274, loss 0.0852828, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:40.262092: step 4275, loss 0.214276, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:40.519520: step 4276, loss 0.0424769, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:40.740833: step 4277, loss 0.175124, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:40.961450: step 4278, loss 0.0912173, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:41.235734: step 4279, loss 0.0607765, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:41.488698: step 4280, loss 0.231786, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:42.108960: step 4280, loss 0.214599, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4280

2017-10-10T12:01:43.174352: step 4281, loss 0.0993622, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:43.400104: step 4282, loss 0.0678209, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:43.632851: step 4283, loss 0.0422787, acc 1, learning_rate 0.0001
2017-10-10T12:01:43.834378: step 4284, loss 0.0881833, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:44.034428: step 4285, loss 0.0712165, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:44.241303: step 4286, loss 0.0746964, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:44.452825: step 4287, loss 0.112747, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:44.741360: step 4288, loss 0.118062, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:44.962312: step 4289, loss 0.0518083, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:45.151157: step 4290, loss 0.0617104, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:45.314817: step 4291, loss 0.109254, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:45.472328: step 4292, loss 0.0458255, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:45.660353: step 4293, loss 0.0808293, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:45.901540: step 4294, loss 0.0588059, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:46.152538: step 4295, loss 0.152385, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:46.384630: step 4296, loss 0.0612934, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:46.617153: step 4297, loss 0.0513753, acc 1, learning_rate 0.0001
2017-10-10T12:01:46.862870: step 4298, loss 0.0706591, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:47.104357: step 4299, loss 0.112246, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:47.312826: step 4300, loss 0.143402, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:47.551530: step 4301, loss 0.0897818, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:47.750706: step 4302, loss 0.119087, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:47.992891: step 4303, loss 0.0300416, acc 1, learning_rate 0.0001
2017-10-10T12:01:48.211699: step 4304, loss 0.139108, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:48.416513: step 4305, loss 0.0714613, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:48.631696: step 4306, loss 0.0455417, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:48.866605: step 4307, loss 0.0612875, acc 1, learning_rate 0.0001
2017-10-10T12:01:49.117941: step 4308, loss 0.142982, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:49.359299: step 4309, loss 0.249449, acc 0.890625, learning_rate 0.0001
2017-10-10T12:01:49.638716: step 4310, loss 0.0534526, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:49.931939: step 4311, loss 0.0943326, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:50.103623: step 4312, loss 0.128042, acc 0.960784, learning_rate 0.0001
2017-10-10T12:01:50.279034: step 4313, loss 0.0512628, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:50.450601: step 4314, loss 0.118707, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:50.628867: step 4315, loss 0.170826, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:50.799835: step 4316, loss 0.026865, acc 1, learning_rate 0.0001
2017-10-10T12:01:50.973529: step 4317, loss 0.0491707, acc 1, learning_rate 0.0001
2017-10-10T12:01:51.228879: step 4318, loss 0.0729824, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:51.479237: step 4319, loss 0.0659953, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:51.727762: step 4320, loss 0.175347, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:52.314781: step 4320, loss 0.213904, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4320

2017-10-10T12:01:53.309600: step 4321, loss 0.209271, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:53.452852: step 4322, loss 0.069412, acc 1, learning_rate 0.0001
2017-10-10T12:01:53.646637: step 4323, loss 0.0840317, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:53.839354: step 4324, loss 0.14249, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:54.046721: step 4325, loss 0.138268, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:54.317497: step 4326, loss 0.0677244, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:54.565623: step 4327, loss 0.0299978, acc 1, learning_rate 0.0001
2017-10-10T12:01:54.820864: step 4328, loss 0.0652484, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:55.071376: step 4329, loss 0.0416485, acc 1, learning_rate 0.0001
2017-10-10T12:01:55.304733: step 4330, loss 0.065609, acc 1, learning_rate 0.0001
2017-10-10T12:01:55.545498: step 4331, loss 0.12055, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:55.786409: step 4332, loss 0.0949491, acc 1, learning_rate 0.0001
2017-10-10T12:01:55.991387: step 4333, loss 0.105133, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:56.196999: step 4334, loss 0.0529528, acc 1, learning_rate 0.0001
2017-10-10T12:01:56.412904: step 4335, loss 0.166217, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:56.638093: step 4336, loss 0.219838, acc 0.90625, learning_rate 0.0001
2017-10-10T12:01:56.846395: step 4337, loss 0.0473766, acc 1, learning_rate 0.0001
2017-10-10T12:01:57.072121: step 4338, loss 0.151203, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:57.296334: step 4339, loss 0.0277478, acc 1, learning_rate 0.0001
2017-10-10T12:01:57.533710: step 4340, loss 0.0400436, acc 1, learning_rate 0.0001
2017-10-10T12:01:57.718528: step 4341, loss 0.118276, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:57.930275: step 4342, loss 0.11823, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:58.158489: step 4343, loss 0.0404472, acc 1, learning_rate 0.0001
2017-10-10T12:01:58.393099: step 4344, loss 0.152195, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:58.638288: step 4345, loss 0.0322892, acc 1, learning_rate 0.0001
2017-10-10T12:01:58.890932: step 4346, loss 0.0962652, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:59.142378: step 4347, loss 0.104828, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:59.426344: step 4348, loss 0.181824, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:59.664113: step 4349, loss 0.0926496, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:59.933207: step 4350, loss 0.0339001, acc 1, learning_rate 0.0001
2017-10-10T12:02:00.173973: step 4351, loss 0.110229, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:00.409422: step 4352, loss 0.120206, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:00.633779: step 4353, loss 0.175862, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:00.858675: step 4354, loss 0.0443624, acc 1, learning_rate 0.0001
2017-10-10T12:02:01.084886: step 4355, loss 0.128252, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:01.390458: step 4356, loss 0.138606, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:01.610613: step 4357, loss 0.180244, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:01.806102: step 4358, loss 0.0755043, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:02.045115: step 4359, loss 0.0920867, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:02.165530: step 4360, loss 0.102004, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:02.547536: step 4360, loss 0.213244, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4360

2017-10-10T12:02:03.707761: step 4361, loss 0.174528, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:03.948411: step 4362, loss 0.057474, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:04.192847: step 4363, loss 0.141139, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:04.433597: step 4364, loss 0.0441446, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:04.692154: step 4365, loss 0.108174, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:04.929175: step 4366, loss 0.102762, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:05.170863: step 4367, loss 0.0954359, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:05.415296: step 4368, loss 0.0740419, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:05.659612: step 4369, loss 0.0371531, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:05.908736: step 4370, loss 0.093149, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:06.165255: step 4371, loss 0.0528904, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:06.423745: step 4372, loss 0.104708, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:06.679840: step 4373, loss 0.0749123, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:06.896875: step 4374, loss 0.0432245, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:07.167500: step 4375, loss 0.0636969, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:07.406458: step 4376, loss 0.044958, acc 1, learning_rate 0.0001
2017-10-10T12:02:07.639204: step 4377, loss 0.108533, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:07.876306: step 4378, loss 0.0300298, acc 1, learning_rate 0.0001
2017-10-10T12:02:08.109097: step 4379, loss 0.0967445, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:08.343680: step 4380, loss 0.0419407, acc 1, learning_rate 0.0001
2017-10-10T12:02:08.591083: step 4381, loss 0.0822524, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:08.815167: step 4382, loss 0.274968, acc 0.890625, learning_rate 0.0001
2017-10-10T12:02:09.034822: step 4383, loss 0.0627938, acc 1, learning_rate 0.0001
2017-10-10T12:02:09.273847: step 4384, loss 0.0319305, acc 1, learning_rate 0.0001
2017-10-10T12:02:09.502970: step 4385, loss 0.0684556, acc 1, learning_rate 0.0001
2017-10-10T12:02:09.732853: step 4386, loss 0.0524092, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:09.947579: step 4387, loss 0.160882, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:10.163469: step 4388, loss 0.0870242, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:10.454765: step 4389, loss 0.023872, acc 1, learning_rate 0.0001
2017-10-10T12:02:10.677751: step 4390, loss 0.0988776, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:10.878789: step 4391, loss 0.138767, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:11.077211: step 4392, loss 0.0587258, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:11.255159: step 4393, loss 0.135452, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:11.454124: step 4394, loss 0.0656018, acc 1, learning_rate 0.0001
2017-10-10T12:02:11.701044: step 4395, loss 0.136853, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:11.973286: step 4396, loss 0.0475535, acc 1, learning_rate 0.0001
2017-10-10T12:02:12.231097: step 4397, loss 0.0679237, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:12.524870: step 4398, loss 0.135158, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:12.757552: step 4399, loss 0.130237, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:12.931186: step 4400, loss 0.0841077, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:13.454758: step 4400, loss 0.215051, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4400

2017-10-10T12:02:14.401035: step 4401, loss 0.0865667, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:14.643316: step 4402, loss 0.0597204, acc 1, learning_rate 0.0001
2017-10-10T12:02:14.909967: step 4403, loss 0.0618395, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:15.138732: step 4404, loss 0.0908075, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:15.355143: step 4405, loss 0.060959, acc 1, learning_rate 0.0001
2017-10-10T12:02:15.580947: step 4406, loss 0.0655804, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:15.797047: step 4407, loss 0.13079, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:16.027323: step 4408, loss 0.0680211, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:16.262204: step 4409, loss 0.121991, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:16.457291: step 4410, loss 0.152778, acc 0.960784, learning_rate 0.0001
2017-10-10T12:02:16.640486: step 4411, loss 0.0948172, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:16.881075: step 4412, loss 0.100118, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:17.107232: step 4413, loss 0.109682, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:17.349636: step 4414, loss 0.150827, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:17.587129: step 4415, loss 0.0972697, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:17.805743: step 4416, loss 0.0637722, acc 1, learning_rate 0.0001
2017-10-10T12:02:18.020987: step 4417, loss 0.0303897, acc 1, learning_rate 0.0001
2017-10-10T12:02:18.244981: step 4418, loss 0.0749457, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:18.463476: step 4419, loss 0.167414, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:18.759859: step 4420, loss 0.0790778, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:19.020839: step 4421, loss 0.0982399, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:19.174593: step 4422, loss 0.0650154, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:19.354796: step 4423, loss 0.17905, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:19.527353: step 4424, loss 0.0879111, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:19.734845: step 4425, loss 0.0479661, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:19.923957: step 4426, loss 0.0516204, acc 1, learning_rate 0.0001
2017-10-10T12:02:20.162072: step 4427, loss 0.0723288, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:20.424031: step 4428, loss 0.123595, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:20.662447: step 4429, loss 0.0568295, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:20.922869: step 4430, loss 0.184368, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:21.161422: step 4431, loss 0.146177, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:21.392159: step 4432, loss 0.113061, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:21.671027: step 4433, loss 0.0564293, acc 1, learning_rate 0.0001
2017-10-10T12:02:21.913171: step 4434, loss 0.0565102, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:22.168302: step 4435, loss 0.0595055, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:22.403021: step 4436, loss 0.207831, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:22.629166: step 4437, loss 0.155132, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:22.824984: step 4438, loss 0.0420063, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:23.075048: step 4439, loss 0.147473, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:23.361387: step 4440, loss 0.116528, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:23.909312: step 4440, loss 0.216705, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4440

2017-10-10T12:02:24.852150: step 4441, loss 0.104329, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:25.108395: step 4442, loss 0.115885, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:25.372893: step 4443, loss 0.0482375, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:25.607447: step 4444, loss 0.147037, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:25.838694: step 4445, loss 0.163002, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:26.124946: step 4446, loss 0.14632, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:26.384064: step 4447, loss 0.195154, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:26.628295: step 4448, loss 0.127181, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:26.873104: step 4449, loss 0.110929, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:27.163507: step 4450, loss 0.102979, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:27.357356: step 4451, loss 0.0581225, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:27.557477: step 4452, loss 0.102933, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:27.730053: step 4453, loss 0.0717051, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:27.935192: step 4454, loss 0.119386, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:28.122374: step 4455, loss 0.143221, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:28.314396: step 4456, loss 0.0726516, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:28.560966: step 4457, loss 0.106722, acc 1, learning_rate 0.0001
2017-10-10T12:02:28.772994: step 4458, loss 0.0674076, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:28.983955: step 4459, loss 0.137746, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:29.231486: step 4460, loss 0.0511605, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:29.480886: step 4461, loss 0.162138, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:29.713849: step 4462, loss 0.0447803, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:29.931424: step 4463, loss 0.0407824, acc 1, learning_rate 0.0001
2017-10-10T12:02:30.127028: step 4464, loss 0.103153, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:30.346997: step 4465, loss 0.0666179, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:30.628958: step 4466, loss 0.0662145, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:30.816266: step 4467, loss 0.0256975, acc 1, learning_rate 0.0001
2017-10-10T12:02:31.053557: step 4468, loss 0.127852, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:31.288852: step 4469, loss 0.0888, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:31.517273: step 4470, loss 0.101368, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:31.724853: step 4471, loss 0.0395773, acc 1, learning_rate 0.0001
2017-10-10T12:02:31.961598: step 4472, loss 0.144414, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:32.180992: step 4473, loss 0.0523777, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:32.409892: step 4474, loss 0.117098, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:32.655443: step 4475, loss 0.0572539, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:32.893109: step 4476, loss 0.0702634, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:33.108861: step 4477, loss 0.11046, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:33.357377: step 4478, loss 0.123807, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:33.611936: step 4479, loss 0.0745786, acc 1, learning_rate 0.0001
2017-10-10T12:02:33.876867: step 4480, loss 0.0723735, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:34.656262: step 4480, loss 0.21379, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4480

2017-10-10T12:02:35.780996: step 4481, loss 0.159984, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:36.021898: step 4482, loss 0.126924, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:36.289583: step 4483, loss 0.0300449, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:36.553036: step 4484, loss 0.190244, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:36.816298: step 4485, loss 0.0950227, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:37.072074: step 4486, loss 0.0888603, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:37.299421: step 4487, loss 0.119716, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:37.528834: step 4488, loss 0.112141, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:37.763306: step 4489, loss 0.0664207, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:37.980677: step 4490, loss 0.0549786, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:38.172540: step 4491, loss 0.128851, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:38.425133: step 4492, loss 0.0591998, acc 1, learning_rate 0.0001
2017-10-10T12:02:38.638647: step 4493, loss 0.111318, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:38.865321: step 4494, loss 0.0972405, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:39.112896: step 4495, loss 0.152428, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:39.340013: step 4496, loss 0.0739623, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:39.584888: step 4497, loss 0.203436, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:39.834242: step 4498, loss 0.0585257, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:40.080603: step 4499, loss 0.0674193, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:40.324866: step 4500, loss 0.0949214, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:40.584534: step 4501, loss 0.0950076, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:40.822263: step 4502, loss 0.136029, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:41.043495: step 4503, loss 0.16016, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:41.297029: step 4504, loss 0.132681, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:41.535036: step 4505, loss 0.033089, acc 1, learning_rate 0.0001
2017-10-10T12:02:41.755682: step 4506, loss 0.113635, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:41.995955: step 4507, loss 0.111106, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:42.176940: step 4508, loss 0.0441832, acc 1, learning_rate 0.0001
2017-10-10T12:02:42.385245: step 4509, loss 0.127429, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:42.606036: step 4510, loss 0.069813, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:42.838838: step 4511, loss 0.0354955, acc 1, learning_rate 0.0001
2017-10-10T12:02:43.037375: step 4512, loss 0.0377939, acc 1, learning_rate 0.0001
2017-10-10T12:02:43.288850: step 4513, loss 0.0749556, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:43.607212: step 4514, loss 0.0537133, acc 1, learning_rate 0.0001
2017-10-10T12:02:43.816716: step 4515, loss 0.0641696, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:44.001440: step 4516, loss 0.119241, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:44.180943: step 4517, loss 0.161714, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:44.376911: step 4518, loss 0.106113, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:44.578039: step 4519, loss 0.048307, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:44.843498: step 4520, loss 0.054891, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:45.423349: step 4520, loss 0.214563, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4520

2017-10-10T12:02:46.600793: step 4521, loss 0.0652291, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:46.792555: step 4522, loss 0.12598, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:47.016978: step 4523, loss 0.121577, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:47.236838: step 4524, loss 0.0332929, acc 1, learning_rate 0.0001
2017-10-10T12:02:47.473143: step 4525, loss 0.112982, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:47.675423: step 4526, loss 0.130937, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:47.905063: step 4527, loss 0.153178, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:48.164822: step 4528, loss 0.0637308, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:48.400871: step 4529, loss 0.098505, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:48.659869: step 4530, loss 0.0737593, acc 1, learning_rate 0.0001
2017-10-10T12:02:48.883992: step 4531, loss 0.112781, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:49.125221: step 4532, loss 0.113478, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:49.380877: step 4533, loss 0.120702, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:49.603584: step 4534, loss 0.0797364, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:49.864092: step 4535, loss 0.051812, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:50.105180: step 4536, loss 0.0913774, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:50.320972: step 4537, loss 0.0271225, acc 1, learning_rate 0.0001
2017-10-10T12:02:50.540828: step 4538, loss 0.078238, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:50.792943: step 4539, loss 0.0518366, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:51.036032: step 4540, loss 0.167253, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:51.269019: step 4541, loss 0.0624091, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:51.493023: step 4542, loss 0.127917, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:51.780857: step 4543, loss 0.0826941, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:51.988501: step 4544, loss 0.112336, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:52.200810: step 4545, loss 0.0666464, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:52.385591: step 4546, loss 0.0593589, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:52.561660: step 4547, loss 0.218437, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:52.732880: step 4548, loss 0.133848, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:52.901214: step 4549, loss 0.061055, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:53.129919: step 4550, loss 0.106782, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:53.368903: step 4551, loss 0.0617426, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:53.598840: step 4552, loss 0.14005, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:53.868944: step 4553, loss 0.0702681, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:54.096836: step 4554, loss 0.0961415, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:54.340968: step 4555, loss 0.121177, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:54.573954: step 4556, loss 0.139857, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:54.789060: step 4557, loss 0.129061, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:55.033024: step 4558, loss 0.0835348, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:55.321523: step 4559, loss 0.163974, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:55.547316: step 4560, loss 0.19091, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:56.154483: step 4560, loss 0.213932, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4560

2017-10-10T12:02:57.286219: step 4561, loss 0.111395, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:57.456926: step 4562, loss 0.0824807, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:57.680760: step 4563, loss 0.16467, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:57.914874: step 4564, loss 0.0905938, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:58.164701: step 4565, loss 0.119118, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:58.393185: step 4566, loss 0.075389, acc 1, learning_rate 0.0001
2017-10-10T12:02:58.652883: step 4567, loss 0.0721568, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:58.872869: step 4568, loss 0.0997613, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:59.097191: step 4569, loss 0.056617, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:59.312635: step 4570, loss 0.084701, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:59.520851: step 4571, loss 0.130102, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:59.720241: step 4572, loss 0.0666246, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:59.956403: step 4573, loss 0.147857, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:00.237212: step 4574, loss 0.108098, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:00.476586: step 4575, loss 0.105519, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:00.653269: step 4576, loss 0.117691, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:00.849816: step 4577, loss 0.205058, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:01.043112: step 4578, loss 0.119776, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:01.253020: step 4579, loss 0.152277, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:01.495676: step 4580, loss 0.0193511, acc 1, learning_rate 0.0001
2017-10-10T12:03:01.720930: step 4581, loss 0.0935541, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:01.968917: step 4582, loss 0.1267, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:02.220135: step 4583, loss 0.0921651, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:02.464981: step 4584, loss 0.0918349, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:02.706091: step 4585, loss 0.151718, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:02.946338: step 4586, loss 0.0350551, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:03.180914: step 4587, loss 0.119486, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:03.364139: step 4588, loss 0.135112, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:03.590810: step 4589, loss 0.0981728, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:03.815428: step 4590, loss 0.0532828, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:04.016843: step 4591, loss 0.092252, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:04.305158: step 4592, loss 0.0894202, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:04.520850: step 4593, loss 0.0572509, acc 1, learning_rate 0.0001
2017-10-10T12:03:04.739253: step 4594, loss 0.0414579, acc 1, learning_rate 0.0001
2017-10-10T12:03:04.983655: step 4595, loss 0.144219, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:05.212753: step 4596, loss 0.17442, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:05.428688: step 4597, loss 0.217905, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:05.675563: step 4598, loss 0.0973635, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:05.893000: step 4599, loss 0.131386, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:06.104851: step 4600, loss 0.0435213, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:06.673684: step 4600, loss 0.215135, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4600

2017-10-10T12:03:07.790617: step 4601, loss 0.112786, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:07.962748: step 4602, loss 0.0947246, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:08.124840: step 4603, loss 0.090625, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:08.344862: step 4604, loss 0.0740346, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:08.574869: step 4605, loss 0.0265294, acc 1, learning_rate 0.0001
2017-10-10T12:03:08.720931: step 4606, loss 0.0732172, acc 0.960784, learning_rate 0.0001
2017-10-10T12:03:08.918457: step 4607, loss 0.0715382, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:09.095266: step 4608, loss 0.0341188, acc 1, learning_rate 0.0001
2017-10-10T12:03:09.298438: step 4609, loss 0.0745864, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:09.459544: step 4610, loss 0.0824713, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:09.689300: step 4611, loss 0.124089, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:09.931726: step 4612, loss 0.09229, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:10.201986: step 4613, loss 0.103425, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:10.439932: step 4614, loss 0.156424, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:10.679694: step 4615, loss 0.0969516, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:10.935628: step 4616, loss 0.175825, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:11.175563: step 4617, loss 0.0513165, acc 1, learning_rate 0.0001
2017-10-10T12:03:11.420415: step 4618, loss 0.145138, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:11.644852: step 4619, loss 0.0617728, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:11.876008: step 4620, loss 0.173333, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:12.100025: step 4621, loss 0.125251, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:12.321905: step 4622, loss 0.124303, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:12.540926: step 4623, loss 0.0979309, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:12.768872: step 4624, loss 0.0962952, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:13.025895: step 4625, loss 0.13663, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:13.253368: step 4626, loss 0.246149, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:13.472101: step 4627, loss 0.0681292, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:13.736269: step 4628, loss 0.0681807, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:13.944057: step 4629, loss 0.0696592, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:14.144251: step 4630, loss 0.092109, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:14.361505: step 4631, loss 0.0873803, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:14.578421: step 4632, loss 0.118232, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:14.834682: step 4633, loss 0.0825553, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:15.079519: step 4634, loss 0.136955, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:15.325522: step 4635, loss 0.075781, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:15.588541: step 4636, loss 0.115259, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:15.817355: step 4637, loss 0.081333, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:16.057889: step 4638, loss 0.138344, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:16.325961: step 4639, loss 0.0735449, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:16.564596: step 4640, loss 0.0487074, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:17.147774: step 4640, loss 0.214048, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4640

2017-10-10T12:03:18.386970: step 4641, loss 0.108322, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:18.585079: step 4642, loss 0.0567122, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:18.759588: step 4643, loss 0.128739, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:18.940052: step 4644, loss 0.0415673, acc 1, learning_rate 0.0001
2017-10-10T12:03:19.124855: step 4645, loss 0.165977, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:19.301928: step 4646, loss 0.0645933, acc 1, learning_rate 0.0001
2017-10-10T12:03:19.470885: step 4647, loss 0.093812, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:19.644813: step 4648, loss 0.0659186, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:19.910623: step 4649, loss 0.0292727, acc 1, learning_rate 0.0001
2017-10-10T12:03:20.140854: step 4650, loss 0.145511, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:20.383861: step 4651, loss 0.0940609, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:20.652845: step 4652, loss 0.0692911, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:20.894300: step 4653, loss 0.0602125, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:21.158137: step 4654, loss 0.101489, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:21.386370: step 4655, loss 0.117826, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:21.618558: step 4656, loss 0.0400983, acc 1, learning_rate 0.0001
2017-10-10T12:03:21.853949: step 4657, loss 0.0994968, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:22.075769: step 4658, loss 0.107111, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:22.305558: step 4659, loss 0.0805784, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:22.545170: step 4660, loss 0.045132, acc 1, learning_rate 0.0001
2017-10-10T12:03:22.780910: step 4661, loss 0.0595681, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:22.996947: step 4662, loss 0.085683, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:23.229247: step 4663, loss 0.115318, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:23.442094: step 4664, loss 0.083988, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:23.632829: step 4665, loss 0.0616214, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:23.856893: step 4666, loss 0.0651346, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:24.113714: step 4667, loss 0.136908, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:24.349599: step 4668, loss 0.198787, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:24.620314: step 4669, loss 0.0718089, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:24.842722: step 4670, loss 0.051877, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:25.082829: step 4671, loss 0.123019, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:25.326708: step 4672, loss 0.0425575, acc 1, learning_rate 0.0001
2017-10-10T12:03:25.574944: step 4673, loss 0.158881, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:25.824895: step 4674, loss 0.0631135, acc 1, learning_rate 0.0001
2017-10-10T12:03:26.089719: step 4675, loss 0.0855907, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:26.268525: step 4676, loss 0.0619444, acc 1, learning_rate 0.0001
2017-10-10T12:03:26.456820: step 4677, loss 0.0536278, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:26.655463: step 4678, loss 0.214264, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:26.829241: step 4679, loss 0.0613193, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:27.048834: step 4680, loss 0.0797533, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:27.628554: step 4680, loss 0.214464, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4680

2017-10-10T12:03:28.614334: step 4681, loss 0.056732, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:28.821100: step 4682, loss 0.136028, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:29.022145: step 4683, loss 0.0364741, acc 1, learning_rate 0.0001
2017-10-10T12:03:29.324877: step 4684, loss 0.0726486, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:29.615995: step 4685, loss 0.171544, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:29.792981: step 4686, loss 0.0500073, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:29.994623: step 4687, loss 0.0510642, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:30.158825: step 4688, loss 0.0707725, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:30.321278: step 4689, loss 0.0992931, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:30.499234: step 4690, loss 0.0613857, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:30.710050: step 4691, loss 0.0767, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:30.951089: step 4692, loss 0.0764349, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:31.214319: step 4693, loss 0.128755, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:31.457322: step 4694, loss 0.0798411, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:31.684358: step 4695, loss 0.126434, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:31.921231: step 4696, loss 0.080832, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:32.172853: step 4697, loss 0.126052, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:32.392990: step 4698, loss 0.0912885, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:32.648028: step 4699, loss 0.0661839, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:32.884919: step 4700, loss 0.0810122, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:33.126159: step 4701, loss 0.0413208, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:33.372978: step 4702, loss 0.0467642, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:33.601442: step 4703, loss 0.0599435, acc 1, learning_rate 0.0001
2017-10-10T12:03:33.780963: step 4704, loss 0.151508, acc 0.921569, learning_rate 0.0001
2017-10-10T12:03:34.030045: step 4705, loss 0.178618, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:34.293216: step 4706, loss 0.0542805, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:34.493503: step 4707, loss 0.0431249, acc 1, learning_rate 0.0001
2017-10-10T12:03:34.689076: step 4708, loss 0.0661804, acc 1, learning_rate 0.0001
2017-10-10T12:03:34.874168: step 4709, loss 0.113066, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:35.056694: step 4710, loss 0.116626, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:35.235651: step 4711, loss 0.0600246, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:35.472548: step 4712, loss 0.132693, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:35.700230: step 4713, loss 0.0545952, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:35.972479: step 4714, loss 0.112009, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:36.222773: step 4715, loss 0.0813638, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:36.484773: step 4716, loss 0.101781, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:36.718132: step 4717, loss 0.123169, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:36.981519: step 4718, loss 0.0589262, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:37.227416: step 4719, loss 0.150178, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:37.468194: step 4720, loss 0.0592851, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:38.084509: step 4720, loss 0.213999, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4720

2017-10-10T12:03:39.254190: step 4721, loss 0.0842151, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:39.508829: step 4722, loss 0.116339, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:39.738586: step 4723, loss 0.0891795, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:39.989239: step 4724, loss 0.129164, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:40.236878: step 4725, loss 0.0679758, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:40.511142: step 4726, loss 0.0856042, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:40.760728: step 4727, loss 0.0860387, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:40.931279: step 4728, loss 0.0477213, acc 1, learning_rate 0.0001
2017-10-10T12:03:41.098641: step 4729, loss 0.142894, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:41.280116: step 4730, loss 0.158281, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:41.467376: step 4731, loss 0.118086, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:41.642570: step 4732, loss 0.11294, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:41.900974: step 4733, loss 0.0866414, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:42.192613: step 4734, loss 0.0769579, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:42.453419: step 4735, loss 0.211466, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:42.652948: step 4736, loss 0.15721, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:42.859564: step 4737, loss 0.138569, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:43.052833: step 4738, loss 0.0885825, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:43.250430: step 4739, loss 0.071023, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:43.437049: step 4740, loss 0.0452489, acc 1, learning_rate 0.0001
2017-10-10T12:03:43.675819: step 4741, loss 0.0255905, acc 1, learning_rate 0.0001
2017-10-10T12:03:43.876977: step 4742, loss 0.0616488, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:44.116468: step 4743, loss 0.0663486, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:44.340868: step 4744, loss 0.155881, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:44.582383: step 4745, loss 0.0574607, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:44.788841: step 4746, loss 0.1521, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:45.021912: step 4747, loss 0.0935957, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:45.255207: step 4748, loss 0.030354, acc 1, learning_rate 0.0001
2017-10-10T12:03:45.481517: step 4749, loss 0.137248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:45.713496: step 4750, loss 0.0892139, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:45.964868: step 4751, loss 0.0881247, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:46.229403: step 4752, loss 0.132199, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:46.465597: step 4753, loss 0.103676, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:46.712273: step 4754, loss 0.0771681, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:46.953837: step 4755, loss 0.0959107, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:47.192978: step 4756, loss 0.103419, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:47.458905: step 4757, loss 0.0542294, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:47.668852: step 4758, loss 0.0676137, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:47.908145: step 4759, loss 0.152268, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:48.128857: step 4760, loss 0.168764, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:48.760437: step 4760, loss 0.213496, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4760

2017-10-10T12:03:50.093851: step 4761, loss 0.0375628, acc 1, learning_rate 0.0001
2017-10-10T12:03:50.343825: step 4762, loss 0.0706404, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:50.669418: step 4763, loss 0.127957, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:50.887027: step 4764, loss 0.0562994, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:51.077515: step 4765, loss 0.0564982, acc 1, learning_rate 0.0001
2017-10-10T12:03:51.254900: step 4766, loss 0.0339493, acc 1, learning_rate 0.0001
2017-10-10T12:03:51.504865: step 4767, loss 0.136596, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:51.745987: step 4768, loss 0.0404908, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:51.873639: step 4769, loss 0.214691, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:52.083963: step 4770, loss 0.072335, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:52.262891: step 4771, loss 0.219104, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:52.429642: step 4772, loss 0.0830653, acc 1, learning_rate 0.0001
2017-10-10T12:03:52.597213: step 4773, loss 0.0828601, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:52.761735: step 4774, loss 0.122467, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:53.005017: step 4775, loss 0.0478045, acc 1, learning_rate 0.0001
2017-10-10T12:03:53.198701: step 4776, loss 0.062747, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:53.377062: step 4777, loss 0.0632603, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:53.637363: step 4778, loss 0.134148, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:53.849383: step 4779, loss 0.0603361, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:54.049484: step 4780, loss 0.115119, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:54.272133: step 4781, loss 0.0450653, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:54.500925: step 4782, loss 0.127019, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:54.776506: step 4783, loss 0.153108, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:55.010911: step 4784, loss 0.134938, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:55.253274: step 4785, loss 0.132351, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:55.468710: step 4786, loss 0.120967, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:55.696837: step 4787, loss 0.0205743, acc 1, learning_rate 0.0001
2017-10-10T12:03:55.949349: step 4788, loss 0.118707, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:56.197224: step 4789, loss 0.0988101, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:56.398004: step 4790, loss 0.0944007, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:56.661604: step 4791, loss 0.0386327, acc 1, learning_rate 0.0001
2017-10-10T12:03:56.898750: step 4792, loss 0.0842604, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:57.126758: step 4793, loss 0.0460809, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:57.387630: step 4794, loss 0.0641105, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:57.618509: step 4795, loss 0.160472, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:57.887837: step 4796, loss 0.0598712, acc 1, learning_rate 0.0001
2017-10-10T12:03:58.151741: step 4797, loss 0.155399, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:58.410922: step 4798, loss 0.0395356, acc 1, learning_rate 0.0001
2017-10-10T12:03:58.644295: step 4799, loss 0.0701922, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:58.862206: step 4800, loss 0.100497, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:59.497723: step 4800, loss 0.211748, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4800

2017-10-10T12:04:00.532856: step 4801, loss 0.145737, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:00.734699: step 4802, loss 0.133142, acc 0.960784, learning_rate 0.0001
2017-10-10T12:04:00.971425: step 4803, loss 0.0512679, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:01.188878: step 4804, loss 0.169607, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:01.464386: step 4805, loss 0.24817, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:01.712958: step 4806, loss 0.0595516, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:01.952053: step 4807, loss 0.041222, acc 1, learning_rate 0.0001
2017-10-10T12:04:02.189956: step 4808, loss 0.11262, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:02.479237: step 4809, loss 0.18468, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:02.738574: step 4810, loss 0.0816558, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:02.942790: step 4811, loss 0.0612584, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:03.125376: step 4812, loss 0.115394, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:03.296553: step 4813, loss 0.0737682, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:03.480846: step 4814, loss 0.167804, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:03.646460: step 4815, loss 0.0618989, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:03.915994: step 4816, loss 0.0697014, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:04.138706: step 4817, loss 0.0952818, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:04.346519: step 4818, loss 0.0496973, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:04.597560: step 4819, loss 0.0958811, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:04.839457: step 4820, loss 0.122693, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:05.101017: step 4821, loss 0.0437516, acc 1, learning_rate 0.0001
2017-10-10T12:04:05.291207: step 4822, loss 0.0975572, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:05.501749: step 4823, loss 0.0810684, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:05.750128: step 4824, loss 0.0532491, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:05.977741: step 4825, loss 0.0766085, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:06.180900: step 4826, loss 0.0525755, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:06.410634: step 4827, loss 0.0765955, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:06.653626: step 4828, loss 0.0949887, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:06.896236: step 4829, loss 0.0882215, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:07.125120: step 4830, loss 0.0288265, acc 1, learning_rate 0.0001
2017-10-10T12:04:07.306473: step 4831, loss 0.117557, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:07.544896: step 4832, loss 0.0939034, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:07.802395: step 4833, loss 0.146375, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:08.004545: step 4834, loss 0.0762082, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:08.197198: step 4835, loss 0.0981397, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:08.388638: step 4836, loss 0.0962117, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:08.587762: step 4837, loss 0.0824862, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:08.805327: step 4838, loss 0.0617329, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:09.013566: step 4839, loss 0.0618919, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:09.224454: step 4840, loss 0.155021, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:09.694349: step 4840, loss 0.214134, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4840

2017-10-10T12:04:10.883824: step 4841, loss 0.107934, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:11.116325: step 4842, loss 0.0267879, acc 1, learning_rate 0.0001
2017-10-10T12:04:11.337744: step 4843, loss 0.0968226, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:11.606278: step 4844, loss 0.0491522, acc 1, learning_rate 0.0001
2017-10-10T12:04:11.840244: step 4845, loss 0.0855921, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:12.063963: step 4846, loss 0.0736558, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:12.286382: step 4847, loss 0.167887, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:12.503417: step 4848, loss 0.064054, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:12.713492: step 4849, loss 0.0468657, acc 1, learning_rate 0.0001
2017-10-10T12:04:12.908834: step 4850, loss 0.0812643, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:13.156899: step 4851, loss 0.0669092, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:13.398160: step 4852, loss 0.0731078, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:13.723837: step 4853, loss 0.171566, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:14.000505: step 4854, loss 0.0615519, acc 1, learning_rate 0.0001
2017-10-10T12:04:14.176146: step 4855, loss 0.0466253, acc 1, learning_rate 0.0001
2017-10-10T12:04:14.343334: step 4856, loss 0.0623999, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:14.540803: step 4857, loss 0.0775004, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:14.716162: step 4858, loss 0.103132, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:14.882066: step 4859, loss 0.0467465, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:15.099358: step 4860, loss 0.0737168, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:15.309774: step 4861, loss 0.0781415, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:15.567827: step 4862, loss 0.114822, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:15.804969: step 4863, loss 0.031809, acc 1, learning_rate 0.0001
2017-10-10T12:04:16.085611: step 4864, loss 0.108325, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:16.284173: step 4865, loss 0.12217, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:16.480057: step 4866, loss 0.0794266, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:16.678383: step 4867, loss 0.0958956, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:16.850349: step 4868, loss 0.0629242, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:17.059062: step 4869, loss 0.0331104, acc 1, learning_rate 0.0001
2017-10-10T12:04:17.277004: step 4870, loss 0.0981526, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:17.526760: step 4871, loss 0.138909, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:17.746680: step 4872, loss 0.0857414, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:17.999748: step 4873, loss 0.0564448, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:18.258456: step 4874, loss 0.0915536, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:18.491730: step 4875, loss 0.111881, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:18.739403: step 4876, loss 0.147271, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:18.994165: step 4877, loss 0.0301755, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:19.216099: step 4878, loss 0.0812507, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:19.481171: step 4879, loss 0.0538034, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:19.711604: step 4880, loss 0.0815575, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:20.272658: step 4880, loss 0.21639, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4880

2017-10-10T12:04:21.663772: step 4881, loss 0.0364488, acc 1, learning_rate 0.0001
2017-10-10T12:04:21.859525: step 4882, loss 0.0920987, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:22.092839: step 4883, loss 0.115499, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:22.317510: step 4884, loss 0.101534, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:22.548895: step 4885, loss 0.0352099, acc 1, learning_rate 0.0001
2017-10-10T12:04:22.804502: step 4886, loss 0.0649032, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:23.040362: step 4887, loss 0.148166, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:23.264246: step 4888, loss 0.0826508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:23.516298: step 4889, loss 0.137802, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:23.754230: step 4890, loss 0.074427, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:23.996995: step 4891, loss 0.0569405, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:24.293178: step 4892, loss 0.0995082, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:24.612906: step 4893, loss 0.0798697, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:24.804942: step 4894, loss 0.0834011, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:24.938401: step 4895, loss 0.113063, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:25.072697: step 4896, loss 0.102404, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:25.205156: step 4897, loss 0.155051, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:25.329013: step 4898, loss 0.0477704, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:25.448709: step 4899, loss 0.0805768, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:25.599479: step 4900, loss 0.0702228, acc 1, learning_rate 0.0001
2017-10-10T12:04:25.816807: step 4901, loss 0.0669779, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:26.080913: step 4902, loss 0.0498205, acc 1, learning_rate 0.0001
2017-10-10T12:04:26.320877: step 4903, loss 0.112904, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:26.551644: step 4904, loss 0.0292904, acc 1, learning_rate 0.0001
2017-10-10T12:04:26.798025: step 4905, loss 0.116512, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:27.032254: step 4906, loss 0.0527584, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:27.280981: step 4907, loss 0.115743, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:27.524851: step 4908, loss 0.0841731, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:27.724854: step 4909, loss 0.159434, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:27.982271: step 4910, loss 0.0510134, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:28.196860: step 4911, loss 0.0777632, acc 1, learning_rate 0.0001
2017-10-10T12:04:28.399016: step 4912, loss 0.0842183, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:28.592886: step 4913, loss 0.0491143, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:28.844935: step 4914, loss 0.0621618, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:29.080537: step 4915, loss 0.0750289, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:29.295292: step 4916, loss 0.0911896, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:29.532857: step 4917, loss 0.0793271, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:29.774454: step 4918, loss 0.0294284, acc 1, learning_rate 0.0001
2017-10-10T12:04:30.028865: step 4919, loss 0.0537762, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:30.277312: step 4920, loss 0.125762, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:30.805959: step 4920, loss 0.214139, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4920

2017-10-10T12:04:31.815487: step 4921, loss 0.0255892, acc 1, learning_rate 0.0001
2017-10-10T12:04:32.050176: step 4922, loss 0.0913925, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:32.291648: step 4923, loss 0.119747, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:32.549488: step 4924, loss 0.103617, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:32.764735: step 4925, loss 0.105416, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:33.042584: step 4926, loss 0.0570939, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:33.247488: step 4927, loss 0.0916488, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:33.454651: step 4928, loss 0.102985, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:33.611992: step 4929, loss 0.129211, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:33.786408: step 4930, loss 0.0771889, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:33.957281: step 4931, loss 0.0880046, acc 1, learning_rate 0.0001
2017-10-10T12:04:34.227868: step 4932, loss 0.0543603, acc 1, learning_rate 0.0001
2017-10-10T12:04:34.500956: step 4933, loss 0.0414869, acc 1, learning_rate 0.0001
2017-10-10T12:04:34.756904: step 4934, loss 0.0624711, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:34.994546: step 4935, loss 0.12956, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:35.192961: step 4936, loss 0.0695075, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:35.545618: step 4937, loss 0.0800327, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:35.783513: step 4938, loss 0.0870166, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:35.956888: step 4939, loss 0.1519, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:36.138927: step 4940, loss 0.0729855, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:36.314432: step 4941, loss 0.0752382, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:36.477857: step 4942, loss 0.0470063, acc 1, learning_rate 0.0001
2017-10-10T12:04:36.704865: step 4943, loss 0.140534, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:36.936871: step 4944, loss 0.213106, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:37.168473: step 4945, loss 0.150288, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:37.387607: step 4946, loss 0.135033, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:37.593995: step 4947, loss 0.0575891, acc 1, learning_rate 0.0001
2017-10-10T12:04:37.802698: step 4948, loss 0.071069, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:38.091350: step 4949, loss 0.0363927, acc 1, learning_rate 0.0001
2017-10-10T12:04:38.332844: step 4950, loss 0.0683819, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:38.534218: step 4951, loss 0.12879, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:38.800594: step 4952, loss 0.126803, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:39.034585: step 4953, loss 0.121097, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:39.261396: step 4954, loss 0.0606559, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:39.489155: step 4955, loss 0.065623, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:39.720261: step 4956, loss 0.0910713, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:39.947756: step 4957, loss 0.0396975, acc 1, learning_rate 0.0001
2017-10-10T12:04:40.204976: step 4958, loss 0.084325, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:40.404630: step 4959, loss 0.0979114, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:40.657152: step 4960, loss 0.0826892, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:41.198156: step 4960, loss 0.211657, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-4960

2017-10-10T12:04:42.246632: step 4961, loss 0.0688527, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:42.463262: step 4962, loss 0.125504, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:42.690891: step 4963, loss 0.0708072, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:42.930833: step 4964, loss 0.0550721, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:43.152933: step 4965, loss 0.102509, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:43.396881: step 4966, loss 0.102815, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:43.606374: step 4967, loss 0.051704, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:43.785606: step 4968, loss 0.049307, acc 1, learning_rate 0.0001
2017-10-10T12:04:44.010453: step 4969, loss 0.0519131, acc 1, learning_rate 0.0001
2017-10-10T12:04:44.259086: step 4970, loss 0.0691686, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:44.516730: step 4971, loss 0.126447, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:44.716916: step 4972, loss 0.0680502, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:44.959714: step 4973, loss 0.0350423, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:45.192519: step 4974, loss 0.211619, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:45.423539: step 4975, loss 0.136209, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:45.642726: step 4976, loss 0.090243, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:45.865220: step 4977, loss 0.141207, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:46.160893: step 4978, loss 0.0624736, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:46.451231: step 4979, loss 0.126561, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:46.663004: step 4980, loss 0.124697, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:46.846201: step 4981, loss 0.100404, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:47.012762: step 4982, loss 0.0743014, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:47.174000: step 4983, loss 0.0408146, acc 1, learning_rate 0.0001
2017-10-10T12:04:47.348697: step 4984, loss 0.0663907, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:47.521808: step 4985, loss 0.097549, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:47.784862: step 4986, loss 0.0625188, acc 1, learning_rate 0.0001
2017-10-10T12:04:48.024841: step 4987, loss 0.0669421, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:48.246582: step 4988, loss 0.0841555, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:48.487793: step 4989, loss 0.10989, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:48.739388: step 4990, loss 0.186715, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:48.988731: step 4991, loss 0.0977402, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:49.249751: step 4992, loss 0.16617, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:49.491475: step 4993, loss 0.0667409, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:49.764303: step 4994, loss 0.104528, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:50.046318: step 4995, loss 0.0635432, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:50.228946: step 4996, loss 0.0826186, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:50.419004: step 4997, loss 0.0693966, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:50.565460: step 4998, loss 0.06939, acc 0.960784, learning_rate 0.0001
2017-10-10T12:04:50.736818: step 4999, loss 0.121321, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:50.963686: step 5000, loss 0.0589837, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:51.502674: step 5000, loss 0.21234, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5000

2017-10-10T12:04:52.773994: step 5001, loss 0.0807867, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:53.002770: step 5002, loss 0.0430877, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:53.221008: step 5003, loss 0.0654593, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:53.471653: step 5004, loss 0.128848, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:53.666476: step 5005, loss 0.0797947, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:53.856865: step 5006, loss 0.10324, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:54.090183: step 5007, loss 0.106509, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:54.345004: step 5008, loss 0.170099, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:54.606395: step 5009, loss 0.124063, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:54.843741: step 5010, loss 0.0428589, acc 1, learning_rate 0.0001
2017-10-10T12:04:55.092908: step 5011, loss 0.0910024, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:55.332850: step 5012, loss 0.108427, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:55.562448: step 5013, loss 0.0293109, acc 1, learning_rate 0.0001
2017-10-10T12:04:55.792878: step 5014, loss 0.146509, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:56.020872: step 5015, loss 0.0729723, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:56.249141: step 5016, loss 0.0659796, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:56.496987: step 5017, loss 0.100347, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:56.715097: step 5018, loss 0.180767, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:56.932983: step 5019, loss 0.116209, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:57.143387: step 5020, loss 0.147034, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:57.496906: step 5021, loss 0.174792, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:57.739178: step 5022, loss 0.0855284, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:57.887001: step 5023, loss 0.0562301, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:58.058250: step 5024, loss 0.084813, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:58.298318: step 5025, loss 0.107403, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:58.429005: step 5026, loss 0.150208, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:58.551971: step 5027, loss 0.0734832, acc 1, learning_rate 0.0001
2017-10-10T12:04:58.676681: step 5028, loss 0.104755, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:58.869587: step 5029, loss 0.059982, acc 1, learning_rate 0.0001
2017-10-10T12:04:59.028901: step 5030, loss 0.172238, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:59.211431: step 5031, loss 0.151642, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:59.464276: step 5032, loss 0.0642889, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:59.720825: step 5033, loss 0.0982284, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:59.935957: step 5034, loss 0.0561437, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:00.182196: step 5035, loss 0.0976625, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:00.400456: step 5036, loss 0.0773323, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:00.616869: step 5037, loss 0.0588432, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:00.875614: step 5038, loss 0.118636, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:01.109800: step 5039, loss 0.0887474, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:01.329778: step 5040, loss 0.0722271, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:01.829093: step 5040, loss 0.212139, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5040

2017-10-10T12:05:02.812708: step 5041, loss 0.157064, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:03.043858: step 5042, loss 0.0707576, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:03.302075: step 5043, loss 0.101735, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:03.588893: step 5044, loss 0.0797919, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:03.809595: step 5045, loss 0.0353164, acc 1, learning_rate 0.0001
2017-10-10T12:05:04.040884: step 5046, loss 0.0282106, acc 1, learning_rate 0.0001
2017-10-10T12:05:04.260438: step 5047, loss 0.0632909, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:04.487330: step 5048, loss 0.0669544, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:04.737054: step 5049, loss 0.13201, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:05.006611: step 5050, loss 0.0430898, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:05.237242: step 5051, loss 0.067358, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:05.476953: step 5052, loss 0.0768305, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:05.695190: step 5053, loss 0.065868, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:05.957199: step 5054, loss 0.0480657, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:06.189466: step 5055, loss 0.0593162, acc 1, learning_rate 0.0001
2017-10-10T12:05:06.414860: step 5056, loss 0.0238074, acc 1, learning_rate 0.0001
2017-10-10T12:05:06.695156: step 5057, loss 0.0335905, acc 1, learning_rate 0.0001
2017-10-10T12:05:06.869882: step 5058, loss 0.0977824, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:07.068868: step 5059, loss 0.050929, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:07.268822: step 5060, loss 0.0810958, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:07.485186: step 5061, loss 0.0788453, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:07.652868: step 5062, loss 0.0427171, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:07.887994: step 5063, loss 0.0892695, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:08.097004: step 5064, loss 0.100908, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:08.344793: step 5065, loss 0.0664881, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:08.664849: step 5066, loss 0.115181, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:08.879893: step 5067, loss 0.0884302, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:09.051877: step 5068, loss 0.102453, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:09.225270: step 5069, loss 0.0841791, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:09.376632: step 5070, loss 0.119726, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:09.547329: step 5071, loss 0.131859, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:09.799648: step 5072, loss 0.0859108, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:10.041763: step 5073, loss 0.10199, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:10.281334: step 5074, loss 0.134003, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:10.529007: step 5075, loss 0.200969, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:10.758630: step 5076, loss 0.0426527, acc 1, learning_rate 0.0001
2017-10-10T12:05:10.997846: step 5077, loss 0.0930258, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:11.231064: step 5078, loss 0.0654371, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:11.465087: step 5079, loss 0.0908023, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:11.684310: step 5080, loss 0.0666219, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:12.207491: step 5080, loss 0.212789, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5080

2017-10-10T12:05:13.326776: step 5081, loss 0.0570222, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:13.568788: step 5082, loss 0.0579979, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:13.848717: step 5083, loss 0.0933857, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:14.065198: step 5084, loss 0.0645011, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:14.284447: step 5085, loss 0.123412, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:14.551138: step 5086, loss 0.0478497, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:14.775612: step 5087, loss 0.117208, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:15.037047: step 5088, loss 0.0755128, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:15.279067: step 5089, loss 0.0433105, acc 1, learning_rate 0.0001
2017-10-10T12:05:15.479663: step 5090, loss 0.152191, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:15.663835: step 5091, loss 0.0506678, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:15.869589: step 5092, loss 0.084044, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:16.069403: step 5093, loss 0.109155, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:16.226204: step 5094, loss 0.0776342, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:16.416871: step 5095, loss 0.0837704, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:16.617033: step 5096, loss 0.102687, acc 0.980392, learning_rate 0.0001
2017-10-10T12:05:16.866613: step 5097, loss 0.0520673, acc 1, learning_rate 0.0001
2017-10-10T12:05:17.077966: step 5098, loss 0.0651386, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:17.308301: step 5099, loss 0.0488117, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:17.548519: step 5100, loss 0.131792, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:17.814640: step 5101, loss 0.0276025, acc 1, learning_rate 0.0001
2017-10-10T12:05:18.052663: step 5102, loss 0.126798, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:18.343841: step 5103, loss 0.120328, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:18.584883: step 5104, loss 0.0709533, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:18.802946: step 5105, loss 0.0942643, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:19.053688: step 5106, loss 0.128061, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:19.289568: step 5107, loss 0.0704644, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:19.577754: step 5108, loss 0.0879765, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:19.766457: step 5109, loss 0.0642469, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:19.961698: step 5110, loss 0.0875008, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:20.144871: step 5111, loss 0.208328, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:20.316919: step 5112, loss 0.0435849, acc 1, learning_rate 0.0001
2017-10-10T12:05:20.471816: step 5113, loss 0.209114, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:20.720397: step 5114, loss 0.150714, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:20.999382: step 5115, loss 0.0208185, acc 1, learning_rate 0.0001
2017-10-10T12:05:21.252020: step 5116, loss 0.0476096, acc 1, learning_rate 0.0001
2017-10-10T12:05:21.493388: step 5117, loss 0.104602, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:21.748869: step 5118, loss 0.205028, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:22.000198: step 5119, loss 0.127594, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:22.231246: step 5120, loss 0.105615, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:22.746868: step 5120, loss 0.210684, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5120

2017-10-10T12:05:23.812818: step 5121, loss 0.0400921, acc 1, learning_rate 0.0001
2017-10-10T12:05:24.012816: step 5122, loss 0.104328, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:24.168919: step 5123, loss 0.0717822, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:24.338694: step 5124, loss 0.0814353, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:24.528636: step 5125, loss 0.126575, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:24.761769: step 5126, loss 0.212069, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:24.977041: step 5127, loss 0.0580794, acc 1, learning_rate 0.0001
2017-10-10T12:05:25.207565: step 5128, loss 0.0831271, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:25.447763: step 5129, loss 0.0827412, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:25.676286: step 5130, loss 0.0472144, acc 1, learning_rate 0.0001
2017-10-10T12:05:25.933705: step 5131, loss 0.051794, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:26.171253: step 5132, loss 0.176485, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:26.429279: step 5133, loss 0.0645766, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:26.684378: step 5134, loss 0.0806411, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:26.923776: step 5135, loss 0.0644688, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:27.154518: step 5136, loss 0.0968443, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:27.408924: step 5137, loss 0.0509886, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:27.646585: step 5138, loss 0.144855, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:27.881976: step 5139, loss 0.0346143, acc 1, learning_rate 0.0001
2017-10-10T12:05:28.096883: step 5140, loss 0.0307651, acc 1, learning_rate 0.0001
2017-10-10T12:05:28.339834: step 5141, loss 0.0459478, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:28.571806: step 5142, loss 0.0742939, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:28.805862: step 5143, loss 0.0909161, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:29.040855: step 5144, loss 0.104448, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:29.279974: step 5145, loss 0.0493038, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:29.483766: step 5146, loss 0.0948796, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:29.692911: step 5147, loss 0.0650108, acc 1, learning_rate 0.0001
2017-10-10T12:05:29.985167: step 5148, loss 0.0689053, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:30.260882: step 5149, loss 0.0549123, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:30.550412: step 5150, loss 0.118719, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:30.724827: step 5151, loss 0.104356, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:30.902167: step 5152, loss 0.110429, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:31.058938: step 5153, loss 0.0691875, acc 1, learning_rate 0.0001
2017-10-10T12:05:31.226558: step 5154, loss 0.0929527, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:31.392738: step 5155, loss 0.172418, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:31.581652: step 5156, loss 0.0686065, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:31.822748: step 5157, loss 0.0662562, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:32.094361: step 5158, loss 0.0392182, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:32.288820: step 5159, loss 0.114025, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:32.490089: step 5160, loss 0.0847954, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:32.896084: step 5160, loss 0.210704, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5160

2017-10-10T12:05:34.200234: step 5161, loss 0.0514369, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:34.458484: step 5162, loss 0.0722693, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:34.704834: step 5163, loss 0.0600707, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:34.956856: step 5164, loss 0.0671608, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:35.175606: step 5165, loss 0.0895184, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:35.443387: step 5166, loss 0.0858297, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:35.662428: step 5167, loss 0.0303699, acc 1, learning_rate 0.0001
2017-10-10T12:05:35.898037: step 5168, loss 0.0453898, acc 1, learning_rate 0.0001
2017-10-10T12:05:36.146930: step 5169, loss 0.0724463, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:36.397877: step 5170, loss 0.204193, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:36.640520: step 5171, loss 0.099374, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:36.879586: step 5172, loss 0.0397833, acc 1, learning_rate 0.0001
2017-10-10T12:05:37.114276: step 5173, loss 0.212985, acc 0.890625, learning_rate 0.0001
2017-10-10T12:05:37.345206: step 5174, loss 0.0822661, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:37.612685: step 5175, loss 0.151615, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:37.837783: step 5176, loss 0.0458398, acc 1, learning_rate 0.0001
2017-10-10T12:05:38.092907: step 5177, loss 0.132082, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:38.317003: step 5178, loss 0.0543099, acc 1, learning_rate 0.0001
2017-10-10T12:05:38.530972: step 5179, loss 0.1136, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:38.779589: step 5180, loss 0.105331, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.051959: step 5181, loss 0.0598504, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.290382: step 5182, loss 0.0736873, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.512539: step 5183, loss 0.130648, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.738479: step 5184, loss 0.0666358, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.975157: step 5185, loss 0.0939614, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:40.208884: step 5186, loss 0.122366, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:40.495600: step 5187, loss 0.0433246, acc 1, learning_rate 0.0001
2017-10-10T12:05:40.685240: step 5188, loss 0.0506128, acc 1, learning_rate 0.0001
2017-10-10T12:05:40.868467: step 5189, loss 0.124828, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:41.096857: step 5190, loss 0.0860334, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:41.361399: step 5191, loss 0.0494389, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:41.486968: step 5192, loss 0.0679538, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:41.686418: step 5193, loss 0.0806228, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:41.831210: step 5194, loss 0.0473678, acc 1, learning_rate 0.0001
2017-10-10T12:05:41.994119: step 5195, loss 0.0683876, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:42.164885: step 5196, loss 0.10697, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:42.433430: step 5197, loss 0.0963345, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:42.678278: step 5198, loss 0.111607, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:42.903458: step 5199, loss 0.142684, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:43.132869: step 5200, loss 0.107787, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:43.624025: step 5200, loss 0.210768, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5200

2017-10-10T12:05:44.605084: step 5201, loss 0.0966933, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:44.825081: step 5202, loss 0.136971, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:45.055190: step 5203, loss 0.07407, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:45.296922: step 5204, loss 0.129251, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:45.504676: step 5205, loss 0.0308394, acc 1, learning_rate 0.0001
2017-10-10T12:05:45.773364: step 5206, loss 0.0762553, acc 1, learning_rate 0.0001
2017-10-10T12:05:46.026455: step 5207, loss 0.0561761, acc 1, learning_rate 0.0001
2017-10-10T12:05:46.294996: step 5208, loss 0.086875, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:46.527965: step 5209, loss 0.111972, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:46.756114: step 5210, loss 0.0952417, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:47.002668: step 5211, loss 0.0679823, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:47.258751: step 5212, loss 0.0626994, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:47.496976: step 5213, loss 0.0521362, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:47.792374: step 5214, loss 0.0781652, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:48.023634: step 5215, loss 0.13214, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:48.261766: step 5216, loss 0.114174, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:48.514097: step 5217, loss 0.0706849, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:48.789411: step 5218, loss 0.115138, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:48.986856: step 5219, loss 0.0946132, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:49.152843: step 5220, loss 0.0783434, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:49.315190: step 5221, loss 0.0842296, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:49.491673: step 5222, loss 0.0426359, acc 1, learning_rate 0.0001
2017-10-10T12:05:49.706037: step 5223, loss 0.0471806, acc 1, learning_rate 0.0001
2017-10-10T12:05:49.955760: step 5224, loss 0.107717, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:50.156921: step 5225, loss 0.102749, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:50.394511: step 5226, loss 0.0739611, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:50.645792: step 5227, loss 0.17218, acc 0.90625, learning_rate 0.0001
2017-10-10T12:05:50.893292: step 5228, loss 0.0662895, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:51.156813: step 5229, loss 0.128122, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:51.377012: step 5230, loss 0.0449769, acc 1, learning_rate 0.0001
2017-10-10T12:05:51.608881: step 5231, loss 0.0711138, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:51.878415: step 5232, loss 0.142671, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:52.168904: step 5233, loss 0.030776, acc 1, learning_rate 0.0001
2017-10-10T12:05:52.450538: step 5234, loss 0.168924, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:52.609286: step 5235, loss 0.0749339, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:52.771905: step 5236, loss 0.124222, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:52.939891: step 5237, loss 0.0400716, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:53.112245: step 5238, loss 0.0823688, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:53.288844: step 5239, loss 0.0752497, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:53.474690: step 5240, loss 0.0276591, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:53.976827: step 5240, loss 0.207925, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5240

2017-10-10T12:05:55.076821: step 5241, loss 0.0553227, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:55.273818: step 5242, loss 0.116829, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:55.505122: step 5243, loss 0.128138, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:55.749211: step 5244, loss 0.0531911, acc 1, learning_rate 0.0001
2017-10-10T12:05:56.000714: step 5245, loss 0.0973102, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:56.241908: step 5246, loss 0.0545987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:56.488871: step 5247, loss 0.117838, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:56.748866: step 5248, loss 0.0680978, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:56.976678: step 5249, loss 0.0387435, acc 1, learning_rate 0.0001
2017-10-10T12:05:57.178777: step 5250, loss 0.0907392, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:57.367827: step 5251, loss 0.0951608, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:57.559179: step 5252, loss 0.0678687, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:57.745055: step 5253, loss 0.105554, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:58.000873: step 5254, loss 0.0904937, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:58.242244: step 5255, loss 0.113955, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:58.482982: step 5256, loss 0.199617, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:58.680212: step 5257, loss 0.0479781, acc 1, learning_rate 0.0001
2017-10-10T12:05:58.899759: step 5258, loss 0.0809079, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:59.106693: step 5259, loss 0.0410837, acc 1, learning_rate 0.0001
2017-10-10T12:05:59.336972: step 5260, loss 0.0498628, acc 1, learning_rate 0.0001
2017-10-10T12:05:59.581009: step 5261, loss 0.0609311, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:59.835569: step 5262, loss 0.12237, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:00.048881: step 5263, loss 0.052514, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:00.316837: step 5264, loss 0.0539951, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:00.532882: step 5265, loss 0.168761, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:00.779301: step 5266, loss 0.0386163, acc 1, learning_rate 0.0001
2017-10-10T12:06:01.004208: step 5267, loss 0.0733916, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:01.268411: step 5268, loss 0.0929998, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:01.511774: step 5269, loss 0.130038, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:01.767315: step 5270, loss 0.173113, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:02.000843: step 5271, loss 0.129026, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:02.220027: step 5272, loss 0.0878538, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:02.450642: step 5273, loss 0.0972994, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:02.675532: step 5274, loss 0.0825584, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:02.920841: step 5275, loss 0.120035, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:03.196888: step 5276, loss 0.109845, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:03.510698: step 5277, loss 0.0499989, acc 1, learning_rate 0.0001
2017-10-10T12:06:03.676676: step 5278, loss 0.0898785, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:03.852285: step 5279, loss 0.0905433, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:04.044671: step 5280, loss 0.0835034, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:04.428265: step 5280, loss 0.211486, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5280

2017-10-10T12:06:05.509740: step 5281, loss 0.0794863, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:05.709280: step 5282, loss 0.0640295, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:05.913881: step 5283, loss 0.0604349, acc 1, learning_rate 0.0001
2017-10-10T12:06:06.106475: step 5284, loss 0.0752221, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:06.333392: step 5285, loss 0.043749, acc 1, learning_rate 0.0001
2017-10-10T12:06:06.569966: step 5286, loss 0.0898585, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:06.821050: step 5287, loss 0.128543, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:07.056167: step 5288, loss 0.112953, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:07.296265: step 5289, loss 0.0870556, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:07.525174: step 5290, loss 0.0512049, acc 1, learning_rate 0.0001
2017-10-10T12:06:07.758153: step 5291, loss 0.0314099, acc 1, learning_rate 0.0001
2017-10-10T12:06:07.943288: step 5292, loss 0.152668, acc 0.921569, learning_rate 0.0001
2017-10-10T12:06:08.148252: step 5293, loss 0.289954, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:08.428947: step 5294, loss 0.0784033, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:08.656885: step 5295, loss 0.0714861, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:08.869037: step 5296, loss 0.0477869, acc 1, learning_rate 0.0001
2017-10-10T12:06:09.096873: step 5297, loss 0.101434, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:09.300239: step 5298, loss 0.0612959, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:09.554922: step 5299, loss 0.0600676, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:09.791842: step 5300, loss 0.0813664, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:10.031896: step 5301, loss 0.0427773, acc 1, learning_rate 0.0001
2017-10-10T12:06:10.286740: step 5302, loss 0.133688, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:10.522442: step 5303, loss 0.125899, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:10.753013: step 5304, loss 0.128384, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:10.997285: step 5305, loss 0.0878743, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:11.231499: step 5306, loss 0.0660112, acc 1, learning_rate 0.0001
2017-10-10T12:06:11.474780: step 5307, loss 0.117946, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:11.705393: step 5308, loss 0.041514, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:11.943627: step 5309, loss 0.0938034, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:12.183824: step 5310, loss 0.0968395, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:12.444782: step 5311, loss 0.0675884, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:12.657001: step 5312, loss 0.0501994, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:12.877757: step 5313, loss 0.0460733, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:13.114661: step 5314, loss 0.0396712, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:13.344765: step 5315, loss 0.0558134, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:13.588990: step 5316, loss 0.0616366, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:13.877637: step 5317, loss 0.129717, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:14.068834: step 5318, loss 0.0841764, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:14.358909: step 5319, loss 0.089881, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:14.497160: step 5320, loss 0.0714873, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:14.952141: step 5320, loss 0.21166, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5320

2017-10-10T12:06:15.929865: step 5321, loss 0.0914501, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:16.150184: step 5322, loss 0.166071, acc 0.90625, learning_rate 0.0001
2017-10-10T12:06:16.356979: step 5323, loss 0.0681677, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:16.565129: step 5324, loss 0.0666789, acc 1, learning_rate 0.0001
2017-10-10T12:06:16.826859: step 5325, loss 0.0609531, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:17.046244: step 5326, loss 0.0284241, acc 1, learning_rate 0.0001
2017-10-10T12:06:17.258547: step 5327, loss 0.101333, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:17.472597: step 5328, loss 0.0757548, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:17.729151: step 5329, loss 0.117486, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:17.969584: step 5330, loss 0.127361, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:18.178218: step 5331, loss 0.0421734, acc 1, learning_rate 0.0001
2017-10-10T12:06:18.416848: step 5332, loss 0.04832, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:18.658493: step 5333, loss 0.079711, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:18.910104: step 5334, loss 0.0445103, acc 1, learning_rate 0.0001
2017-10-10T12:06:19.184077: step 5335, loss 0.0724047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.418826: step 5336, loss 0.0935956, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.657408: step 5337, loss 0.0846932, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.925403: step 5338, loss 0.154086, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:20.164272: step 5339, loss 0.0813966, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:20.392184: step 5340, loss 0.106175, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:20.618909: step 5341, loss 0.0813582, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:20.844863: step 5342, loss 0.0699532, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:21.028884: step 5343, loss 0.0919687, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:21.256979: step 5344, loss 0.0576937, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:21.474139: step 5345, loss 0.134167, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:21.748869: step 5346, loss 0.143383, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:21.954732: step 5347, loss 0.0985747, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:22.155321: step 5348, loss 0.0442313, acc 1, learning_rate 0.0001
2017-10-10T12:06:22.361380: step 5349, loss 0.0361482, acc 1, learning_rate 0.0001
2017-10-10T12:06:22.561362: step 5350, loss 0.0945525, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:22.768491: step 5351, loss 0.165861, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:22.999411: step 5352, loss 0.0760479, acc 1, learning_rate 0.0001
2017-10-10T12:06:23.233273: step 5353, loss 0.0460234, acc 1, learning_rate 0.0001
2017-10-10T12:06:23.477741: step 5354, loss 0.0684695, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:23.723016: step 5355, loss 0.0513889, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:23.959463: step 5356, loss 0.137441, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:24.205418: step 5357, loss 0.0846351, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:24.439721: step 5358, loss 0.0597533, acc 1, learning_rate 0.0001
2017-10-10T12:06:24.662865: step 5359, loss 0.0663185, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:24.902531: step 5360, loss 0.159401, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:25.578392: step 5360, loss 0.209143, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5360

2017-10-10T12:06:26.652417: step 5361, loss 0.0664462, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:26.891210: step 5362, loss 0.054168, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:27.119302: step 5363, loss 0.119873, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:27.340818: step 5364, loss 0.0456154, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:27.587770: step 5365, loss 0.0636909, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:27.819581: step 5366, loss 0.0794085, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:28.059640: step 5367, loss 0.0548254, acc 1, learning_rate 0.0001
2017-10-10T12:06:28.298740: step 5368, loss 0.131001, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:28.535057: step 5369, loss 0.0888123, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:28.796442: step 5370, loss 0.116794, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:29.024847: step 5371, loss 0.0213266, acc 1, learning_rate 0.0001
2017-10-10T12:06:29.252926: step 5372, loss 0.0834293, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:29.469217: step 5373, loss 0.0766102, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:29.769159: step 5374, loss 0.164747, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:30.021294: step 5375, loss 0.0366804, acc 1, learning_rate 0.0001
2017-10-10T12:06:30.176979: step 5376, loss 0.0785852, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:30.330299: step 5377, loss 0.0827429, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:30.527047: step 5378, loss 0.0484071, acc 1, learning_rate 0.0001
2017-10-10T12:06:30.716401: step 5379, loss 0.0555149, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:30.888111: step 5380, loss 0.0651268, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:31.111645: step 5381, loss 0.191543, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:31.344842: step 5382, loss 0.154046, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:31.612866: step 5383, loss 0.109572, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:31.847886: step 5384, loss 0.0882585, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:32.083705: step 5385, loss 0.0511364, acc 1, learning_rate 0.0001
2017-10-10T12:06:32.314863: step 5386, loss 0.0960171, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:32.522598: step 5387, loss 0.0907544, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:32.695910: step 5388, loss 0.0886852, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:32.934480: step 5389, loss 0.0337557, acc 1, learning_rate 0.0001
2017-10-10T12:06:33.142742: step 5390, loss 0.0879454, acc 0.960784, learning_rate 0.0001
2017-10-10T12:06:33.372449: step 5391, loss 0.0963247, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:33.603260: step 5392, loss 0.0885947, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:33.784668: step 5393, loss 0.0540475, acc 1, learning_rate 0.0001
2017-10-10T12:06:34.026829: step 5394, loss 0.118115, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:34.183517: step 5395, loss 0.138022, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:34.402904: step 5396, loss 0.0365435, acc 1, learning_rate 0.0001
2017-10-10T12:06:34.644950: step 5397, loss 0.0770792, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:34.893289: step 5398, loss 0.0992391, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:35.151372: step 5399, loss 0.0647219, acc 1, learning_rate 0.0001
2017-10-10T12:06:35.391633: step 5400, loss 0.103728, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:35.928007: step 5400, loss 0.210173, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5400

2017-10-10T12:06:37.080882: step 5401, loss 0.033858, acc 1, learning_rate 0.0001
2017-10-10T12:06:37.280330: step 5402, loss 0.158666, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:37.515361: step 5403, loss 0.125926, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:37.736841: step 5404, loss 0.118408, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:38.032836: step 5405, loss 0.0605363, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:38.238060: step 5406, loss 0.0506209, acc 1, learning_rate 0.0001
2017-10-10T12:06:38.442801: step 5407, loss 0.0788621, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:38.644568: step 5408, loss 0.0478156, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:38.851642: step 5409, loss 0.107024, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:39.051117: step 5410, loss 0.0527488, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:39.292871: step 5411, loss 0.170961, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:39.560862: step 5412, loss 0.0748652, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:39.801464: step 5413, loss 0.135094, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:40.035032: step 5414, loss 0.0644717, acc 1, learning_rate 0.0001
2017-10-10T12:06:40.259696: step 5415, loss 0.058088, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:40.486321: step 5416, loss 0.0665663, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:40.744394: step 5417, loss 0.0490086, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:40.974687: step 5418, loss 0.115767, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:41.210391: step 5419, loss 0.0376528, acc 1, learning_rate 0.0001
2017-10-10T12:06:41.480236: step 5420, loss 0.0256579, acc 1, learning_rate 0.0001
2017-10-10T12:06:41.712094: step 5421, loss 0.0440098, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:41.975733: step 5422, loss 0.0731653, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:42.227096: step 5423, loss 0.0976307, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:42.492843: step 5424, loss 0.0452375, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:42.742106: step 5425, loss 0.0843448, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:42.972842: step 5426, loss 0.063246, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:43.227184: step 5427, loss 0.0734645, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:43.465535: step 5428, loss 0.171862, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:43.706130: step 5429, loss 0.0589201, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:43.964604: step 5430, loss 0.0552289, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:44.206118: step 5431, loss 0.142171, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:44.445942: step 5432, loss 0.103835, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:44.631098: step 5433, loss 0.0469358, acc 1, learning_rate 0.0001
2017-10-10T12:06:44.836893: step 5434, loss 0.089139, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:45.036917: step 5435, loss 0.101305, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:45.289907: step 5436, loss 0.13997, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:45.544433: step 5437, loss 0.0432077, acc 1, learning_rate 0.0001
2017-10-10T12:06:45.790777: step 5438, loss 0.0926214, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:46.028382: step 5439, loss 0.0632935, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:46.288848: step 5440, loss 0.0753209, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:46.809428: step 5440, loss 0.210741, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5440

2017-10-10T12:06:47.646898: step 5441, loss 0.0595826, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:47.827187: step 5442, loss 0.0452184, acc 1, learning_rate 0.0001
2017-10-10T12:06:48.027754: step 5443, loss 0.177891, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:48.198575: step 5444, loss 0.0942309, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:48.433245: step 5445, loss 0.181718, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:48.692861: step 5446, loss 0.0841317, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:48.954506: step 5447, loss 0.0664603, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:49.191028: step 5448, loss 0.0974206, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:49.456590: step 5449, loss 0.029923, acc 1, learning_rate 0.0001
2017-10-10T12:06:49.709018: step 5450, loss 0.068961, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:49.958812: step 5451, loss 0.0667806, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:50.189036: step 5452, loss 0.102532, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:50.423270: step 5453, loss 0.147071, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:50.686626: step 5454, loss 0.117026, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:50.942114: step 5455, loss 0.11022, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:51.197860: step 5456, loss 0.100251, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:51.432829: step 5457, loss 0.21127, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:51.694046: step 5458, loss 0.0973794, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:51.955265: step 5459, loss 0.0609462, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:52.230286: step 5460, loss 0.0725243, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:52.476820: step 5461, loss 0.106682, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:52.708328: step 5462, loss 0.0944571, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:52.984457: step 5463, loss 0.0456578, acc 1, learning_rate 0.0001
2017-10-10T12:06:53.224396: step 5464, loss 0.0635619, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:53.452842: step 5465, loss 0.067872, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:53.649036: step 5466, loss 0.0512039, acc 1, learning_rate 0.0001
2017-10-10T12:06:53.870708: step 5467, loss 0.0973946, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:54.097434: step 5468, loss 0.111985, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:54.338196: step 5469, loss 0.0841642, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:54.631453: step 5470, loss 0.112562, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:54.805652: step 5471, loss 0.171836, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:55.028829: step 5472, loss 0.0578877, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:55.236865: step 5473, loss 0.113297, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:55.436832: step 5474, loss 0.0799129, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:55.635038: step 5475, loss 0.0645785, acc 1, learning_rate 0.0001
2017-10-10T12:06:55.825642: step 5476, loss 0.128078, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:56.088916: step 5477, loss 0.0642578, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:56.326257: step 5478, loss 0.0516133, acc 1, learning_rate 0.0001
2017-10-10T12:06:56.564355: step 5479, loss 0.0941962, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:56.750044: step 5480, loss 0.0722395, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:57.248457: step 5480, loss 0.210358, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5480

2017-10-10T12:06:58.480874: step 5481, loss 0.0716221, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:58.675526: step 5482, loss 0.101668, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:58.835854: step 5483, loss 0.0754896, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:59.027520: step 5484, loss 0.0917137, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:59.184891: step 5485, loss 0.0612633, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:59.432894: step 5486, loss 0.0632098, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:59.693721: step 5487, loss 0.059395, acc 1, learning_rate 0.0001
2017-10-10T12:06:59.896464: step 5488, loss 0.135276, acc 0.960784, learning_rate 0.0001
2017-10-10T12:07:00.138898: step 5489, loss 0.0686571, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:00.400915: step 5490, loss 0.119349, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:00.655454: step 5491, loss 0.0797218, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:00.904799: step 5492, loss 0.0330274, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:01.137360: step 5493, loss 0.125491, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:01.345085: step 5494, loss 0.0983378, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:01.571009: step 5495, loss 0.129683, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:01.808802: step 5496, loss 0.201538, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:02.052206: step 5497, loss 0.0608668, acc 1, learning_rate 0.0001
2017-10-10T12:07:02.272860: step 5498, loss 0.167191, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:02.488888: step 5499, loss 0.0823081, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:02.727529: step 5500, loss 0.101253, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:02.940861: step 5501, loss 0.149609, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:03.176716: step 5502, loss 0.0407062, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:03.454262: step 5503, loss 0.0786774, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:03.612729: step 5504, loss 0.141684, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:03.806566: step 5505, loss 0.0361019, acc 1, learning_rate 0.0001
2017-10-10T12:07:04.002297: step 5506, loss 0.11407, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:04.204388: step 5507, loss 0.0970159, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:04.409550: step 5508, loss 0.078167, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:04.659691: step 5509, loss 0.062599, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:04.888405: step 5510, loss 0.228431, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:05.129431: step 5511, loss 0.126883, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:05.362376: step 5512, loss 0.120662, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:05.580465: step 5513, loss 0.0508036, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:05.810701: step 5514, loss 0.0708879, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:06.034386: step 5515, loss 0.0703262, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:06.267884: step 5516, loss 0.134962, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:06.512857: step 5517, loss 0.0902425, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:06.791026: step 5518, loss 0.0823668, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:07.024953: step 5519, loss 0.108009, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:07.243082: step 5520, loss 0.0838575, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:07.744755: step 5520, loss 0.21186, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5520

2017-10-10T12:07:09.267693: step 5521, loss 0.0562334, acc 1, learning_rate 0.0001
2017-10-10T12:07:09.444523: step 5522, loss 0.104679, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:09.620935: step 5523, loss 0.0965238, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:09.784892: step 5524, loss 0.0637404, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:09.981270: step 5525, loss 0.07442, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:10.163270: step 5526, loss 0.1091, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:10.440963: step 5527, loss 0.085859, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:10.652843: step 5528, loss 0.171407, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:10.884990: step 5529, loss 0.0739898, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:11.192630: step 5530, loss 0.103677, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:11.373051: step 5531, loss 0.0467198, acc 1, learning_rate 0.0001
2017-10-10T12:07:11.567836: step 5532, loss 0.0493078, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:11.780183: step 5533, loss 0.169474, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:11.956834: step 5534, loss 0.0683039, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:12.149672: step 5535, loss 0.0767417, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:12.410896: step 5536, loss 0.0348556, acc 1, learning_rate 0.0001
2017-10-10T12:07:12.645922: step 5537, loss 0.0608303, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:12.850523: step 5538, loss 0.0725649, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:13.088849: step 5539, loss 0.0413235, acc 1, learning_rate 0.0001
2017-10-10T12:07:13.329114: step 5540, loss 0.0345464, acc 1, learning_rate 0.0001
2017-10-10T12:07:13.556802: step 5541, loss 0.0307661, acc 1, learning_rate 0.0001
2017-10-10T12:07:13.777033: step 5542, loss 0.060369, acc 1, learning_rate 0.0001
2017-10-10T12:07:13.975894: step 5543, loss 0.0598241, acc 1, learning_rate 0.0001
2017-10-10T12:07:14.200997: step 5544, loss 0.0647074, acc 1, learning_rate 0.0001
2017-10-10T12:07:14.410509: step 5545, loss 0.140461, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:14.620247: step 5546, loss 0.0801486, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:14.888981: step 5547, loss 0.0536145, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:15.108519: step 5548, loss 0.0418247, acc 1, learning_rate 0.0001
2017-10-10T12:07:15.349385: step 5549, loss 0.165261, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:15.591427: step 5550, loss 0.126646, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:15.821096: step 5551, loss 0.144941, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:16.072513: step 5552, loss 0.158938, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:16.309884: step 5553, loss 0.150544, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:16.542458: step 5554, loss 0.0478628, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:16.777218: step 5555, loss 0.0382115, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:17.021938: step 5556, loss 0.0581593, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:17.252499: step 5557, loss 0.0403946, acc 1, learning_rate 0.0001
2017-10-10T12:07:17.472923: step 5558, loss 0.101117, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:17.728286: step 5559, loss 0.181061, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:17.950391: step 5560, loss 0.0381743, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:18.416946: step 5560, loss 0.21257, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5560

2017-10-10T12:07:19.756850: step 5561, loss 0.046006, acc 1, learning_rate 0.0001
2017-10-10T12:07:20.018400: step 5562, loss 0.0877015, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:20.142694: step 5563, loss 0.0754692, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:20.263358: step 5564, loss 0.0793974, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:20.384464: step 5565, loss 0.095155, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:20.511543: step 5566, loss 0.0772353, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:20.672873: step 5567, loss 0.205164, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:20.860287: step 5568, loss 0.0747602, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:21.044854: step 5569, loss 0.0676409, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:21.252005: step 5570, loss 0.0684108, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:21.508067: step 5571, loss 0.128188, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:21.753811: step 5572, loss 0.10276, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:21.972570: step 5573, loss 0.0495548, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:22.240860: step 5574, loss 0.0248364, acc 1, learning_rate 0.0001
2017-10-10T12:07:22.506784: step 5575, loss 0.0552115, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:22.744911: step 5576, loss 0.109811, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:22.994716: step 5577, loss 0.0790823, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:23.233016: step 5578, loss 0.119044, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:23.485537: step 5579, loss 0.111225, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:23.709594: step 5580, loss 0.0717544, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:23.952998: step 5581, loss 0.115883, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:24.203967: step 5582, loss 0.0668586, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:24.452918: step 5583, loss 0.0507061, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:24.719980: step 5584, loss 0.0524564, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:24.965826: step 5585, loss 0.10311, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:25.160941: step 5586, loss 0.0520017, acc 1, learning_rate 0.0001
2017-10-10T12:07:25.384438: step 5587, loss 0.0872123, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:25.604911: step 5588, loss 0.189617, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:25.805026: step 5589, loss 0.062131, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:26.049392: step 5590, loss 0.0965665, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:26.242008: step 5591, loss 0.0673072, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:26.451296: step 5592, loss 0.0469083, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:26.676819: step 5593, loss 0.0590975, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:26.908487: step 5594, loss 0.0527665, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:27.151089: step 5595, loss 0.0817215, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:27.385325: step 5596, loss 0.159303, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:27.658443: step 5597, loss 0.0542725, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:27.979544: step 5598, loss 0.110203, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:28.181000: step 5599, loss 0.0428271, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:28.392283: step 5600, loss 0.106154, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:28.911576: step 5600, loss 0.212536, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5600

2017-10-10T12:07:29.892958: step 5601, loss 0.0414851, acc 1, learning_rate 0.0001
2017-10-10T12:07:30.145013: step 5602, loss 0.0765814, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:30.391938: step 5603, loss 0.0872802, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:30.648866: step 5604, loss 0.0292129, acc 1, learning_rate 0.0001
2017-10-10T12:07:30.968821: step 5605, loss 0.138121, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:31.171656: step 5606, loss 0.222309, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:31.317704: step 5607, loss 0.052275, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:31.468780: step 5608, loss 0.099817, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:31.659611: step 5609, loss 0.0439855, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:31.819676: step 5610, loss 0.0517187, acc 1, learning_rate 0.0001
2017-10-10T12:07:31.996110: step 5611, loss 0.0965376, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:32.168858: step 5612, loss 0.031946, acc 1, learning_rate 0.0001
2017-10-10T12:07:32.344913: step 5613, loss 0.221571, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:32.573076: step 5614, loss 0.0629473, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:32.798986: step 5615, loss 0.059507, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:33.016824: step 5616, loss 0.047306, acc 1, learning_rate 0.0001
2017-10-10T12:07:33.237723: step 5617, loss 0.0938901, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:33.506487: step 5618, loss 0.0923987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:33.740863: step 5619, loss 0.0667209, acc 1, learning_rate 0.0001
2017-10-10T12:07:33.992903: step 5620, loss 0.0468536, acc 1, learning_rate 0.0001
2017-10-10T12:07:34.258966: step 5621, loss 0.0335711, acc 1, learning_rate 0.0001
2017-10-10T12:07:34.487777: step 5622, loss 0.0673365, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:34.732857: step 5623, loss 0.074055, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:34.968203: step 5624, loss 0.108854, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:35.203701: step 5625, loss 0.103362, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:35.435937: step 5626, loss 0.0697789, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:35.678064: step 5627, loss 0.109487, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:35.915612: step 5628, loss 0.0649743, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:36.236495: step 5629, loss 0.149122, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:36.437882: step 5630, loss 0.0542971, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:36.651491: step 5631, loss 0.0953444, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:36.840447: step 5632, loss 0.0377278, acc 1, learning_rate 0.0001
2017-10-10T12:07:37.036250: step 5633, loss 0.029043, acc 1, learning_rate 0.0001
2017-10-10T12:07:37.263318: step 5634, loss 0.0805303, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:37.504878: step 5635, loss 0.0695131, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:37.754276: step 5636, loss 0.0609885, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:38.010730: step 5637, loss 0.0508627, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:38.242028: step 5638, loss 0.0583711, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:38.459467: step 5639, loss 0.0496726, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:38.705814: step 5640, loss 0.169286, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:39.213117: step 5640, loss 0.20883, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5640

2017-10-10T12:07:40.316960: step 5641, loss 0.191904, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:40.536816: step 5642, loss 0.0770153, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:40.793927: step 5643, loss 0.0580436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:41.045320: step 5644, loss 0.0982335, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:41.284829: step 5645, loss 0.053403, acc 1, learning_rate 0.0001
2017-10-10T12:07:41.535925: step 5646, loss 0.110167, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:41.781149: step 5647, loss 0.14793, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:42.031266: step 5648, loss 0.117775, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:42.359812: step 5649, loss 0.0687163, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:42.533325: step 5650, loss 0.0800328, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:42.698112: step 5651, loss 0.0241466, acc 1, learning_rate 0.0001
2017-10-10T12:07:42.880811: step 5652, loss 0.116726, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:43.052851: step 5653, loss 0.0467485, acc 1, learning_rate 0.0001
2017-10-10T12:07:43.228102: step 5654, loss 0.13014, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:43.443953: step 5655, loss 0.0681148, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:43.700889: step 5656, loss 0.118618, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:43.923100: step 5657, loss 0.0618829, acc 1, learning_rate 0.0001
2017-10-10T12:07:44.224851: step 5658, loss 0.211161, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:44.467924: step 5659, loss 0.078405, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:44.662904: step 5660, loss 0.0975884, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:44.857229: step 5661, loss 0.0385942, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:45.064050: step 5662, loss 0.0635047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:45.259359: step 5663, loss 0.121265, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:45.518835: step 5664, loss 0.113322, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:45.765283: step 5665, loss 0.0782428, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:46.001041: step 5666, loss 0.0636455, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:46.264832: step 5667, loss 0.117027, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:46.515183: step 5668, loss 0.0374273, acc 1, learning_rate 0.0001
2017-10-10T12:07:46.753523: step 5669, loss 0.040106, acc 1, learning_rate 0.0001
2017-10-10T12:07:47.043869: step 5670, loss 0.0564883, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:47.293959: step 5671, loss 0.054196, acc 1, learning_rate 0.0001
2017-10-10T12:07:47.554203: step 5672, loss 0.121121, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:47.781875: step 5673, loss 0.0532922, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:48.023679: step 5674, loss 0.0882323, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:48.256177: step 5675, loss 0.0758205, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:48.451103: step 5676, loss 0.0633385, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:48.697799: step 5677, loss 0.0786846, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:48.940390: step 5678, loss 0.050035, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:49.192869: step 5679, loss 0.0464604, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:49.415789: step 5680, loss 0.0402396, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:49.992594: step 5680, loss 0.209505, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5680

2017-10-10T12:07:51.261973: step 5681, loss 0.0693083, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:51.486424: step 5682, loss 0.0357814, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:51.728829: step 5683, loss 0.0483832, acc 1, learning_rate 0.0001
2017-10-10T12:07:51.971289: step 5684, loss 0.0531617, acc 0.980392, learning_rate 0.0001
2017-10-10T12:07:52.193086: step 5685, loss 0.117412, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:52.494274: step 5686, loss 0.0716374, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:52.668871: step 5687, loss 0.0552357, acc 1, learning_rate 0.0001
2017-10-10T12:07:52.872851: step 5688, loss 0.0829756, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:53.164420: step 5689, loss 0.189276, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:53.316611: step 5690, loss 0.136799, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:53.455797: step 5691, loss 0.0688091, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:53.629875: step 5692, loss 0.058964, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:53.790080: step 5693, loss 0.0895112, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:53.973718: step 5694, loss 0.0465769, acc 1, learning_rate 0.0001
2017-10-10T12:07:54.169795: step 5695, loss 0.0452474, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:54.359084: step 5696, loss 0.0259914, acc 1, learning_rate 0.0001
2017-10-10T12:07:54.554702: step 5697, loss 0.11055, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:54.775874: step 5698, loss 0.0645719, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:55.038553: step 5699, loss 0.0755437, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:55.278828: step 5700, loss 0.0734205, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:55.508189: step 5701, loss 0.113263, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:55.728835: step 5702, loss 0.137773, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:55.976852: step 5703, loss 0.102178, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:56.220855: step 5704, loss 0.0340955, acc 1, learning_rate 0.0001
2017-10-10T12:07:56.436891: step 5705, loss 0.0363434, acc 1, learning_rate 0.0001
2017-10-10T12:07:56.638820: step 5706, loss 0.0699518, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:56.853162: step 5707, loss 0.101248, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:57.076778: step 5708, loss 0.0532059, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:57.275873: step 5709, loss 0.158184, acc 0.890625, learning_rate 0.0001
2017-10-10T12:07:57.533535: step 5710, loss 0.0449707, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:57.729121: step 5711, loss 0.0833322, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:57.956390: step 5712, loss 0.0946162, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:58.181989: step 5713, loss 0.0894812, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:58.428839: step 5714, loss 0.0472143, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:58.669961: step 5715, loss 0.060071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:58.914204: step 5716, loss 0.124536, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:59.151778: step 5717, loss 0.0738858, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:59.375359: step 5718, loss 0.0628362, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:59.606282: step 5719, loss 0.111213, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:59.820873: step 5720, loss 0.140269, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:00.371285: step 5720, loss 0.210561, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5720

2017-10-10T12:08:01.418799: step 5721, loss 0.0763683, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:01.589555: step 5722, loss 0.0440376, acc 1, learning_rate 0.0001
2017-10-10T12:08:01.780741: step 5723, loss 0.130137, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:02.004890: step 5724, loss 0.0714472, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:02.249265: step 5725, loss 0.219796, acc 0.90625, learning_rate 0.0001
2017-10-10T12:08:02.490368: step 5726, loss 0.162564, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:02.737133: step 5727, loss 0.02598, acc 1, learning_rate 0.0001
2017-10-10T12:08:02.933366: step 5728, loss 0.0960626, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:03.144989: step 5729, loss 0.0930848, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:03.430626: step 5730, loss 0.0563068, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:03.652316: step 5731, loss 0.159283, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:03.908844: step 5732, loss 0.0488634, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:04.181667: step 5733, loss 0.045877, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:04.335962: step 5734, loss 0.0180294, acc 1, learning_rate 0.0001
2017-10-10T12:08:04.488859: step 5735, loss 0.102126, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:04.673047: step 5736, loss 0.0619814, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:04.850483: step 5737, loss 0.0377556, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:05.022684: step 5738, loss 0.0730315, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:05.236384: step 5739, loss 0.0425917, acc 1, learning_rate 0.0001
2017-10-10T12:08:05.507312: step 5740, loss 0.0757886, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:05.739093: step 5741, loss 0.0593747, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:05.944869: step 5742, loss 0.165842, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:06.165050: step 5743, loss 0.114112, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:06.433133: step 5744, loss 0.12226, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:06.656889: step 5745, loss 0.0971032, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:06.920837: step 5746, loss 0.0266362, acc 1, learning_rate 0.0001
2017-10-10T12:08:07.121444: step 5747, loss 0.134572, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:07.289499: step 5748, loss 0.111354, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:07.499475: step 5749, loss 0.0853239, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:07.733043: step 5750, loss 0.0274059, acc 1, learning_rate 0.0001
2017-10-10T12:08:07.989279: step 5751, loss 0.102256, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:08.249373: step 5752, loss 0.0371646, acc 1, learning_rate 0.0001
2017-10-10T12:08:08.497664: step 5753, loss 0.0791656, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.758228: step 5754, loss 0.0608212, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.980915: step 5755, loss 0.077654, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:09.284271: step 5756, loss 0.119339, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:09.476477: step 5757, loss 0.101213, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:09.652818: step 5758, loss 0.0456964, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:09.864838: step 5759, loss 0.121036, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:10.058205: step 5760, loss 0.090832, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:10.513254: step 5760, loss 0.209318, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5760

2017-10-10T12:08:11.652883: step 5761, loss 0.105073, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:11.903204: step 5762, loss 0.0375765, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:12.151160: step 5763, loss 0.0968584, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:12.376822: step 5764, loss 0.113521, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:12.573095: step 5765, loss 0.0496706, acc 1, learning_rate 0.0001
2017-10-10T12:08:12.768017: step 5766, loss 0.0852605, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:13.010755: step 5767, loss 0.0887594, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:13.274658: step 5768, loss 0.0960912, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:13.492862: step 5769, loss 0.0431992, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:13.719678: step 5770, loss 0.115966, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:13.977003: step 5771, loss 0.0441591, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:14.220421: step 5772, loss 0.102493, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:14.464861: step 5773, loss 0.0900487, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:14.682209: step 5774, loss 0.0762028, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:14.889054: step 5775, loss 0.0360949, acc 1, learning_rate 0.0001
2017-10-10T12:08:15.228928: step 5776, loss 0.0876475, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:15.452627: step 5777, loss 0.0441906, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:15.627423: step 5778, loss 0.11015, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:15.796699: step 5779, loss 0.130056, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:15.965693: step 5780, loss 0.0772332, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:16.127146: step 5781, loss 0.0827556, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:16.353457: step 5782, loss 0.0279513, acc 1, learning_rate 0.0001
2017-10-10T12:08:16.600900: step 5783, loss 0.0532515, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:16.828782: step 5784, loss 0.10197, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:17.057664: step 5785, loss 0.0851769, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:17.242431: step 5786, loss 0.129775, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:17.508875: step 5787, loss 0.0942223, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:17.710416: step 5788, loss 0.0963346, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:17.907388: step 5789, loss 0.118782, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:18.095284: step 5790, loss 0.0567251, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:18.283561: step 5791, loss 0.0871117, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:18.475975: step 5792, loss 0.0316361, acc 1, learning_rate 0.0001
2017-10-10T12:08:18.662948: step 5793, loss 0.0423154, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:18.867496: step 5794, loss 0.151721, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:19.090611: step 5795, loss 0.0919926, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:19.306021: step 5796, loss 0.0493017, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:19.540739: step 5797, loss 0.0492827, acc 1, learning_rate 0.0001
2017-10-10T12:08:19.762286: step 5798, loss 0.0744548, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:19.986376: step 5799, loss 0.0951798, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:20.257136: step 5800, loss 0.0643368, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:20.782476: step 5800, loss 0.211246, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5800

2017-10-10T12:08:22.025000: step 5801, loss 0.0593835, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:22.261772: step 5802, loss 0.189502, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:22.448853: step 5803, loss 0.127638, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:22.675864: step 5804, loss 0.0522039, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:22.908886: step 5805, loss 0.0354467, acc 1, learning_rate 0.0001
2017-10-10T12:08:23.176041: step 5806, loss 0.0898818, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:23.408861: step 5807, loss 0.108752, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:23.661323: step 5808, loss 0.105082, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:23.904155: step 5809, loss 0.102508, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:24.175175: step 5810, loss 0.0542733, acc 1, learning_rate 0.0001
2017-10-10T12:08:24.425355: step 5811, loss 0.0807031, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:24.665066: step 5812, loss 0.0579306, acc 1, learning_rate 0.0001
2017-10-10T12:08:24.867908: step 5813, loss 0.0667676, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:25.085011: step 5814, loss 0.0504373, acc 1, learning_rate 0.0001
2017-10-10T12:08:25.305376: step 5815, loss 0.0850406, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:25.534208: step 5816, loss 0.0422403, acc 1, learning_rate 0.0001
2017-10-10T12:08:25.771987: step 5817, loss 0.039173, acc 1, learning_rate 0.0001
2017-10-10T12:08:26.069138: step 5818, loss 0.0786423, acc 1, learning_rate 0.0001
2017-10-10T12:08:26.309306: step 5819, loss 0.0896484, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:26.446431: step 5820, loss 0.100131, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:26.591480: step 5821, loss 0.0409943, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:26.726109: step 5822, loss 0.0686572, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:26.855713: step 5823, loss 0.0375033, acc 1, learning_rate 0.0001
2017-10-10T12:08:26.973658: step 5824, loss 0.0455591, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:27.142642: step 5825, loss 0.0879479, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:27.386881: step 5826, loss 0.0457358, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:27.653006: step 5827, loss 0.0234002, acc 1, learning_rate 0.0001
2017-10-10T12:08:27.887744: step 5828, loss 0.155883, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:28.130374: step 5829, loss 0.0869028, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:28.375757: step 5830, loss 0.051957, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:28.620406: step 5831, loss 0.0509612, acc 1, learning_rate 0.0001
2017-10-10T12:08:28.911091: step 5832, loss 0.0899951, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:29.160599: step 5833, loss 0.110834, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:29.392413: step 5834, loss 0.0892933, acc 1, learning_rate 0.0001
2017-10-10T12:08:29.616319: step 5835, loss 0.0653535, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:29.788375: step 5836, loss 0.107249, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:30.002075: step 5837, loss 0.0720929, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:30.235455: step 5838, loss 0.0681086, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:30.474306: step 5839, loss 0.0446042, acc 1, learning_rate 0.0001
2017-10-10T12:08:30.717172: step 5840, loss 0.0559565, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:31.276841: step 5840, loss 0.209493, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5840

2017-10-10T12:08:32.277193: step 5841, loss 0.0681687, acc 1, learning_rate 0.0001
2017-10-10T12:08:32.511747: step 5842, loss 0.0256968, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:32.754121: step 5843, loss 0.0746694, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:32.974845: step 5844, loss 0.118894, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:33.231415: step 5845, loss 0.0741494, acc 1, learning_rate 0.0001
2017-10-10T12:08:33.480545: step 5846, loss 0.0440517, acc 1, learning_rate 0.0001
2017-10-10T12:08:33.719758: step 5847, loss 0.0556319, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:33.936003: step 5848, loss 0.0593323, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:34.167959: step 5849, loss 0.09766, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:34.503747: step 5850, loss 0.0475196, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:34.717133: step 5851, loss 0.115136, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:34.934592: step 5852, loss 0.0712162, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:35.133101: step 5853, loss 0.087407, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:35.323939: step 5854, loss 0.12374, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:35.529978: step 5855, loss 0.0921179, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:35.789607: step 5856, loss 0.0374621, acc 1, learning_rate 0.0001
2017-10-10T12:08:36.041817: step 5857, loss 0.131729, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:36.275376: step 5858, loss 0.195426, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:36.524964: step 5859, loss 0.0412745, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:36.745438: step 5860, loss 0.11307, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:37.020948: step 5861, loss 0.138159, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:37.311056: step 5862, loss 0.0727762, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:37.496146: step 5863, loss 0.0715523, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:37.673239: step 5864, loss 0.0730881, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:37.844598: step 5865, loss 0.0679464, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:38.008727: step 5866, loss 0.0859248, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:38.164982: step 5867, loss 0.0578328, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:38.334135: step 5868, loss 0.0474307, acc 1, learning_rate 0.0001
2017-10-10T12:08:38.582824: step 5869, loss 0.0235345, acc 1, learning_rate 0.0001
2017-10-10T12:08:38.821835: step 5870, loss 0.0412875, acc 1, learning_rate 0.0001
2017-10-10T12:08:39.040874: step 5871, loss 0.0932972, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:39.289031: step 5872, loss 0.134394, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:39.509252: step 5873, loss 0.101012, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:39.769151: step 5874, loss 0.0786258, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:40.001013: step 5875, loss 0.0926823, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:40.229641: step 5876, loss 0.0939375, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:40.480832: step 5877, loss 0.0168924, acc 1, learning_rate 0.0001
2017-10-10T12:08:40.700797: step 5878, loss 0.0249133, acc 1, learning_rate 0.0001
2017-10-10T12:08:40.914336: step 5879, loss 0.107472, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:41.092979: step 5880, loss 0.137212, acc 0.960784, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:41.672234: step 5880, loss 0.210334, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5880

2017-10-10T12:08:42.818123: step 5881, loss 0.0542524, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:43.012374: step 5882, loss 0.0331502, acc 1, learning_rate 0.0001
2017-10-10T12:08:43.180856: step 5883, loss 0.104045, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:43.361470: step 5884, loss 0.100885, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:43.515580: step 5885, loss 0.0692571, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:43.692178: step 5886, loss 0.0454208, acc 1, learning_rate 0.0001
2017-10-10T12:08:43.942747: step 5887, loss 0.152527, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:44.189244: step 5888, loss 0.0863071, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:44.428931: step 5889, loss 0.0414622, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:44.652355: step 5890, loss 0.0448236, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:44.876412: step 5891, loss 0.0351545, acc 1, learning_rate 0.0001
2017-10-10T12:08:45.073453: step 5892, loss 0.051813, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:45.292577: step 5893, loss 0.0462984, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:45.492441: step 5894, loss 0.0609204, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:45.711183: step 5895, loss 0.0316566, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:45.932846: step 5896, loss 0.100569, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:46.205811: step 5897, loss 0.0765571, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:46.439565: step 5898, loss 0.0740835, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:46.680325: step 5899, loss 0.0943923, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:46.927045: step 5900, loss 0.0814669, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:47.153089: step 5901, loss 0.0460629, acc 1, learning_rate 0.0001
2017-10-10T12:08:47.426174: step 5902, loss 0.0377606, acc 1, learning_rate 0.0001
2017-10-10T12:08:47.694690: step 5903, loss 0.128257, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:47.924712: step 5904, loss 0.0715377, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:48.144917: step 5905, loss 0.103551, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:48.449066: step 5906, loss 0.048127, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:48.661450: step 5907, loss 0.0513232, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:48.822225: step 5908, loss 0.0702806, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:48.996568: step 5909, loss 0.0582115, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:49.180845: step 5910, loss 0.0586629, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:49.348362: step 5911, loss 0.111634, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:49.595703: step 5912, loss 0.135826, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:49.825047: step 5913, loss 0.0509054, acc 1, learning_rate 0.0001
2017-10-10T12:08:50.030386: step 5914, loss 0.0469632, acc 1, learning_rate 0.0001
2017-10-10T12:08:50.226159: step 5915, loss 0.0613861, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:50.460823: step 5916, loss 0.0476789, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:50.724474: step 5917, loss 0.0593376, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:50.980997: step 5918, loss 0.0656031, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:51.235342: step 5919, loss 0.0530917, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:51.447729: step 5920, loss 0.0594341, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:51.945441: step 5920, loss 0.211081, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5920

2017-10-10T12:08:53.234663: step 5921, loss 0.109317, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:53.390224: step 5922, loss 0.0891138, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:53.494540: step 5923, loss 0.0583785, acc 1, learning_rate 0.0001
2017-10-10T12:08:53.599618: step 5924, loss 0.0328012, acc 1, learning_rate 0.0001
2017-10-10T12:08:53.705964: step 5925, loss 0.129237, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:53.939193: step 5926, loss 0.063544, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:54.166056: step 5927, loss 0.0755316, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:54.394739: step 5928, loss 0.0791779, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:54.635696: step 5929, loss 0.0428972, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:54.866558: step 5930, loss 0.135427, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:55.114820: step 5931, loss 0.0624794, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:55.360955: step 5932, loss 0.149522, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:55.563870: step 5933, loss 0.117019, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:55.808818: step 5934, loss 0.0810342, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:56.093026: step 5935, loss 0.106438, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:56.329830: step 5936, loss 0.117223, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:56.575642: step 5937, loss 0.110631, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:56.810219: step 5938, loss 0.100711, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:57.063212: step 5939, loss 0.126707, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:57.254811: step 5940, loss 0.0786306, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:57.444950: step 5941, loss 0.0924167, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:57.678410: step 5942, loss 0.0895327, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:57.925145: step 5943, loss 0.0767604, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:58.145125: step 5944, loss 0.119889, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:58.340841: step 5945, loss 0.126837, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:58.601853: step 5946, loss 0.0304948, acc 1, learning_rate 0.0001
2017-10-10T12:08:58.845349: step 5947, loss 0.0470789, acc 1, learning_rate 0.0001
2017-10-10T12:08:59.107566: step 5948, loss 0.0826076, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:59.387524: step 5949, loss 0.0982205, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:59.622151: step 5950, loss 0.103123, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:59.888624: step 5951, loss 0.0497174, acc 1, learning_rate 0.0001
2017-10-10T12:09:00.059706: step 5952, loss 0.0901614, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:00.179148: step 5953, loss 0.0785602, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:00.312940: step 5954, loss 0.0909259, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:00.432329: step 5955, loss 0.0351624, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:00.555221: step 5956, loss 0.0388872, acc 1, learning_rate 0.0001
2017-10-10T12:09:00.688846: step 5957, loss 0.0836166, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:00.919915: step 5958, loss 0.0838036, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:01.149416: step 5959, loss 0.0573831, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:01.420481: step 5960, loss 0.0109859, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:01.960811: step 5960, loss 0.208782, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-5960

2017-10-10T12:09:02.969036: step 5961, loss 0.0536431, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:03.232870: step 5962, loss 0.0691496, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:03.460824: step 5963, loss 0.127879, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:03.702764: step 5964, loss 0.132146, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:03.935539: step 5965, loss 0.239824, acc 0.921875, learning_rate 0.0001
2017-10-10T12:09:04.201814: step 5966, loss 0.0555785, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:04.428871: step 5967, loss 0.107335, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:04.672868: step 5968, loss 0.0805029, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:04.901257: step 5969, loss 0.0424784, acc 1, learning_rate 0.0001
2017-10-10T12:09:05.148774: step 5970, loss 0.0815365, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:05.381763: step 5971, loss 0.0195317, acc 1, learning_rate 0.0001
2017-10-10T12:09:05.661814: step 5972, loss 0.0918674, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:05.898110: step 5973, loss 0.0487691, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:06.117847: step 5974, loss 0.0565512, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:06.360928: step 5975, loss 0.053122, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:06.622113: step 5976, loss 0.201536, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:06.860857: step 5977, loss 0.0800071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:07.085536: step 5978, loss 0.0404017, acc 1, learning_rate 0.0001
2017-10-10T12:09:07.334417: step 5979, loss 0.11245, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:07.557168: step 5980, loss 0.0662468, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:07.768829: step 5981, loss 0.129748, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:07.989368: step 5982, loss 0.0391298, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:08.284345: step 5983, loss 0.0716935, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:08.443498: step 5984, loss 0.0992671, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:08.644753: step 5985, loss 0.0744006, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:08.831367: step 5986, loss 0.074359, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:09.001427: step 5987, loss 0.113951, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:09.176804: step 5988, loss 0.0605728, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:09.472230: step 5989, loss 0.154819, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:09.707662: step 5990, loss 0.103806, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:09.939030: step 5991, loss 0.053619, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:10.183727: step 5992, loss 0.0334683, acc 1, learning_rate 0.0001
2017-10-10T12:09:10.424854: step 5993, loss 0.0766554, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:10.705339: step 5994, loss 0.0464079, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:10.939582: step 5995, loss 0.167905, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:11.081732: step 5996, loss 0.150404, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:11.267921: step 5997, loss 0.064729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:11.448923: step 5998, loss 0.0222753, acc 1, learning_rate 0.0001
2017-10-10T12:09:11.620396: step 5999, loss 0.0355713, acc 1, learning_rate 0.0001
2017-10-10T12:09:11.799756: step 6000, loss 0.0408304, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:12.366797: step 6000, loss 0.211051, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6000

2017-10-10T12:09:13.505082: step 6001, loss 0.0681965, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:13.729267: step 6002, loss 0.0604811, acc 1, learning_rate 0.0001
2017-10-10T12:09:13.922637: step 6003, loss 0.0634144, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:14.157413: step 6004, loss 0.145999, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:14.395945: step 6005, loss 0.0719127, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:14.592956: step 6006, loss 0.0191533, acc 1, learning_rate 0.0001
2017-10-10T12:09:14.808841: step 6007, loss 0.0929081, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:15.055898: step 6008, loss 0.110402, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:15.334817: step 6009, loss 0.08451, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:15.584497: step 6010, loss 0.136713, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:15.852872: step 6011, loss 0.0984104, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:16.098273: step 6012, loss 0.0713547, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:16.328869: step 6013, loss 0.100019, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:16.581892: step 6014, loss 0.0330916, acc 1, learning_rate 0.0001
2017-10-10T12:09:16.764612: step 6015, loss 0.0796429, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:16.943206: step 6016, loss 0.10479, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:17.135340: step 6017, loss 0.120983, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:17.316862: step 6018, loss 0.0268666, acc 1, learning_rate 0.0001
2017-10-10T12:09:17.456441: step 6019, loss 0.0763592, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:17.664840: step 6020, loss 0.0615466, acc 1, learning_rate 0.0001
2017-10-10T12:09:17.856801: step 6021, loss 0.0903054, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:18.083675: step 6022, loss 0.0620108, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:18.324429: step 6023, loss 0.118729, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:18.559764: step 6024, loss 0.0210887, acc 1, learning_rate 0.0001
2017-10-10T12:09:18.776888: step 6025, loss 0.0964396, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:19.008983: step 6026, loss 0.0688351, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:19.204835: step 6027, loss 0.0896782, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:19.452240: step 6028, loss 0.125731, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:19.720859: step 6029, loss 0.0492819, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:19.938261: step 6030, loss 0.0329157, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:20.157431: step 6031, loss 0.118481, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:20.404969: step 6032, loss 0.0664806, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:20.638677: step 6033, loss 0.0700145, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:20.824852: step 6034, loss 0.103339, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:21.068843: step 6035, loss 0.0559523, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:21.376868: step 6036, loss 0.0906463, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:21.642935: step 6037, loss 0.0289514, acc 1, learning_rate 0.0001
2017-10-10T12:09:21.815185: step 6038, loss 0.0820996, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:21.989132: step 6039, loss 0.116202, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:22.176783: step 6040, loss 0.0766876, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:22.723967: step 6040, loss 0.207901, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6040

2017-10-10T12:09:24.035970: step 6041, loss 0.0749274, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:24.283898: step 6042, loss 0.0625217, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:24.540835: step 6043, loss 0.124482, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:24.824764: step 6044, loss 0.107791, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:25.105433: step 6045, loss 0.0185179, acc 1, learning_rate 0.0001
2017-10-10T12:09:25.354511: step 6046, loss 0.0447475, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:25.544127: step 6047, loss 0.0998097, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:25.716828: step 6048, loss 0.0990849, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:25.890715: step 6049, loss 0.089262, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:26.124996: step 6050, loss 0.146324, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:26.423133: step 6051, loss 0.0402831, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:26.611010: step 6052, loss 0.0736616, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:26.836308: step 6053, loss 0.159521, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:27.063536: step 6054, loss 0.102777, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:27.263574: step 6055, loss 0.0428748, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:27.500863: step 6056, loss 0.0551981, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:27.740848: step 6057, loss 0.131257, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:27.974042: step 6058, loss 0.0755942, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:28.180767: step 6059, loss 0.0414332, acc 1, learning_rate 0.0001
2017-10-10T12:09:28.424894: step 6060, loss 0.0665859, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:28.654430: step 6061, loss 0.0998204, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:28.894240: step 6062, loss 0.0912383, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:29.100866: step 6063, loss 0.0417389, acc 1, learning_rate 0.0001
2017-10-10T12:09:29.310856: step 6064, loss 0.116092, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:29.519620: step 6065, loss 0.0567802, acc 1, learning_rate 0.0001
2017-10-10T12:09:29.744992: step 6066, loss 0.0725567, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:29.966438: step 6067, loss 0.113914, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:30.219880: step 6068, loss 0.102024, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:30.449775: step 6069, loss 0.028616, acc 1, learning_rate 0.0001
2017-10-10T12:09:30.675646: step 6070, loss 0.117016, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:30.878665: step 6071, loss 0.0961454, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:31.108862: step 6072, loss 0.108423, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:31.331845: step 6073, loss 0.079623, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:31.568927: step 6074, loss 0.0995861, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:31.777178: step 6075, loss 0.0889201, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:31.951881: step 6076, loss 0.0668124, acc 1, learning_rate 0.0001
2017-10-10T12:09:32.264718: step 6077, loss 0.10041, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:32.547616: step 6078, loss 0.0381172, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:32.734950: step 6079, loss 0.0724702, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:32.900841: step 6080, loss 0.0235243, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:33.416822: step 6080, loss 0.208211, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6080

2017-10-10T12:09:34.356916: step 6081, loss 0.0761448, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:34.581032: step 6082, loss 0.122047, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:34.801169: step 6083, loss 0.125254, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:35.010870: step 6084, loss 0.0849948, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:35.253072: step 6085, loss 0.113201, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:35.452908: step 6086, loss 0.0342048, acc 1, learning_rate 0.0001
2017-10-10T12:09:35.724865: step 6087, loss 0.0887714, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:35.969994: step 6088, loss 0.0302444, acc 1, learning_rate 0.0001
2017-10-10T12:09:36.216888: step 6089, loss 0.0243815, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:36.435055: step 6090, loss 0.0670314, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:36.644188: step 6091, loss 0.104312, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:36.830119: step 6092, loss 0.0727655, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:37.051110: step 6093, loss 0.102695, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:37.298704: step 6094, loss 0.12252, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:37.568924: step 6095, loss 0.0933355, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:37.822111: step 6096, loss 0.134402, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:38.053712: step 6097, loss 0.16145, acc 0.90625, learning_rate 0.0001
2017-10-10T12:09:38.292858: step 6098, loss 0.134691, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:38.506900: step 6099, loss 0.151684, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:38.748081: step 6100, loss 0.0325033, acc 1, learning_rate 0.0001
2017-10-10T12:09:39.008127: step 6101, loss 0.0601613, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.246096: step 6102, loss 0.163557, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.460031: step 6103, loss 0.0995804, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.697114: step 6104, loss 0.0550676, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.935565: step 6105, loss 0.0518318, acc 1, learning_rate 0.0001
2017-10-10T12:09:40.177052: step 6106, loss 0.066449, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:40.413641: step 6107, loss 0.0969529, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:40.659003: step 6108, loss 0.0554094, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:40.920893: step 6109, loss 0.0975588, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:41.169895: step 6110, loss 0.0624565, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:41.403259: step 6111, loss 0.0684935, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:41.663462: step 6112, loss 0.0946788, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:41.939747: step 6113, loss 0.0418833, acc 1, learning_rate 0.0001
2017-10-10T12:09:42.102548: step 6114, loss 0.0438813, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:42.253011: step 6115, loss 0.0908275, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:42.413356: step 6116, loss 0.111089, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:42.597562: step 6117, loss 0.0814494, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:42.848820: step 6118, loss 0.13372, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:43.132859: step 6119, loss 0.083728, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:43.309571: step 6120, loss 0.0568602, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:43.793899: step 6120, loss 0.207766, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6120

2017-10-10T12:09:44.808725: step 6121, loss 0.101609, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:45.028323: step 6122, loss 0.0733833, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:45.241192: step 6123, loss 0.135734, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:45.500009: step 6124, loss 0.0442474, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:45.729994: step 6125, loss 0.0817768, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:45.967438: step 6126, loss 0.033668, acc 1, learning_rate 0.0001
2017-10-10T12:09:46.252669: step 6127, loss 0.102542, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:46.485604: step 6128, loss 0.0435342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:46.740224: step 6129, loss 0.0412981, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:46.956988: step 6130, loss 0.0689376, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:47.187319: step 6131, loss 0.0690976, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:47.425233: step 6132, loss 0.0698267, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:47.665985: step 6133, loss 0.087012, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:47.912545: step 6134, loss 0.0834919, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:48.155934: step 6135, loss 0.0800615, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:48.400333: step 6136, loss 0.079021, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:48.635697: step 6137, loss 0.0948222, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:48.853137: step 6138, loss 0.086699, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:49.087645: step 6139, loss 0.0400793, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:49.302923: step 6140, loss 0.0580168, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:49.529073: step 6141, loss 0.0747707, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:49.749570: step 6142, loss 0.0725562, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:49.961243: step 6143, loss 0.0381638, acc 1, learning_rate 0.0001
2017-10-10T12:09:50.254071: step 6144, loss 0.136073, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:50.452824: step 6145, loss 0.0890911, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:50.652958: step 6146, loss 0.123595, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:50.851481: step 6147, loss 0.0569177, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:51.035046: step 6148, loss 0.0428532, acc 1, learning_rate 0.0001
2017-10-10T12:09:51.285016: step 6149, loss 0.0393769, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:51.471221: step 6150, loss 0.100053, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:51.697612: step 6151, loss 0.0347918, acc 1, learning_rate 0.0001
2017-10-10T12:09:51.933739: step 6152, loss 0.0331048, acc 1, learning_rate 0.0001
2017-10-10T12:09:52.139288: step 6153, loss 0.0688048, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:52.375209: step 6154, loss 0.0979497, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:52.643468: step 6155, loss 0.0600268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:52.891616: step 6156, loss 0.111344, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:53.124037: step 6157, loss 0.0252299, acc 1, learning_rate 0.0001
2017-10-10T12:09:53.351328: step 6158, loss 0.0403155, acc 1, learning_rate 0.0001
2017-10-10T12:09:53.578331: step 6159, loss 0.0774325, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:53.835352: step 6160, loss 0.0713616, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:54.500678: step 6160, loss 0.207095, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6160

2017-10-10T12:09:55.541520: step 6161, loss 0.0680645, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:55.766744: step 6162, loss 0.0938827, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:55.997447: step 6163, loss 0.0553686, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:56.273739: step 6164, loss 0.0606727, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:56.500200: step 6165, loss 0.124937, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:56.761320: step 6166, loss 0.0672799, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:57.010440: step 6167, loss 0.0378015, acc 1, learning_rate 0.0001
2017-10-10T12:09:57.270342: step 6168, loss 0.034087, acc 1, learning_rate 0.0001
2017-10-10T12:09:57.485406: step 6169, loss 0.101406, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:57.718779: step 6170, loss 0.108995, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:58.049382: step 6171, loss 0.0812111, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:58.252992: step 6172, loss 0.0770831, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:58.432809: step 6173, loss 0.101974, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:58.620610: step 6174, loss 0.100124, acc 0.941176, learning_rate 0.0001
2017-10-10T12:09:58.817025: step 6175, loss 0.0755979, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:58.980269: step 6176, loss 0.0591195, acc 1, learning_rate 0.0001
2017-10-10T12:09:59.123813: step 6177, loss 0.0330587, acc 1, learning_rate 0.0001
2017-10-10T12:09:59.364879: step 6178, loss 0.110244, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:59.573745: step 6179, loss 0.104342, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:59.809150: step 6180, loss 0.0863701, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:00.015078: step 6181, loss 0.105043, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:00.238584: step 6182, loss 0.0662621, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:00.425033: step 6183, loss 0.114745, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:00.657375: step 6184, loss 0.119818, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:00.874009: step 6185, loss 0.102091, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:01.097471: step 6186, loss 0.0410012, acc 1, learning_rate 0.0001
2017-10-10T12:10:01.361075: step 6187, loss 0.143601, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:01.598883: step 6188, loss 0.13125, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:01.824816: step 6189, loss 0.0342615, acc 1, learning_rate 0.0001
2017-10-10T12:10:01.989484: step 6190, loss 0.112186, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:02.213182: step 6191, loss 0.0300418, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:02.424940: step 6192, loss 0.0750065, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:02.656933: step 6193, loss 0.0946293, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:02.905040: step 6194, loss 0.0720206, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:03.149938: step 6195, loss 0.0417241, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:03.396508: step 6196, loss 0.0531646, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:03.630068: step 6197, loss 0.0498401, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:03.883485: step 6198, loss 0.0759338, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:04.123846: step 6199, loss 0.0249803, acc 1, learning_rate 0.0001
2017-10-10T12:10:04.368664: step 6200, loss 0.0432677, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:05.001603: step 6200, loss 0.207269, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6200

2017-10-10T12:10:06.448842: step 6201, loss 0.110397, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:06.633121: step 6202, loss 0.0443045, acc 1, learning_rate 0.0001
2017-10-10T12:10:06.826395: step 6203, loss 0.0865713, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:07.019709: step 6204, loss 0.116467, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:07.212448: step 6205, loss 0.0439267, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:07.412978: step 6206, loss 0.0360144, acc 1, learning_rate 0.0001
2017-10-10T12:10:07.681681: step 6207, loss 0.156532, acc 0.90625, learning_rate 0.0001
2017-10-10T12:10:07.937202: step 6208, loss 0.093039, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:08.164435: step 6209, loss 0.116183, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:08.394918: step 6210, loss 0.043376, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:08.648876: step 6211, loss 0.0384, acc 1, learning_rate 0.0001
2017-10-10T12:10:08.882642: step 6212, loss 0.0290952, acc 1, learning_rate 0.0001
2017-10-10T12:10:09.117795: step 6213, loss 0.0861177, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:09.335601: step 6214, loss 0.142244, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:09.568599: step 6215, loss 0.0532216, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:09.760849: step 6216, loss 0.0875652, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:09.940820: step 6217, loss 0.138358, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:10.168667: step 6218, loss 0.0292161, acc 1, learning_rate 0.0001
2017-10-10T12:10:10.380848: step 6219, loss 0.0890399, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:10.612101: step 6220, loss 0.138401, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:10.846771: step 6221, loss 0.08183, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:11.094836: step 6222, loss 0.0247715, acc 1, learning_rate 0.0001
2017-10-10T12:10:11.317475: step 6223, loss 0.052944, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:11.562879: step 6224, loss 0.115341, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:11.792818: step 6225, loss 0.09727, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:12.026865: step 6226, loss 0.0242648, acc 1, learning_rate 0.0001
2017-10-10T12:10:12.265204: step 6227, loss 0.110636, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:12.482210: step 6228, loss 0.0888931, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:12.704802: step 6229, loss 0.162821, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:12.922796: step 6230, loss 0.0292119, acc 1, learning_rate 0.0001
2017-10-10T12:10:13.129082: step 6231, loss 0.154601, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:13.379144: step 6232, loss 0.0789866, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:13.588957: step 6233, loss 0.124409, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:13.800902: step 6234, loss 0.0482975, acc 1, learning_rate 0.0001
2017-10-10T12:10:14.055577: step 6235, loss 0.039846, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:14.248879: step 6236, loss 0.058787, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:14.503185: step 6237, loss 0.0269537, acc 1, learning_rate 0.0001
2017-10-10T12:10:14.775331: step 6238, loss 0.097324, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:15.020560: step 6239, loss 0.0433878, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:15.226348: step 6240, loss 0.0832451, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:15.745818: step 6240, loss 0.20788, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6240

2017-10-10T12:10:16.727968: step 6241, loss 0.0588947, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:16.893535: step 6242, loss 0.0563139, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:17.064042: step 6243, loss 0.0823483, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:17.237010: step 6244, loss 0.0670829, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:17.465196: step 6245, loss 0.0982555, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:17.712040: step 6246, loss 0.0282431, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:17.954335: step 6247, loss 0.0442841, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:18.186624: step 6248, loss 0.121166, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:18.410773: step 6249, loss 0.0489502, acc 1, learning_rate 0.0001
2017-10-10T12:10:18.655250: step 6250, loss 0.0779435, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:18.906808: step 6251, loss 0.0363011, acc 1, learning_rate 0.0001
2017-10-10T12:10:19.142450: step 6252, loss 0.114307, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:19.360257: step 6253, loss 0.17493, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:19.600629: step 6254, loss 0.0987075, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:19.827155: step 6255, loss 0.158336, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:20.048811: step 6256, loss 0.142328, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:20.264887: step 6257, loss 0.0653463, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:20.499972: step 6258, loss 0.0382542, acc 1, learning_rate 0.0001
2017-10-10T12:10:20.704865: step 6259, loss 0.0574181, acc 1, learning_rate 0.0001
2017-10-10T12:10:20.888989: step 6260, loss 0.0435987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:21.097607: step 6261, loss 0.139366, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:21.341768: step 6262, loss 0.194, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:21.569286: step 6263, loss 0.0428772, acc 1, learning_rate 0.0001
2017-10-10T12:10:21.793922: step 6264, loss 0.0997677, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:22.049124: step 6265, loss 0.079421, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:22.296837: step 6266, loss 0.0397331, acc 1, learning_rate 0.0001
2017-10-10T12:10:22.524881: step 6267, loss 0.0455169, acc 1, learning_rate 0.0001
2017-10-10T12:10:22.741485: step 6268, loss 0.0744852, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:23.004072: step 6269, loss 0.132512, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:23.181787: step 6270, loss 0.116355, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:23.363959: step 6271, loss 0.0254858, acc 1, learning_rate 0.0001
2017-10-10T12:10:23.553201: step 6272, loss 0.0729051, acc 0.980392, learning_rate 0.0001
2017-10-10T12:10:23.737033: step 6273, loss 0.0543658, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:23.937313: step 6274, loss 0.0904515, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:24.176143: step 6275, loss 0.0707529, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:24.406852: step 6276, loss 0.113506, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:24.651102: step 6277, loss 0.0730186, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:24.866042: step 6278, loss 0.0707931, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:25.068557: step 6279, loss 0.0633828, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:25.272765: step 6280, loss 0.11648, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:25.835824: step 6280, loss 0.208797, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6280

2017-10-10T12:10:27.343070: step 6281, loss 0.082409, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:27.509484: step 6282, loss 0.0528461, acc 1, learning_rate 0.0001
2017-10-10T12:10:27.689085: step 6283, loss 0.10116, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:27.864832: step 6284, loss 0.046258, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:28.034472: step 6285, loss 0.103825, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:28.295502: step 6286, loss 0.0909024, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:28.511764: step 6287, loss 0.115542, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:28.696871: step 6288, loss 0.0976504, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:28.941810: step 6289, loss 0.12905, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:29.192893: step 6290, loss 0.0634235, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:29.405188: step 6291, loss 0.0995228, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:29.616921: step 6292, loss 0.0251915, acc 1, learning_rate 0.0001
2017-10-10T12:10:29.834664: step 6293, loss 0.038779, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:30.068874: step 6294, loss 0.190457, acc 0.90625, learning_rate 0.0001
2017-10-10T12:10:30.275967: step 6295, loss 0.0479355, acc 1, learning_rate 0.0001
2017-10-10T12:10:30.524690: step 6296, loss 0.105153, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:30.768834: step 6297, loss 0.15187, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:31.069000: step 6298, loss 0.0493974, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:31.335030: step 6299, loss 0.0701889, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:31.540628: step 6300, loss 0.0678065, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:31.740097: step 6301, loss 0.0693332, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:31.932924: step 6302, loss 0.0245093, acc 1, learning_rate 0.0001
2017-10-10T12:10:32.096888: step 6303, loss 0.0463273, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:32.268825: step 6304, loss 0.0587078, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:32.431996: step 6305, loss 0.0368435, acc 1, learning_rate 0.0001
2017-10-10T12:10:32.620979: step 6306, loss 0.0756213, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:32.874537: step 6307, loss 0.0391396, acc 1, learning_rate 0.0001
2017-10-10T12:10:33.105934: step 6308, loss 0.0741874, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:33.315571: step 6309, loss 0.0864911, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:33.542985: step 6310, loss 0.0499014, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:33.787583: step 6311, loss 0.163173, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:34.035874: step 6312, loss 0.122426, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:34.267269: step 6313, loss 0.0220697, acc 1, learning_rate 0.0001
2017-10-10T12:10:34.496204: step 6314, loss 0.0641842, acc 1, learning_rate 0.0001
2017-10-10T12:10:34.742378: step 6315, loss 0.120912, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:34.968267: step 6316, loss 0.0571389, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:35.203115: step 6317, loss 0.0619361, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:35.416444: step 6318, loss 0.0721663, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:35.636930: step 6319, loss 0.0168572, acc 1, learning_rate 0.0001
2017-10-10T12:10:35.860065: step 6320, loss 0.036035, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:36.448263: step 6320, loss 0.206229, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6320

2017-10-10T12:10:37.735270: step 6321, loss 0.0971699, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:38.032915: step 6322, loss 0.0574305, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:38.220834: step 6323, loss 0.132054, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:38.397554: step 6324, loss 0.141664, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:38.568804: step 6325, loss 0.0211581, acc 1, learning_rate 0.0001
2017-10-10T12:10:38.736901: step 6326, loss 0.0527832, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:38.897150: step 6327, loss 0.167083, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:39.072745: step 6328, loss 0.0296584, acc 1, learning_rate 0.0001
2017-10-10T12:10:39.323300: step 6329, loss 0.0368852, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:39.648034: step 6330, loss 0.0483891, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:39.854327: step 6331, loss 0.108837, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:40.052392: step 6332, loss 0.0915748, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:40.253248: step 6333, loss 0.0820468, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:40.448230: step 6334, loss 0.100565, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:40.631871: step 6335, loss 0.0660158, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:40.837907: step 6336, loss 0.0637989, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:41.058771: step 6337, loss 0.0601315, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:41.280866: step 6338, loss 0.0173592, acc 1, learning_rate 0.0001
2017-10-10T12:10:41.495714: step 6339, loss 0.10328, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:41.752874: step 6340, loss 0.0763779, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:42.002355: step 6341, loss 0.0440365, acc 1, learning_rate 0.0001
2017-10-10T12:10:42.216216: step 6342, loss 0.0723575, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:42.492966: step 6343, loss 0.0893355, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:42.720298: step 6344, loss 0.107177, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:42.952161: step 6345, loss 0.0728581, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:43.154711: step 6346, loss 0.0681106, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:43.382297: step 6347, loss 0.0740173, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:43.593026: step 6348, loss 0.0804668, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:43.813059: step 6349, loss 0.126367, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:44.039739: step 6350, loss 0.0673623, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:44.256074: step 6351, loss 0.0981856, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:44.508844: step 6352, loss 0.0928417, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:44.772902: step 6353, loss 0.0947665, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:44.999389: step 6354, loss 0.111778, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:45.184469: step 6355, loss 0.0767797, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:45.445088: step 6356, loss 0.150758, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:45.655930: step 6357, loss 0.0531603, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:45.871304: step 6358, loss 0.124224, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:46.106750: step 6359, loss 0.0662711, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:46.369476: step 6360, loss 0.10925, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:47.007805: step 6360, loss 0.208744, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6360

2017-10-10T12:10:48.092864: step 6361, loss 0.178882, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:48.298249: step 6362, loss 0.128656, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:48.514467: step 6363, loss 0.136054, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:48.704846: step 6364, loss 0.0640814, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:48.941397: step 6365, loss 0.0851095, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:49.161347: step 6366, loss 0.028561, acc 1, learning_rate 0.0001
2017-10-10T12:10:49.319663: step 6367, loss 0.0421057, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:49.508794: step 6368, loss 0.0293938, acc 1, learning_rate 0.0001
2017-10-10T12:10:49.670099: step 6369, loss 0.0828184, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:49.810480: step 6370, loss 0.0186899, acc 1, learning_rate 0.0001
2017-10-10T12:10:49.979432: step 6371, loss 0.123323, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:50.185081: step 6372, loss 0.0389636, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:50.392136: step 6373, loss 0.0164584, acc 1, learning_rate 0.0001
2017-10-10T12:10:50.600723: step 6374, loss 0.0757676, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:50.847506: step 6375, loss 0.100797, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:51.070697: step 6376, loss 0.122536, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:51.280728: step 6377, loss 0.0634416, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:51.500240: step 6378, loss 0.0414306, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:51.659734: step 6379, loss 0.143147, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:51.880618: step 6380, loss 0.0569683, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:52.121104: step 6381, loss 0.0797152, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:52.367143: step 6382, loss 0.064399, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:52.576982: step 6383, loss 0.0689304, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:52.832807: step 6384, loss 0.0302591, acc 1, learning_rate 0.0001
2017-10-10T12:10:53.089398: step 6385, loss 0.165064, acc 0.90625, learning_rate 0.0001
2017-10-10T12:10:53.334803: step 6386, loss 0.0571088, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:53.605297: step 6387, loss 0.0767791, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:53.842523: step 6388, loss 0.0469847, acc 1, learning_rate 0.0001
2017-10-10T12:10:54.083761: step 6389, loss 0.112775, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:54.303235: step 6390, loss 0.106045, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:54.532821: step 6391, loss 0.0335291, acc 1, learning_rate 0.0001
2017-10-10T12:10:54.774746: step 6392, loss 0.122181, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:55.024836: step 6393, loss 0.055767, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:55.266775: step 6394, loss 0.0537297, acc 1, learning_rate 0.0001
2017-10-10T12:10:55.510277: step 6395, loss 0.068236, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:55.758692: step 6396, loss 0.208559, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:55.990324: step 6397, loss 0.0957715, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:56.244861: step 6398, loss 0.0986483, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:56.557201: step 6399, loss 0.0608441, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:56.731331: step 6400, loss 0.0838411, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:57.248380: step 6400, loss 0.209318, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6400

2017-10-10T12:10:58.287940: step 6401, loss 0.139189, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:58.488528: step 6402, loss 0.0491193, acc 1, learning_rate 0.0001
2017-10-10T12:10:58.761612: step 6403, loss 0.101493, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:58.991553: step 6404, loss 0.1231, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:59.216271: step 6405, loss 0.099116, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:59.462164: step 6406, loss 0.0516819, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:59.703271: step 6407, loss 0.134702, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:00.005072: step 6408, loss 0.10827, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:00.279522: step 6409, loss 0.0362592, acc 1, learning_rate 0.0001
2017-10-10T12:11:00.468020: step 6410, loss 0.155242, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:00.637498: step 6411, loss 0.0504426, acc 1, learning_rate 0.0001
2017-10-10T12:11:00.815157: step 6412, loss 0.0488435, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:00.977131: step 6413, loss 0.107866, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:01.153065: step 6414, loss 0.0262556, acc 1, learning_rate 0.0001
2017-10-10T12:11:01.323609: step 6415, loss 0.0524581, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:01.525052: step 6416, loss 0.103456, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:01.810492: step 6417, loss 0.157144, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:02.084833: step 6418, loss 0.106285, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:02.299468: step 6419, loss 0.0613351, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:02.512820: step 6420, loss 0.107888, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:02.733965: step 6421, loss 0.03294, acc 1, learning_rate 0.0001
2017-10-10T12:11:02.948979: step 6422, loss 0.0784089, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:03.168972: step 6423, loss 0.0861587, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:03.387784: step 6424, loss 0.0270591, acc 1, learning_rate 0.0001
2017-10-10T12:11:03.612543: step 6425, loss 0.0803507, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:03.812738: step 6426, loss 0.0588454, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:04.042919: step 6427, loss 0.0433909, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:04.263357: step 6428, loss 0.0396506, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:04.556924: step 6429, loss 0.0288933, acc 1, learning_rate 0.0001
2017-10-10T12:11:04.799299: step 6430, loss 0.0520726, acc 1, learning_rate 0.0001
2017-10-10T12:11:05.017341: step 6431, loss 0.110715, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:05.210298: step 6432, loss 0.0479182, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:05.393053: step 6433, loss 0.0326477, acc 1, learning_rate 0.0001
2017-10-10T12:11:05.571208: step 6434, loss 0.0464524, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:05.744833: step 6435, loss 0.0992765, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:05.975742: step 6436, loss 0.0590097, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.208731: step 6437, loss 0.0671308, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:06.438752: step 6438, loss 0.0544497, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.640843: step 6439, loss 0.0912342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.886002: step 6440, loss 0.0704866, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:07.524811: step 6440, loss 0.208315, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6440

2017-10-10T12:11:08.823726: step 6441, loss 0.0352722, acc 1, learning_rate 0.0001
2017-10-10T12:11:09.068005: step 6442, loss 0.105388, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:09.312509: step 6443, loss 0.0659724, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:09.540865: step 6444, loss 0.104244, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:09.742990: step 6445, loss 0.115268, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:09.949922: step 6446, loss 0.0315405, acc 1, learning_rate 0.0001
2017-10-10T12:11:10.185084: step 6447, loss 0.0719865, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:10.396847: step 6448, loss 0.0370397, acc 1, learning_rate 0.0001
2017-10-10T12:11:10.627391: step 6449, loss 0.0656096, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:10.903752: step 6450, loss 0.051324, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.163590: step 6451, loss 0.105813, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:11.393632: step 6452, loss 0.077562, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.569206: step 6453, loss 0.0540299, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.732860: step 6454, loss 0.0564711, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.937834: step 6455, loss 0.145648, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:12.124781: step 6456, loss 0.0391139, acc 1, learning_rate 0.0001
2017-10-10T12:11:12.400951: step 6457, loss 0.0401252, acc 1, learning_rate 0.0001
2017-10-10T12:11:12.600730: step 6458, loss 0.0342034, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:12.853018: step 6459, loss 0.117324, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:13.092168: step 6460, loss 0.0920549, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:13.302466: step 6461, loss 0.204851, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:13.477172: step 6462, loss 0.0637685, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:13.669913: step 6463, loss 0.057898, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:13.873490: step 6464, loss 0.0884977, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:14.094649: step 6465, loss 0.0674051, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:14.336179: step 6466, loss 0.0633239, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:14.547245: step 6467, loss 0.0629485, acc 1, learning_rate 0.0001
2017-10-10T12:11:14.719085: step 6468, loss 0.0956884, acc 0.980392, learning_rate 0.0001
2017-10-10T12:11:14.956693: step 6469, loss 0.132909, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:15.179153: step 6470, loss 0.0766127, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:15.429010: step 6471, loss 0.0453652, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:15.647996: step 6472, loss 0.0689255, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:15.861131: step 6473, loss 0.0976064, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:16.084323: step 6474, loss 0.0821739, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:16.337078: step 6475, loss 0.0716204, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:16.600509: step 6476, loss 0.105225, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:16.784908: step 6477, loss 0.0991995, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:17.020858: step 6478, loss 0.0476406, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:17.287090: step 6479, loss 0.0558151, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:17.524878: step 6480, loss 0.13122, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:18.173191: step 6480, loss 0.20914, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6480

2017-10-10T12:11:19.450262: step 6481, loss 0.0820148, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:19.667785: step 6482, loss 0.0717415, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:19.907813: step 6483, loss 0.108713, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:20.148937: step 6484, loss 0.0628834, acc 1, learning_rate 0.0001
2017-10-10T12:11:20.372848: step 6485, loss 0.08535, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:20.612816: step 6486, loss 0.0850766, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:20.845326: step 6487, loss 0.058069, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:21.078884: step 6488, loss 0.0824454, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:21.370677: step 6489, loss 0.0336853, acc 1, learning_rate 0.0001
2017-10-10T12:11:21.685313: step 6490, loss 0.0679753, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:21.866814: step 6491, loss 0.0681012, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:22.009308: step 6492, loss 0.0882621, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:22.127914: step 6493, loss 0.0420517, acc 1, learning_rate 0.0001
2017-10-10T12:11:22.255908: step 6494, loss 0.0462377, acc 1, learning_rate 0.0001
2017-10-10T12:11:22.382569: step 6495, loss 0.111123, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:22.551403: step 6496, loss 0.177054, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:22.788042: step 6497, loss 0.0548378, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:23.031770: step 6498, loss 0.0215939, acc 1, learning_rate 0.0001
2017-10-10T12:11:23.276605: step 6499, loss 0.0541814, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:23.484330: step 6500, loss 0.0516513, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:23.716833: step 6501, loss 0.0260921, acc 1, learning_rate 0.0001
2017-10-10T12:11:23.928895: step 6502, loss 0.0431694, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:24.129067: step 6503, loss 0.0403526, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:24.372958: step 6504, loss 0.0564926, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:24.566105: step 6505, loss 0.129518, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:24.743395: step 6506, loss 0.0353546, acc 1, learning_rate 0.0001
2017-10-10T12:11:24.994744: step 6507, loss 0.0435837, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:25.231804: step 6508, loss 0.044069, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:25.466814: step 6509, loss 0.0807614, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:25.673159: step 6510, loss 0.150284, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:25.869624: step 6511, loss 0.0922959, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:26.091185: step 6512, loss 0.0965165, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:26.337498: step 6513, loss 0.112953, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:26.551825: step 6514, loss 0.0724645, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:26.764213: step 6515, loss 0.0947463, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:26.992832: step 6516, loss 0.0499082, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:27.207140: step 6517, loss 0.11082, acc 0.921875, learning_rate 0.0001
2017-10-10T12:11:27.454533: step 6518, loss 0.0291385, acc 1, learning_rate 0.0001
2017-10-10T12:11:27.709682: step 6519, loss 0.0287478, acc 1, learning_rate 0.0001
2017-10-10T12:11:27.917699: step 6520, loss 0.086169, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:28.519723: step 6520, loss 0.208531, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6520

2017-10-10T12:11:29.614262: step 6521, loss 0.134832, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:29.898757: step 6522, loss 0.0801264, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:30.111069: step 6523, loss 0.0662713, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:30.293626: step 6524, loss 0.154281, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:30.500684: step 6525, loss 0.0552133, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:30.699317: step 6526, loss 0.152549, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:30.903694: step 6527, loss 0.0757534, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:31.100401: step 6528, loss 0.0470764, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:31.302240: step 6529, loss 0.0597186, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:31.577023: step 6530, loss 0.0967032, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:31.792707: step 6531, loss 0.0824357, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:31.996836: step 6532, loss 0.150163, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:32.208969: step 6533, loss 0.0488296, acc 1, learning_rate 0.0001
2017-10-10T12:11:32.469399: step 6534, loss 0.0977739, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:32.736707: step 6535, loss 0.0630713, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:32.925620: step 6536, loss 0.0747414, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:33.104791: step 6537, loss 0.0256905, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:33.284076: step 6538, loss 0.0998511, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:33.451438: step 6539, loss 0.039073, acc 1, learning_rate 0.0001
2017-10-10T12:11:33.624866: step 6540, loss 0.0938013, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:33.807419: step 6541, loss 0.0522834, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:34.047378: step 6542, loss 0.0195195, acc 1, learning_rate 0.0001
2017-10-10T12:11:34.293147: step 6543, loss 0.054016, acc 1, learning_rate 0.0001
2017-10-10T12:11:34.485038: step 6544, loss 0.0649265, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:34.678100: step 6545, loss 0.0980159, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:34.885515: step 6546, loss 0.0861335, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:35.120222: step 6547, loss 0.0777248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:35.377965: step 6548, loss 0.0656311, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:35.596329: step 6549, loss 0.0280582, acc 1, learning_rate 0.0001
2017-10-10T12:11:35.836869: step 6550, loss 0.0864908, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:36.089685: step 6551, loss 0.0440244, acc 1, learning_rate 0.0001
2017-10-10T12:11:36.335076: step 6552, loss 0.0765583, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:36.584819: step 6553, loss 0.0418379, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:36.841466: step 6554, loss 0.0419171, acc 1, learning_rate 0.0001
2017-10-10T12:11:37.088862: step 6555, loss 0.129574, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:37.356836: step 6556, loss 0.0700682, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:37.581454: step 6557, loss 0.0524017, acc 1, learning_rate 0.0001
2017-10-10T12:11:37.819257: step 6558, loss 0.133618, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:38.072519: step 6559, loss 0.0275536, acc 1, learning_rate 0.0001
2017-10-10T12:11:38.328875: step 6560, loss 0.0443422, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:38.898285: step 6560, loss 0.208511, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6560

2017-10-10T12:11:39.889995: step 6561, loss 0.137615, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:40.133782: step 6562, loss 0.141022, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:40.395057: step 6563, loss 0.0921746, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:40.608932: step 6564, loss 0.130931, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:40.833516: step 6565, loss 0.0388699, acc 1, learning_rate 0.0001
2017-10-10T12:11:41.012836: step 6566, loss 0.0312646, acc 1, learning_rate 0.0001
2017-10-10T12:11:41.227399: step 6567, loss 0.102902, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:41.426661: step 6568, loss 0.0855488, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:41.646721: step 6569, loss 0.0365654, acc 1, learning_rate 0.0001
2017-10-10T12:11:41.872857: step 6570, loss 0.0352031, acc 1, learning_rate 0.0001
2017-10-10T12:11:42.068109: step 6571, loss 0.0919459, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:42.281038: step 6572, loss 0.0822899, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:42.534657: step 6573, loss 0.122909, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:42.789679: step 6574, loss 0.0640127, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:42.983798: step 6575, loss 0.0746689, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:43.173827: step 6576, loss 0.088277, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:43.392223: step 6577, loss 0.109784, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:43.696897: step 6578, loss 0.0533783, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:43.981110: step 6579, loss 0.0955031, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:44.153397: step 6580, loss 0.052534, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:44.316835: step 6581, loss 0.0793735, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:44.480461: step 6582, loss 0.0585256, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:44.632873: step 6583, loss 0.104872, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:44.797143: step 6584, loss 0.0460598, acc 1, learning_rate 0.0001
2017-10-10T12:11:45.018754: step 6585, loss 0.0495836, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:45.270036: step 6586, loss 0.0331908, acc 1, learning_rate 0.0001
2017-10-10T12:11:45.490748: step 6587, loss 0.0354766, acc 1, learning_rate 0.0001
2017-10-10T12:11:45.718473: step 6588, loss 0.110211, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:45.951438: step 6589, loss 0.10662, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:46.180807: step 6590, loss 0.0460838, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:46.404870: step 6591, loss 0.0648443, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:46.644858: step 6592, loss 0.0826907, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:46.832936: step 6593, loss 0.101041, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:47.126739: step 6594, loss 0.0837933, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:47.297839: step 6595, loss 0.0708952, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:47.492024: step 6596, loss 0.0649354, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:47.687188: step 6597, loss 0.079222, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:47.865442: step 6598, loss 0.105185, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:48.074298: step 6599, loss 0.0837431, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:48.253175: step 6600, loss 0.0720383, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:48.813991: step 6600, loss 0.20946, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6600

2017-10-10T12:11:50.074560: step 6601, loss 0.0491873, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:50.313337: step 6602, loss 0.0314191, acc 1, learning_rate 0.0001
2017-10-10T12:11:50.501411: step 6603, loss 0.0339083, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:50.694742: step 6604, loss 0.0727207, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:50.931949: step 6605, loss 0.0776276, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:51.144257: step 6606, loss 0.109865, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:51.381846: step 6607, loss 0.0713107, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:51.627946: step 6608, loss 0.0906664, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:51.871374: step 6609, loss 0.0683388, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:52.116393: step 6610, loss 0.120955, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:52.344222: step 6611, loss 0.0762064, acc 1, learning_rate 0.0001
2017-10-10T12:11:52.548969: step 6612, loss 0.0355679, acc 1, learning_rate 0.0001
2017-10-10T12:11:52.808896: step 6613, loss 0.125399, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:53.016363: step 6614, loss 0.138423, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:53.208488: step 6615, loss 0.106984, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:53.435648: step 6616, loss 0.0366007, acc 1, learning_rate 0.0001
2017-10-10T12:11:53.640880: step 6617, loss 0.112391, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:53.868860: step 6618, loss 0.0227552, acc 1, learning_rate 0.0001
2017-10-10T12:11:54.092834: step 6619, loss 0.0229749, acc 1, learning_rate 0.0001
2017-10-10T12:11:54.359709: step 6620, loss 0.0299845, acc 1, learning_rate 0.0001
2017-10-10T12:11:54.601724: step 6621, loss 0.0449458, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:54.870452: step 6622, loss 0.0206115, acc 1, learning_rate 0.0001
2017-10-10T12:11:55.085343: step 6623, loss 0.0947041, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:55.258742: step 6624, loss 0.0835645, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:55.424832: step 6625, loss 0.0549471, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:55.648829: step 6626, loss 0.0544799, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:55.798570: step 6627, loss 0.0839258, acc 1, learning_rate 0.0001
2017-10-10T12:11:55.945129: step 6628, loss 0.0339922, acc 1, learning_rate 0.0001
2017-10-10T12:11:56.152910: step 6629, loss 0.0283765, acc 1, learning_rate 0.0001
2017-10-10T12:11:56.356189: step 6630, loss 0.0269855, acc 1, learning_rate 0.0001
2017-10-10T12:11:56.569941: step 6631, loss 0.0794757, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:56.789246: step 6632, loss 0.111466, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:57.025915: step 6633, loss 0.0705436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:57.240825: step 6634, loss 0.147412, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:57.492123: step 6635, loss 0.112891, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:57.721661: step 6636, loss 0.182572, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:57.912918: step 6637, loss 0.0322288, acc 1, learning_rate 0.0001
2017-10-10T12:11:58.111293: step 6638, loss 0.104414, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:58.326883: step 6639, loss 0.0269166, acc 1, learning_rate 0.0001
2017-10-10T12:11:58.549054: step 6640, loss 0.0459579, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:59.137055: step 6640, loss 0.208579, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6640

2017-10-10T12:12:00.211066: step 6641, loss 0.0689935, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:00.403034: step 6642, loss 0.129238, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:00.637052: step 6643, loss 0.0644872, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:00.870978: step 6644, loss 0.0612044, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:01.137537: step 6645, loss 0.0621233, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:01.356983: step 6646, loss 0.0388011, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:01.602098: step 6647, loss 0.134977, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:01.830678: step 6648, loss 0.0204514, acc 1, learning_rate 0.0001
2017-10-10T12:12:02.062473: step 6649, loss 0.0755198, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:02.298648: step 6650, loss 0.0470775, acc 1, learning_rate 0.0001
2017-10-10T12:12:02.546888: step 6651, loss 0.0454379, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:02.796976: step 6652, loss 0.0185095, acc 1, learning_rate 0.0001
2017-10-10T12:12:03.047457: step 6653, loss 0.0707164, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:03.294758: step 6654, loss 0.113599, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:03.579169: step 6655, loss 0.0755604, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:03.868847: step 6656, loss 0.061436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:04.121411: step 6657, loss 0.0458773, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:04.777196: step 6658, loss 0.0456742, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:04.980250: step 6659, loss 0.0390398, acc 1, learning_rate 0.0001
2017-10-10T12:12:05.178904: step 6660, loss 0.0498386, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:05.362745: step 6661, loss 0.0735671, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:05.591984: step 6662, loss 0.0492756, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:05.787798: step 6663, loss 0.102979, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:05.992840: step 6664, loss 0.0670126, acc 0.980392, learning_rate 0.0001
2017-10-10T12:12:06.278356: step 6665, loss 0.11961, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:06.525860: step 6666, loss 0.0793271, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:06.695930: step 6667, loss 0.0549563, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:06.872424: step 6668, loss 0.107829, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:07.067359: step 6669, loss 0.0423724, acc 1, learning_rate 0.0001
2017-10-10T12:12:07.244108: step 6670, loss 0.0684169, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:07.435704: step 6671, loss 0.0554527, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:07.614253: step 6672, loss 0.0415129, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:07.826969: step 6673, loss 0.0474716, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:08.072843: step 6674, loss 0.096, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:08.341122: step 6675, loss 0.101588, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:08.576134: step 6676, loss 0.0251083, acc 1, learning_rate 0.0001
2017-10-10T12:12:08.821372: step 6677, loss 0.0729628, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:09.037734: step 6678, loss 0.083913, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:09.266235: step 6679, loss 0.0201774, acc 1, learning_rate 0.0001
2017-10-10T12:12:09.504855: step 6680, loss 0.109777, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:10.093634: step 6680, loss 0.212522, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6680

2017-10-10T12:12:11.238208: step 6681, loss 0.123006, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:11.444585: step 6682, loss 0.121048, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:11.648928: step 6683, loss 0.172381, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:11.893171: step 6684, loss 0.133856, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:12.127732: step 6685, loss 0.0648207, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:12.404907: step 6686, loss 0.10425, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:12.649452: step 6687, loss 0.0992471, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:12.832924: step 6688, loss 0.0390124, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:13.041004: step 6689, loss 0.0563335, acc 1, learning_rate 0.0001
2017-10-10T12:12:13.235661: step 6690, loss 0.0648056, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:13.420733: step 6691, loss 0.0397891, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:13.659981: step 6692, loss 0.169872, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:13.869233: step 6693, loss 0.0554634, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:14.093607: step 6694, loss 0.0585343, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:14.324451: step 6695, loss 0.0303883, acc 1, learning_rate 0.0001
2017-10-10T12:12:14.548410: step 6696, loss 0.167188, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:14.754650: step 6697, loss 0.17331, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:14.969258: step 6698, loss 0.0461218, acc 1, learning_rate 0.0001
2017-10-10T12:12:15.203277: step 6699, loss 0.0900077, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:15.424831: step 6700, loss 0.121484, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:15.690703: step 6701, loss 0.0935801, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:15.968224: step 6702, loss 0.0646741, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:16.200865: step 6703, loss 0.0280242, acc 1, learning_rate 0.0001
2017-10-10T12:12:16.464068: step 6704, loss 0.100679, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:16.662567: step 6705, loss 0.0324902, acc 1, learning_rate 0.0001
2017-10-10T12:12:16.910052: step 6706, loss 0.0623678, acc 1, learning_rate 0.0001
2017-10-10T12:12:17.144801: step 6707, loss 0.0167533, acc 1, learning_rate 0.0001
2017-10-10T12:12:17.430076: step 6708, loss 0.0859887, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:17.728733: step 6709, loss 0.107924, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:17.952935: step 6710, loss 0.0579598, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:18.110738: step 6711, loss 0.0295498, acc 1, learning_rate 0.0001
2017-10-10T12:12:18.275909: step 6712, loss 0.0428276, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:18.464381: step 6713, loss 0.0828902, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:18.658288: step 6714, loss 0.107777, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:18.877011: step 6715, loss 0.0922387, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:19.100850: step 6716, loss 0.0581538, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:19.356847: step 6717, loss 0.100978, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:19.593990: step 6718, loss 0.035454, acc 1, learning_rate 0.0001
2017-10-10T12:12:19.856961: step 6719, loss 0.0470703, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:20.131612: step 6720, loss 0.0833476, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:20.760882: step 6720, loss 0.212158, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6720

2017-10-10T12:12:22.031102: step 6721, loss 0.0724535, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:22.255531: step 6722, loss 0.035574, acc 1, learning_rate 0.0001
2017-10-10T12:12:22.498568: step 6723, loss 0.0491151, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:22.701256: step 6724, loss 0.176082, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:22.952855: step 6725, loss 0.0255479, acc 1, learning_rate 0.0001
2017-10-10T12:12:23.148859: step 6726, loss 0.0381266, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:23.345060: step 6727, loss 0.0311739, acc 1, learning_rate 0.0001
2017-10-10T12:12:23.556944: step 6728, loss 0.0633888, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:23.786101: step 6729, loss 0.0570412, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:24.032848: step 6730, loss 0.131867, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:24.249263: step 6731, loss 0.139728, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:24.492081: step 6732, loss 0.0624292, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:24.727588: step 6733, loss 0.136997, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:24.940818: step 6734, loss 0.0604766, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:25.160715: step 6735, loss 0.0726362, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:25.384343: step 6736, loss 0.107504, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:25.640619: step 6737, loss 0.0930566, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:25.880885: step 6738, loss 0.0754864, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:26.125513: step 6739, loss 0.0189987, acc 1, learning_rate 0.0001
2017-10-10T12:12:26.355805: step 6740, loss 0.0765254, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:26.585734: step 6741, loss 0.0295595, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:26.827600: step 6742, loss 0.0779903, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:27.060319: step 6743, loss 0.0772099, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:27.284359: step 6744, loss 0.107778, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:27.485087: step 6745, loss 0.0570106, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:27.664871: step 6746, loss 0.0924042, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:27.889056: step 6747, loss 0.0847143, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:28.159712: step 6748, loss 0.0664259, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:28.440958: step 6749, loss 0.132629, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:28.642287: step 6750, loss 0.152424, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:28.824996: step 6751, loss 0.0626827, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:28.972837: step 6752, loss 0.0556532, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:29.210047: step 6753, loss 0.0442291, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:29.365200: step 6754, loss 0.0580639, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:29.528701: step 6755, loss 0.0241055, acc 1, learning_rate 0.0001
2017-10-10T12:12:29.709163: step 6756, loss 0.114785, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:29.902104: step 6757, loss 0.0287196, acc 1, learning_rate 0.0001
2017-10-10T12:12:30.081659: step 6758, loss 0.048575, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:30.274249: step 6759, loss 0.0784354, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:30.528928: step 6760, loss 0.11034, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:31.158134: step 6760, loss 0.207828, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6760

2017-10-10T12:12:32.134491: step 6761, loss 0.0466427, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:32.359125: step 6762, loss 0.104629, acc 0.960784, learning_rate 0.0001
2017-10-10T12:12:32.596819: step 6763, loss 0.039409, acc 1, learning_rate 0.0001
2017-10-10T12:12:32.872826: step 6764, loss 0.0782094, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:33.106167: step 6765, loss 0.124591, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:33.332682: step 6766, loss 0.0653297, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:33.569176: step 6767, loss 0.0791994, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:33.815650: step 6768, loss 0.0832805, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:34.067810: step 6769, loss 0.0868992, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:34.287768: step 6770, loss 0.0832067, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:34.493082: step 6771, loss 0.0360347, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:34.676837: step 6772, loss 0.0821719, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:34.921024: step 6773, loss 0.0452211, acc 1, learning_rate 0.0001
2017-10-10T12:12:35.140035: step 6774, loss 0.0778092, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:35.337103: step 6775, loss 0.0365274, acc 1, learning_rate 0.0001
2017-10-10T12:12:35.588856: step 6776, loss 0.0627194, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:35.808838: step 6777, loss 0.103501, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:36.050727: step 6778, loss 0.0627067, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:36.324175: step 6779, loss 0.0339757, acc 1, learning_rate 0.0001
2017-10-10T12:12:36.568629: step 6780, loss 0.13284, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:36.795849: step 6781, loss 0.0540401, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:37.048938: step 6782, loss 0.0468867, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:37.316872: step 6783, loss 0.0616176, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:37.534384: step 6784, loss 0.0209351, acc 1, learning_rate 0.0001
2017-10-10T12:12:37.820338: step 6785, loss 0.0523091, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:38.056970: step 6786, loss 0.0349014, acc 1, learning_rate 0.0001
2017-10-10T12:12:38.274795: step 6787, loss 0.0304849, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:38.497050: step 6788, loss 0.0471662, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:38.708457: step 6789, loss 0.0871139, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:38.958914: step 6790, loss 0.0351471, acc 1, learning_rate 0.0001
2017-10-10T12:12:39.240341: step 6791, loss 0.101999, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:39.525277: step 6792, loss 0.115389, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:39.706898: step 6793, loss 0.0937509, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:39.889014: step 6794, loss 0.0515428, acc 1, learning_rate 0.0001
2017-10-10T12:12:40.089313: step 6795, loss 0.104095, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:40.259018: step 6796, loss 0.041081, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:40.437789: step 6797, loss 0.0739018, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:40.633146: step 6798, loss 0.102671, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:40.826720: step 6799, loss 0.0575218, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:41.056272: step 6800, loss 0.0369741, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:41.705014: step 6800, loss 0.207747, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6800

2017-10-10T12:12:42.804201: step 6801, loss 0.0330131, acc 1, learning_rate 0.0001
2017-10-10T12:12:43.013772: step 6802, loss 0.141042, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:43.246286: step 6803, loss 0.0990233, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:43.521726: step 6804, loss 0.111853, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:43.756892: step 6805, loss 0.0250228, acc 1, learning_rate 0.0001
2017-10-10T12:12:43.982680: step 6806, loss 0.081375, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:44.212610: step 6807, loss 0.0587253, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:44.466573: step 6808, loss 0.0738586, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:44.682509: step 6809, loss 0.0752884, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:44.908210: step 6810, loss 0.0872402, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:45.133002: step 6811, loss 0.0964639, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:45.364912: step 6812, loss 0.0897503, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:45.641021: step 6813, loss 0.0332128, acc 1, learning_rate 0.0001
2017-10-10T12:12:45.950892: step 6814, loss 0.0974803, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:46.202471: step 6815, loss 0.0524126, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:46.408334: step 6816, loss 0.0733144, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:46.610397: step 6817, loss 0.0851592, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:46.804095: step 6818, loss 0.0942696, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:46.980804: step 6819, loss 0.0617288, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:47.128065: step 6820, loss 0.0248369, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:47.389049: step 6821, loss 0.111223, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:47.576543: step 6822, loss 0.115825, acc 0.90625, learning_rate 0.0001
2017-10-10T12:12:47.760827: step 6823, loss 0.0792487, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:48.007703: step 6824, loss 0.0598084, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:48.255005: step 6825, loss 0.031677, acc 1, learning_rate 0.0001
2017-10-10T12:12:48.482577: step 6826, loss 0.0825057, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:48.724903: step 6827, loss 0.0573663, acc 1, learning_rate 0.0001
2017-10-10T12:12:48.968813: step 6828, loss 0.0738929, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:49.196185: step 6829, loss 0.0180005, acc 1, learning_rate 0.0001
2017-10-10T12:12:49.360857: step 6830, loss 0.018106, acc 1, learning_rate 0.0001
2017-10-10T12:12:49.608952: step 6831, loss 0.0786463, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:49.846640: step 6832, loss 0.139412, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:50.089031: step 6833, loss 0.0998403, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:50.383785: step 6834, loss 0.0760258, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:50.654432: step 6835, loss 0.0834245, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:50.837549: step 6836, loss 0.0616815, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:50.998441: step 6837, loss 0.044331, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:51.171127: step 6838, loss 0.0431867, acc 1, learning_rate 0.0001
2017-10-10T12:12:51.332621: step 6839, loss 0.027182, acc 1, learning_rate 0.0001
2017-10-10T12:12:51.503811: step 6840, loss 0.0783118, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:52.059077: step 6840, loss 0.208137, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6840

2017-10-10T12:12:53.253505: step 6841, loss 0.0990636, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:53.473183: step 6842, loss 0.0421046, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:53.672985: step 6843, loss 0.18519, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:53.905831: step 6844, loss 0.0848756, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:54.187535: step 6845, loss 0.0988088, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:54.405196: step 6846, loss 0.05168, acc 1, learning_rate 0.0001
2017-10-10T12:12:54.620172: step 6847, loss 0.0715814, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:54.832881: step 6848, loss 0.0719447, acc 1, learning_rate 0.0001
2017-10-10T12:12:55.040974: step 6849, loss 0.0717248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:55.224818: step 6850, loss 0.103992, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:55.388812: step 6851, loss 0.156752, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:55.610738: step 6852, loss 0.0521487, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:55.830362: step 6853, loss 0.0427181, acc 1, learning_rate 0.0001
2017-10-10T12:12:56.039811: step 6854, loss 0.115986, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:56.240075: step 6855, loss 0.109077, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:56.479184: step 6856, loss 0.0731663, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:56.697863: step 6857, loss 0.10601, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:56.944746: step 6858, loss 0.0810567, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:57.170602: step 6859, loss 0.0777089, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:57.400178: step 6860, loss 0.0656462, acc 0.980392, learning_rate 0.0001
2017-10-10T12:12:57.608846: step 6861, loss 0.106678, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:57.875394: step 6862, loss 0.106356, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:58.087666: step 6863, loss 0.0598674, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:58.308864: step 6864, loss 0.0460262, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:58.553959: step 6865, loss 0.0712231, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:58.776635: step 6866, loss 0.0202967, acc 1, learning_rate 0.0001
2017-10-10T12:12:59.029636: step 6867, loss 0.0994209, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:59.252799: step 6868, loss 0.0500598, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:59.483656: step 6869, loss 0.0431576, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:59.708301: step 6870, loss 0.0989144, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:59.955635: step 6871, loss 0.0784479, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:00.185997: step 6872, loss 0.0709495, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:00.396956: step 6873, loss 0.0700376, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:00.640913: step 6874, loss 0.112227, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:00.873350: step 6875, loss 0.0943767, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:01.106175: step 6876, loss 0.0684785, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:01.267969: step 6877, loss 0.0751426, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:01.583448: step 6878, loss 0.107896, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:01.805661: step 6879, loss 0.109999, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:02.015842: step 6880, loss 0.0965052, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:02.521524: step 6880, loss 0.209527, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6880

2017-10-10T12:13:03.431380: step 6881, loss 0.0476574, acc 1, learning_rate 0.0001
2017-10-10T12:13:03.638293: step 6882, loss 0.0690089, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:03.831699: step 6883, loss 0.0577889, acc 1, learning_rate 0.0001
2017-10-10T12:13:04.082256: step 6884, loss 0.0427949, acc 1, learning_rate 0.0001
2017-10-10T12:13:04.324208: step 6885, loss 0.079952, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:04.564610: step 6886, loss 0.113625, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:04.825908: step 6887, loss 0.203787, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:05.069621: step 6888, loss 0.153391, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:05.318609: step 6889, loss 0.0966288, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:05.562304: step 6890, loss 0.199772, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:05.819590: step 6891, loss 0.0746702, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:06.091801: step 6892, loss 0.0103292, acc 1, learning_rate 0.0001
2017-10-10T12:13:06.321230: step 6893, loss 0.0201775, acc 1, learning_rate 0.0001
2017-10-10T12:13:06.562761: step 6894, loss 0.0777584, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:06.806407: step 6895, loss 0.0426799, acc 1, learning_rate 0.0001
2017-10-10T12:13:07.027012: step 6896, loss 0.0208123, acc 1, learning_rate 0.0001
2017-10-10T12:13:07.182607: step 6897, loss 0.0238634, acc 1, learning_rate 0.0001
2017-10-10T12:13:07.390496: step 6898, loss 0.0286292, acc 1, learning_rate 0.0001
2017-10-10T12:13:07.641273: step 6899, loss 0.040782, acc 1, learning_rate 0.0001
2017-10-10T12:13:07.895678: step 6900, loss 0.0428409, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:08.132435: step 6901, loss 0.0527787, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:08.405522: step 6902, loss 0.0629262, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:08.647907: step 6903, loss 0.0950123, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:08.885501: step 6904, loss 0.0334074, acc 1, learning_rate 0.0001
2017-10-10T12:13:09.108881: step 6905, loss 0.0777831, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:09.340847: step 6906, loss 0.109297, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:09.575400: step 6907, loss 0.106432, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:09.792430: step 6908, loss 0.0386013, acc 1, learning_rate 0.0001
2017-10-10T12:13:10.032257: step 6909, loss 0.0689264, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.230993: step 6910, loss 0.0508159, acc 1, learning_rate 0.0001
2017-10-10T12:13:10.437002: step 6911, loss 0.0196089, acc 1, learning_rate 0.0001
2017-10-10T12:13:10.645089: step 6912, loss 0.0579582, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.856914: step 6913, loss 0.053484, acc 1, learning_rate 0.0001
2017-10-10T12:13:11.100889: step 6914, loss 0.0403302, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:11.388477: step 6915, loss 0.0752645, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:11.582413: step 6916, loss 0.0285218, acc 1, learning_rate 0.0001
2017-10-10T12:13:11.768962: step 6917, loss 0.0247519, acc 1, learning_rate 0.0001
2017-10-10T12:13:11.961158: step 6918, loss 0.0506526, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:12.144806: step 6919, loss 0.0355545, acc 1, learning_rate 0.0001
2017-10-10T12:13:12.340841: step 6920, loss 0.0687222, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:12.928064: step 6920, loss 0.20968, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6920

2017-10-10T12:13:13.606133: step 6921, loss 0.0389045, acc 1, learning_rate 0.0001
2017-10-10T12:13:13.799173: step 6922, loss 0.039667, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:13.980384: step 6923, loss 0.0588987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:14.150261: step 6924, loss 0.0680236, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:14.324283: step 6925, loss 0.0892588, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:14.492212: step 6926, loss 0.0862664, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:14.677203: step 6927, loss 0.0963245, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:14.846848: step 6928, loss 0.0443455, acc 1, learning_rate 0.0001
2017-10-10T12:13:15.000841: step 6929, loss 0.016, acc 1, learning_rate 0.0001
2017-10-10T12:13:15.171790: step 6930, loss 0.070388, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:15.327026: step 6931, loss 0.0278171, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:15.457300: step 6932, loss 0.047972, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:15.630397: step 6933, loss 0.0999994, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:15.805999: step 6934, loss 0.0771066, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:15.999144: step 6935, loss 0.0832621, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:16.175788: step 6936, loss 0.0353657, acc 1, learning_rate 0.0001
2017-10-10T12:13:16.367246: step 6937, loss 0.0449047, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:16.544845: step 6938, loss 0.0471925, acc 1, learning_rate 0.0001
2017-10-10T12:13:16.719825: step 6939, loss 0.0443532, acc 1, learning_rate 0.0001
2017-10-10T12:13:16.884569: step 6940, loss 0.101875, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:17.054907: step 6941, loss 0.0724072, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:17.212275: step 6942, loss 0.117847, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:17.394297: step 6943, loss 0.0936468, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:17.535596: step 6944, loss 0.0480773, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:17.668254: step 6945, loss 0.108589, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:17.830722: step 6946, loss 0.0692193, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:18.027340: step 6947, loss 0.0922808, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:18.211279: step 6948, loss 0.0322971, acc 1, learning_rate 0.0001
2017-10-10T12:13:18.394365: step 6949, loss 0.107422, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:18.565764: step 6950, loss 0.0524455, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:18.716732: step 6951, loss 0.0762821, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:18.892887: step 6952, loss 0.077111, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:19.081314: step 6953, loss 0.0335288, acc 1, learning_rate 0.0001
2017-10-10T12:13:19.259426: step 6954, loss 0.081609, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:19.424861: step 6955, loss 0.0365795, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:19.600807: step 6956, loss 0.0530514, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:19.764888: step 6957, loss 0.0421311, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:19.885205: step 6958, loss 0.122963, acc 0.941176, learning_rate 0.0001
2017-10-10T12:13:20.036883: step 6959, loss 0.0422363, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:20.209498: step 6960, loss 0.0773937, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:20.584893: step 6960, loss 0.211176, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-6960

2017-10-10T12:13:21.541428: step 6961, loss 0.0491222, acc 1, learning_rate 0.0001
2017-10-10T12:13:21.650696: step 6962, loss 0.0259631, acc 1, learning_rate 0.0001
2017-10-10T12:13:21.764016: step 6963, loss 0.0157726, acc 1, learning_rate 0.0001
2017-10-10T12:13:21.932840: step 6964, loss 0.0992634, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:22.114096: step 6965, loss 0.0846927, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:22.299071: step 6966, loss 0.0592342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:22.468112: step 6967, loss 0.069382, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:22.633533: step 6968, loss 0.0551211, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:22.792098: step 6969, loss 0.0583022, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:22.939173: step 6970, loss 0.0551381, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:23.096027: step 6971, loss 0.0702787, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:23.253886: step 6972, loss 0.108772, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:23.432259: step 6973, loss 0.0545698, acc 1, learning_rate 0.0001
2017-10-10T12:13:23.624066: step 6974, loss 0.0271293, acc 1, learning_rate 0.0001
2017-10-10T12:13:23.802344: step 6975, loss 0.0996703, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:23.996042: step 6976, loss 0.0205052, acc 1, learning_rate 0.0001
2017-10-10T12:13:24.170795: step 6977, loss 0.121063, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:24.346967: step 6978, loss 0.0284176, acc 1, learning_rate 0.0001
2017-10-10T12:13:24.516945: step 6979, loss 0.150439, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:24.673763: step 6980, loss 0.0982554, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:24.842848: step 6981, loss 0.0697036, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:25.031520: step 6982, loss 0.0790497, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:25.188346: step 6983, loss 0.0761679, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:25.356855: step 6984, loss 0.0748385, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:25.519724: step 6985, loss 0.120444, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:25.674948: step 6986, loss 0.0282284, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:25.831922: step 6987, loss 0.0256496, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:25.994107: step 6988, loss 0.0545333, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:26.153314: step 6989, loss 0.0907951, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:26.325116: step 6990, loss 0.0755288, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:26.520052: step 6991, loss 0.0387728, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:26.699759: step 6992, loss 0.0540871, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:26.849012: step 6993, loss 0.0337549, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:27.028840: step 6994, loss 0.0772177, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:27.184888: step 6995, loss 0.0797517, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:27.368877: step 6996, loss 0.0675362, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:27.504999: step 6997, loss 0.0732777, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:27.681118: step 6998, loss 0.0100741, acc 1, learning_rate 0.0001
2017-10-10T12:13:27.848559: step 6999, loss 0.0479678, acc 1, learning_rate 0.0001
2017-10-10T12:13:28.015897: step 7000, loss 0.0923367, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:28.403609: step 7000, loss 0.212288, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7000

2017-10-10T12:13:29.220929: step 7001, loss 0.0489543, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:29.432770: step 7002, loss 0.0570254, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:29.541680: step 7003, loss 0.137147, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:29.661664: step 7004, loss 0.106462, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:29.773422: step 7005, loss 0.0499215, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:29.885561: step 7006, loss 0.110637, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:29.997318: step 7007, loss 0.0187179, acc 1, learning_rate 0.0001
2017-10-10T12:13:30.147774: step 7008, loss 0.0602062, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:30.305374: step 7009, loss 0.112767, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:30.447306: step 7010, loss 0.0436418, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:30.616478: step 7011, loss 0.0764959, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:30.770289: step 7012, loss 0.0861571, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:30.936680: step 7013, loss 0.13011, acc 0.921875, learning_rate 0.0001
2017-10-10T12:13:31.117069: step 7014, loss 0.118291, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:31.296495: step 7015, loss 0.0642041, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:31.461930: step 7016, loss 0.0737924, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:31.627234: step 7017, loss 0.0619178, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:31.807816: step 7018, loss 0.110465, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:31.981285: step 7019, loss 0.0556468, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:32.140115: step 7020, loss 0.0963934, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:32.308829: step 7021, loss 0.0713699, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:32.483212: step 7022, loss 0.084087, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:32.670806: step 7023, loss 0.037405, acc 1, learning_rate 0.0001
2017-10-10T12:13:32.844844: step 7024, loss 0.0509453, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:32.998898: step 7025, loss 0.1503, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:33.136972: step 7026, loss 0.026148, acc 1, learning_rate 0.0001
2017-10-10T12:13:33.299416: step 7027, loss 0.18984, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:33.464942: step 7028, loss 0.106032, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:33.655914: step 7029, loss 0.0622627, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:33.841627: step 7030, loss 0.0546314, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:34.038740: step 7031, loss 0.0351768, acc 1, learning_rate 0.0001
2017-10-10T12:13:34.201740: step 7032, loss 0.0615164, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:34.353599: step 7033, loss 0.0645478, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:34.500594: step 7034, loss 0.171887, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:34.652913: step 7035, loss 0.103214, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:34.818116: step 7036, loss 0.0212195, acc 1, learning_rate 0.0001
2017-10-10T12:13:35.000795: step 7037, loss 0.0333817, acc 1, learning_rate 0.0001
2017-10-10T12:13:35.201135: step 7038, loss 0.0460452, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:35.380695: step 7039, loss 0.0529032, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:35.554518: step 7040, loss 0.0343332, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:35.952570: step 7040, loss 0.208889, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7040

2017-10-10T12:13:36.917834: step 7041, loss 0.0881566, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:37.100076: step 7042, loss 0.0638867, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:37.360909: step 7043, loss 0.0532506, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:37.543215: step 7044, loss 0.0722104, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:37.651080: step 7045, loss 0.0640354, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:37.761507: step 7046, loss 0.0418542, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:37.874899: step 7047, loss 0.0726994, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:37.989085: step 7048, loss 0.0493654, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:38.102766: step 7049, loss 0.0712751, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:38.216580: step 7050, loss 0.0620242, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:38.338547: step 7051, loss 0.139508, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:38.452368: step 7052, loss 0.0483829, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:38.578448: step 7053, loss 0.0512921, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:38.705998: step 7054, loss 0.144105, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:38.822044: step 7055, loss 0.0619999, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:38.920556: step 7056, loss 0.0305552, acc 1, learning_rate 0.0001
2017-10-10T12:13:39.032534: step 7057, loss 0.0141565, acc 1, learning_rate 0.0001
2017-10-10T12:13:39.139256: step 7058, loss 0.0644135, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:39.241221: step 7059, loss 0.068143, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:39.347012: step 7060, loss 0.0742178, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:39.448183: step 7061, loss 0.0349688, acc 1, learning_rate 0.0001
2017-10-10T12:13:39.549503: step 7062, loss 0.0234726, acc 1, learning_rate 0.0001
2017-10-10T12:13:39.657706: step 7063, loss 0.0633508, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:39.764193: step 7064, loss 0.0387518, acc 1, learning_rate 0.0001
2017-10-10T12:13:39.868860: step 7065, loss 0.0463602, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:39.969790: step 7066, loss 0.0336984, acc 1, learning_rate 0.0001
2017-10-10T12:13:40.071576: step 7067, loss 0.0562503, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:40.175457: step 7068, loss 0.0881149, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:40.284270: step 7069, loss 0.0435482, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:40.389098: step 7070, loss 0.0565813, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:40.495729: step 7071, loss 0.0659454, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:40.597294: step 7072, loss 0.0147381, acc 1, learning_rate 0.0001
2017-10-10T12:13:40.698993: step 7073, loss 0.0462353, acc 1, learning_rate 0.0001
2017-10-10T12:13:40.802486: step 7074, loss 0.0517934, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:40.907966: step 7075, loss 0.0757151, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:41.010004: step 7076, loss 0.037576, acc 1, learning_rate 0.0001
2017-10-10T12:13:41.118056: step 7077, loss 0.049627, acc 1, learning_rate 0.0001
2017-10-10T12:13:41.225688: step 7078, loss 0.082344, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:41.331228: step 7079, loss 0.143228, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:41.436680: step 7080, loss 0.0652685, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:41.695712: step 7080, loss 0.205116, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7080

2017-10-10T12:13:42.350963: step 7081, loss 0.0878954, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:42.457266: step 7082, loss 0.0492916, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:42.560393: step 7083, loss 0.0354954, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:42.668395: step 7084, loss 0.0687549, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:42.773162: step 7085, loss 0.0504974, acc 1, learning_rate 0.0001
2017-10-10T12:13:42.880325: step 7086, loss 0.0377382, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:42.986766: step 7087, loss 0.0687864, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:43.085847: step 7088, loss 0.072506, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:43.188254: step 7089, loss 0.0833262, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:43.289788: step 7090, loss 0.0934244, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:43.393337: step 7091, loss 0.0477742, acc 1, learning_rate 0.0001
2017-10-10T12:13:43.494738: step 7092, loss 0.081111, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:43.598331: step 7093, loss 0.0818082, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:43.699335: step 7094, loss 0.0454169, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:43.803193: step 7095, loss 0.0765797, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:43.913031: step 7096, loss 0.0476576, acc 1, learning_rate 0.0001
2017-10-10T12:13:44.015526: step 7097, loss 0.0587845, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:44.117517: step 7098, loss 0.100375, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:44.220825: step 7099, loss 0.106444, acc 0.921875, learning_rate 0.0001
2017-10-10T12:13:44.326546: step 7100, loss 0.0709663, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:44.430024: step 7101, loss 0.0859908, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:44.533453: step 7102, loss 0.0719427, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:44.636931: step 7103, loss 0.0907456, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:44.741138: step 7104, loss 0.0718768, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:44.843083: step 7105, loss 0.0430606, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:44.946627: step 7106, loss 0.0876902, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:45.052718: step 7107, loss 0.0391706, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:45.158563: step 7108, loss 0.0823905, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:45.263372: step 7109, loss 0.0440256, acc 1, learning_rate 0.0001
2017-10-10T12:13:45.365761: step 7110, loss 0.0684666, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:45.467359: step 7111, loss 0.0782944, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:45.571124: step 7112, loss 0.139065, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:45.676458: step 7113, loss 0.143455, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:45.779294: step 7114, loss 0.128243, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:45.882214: step 7115, loss 0.0764542, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:45.987790: step 7116, loss 0.0618313, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:46.092543: step 7117, loss 0.0253003, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:46.199206: step 7118, loss 0.0328136, acc 1, learning_rate 0.0001
2017-10-10T12:13:46.298567: step 7119, loss 0.0520369, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:46.404233: step 7120, loss 0.0988445, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:46.655663: step 7120, loss 0.204311, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7120

2017-10-10T12:13:47.179130: step 7121, loss 0.0680933, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:47.287327: step 7122, loss 0.14548, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:47.390712: step 7123, loss 0.0921467, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:47.496307: step 7124, loss 0.0159394, acc 1, learning_rate 0.0001
2017-10-10T12:13:47.602547: step 7125, loss 0.0466141, acc 1, learning_rate 0.0001
2017-10-10T12:13:47.705391: step 7126, loss 0.0613095, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:47.811408: step 7127, loss 0.0543033, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:47.919461: step 7128, loss 0.0380888, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:48.027821: step 7129, loss 0.0991565, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:48.136465: step 7130, loss 0.0512436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:48.241666: step 7131, loss 0.0550733, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:48.347489: step 7132, loss 0.0655887, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:48.446678: step 7133, loss 0.116059, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:48.551445: step 7134, loss 0.106108, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:48.656844: step 7135, loss 0.116402, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:48.768557: step 7136, loss 0.0855775, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:48.875846: step 7137, loss 0.053668, acc 1, learning_rate 0.0001
2017-10-10T12:13:48.977704: step 7138, loss 0.0816544, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:49.079969: step 7139, loss 0.067453, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:49.187285: step 7140, loss 0.048287, acc 1, learning_rate 0.0001
2017-10-10T12:13:49.295846: step 7141, loss 0.0442985, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:49.403041: step 7142, loss 0.114853, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:49.504157: step 7143, loss 0.08301, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:49.609636: step 7144, loss 0.0581034, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:49.715322: step 7145, loss 0.0498871, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:49.827984: step 7146, loss 0.109304, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:49.932713: step 7147, loss 0.0851338, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:50.035939: step 7148, loss 0.0425525, acc 1, learning_rate 0.0001
2017-10-10T12:13:50.138051: step 7149, loss 0.104271, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:50.243153: step 7150, loss 0.0553502, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:50.345847: step 7151, loss 0.0760164, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:50.449373: step 7152, loss 0.185653, acc 0.90625, learning_rate 0.0001
2017-10-10T12:13:50.555607: step 7153, loss 0.0621458, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:50.642826: step 7154, loss 0.0422832, acc 1, learning_rate 0.0001
2017-10-10T12:13:50.744732: step 7155, loss 0.0992293, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:50.851984: step 7156, loss 0.0868867, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:50.955048: step 7157, loss 0.0692446, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:51.060748: step 7158, loss 0.0273013, acc 1, learning_rate 0.0001
2017-10-10T12:13:51.166040: step 7159, loss 0.146883, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:51.270366: step 7160, loss 0.0883955, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:51.522728: step 7160, loss 0.207051, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7160

2017-10-10T12:13:52.101893: step 7161, loss 0.0329098, acc 1, learning_rate 0.0001
2017-10-10T12:13:52.206771: step 7162, loss 0.0598787, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:52.306973: step 7163, loss 0.0499606, acc 1, learning_rate 0.0001
2017-10-10T12:13:52.412343: step 7164, loss 0.0539028, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:52.520486: step 7165, loss 0.0668817, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:52.623388: step 7166, loss 0.146488, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:52.727262: step 7167, loss 0.0586699, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:52.833726: step 7168, loss 0.139636, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:52.950467: step 7169, loss 0.0914941, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:53.055488: step 7170, loss 0.0554064, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:53.161435: step 7171, loss 0.093427, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:53.267914: step 7172, loss 0.0372674, acc 1, learning_rate 0.0001
2017-10-10T12:13:53.373405: step 7173, loss 0.0636888, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:53.479772: step 7174, loss 0.0487126, acc 1, learning_rate 0.0001
2017-10-10T12:13:53.583691: step 7175, loss 0.0556736, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:53.687781: step 7176, loss 0.0488342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:53.791549: step 7177, loss 0.0453544, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:53.897809: step 7178, loss 0.0899382, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:54.002838: step 7179, loss 0.0604308, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:54.108978: step 7180, loss 0.0803601, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:54.213938: step 7181, loss 0.0822721, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:54.327443: step 7182, loss 0.0315965, acc 1, learning_rate 0.0001
2017-10-10T12:13:54.428939: step 7183, loss 0.0688648, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:54.529913: step 7184, loss 0.084234, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:54.636542: step 7185, loss 0.141026, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:54.740201: step 7186, loss 0.0470525, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:54.844587: step 7187, loss 0.0674637, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:54.951563: step 7188, loss 0.0757367, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:55.058208: step 7189, loss 0.0365988, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:55.164939: step 7190, loss 0.0898218, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:55.271691: step 7191, loss 0.042902, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:55.372760: step 7192, loss 0.0399143, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:55.478280: step 7193, loss 0.104427, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:55.580368: step 7194, loss 0.0184414, acc 1, learning_rate 0.0001
2017-10-10T12:13:55.682741: step 7195, loss 0.0466505, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:55.785828: step 7196, loss 0.0753329, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:55.890944: step 7197, loss 0.0712149, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:55.994851: step 7198, loss 0.0349638, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:56.099758: step 7199, loss 0.0527238, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:56.199797: step 7200, loss 0.0567611, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:56.456142: step 7200, loss 0.206702, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7200

2017-10-10T12:13:57.034338: step 7201, loss 0.0745237, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:57.138968: step 7202, loss 0.295103, acc 0.890625, learning_rate 0.0001
2017-10-10T12:13:57.245303: step 7203, loss 0.0633332, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:57.347048: step 7204, loss 0.128807, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:57.451592: step 7205, loss 0.0167966, acc 1, learning_rate 0.0001
2017-10-10T12:13:57.555299: step 7206, loss 0.110014, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:57.660149: step 7207, loss 0.108983, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:57.763066: step 7208, loss 0.0419274, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:57.869314: step 7209, loss 0.0159822, acc 1, learning_rate 0.0001
2017-10-10T12:13:57.974431: step 7210, loss 0.0944989, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:58.076529: step 7211, loss 0.044022, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:58.179191: step 7212, loss 0.048373, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:58.283295: step 7213, loss 0.15843, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:58.386877: step 7214, loss 0.128914, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:58.490064: step 7215, loss 0.105261, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:58.596936: step 7216, loss 0.040198, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:58.699908: step 7217, loss 0.0596082, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:58.806506: step 7218, loss 0.0857753, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:58.918777: step 7219, loss 0.0699944, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:59.021800: step 7220, loss 0.103934, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:59.122811: step 7221, loss 0.0578911, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:59.227459: step 7222, loss 0.0955615, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:59.329090: step 7223, loss 0.0337391, acc 1, learning_rate 0.0001
2017-10-10T12:13:59.434094: step 7224, loss 0.100842, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:59.540777: step 7225, loss 0.0565483, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:59.648986: step 7226, loss 0.0632165, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:59.753686: step 7227, loss 0.0647269, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:59.861484: step 7228, loss 0.109988, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:59.961134: step 7229, loss 0.101273, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:00.066300: step 7230, loss 0.0948149, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:00.170679: step 7231, loss 0.0761264, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:00.273743: step 7232, loss 0.0892789, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:00.375644: step 7233, loss 0.0527233, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:00.479838: step 7234, loss 0.04568, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:00.588111: step 7235, loss 0.106604, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:00.696369: step 7236, loss 0.171764, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:00.802952: step 7237, loss 0.120134, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:00.907471: step 7238, loss 0.0735408, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:01.012077: step 7239, loss 0.0857395, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:01.119270: step 7240, loss 0.0266936, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:01.373639: step 7240, loss 0.208178, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7240

2017-10-10T12:14:02.025714: step 7241, loss 0.0386064, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:02.132750: step 7242, loss 0.0958007, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:02.241478: step 7243, loss 0.0638987, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:02.345402: step 7244, loss 0.0517592, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:02.447958: step 7245, loss 0.109316, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:02.555232: step 7246, loss 0.0367544, acc 1, learning_rate 0.0001
2017-10-10T12:14:02.659434: step 7247, loss 0.0359932, acc 1, learning_rate 0.0001
2017-10-10T12:14:02.766987: step 7248, loss 0.0541363, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:02.870282: step 7249, loss 0.0465315, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:02.973752: step 7250, loss 0.057348, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:03.078570: step 7251, loss 0.0225365, acc 1, learning_rate 0.0001
2017-10-10T12:14:03.167001: step 7252, loss 0.146642, acc 0.941176, learning_rate 0.0001
2017-10-10T12:14:03.267479: step 7253, loss 0.127142, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:03.372764: step 7254, loss 0.0683422, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:03.478213: step 7255, loss 0.0681576, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:03.578797: step 7256, loss 0.0810831, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:03.681339: step 7257, loss 0.14123, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:03.790044: step 7258, loss 0.0405117, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:03.901251: step 7259, loss 0.0904618, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:04.004316: step 7260, loss 0.145381, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:04.109670: step 7261, loss 0.0550093, acc 1, learning_rate 0.0001
2017-10-10T12:14:04.215033: step 7262, loss 0.031771, acc 1, learning_rate 0.0001
2017-10-10T12:14:04.318927: step 7263, loss 0.0573314, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:04.421717: step 7264, loss 0.0108031, acc 1, learning_rate 0.0001
2017-10-10T12:14:04.524762: step 7265, loss 0.0681258, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:04.628906: step 7266, loss 0.0662671, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:04.731183: step 7267, loss 0.0390174, acc 1, learning_rate 0.0001
2017-10-10T12:14:04.834386: step 7268, loss 0.047354, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:04.939236: step 7269, loss 0.0834309, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:05.042553: step 7270, loss 0.0826437, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:05.148481: step 7271, loss 0.0750199, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:05.251616: step 7272, loss 0.0321305, acc 1, learning_rate 0.0001
2017-10-10T12:14:05.359142: step 7273, loss 0.0754073, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:05.463489: step 7274, loss 0.072367, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:05.569368: step 7275, loss 0.0894856, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:05.673160: step 7276, loss 0.0667253, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:05.779789: step 7277, loss 0.059781, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:05.888555: step 7278, loss 0.0894833, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:05.992301: step 7279, loss 0.0194771, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:06.096174: step 7280, loss 0.0984105, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:06.346423: step 7280, loss 0.206305, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7280

2017-10-10T12:14:06.863359: step 7281, loss 0.0415154, acc 1, learning_rate 0.0001
2017-10-10T12:14:06.962028: step 7282, loss 0.0727929, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:07.064179: step 7283, loss 0.034901, acc 1, learning_rate 0.0001
2017-10-10T12:14:07.165939: step 7284, loss 0.0506319, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:07.270588: step 7285, loss 0.0415641, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:07.374615: step 7286, loss 0.173996, acc 0.921875, learning_rate 0.0001
2017-10-10T12:14:07.479792: step 7287, loss 0.0882545, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:07.585261: step 7288, loss 0.0598818, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:07.690543: step 7289, loss 0.0513481, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:07.794546: step 7290, loss 0.045921, acc 1, learning_rate 0.0001
2017-10-10T12:14:07.903184: step 7291, loss 0.0883797, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:08.004205: step 7292, loss 0.150541, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:08.108455: step 7293, loss 0.111747, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:08.212214: step 7294, loss 0.0299278, acc 1, learning_rate 0.0001
2017-10-10T12:14:08.315334: step 7295, loss 0.0660995, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:08.419973: step 7296, loss 0.047152, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:08.519739: step 7297, loss 0.0751912, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:08.623707: step 7298, loss 0.0271133, acc 1, learning_rate 0.0001
2017-10-10T12:14:08.727673: step 7299, loss 0.040915, acc 1, learning_rate 0.0001
2017-10-10T12:14:08.832069: step 7300, loss 0.0698029, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:08.940187: step 7301, loss 0.124698, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:09.043062: step 7302, loss 0.0899656, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:09.148457: step 7303, loss 0.0844323, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:09.253101: step 7304, loss 0.0497936, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:09.360201: step 7305, loss 0.0577654, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:09.462090: step 7306, loss 0.140936, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:09.565384: step 7307, loss 0.0701888, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:09.667352: step 7308, loss 0.055863, acc 1, learning_rate 0.0001
2017-10-10T12:14:09.771259: step 7309, loss 0.0355209, acc 1, learning_rate 0.0001
2017-10-10T12:14:09.879004: step 7310, loss 0.0596532, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:09.986046: step 7311, loss 0.0367637, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:10.089472: step 7312, loss 0.0425779, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:10.193720: step 7313, loss 0.0722818, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:10.296906: step 7314, loss 0.141472, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:10.397087: step 7315, loss 0.102107, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:10.500588: step 7316, loss 0.0561235, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:10.607001: step 7317, loss 0.160986, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:10.708422: step 7318, loss 0.0663245, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:10.812571: step 7319, loss 0.0785487, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:10.918846: step 7320, loss 0.0538819, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:11.175561: step 7320, loss 0.207078, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7320

2017-10-10T12:14:11.749099: step 7321, loss 0.0343701, acc 1, learning_rate 0.0001
2017-10-10T12:14:11.854249: step 7322, loss 0.0673169, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:11.959444: step 7323, loss 0.0828707, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:12.069016: step 7324, loss 0.123213, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:12.175652: step 7325, loss 0.0382391, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:12.275705: step 7326, loss 0.0196134, acc 1, learning_rate 0.0001
2017-10-10T12:14:12.382214: step 7327, loss 0.11126, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:12.488278: step 7328, loss 0.0565162, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:12.594373: step 7329, loss 0.0331553, acc 1, learning_rate 0.0001
2017-10-10T12:14:12.698314: step 7330, loss 0.0267745, acc 1, learning_rate 0.0001
2017-10-10T12:14:12.804577: step 7331, loss 0.119946, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:12.910620: step 7332, loss 0.0986658, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:13.019793: step 7333, loss 0.0480641, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:13.122968: step 7334, loss 0.118259, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:13.231411: step 7335, loss 0.0435242, acc 1, learning_rate 0.0001
2017-10-10T12:14:13.341678: step 7336, loss 0.019021, acc 1, learning_rate 0.0001
2017-10-10T12:14:13.443617: step 7337, loss 0.0496062, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:13.550130: step 7338, loss 0.0883252, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:13.651478: step 7339, loss 0.120118, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:13.750869: step 7340, loss 0.0412531, acc 1, learning_rate 0.0001
2017-10-10T12:14:13.855020: step 7341, loss 0.104137, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:13.962172: step 7342, loss 0.157226, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:14.061497: step 7343, loss 0.0518591, acc 1, learning_rate 0.0001
2017-10-10T12:14:14.163065: step 7344, loss 0.0325387, acc 1, learning_rate 0.0001
2017-10-10T12:14:14.267709: step 7345, loss 0.0764403, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:14.376396: step 7346, loss 0.0464471, acc 1, learning_rate 0.0001
2017-10-10T12:14:14.478079: step 7347, loss 0.144262, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:14.583535: step 7348, loss 0.0283625, acc 1, learning_rate 0.0001
2017-10-10T12:14:14.689570: step 7349, loss 0.0453397, acc 1, learning_rate 0.0001
2017-10-10T12:14:14.780020: step 7350, loss 0.0653145, acc 0.960784, learning_rate 0.0001
2017-10-10T12:14:14.883698: step 7351, loss 0.0513278, acc 1, learning_rate 0.0001
2017-10-10T12:14:14.989692: step 7352, loss 0.120007, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:15.093948: step 7353, loss 0.0493274, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:15.199333: step 7354, loss 0.136064, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:15.306069: step 7355, loss 0.0593317, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:15.415683: step 7356, loss 0.0481616, acc 1, learning_rate 0.0001
2017-10-10T12:14:15.518722: step 7357, loss 0.0665045, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:15.623667: step 7358, loss 0.0603364, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:15.725664: step 7359, loss 0.169076, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:15.829203: step 7360, loss 0.013337, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:16.089843: step 7360, loss 0.20908, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7360

2017-10-10T12:14:16.737641: step 7361, loss 0.0761847, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:16.841670: step 7362, loss 0.0324703, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:16.947536: step 7363, loss 0.0383289, acc 1, learning_rate 0.0001
2017-10-10T12:14:17.050610: step 7364, loss 0.0774602, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:17.153846: step 7365, loss 0.0724521, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:17.260009: step 7366, loss 0.0870925, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:17.365582: step 7367, loss 0.0631763, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:17.466447: step 7368, loss 0.0760942, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:17.572343: step 7369, loss 0.0559328, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:17.678245: step 7370, loss 0.0458973, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:17.779186: step 7371, loss 0.132641, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:17.882973: step 7372, loss 0.0411041, acc 1, learning_rate 0.0001
2017-10-10T12:14:17.987731: step 7373, loss 0.0432013, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:18.092845: step 7374, loss 0.0528433, acc 1, learning_rate 0.0001
2017-10-10T12:14:18.194590: step 7375, loss 0.0307081, acc 1, learning_rate 0.0001
2017-10-10T12:14:18.295794: step 7376, loss 0.068549, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:18.398378: step 7377, loss 0.108328, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:18.504553: step 7378, loss 0.106795, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:18.612442: step 7379, loss 0.0569967, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:18.716163: step 7380, loss 0.0290275, acc 1, learning_rate 0.0001
2017-10-10T12:14:18.813910: step 7381, loss 0.0733519, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:18.916367: step 7382, loss 0.0442024, acc 1, learning_rate 0.0001
2017-10-10T12:14:19.016128: step 7383, loss 0.0924977, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:19.119570: step 7384, loss 0.0586662, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:19.222752: step 7385, loss 0.0136119, acc 1, learning_rate 0.0001
2017-10-10T12:14:19.327308: step 7386, loss 0.0744413, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:19.429667: step 7387, loss 0.0975074, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:19.533598: step 7388, loss 0.0618761, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:19.639441: step 7389, loss 0.0271786, acc 1, learning_rate 0.0001
2017-10-10T12:14:19.741686: step 7390, loss 0.0545048, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:19.849120: step 7391, loss 0.0490238, acc 1, learning_rate 0.0001
2017-10-10T12:14:19.952292: step 7392, loss 0.170776, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:20.056030: step 7393, loss 0.0426972, acc 1, learning_rate 0.0001
2017-10-10T12:14:20.160145: step 7394, loss 0.111756, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:20.265022: step 7395, loss 0.0669843, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:20.369398: step 7396, loss 0.0839159, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:20.471428: step 7397, loss 0.181664, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:20.574535: step 7398, loss 0.224957, acc 0.859375, learning_rate 0.0001
2017-10-10T12:14:20.678403: step 7399, loss 0.108134, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:20.780681: step 7400, loss 0.0985274, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:21.030486: step 7400, loss 0.207848, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7400

2017-10-10T12:14:21.552344: step 7401, loss 0.0278744, acc 1, learning_rate 0.0001
2017-10-10T12:14:21.659471: step 7402, loss 0.0549367, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:21.763673: step 7403, loss 0.0716405, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:21.863691: step 7404, loss 0.0271336, acc 1, learning_rate 0.0001
2017-10-10T12:14:21.970946: step 7405, loss 0.0849154, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:22.077197: step 7406, loss 0.0949003, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:22.180080: step 7407, loss 0.0666956, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:22.284703: step 7408, loss 0.021278, acc 1, learning_rate 0.0001
2017-10-10T12:14:22.388909: step 7409, loss 0.0328715, acc 1, learning_rate 0.0001
2017-10-10T12:14:22.492111: step 7410, loss 0.062369, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:22.599305: step 7411, loss 0.0920735, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:22.702070: step 7412, loss 0.141212, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:22.802960: step 7413, loss 0.0533131, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:22.902253: step 7414, loss 0.114986, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:23.005743: step 7415, loss 0.0993085, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:23.112501: step 7416, loss 0.080971, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:23.219711: step 7417, loss 0.0458436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:23.319826: step 7418, loss 0.0309625, acc 1, learning_rate 0.0001
2017-10-10T12:14:23.423721: step 7419, loss 0.0542406, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:23.530386: step 7420, loss 0.0311153, acc 1, learning_rate 0.0001
2017-10-10T12:14:23.636694: step 7421, loss 0.0872166, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:23.739834: step 7422, loss 0.0456169, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:23.839838: step 7423, loss 0.107403, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:23.943909: step 7424, loss 0.0402584, acc 1, learning_rate 0.0001
2017-10-10T12:14:24.045625: step 7425, loss 0.0361744, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:24.150354: step 7426, loss 0.0923233, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:24.251533: step 7427, loss 0.0396281, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:24.355275: step 7428, loss 0.0535545, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:24.461252: step 7429, loss 0.0829555, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:24.563041: step 7430, loss 0.0215325, acc 1, learning_rate 0.0001
2017-10-10T12:14:24.671096: step 7431, loss 0.0553164, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:24.771344: step 7432, loss 0.0401345, acc 1, learning_rate 0.0001
2017-10-10T12:14:24.875538: step 7433, loss 0.0696413, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:24.977538: step 7434, loss 0.130755, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:25.083372: step 7435, loss 0.0359004, acc 1, learning_rate 0.0001
2017-10-10T12:14:25.186317: step 7436, loss 0.114786, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:25.292354: step 7437, loss 0.07002, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:25.393557: step 7438, loss 0.0525617, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:25.497030: step 7439, loss 0.0558734, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:25.603029: step 7440, loss 0.0800097, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:25.859921: step 7440, loss 0.207404, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7440

2017-10-10T12:14:26.435471: step 7441, loss 0.0207782, acc 1, learning_rate 0.0001
2017-10-10T12:14:26.540389: step 7442, loss 0.0294127, acc 1, learning_rate 0.0001
2017-10-10T12:14:26.645134: step 7443, loss 0.0554737, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:26.750405: step 7444, loss 0.108285, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:26.855829: step 7445, loss 0.0315887, acc 1, learning_rate 0.0001
2017-10-10T12:14:26.961881: step 7446, loss 0.0242256, acc 1, learning_rate 0.0001
2017-10-10T12:14:27.065522: step 7447, loss 0.025991, acc 1, learning_rate 0.0001
2017-10-10T12:14:27.151004: step 7448, loss 0.0881932, acc 0.941176, learning_rate 0.0001
2017-10-10T12:14:27.257818: step 7449, loss 0.0274609, acc 1, learning_rate 0.0001
2017-10-10T12:14:27.359191: step 7450, loss 0.0808063, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:27.465671: step 7451, loss 0.0261566, acc 1, learning_rate 0.0001
2017-10-10T12:14:27.569904: step 7452, loss 0.056361, acc 1, learning_rate 0.0001
2017-10-10T12:14:27.675918: step 7453, loss 0.074378, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:27.781961: step 7454, loss 0.0677747, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:27.887801: step 7455, loss 0.142874, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:27.991993: step 7456, loss 0.0994065, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:28.097923: step 7457, loss 0.061189, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:28.198156: step 7458, loss 0.0912296, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:28.304031: step 7459, loss 0.0533553, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:28.410589: step 7460, loss 0.0117152, acc 1, learning_rate 0.0001
2017-10-10T12:14:28.513159: step 7461, loss 0.0840795, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:28.619255: step 7462, loss 0.0423788, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:28.722416: step 7463, loss 0.0831663, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:28.825991: step 7464, loss 0.0481299, acc 1, learning_rate 0.0001
2017-10-10T12:14:28.929798: step 7465, loss 0.10467, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:29.032595: step 7466, loss 0.0220822, acc 1, learning_rate 0.0001
2017-10-10T12:14:29.138198: step 7467, loss 0.129383, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:29.242645: step 7468, loss 0.0758679, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:29.350744: step 7469, loss 0.0382451, acc 1, learning_rate 0.0001
2017-10-10T12:14:29.454145: step 7470, loss 0.0356173, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:29.552551: step 7471, loss 0.0366857, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:29.658472: step 7472, loss 0.0485311, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:29.762086: step 7473, loss 0.0740251, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:29.864969: step 7474, loss 0.0899718, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:29.967672: step 7475, loss 0.0342457, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:30.068636: step 7476, loss 0.0488729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:30.168863: step 7477, loss 0.0401616, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:30.271918: step 7478, loss 0.0765667, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:30.372541: step 7479, loss 0.0373941, acc 1, learning_rate 0.0001
2017-10-10T12:14:30.476503: step 7480, loss 0.0676035, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:30.737012: step 7480, loss 0.206036, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7480

2017-10-10T12:14:31.372632: step 7481, loss 0.112882, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:31.475968: step 7482, loss 0.0559231, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:31.583850: step 7483, loss 0.0373039, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:31.691239: step 7484, loss 0.0369972, acc 1, learning_rate 0.0001
2017-10-10T12:14:31.795799: step 7485, loss 0.0473115, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:31.907834: step 7486, loss 0.0310598, acc 1, learning_rate 0.0001
2017-10-10T12:14:32.012007: step 7487, loss 0.0423623, acc 1, learning_rate 0.0001
2017-10-10T12:14:32.117511: step 7488, loss 0.0788776, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:32.223599: step 7489, loss 0.0265392, acc 1, learning_rate 0.0001
2017-10-10T12:14:32.321512: step 7490, loss 0.0276295, acc 1, learning_rate 0.0001
2017-10-10T12:14:32.422747: step 7491, loss 0.0568471, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:32.526712: step 7492, loss 0.139686, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:32.629345: step 7493, loss 0.0414749, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:32.732802: step 7494, loss 0.047998, acc 1, learning_rate 0.0001
2017-10-10T12:14:32.832990: step 7495, loss 0.0459819, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:32.939966: step 7496, loss 0.0283766, acc 1, learning_rate 0.0001
2017-10-10T12:14:33.043320: step 7497, loss 0.050552, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:33.142878: step 7498, loss 0.0986706, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:33.244135: step 7499, loss 0.048279, acc 1, learning_rate 0.0001
2017-10-10T12:14:33.346496: step 7500, loss 0.0490204, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:33.451318: step 7501, loss 0.119491, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:33.555104: step 7502, loss 0.0754306, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:33.660520: step 7503, loss 0.144115, acc 0.921875, learning_rate 0.0001
2017-10-10T12:14:33.766421: step 7504, loss 0.060763, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:33.869660: step 7505, loss 0.027379, acc 1, learning_rate 0.0001
2017-10-10T12:14:33.973596: step 7506, loss 0.0480123, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:34.074900: step 7507, loss 0.0773773, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:34.181575: step 7508, loss 0.11324, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:34.287185: step 7509, loss 0.0744574, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:34.392176: step 7510, loss 0.0952756, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:34.494408: step 7511, loss 0.140813, acc 0.921875, learning_rate 0.0001
2017-10-10T12:14:34.597651: step 7512, loss 0.0562664, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:34.699701: step 7513, loss 0.0245616, acc 1, learning_rate 0.0001
2017-10-10T12:14:34.802680: step 7514, loss 0.0419723, acc 1, learning_rate 0.0001
2017-10-10T12:14:34.903025: step 7515, loss 0.029043, acc 1, learning_rate 0.0001
2017-10-10T12:14:35.007284: step 7516, loss 0.0650791, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:35.107245: step 7517, loss 0.104808, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:35.212186: step 7518, loss 0.0338822, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:35.319239: step 7519, loss 0.0444623, acc 1, learning_rate 0.0001
2017-10-10T12:14:35.423382: step 7520, loss 0.103714, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:35.673137: step 7520, loss 0.207192, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7520

2017-10-10T12:14:36.183431: step 7521, loss 0.032105, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:36.286482: step 7522, loss 0.128971, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:36.388803: step 7523, loss 0.0599253, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:36.494177: step 7524, loss 0.0561439, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:36.592544: step 7525, loss 0.0687553, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:36.695070: step 7526, loss 0.0374806, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:36.795871: step 7527, loss 0.0586878, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:36.900357: step 7528, loss 0.0610906, acc 1, learning_rate 0.0001
2017-10-10T12:14:37.006436: step 7529, loss 0.0647775, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:37.110792: step 7530, loss 0.0881015, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:37.213751: step 7531, loss 0.110399, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:37.318057: step 7532, loss 0.0629506, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:37.422876: step 7533, loss 0.0304454, acc 1, learning_rate 0.0001
2017-10-10T12:14:37.521414: step 7534, loss 0.074761, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:37.628291: step 7535, loss 0.112047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:37.737103: step 7536, loss 0.0705963, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:37.842045: step 7537, loss 0.0800795, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:37.948765: step 7538, loss 0.100494, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:38.053512: step 7539, loss 0.0302732, acc 1, learning_rate 0.0001
2017-10-10T12:14:38.153748: step 7540, loss 0.044586, acc 1, learning_rate 0.0001
2017-10-10T12:14:38.259424: step 7541, loss 0.060233, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:38.360079: step 7542, loss 0.0857744, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:38.464378: step 7543, loss 0.0868755, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:38.568500: step 7544, loss 0.0563035, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:38.676256: step 7545, loss 0.100711, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:38.764100: step 7546, loss 0.0575989, acc 0.980392, learning_rate 0.0001
2017-10-10T12:14:38.869900: step 7547, loss 0.0842654, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:38.972702: step 7548, loss 0.0945379, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:39.075417: step 7549, loss 0.0819072, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:39.178591: step 7550, loss 0.0537053, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:39.285021: step 7551, loss 0.0325741, acc 1, learning_rate 0.0001
2017-10-10T12:14:39.390909: step 7552, loss 0.0556056, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:39.496567: step 7553, loss 0.0683305, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:39.601427: step 7554, loss 0.0501808, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:39.705759: step 7555, loss 0.057208, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:39.811011: step 7556, loss 0.0255474, acc 1, learning_rate 0.0001
2017-10-10T12:14:39.914340: step 7557, loss 0.142541, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:40.018979: step 7558, loss 0.0632807, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:40.125636: step 7559, loss 0.083128, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:40.239638: step 7560, loss 0.0471786, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:40.492011: step 7560, loss 0.205736, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7560

2017-10-10T12:14:41.066070: step 7561, loss 0.103377, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:41.169733: step 7562, loss 0.0571336, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:41.275046: step 7563, loss 0.0869508, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:41.376668: step 7564, loss 0.0652987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:41.475659: step 7565, loss 0.0840514, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:41.578004: step 7566, loss 0.0764121, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:41.679679: step 7567, loss 0.0798749, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:41.776251: step 7568, loss 0.0439391, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:41.883211: step 7569, loss 0.0508527, acc 1, learning_rate 0.0001
2017-10-10T12:14:41.986594: step 7570, loss 0.0702023, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:42.090970: step 7571, loss 0.0802124, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:42.190513: step 7572, loss 0.0474688, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:42.293523: step 7573, loss 0.0544076, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:42.398344: step 7574, loss 0.0362097, acc 1, learning_rate 0.0001
2017-10-10T12:14:42.501894: step 7575, loss 0.0212476, acc 1, learning_rate 0.0001
2017-10-10T12:14:42.606739: step 7576, loss 0.0661641, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:42.708756: step 7577, loss 0.0458518, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:42.807610: step 7578, loss 0.0377619, acc 1, learning_rate 0.0001
2017-10-10T12:14:42.914741: step 7579, loss 0.0839124, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:43.017996: step 7580, loss 0.0348576, acc 1, learning_rate 0.0001
2017-10-10T12:14:43.122134: step 7581, loss 0.0279481, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:43.227773: step 7582, loss 0.0113608, acc 1, learning_rate 0.0001
2017-10-10T12:14:43.326229: step 7583, loss 0.0429162, acc 1, learning_rate 0.0001
2017-10-10T12:14:43.428779: step 7584, loss 0.0642419, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:43.534818: step 7585, loss 0.0843116, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:43.637356: step 7586, loss 0.0589205, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:43.741436: step 7587, loss 0.0284503, acc 1, learning_rate 0.0001
2017-10-10T12:14:43.846405: step 7588, loss 0.0323013, acc 1, learning_rate 0.0001
2017-10-10T12:14:43.960977: step 7589, loss 0.049268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:44.064826: step 7590, loss 0.0642409, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:44.168828: step 7591, loss 0.0356544, acc 1, learning_rate 0.0001
2017-10-10T12:14:44.274922: step 7592, loss 0.102504, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:44.379632: step 7593, loss 0.076656, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:44.478832: step 7594, loss 0.0433632, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:44.586707: step 7595, loss 0.0952376, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:44.690964: step 7596, loss 0.0919869, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:44.799411: step 7597, loss 0.074244, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:44.905333: step 7598, loss 0.0996795, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:45.014507: step 7599, loss 0.0390934, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:45.117963: step 7600, loss 0.0146869, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:45.370236: step 7600, loss 0.208835, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7600

2017-10-10T12:14:45.937872: step 7601, loss 0.0699756, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:46.039538: step 7602, loss 0.140059, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:46.138428: step 7603, loss 0.0373548, acc 1, learning_rate 0.0001
2017-10-10T12:14:46.240329: step 7604, loss 0.119631, acc 0.921875, learning_rate 0.0001
2017-10-10T12:14:46.345602: step 7605, loss 0.0332814, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:46.447000: step 7606, loss 0.101846, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:46.547214: step 7607, loss 0.0437216, acc 1, learning_rate 0.0001
2017-10-10T12:14:46.649604: step 7608, loss 0.0357603, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:46.751776: step 7609, loss 0.078475, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:46.856242: step 7610, loss 0.104111, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:46.955194: step 7611, loss 0.0623104, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:47.053609: step 7612, loss 0.0395984, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:47.157550: step 7613, loss 0.0617268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:47.260706: step 7614, loss 0.0501324, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:47.364501: step 7615, loss 0.0516483, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:47.469364: step 7616, loss 0.0443891, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:47.574349: step 7617, loss 0.0152521, acc 1, learning_rate 0.0001
2017-10-10T12:14:47.679390: step 7618, loss 0.169901, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:47.780893: step 7619, loss 0.0572826, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:47.886168: step 7620, loss 0.0348756, acc 1, learning_rate 0.0001
2017-10-10T12:14:47.990572: step 7621, loss 0.0624779, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:48.092794: step 7622, loss 0.0758861, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:48.199568: step 7623, loss 0.141124, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:48.303640: step 7624, loss 0.0675316, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:48.406026: step 7625, loss 0.0281864, acc 1, learning_rate 0.0001
2017-10-10T12:14:48.511494: step 7626, loss 0.0849486, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:48.617119: step 7627, loss 0.0967127, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:48.724703: step 7628, loss 0.0593051, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:48.827793: step 7629, loss 0.0389995, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:48.936758: step 7630, loss 0.0301336, acc 1, learning_rate 0.0001
2017-10-10T12:14:49.038970: step 7631, loss 0.0703399, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:49.140340: step 7632, loss 0.069209, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:49.243600: step 7633, loss 0.0877614, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:49.343996: step 7634, loss 0.0433547, acc 1, learning_rate 0.0001
2017-10-10T12:14:49.444090: step 7635, loss 0.110417, acc 0.9375, learning_rate 0.0001
2017-10-10T12:14:49.546865: step 7636, loss 0.0739143, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:49.651495: step 7637, loss 0.0541645, acc 1, learning_rate 0.0001
2017-10-10T12:14:49.757282: step 7638, loss 0.0527844, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:49.864145: step 7639, loss 0.0673312, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:49.967940: step 7640, loss 0.0400418, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:50.226050: step 7640, loss 0.210007, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7640

2017-10-10T12:14:50.868210: step 7641, loss 0.0541223, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:50.975323: step 7642, loss 0.0315474, acc 1, learning_rate 0.0001
2017-10-10T12:14:51.080042: step 7643, loss 0.0334214, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:51.168397: step 7644, loss 0.0701345, acc 0.980392, learning_rate 0.0001
2017-10-10T12:14:51.271917: step 7645, loss 0.0500421, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:51.377254: step 7646, loss 0.104431, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:51.482232: step 7647, loss 0.0944242, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:51.589216: step 7648, loss 0.0227125, acc 1, learning_rate 0.0001
2017-10-10T12:14:51.693011: step 7649, loss 0.12768, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:51.796369: step 7650, loss 0.0405365, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:51.900473: step 7651, loss 0.0625707, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:52.006351: step 7652, loss 0.0573456, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:52.107482: step 7653, loss 0.0247, acc 1, learning_rate 0.0001
2017-10-10T12:14:52.213738: step 7654, loss 0.0668432, acc 1, learning_rate 0.0001
2017-10-10T12:14:52.318401: step 7655, loss 0.0389485, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:52.421046: step 7656, loss 0.0382569, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:52.524258: step 7657, loss 0.0579869, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:52.629874: step 7658, loss 0.0613707, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:52.733288: step 7659, loss 0.072639, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:52.831687: step 7660, loss 0.0121269, acc 1, learning_rate 0.0001
2017-10-10T12:14:52.939521: step 7661, loss 0.0660796, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:53.042451: step 7662, loss 0.0275898, acc 1, learning_rate 0.0001
2017-10-10T12:14:53.148921: step 7663, loss 0.084569, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:53.261119: step 7664, loss 0.0768364, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:53.364963: step 7665, loss 0.0967887, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:53.465561: step 7666, loss 0.0190285, acc 1, learning_rate 0.0001
2017-10-10T12:14:53.570380: step 7667, loss 0.028303, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:53.673120: step 7668, loss 0.0509905, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:53.778754: step 7669, loss 0.0860252, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:53.882307: step 7670, loss 0.0426497, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:53.985913: step 7671, loss 0.0675447, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:54.094283: step 7672, loss 0.108774, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:54.198615: step 7673, loss 0.0843697, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:54.303385: step 7674, loss 0.017476, acc 1, learning_rate 0.0001
2017-10-10T12:14:54.408096: step 7675, loss 0.0302315, acc 1, learning_rate 0.0001
2017-10-10T12:14:54.509522: step 7676, loss 0.0769828, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:54.612688: step 7677, loss 0.0718752, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:54.717890: step 7678, loss 0.0558559, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:54.823862: step 7679, loss 0.0972614, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:54.936368: step 7680, loss 0.022563, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:14:55.195055: step 7680, loss 0.207013, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7680

2017-10-10T12:14:55.703786: step 7681, loss 0.0862027, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:55.808774: step 7682, loss 0.0940182, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:55.913662: step 7683, loss 0.0595194, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:56.018897: step 7684, loss 0.0791214, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:56.131317: step 7685, loss 0.0496239, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:56.234176: step 7686, loss 0.0460373, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:56.337391: step 7687, loss 0.0239567, acc 1, learning_rate 0.0001
2017-10-10T12:14:56.439529: step 7688, loss 0.0258873, acc 1, learning_rate 0.0001
2017-10-10T12:14:56.544183: step 7689, loss 0.102371, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:56.651064: step 7690, loss 0.0584835, acc 1, learning_rate 0.0001
2017-10-10T12:14:56.753415: step 7691, loss 0.112393, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:56.857093: step 7692, loss 0.0308682, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:56.962693: step 7693, loss 0.0645376, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:57.065281: step 7694, loss 0.100648, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:57.176112: step 7695, loss 0.120108, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:57.281990: step 7696, loss 0.0491938, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:57.386185: step 7697, loss 0.029339, acc 1, learning_rate 0.0001
2017-10-10T12:14:57.491366: step 7698, loss 0.0469426, acc 1, learning_rate 0.0001
2017-10-10T12:14:57.595568: step 7699, loss 0.137312, acc 0.953125, learning_rate 0.0001
2017-10-10T12:14:57.694452: step 7700, loss 0.0784886, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:57.796977: step 7701, loss 0.0677043, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:57.904072: step 7702, loss 0.0586519, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:58.008012: step 7703, loss 0.0589856, acc 1, learning_rate 0.0001
2017-10-10T12:14:58.112392: step 7704, loss 0.10058, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:58.215175: step 7705, loss 0.19018, acc 0.921875, learning_rate 0.0001
2017-10-10T12:14:58.320287: step 7706, loss 0.0473589, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:58.424855: step 7707, loss 0.0482013, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:58.527332: step 7708, loss 0.0917652, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:58.627352: step 7709, loss 0.0474449, acc 1, learning_rate 0.0001
2017-10-10T12:14:58.731474: step 7710, loss 0.0719483, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:58.838415: step 7711, loss 0.0510565, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:58.945071: step 7712, loss 0.0536068, acc 1, learning_rate 0.0001
2017-10-10T12:14:59.054964: step 7713, loss 0.0324254, acc 1, learning_rate 0.0001
2017-10-10T12:14:59.154668: step 7714, loss 0.0844769, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:59.259262: step 7715, loss 0.0726625, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:59.365351: step 7716, loss 0.0261509, acc 1, learning_rate 0.0001
2017-10-10T12:14:59.466903: step 7717, loss 0.0230029, acc 0.984375, learning_rate 0.0001
2017-10-10T12:14:59.571967: step 7718, loss 0.0311497, acc 1, learning_rate 0.0001
2017-10-10T12:14:59.676389: step 7719, loss 0.0710722, acc 0.96875, learning_rate 0.0001
2017-10-10T12:14:59.779070: step 7720, loss 0.0353692, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:15:00.036501: step 7720, loss 0.206837, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7720

2017-10-10T12:15:00.607113: step 7721, loss 0.0881297, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:00.715749: step 7722, loss 0.0257116, acc 1, learning_rate 0.0001
2017-10-10T12:15:00.818465: step 7723, loss 0.0327392, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:00.931574: step 7724, loss 0.0943509, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:01.038122: step 7725, loss 0.0356289, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:01.140662: step 7726, loss 0.0697124, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:01.244443: step 7727, loss 0.148952, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:01.346830: step 7728, loss 0.0543389, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:01.450138: step 7729, loss 0.0178981, acc 1, learning_rate 0.0001
2017-10-10T12:15:01.554321: step 7730, loss 0.0262366, acc 1, learning_rate 0.0001
2017-10-10T12:15:01.658532: step 7731, loss 0.179049, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:01.764036: step 7732, loss 0.0884558, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:01.867632: step 7733, loss 0.0610261, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:01.971811: step 7734, loss 0.149444, acc 0.921875, learning_rate 0.0001
2017-10-10T12:15:02.077356: step 7735, loss 0.123424, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:02.183625: step 7736, loss 0.132914, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:02.285860: step 7737, loss 0.0212974, acc 1, learning_rate 0.0001
2017-10-10T12:15:02.390555: step 7738, loss 0.0252844, acc 1, learning_rate 0.0001
2017-10-10T12:15:02.493918: step 7739, loss 0.0258954, acc 1, learning_rate 0.0001
2017-10-10T12:15:02.597826: step 7740, loss 0.143268, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:02.702433: step 7741, loss 0.0233765, acc 1, learning_rate 0.0001
2017-10-10T12:15:02.790538: step 7742, loss 0.103068, acc 0.941176, learning_rate 0.0001
2017-10-10T12:15:02.896822: step 7743, loss 0.0565696, acc 1, learning_rate 0.0001
2017-10-10T12:15:02.997395: step 7744, loss 0.0632899, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:03.103270: step 7745, loss 0.0646145, acc 1, learning_rate 0.0001
2017-10-10T12:15:03.205257: step 7746, loss 0.0452404, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:03.304567: step 7747, loss 0.0671623, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:03.404871: step 7748, loss 0.0530661, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:03.507324: step 7749, loss 0.102087, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:03.611398: step 7750, loss 0.0559342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:03.709450: step 7751, loss 0.0512845, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:03.808137: step 7752, loss 0.0393023, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:03.913805: step 7753, loss 0.101241, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:04.022003: step 7754, loss 0.0556092, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:04.128630: step 7755, loss 0.0595225, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:04.230973: step 7756, loss 0.0968586, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:04.332672: step 7757, loss 0.133966, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:04.442388: step 7758, loss 0.0593074, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:04.548154: step 7759, loss 0.0330658, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:04.653294: step 7760, loss 0.0382497, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:15:04.913933: step 7760, loss 0.206706, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7760

2017-10-10T12:15:05.545066: step 7761, loss 0.0426437, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:05.650309: step 7762, loss 0.032431, acc 1, learning_rate 0.0001
2017-10-10T12:15:05.756872: step 7763, loss 0.0493164, acc 1, learning_rate 0.0001
2017-10-10T12:15:05.861773: step 7764, loss 0.0536247, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:05.966940: step 7765, loss 0.0662616, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:06.068540: step 7766, loss 0.083872, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:06.170172: step 7767, loss 0.0903471, acc 0.9375, learning_rate 0.0001
2017-10-10T12:15:06.275714: step 7768, loss 0.0392523, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:06.379325: step 7769, loss 0.0810632, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:06.486304: step 7770, loss 0.0986871, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:06.592583: step 7771, loss 0.0397782, acc 1, learning_rate 0.0001
2017-10-10T12:15:06.698470: step 7772, loss 0.0393596, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:06.804783: step 7773, loss 0.0917886, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:06.908943: step 7774, loss 0.0793021, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:07.017842: step 7775, loss 0.111242, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:07.121953: step 7776, loss 0.0501651, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:07.224023: step 7777, loss 0.103327, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:07.325688: step 7778, loss 0.0583208, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:07.432900: step 7779, loss 0.0750054, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:07.539314: step 7780, loss 0.105781, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:07.643966: step 7781, loss 0.0597245, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:07.748038: step 7782, loss 0.0512229, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:07.856369: step 7783, loss 0.0326486, acc 1, learning_rate 0.0001
2017-10-10T12:15:07.958668: step 7784, loss 0.0362921, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:08.063585: step 7785, loss 0.0224844, acc 1, learning_rate 0.0001
2017-10-10T12:15:08.164566: step 7786, loss 0.0967002, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:08.267545: step 7787, loss 0.0472849, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:08.372539: step 7788, loss 0.0831475, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:08.478707: step 7789, loss 0.0220041, acc 1, learning_rate 0.0001
2017-10-10T12:15:08.585543: step 7790, loss 0.0719585, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:08.691527: step 7791, loss 0.114723, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:08.798577: step 7792, loss 0.0814399, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:08.903819: step 7793, loss 0.0338588, acc 1, learning_rate 0.0001
2017-10-10T12:15:09.004934: step 7794, loss 0.0794877, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:09.108792: step 7795, loss 0.0321689, acc 1, learning_rate 0.0001
2017-10-10T12:15:09.216721: step 7796, loss 0.097825, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:09.322371: step 7797, loss 0.0371328, acc 1, learning_rate 0.0001
2017-10-10T12:15:09.426808: step 7798, loss 0.0412352, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:09.530414: step 7799, loss 0.0494278, acc 1, learning_rate 0.0001
2017-10-10T12:15:09.631143: step 7800, loss 0.150268, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:15:09.884038: step 7800, loss 0.206719, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7800

2017-10-10T12:15:10.384096: step 7801, loss 0.0456429, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:10.483402: step 7802, loss 0.0355594, acc 1, learning_rate 0.0001
2017-10-10T12:15:10.587340: step 7803, loss 0.0666248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:10.695879: step 7804, loss 0.0332466, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:10.801112: step 7805, loss 0.0367723, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:10.908574: step 7806, loss 0.0600425, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:11.014467: step 7807, loss 0.0620722, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:11.122449: step 7808, loss 0.0279349, acc 1, learning_rate 0.0001
2017-10-10T12:15:11.228688: step 7809, loss 0.0493052, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:11.327357: step 7810, loss 0.0390814, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:11.431957: step 7811, loss 0.13947, acc 0.9375, learning_rate 0.0001
2017-10-10T12:15:11.532427: step 7812, loss 0.0520284, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:11.639870: step 7813, loss 0.0328979, acc 1, learning_rate 0.0001
2017-10-10T12:15:11.741138: step 7814, loss 0.0278396, acc 1, learning_rate 0.0001
2017-10-10T12:15:11.847208: step 7815, loss 0.0358848, acc 1, learning_rate 0.0001
2017-10-10T12:15:11.956161: step 7816, loss 0.0406444, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:12.059315: step 7817, loss 0.0453474, acc 1, learning_rate 0.0001
2017-10-10T12:15:12.163287: step 7818, loss 0.0628628, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:12.266515: step 7819, loss 0.136015, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:12.367723: step 7820, loss 0.138139, acc 0.921875, learning_rate 0.0001
2017-10-10T12:15:12.476471: step 7821, loss 0.0421016, acc 1, learning_rate 0.0001
2017-10-10T12:15:12.580887: step 7822, loss 0.0461667, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:12.686648: step 7823, loss 0.0416972, acc 1, learning_rate 0.0001
2017-10-10T12:15:12.790422: step 7824, loss 0.0698603, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:12.899358: step 7825, loss 0.0488768, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:13.007101: step 7826, loss 0.056367, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:13.107432: step 7827, loss 0.0514589, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:13.211701: step 7828, loss 0.0698946, acc 0.953125, learning_rate 0.0001
2017-10-10T12:15:13.313321: step 7829, loss 0.0256588, acc 1, learning_rate 0.0001
2017-10-10T12:15:13.416189: step 7830, loss 0.0236267, acc 1, learning_rate 0.0001
2017-10-10T12:15:13.517854: step 7831, loss 0.0670238, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:13.622995: step 7832, loss 0.076516, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:13.725782: step 7833, loss 0.0289792, acc 1, learning_rate 0.0001
2017-10-10T12:15:13.829854: step 7834, loss 0.0535725, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:13.933017: step 7835, loss 0.062258, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:14.036165: step 7836, loss 0.0552192, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:14.141305: step 7837, loss 0.0676419, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:14.246491: step 7838, loss 0.0830426, acc 0.96875, learning_rate 0.0001
2017-10-10T12:15:14.346816: step 7839, loss 0.0740388, acc 0.984375, learning_rate 0.0001
2017-10-10T12:15:14.435833: step 7840, loss 0.128963, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-10T12:15:14.698794: step 7840, loss 0.206947, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653779/checkpoints/model-7840

