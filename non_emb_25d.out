
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6954
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
Vocabulary Size: 46116
Train/Dev split: 6259/695
Writing to /home/sheep/bigdata/runs/1506700537

2017-09-29T10:55:39.948086: step 1, loss 4.04335, acc 0.15625, learning_rate 0.005
2017-09-29T10:55:40.174362: step 2, loss 3.21589, acc 0.296875, learning_rate 0.00498
2017-09-29T10:55:40.361685: step 3, loss 3.1955, acc 0.234375, learning_rate 0.00496008
2017-09-29T10:55:40.544972: step 4, loss 3.45499, acc 0.28125, learning_rate 0.00494024
2017-09-29T10:55:40.751861: step 5, loss 3.25968, acc 0.265625, learning_rate 0.00492049
2017-09-29T10:55:40.944515: step 6, loss 3.19811, acc 0.28125, learning_rate 0.00490081
2017-09-29T10:55:41.137705: step 7, loss 2.21109, acc 0.265625, learning_rate 0.00488121
2017-09-29T10:55:41.331671: step 8, loss 2.51913, acc 0.296875, learning_rate 0.0048617
2017-09-29T10:55:41.523542: step 9, loss 2.2562, acc 0.4375, learning_rate 0.00484226
2017-09-29T10:55:41.704999: step 10, loss 2.58672, acc 0.3125, learning_rate 0.00482291
2017-09-29T10:55:41.887439: step 11, loss 2.67025, acc 0.25, learning_rate 0.00480363
2017-09-29T10:55:42.071341: step 12, loss 2.71252, acc 0.296875, learning_rate 0.00478443
2017-09-29T10:55:42.262349: step 13, loss 2.45347, acc 0.40625, learning_rate 0.00476531
2017-09-29T10:55:42.447135: step 14, loss 2.59983, acc 0.296875, learning_rate 0.00474627
2017-09-29T10:55:42.630533: step 15, loss 2.72349, acc 0.21875, learning_rate 0.0047273
2017-09-29T10:55:42.823752: step 16, loss 2.70831, acc 0.25, learning_rate 0.00470841
2017-09-29T10:55:43.007442: step 17, loss 2.26691, acc 0.171875, learning_rate 0.0046896
2017-09-29T10:55:43.189097: step 18, loss 2.30271, acc 0.328125, learning_rate 0.00467087
2017-09-29T10:55:43.374928: step 19, loss 2.21687, acc 0.265625, learning_rate 0.00465221
2017-09-29T10:55:43.558566: step 20, loss 2.05073, acc 0.390625, learning_rate 0.00463363
2017-09-29T10:55:43.743545: step 21, loss 2.30171, acc 0.328125, learning_rate 0.00461513
2017-09-29T10:55:43.923420: step 22, loss 2.22708, acc 0.3125, learning_rate 0.0045967
2017-09-29T10:55:44.117131: step 23, loss 2.38867, acc 0.34375, learning_rate 0.00457834
2017-09-29T10:55:44.304575: step 24, loss 1.88477, acc 0.46875, learning_rate 0.00456006
2017-09-29T10:55:44.503139: step 25, loss 2.3228, acc 0.3125, learning_rate 0.00454186
2017-09-29T10:55:44.703647: step 26, loss 2.69521, acc 0.28125, learning_rate 0.00452373
2017-09-29T10:55:44.895496: step 27, loss 2.4174, acc 0.21875, learning_rate 0.00450567
2017-09-29T10:55:45.082218: step 28, loss 2.19232, acc 0.265625, learning_rate 0.00448769
2017-09-29T10:55:45.263610: step 29, loss 2.56634, acc 0.1875, learning_rate 0.00446978
2017-09-29T10:55:45.459191: step 30, loss 2.42711, acc 0.3125, learning_rate 0.00445194
2017-09-29T10:55:45.648242: step 31, loss 2.33579, acc 0.296875, learning_rate 0.00443418
2017-09-29T10:55:45.850285: step 32, loss 1.60028, acc 0.40625, learning_rate 0.00441649
2017-09-29T10:55:46.037049: step 33, loss 2.01172, acc 0.328125, learning_rate 0.00439887
2017-09-29T10:55:46.234282: step 34, loss 1.76326, acc 0.3125, learning_rate 0.00438132
2017-09-29T10:55:46.447570: step 35, loss 2.06099, acc 0.359375, learning_rate 0.00436385
2017-09-29T10:55:46.646837: step 36, loss 1.96049, acc 0.375, learning_rate 0.00434644
2017-09-29T10:55:46.852813: step 37, loss 1.91448, acc 0.265625, learning_rate 0.00432911
2017-09-29T10:55:47.048900: step 38, loss 1.6118, acc 0.40625, learning_rate 0.00431185
2017-09-29T10:55:47.242409: step 39, loss 2.00156, acc 0.296875, learning_rate 0.00429465
2017-09-29T10:55:47.434909: step 40, loss 1.73315, acc 0.359375, learning_rate 0.00427753

Evaluation:
2017-09-29T10:55:48.041128: step 40, loss 1.11261, acc 0.641727

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-40

2017-09-29T10:55:48.763801: step 41, loss 2.18347, acc 0.296875, learning_rate 0.00426048
2017-09-29T10:55:48.953669: step 42, loss 1.63425, acc 0.359375, learning_rate 0.0042435
2017-09-29T10:55:49.142056: step 43, loss 2.10287, acc 0.328125, learning_rate 0.00422659
2017-09-29T10:55:49.325162: step 44, loss 1.6147, acc 0.515625, learning_rate 0.00420974
2017-09-29T10:55:49.508914: step 45, loss 1.80663, acc 0.484375, learning_rate 0.00419297
2017-09-29T10:55:49.689629: step 46, loss 1.84556, acc 0.40625, learning_rate 0.00417626
2017-09-29T10:55:49.877815: step 47, loss 1.93791, acc 0.3125, learning_rate 0.00415962
2017-09-29T10:55:50.057534: step 48, loss 1.93946, acc 0.3125, learning_rate 0.00414305
2017-09-29T10:55:50.241062: step 49, loss 1.95271, acc 0.28125, learning_rate 0.00412655
2017-09-29T10:55:50.453699: step 50, loss 1.58111, acc 0.421875, learning_rate 0.00411011
2017-09-29T10:55:50.640821: step 51, loss 1.68222, acc 0.375, learning_rate 0.00409375
2017-09-29T10:55:50.833825: step 52, loss 1.67513, acc 0.390625, learning_rate 0.00407744
2017-09-29T10:55:51.015809: step 53, loss 1.80464, acc 0.375, learning_rate 0.00406121
2017-09-29T10:55:51.222206: step 54, loss 1.49092, acc 0.5, learning_rate 0.00404504
2017-09-29T10:55:51.406924: step 55, loss 1.46005, acc 0.484375, learning_rate 0.00402894
2017-09-29T10:55:51.610484: step 56, loss 1.45513, acc 0.5, learning_rate 0.0040129
2017-09-29T10:55:51.817583: step 57, loss 1.49512, acc 0.53125, learning_rate 0.00399693
2017-09-29T10:55:52.026765: step 58, loss 1.80797, acc 0.421875, learning_rate 0.00398102
2017-09-29T10:55:52.210341: step 59, loss 1.62562, acc 0.46875, learning_rate 0.00396518
2017-09-29T10:55:52.394461: step 60, loss 1.58959, acc 0.359375, learning_rate 0.00394941
2017-09-29T10:55:52.581801: step 61, loss 1.37584, acc 0.46875, learning_rate 0.00393369
2017-09-29T10:55:52.769555: step 62, loss 1.53002, acc 0.484375, learning_rate 0.00391804
2017-09-29T10:55:52.947280: step 63, loss 1.60969, acc 0.453125, learning_rate 0.00390246
2017-09-29T10:55:53.128483: step 64, loss 1.20564, acc 0.59375, learning_rate 0.00388694
2017-09-29T10:55:53.310925: step 65, loss 1.51038, acc 0.453125, learning_rate 0.00387148
2017-09-29T10:55:53.501097: step 66, loss 1.43987, acc 0.53125, learning_rate 0.00385609
2017-09-29T10:55:53.698894: step 67, loss 1.26389, acc 0.59375, learning_rate 0.00384076
2017-09-29T10:55:53.880540: step 68, loss 1.31257, acc 0.5625, learning_rate 0.00382549
2017-09-29T10:55:54.066220: step 69, loss 1.55813, acc 0.421875, learning_rate 0.00381028
2017-09-29T10:55:54.247386: step 70, loss 1.4012, acc 0.53125, learning_rate 0.00379514
2017-09-29T10:55:54.429366: step 71, loss 1.48531, acc 0.421875, learning_rate 0.00378005
2017-09-29T10:55:54.623586: step 72, loss 1.25346, acc 0.484375, learning_rate 0.00376503
2017-09-29T10:55:54.832731: step 73, loss 1.39341, acc 0.515625, learning_rate 0.00375007
2017-09-29T10:55:55.038108: step 74, loss 1.59236, acc 0.46875, learning_rate 0.00373517
2017-09-29T10:55:55.239141: step 75, loss 1.21886, acc 0.5625, learning_rate 0.00372034
2017-09-29T10:55:55.427621: step 76, loss 1.70984, acc 0.40625, learning_rate 0.00370556
2017-09-29T10:55:55.609512: step 77, loss 1.28467, acc 0.515625, learning_rate 0.00369084
2017-09-29T10:55:55.792675: step 78, loss 1.4778, acc 0.515625, learning_rate 0.00367619
2017-09-29T10:55:55.982610: step 79, loss 1.44299, acc 0.546875, learning_rate 0.00366159
2017-09-29T10:55:56.177196: step 80, loss 0.954979, acc 0.59375, learning_rate 0.00364705

Evaluation:
2017-09-29T10:55:56.753785: step 80, loss 0.945986, acc 0.644604

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-80

2017-09-29T10:55:57.507359: step 81, loss 1.30753, acc 0.609375, learning_rate 0.00363257
2017-09-29T10:55:57.703099: step 82, loss 1.22572, acc 0.578125, learning_rate 0.00361815
2017-09-29T10:55:57.886819: step 83, loss 1.33587, acc 0.5, learning_rate 0.00360379
2017-09-29T10:55:58.065817: step 84, loss 1.32347, acc 0.546875, learning_rate 0.00358949
2017-09-29T10:55:58.248601: step 85, loss 1.43326, acc 0.4375, learning_rate 0.00357525
2017-09-29T10:55:58.428601: step 86, loss 1.59258, acc 0.421875, learning_rate 0.00356106
2017-09-29T10:55:58.615271: step 87, loss 1.2013, acc 0.53125, learning_rate 0.00354694
2017-09-29T10:55:58.824710: step 88, loss 1.20126, acc 0.53125, learning_rate 0.00353287
2017-09-29T10:55:59.033913: step 89, loss 1.28196, acc 0.578125, learning_rate 0.00351885
2017-09-29T10:55:59.243234: step 90, loss 1.30018, acc 0.5, learning_rate 0.0035049
2017-09-29T10:55:59.444206: step 91, loss 1.10459, acc 0.625, learning_rate 0.003491
2017-09-29T10:55:59.632654: step 92, loss 1.22288, acc 0.609375, learning_rate 0.00347716
2017-09-29T10:55:59.818795: step 93, loss 1.50023, acc 0.46875, learning_rate 0.00346338
2017-09-29T10:56:00.004917: step 94, loss 1.1018, acc 0.59375, learning_rate 0.00344965
2017-09-29T10:56:00.195133: step 95, loss 1.31815, acc 0.546875, learning_rate 0.00343597
2017-09-29T10:56:00.397271: step 96, loss 1.35492, acc 0.546875, learning_rate 0.00342236
2017-09-29T10:56:00.579328: step 97, loss 1.39682, acc 0.4375, learning_rate 0.0034088
2017-09-29T10:56:00.733853: step 98, loss 1.3977, acc 0.509804, learning_rate 0.00339529
2017-09-29T10:56:00.924395: step 99, loss 1.18649, acc 0.578125, learning_rate 0.00338184
2017-09-29T10:56:01.113370: step 100, loss 1.04929, acc 0.546875, learning_rate 0.00336844
2017-09-29T10:56:01.299907: step 101, loss 0.97798, acc 0.65625, learning_rate 0.0033551
2017-09-29T10:56:01.486546: step 102, loss 1.07343, acc 0.59375, learning_rate 0.00334182
2017-09-29T10:56:01.672899: step 103, loss 0.969135, acc 0.671875, learning_rate 0.00332858
2017-09-29T10:56:01.861490: step 104, loss 1.13599, acc 0.625, learning_rate 0.00331541
2017-09-29T10:56:02.045271: step 105, loss 0.998993, acc 0.6875, learning_rate 0.00330228
2017-09-29T10:56:02.244988: step 106, loss 1.30447, acc 0.578125, learning_rate 0.00328921
2017-09-29T10:56:02.444412: step 107, loss 1.31757, acc 0.53125, learning_rate 0.00327619
2017-09-29T10:56:02.632586: step 108, loss 1.08676, acc 0.640625, learning_rate 0.00326323
2017-09-29T10:56:02.819814: step 109, loss 1.11319, acc 0.609375, learning_rate 0.00325032
2017-09-29T10:56:03.008727: step 110, loss 0.937067, acc 0.671875, learning_rate 0.00323746
2017-09-29T10:56:03.198355: step 111, loss 0.840521, acc 0.625, learning_rate 0.00322465
2017-09-29T10:56:03.387098: step 112, loss 0.915258, acc 0.671875, learning_rate 0.0032119
2017-09-29T10:56:03.585604: step 113, loss 0.737495, acc 0.75, learning_rate 0.0031992
2017-09-29T10:56:03.773461: step 114, loss 0.850902, acc 0.6875, learning_rate 0.00318655
2017-09-29T10:56:03.964639: step 115, loss 1.03322, acc 0.59375, learning_rate 0.00317395
2017-09-29T10:56:04.158093: step 116, loss 0.986423, acc 0.671875, learning_rate 0.0031614
2017-09-29T10:56:04.351067: step 117, loss 1.02936, acc 0.671875, learning_rate 0.0031489
2017-09-29T10:56:04.540503: step 118, loss 1.00598, acc 0.625, learning_rate 0.00313646
2017-09-29T10:56:04.725397: step 119, loss 1.22567, acc 0.546875, learning_rate 0.00312407
2017-09-29T10:56:04.913292: step 120, loss 1.03473, acc 0.546875, learning_rate 0.00311172

Evaluation:
2017-09-29T10:56:05.478899: step 120, loss 0.706398, acc 0.776978

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-120

2017-09-29T10:56:06.228088: step 121, loss 1.04003, acc 0.640625, learning_rate 0.00309943
2017-09-29T10:56:06.427078: step 122, loss 0.984294, acc 0.6875, learning_rate 0.00308719
2017-09-29T10:56:06.613713: step 123, loss 0.867931, acc 0.703125, learning_rate 0.00307499
2017-09-29T10:56:06.806410: step 124, loss 0.809949, acc 0.765625, learning_rate 0.00306285
2017-09-29T10:56:06.996776: step 125, loss 0.973746, acc 0.578125, learning_rate 0.00305076
2017-09-29T10:56:07.185732: step 126, loss 0.944637, acc 0.703125, learning_rate 0.00303871
2017-09-29T10:56:07.379574: step 127, loss 0.792323, acc 0.734375, learning_rate 0.00302672
2017-09-29T10:56:07.593262: step 128, loss 1.10704, acc 0.609375, learning_rate 0.00301477
2017-09-29T10:56:07.796958: step 129, loss 1.07085, acc 0.671875, learning_rate 0.00300287
2017-09-29T10:56:08.013645: step 130, loss 1.33392, acc 0.5625, learning_rate 0.00299102
2017-09-29T10:56:08.206028: step 131, loss 0.954967, acc 0.640625, learning_rate 0.00297922
2017-09-29T10:56:08.390698: step 132, loss 0.986597, acc 0.59375, learning_rate 0.00296747
2017-09-29T10:56:08.579376: step 133, loss 1.29056, acc 0.5625, learning_rate 0.00295577
2017-09-29T10:56:08.768226: step 134, loss 0.762548, acc 0.75, learning_rate 0.00294411
2017-09-29T10:56:08.954463: step 135, loss 0.907132, acc 0.65625, learning_rate 0.0029325
2017-09-29T10:56:09.142841: step 136, loss 1.11876, acc 0.578125, learning_rate 0.00292094
2017-09-29T10:56:09.327057: step 137, loss 0.944265, acc 0.625, learning_rate 0.00290943
2017-09-29T10:56:09.525206: step 138, loss 0.584139, acc 0.859375, learning_rate 0.00289796
2017-09-29T10:56:09.711300: step 139, loss 0.848072, acc 0.6875, learning_rate 0.00288654
2017-09-29T10:56:09.900570: step 140, loss 0.916675, acc 0.671875, learning_rate 0.00287516
2017-09-29T10:56:10.084049: step 141, loss 0.94288, acc 0.5625, learning_rate 0.00286384
2017-09-29T10:56:10.277464: step 142, loss 1.08031, acc 0.59375, learning_rate 0.00285256
2017-09-29T10:56:10.470389: step 143, loss 0.851271, acc 0.65625, learning_rate 0.00284132
2017-09-29T10:56:10.660611: step 144, loss 0.905203, acc 0.625, learning_rate 0.00283013
2017-09-29T10:56:10.853531: step 145, loss 0.855578, acc 0.71875, learning_rate 0.00281899
2017-09-29T10:56:11.054930: step 146, loss 0.933611, acc 0.609375, learning_rate 0.00280789
2017-09-29T10:56:11.243301: step 147, loss 0.859615, acc 0.671875, learning_rate 0.00279684
2017-09-29T10:56:11.444459: step 148, loss 1.13909, acc 0.625, learning_rate 0.00278583
2017-09-29T10:56:11.630030: step 149, loss 0.903934, acc 0.625, learning_rate 0.00277486
2017-09-29T10:56:11.829245: step 150, loss 0.946914, acc 0.65625, learning_rate 0.00276395
2017-09-29T10:56:12.023505: step 151, loss 0.916746, acc 0.65625, learning_rate 0.00275307
2017-09-29T10:56:12.212838: step 152, loss 0.678905, acc 0.71875, learning_rate 0.00274224
2017-09-29T10:56:12.405241: step 153, loss 0.994299, acc 0.65625, learning_rate 0.00273146
2017-09-29T10:56:12.591759: step 154, loss 0.906032, acc 0.703125, learning_rate 0.00272072
2017-09-29T10:56:12.781400: step 155, loss 0.964938, acc 0.640625, learning_rate 0.00271002
2017-09-29T10:56:12.968485: step 156, loss 1.0056, acc 0.65625, learning_rate 0.00269937
2017-09-29T10:56:13.154766: step 157, loss 1.13623, acc 0.640625, learning_rate 0.00268876
2017-09-29T10:56:13.344113: step 158, loss 0.917676, acc 0.703125, learning_rate 0.00267819
2017-09-29T10:56:13.549617: step 159, loss 0.784528, acc 0.71875, learning_rate 0.00266767
2017-09-29T10:56:13.747036: step 160, loss 1.05683, acc 0.609375, learning_rate 0.00265719

Evaluation:
2017-09-29T10:56:14.299915: step 160, loss 0.630363, acc 0.753957

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-160

2017-09-29T10:56:15.029086: step 161, loss 0.783949, acc 0.734375, learning_rate 0.00264675
2017-09-29T10:56:15.216592: step 162, loss 0.687479, acc 0.75, learning_rate 0.00263635
2017-09-29T10:56:15.404048: step 163, loss 0.948262, acc 0.65625, learning_rate 0.002626
2017-09-29T10:56:15.595208: step 164, loss 0.591009, acc 0.8125, learning_rate 0.00261569
2017-09-29T10:56:15.789800: step 165, loss 0.843144, acc 0.625, learning_rate 0.00260542
2017-09-29T10:56:15.980122: step 166, loss 0.836001, acc 0.6875, learning_rate 0.0025952
2017-09-29T10:56:16.167895: step 167, loss 0.847493, acc 0.609375, learning_rate 0.00258501
2017-09-29T10:56:16.361898: step 168, loss 0.9889, acc 0.6875, learning_rate 0.00257487
2017-09-29T10:56:16.549005: step 169, loss 0.795996, acc 0.796875, learning_rate 0.00256477
2017-09-29T10:56:16.738619: step 170, loss 0.997166, acc 0.65625, learning_rate 0.0025547
2017-09-29T10:56:16.937097: step 171, loss 0.752624, acc 0.71875, learning_rate 0.00254469
2017-09-29T10:56:17.122210: step 172, loss 0.693799, acc 0.75, learning_rate 0.00253471
2017-09-29T10:56:17.309341: step 173, loss 0.924861, acc 0.640625, learning_rate 0.00252477
2017-09-29T10:56:17.499698: step 174, loss 0.848778, acc 0.703125, learning_rate 0.00251487
2017-09-29T10:56:17.687755: step 175, loss 0.930501, acc 0.71875, learning_rate 0.00250501
2017-09-29T10:56:17.881075: step 176, loss 0.837025, acc 0.671875, learning_rate 0.0024952
2017-09-29T10:56:18.063189: step 177, loss 0.993458, acc 0.671875, learning_rate 0.00248542
2017-09-29T10:56:18.260301: step 178, loss 0.754013, acc 0.6875, learning_rate 0.00247568
2017-09-29T10:56:18.451976: step 179, loss 0.946461, acc 0.65625, learning_rate 0.00246599
2017-09-29T10:56:18.638680: step 180, loss 0.864066, acc 0.65625, learning_rate 0.00245633
2017-09-29T10:56:18.831074: step 181, loss 0.547206, acc 0.828125, learning_rate 0.00244671
2017-09-29T10:56:19.018688: step 182, loss 0.962525, acc 0.65625, learning_rate 0.00243713
2017-09-29T10:56:19.213048: step 183, loss 0.969637, acc 0.625, learning_rate 0.00242759
2017-09-29T10:56:19.418724: step 184, loss 0.782117, acc 0.703125, learning_rate 0.00241809
2017-09-29T10:56:19.625420: step 185, loss 0.674231, acc 0.71875, learning_rate 0.00240863
2017-09-29T10:56:19.814205: step 186, loss 1.12972, acc 0.59375, learning_rate 0.00239921
2017-09-29T10:56:19.998983: step 187, loss 0.952056, acc 0.640625, learning_rate 0.00238982
2017-09-29T10:56:20.185579: step 188, loss 0.817605, acc 0.703125, learning_rate 0.00238048
2017-09-29T10:56:20.368584: step 189, loss 0.778531, acc 0.71875, learning_rate 0.00237117
2017-09-29T10:56:20.553492: step 190, loss 1.0536, acc 0.640625, learning_rate 0.0023619
2017-09-29T10:56:20.746932: step 191, loss 0.842999, acc 0.640625, learning_rate 0.00235267
2017-09-29T10:56:20.936775: step 192, loss 0.834211, acc 0.703125, learning_rate 0.00234347
2017-09-29T10:56:21.141184: step 193, loss 0.747199, acc 0.734375, learning_rate 0.00233431
2017-09-29T10:56:21.330690: step 194, loss 0.769319, acc 0.78125, learning_rate 0.00232519
2017-09-29T10:56:21.532195: step 195, loss 0.570246, acc 0.796875, learning_rate 0.00231611
2017-09-29T10:56:21.692415: step 196, loss 0.654588, acc 0.745098, learning_rate 0.00230707
2017-09-29T10:56:21.895510: step 197, loss 0.558593, acc 0.8125, learning_rate 0.00229806
2017-09-29T10:56:22.074367: step 198, loss 0.872561, acc 0.703125, learning_rate 0.00228908
2017-09-29T10:56:22.261299: step 199, loss 0.863267, acc 0.65625, learning_rate 0.00228015
2017-09-29T10:56:22.446091: step 200, loss 0.812584, acc 0.703125, learning_rate 0.00227125

Evaluation:
2017-09-29T10:56:22.997396: step 200, loss 0.561663, acc 0.795683

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-200

2017-09-29T10:56:23.712869: step 201, loss 0.668195, acc 0.75, learning_rate 0.00226239
2017-09-29T10:56:23.891989: step 202, loss 0.732528, acc 0.71875, learning_rate 0.00225356
2017-09-29T10:56:24.078710: step 203, loss 0.642859, acc 0.75, learning_rate 0.00224477
2017-09-29T10:56:24.260281: step 204, loss 0.608296, acc 0.796875, learning_rate 0.00223602
2017-09-29T10:56:24.446093: step 205, loss 0.6376, acc 0.71875, learning_rate 0.0022273
2017-09-29T10:56:24.630505: step 206, loss 0.780791, acc 0.703125, learning_rate 0.00221862
2017-09-29T10:56:24.813175: step 207, loss 0.598089, acc 0.796875, learning_rate 0.00220997
2017-09-29T10:56:24.995983: step 208, loss 0.818094, acc 0.65625, learning_rate 0.00220136
2017-09-29T10:56:25.182588: step 209, loss 0.725193, acc 0.703125, learning_rate 0.00219278
2017-09-29T10:56:25.365701: step 210, loss 0.783315, acc 0.75, learning_rate 0.00218424
2017-09-29T10:56:25.561150: step 211, loss 0.446808, acc 0.84375, learning_rate 0.00217573
2017-09-29T10:56:25.745752: step 212, loss 0.786463, acc 0.71875, learning_rate 0.00216726
2017-09-29T10:56:25.937230: step 213, loss 0.549893, acc 0.765625, learning_rate 0.00215882
2017-09-29T10:56:26.129581: step 214, loss 0.756776, acc 0.71875, learning_rate 0.00215041
2017-09-29T10:56:26.314442: step 215, loss 0.698699, acc 0.71875, learning_rate 0.00214204
2017-09-29T10:56:26.505629: step 216, loss 0.695576, acc 0.6875, learning_rate 0.00213371
2017-09-29T10:56:26.691917: step 217, loss 0.710375, acc 0.765625, learning_rate 0.00212541
2017-09-29T10:56:26.877240: step 218, loss 0.806156, acc 0.703125, learning_rate 0.00211714
2017-09-29T10:56:27.063488: step 219, loss 0.748319, acc 0.765625, learning_rate 0.00210891
2017-09-29T10:56:27.251937: step 220, loss 0.701044, acc 0.734375, learning_rate 0.00210071
2017-09-29T10:56:27.442726: step 221, loss 0.892798, acc 0.6875, learning_rate 0.00209254
2017-09-29T10:56:27.631613: step 222, loss 0.560927, acc 0.8125, learning_rate 0.00208441
2017-09-29T10:56:27.811484: step 223, loss 0.574383, acc 0.765625, learning_rate 0.00207631
2017-09-29T10:56:27.993975: step 224, loss 0.708204, acc 0.75, learning_rate 0.00206824
2017-09-29T10:56:28.179550: step 225, loss 0.652201, acc 0.703125, learning_rate 0.00206021
2017-09-29T10:56:28.362669: step 226, loss 0.603855, acc 0.75, learning_rate 0.00205221
2017-09-29T10:56:28.555977: step 227, loss 0.664187, acc 0.78125, learning_rate 0.00204424
2017-09-29T10:56:28.740791: step 228, loss 0.478551, acc 0.8125, learning_rate 0.0020363
2017-09-29T10:56:28.928436: step 229, loss 0.792675, acc 0.6875, learning_rate 0.0020284
2017-09-29T10:56:29.121675: step 230, loss 0.684416, acc 0.734375, learning_rate 0.00202053
2017-09-29T10:56:29.303665: step 231, loss 0.601969, acc 0.75, learning_rate 0.00201269
2017-09-29T10:56:29.491122: step 232, loss 0.777673, acc 0.75, learning_rate 0.00200488
2017-09-29T10:56:29.673611: step 233, loss 0.549356, acc 0.828125, learning_rate 0.00199711
2017-09-29T10:56:29.856577: step 234, loss 0.378381, acc 0.828125, learning_rate 0.00198936
2017-09-29T10:56:30.039404: step 235, loss 0.565426, acc 0.734375, learning_rate 0.00198165
2017-09-29T10:56:30.220896: step 236, loss 0.535472, acc 0.8125, learning_rate 0.00197397
2017-09-29T10:56:30.418028: step 237, loss 0.827211, acc 0.734375, learning_rate 0.00196632
2017-09-29T10:56:30.613091: step 238, loss 0.862557, acc 0.6875, learning_rate 0.0019587
2017-09-29T10:56:30.798250: step 239, loss 0.59001, acc 0.734375, learning_rate 0.00195112
2017-09-29T10:56:30.980169: step 240, loss 0.548979, acc 0.765625, learning_rate 0.00194356

Evaluation:
2017-09-29T10:56:31.538935: step 240, loss 0.52208, acc 0.807194

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-240

2017-09-29T10:56:32.268495: step 241, loss 0.625919, acc 0.71875, learning_rate 0.00193604
2017-09-29T10:56:32.472664: step 242, loss 0.714106, acc 0.734375, learning_rate 0.00192854
2017-09-29T10:56:32.659590: step 243, loss 0.585902, acc 0.75, learning_rate 0.00192108
2017-09-29T10:56:32.844999: step 244, loss 1.03672, acc 0.609375, learning_rate 0.00191364
2017-09-29T10:56:33.025542: step 245, loss 0.682993, acc 0.765625, learning_rate 0.00190624
2017-09-29T10:56:33.235256: step 246, loss 0.625523, acc 0.734375, learning_rate 0.00189887
2017-09-29T10:56:33.427221: step 247, loss 0.847256, acc 0.6875, learning_rate 0.00189153
2017-09-29T10:56:33.610674: step 248, loss 0.675091, acc 0.75, learning_rate 0.00188421
2017-09-29T10:56:33.800950: step 249, loss 0.695261, acc 0.734375, learning_rate 0.00187693
2017-09-29T10:56:33.987494: step 250, loss 0.651271, acc 0.765625, learning_rate 0.00186968
2017-09-29T10:56:34.169208: step 251, loss 0.48243, acc 0.78125, learning_rate 0.00186245
2017-09-29T10:56:34.356366: step 252, loss 0.931817, acc 0.65625, learning_rate 0.00185526
2017-09-29T10:56:34.537515: step 253, loss 0.616179, acc 0.75, learning_rate 0.0018481
2017-09-29T10:56:34.733707: step 254, loss 0.469139, acc 0.84375, learning_rate 0.00184096
2017-09-29T10:56:34.939067: step 255, loss 0.571222, acc 0.75, learning_rate 0.00183385
2017-09-29T10:56:35.122548: step 256, loss 0.830031, acc 0.65625, learning_rate 0.00182678
2017-09-29T10:56:35.300858: step 257, loss 0.55367, acc 0.8125, learning_rate 0.00181973
2017-09-29T10:56:35.491962: step 258, loss 0.468631, acc 0.828125, learning_rate 0.00181271
2017-09-29T10:56:35.676102: step 259, loss 0.458652, acc 0.828125, learning_rate 0.00180572
2017-09-29T10:56:35.866531: step 260, loss 0.89029, acc 0.703125, learning_rate 0.00179876
2017-09-29T10:56:36.049537: step 261, loss 0.636505, acc 0.765625, learning_rate 0.00179182
2017-09-29T10:56:36.249744: step 262, loss 0.506591, acc 0.84375, learning_rate 0.00178492
2017-09-29T10:56:36.436440: step 263, loss 0.617492, acc 0.796875, learning_rate 0.00177804
2017-09-29T10:56:36.632599: step 264, loss 0.756744, acc 0.703125, learning_rate 0.00177119
2017-09-29T10:56:36.816738: step 265, loss 0.668235, acc 0.734375, learning_rate 0.00176437
2017-09-29T10:56:37.004787: step 266, loss 0.535064, acc 0.78125, learning_rate 0.00175758
2017-09-29T10:56:37.187161: step 267, loss 0.702533, acc 0.75, learning_rate 0.00175081
2017-09-29T10:56:37.368609: step 268, loss 0.556574, acc 0.78125, learning_rate 0.00174407
2017-09-29T10:56:37.563725: step 269, loss 0.504774, acc 0.84375, learning_rate 0.00173736
2017-09-29T10:56:37.754107: step 270, loss 0.692471, acc 0.765625, learning_rate 0.00173068
2017-09-29T10:56:37.945323: step 271, loss 0.468512, acc 0.828125, learning_rate 0.00172402
2017-09-29T10:56:38.129261: step 272, loss 0.690565, acc 0.765625, learning_rate 0.00171739
2017-09-29T10:56:38.310041: step 273, loss 0.884906, acc 0.671875, learning_rate 0.00171079
2017-09-29T10:56:38.491481: step 274, loss 0.826068, acc 0.609375, learning_rate 0.00170422
2017-09-29T10:56:38.681647: step 275, loss 0.720555, acc 0.75, learning_rate 0.00169767
2017-09-29T10:56:38.867177: step 276, loss 0.552104, acc 0.8125, learning_rate 0.00169115
2017-09-29T10:56:39.049981: step 277, loss 0.902778, acc 0.6875, learning_rate 0.00168465
2017-09-29T10:56:39.234541: step 278, loss 0.934143, acc 0.65625, learning_rate 0.00167818
2017-09-29T10:56:39.422675: step 279, loss 0.460891, acc 0.84375, learning_rate 0.00167174
2017-09-29T10:56:39.605160: step 280, loss 0.549452, acc 0.765625, learning_rate 0.00166533

Evaluation:
2017-09-29T10:56:40.162968: step 280, loss 0.493009, acc 0.821583

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-280

2017-09-29T10:56:40.895229: step 281, loss 0.540762, acc 0.78125, learning_rate 0.00165894
2017-09-29T10:56:41.077203: step 282, loss 0.75369, acc 0.71875, learning_rate 0.00165257
2017-09-29T10:56:41.266144: step 283, loss 0.582261, acc 0.765625, learning_rate 0.00164624
2017-09-29T10:56:41.452749: step 284, loss 0.534892, acc 0.8125, learning_rate 0.00163993
2017-09-29T10:56:41.645305: step 285, loss 0.460366, acc 0.78125, learning_rate 0.00163364
2017-09-29T10:56:41.829657: step 286, loss 0.50156, acc 0.84375, learning_rate 0.00162738
2017-09-29T10:56:42.024225: step 287, loss 0.63795, acc 0.75, learning_rate 0.00162115
2017-09-29T10:56:42.221665: step 288, loss 0.656334, acc 0.734375, learning_rate 0.00161494
2017-09-29T10:56:42.416569: step 289, loss 0.607329, acc 0.765625, learning_rate 0.00160875
2017-09-29T10:56:42.601353: step 290, loss 0.602494, acc 0.78125, learning_rate 0.00160259
2017-09-29T10:56:42.782730: step 291, loss 0.357344, acc 0.859375, learning_rate 0.00159646
2017-09-29T10:56:42.965200: step 292, loss 0.675582, acc 0.703125, learning_rate 0.00159035
2017-09-29T10:56:43.150893: step 293, loss 0.542079, acc 0.84375, learning_rate 0.00158427
2017-09-29T10:56:43.301270: step 294, loss 0.661442, acc 0.803922, learning_rate 0.00157821
2017-09-29T10:56:43.484705: step 295, loss 0.45296, acc 0.859375, learning_rate 0.00157218
2017-09-29T10:56:43.676760: step 296, loss 0.637617, acc 0.78125, learning_rate 0.00156617
2017-09-29T10:56:43.872399: step 297, loss 0.48588, acc 0.8125, learning_rate 0.00156018
2017-09-29T10:56:44.064057: step 298, loss 0.71245, acc 0.71875, learning_rate 0.00155422
2017-09-29T10:56:44.250036: step 299, loss 0.503626, acc 0.84375, learning_rate 0.00154829
2017-09-29T10:56:44.437178: step 300, loss 0.658017, acc 0.78125, learning_rate 0.00154238
2017-09-29T10:56:44.619982: step 301, loss 0.571588, acc 0.796875, learning_rate 0.00153649
2017-09-29T10:56:44.809098: step 302, loss 0.26566, acc 0.921875, learning_rate 0.00153063
2017-09-29T10:56:44.994319: step 303, loss 0.513278, acc 0.78125, learning_rate 0.00152479
2017-09-29T10:56:45.176891: step 304, loss 0.579482, acc 0.765625, learning_rate 0.00151897
2017-09-29T10:56:45.362332: step 305, loss 0.615363, acc 0.734375, learning_rate 0.00151318
2017-09-29T10:56:45.558509: step 306, loss 0.466742, acc 0.828125, learning_rate 0.00150741
2017-09-29T10:56:45.751882: step 307, loss 0.450201, acc 0.8125, learning_rate 0.00150167
2017-09-29T10:56:45.940366: step 308, loss 0.526203, acc 0.84375, learning_rate 0.00149594
2017-09-29T10:56:46.126228: step 309, loss 0.34081, acc 0.859375, learning_rate 0.00149025
2017-09-29T10:56:46.317867: step 310, loss 0.453798, acc 0.875, learning_rate 0.00148457
2017-09-29T10:56:46.523539: step 311, loss 0.570616, acc 0.78125, learning_rate 0.00147892
2017-09-29T10:56:46.713826: step 312, loss 0.512928, acc 0.78125, learning_rate 0.00147329
2017-09-29T10:56:46.899961: step 313, loss 0.42557, acc 0.84375, learning_rate 0.00146769
2017-09-29T10:56:47.088590: step 314, loss 0.489766, acc 0.859375, learning_rate 0.0014621
2017-09-29T10:56:47.279003: step 315, loss 0.685359, acc 0.71875, learning_rate 0.00145654
2017-09-29T10:56:47.479069: step 316, loss 0.376219, acc 0.875, learning_rate 0.00145101
2017-09-29T10:56:47.666401: step 317, loss 0.544886, acc 0.765625, learning_rate 0.00144549
2017-09-29T10:56:47.852568: step 318, loss 0.651608, acc 0.765625, learning_rate 0.00144
2017-09-29T10:56:48.036912: step 319, loss 0.39888, acc 0.84375, learning_rate 0.00143453
2017-09-29T10:56:48.222627: step 320, loss 0.580801, acc 0.8125, learning_rate 0.00142908

Evaluation:
2017-09-29T10:56:48.812417: step 320, loss 0.460058, acc 0.834532

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-320

2017-09-29T10:56:49.453318: step 321, loss 0.519663, acc 0.78125, learning_rate 0.00142366
2017-09-29T10:56:49.641398: step 322, loss 0.404537, acc 0.859375, learning_rate 0.00141826
2017-09-29T10:56:49.832290: step 323, loss 0.533676, acc 0.765625, learning_rate 0.00141288
2017-09-29T10:56:50.036574: step 324, loss 0.367804, acc 0.828125, learning_rate 0.00140752
2017-09-29T10:56:50.245351: step 325, loss 0.467989, acc 0.8125, learning_rate 0.00140218
2017-09-29T10:56:50.429278: step 326, loss 0.551702, acc 0.828125, learning_rate 0.00139686
2017-09-29T10:56:50.612895: step 327, loss 0.552227, acc 0.78125, learning_rate 0.00139157
2017-09-29T10:56:50.796605: step 328, loss 0.572177, acc 0.734375, learning_rate 0.0013863
2017-09-29T10:56:50.992619: step 329, loss 0.709797, acc 0.75, learning_rate 0.00138105
2017-09-29T10:56:51.181089: step 330, loss 0.504852, acc 0.765625, learning_rate 0.00137582
2017-09-29T10:56:51.386000: step 331, loss 0.477192, acc 0.8125, learning_rate 0.00137061
2017-09-29T10:56:51.597329: step 332, loss 0.455765, acc 0.859375, learning_rate 0.00136543
2017-09-29T10:56:51.786216: step 333, loss 0.610223, acc 0.796875, learning_rate 0.00136026
2017-09-29T10:56:51.969692: step 334, loss 0.540908, acc 0.78125, learning_rate 0.00135512
2017-09-29T10:56:52.154519: step 335, loss 0.527703, acc 0.796875, learning_rate 0.00134999
2017-09-29T10:56:52.336587: step 336, loss 0.478464, acc 0.828125, learning_rate 0.00134489
2017-09-29T10:56:52.520300: step 337, loss 0.716573, acc 0.765625, learning_rate 0.00133981
2017-09-29T10:56:52.699065: step 338, loss 0.573393, acc 0.78125, learning_rate 0.00133475
2017-09-29T10:56:52.884021: step 339, loss 0.525987, acc 0.8125, learning_rate 0.00132971
2017-09-29T10:56:53.068325: step 340, loss 0.493099, acc 0.859375, learning_rate 0.00132469
2017-09-29T10:56:53.251008: step 341, loss 0.45542, acc 0.828125, learning_rate 0.00131969
2017-09-29T10:56:53.434272: step 342, loss 0.276881, acc 0.890625, learning_rate 0.00131471
2017-09-29T10:56:53.615790: step 343, loss 0.67625, acc 0.75, learning_rate 0.00130975
2017-09-29T10:56:53.803267: step 344, loss 0.433003, acc 0.859375, learning_rate 0.00130482
2017-09-29T10:56:53.982364: step 345, loss 0.5044, acc 0.828125, learning_rate 0.0012999
2017-09-29T10:56:54.166314: step 346, loss 0.483456, acc 0.84375, learning_rate 0.001295
2017-09-29T10:56:54.351413: step 347, loss 0.546827, acc 0.78125, learning_rate 0.00129012
2017-09-29T10:56:54.559909: step 348, loss 0.356296, acc 0.875, learning_rate 0.00128527
2017-09-29T10:56:54.744521: step 349, loss 0.444636, acc 0.859375, learning_rate 0.00128043
2017-09-29T10:56:54.926291: step 350, loss 0.45737, acc 0.796875, learning_rate 0.00127561
2017-09-29T10:56:55.107753: step 351, loss 0.459259, acc 0.828125, learning_rate 0.00127081
2017-09-29T10:56:55.291002: step 352, loss 0.490433, acc 0.828125, learning_rate 0.00126603
2017-09-29T10:56:55.478192: step 353, loss 0.679303, acc 0.703125, learning_rate 0.00126127
2017-09-29T10:56:55.665552: step 354, loss 0.38553, acc 0.859375, learning_rate 0.00125653
2017-09-29T10:56:55.849020: step 355, loss 0.51248, acc 0.8125, learning_rate 0.00125181
2017-09-29T10:56:56.037944: step 356, loss 0.617316, acc 0.796875, learning_rate 0.00124711
2017-09-29T10:56:56.224369: step 357, loss 0.498251, acc 0.78125, learning_rate 0.00124243
2017-09-29T10:56:56.425274: step 358, loss 0.630878, acc 0.75, learning_rate 0.00123777
2017-09-29T10:56:56.610991: step 359, loss 0.389237, acc 0.84375, learning_rate 0.00123312
2017-09-29T10:56:56.806546: step 360, loss 0.557221, acc 0.8125, learning_rate 0.0012285

Evaluation:
2017-09-29T10:56:57.354668: step 360, loss 0.458773, acc 0.830216

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-360

2017-09-29T10:56:58.083420: step 361, loss 0.512563, acc 0.8125, learning_rate 0.00122389
2017-09-29T10:56:58.268367: step 362, loss 0.478414, acc 0.875, learning_rate 0.0012193
2017-09-29T10:56:58.448822: step 363, loss 0.658493, acc 0.71875, learning_rate 0.00121473
2017-09-29T10:56:58.632351: step 364, loss 0.537271, acc 0.859375, learning_rate 0.00121018
2017-09-29T10:56:58.811468: step 365, loss 0.36282, acc 0.90625, learning_rate 0.00120565
2017-09-29T10:56:58.990499: step 366, loss 0.506945, acc 0.8125, learning_rate 0.00120114
2017-09-29T10:56:59.175664: step 367, loss 0.649523, acc 0.8125, learning_rate 0.00119664
2017-09-29T10:56:59.358847: step 368, loss 0.59834, acc 0.78125, learning_rate 0.00119217
2017-09-29T10:56:59.558845: step 369, loss 0.610906, acc 0.75, learning_rate 0.00118771
2017-09-29T10:56:59.763336: step 370, loss 0.404657, acc 0.875, learning_rate 0.00118327
2017-09-29T10:56:59.946909: step 371, loss 0.572009, acc 0.796875, learning_rate 0.00117885
2017-09-29T10:57:00.129030: step 372, loss 0.557388, acc 0.875, learning_rate 0.00117445
2017-09-29T10:57:00.311294: step 373, loss 0.829497, acc 0.703125, learning_rate 0.00117006
2017-09-29T10:57:00.522024: step 374, loss 0.614639, acc 0.796875, learning_rate 0.00116569
2017-09-29T10:57:00.707186: step 375, loss 0.683909, acc 0.734375, learning_rate 0.00116134
2017-09-29T10:57:00.899330: step 376, loss 0.453723, acc 0.828125, learning_rate 0.00115701
2017-09-29T10:57:01.088295: step 377, loss 0.575342, acc 0.8125, learning_rate 0.0011527
2017-09-29T10:57:01.272046: step 378, loss 0.545692, acc 0.796875, learning_rate 0.0011484
2017-09-29T10:57:01.457717: step 379, loss 0.641634, acc 0.734375, learning_rate 0.00114412
2017-09-29T10:57:01.670574: step 380, loss 0.378516, acc 0.859375, learning_rate 0.00113986
2017-09-29T10:57:01.861037: step 381, loss 0.590942, acc 0.765625, learning_rate 0.00113561
2017-09-29T10:57:02.043074: step 382, loss 0.480216, acc 0.8125, learning_rate 0.00113139
2017-09-29T10:57:02.221659: step 383, loss 0.680801, acc 0.734375, learning_rate 0.00112718
2017-09-29T10:57:02.419530: step 384, loss 0.604932, acc 0.75, learning_rate 0.00112298
2017-09-29T10:57:02.604126: step 385, loss 0.614802, acc 0.796875, learning_rate 0.00111881
2017-09-29T10:57:02.794436: step 386, loss 0.616371, acc 0.828125, learning_rate 0.00111465
2017-09-29T10:57:02.974486: step 387, loss 0.492275, acc 0.828125, learning_rate 0.00111051
2017-09-29T10:57:03.166483: step 388, loss 0.501539, acc 0.78125, learning_rate 0.00110638
2017-09-29T10:57:03.348542: step 389, loss 0.463976, acc 0.875, learning_rate 0.00110228
2017-09-29T10:57:03.547963: step 390, loss 0.425114, acc 0.875, learning_rate 0.00109818
2017-09-29T10:57:03.730859: step 391, loss 0.54052, acc 0.828125, learning_rate 0.00109411
2017-09-29T10:57:03.883548: step 392, loss 0.551226, acc 0.745098, learning_rate 0.00109005
2017-09-29T10:57:04.067293: step 393, loss 0.437908, acc 0.828125, learning_rate 0.00108601
2017-09-29T10:57:04.257437: step 394, loss 0.475566, acc 0.8125, learning_rate 0.00108199
2017-09-29T10:57:04.439924: step 395, loss 0.604863, acc 0.8125, learning_rate 0.00107798
2017-09-29T10:57:04.629594: step 396, loss 0.546438, acc 0.78125, learning_rate 0.00107399
2017-09-29T10:57:04.819792: step 397, loss 0.264154, acc 0.890625, learning_rate 0.00107001
2017-09-29T10:57:05.000817: step 398, loss 0.458017, acc 0.828125, learning_rate 0.00106605
2017-09-29T10:57:05.183235: step 399, loss 0.536929, acc 0.765625, learning_rate 0.00106211
2017-09-29T10:57:05.364214: step 400, loss 0.453966, acc 0.84375, learning_rate 0.00105818

Evaluation:
2017-09-29T10:57:05.921263: step 400, loss 0.43576, acc 0.838849

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-400

2017-09-29T10:57:06.722829: step 401, loss 0.506296, acc 0.765625, learning_rate 0.00105427
2017-09-29T10:57:06.910266: step 402, loss 0.549333, acc 0.796875, learning_rate 0.00105037
2017-09-29T10:57:07.092471: step 403, loss 0.316475, acc 0.9375, learning_rate 0.0010465
2017-09-29T10:57:07.271294: step 404, loss 0.345336, acc 0.875, learning_rate 0.00104263
2017-09-29T10:57:07.452901: step 405, loss 0.474904, acc 0.828125, learning_rate 0.00103878
2017-09-29T10:57:07.636824: step 406, loss 0.559241, acc 0.8125, learning_rate 0.00103495
2017-09-29T10:57:07.819097: step 407, loss 0.626137, acc 0.8125, learning_rate 0.00103114
2017-09-29T10:57:07.997828: step 408, loss 0.43079, acc 0.890625, learning_rate 0.00102734
2017-09-29T10:57:08.177733: step 409, loss 0.523097, acc 0.8125, learning_rate 0.00102355
2017-09-29T10:57:08.359745: step 410, loss 0.643794, acc 0.8125, learning_rate 0.00101978
2017-09-29T10:57:08.549825: step 411, loss 0.683234, acc 0.703125, learning_rate 0.00101603
2017-09-29T10:57:08.735928: step 412, loss 0.841821, acc 0.734375, learning_rate 0.00101229
2017-09-29T10:57:08.924470: step 413, loss 0.56075, acc 0.78125, learning_rate 0.00100856
2017-09-29T10:57:09.108104: step 414, loss 0.610654, acc 0.78125, learning_rate 0.00100486
2017-09-29T10:57:09.293694: step 415, loss 0.505756, acc 0.84375, learning_rate 0.00100116
2017-09-29T10:57:09.487666: step 416, loss 0.450134, acc 0.84375, learning_rate 0.000997483
2017-09-29T10:57:09.681629: step 417, loss 0.511724, acc 0.796875, learning_rate 0.00099382
2017-09-29T10:57:09.864906: step 418, loss 0.332311, acc 0.828125, learning_rate 0.000990172
2017-09-29T10:57:10.047599: step 419, loss 0.384273, acc 0.859375, learning_rate 0.000986538
2017-09-29T10:57:10.230583: step 420, loss 0.40429, acc 0.84375, learning_rate 0.00098292
2017-09-29T10:57:10.418375: step 421, loss 0.527932, acc 0.8125, learning_rate 0.000979316
2017-09-29T10:57:10.602116: step 422, loss 0.586737, acc 0.8125, learning_rate 0.000975727
2017-09-29T10:57:10.785382: step 423, loss 0.32049, acc 0.875, learning_rate 0.000972152
2017-09-29T10:57:10.965174: step 424, loss 0.480748, acc 0.828125, learning_rate 0.000968592
2017-09-29T10:57:11.147341: step 425, loss 0.569556, acc 0.78125, learning_rate 0.000965047
2017-09-29T10:57:11.328835: step 426, loss 0.443622, acc 0.828125, learning_rate 0.000961516
2017-09-29T10:57:11.518817: step 427, loss 0.505603, acc 0.8125, learning_rate 0.000958
2017-09-29T10:57:11.707895: step 428, loss 0.534267, acc 0.796875, learning_rate 0.000954497
2017-09-29T10:57:11.898781: step 429, loss 0.611994, acc 0.765625, learning_rate 0.00095101
2017-09-29T10:57:12.082439: step 430, loss 0.433821, acc 0.859375, learning_rate 0.000947536
2017-09-29T10:57:12.289539: step 431, loss 0.450686, acc 0.8125, learning_rate 0.000944076
2017-09-29T10:57:12.488427: step 432, loss 0.333469, acc 0.890625, learning_rate 0.000940631
2017-09-29T10:57:12.667488: step 433, loss 0.457277, acc 0.8125, learning_rate 0.0009372
2017-09-29T10:57:12.860992: step 434, loss 0.644282, acc 0.75, learning_rate 0.000933783
2017-09-29T10:57:13.044085: step 435, loss 0.337796, acc 0.859375, learning_rate 0.000930379
2017-09-29T10:57:13.226356: step 436, loss 0.42644, acc 0.84375, learning_rate 0.00092699
2017-09-29T10:57:13.408577: step 437, loss 0.419106, acc 0.875, learning_rate 0.000923614
2017-09-29T10:57:13.599842: step 438, loss 0.585859, acc 0.8125, learning_rate 0.000920253
2017-09-29T10:57:13.787221: step 439, loss 0.432097, acc 0.8125, learning_rate 0.000916905
2017-09-29T10:57:13.971416: step 440, loss 0.50833, acc 0.84375, learning_rate 0.00091357

Evaluation:
2017-09-29T10:57:14.521910: step 440, loss 0.425222, acc 0.847482

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-440

2017-09-29T10:57:15.146844: step 441, loss 0.502635, acc 0.8125, learning_rate 0.000910249
2017-09-29T10:57:15.333752: step 442, loss 0.404056, acc 0.84375, learning_rate 0.000906942
2017-09-29T10:57:15.526696: step 443, loss 0.339625, acc 0.875, learning_rate 0.000903648
2017-09-29T10:57:15.706969: step 444, loss 0.469509, acc 0.828125, learning_rate 0.000900368
2017-09-29T10:57:15.892997: step 445, loss 0.534974, acc 0.8125, learning_rate 0.000897101
2017-09-29T10:57:16.085021: step 446, loss 0.375411, acc 0.890625, learning_rate 0.000893848
2017-09-29T10:57:16.273336: step 447, loss 0.624876, acc 0.734375, learning_rate 0.000890607
2017-09-29T10:57:16.467155: step 448, loss 0.411207, acc 0.859375, learning_rate 0.00088738
2017-09-29T10:57:16.652731: step 449, loss 0.342864, acc 0.890625, learning_rate 0.000884166
2017-09-29T10:57:16.846643: step 450, loss 0.513311, acc 0.859375, learning_rate 0.000880966
2017-09-29T10:57:17.030091: step 451, loss 0.347702, acc 0.890625, learning_rate 0.000877778
2017-09-29T10:57:17.214308: step 452, loss 0.476501, acc 0.8125, learning_rate 0.000874603
2017-09-29T10:57:17.398455: step 453, loss 0.584771, acc 0.796875, learning_rate 0.000871441
2017-09-29T10:57:17.584235: step 454, loss 0.471439, acc 0.84375, learning_rate 0.000868293
2017-09-29T10:57:17.769840: step 455, loss 0.540286, acc 0.78125, learning_rate 0.000865157
2017-09-29T10:57:17.955368: step 456, loss 0.53622, acc 0.765625, learning_rate 0.000862033
2017-09-29T10:57:18.139611: step 457, loss 0.386175, acc 0.875, learning_rate 0.000858923
2017-09-29T10:57:18.324573: step 458, loss 0.536272, acc 0.78125, learning_rate 0.000855825
2017-09-29T10:57:18.520426: step 459, loss 0.329168, acc 0.90625, learning_rate 0.00085274
2017-09-29T10:57:18.707244: step 460, loss 0.316283, acc 0.859375, learning_rate 0.000849668
2017-09-29T10:57:18.891406: step 461, loss 0.575345, acc 0.828125, learning_rate 0.000846608
2017-09-29T10:57:19.076733: step 462, loss 0.582941, acc 0.828125, learning_rate 0.00084356
2017-09-29T10:57:19.260391: step 463, loss 0.288717, acc 0.9375, learning_rate 0.000840525
2017-09-29T10:57:19.442207: step 464, loss 0.585146, acc 0.796875, learning_rate 0.000837502
2017-09-29T10:57:19.623202: step 465, loss 0.427665, acc 0.875, learning_rate 0.000834492
2017-09-29T10:57:19.805577: step 466, loss 0.316698, acc 0.921875, learning_rate 0.000831494
2017-09-29T10:57:19.988483: step 467, loss 0.521789, acc 0.765625, learning_rate 0.000828508
2017-09-29T10:57:20.173126: step 468, loss 0.707086, acc 0.71875, learning_rate 0.000825535
2017-09-29T10:57:20.356909: step 469, loss 0.322958, acc 0.859375, learning_rate 0.000822573
2017-09-29T10:57:20.542697: step 470, loss 0.519565, acc 0.828125, learning_rate 0.000819624
2017-09-29T10:57:20.725429: step 471, loss 0.544574, acc 0.78125, learning_rate 0.000816687
2017-09-29T10:57:20.907538: step 472, loss 0.425077, acc 0.84375, learning_rate 0.000813761
2017-09-29T10:57:21.086811: step 473, loss 0.401587, acc 0.828125, learning_rate 0.000810848
2017-09-29T10:57:21.269957: step 474, loss 0.62461, acc 0.75, learning_rate 0.000807946
2017-09-29T10:57:21.468946: step 475, loss 0.374868, acc 0.859375, learning_rate 0.000805057
2017-09-29T10:57:21.656219: step 476, loss 0.382158, acc 0.90625, learning_rate 0.000802179
2017-09-29T10:57:21.845780: step 477, loss 0.445721, acc 0.859375, learning_rate 0.000799313
2017-09-29T10:57:22.033183: step 478, loss 0.462245, acc 0.875, learning_rate 0.000796458
2017-09-29T10:57:22.225332: step 479, loss 0.608546, acc 0.75, learning_rate 0.000793616
2017-09-29T10:57:22.414194: step 480, loss 0.373814, acc 0.859375, learning_rate 0.000790784

Evaluation:
2017-09-29T10:57:22.950139: step 480, loss 0.423253, acc 0.833094

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-480

2017-09-29T10:57:23.664326: step 481, loss 0.575034, acc 0.75, learning_rate 0.000787965
2017-09-29T10:57:23.851303: step 482, loss 0.522633, acc 0.8125, learning_rate 0.000785157
2017-09-29T10:57:24.038226: step 483, loss 0.370559, acc 0.875, learning_rate 0.00078236
2017-09-29T10:57:24.239815: step 484, loss 0.581305, acc 0.8125, learning_rate 0.000779575
2017-09-29T10:57:24.430446: step 485, loss 0.367541, acc 0.890625, learning_rate 0.000776801
2017-09-29T10:57:24.620029: step 486, loss 0.389539, acc 0.859375, learning_rate 0.000774038
2017-09-29T10:57:24.806035: step 487, loss 0.458613, acc 0.828125, learning_rate 0.000771287
2017-09-29T10:57:25.001187: step 488, loss 0.433974, acc 0.796875, learning_rate 0.000768547
2017-09-29T10:57:25.199368: step 489, loss 0.415741, acc 0.84375, learning_rate 0.000765818
2017-09-29T10:57:25.354620: step 490, loss 0.396782, acc 0.862745, learning_rate 0.000763101
2017-09-29T10:57:25.546039: step 491, loss 0.489472, acc 0.828125, learning_rate 0.000760394
2017-09-29T10:57:25.736288: step 492, loss 0.249134, acc 0.90625, learning_rate 0.000757698
2017-09-29T10:57:25.940814: step 493, loss 0.604699, acc 0.78125, learning_rate 0.000755014
2017-09-29T10:57:26.133446: step 494, loss 0.342429, acc 0.890625, learning_rate 0.00075234
2017-09-29T10:57:26.315932: step 495, loss 0.256243, acc 0.890625, learning_rate 0.000749677
2017-09-29T10:57:26.509699: step 496, loss 0.499527, acc 0.796875, learning_rate 0.000747026
2017-09-29T10:57:26.700205: step 497, loss 0.295304, acc 0.875, learning_rate 0.000744385
2017-09-29T10:57:26.895590: step 498, loss 0.412494, acc 0.84375, learning_rate 0.000741754
2017-09-29T10:57:27.079098: step 499, loss 0.355086, acc 0.90625, learning_rate 0.000739135
2017-09-29T10:57:27.263799: step 500, loss 0.44158, acc 0.859375, learning_rate 0.000736526
2017-09-29T10:57:27.452743: step 501, loss 0.512182, acc 0.796875, learning_rate 0.000733928
2017-09-29T10:57:27.639737: step 502, loss 0.536898, acc 0.828125, learning_rate 0.00073134
2017-09-29T10:57:27.834702: step 503, loss 0.426673, acc 0.875, learning_rate 0.000728763
2017-09-29T10:57:28.019124: step 504, loss 0.451054, acc 0.796875, learning_rate 0.000726197
2017-09-29T10:57:28.204519: step 505, loss 0.586033, acc 0.796875, learning_rate 0.000723641
2017-09-29T10:57:28.389037: step 506, loss 0.587232, acc 0.765625, learning_rate 0.000721095
2017-09-29T10:57:28.571081: step 507, loss 0.507658, acc 0.796875, learning_rate 0.00071856
2017-09-29T10:57:28.755497: step 508, loss 0.434029, acc 0.8125, learning_rate 0.000716036
2017-09-29T10:57:28.940828: step 509, loss 0.395512, acc 0.84375, learning_rate 0.000713521
2017-09-29T10:57:29.129914: step 510, loss 0.569079, acc 0.78125, learning_rate 0.000711017
2017-09-29T10:57:29.317816: step 511, loss 0.494787, acc 0.796875, learning_rate 0.000708523
2017-09-29T10:57:29.516477: step 512, loss 0.386477, acc 0.84375, learning_rate 0.000706039
2017-09-29T10:57:29.703836: step 513, loss 0.491164, acc 0.8125, learning_rate 0.000703565
2017-09-29T10:57:29.905925: step 514, loss 0.361379, acc 0.875, learning_rate 0.000701102
2017-09-29T10:57:30.094449: step 515, loss 0.331497, acc 0.859375, learning_rate 0.000698648
2017-09-29T10:57:30.285567: step 516, loss 0.373617, acc 0.875, learning_rate 0.000696204
2017-09-29T10:57:30.476678: step 517, loss 0.504832, acc 0.796875, learning_rate 0.000693771
2017-09-29T10:57:30.662349: step 518, loss 0.538337, acc 0.8125, learning_rate 0.000691347
2017-09-29T10:57:30.852590: step 519, loss 0.284838, acc 0.9375, learning_rate 0.000688934
2017-09-29T10:57:31.048061: step 520, loss 0.390412, acc 0.859375, learning_rate 0.00068653

Evaluation:
2017-09-29T10:57:31.625355: step 520, loss 0.402994, acc 0.863309

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-520

2017-09-29T10:57:32.427593: step 521, loss 0.392394, acc 0.875, learning_rate 0.000684136
2017-09-29T10:57:32.614506: step 522, loss 0.317252, acc 0.90625, learning_rate 0.000681751
2017-09-29T10:57:32.801919: step 523, loss 0.588603, acc 0.75, learning_rate 0.000679377
2017-09-29T10:57:32.988931: step 524, loss 0.421686, acc 0.828125, learning_rate 0.000677012
2017-09-29T10:57:33.170566: step 525, loss 0.468618, acc 0.796875, learning_rate 0.000674657
2017-09-29T10:57:33.357049: step 526, loss 0.249502, acc 0.921875, learning_rate 0.000672311
2017-09-29T10:57:33.550839: step 527, loss 0.533858, acc 0.78125, learning_rate 0.000669975
2017-09-29T10:57:33.736786: step 528, loss 0.501946, acc 0.796875, learning_rate 0.000667648
2017-09-29T10:57:33.919947: step 529, loss 0.321168, acc 0.875, learning_rate 0.000665331
2017-09-29T10:57:34.105770: step 530, loss 0.41561, acc 0.875, learning_rate 0.000663024
2017-09-29T10:57:34.290038: step 531, loss 0.403374, acc 0.828125, learning_rate 0.000660726
2017-09-29T10:57:34.477413: step 532, loss 0.398126, acc 0.859375, learning_rate 0.000658437
2017-09-29T10:57:34.661901: step 533, loss 0.367143, acc 0.859375, learning_rate 0.000656158
2017-09-29T10:57:34.850236: step 534, loss 0.47266, acc 0.78125, learning_rate 0.000653888
2017-09-29T10:57:35.031599: step 535, loss 0.440017, acc 0.859375, learning_rate 0.000651627
2017-09-29T10:57:35.216434: step 536, loss 0.337779, acc 0.84375, learning_rate 0.000649375
2017-09-29T10:57:35.407402: step 537, loss 0.333762, acc 0.859375, learning_rate 0.000647133
2017-09-29T10:57:35.603232: step 538, loss 0.469769, acc 0.78125, learning_rate 0.000644899
2017-09-29T10:57:35.790940: step 539, loss 0.489465, acc 0.796875, learning_rate 0.000642675
2017-09-29T10:57:35.975180: step 540, loss 0.482958, acc 0.8125, learning_rate 0.00064046
2017-09-29T10:57:36.158515: step 541, loss 0.34277, acc 0.875, learning_rate 0.000638254
2017-09-29T10:57:36.344828: step 542, loss 0.379576, acc 0.84375, learning_rate 0.000636057
2017-09-29T10:57:36.535193: step 543, loss 0.407623, acc 0.84375, learning_rate 0.000633869
2017-09-29T10:57:36.727056: step 544, loss 0.434651, acc 0.859375, learning_rate 0.00063169
2017-09-29T10:57:36.915379: step 545, loss 0.365559, acc 0.859375, learning_rate 0.00062952
2017-09-29T10:57:37.104762: step 546, loss 0.482079, acc 0.796875, learning_rate 0.000627358
2017-09-29T10:57:37.287407: step 547, loss 0.502731, acc 0.8125, learning_rate 0.000625206
2017-09-29T10:57:37.478280: step 548, loss 0.359308, acc 0.859375, learning_rate 0.000623062
2017-09-29T10:57:37.663998: step 549, loss 0.58399, acc 0.734375, learning_rate 0.000620927
2017-09-29T10:57:37.851457: step 550, loss 0.429963, acc 0.828125, learning_rate 0.000618801
2017-09-29T10:57:38.032048: step 551, loss 0.347779, acc 0.90625, learning_rate 0.000616683
2017-09-29T10:57:38.227466: step 552, loss 0.521948, acc 0.828125, learning_rate 0.000614574
2017-09-29T10:57:38.412030: step 553, loss 0.427547, acc 0.84375, learning_rate 0.000612474
2017-09-29T10:57:38.597287: step 554, loss 0.397114, acc 0.875, learning_rate 0.000610382
2017-09-29T10:57:38.782154: step 555, loss 0.395877, acc 0.84375, learning_rate 0.000608299
2017-09-29T10:57:38.969591: step 556, loss 0.389158, acc 0.859375, learning_rate 0.000606224
2017-09-29T10:57:39.153337: step 557, loss 0.463842, acc 0.828125, learning_rate 0.000604158
2017-09-29T10:57:39.334610: step 558, loss 0.638008, acc 0.734375, learning_rate 0.0006021
2017-09-29T10:57:39.529034: step 559, loss 0.466591, acc 0.796875, learning_rate 0.00060005
2017-09-29T10:57:39.715935: step 560, loss 0.518861, acc 0.828125, learning_rate 0.000598009

Evaluation:
2017-09-29T10:57:40.265984: step 560, loss 0.390187, acc 0.866187

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-560

2017-09-29T10:57:40.905004: step 561, loss 0.261841, acc 0.9375, learning_rate 0.000595977
2017-09-29T10:57:41.099038: step 562, loss 0.443259, acc 0.8125, learning_rate 0.000593952
2017-09-29T10:57:41.279412: step 563, loss 0.520219, acc 0.8125, learning_rate 0.000591936
2017-09-29T10:57:41.464547: step 564, loss 0.291873, acc 0.890625, learning_rate 0.000589928
2017-09-29T10:57:41.655431: step 565, loss 0.320571, acc 0.84375, learning_rate 0.000587928
2017-09-29T10:57:41.839174: step 566, loss 0.571752, acc 0.828125, learning_rate 0.000585937
2017-09-29T10:57:42.030916: step 567, loss 0.431317, acc 0.828125, learning_rate 0.000583953
2017-09-29T10:57:42.222999: step 568, loss 0.340014, acc 0.859375, learning_rate 0.000581978
2017-09-29T10:57:42.409754: step 569, loss 0.446288, acc 0.828125, learning_rate 0.00058001
2017-09-29T10:57:42.597779: step 570, loss 0.451934, acc 0.828125, learning_rate 0.000578051
2017-09-29T10:57:42.781801: step 571, loss 0.387906, acc 0.8125, learning_rate 0.0005761
2017-09-29T10:57:42.965911: step 572, loss 0.551583, acc 0.8125, learning_rate 0.000574157
2017-09-29T10:57:43.145413: step 573, loss 0.34194, acc 0.84375, learning_rate 0.000572221
2017-09-29T10:57:43.327778: step 574, loss 0.316594, acc 0.890625, learning_rate 0.000570294
2017-09-29T10:57:43.511001: step 575, loss 0.44016, acc 0.84375, learning_rate 0.000568374
2017-09-29T10:57:43.706092: step 576, loss 0.309975, acc 0.890625, learning_rate 0.000566462
2017-09-29T10:57:43.893632: step 577, loss 0.519706, acc 0.78125, learning_rate 0.000564558
2017-09-29T10:57:44.079097: step 578, loss 0.542286, acc 0.75, learning_rate 0.000562662
2017-09-29T10:57:44.265442: step 579, loss 0.455526, acc 0.859375, learning_rate 0.000560774
2017-09-29T10:57:44.451421: step 580, loss 0.40619, acc 0.828125, learning_rate 0.000558893
2017-09-29T10:57:44.633050: step 581, loss 0.549439, acc 0.828125, learning_rate 0.00055702
2017-09-29T10:57:44.816994: step 582, loss 0.505094, acc 0.8125, learning_rate 0.000555154
2017-09-29T10:57:45.004278: step 583, loss 0.278297, acc 0.90625, learning_rate 0.000553296
2017-09-29T10:57:45.193406: step 584, loss 0.274758, acc 0.859375, learning_rate 0.000551446
2017-09-29T10:57:45.378527: step 585, loss 0.4758, acc 0.8125, learning_rate 0.000549604
2017-09-29T10:57:45.572192: step 586, loss 0.47412, acc 0.828125, learning_rate 0.000547768
2017-09-29T10:57:45.754876: step 587, loss 0.447048, acc 0.828125, learning_rate 0.000545941
2017-09-29T10:57:45.910566: step 588, loss 0.2645, acc 0.921569, learning_rate 0.00054412
2017-09-29T10:57:46.099029: step 589, loss 0.347831, acc 0.875, learning_rate 0.000542308
2017-09-29T10:57:46.282137: step 590, loss 0.493527, acc 0.796875, learning_rate 0.000540502
2017-09-29T10:57:46.477041: step 591, loss 0.391089, acc 0.828125, learning_rate 0.000538704
2017-09-29T10:57:46.665400: step 592, loss 0.43375, acc 0.828125, learning_rate 0.000536914
2017-09-29T10:57:46.849931: step 593, loss 0.326773, acc 0.859375, learning_rate 0.00053513
2017-09-29T10:57:47.043656: step 594, loss 0.502081, acc 0.828125, learning_rate 0.000533354
2017-09-29T10:57:47.228312: step 595, loss 0.422622, acc 0.828125, learning_rate 0.000531585
2017-09-29T10:57:47.409866: step 596, loss 0.359987, acc 0.890625, learning_rate 0.000529824
2017-09-29T10:57:47.595705: step 597, loss 0.500633, acc 0.84375, learning_rate 0.000528069
2017-09-29T10:57:47.782589: step 598, loss 0.379528, acc 0.8125, learning_rate 0.000526322
2017-09-29T10:57:47.967950: step 599, loss 0.317956, acc 0.859375, learning_rate 0.000524582
2017-09-29T10:57:48.150115: step 600, loss 0.526407, acc 0.859375, learning_rate 0.000522849

Evaluation:
2017-09-29T10:57:48.698539: step 600, loss 0.391058, acc 0.864748

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-600

2017-09-29T10:57:49.413180: step 601, loss 0.317893, acc 0.890625, learning_rate 0.000521123
2017-09-29T10:57:49.599440: step 602, loss 0.421275, acc 0.859375, learning_rate 0.000519404
2017-09-29T10:57:49.782114: step 603, loss 0.257695, acc 0.921875, learning_rate 0.000517692
2017-09-29T10:57:49.966249: step 604, loss 0.308012, acc 0.859375, learning_rate 0.000515987
2017-09-29T10:57:50.151175: step 605, loss 0.378289, acc 0.828125, learning_rate 0.000514289
2017-09-29T10:57:50.335573: step 606, loss 0.565044, acc 0.71875, learning_rate 0.000512598
2017-09-29T10:57:50.519228: step 607, loss 0.484318, acc 0.84375, learning_rate 0.000510914
2017-09-29T10:57:50.708552: step 608, loss 0.392925, acc 0.859375, learning_rate 0.000509237
2017-09-29T10:57:50.891026: step 609, loss 0.273334, acc 0.921875, learning_rate 0.000507566
2017-09-29T10:57:51.075878: step 610, loss 0.480865, acc 0.796875, learning_rate 0.000505903
2017-09-29T10:57:51.262478: step 611, loss 0.384664, acc 0.84375, learning_rate 0.000504246
2017-09-29T10:57:51.452862: step 612, loss 0.402744, acc 0.796875, learning_rate 0.000502596
2017-09-29T10:57:51.638829: step 613, loss 0.328399, acc 0.890625, learning_rate 0.000500953
2017-09-29T10:57:51.839466: step 614, loss 0.388221, acc 0.84375, learning_rate 0.000499316
2017-09-29T10:57:52.027542: step 615, loss 0.281135, acc 0.921875, learning_rate 0.000497686
2017-09-29T10:57:52.212050: step 616, loss 0.595586, acc 0.796875, learning_rate 0.000496063
2017-09-29T10:57:52.396344: step 617, loss 0.402347, acc 0.84375, learning_rate 0.000494446
2017-09-29T10:57:52.588782: step 618, loss 0.390235, acc 0.859375, learning_rate 0.000492836
2017-09-29T10:57:52.770919: step 619, loss 0.338144, acc 0.859375, learning_rate 0.000491233
2017-09-29T10:57:52.958350: step 620, loss 0.360407, acc 0.890625, learning_rate 0.000489636
2017-09-29T10:57:53.138837: step 621, loss 0.591685, acc 0.78125, learning_rate 0.000488045
2017-09-29T10:57:53.334390: step 622, loss 0.387659, acc 0.859375, learning_rate 0.000486461
2017-09-29T10:57:53.519981: step 623, loss 0.185093, acc 0.9375, learning_rate 0.000484884
2017-09-29T10:57:53.706161: step 624, loss 0.297721, acc 0.890625, learning_rate 0.000483313
2017-09-29T10:57:53.892067: step 625, loss 0.346711, acc 0.859375, learning_rate 0.000481748
2017-09-29T10:57:54.073589: step 626, loss 0.619324, acc 0.796875, learning_rate 0.00048019
2017-09-29T10:57:54.259728: step 627, loss 0.3432, acc 0.875, learning_rate 0.000478638
2017-09-29T10:57:54.442391: step 628, loss 0.31041, acc 0.921875, learning_rate 0.000477093
2017-09-29T10:57:54.624194: step 629, loss 0.283909, acc 0.953125, learning_rate 0.000475554
2017-09-29T10:57:54.804915: step 630, loss 0.300765, acc 0.875, learning_rate 0.000474021
2017-09-29T10:57:54.989845: step 631, loss 0.332892, acc 0.875, learning_rate 0.000472494
2017-09-29T10:57:55.173697: step 632, loss 0.258163, acc 0.890625, learning_rate 0.000470974
2017-09-29T10:57:55.356998: step 633, loss 0.383591, acc 0.890625, learning_rate 0.000469459
2017-09-29T10:57:55.539978: step 634, loss 0.360661, acc 0.84375, learning_rate 0.000467951
2017-09-29T10:57:55.725178: step 635, loss 0.629689, acc 0.796875, learning_rate 0.000466449
2017-09-29T10:57:55.908192: step 636, loss 0.432251, acc 0.875, learning_rate 0.000464954
2017-09-29T10:57:56.091693: step 637, loss 0.30089, acc 0.859375, learning_rate 0.000463464
2017-09-29T10:57:56.286414: step 638, loss 0.45873, acc 0.78125, learning_rate 0.00046198
2017-09-29T10:57:56.470011: step 639, loss 0.207405, acc 0.9375, learning_rate 0.000460503
2017-09-29T10:57:56.669061: step 640, loss 0.287311, acc 0.90625, learning_rate 0.000459031

Evaluation:
2017-09-29T10:57:57.231357: step 640, loss 0.39385, acc 0.863309

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-640

2017-09-29T10:57:57.965042: step 641, loss 0.405823, acc 0.859375, learning_rate 0.000457566
2017-09-29T10:57:58.146838: step 642, loss 0.380817, acc 0.875, learning_rate 0.000456106
2017-09-29T10:57:58.328024: step 643, loss 0.232675, acc 0.921875, learning_rate 0.000454653
2017-09-29T10:57:58.524388: step 644, loss 0.352696, acc 0.828125, learning_rate 0.000453205
2017-09-29T10:57:58.710889: step 645, loss 0.309031, acc 0.859375, learning_rate 0.000451764
2017-09-29T10:57:58.895562: step 646, loss 0.375605, acc 0.90625, learning_rate 0.000450328
2017-09-29T10:57:59.078939: step 647, loss 0.35185, acc 0.875, learning_rate 0.000448898
2017-09-29T10:57:59.264064: step 648, loss 0.189317, acc 0.96875, learning_rate 0.000447474
2017-09-29T10:57:59.448430: step 649, loss 0.405688, acc 0.859375, learning_rate 0.000446055
2017-09-29T10:57:59.633490: step 650, loss 0.402819, acc 0.875, learning_rate 0.000444643
2017-09-29T10:57:59.812783: step 651, loss 0.407638, acc 0.796875, learning_rate 0.000443236
2017-09-29T10:58:00.004105: step 652, loss 0.400559, acc 0.84375, learning_rate 0.000441835
2017-09-29T10:58:00.190226: step 653, loss 0.502048, acc 0.859375, learning_rate 0.00044044
2017-09-29T10:58:00.371538: step 654, loss 0.320423, acc 0.875, learning_rate 0.00043905
2017-09-29T10:58:00.564089: step 655, loss 0.453561, acc 0.796875, learning_rate 0.000437666
2017-09-29T10:58:00.748248: step 656, loss 0.475723, acc 0.8125, learning_rate 0.000436288
2017-09-29T10:58:00.932713: step 657, loss 0.271355, acc 0.890625, learning_rate 0.000434915
2017-09-29T10:58:01.115877: step 658, loss 0.278075, acc 0.859375, learning_rate 0.000433548
2017-09-29T10:58:01.303791: step 659, loss 0.400132, acc 0.84375, learning_rate 0.000432187
2017-09-29T10:58:01.496452: step 660, loss 0.649901, acc 0.75, learning_rate 0.000430831
2017-09-29T10:58:01.685245: step 661, loss 0.624679, acc 0.796875, learning_rate 0.000429481
2017-09-29T10:58:01.868839: step 662, loss 0.265434, acc 0.90625, learning_rate 0.000428136
2017-09-29T10:58:02.057090: step 663, loss 0.585104, acc 0.796875, learning_rate 0.000426796
2017-09-29T10:58:02.244608: step 664, loss 0.460772, acc 0.828125, learning_rate 0.000425463
2017-09-29T10:58:02.442189: step 665, loss 0.544306, acc 0.8125, learning_rate 0.000424134
2017-09-29T10:58:02.624511: step 666, loss 0.396585, acc 0.84375, learning_rate 0.000422811
2017-09-29T10:58:02.807855: step 667, loss 0.293208, acc 0.921875, learning_rate 0.000421493
2017-09-29T10:58:02.990079: step 668, loss 0.237934, acc 0.9375, learning_rate 0.000420181
2017-09-29T10:58:03.173938: step 669, loss 0.400141, acc 0.890625, learning_rate 0.000418874
2017-09-29T10:58:03.355500: step 670, loss 0.435907, acc 0.875, learning_rate 0.000417573
2017-09-29T10:58:03.542185: step 671, loss 0.350307, acc 0.890625, learning_rate 0.000416276
2017-09-29T10:58:03.723981: step 672, loss 0.339113, acc 0.875, learning_rate 0.000414985
2017-09-29T10:58:03.910867: step 673, loss 0.415341, acc 0.859375, learning_rate 0.0004137
2017-09-29T10:58:04.095631: step 674, loss 0.292945, acc 0.875, learning_rate 0.000412419
2017-09-29T10:58:04.280355: step 675, loss 0.509181, acc 0.828125, learning_rate 0.000411144
2017-09-29T10:58:04.474821: step 676, loss 0.482143, acc 0.859375, learning_rate 0.000409874
2017-09-29T10:58:04.662345: step 677, loss 0.483904, acc 0.84375, learning_rate 0.000408609
2017-09-29T10:58:04.844840: step 678, loss 0.530198, acc 0.765625, learning_rate 0.00040735
2017-09-29T10:58:05.028913: step 679, loss 0.29073, acc 0.90625, learning_rate 0.000406095
2017-09-29T10:58:05.212028: step 680, loss 0.423665, acc 0.8125, learning_rate 0.000404846

Evaluation:
2017-09-29T10:58:05.763366: step 680, loss 0.385936, acc 0.861871

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-680

2017-09-29T10:58:06.552889: step 681, loss 0.198428, acc 0.953125, learning_rate 0.000403601
2017-09-29T10:58:06.746995: step 682, loss 0.417964, acc 0.890625, learning_rate 0.000402362
2017-09-29T10:58:06.933937: step 683, loss 0.308992, acc 0.859375, learning_rate 0.000401128
2017-09-29T10:58:07.124290: step 684, loss 0.362374, acc 0.875, learning_rate 0.000399899
2017-09-29T10:58:07.306055: step 685, loss 0.249335, acc 0.9375, learning_rate 0.000398675
2017-09-29T10:58:07.460061: step 686, loss 0.302855, acc 0.921569, learning_rate 0.000397456
2017-09-29T10:58:07.652026: step 687, loss 0.522885, acc 0.734375, learning_rate 0.000396241
2017-09-29T10:58:07.836130: step 688, loss 0.212157, acc 0.921875, learning_rate 0.000395032
2017-09-29T10:58:08.019259: step 689, loss 0.389984, acc 0.84375, learning_rate 0.000393828
2017-09-29T10:58:08.203654: step 690, loss 0.351099, acc 0.875, learning_rate 0.000392629
2017-09-29T10:58:08.397804: step 691, loss 0.492588, acc 0.84375, learning_rate 0.000391434
2017-09-29T10:58:08.585073: step 692, loss 0.358773, acc 0.890625, learning_rate 0.000390245
2017-09-29T10:58:08.769827: step 693, loss 0.269187, acc 0.90625, learning_rate 0.00038906
2017-09-29T10:58:08.962477: step 694, loss 0.317826, acc 0.875, learning_rate 0.00038788
2017-09-29T10:58:09.150766: step 695, loss 0.451343, acc 0.8125, learning_rate 0.000386705
2017-09-29T10:58:09.334137: step 696, loss 0.365698, acc 0.859375, learning_rate 0.000385535
2017-09-29T10:58:09.518059: step 697, loss 0.457539, acc 0.828125, learning_rate 0.000384369
2017-09-29T10:58:09.698540: step 698, loss 0.408854, acc 0.828125, learning_rate 0.000383209
2017-09-29T10:58:09.883298: step 699, loss 0.365118, acc 0.890625, learning_rate 0.000382053
2017-09-29T10:58:10.070419: step 700, loss 0.421694, acc 0.84375, learning_rate 0.000380901
2017-09-29T10:58:10.251586: step 701, loss 0.282046, acc 0.90625, learning_rate 0.000379755
2017-09-29T10:58:10.442204: step 702, loss 0.463999, acc 0.828125, learning_rate 0.000378613
2017-09-29T10:58:10.633197: step 703, loss 0.353907, acc 0.90625, learning_rate 0.000377476
2017-09-29T10:58:10.824225: step 704, loss 0.268323, acc 0.90625, learning_rate 0.000376343
2017-09-29T10:58:11.012658: step 705, loss 0.327262, acc 0.859375, learning_rate 0.000375215
2017-09-29T10:58:11.196531: step 706, loss 0.544189, acc 0.796875, learning_rate 0.000374092
2017-09-29T10:58:11.381538: step 707, loss 0.46817, acc 0.859375, learning_rate 0.000372973
2017-09-29T10:58:11.567101: step 708, loss 0.34726, acc 0.875, learning_rate 0.000371859
2017-09-29T10:58:11.756188: step 709, loss 0.489259, acc 0.796875, learning_rate 0.000370749
2017-09-29T10:58:11.948804: step 710, loss 0.557383, acc 0.828125, learning_rate 0.000369644
2017-09-29T10:58:12.140812: step 711, loss 0.483747, acc 0.828125, learning_rate 0.000368543
2017-09-29T10:58:12.320188: step 712, loss 0.27941, acc 0.953125, learning_rate 0.000367447
2017-09-29T10:58:12.501868: step 713, loss 0.343247, acc 0.875, learning_rate 0.000366356
2017-09-29T10:58:12.687402: step 714, loss 0.332917, acc 0.890625, learning_rate 0.000365268
2017-09-29T10:58:12.869446: step 715, loss 0.45985, acc 0.78125, learning_rate 0.000364186
2017-09-29T10:58:13.053631: step 716, loss 0.514401, acc 0.8125, learning_rate 0.000363107
2017-09-29T10:58:13.236062: step 717, loss 0.410572, acc 0.875, learning_rate 0.000362033
2017-09-29T10:58:13.420400: step 718, loss 0.362649, acc 0.90625, learning_rate 0.000360964
2017-09-29T10:58:13.607791: step 719, loss 0.338357, acc 0.859375, learning_rate 0.000359899
2017-09-29T10:58:13.791146: step 720, loss 0.360675, acc 0.875, learning_rate 0.000358838

Evaluation:
2017-09-29T10:58:14.338136: step 720, loss 0.382134, acc 0.87482

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-720

2017-09-29T10:58:14.967050: step 721, loss 0.355268, acc 0.84375, learning_rate 0.000357781
2017-09-29T10:58:15.152829: step 722, loss 0.579782, acc 0.828125, learning_rate 0.000356729
2017-09-29T10:58:15.334445: step 723, loss 0.265101, acc 0.90625, learning_rate 0.000355681
2017-09-29T10:58:15.533868: step 724, loss 0.374509, acc 0.84375, learning_rate 0.000354637
2017-09-29T10:58:15.718760: step 725, loss 0.468488, acc 0.84375, learning_rate 0.000353598
2017-09-29T10:58:15.910488: step 726, loss 0.502023, acc 0.765625, learning_rate 0.000352563
2017-09-29T10:58:16.098142: step 727, loss 0.353182, acc 0.875, learning_rate 0.000351532
2017-09-29T10:58:16.282605: step 728, loss 0.490203, acc 0.796875, learning_rate 0.000350505
2017-09-29T10:58:16.465271: step 729, loss 0.390072, acc 0.859375, learning_rate 0.000349483
2017-09-29T10:58:16.655226: step 730, loss 0.347542, acc 0.875, learning_rate 0.000348465
2017-09-29T10:58:16.842564: step 731, loss 0.285244, acc 0.90625, learning_rate 0.00034745
2017-09-29T10:58:17.031213: step 732, loss 0.550569, acc 0.796875, learning_rate 0.00034644
2017-09-29T10:58:17.221920: step 733, loss 0.357302, acc 0.828125, learning_rate 0.000345434
2017-09-29T10:58:17.408853: step 734, loss 0.354768, acc 0.84375, learning_rate 0.000344433
2017-09-29T10:58:17.600990: step 735, loss 0.370591, acc 0.828125, learning_rate 0.000343435
2017-09-29T10:58:17.783111: step 736, loss 0.396505, acc 0.84375, learning_rate 0.000342441
2017-09-29T10:58:17.987996: step 737, loss 0.436506, acc 0.875, learning_rate 0.000341452
2017-09-29T10:58:18.174214: step 738, loss 0.346495, acc 0.90625, learning_rate 0.000340466
2017-09-29T10:58:18.353390: step 739, loss 0.340709, acc 0.84375, learning_rate 0.000339485
2017-09-29T10:58:18.537314: step 740, loss 0.33103, acc 0.875, learning_rate 0.000338507
2017-09-29T10:58:18.726357: step 741, loss 0.190761, acc 0.96875, learning_rate 0.000337534
2017-09-29T10:58:18.910160: step 742, loss 0.402554, acc 0.859375, learning_rate 0.000336564
2017-09-29T10:58:19.095762: step 743, loss 0.368208, acc 0.875, learning_rate 0.000335598
2017-09-29T10:58:19.282189: step 744, loss 0.405916, acc 0.90625, learning_rate 0.000334637
2017-09-29T10:58:19.463509: step 745, loss 0.211842, acc 0.9375, learning_rate 0.000333679
2017-09-29T10:58:19.647130: step 746, loss 0.470311, acc 0.8125, learning_rate 0.000332725
2017-09-29T10:58:19.830178: step 747, loss 0.269111, acc 0.921875, learning_rate 0.000331775
2017-09-29T10:58:20.016888: step 748, loss 0.20833, acc 0.890625, learning_rate 0.000330829
2017-09-29T10:58:20.200130: step 749, loss 0.355886, acc 0.890625, learning_rate 0.000329887
2017-09-29T10:58:20.379693: step 750, loss 0.313495, acc 0.875, learning_rate 0.000328949
2017-09-29T10:58:20.581701: step 751, loss 0.300957, acc 0.90625, learning_rate 0.000328014
2017-09-29T10:58:20.768641: step 752, loss 0.372632, acc 0.875, learning_rate 0.000327083
2017-09-29T10:58:20.962021: step 753, loss 0.476818, acc 0.84375, learning_rate 0.000326157
2017-09-29T10:58:21.143425: step 754, loss 0.386862, acc 0.859375, learning_rate 0.000325233
2017-09-29T10:58:21.326976: step 755, loss 0.525006, acc 0.84375, learning_rate 0.000324314
2017-09-29T10:58:21.521105: step 756, loss 0.415089, acc 0.828125, learning_rate 0.000323399
2017-09-29T10:58:21.703694: step 757, loss 0.24676, acc 0.921875, learning_rate 0.000322487
2017-09-29T10:58:21.889391: step 758, loss 0.260524, acc 0.921875, learning_rate 0.000321579
2017-09-29T10:58:22.088216: step 759, loss 0.295978, acc 0.859375, learning_rate 0.000320674
2017-09-29T10:58:22.311764: step 760, loss 0.527915, acc 0.828125, learning_rate 0.000319773

Evaluation:
2017-09-29T10:58:22.950024: step 760, loss 0.376439, acc 0.867626

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-760

2017-09-29T10:58:23.667498: step 761, loss 0.413931, acc 0.875, learning_rate 0.000318876
2017-09-29T10:58:23.854598: step 762, loss 0.295185, acc 0.875, learning_rate 0.000317983
2017-09-29T10:58:24.041194: step 763, loss 0.287103, acc 0.9375, learning_rate 0.000317093
2017-09-29T10:58:24.223970: step 764, loss 0.603361, acc 0.78125, learning_rate 0.000316207
2017-09-29T10:58:24.409091: step 765, loss 0.333911, acc 0.859375, learning_rate 0.000315325
2017-09-29T10:58:24.594551: step 766, loss 0.386559, acc 0.875, learning_rate 0.000314446
2017-09-29T10:58:24.779853: step 767, loss 0.294665, acc 0.90625, learning_rate 0.00031357
2017-09-29T10:58:24.965310: step 768, loss 0.316933, acc 0.90625, learning_rate 0.000312699
2017-09-29T10:58:25.145990: step 769, loss 0.324364, acc 0.875, learning_rate 0.00031183
2017-09-29T10:58:25.332796: step 770, loss 0.317744, acc 0.921875, learning_rate 0.000310966
2017-09-29T10:58:25.522695: step 771, loss 0.362384, acc 0.84375, learning_rate 0.000310105
2017-09-29T10:58:25.709289: step 772, loss 0.397907, acc 0.890625, learning_rate 0.000309247
2017-09-29T10:58:25.894365: step 773, loss 0.372901, acc 0.859375, learning_rate 0.000308393
2017-09-29T10:58:26.076270: step 774, loss 0.337241, acc 0.90625, learning_rate 0.000307542
2017-09-29T10:58:26.259305: step 775, loss 0.286399, acc 0.875, learning_rate 0.000306695
2017-09-29T10:58:26.448512: step 776, loss 0.402335, acc 0.84375, learning_rate 0.000305852
2017-09-29T10:58:26.630406: step 777, loss 0.297129, acc 0.921875, learning_rate 0.000305011
2017-09-29T10:58:26.816250: step 778, loss 0.478155, acc 0.859375, learning_rate 0.000304174
2017-09-29T10:58:27.000239: step 779, loss 0.368462, acc 0.859375, learning_rate 0.000303341
2017-09-29T10:58:27.185167: step 780, loss 0.27982, acc 0.90625, learning_rate 0.000302511
2017-09-29T10:58:27.376114: step 781, loss 0.386368, acc 0.84375, learning_rate 0.000301684
2017-09-29T10:58:27.579630: step 782, loss 0.373612, acc 0.84375, learning_rate 0.000300861
2017-09-29T10:58:27.765959: step 783, loss 0.26629, acc 0.90625, learning_rate 0.000300041
2017-09-29T10:58:27.925267: step 784, loss 0.342812, acc 0.882353, learning_rate 0.000299225
2017-09-29T10:58:28.114379: step 785, loss 0.443317, acc 0.875, learning_rate 0.000298412
2017-09-29T10:58:28.303120: step 786, loss 0.461724, acc 0.796875, learning_rate 0.000297602
2017-09-29T10:58:28.488960: step 787, loss 0.288173, acc 0.921875, learning_rate 0.000296795
2017-09-29T10:58:28.672453: step 788, loss 0.286337, acc 0.90625, learning_rate 0.000295992
2017-09-29T10:58:28.855191: step 789, loss 0.391078, acc 0.859375, learning_rate 0.000295192
2017-09-29T10:58:29.043682: step 790, loss 0.226779, acc 0.890625, learning_rate 0.000294395
2017-09-29T10:58:29.224050: step 791, loss 0.540313, acc 0.8125, learning_rate 0.000293602
2017-09-29T10:58:29.408765: step 792, loss 0.36076, acc 0.90625, learning_rate 0.000292812
2017-09-29T10:58:29.593288: step 793, loss 0.476604, acc 0.859375, learning_rate 0.000292025
2017-09-29T10:58:29.772863: step 794, loss 0.483809, acc 0.828125, learning_rate 0.000291241
2017-09-29T10:58:29.956788: step 795, loss 0.502732, acc 0.8125, learning_rate 0.00029046
2017-09-29T10:58:30.143124: step 796, loss 0.418675, acc 0.875, learning_rate 0.000289683
2017-09-29T10:58:30.329154: step 797, loss 0.400406, acc 0.890625, learning_rate 0.000288908
2017-09-29T10:58:30.517738: step 798, loss 0.350676, acc 0.859375, learning_rate 0.000288137
2017-09-29T10:58:30.701667: step 799, loss 0.457583, acc 0.8125, learning_rate 0.000287369
2017-09-29T10:58:30.886326: step 800, loss 0.269538, acc 0.921875, learning_rate 0.000286605

Evaluation:
2017-09-29T10:58:31.416579: step 800, loss 0.369171, acc 0.870504

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-800

2017-09-29T10:58:32.145019: step 801, loss 0.416933, acc 0.875, learning_rate 0.000285843
2017-09-29T10:58:32.338194: step 802, loss 0.343273, acc 0.875, learning_rate 0.000285084
2017-09-29T10:58:32.551426: step 803, loss 0.347137, acc 0.875, learning_rate 0.000284329
2017-09-29T10:58:32.752192: step 804, loss 0.435625, acc 0.84375, learning_rate 0.000283577
2017-09-29T10:58:32.954715: step 805, loss 0.349791, acc 0.84375, learning_rate 0.000282827
2017-09-29T10:58:33.183829: step 806, loss 0.509429, acc 0.828125, learning_rate 0.000282081
2017-09-29T10:58:33.370762: step 807, loss 0.273432, acc 0.90625, learning_rate 0.000281338
2017-09-29T10:58:33.579438: step 808, loss 0.369457, acc 0.890625, learning_rate 0.000280598
2017-09-29T10:58:33.764119: step 809, loss 0.363575, acc 0.921875, learning_rate 0.00027986
2017-09-29T10:58:33.953082: step 810, loss 0.407376, acc 0.84375, learning_rate 0.000279126
2017-09-29T10:58:34.136111: step 811, loss 0.414756, acc 0.828125, learning_rate 0.000278395
2017-09-29T10:58:34.329986: step 812, loss 0.42031, acc 0.859375, learning_rate 0.000277667
2017-09-29T10:58:34.515915: step 813, loss 0.351721, acc 0.875, learning_rate 0.000276942
2017-09-29T10:58:34.699567: step 814, loss 0.569645, acc 0.828125, learning_rate 0.00027622
2017-09-29T10:58:34.894814: step 815, loss 0.254343, acc 0.90625, learning_rate 0.0002755
2017-09-29T10:58:35.081515: step 816, loss 0.466592, acc 0.875, learning_rate 0.000274784
2017-09-29T10:58:35.266263: step 817, loss 0.329556, acc 0.875, learning_rate 0.000274071
2017-09-29T10:58:35.451463: step 818, loss 0.297818, acc 0.921875, learning_rate 0.00027336
2017-09-29T10:58:35.634603: step 819, loss 0.327829, acc 0.90625, learning_rate 0.000272652
2017-09-29T10:58:35.815548: step 820, loss 0.429516, acc 0.765625, learning_rate 0.000271948
2017-09-29T10:58:35.997793: step 821, loss 0.250825, acc 0.9375, learning_rate 0.000271246
2017-09-29T10:58:36.183202: step 822, loss 0.476807, acc 0.8125, learning_rate 0.000270547
2017-09-29T10:58:36.365729: step 823, loss 0.383669, acc 0.859375, learning_rate 0.000269851
2017-09-29T10:58:36.553230: step 824, loss 0.441767, acc 0.84375, learning_rate 0.000269157
2017-09-29T10:58:36.742878: step 825, loss 0.253094, acc 0.9375, learning_rate 0.000268467
2017-09-29T10:58:36.935073: step 826, loss 0.347839, acc 0.875, learning_rate 0.000267779
2017-09-29T10:58:37.127003: step 827, loss 0.334218, acc 0.90625, learning_rate 0.000267094
2017-09-29T10:58:37.315627: step 828, loss 0.445273, acc 0.859375, learning_rate 0.000266412
2017-09-29T10:58:37.510687: step 829, loss 0.398423, acc 0.90625, learning_rate 0.000265733
2017-09-29T10:58:37.700544: step 830, loss 0.460749, acc 0.8125, learning_rate 0.000265057
2017-09-29T10:58:37.894486: step 831, loss 0.287505, acc 0.90625, learning_rate 0.000264383
2017-09-29T10:58:38.080706: step 832, loss 0.2928, acc 0.890625, learning_rate 0.000263712
2017-09-29T10:58:38.269032: step 833, loss 0.303686, acc 0.875, learning_rate 0.000263044
2017-09-29T10:58:38.452770: step 834, loss 0.302076, acc 0.921875, learning_rate 0.000262378
2017-09-29T10:58:38.637620: step 835, loss 0.359577, acc 0.828125, learning_rate 0.000261715
2017-09-29T10:58:38.819060: step 836, loss 0.302397, acc 0.890625, learning_rate 0.000261055
2017-09-29T10:58:39.000104: step 837, loss 0.271152, acc 0.921875, learning_rate 0.000260398
2017-09-29T10:58:39.182387: step 838, loss 0.356413, acc 0.890625, learning_rate 0.000259743
2017-09-29T10:58:39.374047: step 839, loss 0.250263, acc 0.90625, learning_rate 0.000259091
2017-09-29T10:58:39.571329: step 840, loss 0.605316, acc 0.75, learning_rate 0.000258442

Evaluation:
2017-09-29T10:58:40.058715: step 840, loss 0.375684, acc 0.870504

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-840

2017-09-29T10:58:40.848062: step 841, loss 0.49946, acc 0.78125, learning_rate 0.000257795
2017-09-29T10:58:41.033222: step 842, loss 0.24049, acc 0.9375, learning_rate 0.000257151
2017-09-29T10:58:41.208959: step 843, loss 0.280105, acc 0.90625, learning_rate 0.00025651
2017-09-29T10:58:41.395334: step 844, loss 0.330688, acc 0.875, learning_rate 0.000255871
2017-09-29T10:58:41.578127: step 845, loss 0.44757, acc 0.859375, learning_rate 0.000255235
2017-09-29T10:58:41.761521: step 846, loss 0.406826, acc 0.875, learning_rate 0.000254601
2017-09-29T10:58:41.948576: step 847, loss 0.535067, acc 0.8125, learning_rate 0.00025397
2017-09-29T10:58:42.134152: step 848, loss 0.403342, acc 0.84375, learning_rate 0.000253341
2017-09-29T10:58:42.321644: step 849, loss 0.361425, acc 0.84375, learning_rate 0.000252716
2017-09-29T10:58:42.506780: step 850, loss 0.319546, acc 0.90625, learning_rate 0.000252092
2017-09-29T10:58:42.700167: step 851, loss 0.341584, acc 0.859375, learning_rate 0.000251471
2017-09-29T10:58:42.894633: step 852, loss 0.387313, acc 0.84375, learning_rate 0.000250853
2017-09-29T10:58:43.081359: step 853, loss 0.505023, acc 0.828125, learning_rate 0.000250237
2017-09-29T10:58:43.263695: step 854, loss 0.327352, acc 0.875, learning_rate 0.000249624
2017-09-29T10:58:43.456335: step 855, loss 0.323641, acc 0.84375, learning_rate 0.000249013
2017-09-29T10:58:43.638974: step 856, loss 0.309839, acc 0.90625, learning_rate 0.000248405
2017-09-29T10:58:43.822256: step 857, loss 0.2955, acc 0.875, learning_rate 0.000247799
2017-09-29T10:58:44.008893: step 858, loss 0.203052, acc 0.921875, learning_rate 0.000247196
2017-09-29T10:58:44.190142: step 859, loss 0.233366, acc 0.90625, learning_rate 0.000246595
2017-09-29T10:58:44.373711: step 860, loss 0.369317, acc 0.890625, learning_rate 0.000245997
2017-09-29T10:58:44.566337: step 861, loss 0.273428, acc 0.921875, learning_rate 0.000245401
2017-09-29T10:58:44.748045: step 862, loss 0.398875, acc 0.859375, learning_rate 0.000244808
2017-09-29T10:58:44.931656: step 863, loss 0.375145, acc 0.84375, learning_rate 0.000244216
2017-09-29T10:58:45.115259: step 864, loss 0.347074, acc 0.859375, learning_rate 0.000243628
2017-09-29T10:58:45.297616: step 865, loss 0.422526, acc 0.828125, learning_rate 0.000243042
2017-09-29T10:58:45.491919: step 866, loss 0.330225, acc 0.859375, learning_rate 0.000242458
2017-09-29T10:58:45.677026: step 867, loss 0.258018, acc 0.921875, learning_rate 0.000241876
2017-09-29T10:58:45.862293: step 868, loss 0.413955, acc 0.859375, learning_rate 0.000241297
2017-09-29T10:58:46.045733: step 869, loss 0.367516, acc 0.828125, learning_rate 0.00024072
2017-09-29T10:58:46.229842: step 870, loss 0.377761, acc 0.921875, learning_rate 0.000240146
2017-09-29T10:58:46.414369: step 871, loss 0.429079, acc 0.84375, learning_rate 0.000239574
2017-09-29T10:58:46.602886: step 872, loss 0.255674, acc 0.90625, learning_rate 0.000239004
2017-09-29T10:58:46.785408: step 873, loss 0.35537, acc 0.875, learning_rate 0.000238437
2017-09-29T10:58:46.974812: step 874, loss 0.39072, acc 0.859375, learning_rate 0.000237872
2017-09-29T10:58:47.161233: step 875, loss 0.428935, acc 0.875, learning_rate 0.000237309
2017-09-29T10:58:47.346871: step 876, loss 0.334202, acc 0.90625, learning_rate 0.000236749
2017-09-29T10:58:47.530700: step 877, loss 0.200878, acc 0.9375, learning_rate 0.00023619
2017-09-29T10:58:47.712428: step 878, loss 0.327154, acc 0.921875, learning_rate 0.000235635
2017-09-29T10:58:47.905537: step 879, loss 0.548967, acc 0.765625, learning_rate 0.000235081
2017-09-29T10:58:48.093919: step 880, loss 0.309513, acc 0.875, learning_rate 0.00023453

Evaluation:
2017-09-29T10:58:48.592259: step 880, loss 0.372096, acc 0.869065

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-880

2017-09-29T10:58:49.225430: step 881, loss 0.212679, acc 0.953125, learning_rate 0.00023398
2017-09-29T10:58:49.379510: step 882, loss 0.337365, acc 0.921569, learning_rate 0.000233434
2017-09-29T10:58:49.564484: step 883, loss 0.41446, acc 0.828125, learning_rate 0.000232889
2017-09-29T10:58:49.748586: step 884, loss 0.520002, acc 0.828125, learning_rate 0.000232346
2017-09-29T10:58:49.940457: step 885, loss 0.371122, acc 0.828125, learning_rate 0.000231806
2017-09-29T10:58:50.126074: step 886, loss 0.437067, acc 0.8125, learning_rate 0.000231268
2017-09-29T10:58:50.312595: step 887, loss 0.560308, acc 0.828125, learning_rate 0.000230732
2017-09-29T10:58:50.496149: step 888, loss 0.331125, acc 0.875, learning_rate 0.000230199
2017-09-29T10:58:50.679906: step 889, loss 0.410603, acc 0.90625, learning_rate 0.000229667
2017-09-29T10:58:50.863322: step 890, loss 0.437867, acc 0.859375, learning_rate 0.000229138
2017-09-29T10:58:51.048172: step 891, loss 0.457211, acc 0.8125, learning_rate 0.000228611
2017-09-29T10:58:51.234775: step 892, loss 0.466187, acc 0.859375, learning_rate 0.000228086
2017-09-29T10:58:51.424733: step 893, loss 0.295514, acc 0.890625, learning_rate 0.000227563
2017-09-29T10:58:51.608911: step 894, loss 0.504404, acc 0.796875, learning_rate 0.000227043
2017-09-29T10:58:51.789197: step 895, loss 0.462131, acc 0.828125, learning_rate 0.000226524
2017-09-29T10:58:51.978887: step 896, loss 0.297664, acc 0.90625, learning_rate 0.000226008
2017-09-29T10:58:52.164117: step 897, loss 0.36421, acc 0.84375, learning_rate 0.000225493
2017-09-29T10:58:52.348766: step 898, loss 0.335944, acc 0.859375, learning_rate 0.000224981
2017-09-29T10:58:52.533064: step 899, loss 0.313466, acc 0.90625, learning_rate 0.000224471
2017-09-29T10:58:52.717599: step 900, loss 0.458013, acc 0.84375, learning_rate 0.000223963
2017-09-29T10:58:52.902848: step 901, loss 0.31309, acc 0.921875, learning_rate 0.000223457
2017-09-29T10:58:53.087425: step 902, loss 0.386007, acc 0.84375, learning_rate 0.000222953
2017-09-29T10:58:53.271645: step 903, loss 0.350711, acc 0.875, learning_rate 0.000222451
2017-09-29T10:58:53.466751: step 904, loss 0.379872, acc 0.84375, learning_rate 0.000221951
2017-09-29T10:58:53.655098: step 905, loss 0.541275, acc 0.78125, learning_rate 0.000221453
2017-09-29T10:58:53.839649: step 906, loss 0.323307, acc 0.890625, learning_rate 0.000220958
2017-09-29T10:58:54.026286: step 907, loss 0.332095, acc 0.875, learning_rate 0.000220464
2017-09-29T10:58:54.208902: step 908, loss 0.445305, acc 0.875, learning_rate 0.000219972
2017-09-29T10:58:54.394373: step 909, loss 0.39037, acc 0.890625, learning_rate 0.000219483
2017-09-29T10:58:54.589478: step 910, loss 0.364314, acc 0.890625, learning_rate 0.000218995
2017-09-29T10:58:54.774200: step 911, loss 0.334052, acc 0.9375, learning_rate 0.000218509
2017-09-29T10:58:54.956339: step 912, loss 0.272572, acc 0.90625, learning_rate 0.000218025
2017-09-29T10:58:55.139285: step 913, loss 0.41667, acc 0.859375, learning_rate 0.000217544
2017-09-29T10:58:55.330521: step 914, loss 0.314981, acc 0.890625, learning_rate 0.000217064
2017-09-29T10:58:55.512545: step 915, loss 0.308859, acc 0.90625, learning_rate 0.000216586
2017-09-29T10:58:55.704015: step 916, loss 0.214372, acc 0.96875, learning_rate 0.00021611
2017-09-29T10:58:55.886976: step 917, loss 0.34879, acc 0.828125, learning_rate 0.000215636
2017-09-29T10:58:56.071030: step 918, loss 0.334462, acc 0.921875, learning_rate 0.000215164
2017-09-29T10:58:56.255409: step 919, loss 0.370879, acc 0.890625, learning_rate 0.000214694
2017-09-29T10:58:56.444489: step 920, loss 0.222919, acc 0.953125, learning_rate 0.000214226

Evaluation:
2017-09-29T10:58:56.914285: step 920, loss 0.367109, acc 0.870504

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-920

2017-09-29T10:58:57.629208: step 921, loss 0.453385, acc 0.828125, learning_rate 0.00021376
2017-09-29T10:58:57.810514: step 922, loss 0.245522, acc 0.90625, learning_rate 0.000213295
2017-09-29T10:58:57.993413: step 923, loss 0.314229, acc 0.890625, learning_rate 0.000212833
2017-09-29T10:58:58.178291: step 924, loss 0.540971, acc 0.8125, learning_rate 0.000212372
2017-09-29T10:58:58.359490: step 925, loss 0.40093, acc 0.828125, learning_rate 0.000211914
2017-09-29T10:58:58.543452: step 926, loss 0.308125, acc 0.90625, learning_rate 0.000211457
2017-09-29T10:58:58.729554: step 927, loss 0.347339, acc 0.84375, learning_rate 0.000211002
2017-09-29T10:58:58.911852: step 928, loss 0.43443, acc 0.859375, learning_rate 0.000210549
2017-09-29T10:58:59.094562: step 929, loss 0.357585, acc 0.890625, learning_rate 0.000210098
2017-09-29T10:58:59.275151: step 930, loss 0.399132, acc 0.84375, learning_rate 0.000209648
2017-09-29T10:58:59.459132: step 931, loss 0.462368, acc 0.859375, learning_rate 0.000209201
2017-09-29T10:58:59.654996: step 932, loss 0.372911, acc 0.859375, learning_rate 0.000208755
2017-09-29T10:58:59.844474: step 933, loss 0.332959, acc 0.890625, learning_rate 0.000208311
2017-09-29T10:59:00.026137: step 934, loss 0.462605, acc 0.796875, learning_rate 0.000207869
2017-09-29T10:59:00.212110: step 935, loss 0.391289, acc 0.875, learning_rate 0.000207429
2017-09-29T10:59:00.396344: step 936, loss 0.408406, acc 0.859375, learning_rate 0.00020699
2017-09-29T10:59:00.582842: step 937, loss 0.366567, acc 0.859375, learning_rate 0.000206554
2017-09-29T10:59:00.764896: step 938, loss 0.333722, acc 0.875, learning_rate 0.000206119
2017-09-29T10:59:00.951860: step 939, loss 0.340478, acc 0.859375, learning_rate 0.000205685
2017-09-29T10:59:01.136153: step 940, loss 0.315418, acc 0.890625, learning_rate 0.000205254
2017-09-29T10:59:01.335502: step 941, loss 0.215995, acc 0.921875, learning_rate 0.000204824
2017-09-29T10:59:01.519983: step 942, loss 0.169901, acc 0.96875, learning_rate 0.000204397
2017-09-29T10:59:01.716682: step 943, loss 0.482072, acc 0.8125, learning_rate 0.00020397
2017-09-29T10:59:01.902075: step 944, loss 0.232105, acc 0.953125, learning_rate 0.000203546
2017-09-29T10:59:02.098741: step 945, loss 0.223461, acc 0.9375, learning_rate 0.000203123
2017-09-29T10:59:02.287604: step 946, loss 0.353712, acc 0.875, learning_rate 0.000202702
2017-09-29T10:59:02.476765: step 947, loss 0.306855, acc 0.890625, learning_rate 0.000202283
2017-09-29T10:59:02.664240: step 948, loss 0.297289, acc 0.875, learning_rate 0.000201866
2017-09-29T10:59:02.846268: step 949, loss 0.375265, acc 0.828125, learning_rate 0.00020145
2017-09-29T10:59:03.028268: step 950, loss 0.230573, acc 0.921875, learning_rate 0.000201036
2017-09-29T10:59:03.208476: step 951, loss 0.216778, acc 0.921875, learning_rate 0.000200623
2017-09-29T10:59:03.394982: step 952, loss 0.288255, acc 0.90625, learning_rate 0.000200213
2017-09-29T10:59:03.586591: step 953, loss 0.350501, acc 0.875, learning_rate 0.000199804
2017-09-29T10:59:03.770028: step 954, loss 0.327826, acc 0.90625, learning_rate 0.000199396
2017-09-29T10:59:03.955455: step 955, loss 0.493202, acc 0.859375, learning_rate 0.000198991
2017-09-29T10:59:04.140171: step 956, loss 0.426315, acc 0.875, learning_rate 0.000198587
2017-09-29T10:59:04.321939: step 957, loss 0.251367, acc 0.921875, learning_rate 0.000198184
2017-09-29T10:59:04.513709: step 958, loss 0.401364, acc 0.859375, learning_rate 0.000197783
2017-09-29T10:59:04.700163: step 959, loss 0.308275, acc 0.921875, learning_rate 0.000197384
2017-09-29T10:59:04.881270: step 960, loss 0.432554, acc 0.890625, learning_rate 0.000196987

Evaluation:
2017-09-29T10:59:05.371859: step 960, loss 0.366079, acc 0.870504

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-960

2017-09-29T10:59:06.069666: step 961, loss 0.36847, acc 0.859375, learning_rate 0.000196591
2017-09-29T10:59:06.258618: step 962, loss 0.384686, acc 0.84375, learning_rate 0.000196197
2017-09-29T10:59:06.447339: step 963, loss 0.398215, acc 0.859375, learning_rate 0.000195804
2017-09-29T10:59:06.630185: step 964, loss 0.493855, acc 0.859375, learning_rate 0.000195413
2017-09-29T10:59:06.812722: step 965, loss 0.332173, acc 0.84375, learning_rate 0.000195023
2017-09-29T10:59:06.994669: step 966, loss 0.38188, acc 0.859375, learning_rate 0.000194636
2017-09-29T10:59:07.183156: step 967, loss 0.332379, acc 0.828125, learning_rate 0.000194249
2017-09-29T10:59:07.378441: step 968, loss 0.379149, acc 0.828125, learning_rate 0.000193865
2017-09-29T10:59:07.569591: step 969, loss 0.266578, acc 0.9375, learning_rate 0.000193482
2017-09-29T10:59:07.751024: step 970, loss 0.312381, acc 0.890625, learning_rate 0.0001931
2017-09-29T10:59:07.935607: step 971, loss 0.43043, acc 0.8125, learning_rate 0.00019272
2017-09-29T10:59:08.121701: step 972, loss 0.376729, acc 0.890625, learning_rate 0.000192341
2017-09-29T10:59:08.303736: step 973, loss 0.296725, acc 0.890625, learning_rate 0.000191965
2017-09-29T10:59:08.488375: step 974, loss 0.35513, acc 0.859375, learning_rate 0.000191589
2017-09-29T10:59:08.683295: step 975, loss 0.510906, acc 0.765625, learning_rate 0.000191215
2017-09-29T10:59:08.865377: step 976, loss 0.322369, acc 0.890625, learning_rate 0.000190843
2017-09-29T10:59:09.051838: step 977, loss 0.186562, acc 0.9375, learning_rate 0.000190472
2017-09-29T10:59:09.234016: step 978, loss 0.362528, acc 0.890625, learning_rate 0.000190103
2017-09-29T10:59:09.418824: step 979, loss 0.403216, acc 0.859375, learning_rate 0.000189735
2017-09-29T10:59:09.576086: step 980, loss 0.392891, acc 0.882353, learning_rate 0.000189369
2017-09-29T10:59:09.759619: step 981, loss 0.324276, acc 0.890625, learning_rate 0.000189004
2017-09-29T10:59:09.943894: step 982, loss 0.279663, acc 0.875, learning_rate 0.000188641
2017-09-29T10:59:10.127166: step 983, loss 0.27315, acc 0.875, learning_rate 0.000188279
2017-09-29T10:59:10.320973: step 984, loss 0.283769, acc 0.890625, learning_rate 0.000187919
2017-09-29T10:59:10.503964: step 985, loss 0.344015, acc 0.84375, learning_rate 0.00018756
2017-09-29T10:59:10.691397: step 986, loss 0.245968, acc 0.921875, learning_rate 0.000187202
2017-09-29T10:59:10.884007: step 987, loss 0.414613, acc 0.875, learning_rate 0.000186846
2017-09-29T10:59:11.067622: step 988, loss 0.396017, acc 0.859375, learning_rate 0.000186492
2017-09-29T10:59:11.253578: step 989, loss 0.287914, acc 0.9375, learning_rate 0.000186139
2017-09-29T10:59:11.437489: step 990, loss 0.418039, acc 0.78125, learning_rate 0.000185787
2017-09-29T10:59:11.618113: step 991, loss 0.451627, acc 0.859375, learning_rate 0.000185437
2017-09-29T10:59:11.798787: step 992, loss 0.356032, acc 0.84375, learning_rate 0.000185088
2017-09-29T10:59:11.982601: step 993, loss 0.313605, acc 0.890625, learning_rate 0.000184741
2017-09-29T10:59:12.172178: step 994, loss 0.344619, acc 0.875, learning_rate 0.000184395
2017-09-29T10:59:12.366870: step 995, loss 0.214966, acc 0.953125, learning_rate 0.000184051
2017-09-29T10:59:12.571037: step 996, loss 0.30519, acc 0.921875, learning_rate 0.000183708
2017-09-29T10:59:12.753061: step 997, loss 0.394298, acc 0.8125, learning_rate 0.000183366
2017-09-29T10:59:12.937809: step 998, loss 0.395517, acc 0.828125, learning_rate 0.000183026
2017-09-29T10:59:13.123930: step 999, loss 0.450097, acc 0.78125, learning_rate 0.000182687
2017-09-29T10:59:13.311337: step 1000, loss 0.358025, acc 0.859375, learning_rate 0.000182349

Evaluation:
2017-09-29T10:59:13.796465: step 1000, loss 0.363557, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1000

2017-09-29T10:59:14.596604: step 1001, loss 0.335053, acc 0.90625, learning_rate 0.000182013
2017-09-29T10:59:14.778994: step 1002, loss 0.324408, acc 0.890625, learning_rate 0.000181678
2017-09-29T10:59:14.972189: step 1003, loss 0.242918, acc 0.9375, learning_rate 0.000181345
2017-09-29T10:59:15.159117: step 1004, loss 0.259139, acc 0.921875, learning_rate 0.000181013
2017-09-29T10:59:15.344054: step 1005, loss 0.430225, acc 0.828125, learning_rate 0.000180682
2017-09-29T10:59:15.525704: step 1006, loss 0.383048, acc 0.875, learning_rate 0.000180353
2017-09-29T10:59:15.711067: step 1007, loss 0.273577, acc 0.9375, learning_rate 0.000180025
2017-09-29T10:59:15.896734: step 1008, loss 0.35177, acc 0.890625, learning_rate 0.000179698
2017-09-29T10:59:16.082111: step 1009, loss 0.258821, acc 0.90625, learning_rate 0.000179373
2017-09-29T10:59:16.262690: step 1010, loss 0.408936, acc 0.859375, learning_rate 0.000179049
2017-09-29T10:59:16.449065: step 1011, loss 0.37141, acc 0.875, learning_rate 0.000178726
2017-09-29T10:59:16.633523: step 1012, loss 0.22815, acc 0.890625, learning_rate 0.000178405
2017-09-29T10:59:16.817801: step 1013, loss 0.494502, acc 0.84375, learning_rate 0.000178085
2017-09-29T10:59:17.005948: step 1014, loss 0.389448, acc 0.875, learning_rate 0.000177766
2017-09-29T10:59:17.194248: step 1015, loss 0.322791, acc 0.890625, learning_rate 0.000177449
2017-09-29T10:59:17.385969: step 1016, loss 0.203738, acc 0.9375, learning_rate 0.000177133
2017-09-29T10:59:17.580277: step 1017, loss 0.387015, acc 0.875, learning_rate 0.000176818
2017-09-29T10:59:17.764949: step 1018, loss 0.396991, acc 0.859375, learning_rate 0.000176504
2017-09-29T10:59:17.950853: step 1019, loss 0.325505, acc 0.921875, learning_rate 0.000176192
2017-09-29T10:59:18.134324: step 1020, loss 0.331244, acc 0.859375, learning_rate 0.000175881
2017-09-29T10:59:18.316915: step 1021, loss 0.369619, acc 0.84375, learning_rate 0.000175571
2017-09-29T10:59:18.520884: step 1022, loss 0.535895, acc 0.765625, learning_rate 0.000175263
2017-09-29T10:59:18.703920: step 1023, loss 0.335211, acc 0.859375, learning_rate 0.000174956
2017-09-29T10:59:18.886370: step 1024, loss 0.323397, acc 0.890625, learning_rate 0.00017465
2017-09-29T10:59:19.073616: step 1025, loss 0.37929, acc 0.890625, learning_rate 0.000174345
2017-09-29T10:59:19.256863: step 1026, loss 0.387508, acc 0.828125, learning_rate 0.000174042
2017-09-29T10:59:19.443738: step 1027, loss 0.484084, acc 0.828125, learning_rate 0.000173739
2017-09-29T10:59:19.628317: step 1028, loss 0.400865, acc 0.859375, learning_rate 0.000173438
2017-09-29T10:59:19.811230: step 1029, loss 0.310034, acc 0.921875, learning_rate 0.000173139
2017-09-29T10:59:19.994894: step 1030, loss 0.395888, acc 0.84375, learning_rate 0.00017284
2017-09-29T10:59:20.180173: step 1031, loss 0.336243, acc 0.84375, learning_rate 0.000172543
2017-09-29T10:59:20.366528: step 1032, loss 0.155254, acc 0.96875, learning_rate 0.000172247
2017-09-29T10:59:20.571898: step 1033, loss 0.202496, acc 0.9375, learning_rate 0.000171952
2017-09-29T10:59:20.758595: step 1034, loss 0.339385, acc 0.890625, learning_rate 0.000171658
2017-09-29T10:59:20.942503: step 1035, loss 0.29839, acc 0.890625, learning_rate 0.000171366
2017-09-29T10:59:21.127698: step 1036, loss 0.282946, acc 0.90625, learning_rate 0.000171074
2017-09-29T10:59:21.313986: step 1037, loss 0.416229, acc 0.8125, learning_rate 0.000170784
2017-09-29T10:59:21.500024: step 1038, loss 0.464734, acc 0.8125, learning_rate 0.000170495
2017-09-29T10:59:21.686681: step 1039, loss 0.316605, acc 0.921875, learning_rate 0.000170208
2017-09-29T10:59:21.872656: step 1040, loss 0.377611, acc 0.859375, learning_rate 0.000169921

Evaluation:
2017-09-29T10:59:22.369952: step 1040, loss 0.364521, acc 0.869065

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1040

2017-09-29T10:59:22.995467: step 1041, loss 0.212394, acc 0.953125, learning_rate 0.000169636
2017-09-29T10:59:23.178061: step 1042, loss 0.288011, acc 0.890625, learning_rate 0.000169351
2017-09-29T10:59:23.364808: step 1043, loss 0.325577, acc 0.828125, learning_rate 0.000169068
2017-09-29T10:59:23.548267: step 1044, loss 0.376653, acc 0.84375, learning_rate 0.000168786
2017-09-29T10:59:23.731923: step 1045, loss 0.308638, acc 0.90625, learning_rate 0.000168506
2017-09-29T10:59:23.915611: step 1046, loss 0.225326, acc 0.9375, learning_rate 0.000168226
2017-09-29T10:59:24.098602: step 1047, loss 0.247588, acc 0.90625, learning_rate 0.000167947
2017-09-29T10:59:24.283073: step 1048, loss 0.397254, acc 0.859375, learning_rate 0.00016767
2017-09-29T10:59:24.476753: step 1049, loss 0.38088, acc 0.859375, learning_rate 0.000167394
2017-09-29T10:59:24.663218: step 1050, loss 0.306377, acc 0.921875, learning_rate 0.000167119
2017-09-29T10:59:24.844109: step 1051, loss 0.329041, acc 0.890625, learning_rate 0.000166845
2017-09-29T10:59:25.030321: step 1052, loss 0.413644, acc 0.84375, learning_rate 0.000166572
2017-09-29T10:59:25.214133: step 1053, loss 0.307768, acc 0.890625, learning_rate 0.0001663
2017-09-29T10:59:25.397189: step 1054, loss 0.44114, acc 0.875, learning_rate 0.00016603
2017-09-29T10:59:25.580126: step 1055, loss 0.280664, acc 0.9375, learning_rate 0.00016576
2017-09-29T10:59:25.763766: step 1056, loss 0.387626, acc 0.859375, learning_rate 0.000165492
2017-09-29T10:59:25.950946: step 1057, loss 0.338513, acc 0.890625, learning_rate 0.000165224
2017-09-29T10:59:26.135158: step 1058, loss 0.364181, acc 0.90625, learning_rate 0.000164958
2017-09-29T10:59:26.316277: step 1059, loss 0.205423, acc 0.9375, learning_rate 0.000164693
2017-09-29T10:59:26.511039: step 1060, loss 0.246755, acc 0.90625, learning_rate 0.000164429
2017-09-29T10:59:26.694688: step 1061, loss 0.200276, acc 0.953125, learning_rate 0.000164166
2017-09-29T10:59:26.881791: step 1062, loss 0.245745, acc 0.90625, learning_rate 0.000163904
2017-09-29T10:59:27.064939: step 1063, loss 0.294712, acc 0.90625, learning_rate 0.000163643
2017-09-29T10:59:27.250995: step 1064, loss 0.351874, acc 0.890625, learning_rate 0.000163383
2017-09-29T10:59:27.437902: step 1065, loss 0.217579, acc 0.9375, learning_rate 0.000163125
2017-09-29T10:59:27.631475: step 1066, loss 0.470889, acc 0.8125, learning_rate 0.000162867
2017-09-29T10:59:27.816522: step 1067, loss 0.352808, acc 0.875, learning_rate 0.00016261
2017-09-29T10:59:27.999853: step 1068, loss 0.333117, acc 0.890625, learning_rate 0.000162355
2017-09-29T10:59:28.182565: step 1069, loss 0.276636, acc 0.90625, learning_rate 0.0001621
2017-09-29T10:59:28.362220: step 1070, loss 0.216269, acc 0.90625, learning_rate 0.000161847
2017-09-29T10:59:28.544600: step 1071, loss 0.316713, acc 0.890625, learning_rate 0.000161594
2017-09-29T10:59:28.731932: step 1072, loss 0.627128, acc 0.78125, learning_rate 0.000161343
2017-09-29T10:59:28.916378: step 1073, loss 0.496367, acc 0.859375, learning_rate 0.000161093
2017-09-29T10:59:29.100397: step 1074, loss 0.354477, acc 0.875, learning_rate 0.000160843
2017-09-29T10:59:29.285318: step 1075, loss 0.267017, acc 0.953125, learning_rate 0.000160595
2017-09-29T10:59:29.470131: step 1076, loss 0.340682, acc 0.890625, learning_rate 0.000160348
2017-09-29T10:59:29.654287: step 1077, loss 0.254618, acc 0.890625, learning_rate 0.000160101
2017-09-29T10:59:29.805867: step 1078, loss 0.40809, acc 0.784314, learning_rate 0.000159856
2017-09-29T10:59:29.988267: step 1079, loss 0.285257, acc 0.90625, learning_rate 0.000159612
2017-09-29T10:59:30.171420: step 1080, loss 0.237547, acc 0.921875, learning_rate 0.000159368

Evaluation:
2017-09-29T10:59:30.663834: step 1080, loss 0.358243, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1080

2017-09-29T10:59:31.374201: step 1081, loss 0.332201, acc 0.921875, learning_rate 0.000159126
2017-09-29T10:59:31.569127: step 1082, loss 0.32819, acc 0.875, learning_rate 0.000158885
2017-09-29T10:59:31.754136: step 1083, loss 0.388942, acc 0.875, learning_rate 0.000158644
2017-09-29T10:59:31.938495: step 1084, loss 0.27726, acc 0.859375, learning_rate 0.000158405
2017-09-29T10:59:32.128136: step 1085, loss 0.312204, acc 0.90625, learning_rate 0.000158167
2017-09-29T10:59:32.334333: step 1086, loss 0.367262, acc 0.875, learning_rate 0.000157929
2017-09-29T10:59:32.534200: step 1087, loss 0.373179, acc 0.890625, learning_rate 0.000157693
2017-09-29T10:59:32.726195: step 1088, loss 0.317295, acc 0.921875, learning_rate 0.000157457
2017-09-29T10:59:32.909985: step 1089, loss 0.388229, acc 0.875, learning_rate 0.000157223
2017-09-29T10:59:33.096102: step 1090, loss 0.297136, acc 0.90625, learning_rate 0.000156989
2017-09-29T10:59:33.276591: step 1091, loss 0.424956, acc 0.859375, learning_rate 0.000156757
2017-09-29T10:59:33.463519: step 1092, loss 0.324654, acc 0.921875, learning_rate 0.000156525
2017-09-29T10:59:33.647885: step 1093, loss 0.473506, acc 0.859375, learning_rate 0.000156294
2017-09-29T10:59:33.832887: step 1094, loss 0.445167, acc 0.890625, learning_rate 0.000156064
2017-09-29T10:59:34.015992: step 1095, loss 0.247446, acc 0.890625, learning_rate 0.000155836
2017-09-29T10:59:34.198978: step 1096, loss 0.33884, acc 0.890625, learning_rate 0.000155608
2017-09-29T10:59:34.383536: step 1097, loss 0.526626, acc 0.78125, learning_rate 0.000155381
2017-09-29T10:59:34.570317: step 1098, loss 0.320677, acc 0.890625, learning_rate 0.000155155
2017-09-29T10:59:34.755295: step 1099, loss 0.287902, acc 0.890625, learning_rate 0.000154929
2017-09-29T10:59:34.937510: step 1100, loss 0.350795, acc 0.90625, learning_rate 0.000154705
2017-09-29T10:59:35.123136: step 1101, loss 0.444861, acc 0.828125, learning_rate 0.000154482
2017-09-29T10:59:35.303421: step 1102, loss 0.267367, acc 0.90625, learning_rate 0.00015426
2017-09-29T10:59:35.486032: step 1103, loss 0.315345, acc 0.90625, learning_rate 0.000154038
2017-09-29T10:59:35.669460: step 1104, loss 0.325346, acc 0.921875, learning_rate 0.000153818
2017-09-29T10:59:35.854181: step 1105, loss 0.350391, acc 0.859375, learning_rate 0.000153598
2017-09-29T10:59:36.037865: step 1106, loss 0.296016, acc 0.90625, learning_rate 0.000153379
2017-09-29T10:59:36.223588: step 1107, loss 0.365098, acc 0.828125, learning_rate 0.000153161
2017-09-29T10:59:36.409461: step 1108, loss 0.240956, acc 0.90625, learning_rate 0.000152944
2017-09-29T10:59:36.596326: step 1109, loss 0.256261, acc 0.875, learning_rate 0.000152728
2017-09-29T10:59:36.778533: step 1110, loss 0.427837, acc 0.84375, learning_rate 0.000152513
2017-09-29T10:59:36.961857: step 1111, loss 0.312976, acc 0.90625, learning_rate 0.000152299
2017-09-29T10:59:37.144202: step 1112, loss 0.568395, acc 0.796875, learning_rate 0.000152085
2017-09-29T10:59:37.328783: step 1113, loss 0.186774, acc 0.953125, learning_rate 0.000151872
2017-09-29T10:59:37.515191: step 1114, loss 0.311498, acc 0.859375, learning_rate 0.000151661
2017-09-29T10:59:37.715562: step 1115, loss 0.359777, acc 0.84375, learning_rate 0.00015145
2017-09-29T10:59:37.899379: step 1116, loss 0.263201, acc 0.875, learning_rate 0.00015124
2017-09-29T10:59:38.085918: step 1117, loss 0.471303, acc 0.859375, learning_rate 0.000151031
2017-09-29T10:59:38.265529: step 1118, loss 0.203949, acc 0.96875, learning_rate 0.000150822
2017-09-29T10:59:38.461535: step 1119, loss 0.351744, acc 0.859375, learning_rate 0.000150615
2017-09-29T10:59:38.649846: step 1120, loss 0.243342, acc 0.9375, learning_rate 0.000150408

Evaluation:
2017-09-29T10:59:39.128718: step 1120, loss 0.357775, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1120

2017-09-29T10:59:39.831215: step 1121, loss 0.450994, acc 0.859375, learning_rate 0.000150203
2017-09-29T10:59:40.017048: step 1122, loss 0.253519, acc 0.890625, learning_rate 0.000149998
2017-09-29T10:59:40.200382: step 1123, loss 0.423805, acc 0.84375, learning_rate 0.000149794
2017-09-29T10:59:40.389726: step 1124, loss 0.310293, acc 0.875, learning_rate 0.00014959
2017-09-29T10:59:40.578951: step 1125, loss 0.329759, acc 0.890625, learning_rate 0.000149388
2017-09-29T10:59:40.765793: step 1126, loss 0.284623, acc 0.90625, learning_rate 0.000149186
2017-09-29T10:59:40.957503: step 1127, loss 0.37096, acc 0.875, learning_rate 0.000148986
2017-09-29T10:59:41.141942: step 1128, loss 0.351404, acc 0.890625, learning_rate 0.000148786
2017-09-29T10:59:41.324286: step 1129, loss 0.335644, acc 0.875, learning_rate 0.000148587
2017-09-29T10:59:41.506989: step 1130, loss 0.27396, acc 0.921875, learning_rate 0.000148388
2017-09-29T10:59:41.693486: step 1131, loss 0.28075, acc 0.875, learning_rate 0.000148191
2017-09-29T10:59:41.876045: step 1132, loss 0.406598, acc 0.828125, learning_rate 0.000147994
2017-09-29T10:59:42.057005: step 1133, loss 0.375281, acc 0.875, learning_rate 0.000147798
2017-09-29T10:59:42.239002: step 1134, loss 0.299387, acc 0.90625, learning_rate 0.000147603
2017-09-29T10:59:42.431329: step 1135, loss 0.284695, acc 0.921875, learning_rate 0.000147409
2017-09-29T10:59:42.620061: step 1136, loss 0.316914, acc 0.890625, learning_rate 0.000147215
2017-09-29T10:59:42.808508: step 1137, loss 0.310702, acc 0.90625, learning_rate 0.000147022
2017-09-29T10:59:42.997594: step 1138, loss 0.318104, acc 0.890625, learning_rate 0.000146831
2017-09-29T10:59:43.182071: step 1139, loss 0.371061, acc 0.875, learning_rate 0.000146639
2017-09-29T10:59:43.366995: step 1140, loss 0.352297, acc 0.90625, learning_rate 0.000146449
2017-09-29T10:59:43.550388: step 1141, loss 0.401454, acc 0.875, learning_rate 0.000146259
2017-09-29T10:59:43.734473: step 1142, loss 0.317066, acc 0.921875, learning_rate 0.000146071
2017-09-29T10:59:43.918644: step 1143, loss 0.441168, acc 0.84375, learning_rate 0.000145883
2017-09-29T10:59:44.104485: step 1144, loss 0.220626, acc 0.96875, learning_rate 0.000145695
2017-09-29T10:59:44.284978: step 1145, loss 0.357215, acc 0.859375, learning_rate 0.000145509
2017-09-29T10:59:44.481340: step 1146, loss 0.305335, acc 0.875, learning_rate 0.000145323
2017-09-29T10:59:44.663669: step 1147, loss 0.31255, acc 0.859375, learning_rate 0.000145138
2017-09-29T10:59:44.847010: step 1148, loss 0.572438, acc 0.734375, learning_rate 0.000144954
2017-09-29T10:59:45.033606: step 1149, loss 0.555265, acc 0.78125, learning_rate 0.00014477
2017-09-29T10:59:45.217687: step 1150, loss 0.32359, acc 0.890625, learning_rate 0.000144588
2017-09-29T10:59:45.415596: step 1151, loss 0.218021, acc 0.9375, learning_rate 0.000144406
2017-09-29T10:59:45.600756: step 1152, loss 0.410583, acc 0.84375, learning_rate 0.000144224
2017-09-29T10:59:45.783862: step 1153, loss 0.41719, acc 0.859375, learning_rate 0.000144044
2017-09-29T10:59:45.965851: step 1154, loss 0.248935, acc 0.875, learning_rate 0.000143864
2017-09-29T10:59:46.152141: step 1155, loss 0.324197, acc 0.890625, learning_rate 0.000143685
2017-09-29T10:59:46.338955: step 1156, loss 0.331866, acc 0.875, learning_rate 0.000143507
2017-09-29T10:59:46.525281: step 1157, loss 0.314351, acc 0.84375, learning_rate 0.000143329
2017-09-29T10:59:46.710286: step 1158, loss 0.337794, acc 0.890625, learning_rate 0.000143152
2017-09-29T10:59:46.894243: step 1159, loss 0.426896, acc 0.828125, learning_rate 0.000142976
2017-09-29T10:59:47.079357: step 1160, loss 0.297195, acc 0.921875, learning_rate 0.000142801

Evaluation:
2017-09-29T10:59:47.571758: step 1160, loss 0.368617, acc 0.866187

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1160

2017-09-29T10:59:48.361686: step 1161, loss 0.396561, acc 0.875, learning_rate 0.000142626
2017-09-29T10:59:48.557619: step 1162, loss 0.392541, acc 0.859375, learning_rate 0.000142452
2017-09-29T10:59:48.745812: step 1163, loss 0.243374, acc 0.890625, learning_rate 0.000142279
2017-09-29T10:59:48.928436: step 1164, loss 0.292286, acc 0.875, learning_rate 0.000142106
2017-09-29T10:59:49.115596: step 1165, loss 0.348372, acc 0.890625, learning_rate 0.000141934
2017-09-29T10:59:49.309411: step 1166, loss 0.370715, acc 0.84375, learning_rate 0.000141763
2017-09-29T10:59:49.495614: step 1167, loss 0.514574, acc 0.796875, learning_rate 0.000141593
2017-09-29T10:59:49.679796: step 1168, loss 0.423913, acc 0.828125, learning_rate 0.000141423
2017-09-29T10:59:49.863804: step 1169, loss 0.309792, acc 0.9375, learning_rate 0.000141254
2017-09-29T10:59:50.047448: step 1170, loss 0.440108, acc 0.828125, learning_rate 0.000141085
2017-09-29T10:59:50.229479: step 1171, loss 0.266523, acc 0.90625, learning_rate 0.000140918
2017-09-29T10:59:50.424653: step 1172, loss 0.522348, acc 0.796875, learning_rate 0.000140751
2017-09-29T10:59:50.611655: step 1173, loss 0.334799, acc 0.890625, learning_rate 0.000140584
2017-09-29T10:59:50.795899: step 1174, loss 0.267424, acc 0.9375, learning_rate 0.000140419
2017-09-29T10:59:50.980339: step 1175, loss 0.377701, acc 0.828125, learning_rate 0.000140254
2017-09-29T10:59:51.132319: step 1176, loss 0.289802, acc 0.921569, learning_rate 0.000140089
2017-09-29T10:59:51.316367: step 1177, loss 0.26117, acc 0.9375, learning_rate 0.000139926
2017-09-29T10:59:51.501290: step 1178, loss 0.184492, acc 0.953125, learning_rate 0.000139763
2017-09-29T10:59:51.689600: step 1179, loss 0.507821, acc 0.828125, learning_rate 0.0001396
2017-09-29T10:59:51.876287: step 1180, loss 0.309162, acc 0.890625, learning_rate 0.000139439
2017-09-29T10:59:52.059976: step 1181, loss 0.308565, acc 0.90625, learning_rate 0.000139278
2017-09-29T10:59:52.244037: step 1182, loss 0.374722, acc 0.859375, learning_rate 0.000139118
2017-09-29T10:59:52.433534: step 1183, loss 0.323514, acc 0.890625, learning_rate 0.000138958
2017-09-29T10:59:52.621522: step 1184, loss 0.329214, acc 0.890625, learning_rate 0.000138799
2017-09-29T10:59:52.814136: step 1185, loss 0.41236, acc 0.828125, learning_rate 0.00013864
2017-09-29T10:59:52.997534: step 1186, loss 0.360446, acc 0.84375, learning_rate 0.000138483
2017-09-29T10:59:53.182391: step 1187, loss 0.271636, acc 0.90625, learning_rate 0.000138326
2017-09-29T10:59:53.365210: step 1188, loss 0.454617, acc 0.890625, learning_rate 0.000138169
2017-09-29T10:59:53.561105: step 1189, loss 0.4858, acc 0.8125, learning_rate 0.000138013
2017-09-29T10:59:53.745108: step 1190, loss 0.270689, acc 0.875, learning_rate 0.000137858
2017-09-29T10:59:53.926276: step 1191, loss 0.402301, acc 0.859375, learning_rate 0.000137704
2017-09-29T10:59:54.113509: step 1192, loss 0.261766, acc 0.921875, learning_rate 0.00013755
2017-09-29T10:59:54.294358: step 1193, loss 0.355601, acc 0.875, learning_rate 0.000137397
2017-09-29T10:59:54.481329: step 1194, loss 0.238788, acc 0.9375, learning_rate 0.000137244
2017-09-29T10:59:54.667263: step 1195, loss 0.385224, acc 0.875, learning_rate 0.000137092
2017-09-29T10:59:54.848801: step 1196, loss 0.197098, acc 0.9375, learning_rate 0.000136941
2017-09-29T10:59:55.033623: step 1197, loss 0.312215, acc 0.90625, learning_rate 0.00013679
2017-09-29T10:59:55.216592: step 1198, loss 0.368155, acc 0.875, learning_rate 0.00013664
2017-09-29T10:59:55.401761: step 1199, loss 0.245872, acc 0.890625, learning_rate 0.00013649
2017-09-29T10:59:55.582944: step 1200, loss 0.286754, acc 0.90625, learning_rate 0.000136341

Evaluation:
2017-09-29T10:59:56.048769: step 1200, loss 0.352365, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1200

2017-09-29T10:59:56.673548: step 1201, loss 0.227189, acc 0.9375, learning_rate 0.000136193
2017-09-29T10:59:56.855908: step 1202, loss 0.224925, acc 0.9375, learning_rate 0.000136045
2017-09-29T10:59:57.042466: step 1203, loss 0.396995, acc 0.859375, learning_rate 0.000135898
2017-09-29T10:59:57.225722: step 1204, loss 0.309009, acc 0.890625, learning_rate 0.000135751
2017-09-29T10:59:57.408621: step 1205, loss 0.313302, acc 0.890625, learning_rate 0.000135605
2017-09-29T10:59:57.594492: step 1206, loss 0.309994, acc 0.890625, learning_rate 0.00013546
2017-09-29T10:59:57.783898: step 1207, loss 0.337231, acc 0.875, learning_rate 0.000135315
2017-09-29T10:59:57.964749: step 1208, loss 0.337885, acc 0.890625, learning_rate 0.000135171
2017-09-29T10:59:58.156910: step 1209, loss 0.226527, acc 0.921875, learning_rate 0.000135028
2017-09-29T10:59:58.341127: step 1210, loss 0.41018, acc 0.859375, learning_rate 0.000134885
2017-09-29T10:59:58.530138: step 1211, loss 0.279479, acc 0.90625, learning_rate 0.000134742
2017-09-29T10:59:58.719929: step 1212, loss 0.281252, acc 0.890625, learning_rate 0.0001346
2017-09-29T10:59:58.902776: step 1213, loss 0.411584, acc 0.828125, learning_rate 0.000134459
2017-09-29T10:59:59.086221: step 1214, loss 0.389206, acc 0.859375, learning_rate 0.000134319
2017-09-29T10:59:59.271224: step 1215, loss 0.508745, acc 0.78125, learning_rate 0.000134178
2017-09-29T10:59:59.469102: step 1216, loss 0.405976, acc 0.859375, learning_rate 0.000134039
2017-09-29T10:59:59.658629: step 1217, loss 0.312786, acc 0.84375, learning_rate 0.0001339
2017-09-29T10:59:59.840177: step 1218, loss 0.27684, acc 0.953125, learning_rate 0.000133762
2017-09-29T11:00:00.024334: step 1219, loss 0.305415, acc 0.890625, learning_rate 0.000133624
2017-09-29T11:00:00.211515: step 1220, loss 0.350493, acc 0.859375, learning_rate 0.000133487
2017-09-29T11:00:00.390213: step 1221, loss 0.309204, acc 0.890625, learning_rate 0.00013335
2017-09-29T11:00:00.572157: step 1222, loss 0.41293, acc 0.78125, learning_rate 0.000133214
2017-09-29T11:00:00.758421: step 1223, loss 0.432602, acc 0.875, learning_rate 0.000133078
2017-09-29T11:00:00.938685: step 1224, loss 0.231861, acc 0.921875, learning_rate 0.000132943
2017-09-29T11:00:01.121589: step 1225, loss 0.524174, acc 0.84375, learning_rate 0.000132809
2017-09-29T11:00:01.302964: step 1226, loss 0.454836, acc 0.84375, learning_rate 0.000132675
2017-09-29T11:00:01.490260: step 1227, loss 0.278148, acc 0.859375, learning_rate 0.000132541
2017-09-29T11:00:01.676872: step 1228, loss 0.266247, acc 0.921875, learning_rate 0.000132409
2017-09-29T11:00:01.861258: step 1229, loss 0.378124, acc 0.875, learning_rate 0.000132276
2017-09-29T11:00:02.045555: step 1230, loss 0.401892, acc 0.890625, learning_rate 0.000132145
2017-09-29T11:00:02.225629: step 1231, loss 0.390797, acc 0.859375, learning_rate 0.000132013
2017-09-29T11:00:02.409183: step 1232, loss 0.247795, acc 0.921875, learning_rate 0.000131883
2017-09-29T11:00:02.599665: step 1233, loss 0.294202, acc 0.875, learning_rate 0.000131753
2017-09-29T11:00:02.787506: step 1234, loss 0.437157, acc 0.859375, learning_rate 0.000131623
2017-09-29T11:00:02.983673: step 1235, loss 0.206359, acc 0.9375, learning_rate 0.000131494
2017-09-29T11:00:03.177997: step 1236, loss 0.366505, acc 0.875, learning_rate 0.000131365
2017-09-29T11:00:03.363459: step 1237, loss 0.401616, acc 0.84375, learning_rate 0.000131237
2017-09-29T11:00:03.544646: step 1238, loss 0.341071, acc 0.90625, learning_rate 0.00013111
2017-09-29T11:00:03.724496: step 1239, loss 0.567108, acc 0.765625, learning_rate 0.000130983
2017-09-29T11:00:03.908554: step 1240, loss 0.319646, acc 0.90625, learning_rate 0.000130856

Evaluation:
2017-09-29T11:00:04.394727: step 1240, loss 0.351994, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1240

2017-09-29T11:00:05.100065: step 1241, loss 0.51602, acc 0.828125, learning_rate 0.00013073
2017-09-29T11:00:05.283861: step 1242, loss 0.284042, acc 0.90625, learning_rate 0.000130605
2017-09-29T11:00:05.489444: step 1243, loss 0.357296, acc 0.859375, learning_rate 0.00013048
2017-09-29T11:00:05.675498: step 1244, loss 0.385513, acc 0.875, learning_rate 0.000130356
2017-09-29T11:00:05.858488: step 1245, loss 0.290465, acc 0.921875, learning_rate 0.000130232
2017-09-29T11:00:06.040794: step 1246, loss 0.348219, acc 0.859375, learning_rate 0.000130108
2017-09-29T11:00:06.223157: step 1247, loss 0.406382, acc 0.875, learning_rate 0.000129985
2017-09-29T11:00:06.405161: step 1248, loss 0.211819, acc 0.921875, learning_rate 0.000129863
2017-09-29T11:00:06.587410: step 1249, loss 0.348162, acc 0.859375, learning_rate 0.000129741
2017-09-29T11:00:06.769258: step 1250, loss 0.284643, acc 0.890625, learning_rate 0.00012962
2017-09-29T11:00:06.953174: step 1251, loss 0.398952, acc 0.828125, learning_rate 0.000129499
2017-09-29T11:00:07.146965: step 1252, loss 0.27831, acc 0.921875, learning_rate 0.000129378
2017-09-29T11:00:07.348878: step 1253, loss 0.273651, acc 0.890625, learning_rate 0.000129259
2017-09-29T11:00:07.536795: step 1254, loss 0.318054, acc 0.90625, learning_rate 0.000129139
2017-09-29T11:00:07.727193: step 1255, loss 0.237658, acc 0.953125, learning_rate 0.00012902
2017-09-29T11:00:07.917701: step 1256, loss 0.225262, acc 0.90625, learning_rate 0.000128902
2017-09-29T11:00:08.104057: step 1257, loss 0.269794, acc 0.90625, learning_rate 0.000128784
2017-09-29T11:00:08.285843: step 1258, loss 0.254572, acc 0.953125, learning_rate 0.000128666
2017-09-29T11:00:08.478962: step 1259, loss 0.152126, acc 0.984375, learning_rate 0.000128549
2017-09-29T11:00:08.665102: step 1260, loss 0.451295, acc 0.828125, learning_rate 0.000128433
2017-09-29T11:00:08.848061: step 1261, loss 0.326156, acc 0.875, learning_rate 0.000128317
2017-09-29T11:00:09.033940: step 1262, loss 0.437083, acc 0.84375, learning_rate 0.000128201
2017-09-29T11:00:09.218240: step 1263, loss 0.237597, acc 0.9375, learning_rate 0.000128086
2017-09-29T11:00:09.403092: step 1264, loss 0.316113, acc 0.90625, learning_rate 0.000127971
2017-09-29T11:00:09.593567: step 1265, loss 0.358588, acc 0.828125, learning_rate 0.000127857
2017-09-29T11:00:09.790841: step 1266, loss 0.238739, acc 0.90625, learning_rate 0.000127743
2017-09-29T11:00:09.983213: step 1267, loss 0.370419, acc 0.859375, learning_rate 0.00012763
2017-09-29T11:00:10.170505: step 1268, loss 0.219182, acc 0.90625, learning_rate 0.000127517
2017-09-29T11:00:10.359222: step 1269, loss 0.388055, acc 0.875, learning_rate 0.000127405
2017-09-29T11:00:10.544941: step 1270, loss 0.257245, acc 0.921875, learning_rate 0.000127293
2017-09-29T11:00:10.729811: step 1271, loss 0.282492, acc 0.90625, learning_rate 0.000127182
2017-09-29T11:00:10.911456: step 1272, loss 0.35883, acc 0.859375, learning_rate 0.000127071
2017-09-29T11:00:11.102467: step 1273, loss 0.390237, acc 0.875, learning_rate 0.00012696
2017-09-29T11:00:11.258719: step 1274, loss 0.267337, acc 0.882353, learning_rate 0.00012685
2017-09-29T11:00:11.455033: step 1275, loss 0.200823, acc 0.953125, learning_rate 0.000126741
2017-09-29T11:00:11.647632: step 1276, loss 0.565827, acc 0.8125, learning_rate 0.000126632
2017-09-29T11:00:11.835451: step 1277, loss 0.36209, acc 0.890625, learning_rate 0.000126523
2017-09-29T11:00:12.029858: step 1278, loss 0.406077, acc 0.875, learning_rate 0.000126415
2017-09-29T11:00:12.222871: step 1279, loss 0.261513, acc 0.890625, learning_rate 0.000126307
2017-09-29T11:00:12.412453: step 1280, loss 0.202341, acc 0.953125, learning_rate 0.000126199

Evaluation:
2017-09-29T11:00:12.941806: step 1280, loss 0.355239, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1280

2017-09-29T11:00:13.660896: step 1281, loss 0.275065, acc 0.90625, learning_rate 0.000126093
2017-09-29T11:00:13.843482: step 1282, loss 0.33299, acc 0.859375, learning_rate 0.000125986
2017-09-29T11:00:14.027480: step 1283, loss 0.230668, acc 0.953125, learning_rate 0.00012588
2017-09-29T11:00:14.218057: step 1284, loss 0.349447, acc 0.90625, learning_rate 0.000125774
2017-09-29T11:00:14.405309: step 1285, loss 0.33673, acc 0.859375, learning_rate 0.000125669
2017-09-29T11:00:14.593257: step 1286, loss 0.206094, acc 0.953125, learning_rate 0.000125564
2017-09-29T11:00:14.774017: step 1287, loss 0.294669, acc 0.90625, learning_rate 0.00012546
2017-09-29T11:00:14.960724: step 1288, loss 0.390749, acc 0.859375, learning_rate 0.000125356
2017-09-29T11:00:15.142218: step 1289, loss 0.351639, acc 0.890625, learning_rate 0.000125253
2017-09-29T11:00:15.323501: step 1290, loss 0.265751, acc 0.90625, learning_rate 0.00012515
2017-09-29T11:00:15.511082: step 1291, loss 0.284808, acc 0.890625, learning_rate 0.000125047
2017-09-29T11:00:15.697206: step 1292, loss 0.28075, acc 0.90625, learning_rate 0.000124945
2017-09-29T11:00:15.876143: step 1293, loss 0.346983, acc 0.890625, learning_rate 0.000124843
2017-09-29T11:00:16.059920: step 1294, loss 0.414261, acc 0.8125, learning_rate 0.000124741
2017-09-29T11:00:16.247754: step 1295, loss 0.331583, acc 0.875, learning_rate 0.00012464
2017-09-29T11:00:16.433894: step 1296, loss 0.322358, acc 0.875, learning_rate 0.00012454
2017-09-29T11:00:16.624203: step 1297, loss 0.328577, acc 0.890625, learning_rate 0.00012444
2017-09-29T11:00:16.809339: step 1298, loss 0.300001, acc 0.90625, learning_rate 0.00012434
2017-09-29T11:00:16.991090: step 1299, loss 0.281756, acc 0.90625, learning_rate 0.000124241
2017-09-29T11:00:17.176648: step 1300, loss 0.283154, acc 0.875, learning_rate 0.000124142
2017-09-29T11:00:17.357161: step 1301, loss 0.377971, acc 0.875, learning_rate 0.000124043
2017-09-29T11:00:17.562182: step 1302, loss 0.411044, acc 0.84375, learning_rate 0.000123945
2017-09-29T11:00:17.747417: step 1303, loss 0.43419, acc 0.8125, learning_rate 0.000123847
2017-09-29T11:00:17.928817: step 1304, loss 0.365005, acc 0.875, learning_rate 0.00012375
2017-09-29T11:00:18.121914: step 1305, loss 0.497465, acc 0.84375, learning_rate 0.000123653
2017-09-29T11:00:18.303963: step 1306, loss 0.25846, acc 0.921875, learning_rate 0.000123556
2017-09-29T11:00:18.487129: step 1307, loss 0.409947, acc 0.828125, learning_rate 0.00012346
2017-09-29T11:00:18.670971: step 1308, loss 0.252951, acc 0.875, learning_rate 0.000123364
2017-09-29T11:00:18.852402: step 1309, loss 0.236001, acc 0.890625, learning_rate 0.000123269
2017-09-29T11:00:19.033353: step 1310, loss 0.359328, acc 0.8125, learning_rate 0.000123174
2017-09-29T11:00:19.217184: step 1311, loss 0.240703, acc 0.921875, learning_rate 0.00012308
2017-09-29T11:00:19.400719: step 1312, loss 0.292312, acc 0.921875, learning_rate 0.000122985
2017-09-29T11:00:19.583615: step 1313, loss 0.347952, acc 0.84375, learning_rate 0.000122892
2017-09-29T11:00:19.767519: step 1314, loss 0.356763, acc 0.84375, learning_rate 0.000122798
2017-09-29T11:00:19.949283: step 1315, loss 0.364616, acc 0.859375, learning_rate 0.000122705
2017-09-29T11:00:20.133377: step 1316, loss 0.463392, acc 0.859375, learning_rate 0.000122612
2017-09-29T11:00:20.315686: step 1317, loss 0.322676, acc 0.890625, learning_rate 0.00012252
2017-09-29T11:00:20.519598: step 1318, loss 0.292468, acc 0.90625, learning_rate 0.000122428
2017-09-29T11:00:20.715145: step 1319, loss 0.420557, acc 0.8125, learning_rate 0.000122337
2017-09-29T11:00:20.899498: step 1320, loss 0.329417, acc 0.890625, learning_rate 0.000122245

Evaluation:
2017-09-29T11:00:21.404552: step 1320, loss 0.351786, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1320

2017-09-29T11:00:22.190272: step 1321, loss 0.347535, acc 0.875, learning_rate 0.000122155
2017-09-29T11:00:22.372224: step 1322, loss 0.300065, acc 0.859375, learning_rate 0.000122064
2017-09-29T11:00:22.558128: step 1323, loss 0.392328, acc 0.859375, learning_rate 0.000121974
2017-09-29T11:00:22.749142: step 1324, loss 0.550178, acc 0.8125, learning_rate 0.000121884
2017-09-29T11:00:22.931918: step 1325, loss 0.295463, acc 0.90625, learning_rate 0.000121795
2017-09-29T11:00:23.123179: step 1326, loss 0.509263, acc 0.8125, learning_rate 0.000121706
2017-09-29T11:00:23.305010: step 1327, loss 0.22817, acc 0.921875, learning_rate 0.000121618
2017-09-29T11:00:23.501635: step 1328, loss 0.263738, acc 0.875, learning_rate 0.000121529
2017-09-29T11:00:23.685903: step 1329, loss 0.360006, acc 0.859375, learning_rate 0.000121441
2017-09-29T11:00:23.867919: step 1330, loss 0.244253, acc 0.9375, learning_rate 0.000121354
2017-09-29T11:00:24.050320: step 1331, loss 0.233391, acc 0.90625, learning_rate 0.000121267
2017-09-29T11:00:24.231486: step 1332, loss 0.344578, acc 0.859375, learning_rate 0.00012118
2017-09-29T11:00:24.417474: step 1333, loss 0.211498, acc 0.9375, learning_rate 0.000121093
2017-09-29T11:00:24.603516: step 1334, loss 0.210122, acc 0.921875, learning_rate 0.000121007
2017-09-29T11:00:24.786358: step 1335, loss 0.303748, acc 0.84375, learning_rate 0.000120922
2017-09-29T11:00:24.975784: step 1336, loss 0.329017, acc 0.90625, learning_rate 0.000120836
2017-09-29T11:00:25.159841: step 1337, loss 0.542679, acc 0.828125, learning_rate 0.000120751
2017-09-29T11:00:25.340466: step 1338, loss 0.32802, acc 0.859375, learning_rate 0.000120666
2017-09-29T11:00:25.521218: step 1339, loss 0.120509, acc 0.984375, learning_rate 0.000120582
2017-09-29T11:00:25.706955: step 1340, loss 0.415967, acc 0.875, learning_rate 0.000120498
2017-09-29T11:00:25.890272: step 1341, loss 0.543599, acc 0.8125, learning_rate 0.000120414
2017-09-29T11:00:26.073687: step 1342, loss 0.402757, acc 0.921875, learning_rate 0.000120331
2017-09-29T11:00:26.257216: step 1343, loss 0.357873, acc 0.796875, learning_rate 0.000120248
2017-09-29T11:00:26.450697: step 1344, loss 0.243223, acc 0.9375, learning_rate 0.000120165
2017-09-29T11:00:26.635362: step 1345, loss 0.302546, acc 0.890625, learning_rate 0.000120083
2017-09-29T11:00:26.823492: step 1346, loss 0.27464, acc 0.921875, learning_rate 0.000120001
2017-09-29T11:00:27.018244: step 1347, loss 0.314138, acc 0.84375, learning_rate 0.00011992
2017-09-29T11:00:27.204931: step 1348, loss 0.424504, acc 0.859375, learning_rate 0.000119838
2017-09-29T11:00:27.385782: step 1349, loss 0.209921, acc 0.9375, learning_rate 0.000119757
2017-09-29T11:00:27.586522: step 1350, loss 0.402312, acc 0.84375, learning_rate 0.000119677
2017-09-29T11:00:27.768190: step 1351, loss 0.274459, acc 0.890625, learning_rate 0.000119596
2017-09-29T11:00:27.954125: step 1352, loss 0.252996, acc 0.921875, learning_rate 0.000119516
2017-09-29T11:00:28.145565: step 1353, loss 0.49049, acc 0.796875, learning_rate 0.000119437
2017-09-29T11:00:28.328593: step 1354, loss 0.41218, acc 0.890625, learning_rate 0.000119357
2017-09-29T11:00:28.514601: step 1355, loss 0.35654, acc 0.84375, learning_rate 0.000119278
2017-09-29T11:00:28.699441: step 1356, loss 0.394845, acc 0.875, learning_rate 0.0001192
2017-09-29T11:00:28.883770: step 1357, loss 0.195053, acc 0.96875, learning_rate 0.000119121
2017-09-29T11:00:29.065193: step 1358, loss 0.429993, acc 0.84375, learning_rate 0.000119043
2017-09-29T11:00:29.253258: step 1359, loss 0.302205, acc 0.875, learning_rate 0.000118965
2017-09-29T11:00:29.450579: step 1360, loss 0.303993, acc 0.890625, learning_rate 0.000118888

Evaluation:
2017-09-29T11:00:29.973767: step 1360, loss 0.351377, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1360

2017-09-29T11:00:30.610236: step 1361, loss 0.397525, acc 0.8125, learning_rate 0.000118811
2017-09-29T11:00:30.792070: step 1362, loss 0.209974, acc 0.953125, learning_rate 0.000118734
2017-09-29T11:00:30.979696: step 1363, loss 0.250467, acc 0.890625, learning_rate 0.000118658
2017-09-29T11:00:31.163228: step 1364, loss 0.369641, acc 0.84375, learning_rate 0.000118582
2017-09-29T11:00:31.344787: step 1365, loss 0.40175, acc 0.828125, learning_rate 0.000118506
2017-09-29T11:00:31.538732: step 1366, loss 0.415364, acc 0.8125, learning_rate 0.00011843
2017-09-29T11:00:31.721915: step 1367, loss 0.303645, acc 0.90625, learning_rate 0.000118355
2017-09-29T11:00:31.907770: step 1368, loss 0.414706, acc 0.890625, learning_rate 0.00011828
2017-09-29T11:00:32.089466: step 1369, loss 0.433247, acc 0.828125, learning_rate 0.000118205
2017-09-29T11:00:32.275400: step 1370, loss 0.238878, acc 0.9375, learning_rate 0.000118131
2017-09-29T11:00:32.468067: step 1371, loss 0.241594, acc 0.90625, learning_rate 0.000118057
2017-09-29T11:00:32.622519: step 1372, loss 0.450734, acc 0.784314, learning_rate 0.000117983
2017-09-29T11:00:32.808133: step 1373, loss 0.460891, acc 0.796875, learning_rate 0.00011791
2017-09-29T11:00:32.991884: step 1374, loss 0.229269, acc 0.921875, learning_rate 0.000117837
2017-09-29T11:00:33.180890: step 1375, loss 0.303399, acc 0.90625, learning_rate 0.000117764
2017-09-29T11:00:33.364130: step 1376, loss 0.399599, acc 0.84375, learning_rate 0.000117692
2017-09-29T11:00:33.550338: step 1377, loss 0.493996, acc 0.8125, learning_rate 0.000117619
2017-09-29T11:00:33.734444: step 1378, loss 0.4024, acc 0.875, learning_rate 0.000117547
2017-09-29T11:00:33.917204: step 1379, loss 0.404362, acc 0.84375, learning_rate 0.000117476
2017-09-29T11:00:34.099609: step 1380, loss 0.25327, acc 0.921875, learning_rate 0.000117404
2017-09-29T11:00:34.281621: step 1381, loss 0.210113, acc 0.921875, learning_rate 0.000117333
2017-09-29T11:00:34.475866: step 1382, loss 0.287237, acc 0.859375, learning_rate 0.000117263
2017-09-29T11:00:34.661009: step 1383, loss 0.43986, acc 0.8125, learning_rate 0.000117192
2017-09-29T11:00:34.847175: step 1384, loss 0.343144, acc 0.875, learning_rate 0.000117122
2017-09-29T11:00:35.030095: step 1385, loss 0.22048, acc 0.921875, learning_rate 0.000117052
2017-09-29T11:00:35.213196: step 1386, loss 0.481213, acc 0.828125, learning_rate 0.000116983
2017-09-29T11:00:35.397843: step 1387, loss 0.354316, acc 0.890625, learning_rate 0.000116913
2017-09-29T11:00:35.588586: step 1388, loss 0.305786, acc 0.875, learning_rate 0.000116844
2017-09-29T11:00:35.781534: step 1389, loss 0.328006, acc 0.859375, learning_rate 0.000116775
2017-09-29T11:00:35.967424: step 1390, loss 0.24988, acc 0.890625, learning_rate 0.000116707
2017-09-29T11:00:36.160090: step 1391, loss 0.466144, acc 0.796875, learning_rate 0.000116639
2017-09-29T11:00:36.341918: step 1392, loss 0.293418, acc 0.90625, learning_rate 0.000116571
2017-09-29T11:00:36.526861: step 1393, loss 0.421239, acc 0.84375, learning_rate 0.000116503
2017-09-29T11:00:36.707781: step 1394, loss 0.560683, acc 0.8125, learning_rate 0.000116436
2017-09-29T11:00:36.884776: step 1395, loss 0.332849, acc 0.859375, learning_rate 0.000116369
2017-09-29T11:00:37.068095: step 1396, loss 0.280351, acc 0.890625, learning_rate 0.000116302
2017-09-29T11:00:37.252499: step 1397, loss 0.192326, acc 0.953125, learning_rate 0.000116235
2017-09-29T11:00:37.437335: step 1398, loss 0.309185, acc 0.875, learning_rate 0.000116169
2017-09-29T11:00:37.637550: step 1399, loss 0.267703, acc 0.921875, learning_rate 0.000116103
2017-09-29T11:00:37.824581: step 1400, loss 0.360608, acc 0.84375, learning_rate 0.000116037

Evaluation:
2017-09-29T11:00:38.319735: step 1400, loss 0.351051, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1400

2017-09-29T11:00:39.014526: step 1401, loss 0.369662, acc 0.875, learning_rate 0.000115972
2017-09-29T11:00:39.196951: step 1402, loss 0.319643, acc 0.890625, learning_rate 0.000115907
2017-09-29T11:00:39.380004: step 1403, loss 0.220697, acc 0.90625, learning_rate 0.000115842
2017-09-29T11:00:39.567054: step 1404, loss 0.398341, acc 0.890625, learning_rate 0.000115777
2017-09-29T11:00:39.754060: step 1405, loss 0.302989, acc 0.890625, learning_rate 0.000115713
2017-09-29T11:00:39.936915: step 1406, loss 0.417937, acc 0.859375, learning_rate 0.000115649
2017-09-29T11:00:40.120837: step 1407, loss 0.285032, acc 0.921875, learning_rate 0.000115585
2017-09-29T11:00:40.303820: step 1408, loss 0.484956, acc 0.765625, learning_rate 0.000115521
2017-09-29T11:00:40.489934: step 1409, loss 0.322824, acc 0.890625, learning_rate 0.000115458
2017-09-29T11:00:40.676068: step 1410, loss 0.183015, acc 0.9375, learning_rate 0.000115395
2017-09-29T11:00:40.859864: step 1411, loss 0.518678, acc 0.8125, learning_rate 0.000115332
2017-09-29T11:00:41.045460: step 1412, loss 0.37737, acc 0.90625, learning_rate 0.000115269
2017-09-29T11:00:41.239036: step 1413, loss 0.282755, acc 0.90625, learning_rate 0.000115207
2017-09-29T11:00:41.435145: step 1414, loss 0.322695, acc 0.890625, learning_rate 0.000115145
2017-09-29T11:00:41.622518: step 1415, loss 0.43958, acc 0.796875, learning_rate 0.000115083
2017-09-29T11:00:41.805769: step 1416, loss 0.298166, acc 0.890625, learning_rate 0.000115022
2017-09-29T11:00:41.990808: step 1417, loss 0.342019, acc 0.90625, learning_rate 0.00011496
2017-09-29T11:00:42.173188: step 1418, loss 0.263106, acc 0.921875, learning_rate 0.000114899
2017-09-29T11:00:42.357509: step 1419, loss 0.420749, acc 0.78125, learning_rate 0.000114838
2017-09-29T11:00:42.542403: step 1420, loss 0.363226, acc 0.859375, learning_rate 0.000114778
2017-09-29T11:00:42.730670: step 1421, loss 0.311966, acc 0.859375, learning_rate 0.000114717
2017-09-29T11:00:42.917111: step 1422, loss 0.377541, acc 0.859375, learning_rate 0.000114657
2017-09-29T11:00:43.105014: step 1423, loss 0.345131, acc 0.890625, learning_rate 0.000114598
2017-09-29T11:00:43.317400: step 1424, loss 0.318797, acc 0.859375, learning_rate 0.000114538
2017-09-29T11:00:43.521798: step 1425, loss 0.239458, acc 0.90625, learning_rate 0.000114479
2017-09-29T11:00:43.710211: step 1426, loss 0.38391, acc 0.875, learning_rate 0.00011442
2017-09-29T11:00:43.898415: step 1427, loss 0.180114, acc 0.9375, learning_rate 0.000114361
2017-09-29T11:00:44.092077: step 1428, loss 0.237542, acc 0.90625, learning_rate 0.000114302
2017-09-29T11:00:44.287728: step 1429, loss 0.182253, acc 0.953125, learning_rate 0.000114244
2017-09-29T11:00:44.473841: step 1430, loss 0.306558, acc 0.875, learning_rate 0.000114186
2017-09-29T11:00:44.670381: step 1431, loss 0.307899, acc 0.890625, learning_rate 0.000114128
2017-09-29T11:00:44.858164: step 1432, loss 0.284736, acc 0.859375, learning_rate 0.00011407
2017-09-29T11:00:45.046180: step 1433, loss 0.440255, acc 0.84375, learning_rate 0.000114013
2017-09-29T11:00:45.250654: step 1434, loss 0.340037, acc 0.859375, learning_rate 0.000113955
2017-09-29T11:00:45.442985: step 1435, loss 0.27927, acc 0.953125, learning_rate 0.000113898
2017-09-29T11:00:45.625312: step 1436, loss 0.275386, acc 0.90625, learning_rate 0.000113842
2017-09-29T11:00:45.809299: step 1437, loss 0.37215, acc 0.875, learning_rate 0.000113785
2017-09-29T11:00:46.017841: step 1438, loss 0.397158, acc 0.84375, learning_rate 0.000113729
2017-09-29T11:00:46.228194: step 1439, loss 0.345887, acc 0.890625, learning_rate 0.000113673
2017-09-29T11:00:46.428345: step 1440, loss 0.30206, acc 0.90625, learning_rate 0.000113617

Evaluation:
2017-09-29T11:00:46.969569: step 1440, loss 0.347655, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1440

2017-09-29T11:00:47.674101: step 1441, loss 0.438353, acc 0.875, learning_rate 0.000113561
2017-09-29T11:00:47.860590: step 1442, loss 0.312994, acc 0.859375, learning_rate 0.000113506
2017-09-29T11:00:48.049756: step 1443, loss 0.348928, acc 0.890625, learning_rate 0.000113451
2017-09-29T11:00:48.240881: step 1444, loss 0.170343, acc 0.96875, learning_rate 0.000113396
2017-09-29T11:00:48.438643: step 1445, loss 0.339837, acc 0.875, learning_rate 0.000113341
2017-09-29T11:00:48.625842: step 1446, loss 0.301599, acc 0.90625, learning_rate 0.000113287
2017-09-29T11:00:48.825897: step 1447, loss 0.318875, acc 0.890625, learning_rate 0.000113233
2017-09-29T11:00:49.016600: step 1448, loss 0.182395, acc 0.953125, learning_rate 0.000113179
2017-09-29T11:00:49.202811: step 1449, loss 0.449913, acc 0.796875, learning_rate 0.000113125
2017-09-29T11:00:49.404854: step 1450, loss 0.497161, acc 0.859375, learning_rate 0.000113071
2017-09-29T11:00:49.605304: step 1451, loss 0.234757, acc 0.9375, learning_rate 0.000113018
2017-09-29T11:00:49.803166: step 1452, loss 0.204088, acc 0.921875, learning_rate 0.000112965
2017-09-29T11:00:49.992243: step 1453, loss 0.328294, acc 0.84375, learning_rate 0.000112912
2017-09-29T11:00:50.184570: step 1454, loss 0.354037, acc 0.859375, learning_rate 0.000112859
2017-09-29T11:00:50.372643: step 1455, loss 0.306969, acc 0.875, learning_rate 0.000112807
2017-09-29T11:00:50.559944: step 1456, loss 0.25674, acc 0.890625, learning_rate 0.000112754
2017-09-29T11:00:50.744306: step 1457, loss 0.277579, acc 0.921875, learning_rate 0.000112702
2017-09-29T11:00:50.924188: step 1458, loss 0.256147, acc 0.90625, learning_rate 0.000112651
2017-09-29T11:00:51.106075: step 1459, loss 0.338716, acc 0.90625, learning_rate 0.000112599
2017-09-29T11:00:51.297587: step 1460, loss 0.21131, acc 0.9375, learning_rate 0.000112547
2017-09-29T11:00:51.487732: step 1461, loss 0.328398, acc 0.859375, learning_rate 0.000112496
2017-09-29T11:00:51.676074: step 1462, loss 0.181747, acc 0.953125, learning_rate 0.000112445
2017-09-29T11:00:51.860635: step 1463, loss 0.279045, acc 0.90625, learning_rate 0.000112394
2017-09-29T11:00:52.047611: step 1464, loss 0.276092, acc 0.890625, learning_rate 0.000112344
2017-09-29T11:00:52.237580: step 1465, loss 0.253168, acc 0.90625, learning_rate 0.000112293
2017-09-29T11:00:52.436430: step 1466, loss 0.23156, acc 0.875, learning_rate 0.000112243
2017-09-29T11:00:52.631458: step 1467, loss 0.30924, acc 0.859375, learning_rate 0.000112193
2017-09-29T11:00:52.819175: step 1468, loss 0.225724, acc 0.90625, learning_rate 0.000112144
2017-09-29T11:00:53.009524: step 1469, loss 0.352529, acc 0.890625, learning_rate 0.000112094
2017-09-29T11:00:53.166698: step 1470, loss 0.295329, acc 0.901961, learning_rate 0.000112045
2017-09-29T11:00:53.356424: step 1471, loss 0.258406, acc 0.9375, learning_rate 0.000111995
2017-09-29T11:00:53.544346: step 1472, loss 0.303389, acc 0.90625, learning_rate 0.000111946
2017-09-29T11:00:53.730447: step 1473, loss 0.291578, acc 0.921875, learning_rate 0.000111898
2017-09-29T11:00:53.916489: step 1474, loss 0.324984, acc 0.890625, learning_rate 0.000111849
2017-09-29T11:00:54.104090: step 1475, loss 0.293403, acc 0.90625, learning_rate 0.000111801
2017-09-29T11:00:54.288470: step 1476, loss 0.33534, acc 0.875, learning_rate 0.000111753
2017-09-29T11:00:54.496163: step 1477, loss 0.30005, acc 0.875, learning_rate 0.000111705
2017-09-29T11:00:54.688817: step 1478, loss 0.296668, acc 0.90625, learning_rate 0.000111657
2017-09-29T11:00:54.878317: step 1479, loss 0.194289, acc 0.921875, learning_rate 0.000111609
2017-09-29T11:00:55.068383: step 1480, loss 0.486204, acc 0.78125, learning_rate 0.000111562

Evaluation:
2017-09-29T11:00:55.615301: step 1480, loss 0.352446, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1480

2017-09-29T11:00:56.488241: step 1481, loss 0.348015, acc 0.859375, learning_rate 0.000111515
2017-09-29T11:00:56.706677: step 1482, loss 0.185755, acc 0.953125, learning_rate 0.000111468
2017-09-29T11:00:56.943775: step 1483, loss 0.232402, acc 0.921875, learning_rate 0.000111421
2017-09-29T11:00:57.173468: step 1484, loss 0.234373, acc 0.90625, learning_rate 0.000111374
2017-09-29T11:00:57.377221: step 1485, loss 0.265363, acc 0.890625, learning_rate 0.000111328
2017-09-29T11:00:57.595846: step 1486, loss 0.296608, acc 0.890625, learning_rate 0.000111282
2017-09-29T11:00:57.814736: step 1487, loss 0.353677, acc 0.875, learning_rate 0.000111236
2017-09-29T11:00:58.022035: step 1488, loss 0.230775, acc 0.921875, learning_rate 0.00011119
2017-09-29T11:00:58.223672: step 1489, loss 0.253651, acc 0.875, learning_rate 0.000111144
2017-09-29T11:00:58.430446: step 1490, loss 0.313888, acc 0.9375, learning_rate 0.000111099
2017-09-29T11:00:58.644717: step 1491, loss 0.308577, acc 0.859375, learning_rate 0.000111053
2017-09-29T11:00:58.863147: step 1492, loss 0.177778, acc 0.9375, learning_rate 0.000111008
2017-09-29T11:00:59.061948: step 1493, loss 0.19464, acc 0.921875, learning_rate 0.000110963
2017-09-29T11:00:59.285321: step 1494, loss 0.222628, acc 0.890625, learning_rate 0.000110918
2017-09-29T11:00:59.480709: step 1495, loss 0.358993, acc 0.859375, learning_rate 0.000110874
2017-09-29T11:00:59.670129: step 1496, loss 0.408231, acc 0.875, learning_rate 0.00011083
2017-09-29T11:00:59.860490: step 1497, loss 0.25322, acc 0.921875, learning_rate 0.000110785
2017-09-29T11:01:00.057494: step 1498, loss 0.237992, acc 0.9375, learning_rate 0.000110741
2017-09-29T11:01:00.245023: step 1499, loss 0.194124, acc 0.96875, learning_rate 0.000110697
2017-09-29T11:01:00.431953: step 1500, loss 0.214882, acc 0.921875, learning_rate 0.000110654
2017-09-29T11:01:00.623036: step 1501, loss 0.271084, acc 0.90625, learning_rate 0.00011061
2017-09-29T11:01:00.811082: step 1502, loss 0.357712, acc 0.90625, learning_rate 0.000110567
2017-09-29T11:01:01.008158: step 1503, loss 0.311238, acc 0.875, learning_rate 0.000110524
2017-09-29T11:01:01.199700: step 1504, loss 0.260327, acc 0.921875, learning_rate 0.000110481
2017-09-29T11:01:01.402434: step 1505, loss 0.433139, acc 0.890625, learning_rate 0.000110438
2017-09-29T11:01:01.596441: step 1506, loss 0.367747, acc 0.859375, learning_rate 0.000110396
2017-09-29T11:01:01.816657: step 1507, loss 0.324912, acc 0.921875, learning_rate 0.000110353
2017-09-29T11:01:02.042221: step 1508, loss 0.46251, acc 0.90625, learning_rate 0.000110311
2017-09-29T11:01:02.255942: step 1509, loss 0.358748, acc 0.875, learning_rate 0.000110269
2017-09-29T11:01:02.463881: step 1510, loss 0.30949, acc 0.90625, learning_rate 0.000110227
2017-09-29T11:01:02.685688: step 1511, loss 0.373681, acc 0.828125, learning_rate 0.000110185
2017-09-29T11:01:02.907966: step 1512, loss 0.294679, acc 0.890625, learning_rate 0.000110144
2017-09-29T11:01:03.140298: step 1513, loss 0.303837, acc 0.859375, learning_rate 0.000110102
2017-09-29T11:01:03.355822: step 1514, loss 0.429624, acc 0.8125, learning_rate 0.000110061
2017-09-29T11:01:03.564027: step 1515, loss 0.304987, acc 0.921875, learning_rate 0.00011002
2017-09-29T11:01:03.773650: step 1516, loss 0.344942, acc 0.875, learning_rate 0.000109979
2017-09-29T11:01:03.979800: step 1517, loss 0.277738, acc 0.90625, learning_rate 0.000109938
2017-09-29T11:01:04.188730: step 1518, loss 0.287352, acc 0.90625, learning_rate 0.000109898
2017-09-29T11:01:04.397489: step 1519, loss 0.298605, acc 0.890625, learning_rate 0.000109857
2017-09-29T11:01:04.612198: step 1520, loss 0.373987, acc 0.859375, learning_rate 0.000109817

Evaluation:
2017-09-29T11:01:05.579370: step 1520, loss 0.344664, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1520

2017-09-29T11:01:06.270638: step 1521, loss 0.408022, acc 0.875, learning_rate 0.000109777
2017-09-29T11:01:06.489975: step 1522, loss 0.52312, acc 0.859375, learning_rate 0.000109737
2017-09-29T11:01:06.702569: step 1523, loss 0.30784, acc 0.875, learning_rate 0.000109697
2017-09-29T11:01:06.913779: step 1524, loss 0.234403, acc 0.921875, learning_rate 0.000109658
2017-09-29T11:01:07.120220: step 1525, loss 0.388189, acc 0.890625, learning_rate 0.000109618
2017-09-29T11:01:07.328537: step 1526, loss 0.419529, acc 0.828125, learning_rate 0.000109579
2017-09-29T11:01:07.539432: step 1527, loss 0.381488, acc 0.828125, learning_rate 0.00010954
2017-09-29T11:01:07.751091: step 1528, loss 0.481801, acc 0.796875, learning_rate 0.000109501
2017-09-29T11:01:07.969934: step 1529, loss 0.458851, acc 0.859375, learning_rate 0.000109462
2017-09-29T11:01:08.190379: step 1530, loss 0.412309, acc 0.859375, learning_rate 0.000109424
2017-09-29T11:01:08.424807: step 1531, loss 0.268306, acc 0.9375, learning_rate 0.000109385
2017-09-29T11:01:08.635069: step 1532, loss 0.348788, acc 0.890625, learning_rate 0.000109347
2017-09-29T11:01:08.854237: step 1533, loss 0.355982, acc 0.859375, learning_rate 0.000109309
2017-09-29T11:01:09.077104: step 1534, loss 0.380598, acc 0.828125, learning_rate 0.000109271
2017-09-29T11:01:09.277267: step 1535, loss 0.277912, acc 0.890625, learning_rate 0.000109233
2017-09-29T11:01:09.505411: step 1536, loss 0.374192, acc 0.875, learning_rate 0.000109195
2017-09-29T11:01:09.716832: step 1537, loss 0.351362, acc 0.875, learning_rate 0.000109158
2017-09-29T11:01:09.935902: step 1538, loss 0.423369, acc 0.859375, learning_rate 0.00010912
2017-09-29T11:01:10.146418: step 1539, loss 0.356822, acc 0.859375, learning_rate 0.000109083
2017-09-29T11:01:10.355034: step 1540, loss 0.471803, acc 0.78125, learning_rate 0.000109046
2017-09-29T11:01:10.572756: step 1541, loss 0.273632, acc 0.90625, learning_rate 0.000109009
2017-09-29T11:01:10.797994: step 1542, loss 0.288565, acc 0.859375, learning_rate 0.000108972
2017-09-29T11:01:11.007400: step 1543, loss 0.408196, acc 0.84375, learning_rate 0.000108936
2017-09-29T11:01:11.222459: step 1544, loss 0.394599, acc 0.859375, learning_rate 0.000108899
2017-09-29T11:01:11.435401: step 1545, loss 0.291797, acc 0.859375, learning_rate 0.000108863
2017-09-29T11:01:11.653380: step 1546, loss 0.264475, acc 0.890625, learning_rate 0.000108827
2017-09-29T11:01:11.869184: step 1547, loss 0.354216, acc 0.890625, learning_rate 0.000108791
2017-09-29T11:01:12.081233: step 1548, loss 0.365144, acc 0.875, learning_rate 0.000108755
2017-09-29T11:01:12.291298: step 1549, loss 0.365052, acc 0.84375, learning_rate 0.000108719
2017-09-29T11:01:12.517522: step 1550, loss 0.423808, acc 0.890625, learning_rate 0.000108683
2017-09-29T11:01:12.747911: step 1551, loss 0.394688, acc 0.859375, learning_rate 0.000108648
2017-09-29T11:01:12.975407: step 1552, loss 0.245439, acc 0.921875, learning_rate 0.000108613
2017-09-29T11:01:13.183622: step 1553, loss 0.473801, acc 0.84375, learning_rate 0.000108577
2017-09-29T11:01:13.413480: step 1554, loss 0.32265, acc 0.890625, learning_rate 0.000108542
2017-09-29T11:01:13.640395: step 1555, loss 0.374479, acc 0.84375, learning_rate 0.000108508
2017-09-29T11:01:13.866256: step 1556, loss 0.235812, acc 0.890625, learning_rate 0.000108473
2017-09-29T11:01:14.078898: step 1557, loss 0.296844, acc 0.90625, learning_rate 0.000108438
2017-09-29T11:01:14.299259: step 1558, loss 0.288531, acc 0.90625, learning_rate 0.000108404
2017-09-29T11:01:14.516310: step 1559, loss 0.460279, acc 0.84375, learning_rate 0.00010837
2017-09-29T11:01:14.725823: step 1560, loss 0.240913, acc 0.953125, learning_rate 0.000108335

Evaluation:
2017-09-29T11:01:16.176027: step 1560, loss 0.350049, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1560

2017-09-29T11:01:16.903373: step 1561, loss 0.224462, acc 0.90625, learning_rate 0.000108301
2017-09-29T11:01:17.088402: step 1562, loss 0.195529, acc 0.921875, learning_rate 0.000108267
2017-09-29T11:01:17.288917: step 1563, loss 0.277948, acc 0.875, learning_rate 0.000108234
2017-09-29T11:01:17.486197: step 1564, loss 0.30885, acc 0.84375, learning_rate 0.0001082
2017-09-29T11:01:17.680677: step 1565, loss 0.206344, acc 0.921875, learning_rate 0.000108167
2017-09-29T11:01:17.876047: step 1566, loss 0.197554, acc 0.953125, learning_rate 0.000108133
2017-09-29T11:01:18.070517: step 1567, loss 0.376275, acc 0.859375, learning_rate 0.0001081
2017-09-29T11:01:18.234325: step 1568, loss 0.340497, acc 0.882353, learning_rate 0.000108067
2017-09-29T11:01:18.440002: step 1569, loss 0.395332, acc 0.84375, learning_rate 0.000108034
2017-09-29T11:01:18.634819: step 1570, loss 0.26597, acc 0.859375, learning_rate 0.000108001
2017-09-29T11:01:18.825443: step 1571, loss 0.346995, acc 0.875, learning_rate 0.000107969
2017-09-29T11:01:19.018566: step 1572, loss 0.204884, acc 0.96875, learning_rate 0.000107936
2017-09-29T11:01:19.210154: step 1573, loss 0.230404, acc 0.921875, learning_rate 0.000107904
2017-09-29T11:01:19.399122: step 1574, loss 0.377238, acc 0.890625, learning_rate 0.000107871
2017-09-29T11:01:19.590520: step 1575, loss 0.442883, acc 0.859375, learning_rate 0.000107839
2017-09-29T11:01:19.785120: step 1576, loss 0.355157, acc 0.875, learning_rate 0.000107807
2017-09-29T11:01:19.972614: step 1577, loss 0.424992, acc 0.859375, learning_rate 0.000107775
2017-09-29T11:01:20.151760: step 1578, loss 0.246343, acc 0.953125, learning_rate 0.000107744
2017-09-29T11:01:20.335236: step 1579, loss 0.289914, acc 0.90625, learning_rate 0.000107712
2017-09-29T11:01:20.528314: step 1580, loss 0.226479, acc 0.921875, learning_rate 0.000107681
2017-09-29T11:01:20.714718: step 1581, loss 0.424922, acc 0.828125, learning_rate 0.000107649
2017-09-29T11:01:20.897385: step 1582, loss 0.252893, acc 0.90625, learning_rate 0.000107618
2017-09-29T11:01:21.084909: step 1583, loss 0.319508, acc 0.890625, learning_rate 0.000107587
2017-09-29T11:01:21.270749: step 1584, loss 0.421053, acc 0.859375, learning_rate 0.000107556
2017-09-29T11:01:21.456990: step 1585, loss 0.247404, acc 0.921875, learning_rate 0.000107525
2017-09-29T11:01:21.654703: step 1586, loss 0.253008, acc 0.890625, learning_rate 0.000107494
2017-09-29T11:01:21.835749: step 1587, loss 0.385409, acc 0.875, learning_rate 0.000107464
2017-09-29T11:01:22.021328: step 1588, loss 0.349017, acc 0.890625, learning_rate 0.000107433
2017-09-29T11:01:22.206858: step 1589, loss 0.307931, acc 0.890625, learning_rate 0.000107403
2017-09-29T11:01:22.392285: step 1590, loss 0.28526, acc 0.890625, learning_rate 0.000107373
2017-09-29T11:01:22.579489: step 1591, loss 0.22073, acc 0.953125, learning_rate 0.000107343
2017-09-29T11:01:22.761646: step 1592, loss 0.368814, acc 0.859375, learning_rate 0.000107313
2017-09-29T11:01:22.945452: step 1593, loss 0.28236, acc 0.9375, learning_rate 0.000107283
2017-09-29T11:01:23.135534: step 1594, loss 0.161963, acc 0.921875, learning_rate 0.000107253
2017-09-29T11:01:23.336603: step 1595, loss 0.160055, acc 0.953125, learning_rate 0.000107224
2017-09-29T11:01:23.541310: step 1596, loss 0.255112, acc 0.921875, learning_rate 0.000107194
2017-09-29T11:01:23.729230: step 1597, loss 0.406375, acc 0.875, learning_rate 0.000107165
2017-09-29T11:01:23.925691: step 1598, loss 0.428344, acc 0.859375, learning_rate 0.000107136
2017-09-29T11:01:24.116308: step 1599, loss 0.339293, acc 0.921875, learning_rate 0.000107106
2017-09-29T11:01:24.307910: step 1600, loss 0.302362, acc 0.828125, learning_rate 0.000107077

Evaluation:
2017-09-29T11:01:24.860232: step 1600, loss 0.355335, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1600

2017-09-29T11:01:25.613883: step 1601, loss 0.286155, acc 0.921875, learning_rate 0.000107048
2017-09-29T11:01:25.803125: step 1602, loss 0.367404, acc 0.84375, learning_rate 0.00010702
2017-09-29T11:01:25.988867: step 1603, loss 0.411237, acc 0.84375, learning_rate 0.000106991
2017-09-29T11:01:26.177484: step 1604, loss 0.32453, acc 0.875, learning_rate 0.000106963
2017-09-29T11:01:26.372277: step 1605, loss 0.299071, acc 0.90625, learning_rate 0.000106934
2017-09-29T11:01:26.577085: step 1606, loss 0.373197, acc 0.859375, learning_rate 0.000106906
2017-09-29T11:01:26.767892: step 1607, loss 0.285783, acc 0.921875, learning_rate 0.000106878
2017-09-29T11:01:26.963755: step 1608, loss 0.284567, acc 0.90625, learning_rate 0.00010685
2017-09-29T11:01:27.152133: step 1609, loss 0.405425, acc 0.828125, learning_rate 0.000106822
2017-09-29T11:01:27.339885: step 1610, loss 0.242048, acc 0.921875, learning_rate 0.000106794
2017-09-29T11:01:27.528604: step 1611, loss 0.20431, acc 0.921875, learning_rate 0.000106766
2017-09-29T11:01:27.712012: step 1612, loss 0.248839, acc 0.875, learning_rate 0.000106738
2017-09-29T11:01:27.898199: step 1613, loss 0.447398, acc 0.84375, learning_rate 0.000106711
2017-09-29T11:01:28.088133: step 1614, loss 0.374115, acc 0.828125, learning_rate 0.000106684
2017-09-29T11:01:28.288633: step 1615, loss 0.211961, acc 0.9375, learning_rate 0.000106656
2017-09-29T11:01:28.481831: step 1616, loss 0.233481, acc 0.890625, learning_rate 0.000106629
2017-09-29T11:01:28.669431: step 1617, loss 0.349963, acc 0.875, learning_rate 0.000106602
2017-09-29T11:01:28.859738: step 1618, loss 0.246852, acc 0.921875, learning_rate 0.000106575
2017-09-29T11:01:29.046175: step 1619, loss 0.305285, acc 0.890625, learning_rate 0.000106548
2017-09-29T11:01:29.234402: step 1620, loss 0.305102, acc 0.90625, learning_rate 0.000106521
2017-09-29T11:01:29.426913: step 1621, loss 0.387029, acc 0.859375, learning_rate 0.000106495
2017-09-29T11:01:29.613523: step 1622, loss 0.254689, acc 0.90625, learning_rate 0.000106468
2017-09-29T11:01:29.815605: step 1623, loss 0.297511, acc 0.875, learning_rate 0.000106442
2017-09-29T11:01:30.008854: step 1624, loss 0.401645, acc 0.890625, learning_rate 0.000106416
2017-09-29T11:01:30.196489: step 1625, loss 0.314246, acc 0.875, learning_rate 0.000106389
2017-09-29T11:01:30.402334: step 1626, loss 0.261199, acc 0.921875, learning_rate 0.000106363
2017-09-29T11:01:30.592634: step 1627, loss 0.287847, acc 0.890625, learning_rate 0.000106337
2017-09-29T11:01:30.783830: step 1628, loss 0.170981, acc 0.953125, learning_rate 0.000106312
2017-09-29T11:01:30.970468: step 1629, loss 0.207327, acc 0.9375, learning_rate 0.000106286
2017-09-29T11:01:31.152445: step 1630, loss 0.271802, acc 0.890625, learning_rate 0.00010626
2017-09-29T11:01:31.336062: step 1631, loss 0.42363, acc 0.84375, learning_rate 0.000106235
2017-09-29T11:01:31.520866: step 1632, loss 0.32103, acc 0.859375, learning_rate 0.000106209
2017-09-29T11:01:31.703503: step 1633, loss 0.399966, acc 0.875, learning_rate 0.000106184
2017-09-29T11:01:31.887841: step 1634, loss 0.265601, acc 0.921875, learning_rate 0.000106159
2017-09-29T11:01:32.085388: step 1635, loss 0.32442, acc 0.84375, learning_rate 0.000106133
2017-09-29T11:01:32.273882: step 1636, loss 0.30188, acc 0.890625, learning_rate 0.000106108
2017-09-29T11:01:32.467996: step 1637, loss 0.402333, acc 0.859375, learning_rate 0.000106083
2017-09-29T11:01:32.657025: step 1638, loss 0.330176, acc 0.875, learning_rate 0.000106059
2017-09-29T11:01:32.842872: step 1639, loss 0.250673, acc 0.90625, learning_rate 0.000106034
2017-09-29T11:01:33.044218: step 1640, loss 0.166231, acc 0.96875, learning_rate 0.000106009

Evaluation:
2017-09-29T11:01:33.578077: step 1640, loss 0.344975, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1640

2017-09-29T11:01:34.382755: step 1641, loss 0.263345, acc 0.875, learning_rate 0.000105985
2017-09-29T11:01:34.567091: step 1642, loss 0.338707, acc 0.84375, learning_rate 0.00010596
2017-09-29T11:01:34.754965: step 1643, loss 0.31577, acc 0.890625, learning_rate 0.000105936
2017-09-29T11:01:34.942280: step 1644, loss 0.390849, acc 0.859375, learning_rate 0.000105912
2017-09-29T11:01:35.132340: step 1645, loss 0.333598, acc 0.84375, learning_rate 0.000105888
2017-09-29T11:01:35.321099: step 1646, loss 0.4912, acc 0.859375, learning_rate 0.000105864
2017-09-29T11:01:35.531786: step 1647, loss 0.43212, acc 0.84375, learning_rate 0.00010584
2017-09-29T11:01:35.740197: step 1648, loss 0.317836, acc 0.890625, learning_rate 0.000105816
2017-09-29T11:01:35.931506: step 1649, loss 0.422136, acc 0.859375, learning_rate 0.000105792
2017-09-29T11:01:36.115434: step 1650, loss 0.268884, acc 0.859375, learning_rate 0.000105768
2017-09-29T11:01:36.303936: step 1651, loss 0.428857, acc 0.859375, learning_rate 0.000105745
2017-09-29T11:01:36.485748: step 1652, loss 0.295281, acc 0.90625, learning_rate 0.000105721
2017-09-29T11:01:36.673328: step 1653, loss 0.282884, acc 0.890625, learning_rate 0.000105698
2017-09-29T11:01:36.855688: step 1654, loss 0.334431, acc 0.90625, learning_rate 0.000105675
2017-09-29T11:01:37.039280: step 1655, loss 0.162937, acc 0.953125, learning_rate 0.000105652
2017-09-29T11:01:37.225947: step 1656, loss 0.309521, acc 0.90625, learning_rate 0.000105629
2017-09-29T11:01:37.415754: step 1657, loss 0.296558, acc 0.90625, learning_rate 0.000105606
2017-09-29T11:01:37.600882: step 1658, loss 0.354342, acc 0.875, learning_rate 0.000105583
2017-09-29T11:01:37.789635: step 1659, loss 0.406818, acc 0.859375, learning_rate 0.00010556
2017-09-29T11:01:37.979846: step 1660, loss 0.336824, acc 0.890625, learning_rate 0.000105537
2017-09-29T11:01:38.177696: step 1661, loss 0.256028, acc 0.921875, learning_rate 0.000105515
2017-09-29T11:01:38.363599: step 1662, loss 0.308005, acc 0.875, learning_rate 0.000105492
2017-09-29T11:01:38.566605: step 1663, loss 0.28698, acc 0.90625, learning_rate 0.00010547
2017-09-29T11:01:38.751216: step 1664, loss 0.196753, acc 0.96875, learning_rate 0.000105447
2017-09-29T11:01:38.935284: step 1665, loss 0.293777, acc 0.90625, learning_rate 0.000105425
2017-09-29T11:01:39.088574: step 1666, loss 0.354533, acc 0.882353, learning_rate 0.000105403
2017-09-29T11:01:39.271934: step 1667, loss 0.256831, acc 0.90625, learning_rate 0.000105381
2017-09-29T11:01:39.455076: step 1668, loss 0.243362, acc 0.890625, learning_rate 0.000105359
2017-09-29T11:01:39.638233: step 1669, loss 0.327561, acc 0.828125, learning_rate 0.000105337
2017-09-29T11:01:39.834020: step 1670, loss 0.333066, acc 0.890625, learning_rate 0.000105315
2017-09-29T11:01:40.026294: step 1671, loss 0.35235, acc 0.875, learning_rate 0.000105294
2017-09-29T11:01:40.219573: step 1672, loss 0.383163, acc 0.875, learning_rate 0.000105272
2017-09-29T11:01:40.406506: step 1673, loss 0.272698, acc 0.90625, learning_rate 0.000105251
2017-09-29T11:01:40.588271: step 1674, loss 0.170676, acc 0.953125, learning_rate 0.000105229
2017-09-29T11:01:40.770785: step 1675, loss 0.455904, acc 0.828125, learning_rate 0.000105208
2017-09-29T11:01:40.959075: step 1676, loss 0.303219, acc 0.859375, learning_rate 0.000105186
2017-09-29T11:01:41.150262: step 1677, loss 0.285716, acc 0.875, learning_rate 0.000105165
2017-09-29T11:01:41.345016: step 1678, loss 0.383348, acc 0.84375, learning_rate 0.000105144
2017-09-29T11:01:41.536196: step 1679, loss 0.247555, acc 0.90625, learning_rate 0.000105123
2017-09-29T11:01:41.722186: step 1680, loss 0.372699, acc 0.90625, learning_rate 0.000105102

Evaluation:
2017-09-29T11:01:42.273348: step 1680, loss 0.340433, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1680

2017-09-29T11:01:42.905668: step 1681, loss 0.416374, acc 0.828125, learning_rate 0.000105081
2017-09-29T11:01:43.091924: step 1682, loss 0.425632, acc 0.84375, learning_rate 0.000105061
2017-09-29T11:01:43.288479: step 1683, loss 0.292437, acc 0.90625, learning_rate 0.00010504
2017-09-29T11:01:43.486956: step 1684, loss 0.303236, acc 0.875, learning_rate 0.00010502
2017-09-29T11:01:43.675213: step 1685, loss 0.219073, acc 0.9375, learning_rate 0.000104999
2017-09-29T11:01:43.866180: step 1686, loss 0.345031, acc 0.875, learning_rate 0.000104979
2017-09-29T11:01:44.047139: step 1687, loss 0.168091, acc 0.921875, learning_rate 0.000104958
2017-09-29T11:01:44.231496: step 1688, loss 0.481554, acc 0.8125, learning_rate 0.000104938
2017-09-29T11:01:44.416852: step 1689, loss 0.495435, acc 0.8125, learning_rate 0.000104918
2017-09-29T11:01:44.600544: step 1690, loss 0.356079, acc 0.90625, learning_rate 0.000104898
2017-09-29T11:01:44.794441: step 1691, loss 0.246443, acc 0.921875, learning_rate 0.000104878
2017-09-29T11:01:44.979654: step 1692, loss 0.231806, acc 0.875, learning_rate 0.000104858
2017-09-29T11:01:45.162610: step 1693, loss 0.276955, acc 0.921875, learning_rate 0.000104838
2017-09-29T11:01:45.349583: step 1694, loss 0.339208, acc 0.859375, learning_rate 0.000104818
2017-09-29T11:01:45.536963: step 1695, loss 0.289262, acc 0.921875, learning_rate 0.000104799
2017-09-29T11:01:45.723855: step 1696, loss 0.257349, acc 0.921875, learning_rate 0.000104779
2017-09-29T11:01:45.905643: step 1697, loss 0.189732, acc 0.953125, learning_rate 0.00010476
2017-09-29T11:01:46.091488: step 1698, loss 0.324363, acc 0.875, learning_rate 0.00010474
2017-09-29T11:01:46.282118: step 1699, loss 0.258387, acc 0.90625, learning_rate 0.000104721
2017-09-29T11:01:46.468078: step 1700, loss 0.257958, acc 0.921875, learning_rate 0.000104702
2017-09-29T11:01:46.653253: step 1701, loss 0.293751, acc 0.890625, learning_rate 0.000104682
2017-09-29T11:01:46.837991: step 1702, loss 0.225943, acc 0.9375, learning_rate 0.000104663
2017-09-29T11:01:47.021894: step 1703, loss 0.473234, acc 0.8125, learning_rate 0.000104644
2017-09-29T11:01:47.204017: step 1704, loss 0.183427, acc 0.9375, learning_rate 0.000104625
2017-09-29T11:01:47.385218: step 1705, loss 0.286435, acc 0.90625, learning_rate 0.000104606
2017-09-29T11:01:47.569117: step 1706, loss 0.297381, acc 0.921875, learning_rate 0.000104588
2017-09-29T11:01:47.753919: step 1707, loss 0.417221, acc 0.828125, learning_rate 0.000104569
2017-09-29T11:01:47.940511: step 1708, loss 0.262404, acc 0.921875, learning_rate 0.00010455
2017-09-29T11:01:48.124730: step 1709, loss 0.61643, acc 0.8125, learning_rate 0.000104532
2017-09-29T11:01:48.312057: step 1710, loss 0.157, acc 0.96875, learning_rate 0.000104513
2017-09-29T11:01:48.502496: step 1711, loss 0.227965, acc 0.953125, learning_rate 0.000104495
2017-09-29T11:01:48.695846: step 1712, loss 0.151278, acc 0.96875, learning_rate 0.000104476
2017-09-29T11:01:48.875613: step 1713, loss 0.291006, acc 0.90625, learning_rate 0.000104458
2017-09-29T11:01:49.059503: step 1714, loss 0.31624, acc 0.859375, learning_rate 0.00010444
2017-09-29T11:01:49.241623: step 1715, loss 0.344629, acc 0.875, learning_rate 0.000104422
2017-09-29T11:01:49.431868: step 1716, loss 0.0899428, acc 0.984375, learning_rate 0.000104404
2017-09-29T11:01:49.613019: step 1717, loss 0.253155, acc 0.921875, learning_rate 0.000104386
2017-09-29T11:01:49.793704: step 1718, loss 0.20454, acc 0.921875, learning_rate 0.000104368
2017-09-29T11:01:49.974495: step 1719, loss 0.345235, acc 0.84375, learning_rate 0.00010435
2017-09-29T11:01:50.155390: step 1720, loss 0.237079, acc 0.90625, learning_rate 0.000104332

Evaluation:
2017-09-29T11:01:50.694243: step 1720, loss 0.341704, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1720

2017-09-29T11:01:51.407402: step 1721, loss 0.269583, acc 0.90625, learning_rate 0.000104315
2017-09-29T11:01:51.590067: step 1722, loss 0.317155, acc 0.890625, learning_rate 0.000104297
2017-09-29T11:01:51.774584: step 1723, loss 0.327411, acc 0.859375, learning_rate 0.000104279
2017-09-29T11:01:51.958923: step 1724, loss 0.301385, acc 0.890625, learning_rate 0.000104262
2017-09-29T11:01:52.143822: step 1725, loss 0.410703, acc 0.84375, learning_rate 0.000104245
2017-09-29T11:01:52.327084: step 1726, loss 0.23823, acc 0.875, learning_rate 0.000104227
2017-09-29T11:01:52.510470: step 1727, loss 0.278829, acc 0.90625, learning_rate 0.00010421
2017-09-29T11:01:52.693042: step 1728, loss 0.316877, acc 0.875, learning_rate 0.000104193
2017-09-29T11:01:52.873615: step 1729, loss 0.400301, acc 0.84375, learning_rate 0.000104176
2017-09-29T11:01:53.056278: step 1730, loss 0.353547, acc 0.859375, learning_rate 0.000104159
2017-09-29T11:01:53.241690: step 1731, loss 0.357468, acc 0.875, learning_rate 0.000104142
2017-09-29T11:01:53.432144: step 1732, loss 0.348764, acc 0.875, learning_rate 0.000104125
2017-09-29T11:01:53.623261: step 1733, loss 0.201559, acc 0.9375, learning_rate 0.000104108
2017-09-29T11:01:53.811947: step 1734, loss 0.301526, acc 0.921875, learning_rate 0.000104091
2017-09-29T11:01:53.997054: step 1735, loss 0.331134, acc 0.84375, learning_rate 0.000104074
2017-09-29T11:01:54.202821: step 1736, loss 0.366668, acc 0.84375, learning_rate 0.000104058
2017-09-29T11:01:54.411732: step 1737, loss 0.472949, acc 0.828125, learning_rate 0.000104041
2017-09-29T11:01:54.611553: step 1738, loss 0.345359, acc 0.890625, learning_rate 0.000104025
2017-09-29T11:01:54.816233: step 1739, loss 0.202681, acc 0.921875, learning_rate 0.000104008
2017-09-29T11:01:55.021898: step 1740, loss 0.329774, acc 0.875, learning_rate 0.000103992
2017-09-29T11:01:55.226422: step 1741, loss 0.373253, acc 0.90625, learning_rate 0.000103976
2017-09-29T11:01:55.440735: step 1742, loss 0.293713, acc 0.890625, learning_rate 0.000103959
2017-09-29T11:01:55.623432: step 1743, loss 0.348057, acc 0.890625, learning_rate 0.000103943
2017-09-29T11:01:55.804664: step 1744, loss 0.244704, acc 0.90625, learning_rate 0.000103927
2017-09-29T11:01:55.987436: step 1745, loss 0.198035, acc 0.96875, learning_rate 0.000103911
2017-09-29T11:01:56.176849: step 1746, loss 0.148852, acc 0.953125, learning_rate 0.000103895
2017-09-29T11:01:56.362691: step 1747, loss 0.244911, acc 0.90625, learning_rate 0.000103879
2017-09-29T11:01:56.547657: step 1748, loss 0.427335, acc 0.828125, learning_rate 0.000103863
2017-09-29T11:01:56.730643: step 1749, loss 0.401606, acc 0.875, learning_rate 0.000103848
2017-09-29T11:01:56.913124: step 1750, loss 0.289175, acc 0.90625, learning_rate 0.000103832
2017-09-29T11:01:57.101738: step 1751, loss 0.394806, acc 0.828125, learning_rate 0.000103816
2017-09-29T11:01:57.291570: step 1752, loss 0.333057, acc 0.84375, learning_rate 0.000103801
2017-09-29T11:01:57.480850: step 1753, loss 0.230599, acc 0.9375, learning_rate 0.000103785
2017-09-29T11:01:57.687063: step 1754, loss 0.215902, acc 0.921875, learning_rate 0.00010377
2017-09-29T11:01:57.894547: step 1755, loss 0.168669, acc 0.90625, learning_rate 0.000103754
2017-09-29T11:01:58.099413: step 1756, loss 0.355787, acc 0.859375, learning_rate 0.000103739
2017-09-29T11:01:58.295891: step 1757, loss 0.312612, acc 0.875, learning_rate 0.000103724
2017-09-29T11:01:58.486644: step 1758, loss 0.242359, acc 0.9375, learning_rate 0.000103709
2017-09-29T11:01:58.672732: step 1759, loss 0.233929, acc 0.90625, learning_rate 0.000103694
2017-09-29T11:01:58.863965: step 1760, loss 0.487637, acc 0.84375, learning_rate 0.000103678

Evaluation:
2017-09-29T11:01:59.406688: step 1760, loss 0.343229, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1760

2017-09-29T11:02:00.106552: step 1761, loss 0.278772, acc 0.90625, learning_rate 0.000103663
2017-09-29T11:02:00.289896: step 1762, loss 0.397457, acc 0.875, learning_rate 0.000103648
2017-09-29T11:02:00.476223: step 1763, loss 0.458012, acc 0.859375, learning_rate 0.000103634
2017-09-29T11:02:00.631252: step 1764, loss 0.189965, acc 0.941176, learning_rate 0.000103619
2017-09-29T11:02:00.818741: step 1765, loss 0.392968, acc 0.90625, learning_rate 0.000103604
2017-09-29T11:02:01.006767: step 1766, loss 0.408938, acc 0.875, learning_rate 0.000103589
2017-09-29T11:02:01.188067: step 1767, loss 0.339924, acc 0.859375, learning_rate 0.000103575
2017-09-29T11:02:01.369242: step 1768, loss 0.213668, acc 0.9375, learning_rate 0.00010356
2017-09-29T11:02:01.566948: step 1769, loss 0.254247, acc 0.890625, learning_rate 0.000103545
2017-09-29T11:02:01.749567: step 1770, loss 0.21671, acc 0.921875, learning_rate 0.000103531
2017-09-29T11:02:01.932441: step 1771, loss 0.278675, acc 0.890625, learning_rate 0.000103517
2017-09-29T11:02:02.114562: step 1772, loss 0.423813, acc 0.828125, learning_rate 0.000103502
2017-09-29T11:02:02.301008: step 1773, loss 0.378252, acc 0.828125, learning_rate 0.000103488
2017-09-29T11:02:02.483609: step 1774, loss 0.389921, acc 0.859375, learning_rate 0.000103474
2017-09-29T11:02:02.664679: step 1775, loss 0.245515, acc 0.90625, learning_rate 0.00010346
2017-09-29T11:02:02.850929: step 1776, loss 0.367499, acc 0.828125, learning_rate 0.000103445
2017-09-29T11:02:03.033489: step 1777, loss 0.457741, acc 0.828125, learning_rate 0.000103431
2017-09-29T11:02:03.215932: step 1778, loss 0.542957, acc 0.84375, learning_rate 0.000103417
2017-09-29T11:02:03.420491: step 1779, loss 0.439918, acc 0.890625, learning_rate 0.000103403
2017-09-29T11:02:03.619434: step 1780, loss 0.178776, acc 0.953125, learning_rate 0.00010339
2017-09-29T11:02:03.809495: step 1781, loss 0.186257, acc 0.953125, learning_rate 0.000103376
2017-09-29T11:02:03.989608: step 1782, loss 0.22187, acc 0.90625, learning_rate 0.000103362
2017-09-29T11:02:04.176601: step 1783, loss 0.23288, acc 0.921875, learning_rate 0.000103348
2017-09-29T11:02:04.357365: step 1784, loss 0.190952, acc 0.921875, learning_rate 0.000103335
2017-09-29T11:02:04.550870: step 1785, loss 0.237072, acc 0.921875, learning_rate 0.000103321
2017-09-29T11:02:04.740698: step 1786, loss 0.246362, acc 0.890625, learning_rate 0.000103307
2017-09-29T11:02:04.945784: step 1787, loss 0.295947, acc 0.9375, learning_rate 0.000103294
2017-09-29T11:02:05.154693: step 1788, loss 0.249491, acc 0.953125, learning_rate 0.00010328
2017-09-29T11:02:05.341000: step 1789, loss 0.210581, acc 0.953125, learning_rate 0.000103267
2017-09-29T11:02:05.519864: step 1790, loss 0.282243, acc 0.875, learning_rate 0.000103254
2017-09-29T11:02:05.702364: step 1791, loss 0.397883, acc 0.875, learning_rate 0.00010324
2017-09-29T11:02:05.883492: step 1792, loss 0.35541, acc 0.875, learning_rate 0.000103227
2017-09-29T11:02:06.064567: step 1793, loss 0.462495, acc 0.828125, learning_rate 0.000103214
2017-09-29T11:02:06.259673: step 1794, loss 0.293739, acc 0.90625, learning_rate 0.000103201
2017-09-29T11:02:06.446780: step 1795, loss 0.453709, acc 0.84375, learning_rate 0.000103188
2017-09-29T11:02:06.632022: step 1796, loss 0.327509, acc 0.90625, learning_rate 0.000103175
2017-09-29T11:02:06.812629: step 1797, loss 0.438069, acc 0.875, learning_rate 0.000103162
2017-09-29T11:02:06.996464: step 1798, loss 0.440308, acc 0.875, learning_rate 0.000103149
2017-09-29T11:02:07.179968: step 1799, loss 0.140974, acc 0.953125, learning_rate 0.000103136
2017-09-29T11:02:07.364505: step 1800, loss 0.309709, acc 0.875, learning_rate 0.000103123

Evaluation:
2017-09-29T11:02:07.905210: step 1800, loss 0.340038, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1800

2017-09-29T11:02:08.708323: step 1801, loss 0.2559, acc 0.921875, learning_rate 0.000103111
2017-09-29T11:02:08.905973: step 1802, loss 0.343259, acc 0.84375, learning_rate 0.000103098
2017-09-29T11:02:09.101302: step 1803, loss 0.242943, acc 0.9375, learning_rate 0.000103085
2017-09-29T11:02:09.292428: step 1804, loss 0.255911, acc 0.890625, learning_rate 0.000103073
2017-09-29T11:02:09.478994: step 1805, loss 0.308123, acc 0.90625, learning_rate 0.00010306
2017-09-29T11:02:09.675731: step 1806, loss 0.303483, acc 0.890625, learning_rate 0.000103048
2017-09-29T11:02:09.857597: step 1807, loss 0.564966, acc 0.78125, learning_rate 0.000103035
2017-09-29T11:02:10.041957: step 1808, loss 0.169552, acc 0.9375, learning_rate 0.000103023
2017-09-29T11:02:10.221041: step 1809, loss 0.317872, acc 0.890625, learning_rate 0.00010301
2017-09-29T11:02:10.402367: step 1810, loss 0.353116, acc 0.859375, learning_rate 0.000102998
2017-09-29T11:02:10.586919: step 1811, loss 0.352341, acc 0.890625, learning_rate 0.000102986
2017-09-29T11:02:10.771864: step 1812, loss 0.402221, acc 0.921875, learning_rate 0.000102974
2017-09-29T11:02:10.954480: step 1813, loss 0.211336, acc 0.90625, learning_rate 0.000102962
2017-09-29T11:02:11.145707: step 1814, loss 0.319735, acc 0.890625, learning_rate 0.000102949
2017-09-29T11:02:11.345899: step 1815, loss 0.376983, acc 0.828125, learning_rate 0.000102937
2017-09-29T11:02:11.547087: step 1816, loss 0.332583, acc 0.859375, learning_rate 0.000102925
2017-09-29T11:02:11.743271: step 1817, loss 0.342338, acc 0.859375, learning_rate 0.000102913
2017-09-29T11:02:11.943274: step 1818, loss 0.306737, acc 0.84375, learning_rate 0.000102902
2017-09-29T11:02:12.141405: step 1819, loss 0.22735, acc 0.921875, learning_rate 0.00010289
2017-09-29T11:02:12.349098: step 1820, loss 0.221428, acc 0.9375, learning_rate 0.000102878
2017-09-29T11:02:12.568142: step 1821, loss 0.236756, acc 0.9375, learning_rate 0.000102866
2017-09-29T11:02:12.764546: step 1822, loss 0.295512, acc 0.875, learning_rate 0.000102855
2017-09-29T11:02:12.952330: step 1823, loss 0.615757, acc 0.8125, learning_rate 0.000102843
2017-09-29T11:02:13.138747: step 1824, loss 0.334614, acc 0.875, learning_rate 0.000102831
2017-09-29T11:02:13.324683: step 1825, loss 0.283263, acc 0.890625, learning_rate 0.00010282
2017-09-29T11:02:13.508635: step 1826, loss 0.371497, acc 0.84375, learning_rate 0.000102808
2017-09-29T11:02:13.706824: step 1827, loss 0.54024, acc 0.828125, learning_rate 0.000102797
2017-09-29T11:02:13.936660: step 1828, loss 0.380072, acc 0.84375, learning_rate 0.000102785
2017-09-29T11:02:14.137146: step 1829, loss 0.366499, acc 0.859375, learning_rate 0.000102774
2017-09-29T11:02:14.323932: step 1830, loss 0.400758, acc 0.84375, learning_rate 0.000102763
2017-09-29T11:02:14.508465: step 1831, loss 0.401891, acc 0.84375, learning_rate 0.000102751
2017-09-29T11:02:14.699691: step 1832, loss 0.34811, acc 0.890625, learning_rate 0.00010274
2017-09-29T11:02:14.902644: step 1833, loss 0.341532, acc 0.890625, learning_rate 0.000102729
2017-09-29T11:02:15.106991: step 1834, loss 0.185251, acc 0.921875, learning_rate 0.000102718
2017-09-29T11:02:15.303271: step 1835, loss 0.212302, acc 0.921875, learning_rate 0.000102707
2017-09-29T11:02:15.491335: step 1836, loss 0.300561, acc 0.875, learning_rate 0.000102696
2017-09-29T11:02:15.713351: step 1837, loss 0.379568, acc 0.859375, learning_rate 0.000102685
2017-09-29T11:02:15.930868: step 1838, loss 0.257435, acc 0.90625, learning_rate 0.000102674
2017-09-29T11:02:16.121712: step 1839, loss 0.289776, acc 0.890625, learning_rate 0.000102663
2017-09-29T11:02:16.309802: step 1840, loss 0.269737, acc 0.875, learning_rate 0.000102652

Evaluation:
2017-09-29T11:02:17.293221: step 1840, loss 0.342306, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1840

2017-09-29T11:02:17.951582: step 1841, loss 0.276733, acc 0.90625, learning_rate 0.000102641
2017-09-29T11:02:18.156272: step 1842, loss 0.480437, acc 0.890625, learning_rate 0.00010263
2017-09-29T11:02:18.363158: step 1843, loss 0.347306, acc 0.875, learning_rate 0.00010262
2017-09-29T11:02:18.579084: step 1844, loss 0.140118, acc 0.953125, learning_rate 0.000102609
2017-09-29T11:02:18.783705: step 1845, loss 0.236143, acc 0.90625, learning_rate 0.000102598
2017-09-29T11:02:18.995209: step 1846, loss 0.455941, acc 0.875, learning_rate 0.000102588
2017-09-29T11:02:19.185304: step 1847, loss 0.409122, acc 0.828125, learning_rate 0.000102577
2017-09-29T11:02:19.371644: step 1848, loss 0.215365, acc 0.953125, learning_rate 0.000102567
2017-09-29T11:02:19.556301: step 1849, loss 0.273112, acc 0.921875, learning_rate 0.000102556
2017-09-29T11:02:19.742085: step 1850, loss 0.34553, acc 0.84375, learning_rate 0.000102546
2017-09-29T11:02:19.935063: step 1851, loss 0.267251, acc 0.90625, learning_rate 0.000102535
2017-09-29T11:02:20.129509: step 1852, loss 0.299379, acc 0.890625, learning_rate 0.000102525
2017-09-29T11:02:20.331007: step 1853, loss 0.280209, acc 0.890625, learning_rate 0.000102515
2017-09-29T11:02:20.519242: step 1854, loss 0.249123, acc 0.9375, learning_rate 0.000102504
2017-09-29T11:02:20.720014: step 1855, loss 0.200871, acc 0.9375, learning_rate 0.000102494
2017-09-29T11:02:20.913815: step 1856, loss 0.384814, acc 0.859375, learning_rate 0.000102484
2017-09-29T11:02:21.106709: step 1857, loss 0.398328, acc 0.890625, learning_rate 0.000102474
2017-09-29T11:02:21.304769: step 1858, loss 0.294373, acc 0.859375, learning_rate 0.000102464
2017-09-29T11:02:21.514218: step 1859, loss 0.327434, acc 0.921875, learning_rate 0.000102454
2017-09-29T11:02:21.694682: step 1860, loss 0.310034, acc 0.84375, learning_rate 0.000102444
2017-09-29T11:02:21.875584: step 1861, loss 0.225985, acc 0.90625, learning_rate 0.000102434
2017-09-29T11:02:22.026495: step 1862, loss 0.411739, acc 0.843137, learning_rate 0.000102424
2017-09-29T11:02:22.211588: step 1863, loss 0.254484, acc 0.921875, learning_rate 0.000102414
2017-09-29T11:02:22.395249: step 1864, loss 0.363863, acc 0.875, learning_rate 0.000102404
2017-09-29T11:02:22.580273: step 1865, loss 0.292214, acc 0.90625, learning_rate 0.000102394
2017-09-29T11:02:22.765446: step 1866, loss 0.335901, acc 0.921875, learning_rate 0.000102384
2017-09-29T11:02:22.952110: step 1867, loss 0.432117, acc 0.84375, learning_rate 0.000102375
2017-09-29T11:02:23.156298: step 1868, loss 0.35829, acc 0.84375, learning_rate 0.000102365
2017-09-29T11:02:23.357792: step 1869, loss 0.316769, acc 0.890625, learning_rate 0.000102355
2017-09-29T11:02:23.562542: step 1870, loss 0.280249, acc 0.90625, learning_rate 0.000102346
2017-09-29T11:02:23.765495: step 1871, loss 0.282977, acc 0.953125, learning_rate 0.000102336
2017-09-29T11:02:23.976110: step 1872, loss 0.347208, acc 0.890625, learning_rate 0.000102327
2017-09-29T11:02:24.180277: step 1873, loss 0.212954, acc 0.921875, learning_rate 0.000102317
2017-09-29T11:02:24.380887: step 1874, loss 0.439249, acc 0.828125, learning_rate 0.000102308
2017-09-29T11:02:24.595782: step 1875, loss 0.494456, acc 0.796875, learning_rate 0.000102298
2017-09-29T11:02:24.798034: step 1876, loss 0.384563, acc 0.859375, learning_rate 0.000102289
2017-09-29T11:02:24.992807: step 1877, loss 0.355016, acc 0.84375, learning_rate 0.000102279
2017-09-29T11:02:25.190663: step 1878, loss 0.291965, acc 0.890625, learning_rate 0.00010227
2017-09-29T11:02:25.376378: step 1879, loss 0.299065, acc 0.890625, learning_rate 0.000102261
2017-09-29T11:02:25.564045: step 1880, loss 0.272262, acc 0.90625, learning_rate 0.000102252

Evaluation:
2017-09-29T11:02:26.145796: step 1880, loss 0.340306, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1880

2017-09-29T11:02:26.878620: step 1881, loss 0.178714, acc 0.953125, learning_rate 0.000102242
2017-09-29T11:02:27.082966: step 1882, loss 0.202264, acc 0.953125, learning_rate 0.000102233
2017-09-29T11:02:27.283344: step 1883, loss 0.309976, acc 0.921875, learning_rate 0.000102224
2017-09-29T11:02:27.484142: step 1884, loss 0.441389, acc 0.828125, learning_rate 0.000102215
2017-09-29T11:02:27.672797: step 1885, loss 0.224811, acc 0.921875, learning_rate 0.000102206
2017-09-29T11:02:27.859460: step 1886, loss 0.172892, acc 0.921875, learning_rate 0.000102197
2017-09-29T11:02:28.041704: step 1887, loss 0.358355, acc 0.890625, learning_rate 0.000102188
2017-09-29T11:02:28.224460: step 1888, loss 0.293778, acc 0.890625, learning_rate 0.000102179
2017-09-29T11:02:28.418408: step 1889, loss 0.323856, acc 0.875, learning_rate 0.00010217
2017-09-29T11:02:28.607468: step 1890, loss 0.203325, acc 0.953125, learning_rate 0.000102161
2017-09-29T11:02:29.005405: step 1891, loss 0.364053, acc 0.875, learning_rate 0.000102153
2017-09-29T11:02:29.198465: step 1892, loss 0.27944, acc 0.90625, learning_rate 0.000102144
2017-09-29T11:02:29.399639: step 1893, loss 0.363316, acc 0.859375, learning_rate 0.000102135
2017-09-29T11:02:29.582766: step 1894, loss 0.299777, acc 0.859375, learning_rate 0.000102126
2017-09-29T11:02:29.777676: step 1895, loss 0.313927, acc 0.84375, learning_rate 0.000102118
2017-09-29T11:02:29.975700: step 1896, loss 0.254309, acc 0.9375, learning_rate 0.000102109
2017-09-29T11:02:30.175892: step 1897, loss 0.357517, acc 0.890625, learning_rate 0.0001021
2017-09-29T11:02:30.382727: step 1898, loss 0.235547, acc 0.921875, learning_rate 0.000102092
2017-09-29T11:02:30.590987: step 1899, loss 0.317443, acc 0.828125, learning_rate 0.000102083
2017-09-29T11:02:30.796450: step 1900, loss 0.399905, acc 0.890625, learning_rate 0.000102075
2017-09-29T11:02:30.998051: step 1901, loss 0.140136, acc 0.953125, learning_rate 0.000102066
2017-09-29T11:02:31.197883: step 1902, loss 0.457792, acc 0.875, learning_rate 0.000102058
2017-09-29T11:02:31.395924: step 1903, loss 0.295176, acc 0.875, learning_rate 0.00010205
2017-09-29T11:02:31.597244: step 1904, loss 0.365954, acc 0.875, learning_rate 0.000102041
2017-09-29T11:02:31.781871: step 1905, loss 0.265438, acc 0.90625, learning_rate 0.000102033
2017-09-29T11:02:31.962931: step 1906, loss 0.268494, acc 0.9375, learning_rate 0.000102025
2017-09-29T11:02:32.146346: step 1907, loss 0.309319, acc 0.90625, learning_rate 0.000102016
2017-09-29T11:02:32.340621: step 1908, loss 0.341163, acc 0.90625, learning_rate 0.000102008
2017-09-29T11:02:32.543078: step 1909, loss 0.386087, acc 0.859375, learning_rate 0.000102
2017-09-29T11:02:32.746109: step 1910, loss 0.280814, acc 0.921875, learning_rate 0.000101992
2017-09-29T11:02:32.947174: step 1911, loss 0.238581, acc 0.90625, learning_rate 0.000101984
2017-09-29T11:02:33.153297: step 1912, loss 0.358629, acc 0.875, learning_rate 0.000101975
2017-09-29T11:02:33.344870: step 1913, loss 0.357212, acc 0.875, learning_rate 0.000101967
2017-09-29T11:02:33.532585: step 1914, loss 0.21134, acc 0.875, learning_rate 0.000101959
2017-09-29T11:02:33.718453: step 1915, loss 0.234855, acc 0.890625, learning_rate 0.000101951
2017-09-29T11:02:33.899420: step 1916, loss 0.330743, acc 0.890625, learning_rate 0.000101943
2017-09-29T11:02:34.096379: step 1917, loss 0.134297, acc 0.96875, learning_rate 0.000101935
2017-09-29T11:02:34.280896: step 1918, loss 0.271868, acc 0.921875, learning_rate 0.000101928
2017-09-29T11:02:34.467851: step 1919, loss 0.336211, acc 0.890625, learning_rate 0.00010192
2017-09-29T11:02:34.649620: step 1920, loss 0.401519, acc 0.84375, learning_rate 0.000101912

Evaluation:
2017-09-29T11:02:35.193881: step 1920, loss 0.340375, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1920

2017-09-29T11:02:35.918518: step 1921, loss 0.317809, acc 0.921875, learning_rate 0.000101904
2017-09-29T11:02:36.101991: step 1922, loss 0.194138, acc 0.9375, learning_rate 0.000101896
2017-09-29T11:02:36.281475: step 1923, loss 0.321574, acc 0.859375, learning_rate 0.000101889
2017-09-29T11:02:36.475709: step 1924, loss 0.265062, acc 0.859375, learning_rate 0.000101881
2017-09-29T11:02:36.658321: step 1925, loss 0.262829, acc 0.890625, learning_rate 0.000101873
2017-09-29T11:02:36.844828: step 1926, loss 0.250656, acc 0.921875, learning_rate 0.000101865
2017-09-29T11:02:37.026087: step 1927, loss 0.392162, acc 0.859375, learning_rate 0.000101858
2017-09-29T11:02:37.209950: step 1928, loss 0.262844, acc 0.9375, learning_rate 0.00010185
2017-09-29T11:02:37.397064: step 1929, loss 0.186797, acc 0.90625, learning_rate 0.000101843
2017-09-29T11:02:37.581875: step 1930, loss 0.266152, acc 0.921875, learning_rate 0.000101835
2017-09-29T11:02:37.764297: step 1931, loss 0.256366, acc 0.90625, learning_rate 0.000101828
2017-09-29T11:02:37.946705: step 1932, loss 0.356911, acc 0.828125, learning_rate 0.00010182
2017-09-29T11:02:38.128810: step 1933, loss 0.217478, acc 0.921875, learning_rate 0.000101813
2017-09-29T11:02:38.309416: step 1934, loss 0.401568, acc 0.84375, learning_rate 0.000101805
2017-09-29T11:02:38.497804: step 1935, loss 0.220004, acc 0.90625, learning_rate 0.000101798
2017-09-29T11:02:38.682592: step 1936, loss 0.246774, acc 0.9375, learning_rate 0.000101791
2017-09-29T11:02:38.864979: step 1937, loss 0.303123, acc 0.890625, learning_rate 0.000101783
2017-09-29T11:02:39.060085: step 1938, loss 0.324465, acc 0.875, learning_rate 0.000101776
2017-09-29T11:02:39.244657: step 1939, loss 0.334006, acc 0.90625, learning_rate 0.000101769
2017-09-29T11:02:39.428658: step 1940, loss 0.316489, acc 0.890625, learning_rate 0.000101762
2017-09-29T11:02:39.613230: step 1941, loss 0.429788, acc 0.828125, learning_rate 0.000101754
2017-09-29T11:02:39.796535: step 1942, loss 0.115749, acc 0.984375, learning_rate 0.000101747
2017-09-29T11:02:39.985181: step 1943, loss 0.221819, acc 0.90625, learning_rate 0.00010174
2017-09-29T11:02:40.165849: step 1944, loss 0.222065, acc 0.90625, learning_rate 0.000101733
2017-09-29T11:02:40.350555: step 1945, loss 0.386033, acc 0.890625, learning_rate 0.000101726
2017-09-29T11:02:40.533620: step 1946, loss 0.28821, acc 0.890625, learning_rate 0.000101719
2017-09-29T11:02:40.721012: step 1947, loss 0.205018, acc 0.953125, learning_rate 0.000101712
2017-09-29T11:02:40.911766: step 1948, loss 0.178305, acc 0.953125, learning_rate 0.000101705
2017-09-29T11:02:41.102614: step 1949, loss 0.479128, acc 0.84375, learning_rate 0.000101698
2017-09-29T11:02:41.288203: step 1950, loss 0.388181, acc 0.84375, learning_rate 0.000101691
2017-09-29T11:02:41.475413: step 1951, loss 0.30377, acc 0.921875, learning_rate 0.000101684
2017-09-29T11:02:41.656677: step 1952, loss 0.0822155, acc 1, learning_rate 0.000101677
2017-09-29T11:02:41.839116: step 1953, loss 0.407859, acc 0.8125, learning_rate 0.00010167
2017-09-29T11:02:42.023480: step 1954, loss 0.232427, acc 0.90625, learning_rate 0.000101664
2017-09-29T11:02:42.222955: step 1955, loss 0.207262, acc 0.984375, learning_rate 0.000101657
2017-09-29T11:02:42.405974: step 1956, loss 0.288611, acc 0.90625, learning_rate 0.00010165
2017-09-29T11:02:42.601137: step 1957, loss 0.309579, acc 0.890625, learning_rate 0.000101643
2017-09-29T11:02:42.781811: step 1958, loss 0.295803, acc 0.9375, learning_rate 0.000101637
2017-09-29T11:02:42.968660: step 1959, loss 0.351904, acc 0.890625, learning_rate 0.00010163
2017-09-29T11:02:43.117431: step 1960, loss 0.328844, acc 0.882353, learning_rate 0.000101623

Evaluation:
2017-09-29T11:02:43.673032: step 1960, loss 0.343015, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-1960

2017-09-29T11:02:44.500926: step 1961, loss 0.366468, acc 0.859375, learning_rate 0.000101617
2017-09-29T11:02:44.699973: step 1962, loss 0.41551, acc 0.859375, learning_rate 0.00010161
2017-09-29T11:02:44.890736: step 1963, loss 0.248042, acc 0.921875, learning_rate 0.000101604
2017-09-29T11:02:45.097962: step 1964, loss 0.22666, acc 0.921875, learning_rate 0.000101597
2017-09-29T11:02:45.286793: step 1965, loss 0.394868, acc 0.84375, learning_rate 0.00010159
2017-09-29T11:02:45.472177: step 1966, loss 0.391557, acc 0.859375, learning_rate 0.000101584
2017-09-29T11:02:45.655009: step 1967, loss 0.360482, acc 0.875, learning_rate 0.000101577
2017-09-29T11:02:45.839588: step 1968, loss 0.272428, acc 0.875, learning_rate 0.000101571
2017-09-29T11:02:46.020629: step 1969, loss 0.336768, acc 0.890625, learning_rate 0.000101565
2017-09-29T11:02:46.202894: step 1970, loss 0.292315, acc 0.921875, learning_rate 0.000101558
2017-09-29T11:02:46.386233: step 1971, loss 0.231952, acc 0.9375, learning_rate 0.000101552
2017-09-29T11:02:46.570079: step 1972, loss 0.190029, acc 0.96875, learning_rate 0.000101546
2017-09-29T11:02:46.755115: step 1973, loss 0.224394, acc 0.953125, learning_rate 0.000101539
2017-09-29T11:02:46.934274: step 1974, loss 0.372583, acc 0.890625, learning_rate 0.000101533
2017-09-29T11:02:47.119367: step 1975, loss 0.228129, acc 0.90625, learning_rate 0.000101527
2017-09-29T11:02:47.299792: step 1976, loss 0.37274, acc 0.90625, learning_rate 0.00010152
2017-09-29T11:02:47.493207: step 1977, loss 0.373627, acc 0.890625, learning_rate 0.000101514
2017-09-29T11:02:47.680645: step 1978, loss 0.312937, acc 0.875, learning_rate 0.000101508
2017-09-29T11:02:47.867900: step 1979, loss 0.326768, acc 0.921875, learning_rate 0.000101502
2017-09-29T11:02:48.051511: step 1980, loss 0.373987, acc 0.84375, learning_rate 0.000101496
2017-09-29T11:02:48.234174: step 1981, loss 0.341866, acc 0.859375, learning_rate 0.00010149
2017-09-29T11:02:48.418693: step 1982, loss 0.257735, acc 0.875, learning_rate 0.000101484
2017-09-29T11:02:48.606708: step 1983, loss 0.387229, acc 0.890625, learning_rate 0.000101478
2017-09-29T11:02:48.805451: step 1984, loss 0.288399, acc 0.875, learning_rate 0.000101472
2017-09-29T11:02:48.986238: step 1985, loss 0.379062, acc 0.875, learning_rate 0.000101466
2017-09-29T11:02:49.175447: step 1986, loss 0.359922, acc 0.875, learning_rate 0.00010146
2017-09-29T11:02:49.359606: step 1987, loss 0.296662, acc 0.921875, learning_rate 0.000101454
2017-09-29T11:02:49.542395: step 1988, loss 0.277397, acc 0.90625, learning_rate 0.000101448
2017-09-29T11:02:49.724152: step 1989, loss 0.200682, acc 0.921875, learning_rate 0.000101442
2017-09-29T11:02:49.904603: step 1990, loss 0.197338, acc 0.953125, learning_rate 0.000101436
2017-09-29T11:02:50.096714: step 1991, loss 0.238266, acc 0.921875, learning_rate 0.00010143
2017-09-29T11:02:50.282161: step 1992, loss 0.251388, acc 0.921875, learning_rate 0.000101424
2017-09-29T11:02:50.468342: step 1993, loss 0.373818, acc 0.859375, learning_rate 0.000101418
2017-09-29T11:02:50.664579: step 1994, loss 0.285284, acc 0.9375, learning_rate 0.000101413
2017-09-29T11:02:50.873266: step 1995, loss 0.332805, acc 0.890625, learning_rate 0.000101407
2017-09-29T11:02:51.064459: step 1996, loss 0.276995, acc 0.90625, learning_rate 0.000101401
2017-09-29T11:02:51.249298: step 1997, loss 0.320157, acc 0.890625, learning_rate 0.000101395
2017-09-29T11:02:51.436498: step 1998, loss 0.424213, acc 0.828125, learning_rate 0.00010139
2017-09-29T11:02:51.620110: step 1999, loss 0.245674, acc 0.875, learning_rate 0.000101384
2017-09-29T11:02:51.805331: step 2000, loss 0.265255, acc 0.921875, learning_rate 0.000101378

Evaluation:
2017-09-29T11:02:52.355602: step 2000, loss 0.334143, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2000

2017-09-29T11:02:52.984637: step 2001, loss 0.189349, acc 0.9375, learning_rate 0.000101373
2017-09-29T11:02:53.165536: step 2002, loss 0.358962, acc 0.84375, learning_rate 0.000101367
2017-09-29T11:02:53.347430: step 2003, loss 0.265009, acc 0.90625, learning_rate 0.000101362
2017-09-29T11:02:53.550565: step 2004, loss 0.350593, acc 0.875, learning_rate 0.000101356
2017-09-29T11:02:53.749686: step 2005, loss 0.299835, acc 0.90625, learning_rate 0.00010135
2017-09-29T11:02:53.934669: step 2006, loss 0.26844, acc 0.875, learning_rate 0.000101345
2017-09-29T11:02:54.124264: step 2007, loss 0.204393, acc 0.9375, learning_rate 0.000101339
2017-09-29T11:02:54.305153: step 2008, loss 0.289752, acc 0.875, learning_rate 0.000101334
2017-09-29T11:02:54.489189: step 2009, loss 0.193807, acc 0.9375, learning_rate 0.000101328
2017-09-29T11:02:54.675264: step 2010, loss 0.322102, acc 0.84375, learning_rate 0.000101323
2017-09-29T11:02:54.855676: step 2011, loss 0.166171, acc 0.9375, learning_rate 0.000101318
2017-09-29T11:02:55.039047: step 2012, loss 0.218714, acc 0.90625, learning_rate 0.000101312
2017-09-29T11:02:55.231217: step 2013, loss 0.407304, acc 0.828125, learning_rate 0.000101307
2017-09-29T11:02:55.412558: step 2014, loss 0.228328, acc 0.953125, learning_rate 0.000101302
2017-09-29T11:02:55.608187: step 2015, loss 0.344121, acc 0.875, learning_rate 0.000101296
2017-09-29T11:02:55.796001: step 2016, loss 0.192833, acc 0.9375, learning_rate 0.000101291
2017-09-29T11:02:55.978602: step 2017, loss 0.253571, acc 0.90625, learning_rate 0.000101286
2017-09-29T11:02:56.171057: step 2018, loss 0.355914, acc 0.90625, learning_rate 0.00010128
2017-09-29T11:02:56.356765: step 2019, loss 0.462975, acc 0.75, learning_rate 0.000101275
2017-09-29T11:02:56.542954: step 2020, loss 0.254565, acc 0.875, learning_rate 0.00010127
2017-09-29T11:02:56.726744: step 2021, loss 0.40577, acc 0.875, learning_rate 0.000101265
2017-09-29T11:02:56.918404: step 2022, loss 0.38193, acc 0.859375, learning_rate 0.00010126
2017-09-29T11:02:57.107033: step 2023, loss 0.517502, acc 0.8125, learning_rate 0.000101255
2017-09-29T11:02:57.291409: step 2024, loss 0.228496, acc 0.9375, learning_rate 0.000101249
2017-09-29T11:02:57.479461: step 2025, loss 0.25621, acc 0.9375, learning_rate 0.000101244
2017-09-29T11:02:57.663848: step 2026, loss 0.508629, acc 0.875, learning_rate 0.000101239
2017-09-29T11:02:57.844557: step 2027, loss 0.300368, acc 0.890625, learning_rate 0.000101234
2017-09-29T11:02:58.032503: step 2028, loss 0.387713, acc 0.8125, learning_rate 0.000101229
2017-09-29T11:02:58.223685: step 2029, loss 0.315213, acc 0.875, learning_rate 0.000101224
2017-09-29T11:02:58.407151: step 2030, loss 0.225489, acc 0.90625, learning_rate 0.000101219
2017-09-29T11:02:58.597733: step 2031, loss 0.249358, acc 0.9375, learning_rate 0.000101214
2017-09-29T11:02:58.789943: step 2032, loss 0.204284, acc 0.921875, learning_rate 0.000101209
2017-09-29T11:02:58.971629: step 2033, loss 0.279929, acc 0.875, learning_rate 0.000101204
2017-09-29T11:02:59.161905: step 2034, loss 0.370092, acc 0.875, learning_rate 0.000101199
2017-09-29T11:02:59.343391: step 2035, loss 0.23468, acc 0.890625, learning_rate 0.000101194
2017-09-29T11:02:59.545664: step 2036, loss 0.238383, acc 0.890625, learning_rate 0.00010119
2017-09-29T11:02:59.730107: step 2037, loss 0.225424, acc 0.921875, learning_rate 0.000101185
2017-09-29T11:02:59.921190: step 2038, loss 0.219281, acc 0.9375, learning_rate 0.00010118
2017-09-29T11:03:00.112069: step 2039, loss 0.264913, acc 0.90625, learning_rate 0.000101175
2017-09-29T11:03:00.293137: step 2040, loss 0.336551, acc 0.890625, learning_rate 0.00010117

Evaluation:
2017-09-29T11:03:00.834934: step 2040, loss 0.336993, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2040

2017-09-29T11:03:01.568327: step 2041, loss 0.458882, acc 0.828125, learning_rate 0.000101166
2017-09-29T11:03:01.751879: step 2042, loss 0.225523, acc 0.9375, learning_rate 0.000101161
2017-09-29T11:03:01.934768: step 2043, loss 0.264469, acc 0.90625, learning_rate 0.000101156
2017-09-29T11:03:02.118963: step 2044, loss 0.280086, acc 0.90625, learning_rate 0.000101151
2017-09-29T11:03:02.312211: step 2045, loss 0.39167, acc 0.859375, learning_rate 0.000101147
2017-09-29T11:03:02.498590: step 2046, loss 0.399904, acc 0.875, learning_rate 0.000101142
2017-09-29T11:03:02.682407: step 2047, loss 0.487608, acc 0.8125, learning_rate 0.000101137
2017-09-29T11:03:02.866368: step 2048, loss 0.349815, acc 0.84375, learning_rate 0.000101133
2017-09-29T11:03:03.056075: step 2049, loss 0.2752, acc 0.890625, learning_rate 0.000101128
2017-09-29T11:03:03.248644: step 2050, loss 0.295079, acc 0.90625, learning_rate 0.000101123
2017-09-29T11:03:03.432459: step 2051, loss 0.327258, acc 0.890625, learning_rate 0.000101119
2017-09-29T11:03:03.631155: step 2052, loss 0.25518, acc 0.9375, learning_rate 0.000101114
2017-09-29T11:03:03.820367: step 2053, loss 0.236314, acc 0.9375, learning_rate 0.00010111
2017-09-29T11:03:04.001342: step 2054, loss 0.301856, acc 0.890625, learning_rate 0.000101105
2017-09-29T11:03:04.193579: step 2055, loss 0.359171, acc 0.921875, learning_rate 0.000101101
2017-09-29T11:03:04.376912: step 2056, loss 0.442059, acc 0.828125, learning_rate 0.000101096
2017-09-29T11:03:04.571241: step 2057, loss 0.31058, acc 0.875, learning_rate 0.000101092
2017-09-29T11:03:04.722152: step 2058, loss 0.252272, acc 0.960784, learning_rate 0.000101087
2017-09-29T11:03:04.911218: step 2059, loss 0.248318, acc 0.921875, learning_rate 0.000101083
2017-09-29T11:03:05.099159: step 2060, loss 0.195842, acc 0.921875, learning_rate 0.000101078
2017-09-29T11:03:05.280036: step 2061, loss 0.433913, acc 0.84375, learning_rate 0.000101074
2017-09-29T11:03:05.464837: step 2062, loss 0.348116, acc 0.84375, learning_rate 0.00010107
2017-09-29T11:03:05.657846: step 2063, loss 0.302593, acc 0.859375, learning_rate 0.000101065
2017-09-29T11:03:05.848219: step 2064, loss 0.293559, acc 0.921875, learning_rate 0.000101061
2017-09-29T11:03:06.032319: step 2065, loss 0.221486, acc 0.9375, learning_rate 0.000101057
2017-09-29T11:03:06.213396: step 2066, loss 0.158239, acc 0.921875, learning_rate 0.000101052
2017-09-29T11:03:06.411337: step 2067, loss 0.236978, acc 0.890625, learning_rate 0.000101048
2017-09-29T11:03:06.598673: step 2068, loss 0.26739, acc 0.9375, learning_rate 0.000101044
2017-09-29T11:03:06.784744: step 2069, loss 0.237793, acc 0.90625, learning_rate 0.000101039
2017-09-29T11:03:06.979948: step 2070, loss 0.186607, acc 0.96875, learning_rate 0.000101035
2017-09-29T11:03:07.166868: step 2071, loss 0.157337, acc 0.9375, learning_rate 0.000101031
2017-09-29T11:03:07.349878: step 2072, loss 0.427877, acc 0.828125, learning_rate 0.000101027
2017-09-29T11:03:07.533182: step 2073, loss 0.383951, acc 0.859375, learning_rate 0.000101023
2017-09-29T11:03:07.716075: step 2074, loss 0.5247, acc 0.8125, learning_rate 0.000101018
2017-09-29T11:03:07.904281: step 2075, loss 0.414955, acc 0.828125, learning_rate 0.000101014
2017-09-29T11:03:08.086100: step 2076, loss 0.254133, acc 0.90625, learning_rate 0.00010101
2017-09-29T11:03:08.271848: step 2077, loss 0.324462, acc 0.875, learning_rate 0.000101006
2017-09-29T11:03:08.453734: step 2078, loss 0.269878, acc 0.90625, learning_rate 0.000101002
2017-09-29T11:03:08.639517: step 2079, loss 0.31015, acc 0.875, learning_rate 0.000100998
2017-09-29T11:03:08.825250: step 2080, loss 0.332423, acc 0.859375, learning_rate 0.000100994

Evaluation:
2017-09-29T11:03:09.365766: step 2080, loss 0.333391, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2080

2017-09-29T11:03:10.072401: step 2081, loss 0.260527, acc 0.890625, learning_rate 0.00010099
2017-09-29T11:03:10.250477: step 2082, loss 0.194624, acc 0.9375, learning_rate 0.000100986
2017-09-29T11:03:10.442027: step 2083, loss 0.35634, acc 0.875, learning_rate 0.000100982
2017-09-29T11:03:10.633222: step 2084, loss 0.269819, acc 0.90625, learning_rate 0.000100978
2017-09-29T11:03:10.818308: step 2085, loss 0.242314, acc 0.921875, learning_rate 0.000100974
2017-09-29T11:03:11.001794: step 2086, loss 0.303348, acc 0.890625, learning_rate 0.00010097
2017-09-29T11:03:11.183790: step 2087, loss 0.36193, acc 0.875, learning_rate 0.000100966
2017-09-29T11:03:11.367589: step 2088, loss 0.321191, acc 0.859375, learning_rate 0.000100962
2017-09-29T11:03:11.560110: step 2089, loss 0.342463, acc 0.90625, learning_rate 0.000100958
2017-09-29T11:03:11.746184: step 2090, loss 0.289862, acc 0.890625, learning_rate 0.000100954
2017-09-29T11:03:11.925428: step 2091, loss 0.415466, acc 0.875, learning_rate 0.00010095
2017-09-29T11:03:12.107681: step 2092, loss 0.1973, acc 0.9375, learning_rate 0.000100946
2017-09-29T11:03:12.292640: step 2093, loss 0.424971, acc 0.828125, learning_rate 0.000100942
2017-09-29T11:03:12.484672: step 2094, loss 0.222134, acc 0.921875, learning_rate 0.000100938
2017-09-29T11:03:12.666237: step 2095, loss 0.271358, acc 0.890625, learning_rate 0.000100935
2017-09-29T11:03:12.849474: step 2096, loss 0.272942, acc 0.890625, learning_rate 0.000100931
2017-09-29T11:03:13.032722: step 2097, loss 0.227864, acc 0.953125, learning_rate 0.000100927
2017-09-29T11:03:13.216978: step 2098, loss 0.423566, acc 0.828125, learning_rate 0.000100923
2017-09-29T11:03:13.398182: step 2099, loss 0.308839, acc 0.875, learning_rate 0.000100919
2017-09-29T11:03:13.579601: step 2100, loss 0.191614, acc 0.90625, learning_rate 0.000100916
2017-09-29T11:03:13.764862: step 2101, loss 0.297458, acc 0.890625, learning_rate 0.000100912
2017-09-29T11:03:13.952327: step 2102, loss 0.234306, acc 0.90625, learning_rate 0.000100908
2017-09-29T11:03:14.135152: step 2103, loss 0.283536, acc 0.90625, learning_rate 0.000100904
2017-09-29T11:03:14.330281: step 2104, loss 0.421375, acc 0.859375, learning_rate 0.000100901
2017-09-29T11:03:14.514636: step 2105, loss 0.199506, acc 0.9375, learning_rate 0.000100897
2017-09-29T11:03:14.696954: step 2106, loss 0.243528, acc 0.921875, learning_rate 0.000100893
2017-09-29T11:03:14.879615: step 2107, loss 0.182531, acc 0.9375, learning_rate 0.00010089
2017-09-29T11:03:15.061988: step 2108, loss 0.284541, acc 0.90625, learning_rate 0.000100886
2017-09-29T11:03:15.246361: step 2109, loss 0.30797, acc 0.921875, learning_rate 0.000100883
2017-09-29T11:03:15.427280: step 2110, loss 0.314337, acc 0.890625, learning_rate 0.000100879
2017-09-29T11:03:15.610250: step 2111, loss 0.229449, acc 0.890625, learning_rate 0.000100875
2017-09-29T11:03:15.792570: step 2112, loss 0.215518, acc 0.921875, learning_rate 0.000100872
2017-09-29T11:03:15.975711: step 2113, loss 0.388377, acc 0.859375, learning_rate 0.000100868
2017-09-29T11:03:16.158303: step 2114, loss 0.334243, acc 0.859375, learning_rate 0.000100865
2017-09-29T11:03:16.340934: step 2115, loss 0.252558, acc 0.890625, learning_rate 0.000100861
2017-09-29T11:03:16.546787: step 2116, loss 0.32749, acc 0.84375, learning_rate 0.000100858
2017-09-29T11:03:16.747505: step 2117, loss 0.258906, acc 0.921875, learning_rate 0.000100854
2017-09-29T11:03:16.936710: step 2118, loss 0.22347, acc 0.9375, learning_rate 0.000100851
2017-09-29T11:03:17.123514: step 2119, loss 0.450392, acc 0.890625, learning_rate 0.000100847
2017-09-29T11:03:17.307293: step 2120, loss 0.274111, acc 0.90625, learning_rate 0.000100844

Evaluation:
2017-09-29T11:03:17.867057: step 2120, loss 0.34009, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2120

2017-09-29T11:03:18.668363: step 2121, loss 0.163855, acc 0.953125, learning_rate 0.00010084
2017-09-29T11:03:18.856886: step 2122, loss 0.277574, acc 0.921875, learning_rate 0.000100837
2017-09-29T11:03:19.037981: step 2123, loss 0.336475, acc 0.875, learning_rate 0.000100833
2017-09-29T11:03:19.220877: step 2124, loss 0.169088, acc 0.96875, learning_rate 0.00010083
2017-09-29T11:03:19.415533: step 2125, loss 0.31181, acc 0.921875, learning_rate 0.000100827
2017-09-29T11:03:19.595753: step 2126, loss 0.410108, acc 0.828125, learning_rate 0.000100823
2017-09-29T11:03:19.777603: step 2127, loss 0.347405, acc 0.875, learning_rate 0.00010082
2017-09-29T11:03:19.958991: step 2128, loss 0.166921, acc 0.96875, learning_rate 0.000100817
2017-09-29T11:03:20.139842: step 2129, loss 0.226648, acc 0.890625, learning_rate 0.000100813
2017-09-29T11:03:20.324635: step 2130, loss 0.317801, acc 0.90625, learning_rate 0.00010081
2017-09-29T11:03:20.509822: step 2131, loss 0.37303, acc 0.875, learning_rate 0.000100807
2017-09-29T11:03:20.693174: step 2132, loss 0.28219, acc 0.890625, learning_rate 0.000100803
2017-09-29T11:03:20.875977: step 2133, loss 0.450929, acc 0.875, learning_rate 0.0001008
2017-09-29T11:03:21.059932: step 2134, loss 0.278233, acc 0.890625, learning_rate 0.000100797
2017-09-29T11:03:21.242061: step 2135, loss 0.22717, acc 0.90625, learning_rate 0.000100793
2017-09-29T11:03:21.427981: step 2136, loss 0.317092, acc 0.875, learning_rate 0.00010079
2017-09-29T11:03:21.608378: step 2137, loss 0.311864, acc 0.890625, learning_rate 0.000100787
2017-09-29T11:03:21.792462: step 2138, loss 0.286267, acc 0.890625, learning_rate 0.000100784
2017-09-29T11:03:21.974853: step 2139, loss 0.39263, acc 0.8125, learning_rate 0.000100781
2017-09-29T11:03:22.161261: step 2140, loss 0.296898, acc 0.90625, learning_rate 0.000100777
2017-09-29T11:03:22.342461: step 2141, loss 0.300092, acc 0.9375, learning_rate 0.000100774
2017-09-29T11:03:22.538392: step 2142, loss 0.309486, acc 0.890625, learning_rate 0.000100771
2017-09-29T11:03:22.720689: step 2143, loss 0.239877, acc 0.921875, learning_rate 0.000100768
2017-09-29T11:03:22.903902: step 2144, loss 0.317622, acc 0.90625, learning_rate 0.000100765
2017-09-29T11:03:23.089420: step 2145, loss 0.32566, acc 0.90625, learning_rate 0.000100762
2017-09-29T11:03:23.268333: step 2146, loss 0.351467, acc 0.875, learning_rate 0.000100759
2017-09-29T11:03:23.452898: step 2147, loss 0.263435, acc 0.90625, learning_rate 0.000100755
2017-09-29T11:03:23.634431: step 2148, loss 0.344994, acc 0.84375, learning_rate 0.000100752
2017-09-29T11:03:23.819336: step 2149, loss 0.271759, acc 0.90625, learning_rate 0.000100749
2017-09-29T11:03:24.004107: step 2150, loss 0.363125, acc 0.859375, learning_rate 0.000100746
2017-09-29T11:03:24.195614: step 2151, loss 0.311873, acc 0.875, learning_rate 0.000100743
2017-09-29T11:03:24.390733: step 2152, loss 0.230643, acc 0.90625, learning_rate 0.00010074
2017-09-29T11:03:24.591270: step 2153, loss 0.438593, acc 0.828125, learning_rate 0.000100737
2017-09-29T11:03:24.780744: step 2154, loss 0.314776, acc 0.8125, learning_rate 0.000100734
2017-09-29T11:03:24.960118: step 2155, loss 0.303433, acc 0.890625, learning_rate 0.000100731
2017-09-29T11:03:25.112595: step 2156, loss 0.254761, acc 0.941176, learning_rate 0.000100728
2017-09-29T11:03:25.305621: step 2157, loss 0.269683, acc 0.9375, learning_rate 0.000100725
2017-09-29T11:03:25.491523: step 2158, loss 0.235151, acc 0.90625, learning_rate 0.000100722
2017-09-29T11:03:25.675469: step 2159, loss 0.361505, acc 0.875, learning_rate 0.000100719
2017-09-29T11:03:25.862386: step 2160, loss 0.261455, acc 0.875, learning_rate 0.000100716

Evaluation:
2017-09-29T11:03:26.408710: step 2160, loss 0.332464, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2160

2017-09-29T11:03:27.040549: step 2161, loss 0.237219, acc 0.90625, learning_rate 0.000100713
2017-09-29T11:03:27.224145: step 2162, loss 0.354559, acc 0.875, learning_rate 0.000100711
2017-09-29T11:03:27.405091: step 2163, loss 0.262534, acc 0.890625, learning_rate 0.000100708
2017-09-29T11:03:27.587884: step 2164, loss 0.260841, acc 0.890625, learning_rate 0.000100705
2017-09-29T11:03:27.769585: step 2165, loss 0.267021, acc 0.90625, learning_rate 0.000100702
2017-09-29T11:03:27.952612: step 2166, loss 0.34256, acc 0.859375, learning_rate 0.000100699
2017-09-29T11:03:28.137370: step 2167, loss 0.332733, acc 0.859375, learning_rate 0.000100696
2017-09-29T11:03:28.318554: step 2168, loss 0.399543, acc 0.84375, learning_rate 0.000100693
2017-09-29T11:03:28.509762: step 2169, loss 0.231897, acc 0.9375, learning_rate 0.00010069
2017-09-29T11:03:28.700216: step 2170, loss 0.298609, acc 0.921875, learning_rate 0.000100688
2017-09-29T11:03:28.900390: step 2171, loss 0.246296, acc 0.9375, learning_rate 0.000100685
2017-09-29T11:03:29.087875: step 2172, loss 0.375626, acc 0.890625, learning_rate 0.000100682
2017-09-29T11:03:29.272312: step 2173, loss 0.258826, acc 0.90625, learning_rate 0.000100679
2017-09-29T11:03:29.462583: step 2174, loss 0.418719, acc 0.859375, learning_rate 0.000100677
2017-09-29T11:03:29.643838: step 2175, loss 0.256907, acc 0.90625, learning_rate 0.000100674
2017-09-29T11:03:29.826508: step 2176, loss 0.252681, acc 0.890625, learning_rate 0.000100671
2017-09-29T11:03:30.010545: step 2177, loss 0.416092, acc 0.828125, learning_rate 0.000100668
2017-09-29T11:03:30.192221: step 2178, loss 0.369919, acc 0.875, learning_rate 0.000100666
2017-09-29T11:03:30.375994: step 2179, loss 0.302841, acc 0.859375, learning_rate 0.000100663
2017-09-29T11:03:30.560372: step 2180, loss 0.245366, acc 0.90625, learning_rate 0.00010066
2017-09-29T11:03:30.743586: step 2181, loss 0.283472, acc 0.84375, learning_rate 0.000100657
2017-09-29T11:03:30.926443: step 2182, loss 0.262834, acc 0.890625, learning_rate 0.000100655
2017-09-29T11:03:31.110772: step 2183, loss 0.256096, acc 0.9375, learning_rate 0.000100652
2017-09-29T11:03:31.297152: step 2184, loss 0.330826, acc 0.875, learning_rate 0.000100649
2017-09-29T11:03:31.483212: step 2185, loss 0.244363, acc 0.921875, learning_rate 0.000100647
2017-09-29T11:03:31.664148: step 2186, loss 0.396184, acc 0.875, learning_rate 0.000100644
2017-09-29T11:03:31.847565: step 2187, loss 0.220376, acc 0.90625, learning_rate 0.000100641
2017-09-29T11:03:32.031225: step 2188, loss 0.271035, acc 0.90625, learning_rate 0.000100639
2017-09-29T11:03:32.213847: step 2189, loss 0.53196, acc 0.796875, learning_rate 0.000100636
2017-09-29T11:03:32.401221: step 2190, loss 0.345393, acc 0.859375, learning_rate 0.000100634
2017-09-29T11:03:32.583206: step 2191, loss 0.518405, acc 0.84375, learning_rate 0.000100631
2017-09-29T11:03:32.769824: step 2192, loss 0.32077, acc 0.890625, learning_rate 0.000100628
2017-09-29T11:03:32.962495: step 2193, loss 0.235283, acc 0.90625, learning_rate 0.000100626
2017-09-29T11:03:33.159455: step 2194, loss 0.208077, acc 0.9375, learning_rate 0.000100623
2017-09-29T11:03:33.345233: step 2195, loss 0.259257, acc 0.84375, learning_rate 0.000100621
2017-09-29T11:03:33.541752: step 2196, loss 0.235504, acc 0.921875, learning_rate 0.000100618
2017-09-29T11:03:33.723353: step 2197, loss 0.145782, acc 0.953125, learning_rate 0.000100616
2017-09-29T11:03:33.907577: step 2198, loss 0.231834, acc 0.90625, learning_rate 0.000100613
2017-09-29T11:03:34.091480: step 2199, loss 0.366117, acc 0.859375, learning_rate 0.000100611
2017-09-29T11:03:34.275026: step 2200, loss 0.289678, acc 0.875, learning_rate 0.000100608

Evaluation:
2017-09-29T11:03:34.831397: step 2200, loss 0.336153, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2200

2017-09-29T11:03:35.532841: step 2201, loss 0.264397, acc 0.921875, learning_rate 0.000100606
2017-09-29T11:03:35.721453: step 2202, loss 0.342171, acc 0.84375, learning_rate 0.000100603
2017-09-29T11:03:35.904879: step 2203, loss 0.279165, acc 0.875, learning_rate 0.000100601
2017-09-29T11:03:36.088570: step 2204, loss 0.32451, acc 0.921875, learning_rate 0.000100598
2017-09-29T11:03:36.271815: step 2205, loss 0.19163, acc 0.96875, learning_rate 0.000100596
2017-09-29T11:03:36.462462: step 2206, loss 0.27005, acc 0.890625, learning_rate 0.000100594
2017-09-29T11:03:36.659565: step 2207, loss 0.339942, acc 0.921875, learning_rate 0.000100591
2017-09-29T11:03:36.843484: step 2208, loss 0.40847, acc 0.8125, learning_rate 0.000100589
2017-09-29T11:03:37.025029: step 2209, loss 0.221875, acc 0.921875, learning_rate 0.000100586
2017-09-29T11:03:37.220605: step 2210, loss 0.202652, acc 0.9375, learning_rate 0.000100584
2017-09-29T11:03:37.402902: step 2211, loss 0.230843, acc 0.921875, learning_rate 0.000100581
2017-09-29T11:03:37.585816: step 2212, loss 0.301496, acc 0.890625, learning_rate 0.000100579
2017-09-29T11:03:37.766092: step 2213, loss 0.316251, acc 0.890625, learning_rate 0.000100577
2017-09-29T11:03:37.949963: step 2214, loss 0.21896, acc 0.96875, learning_rate 0.000100574
2017-09-29T11:03:38.138189: step 2215, loss 0.279635, acc 0.890625, learning_rate 0.000100572
2017-09-29T11:03:38.319810: step 2216, loss 0.287228, acc 0.90625, learning_rate 0.00010057
2017-09-29T11:03:38.503375: step 2217, loss 0.169462, acc 0.953125, learning_rate 0.000100567
2017-09-29T11:03:38.685040: step 2218, loss 0.38583, acc 0.90625, learning_rate 0.000100565
2017-09-29T11:03:38.867642: step 2219, loss 0.461014, acc 0.8125, learning_rate 0.000100563
2017-09-29T11:03:39.054499: step 2220, loss 0.329498, acc 0.859375, learning_rate 0.00010056
2017-09-29T11:03:39.246131: step 2221, loss 0.194162, acc 0.953125, learning_rate 0.000100558
2017-09-29T11:03:39.443302: step 2222, loss 0.35593, acc 0.90625, learning_rate 0.000100556
2017-09-29T11:03:39.627719: step 2223, loss 0.263225, acc 0.90625, learning_rate 0.000100554
2017-09-29T11:03:39.811744: step 2224, loss 0.348413, acc 0.859375, learning_rate 0.000100551
2017-09-29T11:03:40.010788: step 2225, loss 0.251671, acc 0.90625, learning_rate 0.000100549
2017-09-29T11:03:40.217708: step 2226, loss 0.211129, acc 0.9375, learning_rate 0.000100547
2017-09-29T11:03:40.400579: step 2227, loss 0.337049, acc 0.875, learning_rate 0.000100545
2017-09-29T11:03:40.580445: step 2228, loss 0.27733, acc 0.90625, learning_rate 0.000100542
2017-09-29T11:03:40.764342: step 2229, loss 0.233398, acc 0.9375, learning_rate 0.00010054
2017-09-29T11:03:40.949013: step 2230, loss 0.242244, acc 0.90625, learning_rate 0.000100538
2017-09-29T11:03:41.132176: step 2231, loss 0.205084, acc 0.9375, learning_rate 0.000100536
2017-09-29T11:03:41.313724: step 2232, loss 0.258786, acc 0.875, learning_rate 0.000100534
2017-09-29T11:03:41.500570: step 2233, loss 0.368963, acc 0.890625, learning_rate 0.000100531
2017-09-29T11:03:41.701768: step 2234, loss 0.388405, acc 0.875, learning_rate 0.000100529
2017-09-29T11:03:41.888634: step 2235, loss 0.393667, acc 0.828125, learning_rate 0.000100527
2017-09-29T11:03:42.072018: step 2236, loss 0.276749, acc 0.875, learning_rate 0.000100525
2017-09-29T11:03:42.256948: step 2237, loss 0.155078, acc 0.9375, learning_rate 0.000100523
2017-09-29T11:03:42.439252: step 2238, loss 0.382716, acc 0.875, learning_rate 0.000100521
2017-09-29T11:03:42.623075: step 2239, loss 0.34627, acc 0.875, learning_rate 0.000100519
2017-09-29T11:03:42.803875: step 2240, loss 0.333185, acc 0.875, learning_rate 0.000100516

Evaluation:
2017-09-29T11:03:43.370214: step 2240, loss 0.330515, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2240

2017-09-29T11:03:44.081439: step 2241, loss 0.248574, acc 0.90625, learning_rate 0.000100514
2017-09-29T11:03:44.260784: step 2242, loss 0.391478, acc 0.859375, learning_rate 0.000100512
2017-09-29T11:03:44.461445: step 2243, loss 0.261132, acc 0.921875, learning_rate 0.00010051
2017-09-29T11:03:44.646189: step 2244, loss 0.489839, acc 0.859375, learning_rate 0.000100508
2017-09-29T11:03:44.827418: step 2245, loss 0.241441, acc 0.9375, learning_rate 0.000100506
2017-09-29T11:03:45.022205: step 2246, loss 0.194491, acc 0.96875, learning_rate 0.000100504
2017-09-29T11:03:45.204443: step 2247, loss 0.291032, acc 0.921875, learning_rate 0.000100502
2017-09-29T11:03:45.395868: step 2248, loss 0.155527, acc 0.96875, learning_rate 0.0001005
2017-09-29T11:03:45.584671: step 2249, loss 0.316141, acc 0.875, learning_rate 0.000100498
2017-09-29T11:03:45.767029: step 2250, loss 0.288154, acc 0.875, learning_rate 0.000100496
2017-09-29T11:03:45.950413: step 2251, loss 0.319058, acc 0.890625, learning_rate 0.000100494
2017-09-29T11:03:46.134472: step 2252, loss 0.262161, acc 0.90625, learning_rate 0.000100492
2017-09-29T11:03:46.317203: step 2253, loss 0.263875, acc 0.90625, learning_rate 0.00010049
2017-09-29T11:03:46.469457: step 2254, loss 0.446857, acc 0.803922, learning_rate 0.000100488
2017-09-29T11:03:46.660405: step 2255, loss 0.248907, acc 0.9375, learning_rate 0.000100486
2017-09-29T11:03:46.865072: step 2256, loss 0.311925, acc 0.859375, learning_rate 0.000100484
2017-09-29T11:03:47.066990: step 2257, loss 0.23328, acc 0.953125, learning_rate 0.000100482
2017-09-29T11:03:47.270094: step 2258, loss 0.136023, acc 0.9375, learning_rate 0.00010048
2017-09-29T11:03:47.468900: step 2259, loss 0.206596, acc 0.9375, learning_rate 0.000100478
2017-09-29T11:03:47.651879: step 2260, loss 0.256708, acc 0.90625, learning_rate 0.000100476
2017-09-29T11:03:47.832789: step 2261, loss 0.295189, acc 0.90625, learning_rate 0.000100474
2017-09-29T11:03:48.016490: step 2262, loss 0.219471, acc 0.953125, learning_rate 0.000100472
2017-09-29T11:03:48.199203: step 2263, loss 0.366571, acc 0.875, learning_rate 0.00010047
2017-09-29T11:03:48.382072: step 2264, loss 0.371008, acc 0.828125, learning_rate 0.000100468
2017-09-29T11:03:48.563654: step 2265, loss 0.183354, acc 0.90625, learning_rate 0.000100466
2017-09-29T11:03:48.753103: step 2266, loss 0.185, acc 0.953125, learning_rate 0.000100464
2017-09-29T11:03:48.940794: step 2267, loss 0.259636, acc 0.90625, learning_rate 0.000100462
2017-09-29T11:03:49.126671: step 2268, loss 0.211851, acc 0.921875, learning_rate 0.000100461
2017-09-29T11:03:49.307622: step 2269, loss 0.199538, acc 0.921875, learning_rate 0.000100459
2017-09-29T11:03:49.504306: step 2270, loss 0.38549, acc 0.828125, learning_rate 0.000100457
2017-09-29T11:03:49.690190: step 2271, loss 0.18144, acc 0.9375, learning_rate 0.000100455
2017-09-29T11:03:49.870275: step 2272, loss 0.336294, acc 0.859375, learning_rate 0.000100453
2017-09-29T11:03:50.049922: step 2273, loss 0.12494, acc 0.984375, learning_rate 0.000100451
2017-09-29T11:03:50.233886: step 2274, loss 0.186865, acc 0.921875, learning_rate 0.000100449
2017-09-29T11:03:50.417497: step 2275, loss 0.307289, acc 0.90625, learning_rate 0.000100448
2017-09-29T11:03:50.601809: step 2276, loss 0.227018, acc 0.953125, learning_rate 0.000100446
2017-09-29T11:03:50.794877: step 2277, loss 0.403794, acc 0.8125, learning_rate 0.000100444
2017-09-29T11:03:50.975207: step 2278, loss 0.248601, acc 0.875, learning_rate 0.000100442
2017-09-29T11:03:51.159577: step 2279, loss 0.281345, acc 0.890625, learning_rate 0.00010044
2017-09-29T11:03:51.343511: step 2280, loss 0.282547, acc 0.90625, learning_rate 0.000100439

Evaluation:
2017-09-29T11:03:51.898444: step 2280, loss 0.329633, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2280

2017-09-29T11:03:52.705933: step 2281, loss 0.180063, acc 0.953125, learning_rate 0.000100437
2017-09-29T11:03:52.885883: step 2282, loss 0.283091, acc 0.890625, learning_rate 0.000100435
2017-09-29T11:03:53.069661: step 2283, loss 0.272911, acc 0.890625, learning_rate 0.000100433
2017-09-29T11:03:53.253507: step 2284, loss 0.192344, acc 0.9375, learning_rate 0.000100431
2017-09-29T11:03:53.434205: step 2285, loss 0.42312, acc 0.828125, learning_rate 0.00010043
2017-09-29T11:03:53.617845: step 2286, loss 0.176746, acc 0.953125, learning_rate 0.000100428
2017-09-29T11:03:53.795853: step 2287, loss 0.222721, acc 0.9375, learning_rate 0.000100426
2017-09-29T11:03:53.984463: step 2288, loss 0.207433, acc 0.921875, learning_rate 0.000100424
2017-09-29T11:03:54.172658: step 2289, loss 0.433136, acc 0.890625, learning_rate 0.000100423
2017-09-29T11:03:54.355373: step 2290, loss 0.333221, acc 0.859375, learning_rate 0.000100421
2017-09-29T11:03:54.549672: step 2291, loss 0.238411, acc 0.921875, learning_rate 0.000100419
2017-09-29T11:03:54.746445: step 2292, loss 0.285132, acc 0.890625, learning_rate 0.000100418
2017-09-29T11:03:54.930608: step 2293, loss 0.15781, acc 0.9375, learning_rate 0.000100416
2017-09-29T11:03:55.114417: step 2294, loss 0.427881, acc 0.84375, learning_rate 0.000100414
2017-09-29T11:03:55.295842: step 2295, loss 0.218947, acc 0.90625, learning_rate 0.000100412
2017-09-29T11:03:55.485018: step 2296, loss 0.352433, acc 0.859375, learning_rate 0.000100411
2017-09-29T11:03:55.669245: step 2297, loss 0.155241, acc 0.953125, learning_rate 0.000100409
2017-09-29T11:03:55.851886: step 2298, loss 0.338445, acc 0.890625, learning_rate 0.000100407
2017-09-29T11:03:56.036722: step 2299, loss 0.301903, acc 0.890625, learning_rate 0.000100406
2017-09-29T11:03:56.230431: step 2300, loss 0.291043, acc 0.90625, learning_rate 0.000100404
2017-09-29T11:03:56.416371: step 2301, loss 0.261939, acc 0.90625, learning_rate 0.000100402
2017-09-29T11:03:56.606254: step 2302, loss 0.385585, acc 0.84375, learning_rate 0.000100401
2017-09-29T11:03:56.791891: step 2303, loss 0.374002, acc 0.84375, learning_rate 0.000100399
2017-09-29T11:03:56.980450: step 2304, loss 0.406057, acc 0.8125, learning_rate 0.000100398
2017-09-29T11:03:57.168695: step 2305, loss 0.401135, acc 0.84375, learning_rate 0.000100396
2017-09-29T11:03:57.356242: step 2306, loss 0.350476, acc 0.890625, learning_rate 0.000100394
2017-09-29T11:03:57.542405: step 2307, loss 0.208823, acc 0.921875, learning_rate 0.000100393
2017-09-29T11:03:57.727818: step 2308, loss 0.286552, acc 0.90625, learning_rate 0.000100391
2017-09-29T11:03:57.913819: step 2309, loss 0.360352, acc 0.84375, learning_rate 0.000100389
2017-09-29T11:03:58.115822: step 2310, loss 0.306279, acc 0.859375, learning_rate 0.000100388
2017-09-29T11:03:58.299067: step 2311, loss 0.285067, acc 0.890625, learning_rate 0.000100386
2017-09-29T11:03:58.494947: step 2312, loss 0.242506, acc 0.90625, learning_rate 0.000100385
2017-09-29T11:03:58.679606: step 2313, loss 0.263786, acc 0.9375, learning_rate 0.000100383
2017-09-29T11:03:58.865566: step 2314, loss 0.395621, acc 0.859375, learning_rate 0.000100382
2017-09-29T11:03:59.052686: step 2315, loss 0.216203, acc 0.90625, learning_rate 0.00010038
2017-09-29T11:03:59.248592: step 2316, loss 0.190071, acc 0.921875, learning_rate 0.000100378
2017-09-29T11:03:59.443063: step 2317, loss 0.296018, acc 0.875, learning_rate 0.000100377
2017-09-29T11:03:59.640294: step 2318, loss 0.239269, acc 0.890625, learning_rate 0.000100375
2017-09-29T11:03:59.824295: step 2319, loss 0.310367, acc 0.890625, learning_rate 0.000100374
2017-09-29T11:04:00.005637: step 2320, loss 0.380976, acc 0.859375, learning_rate 0.000100372

Evaluation:
2017-09-29T11:04:00.555265: step 2320, loss 0.331997, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2320

2017-09-29T11:04:01.184217: step 2321, loss 0.281845, acc 0.890625, learning_rate 0.000100371
2017-09-29T11:04:01.373770: step 2322, loss 0.251682, acc 0.90625, learning_rate 0.000100369
2017-09-29T11:04:01.560083: step 2323, loss 0.228768, acc 0.9375, learning_rate 0.000100368
2017-09-29T11:04:01.749307: step 2324, loss 0.357058, acc 0.875, learning_rate 0.000100366
2017-09-29T11:04:01.938471: step 2325, loss 0.220237, acc 0.921875, learning_rate 0.000100365
2017-09-29T11:04:02.129824: step 2326, loss 0.260161, acc 0.90625, learning_rate 0.000100363
2017-09-29T11:04:02.315591: step 2327, loss 0.322912, acc 0.828125, learning_rate 0.000100362
2017-09-29T11:04:02.513729: step 2328, loss 0.256514, acc 0.90625, learning_rate 0.00010036
2017-09-29T11:04:02.697606: step 2329, loss 0.274964, acc 0.90625, learning_rate 0.000100359
2017-09-29T11:04:02.879938: step 2330, loss 0.328309, acc 0.90625, learning_rate 0.000100357
2017-09-29T11:04:03.064842: step 2331, loss 0.223944, acc 0.921875, learning_rate 0.000100356
2017-09-29T11:04:03.246568: step 2332, loss 0.469562, acc 0.84375, learning_rate 0.000100354
2017-09-29T11:04:03.432759: step 2333, loss 0.379121, acc 0.875, learning_rate 0.000100353
2017-09-29T11:04:03.616418: step 2334, loss 0.332882, acc 0.859375, learning_rate 0.000100352
2017-09-29T11:04:03.803020: step 2335, loss 0.238754, acc 0.953125, learning_rate 0.00010035
2017-09-29T11:04:03.991718: step 2336, loss 0.407438, acc 0.859375, learning_rate 0.000100349
2017-09-29T11:04:04.188943: step 2337, loss 0.308949, acc 0.859375, learning_rate 0.000100347
2017-09-29T11:04:04.375072: step 2338, loss 0.188782, acc 0.9375, learning_rate 0.000100346
2017-09-29T11:04:04.566119: step 2339, loss 0.2274, acc 0.9375, learning_rate 0.000100344
2017-09-29T11:04:04.756391: step 2340, loss 0.335577, acc 0.890625, learning_rate 0.000100343
2017-09-29T11:04:04.945524: step 2341, loss 0.308453, acc 0.90625, learning_rate 0.000100342
2017-09-29T11:04:05.133613: step 2342, loss 0.308239, acc 0.875, learning_rate 0.00010034
2017-09-29T11:04:05.316253: step 2343, loss 0.237131, acc 0.921875, learning_rate 0.000100339
2017-09-29T11:04:05.502265: step 2344, loss 0.452401, acc 0.84375, learning_rate 0.000100338
2017-09-29T11:04:05.684036: step 2345, loss 0.185014, acc 0.921875, learning_rate 0.000100336
2017-09-29T11:04:05.870029: step 2346, loss 0.232952, acc 0.921875, learning_rate 0.000100335
2017-09-29T11:04:06.050507: step 2347, loss 0.427228, acc 0.84375, learning_rate 0.000100333
2017-09-29T11:04:06.234314: step 2348, loss 0.190131, acc 0.953125, learning_rate 0.000100332
2017-09-29T11:04:06.421279: step 2349, loss 0.180244, acc 0.96875, learning_rate 0.000100331
2017-09-29T11:04:06.605491: step 2350, loss 0.300687, acc 0.921875, learning_rate 0.000100329
2017-09-29T11:04:06.789817: step 2351, loss 0.277877, acc 0.890625, learning_rate 0.000100328
2017-09-29T11:04:06.942072: step 2352, loss 0.361761, acc 0.843137, learning_rate 0.000100327
2017-09-29T11:04:07.138955: step 2353, loss 0.30907, acc 0.921875, learning_rate 0.000100325
2017-09-29T11:04:07.322768: step 2354, loss 0.254047, acc 0.921875, learning_rate 0.000100324
2017-09-29T11:04:07.508401: step 2355, loss 0.274382, acc 0.890625, learning_rate 0.000100323
2017-09-29T11:04:07.700797: step 2356, loss 0.380582, acc 0.84375, learning_rate 0.000100321
2017-09-29T11:04:07.883404: step 2357, loss 0.280508, acc 0.875, learning_rate 0.00010032
2017-09-29T11:04:08.065752: step 2358, loss 0.350298, acc 0.84375, learning_rate 0.000100319
2017-09-29T11:04:08.261843: step 2359, loss 0.298609, acc 0.90625, learning_rate 0.000100317
2017-09-29T11:04:08.454957: step 2360, loss 0.311567, acc 0.90625, learning_rate 0.000100316

Evaluation:
2017-09-29T11:04:09.013441: step 2360, loss 0.327762, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2360

2017-09-29T11:04:09.729977: step 2361, loss 0.17168, acc 0.9375, learning_rate 0.000100315
2017-09-29T11:04:09.913305: step 2362, loss 0.314618, acc 0.859375, learning_rate 0.000100314
2017-09-29T11:04:10.097912: step 2363, loss 0.276888, acc 0.90625, learning_rate 0.000100312
2017-09-29T11:04:10.279357: step 2364, loss 0.172441, acc 0.9375, learning_rate 0.000100311
2017-09-29T11:04:10.464031: step 2365, loss 0.268249, acc 0.921875, learning_rate 0.00010031
2017-09-29T11:04:10.650195: step 2366, loss 0.394539, acc 0.875, learning_rate 0.000100308
2017-09-29T11:04:10.834610: step 2367, loss 0.252445, acc 0.90625, learning_rate 0.000100307
2017-09-29T11:04:11.019651: step 2368, loss 0.29562, acc 0.84375, learning_rate 0.000100306
2017-09-29T11:04:11.201847: step 2369, loss 0.281624, acc 0.921875, learning_rate 0.000100305
2017-09-29T11:04:11.388416: step 2370, loss 0.401796, acc 0.875, learning_rate 0.000100303
2017-09-29T11:04:11.575960: step 2371, loss 0.273855, acc 0.921875, learning_rate 0.000100302
2017-09-29T11:04:11.756248: step 2372, loss 0.302263, acc 0.859375, learning_rate 0.000100301
2017-09-29T11:04:11.943264: step 2373, loss 0.314944, acc 0.859375, learning_rate 0.0001003
2017-09-29T11:04:12.126216: step 2374, loss 0.213329, acc 0.90625, learning_rate 0.000100299
2017-09-29T11:04:12.309297: step 2375, loss 0.191693, acc 0.9375, learning_rate 0.000100297
2017-09-29T11:04:12.492220: step 2376, loss 0.244131, acc 0.90625, learning_rate 0.000100296
2017-09-29T11:04:12.673986: step 2377, loss 0.236171, acc 0.921875, learning_rate 0.000100295
2017-09-29T11:04:12.856965: step 2378, loss 0.283401, acc 0.90625, learning_rate 0.000100294
2017-09-29T11:04:13.038572: step 2379, loss 0.215581, acc 0.921875, learning_rate 0.000100292
2017-09-29T11:04:13.226346: step 2380, loss 0.206236, acc 0.921875, learning_rate 0.000100291
2017-09-29T11:04:13.416736: step 2381, loss 0.191873, acc 0.9375, learning_rate 0.00010029
2017-09-29T11:04:13.605379: step 2382, loss 0.248907, acc 0.9375, learning_rate 0.000100289
2017-09-29T11:04:13.786719: step 2383, loss 0.297038, acc 0.90625, learning_rate 0.000100288
2017-09-29T11:04:13.970348: step 2384, loss 0.24532, acc 0.90625, learning_rate 0.000100287
2017-09-29T11:04:14.153955: step 2385, loss 0.372051, acc 0.84375, learning_rate 0.000100285
2017-09-29T11:04:14.341065: step 2386, loss 0.276678, acc 0.875, learning_rate 0.000100284
2017-09-29T11:04:14.533617: step 2387, loss 0.297425, acc 0.859375, learning_rate 0.000100283
2017-09-29T11:04:14.724202: step 2388, loss 0.265042, acc 0.875, learning_rate 0.000100282
2017-09-29T11:04:14.908129: step 2389, loss 0.434883, acc 0.84375, learning_rate 0.000100281
2017-09-29T11:04:15.092007: step 2390, loss 0.276152, acc 0.921875, learning_rate 0.00010028
2017-09-29T11:04:15.277062: step 2391, loss 0.418542, acc 0.859375, learning_rate 0.000100278
2017-09-29T11:04:15.459170: step 2392, loss 0.267274, acc 0.875, learning_rate 0.000100277
2017-09-29T11:04:15.638046: step 2393, loss 0.291855, acc 0.890625, learning_rate 0.000100276
2017-09-29T11:04:15.819488: step 2394, loss 0.41817, acc 0.8125, learning_rate 0.000100275
2017-09-29T11:04:16.012330: step 2395, loss 0.393185, acc 0.875, learning_rate 0.000100274
2017-09-29T11:04:16.197606: step 2396, loss 0.377424, acc 0.84375, learning_rate 0.000100273
2017-09-29T11:04:16.382171: step 2397, loss 0.216104, acc 0.90625, learning_rate 0.000100272
2017-09-29T11:04:16.571901: step 2398, loss 0.362457, acc 0.90625, learning_rate 0.000100271
2017-09-29T11:04:16.766694: step 2399, loss 0.266982, acc 0.90625, learning_rate 0.00010027
2017-09-29T11:04:16.956491: step 2400, loss 0.288817, acc 0.90625, learning_rate 0.000100268

Evaluation:
2017-09-29T11:04:17.517383: step 2400, loss 0.33005, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2400

2017-09-29T11:04:18.214442: step 2401, loss 0.32989, acc 0.859375, learning_rate 0.000100267
2017-09-29T11:04:18.394772: step 2402, loss 0.297566, acc 0.921875, learning_rate 0.000100266
2017-09-29T11:04:18.577597: step 2403, loss 0.369341, acc 0.84375, learning_rate 0.000100265
2017-09-29T11:04:18.760035: step 2404, loss 0.281618, acc 0.890625, learning_rate 0.000100264
2017-09-29T11:04:18.952704: step 2405, loss 0.247624, acc 0.890625, learning_rate 0.000100263
2017-09-29T11:04:19.138564: step 2406, loss 0.257006, acc 0.9375, learning_rate 0.000100262
2017-09-29T11:04:19.320288: step 2407, loss 0.251453, acc 0.890625, learning_rate 0.000100261
2017-09-29T11:04:19.513267: step 2408, loss 0.486502, acc 0.796875, learning_rate 0.00010026
2017-09-29T11:04:19.706965: step 2409, loss 0.274637, acc 0.921875, learning_rate 0.000100259
2017-09-29T11:04:19.890844: step 2410, loss 0.300935, acc 0.890625, learning_rate 0.000100258
2017-09-29T11:04:20.068957: step 2411, loss 0.320506, acc 0.859375, learning_rate 0.000100257
2017-09-29T11:04:20.253906: step 2412, loss 0.279793, acc 0.890625, learning_rate 0.000100256
2017-09-29T11:04:20.438474: step 2413, loss 0.341433, acc 0.859375, learning_rate 0.000100255
2017-09-29T11:04:20.621035: step 2414, loss 0.220616, acc 0.90625, learning_rate 0.000100253
2017-09-29T11:04:20.809060: step 2415, loss 0.373373, acc 0.8125, learning_rate 0.000100252
2017-09-29T11:04:20.988364: step 2416, loss 0.329108, acc 0.890625, learning_rate 0.000100251
2017-09-29T11:04:21.170221: step 2417, loss 0.235801, acc 0.890625, learning_rate 0.00010025
2017-09-29T11:04:21.352869: step 2418, loss 0.325177, acc 0.84375, learning_rate 0.000100249
2017-09-29T11:04:21.538194: step 2419, loss 0.206, acc 0.953125, learning_rate 0.000100248
2017-09-29T11:04:21.723420: step 2420, loss 0.174904, acc 0.9375, learning_rate 0.000100247
2017-09-29T11:04:21.905982: step 2421, loss 0.217263, acc 0.90625, learning_rate 0.000100246
2017-09-29T11:04:22.091255: step 2422, loss 0.374332, acc 0.859375, learning_rate 0.000100245
2017-09-29T11:04:22.278986: step 2423, loss 0.237144, acc 0.921875, learning_rate 0.000100244
2017-09-29T11:04:22.465448: step 2424, loss 0.177072, acc 0.96875, learning_rate 0.000100243
2017-09-29T11:04:22.646545: step 2425, loss 0.256028, acc 0.890625, learning_rate 0.000100242
2017-09-29T11:04:22.824534: step 2426, loss 0.266843, acc 0.890625, learning_rate 0.000100241
2017-09-29T11:04:23.006881: step 2427, loss 0.397815, acc 0.84375, learning_rate 0.00010024
2017-09-29T11:04:23.190442: step 2428, loss 0.248502, acc 0.921875, learning_rate 0.000100239
2017-09-29T11:04:23.373081: step 2429, loss 0.213487, acc 0.921875, learning_rate 0.000100238
2017-09-29T11:04:23.556011: step 2430, loss 0.276523, acc 0.890625, learning_rate 0.000100237
2017-09-29T11:04:23.742884: step 2431, loss 0.221843, acc 0.90625, learning_rate 0.000100236
2017-09-29T11:04:23.927597: step 2432, loss 0.185914, acc 0.953125, learning_rate 0.000100235
2017-09-29T11:04:24.122064: step 2433, loss 0.234497, acc 0.921875, learning_rate 0.000100235
2017-09-29T11:04:24.303762: step 2434, loss 0.321973, acc 0.875, learning_rate 0.000100234
2017-09-29T11:04:24.491506: step 2435, loss 0.155048, acc 0.9375, learning_rate 0.000100233
2017-09-29T11:04:24.675254: step 2436, loss 0.299683, acc 0.875, learning_rate 0.000100232
2017-09-29T11:04:24.859921: step 2437, loss 0.390045, acc 0.828125, learning_rate 0.000100231
2017-09-29T11:04:25.042794: step 2438, loss 0.248498, acc 0.921875, learning_rate 0.00010023
2017-09-29T11:04:25.224395: step 2439, loss 0.32643, acc 0.890625, learning_rate 0.000100229
2017-09-29T11:04:25.415132: step 2440, loss 0.451221, acc 0.84375, learning_rate 0.000100228

Evaluation:
2017-09-29T11:04:25.967442: step 2440, loss 0.328358, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2440

2017-09-29T11:04:26.761155: step 2441, loss 0.240953, acc 0.921875, learning_rate 0.000100227
2017-09-29T11:04:26.939372: step 2442, loss 0.258799, acc 0.9375, learning_rate 0.000100226
2017-09-29T11:04:27.122671: step 2443, loss 0.312858, acc 0.890625, learning_rate 0.000100225
2017-09-29T11:04:27.306277: step 2444, loss 0.299301, acc 0.890625, learning_rate 0.000100224
2017-09-29T11:04:27.493570: step 2445, loss 0.400862, acc 0.859375, learning_rate 0.000100223
2017-09-29T11:04:27.678595: step 2446, loss 0.334431, acc 0.890625, learning_rate 0.000100222
2017-09-29T11:04:27.862029: step 2447, loss 0.38915, acc 0.890625, learning_rate 0.000100221
2017-09-29T11:04:28.044017: step 2448, loss 0.282137, acc 0.921875, learning_rate 0.000100221
2017-09-29T11:04:28.231166: step 2449, loss 0.204928, acc 0.921875, learning_rate 0.00010022
2017-09-29T11:04:28.385478: step 2450, loss 0.254014, acc 0.882353, learning_rate 0.000100219
2017-09-29T11:04:28.570793: step 2451, loss 0.273351, acc 0.90625, learning_rate 0.000100218
2017-09-29T11:04:28.757719: step 2452, loss 0.261794, acc 0.890625, learning_rate 0.000100217
2017-09-29T11:04:28.941470: step 2453, loss 0.319456, acc 0.875, learning_rate 0.000100216
2017-09-29T11:04:29.123100: step 2454, loss 0.179878, acc 0.953125, learning_rate 0.000100215
2017-09-29T11:04:29.305206: step 2455, loss 0.343628, acc 0.84375, learning_rate 0.000100214
2017-09-29T11:04:29.501255: step 2456, loss 0.328805, acc 0.890625, learning_rate 0.000100213
2017-09-29T11:04:29.684352: step 2457, loss 0.286839, acc 0.875, learning_rate 0.000100213
2017-09-29T11:04:29.872527: step 2458, loss 0.280978, acc 0.90625, learning_rate 0.000100212
2017-09-29T11:04:30.057648: step 2459, loss 0.263936, acc 0.875, learning_rate 0.000100211
2017-09-29T11:04:30.240607: step 2460, loss 0.256503, acc 0.90625, learning_rate 0.00010021
2017-09-29T11:04:30.435362: step 2461, loss 0.254224, acc 0.90625, learning_rate 0.000100209
2017-09-29T11:04:30.614775: step 2462, loss 0.468078, acc 0.828125, learning_rate 0.000100208
2017-09-29T11:04:30.798704: step 2463, loss 0.155461, acc 0.953125, learning_rate 0.000100207
2017-09-29T11:04:30.976050: step 2464, loss 0.289344, acc 0.90625, learning_rate 0.000100207
2017-09-29T11:04:31.159017: step 2465, loss 0.257409, acc 0.890625, learning_rate 0.000100206
2017-09-29T11:04:31.339594: step 2466, loss 0.166782, acc 0.9375, learning_rate 0.000100205
2017-09-29T11:04:31.532399: step 2467, loss 0.24906, acc 0.90625, learning_rate 0.000100204
2017-09-29T11:04:31.714030: step 2468, loss 0.269358, acc 0.890625, learning_rate 0.000100203
2017-09-29T11:04:31.894434: step 2469, loss 0.242823, acc 0.9375, learning_rate 0.000100202
2017-09-29T11:04:32.078283: step 2470, loss 0.24699, acc 0.90625, learning_rate 0.000100202
2017-09-29T11:04:32.263758: step 2471, loss 0.381008, acc 0.828125, learning_rate 0.000100201
2017-09-29T11:04:32.449337: step 2472, loss 0.298059, acc 0.875, learning_rate 0.0001002
2017-09-29T11:04:32.634738: step 2473, loss 0.311277, acc 0.890625, learning_rate 0.000100199
2017-09-29T11:04:32.816031: step 2474, loss 0.341829, acc 0.875, learning_rate 0.000100198
2017-09-29T11:04:32.999854: step 2475, loss 0.402178, acc 0.921875, learning_rate 0.000100198
2017-09-29T11:04:33.183079: step 2476, loss 0.335392, acc 0.90625, learning_rate 0.000100197
2017-09-29T11:04:33.367158: step 2477, loss 0.307513, acc 0.890625, learning_rate 0.000100196
2017-09-29T11:04:33.563030: step 2478, loss 0.35526, acc 0.859375, learning_rate 0.000100195
2017-09-29T11:04:33.743834: step 2479, loss 0.336731, acc 0.90625, learning_rate 0.000100194
2017-09-29T11:04:33.929815: step 2480, loss 0.340996, acc 0.890625, learning_rate 0.000100194

Evaluation:
2017-09-29T11:04:34.483545: step 2480, loss 0.32741, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2480

2017-09-29T11:04:35.122919: step 2481, loss 0.308664, acc 0.890625, learning_rate 0.000100193
2017-09-29T11:04:35.304126: step 2482, loss 0.259421, acc 0.875, learning_rate 0.000100192
2017-09-29T11:04:35.488563: step 2483, loss 0.225002, acc 0.90625, learning_rate 0.000100191
2017-09-29T11:04:35.675732: step 2484, loss 0.188446, acc 0.953125, learning_rate 0.00010019
2017-09-29T11:04:35.866532: step 2485, loss 0.162331, acc 0.953125, learning_rate 0.00010019
2017-09-29T11:04:36.054449: step 2486, loss 0.2781, acc 0.9375, learning_rate 0.000100189
2017-09-29T11:04:36.236742: step 2487, loss 0.306175, acc 0.875, learning_rate 0.000100188
2017-09-29T11:04:36.416944: step 2488, loss 0.264685, acc 0.921875, learning_rate 0.000100187
2017-09-29T11:04:36.600736: step 2489, loss 0.200112, acc 0.9375, learning_rate 0.000100187
2017-09-29T11:04:36.784866: step 2490, loss 0.178587, acc 0.9375, learning_rate 0.000100186
2017-09-29T11:04:36.966239: step 2491, loss 0.183231, acc 0.953125, learning_rate 0.000100185
2017-09-29T11:04:37.145871: step 2492, loss 0.413993, acc 0.859375, learning_rate 0.000100184
2017-09-29T11:04:37.326891: step 2493, loss 0.271728, acc 0.890625, learning_rate 0.000100183
2017-09-29T11:04:37.525193: step 2494, loss 0.194082, acc 0.9375, learning_rate 0.000100183
2017-09-29T11:04:37.715007: step 2495, loss 0.39957, acc 0.859375, learning_rate 0.000100182
2017-09-29T11:04:37.896898: step 2496, loss 0.192514, acc 0.96875, learning_rate 0.000100181
2017-09-29T11:04:38.078300: step 2497, loss 0.315281, acc 0.890625, learning_rate 0.000100181
2017-09-29T11:04:38.261072: step 2498, loss 0.161333, acc 0.953125, learning_rate 0.00010018
2017-09-29T11:04:38.444957: step 2499, loss 0.289922, acc 0.921875, learning_rate 0.000100179
2017-09-29T11:04:38.628980: step 2500, loss 0.285417, acc 0.890625, learning_rate 0.000100178
2017-09-29T11:04:38.821250: step 2501, loss 0.298266, acc 0.890625, learning_rate 0.000100178
2017-09-29T11:04:39.006884: step 2502, loss 0.197379, acc 0.890625, learning_rate 0.000100177
2017-09-29T11:04:39.191654: step 2503, loss 0.19247, acc 0.9375, learning_rate 0.000100176
2017-09-29T11:04:39.374392: step 2504, loss 0.257497, acc 0.9375, learning_rate 0.000100175
2017-09-29T11:04:39.561660: step 2505, loss 0.242278, acc 0.90625, learning_rate 0.000100175
2017-09-29T11:04:39.742868: step 2506, loss 0.306174, acc 0.890625, learning_rate 0.000100174
2017-09-29T11:04:39.935996: step 2507, loss 0.219028, acc 0.921875, learning_rate 0.000100173
2017-09-29T11:04:40.115025: step 2508, loss 0.442976, acc 0.859375, learning_rate 0.000100173
2017-09-29T11:04:40.294883: step 2509, loss 0.293829, acc 0.890625, learning_rate 0.000100172
2017-09-29T11:04:40.481264: step 2510, loss 0.186344, acc 0.921875, learning_rate 0.000100171
2017-09-29T11:04:40.663650: step 2511, loss 0.328745, acc 0.859375, learning_rate 0.00010017
2017-09-29T11:04:40.846856: step 2512, loss 0.213193, acc 0.921875, learning_rate 0.00010017
2017-09-29T11:04:41.030514: step 2513, loss 0.342433, acc 0.890625, learning_rate 0.000100169
2017-09-29T11:04:41.215245: step 2514, loss 0.304417, acc 0.90625, learning_rate 0.000100168
2017-09-29T11:04:41.398201: step 2515, loss 0.309932, acc 0.84375, learning_rate 0.000100168
2017-09-29T11:04:41.585969: step 2516, loss 0.317603, acc 0.875, learning_rate 0.000100167
2017-09-29T11:04:41.768309: step 2517, loss 0.398654, acc 0.84375, learning_rate 0.000100166
2017-09-29T11:04:41.952544: step 2518, loss 0.422464, acc 0.875, learning_rate 0.000100166
2017-09-29T11:04:42.132765: step 2519, loss 0.185083, acc 0.953125, learning_rate 0.000100165
2017-09-29T11:04:42.315083: step 2520, loss 0.225628, acc 0.9375, learning_rate 0.000100164

Evaluation:
2017-09-29T11:04:42.876746: step 2520, loss 0.330718, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2520

2017-09-29T11:04:43.578887: step 2521, loss 0.273174, acc 0.90625, learning_rate 0.000100164
2017-09-29T11:04:43.768260: step 2522, loss 0.169381, acc 0.953125, learning_rate 0.000100163
2017-09-29T11:04:43.955458: step 2523, loss 0.148887, acc 0.96875, learning_rate 0.000100162
2017-09-29T11:04:44.139808: step 2524, loss 0.226333, acc 0.890625, learning_rate 0.000100162
2017-09-29T11:04:44.328144: step 2525, loss 0.180691, acc 0.9375, learning_rate 0.000100161
2017-09-29T11:04:44.525049: step 2526, loss 0.320616, acc 0.875, learning_rate 0.00010016
2017-09-29T11:04:44.712412: step 2527, loss 0.380808, acc 0.796875, learning_rate 0.00010016
2017-09-29T11:04:44.904839: step 2528, loss 0.155723, acc 0.9375, learning_rate 0.000100159
2017-09-29T11:04:45.096002: step 2529, loss 0.19255, acc 0.953125, learning_rate 0.000100158
2017-09-29T11:04:45.277272: step 2530, loss 0.114292, acc 0.96875, learning_rate 0.000100158
2017-09-29T11:04:45.462367: step 2531, loss 0.364589, acc 0.90625, learning_rate 0.000100157
2017-09-29T11:04:45.648388: step 2532, loss 0.249403, acc 0.90625, learning_rate 0.000100156
2017-09-29T11:04:45.830066: step 2533, loss 0.228549, acc 0.90625, learning_rate 0.000100156
2017-09-29T11:04:46.018624: step 2534, loss 0.352172, acc 0.875, learning_rate 0.000100155
2017-09-29T11:04:46.204401: step 2535, loss 0.290273, acc 0.859375, learning_rate 0.000100155
2017-09-29T11:04:46.389090: step 2536, loss 0.227179, acc 0.921875, learning_rate 0.000100154
2017-09-29T11:04:46.572243: step 2537, loss 0.310898, acc 0.84375, learning_rate 0.000100153
2017-09-29T11:04:46.757094: step 2538, loss 0.212332, acc 0.9375, learning_rate 0.000100153
2017-09-29T11:04:46.940112: step 2539, loss 0.305776, acc 0.890625, learning_rate 0.000100152
2017-09-29T11:04:47.124888: step 2540, loss 0.232662, acc 0.90625, learning_rate 0.000100151
2017-09-29T11:04:47.306505: step 2541, loss 0.279288, acc 0.921875, learning_rate 0.000100151
2017-09-29T11:04:47.490938: step 2542, loss 0.258471, acc 0.90625, learning_rate 0.00010015
2017-09-29T11:04:47.677786: step 2543, loss 0.303193, acc 0.890625, learning_rate 0.00010015
2017-09-29T11:04:47.860981: step 2544, loss 0.447863, acc 0.828125, learning_rate 0.000100149
2017-09-29T11:04:48.046074: step 2545, loss 0.179848, acc 0.96875, learning_rate 0.000100148
2017-09-29T11:04:48.226858: step 2546, loss 0.421469, acc 0.875, learning_rate 0.000100148
2017-09-29T11:04:48.405025: step 2547, loss 0.317434, acc 0.90625, learning_rate 0.000100147
2017-09-29T11:04:48.567415: step 2548, loss 0.425429, acc 0.823529, learning_rate 0.000100147
2017-09-29T11:04:48.750561: step 2549, loss 0.257539, acc 0.890625, learning_rate 0.000100146
2017-09-29T11:04:48.933851: step 2550, loss 0.255138, acc 0.90625, learning_rate 0.000100145
2017-09-29T11:04:49.125551: step 2551, loss 0.314877, acc 0.90625, learning_rate 0.000100145
2017-09-29T11:04:49.310920: step 2552, loss 0.388065, acc 0.84375, learning_rate 0.000100144
2017-09-29T11:04:49.501708: step 2553, loss 0.282808, acc 0.890625, learning_rate 0.000100144
2017-09-29T11:04:49.686720: step 2554, loss 0.199698, acc 0.953125, learning_rate 0.000100143
2017-09-29T11:04:49.874279: step 2555, loss 0.291992, acc 0.90625, learning_rate 0.000100142
2017-09-29T11:04:50.068557: step 2556, loss 0.193531, acc 0.9375, learning_rate 0.000100142
2017-09-29T11:04:50.252225: step 2557, loss 0.343624, acc 0.859375, learning_rate 0.000100141
2017-09-29T11:04:50.436111: step 2558, loss 0.281454, acc 0.90625, learning_rate 0.000100141
2017-09-29T11:04:50.633752: step 2559, loss 0.351669, acc 0.859375, learning_rate 0.00010014
2017-09-29T11:04:50.821605: step 2560, loss 0.283322, acc 0.921875, learning_rate 0.00010014

Evaluation:
2017-09-29T11:04:51.371907: step 2560, loss 0.323755, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2560

2017-09-29T11:04:52.105021: step 2561, loss 0.213434, acc 0.953125, learning_rate 0.000100139
2017-09-29T11:04:52.298299: step 2562, loss 0.378322, acc 0.84375, learning_rate 0.000100138
2017-09-29T11:04:52.494642: step 2563, loss 0.269681, acc 0.890625, learning_rate 0.000100138
2017-09-29T11:04:52.683511: step 2564, loss 0.211242, acc 0.9375, learning_rate 0.000100137
2017-09-29T11:04:52.869149: step 2565, loss 0.251077, acc 0.890625, learning_rate 0.000100137
2017-09-29T11:04:53.060250: step 2566, loss 0.255749, acc 0.875, learning_rate 0.000100136
2017-09-29T11:04:53.247414: step 2567, loss 0.227414, acc 0.9375, learning_rate 0.000100136
2017-09-29T11:04:53.434857: step 2568, loss 0.393717, acc 0.859375, learning_rate 0.000100135
2017-09-29T11:04:53.623625: step 2569, loss 0.330361, acc 0.90625, learning_rate 0.000100134
2017-09-29T11:04:53.806469: step 2570, loss 0.324515, acc 0.875, learning_rate 0.000100134
2017-09-29T11:04:53.988448: step 2571, loss 0.16299, acc 0.921875, learning_rate 0.000100133
2017-09-29T11:04:54.171945: step 2572, loss 0.199096, acc 0.9375, learning_rate 0.000100133
2017-09-29T11:04:54.360766: step 2573, loss 0.21059, acc 0.953125, learning_rate 0.000100132
2017-09-29T11:04:54.561374: step 2574, loss 0.222157, acc 0.921875, learning_rate 0.000100132
2017-09-29T11:04:54.748568: step 2575, loss 0.289164, acc 0.859375, learning_rate 0.000100131
2017-09-29T11:04:54.943105: step 2576, loss 0.249618, acc 0.90625, learning_rate 0.000100131
2017-09-29T11:04:55.134013: step 2577, loss 0.331042, acc 0.921875, learning_rate 0.00010013
2017-09-29T11:04:55.326243: step 2578, loss 0.270389, acc 0.921875, learning_rate 0.00010013
2017-09-29T11:04:55.516020: step 2579, loss 0.324268, acc 0.90625, learning_rate 0.000100129
2017-09-29T11:04:55.700959: step 2580, loss 0.386573, acc 0.90625, learning_rate 0.000100129
2017-09-29T11:04:55.893689: step 2581, loss 0.193953, acc 0.953125, learning_rate 0.000100128
2017-09-29T11:04:56.086907: step 2582, loss 0.395736, acc 0.890625, learning_rate 0.000100128
2017-09-29T11:04:56.273640: step 2583, loss 0.234465, acc 0.953125, learning_rate 0.000100127
2017-09-29T11:04:56.457733: step 2584, loss 0.241996, acc 0.875, learning_rate 0.000100126
2017-09-29T11:04:56.644412: step 2585, loss 0.309737, acc 0.90625, learning_rate 0.000100126
2017-09-29T11:04:56.834910: step 2586, loss 0.14341, acc 0.953125, learning_rate 0.000100125
2017-09-29T11:04:57.035906: step 2587, loss 0.370448, acc 0.859375, learning_rate 0.000100125
2017-09-29T11:04:57.221293: step 2588, loss 0.285772, acc 0.890625, learning_rate 0.000100124
2017-09-29T11:04:57.410270: step 2589, loss 0.253056, acc 0.90625, learning_rate 0.000100124
2017-09-29T11:04:57.606729: step 2590, loss 0.276483, acc 0.859375, learning_rate 0.000100123
2017-09-29T11:04:57.791246: step 2591, loss 0.300064, acc 0.921875, learning_rate 0.000100123
2017-09-29T11:04:57.979568: step 2592, loss 0.353218, acc 0.890625, learning_rate 0.000100122
2017-09-29T11:04:58.163136: step 2593, loss 0.305675, acc 0.875, learning_rate 0.000100122
2017-09-29T11:04:58.349631: step 2594, loss 0.338184, acc 0.890625, learning_rate 0.000100121
2017-09-29T11:04:58.535026: step 2595, loss 0.181544, acc 0.96875, learning_rate 0.000100121
2017-09-29T11:04:58.724828: step 2596, loss 0.280555, acc 0.921875, learning_rate 0.00010012
2017-09-29T11:04:58.918878: step 2597, loss 0.500651, acc 0.828125, learning_rate 0.00010012
2017-09-29T11:04:59.103584: step 2598, loss 0.286286, acc 0.90625, learning_rate 0.000100119
2017-09-29T11:04:59.286871: step 2599, loss 0.300517, acc 0.859375, learning_rate 0.000100119
2017-09-29T11:04:59.482278: step 2600, loss 0.259946, acc 0.9375, learning_rate 0.000100118

Evaluation:
2017-09-29T11:05:00.082988: step 2600, loss 0.33276, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2600

2017-09-29T11:05:00.875391: step 2601, loss 0.22598, acc 0.921875, learning_rate 0.000100118
2017-09-29T11:05:01.060809: step 2602, loss 0.326333, acc 0.859375, learning_rate 0.000100117
2017-09-29T11:05:01.248483: step 2603, loss 0.262276, acc 0.921875, learning_rate 0.000100117
2017-09-29T11:05:01.436508: step 2604, loss 0.297131, acc 0.890625, learning_rate 0.000100117
2017-09-29T11:05:01.625422: step 2605, loss 0.233404, acc 0.890625, learning_rate 0.000100116
2017-09-29T11:05:01.816195: step 2606, loss 0.31301, acc 0.90625, learning_rate 0.000100116
2017-09-29T11:05:02.005187: step 2607, loss 0.251912, acc 0.921875, learning_rate 0.000100115
2017-09-29T11:05:02.194572: step 2608, loss 0.167116, acc 0.96875, learning_rate 0.000100115
2017-09-29T11:05:02.384726: step 2609, loss 0.22141, acc 0.9375, learning_rate 0.000100114
2017-09-29T11:05:02.567588: step 2610, loss 0.201735, acc 0.921875, learning_rate 0.000100114
2017-09-29T11:05:02.753939: step 2611, loss 0.298193, acc 0.90625, learning_rate 0.000100113
2017-09-29T11:05:02.939927: step 2612, loss 0.185845, acc 0.9375, learning_rate 0.000100113
2017-09-29T11:05:03.128476: step 2613, loss 0.290864, acc 0.84375, learning_rate 0.000100112
2017-09-29T11:05:03.313182: step 2614, loss 0.281566, acc 0.890625, learning_rate 0.000100112
2017-09-29T11:05:03.500747: step 2615, loss 0.358451, acc 0.875, learning_rate 0.000100111
2017-09-29T11:05:03.689160: step 2616, loss 0.222961, acc 0.953125, learning_rate 0.000100111
2017-09-29T11:05:03.883185: step 2617, loss 0.268748, acc 0.859375, learning_rate 0.000100111
2017-09-29T11:05:04.071522: step 2618, loss 0.225069, acc 0.921875, learning_rate 0.00010011
2017-09-29T11:05:04.260669: step 2619, loss 0.299029, acc 0.890625, learning_rate 0.00010011
2017-09-29T11:05:04.448886: step 2620, loss 0.220082, acc 0.9375, learning_rate 0.000100109
2017-09-29T11:05:04.638411: step 2621, loss 0.216126, acc 0.921875, learning_rate 0.000100109
2017-09-29T11:05:04.819504: step 2622, loss 0.16326, acc 0.9375, learning_rate 0.000100108
2017-09-29T11:05:05.018770: step 2623, loss 0.306827, acc 0.921875, learning_rate 0.000100108
2017-09-29T11:05:05.202202: step 2624, loss 0.489861, acc 0.859375, learning_rate 0.000100107
2017-09-29T11:05:05.392617: step 2625, loss 0.302242, acc 0.875, learning_rate 0.000100107
2017-09-29T11:05:05.593287: step 2626, loss 0.192319, acc 0.921875, learning_rate 0.000100107
2017-09-29T11:05:05.789004: step 2627, loss 0.245966, acc 0.90625, learning_rate 0.000100106
2017-09-29T11:05:05.982148: step 2628, loss 0.33695, acc 0.875, learning_rate 0.000100106
2017-09-29T11:05:06.172945: step 2629, loss 0.300821, acc 0.890625, learning_rate 0.000100105
2017-09-29T11:05:06.361213: step 2630, loss 0.211175, acc 0.9375, learning_rate 0.000100105
2017-09-29T11:05:06.555496: step 2631, loss 0.426039, acc 0.8125, learning_rate 0.000100104
2017-09-29T11:05:06.744906: step 2632, loss 0.353807, acc 0.875, learning_rate 0.000100104
2017-09-29T11:05:06.937326: step 2633, loss 0.233784, acc 0.90625, learning_rate 0.000100104
2017-09-29T11:05:07.128024: step 2634, loss 0.3852, acc 0.828125, learning_rate 0.000100103
2017-09-29T11:05:07.312804: step 2635, loss 0.210689, acc 0.921875, learning_rate 0.000100103
2017-09-29T11:05:07.499876: step 2636, loss 0.240592, acc 0.875, learning_rate 0.000100102
2017-09-29T11:05:07.692332: step 2637, loss 0.432123, acc 0.84375, learning_rate 0.000100102
2017-09-29T11:05:07.878480: step 2638, loss 0.343197, acc 0.859375, learning_rate 0.000100101
2017-09-29T11:05:08.067389: step 2639, loss 0.217122, acc 0.921875, learning_rate 0.000100101
2017-09-29T11:05:08.248238: step 2640, loss 0.272771, acc 0.921875, learning_rate 0.000100101

Evaluation:
2017-09-29T11:05:08.811543: step 2640, loss 0.324889, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2640

2017-09-29T11:05:09.462436: step 2641, loss 0.306565, acc 0.921875, learning_rate 0.0001001
2017-09-29T11:05:10.846268: step 2642, loss 0.528132, acc 0.796875, learning_rate 0.0001001
2017-09-29T11:05:11.037274: step 2643, loss 0.330255, acc 0.890625, learning_rate 0.000100099
2017-09-29T11:05:11.227939: step 2644, loss 0.237054, acc 0.921875, learning_rate 0.000100099
2017-09-29T11:05:11.419122: step 2645, loss 0.340748, acc 0.84375, learning_rate 0.000100099
2017-09-29T11:05:11.572634: step 2646, loss 0.190728, acc 0.941176, learning_rate 0.000100098
2017-09-29T11:05:11.756403: step 2647, loss 0.155954, acc 0.984375, learning_rate 0.000100098
2017-09-29T11:05:11.940317: step 2648, loss 0.306465, acc 0.84375, learning_rate 0.000100097
2017-09-29T11:05:12.125696: step 2649, loss 0.266638, acc 0.890625, learning_rate 0.000100097
2017-09-29T11:05:12.308705: step 2650, loss 0.170588, acc 0.9375, learning_rate 0.000100097
2017-09-29T11:05:12.491330: step 2651, loss 0.207334, acc 0.9375, learning_rate 0.000100096
2017-09-29T11:05:12.674467: step 2652, loss 0.183784, acc 0.921875, learning_rate 0.000100096
2017-09-29T11:05:12.860722: step 2653, loss 0.263784, acc 0.90625, learning_rate 0.000100095
2017-09-29T11:05:13.041404: step 2654, loss 0.259521, acc 0.890625, learning_rate 0.000100095
2017-09-29T11:05:13.222666: step 2655, loss 0.235414, acc 0.9375, learning_rate 0.000100095
2017-09-29T11:05:13.410780: step 2656, loss 0.385533, acc 0.859375, learning_rate 0.000100094
2017-09-29T11:05:13.602959: step 2657, loss 0.238438, acc 0.90625, learning_rate 0.000100094
2017-09-29T11:05:13.788755: step 2658, loss 0.210188, acc 0.9375, learning_rate 0.000100093
2017-09-29T11:05:13.972451: step 2659, loss 0.237087, acc 0.90625, learning_rate 0.000100093
2017-09-29T11:05:14.158123: step 2660, loss 0.274361, acc 0.921875, learning_rate 0.000100093
2017-09-29T11:05:14.342752: step 2661, loss 0.279074, acc 0.9375, learning_rate 0.000100092
2017-09-29T11:05:14.529378: step 2662, loss 0.465142, acc 0.828125, learning_rate 0.000100092
2017-09-29T11:05:14.722076: step 2663, loss 0.15045, acc 0.953125, learning_rate 0.000100092
2017-09-29T11:05:14.906731: step 2664, loss 0.34055, acc 0.890625, learning_rate 0.000100091
2017-09-29T11:05:15.099049: step 2665, loss 0.23733, acc 0.921875, learning_rate 0.000100091
2017-09-29T11:05:15.284807: step 2666, loss 0.245419, acc 0.921875, learning_rate 0.00010009
2017-09-29T11:05:15.468881: step 2667, loss 0.25722, acc 0.921875, learning_rate 0.00010009
2017-09-29T11:05:15.647426: step 2668, loss 0.213676, acc 0.953125, learning_rate 0.00010009
2017-09-29T11:05:15.829444: step 2669, loss 0.262807, acc 0.90625, learning_rate 0.000100089
2017-09-29T11:05:16.013618: step 2670, loss 0.303548, acc 0.875, learning_rate 0.000100089
2017-09-29T11:05:16.198258: step 2671, loss 0.251737, acc 0.875, learning_rate 0.000100089
2017-09-29T11:05:16.382939: step 2672, loss 0.213813, acc 0.9375, learning_rate 0.000100088
2017-09-29T11:05:16.567981: step 2673, loss 0.370314, acc 0.890625, learning_rate 0.000100088
2017-09-29T11:05:16.751926: step 2674, loss 0.293446, acc 0.859375, learning_rate 0.000100088
2017-09-29T11:05:16.935863: step 2675, loss 0.32692, acc 0.859375, learning_rate 0.000100087
2017-09-29T11:05:17.120914: step 2676, loss 0.228221, acc 0.90625, learning_rate 0.000100087
2017-09-29T11:05:17.306076: step 2677, loss 0.28611, acc 0.9375, learning_rate 0.000100086
2017-09-29T11:05:17.502181: step 2678, loss 0.288991, acc 0.890625, learning_rate 0.000100086
2017-09-29T11:05:17.688580: step 2679, loss 0.352079, acc 0.90625, learning_rate 0.000100086
2017-09-29T11:05:17.874481: step 2680, loss 0.398366, acc 0.890625, learning_rate 0.000100085

Evaluation:
2017-09-29T11:05:18.442153: step 2680, loss 0.323963, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2680

2017-09-29T11:05:19.143962: step 2681, loss 0.189555, acc 0.9375, learning_rate 0.000100085
2017-09-29T11:05:19.325847: step 2682, loss 0.156235, acc 0.953125, learning_rate 0.000100085
2017-09-29T11:05:19.515762: step 2683, loss 0.278954, acc 0.921875, learning_rate 0.000100084
2017-09-29T11:05:19.703564: step 2684, loss 0.373912, acc 0.84375, learning_rate 0.000100084
2017-09-29T11:05:19.890430: step 2685, loss 0.30155, acc 0.921875, learning_rate 0.000100084
2017-09-29T11:05:20.084171: step 2686, loss 0.333498, acc 0.890625, learning_rate 0.000100083
2017-09-29T11:05:20.270470: step 2687, loss 0.165089, acc 0.96875, learning_rate 0.000100083
2017-09-29T11:05:20.464636: step 2688, loss 0.152478, acc 0.953125, learning_rate 0.000100083
2017-09-29T11:05:20.647830: step 2689, loss 0.3202, acc 0.859375, learning_rate 0.000100082
2017-09-29T11:05:20.831449: step 2690, loss 0.353812, acc 0.875, learning_rate 0.000100082
2017-09-29T11:05:21.018703: step 2691, loss 0.214406, acc 0.90625, learning_rate 0.000100082
2017-09-29T11:05:21.204813: step 2692, loss 0.303993, acc 0.921875, learning_rate 0.000100081
2017-09-29T11:05:21.385960: step 2693, loss 0.12443, acc 0.953125, learning_rate 0.000100081
2017-09-29T11:05:21.567526: step 2694, loss 0.20936, acc 0.921875, learning_rate 0.000100081
2017-09-29T11:05:21.752459: step 2695, loss 0.427413, acc 0.84375, learning_rate 0.00010008
2017-09-29T11:05:21.944223: step 2696, loss 0.164461, acc 0.9375, learning_rate 0.00010008
2017-09-29T11:05:22.134981: step 2697, loss 0.230763, acc 0.9375, learning_rate 0.00010008
2017-09-29T11:05:22.316902: step 2698, loss 0.450187, acc 0.875, learning_rate 0.000100079
2017-09-29T11:05:22.500216: step 2699, loss 0.367059, acc 0.84375, learning_rate 0.000100079
2017-09-29T11:05:22.682793: step 2700, loss 0.211044, acc 0.9375, learning_rate 0.000100079
2017-09-29T11:05:22.872854: step 2701, loss 0.225601, acc 0.953125, learning_rate 0.000100078
2017-09-29T11:05:23.059119: step 2702, loss 0.210183, acc 0.90625, learning_rate 0.000100078
2017-09-29T11:05:23.243988: step 2703, loss 0.158675, acc 0.953125, learning_rate 0.000100078
2017-09-29T11:05:23.429472: step 2704, loss 0.216681, acc 0.9375, learning_rate 0.000100077
2017-09-29T11:05:23.628118: step 2705, loss 0.162657, acc 0.953125, learning_rate 0.000100077
2017-09-29T11:05:23.815041: step 2706, loss 0.381458, acc 0.828125, learning_rate 0.000100077
2017-09-29T11:05:24.009512: step 2707, loss 0.29753, acc 0.90625, learning_rate 0.000100076
2017-09-29T11:05:24.199524: step 2708, loss 0.410305, acc 0.875, learning_rate 0.000100076
2017-09-29T11:05:24.385592: step 2709, loss 0.402775, acc 0.890625, learning_rate 0.000100076
2017-09-29T11:05:24.574995: step 2710, loss 0.238813, acc 0.953125, learning_rate 0.000100076
2017-09-29T11:05:24.760703: step 2711, loss 0.243945, acc 0.921875, learning_rate 0.000100075
2017-09-29T11:05:24.945545: step 2712, loss 0.399101, acc 0.875, learning_rate 0.000100075
2017-09-29T11:05:25.135468: step 2713, loss 0.401476, acc 0.828125, learning_rate 0.000100075
2017-09-29T11:05:25.324630: step 2714, loss 0.25758, acc 0.9375, learning_rate 0.000100074
2017-09-29T11:05:25.518185: step 2715, loss 0.272891, acc 0.890625, learning_rate 0.000100074
2017-09-29T11:05:25.702609: step 2716, loss 0.380058, acc 0.890625, learning_rate 0.000100074
2017-09-29T11:05:25.883960: step 2717, loss 0.310224, acc 0.890625, learning_rate 0.000100073
2017-09-29T11:05:26.066303: step 2718, loss 0.243142, acc 0.90625, learning_rate 0.000100073
2017-09-29T11:05:26.250788: step 2719, loss 0.162159, acc 0.96875, learning_rate 0.000100073
2017-09-29T11:05:26.440736: step 2720, loss 0.367368, acc 0.875, learning_rate 0.000100073

Evaluation:
2017-09-29T11:05:26.993501: step 2720, loss 0.319281, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2720

2017-09-29T11:05:27.695643: step 2721, loss 0.335513, acc 0.921875, learning_rate 0.000100072
2017-09-29T11:05:27.877675: step 2722, loss 0.344862, acc 0.890625, learning_rate 0.000100072
2017-09-29T11:05:28.061370: step 2723, loss 0.295097, acc 0.9375, learning_rate 0.000100072
2017-09-29T11:05:28.241524: step 2724, loss 0.328829, acc 0.890625, learning_rate 0.000100071
2017-09-29T11:05:28.427263: step 2725, loss 0.300019, acc 0.875, learning_rate 0.000100071
2017-09-29T11:05:28.608217: step 2726, loss 0.251185, acc 0.890625, learning_rate 0.000100071
2017-09-29T11:05:28.799645: step 2727, loss 0.274898, acc 0.890625, learning_rate 0.00010007
2017-09-29T11:05:28.986457: step 2728, loss 0.319555, acc 0.90625, learning_rate 0.00010007
2017-09-29T11:05:29.170105: step 2729, loss 0.281909, acc 0.90625, learning_rate 0.00010007
2017-09-29T11:05:29.355409: step 2730, loss 0.337415, acc 0.84375, learning_rate 0.00010007
2017-09-29T11:05:29.554466: step 2731, loss 0.263549, acc 0.859375, learning_rate 0.000100069
2017-09-29T11:05:29.745787: step 2732, loss 0.233638, acc 0.9375, learning_rate 0.000100069
2017-09-29T11:05:29.927907: step 2733, loss 0.242158, acc 0.9375, learning_rate 0.000100069
2017-09-29T11:05:30.127504: step 2734, loss 0.327259, acc 0.875, learning_rate 0.000100068
2017-09-29T11:05:30.313368: step 2735, loss 0.305992, acc 0.90625, learning_rate 0.000100068
2017-09-29T11:05:30.513334: step 2736, loss 0.276916, acc 0.875, learning_rate 0.000100068
2017-09-29T11:05:30.698608: step 2737, loss 0.332071, acc 0.859375, learning_rate 0.000100068
2017-09-29T11:05:30.885494: step 2738, loss 0.340688, acc 0.875, learning_rate 0.000100067
2017-09-29T11:05:31.070082: step 2739, loss 0.287432, acc 0.875, learning_rate 0.000100067
2017-09-29T11:05:31.265061: step 2740, loss 0.12812, acc 0.96875, learning_rate 0.000100067
2017-09-29T11:05:31.461230: step 2741, loss 0.411662, acc 0.890625, learning_rate 0.000100067
2017-09-29T11:05:31.643038: step 2742, loss 0.242778, acc 0.90625, learning_rate 0.000100066
2017-09-29T11:05:31.821937: step 2743, loss 0.297509, acc 0.921875, learning_rate 0.000100066
2017-09-29T11:05:31.983725: step 2744, loss 0.285301, acc 0.901961, learning_rate 0.000100066
2017-09-29T11:05:32.167683: step 2745, loss 0.375118, acc 0.859375, learning_rate 0.000100065
2017-09-29T11:05:32.349806: step 2746, loss 0.209675, acc 0.953125, learning_rate 0.000100065
2017-09-29T11:05:32.535146: step 2747, loss 0.25124, acc 0.90625, learning_rate 0.000100065
2017-09-29T11:05:32.740706: step 2748, loss 0.329253, acc 0.875, learning_rate 0.000100065
2017-09-29T11:05:32.924028: step 2749, loss 0.292748, acc 0.9375, learning_rate 0.000100064
2017-09-29T11:05:33.106652: step 2750, loss 0.244666, acc 0.921875, learning_rate 0.000100064
2017-09-29T11:05:33.292243: step 2751, loss 0.243377, acc 0.90625, learning_rate 0.000100064
2017-09-29T11:05:33.476857: step 2752, loss 0.468165, acc 0.8125, learning_rate 0.000100064
2017-09-29T11:05:33.659981: step 2753, loss 0.265771, acc 0.890625, learning_rate 0.000100063
2017-09-29T11:05:33.843024: step 2754, loss 0.208727, acc 0.9375, learning_rate 0.000100063
2017-09-29T11:05:34.030150: step 2755, loss 0.193342, acc 0.9375, learning_rate 0.000100063
2017-09-29T11:05:34.216441: step 2756, loss 0.320219, acc 0.875, learning_rate 0.000100063
2017-09-29T11:05:34.401839: step 2757, loss 0.161712, acc 0.953125, learning_rate 0.000100062
2017-09-29T11:05:34.591020: step 2758, loss 0.262667, acc 0.9375, learning_rate 0.000100062
2017-09-29T11:05:34.778361: step 2759, loss 0.350663, acc 0.875, learning_rate 0.000100062
2017-09-29T11:05:34.970598: step 2760, loss 0.373728, acc 0.84375, learning_rate 0.000100062

Evaluation:
2017-09-29T11:05:35.553227: step 2760, loss 0.322589, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2760

2017-09-29T11:05:36.336233: step 2761, loss 0.300487, acc 0.90625, learning_rate 0.000100061
2017-09-29T11:05:36.522506: step 2762, loss 0.137977, acc 0.96875, learning_rate 0.000100061
2017-09-29T11:05:36.707785: step 2763, loss 0.316604, acc 0.859375, learning_rate 0.000100061
2017-09-29T11:05:36.887952: step 2764, loss 0.285919, acc 0.90625, learning_rate 0.000100061
2017-09-29T11:05:37.069827: step 2765, loss 0.369848, acc 0.875, learning_rate 0.00010006
2017-09-29T11:05:37.254201: step 2766, loss 0.291138, acc 0.890625, learning_rate 0.00010006
2017-09-29T11:05:37.443563: step 2767, loss 0.40679, acc 0.859375, learning_rate 0.00010006
2017-09-29T11:05:37.639219: step 2768, loss 0.165032, acc 0.96875, learning_rate 0.00010006
2017-09-29T11:05:37.823898: step 2769, loss 0.194626, acc 0.953125, learning_rate 0.000100059
2017-09-29T11:05:38.007142: step 2770, loss 0.249186, acc 0.9375, learning_rate 0.000100059
2017-09-29T11:05:38.219154: step 2771, loss 0.362678, acc 0.859375, learning_rate 0.000100059
2017-09-29T11:05:38.400937: step 2772, loss 0.288279, acc 0.875, learning_rate 0.000100059
2017-09-29T11:05:38.591183: step 2773, loss 0.281429, acc 0.890625, learning_rate 0.000100058
2017-09-29T11:05:38.784394: step 2774, loss 0.299951, acc 0.859375, learning_rate 0.000100058
2017-09-29T11:05:38.967877: step 2775, loss 0.291493, acc 0.890625, learning_rate 0.000100058
2017-09-29T11:05:39.159748: step 2776, loss 0.261077, acc 0.890625, learning_rate 0.000100058
2017-09-29T11:05:39.346040: step 2777, loss 0.305411, acc 0.890625, learning_rate 0.000100057
2017-09-29T11:05:39.531051: step 2778, loss 0.199477, acc 0.921875, learning_rate 0.000100057
2017-09-29T11:05:39.719793: step 2779, loss 0.158515, acc 0.953125, learning_rate 0.000100057
2017-09-29T11:05:39.905617: step 2780, loss 0.1911, acc 0.9375, learning_rate 0.000100057
2017-09-29T11:05:40.090016: step 2781, loss 0.248297, acc 0.90625, learning_rate 0.000100056
2017-09-29T11:05:40.289787: step 2782, loss 0.259032, acc 0.921875, learning_rate 0.000100056
2017-09-29T11:05:40.469773: step 2783, loss 0.209174, acc 0.921875, learning_rate 0.000100056
2017-09-29T11:05:40.654660: step 2784, loss 0.161844, acc 0.9375, learning_rate 0.000100056
2017-09-29T11:05:40.840239: step 2785, loss 0.271204, acc 0.890625, learning_rate 0.000100056
2017-09-29T11:05:41.029526: step 2786, loss 0.180201, acc 0.921875, learning_rate 0.000100055
2017-09-29T11:05:41.212281: step 2787, loss 0.326105, acc 0.859375, learning_rate 0.000100055
2017-09-29T11:05:41.397030: step 2788, loss 0.239395, acc 0.9375, learning_rate 0.000100055
2017-09-29T11:05:41.591688: step 2789, loss 0.311114, acc 0.875, learning_rate 0.000100055
2017-09-29T11:05:41.774360: step 2790, loss 0.117225, acc 0.96875, learning_rate 0.000100054
2017-09-29T11:05:41.953730: step 2791, loss 0.238264, acc 0.9375, learning_rate 0.000100054
2017-09-29T11:05:42.137697: step 2792, loss 0.269401, acc 0.921875, learning_rate 0.000100054
2017-09-29T11:05:42.320308: step 2793, loss 0.287621, acc 0.890625, learning_rate 0.000100054
2017-09-29T11:05:42.503975: step 2794, loss 0.172936, acc 0.953125, learning_rate 0.000100054
2017-09-29T11:05:42.684926: step 2795, loss 0.239269, acc 0.90625, learning_rate 0.000100053
2017-09-29T11:05:42.868134: step 2796, loss 0.48422, acc 0.875, learning_rate 0.000100053
2017-09-29T11:05:43.049002: step 2797, loss 0.312021, acc 0.921875, learning_rate 0.000100053
2017-09-29T11:05:43.236338: step 2798, loss 0.26595, acc 0.890625, learning_rate 0.000100053
2017-09-29T11:05:43.421977: step 2799, loss 0.229034, acc 0.921875, learning_rate 0.000100052
2017-09-29T11:05:43.605587: step 2800, loss 0.315349, acc 0.890625, learning_rate 0.000100052

Evaluation:
2017-09-29T11:05:44.159680: step 2800, loss 0.321157, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2800

2017-09-29T11:05:44.806186: step 2801, loss 0.201985, acc 0.9375, learning_rate 0.000100052
2017-09-29T11:05:44.987404: step 2802, loss 0.283015, acc 0.875, learning_rate 0.000100052
2017-09-29T11:05:45.171542: step 2803, loss 0.156265, acc 0.96875, learning_rate 0.000100052
2017-09-29T11:05:45.359807: step 2804, loss 0.297231, acc 0.859375, learning_rate 0.000100051
2017-09-29T11:05:45.544891: step 2805, loss 0.305047, acc 0.890625, learning_rate 0.000100051
2017-09-29T11:05:45.728928: step 2806, loss 0.241123, acc 0.90625, learning_rate 0.000100051
2017-09-29T11:05:45.923049: step 2807, loss 0.388758, acc 0.859375, learning_rate 0.000100051
2017-09-29T11:05:46.109243: step 2808, loss 0.202586, acc 0.953125, learning_rate 0.000100051
2017-09-29T11:05:46.291820: step 2809, loss 0.28527, acc 0.921875, learning_rate 0.00010005
2017-09-29T11:05:46.488857: step 2810, loss 0.19462, acc 0.953125, learning_rate 0.00010005
2017-09-29T11:05:46.689026: step 2811, loss 0.261242, acc 0.921875, learning_rate 0.00010005
2017-09-29T11:05:46.892009: step 2812, loss 0.279658, acc 0.921875, learning_rate 0.00010005
2017-09-29T11:05:47.097023: step 2813, loss 0.262356, acc 0.921875, learning_rate 0.00010005
2017-09-29T11:05:47.290343: step 2814, loss 0.471893, acc 0.859375, learning_rate 0.000100049
2017-09-29T11:05:47.476391: step 2815, loss 0.231518, acc 0.90625, learning_rate 0.000100049
2017-09-29T11:05:47.659733: step 2816, loss 0.213865, acc 0.9375, learning_rate 0.000100049
2017-09-29T11:05:47.840937: step 2817, loss 0.16051, acc 0.96875, learning_rate 0.000100049
2017-09-29T11:05:48.027342: step 2818, loss 0.167689, acc 0.96875, learning_rate 0.000100049
2017-09-29T11:05:48.222157: step 2819, loss 0.208296, acc 0.921875, learning_rate 0.000100048
2017-09-29T11:05:48.413190: step 2820, loss 0.203842, acc 0.921875, learning_rate 0.000100048
2017-09-29T11:05:48.594133: step 2821, loss 0.119214, acc 0.96875, learning_rate 0.000100048
2017-09-29T11:05:48.776584: step 2822, loss 0.272502, acc 0.875, learning_rate 0.000100048
2017-09-29T11:05:48.963834: step 2823, loss 0.332111, acc 0.90625, learning_rate 0.000100048
2017-09-29T11:05:49.153098: step 2824, loss 0.266668, acc 0.875, learning_rate 0.000100047
2017-09-29T11:05:49.341737: step 2825, loss 0.388363, acc 0.8125, learning_rate 0.000100047
2017-09-29T11:05:49.529463: step 2826, loss 0.271424, acc 0.9375, learning_rate 0.000100047
2017-09-29T11:05:49.719359: step 2827, loss 0.231494, acc 0.9375, learning_rate 0.000100047
2017-09-29T11:05:49.918707: step 2828, loss 0.233313, acc 0.90625, learning_rate 0.000100047
2017-09-29T11:05:50.115454: step 2829, loss 0.301975, acc 0.890625, learning_rate 0.000100046
2017-09-29T11:05:50.305077: step 2830, loss 0.209174, acc 0.90625, learning_rate 0.000100046
2017-09-29T11:05:50.486710: step 2831, loss 0.20211, acc 0.96875, learning_rate 0.000100046
2017-09-29T11:05:50.672518: step 2832, loss 0.325693, acc 0.828125, learning_rate 0.000100046
2017-09-29T11:05:50.859283: step 2833, loss 0.27727, acc 0.90625, learning_rate 0.000100046
2017-09-29T11:05:51.042096: step 2834, loss 0.232398, acc 0.921875, learning_rate 0.000100045
2017-09-29T11:05:51.228109: step 2835, loss 0.173697, acc 0.9375, learning_rate 0.000100045
2017-09-29T11:05:51.416404: step 2836, loss 0.377576, acc 0.84375, learning_rate 0.000100045
2017-09-29T11:05:51.599259: step 2837, loss 0.400509, acc 0.84375, learning_rate 0.000100045
2017-09-29T11:05:51.780679: step 2838, loss 0.330567, acc 0.90625, learning_rate 0.000100045
2017-09-29T11:05:51.960565: step 2839, loss 0.276356, acc 0.953125, learning_rate 0.000100045
2017-09-29T11:05:52.153714: step 2840, loss 0.193367, acc 0.953125, learning_rate 0.000100044

Evaluation:
2017-09-29T11:05:52.718471: step 2840, loss 0.323005, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2840

2017-09-29T11:05:53.437212: step 2841, loss 0.178566, acc 0.9375, learning_rate 0.000100044
2017-09-29T11:05:53.588678: step 2842, loss 0.153154, acc 0.980392, learning_rate 0.000100044
2017-09-29T11:05:53.768723: step 2843, loss 0.304751, acc 0.875, learning_rate 0.000100044
2017-09-29T11:05:53.949681: step 2844, loss 0.366665, acc 0.84375, learning_rate 0.000100044
2017-09-29T11:05:54.132981: step 2845, loss 0.218674, acc 0.9375, learning_rate 0.000100043
2017-09-29T11:05:54.317828: step 2846, loss 0.305877, acc 0.859375, learning_rate 0.000100043
2017-09-29T11:05:54.498692: step 2847, loss 0.152021, acc 0.96875, learning_rate 0.000100043
2017-09-29T11:05:54.682332: step 2848, loss 0.199636, acc 0.921875, learning_rate 0.000100043
2017-09-29T11:05:54.870308: step 2849, loss 0.233816, acc 0.890625, learning_rate 0.000100043
2017-09-29T11:05:55.066237: step 2850, loss 0.171282, acc 0.953125, learning_rate 0.000100043
2017-09-29T11:05:55.247796: step 2851, loss 0.216109, acc 0.9375, learning_rate 0.000100042
2017-09-29T11:05:55.439091: step 2852, loss 0.204609, acc 0.90625, learning_rate 0.000100042
2017-09-29T11:05:55.623123: step 2853, loss 0.21201, acc 0.9375, learning_rate 0.000100042
2017-09-29T11:05:55.814119: step 2854, loss 0.201303, acc 0.921875, learning_rate 0.000100042
2017-09-29T11:05:55.998267: step 2855, loss 0.244978, acc 0.875, learning_rate 0.000100042
2017-09-29T11:05:56.183548: step 2856, loss 0.348466, acc 0.90625, learning_rate 0.000100042
2017-09-29T11:05:56.366637: step 2857, loss 0.194394, acc 0.9375, learning_rate 0.000100041
2017-09-29T11:05:56.547681: step 2858, loss 0.263937, acc 0.921875, learning_rate 0.000100041
2017-09-29T11:05:56.735096: step 2859, loss 0.298907, acc 0.84375, learning_rate 0.000100041
2017-09-29T11:05:56.919265: step 2860, loss 0.35412, acc 0.84375, learning_rate 0.000100041
2017-09-29T11:05:57.106394: step 2861, loss 0.31675, acc 0.875, learning_rate 0.000100041
2017-09-29T11:05:57.290842: step 2862, loss 0.509668, acc 0.84375, learning_rate 0.000100041
2017-09-29T11:05:57.474227: step 2863, loss 0.158743, acc 0.96875, learning_rate 0.00010004
2017-09-29T11:05:57.658712: step 2864, loss 0.397929, acc 0.859375, learning_rate 0.00010004
2017-09-29T11:05:57.847748: step 2865, loss 0.38292, acc 0.859375, learning_rate 0.00010004
2017-09-29T11:05:58.039551: step 2866, loss 0.381985, acc 0.890625, learning_rate 0.00010004
2017-09-29T11:05:58.223780: step 2867, loss 0.282612, acc 0.9375, learning_rate 0.00010004
2017-09-29T11:05:58.414601: step 2868, loss 0.214972, acc 0.90625, learning_rate 0.00010004
2017-09-29T11:05:58.617448: step 2869, loss 0.231355, acc 0.921875, learning_rate 0.000100039
2017-09-29T11:05:58.807641: step 2870, loss 0.207899, acc 0.921875, learning_rate 0.000100039
2017-09-29T11:05:58.998265: step 2871, loss 0.136184, acc 0.96875, learning_rate 0.000100039
2017-09-29T11:05:59.189061: step 2872, loss 0.277354, acc 0.90625, learning_rate 0.000100039
2017-09-29T11:05:59.371454: step 2873, loss 0.236393, acc 0.890625, learning_rate 0.000100039
2017-09-29T11:05:59.559790: step 2874, loss 0.227674, acc 0.90625, learning_rate 0.000100039
2017-09-29T11:05:59.750314: step 2875, loss 0.378495, acc 0.875, learning_rate 0.000100038
2017-09-29T11:05:59.937996: step 2876, loss 0.263885, acc 0.890625, learning_rate 0.000100038
2017-09-29T11:06:00.120772: step 2877, loss 0.23473, acc 0.90625, learning_rate 0.000100038
2017-09-29T11:06:00.302763: step 2878, loss 0.255268, acc 0.9375, learning_rate 0.000100038
2017-09-29T11:06:00.488030: step 2879, loss 0.354228, acc 0.84375, learning_rate 0.000100038
2017-09-29T11:06:00.673788: step 2880, loss 0.281983, acc 0.90625, learning_rate 0.000100038

Evaluation:
2017-09-29T11:06:01.238553: step 2880, loss 0.318043, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2880

2017-09-29T11:06:01.940858: step 2881, loss 0.337673, acc 0.8125, learning_rate 0.000100038
2017-09-29T11:06:02.131743: step 2882, loss 0.266298, acc 0.96875, learning_rate 0.000100037
2017-09-29T11:06:02.313268: step 2883, loss 0.24659, acc 0.921875, learning_rate 0.000100037
2017-09-29T11:06:02.500011: step 2884, loss 0.238979, acc 0.90625, learning_rate 0.000100037
2017-09-29T11:06:02.685578: step 2885, loss 0.211164, acc 0.953125, learning_rate 0.000100037
2017-09-29T11:06:02.870522: step 2886, loss 0.409712, acc 0.796875, learning_rate 0.000100037
2017-09-29T11:06:03.051622: step 2887, loss 0.374572, acc 0.875, learning_rate 0.000100037
2017-09-29T11:06:03.235237: step 2888, loss 0.395702, acc 0.875, learning_rate 0.000100036
2017-09-29T11:06:03.416467: step 2889, loss 0.39862, acc 0.90625, learning_rate 0.000100036
2017-09-29T11:06:03.598307: step 2890, loss 0.300537, acc 0.859375, learning_rate 0.000100036
2017-09-29T11:06:03.779144: step 2891, loss 0.297042, acc 0.875, learning_rate 0.000100036
2017-09-29T11:06:03.964791: step 2892, loss 0.235225, acc 0.90625, learning_rate 0.000100036
2017-09-29T11:06:04.148685: step 2893, loss 0.311871, acc 0.90625, learning_rate 0.000100036
2017-09-29T11:06:04.332843: step 2894, loss 0.347727, acc 0.875, learning_rate 0.000100036
2017-09-29T11:06:04.528120: step 2895, loss 0.286545, acc 0.90625, learning_rate 0.000100035
2017-09-29T11:06:04.715372: step 2896, loss 0.298286, acc 0.875, learning_rate 0.000100035
2017-09-29T11:06:04.903946: step 2897, loss 0.159573, acc 0.9375, learning_rate 0.000100035
2017-09-29T11:06:05.095614: step 2898, loss 0.315171, acc 0.90625, learning_rate 0.000100035
2017-09-29T11:06:05.294921: step 2899, loss 0.40089, acc 0.875, learning_rate 0.000100035
2017-09-29T11:06:05.500163: step 2900, loss 0.305386, acc 0.875, learning_rate 0.000100035
2017-09-29T11:06:05.685341: step 2901, loss 0.229724, acc 0.9375, learning_rate 0.000100035
2017-09-29T11:06:05.872811: step 2902, loss 0.280415, acc 0.9375, learning_rate 0.000100034
2017-09-29T11:06:06.062839: step 2903, loss 0.437383, acc 0.890625, learning_rate 0.000100034
2017-09-29T11:06:06.254469: step 2904, loss 0.264338, acc 0.921875, learning_rate 0.000100034
2017-09-29T11:06:06.449785: step 2905, loss 0.250444, acc 0.890625, learning_rate 0.000100034
2017-09-29T11:06:06.642263: step 2906, loss 0.214697, acc 0.9375, learning_rate 0.000100034
2017-09-29T11:06:06.833965: step 2907, loss 0.446335, acc 0.890625, learning_rate 0.000100034
2017-09-29T11:06:07.026300: step 2908, loss 0.259529, acc 0.890625, learning_rate 0.000100034
2017-09-29T11:06:07.217897: step 2909, loss 0.275004, acc 0.90625, learning_rate 0.000100033
2017-09-29T11:06:07.413132: step 2910, loss 0.146861, acc 0.953125, learning_rate 0.000100033
2017-09-29T11:06:07.602852: step 2911, loss 0.364415, acc 0.859375, learning_rate 0.000100033
2017-09-29T11:06:07.780123: step 2912, loss 0.269884, acc 0.953125, learning_rate 0.000100033
2017-09-29T11:06:07.964935: step 2913, loss 0.32688, acc 0.859375, learning_rate 0.000100033
2017-09-29T11:06:08.149091: step 2914, loss 0.258349, acc 0.90625, learning_rate 0.000100033
2017-09-29T11:06:08.330642: step 2915, loss 0.201354, acc 0.921875, learning_rate 0.000100033
2017-09-29T11:06:08.512412: step 2916, loss 0.393139, acc 0.8125, learning_rate 0.000100033
2017-09-29T11:06:08.698161: step 2917, loss 0.222681, acc 0.921875, learning_rate 0.000100032
2017-09-29T11:06:08.883201: step 2918, loss 0.192243, acc 0.953125, learning_rate 0.000100032
2017-09-29T11:06:09.077589: step 2919, loss 0.235702, acc 0.921875, learning_rate 0.000100032
2017-09-29T11:06:09.263919: step 2920, loss 0.270931, acc 0.921875, learning_rate 0.000100032

Evaluation:
2017-09-29T11:06:09.830988: step 2920, loss 0.32542, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2920

2017-09-29T11:06:10.626987: step 2921, loss 0.228184, acc 0.9375, learning_rate 0.000100032
2017-09-29T11:06:10.813654: step 2922, loss 0.241781, acc 0.921875, learning_rate 0.000100032
2017-09-29T11:06:10.998993: step 2923, loss 0.427806, acc 0.828125, learning_rate 0.000100032
2017-09-29T11:06:11.189472: step 2924, loss 0.170733, acc 0.953125, learning_rate 0.000100031
2017-09-29T11:06:11.373540: step 2925, loss 0.153216, acc 0.96875, learning_rate 0.000100031
2017-09-29T11:06:11.559203: step 2926, loss 0.326988, acc 0.90625, learning_rate 0.000100031
2017-09-29T11:06:11.743867: step 2927, loss 0.355467, acc 0.9375, learning_rate 0.000100031
2017-09-29T11:06:11.925895: step 2928, loss 0.236009, acc 0.9375, learning_rate 0.000100031
2017-09-29T11:06:12.107474: step 2929, loss 0.141358, acc 0.984375, learning_rate 0.000100031
2017-09-29T11:06:12.293605: step 2930, loss 0.511283, acc 0.78125, learning_rate 0.000100031
2017-09-29T11:06:12.480953: step 2931, loss 0.376894, acc 0.875, learning_rate 0.000100031
2017-09-29T11:06:12.663047: step 2932, loss 0.235705, acc 0.96875, learning_rate 0.00010003
2017-09-29T11:06:12.844649: step 2933, loss 0.225214, acc 0.921875, learning_rate 0.00010003
2017-09-29T11:06:13.029248: step 2934, loss 0.391386, acc 0.875, learning_rate 0.00010003
2017-09-29T11:06:13.212028: step 2935, loss 0.176708, acc 0.90625, learning_rate 0.00010003
2017-09-29T11:06:13.397495: step 2936, loss 0.263431, acc 0.9375, learning_rate 0.00010003
2017-09-29T11:06:13.581568: step 2937, loss 0.355439, acc 0.859375, learning_rate 0.00010003
2017-09-29T11:06:13.763550: step 2938, loss 0.243343, acc 0.90625, learning_rate 0.00010003
2017-09-29T11:06:13.942854: step 2939, loss 0.189191, acc 0.9375, learning_rate 0.00010003
2017-09-29T11:06:14.092997: step 2940, loss 0.195296, acc 0.941176, learning_rate 0.000100029
2017-09-29T11:06:14.274437: step 2941, loss 0.142691, acc 0.9375, learning_rate 0.000100029
2017-09-29T11:06:14.455118: step 2942, loss 0.146116, acc 0.9375, learning_rate 0.000100029
2017-09-29T11:06:14.638294: step 2943, loss 0.339049, acc 0.859375, learning_rate 0.000100029
2017-09-29T11:06:14.827551: step 2944, loss 0.152588, acc 0.953125, learning_rate 0.000100029
2017-09-29T11:06:15.030141: step 2945, loss 0.257861, acc 0.90625, learning_rate 0.000100029
2017-09-29T11:06:15.215777: step 2946, loss 0.26742, acc 0.953125, learning_rate 0.000100029
2017-09-29T11:06:15.414145: step 2947, loss 0.280564, acc 0.890625, learning_rate 0.000100029
2017-09-29T11:06:15.614112: step 2948, loss 0.306394, acc 0.859375, learning_rate 0.000100029
2017-09-29T11:06:15.795094: step 2949, loss 0.293208, acc 0.90625, learning_rate 0.000100028
2017-09-29T11:06:15.980559: step 2950, loss 0.364935, acc 0.890625, learning_rate 0.000100028
2017-09-29T11:06:16.165240: step 2951, loss 0.254613, acc 0.921875, learning_rate 0.000100028
2017-09-29T11:06:16.348749: step 2952, loss 0.356256, acc 0.875, learning_rate 0.000100028
2017-09-29T11:06:16.540276: step 2953, loss 0.280154, acc 0.90625, learning_rate 0.000100028
2017-09-29T11:06:16.726175: step 2954, loss 0.271939, acc 0.890625, learning_rate 0.000100028
2017-09-29T11:06:16.909484: step 2955, loss 0.308206, acc 0.921875, learning_rate 0.000100028
2017-09-29T11:06:17.093510: step 2956, loss 0.272183, acc 0.90625, learning_rate 0.000100028
2017-09-29T11:06:17.280762: step 2957, loss 0.286814, acc 0.9375, learning_rate 0.000100028
2017-09-29T11:06:17.466899: step 2958, loss 0.336361, acc 0.859375, learning_rate 0.000100027
2017-09-29T11:06:17.653411: step 2959, loss 0.310743, acc 0.828125, learning_rate 0.000100027
2017-09-29T11:06:17.838488: step 2960, loss 0.266502, acc 0.90625, learning_rate 0.000100027

Evaluation:
2017-09-29T11:06:18.383391: step 2960, loss 0.312009, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-2960

2017-09-29T11:06:19.012368: step 2961, loss 0.391366, acc 0.859375, learning_rate 0.000100027
2017-09-29T11:06:19.194306: step 2962, loss 0.172189, acc 0.9375, learning_rate 0.000100027
2017-09-29T11:06:19.379967: step 2963, loss 0.259514, acc 0.90625, learning_rate 0.000100027
2017-09-29T11:06:19.565187: step 2964, loss 0.217273, acc 0.90625, learning_rate 0.000100027
2017-09-29T11:06:19.749696: step 2965, loss 0.2421, acc 0.9375, learning_rate 0.000100027
2017-09-29T11:06:19.932962: step 2966, loss 0.344908, acc 0.875, learning_rate 0.000100027
2017-09-29T11:06:20.117534: step 2967, loss 0.278513, acc 0.875, learning_rate 0.000100026
2017-09-29T11:06:20.302385: step 2968, loss 0.218423, acc 0.890625, learning_rate 0.000100026
2017-09-29T11:06:20.493559: step 2969, loss 0.231942, acc 0.921875, learning_rate 0.000100026
2017-09-29T11:06:20.690911: step 2970, loss 0.342159, acc 0.890625, learning_rate 0.000100026
2017-09-29T11:06:20.875556: step 2971, loss 0.204265, acc 0.921875, learning_rate 0.000100026
2017-09-29T11:06:21.058009: step 2972, loss 0.409299, acc 0.8125, learning_rate 0.000100026
2017-09-29T11:06:21.242564: step 2973, loss 0.234899, acc 0.90625, learning_rate 0.000100026
2017-09-29T11:06:21.431969: step 2974, loss 0.196954, acc 0.9375, learning_rate 0.000100026
2017-09-29T11:06:21.618422: step 2975, loss 0.317088, acc 0.875, learning_rate 0.000100026
2017-09-29T11:06:21.803232: step 2976, loss 0.297417, acc 0.90625, learning_rate 0.000100025
2017-09-29T11:06:21.984506: step 2977, loss 0.270627, acc 0.90625, learning_rate 0.000100025
2017-09-29T11:06:22.167644: step 2978, loss 0.414651, acc 0.84375, learning_rate 0.000100025
2017-09-29T11:06:22.354403: step 2979, loss 0.210633, acc 0.96875, learning_rate 0.000100025
2017-09-29T11:06:22.538536: step 2980, loss 0.284119, acc 0.9375, learning_rate 0.000100025
2017-09-29T11:06:22.726711: step 2981, loss 0.195978, acc 0.953125, learning_rate 0.000100025
2017-09-29T11:06:22.914398: step 2982, loss 0.273698, acc 0.890625, learning_rate 0.000100025
2017-09-29T11:06:23.103456: step 2983, loss 0.193365, acc 0.921875, learning_rate 0.000100025
2017-09-29T11:06:23.295876: step 2984, loss 0.173769, acc 0.9375, learning_rate 0.000100025
2017-09-29T11:06:23.494514: step 2985, loss 0.20865, acc 0.96875, learning_rate 0.000100025
2017-09-29T11:06:23.682035: step 2986, loss 0.179164, acc 0.96875, learning_rate 0.000100024
2017-09-29T11:06:23.864124: step 2987, loss 0.293142, acc 0.90625, learning_rate 0.000100024
2017-09-29T11:06:24.047416: step 2988, loss 0.18158, acc 0.953125, learning_rate 0.000100024
2017-09-29T11:06:24.230666: step 2989, loss 0.243731, acc 0.9375, learning_rate 0.000100024
2017-09-29T11:06:24.413996: step 2990, loss 0.293199, acc 0.890625, learning_rate 0.000100024
2017-09-29T11:06:24.598853: step 2991, loss 0.251061, acc 0.9375, learning_rate 0.000100024
2017-09-29T11:06:24.783488: step 2992, loss 0.242856, acc 0.890625, learning_rate 0.000100024
2017-09-29T11:06:24.972878: step 2993, loss 0.275113, acc 0.90625, learning_rate 0.000100024
2017-09-29T11:06:25.164317: step 2994, loss 0.198945, acc 0.921875, learning_rate 0.000100024
2017-09-29T11:06:25.359223: step 2995, loss 0.259701, acc 0.875, learning_rate 0.000100024
2017-09-29T11:06:25.553259: step 2996, loss 0.197643, acc 0.953125, learning_rate 0.000100023
2017-09-29T11:06:25.735617: step 2997, loss 0.271408, acc 0.890625, learning_rate 0.000100023
2017-09-29T11:06:25.918299: step 2998, loss 0.156913, acc 0.96875, learning_rate 0.000100023
2017-09-29T11:06:26.110507: step 2999, loss 0.262564, acc 0.921875, learning_rate 0.000100023
2017-09-29T11:06:26.295518: step 3000, loss 0.207802, acc 0.921875, learning_rate 0.000100023

Evaluation:
2017-09-29T11:06:26.865093: step 3000, loss 0.317505, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3000

2017-09-29T11:06:27.579804: step 3001, loss 0.297785, acc 0.90625, learning_rate 0.000100023
2017-09-29T11:06:27.768631: step 3002, loss 0.251994, acc 0.90625, learning_rate 0.000100023
2017-09-29T11:06:27.953016: step 3003, loss 0.258009, acc 0.921875, learning_rate 0.000100023
2017-09-29T11:06:28.135919: step 3004, loss 0.192987, acc 0.921875, learning_rate 0.000100023
2017-09-29T11:06:28.319053: step 3005, loss 0.211647, acc 0.921875, learning_rate 0.000100023
2017-09-29T11:06:28.503600: step 3006, loss 0.289257, acc 0.90625, learning_rate 0.000100023
2017-09-29T11:06:28.690529: step 3007, loss 0.244972, acc 0.890625, learning_rate 0.000100022
2017-09-29T11:06:28.873646: step 3008, loss 0.300256, acc 0.90625, learning_rate 0.000100022
2017-09-29T11:06:29.063597: step 3009, loss 0.18216, acc 0.9375, learning_rate 0.000100022
2017-09-29T11:06:29.251516: step 3010, loss 0.182523, acc 0.9375, learning_rate 0.000100022
2017-09-29T11:06:29.443756: step 3011, loss 0.402476, acc 0.890625, learning_rate 0.000100022
2017-09-29T11:06:29.637009: step 3012, loss 0.261683, acc 0.921875, learning_rate 0.000100022
2017-09-29T11:06:29.822005: step 3013, loss 0.247669, acc 0.9375, learning_rate 0.000100022
2017-09-29T11:06:30.011441: step 3014, loss 0.318191, acc 0.875, learning_rate 0.000100022
2017-09-29T11:06:30.196911: step 3015, loss 0.356696, acc 0.875, learning_rate 0.000100022
2017-09-29T11:06:30.386345: step 3016, loss 0.374401, acc 0.875, learning_rate 0.000100022
2017-09-29T11:06:30.591693: step 3017, loss 0.270289, acc 0.890625, learning_rate 0.000100022
2017-09-29T11:06:30.776353: step 3018, loss 0.335657, acc 0.875, learning_rate 0.000100021
2017-09-29T11:06:30.957151: step 3019, loss 0.230767, acc 0.90625, learning_rate 0.000100021
2017-09-29T11:06:31.143276: step 3020, loss 0.212082, acc 0.90625, learning_rate 0.000100021
2017-09-29T11:06:31.323593: step 3021, loss 0.215232, acc 0.9375, learning_rate 0.000100021
2017-09-29T11:06:31.507523: step 3022, loss 0.168987, acc 0.953125, learning_rate 0.000100021
2017-09-29T11:06:31.690769: step 3023, loss 0.199933, acc 0.953125, learning_rate 0.000100021
2017-09-29T11:06:31.877447: step 3024, loss 0.53964, acc 0.859375, learning_rate 0.000100021
2017-09-29T11:06:32.063361: step 3025, loss 0.291989, acc 0.859375, learning_rate 0.000100021
2017-09-29T11:06:32.259677: step 3026, loss 0.35091, acc 0.84375, learning_rate 0.000100021
2017-09-29T11:06:32.459125: step 3027, loss 0.286993, acc 0.890625, learning_rate 0.000100021
2017-09-29T11:06:32.651621: step 3028, loss 0.326991, acc 0.828125, learning_rate 0.000100021
2017-09-29T11:06:32.839009: step 3029, loss 0.280165, acc 0.90625, learning_rate 0.00010002
2017-09-29T11:06:33.026604: step 3030, loss 0.324211, acc 0.890625, learning_rate 0.00010002
2017-09-29T11:06:33.215279: step 3031, loss 0.219953, acc 0.90625, learning_rate 0.00010002
2017-09-29T11:06:33.402167: step 3032, loss 0.241352, acc 0.921875, learning_rate 0.00010002
2017-09-29T11:06:33.583029: step 3033, loss 0.34676, acc 0.84375, learning_rate 0.00010002
2017-09-29T11:06:33.766530: step 3034, loss 0.245532, acc 0.921875, learning_rate 0.00010002
2017-09-29T11:06:33.947518: step 3035, loss 0.313393, acc 0.890625, learning_rate 0.00010002
2017-09-29T11:06:34.128088: step 3036, loss 0.151377, acc 0.953125, learning_rate 0.00010002
2017-09-29T11:06:34.316105: step 3037, loss 0.277666, acc 0.875, learning_rate 0.00010002
2017-09-29T11:06:34.473190: step 3038, loss 0.43577, acc 0.784314, learning_rate 0.00010002
2017-09-29T11:06:34.655609: step 3039, loss 0.173083, acc 0.9375, learning_rate 0.00010002
2017-09-29T11:06:34.840679: step 3040, loss 0.153806, acc 0.96875, learning_rate 0.00010002

Evaluation:
2017-09-29T11:06:35.415622: step 3040, loss 0.322564, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3040

2017-09-29T11:06:36.135597: step 3041, loss 0.207399, acc 0.953125, learning_rate 0.00010002
2017-09-29T11:06:36.329043: step 3042, loss 0.290814, acc 0.890625, learning_rate 0.000100019
2017-09-29T11:06:36.520713: step 3043, loss 0.357697, acc 0.859375, learning_rate 0.000100019
2017-09-29T11:06:36.711881: step 3044, loss 0.12446, acc 0.96875, learning_rate 0.000100019
2017-09-29T11:06:36.900390: step 3045, loss 0.191459, acc 0.96875, learning_rate 0.000100019
2017-09-29T11:06:37.087754: step 3046, loss 0.195192, acc 0.90625, learning_rate 0.000100019
2017-09-29T11:06:37.273769: step 3047, loss 0.214122, acc 0.953125, learning_rate 0.000100019
2017-09-29T11:06:37.465771: step 3048, loss 0.167802, acc 0.921875, learning_rate 0.000100019
2017-09-29T11:06:37.656528: step 3049, loss 0.245973, acc 0.890625, learning_rate 0.000100019
2017-09-29T11:06:37.853313: step 3050, loss 0.268475, acc 0.921875, learning_rate 0.000100019
2017-09-29T11:06:38.036938: step 3051, loss 0.173947, acc 0.9375, learning_rate 0.000100019
2017-09-29T11:06:38.218350: step 3052, loss 0.257107, acc 0.9375, learning_rate 0.000100019
2017-09-29T11:06:38.411449: step 3053, loss 0.314387, acc 0.875, learning_rate 0.000100019
2017-09-29T11:06:38.594712: step 3054, loss 0.35118, acc 0.859375, learning_rate 0.000100018
2017-09-29T11:06:38.776321: step 3055, loss 0.222603, acc 0.921875, learning_rate 0.000100018
2017-09-29T11:06:38.960477: step 3056, loss 0.310034, acc 0.890625, learning_rate 0.000100018
2017-09-29T11:06:39.146286: step 3057, loss 0.278707, acc 0.875, learning_rate 0.000100018
2017-09-29T11:06:39.332795: step 3058, loss 0.214667, acc 0.9375, learning_rate 0.000100018
2017-09-29T11:06:39.521037: step 3059, loss 0.197304, acc 0.953125, learning_rate 0.000100018
2017-09-29T11:06:39.703988: step 3060, loss 0.428996, acc 0.859375, learning_rate 0.000100018
2017-09-29T11:06:39.884999: step 3061, loss 0.265135, acc 0.890625, learning_rate 0.000100018
2017-09-29T11:06:40.074332: step 3062, loss 0.418257, acc 0.859375, learning_rate 0.000100018
2017-09-29T11:06:40.257594: step 3063, loss 0.176373, acc 0.953125, learning_rate 0.000100018
2017-09-29T11:06:40.453046: step 3064, loss 0.251495, acc 0.921875, learning_rate 0.000100018
2017-09-29T11:06:40.647304: step 3065, loss 0.168315, acc 0.953125, learning_rate 0.000100018
2017-09-29T11:06:40.827637: step 3066, loss 0.292386, acc 0.90625, learning_rate 0.000100018
2017-09-29T11:06:41.016612: step 3067, loss 0.297185, acc 0.90625, learning_rate 0.000100018
2017-09-29T11:06:41.201832: step 3068, loss 0.411579, acc 0.828125, learning_rate 0.000100017
2017-09-29T11:06:41.389278: step 3069, loss 0.316687, acc 0.890625, learning_rate 0.000100017
2017-09-29T11:06:41.578045: step 3070, loss 0.273276, acc 0.90625, learning_rate 0.000100017
2017-09-29T11:06:41.773949: step 3071, loss 0.215287, acc 0.90625, learning_rate 0.000100017
2017-09-29T11:06:41.961491: step 3072, loss 0.250241, acc 0.890625, learning_rate 0.000100017
2017-09-29T11:06:42.150908: step 3073, loss 0.336582, acc 0.859375, learning_rate 0.000100017
2017-09-29T11:06:42.342074: step 3074, loss 0.353602, acc 0.875, learning_rate 0.000100017
2017-09-29T11:06:42.532348: step 3075, loss 0.245657, acc 0.921875, learning_rate 0.000100017
2017-09-29T11:06:42.720039: step 3076, loss 0.208251, acc 0.9375, learning_rate 0.000100017
2017-09-29T11:06:42.905613: step 3077, loss 0.240565, acc 0.90625, learning_rate 0.000100017
2017-09-29T11:06:43.093256: step 3078, loss 0.227992, acc 0.90625, learning_rate 0.000100017
2017-09-29T11:06:43.278694: step 3079, loss 0.352049, acc 0.875, learning_rate 0.000100017
2017-09-29T11:06:43.474713: step 3080, loss 0.205515, acc 0.9375, learning_rate 0.000100017

Evaluation:
2017-09-29T11:06:44.054159: step 3080, loss 0.31724, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3080

2017-09-29T11:06:44.838165: step 3081, loss 0.235763, acc 0.875, learning_rate 0.000100017
2017-09-29T11:06:45.021778: step 3082, loss 0.279663, acc 0.890625, learning_rate 0.000100016
2017-09-29T11:06:45.215361: step 3083, loss 0.266965, acc 0.90625, learning_rate 0.000100016
2017-09-29T11:06:45.399401: step 3084, loss 0.285474, acc 0.90625, learning_rate 0.000100016
2017-09-29T11:06:45.580836: step 3085, loss 0.253799, acc 0.875, learning_rate 0.000100016
2017-09-29T11:06:45.774495: step 3086, loss 0.269839, acc 0.890625, learning_rate 0.000100016
2017-09-29T11:06:45.955719: step 3087, loss 0.232014, acc 0.953125, learning_rate 0.000100016
2017-09-29T11:06:46.148598: step 3088, loss 0.390952, acc 0.859375, learning_rate 0.000100016
2017-09-29T11:06:46.334749: step 3089, loss 0.121584, acc 0.953125, learning_rate 0.000100016
2017-09-29T11:06:46.527059: step 3090, loss 0.299657, acc 0.875, learning_rate 0.000100016
2017-09-29T11:06:46.709770: step 3091, loss 0.192993, acc 0.921875, learning_rate 0.000100016
2017-09-29T11:06:46.891672: step 3092, loss 0.249632, acc 0.890625, learning_rate 0.000100016
2017-09-29T11:06:47.073383: step 3093, loss 0.308373, acc 0.921875, learning_rate 0.000100016
2017-09-29T11:06:47.256129: step 3094, loss 0.320819, acc 0.890625, learning_rate 0.000100016
2017-09-29T11:06:47.439558: step 3095, loss 0.229133, acc 0.921875, learning_rate 0.000100016
2017-09-29T11:06:47.621749: step 3096, loss 0.292428, acc 0.890625, learning_rate 0.000100016
2017-09-29T11:06:47.804946: step 3097, loss 0.262342, acc 0.890625, learning_rate 0.000100016
2017-09-29T11:06:47.987093: step 3098, loss 0.304002, acc 0.890625, learning_rate 0.000100015
2017-09-29T11:06:48.169306: step 3099, loss 0.221969, acc 0.9375, learning_rate 0.000100015
2017-09-29T11:06:48.353908: step 3100, loss 0.261451, acc 0.9375, learning_rate 0.000100015
2017-09-29T11:06:48.540472: step 3101, loss 0.223187, acc 0.90625, learning_rate 0.000100015
2017-09-29T11:06:48.735492: step 3102, loss 0.470835, acc 0.828125, learning_rate 0.000100015
2017-09-29T11:06:48.925662: step 3103, loss 0.420608, acc 0.828125, learning_rate 0.000100015
2017-09-29T11:06:49.112994: step 3104, loss 0.180022, acc 0.9375, learning_rate 0.000100015
2017-09-29T11:06:49.297857: step 3105, loss 0.227959, acc 0.90625, learning_rate 0.000100015
2017-09-29T11:06:49.494717: step 3106, loss 0.331295, acc 0.890625, learning_rate 0.000100015
2017-09-29T11:06:49.677287: step 3107, loss 0.266047, acc 0.90625, learning_rate 0.000100015
2017-09-29T11:06:49.859914: step 3108, loss 0.37224, acc 0.890625, learning_rate 0.000100015
2017-09-29T11:06:50.049144: step 3109, loss 0.331313, acc 0.890625, learning_rate 0.000100015
2017-09-29T11:06:50.239404: step 3110, loss 0.376938, acc 0.875, learning_rate 0.000100015
2017-09-29T11:06:50.426954: step 3111, loss 0.292988, acc 0.9375, learning_rate 0.000100015
2017-09-29T11:06:50.609300: step 3112, loss 0.338833, acc 0.890625, learning_rate 0.000100015
2017-09-29T11:06:50.796911: step 3113, loss 0.283175, acc 0.875, learning_rate 0.000100015
2017-09-29T11:06:50.977343: step 3114, loss 0.308919, acc 0.875, learning_rate 0.000100014
2017-09-29T11:06:51.158891: step 3115, loss 0.27173, acc 0.9375, learning_rate 0.000100014
2017-09-29T11:06:51.341780: step 3116, loss 0.337802, acc 0.90625, learning_rate 0.000100014
2017-09-29T11:06:51.529739: step 3117, loss 0.293386, acc 0.84375, learning_rate 0.000100014
2017-09-29T11:06:51.715158: step 3118, loss 0.31811, acc 0.875, learning_rate 0.000100014
2017-09-29T11:06:51.900384: step 3119, loss 0.290704, acc 0.890625, learning_rate 0.000100014
2017-09-29T11:06:52.080453: step 3120, loss 0.347306, acc 0.875, learning_rate 0.000100014

Evaluation:
2017-09-29T11:06:52.645585: step 3120, loss 0.315195, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3120

2017-09-29T11:06:53.264865: step 3121, loss 0.199541, acc 0.921875, learning_rate 0.000100014
2017-09-29T11:06:53.456198: step 3122, loss 0.246568, acc 0.921875, learning_rate 0.000100014
2017-09-29T11:06:53.642598: step 3123, loss 0.241695, acc 0.875, learning_rate 0.000100014
2017-09-29T11:06:53.825492: step 3124, loss 0.221845, acc 0.90625, learning_rate 0.000100014
2017-09-29T11:06:54.013118: step 3125, loss 0.28468, acc 0.859375, learning_rate 0.000100014
2017-09-29T11:06:54.195859: step 3126, loss 0.248182, acc 0.90625, learning_rate 0.000100014
2017-09-29T11:06:54.377664: step 3127, loss 0.258003, acc 0.921875, learning_rate 0.000100014
2017-09-29T11:06:54.560003: step 3128, loss 0.310937, acc 0.84375, learning_rate 0.000100014
2017-09-29T11:06:54.744372: step 3129, loss 0.188291, acc 0.9375, learning_rate 0.000100014
2017-09-29T11:06:54.928601: step 3130, loss 0.262982, acc 0.890625, learning_rate 0.000100014
2017-09-29T11:06:55.120955: step 3131, loss 0.270561, acc 0.890625, learning_rate 0.000100014
2017-09-29T11:06:55.305137: step 3132, loss 0.238314, acc 0.90625, learning_rate 0.000100013
2017-09-29T11:06:55.500018: step 3133, loss 0.170922, acc 0.9375, learning_rate 0.000100013
2017-09-29T11:06:55.694681: step 3134, loss 0.34611, acc 0.859375, learning_rate 0.000100013
2017-09-29T11:06:55.877316: step 3135, loss 0.40211, acc 0.84375, learning_rate 0.000100013
2017-09-29T11:06:56.030119: step 3136, loss 0.182274, acc 0.941176, learning_rate 0.000100013
2017-09-29T11:06:56.218657: step 3137, loss 0.203689, acc 0.921875, learning_rate 0.000100013
2017-09-29T11:06:56.401960: step 3138, loss 0.297328, acc 0.890625, learning_rate 0.000100013
2017-09-29T11:06:56.591419: step 3139, loss 0.23192, acc 0.9375, learning_rate 0.000100013
2017-09-29T11:06:56.774758: step 3140, loss 0.217972, acc 0.953125, learning_rate 0.000100013
2017-09-29T11:06:56.962776: step 3141, loss 0.331104, acc 0.890625, learning_rate 0.000100013
2017-09-29T11:06:57.146816: step 3142, loss 0.158736, acc 0.953125, learning_rate 0.000100013
2017-09-29T11:06:57.334573: step 3143, loss 0.200769, acc 0.921875, learning_rate 0.000100013
2017-09-29T11:06:57.521559: step 3144, loss 0.225828, acc 0.90625, learning_rate 0.000100013
2017-09-29T11:06:57.704816: step 3145, loss 0.202475, acc 0.9375, learning_rate 0.000100013
2017-09-29T11:06:57.888253: step 3146, loss 0.182475, acc 0.9375, learning_rate 0.000100013
2017-09-29T11:06:58.071526: step 3147, loss 0.297767, acc 0.90625, learning_rate 0.000100013
2017-09-29T11:06:58.254132: step 3148, loss 0.364134, acc 0.890625, learning_rate 0.000100013
2017-09-29T11:06:58.439052: step 3149, loss 0.212106, acc 0.875, learning_rate 0.000100013
2017-09-29T11:06:58.623144: step 3150, loss 0.206221, acc 0.921875, learning_rate 0.000100012
2017-09-29T11:06:58.821071: step 3151, loss 0.16534, acc 0.953125, learning_rate 0.000100012
2017-09-29T11:06:59.006492: step 3152, loss 0.251949, acc 0.921875, learning_rate 0.000100012
2017-09-29T11:06:59.188795: step 3153, loss 0.378422, acc 0.84375, learning_rate 0.000100012
2017-09-29T11:06:59.377018: step 3154, loss 0.3596, acc 0.859375, learning_rate 0.000100012
2017-09-29T11:06:59.562602: step 3155, loss 0.242775, acc 0.90625, learning_rate 0.000100012
2017-09-29T11:06:59.754546: step 3156, loss 0.187355, acc 0.90625, learning_rate 0.000100012
2017-09-29T11:06:59.937904: step 3157, loss 0.292268, acc 0.90625, learning_rate 0.000100012
2017-09-29T11:07:00.126512: step 3158, loss 0.276304, acc 0.890625, learning_rate 0.000100012
2017-09-29T11:07:00.313251: step 3159, loss 0.230793, acc 0.90625, learning_rate 0.000100012
2017-09-29T11:07:00.495724: step 3160, loss 0.405802, acc 0.84375, learning_rate 0.000100012

Evaluation:
2017-09-29T11:07:01.057452: step 3160, loss 0.315273, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3160

2017-09-29T11:07:01.785210: step 3161, loss 0.241827, acc 0.890625, learning_rate 0.000100012
2017-09-29T11:07:01.966461: step 3162, loss 0.302787, acc 0.875, learning_rate 0.000100012
2017-09-29T11:07:02.150370: step 3163, loss 0.288977, acc 0.875, learning_rate 0.000100012
2017-09-29T11:07:02.331426: step 3164, loss 0.32767, acc 0.875, learning_rate 0.000100012
2017-09-29T11:07:02.515988: step 3165, loss 0.178835, acc 0.953125, learning_rate 0.000100012
2017-09-29T11:07:02.695454: step 3166, loss 0.463911, acc 0.875, learning_rate 0.000100012
2017-09-29T11:07:02.877682: step 3167, loss 0.283672, acc 0.921875, learning_rate 0.000100012
2017-09-29T11:07:03.062541: step 3168, loss 0.14174, acc 0.953125, learning_rate 0.000100012
2017-09-29T11:07:03.243701: step 3169, loss 0.265352, acc 0.921875, learning_rate 0.000100012
2017-09-29T11:07:03.428692: step 3170, loss 0.268911, acc 0.875, learning_rate 0.000100012
2017-09-29T11:07:03.621917: step 3171, loss 0.263509, acc 0.90625, learning_rate 0.000100011
2017-09-29T11:07:03.803758: step 3172, loss 0.206585, acc 0.9375, learning_rate 0.000100011
2017-09-29T11:07:03.989836: step 3173, loss 0.34112, acc 0.890625, learning_rate 0.000100011
2017-09-29T11:07:04.172465: step 3174, loss 0.152731, acc 0.984375, learning_rate 0.000100011
2017-09-29T11:07:04.368180: step 3175, loss 0.312697, acc 0.953125, learning_rate 0.000100011
2017-09-29T11:07:04.553423: step 3176, loss 0.343352, acc 0.875, learning_rate 0.000100011
2017-09-29T11:07:04.734036: step 3177, loss 0.200961, acc 0.96875, learning_rate 0.000100011
2017-09-29T11:07:04.917426: step 3178, loss 0.463908, acc 0.8125, learning_rate 0.000100011
2017-09-29T11:07:05.101708: step 3179, loss 0.256053, acc 0.90625, learning_rate 0.000100011
2017-09-29T11:07:05.289148: step 3180, loss 0.287732, acc 0.90625, learning_rate 0.000100011
2017-09-29T11:07:05.471884: step 3181, loss 0.255491, acc 0.890625, learning_rate 0.000100011
2017-09-29T11:07:05.650027: step 3182, loss 0.261277, acc 0.90625, learning_rate 0.000100011
2017-09-29T11:07:05.838499: step 3183, loss 0.338195, acc 0.859375, learning_rate 0.000100011
2017-09-29T11:07:06.024136: step 3184, loss 0.256544, acc 0.90625, learning_rate 0.000100011
2017-09-29T11:07:06.207817: step 3185, loss 0.233501, acc 0.921875, learning_rate 0.000100011
2017-09-29T11:07:06.392767: step 3186, loss 0.201184, acc 0.921875, learning_rate 0.000100011
2017-09-29T11:07:06.590584: step 3187, loss 0.173957, acc 0.9375, learning_rate 0.000100011
2017-09-29T11:07:06.775272: step 3188, loss 0.258738, acc 0.921875, learning_rate 0.000100011
2017-09-29T11:07:06.961013: step 3189, loss 0.243865, acc 0.921875, learning_rate 0.000100011
2017-09-29T11:07:07.146153: step 3190, loss 0.272147, acc 0.90625, learning_rate 0.000100011
2017-09-29T11:07:07.328758: step 3191, loss 0.269138, acc 0.890625, learning_rate 0.000100011
2017-09-29T11:07:07.511668: step 3192, loss 0.410593, acc 0.84375, learning_rate 0.000100011
2017-09-29T11:07:07.698303: step 3193, loss 0.415782, acc 0.828125, learning_rate 0.00010001
2017-09-29T11:07:07.893883: step 3194, loss 0.263785, acc 0.859375, learning_rate 0.00010001
2017-09-29T11:07:08.073528: step 3195, loss 0.259833, acc 0.921875, learning_rate 0.00010001
2017-09-29T11:07:08.256099: step 3196, loss 0.224726, acc 0.921875, learning_rate 0.00010001
2017-09-29T11:07:08.438609: step 3197, loss 0.180441, acc 0.921875, learning_rate 0.00010001
2017-09-29T11:07:08.620973: step 3198, loss 0.160358, acc 0.96875, learning_rate 0.00010001
2017-09-29T11:07:08.802548: step 3199, loss 0.195036, acc 0.9375, learning_rate 0.00010001
2017-09-29T11:07:08.985157: step 3200, loss 0.322547, acc 0.875, learning_rate 0.00010001

Evaluation:
2017-09-29T11:07:09.535007: step 3200, loss 0.317297, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3200

2017-09-29T11:07:10.244671: step 3201, loss 0.217076, acc 0.921875, learning_rate 0.00010001
2017-09-29T11:07:10.430478: step 3202, loss 0.131215, acc 0.96875, learning_rate 0.00010001
2017-09-29T11:07:10.619972: step 3203, loss 0.258005, acc 0.90625, learning_rate 0.00010001
2017-09-29T11:07:10.810306: step 3204, loss 0.23112, acc 0.953125, learning_rate 0.00010001
2017-09-29T11:07:10.991610: step 3205, loss 0.247303, acc 0.953125, learning_rate 0.00010001
2017-09-29T11:07:11.177156: step 3206, loss 0.200719, acc 0.9375, learning_rate 0.00010001
2017-09-29T11:07:11.359323: step 3207, loss 0.141466, acc 0.984375, learning_rate 0.00010001
2017-09-29T11:07:11.542631: step 3208, loss 0.186569, acc 0.9375, learning_rate 0.00010001
2017-09-29T11:07:11.723327: step 3209, loss 0.201476, acc 0.921875, learning_rate 0.00010001
2017-09-29T11:07:11.899872: step 3210, loss 0.2602, acc 0.921875, learning_rate 0.00010001
2017-09-29T11:07:12.082065: step 3211, loss 0.370188, acc 0.875, learning_rate 0.00010001
2017-09-29T11:07:12.263810: step 3212, loss 0.224015, acc 0.90625, learning_rate 0.00010001
2017-09-29T11:07:12.455145: step 3213, loss 0.219872, acc 0.96875, learning_rate 0.00010001
2017-09-29T11:07:12.648729: step 3214, loss 0.348562, acc 0.84375, learning_rate 0.00010001
2017-09-29T11:07:12.831202: step 3215, loss 0.200199, acc 0.90625, learning_rate 0.00010001
2017-09-29T11:07:13.012453: step 3216, loss 0.213192, acc 0.921875, learning_rate 0.00010001
2017-09-29T11:07:13.196616: step 3217, loss 0.206507, acc 0.9375, learning_rate 0.000100009
2017-09-29T11:07:13.390149: step 3218, loss 0.319345, acc 0.890625, learning_rate 0.000100009
2017-09-29T11:07:13.573584: step 3219, loss 0.270968, acc 0.921875, learning_rate 0.000100009
2017-09-29T11:07:13.755391: step 3220, loss 0.226085, acc 0.875, learning_rate 0.000100009
2017-09-29T11:07:13.939753: step 3221, loss 0.339593, acc 0.859375, learning_rate 0.000100009
2017-09-29T11:07:14.122901: step 3222, loss 0.258691, acc 0.921875, learning_rate 0.000100009
2017-09-29T11:07:14.306687: step 3223, loss 0.21664, acc 0.90625, learning_rate 0.000100009
2017-09-29T11:07:14.491754: step 3224, loss 0.198342, acc 0.953125, learning_rate 0.000100009
2017-09-29T11:07:14.674605: step 3225, loss 0.168558, acc 0.9375, learning_rate 0.000100009
2017-09-29T11:07:14.862801: step 3226, loss 0.258419, acc 0.90625, learning_rate 0.000100009
2017-09-29T11:07:15.046452: step 3227, loss 0.288867, acc 0.890625, learning_rate 0.000100009
2017-09-29T11:07:15.234614: step 3228, loss 0.159311, acc 0.953125, learning_rate 0.000100009
2017-09-29T11:07:15.423675: step 3229, loss 0.367451, acc 0.875, learning_rate 0.000100009
2017-09-29T11:07:15.605453: step 3230, loss 0.188175, acc 0.953125, learning_rate 0.000100009
2017-09-29T11:07:15.790120: step 3231, loss 0.355819, acc 0.875, learning_rate 0.000100009
2017-09-29T11:07:15.974415: step 3232, loss 0.340685, acc 0.875, learning_rate 0.000100009
2017-09-29T11:07:16.157382: step 3233, loss 0.177805, acc 0.921875, learning_rate 0.000100009
2017-09-29T11:07:16.317929: step 3234, loss 0.184045, acc 0.941176, learning_rate 0.000100009
2017-09-29T11:07:16.502220: step 3235, loss 0.373989, acc 0.890625, learning_rate 0.000100009
2017-09-29T11:07:16.686926: step 3236, loss 0.419192, acc 0.84375, learning_rate 0.000100009
2017-09-29T11:07:16.871438: step 3237, loss 0.186647, acc 0.96875, learning_rate 0.000100009
2017-09-29T11:07:17.055978: step 3238, loss 0.198104, acc 0.9375, learning_rate 0.000100009
2017-09-29T11:07:17.249514: step 3239, loss 0.134553, acc 0.984375, learning_rate 0.000100009
2017-09-29T11:07:17.432770: step 3240, loss 0.200191, acc 0.9375, learning_rate 0.000100009

Evaluation:
2017-09-29T11:07:17.997796: step 3240, loss 0.314232, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3240

2017-09-29T11:07:18.808025: step 3241, loss 0.212875, acc 0.90625, learning_rate 0.000100009
2017-09-29T11:07:18.990441: step 3242, loss 0.206599, acc 0.9375, learning_rate 0.000100009
2017-09-29T11:07:19.182821: step 3243, loss 0.273713, acc 0.9375, learning_rate 0.000100009
2017-09-29T11:07:19.365779: step 3244, loss 0.287804, acc 0.90625, learning_rate 0.000100009
2017-09-29T11:07:19.552030: step 3245, loss 0.202526, acc 0.921875, learning_rate 0.000100008
2017-09-29T11:07:19.755987: step 3246, loss 0.210363, acc 0.9375, learning_rate 0.000100008
2017-09-29T11:07:19.937079: step 3247, loss 0.222621, acc 0.953125, learning_rate 0.000100008
2017-09-29T11:07:20.120702: step 3248, loss 0.264726, acc 0.921875, learning_rate 0.000100008
2017-09-29T11:07:20.309482: step 3249, loss 0.160825, acc 0.953125, learning_rate 0.000100008
2017-09-29T11:07:20.500757: step 3250, loss 0.168196, acc 0.984375, learning_rate 0.000100008
2017-09-29T11:07:20.691060: step 3251, loss 0.289474, acc 0.875, learning_rate 0.000100008
2017-09-29T11:07:20.881334: step 3252, loss 0.28204, acc 0.9375, learning_rate 0.000100008
2017-09-29T11:07:21.063346: step 3253, loss 0.264615, acc 0.90625, learning_rate 0.000100008
2017-09-29T11:07:21.250280: step 3254, loss 0.256895, acc 0.875, learning_rate 0.000100008
2017-09-29T11:07:21.442577: step 3255, loss 0.165861, acc 0.9375, learning_rate 0.000100008
2017-09-29T11:07:21.628814: step 3256, loss 0.38986, acc 0.875, learning_rate 0.000100008
2017-09-29T11:07:21.812646: step 3257, loss 0.182798, acc 0.953125, learning_rate 0.000100008
2017-09-29T11:07:21.992681: step 3258, loss 0.174887, acc 0.953125, learning_rate 0.000100008
2017-09-29T11:07:22.180471: step 3259, loss 0.168612, acc 0.953125, learning_rate 0.000100008
2017-09-29T11:07:22.362849: step 3260, loss 0.247755, acc 0.875, learning_rate 0.000100008
2017-09-29T11:07:22.556146: step 3261, loss 0.222208, acc 0.90625, learning_rate 0.000100008
2017-09-29T11:07:22.737873: step 3262, loss 0.249902, acc 0.921875, learning_rate 0.000100008
2017-09-29T11:07:22.919848: step 3263, loss 0.429677, acc 0.8125, learning_rate 0.000100008
2017-09-29T11:07:23.100905: step 3264, loss 0.1714, acc 0.9375, learning_rate 0.000100008
2017-09-29T11:07:23.285758: step 3265, loss 0.196256, acc 0.921875, learning_rate 0.000100008
2017-09-29T11:07:23.471441: step 3266, loss 0.344104, acc 0.859375, learning_rate 0.000100008
2017-09-29T11:07:23.654896: step 3267, loss 0.18441, acc 0.921875, learning_rate 0.000100008
2017-09-29T11:07:23.842363: step 3268, loss 0.328318, acc 0.921875, learning_rate 0.000100008
2017-09-29T11:07:24.027333: step 3269, loss 0.266882, acc 0.890625, learning_rate 0.000100008
2017-09-29T11:07:24.212370: step 3270, loss 0.356513, acc 0.859375, learning_rate 0.000100008
2017-09-29T11:07:24.398700: step 3271, loss 0.388315, acc 0.890625, learning_rate 0.000100008
2017-09-29T11:07:24.586874: step 3272, loss 0.196766, acc 0.90625, learning_rate 0.000100008
2017-09-29T11:07:24.770046: step 3273, loss 0.301788, acc 0.890625, learning_rate 0.000100008
2017-09-29T11:07:24.953926: step 3274, loss 0.374296, acc 0.8125, learning_rate 0.000100008
2017-09-29T11:07:25.139404: step 3275, loss 0.239977, acc 0.9375, learning_rate 0.000100007
2017-09-29T11:07:25.326415: step 3276, loss 0.222968, acc 0.90625, learning_rate 0.000100007
2017-09-29T11:07:25.517835: step 3277, loss 0.238686, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:25.712022: step 3278, loss 0.30312, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:25.902611: step 3279, loss 0.3355, acc 0.90625, learning_rate 0.000100007
2017-09-29T11:07:26.086754: step 3280, loss 0.272265, acc 0.890625, learning_rate 0.000100007

Evaluation:
2017-09-29T11:07:26.637755: step 3280, loss 0.314068, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3280

2017-09-29T11:07:27.254363: step 3281, loss 0.289022, acc 0.890625, learning_rate 0.000100007
2017-09-29T11:07:27.437040: step 3282, loss 0.183837, acc 0.9375, learning_rate 0.000100007
2017-09-29T11:07:27.620514: step 3283, loss 0.144993, acc 0.9375, learning_rate 0.000100007
2017-09-29T11:07:27.814517: step 3284, loss 0.173579, acc 0.9375, learning_rate 0.000100007
2017-09-29T11:07:27.996124: step 3285, loss 0.311732, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:28.185387: step 3286, loss 0.189221, acc 0.9375, learning_rate 0.000100007
2017-09-29T11:07:28.373139: step 3287, loss 0.234522, acc 0.9375, learning_rate 0.000100007
2017-09-29T11:07:28.555603: step 3288, loss 0.189638, acc 0.96875, learning_rate 0.000100007
2017-09-29T11:07:28.737635: step 3289, loss 0.11305, acc 0.96875, learning_rate 0.000100007
2017-09-29T11:07:28.919668: step 3290, loss 0.150115, acc 0.9375, learning_rate 0.000100007
2017-09-29T11:07:29.103179: step 3291, loss 0.2741, acc 0.890625, learning_rate 0.000100007
2017-09-29T11:07:29.287463: step 3292, loss 0.129576, acc 0.96875, learning_rate 0.000100007
2017-09-29T11:07:29.482282: step 3293, loss 0.294685, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:29.666529: step 3294, loss 0.311731, acc 0.875, learning_rate 0.000100007
2017-09-29T11:07:29.848201: step 3295, loss 0.201372, acc 0.953125, learning_rate 0.000100007
2017-09-29T11:07:30.035473: step 3296, loss 0.235588, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:30.214905: step 3297, loss 0.246597, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:30.405928: step 3298, loss 0.284847, acc 0.875, learning_rate 0.000100007
2017-09-29T11:07:30.592892: step 3299, loss 0.292766, acc 0.875, learning_rate 0.000100007
2017-09-29T11:07:30.778988: step 3300, loss 0.246849, acc 0.9375, learning_rate 0.000100007
2017-09-29T11:07:30.970120: step 3301, loss 0.307704, acc 0.875, learning_rate 0.000100007
2017-09-29T11:07:31.154755: step 3302, loss 0.288625, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:31.337130: step 3303, loss 0.164096, acc 0.953125, learning_rate 0.000100007
2017-09-29T11:07:31.525218: step 3304, loss 0.264602, acc 0.890625, learning_rate 0.000100007
2017-09-29T11:07:31.709858: step 3305, loss 0.198972, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:31.897780: step 3306, loss 0.180437, acc 0.953125, learning_rate 0.000100007
2017-09-29T11:07:32.081134: step 3307, loss 0.148102, acc 0.953125, learning_rate 0.000100007
2017-09-29T11:07:32.266475: step 3308, loss 0.277065, acc 0.890625, learning_rate 0.000100007
2017-09-29T11:07:32.450881: step 3309, loss 0.211062, acc 0.921875, learning_rate 0.000100007
2017-09-29T11:07:32.632632: step 3310, loss 0.360054, acc 0.859375, learning_rate 0.000100006
2017-09-29T11:07:32.814794: step 3311, loss 0.199751, acc 0.90625, learning_rate 0.000100006
2017-09-29T11:07:32.998887: step 3312, loss 0.226537, acc 0.90625, learning_rate 0.000100006
2017-09-29T11:07:33.180190: step 3313, loss 0.260661, acc 0.890625, learning_rate 0.000100006
2017-09-29T11:07:33.365692: step 3314, loss 0.165294, acc 0.9375, learning_rate 0.000100006
2017-09-29T11:07:33.551157: step 3315, loss 0.354556, acc 0.875, learning_rate 0.000100006
2017-09-29T11:07:33.732600: step 3316, loss 0.454966, acc 0.828125, learning_rate 0.000100006
2017-09-29T11:07:33.913405: step 3317, loss 0.150762, acc 0.953125, learning_rate 0.000100006
2017-09-29T11:07:34.097433: step 3318, loss 0.312604, acc 0.875, learning_rate 0.000100006
2017-09-29T11:07:34.288160: step 3319, loss 0.377699, acc 0.859375, learning_rate 0.000100006
2017-09-29T11:07:34.473695: step 3320, loss 0.234176, acc 0.890625, learning_rate 0.000100006

Evaluation:
2017-09-29T11:07:35.021201: step 3320, loss 0.314625, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3320

2017-09-29T11:07:35.732466: step 3321, loss 0.275134, acc 0.875, learning_rate 0.000100006
2017-09-29T11:07:35.914633: step 3322, loss 0.154649, acc 0.953125, learning_rate 0.000100006
2017-09-29T11:07:36.105048: step 3323, loss 0.375079, acc 0.90625, learning_rate 0.000100006
2017-09-29T11:07:36.288876: step 3324, loss 0.187816, acc 0.9375, learning_rate 0.000100006
2017-09-29T11:07:36.473853: step 3325, loss 0.255913, acc 0.9375, learning_rate 0.000100006
2017-09-29T11:07:36.656507: step 3326, loss 0.346199, acc 0.875, learning_rate 0.000100006
2017-09-29T11:07:36.840211: step 3327, loss 0.190558, acc 0.9375, learning_rate 0.000100006
2017-09-29T11:07:37.023948: step 3328, loss 0.224476, acc 0.921875, learning_rate 0.000100006
2017-09-29T11:07:37.223772: step 3329, loss 0.380418, acc 0.859375, learning_rate 0.000100006
2017-09-29T11:07:37.409712: step 3330, loss 0.267131, acc 0.9375, learning_rate 0.000100006
2017-09-29T11:07:37.597757: step 3331, loss 0.30239, acc 0.90625, learning_rate 0.000100006
2017-09-29T11:07:37.749881: step 3332, loss 0.242793, acc 0.941176, learning_rate 0.000100006
2017-09-29T11:07:37.945234: step 3333, loss 0.138741, acc 0.96875, learning_rate 0.000100006
2017-09-29T11:07:38.131678: step 3334, loss 0.201525, acc 0.953125, learning_rate 0.000100006
2017-09-29T11:07:38.316065: step 3335, loss 0.196801, acc 0.921875, learning_rate 0.000100006
2017-09-29T11:07:38.501631: step 3336, loss 0.180751, acc 0.9375, learning_rate 0.000100006
2017-09-29T11:07:38.690488: step 3337, loss 0.2118, acc 0.96875, learning_rate 0.000100006
2017-09-29T11:07:38.875808: step 3338, loss 0.215727, acc 0.90625, learning_rate 0.000100006
2017-09-29T11:07:39.061158: step 3339, loss 0.332448, acc 0.859375, learning_rate 0.000100006
2017-09-29T11:07:39.244389: step 3340, loss 0.202337, acc 0.9375, learning_rate 0.000100006
2017-09-29T11:07:39.438125: step 3341, loss 0.196281, acc 0.890625, learning_rate 0.000100006
2017-09-29T11:07:39.623765: step 3342, loss 0.374, acc 0.875, learning_rate 0.000100006
2017-09-29T11:07:39.814466: step 3343, loss 0.415035, acc 0.859375, learning_rate 0.000100006
2017-09-29T11:07:39.996323: step 3344, loss 0.284649, acc 0.890625, learning_rate 0.000100006
2017-09-29T11:07:40.179913: step 3345, loss 0.274786, acc 0.921875, learning_rate 0.000100006
2017-09-29T11:07:40.366265: step 3346, loss 0.202734, acc 0.9375, learning_rate 0.000100006
2017-09-29T11:07:40.559033: step 3347, loss 0.259122, acc 0.921875, learning_rate 0.000100006
2017-09-29T11:07:40.753249: step 3348, loss 0.229496, acc 0.90625, learning_rate 0.000100006
2017-09-29T11:07:40.944355: step 3349, loss 0.437251, acc 0.875, learning_rate 0.000100006
2017-09-29T11:07:41.138116: step 3350, loss 0.296565, acc 0.890625, learning_rate 0.000100006
2017-09-29T11:07:41.327763: step 3351, loss 0.153839, acc 0.953125, learning_rate 0.000100005
2017-09-29T11:07:41.520624: step 3352, loss 0.273204, acc 0.890625, learning_rate 0.000100005
2017-09-29T11:07:41.701021: step 3353, loss 0.238193, acc 0.875, learning_rate 0.000100005
2017-09-29T11:07:41.883948: step 3354, loss 0.408417, acc 0.796875, learning_rate 0.000100005
2017-09-29T11:07:42.067829: step 3355, loss 0.26798, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:42.250028: step 3356, loss 0.328967, acc 0.859375, learning_rate 0.000100005
2017-09-29T11:07:42.432378: step 3357, loss 0.110414, acc 0.96875, learning_rate 0.000100005
2017-09-29T11:07:42.617321: step 3358, loss 0.24814, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:42.802923: step 3359, loss 0.192727, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:42.985803: step 3360, loss 0.237935, acc 0.90625, learning_rate 0.000100005

Evaluation:
2017-09-29T11:07:43.540341: step 3360, loss 0.31519, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3360

2017-09-29T11:07:44.241433: step 3361, loss 0.196767, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:44.430438: step 3362, loss 0.230723, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:44.612877: step 3363, loss 0.128562, acc 0.96875, learning_rate 0.000100005
2017-09-29T11:07:44.794451: step 3364, loss 0.202141, acc 0.921875, learning_rate 0.000100005
2017-09-29T11:07:44.978165: step 3365, loss 0.254303, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:45.162858: step 3366, loss 0.19875, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:45.347334: step 3367, loss 0.167658, acc 0.953125, learning_rate 0.000100005
2017-09-29T11:07:45.541980: step 3368, loss 0.246278, acc 0.921875, learning_rate 0.000100005
2017-09-29T11:07:45.724418: step 3369, loss 0.258743, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:45.905876: step 3370, loss 0.250784, acc 0.875, learning_rate 0.000100005
2017-09-29T11:07:46.096126: step 3371, loss 0.211361, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:46.278413: step 3372, loss 0.297662, acc 0.875, learning_rate 0.000100005
2017-09-29T11:07:46.459919: step 3373, loss 0.310124, acc 0.921875, learning_rate 0.000100005
2017-09-29T11:07:46.646545: step 3374, loss 0.194046, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:46.829974: step 3375, loss 0.253401, acc 0.875, learning_rate 0.000100005
2017-09-29T11:07:47.011868: step 3376, loss 0.23594, acc 0.953125, learning_rate 0.000100005
2017-09-29T11:07:47.198388: step 3377, loss 0.158611, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:47.381701: step 3378, loss 0.276075, acc 0.859375, learning_rate 0.000100005
2017-09-29T11:07:47.578467: step 3379, loss 0.278617, acc 0.890625, learning_rate 0.000100005
2017-09-29T11:07:47.760654: step 3380, loss 0.290296, acc 0.875, learning_rate 0.000100005
2017-09-29T11:07:47.942919: step 3381, loss 0.183402, acc 0.921875, learning_rate 0.000100005
2017-09-29T11:07:48.124937: step 3382, loss 0.301995, acc 0.890625, learning_rate 0.000100005
2017-09-29T11:07:48.305684: step 3383, loss 0.234851, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:48.492428: step 3384, loss 0.309986, acc 0.890625, learning_rate 0.000100005
2017-09-29T11:07:48.677083: step 3385, loss 0.291258, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:48.860813: step 3386, loss 0.241068, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:49.042452: step 3387, loss 0.288663, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:49.227783: step 3388, loss 0.218076, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:49.416867: step 3389, loss 0.242497, acc 0.953125, learning_rate 0.000100005
2017-09-29T11:07:49.600801: step 3390, loss 0.212746, acc 0.953125, learning_rate 0.000100005
2017-09-29T11:07:49.783393: step 3391, loss 0.269936, acc 0.890625, learning_rate 0.000100005
2017-09-29T11:07:49.971244: step 3392, loss 0.311574, acc 0.859375, learning_rate 0.000100005
2017-09-29T11:07:50.154165: step 3393, loss 0.127093, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:50.335704: step 3394, loss 0.260541, acc 0.890625, learning_rate 0.000100005
2017-09-29T11:07:50.523529: step 3395, loss 0.307253, acc 0.90625, learning_rate 0.000100005
2017-09-29T11:07:50.708350: step 3396, loss 0.138294, acc 0.96875, learning_rate 0.000100005
2017-09-29T11:07:50.896723: step 3397, loss 0.285614, acc 0.890625, learning_rate 0.000100005
2017-09-29T11:07:51.085177: step 3398, loss 0.244799, acc 0.953125, learning_rate 0.000100005
2017-09-29T11:07:51.268806: step 3399, loss 0.199458, acc 0.9375, learning_rate 0.000100005
2017-09-29T11:07:51.451671: step 3400, loss 0.395058, acc 0.875, learning_rate 0.000100004

Evaluation:
2017-09-29T11:07:52.007355: step 3400, loss 0.313727, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3400

2017-09-29T11:07:52.797463: step 3401, loss 0.191777, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:07:52.983453: step 3402, loss 0.191033, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:07:53.168555: step 3403, loss 0.332998, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:07:53.352481: step 3404, loss 0.183126, acc 0.953125, learning_rate 0.000100004
2017-09-29T11:07:53.547137: step 3405, loss 0.275017, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:07:53.727760: step 3406, loss 0.247076, acc 0.875, learning_rate 0.000100004
2017-09-29T11:07:53.909092: step 3407, loss 0.383184, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:07:54.091356: step 3408, loss 0.15284, acc 0.953125, learning_rate 0.000100004
2017-09-29T11:07:54.276282: step 3409, loss 0.248539, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:07:54.464565: step 3410, loss 0.396376, acc 0.828125, learning_rate 0.000100004
2017-09-29T11:07:54.646532: step 3411, loss 0.196452, acc 0.953125, learning_rate 0.000100004
2017-09-29T11:07:54.832098: step 3412, loss 0.19407, acc 0.984375, learning_rate 0.000100004
2017-09-29T11:07:55.013577: step 3413, loss 0.355418, acc 0.84375, learning_rate 0.000100004
2017-09-29T11:07:55.195744: step 3414, loss 0.216113, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:07:55.380645: step 3415, loss 0.165093, acc 0.953125, learning_rate 0.000100004
2017-09-29T11:07:55.580565: step 3416, loss 0.330089, acc 0.875, learning_rate 0.000100004
2017-09-29T11:07:55.776312: step 3417, loss 0.266238, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:07:55.961199: step 3418, loss 0.155435, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:07:56.157071: step 3419, loss 0.183443, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:07:56.341328: step 3420, loss 0.197384, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:07:56.522046: step 3421, loss 0.274152, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:07:56.709205: step 3422, loss 0.385637, acc 0.875, learning_rate 0.000100004
2017-09-29T11:07:56.890765: step 3423, loss 0.170463, acc 0.953125, learning_rate 0.000100004
2017-09-29T11:07:57.072453: step 3424, loss 0.208075, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:07:57.254749: step 3425, loss 0.332727, acc 0.875, learning_rate 0.000100004
2017-09-29T11:07:57.442451: step 3426, loss 0.113281, acc 0.96875, learning_rate 0.000100004
2017-09-29T11:07:57.631768: step 3427, loss 0.338696, acc 0.84375, learning_rate 0.000100004
2017-09-29T11:07:57.812033: step 3428, loss 0.265671, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:07:57.996455: step 3429, loss 0.386128, acc 0.875, learning_rate 0.000100004
2017-09-29T11:07:58.149730: step 3430, loss 0.256523, acc 0.901961, learning_rate 0.000100004
2017-09-29T11:07:58.330520: step 3431, loss 0.343288, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:07:58.513737: step 3432, loss 0.198981, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:07:58.700546: step 3433, loss 0.286603, acc 0.859375, learning_rate 0.000100004
2017-09-29T11:07:58.884729: step 3434, loss 0.204359, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:07:59.067504: step 3435, loss 0.203657, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:07:59.254532: step 3436, loss 0.207315, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:07:59.444242: step 3437, loss 0.241534, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:07:59.632529: step 3438, loss 0.243864, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:07:59.814237: step 3439, loss 0.245062, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:07:59.998708: step 3440, loss 0.221254, acc 0.921875, learning_rate 0.000100004

Evaluation:
2017-09-29T11:08:00.556536: step 3440, loss 0.308579, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3440

2017-09-29T11:08:01.193654: step 3441, loss 0.295631, acc 0.859375, learning_rate 0.000100004
2017-09-29T11:08:01.376710: step 3442, loss 0.358089, acc 0.859375, learning_rate 0.000100004
2017-09-29T11:08:01.563564: step 3443, loss 0.253969, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:08:01.745146: step 3444, loss 0.278295, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:08:01.925502: step 3445, loss 0.306622, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:08:02.106968: step 3446, loss 0.188166, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:08:02.285652: step 3447, loss 0.275202, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:08:02.478436: step 3448, loss 0.272497, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:08:02.660562: step 3449, loss 0.247864, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:08:02.845845: step 3450, loss 0.159406, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:08:03.033617: step 3451, loss 0.18427, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:08:03.213982: step 3452, loss 0.201579, acc 0.953125, learning_rate 0.000100004
2017-09-29T11:08:03.400770: step 3453, loss 0.356757, acc 0.875, learning_rate 0.000100004
2017-09-29T11:08:03.585130: step 3454, loss 0.369853, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:08:03.768307: step 3455, loss 0.257762, acc 0.875, learning_rate 0.000100004
2017-09-29T11:08:03.952140: step 3456, loss 0.242196, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:08:04.136501: step 3457, loss 0.286952, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:08:04.327839: step 3458, loss 0.239219, acc 0.90625, learning_rate 0.000100004
2017-09-29T11:08:04.522567: step 3459, loss 0.289611, acc 0.890625, learning_rate 0.000100004
2017-09-29T11:08:04.718963: step 3460, loss 0.225615, acc 0.921875, learning_rate 0.000100004
2017-09-29T11:08:04.898508: step 3461, loss 0.201917, acc 0.9375, learning_rate 0.000100004
2017-09-29T11:08:05.082064: step 3462, loss 0.278447, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:05.267711: step 3463, loss 0.242825, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:05.459784: step 3464, loss 0.241696, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:05.648002: step 3465, loss 0.23545, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:05.828772: step 3466, loss 0.343773, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:06.010000: step 3467, loss 0.201711, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:06.203176: step 3468, loss 0.212959, acc 0.953125, learning_rate 0.000100003
2017-09-29T11:08:06.381358: step 3469, loss 0.364145, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:06.572217: step 3470, loss 0.423761, acc 0.84375, learning_rate 0.000100003
2017-09-29T11:08:06.760086: step 3471, loss 0.338219, acc 0.84375, learning_rate 0.000100003
2017-09-29T11:08:06.949253: step 3472, loss 0.265417, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:07.137867: step 3473, loss 0.250921, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:07.327568: step 3474, loss 0.229269, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:07.519426: step 3475, loss 0.312335, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:07.712383: step 3476, loss 0.241301, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:07.905144: step 3477, loss 0.224383, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:08.106637: step 3478, loss 0.225478, acc 0.9375, learning_rate 0.000100003
2017-09-29T11:08:08.285549: step 3479, loss 0.235982, acc 0.859375, learning_rate 0.000100003
2017-09-29T11:08:08.476115: step 3480, loss 0.104301, acc 0.96875, learning_rate 0.000100003

Evaluation:
2017-09-29T11:08:09.019519: step 3480, loss 0.307591, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3480

2017-09-29T11:08:09.724753: step 3481, loss 0.279217, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:09.912812: step 3482, loss 0.334301, acc 0.828125, learning_rate 0.000100003
2017-09-29T11:08:10.093896: step 3483, loss 0.287262, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:10.277331: step 3484, loss 0.280235, acc 0.859375, learning_rate 0.000100003
2017-09-29T11:08:10.464261: step 3485, loss 0.21976, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:10.650048: step 3486, loss 0.360904, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:10.839658: step 3487, loss 0.243643, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:11.023619: step 3488, loss 0.235704, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:11.215110: step 3489, loss 0.163685, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:11.400367: step 3490, loss 0.183804, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:11.588757: step 3491, loss 0.213792, acc 0.9375, learning_rate 0.000100003
2017-09-29T11:08:11.771231: step 3492, loss 0.24527, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:11.963478: step 3493, loss 0.144176, acc 0.96875, learning_rate 0.000100003
2017-09-29T11:08:12.151031: step 3494, loss 0.1652, acc 0.9375, learning_rate 0.000100003
2017-09-29T11:08:12.337205: step 3495, loss 0.228866, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:12.526293: step 3496, loss 0.268977, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:12.715249: step 3497, loss 0.273823, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:12.901498: step 3498, loss 0.190643, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:13.086541: step 3499, loss 0.324581, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:13.272677: step 3500, loss 0.203251, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:13.462804: step 3501, loss 0.172545, acc 0.953125, learning_rate 0.000100003
2017-09-29T11:08:13.649693: step 3502, loss 0.125179, acc 0.953125, learning_rate 0.000100003
2017-09-29T11:08:13.839628: step 3503, loss 0.294206, acc 0.859375, learning_rate 0.000100003
2017-09-29T11:08:14.023522: step 3504, loss 0.161774, acc 0.953125, learning_rate 0.000100003
2017-09-29T11:08:14.208486: step 3505, loss 0.120522, acc 0.984375, learning_rate 0.000100003
2017-09-29T11:08:14.404667: step 3506, loss 0.21281, acc 0.9375, learning_rate 0.000100003
2017-09-29T11:08:14.595537: step 3507, loss 0.189336, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:14.783201: step 3508, loss 0.284753, acc 0.859375, learning_rate 0.000100003
2017-09-29T11:08:14.972395: step 3509, loss 0.346034, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:15.160476: step 3510, loss 0.239505, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:15.359109: step 3511, loss 0.170693, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:15.551774: step 3512, loss 0.321094, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:15.743537: step 3513, loss 0.289411, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:15.931125: step 3514, loss 0.308953, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:16.124217: step 3515, loss 0.337624, acc 0.84375, learning_rate 0.000100003
2017-09-29T11:08:16.318278: step 3516, loss 0.248817, acc 0.9375, learning_rate 0.000100003
2017-09-29T11:08:16.506202: step 3517, loss 0.272658, acc 0.875, learning_rate 0.000100003
2017-09-29T11:08:16.688183: step 3518, loss 0.171807, acc 0.9375, learning_rate 0.000100003
2017-09-29T11:08:16.872936: step 3519, loss 0.276633, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:17.058397: step 3520, loss 0.248187, acc 0.90625, learning_rate 0.000100003

Evaluation:
2017-09-29T11:08:17.623491: step 3520, loss 0.312576, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3520

2017-09-29T11:08:18.325925: step 3521, loss 0.141845, acc 0.96875, learning_rate 0.000100003
2017-09-29T11:08:18.511382: step 3522, loss 0.365822, acc 0.859375, learning_rate 0.000100003
2017-09-29T11:08:18.695016: step 3523, loss 0.240473, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:18.881802: step 3524, loss 0.306488, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:19.067694: step 3525, loss 0.334086, acc 0.84375, learning_rate 0.000100003
2017-09-29T11:08:19.259012: step 3526, loss 0.224852, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:19.446095: step 3527, loss 0.302509, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:19.602480: step 3528, loss 0.18898, acc 0.941176, learning_rate 0.000100003
2017-09-29T11:08:19.785378: step 3529, loss 0.165966, acc 0.953125, learning_rate 0.000100003
2017-09-29T11:08:19.968600: step 3530, loss 0.352477, acc 0.84375, learning_rate 0.000100003
2017-09-29T11:08:20.158880: step 3531, loss 0.210944, acc 0.921875, learning_rate 0.000100003
2017-09-29T11:08:20.347139: step 3532, loss 0.346819, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:20.550162: step 3533, loss 0.107076, acc 0.96875, learning_rate 0.000100003
2017-09-29T11:08:20.745137: step 3534, loss 0.118493, acc 0.96875, learning_rate 0.000100003
2017-09-29T11:08:20.933189: step 3535, loss 0.257801, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:21.130216: step 3536, loss 0.260942, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:21.330348: step 3537, loss 0.206221, acc 0.953125, learning_rate 0.000100003
2017-09-29T11:08:21.526721: step 3538, loss 0.276602, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:21.726448: step 3539, loss 0.228082, acc 0.9375, learning_rate 0.000100003
2017-09-29T11:08:21.924462: step 3540, loss 0.158983, acc 0.96875, learning_rate 0.000100003
2017-09-29T11:08:22.125123: step 3541, loss 0.157278, acc 0.96875, learning_rate 0.000100003
2017-09-29T11:08:22.310312: step 3542, loss 0.32099, acc 0.890625, learning_rate 0.000100003
2017-09-29T11:08:22.495750: step 3543, loss 0.262724, acc 0.90625, learning_rate 0.000100003
2017-09-29T11:08:22.680650: step 3544, loss 0.249219, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:22.866469: step 3545, loss 0.202336, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:23.051748: step 3546, loss 0.258665, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:23.238684: step 3547, loss 0.160757, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:23.447165: step 3548, loss 0.222168, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:23.635119: step 3549, loss 0.290286, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:23.821650: step 3550, loss 0.150443, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:24.009432: step 3551, loss 0.27424, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:24.198186: step 3552, loss 0.196543, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:24.388771: step 3553, loss 0.258804, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:24.573247: step 3554, loss 0.34881, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:24.757930: step 3555, loss 0.287749, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:24.950478: step 3556, loss 0.298867, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:25.157044: step 3557, loss 0.18315, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:25.352797: step 3558, loss 0.235955, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:25.550365: step 3559, loss 0.13807, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:25.754309: step 3560, loss 0.17386, acc 0.953125, learning_rate 0.000100002

Evaluation:
2017-09-29T11:08:26.314591: step 3560, loss 0.312405, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3560

2017-09-29T11:08:27.141160: step 3561, loss 0.151963, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:27.327523: step 3562, loss 0.280902, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:27.519130: step 3563, loss 0.154244, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:27.713417: step 3564, loss 0.168872, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:27.916412: step 3565, loss 0.312326, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:28.122564: step 3566, loss 0.156818, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:28.337038: step 3567, loss 0.200772, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:28.547895: step 3568, loss 0.171194, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:28.744099: step 3569, loss 0.258073, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:28.933931: step 3570, loss 0.232766, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:29.120662: step 3571, loss 0.253506, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:29.312176: step 3572, loss 0.247742, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:29.528122: step 3573, loss 0.241544, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:29.734957: step 3574, loss 0.276954, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:29.935488: step 3575, loss 0.338208, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:30.124608: step 3576, loss 0.312978, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:30.311449: step 3577, loss 0.270357, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:30.514099: step 3578, loss 0.20956, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:30.713210: step 3579, loss 0.257396, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:30.907095: step 3580, loss 0.245068, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:31.095151: step 3581, loss 0.178684, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:31.284191: step 3582, loss 0.140411, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:31.482077: step 3583, loss 0.285563, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:31.672152: step 3584, loss 0.216384, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:31.866953: step 3585, loss 0.165302, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:32.052518: step 3586, loss 0.254789, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:32.241460: step 3587, loss 0.299964, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:32.430481: step 3588, loss 0.242388, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:32.616210: step 3589, loss 0.203835, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:32.796102: step 3590, loss 0.296042, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:32.982265: step 3591, loss 0.169401, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:33.176881: step 3592, loss 0.430266, acc 0.84375, learning_rate 0.000100002
2017-09-29T11:08:33.362176: step 3593, loss 0.162368, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:33.553039: step 3594, loss 0.256054, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:33.740127: step 3595, loss 0.280883, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:33.924863: step 3596, loss 0.225489, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:34.111474: step 3597, loss 0.324872, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:34.298066: step 3598, loss 0.30433, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:34.494282: step 3599, loss 0.241631, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:34.679039: step 3600, loss 0.15262, acc 0.9375, learning_rate 0.000100002

Evaluation:
2017-09-29T11:08:35.165984: step 3600, loss 0.305779, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3600

2017-09-29T11:08:35.794524: step 3601, loss 0.192022, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:35.977152: step 3602, loss 0.25413, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:36.159023: step 3603, loss 0.201368, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:36.342330: step 3604, loss 0.269555, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:36.536513: step 3605, loss 0.272663, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:36.726430: step 3606, loss 0.296267, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:36.907491: step 3607, loss 0.138273, acc 0.984375, learning_rate 0.000100002
2017-09-29T11:08:37.090254: step 3608, loss 0.219204, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:37.275628: step 3609, loss 0.360882, acc 0.84375, learning_rate 0.000100002
2017-09-29T11:08:37.461078: step 3610, loss 0.303388, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:37.646498: step 3611, loss 0.182967, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:37.830956: step 3612, loss 0.233282, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:38.011960: step 3613, loss 0.286212, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:38.196813: step 3614, loss 0.254638, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:38.378688: step 3615, loss 0.358222, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:38.563698: step 3616, loss 0.222799, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:38.756824: step 3617, loss 0.332331, acc 0.828125, learning_rate 0.000100002
2017-09-29T11:08:38.941116: step 3618, loss 0.121949, acc 0.984375, learning_rate 0.000100002
2017-09-29T11:08:39.145687: step 3619, loss 0.26402, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:39.340329: step 3620, loss 0.197304, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:39.532958: step 3621, loss 0.103594, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:39.721563: step 3622, loss 0.346252, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:39.909548: step 3623, loss 0.107634, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:40.089056: step 3624, loss 0.325219, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:40.271168: step 3625, loss 0.122609, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:40.429828: step 3626, loss 0.407213, acc 0.784314, learning_rate 0.000100002
2017-09-29T11:08:40.617662: step 3627, loss 0.330563, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:40.809645: step 3628, loss 0.39715, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:41.006689: step 3629, loss 0.230429, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:41.198411: step 3630, loss 0.277962, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:41.396619: step 3631, loss 0.328975, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:41.588227: step 3632, loss 0.133977, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:41.771951: step 3633, loss 0.170899, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:41.958003: step 3634, loss 0.381125, acc 0.828125, learning_rate 0.000100002
2017-09-29T11:08:42.142660: step 3635, loss 0.158783, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:42.335238: step 3636, loss 0.187535, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:42.524855: step 3637, loss 0.338495, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:42.713511: step 3638, loss 0.319508, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:42.907138: step 3639, loss 0.229556, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:43.092784: step 3640, loss 0.290857, acc 0.875, learning_rate 0.000100002

Evaluation:
2017-09-29T11:08:43.582286: step 3640, loss 0.310833, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3640

2017-09-29T11:08:44.294883: step 3641, loss 0.438891, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:44.482207: step 3642, loss 0.239499, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:44.670955: step 3643, loss 0.232803, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:44.855926: step 3644, loss 0.240951, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:45.041759: step 3645, loss 0.183442, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:45.229989: step 3646, loss 0.299011, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:45.416666: step 3647, loss 0.251921, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:45.598927: step 3648, loss 0.172614, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:45.788010: step 3649, loss 0.189399, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:45.979383: step 3650, loss 0.309776, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:46.167051: step 3651, loss 0.316997, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:46.353024: step 3652, loss 0.117249, acc 1, learning_rate 0.000100002
2017-09-29T11:08:46.557611: step 3653, loss 0.15985, acc 0.953125, learning_rate 0.000100002
2017-09-29T11:08:46.744214: step 3654, loss 0.352273, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:46.934826: step 3655, loss 0.196548, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:47.119482: step 3656, loss 0.209944, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:47.305004: step 3657, loss 0.249632, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:47.489211: step 3658, loss 0.332862, acc 0.859375, learning_rate 0.000100002
2017-09-29T11:08:47.673229: step 3659, loss 0.350119, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:47.857477: step 3660, loss 0.288556, acc 0.890625, learning_rate 0.000100002
2017-09-29T11:08:48.038479: step 3661, loss 0.179742, acc 0.96875, learning_rate 0.000100002
2017-09-29T11:08:48.223851: step 3662, loss 0.172994, acc 0.9375, learning_rate 0.000100002
2017-09-29T11:08:48.406213: step 3663, loss 0.334243, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:48.588776: step 3664, loss 0.267361, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:48.774344: step 3665, loss 0.231141, acc 0.921875, learning_rate 0.000100002
2017-09-29T11:08:48.955446: step 3666, loss 0.218312, acc 0.90625, learning_rate 0.000100002
2017-09-29T11:08:49.138334: step 3667, loss 0.262019, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:49.319839: step 3668, loss 0.268556, acc 0.875, learning_rate 0.000100002
2017-09-29T11:08:49.503925: step 3669, loss 0.174789, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:08:49.695227: step 3670, loss 0.2067, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:49.884773: step 3671, loss 0.369249, acc 0.875, learning_rate 0.000100001
2017-09-29T11:08:50.062835: step 3672, loss 0.266419, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:08:50.243520: step 3673, loss 0.307473, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:08:50.427485: step 3674, loss 0.318923, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:08:50.611881: step 3675, loss 0.302286, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:08:50.799929: step 3676, loss 0.226635, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:50.993841: step 3677, loss 0.17417, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:08:51.175806: step 3678, loss 0.244754, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:08:51.359771: step 3679, loss 0.187075, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:51.550981: step 3680, loss 0.364408, acc 0.859375, learning_rate 0.000100001

Evaluation:
2017-09-29T11:08:52.021367: step 3680, loss 0.307277, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3680

2017-09-29T11:08:52.719673: step 3681, loss 0.20666, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:52.902502: step 3682, loss 0.222132, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:08:53.088090: step 3683, loss 0.19943, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:53.279816: step 3684, loss 0.283033, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:53.464902: step 3685, loss 0.158802, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:08:53.651288: step 3686, loss 0.163223, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:53.834231: step 3687, loss 0.233981, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:54.014809: step 3688, loss 0.213193, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:08:54.198875: step 3689, loss 0.234592, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:08:54.394420: step 3690, loss 0.345698, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:08:54.579138: step 3691, loss 0.386593, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:08:54.767353: step 3692, loss 0.137211, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:08:54.953399: step 3693, loss 0.269429, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:55.139254: step 3694, loss 0.404389, acc 0.828125, learning_rate 0.000100001
2017-09-29T11:08:55.325029: step 3695, loss 0.185369, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:55.512256: step 3696, loss 0.185488, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:08:55.707063: step 3697, loss 0.207595, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:08:55.899475: step 3698, loss 0.218922, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:08:56.084203: step 3699, loss 0.226364, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:56.283277: step 3700, loss 0.166215, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:56.478219: step 3701, loss 0.338479, acc 0.875, learning_rate 0.000100001
2017-09-29T11:08:56.672552: step 3702, loss 0.329902, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:08:56.858530: step 3703, loss 0.291182, acc 0.828125, learning_rate 0.000100001
2017-09-29T11:08:57.041092: step 3704, loss 0.332745, acc 0.84375, learning_rate 0.000100001
2017-09-29T11:08:57.235028: step 3705, loss 0.218474, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:57.421954: step 3706, loss 0.134152, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:57.613767: step 3707, loss 0.24181, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:08:57.799499: step 3708, loss 0.181938, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:08:57.983111: step 3709, loss 0.156066, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:08:58.170704: step 3710, loss 0.261715, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:58.355008: step 3711, loss 0.285138, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:08:58.558952: step 3712, loss 0.206699, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:08:58.750822: step 3713, loss 0.197468, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:08:58.939106: step 3714, loss 0.222258, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:59.123388: step 3715, loss 0.249323, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:08:59.309964: step 3716, loss 0.175271, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:08:59.500852: step 3717, loss 0.130774, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:08:59.688602: step 3718, loss 0.191416, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:08:59.872523: step 3719, loss 0.245862, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:00.058308: step 3720, loss 0.185087, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-09-29T11:09:00.545024: step 3720, loss 0.309936, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3720

2017-09-29T11:09:01.327440: step 3721, loss 0.154526, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:01.513702: step 3722, loss 0.203806, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:01.704355: step 3723, loss 0.176329, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:01.856310: step 3724, loss 0.202747, acc 0.921569, learning_rate 0.000100001
2017-09-29T11:09:02.040803: step 3725, loss 0.261863, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:02.224969: step 3726, loss 0.256332, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:02.404867: step 3727, loss 0.254181, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:02.588942: step 3728, loss 0.24699, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:02.776161: step 3729, loss 0.105642, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:02.960612: step 3730, loss 0.153072, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:03.147018: step 3731, loss 0.246793, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:03.333037: step 3732, loss 0.112518, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:03.519253: step 3733, loss 0.172825, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:03.703664: step 3734, loss 0.203387, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:03.886586: step 3735, loss 0.175455, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:04.074120: step 3736, loss 0.23579, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:04.259490: step 3737, loss 0.26251, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:04.451706: step 3738, loss 0.203581, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:04.638743: step 3739, loss 0.352833, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:04.824921: step 3740, loss 0.195659, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:05.006300: step 3741, loss 0.354534, acc 0.828125, learning_rate 0.000100001
2017-09-29T11:09:05.187764: step 3742, loss 0.174743, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:05.371135: step 3743, loss 0.178743, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:05.558049: step 3744, loss 0.16724, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:05.746509: step 3745, loss 0.137072, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:05.999381: step 3746, loss 0.247847, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:06.185472: step 3747, loss 0.0471084, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:06.369167: step 3748, loss 0.327971, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:06.550465: step 3749, loss 0.23052, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:06.740351: step 3750, loss 0.184063, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:06.922065: step 3751, loss 0.330146, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:07.106115: step 3752, loss 0.34689, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:07.289255: step 3753, loss 0.255548, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:07.475892: step 3754, loss 0.080193, acc 1, learning_rate 0.000100001
2017-09-29T11:09:07.663201: step 3755, loss 0.119212, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:07.848789: step 3756, loss 0.282887, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:08.034351: step 3757, loss 0.169577, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:08.217251: step 3758, loss 0.347065, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:08.399581: step 3759, loss 0.316893, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:08.585738: step 3760, loss 0.238881, acc 0.890625, learning_rate 0.000100001

Evaluation:
2017-09-29T11:09:09.062921: step 3760, loss 0.302853, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3760

2017-09-29T11:09:09.704940: step 3761, loss 0.285116, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:09.900927: step 3762, loss 0.340118, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:10.083361: step 3763, loss 0.204864, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:10.268592: step 3764, loss 0.199147, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:10.460422: step 3765, loss 0.175679, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:10.640620: step 3766, loss 0.213004, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:10.825470: step 3767, loss 0.265634, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:11.009103: step 3768, loss 0.253306, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:11.198943: step 3769, loss 0.196935, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:11.378907: step 3770, loss 0.14147, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:11.563016: step 3771, loss 0.296874, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:11.758062: step 3772, loss 0.225925, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:11.948783: step 3773, loss 0.101799, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:12.149005: step 3774, loss 0.316045, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:12.329947: step 3775, loss 0.141577, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:12.512496: step 3776, loss 0.312656, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:12.699061: step 3777, loss 0.188528, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:12.886805: step 3778, loss 0.231709, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:13.072312: step 3779, loss 0.283267, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:13.253448: step 3780, loss 0.290041, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:09:13.434290: step 3781, loss 0.27169, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:13.617377: step 3782, loss 0.167451, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:13.800408: step 3783, loss 0.276651, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:13.984847: step 3784, loss 0.199021, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:14.172138: step 3785, loss 0.134854, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:14.360322: step 3786, loss 0.281206, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:14.543550: step 3787, loss 0.158866, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:14.727944: step 3788, loss 0.268484, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:14.906194: step 3789, loss 0.236794, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:15.093208: step 3790, loss 0.213924, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:15.275206: step 3791, loss 0.245914, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:15.465437: step 3792, loss 0.25022, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:15.650395: step 3793, loss 0.16698, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:15.834657: step 3794, loss 0.17587, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:16.019935: step 3795, loss 0.270709, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:16.205999: step 3796, loss 0.348319, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:16.394981: step 3797, loss 0.382967, acc 0.828125, learning_rate 0.000100001
2017-09-29T11:09:16.584742: step 3798, loss 0.332584, acc 0.84375, learning_rate 0.000100001
2017-09-29T11:09:16.773728: step 3799, loss 0.234639, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:16.957398: step 3800, loss 0.304811, acc 0.890625, learning_rate 0.000100001

Evaluation:
2017-09-29T11:09:17.427382: step 3800, loss 0.310054, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3800

2017-09-29T11:09:18.133409: step 3801, loss 0.31061, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:09:18.315616: step 3802, loss 0.31527, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:18.498956: step 3803, loss 0.206958, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:18.688998: step 3804, loss 0.250616, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:18.871420: step 3805, loss 0.257146, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:09:19.058677: step 3806, loss 0.256336, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:19.242031: step 3807, loss 0.257994, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:19.425942: step 3808, loss 0.30404, acc 0.84375, learning_rate 0.000100001
2017-09-29T11:09:19.613117: step 3809, loss 0.163949, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:19.795887: step 3810, loss 0.152925, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:19.978362: step 3811, loss 0.225795, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:20.157609: step 3812, loss 0.250846, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:20.341782: step 3813, loss 0.323011, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:20.524800: step 3814, loss 0.240528, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:20.709509: step 3815, loss 0.144458, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:20.897218: step 3816, loss 0.192105, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:21.085291: step 3817, loss 0.127816, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:21.273970: step 3818, loss 0.201411, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:21.465129: step 3819, loss 0.221242, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:21.653505: step 3820, loss 0.25733, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:21.847556: step 3821, loss 0.484584, acc 0.796875, learning_rate 0.000100001
2017-09-29T11:09:22.001965: step 3822, loss 0.262555, acc 0.901961, learning_rate 0.000100001
2017-09-29T11:09:22.183789: step 3823, loss 0.243563, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:22.365391: step 3824, loss 0.370506, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:09:22.552242: step 3825, loss 0.227736, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:22.736963: step 3826, loss 0.192105, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:22.921051: step 3827, loss 0.235367, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:23.108880: step 3828, loss 0.297989, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:23.289756: step 3829, loss 0.311647, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:23.477617: step 3830, loss 0.295584, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:23.662166: step 3831, loss 0.251086, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:23.845620: step 3832, loss 0.129198, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:24.025942: step 3833, loss 0.180772, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:24.207407: step 3834, loss 0.292169, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:24.389704: step 3835, loss 0.237144, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:24.570599: step 3836, loss 0.297244, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:24.764858: step 3837, loss 0.183589, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:24.949201: step 3838, loss 0.25855, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:25.132341: step 3839, loss 0.285539, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:25.314107: step 3840, loss 0.324191, acc 0.890625, learning_rate 0.000100001

Evaluation:
2017-09-29T11:09:25.797417: step 3840, loss 0.300114, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3840

2017-09-29T11:09:26.508358: step 3841, loss 0.233361, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:26.693292: step 3842, loss 0.246983, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:26.885047: step 3843, loss 0.181978, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:27.069404: step 3844, loss 0.287597, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:09:27.252183: step 3845, loss 0.205567, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:27.444012: step 3846, loss 0.210555, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:27.638083: step 3847, loss 0.28829, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:27.820599: step 3848, loss 0.189885, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:28.012087: step 3849, loss 0.239603, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:28.193760: step 3850, loss 0.396901, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:09:28.378651: step 3851, loss 0.384655, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:28.563438: step 3852, loss 0.283875, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:28.746013: step 3853, loss 0.313955, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:28.931437: step 3854, loss 0.113914, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:29.120580: step 3855, loss 0.381648, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:29.317170: step 3856, loss 0.212462, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:29.498668: step 3857, loss 0.216721, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:29.679221: step 3858, loss 0.252644, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:29.860832: step 3859, loss 0.188767, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:30.063685: step 3860, loss 0.322609, acc 0.828125, learning_rate 0.000100001
2017-09-29T11:09:30.246736: step 3861, loss 0.227021, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:30.455054: step 3862, loss 0.191271, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:30.642451: step 3863, loss 0.420164, acc 0.859375, learning_rate 0.000100001
2017-09-29T11:09:30.824202: step 3864, loss 0.232257, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:31.008080: step 3865, loss 0.239992, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:31.218038: step 3866, loss 0.213459, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:31.423134: step 3867, loss 0.232848, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:31.613485: step 3868, loss 0.203503, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:31.799414: step 3869, loss 0.160978, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:31.987040: step 3870, loss 0.286582, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:32.176203: step 3871, loss 0.235322, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:32.359764: step 3872, loss 0.199365, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:32.542954: step 3873, loss 0.16101, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:32.727557: step 3874, loss 0.258269, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:32.907260: step 3875, loss 0.288788, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:33.088314: step 3876, loss 0.229017, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:33.269520: step 3877, loss 0.168817, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:33.479501: step 3878, loss 0.17625, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:33.674569: step 3879, loss 0.434534, acc 0.796875, learning_rate 0.000100001
2017-09-29T11:09:33.881656: step 3880, loss 0.181621, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-09-29T11:09:34.396299: step 3880, loss 0.30743, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3880

2017-09-29T11:09:35.185723: step 3881, loss 0.265431, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:35.376414: step 3882, loss 0.320272, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:35.560976: step 3883, loss 0.228221, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:35.745884: step 3884, loss 0.149509, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:35.926835: step 3885, loss 0.0972305, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:36.109863: step 3886, loss 0.187559, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:36.294322: step 3887, loss 0.145553, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:36.475571: step 3888, loss 0.255162, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:36.658714: step 3889, loss 0.189928, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:36.843630: step 3890, loss 0.114067, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:37.031418: step 3891, loss 0.137299, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:37.215175: step 3892, loss 0.315138, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:37.400950: step 3893, loss 0.218907, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:37.586720: step 3894, loss 0.0905198, acc 1, learning_rate 0.000100001
2017-09-29T11:09:37.768230: step 3895, loss 0.239654, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:37.951652: step 3896, loss 0.355528, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:38.135989: step 3897, loss 0.264, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:38.321175: step 3898, loss 0.164851, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:38.505918: step 3899, loss 0.174003, acc 0.96875, learning_rate 0.000100001
2017-09-29T11:09:38.696297: step 3900, loss 0.236738, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:38.879199: step 3901, loss 0.385985, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:39.063420: step 3902, loss 0.386323, acc 0.84375, learning_rate 0.000100001
2017-09-29T11:09:39.248409: step 3903, loss 0.197204, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:39.436282: step 3904, loss 0.297578, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:39.622167: step 3905, loss 0.341776, acc 0.828125, learning_rate 0.000100001
2017-09-29T11:09:39.812402: step 3906, loss 0.211513, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:39.993969: step 3907, loss 0.233093, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:40.177276: step 3908, loss 0.258599, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:40.359728: step 3909, loss 0.273024, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:40.545198: step 3910, loss 0.179188, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:40.725412: step 3911, loss 0.174763, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:40.916353: step 3912, loss 0.239269, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:41.096361: step 3913, loss 0.308373, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:41.285740: step 3914, loss 0.208064, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:41.473598: step 3915, loss 0.192164, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:41.656030: step 3916, loss 0.341827, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:41.839077: step 3917, loss 0.235034, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:42.029812: step 3918, loss 0.305725, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:42.212436: step 3919, loss 0.244264, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:42.364247: step 3920, loss 0.340414, acc 0.901961, learning_rate 0.000100001

Evaluation:
2017-09-29T11:09:42.855904: step 3920, loss 0.300628, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3920

2017-09-29T11:09:43.496741: step 3921, loss 0.203648, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:43.681599: step 3922, loss 0.256179, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:43.867857: step 3923, loss 0.21841, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:44.050221: step 3924, loss 0.229251, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:44.230953: step 3925, loss 0.224432, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:44.422252: step 3926, loss 0.149685, acc 0.953125, learning_rate 0.000100001
2017-09-29T11:09:44.611304: step 3927, loss 0.125887, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:44.797404: step 3928, loss 0.317562, acc 0.875, learning_rate 0.000100001
2017-09-29T11:09:44.976783: step 3929, loss 0.225532, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:45.159722: step 3930, loss 0.304037, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:45.341177: step 3931, loss 0.335594, acc 0.90625, learning_rate 0.000100001
2017-09-29T11:09:45.537300: step 3932, loss 0.15421, acc 0.984375, learning_rate 0.000100001
2017-09-29T11:09:45.721062: step 3933, loss 0.231121, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:45.915270: step 3934, loss 0.196876, acc 0.9375, learning_rate 0.000100001
2017-09-29T11:09:46.098749: step 3935, loss 0.286228, acc 0.890625, learning_rate 0.000100001
2017-09-29T11:09:46.286653: step 3936, loss 0.254545, acc 0.921875, learning_rate 0.000100001
2017-09-29T11:09:46.471231: step 3937, loss 0.26299, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:46.655628: step 3938, loss 0.274785, acc 0.890625, learning_rate 0.0001
2017-09-29T11:09:46.836696: step 3939, loss 0.268895, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:47.026510: step 3940, loss 0.193556, acc 0.953125, learning_rate 0.0001
2017-09-29T11:09:47.211306: step 3941, loss 0.223122, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:47.391713: step 3942, loss 0.295182, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:47.575117: step 3943, loss 0.342104, acc 0.84375, learning_rate 0.0001
2017-09-29T11:09:47.760832: step 3944, loss 0.185312, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:47.945030: step 3945, loss 0.187424, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:48.140188: step 3946, loss 0.195146, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:48.323797: step 3947, loss 0.234231, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:48.502973: step 3948, loss 0.112006, acc 0.96875, learning_rate 0.0001
2017-09-29T11:09:48.692198: step 3949, loss 0.176465, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:48.874705: step 3950, loss 0.139705, acc 0.953125, learning_rate 0.0001
2017-09-29T11:09:49.058858: step 3951, loss 0.179208, acc 0.953125, learning_rate 0.0001
2017-09-29T11:09:49.240157: step 3952, loss 0.255503, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:49.423980: step 3953, loss 0.24634, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:49.605397: step 3954, loss 0.242902, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:49.790759: step 3955, loss 0.306281, acc 0.875, learning_rate 0.0001
2017-09-29T11:09:49.974096: step 3956, loss 0.318862, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:50.158568: step 3957, loss 0.215922, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:50.339733: step 3958, loss 0.248315, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:50.524736: step 3959, loss 0.186965, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:50.706856: step 3960, loss 0.179195, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:09:51.189304: step 3960, loss 0.304111, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-3960

2017-09-29T11:09:51.911365: step 3961, loss 0.172123, acc 0.953125, learning_rate 0.0001
2017-09-29T11:09:52.102161: step 3962, loss 0.133312, acc 0.96875, learning_rate 0.0001
2017-09-29T11:09:52.285443: step 3963, loss 0.250718, acc 0.890625, learning_rate 0.0001
2017-09-29T11:09:52.471520: step 3964, loss 0.138139, acc 0.953125, learning_rate 0.0001
2017-09-29T11:09:52.654149: step 3965, loss 0.0998993, acc 0.984375, learning_rate 0.0001
2017-09-29T11:09:52.833818: step 3966, loss 0.246104, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:53.016330: step 3967, loss 0.309467, acc 0.875, learning_rate 0.0001
2017-09-29T11:09:53.200463: step 3968, loss 0.212213, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:53.382767: step 3969, loss 0.222836, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:53.564106: step 3970, loss 0.2851, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:53.748782: step 3971, loss 0.135831, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:53.931047: step 3972, loss 0.213077, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:54.111448: step 3973, loss 0.277769, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:54.294661: step 3974, loss 0.295823, acc 0.890625, learning_rate 0.0001
2017-09-29T11:09:54.477706: step 3975, loss 0.29873, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:54.663856: step 3976, loss 0.287463, acc 0.859375, learning_rate 0.0001
2017-09-29T11:09:54.845163: step 3977, loss 0.0963285, acc 1, learning_rate 0.0001
2017-09-29T11:09:55.025450: step 3978, loss 0.20566, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:55.209920: step 3979, loss 0.1944, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:55.392790: step 3980, loss 0.326998, acc 0.875, learning_rate 0.0001
2017-09-29T11:09:55.576871: step 3981, loss 0.254294, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:55.762143: step 3982, loss 0.246888, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:55.943450: step 3983, loss 0.346122, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:56.124671: step 3984, loss 0.216381, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:56.317297: step 3985, loss 0.210459, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:56.505042: step 3986, loss 0.143175, acc 0.984375, learning_rate 0.0001
2017-09-29T11:09:56.691746: step 3987, loss 0.288506, acc 0.890625, learning_rate 0.0001
2017-09-29T11:09:56.881060: step 3988, loss 0.195031, acc 0.9375, learning_rate 0.0001
2017-09-29T11:09:57.071478: step 3989, loss 0.216956, acc 0.90625, learning_rate 0.0001
2017-09-29T11:09:57.258983: step 3990, loss 0.280507, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:57.449588: step 3991, loss 0.321232, acc 0.859375, learning_rate 0.0001
2017-09-29T11:09:57.631624: step 3992, loss 0.184781, acc 0.96875, learning_rate 0.0001
2017-09-29T11:09:57.815919: step 3993, loss 0.225373, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:58.002280: step 3994, loss 0.218685, acc 0.890625, learning_rate 0.0001
2017-09-29T11:09:58.186370: step 3995, loss 0.164842, acc 0.96875, learning_rate 0.0001
2017-09-29T11:09:58.367606: step 3996, loss 0.349197, acc 0.875, learning_rate 0.0001
2017-09-29T11:09:58.560114: step 3997, loss 0.162299, acc 0.953125, learning_rate 0.0001
2017-09-29T11:09:58.750490: step 3998, loss 0.180541, acc 0.953125, learning_rate 0.0001
2017-09-29T11:09:58.935136: step 3999, loss 0.234271, acc 0.921875, learning_rate 0.0001
2017-09-29T11:09:59.119129: step 4000, loss 0.250827, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:09:59.603640: step 4000, loss 0.30776, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4000

2017-09-29T11:10:00.305922: step 4001, loss 0.318247, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:00.491844: step 4002, loss 0.168214, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:00.674389: step 4003, loss 0.251558, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:00.855264: step 4004, loss 0.0955858, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:01.036053: step 4005, loss 0.182675, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:01.220391: step 4006, loss 0.257219, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:01.411211: step 4007, loss 0.194758, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:01.612415: step 4008, loss 0.186933, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:01.819457: step 4009, loss 0.139826, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:02.024000: step 4010, loss 0.316071, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:02.234737: step 4011, loss 0.378258, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:02.445635: step 4012, loss 0.272755, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:02.650265: step 4013, loss 0.188355, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:02.856006: step 4014, loss 0.281979, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:03.058848: step 4015, loss 0.251418, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:03.255763: step 4016, loss 0.194397, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:03.451468: step 4017, loss 0.266173, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:03.603372: step 4018, loss 0.254596, acc 0.882353, learning_rate 0.0001
2017-09-29T11:10:03.782463: step 4019, loss 0.383853, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:03.965322: step 4020, loss 0.264033, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:04.155239: step 4021, loss 0.170935, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:04.335422: step 4022, loss 0.225652, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:04.518478: step 4023, loss 0.13275, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:04.698683: step 4024, loss 0.24417, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:04.880864: step 4025, loss 0.24901, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:05.065327: step 4026, loss 0.225329, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:05.248853: step 4027, loss 0.230908, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:05.436170: step 4028, loss 0.20859, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:05.617907: step 4029, loss 0.222327, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:05.800723: step 4030, loss 0.19829, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:05.984058: step 4031, loss 0.297796, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:06.166251: step 4032, loss 0.21615, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:06.352663: step 4033, loss 0.208187, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:06.537223: step 4034, loss 0.255647, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:06.721933: step 4035, loss 0.239726, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:06.904177: step 4036, loss 0.198526, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:07.095366: step 4037, loss 0.190867, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:07.276318: step 4038, loss 0.317472, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:07.460784: step 4039, loss 0.217814, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:07.647184: step 4040, loss 0.296175, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-09-29T11:10:08.145560: step 4040, loss 0.301053, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4040

2017-09-29T11:10:08.942499: step 4041, loss 0.203928, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:09.123639: step 4042, loss 0.23008, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:09.303518: step 4043, loss 0.193639, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:09.488863: step 4044, loss 0.134484, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:09.675208: step 4045, loss 0.2907, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:09.858399: step 4046, loss 0.265871, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:10.038438: step 4047, loss 0.217512, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:10.221264: step 4048, loss 0.218131, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:10.407606: step 4049, loss 0.234178, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:10.589402: step 4050, loss 0.285819, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:10.773410: step 4051, loss 0.16045, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:10.958441: step 4052, loss 0.274773, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:11.139089: step 4053, loss 0.383102, acc 0.84375, learning_rate 0.0001
2017-09-29T11:10:11.321616: step 4054, loss 0.268997, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:11.508870: step 4055, loss 0.207418, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:11.703399: step 4056, loss 0.102305, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:11.889136: step 4057, loss 0.238846, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:12.076587: step 4058, loss 0.319862, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:12.264354: step 4059, loss 0.130987, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:12.450152: step 4060, loss 0.240876, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:12.635095: step 4061, loss 0.221456, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:12.816075: step 4062, loss 0.203572, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:13.000548: step 4063, loss 0.196299, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:13.183826: step 4064, loss 0.324869, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:13.366713: step 4065, loss 0.144011, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:13.548457: step 4066, loss 0.331404, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:13.732587: step 4067, loss 0.299958, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:13.918631: step 4068, loss 0.140803, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:14.102078: step 4069, loss 0.125391, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:14.301582: step 4070, loss 0.316744, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:14.504028: step 4071, loss 0.191638, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:14.691819: step 4072, loss 0.302317, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:14.877031: step 4073, loss 0.351275, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:15.055768: step 4074, loss 0.142525, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:15.241048: step 4075, loss 0.100317, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:15.424891: step 4076, loss 0.205611, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:15.606704: step 4077, loss 0.247923, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:15.792303: step 4078, loss 0.158784, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:15.977782: step 4079, loss 0.209808, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:16.159203: step 4080, loss 0.21108, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:10:16.640450: step 4080, loss 0.301422, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4080

2017-09-29T11:10:17.276770: step 4081, loss 0.0856095, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:17.463128: step 4082, loss 0.258014, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:17.648038: step 4083, loss 0.350248, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:17.837564: step 4084, loss 0.184902, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:18.020390: step 4085, loss 0.220285, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:18.203640: step 4086, loss 0.146477, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:18.384708: step 4087, loss 0.311009, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:18.571650: step 4088, loss 0.267538, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:18.757545: step 4089, loss 0.242319, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:18.942602: step 4090, loss 0.422045, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:19.125127: step 4091, loss 0.176916, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:19.308869: step 4092, loss 0.216863, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:19.492987: step 4093, loss 0.255393, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:19.686958: step 4094, loss 0.229683, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:19.874283: step 4095, loss 0.380591, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:20.066903: step 4096, loss 0.162982, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:20.252633: step 4097, loss 0.170516, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:20.452446: step 4098, loss 0.336751, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:20.653406: step 4099, loss 0.247191, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:20.874802: step 4100, loss 0.221506, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:21.097280: step 4101, loss 0.229168, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:21.306551: step 4102, loss 0.125694, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:21.514984: step 4103, loss 0.1158, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:21.757830: step 4104, loss 0.28684, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:21.970718: step 4105, loss 0.299501, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:22.192588: step 4106, loss 0.216682, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:22.440328: step 4107, loss 0.136616, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:22.647436: step 4108, loss 0.158411, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:22.860757: step 4109, loss 0.394463, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:23.095016: step 4110, loss 0.203716, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:23.318326: step 4111, loss 0.164707, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:23.520131: step 4112, loss 0.181473, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:23.715404: step 4113, loss 0.262864, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:23.902320: step 4114, loss 0.144987, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:24.089932: step 4115, loss 0.202955, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:24.241795: step 4116, loss 0.0731192, acc 1, learning_rate 0.0001
2017-09-29T11:10:24.428117: step 4117, loss 0.263975, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:24.610934: step 4118, loss 0.132592, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:24.798179: step 4119, loss 0.29648, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:24.980527: step 4120, loss 0.129513, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:10:25.471089: step 4120, loss 0.301291, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4120

2017-09-29T11:10:26.177905: step 4121, loss 0.346027, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:26.363621: step 4122, loss 0.21645, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:26.564764: step 4123, loss 0.18352, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:26.749261: step 4124, loss 0.312156, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:26.931766: step 4125, loss 0.280789, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:27.115343: step 4126, loss 0.195107, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:27.299058: step 4127, loss 0.196282, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:27.483775: step 4128, loss 0.250663, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:27.668278: step 4129, loss 0.215801, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:27.856198: step 4130, loss 0.20415, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:28.042166: step 4131, loss 0.150445, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:28.226870: step 4132, loss 0.298165, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:28.413937: step 4133, loss 0.256654, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:28.595028: step 4134, loss 0.316101, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:28.780334: step 4135, loss 0.279056, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:28.964023: step 4136, loss 0.163389, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:29.148896: step 4137, loss 0.205349, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:29.341871: step 4138, loss 0.212287, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:29.532377: step 4139, loss 0.294808, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:29.733565: step 4140, loss 0.161672, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:29.926592: step 4141, loss 0.345985, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:30.145176: step 4142, loss 0.1421, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:30.324528: step 4143, loss 0.0748953, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:30.506498: step 4144, loss 0.143361, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:30.691951: step 4145, loss 0.236696, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:30.888500: step 4146, loss 0.189344, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:31.088433: step 4147, loss 0.179229, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:31.273697: step 4148, loss 0.315304, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:31.470592: step 4149, loss 0.142842, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:31.661544: step 4150, loss 0.23231, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:31.847169: step 4151, loss 0.23148, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:32.030097: step 4152, loss 0.222412, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:32.211305: step 4153, loss 0.177495, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:32.397617: step 4154, loss 0.345746, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:32.579959: step 4155, loss 0.276576, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:32.762313: step 4156, loss 0.137885, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:32.956116: step 4157, loss 0.192445, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:33.160572: step 4158, loss 0.240948, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:33.365901: step 4159, loss 0.206651, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:33.575059: step 4160, loss 0.187387, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:10:34.132604: step 4160, loss 0.297789, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4160

2017-09-29T11:10:34.857205: step 4161, loss 0.0810416, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:35.062627: step 4162, loss 0.176675, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:35.252466: step 4163, loss 0.157602, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:35.438559: step 4164, loss 0.208928, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:35.627140: step 4165, loss 0.242749, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:35.809573: step 4166, loss 0.260908, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:35.995540: step 4167, loss 0.18378, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:36.179472: step 4168, loss 0.18104, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:36.362733: step 4169, loss 0.254945, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:36.545992: step 4170, loss 0.194581, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:36.733237: step 4171, loss 0.221946, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:36.915390: step 4172, loss 0.270621, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:37.100680: step 4173, loss 0.212639, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:37.295797: step 4174, loss 0.202476, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:37.493133: step 4175, loss 0.354359, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:37.681774: step 4176, loss 0.211913, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:37.866638: step 4177, loss 0.261215, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:38.053507: step 4178, loss 0.178595, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:38.236548: step 4179, loss 0.233616, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:38.420618: step 4180, loss 0.161424, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:38.600452: step 4181, loss 0.173601, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:38.794243: step 4182, loss 0.189042, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:38.977878: step 4183, loss 0.169247, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:39.162823: step 4184, loss 0.183275, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:39.341471: step 4185, loss 0.306354, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:39.524009: step 4186, loss 0.361212, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:39.707813: step 4187, loss 0.189052, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:39.890073: step 4188, loss 0.235555, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:40.076177: step 4189, loss 0.2531, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:40.261451: step 4190, loss 0.310433, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:40.443987: step 4191, loss 0.248183, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:40.624676: step 4192, loss 0.163976, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:40.812679: step 4193, loss 0.418244, acc 0.84375, learning_rate 0.0001
2017-09-29T11:10:40.993423: step 4194, loss 0.127411, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:41.178952: step 4195, loss 0.290708, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:41.360304: step 4196, loss 0.242816, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:41.543805: step 4197, loss 0.196122, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:41.731813: step 4198, loss 0.264178, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:41.912852: step 4199, loss 0.116991, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:42.091669: step 4200, loss 0.265976, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:10:42.605704: step 4200, loss 0.305084, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4200

2017-09-29T11:10:43.396816: step 4201, loss 0.170072, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:43.599892: step 4202, loss 0.225207, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:43.781932: step 4203, loss 0.0799255, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:43.964658: step 4204, loss 0.148412, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:44.149086: step 4205, loss 0.259817, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:44.335373: step 4206, loss 0.344604, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:44.520104: step 4207, loss 0.146926, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:44.702636: step 4208, loss 0.225729, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:44.885819: step 4209, loss 0.296686, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:45.069347: step 4210, loss 0.31913, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:45.252898: step 4211, loss 0.287401, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:45.443961: step 4212, loss 0.194244, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:45.628715: step 4213, loss 0.21228, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:45.781369: step 4214, loss 0.268964, acc 0.941176, learning_rate 0.0001
2017-09-29T11:10:45.966306: step 4215, loss 0.313361, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:46.150781: step 4216, loss 0.182906, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:46.333507: step 4217, loss 0.246563, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:46.517181: step 4218, loss 0.184382, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:46.708828: step 4219, loss 0.0927906, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:46.892890: step 4220, loss 0.0959917, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:47.073529: step 4221, loss 0.266713, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:47.257170: step 4222, loss 0.204948, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:47.444823: step 4223, loss 0.132761, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:47.630171: step 4224, loss 0.178301, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:47.812363: step 4225, loss 0.286162, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:47.995983: step 4226, loss 0.177184, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:48.178591: step 4227, loss 0.273859, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:48.368766: step 4228, loss 0.183592, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:48.571812: step 4229, loss 0.268811, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:48.760806: step 4230, loss 0.256223, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:48.945599: step 4231, loss 0.333292, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:49.131946: step 4232, loss 0.147432, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:49.311326: step 4233, loss 0.22654, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:49.493512: step 4234, loss 0.234199, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:49.676179: step 4235, loss 0.113072, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:49.861902: step 4236, loss 0.262975, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:50.045777: step 4237, loss 0.176501, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:50.228490: step 4238, loss 0.231086, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:50.413409: step 4239, loss 0.13571, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:50.593154: step 4240, loss 0.371784, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-09-29T11:10:51.098298: step 4240, loss 0.29867, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4240

2017-09-29T11:10:51.729229: step 4241, loss 0.338168, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:51.911696: step 4242, loss 0.244429, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:52.090586: step 4243, loss 0.300266, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:52.274323: step 4244, loss 0.369252, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:52.462035: step 4245, loss 0.26533, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:52.643919: step 4246, loss 0.218684, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:52.825444: step 4247, loss 0.186192, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:53.013561: step 4248, loss 0.153605, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:53.200593: step 4249, loss 0.188731, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:53.384908: step 4250, loss 0.255181, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:53.571183: step 4251, loss 0.291021, acc 0.859375, learning_rate 0.0001
2017-09-29T11:10:53.751858: step 4252, loss 0.238201, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:53.930889: step 4253, loss 0.252141, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:54.116287: step 4254, loss 0.188652, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:54.328757: step 4255, loss 0.350141, acc 0.890625, learning_rate 0.0001
2017-09-29T11:10:54.523572: step 4256, loss 0.259453, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:54.705786: step 4257, loss 0.309898, acc 0.84375, learning_rate 0.0001
2017-09-29T11:10:54.887842: step 4258, loss 0.173113, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:55.070043: step 4259, loss 0.207791, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:55.256107: step 4260, loss 0.114018, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:55.437921: step 4261, loss 0.0871526, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:55.621589: step 4262, loss 0.31045, acc 0.84375, learning_rate 0.0001
2017-09-29T11:10:55.801826: step 4263, loss 0.222218, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:55.986493: step 4264, loss 0.167122, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:56.166765: step 4265, loss 0.302164, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:56.349578: step 4266, loss 0.177767, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:56.531581: step 4267, loss 0.315113, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:56.719930: step 4268, loss 0.214502, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:56.899671: step 4269, loss 0.171952, acc 0.9375, learning_rate 0.0001
2017-09-29T11:10:57.086553: step 4270, loss 0.210212, acc 0.953125, learning_rate 0.0001
2017-09-29T11:10:57.270429: step 4271, loss 0.230592, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:57.458785: step 4272, loss 0.210243, acc 0.96875, learning_rate 0.0001
2017-09-29T11:10:57.650988: step 4273, loss 0.10673, acc 0.984375, learning_rate 0.0001
2017-09-29T11:10:57.841146: step 4274, loss 0.241247, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:58.033010: step 4275, loss 0.191775, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:58.219245: step 4276, loss 0.275564, acc 0.90625, learning_rate 0.0001
2017-09-29T11:10:58.402906: step 4277, loss 0.187317, acc 0.921875, learning_rate 0.0001
2017-09-29T11:10:58.583024: step 4278, loss 0.304913, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:58.767659: step 4279, loss 0.30477, acc 0.875, learning_rate 0.0001
2017-09-29T11:10:58.955745: step 4280, loss 0.230693, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:10:59.468518: step 4280, loss 0.299119, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4280

2017-09-29T11:11:00.186451: step 4281, loss 0.359744, acc 0.84375, learning_rate 0.0001
2017-09-29T11:11:00.373341: step 4282, loss 0.262556, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:00.563438: step 4283, loss 0.297366, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:00.753089: step 4284, loss 0.179169, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:00.942279: step 4285, loss 0.287515, acc 0.84375, learning_rate 0.0001
2017-09-29T11:11:01.135079: step 4286, loss 0.27896, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:01.326798: step 4287, loss 0.217732, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:01.519375: step 4288, loss 0.269988, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:01.708558: step 4289, loss 0.208294, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:01.899165: step 4290, loss 0.311576, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:02.087732: step 4291, loss 0.162529, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:02.274922: step 4292, loss 0.203873, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:02.462494: step 4293, loss 0.260409, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:02.646856: step 4294, loss 0.171862, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:02.833833: step 4295, loss 0.101075, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:03.020946: step 4296, loss 0.300669, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:03.208365: step 4297, loss 0.143944, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:03.397213: step 4298, loss 0.169634, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:03.584009: step 4299, loss 0.306844, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:03.768795: step 4300, loss 0.19614, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:03.956912: step 4301, loss 0.27924, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:04.154092: step 4302, loss 0.172806, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:04.339170: step 4303, loss 0.219142, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:04.529222: step 4304, loss 0.231149, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:04.716295: step 4305, loss 0.240188, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:04.906074: step 4306, loss 0.152446, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:05.093668: step 4307, loss 0.151557, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:05.278604: step 4308, loss 0.219684, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:05.478475: step 4309, loss 0.34546, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:05.680674: step 4310, loss 0.377722, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:05.869091: step 4311, loss 0.263346, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:06.025891: step 4312, loss 0.10615, acc 0.980392, learning_rate 0.0001
2017-09-29T11:11:06.220955: step 4313, loss 0.214327, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:06.410388: step 4314, loss 0.213683, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:06.600648: step 4315, loss 0.206848, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:06.797104: step 4316, loss 0.303254, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:06.978348: step 4317, loss 0.205254, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:07.161486: step 4318, loss 0.309585, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:07.344610: step 4319, loss 0.0908608, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:07.531068: step 4320, loss 0.184033, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:11:08.029627: step 4320, loss 0.292821, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4320

2017-09-29T11:11:08.726018: step 4321, loss 0.156897, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:08.913395: step 4322, loss 0.240703, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:09.096337: step 4323, loss 0.147024, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:09.280789: step 4324, loss 0.180234, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:09.471275: step 4325, loss 0.260736, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:09.655689: step 4326, loss 0.208119, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:09.837454: step 4327, loss 0.179282, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:10.021165: step 4328, loss 0.116553, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:10.199794: step 4329, loss 0.188234, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:10.393351: step 4330, loss 0.317603, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:10.578668: step 4331, loss 0.132722, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:10.769433: step 4332, loss 0.232556, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:10.952070: step 4333, loss 0.199544, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:11.138222: step 4334, loss 0.203887, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:11.318826: step 4335, loss 0.261626, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:11.515657: step 4336, loss 0.224559, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:11.708386: step 4337, loss 0.217155, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:11.904786: step 4338, loss 0.187132, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:12.086178: step 4339, loss 0.323575, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:12.268344: step 4340, loss 0.132038, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:12.450089: step 4341, loss 0.239588, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:12.637519: step 4342, loss 0.211813, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:12.821779: step 4343, loss 0.0820325, acc 0.984375, learning_rate 0.0001
2017-09-29T11:11:13.014028: step 4344, loss 0.408836, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:13.197564: step 4345, loss 0.202624, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:13.383249: step 4346, loss 0.291776, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:13.568983: step 4347, loss 0.140821, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:13.753838: step 4348, loss 0.228401, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:13.936937: step 4349, loss 0.19118, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:14.118973: step 4350, loss 0.230142, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:14.305118: step 4351, loss 0.196838, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:14.488861: step 4352, loss 0.173191, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:14.674384: step 4353, loss 0.128906, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:14.857508: step 4354, loss 0.265724, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:15.037322: step 4355, loss 0.189457, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:15.220102: step 4356, loss 0.284031, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:15.405329: step 4357, loss 0.228676, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:15.587600: step 4358, loss 0.340576, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:15.770487: step 4359, loss 0.128214, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:15.958439: step 4360, loss 0.206807, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:11:16.464711: step 4360, loss 0.296209, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4360

2017-09-29T11:11:17.263396: step 4361, loss 0.207063, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:17.451147: step 4362, loss 0.142493, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:17.642974: step 4363, loss 0.152597, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:17.829164: step 4364, loss 0.253508, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:18.011501: step 4365, loss 0.192874, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:18.192456: step 4366, loss 0.163603, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:18.370498: step 4367, loss 0.13652, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:18.553466: step 4368, loss 0.363168, acc 0.84375, learning_rate 0.0001
2017-09-29T11:11:18.744373: step 4369, loss 0.249448, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:18.930686: step 4370, loss 0.164729, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:19.116456: step 4371, loss 0.269945, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:19.299221: step 4372, loss 0.206222, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:19.484676: step 4373, loss 0.221168, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:19.671476: step 4374, loss 0.220004, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:19.853703: step 4375, loss 0.153854, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:20.037027: step 4376, loss 0.213455, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:20.218349: step 4377, loss 0.291221, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:20.401243: step 4378, loss 0.202425, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:20.581165: step 4379, loss 0.268538, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:20.768149: step 4380, loss 0.262264, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:20.953574: step 4381, loss 0.209676, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:21.135985: step 4382, loss 0.148873, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:21.322563: step 4383, loss 0.271517, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:21.505251: step 4384, loss 0.343367, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:21.693021: step 4385, loss 0.224797, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:21.881602: step 4386, loss 0.242715, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:22.066159: step 4387, loss 0.305603, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:22.249924: step 4388, loss 0.275921, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:22.446143: step 4389, loss 0.148593, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:22.630586: step 4390, loss 0.225502, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:22.814146: step 4391, loss 0.27643, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:22.994846: step 4392, loss 0.30503, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:23.185060: step 4393, loss 0.208924, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:23.373496: step 4394, loss 0.189879, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:23.559497: step 4395, loss 0.136297, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:23.753173: step 4396, loss 0.300606, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:23.934554: step 4397, loss 0.140109, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:24.119574: step 4398, loss 0.207866, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:24.303304: step 4399, loss 0.265455, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:24.490960: step 4400, loss 0.178705, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:11:24.986855: step 4400, loss 0.302957, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4400

2017-09-29T11:11:25.612931: step 4401, loss 0.177218, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:25.797462: step 4402, loss 0.226946, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:25.978201: step 4403, loss 0.230415, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:26.161570: step 4404, loss 0.325608, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:26.352524: step 4405, loss 0.19902, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:26.544949: step 4406, loss 0.143023, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:26.736263: step 4407, loss 0.201575, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:26.943724: step 4408, loss 0.209124, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:27.130779: step 4409, loss 0.177392, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:27.303805: step 4410, loss 0.224537, acc 0.941176, learning_rate 0.0001
2017-09-29T11:11:27.491241: step 4411, loss 0.19289, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:27.676270: step 4412, loss 0.253415, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:27.873900: step 4413, loss 0.144478, acc 0.984375, learning_rate 0.0001
2017-09-29T11:11:28.061181: step 4414, loss 0.165076, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:28.251048: step 4415, loss 0.299586, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:28.446565: step 4416, loss 0.116632, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:28.637556: step 4417, loss 0.203391, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:28.830514: step 4418, loss 0.26261, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:29.016880: step 4419, loss 0.162811, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:29.208494: step 4420, loss 0.104946, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:29.408848: step 4421, loss 0.120282, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:29.598122: step 4422, loss 0.102977, acc 1, learning_rate 0.0001
2017-09-29T11:11:29.786698: step 4423, loss 0.141095, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:29.978171: step 4424, loss 0.190104, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:30.170365: step 4425, loss 0.22447, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:30.355189: step 4426, loss 0.24715, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:30.541240: step 4427, loss 0.24704, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:30.723541: step 4428, loss 0.240192, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:30.906930: step 4429, loss 0.116297, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:31.091301: step 4430, loss 0.266046, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:31.271863: step 4431, loss 0.236643, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:31.462639: step 4432, loss 0.195884, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:31.643910: step 4433, loss 0.148469, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:31.827374: step 4434, loss 0.154037, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:32.015397: step 4435, loss 0.188279, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:32.197659: step 4436, loss 0.160927, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:32.380474: step 4437, loss 0.238827, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:32.562634: step 4438, loss 0.193913, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:32.753210: step 4439, loss 0.188443, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:32.933640: step 4440, loss 0.289616, acc 0.875, learning_rate 0.0001

Evaluation:
2017-09-29T11:11:33.423618: step 4440, loss 0.30136, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4440

2017-09-29T11:11:34.139692: step 4441, loss 0.197748, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:34.344201: step 4442, loss 0.212361, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:34.532641: step 4443, loss 0.231096, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:34.725624: step 4444, loss 0.156445, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:34.907429: step 4445, loss 0.213207, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:35.091977: step 4446, loss 0.19082, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:35.274423: step 4447, loss 0.426574, acc 0.828125, learning_rate 0.0001
2017-09-29T11:11:35.459725: step 4448, loss 0.246317, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:35.646123: step 4449, loss 0.30149, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:35.829127: step 4450, loss 0.188673, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:36.014956: step 4451, loss 0.102517, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:36.199393: step 4452, loss 0.227587, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:36.381196: step 4453, loss 0.173492, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:36.566816: step 4454, loss 0.158347, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:36.750135: step 4455, loss 0.182062, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:36.934685: step 4456, loss 0.175723, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:37.119932: step 4457, loss 0.122705, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:37.301511: step 4458, loss 0.254417, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:37.496025: step 4459, loss 0.211188, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:37.681112: step 4460, loss 0.284375, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:37.872068: step 4461, loss 0.340311, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:38.058337: step 4462, loss 0.202152, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:38.238948: step 4463, loss 0.31364, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:38.424204: step 4464, loss 0.146787, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:38.609976: step 4465, loss 0.282674, acc 0.84375, learning_rate 0.0001
2017-09-29T11:11:38.793215: step 4466, loss 0.221992, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:38.975528: step 4467, loss 0.229982, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:39.159178: step 4468, loss 0.169384, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:39.343491: step 4469, loss 0.225815, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:39.538077: step 4470, loss 0.240527, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:39.722304: step 4471, loss 0.287327, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:39.907611: step 4472, loss 0.238215, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:40.088096: step 4473, loss 0.108426, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:40.278300: step 4474, loss 0.230125, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:40.463801: step 4475, loss 0.224267, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:40.647752: step 4476, loss 0.263245, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:40.834186: step 4477, loss 0.277512, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:41.014189: step 4478, loss 0.200848, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:41.199750: step 4479, loss 0.131487, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:41.387084: step 4480, loss 0.36695, acc 0.875, learning_rate 0.0001

Evaluation:
2017-09-29T11:11:41.884775: step 4480, loss 0.297241, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4480

2017-09-29T11:11:42.590867: step 4481, loss 0.207877, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:42.784165: step 4482, loss 0.130581, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:42.968358: step 4483, loss 0.193054, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:43.151083: step 4484, loss 0.153751, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:43.338278: step 4485, loss 0.303724, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:43.535582: step 4486, loss 0.187565, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:43.721091: step 4487, loss 0.362744, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:43.910280: step 4488, loss 0.233105, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:44.092748: step 4489, loss 0.126846, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:44.274439: step 4490, loss 0.193713, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:44.456413: step 4491, loss 0.180969, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:44.647413: step 4492, loss 0.210714, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:44.832592: step 4493, loss 0.322088, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:45.017164: step 4494, loss 0.10479, acc 1, learning_rate 0.0001
2017-09-29T11:11:45.206107: step 4495, loss 0.224602, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:45.394197: step 4496, loss 0.3909, acc 0.859375, learning_rate 0.0001
2017-09-29T11:11:45.592290: step 4497, loss 0.175495, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:45.779228: step 4498, loss 0.15364, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:45.962527: step 4499, loss 0.159388, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:46.157698: step 4500, loss 0.281228, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:46.355085: step 4501, loss 0.220339, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:46.543502: step 4502, loss 0.248286, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:46.729739: step 4503, loss 0.16845, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:46.912154: step 4504, loss 0.284848, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:47.100379: step 4505, loss 0.377304, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:47.286270: step 4506, loss 0.176134, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:47.474649: step 4507, loss 0.179251, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:47.627140: step 4508, loss 0.22112, acc 0.921569, learning_rate 0.0001
2017-09-29T11:11:47.817290: step 4509, loss 0.228652, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:48.001506: step 4510, loss 0.132335, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:48.187617: step 4511, loss 0.177351, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:48.371777: step 4512, loss 0.268669, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:48.565110: step 4513, loss 0.233092, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:48.751702: step 4514, loss 0.255635, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:48.935956: step 4515, loss 0.288702, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:49.118787: step 4516, loss 0.163057, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:49.303577: step 4517, loss 0.11063, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:49.508369: step 4518, loss 0.250398, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:49.711003: step 4519, loss 0.336545, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:49.907848: step 4520, loss 0.239838, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:11:50.433732: step 4520, loss 0.291993, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4520

2017-09-29T11:11:51.229690: step 4521, loss 0.239282, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:51.431403: step 4522, loss 0.204581, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:51.628402: step 4523, loss 0.239228, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:51.810733: step 4524, loss 0.271024, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:51.993471: step 4525, loss 0.216897, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:52.182161: step 4526, loss 0.1745, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:52.368144: step 4527, loss 0.257683, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:52.555752: step 4528, loss 0.235165, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:52.743455: step 4529, loss 0.346432, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:52.931295: step 4530, loss 0.177734, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:53.116829: step 4531, loss 0.370529, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:53.300638: step 4532, loss 0.158984, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:53.488843: step 4533, loss 0.144673, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:53.676734: step 4534, loss 0.323373, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:53.868076: step 4535, loss 0.185092, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:54.050267: step 4536, loss 0.226334, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:54.238014: step 4537, loss 0.139432, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:54.423122: step 4538, loss 0.223358, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:54.611924: step 4539, loss 0.165084, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:54.793511: step 4540, loss 0.305208, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:54.979547: step 4541, loss 0.198529, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:55.174730: step 4542, loss 0.262762, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:55.362190: step 4543, loss 0.284527, acc 0.890625, learning_rate 0.0001
2017-09-29T11:11:55.551399: step 4544, loss 0.163927, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:55.737978: step 4545, loss 0.102022, acc 0.984375, learning_rate 0.0001
2017-09-29T11:11:55.926572: step 4546, loss 0.128285, acc 0.953125, learning_rate 0.0001
2017-09-29T11:11:56.112462: step 4547, loss 0.190113, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:56.297623: step 4548, loss 0.155324, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:56.495708: step 4549, loss 0.101946, acc 0.984375, learning_rate 0.0001
2017-09-29T11:11:56.688363: step 4550, loss 0.255142, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:56.891891: step 4551, loss 0.194086, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:57.079744: step 4552, loss 0.26746, acc 0.875, learning_rate 0.0001
2017-09-29T11:11:57.269168: step 4553, loss 0.164375, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:57.452989: step 4554, loss 0.182066, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:57.641974: step 4555, loss 0.255796, acc 0.9375, learning_rate 0.0001
2017-09-29T11:11:57.833277: step 4556, loss 0.177743, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:58.020911: step 4557, loss 0.217385, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:58.208776: step 4558, loss 0.256046, acc 0.90625, learning_rate 0.0001
2017-09-29T11:11:58.390985: step 4559, loss 0.209983, acc 0.921875, learning_rate 0.0001
2017-09-29T11:11:58.572612: step 4560, loss 0.147599, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:11:59.061396: step 4560, loss 0.300779, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4560

2017-09-29T11:11:59.686531: step 4561, loss 0.108766, acc 0.96875, learning_rate 0.0001
2017-09-29T11:11:59.877467: step 4562, loss 0.180917, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:00.069969: step 4563, loss 0.160272, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:00.253381: step 4564, loss 0.213481, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:00.438965: step 4565, loss 0.18777, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:00.623910: step 4566, loss 0.17143, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:00.811502: step 4567, loss 0.198311, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:00.996272: step 4568, loss 0.267512, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:01.181265: step 4569, loss 0.219778, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:01.370766: step 4570, loss 0.114509, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:01.565685: step 4571, loss 0.212624, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:01.757719: step 4572, loss 0.188172, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:01.942300: step 4573, loss 0.0901392, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:02.132094: step 4574, loss 0.165692, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:02.320016: step 4575, loss 0.264336, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:02.516765: step 4576, loss 0.253098, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:02.699465: step 4577, loss 0.145441, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:02.888314: step 4578, loss 0.217616, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:03.068520: step 4579, loss 0.298128, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:03.249195: step 4580, loss 0.301225, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:03.436793: step 4581, loss 0.238795, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:03.623573: step 4582, loss 0.213889, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:03.807094: step 4583, loss 0.259863, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:03.989662: step 4584, loss 0.24356, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:04.180171: step 4585, loss 0.235108, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:04.365978: step 4586, loss 0.265329, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:04.555962: step 4587, loss 0.191346, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:04.739165: step 4588, loss 0.297444, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:04.941099: step 4589, loss 0.190586, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:05.126060: step 4590, loss 0.343029, acc 0.859375, learning_rate 0.0001
2017-09-29T11:12:05.314912: step 4591, loss 0.199622, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:05.504828: step 4592, loss 0.240859, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:05.692516: step 4593, loss 0.226242, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:05.878251: step 4594, loss 0.249265, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:06.061410: step 4595, loss 0.180882, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:06.248103: step 4596, loss 0.155543, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:06.429310: step 4597, loss 0.243524, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:06.624465: step 4598, loss 0.453395, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:06.833850: step 4599, loss 0.197637, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:07.017539: step 4600, loss 0.221449, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:12:07.540600: step 4600, loss 0.296147, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4600

2017-09-29T11:12:08.262868: step 4601, loss 0.454741, acc 0.796875, learning_rate 0.0001
2017-09-29T11:12:08.458446: step 4602, loss 0.196155, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:08.647708: step 4603, loss 0.175198, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:08.836474: step 4604, loss 0.287737, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:09.024957: step 4605, loss 0.2123, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:09.186692: step 4606, loss 0.103011, acc 0.980392, learning_rate 0.0001
2017-09-29T11:12:09.379219: step 4607, loss 0.248205, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:09.568442: step 4608, loss 0.341119, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:09.758170: step 4609, loss 0.25047, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:09.942308: step 4610, loss 0.205035, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:10.130720: step 4611, loss 0.242619, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:10.320536: step 4612, loss 0.209716, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:10.509856: step 4613, loss 0.236652, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:10.700554: step 4614, loss 0.25279, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:10.891142: step 4615, loss 0.160918, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:11.080351: step 4616, loss 0.271217, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:11.267941: step 4617, loss 0.200514, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:11.468321: step 4618, loss 0.112645, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:11.659821: step 4619, loss 0.169629, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:11.858279: step 4620, loss 0.189685, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:12.040873: step 4621, loss 0.116828, acc 0.984375, learning_rate 0.0001
2017-09-29T11:12:12.235950: step 4622, loss 0.203923, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:12.424149: step 4623, loss 0.140249, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:12.611003: step 4624, loss 0.260864, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:12.800226: step 4625, loss 0.270849, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:12.986923: step 4626, loss 0.301835, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:13.171946: step 4627, loss 0.19452, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:13.356070: step 4628, loss 0.150148, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:13.551253: step 4629, loss 0.160435, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:13.742680: step 4630, loss 0.181064, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:13.929141: step 4631, loss 0.176698, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:14.113336: step 4632, loss 0.234472, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:14.299419: step 4633, loss 0.212318, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:14.485003: step 4634, loss 0.16034, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:14.670273: step 4635, loss 0.239064, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:14.851488: step 4636, loss 0.197642, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:15.034650: step 4637, loss 0.179417, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:15.218237: step 4638, loss 0.21488, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:15.401929: step 4639, loss 0.230421, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:15.585576: step 4640, loss 0.124366, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:12:16.089994: step 4640, loss 0.299444, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4640

2017-09-29T11:12:16.801482: step 4641, loss 0.219453, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:16.985023: step 4642, loss 0.0791094, acc 0.984375, learning_rate 0.0001
2017-09-29T11:12:17.169985: step 4643, loss 0.157888, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:17.355922: step 4644, loss 0.325923, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:17.537064: step 4645, loss 0.201254, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:17.724019: step 4646, loss 0.182398, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:17.905022: step 4647, loss 0.208716, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:18.092296: step 4648, loss 0.170677, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:18.276772: step 4649, loss 0.14573, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:18.479854: step 4650, loss 0.177059, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:18.664233: step 4651, loss 0.222903, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:18.847760: step 4652, loss 0.178937, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:19.033232: step 4653, loss 0.260755, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:19.217459: step 4654, loss 0.157567, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:19.403260: step 4655, loss 0.460608, acc 0.796875, learning_rate 0.0001
2017-09-29T11:12:19.590829: step 4656, loss 0.172801, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:19.773173: step 4657, loss 0.206886, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:19.954238: step 4658, loss 0.280429, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:20.138697: step 4659, loss 0.145758, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:20.321036: step 4660, loss 0.221587, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:20.503650: step 4661, loss 0.260384, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:20.689027: step 4662, loss 0.133401, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:20.874472: step 4663, loss 0.306932, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:21.055724: step 4664, loss 0.182984, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:21.243629: step 4665, loss 0.278104, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:21.427099: step 4666, loss 0.32022, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:21.610356: step 4667, loss 0.346948, acc 0.828125, learning_rate 0.0001
2017-09-29T11:12:21.801270: step 4668, loss 0.304843, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:21.981695: step 4669, loss 0.159707, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:22.162831: step 4670, loss 0.284175, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:22.352173: step 4671, loss 0.215578, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:22.534167: step 4672, loss 0.220873, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:22.721999: step 4673, loss 0.248762, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:22.899123: step 4674, loss 0.22981, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:23.084287: step 4675, loss 0.285356, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:23.266009: step 4676, loss 0.195446, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:23.452066: step 4677, loss 0.234019, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:23.644616: step 4678, loss 0.18988, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:23.836661: step 4679, loss 0.305396, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:24.018318: step 4680, loss 0.346187, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-09-29T11:12:24.541409: step 4680, loss 0.29465, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4680

2017-09-29T11:12:25.337334: step 4681, loss 0.171935, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:25.520719: step 4682, loss 0.174599, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:25.703837: step 4683, loss 0.265955, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:25.885951: step 4684, loss 0.237407, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:26.069485: step 4685, loss 0.156939, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:26.254084: step 4686, loss 0.182299, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:26.439261: step 4687, loss 0.237791, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:26.622192: step 4688, loss 0.258825, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:26.806880: step 4689, loss 0.300486, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:26.988983: step 4690, loss 0.115973, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:27.180991: step 4691, loss 0.149602, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:27.372905: step 4692, loss 0.249972, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:27.557915: step 4693, loss 0.14231, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:27.744032: step 4694, loss 0.328513, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:27.932068: step 4695, loss 0.131264, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:28.120776: step 4696, loss 0.321872, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:28.301323: step 4697, loss 0.278834, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:28.486007: step 4698, loss 0.124626, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:28.671951: step 4699, loss 0.217026, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:28.853346: step 4700, loss 0.162137, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:29.046289: step 4701, loss 0.158743, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:29.232731: step 4702, loss 0.173459, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:29.419253: step 4703, loss 0.31893, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:29.571562: step 4704, loss 0.192106, acc 0.941176, learning_rate 0.0001
2017-09-29T11:12:29.758462: step 4705, loss 0.208155, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:29.957776: step 4706, loss 0.20229, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:30.159043: step 4707, loss 0.112993, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:30.365970: step 4708, loss 0.195964, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:30.585036: step 4709, loss 0.201175, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:30.794408: step 4710, loss 0.257175, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:30.994539: step 4711, loss 0.169536, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:31.196184: step 4712, loss 0.212016, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:31.382048: step 4713, loss 0.195354, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:31.584487: step 4714, loss 0.121377, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:31.772660: step 4715, loss 0.178818, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:31.963576: step 4716, loss 0.209086, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:32.147428: step 4717, loss 0.241527, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:32.335854: step 4718, loss 0.180723, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:32.530959: step 4719, loss 0.223352, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:32.715962: step 4720, loss 0.0949446, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:12:33.225530: step 4720, loss 0.298155, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4720

2017-09-29T11:12:33.857061: step 4721, loss 0.372613, acc 0.8125, learning_rate 0.0001
2017-09-29T11:12:34.041292: step 4722, loss 0.199513, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:34.225726: step 4723, loss 0.0992047, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:34.409435: step 4724, loss 0.194985, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:34.594053: step 4725, loss 0.123879, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:34.778638: step 4726, loss 0.205641, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:34.965402: step 4727, loss 0.269186, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:35.146393: step 4728, loss 0.135406, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:35.334592: step 4729, loss 0.148068, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:35.524790: step 4730, loss 0.267575, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:35.710663: step 4731, loss 0.179928, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:35.891405: step 4732, loss 0.23266, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:36.083135: step 4733, loss 0.149445, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:36.266506: step 4734, loss 0.32424, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:36.463659: step 4735, loss 0.418459, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:36.645490: step 4736, loss 0.108697, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:36.827982: step 4737, loss 0.227393, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:37.010116: step 4738, loss 0.167045, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:37.193029: step 4739, loss 0.265426, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:37.382672: step 4740, loss 0.247626, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:37.563174: step 4741, loss 0.261268, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:37.749064: step 4742, loss 0.341714, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:37.931863: step 4743, loss 0.237309, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:38.118436: step 4744, loss 0.0959564, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:38.304238: step 4745, loss 0.0878801, acc 0.984375, learning_rate 0.0001
2017-09-29T11:12:38.491477: step 4746, loss 0.160201, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:38.684974: step 4747, loss 0.172972, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:38.868932: step 4748, loss 0.181672, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:39.054652: step 4749, loss 0.155893, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:39.237512: step 4750, loss 0.180506, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:39.422173: step 4751, loss 0.189953, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:39.605856: step 4752, loss 0.236784, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:39.790261: step 4753, loss 0.190069, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:39.971712: step 4754, loss 0.186903, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:40.164279: step 4755, loss 0.260498, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:40.351578: step 4756, loss 0.359441, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:40.532531: step 4757, loss 0.219506, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:40.718090: step 4758, loss 0.213824, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:40.903744: step 4759, loss 0.270326, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:41.086639: step 4760, loss 0.158598, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:12:41.596781: step 4760, loss 0.292264, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4760

2017-09-29T11:12:42.314477: step 4761, loss 0.250372, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:42.527897: step 4762, loss 0.163687, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:42.719508: step 4763, loss 0.211412, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:42.911554: step 4764, loss 0.155881, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:43.097928: step 4765, loss 0.160494, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:43.283049: step 4766, loss 0.143976, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:43.471739: step 4767, loss 0.100504, acc 0.984375, learning_rate 0.0001
2017-09-29T11:12:43.662148: step 4768, loss 0.111368, acc 0.984375, learning_rate 0.0001
2017-09-29T11:12:43.848902: step 4769, loss 0.111982, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:44.036442: step 4770, loss 0.152068, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:44.229859: step 4771, loss 0.146586, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:44.418998: step 4772, loss 0.175543, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:44.602033: step 4773, loss 0.128224, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:44.785735: step 4774, loss 0.331804, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:44.974211: step 4775, loss 0.221098, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:45.158739: step 4776, loss 0.0989832, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:45.349560: step 4777, loss 0.228764, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:45.536414: step 4778, loss 0.285084, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:45.727225: step 4779, loss 0.325227, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:45.919734: step 4780, loss 0.130802, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:46.104218: step 4781, loss 0.153196, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:46.294900: step 4782, loss 0.207601, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:46.478590: step 4783, loss 0.146333, acc 0.984375, learning_rate 0.0001
2017-09-29T11:12:46.663809: step 4784, loss 0.153364, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:46.846085: step 4785, loss 0.174172, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:47.025437: step 4786, loss 0.318906, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:47.207988: step 4787, loss 0.201029, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:47.401299: step 4788, loss 0.241774, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:47.590099: step 4789, loss 0.332432, acc 0.859375, learning_rate 0.0001
2017-09-29T11:12:47.776503: step 4790, loss 0.17856, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:47.958112: step 4791, loss 0.3087, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:48.146686: step 4792, loss 0.181086, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:48.331204: step 4793, loss 0.245551, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:48.534345: step 4794, loss 0.200312, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:48.717669: step 4795, loss 0.249906, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:48.900543: step 4796, loss 0.332085, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:49.095753: step 4797, loss 0.222699, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:49.278747: step 4798, loss 0.277353, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:49.465986: step 4799, loss 0.168535, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:49.655228: step 4800, loss 0.214597, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:12:50.165163: step 4800, loss 0.290183, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4800

2017-09-29T11:12:50.862176: step 4801, loss 0.245499, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:51.016653: step 4802, loss 0.207397, acc 0.941176, learning_rate 0.0001
2017-09-29T11:12:51.198731: step 4803, loss 0.202184, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:51.382167: step 4804, loss 0.1831, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:51.570503: step 4805, loss 0.180867, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:51.752193: step 4806, loss 0.203238, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:51.943926: step 4807, loss 0.24009, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:52.126002: step 4808, loss 0.150828, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:52.312041: step 4809, loss 0.2773, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:52.501384: step 4810, loss 0.168029, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:52.684826: step 4811, loss 0.118384, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:52.866445: step 4812, loss 0.240842, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:53.050484: step 4813, loss 0.119817, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:53.236492: step 4814, loss 0.240858, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:53.419927: step 4815, loss 0.191557, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:53.605560: step 4816, loss 0.156417, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:53.787643: step 4817, loss 0.175264, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:53.967471: step 4818, loss 0.196653, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:54.163452: step 4819, loss 0.220713, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:54.349238: step 4820, loss 0.250184, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:54.542500: step 4821, loss 0.308774, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:54.732367: step 4822, loss 0.14185, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:54.916071: step 4823, loss 0.294304, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:55.100612: step 4824, loss 0.112411, acc 0.96875, learning_rate 0.0001
2017-09-29T11:12:55.286265: step 4825, loss 0.153124, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:55.480501: step 4826, loss 0.165654, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:55.669783: step 4827, loss 0.172655, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:55.858558: step 4828, loss 0.381877, acc 0.84375, learning_rate 0.0001
2017-09-29T11:12:56.043070: step 4829, loss 0.273888, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:56.230109: step 4830, loss 0.207417, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:56.412445: step 4831, loss 0.191855, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:56.596657: step 4832, loss 0.165468, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:56.781052: step 4833, loss 0.199854, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:56.964751: step 4834, loss 0.144685, acc 0.953125, learning_rate 0.0001
2017-09-29T11:12:57.156933: step 4835, loss 0.205593, acc 0.921875, learning_rate 0.0001
2017-09-29T11:12:57.342477: step 4836, loss 0.320394, acc 0.875, learning_rate 0.0001
2017-09-29T11:12:57.535799: step 4837, loss 0.209586, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:57.720081: step 4838, loss 0.0981179, acc 0.984375, learning_rate 0.0001
2017-09-29T11:12:57.901014: step 4839, loss 0.338747, acc 0.859375, learning_rate 0.0001
2017-09-29T11:12:58.086326: step 4840, loss 0.396326, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-09-29T11:12:58.600182: step 4840, loss 0.297374, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4840

2017-09-29T11:12:59.382392: step 4841, loss 0.285256, acc 0.90625, learning_rate 0.0001
2017-09-29T11:12:59.566813: step 4842, loss 0.356167, acc 0.890625, learning_rate 0.0001
2017-09-29T11:12:59.749825: step 4843, loss 0.178312, acc 0.9375, learning_rate 0.0001
2017-09-29T11:12:59.940376: step 4844, loss 0.281737, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:00.130822: step 4845, loss 0.253164, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:00.325552: step 4846, loss 0.113526, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:00.524673: step 4847, loss 0.174219, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:00.708335: step 4848, loss 0.205609, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:00.893114: step 4849, loss 0.219674, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:01.076900: step 4850, loss 0.266081, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:01.262955: step 4851, loss 0.372848, acc 0.859375, learning_rate 0.0001
2017-09-29T11:13:01.447909: step 4852, loss 0.178763, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:01.634802: step 4853, loss 0.222875, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:01.819375: step 4854, loss 0.225834, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:01.999188: step 4855, loss 0.169325, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:02.181681: step 4856, loss 0.209388, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:02.362975: step 4857, loss 0.21524, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:02.553265: step 4858, loss 0.210759, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:02.735970: step 4859, loss 0.232668, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:02.928312: step 4860, loss 0.166069, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:03.115200: step 4861, loss 0.206422, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:03.303025: step 4862, loss 0.167784, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:03.490992: step 4863, loss 0.254969, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:03.678198: step 4864, loss 0.19159, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:03.872148: step 4865, loss 0.269768, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:04.052275: step 4866, loss 0.181508, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:04.244014: step 4867, loss 0.23876, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:04.441900: step 4868, loss 0.386403, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:04.625767: step 4869, loss 0.245973, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:04.809279: step 4870, loss 0.197606, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:04.991018: step 4871, loss 0.209119, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:05.176001: step 4872, loss 0.364423, acc 0.84375, learning_rate 0.0001
2017-09-29T11:13:05.363008: step 4873, loss 0.0713495, acc 1, learning_rate 0.0001
2017-09-29T11:13:05.556650: step 4874, loss 0.172956, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:05.746182: step 4875, loss 0.212169, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:05.933493: step 4876, loss 0.0965386, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:06.117890: step 4877, loss 0.070526, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:06.307857: step 4878, loss 0.200078, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:06.508809: step 4879, loss 0.184052, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:06.697179: step 4880, loss 0.159548, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:13:07.186407: step 4880, loss 0.293877, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4880

2017-09-29T11:13:07.821555: step 4881, loss 0.163917, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:08.006248: step 4882, loss 0.235257, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:08.200815: step 4883, loss 0.170814, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:08.394232: step 4884, loss 0.256365, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:08.590595: step 4885, loss 0.188041, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:08.785156: step 4886, loss 0.191414, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:08.993223: step 4887, loss 0.131037, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:09.183025: step 4888, loss 0.118923, acc 1, learning_rate 0.0001
2017-09-29T11:13:09.372138: step 4889, loss 0.225451, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:09.561328: step 4890, loss 0.167712, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:09.745352: step 4891, loss 0.174404, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:09.930892: step 4892, loss 0.307369, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:10.113417: step 4893, loss 0.170329, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:10.302621: step 4894, loss 0.256566, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:10.488305: step 4895, loss 0.115799, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:10.673897: step 4896, loss 0.206099, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:10.856775: step 4897, loss 0.225614, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:11.041999: step 4898, loss 0.138139, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:11.228397: step 4899, loss 0.232348, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:11.384287: step 4900, loss 0.380575, acc 0.843137, learning_rate 0.0001
2017-09-29T11:13:11.574688: step 4901, loss 0.155723, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:11.761738: step 4902, loss 0.144536, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:11.964970: step 4903, loss 0.213986, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:12.174713: step 4904, loss 0.110483, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:12.382539: step 4905, loss 0.155706, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:12.577560: step 4906, loss 0.112038, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:12.767034: step 4907, loss 0.209291, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:12.952299: step 4908, loss 0.300356, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:13.137432: step 4909, loss 0.19478, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:13.327934: step 4910, loss 0.19547, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:13.516576: step 4911, loss 0.123909, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:13.700038: step 4912, loss 0.156785, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:13.891464: step 4913, loss 0.166774, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:14.078656: step 4914, loss 0.175493, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:14.286203: step 4915, loss 0.228316, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:14.485666: step 4916, loss 0.188165, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:14.675895: step 4917, loss 0.149912, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:14.861048: step 4918, loss 0.222842, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:15.050717: step 4919, loss 0.106973, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:15.239804: step 4920, loss 0.222328, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:13:15.749421: step 4920, loss 0.286652, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4920

2017-09-29T11:13:16.470304: step 4921, loss 0.200588, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:16.656870: step 4922, loss 0.238066, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:16.839016: step 4923, loss 0.193238, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:17.022772: step 4924, loss 0.212, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:17.205735: step 4925, loss 0.223662, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:17.389486: step 4926, loss 0.123053, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:17.574903: step 4927, loss 0.253478, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:17.781425: step 4928, loss 0.229495, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:17.976099: step 4929, loss 0.181062, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:18.161544: step 4930, loss 0.222145, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:18.345431: step 4931, loss 0.15451, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:18.540271: step 4932, loss 0.240863, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:18.749122: step 4933, loss 0.161842, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:18.943486: step 4934, loss 0.165242, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:19.128235: step 4935, loss 0.226258, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:19.315407: step 4936, loss 0.116171, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:19.500473: step 4937, loss 0.224169, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:19.688025: step 4938, loss 0.178541, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:19.882476: step 4939, loss 0.183653, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:20.069635: step 4940, loss 0.140721, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:20.265124: step 4941, loss 0.253608, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:20.456496: step 4942, loss 0.203602, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:20.649315: step 4943, loss 0.248224, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:20.834662: step 4944, loss 0.339089, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:21.022681: step 4945, loss 0.311302, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:21.205899: step 4946, loss 0.235554, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:21.391995: step 4947, loss 0.246137, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:21.574159: step 4948, loss 0.237187, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:21.755646: step 4949, loss 0.22035, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:21.937924: step 4950, loss 0.181754, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:22.118906: step 4951, loss 0.194517, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:22.303547: step 4952, loss 0.205819, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:22.488807: step 4953, loss 0.215548, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:22.682375: step 4954, loss 0.191395, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:22.865105: step 4955, loss 0.21244, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:23.059519: step 4956, loss 0.287991, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:23.259107: step 4957, loss 0.111385, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:23.459181: step 4958, loss 0.320453, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:23.653831: step 4959, loss 0.261531, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:23.851081: step 4960, loss 0.231704, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:13:24.360147: step 4960, loss 0.289123, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-4960

2017-09-29T11:13:25.077291: step 4961, loss 0.310923, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:25.257795: step 4962, loss 0.269901, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:25.451483: step 4963, loss 0.199346, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:25.646911: step 4964, loss 0.178915, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:25.849516: step 4965, loss 0.232741, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:26.047509: step 4966, loss 0.165351, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:26.235411: step 4967, loss 0.310785, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:26.424040: step 4968, loss 0.133253, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:26.609681: step 4969, loss 0.154975, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:26.808193: step 4970, loss 0.171271, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:26.990106: step 4971, loss 0.273313, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:27.178438: step 4972, loss 0.241089, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:27.366217: step 4973, loss 0.248756, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:27.551043: step 4974, loss 0.200698, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:27.743941: step 4975, loss 0.10414, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:27.925543: step 4976, loss 0.219665, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:28.108731: step 4977, loss 0.210176, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:28.300451: step 4978, loss 0.271824, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:28.489924: step 4979, loss 0.170987, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:28.673767: step 4980, loss 0.285023, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:28.859109: step 4981, loss 0.187741, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:29.042776: step 4982, loss 0.108655, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:29.241889: step 4983, loss 0.235051, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:29.447591: step 4984, loss 0.214732, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:29.640129: step 4985, loss 0.191946, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:29.822595: step 4986, loss 0.224215, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:30.012024: step 4987, loss 0.119921, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:30.198990: step 4988, loss 0.336874, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:30.384276: step 4989, loss 0.199959, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:30.575028: step 4990, loss 0.143747, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:30.773767: step 4991, loss 0.121999, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:30.958511: step 4992, loss 0.146906, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:31.142295: step 4993, loss 0.174297, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:31.328723: step 4994, loss 0.22108, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:31.514503: step 4995, loss 0.201866, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:31.698167: step 4996, loss 0.251531, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:31.879728: step 4997, loss 0.215992, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:32.034513: step 4998, loss 0.256287, acc 0.941176, learning_rate 0.0001
2017-09-29T11:13:32.215562: step 4999, loss 0.131689, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:32.400134: step 5000, loss 0.154683, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:13:32.909329: step 5000, loss 0.293525, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5000

2017-09-29T11:13:33.702294: step 5001, loss 0.211833, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:33.885962: step 5002, loss 0.253487, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:34.070234: step 5003, loss 0.227554, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:34.257042: step 5004, loss 0.17707, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:34.440390: step 5005, loss 0.185063, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:34.629012: step 5006, loss 0.175212, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:34.810733: step 5007, loss 0.315236, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:34.999682: step 5008, loss 0.328681, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:35.185112: step 5009, loss 0.120839, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:35.389609: step 5010, loss 0.233502, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:35.606323: step 5011, loss 0.305517, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:35.809711: step 5012, loss 0.163078, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:36.015233: step 5013, loss 0.249761, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:36.219785: step 5014, loss 0.253297, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:36.402325: step 5015, loss 0.260525, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:36.586359: step 5016, loss 0.263422, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:36.769564: step 5017, loss 0.300823, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:36.954429: step 5018, loss 0.115901, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:37.146781: step 5019, loss 0.186615, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:37.333511: step 5020, loss 0.198406, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:37.521358: step 5021, loss 0.140801, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:37.712357: step 5022, loss 0.223028, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:37.897863: step 5023, loss 0.295844, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:38.082002: step 5024, loss 0.223916, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:38.265587: step 5025, loss 0.104704, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:38.450942: step 5026, loss 0.327951, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:38.637936: step 5027, loss 0.157963, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:38.820047: step 5028, loss 0.290717, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:39.015165: step 5029, loss 0.354332, acc 0.828125, learning_rate 0.0001
2017-09-29T11:13:39.206896: step 5030, loss 0.161263, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:39.404182: step 5031, loss 0.292535, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:39.590938: step 5032, loss 0.267515, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:39.775974: step 5033, loss 0.168252, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:39.959965: step 5034, loss 0.0421708, acc 1, learning_rate 0.0001
2017-09-29T11:13:40.145163: step 5035, loss 0.301131, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:40.328279: step 5036, loss 0.347043, acc 0.859375, learning_rate 0.0001
2017-09-29T11:13:40.536649: step 5037, loss 0.246525, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:40.720199: step 5038, loss 0.119562, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:40.913201: step 5039, loss 0.210856, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:41.106412: step 5040, loss 0.110854, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T11:13:41.646847: step 5040, loss 0.292251, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5040

2017-09-29T11:13:42.270981: step 5041, loss 0.159704, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:42.456707: step 5042, loss 0.222821, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:42.650414: step 5043, loss 0.094248, acc 1, learning_rate 0.0001
2017-09-29T11:13:42.840451: step 5044, loss 0.168398, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:43.021850: step 5045, loss 0.215484, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:43.215621: step 5046, loss 0.138737, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:43.398937: step 5047, loss 0.222372, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:43.602116: step 5048, loss 0.197131, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:43.795426: step 5049, loss 0.19754, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:43.985485: step 5050, loss 0.174738, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:44.172492: step 5051, loss 0.228837, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:44.358078: step 5052, loss 0.182838, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:44.544728: step 5053, loss 0.229134, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:44.726202: step 5054, loss 0.202706, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:44.911647: step 5055, loss 0.272023, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:45.107383: step 5056, loss 0.272937, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:45.292383: step 5057, loss 0.113077, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:45.474468: step 5058, loss 0.242962, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:45.662434: step 5059, loss 0.273634, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:45.844777: step 5060, loss 0.265021, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:46.031523: step 5061, loss 0.171809, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:46.228970: step 5062, loss 0.208084, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:46.415991: step 5063, loss 0.292177, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:46.613662: step 5064, loss 0.105828, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:46.797214: step 5065, loss 0.234305, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:46.982285: step 5066, loss 0.298887, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:47.165090: step 5067, loss 0.144623, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:47.347482: step 5068, loss 0.15668, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:47.533363: step 5069, loss 0.192759, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:47.716806: step 5070, loss 0.228185, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:47.906583: step 5071, loss 0.238889, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:48.089531: step 5072, loss 0.172489, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:48.276812: step 5073, loss 0.165361, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:48.468019: step 5074, loss 0.208176, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:48.659429: step 5075, loss 0.197797, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:48.842592: step 5076, loss 0.180768, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:49.026338: step 5077, loss 0.149479, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:49.208595: step 5078, loss 0.325475, acc 0.859375, learning_rate 0.0001
2017-09-29T11:13:49.393876: step 5079, loss 0.106191, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:49.583338: step 5080, loss 0.134313, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:13:50.087392: step 5080, loss 0.29223, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5080

2017-09-29T11:13:50.782485: step 5081, loss 0.135894, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:50.962695: step 5082, loss 0.0826935, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:51.146139: step 5083, loss 0.226602, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:51.329538: step 5084, loss 0.141232, acc 0.96875, learning_rate 0.0001
2017-09-29T11:13:51.514978: step 5085, loss 0.167612, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:51.698338: step 5086, loss 0.176448, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:51.882755: step 5087, loss 0.269113, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:52.065876: step 5088, loss 0.196757, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:52.246547: step 5089, loss 0.2151, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:52.431228: step 5090, loss 0.0856185, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:52.617942: step 5091, loss 0.354968, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:52.809950: step 5092, loss 0.100673, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:53.007738: step 5093, loss 0.209099, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:53.198700: step 5094, loss 0.171014, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:53.384049: step 5095, loss 0.184565, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:53.535832: step 5096, loss 0.241225, acc 0.921569, learning_rate 0.0001
2017-09-29T11:13:53.723626: step 5097, loss 0.312534, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:53.915578: step 5098, loss 0.297512, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:54.108900: step 5099, loss 0.221714, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:54.294028: step 5100, loss 0.179276, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:54.479784: step 5101, loss 0.265085, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:54.666854: step 5102, loss 0.166519, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:54.859482: step 5103, loss 0.302551, acc 0.875, learning_rate 0.0001
2017-09-29T11:13:55.042014: step 5104, loss 0.165355, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:55.226256: step 5105, loss 0.119082, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:55.409643: step 5106, loss 0.237854, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:55.597897: step 5107, loss 0.191576, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:55.781717: step 5108, loss 0.142495, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:55.963354: step 5109, loss 0.235439, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:56.147209: step 5110, loss 0.230755, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:56.344123: step 5111, loss 0.302167, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:56.524715: step 5112, loss 0.208928, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:56.705769: step 5113, loss 0.183212, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:56.889136: step 5114, loss 0.289842, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:57.071979: step 5115, loss 0.199104, acc 0.921875, learning_rate 0.0001
2017-09-29T11:13:57.255018: step 5116, loss 0.169164, acc 0.953125, learning_rate 0.0001
2017-09-29T11:13:57.454353: step 5117, loss 0.156211, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:57.641413: step 5118, loss 0.170927, acc 0.9375, learning_rate 0.0001
2017-09-29T11:13:57.823479: step 5119, loss 0.24336, acc 0.90625, learning_rate 0.0001
2017-09-29T11:13:58.023131: step 5120, loss 0.177585, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:13:58.564807: step 5120, loss 0.287647, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5120

2017-09-29T11:13:59.279475: step 5121, loss 0.0998289, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:59.466995: step 5122, loss 0.0904256, acc 0.984375, learning_rate 0.0001
2017-09-29T11:13:59.650868: step 5123, loss 0.346767, acc 0.890625, learning_rate 0.0001
2017-09-29T11:13:59.838937: step 5124, loss 0.290391, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:00.030938: step 5125, loss 0.160962, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:00.212980: step 5126, loss 0.149719, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:00.398596: step 5127, loss 0.152802, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:00.584480: step 5128, loss 0.113586, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:00.769010: step 5129, loss 0.212138, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:00.954069: step 5130, loss 0.149343, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:01.136324: step 5131, loss 0.218333, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:01.322854: step 5132, loss 0.139431, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:01.515396: step 5133, loss 0.219817, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:01.704016: step 5134, loss 0.238873, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:01.890964: step 5135, loss 0.235326, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:02.072000: step 5136, loss 0.102678, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:02.255059: step 5137, loss 0.172962, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:02.441890: step 5138, loss 0.212541, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:02.632589: step 5139, loss 0.211613, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:02.812821: step 5140, loss 0.186767, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:03.011902: step 5141, loss 0.115167, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:03.194986: step 5142, loss 0.183792, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:03.385182: step 5143, loss 0.253115, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:03.591702: step 5144, loss 0.255744, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:03.783917: step 5145, loss 0.171093, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:03.967763: step 5146, loss 0.212741, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:04.151571: step 5147, loss 0.214122, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:04.333942: step 5148, loss 0.1486, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:04.527688: step 5149, loss 0.161974, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:04.717205: step 5150, loss 0.204898, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:04.905469: step 5151, loss 0.113521, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:05.088121: step 5152, loss 0.313058, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:05.275751: step 5153, loss 0.176853, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:05.460322: step 5154, loss 0.116507, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:05.643088: step 5155, loss 0.196773, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:05.833695: step 5156, loss 0.159542, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:06.021371: step 5157, loss 0.137642, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:06.203433: step 5158, loss 0.108509, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:06.385789: step 5159, loss 0.134044, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:06.566658: step 5160, loss 0.169147, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:14:07.080451: step 5160, loss 0.290341, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5160

2017-09-29T11:14:07.898427: step 5161, loss 0.182853, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:08.088044: step 5162, loss 0.238262, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:08.278545: step 5163, loss 0.240439, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:08.485255: step 5164, loss 0.33832, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:08.671779: step 5165, loss 0.230545, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:08.861081: step 5166, loss 0.190622, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:09.043224: step 5167, loss 0.312537, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:09.224209: step 5168, loss 0.183239, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:09.418304: step 5169, loss 0.349026, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:09.606142: step 5170, loss 0.210714, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:09.792358: step 5171, loss 0.190488, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:09.973867: step 5172, loss 0.212999, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:10.156825: step 5173, loss 0.216604, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:10.343887: step 5174, loss 0.251874, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:10.526157: step 5175, loss 0.171061, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:10.720893: step 5176, loss 0.156539, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:10.902758: step 5177, loss 0.154947, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:11.089035: step 5178, loss 0.318924, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:11.270729: step 5179, loss 0.278103, acc 0.859375, learning_rate 0.0001
2017-09-29T11:14:11.456889: step 5180, loss 0.151317, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:11.657172: step 5181, loss 0.175644, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:11.839658: step 5182, loss 0.17511, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:12.022533: step 5183, loss 0.157286, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:12.205084: step 5184, loss 0.233001, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:12.393116: step 5185, loss 0.392492, acc 0.84375, learning_rate 0.0001
2017-09-29T11:14:12.576053: step 5186, loss 0.185284, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:12.760603: step 5187, loss 0.175651, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:12.943088: step 5188, loss 0.169084, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:13.130889: step 5189, loss 0.0958753, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:13.315572: step 5190, loss 0.291339, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:13.498185: step 5191, loss 0.27031, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:13.682800: step 5192, loss 0.158653, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:13.863707: step 5193, loss 0.212623, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:14.027832: step 5194, loss 0.158573, acc 0.960784, learning_rate 0.0001
2017-09-29T11:14:14.211371: step 5195, loss 0.132279, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:14.404808: step 5196, loss 0.341936, acc 0.859375, learning_rate 0.0001
2017-09-29T11:14:14.611378: step 5197, loss 0.218185, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:14.796657: step 5198, loss 0.240446, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:14.981285: step 5199, loss 0.207002, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:15.166389: step 5200, loss 0.125308, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:14:15.679099: step 5200, loss 0.29155, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5200

2017-09-29T11:14:16.342847: step 5201, loss 0.24584, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:16.529111: step 5202, loss 0.244465, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:16.730639: step 5203, loss 0.178092, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:16.937414: step 5204, loss 0.125698, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:17.128865: step 5205, loss 0.256921, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:17.314151: step 5206, loss 0.191096, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:17.504172: step 5207, loss 0.30747, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:17.699610: step 5208, loss 0.293826, acc 0.859375, learning_rate 0.0001
2017-09-29T11:14:17.885451: step 5209, loss 0.0768742, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:18.087386: step 5210, loss 0.147371, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:18.269340: step 5211, loss 0.109238, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:18.454584: step 5212, loss 0.190706, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:18.642058: step 5213, loss 0.0724124, acc 1, learning_rate 0.0001
2017-09-29T11:14:18.826778: step 5214, loss 0.213616, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:19.017433: step 5215, loss 0.150311, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:19.200461: step 5216, loss 0.176246, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:19.383136: step 5217, loss 0.170466, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:19.568633: step 5218, loss 0.152779, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:19.760940: step 5219, loss 0.128725, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:19.947439: step 5220, loss 0.170622, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:20.130756: step 5221, loss 0.300508, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:20.311191: step 5222, loss 0.251507, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:20.507508: step 5223, loss 0.264561, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:20.703374: step 5224, loss 0.345385, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:20.884417: step 5225, loss 0.161143, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:21.076662: step 5226, loss 0.100752, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:21.260671: step 5227, loss 0.173987, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:21.447466: step 5228, loss 0.123402, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:21.628879: step 5229, loss 0.130486, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:21.815895: step 5230, loss 0.275033, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:21.999061: step 5231, loss 0.207761, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:22.186245: step 5232, loss 0.155909, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:22.372068: step 5233, loss 0.298096, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:22.558065: step 5234, loss 0.183056, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:22.741782: step 5235, loss 0.180292, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:22.924319: step 5236, loss 0.266213, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:23.113927: step 5237, loss 0.19383, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:23.294178: step 5238, loss 0.220903, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:23.485377: step 5239, loss 0.274626, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:23.672717: step 5240, loss 0.0866722, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:14:24.195427: step 5240, loss 0.284331, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5240

2017-09-29T11:14:24.911185: step 5241, loss 0.133792, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:25.093580: step 5242, loss 0.170605, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:25.277269: step 5243, loss 0.20778, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:25.470984: step 5244, loss 0.199051, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:25.656247: step 5245, loss 0.0828431, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:25.849334: step 5246, loss 0.175939, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:26.034549: step 5247, loss 0.203393, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:26.216150: step 5248, loss 0.191571, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:26.399876: step 5249, loss 0.198987, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:26.582328: step 5250, loss 0.304389, acc 0.859375, learning_rate 0.0001
2017-09-29T11:14:26.763533: step 5251, loss 0.224684, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:26.958480: step 5252, loss 0.145945, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:27.146269: step 5253, loss 0.237784, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:27.329502: step 5254, loss 0.208166, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:27.515435: step 5255, loss 0.16102, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:27.702897: step 5256, loss 0.181459, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:27.886881: step 5257, loss 0.38884, acc 0.84375, learning_rate 0.0001
2017-09-29T11:14:28.071059: step 5258, loss 0.183567, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:28.262161: step 5259, loss 0.230994, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:28.448267: step 5260, loss 0.190424, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:28.632004: step 5261, loss 0.282128, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:28.822811: step 5262, loss 0.193556, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:29.014794: step 5263, loss 0.261462, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:29.200810: step 5264, loss 0.212339, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:29.390098: step 5265, loss 0.142375, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:29.575009: step 5266, loss 0.239951, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:29.758946: step 5267, loss 0.146131, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:29.940555: step 5268, loss 0.223629, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:30.125904: step 5269, loss 0.395005, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:30.307745: step 5270, loss 0.257635, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:30.496757: step 5271, loss 0.115099, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:30.683432: step 5272, loss 0.153599, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:30.886577: step 5273, loss 0.134098, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:31.093738: step 5274, loss 0.172745, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:31.285353: step 5275, loss 0.140827, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:31.482769: step 5276, loss 0.173033, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:31.667374: step 5277, loss 0.189346, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:31.854905: step 5278, loss 0.186305, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:32.042117: step 5279, loss 0.10622, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:32.226934: step 5280, loss 0.219253, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-09-29T11:14:32.754609: step 5280, loss 0.285561, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5280

2017-09-29T11:14:33.457768: step 5281, loss 0.213937, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:33.643154: step 5282, loss 0.145202, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:33.832815: step 5283, loss 0.15204, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:34.013834: step 5284, loss 0.215484, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:34.196737: step 5285, loss 0.203164, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:34.393202: step 5286, loss 0.183264, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:34.574674: step 5287, loss 0.300716, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:34.756012: step 5288, loss 0.151285, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:34.939583: step 5289, loss 0.183596, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:35.121842: step 5290, loss 0.118373, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:35.303906: step 5291, loss 0.0856366, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:35.469322: step 5292, loss 0.0884339, acc 1, learning_rate 0.0001
2017-09-29T11:14:35.652279: step 5293, loss 0.075128, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:35.834005: step 5294, loss 0.235773, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:36.021730: step 5295, loss 0.175541, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:36.205399: step 5296, loss 0.202384, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:36.389303: step 5297, loss 0.208945, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:36.574460: step 5298, loss 0.107792, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:36.757566: step 5299, loss 0.0963074, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:36.936106: step 5300, loss 0.118673, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:37.123987: step 5301, loss 0.197596, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:37.307911: step 5302, loss 0.239562, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:37.509790: step 5303, loss 0.357212, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:37.703686: step 5304, loss 0.243178, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:37.888292: step 5305, loss 0.135278, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:38.075362: step 5306, loss 0.322725, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:38.268411: step 5307, loss 0.243766, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:38.462409: step 5308, loss 0.348284, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:38.645746: step 5309, loss 0.223787, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:38.845281: step 5310, loss 0.167691, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:39.038673: step 5311, loss 0.283467, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:39.224410: step 5312, loss 0.149717, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:39.412039: step 5313, loss 0.147076, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:39.594327: step 5314, loss 0.146582, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:39.790344: step 5315, loss 0.251036, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:39.974439: step 5316, loss 0.192531, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:40.159169: step 5317, loss 0.145357, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:40.340958: step 5318, loss 0.337678, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:40.525546: step 5319, loss 0.123128, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:40.707760: step 5320, loss 0.224112, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:14:41.200005: step 5320, loss 0.28488, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5320

2017-09-29T11:14:41.997891: step 5321, loss 0.119212, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:42.180465: step 5322, loss 0.0847607, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:42.362984: step 5323, loss 0.127911, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:42.561194: step 5324, loss 0.188522, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:42.753219: step 5325, loss 0.19634, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:42.934245: step 5326, loss 0.122648, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:43.116926: step 5327, loss 0.16604, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:43.306126: step 5328, loss 0.247859, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:43.494706: step 5329, loss 0.121162, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:43.683016: step 5330, loss 0.165233, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:43.869046: step 5331, loss 0.136863, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:44.048301: step 5332, loss 0.134339, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:44.232558: step 5333, loss 0.185757, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:44.415914: step 5334, loss 0.239277, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:44.597860: step 5335, loss 0.227878, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:44.782537: step 5336, loss 0.190007, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:44.965956: step 5337, loss 0.133778, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:45.155655: step 5338, loss 0.329946, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:45.341739: step 5339, loss 0.132165, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:45.530893: step 5340, loss 0.191294, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:45.718649: step 5341, loss 0.246702, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:45.900100: step 5342, loss 0.231749, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:46.085117: step 5343, loss 0.19051, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:46.269653: step 5344, loss 0.407489, acc 0.859375, learning_rate 0.0001
2017-09-29T11:14:46.455701: step 5345, loss 0.172667, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:46.649421: step 5346, loss 0.129019, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:46.831648: step 5347, loss 0.14375, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:47.013994: step 5348, loss 0.177203, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:47.198462: step 5349, loss 0.220068, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:47.389667: step 5350, loss 0.253749, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:47.570253: step 5351, loss 0.207459, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:47.758117: step 5352, loss 0.123497, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:47.939106: step 5353, loss 0.206403, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:48.120651: step 5354, loss 0.172018, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:48.309078: step 5355, loss 0.0915875, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:48.510209: step 5356, loss 0.172323, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:48.693784: step 5357, loss 0.265141, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:48.889669: step 5358, loss 0.156018, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:49.079109: step 5359, loss 0.20586, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:49.263343: step 5360, loss 0.108987, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T11:14:49.848989: step 5360, loss 0.286975, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5360

2017-09-29T11:14:50.472599: step 5361, loss 0.219088, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:50.655447: step 5362, loss 0.251219, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:50.839841: step 5363, loss 0.201631, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:51.022385: step 5364, loss 0.17354, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:51.201295: step 5365, loss 0.159111, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:51.383259: step 5366, loss 0.276615, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:51.564687: step 5367, loss 0.197964, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:51.749520: step 5368, loss 0.117565, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:51.956752: step 5369, loss 0.124186, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:52.161494: step 5370, loss 0.180867, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:52.358145: step 5371, loss 0.206006, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:52.539753: step 5372, loss 0.172183, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:52.722645: step 5373, loss 0.188308, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:52.908112: step 5374, loss 0.1401, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:53.097759: step 5375, loss 0.240175, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:53.281627: step 5376, loss 0.21963, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:53.482503: step 5377, loss 0.321087, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:53.677638: step 5378, loss 0.190036, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:53.861872: step 5379, loss 0.346092, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:54.045877: step 5380, loss 0.134001, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:54.232933: step 5381, loss 0.140054, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:54.417898: step 5382, loss 0.258251, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:54.606607: step 5383, loss 0.0689862, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:54.794893: step 5384, loss 0.370653, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:54.985753: step 5385, loss 0.214818, acc 0.90625, learning_rate 0.0001
2017-09-29T11:14:55.178495: step 5386, loss 0.19452, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:55.375199: step 5387, loss 0.190499, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:55.562670: step 5388, loss 0.123134, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:55.755662: step 5389, loss 0.165895, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:55.905598: step 5390, loss 0.175884, acc 0.960784, learning_rate 0.0001
2017-09-29T11:14:56.099547: step 5391, loss 0.201111, acc 0.9375, learning_rate 0.0001
2017-09-29T11:14:56.287444: step 5392, loss 0.203294, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:56.471049: step 5393, loss 0.172626, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:56.653165: step 5394, loss 0.145805, acc 0.96875, learning_rate 0.0001
2017-09-29T11:14:56.838631: step 5395, loss 0.117716, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:57.025519: step 5396, loss 0.134682, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:57.209078: step 5397, loss 0.219272, acc 0.921875, learning_rate 0.0001
2017-09-29T11:14:57.400690: step 5398, loss 0.184236, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:57.584859: step 5399, loss 0.140161, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:57.770555: step 5400, loss 0.156787, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:14:58.301906: step 5400, loss 0.286592, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5400

2017-09-29T11:14:59.015829: step 5401, loss 0.308127, acc 0.875, learning_rate 0.0001
2017-09-29T11:14:59.201911: step 5402, loss 0.240751, acc 0.890625, learning_rate 0.0001
2017-09-29T11:14:59.386373: step 5403, loss 0.0836677, acc 0.984375, learning_rate 0.0001
2017-09-29T11:14:59.581554: step 5404, loss 0.132261, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:59.766076: step 5405, loss 0.138502, acc 0.953125, learning_rate 0.0001
2017-09-29T11:14:59.951438: step 5406, loss 0.181593, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:00.133452: step 5407, loss 0.209781, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:00.316902: step 5408, loss 0.220498, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:00.502422: step 5409, loss 0.185675, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:00.692894: step 5410, loss 0.171326, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:00.877303: step 5411, loss 0.137555, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:01.088324: step 5412, loss 0.137964, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:01.282309: step 5413, loss 0.215769, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:01.472342: step 5414, loss 0.13916, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:01.656047: step 5415, loss 0.20213, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:01.855753: step 5416, loss 0.215611, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:02.045031: step 5417, loss 0.307926, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:02.229076: step 5418, loss 0.361362, acc 0.859375, learning_rate 0.0001
2017-09-29T11:15:02.412349: step 5419, loss 0.137651, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:02.597505: step 5420, loss 0.251136, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:02.789868: step 5421, loss 0.222476, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:02.973990: step 5422, loss 0.31135, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:03.165303: step 5423, loss 0.169717, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:03.366303: step 5424, loss 0.177077, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:03.551451: step 5425, loss 0.27027, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:03.732065: step 5426, loss 0.199575, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:03.916533: step 5427, loss 0.167971, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:04.104903: step 5428, loss 0.126037, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:04.295894: step 5429, loss 0.153568, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:04.482911: step 5430, loss 0.198004, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:04.673868: step 5431, loss 0.191558, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:04.857643: step 5432, loss 0.220049, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:05.037980: step 5433, loss 0.168557, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:05.220811: step 5434, loss 0.115588, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:05.410594: step 5435, loss 0.1897, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:05.602340: step 5436, loss 0.210666, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:05.785496: step 5437, loss 0.179439, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:05.965784: step 5438, loss 0.124279, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:06.148671: step 5439, loss 0.197267, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:06.335413: step 5440, loss 0.274484, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:15:06.849721: step 5440, loss 0.284237, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5440

2017-09-29T11:15:07.559034: step 5441, loss 0.211756, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:07.742535: step 5442, loss 0.137623, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:07.929648: step 5443, loss 0.266601, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:08.110224: step 5444, loss 0.120797, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:08.297604: step 5445, loss 0.0961737, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:08.488120: step 5446, loss 0.0730442, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:08.672050: step 5447, loss 0.134999, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:08.852899: step 5448, loss 0.218936, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:09.037450: step 5449, loss 0.12206, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:09.220355: step 5450, loss 0.194665, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:09.405893: step 5451, loss 0.182299, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:09.588942: step 5452, loss 0.139154, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:09.770532: step 5453, loss 0.151748, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:09.953061: step 5454, loss 0.277603, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:10.151394: step 5455, loss 0.0917322, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:10.331656: step 5456, loss 0.138169, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:10.515033: step 5457, loss 0.295907, acc 0.859375, learning_rate 0.0001
2017-09-29T11:15:10.699410: step 5458, loss 0.162509, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:10.900473: step 5459, loss 0.176922, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:11.103662: step 5460, loss 0.134709, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:11.292698: step 5461, loss 0.165313, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:11.496573: step 5462, loss 0.16126, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:11.688270: step 5463, loss 0.155076, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:11.871149: step 5464, loss 0.219985, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:12.061241: step 5465, loss 0.421206, acc 0.828125, learning_rate 0.0001
2017-09-29T11:15:12.244093: step 5466, loss 0.125361, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:12.432225: step 5467, loss 0.276886, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:12.613782: step 5468, loss 0.151223, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:12.796880: step 5469, loss 0.191425, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:12.985828: step 5470, loss 0.195113, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:13.169718: step 5471, loss 0.318791, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:13.353806: step 5472, loss 0.138053, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:13.546967: step 5473, loss 0.234457, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:13.736399: step 5474, loss 0.194486, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:13.920799: step 5475, loss 0.148009, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:14.105852: step 5476, loss 0.173913, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:14.287072: step 5477, loss 0.294114, acc 0.875, learning_rate 0.0001
2017-09-29T11:15:14.471469: step 5478, loss 0.233034, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:14.657023: step 5479, loss 0.091386, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:14.839839: step 5480, loss 0.120609, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:15:15.364979: step 5480, loss 0.283125, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5480

2017-09-29T11:15:16.154729: step 5481, loss 0.150552, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:16.341216: step 5482, loss 0.174045, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:16.538800: step 5483, loss 0.133659, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:16.731122: step 5484, loss 0.238625, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:16.921158: step 5485, loss 0.245394, acc 0.875, learning_rate 0.0001
2017-09-29T11:15:17.105191: step 5486, loss 0.140487, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:17.286690: step 5487, loss 0.245308, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:17.439982: step 5488, loss 0.169004, acc 0.941176, learning_rate 0.0001
2017-09-29T11:15:17.626146: step 5489, loss 0.266509, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:17.810556: step 5490, loss 0.143059, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:18.001096: step 5491, loss 0.220721, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:18.193295: step 5492, loss 0.206017, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:18.400513: step 5493, loss 0.200123, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:18.602002: step 5494, loss 0.164633, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:18.796086: step 5495, loss 0.130376, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:18.980671: step 5496, loss 0.15357, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:19.166734: step 5497, loss 0.329839, acc 0.84375, learning_rate 0.0001
2017-09-29T11:15:19.347086: step 5498, loss 0.17408, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:19.534383: step 5499, loss 0.164751, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:19.722443: step 5500, loss 0.175758, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:19.906076: step 5501, loss 0.222133, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:20.086179: step 5502, loss 0.157335, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:20.268025: step 5503, loss 0.182369, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:20.454927: step 5504, loss 0.236509, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:20.636079: step 5505, loss 0.253677, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:20.818896: step 5506, loss 0.302866, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:21.002841: step 5507, loss 0.0705817, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:21.189496: step 5508, loss 0.184921, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:21.369075: step 5509, loss 0.309505, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:21.553952: step 5510, loss 0.23901, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:21.737704: step 5511, loss 0.170313, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:21.930720: step 5512, loss 0.0602279, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:22.116202: step 5513, loss 0.231607, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:22.305883: step 5514, loss 0.18101, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:22.499936: step 5515, loss 0.181853, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:22.683190: step 5516, loss 0.163497, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:22.868864: step 5517, loss 0.167242, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:23.053227: step 5518, loss 0.143981, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:23.233479: step 5519, loss 0.149007, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:23.425464: step 5520, loss 0.124297, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:15:23.988442: step 5520, loss 0.282141, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5520

2017-09-29T11:15:24.616989: step 5521, loss 0.233686, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:24.800338: step 5522, loss 0.190197, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:24.985589: step 5523, loss 0.181114, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:25.171997: step 5524, loss 0.206686, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:25.354805: step 5525, loss 0.236299, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:25.541642: step 5526, loss 0.226783, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:25.736913: step 5527, loss 0.198744, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:25.922971: step 5528, loss 0.319856, acc 0.859375, learning_rate 0.0001
2017-09-29T11:15:26.113133: step 5529, loss 0.287328, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:26.297084: step 5530, loss 0.212961, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:26.483849: step 5531, loss 0.123579, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:26.675467: step 5532, loss 0.15541, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:26.859362: step 5533, loss 0.230891, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:27.043944: step 5534, loss 0.117092, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:27.228825: step 5535, loss 0.290936, acc 0.875, learning_rate 0.0001
2017-09-29T11:15:27.410917: step 5536, loss 0.19298, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:27.609593: step 5537, loss 0.183576, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:27.789128: step 5538, loss 0.156607, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:27.970781: step 5539, loss 0.319249, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:28.155557: step 5540, loss 0.122309, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:28.343252: step 5541, loss 0.205246, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:28.561335: step 5542, loss 0.185084, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:28.744560: step 5543, loss 0.183999, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:28.929702: step 5544, loss 0.181684, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:29.111960: step 5545, loss 0.243855, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:29.300262: step 5546, loss 0.212344, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:29.490077: step 5547, loss 0.210435, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:29.675681: step 5548, loss 0.18411, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:29.866014: step 5549, loss 0.325263, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:30.056278: step 5550, loss 0.158316, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:30.238449: step 5551, loss 0.207386, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:30.433538: step 5552, loss 0.162827, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:30.618400: step 5553, loss 0.254941, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:30.799976: step 5554, loss 0.431915, acc 0.8125, learning_rate 0.0001
2017-09-29T11:15:30.988545: step 5555, loss 0.160913, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:31.175090: step 5556, loss 0.122827, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:31.372031: step 5557, loss 0.212704, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:31.578499: step 5558, loss 0.121509, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:31.763117: step 5559, loss 0.159911, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:31.942978: step 5560, loss 0.393244, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-09-29T11:15:32.470469: step 5560, loss 0.290062, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5560

2017-09-29T11:15:33.167985: step 5561, loss 0.211811, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:33.352978: step 5562, loss 0.133308, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:33.551064: step 5563, loss 0.181354, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:33.735583: step 5564, loss 0.132507, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:33.921299: step 5565, loss 0.168587, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:34.104446: step 5566, loss 0.187363, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:34.291913: step 5567, loss 0.143875, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:34.475974: step 5568, loss 0.126384, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:34.659130: step 5569, loss 0.187272, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:34.843756: step 5570, loss 0.230622, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:35.029079: step 5571, loss 0.163875, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:35.212263: step 5572, loss 0.128932, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:35.396991: step 5573, loss 0.214228, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:35.578443: step 5574, loss 0.204373, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:35.759567: step 5575, loss 0.120349, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:35.945287: step 5576, loss 0.251825, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:36.126922: step 5577, loss 0.18018, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:36.317460: step 5578, loss 0.203349, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:36.505459: step 5579, loss 0.134427, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:36.690964: step 5580, loss 0.260424, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:36.871241: step 5581, loss 0.136137, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:37.054430: step 5582, loss 0.170361, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:37.233873: step 5583, loss 0.120315, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:37.416419: step 5584, loss 0.224766, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:37.609761: step 5585, loss 0.130938, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:37.765732: step 5586, loss 0.178854, acc 0.941176, learning_rate 0.0001
2017-09-29T11:15:37.950117: step 5587, loss 0.123384, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:38.135609: step 5588, loss 0.0935676, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:38.341664: step 5589, loss 0.191047, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:38.528534: step 5590, loss 0.194307, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:38.718140: step 5591, loss 0.218979, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:38.901765: step 5592, loss 0.181852, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:39.084238: step 5593, loss 0.257518, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:39.279689: step 5594, loss 0.254856, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:39.473979: step 5595, loss 0.0953645, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:39.677748: step 5596, loss 0.413017, acc 0.875, learning_rate 0.0001
2017-09-29T11:15:39.888850: step 5597, loss 0.127477, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:40.090776: step 5598, loss 0.191875, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:40.281149: step 5599, loss 0.240353, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:40.468531: step 5600, loss 0.307577, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-09-29T11:15:40.990776: step 5600, loss 0.281902, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5600

2017-09-29T11:15:41.696662: step 5601, loss 0.221304, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:41.878962: step 5602, loss 0.217398, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:42.062321: step 5603, loss 0.194296, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:42.249552: step 5604, loss 0.171637, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:42.441111: step 5605, loss 0.155584, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:42.624019: step 5606, loss 0.118596, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:42.804683: step 5607, loss 0.250098, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:42.990741: step 5608, loss 0.210849, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:43.184325: step 5609, loss 0.226251, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:43.370158: step 5610, loss 0.253716, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:43.559730: step 5611, loss 0.118279, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:43.746755: step 5612, loss 0.252355, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:43.932892: step 5613, loss 0.268331, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:44.116328: step 5614, loss 0.148566, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:44.302872: step 5615, loss 0.174661, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:44.496561: step 5616, loss 0.131717, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:44.683530: step 5617, loss 0.252159, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:44.867562: step 5618, loss 0.105588, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:45.049657: step 5619, loss 0.208854, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:45.232375: step 5620, loss 0.279336, acc 0.890625, learning_rate 0.0001
2017-09-29T11:15:45.416168: step 5621, loss 0.147136, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:45.604321: step 5622, loss 0.245313, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:45.797418: step 5623, loss 0.144024, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:45.983923: step 5624, loss 0.152649, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:46.170696: step 5625, loss 0.145137, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:46.354210: step 5626, loss 0.254458, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:46.540708: step 5627, loss 0.120549, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:46.726613: step 5628, loss 0.13342, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:46.927543: step 5629, loss 0.217494, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:47.132811: step 5630, loss 0.266977, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:47.340110: step 5631, loss 0.258195, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:47.538697: step 5632, loss 0.188459, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:47.724311: step 5633, loss 0.253994, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:47.911205: step 5634, loss 0.152539, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:48.093817: step 5635, loss 0.194113, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:48.276507: step 5636, loss 0.124096, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:48.456824: step 5637, loss 0.149865, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:48.649624: step 5638, loss 0.142443, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:48.851028: step 5639, loss 0.242279, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:49.054983: step 5640, loss 0.160759, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:15:49.582161: step 5640, loss 0.284617, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5640

2017-09-29T11:15:50.368983: step 5641, loss 0.0866647, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:50.560337: step 5642, loss 0.202075, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:50.742209: step 5643, loss 0.21368, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:50.924087: step 5644, loss 0.310216, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:51.102376: step 5645, loss 0.107975, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:51.292951: step 5646, loss 0.209284, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:51.502700: step 5647, loss 0.16181, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:51.686459: step 5648, loss 0.103302, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:51.872416: step 5649, loss 0.202045, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:52.052573: step 5650, loss 0.153638, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:52.236073: step 5651, loss 0.133726, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:52.420438: step 5652, loss 0.123229, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:52.604451: step 5653, loss 0.20145, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:52.787603: step 5654, loss 0.207287, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:52.974545: step 5655, loss 0.161643, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:53.157662: step 5656, loss 0.14416, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:53.348887: step 5657, loss 0.110814, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:53.537779: step 5658, loss 0.240375, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:53.727764: step 5659, loss 0.13228, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:53.910507: step 5660, loss 0.153261, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:54.093460: step 5661, loss 0.200566, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:54.277047: step 5662, loss 0.129853, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:54.471252: step 5663, loss 0.17979, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:54.663482: step 5664, loss 0.232494, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:54.844089: step 5665, loss 0.307933, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:55.028624: step 5666, loss 0.150529, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:55.213668: step 5667, loss 0.147684, acc 0.96875, learning_rate 0.0001
2017-09-29T11:15:55.397389: step 5668, loss 0.216126, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:55.594846: step 5669, loss 0.167658, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:55.774823: step 5670, loss 0.207036, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:55.955856: step 5671, loss 0.190297, acc 0.9375, learning_rate 0.0001
2017-09-29T11:15:56.141803: step 5672, loss 0.131859, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:56.337759: step 5673, loss 0.154789, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:56.536533: step 5674, loss 0.12233, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:56.719302: step 5675, loss 0.322752, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:56.911331: step 5676, loss 0.241068, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:57.098902: step 5677, loss 0.25072, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:57.283809: step 5678, loss 0.217232, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:57.475839: step 5679, loss 0.162545, acc 0.984375, learning_rate 0.0001
2017-09-29T11:15:57.662938: step 5680, loss 0.243378, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:15:58.185184: step 5680, loss 0.28389, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5680

2017-09-29T11:15:58.814101: step 5681, loss 0.199946, acc 0.921875, learning_rate 0.0001
2017-09-29T11:15:58.997048: step 5682, loss 0.252344, acc 0.875, learning_rate 0.0001
2017-09-29T11:15:59.177499: step 5683, loss 0.262891, acc 0.90625, learning_rate 0.0001
2017-09-29T11:15:59.330845: step 5684, loss 0.137182, acc 0.960784, learning_rate 0.0001
2017-09-29T11:15:59.520809: step 5685, loss 0.235035, acc 0.875, learning_rate 0.0001
2017-09-29T11:15:59.709494: step 5686, loss 0.139005, acc 0.953125, learning_rate 0.0001
2017-09-29T11:15:59.899251: step 5687, loss 0.105212, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:00.090852: step 5688, loss 0.181838, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:00.277005: step 5689, loss 0.22358, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:00.459895: step 5690, loss 0.12289, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:00.644230: step 5691, loss 0.271651, acc 0.859375, learning_rate 0.0001
2017-09-29T11:16:00.827430: step 5692, loss 0.133353, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:01.011610: step 5693, loss 0.190906, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:01.196557: step 5694, loss 0.188853, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:01.380579: step 5695, loss 0.195881, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:01.573319: step 5696, loss 0.173463, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:01.756807: step 5697, loss 0.218624, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:01.938234: step 5698, loss 0.142059, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:02.121910: step 5699, loss 0.334473, acc 0.875, learning_rate 0.0001
2017-09-29T11:16:02.303883: step 5700, loss 0.226716, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:02.495955: step 5701, loss 0.214027, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:02.682909: step 5702, loss 0.138446, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:02.866230: step 5703, loss 0.267869, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:03.045408: step 5704, loss 0.146685, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:03.229257: step 5705, loss 0.259758, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:03.424778: step 5706, loss 0.19785, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:03.608171: step 5707, loss 0.289572, acc 0.875, learning_rate 0.0001
2017-09-29T11:16:03.795490: step 5708, loss 0.136675, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:03.976782: step 5709, loss 0.137399, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:04.157049: step 5710, loss 0.213268, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:04.338236: step 5711, loss 0.198538, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:04.533434: step 5712, loss 0.106177, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:04.737879: step 5713, loss 0.142755, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:04.940953: step 5714, loss 0.268198, acc 0.875, learning_rate 0.0001
2017-09-29T11:16:05.145632: step 5715, loss 0.130598, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:05.348991: step 5716, loss 0.278106, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:05.555089: step 5717, loss 0.181886, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:05.754099: step 5718, loss 0.289556, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:05.959178: step 5719, loss 0.11118, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:06.165240: step 5720, loss 0.188171, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:16:06.697868: step 5720, loss 0.279009, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5720

2017-09-29T11:16:07.408599: step 5721, loss 0.179953, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:07.594060: step 5722, loss 0.34526, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:07.778865: step 5723, loss 0.218874, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:07.971527: step 5724, loss 0.223146, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:08.161500: step 5725, loss 0.179159, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:08.343566: step 5726, loss 0.325973, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:08.540438: step 5727, loss 0.139317, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:08.723791: step 5728, loss 0.234256, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:08.914249: step 5729, loss 0.122401, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:09.101205: step 5730, loss 0.172774, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:09.287441: step 5731, loss 0.176693, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:09.490646: step 5732, loss 0.161108, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:09.674843: step 5733, loss 0.168003, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:09.855578: step 5734, loss 0.200088, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:10.035145: step 5735, loss 0.235942, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:10.222994: step 5736, loss 0.125551, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:10.411823: step 5737, loss 0.157508, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:10.597891: step 5738, loss 0.210085, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:10.784872: step 5739, loss 0.093566, acc 1, learning_rate 0.0001
2017-09-29T11:16:10.972737: step 5740, loss 0.160152, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:11.158300: step 5741, loss 0.320961, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:11.360496: step 5742, loss 0.284464, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:11.549432: step 5743, loss 0.118719, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:11.735490: step 5744, loss 0.267182, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:11.923382: step 5745, loss 0.112146, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:12.109494: step 5746, loss 0.277874, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:12.297189: step 5747, loss 0.175582, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:12.480837: step 5748, loss 0.122683, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:12.667696: step 5749, loss 0.232909, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:12.854507: step 5750, loss 0.189244, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:13.036154: step 5751, loss 0.208069, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:13.218477: step 5752, loss 0.0960228, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:13.399523: step 5753, loss 0.163057, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:13.585865: step 5754, loss 0.204789, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:13.773985: step 5755, loss 0.148283, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:13.963962: step 5756, loss 0.175824, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:14.158157: step 5757, loss 0.0854123, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:14.345071: step 5758, loss 0.205666, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:14.530552: step 5759, loss 0.195845, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:14.712726: step 5760, loss 0.170613, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:16:15.244168: step 5760, loss 0.286418, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5760

2017-09-29T11:16:15.963167: step 5761, loss 0.253771, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:16.146527: step 5762, loss 0.140151, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:16.329219: step 5763, loss 0.144711, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:16.513894: step 5764, loss 0.0942578, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:16.697543: step 5765, loss 0.161579, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:16.880865: step 5766, loss 0.172785, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:17.077140: step 5767, loss 0.166367, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:17.266527: step 5768, loss 0.234089, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:17.455653: step 5769, loss 0.265946, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:17.648413: step 5770, loss 0.248545, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:17.835051: step 5771, loss 0.182708, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:18.016190: step 5772, loss 0.0952616, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:18.203732: step 5773, loss 0.186062, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:18.391275: step 5774, loss 0.15863, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:18.576447: step 5775, loss 0.203097, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:18.760716: step 5776, loss 0.194764, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:18.952649: step 5777, loss 0.140126, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:19.137501: step 5778, loss 0.263822, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:19.319451: step 5779, loss 0.293051, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:19.519725: step 5780, loss 0.12196, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:19.702140: step 5781, loss 0.10113, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:19.864851: step 5782, loss 0.210615, acc 0.941176, learning_rate 0.0001
2017-09-29T11:16:20.072527: step 5783, loss 0.159678, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:20.262924: step 5784, loss 0.148907, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:20.450165: step 5785, loss 0.195001, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:20.633726: step 5786, loss 0.0782723, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:20.831974: step 5787, loss 0.273963, acc 0.859375, learning_rate 0.0001
2017-09-29T11:16:21.020759: step 5788, loss 0.102956, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:21.215699: step 5789, loss 0.110245, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:21.399441: step 5790, loss 0.322122, acc 0.84375, learning_rate 0.0001
2017-09-29T11:16:21.586309: step 5791, loss 0.121184, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:21.770523: step 5792, loss 0.14387, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:21.952411: step 5793, loss 0.200582, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:22.135844: step 5794, loss 0.2681, acc 0.859375, learning_rate 0.0001
2017-09-29T11:16:22.322134: step 5795, loss 0.267945, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:22.504903: step 5796, loss 0.266416, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:22.687353: step 5797, loss 0.117334, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:22.880891: step 5798, loss 0.152294, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:23.061302: step 5799, loss 0.310213, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:23.243546: step 5800, loss 0.282613, acc 0.84375, learning_rate 0.0001

Evaluation:
2017-09-29T11:16:23.774721: step 5800, loss 0.278083, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5800

2017-09-29T11:16:24.590856: step 5801, loss 0.101595, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:24.775161: step 5802, loss 0.350681, acc 0.875, learning_rate 0.0001
2017-09-29T11:16:24.956637: step 5803, loss 0.181391, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:25.138879: step 5804, loss 0.164917, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:25.321975: step 5805, loss 0.112264, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:25.517693: step 5806, loss 0.208138, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:25.703809: step 5807, loss 0.152013, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:25.887110: step 5808, loss 0.113158, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:26.070972: step 5809, loss 0.338984, acc 0.875, learning_rate 0.0001
2017-09-29T11:16:26.253291: step 5810, loss 0.157929, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:26.436565: step 5811, loss 0.217255, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:26.624985: step 5812, loss 0.208928, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:26.807523: step 5813, loss 0.207308, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:26.989963: step 5814, loss 0.364267, acc 0.859375, learning_rate 0.0001
2017-09-29T11:16:27.181311: step 5815, loss 0.120946, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:27.367573: step 5816, loss 0.171466, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:27.553522: step 5817, loss 0.110373, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:27.732629: step 5818, loss 0.14061, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:27.918379: step 5819, loss 0.140478, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:28.108436: step 5820, loss 0.21838, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:28.292188: step 5821, loss 0.122475, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:28.482932: step 5822, loss 0.164891, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:28.664098: step 5823, loss 0.101476, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:28.849317: step 5824, loss 0.281233, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:29.035968: step 5825, loss 0.122871, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:29.221429: step 5826, loss 0.197805, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:29.426596: step 5827, loss 0.174863, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:29.614130: step 5828, loss 0.156305, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:29.796827: step 5829, loss 0.0776555, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:29.979043: step 5830, loss 0.151556, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:30.163649: step 5831, loss 0.227479, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:30.343063: step 5832, loss 0.136974, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:30.527807: step 5833, loss 0.136397, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:30.710674: step 5834, loss 0.145791, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:30.895505: step 5835, loss 0.203484, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:31.081961: step 5836, loss 0.119432, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:31.263276: step 5837, loss 0.167976, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:31.453380: step 5838, loss 0.0764097, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:31.639814: step 5839, loss 0.090822, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:31.824412: step 5840, loss 0.149976, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:16:32.336497: step 5840, loss 0.280516, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5840

2017-09-29T11:16:32.977055: step 5841, loss 0.201038, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:33.166255: step 5842, loss 0.254464, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:33.352212: step 5843, loss 0.187357, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:33.538307: step 5844, loss 0.242927, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:33.724022: step 5845, loss 0.297597, acc 0.875, learning_rate 0.0001
2017-09-29T11:16:33.911412: step 5846, loss 0.292377, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:34.098335: step 5847, loss 0.224284, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:34.281292: step 5848, loss 0.130136, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:34.475274: step 5849, loss 0.156809, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:34.686109: step 5850, loss 0.14104, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:34.892818: step 5851, loss 0.286554, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:35.102599: step 5852, loss 0.132872, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:35.310845: step 5853, loss 0.221691, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:35.503604: step 5854, loss 0.103509, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:35.691022: step 5855, loss 0.16698, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:35.875005: step 5856, loss 0.0913071, acc 1, learning_rate 0.0001
2017-09-29T11:16:36.059533: step 5857, loss 0.33191, acc 0.828125, learning_rate 0.0001
2017-09-29T11:16:36.251253: step 5858, loss 0.208557, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:36.436893: step 5859, loss 0.24507, acc 0.875, learning_rate 0.0001
2017-09-29T11:16:36.632348: step 5860, loss 0.213618, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:36.827538: step 5861, loss 0.135248, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:37.021037: step 5862, loss 0.149455, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:37.202480: step 5863, loss 0.168606, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:37.384403: step 5864, loss 0.219095, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:37.584251: step 5865, loss 0.205351, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:37.768309: step 5866, loss 0.29193, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:37.949812: step 5867, loss 0.115855, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:38.134362: step 5868, loss 0.267326, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:38.319837: step 5869, loss 0.150891, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:38.514215: step 5870, loss 0.289521, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:38.703480: step 5871, loss 0.171609, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:38.897203: step 5872, loss 0.175416, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:39.105106: step 5873, loss 0.279842, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:39.304853: step 5874, loss 0.226057, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:39.497547: step 5875, loss 0.179844, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:39.681648: step 5876, loss 0.203578, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:39.879734: step 5877, loss 0.134545, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:40.082774: step 5878, loss 0.159644, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:40.286908: step 5879, loss 0.247982, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:40.458943: step 5880, loss 0.210072, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-09-29T11:16:41.056354: step 5880, loss 0.281579, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5880

2017-09-29T11:16:41.793905: step 5881, loss 0.328585, acc 0.859375, learning_rate 0.0001
2017-09-29T11:16:42.000468: step 5882, loss 0.201805, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:42.201682: step 5883, loss 0.156669, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:42.405647: step 5884, loss 0.115855, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:42.615389: step 5885, loss 0.125806, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:42.821846: step 5886, loss 0.15722, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:43.026914: step 5887, loss 0.242713, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:43.231498: step 5888, loss 0.214371, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:43.417145: step 5889, loss 0.205666, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:43.611015: step 5890, loss 0.123459, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:43.793207: step 5891, loss 0.157755, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:43.996396: step 5892, loss 0.168662, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:44.182588: step 5893, loss 0.137854, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:44.364783: step 5894, loss 0.131181, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:44.548527: step 5895, loss 0.278614, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:44.738036: step 5896, loss 0.233147, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:44.920588: step 5897, loss 0.190147, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:45.117442: step 5898, loss 0.193799, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:45.308918: step 5899, loss 0.190006, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:45.504055: step 5900, loss 0.163914, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:45.694665: step 5901, loss 0.217606, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:45.885606: step 5902, loss 0.308868, acc 0.875, learning_rate 0.0001
2017-09-29T11:16:46.072162: step 5903, loss 0.114784, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:46.261913: step 5904, loss 0.238247, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:46.459888: step 5905, loss 0.133207, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:46.642835: step 5906, loss 0.0916257, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:46.829137: step 5907, loss 0.215275, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:47.015035: step 5908, loss 0.193253, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:47.196423: step 5909, loss 0.132729, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:47.391503: step 5910, loss 0.131371, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:47.604539: step 5911, loss 0.157946, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:47.790180: step 5912, loss 0.174187, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:47.981810: step 5913, loss 0.163288, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:48.164609: step 5914, loss 0.1623, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:48.347553: step 5915, loss 0.26108, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:48.531705: step 5916, loss 0.121777, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:48.715860: step 5917, loss 0.115731, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:48.912076: step 5918, loss 0.203874, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:49.099396: step 5919, loss 0.139415, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:49.280873: step 5920, loss 0.15313, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:16:49.817540: step 5920, loss 0.282141, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5920

2017-09-29T11:16:50.518103: step 5921, loss 0.199953, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:50.709093: step 5922, loss 0.259356, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:50.895594: step 5923, loss 0.127397, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:51.078894: step 5924, loss 0.16377, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:51.261348: step 5925, loss 0.10242, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:51.443712: step 5926, loss 0.102632, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:51.634078: step 5927, loss 0.211609, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:51.818556: step 5928, loss 0.211391, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:52.000846: step 5929, loss 0.21828, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:52.182934: step 5930, loss 0.180391, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:52.366251: step 5931, loss 0.213385, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:52.549252: step 5932, loss 0.218958, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:52.736011: step 5933, loss 0.0873151, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:52.919489: step 5934, loss 0.194573, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:53.101722: step 5935, loss 0.119101, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:53.282368: step 5936, loss 0.0958818, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:53.477233: step 5937, loss 0.183612, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:53.662643: step 5938, loss 0.0945705, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:53.848781: step 5939, loss 0.0709056, acc 0.984375, learning_rate 0.0001
2017-09-29T11:16:54.051796: step 5940, loss 0.191454, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:54.244507: step 5941, loss 0.147816, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:54.427046: step 5942, loss 0.184004, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:54.610576: step 5943, loss 0.155682, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:54.796188: step 5944, loss 0.156861, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:54.979988: step 5945, loss 0.216491, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:55.169279: step 5946, loss 0.170465, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:55.352766: step 5947, loss 0.203607, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:55.538000: step 5948, loss 0.287304, acc 0.90625, learning_rate 0.0001
2017-09-29T11:16:55.733858: step 5949, loss 0.200343, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:55.921932: step 5950, loss 0.237899, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:56.104330: step 5951, loss 0.129523, acc 0.953125, learning_rate 0.0001
2017-09-29T11:16:56.287233: step 5952, loss 0.246929, acc 0.890625, learning_rate 0.0001
2017-09-29T11:16:56.471276: step 5953, loss 0.134203, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:56.661712: step 5954, loss 0.147712, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:56.846749: step 5955, loss 0.14469, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:57.030275: step 5956, loss 0.125193, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:57.217887: step 5957, loss 0.15546, acc 0.9375, learning_rate 0.0001
2017-09-29T11:16:57.408378: step 5958, loss 0.133916, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:57.618542: step 5959, loss 0.210294, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:57.825129: step 5960, loss 0.160606, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:16:58.411112: step 5960, loss 0.278026, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-5960

2017-09-29T11:16:59.222760: step 5961, loss 0.176263, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:59.416098: step 5962, loss 0.34, acc 0.859375, learning_rate 0.0001
2017-09-29T11:16:59.605976: step 5963, loss 0.130774, acc 0.96875, learning_rate 0.0001
2017-09-29T11:16:59.792594: step 5964, loss 0.25896, acc 0.921875, learning_rate 0.0001
2017-09-29T11:16:59.978588: step 5965, loss 0.0460814, acc 1, learning_rate 0.0001
2017-09-29T11:17:00.163266: step 5966, loss 0.311243, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:00.345316: step 5967, loss 0.246724, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:00.541659: step 5968, loss 0.185547, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:00.728881: step 5969, loss 0.147163, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:00.911610: step 5970, loss 0.233672, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:01.096855: step 5971, loss 0.150598, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:01.279941: step 5972, loss 0.182997, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:01.463400: step 5973, loss 0.189376, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:01.658356: step 5974, loss 0.184494, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:01.839838: step 5975, loss 0.148997, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:02.021934: step 5976, loss 0.110693, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:02.201455: step 5977, loss 0.319073, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:02.353766: step 5978, loss 0.282537, acc 0.882353, learning_rate 0.0001
2017-09-29T11:17:02.539746: step 5979, loss 0.217608, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:02.726407: step 5980, loss 0.0827633, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:02.910294: step 5981, loss 0.156859, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:03.092562: step 5982, loss 0.136844, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:03.274721: step 5983, loss 0.306352, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:03.460502: step 5984, loss 0.207388, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:03.647013: step 5985, loss 0.205269, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:03.832542: step 5986, loss 0.135334, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:04.014054: step 5987, loss 0.196685, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:04.203733: step 5988, loss 0.156244, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:04.389463: step 5989, loss 0.243055, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:04.581171: step 5990, loss 0.181183, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:04.762436: step 5991, loss 0.203639, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:04.953438: step 5992, loss 0.170442, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:05.150331: step 5993, loss 0.167042, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:05.336930: step 5994, loss 0.159314, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:05.532707: step 5995, loss 0.104097, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:05.714163: step 5996, loss 0.18857, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:05.898337: step 5997, loss 0.270863, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:06.088923: step 5998, loss 0.184388, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:06.273574: step 5999, loss 0.176632, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:06.460281: step 6000, loss 0.238885, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:17:07.015906: step 6000, loss 0.282481, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6000

2017-09-29T11:17:07.654148: step 6001, loss 0.209928, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:07.834747: step 6002, loss 0.126069, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:08.012908: step 6003, loss 0.227077, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:08.203433: step 6004, loss 0.139627, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:08.385906: step 6005, loss 0.240162, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:08.566871: step 6006, loss 0.10138, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:08.749507: step 6007, loss 0.245578, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:08.933032: step 6008, loss 0.131746, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:09.129128: step 6009, loss 0.175881, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:09.317864: step 6010, loss 0.121212, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:09.504468: step 6011, loss 0.206993, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:09.690201: step 6012, loss 0.276878, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:09.881030: step 6013, loss 0.194494, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:10.065180: step 6014, loss 0.183289, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:10.245576: step 6015, loss 0.234419, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:10.434481: step 6016, loss 0.137107, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:10.618400: step 6017, loss 0.185471, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:10.799979: step 6018, loss 0.0665989, acc 1, learning_rate 0.0001
2017-09-29T11:17:10.983683: step 6019, loss 0.0899679, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:11.167175: step 6020, loss 0.222359, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:11.350419: step 6021, loss 0.226314, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:11.533293: step 6022, loss 0.241875, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:11.718827: step 6023, loss 0.142948, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:11.900768: step 6024, loss 0.228686, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:12.081759: step 6025, loss 0.148657, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:12.265293: step 6026, loss 0.172194, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:12.449922: step 6027, loss 0.194192, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:12.633389: step 6028, loss 0.164529, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:12.816019: step 6029, loss 0.208043, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:13.000092: step 6030, loss 0.165841, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:13.183986: step 6031, loss 0.259075, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:13.378263: step 6032, loss 0.110513, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:13.560682: step 6033, loss 0.110218, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:13.743990: step 6034, loss 0.222209, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:13.930715: step 6035, loss 0.116261, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:14.118827: step 6036, loss 0.118397, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:14.306515: step 6037, loss 0.339405, acc 0.84375, learning_rate 0.0001
2017-09-29T11:17:14.494888: step 6038, loss 0.132068, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:14.677034: step 6039, loss 0.258888, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:14.862696: step 6040, loss 0.268247, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:17:15.395986: step 6040, loss 0.278694, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6040

2017-09-29T11:17:16.095500: step 6041, loss 0.185311, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:16.277250: step 6042, loss 0.134493, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:16.474179: step 6043, loss 0.170228, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:16.667120: step 6044, loss 0.362023, acc 0.84375, learning_rate 0.0001
2017-09-29T11:17:16.851431: step 6045, loss 0.161517, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:17.034840: step 6046, loss 0.219096, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:17.220726: step 6047, loss 0.154648, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:17.405095: step 6048, loss 0.167696, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:17.597880: step 6049, loss 0.285833, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:17.782679: step 6050, loss 0.179724, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:17.969069: step 6051, loss 0.148091, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:18.152537: step 6052, loss 0.189465, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:18.341548: step 6053, loss 0.175136, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:18.527633: step 6054, loss 0.171645, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:18.714108: step 6055, loss 0.265878, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:18.897705: step 6056, loss 0.218584, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:19.081113: step 6057, loss 0.108884, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:19.269516: step 6058, loss 0.254385, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:19.455749: step 6059, loss 0.116224, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:19.654704: step 6060, loss 0.364152, acc 0.859375, learning_rate 0.0001
2017-09-29T11:17:19.849740: step 6061, loss 0.122833, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:20.044034: step 6062, loss 0.273289, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:20.229236: step 6063, loss 0.147074, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:20.420620: step 6064, loss 0.229598, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:20.603532: step 6065, loss 0.196624, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:20.790427: step 6066, loss 0.140116, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:20.972849: step 6067, loss 0.120398, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:21.156957: step 6068, loss 0.188133, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:21.340836: step 6069, loss 0.182116, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:21.526952: step 6070, loss 0.13282, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:21.712820: step 6071, loss 0.156752, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:21.897477: step 6072, loss 0.161182, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:22.095252: step 6073, loss 0.19464, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:22.279526: step 6074, loss 0.172292, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:22.473695: step 6075, loss 0.169993, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:22.628521: step 6076, loss 0.136426, acc 0.980392, learning_rate 0.0001
2017-09-29T11:17:22.813293: step 6077, loss 0.160607, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:22.992693: step 6078, loss 0.116485, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:23.177712: step 6079, loss 0.11844, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:23.358348: step 6080, loss 0.134148, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:17:23.896002: step 6080, loss 0.277438, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6080

2017-09-29T11:17:24.614616: step 6081, loss 0.0891709, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:24.798606: step 6082, loss 0.194004, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:24.987362: step 6083, loss 0.0905844, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:25.170275: step 6084, loss 0.204605, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:25.354575: step 6085, loss 0.233013, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:25.538625: step 6086, loss 0.166658, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:25.722509: step 6087, loss 0.132325, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:25.908109: step 6088, loss 0.105665, acc 1, learning_rate 0.0001
2017-09-29T11:17:26.092984: step 6089, loss 0.143437, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:26.277632: step 6090, loss 0.163808, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:26.462494: step 6091, loss 0.123811, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:26.648056: step 6092, loss 0.0864655, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:26.829497: step 6093, loss 0.2275, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:27.021795: step 6094, loss 0.0864823, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:27.202016: step 6095, loss 0.101254, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:27.395922: step 6096, loss 0.147683, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:27.588389: step 6097, loss 0.12872, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:27.773731: step 6098, loss 0.152154, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:27.955150: step 6099, loss 0.193845, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:28.139762: step 6100, loss 0.19607, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:28.322016: step 6101, loss 0.289549, acc 0.859375, learning_rate 0.0001
2017-09-29T11:17:28.517890: step 6102, loss 0.186081, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:28.702551: step 6103, loss 0.0719252, acc 1, learning_rate 0.0001
2017-09-29T11:17:28.885173: step 6104, loss 0.079285, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:29.069046: step 6105, loss 0.151245, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:29.262729: step 6106, loss 0.261689, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:29.451918: step 6107, loss 0.157528, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:29.637582: step 6108, loss 0.23783, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:29.819436: step 6109, loss 0.261418, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:30.004270: step 6110, loss 0.118591, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:30.188171: step 6111, loss 0.136172, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:30.370899: step 6112, loss 0.173308, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:30.557574: step 6113, loss 0.17908, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:30.744288: step 6114, loss 0.223205, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:30.937906: step 6115, loss 0.218172, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:31.122790: step 6116, loss 0.2098, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:31.313605: step 6117, loss 0.143302, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:31.502081: step 6118, loss 0.170038, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:31.685063: step 6119, loss 0.0987237, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:31.869942: step 6120, loss 0.154871, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:17:32.411690: step 6120, loss 0.27601, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6120

2017-09-29T11:17:33.198886: step 6121, loss 0.123876, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:33.381425: step 6122, loss 0.184672, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:33.579383: step 6123, loss 0.184299, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:33.762809: step 6124, loss 0.185459, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:33.946727: step 6125, loss 0.141065, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:34.130824: step 6126, loss 0.114638, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:34.318146: step 6127, loss 0.220742, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:34.511943: step 6128, loss 0.290971, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:34.698936: step 6129, loss 0.211528, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:34.887865: step 6130, loss 0.131308, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:35.078134: step 6131, loss 0.124188, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:35.262427: step 6132, loss 0.123534, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:35.451571: step 6133, loss 0.217754, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:35.632548: step 6134, loss 0.283444, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:35.820763: step 6135, loss 0.23115, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:36.007386: step 6136, loss 0.119317, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:36.193318: step 6137, loss 0.134432, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:36.378339: step 6138, loss 0.142466, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:36.565380: step 6139, loss 0.0785705, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:36.750734: step 6140, loss 0.315507, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:36.931732: step 6141, loss 0.270613, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:37.113183: step 6142, loss 0.184741, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:37.299461: step 6143, loss 0.280626, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:37.483099: step 6144, loss 0.0944601, acc 1, learning_rate 0.0001
2017-09-29T11:17:37.667946: step 6145, loss 0.219032, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:37.859252: step 6146, loss 0.0838082, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:38.042643: step 6147, loss 0.21201, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:38.226716: step 6148, loss 0.233541, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:38.412357: step 6149, loss 0.125821, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:38.599743: step 6150, loss 0.206513, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:38.783826: step 6151, loss 0.19711, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:38.967246: step 6152, loss 0.258102, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:39.153050: step 6153, loss 0.116263, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:39.339018: step 6154, loss 0.130463, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:39.537585: step 6155, loss 0.047512, acc 1, learning_rate 0.0001
2017-09-29T11:17:39.719434: step 6156, loss 0.168812, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:39.900904: step 6157, loss 0.229079, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:40.092449: step 6158, loss 0.143817, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:40.278701: step 6159, loss 0.110501, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:40.465070: step 6160, loss 0.185293, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:17:41.004135: step 6160, loss 0.279835, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6160

2017-09-29T11:17:41.648992: step 6161, loss 0.188206, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:41.834610: step 6162, loss 0.114504, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:42.026577: step 6163, loss 0.164501, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:42.212399: step 6164, loss 0.11861, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:42.396832: step 6165, loss 0.144985, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:42.578365: step 6166, loss 0.151504, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:42.771001: step 6167, loss 0.110356, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:42.954641: step 6168, loss 0.207364, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:43.134916: step 6169, loss 0.098393, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:43.314334: step 6170, loss 0.110133, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:43.498026: step 6171, loss 0.181429, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:43.690669: step 6172, loss 0.224826, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:43.873861: step 6173, loss 0.138193, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:44.025148: step 6174, loss 0.191904, acc 0.941176, learning_rate 0.0001
2017-09-29T11:17:44.215251: step 6175, loss 0.142357, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:44.404840: step 6176, loss 0.0893288, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:44.588034: step 6177, loss 0.216098, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:44.782331: step 6178, loss 0.1611, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:44.965057: step 6179, loss 0.115271, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:45.149963: step 6180, loss 0.254479, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:45.341766: step 6181, loss 0.0737594, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:45.547174: step 6182, loss 0.0827531, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:45.730573: step 6183, loss 0.221003, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:45.913639: step 6184, loss 0.108414, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:46.096827: step 6185, loss 0.137625, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:46.289094: step 6186, loss 0.252563, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:46.472169: step 6187, loss 0.195113, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:46.655978: step 6188, loss 0.152635, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:46.842275: step 6189, loss 0.145274, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:47.026358: step 6190, loss 0.184655, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:47.210755: step 6191, loss 0.0824715, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:47.401272: step 6192, loss 0.324049, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:47.584331: step 6193, loss 0.283985, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:47.769739: step 6194, loss 0.309393, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:47.951290: step 6195, loss 0.230785, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:48.136434: step 6196, loss 0.27956, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:48.324314: step 6197, loss 0.0858655, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:48.507777: step 6198, loss 0.160919, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:48.699491: step 6199, loss 0.101875, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:48.884960: step 6200, loss 0.321337, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:17:49.417425: step 6200, loss 0.275703, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6200

2017-09-29T11:17:50.145536: step 6201, loss 0.104326, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:50.349020: step 6202, loss 0.185166, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:50.550763: step 6203, loss 0.183553, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:50.739893: step 6204, loss 0.202019, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:50.933035: step 6205, loss 0.114028, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:51.114063: step 6206, loss 0.156445, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:51.298170: step 6207, loss 0.13834, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:51.484227: step 6208, loss 0.231462, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:51.667211: step 6209, loss 0.0881667, acc 0.984375, learning_rate 0.0001
2017-09-29T11:17:51.850003: step 6210, loss 0.106736, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:52.029735: step 6211, loss 0.143595, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:52.213540: step 6212, loss 0.108077, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:52.397319: step 6213, loss 0.233128, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:52.591332: step 6214, loss 0.155133, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:52.788484: step 6215, loss 0.137049, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:52.968142: step 6216, loss 0.182596, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:53.153415: step 6217, loss 0.150329, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:53.337837: step 6218, loss 0.163756, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:53.523487: step 6219, loss 0.131682, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:53.707876: step 6220, loss 0.22508, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:53.900380: step 6221, loss 0.117606, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:54.084736: step 6222, loss 0.154384, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:54.270315: step 6223, loss 0.144353, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:54.457680: step 6224, loss 0.160277, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:54.648507: step 6225, loss 0.187374, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:54.831969: step 6226, loss 0.168406, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:55.016564: step 6227, loss 0.11883, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:55.217442: step 6228, loss 0.173282, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:55.401855: step 6229, loss 0.159523, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:55.585665: step 6230, loss 0.273906, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:55.771831: step 6231, loss 0.171932, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:55.954456: step 6232, loss 0.294797, acc 0.890625, learning_rate 0.0001
2017-09-29T11:17:56.156242: step 6233, loss 0.17897, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:56.364592: step 6234, loss 0.172966, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:56.566641: step 6235, loss 0.253542, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:56.751764: step 6236, loss 0.128178, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:56.933238: step 6237, loss 0.277559, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:57.118008: step 6238, loss 0.184291, acc 0.953125, learning_rate 0.0001
2017-09-29T11:17:57.299494: step 6239, loss 0.247269, acc 0.90625, learning_rate 0.0001
2017-09-29T11:17:57.486333: step 6240, loss 0.20366, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:17:58.015619: step 6240, loss 0.279205, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6240

2017-09-29T11:17:58.722688: step 6241, loss 0.34623, acc 0.875, learning_rate 0.0001
2017-09-29T11:17:58.905357: step 6242, loss 0.17882, acc 0.921875, learning_rate 0.0001
2017-09-29T11:17:59.087482: step 6243, loss 0.126932, acc 0.96875, learning_rate 0.0001
2017-09-29T11:17:59.272320: step 6244, loss 0.332255, acc 0.859375, learning_rate 0.0001
2017-09-29T11:17:59.468918: step 6245, loss 0.243342, acc 0.9375, learning_rate 0.0001
2017-09-29T11:17:59.650652: step 6246, loss 0.295406, acc 0.859375, learning_rate 0.0001
2017-09-29T11:17:59.833215: step 6247, loss 0.198535, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:00.019369: step 6248, loss 0.185156, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:00.213170: step 6249, loss 0.200745, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:00.399848: step 6250, loss 0.168087, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:00.590675: step 6251, loss 0.128536, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:00.772484: step 6252, loss 0.150867, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:00.956649: step 6253, loss 0.256149, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:01.145816: step 6254, loss 0.195538, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:01.333141: step 6255, loss 0.193927, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:01.518820: step 6256, loss 0.145359, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:01.703473: step 6257, loss 0.194559, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:01.899051: step 6258, loss 0.191618, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:02.088496: step 6259, loss 0.0695963, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:02.272498: step 6260, loss 0.114196, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:02.466671: step 6261, loss 0.126935, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:02.650790: step 6262, loss 0.139773, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:02.834773: step 6263, loss 0.102831, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:03.015519: step 6264, loss 0.147273, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:03.201523: step 6265, loss 0.264961, acc 0.875, learning_rate 0.0001
2017-09-29T11:18:03.389033: step 6266, loss 0.194916, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:03.573067: step 6267, loss 0.206247, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:03.759106: step 6268, loss 0.168307, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:03.948649: step 6269, loss 0.186949, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:04.135096: step 6270, loss 0.0872996, acc 1, learning_rate 0.0001
2017-09-29T11:18:04.316286: step 6271, loss 0.140606, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:04.475486: step 6272, loss 0.119558, acc 0.980392, learning_rate 0.0001
2017-09-29T11:18:04.660678: step 6273, loss 0.167685, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:04.846896: step 6274, loss 0.187573, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:05.033523: step 6275, loss 0.124338, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:05.221130: step 6276, loss 0.090638, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:05.405502: step 6277, loss 0.183374, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:05.592119: step 6278, loss 0.16979, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:05.775287: step 6279, loss 0.146644, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:05.958751: step 6280, loss 0.163599, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:18:06.499457: step 6280, loss 0.274244, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6280

2017-09-29T11:18:07.284739: step 6281, loss 0.0603838, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:07.482505: step 6282, loss 0.341622, acc 0.859375, learning_rate 0.0001
2017-09-29T11:18:07.668714: step 6283, loss 0.19181, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:07.854640: step 6284, loss 0.219189, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:08.049801: step 6285, loss 0.0594293, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:08.234386: step 6286, loss 0.118953, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:08.421674: step 6287, loss 0.231774, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:08.606133: step 6288, loss 0.0942389, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:08.793024: step 6289, loss 0.21805, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:08.974302: step 6290, loss 0.223212, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:09.155483: step 6291, loss 0.164624, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:09.338659: step 6292, loss 0.12117, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:09.528883: step 6293, loss 0.100338, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:09.713596: step 6294, loss 0.240263, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:09.896777: step 6295, loss 0.0875187, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:10.090741: step 6296, loss 0.130001, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:10.277459: step 6297, loss 0.321252, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:10.460870: step 6298, loss 0.190015, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:10.644182: step 6299, loss 0.202148, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:10.826844: step 6300, loss 0.241993, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:11.006725: step 6301, loss 0.0841594, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:11.198706: step 6302, loss 0.198053, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:11.387262: step 6303, loss 0.10825, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:11.573235: step 6304, loss 0.125515, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:11.763019: step 6305, loss 0.146264, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:11.945873: step 6306, loss 0.154011, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:12.129220: step 6307, loss 0.0928886, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:12.343537: step 6308, loss 0.173358, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:12.542377: step 6309, loss 0.142006, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:12.734496: step 6310, loss 0.197684, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:12.923235: step 6311, loss 0.176782, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:13.115306: step 6312, loss 0.0905838, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:13.302297: step 6313, loss 0.123758, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:13.505364: step 6314, loss 0.157096, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:13.698979: step 6315, loss 0.0830387, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:13.888619: step 6316, loss 0.140434, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:14.079370: step 6317, loss 0.221626, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:14.266763: step 6318, loss 0.266789, acc 0.875, learning_rate 0.0001
2017-09-29T11:18:14.457299: step 6319, loss 0.253941, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:14.650605: step 6320, loss 0.125143, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:18:15.193652: step 6320, loss 0.280912, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6320

2017-09-29T11:18:15.847890: step 6321, loss 0.152013, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:16.033270: step 6322, loss 0.162311, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:16.233045: step 6323, loss 0.143683, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:16.436463: step 6324, loss 0.0943101, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:16.624024: step 6325, loss 0.140752, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:16.812144: step 6326, loss 0.197146, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:16.994949: step 6327, loss 0.199156, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:17.177556: step 6328, loss 0.204694, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:17.368784: step 6329, loss 0.152918, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:17.557426: step 6330, loss 0.149633, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:17.741477: step 6331, loss 0.20508, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:17.925072: step 6332, loss 0.210647, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:18.109618: step 6333, loss 0.137176, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:18.295055: step 6334, loss 0.180012, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:18.476833: step 6335, loss 0.195771, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:18.662758: step 6336, loss 0.192315, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:18.848374: step 6337, loss 0.113173, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:19.040763: step 6338, loss 0.187577, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:19.225118: step 6339, loss 0.145637, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:19.413032: step 6340, loss 0.181369, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:19.616501: step 6341, loss 0.132615, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:19.800052: step 6342, loss 0.127998, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:19.983089: step 6343, loss 0.156787, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:20.169824: step 6344, loss 0.195805, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:20.359649: step 6345, loss 0.164865, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:20.553280: step 6346, loss 0.166096, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:20.761253: step 6347, loss 0.199613, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:20.964073: step 6348, loss 0.167703, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:21.160129: step 6349, loss 0.053055, acc 1, learning_rate 0.0001
2017-09-29T11:18:21.341857: step 6350, loss 0.197251, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:21.527860: step 6351, loss 0.150459, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:21.714084: step 6352, loss 0.176183, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:21.900007: step 6353, loss 0.244336, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:22.083751: step 6354, loss 0.185795, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:22.264380: step 6355, loss 0.127393, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:22.446284: step 6356, loss 0.197825, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:22.634964: step 6357, loss 0.240554, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:22.818976: step 6358, loss 0.237629, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:23.002800: step 6359, loss 0.182491, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:23.185644: step 6360, loss 0.190883, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:18:23.728063: step 6360, loss 0.273519, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6360

2017-09-29T11:18:24.439748: step 6361, loss 0.197431, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:24.626285: step 6362, loss 0.208801, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:24.811134: step 6363, loss 0.179036, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:24.993859: step 6364, loss 0.10528, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:25.175198: step 6365, loss 0.215382, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:25.363365: step 6366, loss 0.1425, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:25.546388: step 6367, loss 0.347028, acc 0.859375, learning_rate 0.0001
2017-09-29T11:18:25.733174: step 6368, loss 0.205825, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:25.914881: step 6369, loss 0.206808, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:26.064438: step 6370, loss 0.243328, acc 0.882353, learning_rate 0.0001
2017-09-29T11:18:26.248440: step 6371, loss 0.193019, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:26.431023: step 6372, loss 0.120583, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:26.622274: step 6373, loss 0.11305, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:26.805088: step 6374, loss 0.199086, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:26.997550: step 6375, loss 0.121818, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:27.182179: step 6376, loss 0.119712, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:27.366134: step 6377, loss 0.138764, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:27.549238: step 6378, loss 0.181484, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:27.730713: step 6379, loss 0.186046, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:27.916359: step 6380, loss 0.171994, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:28.112321: step 6381, loss 0.333962, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:28.294967: step 6382, loss 0.141959, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:28.482150: step 6383, loss 0.201998, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:28.667034: step 6384, loss 0.170392, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:28.852650: step 6385, loss 0.114994, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:29.037720: step 6386, loss 0.317556, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:29.217748: step 6387, loss 0.149608, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:29.414445: step 6388, loss 0.12988, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:29.605152: step 6389, loss 0.253293, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:29.793230: step 6390, loss 0.180182, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:29.975486: step 6391, loss 0.219536, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:30.161138: step 6392, loss 0.133204, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:30.351144: step 6393, loss 0.209658, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:30.546173: step 6394, loss 0.20011, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:30.730948: step 6395, loss 0.217755, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:30.916742: step 6396, loss 0.142166, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:31.103299: step 6397, loss 0.152626, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:31.290604: step 6398, loss 0.133978, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:31.475337: step 6399, loss 0.151326, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:31.658914: step 6400, loss 0.294906, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-09-29T11:18:32.270130: step 6400, loss 0.277827, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6400

2017-09-29T11:18:32.981585: step 6401, loss 0.249542, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:33.164738: step 6402, loss 0.175672, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:33.348807: step 6403, loss 0.240597, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:33.543500: step 6404, loss 0.13293, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:33.740862: step 6405, loss 0.087379, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:33.927399: step 6406, loss 0.171943, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:34.112694: step 6407, loss 0.122709, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:34.294999: step 6408, loss 0.202628, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:34.480618: step 6409, loss 0.209477, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:34.671337: step 6410, loss 0.138478, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:34.857428: step 6411, loss 0.197785, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:35.042468: step 6412, loss 0.25273, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:35.223461: step 6413, loss 0.233591, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:35.407621: step 6414, loss 0.179852, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:35.592990: step 6415, loss 0.190521, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:35.776584: step 6416, loss 0.163301, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:35.959649: step 6417, loss 0.0621221, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:36.151623: step 6418, loss 0.184363, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:36.343063: step 6419, loss 0.264602, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:36.541418: step 6420, loss 0.202899, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:36.729731: step 6421, loss 0.109859, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:36.913741: step 6422, loss 0.117236, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:37.094863: step 6423, loss 0.323242, acc 0.859375, learning_rate 0.0001
2017-09-29T11:18:37.278378: step 6424, loss 0.234233, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:37.463183: step 6425, loss 0.238657, acc 0.875, learning_rate 0.0001
2017-09-29T11:18:37.648096: step 6426, loss 0.0779795, acc 1, learning_rate 0.0001
2017-09-29T11:18:37.831512: step 6427, loss 0.0886333, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:38.015825: step 6428, loss 0.183451, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:38.203608: step 6429, loss 0.103715, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:38.386391: step 6430, loss 0.206954, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:38.572285: step 6431, loss 0.176358, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:38.758311: step 6432, loss 0.142131, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:38.945025: step 6433, loss 0.117637, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:39.126090: step 6434, loss 0.0861143, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:39.308707: step 6435, loss 0.126882, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:39.500231: step 6436, loss 0.193973, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:39.692429: step 6437, loss 0.175921, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:39.897028: step 6438, loss 0.125073, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:40.102862: step 6439, loss 0.107835, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:40.295551: step 6440, loss 0.102055, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:18:40.846242: step 6440, loss 0.273481, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6440

2017-09-29T11:18:41.631911: step 6441, loss 0.175615, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:41.824270: step 6442, loss 0.224629, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:42.014352: step 6443, loss 0.16942, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:42.198981: step 6444, loss 0.152037, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:42.381267: step 6445, loss 0.164394, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:42.589799: step 6446, loss 0.110614, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:42.774498: step 6447, loss 0.125442, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:42.957393: step 6448, loss 0.157562, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:43.138819: step 6449, loss 0.0753108, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:43.326132: step 6450, loss 0.109142, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:43.509770: step 6451, loss 0.314306, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:43.691148: step 6452, loss 0.137441, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:43.872316: step 6453, loss 0.230983, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:44.059148: step 6454, loss 0.108799, acc 1, learning_rate 0.0001
2017-09-29T11:18:44.243573: step 6455, loss 0.221394, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:44.428661: step 6456, loss 0.215478, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:44.623008: step 6457, loss 0.187965, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:44.810419: step 6458, loss 0.148941, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:45.004414: step 6459, loss 0.163313, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:45.187504: step 6460, loss 0.129731, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:45.386415: step 6461, loss 0.0930397, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:45.573269: step 6462, loss 0.198821, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:45.764975: step 6463, loss 0.204459, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:45.952010: step 6464, loss 0.244231, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:46.136514: step 6465, loss 0.119705, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:46.318443: step 6466, loss 0.171435, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:46.505211: step 6467, loss 0.288068, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:46.662812: step 6468, loss 0.190529, acc 0.960784, learning_rate 0.0001
2017-09-29T11:18:46.849683: step 6469, loss 0.198712, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:47.045086: step 6470, loss 0.191548, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:47.243229: step 6471, loss 0.333153, acc 0.859375, learning_rate 0.0001
2017-09-29T11:18:47.427437: step 6472, loss 0.168996, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:47.613549: step 6473, loss 0.0713422, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:47.795886: step 6474, loss 0.196325, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:47.978962: step 6475, loss 0.0867232, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:48.161860: step 6476, loss 0.111747, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:48.342901: step 6477, loss 0.141036, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:48.536460: step 6478, loss 0.146479, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:48.720792: step 6479, loss 0.243313, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:48.905682: step 6480, loss 0.227265, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:18:49.465905: step 6480, loss 0.272154, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6480

2017-09-29T11:18:50.101377: step 6481, loss 0.166778, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:50.285197: step 6482, loss 0.108184, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:50.474350: step 6483, loss 0.151894, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:50.658158: step 6484, loss 0.183381, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:50.865549: step 6485, loss 0.240833, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:51.049060: step 6486, loss 0.146188, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:51.238144: step 6487, loss 0.143608, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:51.420369: step 6488, loss 0.126781, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:51.603749: step 6489, loss 0.121937, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:51.786823: step 6490, loss 0.219692, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:51.970962: step 6491, loss 0.18958, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:52.162206: step 6492, loss 0.0728644, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:52.345849: step 6493, loss 0.141582, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:52.531068: step 6494, loss 0.263991, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:52.714258: step 6495, loss 0.130451, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:52.899486: step 6496, loss 0.216955, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:53.083641: step 6497, loss 0.143679, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:53.273480: step 6498, loss 0.215339, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:53.456812: step 6499, loss 0.0834623, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:53.647652: step 6500, loss 0.244564, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:53.839771: step 6501, loss 0.229744, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:54.024999: step 6502, loss 0.140323, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:54.217001: step 6503, loss 0.0870892, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:54.401823: step 6504, loss 0.28039, acc 0.875, learning_rate 0.0001
2017-09-29T11:18:54.599382: step 6505, loss 0.211541, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:54.788002: step 6506, loss 0.154138, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:54.973489: step 6507, loss 0.102473, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:55.155051: step 6508, loss 0.221281, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:55.336242: step 6509, loss 0.0878657, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:55.524982: step 6510, loss 0.204958, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:55.713639: step 6511, loss 0.187048, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:55.914781: step 6512, loss 0.150902, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:56.096689: step 6513, loss 0.148645, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:56.284129: step 6514, loss 0.240155, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:56.484939: step 6515, loss 0.175843, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:56.666884: step 6516, loss 0.178733, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:56.848948: step 6517, loss 0.127876, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:57.030864: step 6518, loss 0.302822, acc 0.875, learning_rate 0.0001
2017-09-29T11:18:57.211051: step 6519, loss 0.242507, acc 0.890625, learning_rate 0.0001
2017-09-29T11:18:57.394545: step 6520, loss 0.124799, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T11:18:57.956075: step 6520, loss 0.276819, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6520

2017-09-29T11:18:58.660387: step 6521, loss 0.170948, acc 0.921875, learning_rate 0.0001
2017-09-29T11:18:58.846630: step 6522, loss 0.155971, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:59.028551: step 6523, loss 0.264309, acc 0.90625, learning_rate 0.0001
2017-09-29T11:18:59.210991: step 6524, loss 0.130795, acc 0.953125, learning_rate 0.0001
2017-09-29T11:18:59.396757: step 6525, loss 0.0673048, acc 0.96875, learning_rate 0.0001
2017-09-29T11:18:59.582400: step 6526, loss 0.158226, acc 0.9375, learning_rate 0.0001
2017-09-29T11:18:59.762583: step 6527, loss 0.0886338, acc 0.984375, learning_rate 0.0001
2017-09-29T11:18:59.956856: step 6528, loss 0.0868204, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:00.139444: step 6529, loss 0.296448, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:00.319692: step 6530, loss 0.120985, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:00.516598: step 6531, loss 0.189285, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:00.699662: step 6532, loss 0.257774, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:00.886926: step 6533, loss 0.112831, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:01.068362: step 6534, loss 0.183528, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:01.249066: step 6535, loss 0.139827, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:01.434849: step 6536, loss 0.201492, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:01.628162: step 6537, loss 0.0904655, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:01.814394: step 6538, loss 0.0874272, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:01.997599: step 6539, loss 0.0855257, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:02.183721: step 6540, loss 0.129537, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:02.363776: step 6541, loss 0.100209, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:02.547090: step 6542, loss 0.266006, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:02.733100: step 6543, loss 0.193347, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:02.923130: step 6544, loss 0.239808, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:03.107269: step 6545, loss 0.142033, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:03.291172: step 6546, loss 0.23452, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:03.477238: step 6547, loss 0.157416, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:03.664329: step 6548, loss 0.124258, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:03.850045: step 6549, loss 0.192561, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:04.031062: step 6550, loss 0.155324, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:04.214981: step 6551, loss 0.169881, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:04.397340: step 6552, loss 0.183529, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:04.577600: step 6553, loss 0.129934, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:04.760206: step 6554, loss 0.185461, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:04.947881: step 6555, loss 0.116787, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:05.140588: step 6556, loss 0.180636, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:05.324266: step 6557, loss 0.107322, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:05.527298: step 6558, loss 0.133565, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:05.724176: step 6559, loss 0.154603, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:05.927243: step 6560, loss 0.177214, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:19:06.522344: step 6560, loss 0.27633, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6560

2017-09-29T11:19:07.242379: step 6561, loss 0.213384, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:07.424173: step 6562, loss 0.131906, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:07.609349: step 6563, loss 0.0975209, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:07.800549: step 6564, loss 0.112874, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:07.989382: step 6565, loss 0.141405, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:08.142376: step 6566, loss 0.207375, acc 0.941176, learning_rate 0.0001
2017-09-29T11:19:08.348459: step 6567, loss 0.14329, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:08.557963: step 6568, loss 0.161231, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:08.745987: step 6569, loss 0.0856761, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:08.932393: step 6570, loss 0.0547168, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:09.113544: step 6571, loss 0.183532, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:09.298101: step 6572, loss 0.094564, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:09.482485: step 6573, loss 0.149931, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:09.668133: step 6574, loss 0.170781, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:09.860021: step 6575, loss 0.138981, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:10.048873: step 6576, loss 0.146678, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:10.244053: step 6577, loss 0.117335, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:10.429289: step 6578, loss 0.116719, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:10.616488: step 6579, loss 0.12154, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:10.801625: step 6580, loss 0.243879, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:10.984659: step 6581, loss 0.119755, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:11.171904: step 6582, loss 0.243696, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:11.355528: step 6583, loss 0.144795, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:11.552816: step 6584, loss 0.104622, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:11.741646: step 6585, loss 0.122717, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:11.922992: step 6586, loss 0.162806, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:12.116228: step 6587, loss 0.268025, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:12.302118: step 6588, loss 0.165531, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:12.513341: step 6589, loss 0.151245, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:12.706171: step 6590, loss 0.165749, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:12.888631: step 6591, loss 0.107711, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:13.070518: step 6592, loss 0.141909, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:13.264069: step 6593, loss 0.159431, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:13.454418: step 6594, loss 0.252764, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:13.638665: step 6595, loss 0.183627, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:13.821676: step 6596, loss 0.166672, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:14.007967: step 6597, loss 0.129099, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:14.192809: step 6598, loss 0.215202, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:14.375305: step 6599, loss 0.13864, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:14.560329: step 6600, loss 0.112079, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:19:15.116690: step 6600, loss 0.271117, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6600

2017-09-29T11:19:15.929879: step 6601, loss 0.1879, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:16.135895: step 6602, loss 0.189123, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:16.322910: step 6603, loss 0.092834, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:16.518469: step 6604, loss 0.198786, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:16.725501: step 6605, loss 0.171096, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:16.930771: step 6606, loss 0.119728, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:17.116201: step 6607, loss 0.137944, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:17.296028: step 6608, loss 0.144437, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:17.508405: step 6609, loss 0.126454, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:17.719030: step 6610, loss 0.0933406, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:17.908801: step 6611, loss 0.0937315, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:18.093733: step 6612, loss 0.150025, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:18.273942: step 6613, loss 0.334129, acc 0.84375, learning_rate 0.0001
2017-09-29T11:19:18.459682: step 6614, loss 0.177316, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:18.645271: step 6615, loss 0.103212, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:18.842598: step 6616, loss 0.0636527, acc 1, learning_rate 0.0001
2017-09-29T11:19:19.027690: step 6617, loss 0.129228, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:19.212413: step 6618, loss 0.317835, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:19.398759: step 6619, loss 0.154402, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:19.580836: step 6620, loss 0.16935, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:19.765400: step 6621, loss 0.163387, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:19.952624: step 6622, loss 0.175237, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:20.133873: step 6623, loss 0.176134, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:20.321910: step 6624, loss 0.164171, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:20.505568: step 6625, loss 0.0862487, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:20.701514: step 6626, loss 0.196027, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:20.895133: step 6627, loss 0.194664, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:21.086735: step 6628, loss 0.0693669, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:21.283886: step 6629, loss 0.0672935, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:21.471512: step 6630, loss 0.174311, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:21.655012: step 6631, loss 0.169505, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:21.839950: step 6632, loss 0.170334, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:22.028905: step 6633, loss 0.112594, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:22.218685: step 6634, loss 0.177767, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:22.407678: step 6635, loss 0.190618, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:22.594560: step 6636, loss 0.293338, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:22.782617: step 6637, loss 0.241532, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:22.974887: step 6638, loss 0.159547, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:23.158931: step 6639, loss 0.0561272, acc 1, learning_rate 0.0001
2017-09-29T11:19:23.359447: step 6640, loss 0.163816, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:19:23.939891: step 6640, loss 0.269086, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6640

2017-09-29T11:19:24.581808: step 6641, loss 0.246627, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:24.768918: step 6642, loss 0.134369, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:24.967406: step 6643, loss 0.151873, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:25.151034: step 6644, loss 0.1386, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:25.337507: step 6645, loss 0.18014, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:25.527718: step 6646, loss 0.125157, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:25.722856: step 6647, loss 0.200612, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:25.918598: step 6648, loss 0.263819, acc 0.875, learning_rate 0.0001
2017-09-29T11:19:26.110673: step 6649, loss 0.165092, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:26.294931: step 6650, loss 0.268142, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:26.483651: step 6651, loss 0.112642, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:26.666530: step 6652, loss 0.184839, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:26.853666: step 6653, loss 0.224878, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:27.038403: step 6654, loss 0.103689, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:27.222598: step 6655, loss 0.164885, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:27.412171: step 6656, loss 0.120599, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:27.598823: step 6657, loss 0.256478, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:27.791659: step 6658, loss 0.2231, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:27.975720: step 6659, loss 0.2129, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:28.158127: step 6660, loss 0.140845, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:28.343559: step 6661, loss 0.125331, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:28.541267: step 6662, loss 0.125244, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:28.727526: step 6663, loss 0.129319, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:28.885538: step 6664, loss 0.146376, acc 0.960784, learning_rate 0.0001
2017-09-29T11:19:29.069215: step 6665, loss 0.204311, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:29.254092: step 6666, loss 0.137781, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:29.451290: step 6667, loss 0.143104, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:29.638535: step 6668, loss 0.236889, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:29.825520: step 6669, loss 0.128247, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:30.018214: step 6670, loss 0.0920884, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:30.205335: step 6671, loss 0.22626, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:30.392074: step 6672, loss 0.148234, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:30.571862: step 6673, loss 0.157814, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:30.759952: step 6674, loss 0.27155, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:30.943549: step 6675, loss 0.07635, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:31.130510: step 6676, loss 0.113816, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:31.322685: step 6677, loss 0.219099, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:31.510678: step 6678, loss 0.143355, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:31.707021: step 6679, loss 0.114839, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:31.892606: step 6680, loss 0.166941, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:19:32.429720: step 6680, loss 0.276234, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6680

2017-09-29T11:19:33.157006: step 6681, loss 0.166412, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:33.342252: step 6682, loss 0.181499, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:33.539368: step 6683, loss 0.224294, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:33.724814: step 6684, loss 0.0578612, acc 1, learning_rate 0.0001
2017-09-29T11:19:33.909164: step 6685, loss 0.12619, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:34.095000: step 6686, loss 0.140097, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:34.285641: step 6687, loss 0.184046, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:34.503034: step 6688, loss 0.0805144, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:34.696483: step 6689, loss 0.24604, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:34.878290: step 6690, loss 0.174745, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:35.071142: step 6691, loss 0.21084, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:35.254136: step 6692, loss 0.183769, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:35.444205: step 6693, loss 0.203339, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:35.630058: step 6694, loss 0.151784, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:35.814958: step 6695, loss 0.126021, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:35.999753: step 6696, loss 0.115021, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:36.184207: step 6697, loss 0.171451, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:36.369500: step 6698, loss 0.165204, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:36.562349: step 6699, loss 0.13753, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:36.747375: step 6700, loss 0.16151, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:36.929980: step 6701, loss 0.21315, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:37.114243: step 6702, loss 0.123231, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:37.294735: step 6703, loss 0.200206, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:37.478797: step 6704, loss 0.107595, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:37.664278: step 6705, loss 0.176508, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:37.848105: step 6706, loss 0.169269, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:38.031747: step 6707, loss 0.174028, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:38.218784: step 6708, loss 0.12518, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:38.402424: step 6709, loss 0.159267, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:38.593856: step 6710, loss 0.207099, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:38.784053: step 6711, loss 0.091045, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:38.965546: step 6712, loss 0.128509, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:39.144346: step 6713, loss 0.132931, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:39.335825: step 6714, loss 0.14711, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:39.523133: step 6715, loss 0.123414, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:39.722894: step 6716, loss 0.161159, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:39.905503: step 6717, loss 0.188536, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:40.094361: step 6718, loss 0.147511, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:40.277027: step 6719, loss 0.250375, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:40.467660: step 6720, loss 0.167247, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:19:41.017861: step 6720, loss 0.272918, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6720

2017-09-29T11:19:41.724988: step 6721, loss 0.0500227, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:41.908847: step 6722, loss 0.142064, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:42.090470: step 6723, loss 0.224869, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:42.272547: step 6724, loss 0.145764, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:42.457135: step 6725, loss 0.246667, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:42.641450: step 6726, loss 0.21679, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:42.825516: step 6727, loss 0.181113, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:43.013999: step 6728, loss 0.182468, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:43.194437: step 6729, loss 0.179063, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:43.375597: step 6730, loss 0.104435, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:43.557610: step 6731, loss 0.214481, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:43.737138: step 6732, loss 0.194103, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:43.936444: step 6733, loss 0.172402, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:44.126835: step 6734, loss 0.116579, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:44.312688: step 6735, loss 0.0992207, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:44.510324: step 6736, loss 0.178507, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:44.705603: step 6737, loss 0.125922, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:44.890701: step 6738, loss 0.236395, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:45.080626: step 6739, loss 0.18174, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:45.261674: step 6740, loss 0.217126, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:45.449936: step 6741, loss 0.152115, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:45.645874: step 6742, loss 0.135441, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:45.831360: step 6743, loss 0.256123, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:46.015209: step 6744, loss 0.190276, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:46.222034: step 6745, loss 0.185178, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:46.410238: step 6746, loss 0.104224, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:46.598306: step 6747, loss 0.146922, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:46.782509: step 6748, loss 0.0724006, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:46.968715: step 6749, loss 0.0904409, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:47.155221: step 6750, loss 0.2396, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:47.351518: step 6751, loss 0.14046, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:47.538513: step 6752, loss 0.189066, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:47.726777: step 6753, loss 0.388976, acc 0.859375, learning_rate 0.0001
2017-09-29T11:19:47.907931: step 6754, loss 0.183241, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:48.093735: step 6755, loss 0.105445, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:48.275011: step 6756, loss 0.233817, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:48.460241: step 6757, loss 0.0840285, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:48.651428: step 6758, loss 0.314144, acc 0.890625, learning_rate 0.0001
2017-09-29T11:19:48.835706: step 6759, loss 0.144529, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:49.022681: step 6760, loss 0.0955531, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T11:19:49.566662: step 6760, loss 0.271098, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6760

2017-09-29T11:19:50.369916: step 6761, loss 0.140115, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:50.531010: step 6762, loss 0.154469, acc 0.941176, learning_rate 0.0001
2017-09-29T11:19:50.715120: step 6763, loss 0.159844, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:50.903908: step 6764, loss 0.257147, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:51.087680: step 6765, loss 0.176764, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:51.271288: step 6766, loss 0.124133, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:51.460975: step 6767, loss 0.119011, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:51.653042: step 6768, loss 0.108879, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:51.837278: step 6769, loss 0.110831, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:52.023596: step 6770, loss 0.205055, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:52.206963: step 6771, loss 0.175722, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:52.387469: step 6772, loss 0.155338, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:52.577913: step 6773, loss 0.215517, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:52.762054: step 6774, loss 0.122531, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:52.949946: step 6775, loss 0.205465, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:53.132772: step 6776, loss 0.19123, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:53.322129: step 6777, loss 0.287266, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:53.513775: step 6778, loss 0.189618, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:53.699064: step 6779, loss 0.153354, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:53.889592: step 6780, loss 0.124663, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:54.077635: step 6781, loss 0.205826, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:54.260043: step 6782, loss 0.252105, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:54.446515: step 6783, loss 0.185292, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:54.631516: step 6784, loss 0.244399, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:54.823728: step 6785, loss 0.210991, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:55.007508: step 6786, loss 0.036071, acc 1, learning_rate 0.0001
2017-09-29T11:19:55.203905: step 6787, loss 0.160732, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:55.389528: step 6788, loss 0.16324, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:55.571775: step 6789, loss 0.211582, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:55.757715: step 6790, loss 0.0685297, acc 1, learning_rate 0.0001
2017-09-29T11:19:55.948433: step 6791, loss 0.106143, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:56.139187: step 6792, loss 0.0839751, acc 0.984375, learning_rate 0.0001
2017-09-29T11:19:56.321228: step 6793, loss 0.194128, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:56.518818: step 6794, loss 0.182526, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:56.712586: step 6795, loss 0.188671, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:56.894876: step 6796, loss 0.228508, acc 0.90625, learning_rate 0.0001
2017-09-29T11:19:57.079521: step 6797, loss 0.144791, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:57.262548: step 6798, loss 0.160772, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:57.450809: step 6799, loss 0.106348, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:57.645667: step 6800, loss 0.190142, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:19:58.263758: step 6800, loss 0.273338, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6800

2017-09-29T11:19:58.907898: step 6801, loss 0.122468, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:59.096567: step 6802, loss 0.165035, acc 0.953125, learning_rate 0.0001
2017-09-29T11:19:59.279635: step 6803, loss 0.233387, acc 0.921875, learning_rate 0.0001
2017-09-29T11:19:59.469020: step 6804, loss 0.165765, acc 0.96875, learning_rate 0.0001
2017-09-29T11:19:59.653567: step 6805, loss 0.192347, acc 0.9375, learning_rate 0.0001
2017-09-29T11:19:59.835516: step 6806, loss 0.156301, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:00.020376: step 6807, loss 0.100138, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:00.213449: step 6808, loss 0.12566, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:00.399040: step 6809, loss 0.196104, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:00.581164: step 6810, loss 0.0828237, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:00.767109: step 6811, loss 0.203794, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:00.957591: step 6812, loss 0.254989, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:01.139444: step 6813, loss 0.233338, acc 0.875, learning_rate 0.0001
2017-09-29T11:20:01.319037: step 6814, loss 0.221397, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:01.513321: step 6815, loss 0.124729, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:01.695787: step 6816, loss 0.11439, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:01.881884: step 6817, loss 0.154818, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:02.071126: step 6818, loss 0.114676, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:02.257282: step 6819, loss 0.174671, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:02.441618: step 6820, loss 0.272235, acc 0.859375, learning_rate 0.0001
2017-09-29T11:20:02.633697: step 6821, loss 0.162724, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:02.818213: step 6822, loss 0.129652, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:03.001147: step 6823, loss 0.132567, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:03.184734: step 6824, loss 0.0709698, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:03.370541: step 6825, loss 0.123002, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:03.570021: step 6826, loss 0.198506, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:03.757082: step 6827, loss 0.0653453, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:03.950044: step 6828, loss 0.10662, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:04.132868: step 6829, loss 0.176006, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:04.319825: step 6830, loss 0.204816, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:04.506376: step 6831, loss 0.168646, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:04.688944: step 6832, loss 0.143777, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:04.884760: step 6833, loss 0.173131, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:05.078703: step 6834, loss 0.141867, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:05.273253: step 6835, loss 0.172216, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:05.460353: step 6836, loss 0.119442, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:05.641397: step 6837, loss 0.198804, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:05.833041: step 6838, loss 0.0632834, acc 1, learning_rate 0.0001
2017-09-29T11:20:06.030704: step 6839, loss 0.163356, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:06.222107: step 6840, loss 0.153247, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:20:06.785406: step 6840, loss 0.27135, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6840

2017-09-29T11:20:07.523448: step 6841, loss 0.119867, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:07.729940: step 6842, loss 0.135885, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:07.928825: step 6843, loss 0.104381, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:08.113643: step 6844, loss 0.140062, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:08.295558: step 6845, loss 0.124541, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:08.481644: step 6846, loss 0.201346, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:08.674558: step 6847, loss 0.0916434, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:08.858942: step 6848, loss 0.189117, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:09.053831: step 6849, loss 0.140937, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:09.238747: step 6850, loss 0.175617, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:09.432565: step 6851, loss 0.167298, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:09.619709: step 6852, loss 0.165739, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:09.828267: step 6853, loss 0.216045, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:10.013040: step 6854, loss 0.211757, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:10.199468: step 6855, loss 0.121485, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:10.392401: step 6856, loss 0.173075, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:10.574236: step 6857, loss 0.2908, acc 0.875, learning_rate 0.0001
2017-09-29T11:20:10.760408: step 6858, loss 0.216142, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:10.944483: step 6859, loss 0.0980427, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:11.100543: step 6860, loss 0.143858, acc 0.941176, learning_rate 0.0001
2017-09-29T11:20:11.285159: step 6861, loss 0.172555, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:11.468888: step 6862, loss 0.0880387, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:11.653206: step 6863, loss 0.272006, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:11.835521: step 6864, loss 0.203103, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:12.017667: step 6865, loss 0.196287, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:12.198829: step 6866, loss 0.30038, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:12.381210: step 6867, loss 0.122466, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:12.562835: step 6868, loss 0.0760577, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:12.747017: step 6869, loss 0.0798851, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:12.940180: step 6870, loss 0.128729, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:13.122929: step 6871, loss 0.113848, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:13.303608: step 6872, loss 0.159292, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:13.491849: step 6873, loss 0.143807, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:13.675671: step 6874, loss 0.284696, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:13.858448: step 6875, loss 0.125278, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:14.044749: step 6876, loss 0.143431, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:14.227354: step 6877, loss 0.250177, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:14.436614: step 6878, loss 0.129758, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:14.618932: step 6879, loss 0.097106, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:14.803859: step 6880, loss 0.0826499, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T11:20:15.373075: step 6880, loss 0.26734, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6880

2017-09-29T11:20:16.143106: step 6881, loss 0.114146, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:16.329984: step 6882, loss 0.116346, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:16.512690: step 6883, loss 0.119136, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:16.702203: step 6884, loss 0.259668, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:16.888803: step 6885, loss 0.132532, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:17.071174: step 6886, loss 0.187027, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:17.253242: step 6887, loss 0.122644, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:17.441615: step 6888, loss 0.21418, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:17.627900: step 6889, loss 0.253618, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:17.813452: step 6890, loss 0.215417, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:18.001446: step 6891, loss 0.221802, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:18.187750: step 6892, loss 0.199246, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:18.375305: step 6893, loss 0.131821, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:18.561249: step 6894, loss 0.0923067, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:18.745482: step 6895, loss 0.197861, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:18.935537: step 6896, loss 0.182297, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:19.121393: step 6897, loss 0.0842403, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:19.305278: step 6898, loss 0.0607733, acc 1, learning_rate 0.0001
2017-09-29T11:20:19.501693: step 6899, loss 0.0774852, acc 1, learning_rate 0.0001
2017-09-29T11:20:19.683054: step 6900, loss 0.145174, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:19.865842: step 6901, loss 0.2278, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:20.051997: step 6902, loss 0.179916, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:20.238004: step 6903, loss 0.0943514, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:20.440737: step 6904, loss 0.279835, acc 0.875, learning_rate 0.0001
2017-09-29T11:20:20.630437: step 6905, loss 0.0877167, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:20.820127: step 6906, loss 0.175078, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:21.007348: step 6907, loss 0.107978, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:21.198240: step 6908, loss 0.176614, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:21.384610: step 6909, loss 0.283046, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:21.567649: step 6910, loss 0.135695, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:21.754642: step 6911, loss 0.0840609, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:21.936498: step 6912, loss 0.193791, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:22.122494: step 6913, loss 0.214263, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:22.308427: step 6914, loss 0.153331, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:22.495454: step 6915, loss 0.165672, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:22.679337: step 6916, loss 0.182011, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:22.865389: step 6917, loss 0.111584, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:23.053339: step 6918, loss 0.150763, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:23.240035: step 6919, loss 0.144273, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:23.428021: step 6920, loss 0.128676, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:20:23.977611: step 6920, loss 0.270421, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6920

2017-09-29T11:20:24.771411: step 6921, loss 0.189705, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:24.968780: step 6922, loss 0.0886195, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:25.180362: step 6923, loss 0.153392, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:25.396766: step 6924, loss 0.153491, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:25.587599: step 6925, loss 0.14781, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:25.781236: step 6926, loss 0.124669, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:25.983666: step 6927, loss 0.2531, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:26.174216: step 6928, loss 0.249733, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:26.373027: step 6929, loss 0.20735, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:26.558207: step 6930, loss 0.248157, acc 0.875, learning_rate 0.0001
2017-09-29T11:20:26.754802: step 6931, loss 0.105624, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:26.948567: step 6932, loss 0.0755271, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:27.126889: step 6933, loss 0.220355, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:27.312036: step 6934, loss 0.191038, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:27.499970: step 6935, loss 0.183334, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:27.695239: step 6936, loss 0.237771, acc 0.859375, learning_rate 0.0001
2017-09-29T11:20:27.878044: step 6937, loss 0.168113, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:28.069613: step 6938, loss 0.138306, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:28.252204: step 6939, loss 0.200069, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:28.438834: step 6940, loss 0.168476, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:28.627685: step 6941, loss 0.163929, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:28.817592: step 6942, loss 0.23562, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:29.007159: step 6943, loss 0.11881, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:29.194210: step 6944, loss 0.237854, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:29.379398: step 6945, loss 0.126021, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:29.565955: step 6946, loss 0.167587, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:29.753501: step 6947, loss 0.126815, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:29.936587: step 6948, loss 0.263317, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:30.124197: step 6949, loss 0.239145, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:30.310718: step 6950, loss 0.169992, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:30.503024: step 6951, loss 0.203186, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:30.688263: step 6952, loss 0.133246, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:30.869715: step 6953, loss 0.199263, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:31.056480: step 6954, loss 0.163323, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:31.246396: step 6955, loss 0.135037, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:31.444587: step 6956, loss 0.243921, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:31.631733: step 6957, loss 0.050243, acc 1, learning_rate 0.0001
2017-09-29T11:20:31.785962: step 6958, loss 0.0833557, acc 1, learning_rate 0.0001
2017-09-29T11:20:31.971797: step 6959, loss 0.172374, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:32.155057: step 6960, loss 0.131003, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:20:32.704671: step 6960, loss 0.264163, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-6960

2017-09-29T11:20:33.339853: step 6961, loss 0.22114, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:33.533234: step 6962, loss 0.170454, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:33.716360: step 6963, loss 0.138432, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:33.907019: step 6964, loss 0.114346, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:34.092849: step 6965, loss 0.166296, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:34.275076: step 6966, loss 0.120018, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:34.460172: step 6967, loss 0.0798717, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:34.647508: step 6968, loss 0.183374, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:34.841741: step 6969, loss 0.234176, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:35.029973: step 6970, loss 0.10935, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:35.216079: step 6971, loss 0.129763, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:35.406605: step 6972, loss 0.143929, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:35.599713: step 6973, loss 0.1405, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:35.805273: step 6974, loss 0.121719, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:36.010959: step 6975, loss 0.165198, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:36.228211: step 6976, loss 0.16892, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:36.414285: step 6977, loss 0.177164, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:36.593813: step 6978, loss 0.207997, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:36.777816: step 6979, loss 0.118915, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:36.961148: step 6980, loss 0.213152, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:37.145128: step 6981, loss 0.155357, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:37.329034: step 6982, loss 0.194684, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:37.524060: step 6983, loss 0.13482, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:37.706675: step 6984, loss 0.258409, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:37.890453: step 6985, loss 0.0599092, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:38.085143: step 6986, loss 0.261212, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:38.283735: step 6987, loss 0.106993, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:38.469322: step 6988, loss 0.081649, acc 1, learning_rate 0.0001
2017-09-29T11:20:38.663908: step 6989, loss 0.120734, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:38.853115: step 6990, loss 0.119003, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:39.046453: step 6991, loss 0.168577, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:39.229787: step 6992, loss 0.070421, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:39.416460: step 6993, loss 0.166045, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:39.604931: step 6994, loss 0.256628, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:39.787067: step 6995, loss 0.221908, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:39.987760: step 6996, loss 0.140805, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:40.194961: step 6997, loss 0.25544, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:40.404021: step 6998, loss 0.0819128, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:40.588239: step 6999, loss 0.198687, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:40.773422: step 7000, loss 0.082609, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:20:41.331300: step 7000, loss 0.272032, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7000

2017-09-29T11:20:42.044766: step 7001, loss 0.118977, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:42.228795: step 7002, loss 0.140357, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:42.415615: step 7003, loss 0.179621, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:42.602015: step 7004, loss 0.135124, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:42.785897: step 7005, loss 0.0993447, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:42.972474: step 7006, loss 0.150018, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:43.159382: step 7007, loss 0.0849168, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:43.342677: step 7008, loss 0.119082, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:43.546604: step 7009, loss 0.133831, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:43.736239: step 7010, loss 0.125464, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:43.924064: step 7011, loss 0.208234, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:44.129295: step 7012, loss 0.14745, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:44.315144: step 7013, loss 0.0963448, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:44.502514: step 7014, loss 0.175125, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:44.687258: step 7015, loss 0.237289, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:44.872876: step 7016, loss 0.151984, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:45.059112: step 7017, loss 0.137889, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:45.242112: step 7018, loss 0.189858, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:45.430610: step 7019, loss 0.216898, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:45.614940: step 7020, loss 0.155698, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:45.799741: step 7021, loss 0.108166, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:45.985859: step 7022, loss 0.116349, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:46.173013: step 7023, loss 0.30776, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:46.357753: step 7024, loss 0.252577, acc 0.875, learning_rate 0.0001
2017-09-29T11:20:46.545262: step 7025, loss 0.192905, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:46.730229: step 7026, loss 0.179765, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:46.923265: step 7027, loss 0.269784, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:47.107683: step 7028, loss 0.174302, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:47.293336: step 7029, loss 0.171239, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:47.483855: step 7030, loss 0.244066, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:47.670455: step 7031, loss 0.159262, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:47.859152: step 7032, loss 0.147363, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:48.046288: step 7033, loss 0.149624, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:48.233514: step 7034, loss 0.190405, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:48.425608: step 7035, loss 0.155865, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:48.618833: step 7036, loss 0.31716, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:48.809556: step 7037, loss 0.142657, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:48.998110: step 7038, loss 0.31009, acc 0.875, learning_rate 0.0001
2017-09-29T11:20:49.185460: step 7039, loss 0.196488, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:49.369077: step 7040, loss 0.159003, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-09-29T11:20:49.949330: step 7040, loss 0.273741, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7040

2017-09-29T11:20:50.683487: step 7041, loss 0.189523, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:50.879338: step 7042, loss 0.0849385, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:51.060377: step 7043, loss 0.283549, acc 0.875, learning_rate 0.0001
2017-09-29T11:20:51.243960: step 7044, loss 0.124901, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:51.435449: step 7045, loss 0.127623, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:51.616184: step 7046, loss 0.179169, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:51.800257: step 7047, loss 0.0790501, acc 1, learning_rate 0.0001
2017-09-29T11:20:51.985517: step 7048, loss 0.150753, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:52.169447: step 7049, loss 0.157145, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:52.353029: step 7050, loss 0.177676, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:52.541322: step 7051, loss 0.119823, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:52.734274: step 7052, loss 0.220639, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:52.930545: step 7053, loss 0.217332, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:53.128514: step 7054, loss 0.114754, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:53.312461: step 7055, loss 0.152736, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:53.464343: step 7056, loss 0.124045, acc 0.980392, learning_rate 0.0001
2017-09-29T11:20:53.652265: step 7057, loss 0.0973608, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:53.836146: step 7058, loss 0.13459, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:54.020125: step 7059, loss 0.106913, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:54.204843: step 7060, loss 0.204835, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:54.390122: step 7061, loss 0.135291, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:54.571782: step 7062, loss 0.148549, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:54.757408: step 7063, loss 0.128113, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:54.948308: step 7064, loss 0.182069, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:55.131162: step 7065, loss 0.275457, acc 0.890625, learning_rate 0.0001
2017-09-29T11:20:55.312424: step 7066, loss 0.156234, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:55.516742: step 7067, loss 0.156843, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:55.717941: step 7068, loss 0.131191, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:55.898604: step 7069, loss 0.142844, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:56.082583: step 7070, loss 0.118791, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:56.266304: step 7071, loss 0.153308, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:56.454793: step 7072, loss 0.0973995, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:56.647853: step 7073, loss 0.218463, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:56.847484: step 7074, loss 0.178257, acc 0.9375, learning_rate 0.0001
2017-09-29T11:20:57.029354: step 7075, loss 0.222704, acc 0.90625, learning_rate 0.0001
2017-09-29T11:20:57.214090: step 7076, loss 0.118857, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:57.406100: step 7077, loss 0.195268, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:57.589600: step 7078, loss 0.392676, acc 0.875, learning_rate 0.0001
2017-09-29T11:20:57.775500: step 7079, loss 0.115933, acc 0.96875, learning_rate 0.0001
2017-09-29T11:20:57.958977: step 7080, loss 0.177577, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:20:58.502569: step 7080, loss 0.267122, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7080

2017-09-29T11:20:59.299951: step 7081, loss 0.0614586, acc 0.984375, learning_rate 0.0001
2017-09-29T11:20:59.488304: step 7082, loss 0.127856, acc 0.953125, learning_rate 0.0001
2017-09-29T11:20:59.673516: step 7083, loss 0.193263, acc 0.921875, learning_rate 0.0001
2017-09-29T11:20:59.855894: step 7084, loss 0.168999, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:00.041793: step 7085, loss 0.0764336, acc 1, learning_rate 0.0001
2017-09-29T11:21:00.228002: step 7086, loss 0.176401, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:00.417978: step 7087, loss 0.247232, acc 0.875, learning_rate 0.0001
2017-09-29T11:21:00.609902: step 7088, loss 0.15983, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:00.801537: step 7089, loss 0.155931, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:00.985801: step 7090, loss 0.178468, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:01.169375: step 7091, loss 0.0975818, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:01.352539: step 7092, loss 0.182293, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:01.566036: step 7093, loss 0.0778318, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:01.772581: step 7094, loss 0.213816, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:01.979247: step 7095, loss 0.134558, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:02.183169: step 7096, loss 0.0917604, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:02.366085: step 7097, loss 0.136682, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:02.548881: step 7098, loss 0.172349, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:02.736061: step 7099, loss 0.117262, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:02.932835: step 7100, loss 0.0947433, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:03.117495: step 7101, loss 0.124972, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:03.300796: step 7102, loss 0.142539, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:03.483795: step 7103, loss 0.138808, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:03.678395: step 7104, loss 0.18337, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:03.861816: step 7105, loss 0.18782, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:04.043502: step 7106, loss 0.158572, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:04.225446: step 7107, loss 0.280178, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:04.410133: step 7108, loss 0.230785, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:04.591492: step 7109, loss 0.127591, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:04.774699: step 7110, loss 0.158736, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:04.955030: step 7111, loss 0.138415, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:05.149251: step 7112, loss 0.203305, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:05.332126: step 7113, loss 0.134851, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:05.516381: step 7114, loss 0.205526, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:05.706428: step 7115, loss 0.157361, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:05.891640: step 7116, loss 0.224922, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:06.073767: step 7117, loss 0.117336, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:06.259397: step 7118, loss 0.186301, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:06.443130: step 7119, loss 0.202061, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:06.636563: step 7120, loss 0.150114, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:21:07.177874: step 7120, loss 0.264253, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7120

2017-09-29T11:21:07.830681: step 7121, loss 0.154839, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:08.013666: step 7122, loss 0.17308, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:08.198849: step 7123, loss 0.170037, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:08.382874: step 7124, loss 0.109407, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:08.569791: step 7125, loss 0.229016, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:08.754425: step 7126, loss 0.164625, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:08.938549: step 7127, loss 0.140856, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:09.122505: step 7128, loss 0.129226, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:09.305496: step 7129, loss 0.138864, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:09.493586: step 7130, loss 0.0963995, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:09.678666: step 7131, loss 0.149162, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:09.863954: step 7132, loss 0.217918, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:10.056328: step 7133, loss 0.170832, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:10.237266: step 7134, loss 0.139239, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:10.422072: step 7135, loss 0.199616, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:10.615301: step 7136, loss 0.0545908, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:10.794204: step 7137, loss 0.190497, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:10.975632: step 7138, loss 0.115182, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:11.161098: step 7139, loss 0.1478, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:11.343652: step 7140, loss 0.0853996, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:11.534502: step 7141, loss 0.1685, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:11.719194: step 7142, loss 0.14685, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:11.903768: step 7143, loss 0.0736309, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:12.089075: step 7144, loss 0.127195, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:12.275355: step 7145, loss 0.0747529, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:12.471325: step 7146, loss 0.171957, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:12.655388: step 7147, loss 0.127778, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:12.837121: step 7148, loss 0.123944, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:13.024795: step 7149, loss 0.121658, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:13.207250: step 7150, loss 0.174564, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:13.402575: step 7151, loss 0.276896, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:13.586226: step 7152, loss 0.124403, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:13.782157: step 7153, loss 0.0831769, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:13.938663: step 7154, loss 0.149424, acc 0.960784, learning_rate 0.0001
2017-09-29T11:21:14.132271: step 7155, loss 0.125213, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:14.318044: step 7156, loss 0.166746, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:14.504628: step 7157, loss 0.0784623, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:14.690542: step 7158, loss 0.246574, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:14.875618: step 7159, loss 0.186858, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:15.057300: step 7160, loss 0.152194, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:21:15.599503: step 7160, loss 0.26927, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7160

2017-09-29T11:21:16.320853: step 7161, loss 0.0646005, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:16.508638: step 7162, loss 0.243488, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:16.692541: step 7163, loss 0.14716, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:16.880317: step 7164, loss 0.170692, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:17.061781: step 7165, loss 0.129905, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:17.243275: step 7166, loss 0.165487, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:17.428653: step 7167, loss 0.187242, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:17.612372: step 7168, loss 0.138346, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:17.800096: step 7169, loss 0.246601, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:17.983086: step 7170, loss 0.0792246, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:18.186414: step 7171, loss 0.128187, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:18.394359: step 7172, loss 0.164881, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:18.583803: step 7173, loss 0.132719, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:18.769480: step 7174, loss 0.182088, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:18.950877: step 7175, loss 0.278384, acc 0.859375, learning_rate 0.0001
2017-09-29T11:21:19.144928: step 7176, loss 0.132465, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:19.334347: step 7177, loss 0.154397, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:19.520332: step 7178, loss 0.151816, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:19.713417: step 7179, loss 0.201568, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:19.896432: step 7180, loss 0.136197, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:20.077428: step 7181, loss 0.0820076, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:20.257509: step 7182, loss 0.22017, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:20.442324: step 7183, loss 0.0667807, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:20.631222: step 7184, loss 0.120765, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:20.810195: step 7185, loss 0.201863, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:20.991459: step 7186, loss 0.207125, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:21.175294: step 7187, loss 0.0882989, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:21.356755: step 7188, loss 0.179526, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:21.556626: step 7189, loss 0.166235, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:21.744747: step 7190, loss 0.141394, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:21.928672: step 7191, loss 0.147443, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:22.114466: step 7192, loss 0.221541, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:22.301043: step 7193, loss 0.130855, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:22.485218: step 7194, loss 0.146643, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:22.669958: step 7195, loss 0.171043, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:22.850876: step 7196, loss 0.136218, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:23.034286: step 7197, loss 0.113607, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:23.215109: step 7198, loss 0.0789541, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:23.401805: step 7199, loss 0.199066, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:23.587391: step 7200, loss 0.163157, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:21:24.092984: step 7200, loss 0.272699, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7200

2017-09-29T11:21:24.799261: step 7201, loss 0.361207, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:24.982308: step 7202, loss 0.231666, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:25.167258: step 7203, loss 0.279007, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:25.357414: step 7204, loss 0.134162, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:25.541098: step 7205, loss 0.123996, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:25.732694: step 7206, loss 0.167488, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:25.918347: step 7207, loss 0.193095, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:26.100197: step 7208, loss 0.14511, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:26.285731: step 7209, loss 0.147016, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:26.469761: step 7210, loss 0.123164, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:26.659666: step 7211, loss 0.111003, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:26.842739: step 7212, loss 0.159782, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:27.023759: step 7213, loss 0.133944, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:27.208218: step 7214, loss 0.243953, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:27.393738: step 7215, loss 0.0774655, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:27.578921: step 7216, loss 0.0998169, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:27.767425: step 7217, loss 0.108568, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:27.955488: step 7218, loss 0.129952, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:28.139109: step 7219, loss 0.0634711, acc 1, learning_rate 0.0001
2017-09-29T11:21:28.320293: step 7220, loss 0.172564, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:28.503567: step 7221, loss 0.0751003, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:28.686014: step 7222, loss 0.295705, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:28.870330: step 7223, loss 0.0874084, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:29.053489: step 7224, loss 0.192428, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:29.239582: step 7225, loss 0.186102, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:29.427005: step 7226, loss 0.313735, acc 0.875, learning_rate 0.0001
2017-09-29T11:21:29.614076: step 7227, loss 0.142614, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:29.811166: step 7228, loss 0.158044, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:29.998326: step 7229, loss 0.224055, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:30.184003: step 7230, loss 0.0781087, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:30.376130: step 7231, loss 0.0784438, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:30.571913: step 7232, loss 0.188231, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:30.763425: step 7233, loss 0.046627, acc 1, learning_rate 0.0001
2017-09-29T11:21:30.943583: step 7234, loss 0.217688, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:31.132845: step 7235, loss 0.142608, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:31.338050: step 7236, loss 0.204038, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:31.528352: step 7237, loss 0.0802096, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:31.715127: step 7238, loss 0.237575, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:31.902464: step 7239, loss 0.248638, acc 0.875, learning_rate 0.0001
2017-09-29T11:21:32.081931: step 7240, loss 0.171129, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:21:32.611975: step 7240, loss 0.266486, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7240

2017-09-29T11:21:33.440929: step 7241, loss 0.18858, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:33.623533: step 7242, loss 0.174328, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:33.806768: step 7243, loss 0.146725, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:34.007098: step 7244, loss 0.14869, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:34.207774: step 7245, loss 0.143617, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:34.393096: step 7246, loss 0.290993, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:34.578660: step 7247, loss 0.244595, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:34.760461: step 7248, loss 0.166315, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:34.942131: step 7249, loss 0.269582, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:35.125061: step 7250, loss 0.228267, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:35.324447: step 7251, loss 0.169408, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:35.483665: step 7252, loss 0.118614, acc 0.980392, learning_rate 0.0001
2017-09-29T11:21:35.670562: step 7253, loss 0.136692, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:35.861460: step 7254, loss 0.0678367, acc 1, learning_rate 0.0001
2017-09-29T11:21:36.050548: step 7255, loss 0.143468, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:36.234064: step 7256, loss 0.182492, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:36.418833: step 7257, loss 0.123233, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:36.627239: step 7258, loss 0.152916, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:36.834303: step 7259, loss 0.136045, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:37.037980: step 7260, loss 0.100971, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:37.226541: step 7261, loss 0.0927715, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:37.414112: step 7262, loss 0.205639, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:37.595617: step 7263, loss 0.048584, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:37.782084: step 7264, loss 0.103965, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:37.962344: step 7265, loss 0.149831, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:38.146214: step 7266, loss 0.22778, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:38.328495: step 7267, loss 0.274862, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:38.510715: step 7268, loss 0.187834, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:38.695169: step 7269, loss 0.113648, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:38.877281: step 7270, loss 0.226391, acc 0.875, learning_rate 0.0001
2017-09-29T11:21:39.061576: step 7271, loss 0.0615032, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:39.242631: step 7272, loss 0.106818, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:39.428319: step 7273, loss 0.126943, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:39.612187: step 7274, loss 0.158563, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:39.798766: step 7275, loss 0.0884757, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:39.979889: step 7276, loss 0.0933525, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:40.162256: step 7277, loss 0.21188, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:40.347870: step 7278, loss 0.262232, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:40.532520: step 7279, loss 0.163656, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:40.716754: step 7280, loss 0.18838, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:21:41.247566: step 7280, loss 0.267526, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7280

2017-09-29T11:21:41.888629: step 7281, loss 0.198193, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:42.083712: step 7282, loss 0.225409, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:42.271800: step 7283, loss 0.125551, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:42.455610: step 7284, loss 0.275786, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:42.643423: step 7285, loss 0.170206, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:42.826056: step 7286, loss 0.156949, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:43.006189: step 7287, loss 0.198353, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:43.192256: step 7288, loss 0.111066, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:43.374701: step 7289, loss 0.119477, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:43.560137: step 7290, loss 0.145613, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:43.749948: step 7291, loss 0.0940119, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:43.944509: step 7292, loss 0.143586, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:44.143104: step 7293, loss 0.228216, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:44.328967: step 7294, loss 0.226841, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:44.515152: step 7295, loss 0.261297, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:44.697525: step 7296, loss 0.163307, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:44.881540: step 7297, loss 0.172996, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:45.064971: step 7298, loss 0.152714, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:45.250109: step 7299, loss 0.129698, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:45.436443: step 7300, loss 0.150498, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:45.619977: step 7301, loss 0.1087, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:45.804520: step 7302, loss 0.105646, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:45.997695: step 7303, loss 0.187088, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:46.191002: step 7304, loss 0.163609, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:46.375453: step 7305, loss 0.18102, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:46.569199: step 7306, loss 0.151249, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:46.766107: step 7307, loss 0.112696, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:46.950447: step 7308, loss 0.170783, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:47.134439: step 7309, loss 0.105551, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:47.324867: step 7310, loss 0.149348, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:47.520656: step 7311, loss 0.113695, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:47.707596: step 7312, loss 0.208687, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:47.891144: step 7313, loss 0.0832436, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:48.092770: step 7314, loss 0.182708, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:48.276133: step 7315, loss 0.136519, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:48.461878: step 7316, loss 0.21029, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:48.657346: step 7317, loss 0.09956, acc 1, learning_rate 0.0001
2017-09-29T11:21:48.846761: step 7318, loss 0.168729, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:49.035099: step 7319, loss 0.104483, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:49.220628: step 7320, loss 0.149966, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:21:49.744766: step 7320, loss 0.265826, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7320

2017-09-29T11:21:50.459524: step 7321, loss 0.161639, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:50.647405: step 7322, loss 0.270062, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:50.843153: step 7323, loss 0.187792, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:51.030430: step 7324, loss 0.259695, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:51.216060: step 7325, loss 0.266899, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:51.401392: step 7326, loss 0.250765, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:51.585842: step 7327, loss 0.0744675, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:51.785205: step 7328, loss 0.2144, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:51.967876: step 7329, loss 0.134783, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:52.152760: step 7330, loss 0.177366, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:52.342710: step 7331, loss 0.168577, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:52.525943: step 7332, loss 0.180166, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:52.703673: step 7333, loss 0.158461, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:52.883655: step 7334, loss 0.0894963, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:53.078674: step 7335, loss 0.105581, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:53.260482: step 7336, loss 0.179593, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:53.462017: step 7337, loss 0.292902, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:53.645941: step 7338, loss 0.16785, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:53.839057: step 7339, loss 0.108603, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:54.023486: step 7340, loss 0.190853, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:54.211668: step 7341, loss 0.0756408, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:54.393838: step 7342, loss 0.270965, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:54.577151: step 7343, loss 0.11626, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:54.763714: step 7344, loss 0.102526, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:54.951329: step 7345, loss 0.188755, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:55.136815: step 7346, loss 0.162626, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:55.320910: step 7347, loss 0.216413, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:55.506422: step 7348, loss 0.169618, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:55.689560: step 7349, loss 0.135778, acc 0.921875, learning_rate 0.0001
2017-09-29T11:21:55.855169: step 7350, loss 0.0895414, acc 0.941176, learning_rate 0.0001
2017-09-29T11:21:56.043564: step 7351, loss 0.117413, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:56.231413: step 7352, loss 0.180929, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:56.411347: step 7353, loss 0.143884, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:56.596381: step 7354, loss 0.119381, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:56.783499: step 7355, loss 0.112866, acc 0.96875, learning_rate 0.0001
2017-09-29T11:21:56.969697: step 7356, loss 0.211167, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:57.150728: step 7357, loss 0.259821, acc 0.90625, learning_rate 0.0001
2017-09-29T11:21:57.332897: step 7358, loss 0.138601, acc 0.953125, learning_rate 0.0001
2017-09-29T11:21:57.518468: step 7359, loss 0.0809945, acc 0.984375, learning_rate 0.0001
2017-09-29T11:21:57.703975: step 7360, loss 0.115053, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:21:58.229677: step 7360, loss 0.261253, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7360

2017-09-29T11:21:58.938129: step 7361, loss 0.156216, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:59.122010: step 7362, loss 0.148742, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:59.307890: step 7363, loss 0.138258, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:59.503062: step 7364, loss 0.145985, acc 0.9375, learning_rate 0.0001
2017-09-29T11:21:59.691860: step 7365, loss 0.283591, acc 0.890625, learning_rate 0.0001
2017-09-29T11:21:59.875144: step 7366, loss 0.132768, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:00.066287: step 7367, loss 0.127569, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:00.259242: step 7368, loss 0.310675, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:00.451142: step 7369, loss 0.105257, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:00.641234: step 7370, loss 0.109737, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:00.825464: step 7371, loss 0.109248, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:01.012219: step 7372, loss 0.249603, acc 0.859375, learning_rate 0.0001
2017-09-29T11:22:01.196877: step 7373, loss 0.132355, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:01.379654: step 7374, loss 0.0882896, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:01.564904: step 7375, loss 0.139583, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:01.749260: step 7376, loss 0.16312, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:01.935831: step 7377, loss 0.129905, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:02.120677: step 7378, loss 0.0611881, acc 1, learning_rate 0.0001
2017-09-29T11:22:02.310075: step 7379, loss 0.142356, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:02.492538: step 7380, loss 0.20324, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:02.681138: step 7381, loss 0.129792, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:02.881126: step 7382, loss 0.139961, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:03.060936: step 7383, loss 0.200437, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:03.245232: step 7384, loss 0.200867, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:03.434248: step 7385, loss 0.222671, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:03.627854: step 7386, loss 0.131018, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:03.811983: step 7387, loss 0.071561, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:03.990639: step 7388, loss 0.101721, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:04.178267: step 7389, loss 0.181671, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:04.385333: step 7390, loss 0.204881, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:04.587623: step 7391, loss 0.0689315, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:04.771175: step 7392, loss 0.0672635, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:04.958977: step 7393, loss 0.205989, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:05.143420: step 7394, loss 0.148889, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:05.329147: step 7395, loss 0.140745, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:05.515943: step 7396, loss 0.252221, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:05.711556: step 7397, loss 0.162239, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:05.893808: step 7398, loss 0.130728, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:06.098174: step 7399, loss 0.0998786, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:06.295381: step 7400, loss 0.121578, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:22:06.830160: step 7400, loss 0.265914, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7400

2017-09-29T11:22:07.643786: step 7401, loss 0.128324, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:07.849354: step 7402, loss 0.125633, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:08.040295: step 7403, loss 0.095283, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:08.223555: step 7404, loss 0.0985719, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:08.407447: step 7405, loss 0.0412961, acc 1, learning_rate 0.0001
2017-09-29T11:22:08.587221: step 7406, loss 0.216889, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:08.770008: step 7407, loss 0.147, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:08.957840: step 7408, loss 0.119956, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:09.140341: step 7409, loss 0.219833, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:09.325356: step 7410, loss 0.173004, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:09.510798: step 7411, loss 0.123486, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:09.694878: step 7412, loss 0.218754, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:09.880273: step 7413, loss 0.078978, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:10.063210: step 7414, loss 0.282578, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:10.246706: step 7415, loss 0.0610795, acc 1, learning_rate 0.0001
2017-09-29T11:22:10.433486: step 7416, loss 0.195419, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:10.622764: step 7417, loss 0.218583, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:10.808173: step 7418, loss 0.141618, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:10.999154: step 7419, loss 0.21392, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:11.181236: step 7420, loss 0.126171, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:11.362607: step 7421, loss 0.0817557, acc 1, learning_rate 0.0001
2017-09-29T11:22:11.547427: step 7422, loss 0.212773, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:11.732065: step 7423, loss 0.281131, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:11.920103: step 7424, loss 0.106101, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:12.104944: step 7425, loss 0.141267, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:12.287196: step 7426, loss 0.176006, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:12.473159: step 7427, loss 0.160101, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:12.658651: step 7428, loss 0.117803, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:12.843374: step 7429, loss 0.136574, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:13.025094: step 7430, loss 0.157497, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:13.210245: step 7431, loss 0.119938, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:13.399325: step 7432, loss 0.104845, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:13.603428: step 7433, loss 0.225409, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:13.803148: step 7434, loss 0.104568, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:13.994320: step 7435, loss 0.163563, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:14.182744: step 7436, loss 0.149506, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:14.363671: step 7437, loss 0.0805165, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:14.546400: step 7438, loss 0.158177, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:14.730454: step 7439, loss 0.171983, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:14.912323: step 7440, loss 0.0921589, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:22:15.440561: step 7440, loss 0.269774, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7440

2017-09-29T11:22:16.094804: step 7441, loss 0.133472, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:16.278239: step 7442, loss 0.198446, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:16.466561: step 7443, loss 0.206055, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:16.652219: step 7444, loss 0.0917216, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:16.834118: step 7445, loss 0.254484, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:17.025275: step 7446, loss 0.176455, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:17.212792: step 7447, loss 0.0671102, acc 1, learning_rate 0.0001
2017-09-29T11:22:17.367268: step 7448, loss 0.222663, acc 0.941176, learning_rate 0.0001
2017-09-29T11:22:17.557057: step 7449, loss 0.208947, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:17.745311: step 7450, loss 0.254722, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:17.929375: step 7451, loss 0.0977291, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:18.123590: step 7452, loss 0.141506, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:18.320557: step 7453, loss 0.13506, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:18.506872: step 7454, loss 0.273078, acc 0.84375, learning_rate 0.0001
2017-09-29T11:22:18.690848: step 7455, loss 0.191018, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:18.874450: step 7456, loss 0.160754, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:19.057866: step 7457, loss 0.104747, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:19.240491: step 7458, loss 0.201699, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:19.452338: step 7459, loss 0.151431, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:19.654045: step 7460, loss 0.15351, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:19.871264: step 7461, loss 0.122854, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:20.068686: step 7462, loss 0.211664, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:20.253166: step 7463, loss 0.108486, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:20.440102: step 7464, loss 0.0826205, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:20.627441: step 7465, loss 0.190067, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:20.815761: step 7466, loss 0.130464, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:20.996928: step 7467, loss 0.152454, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:21.180429: step 7468, loss 0.159468, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:21.365588: step 7469, loss 0.124146, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:21.571465: step 7470, loss 0.204424, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:21.759128: step 7471, loss 0.128189, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:21.946873: step 7472, loss 0.227844, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:22.137032: step 7473, loss 0.158131, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:22.323033: step 7474, loss 0.27306, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:22.507275: step 7475, loss 0.151777, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:22.691049: step 7476, loss 0.12556, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:22.871864: step 7477, loss 0.236027, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:23.055977: step 7478, loss 0.191519, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:23.240377: step 7479, loss 0.160353, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:23.427089: step 7480, loss 0.192807, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:22:23.955465: step 7480, loss 0.264958, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7480

2017-09-29T11:22:24.672298: step 7481, loss 0.243503, acc 0.890625, learning_rate 0.0001
2017-09-29T11:22:24.857004: step 7482, loss 0.100719, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:25.039082: step 7483, loss 0.0803781, acc 1, learning_rate 0.0001
2017-09-29T11:22:25.232429: step 7484, loss 0.0727587, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:25.417428: step 7485, loss 0.132034, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:25.605522: step 7486, loss 0.173558, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:25.789664: step 7487, loss 0.112885, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:25.980456: step 7488, loss 0.130031, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:26.168961: step 7489, loss 0.160088, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:26.354298: step 7490, loss 0.132568, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:26.540435: step 7491, loss 0.0753642, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:26.736856: step 7492, loss 0.187103, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:26.940436: step 7493, loss 0.150164, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:27.149300: step 7494, loss 0.308987, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:27.343425: step 7495, loss 0.177876, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:27.540348: step 7496, loss 0.148357, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:27.726448: step 7497, loss 0.123091, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:27.905017: step 7498, loss 0.0783702, acc 1, learning_rate 0.0001
2017-09-29T11:22:28.094605: step 7499, loss 0.175372, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:28.286814: step 7500, loss 0.0873712, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:28.475935: step 7501, loss 0.106047, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:28.659093: step 7502, loss 0.235491, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:28.841934: step 7503, loss 0.113347, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:29.027923: step 7504, loss 0.09873, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:29.212315: step 7505, loss 0.125534, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:29.395069: step 7506, loss 0.186379, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:29.576948: step 7507, loss 0.0504536, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:29.761668: step 7508, loss 0.145484, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:29.951626: step 7509, loss 0.0997397, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:30.134442: step 7510, loss 0.132195, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:30.318553: step 7511, loss 0.191395, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:30.510895: step 7512, loss 0.0875171, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:30.695033: step 7513, loss 0.210029, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:30.882806: step 7514, loss 0.121781, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:31.077226: step 7515, loss 0.230957, acc 0.890625, learning_rate 0.0001
2017-09-29T11:22:31.259871: step 7516, loss 0.183426, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:31.444535: step 7517, loss 0.221301, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:31.625457: step 7518, loss 0.193741, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:31.810323: step 7519, loss 0.170976, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:31.994376: step 7520, loss 0.098064, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T11:22:32.527601: step 7520, loss 0.265349, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7520

2017-09-29T11:22:33.226913: step 7521, loss 0.107247, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:33.416522: step 7522, loss 0.0562177, acc 1, learning_rate 0.0001
2017-09-29T11:22:33.601383: step 7523, loss 0.139115, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:33.786756: step 7524, loss 0.15676, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:33.969914: step 7525, loss 0.113824, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:34.153629: step 7526, loss 0.152246, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:34.336906: step 7527, loss 0.179338, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:34.517769: step 7528, loss 0.21393, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:34.701875: step 7529, loss 0.064112, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:34.887896: step 7530, loss 0.323394, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:35.072012: step 7531, loss 0.198552, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:35.261912: step 7532, loss 0.240972, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:35.445927: step 7533, loss 0.2491, acc 0.875, learning_rate 0.0001
2017-09-29T11:22:35.628364: step 7534, loss 0.0837941, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:35.812534: step 7535, loss 0.150879, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:36.014147: step 7536, loss 0.128705, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:36.205083: step 7537, loss 0.123162, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:36.391023: step 7538, loss 0.128549, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:36.569207: step 7539, loss 0.105237, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:36.755472: step 7540, loss 0.0682439, acc 1, learning_rate 0.0001
2017-09-29T11:22:36.939392: step 7541, loss 0.156741, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:37.128068: step 7542, loss 0.113332, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:37.314223: step 7543, loss 0.225658, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:37.500227: step 7544, loss 0.0840796, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:37.686028: step 7545, loss 0.0940591, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:37.843379: step 7546, loss 0.217947, acc 0.941176, learning_rate 0.0001
2017-09-29T11:22:38.030897: step 7547, loss 0.0964668, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:38.219424: step 7548, loss 0.0776671, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:38.411181: step 7549, loss 0.0891193, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:38.596670: step 7550, loss 0.0954945, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:38.782629: step 7551, loss 0.121865, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:38.964833: step 7552, loss 0.102873, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:39.149646: step 7553, loss 0.125636, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:39.332888: step 7554, loss 0.182241, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:39.517143: step 7555, loss 0.133664, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:39.702741: step 7556, loss 0.150456, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:39.895517: step 7557, loss 0.158437, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:40.077484: step 7558, loss 0.16871, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:40.259248: step 7559, loss 0.184104, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:40.445219: step 7560, loss 0.202362, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-09-29T11:22:40.973437: step 7560, loss 0.264598, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7560

2017-09-29T11:22:41.763360: step 7561, loss 0.136145, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:41.946826: step 7562, loss 0.148488, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:42.134631: step 7563, loss 0.222038, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:42.315241: step 7564, loss 0.125178, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:42.500695: step 7565, loss 0.229757, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:42.685243: step 7566, loss 0.158784, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:42.874693: step 7567, loss 0.112178, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:43.076486: step 7568, loss 0.0958252, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:43.258016: step 7569, loss 0.219415, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:43.446425: step 7570, loss 0.11505, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:43.629655: step 7571, loss 0.204881, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:43.814148: step 7572, loss 0.143873, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:43.996329: step 7573, loss 0.11389, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:44.180489: step 7574, loss 0.129538, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:44.360952: step 7575, loss 0.134165, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:44.558324: step 7576, loss 0.10285, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:44.744877: step 7577, loss 0.17944, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:44.927745: step 7578, loss 0.170522, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:45.109735: step 7579, loss 0.136106, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:45.295738: step 7580, loss 0.289223, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:45.482744: step 7581, loss 0.220532, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:45.668048: step 7582, loss 0.108804, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:45.855035: step 7583, loss 0.157489, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:46.039533: step 7584, loss 0.157928, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:46.238354: step 7585, loss 0.107908, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:46.421812: step 7586, loss 0.128684, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:46.605695: step 7587, loss 0.165788, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:46.786298: step 7588, loss 0.249812, acc 0.890625, learning_rate 0.0001
2017-09-29T11:22:46.977529: step 7589, loss 0.197648, acc 0.90625, learning_rate 0.0001
2017-09-29T11:22:47.171979: step 7590, loss 0.118637, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:47.353879: step 7591, loss 0.10637, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:47.540452: step 7592, loss 0.152697, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:47.730510: step 7593, loss 0.112041, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:47.912348: step 7594, loss 0.0775699, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:48.095605: step 7595, loss 0.172567, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:48.283270: step 7596, loss 0.126096, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:48.471802: step 7597, loss 0.102943, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:48.660247: step 7598, loss 0.140783, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:48.844070: step 7599, loss 0.165727, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:49.036585: step 7600, loss 0.19849, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:22:49.586462: step 7600, loss 0.272202, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7600

2017-09-29T11:22:50.219903: step 7601, loss 0.107148, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:50.403464: step 7602, loss 0.0778255, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:50.587642: step 7603, loss 0.0921976, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:50.771807: step 7604, loss 0.215686, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:50.956513: step 7605, loss 0.185893, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:51.139498: step 7606, loss 0.115353, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:51.333419: step 7607, loss 0.173995, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:51.519410: step 7608, loss 0.272956, acc 0.875, learning_rate 0.0001
2017-09-29T11:22:51.704983: step 7609, loss 0.0705985, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:51.889645: step 7610, loss 0.13842, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:52.077565: step 7611, loss 0.110404, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:52.264498: step 7612, loss 0.13845, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:52.455116: step 7613, loss 0.0947805, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:52.640717: step 7614, loss 0.203976, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:52.824292: step 7615, loss 0.106064, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:53.009753: step 7616, loss 0.203316, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:53.193300: step 7617, loss 0.169433, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:53.377130: step 7618, loss 0.216225, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:53.562186: step 7619, loss 0.0842963, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:53.749914: step 7620, loss 0.129778, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:53.931586: step 7621, loss 0.108548, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:54.116612: step 7622, loss 0.22953, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:54.305909: step 7623, loss 0.100242, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:54.491095: step 7624, loss 0.217119, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:54.674132: step 7625, loss 0.247087, acc 0.890625, learning_rate 0.0001
2017-09-29T11:22:54.859229: step 7626, loss 0.126298, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:55.042411: step 7627, loss 0.126033, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:55.227691: step 7628, loss 0.123299, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:55.412645: step 7629, loss 0.0953948, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:55.597658: step 7630, loss 0.070259, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:55.785735: step 7631, loss 0.172002, acc 0.890625, learning_rate 0.0001
2017-09-29T11:22:55.970715: step 7632, loss 0.123017, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:56.157197: step 7633, loss 0.122682, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:56.350220: step 7634, loss 0.272752, acc 0.890625, learning_rate 0.0001
2017-09-29T11:22:56.537028: step 7635, loss 0.124947, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:56.722470: step 7636, loss 0.0897407, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:56.915725: step 7637, loss 0.17647, acc 0.921875, learning_rate 0.0001
2017-09-29T11:22:57.097009: step 7638, loss 0.146989, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:57.284347: step 7639, loss 0.103067, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:57.471026: step 7640, loss 0.146843, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:22:58.023138: step 7640, loss 0.25857, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7640

2017-09-29T11:22:58.732529: step 7641, loss 0.13421, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:58.917320: step 7642, loss 0.140105, acc 0.96875, learning_rate 0.0001
2017-09-29T11:22:59.100753: step 7643, loss 0.178615, acc 0.9375, learning_rate 0.0001
2017-09-29T11:22:59.267349: step 7644, loss 0.185835, acc 0.901961, learning_rate 0.0001
2017-09-29T11:22:59.451371: step 7645, loss 0.0725247, acc 0.984375, learning_rate 0.0001
2017-09-29T11:22:59.646007: step 7646, loss 0.170352, acc 0.953125, learning_rate 0.0001
2017-09-29T11:22:59.829614: step 7647, loss 0.211729, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:00.012054: step 7648, loss 0.188212, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:00.195978: step 7649, loss 0.0936254, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:00.380800: step 7650, loss 0.147868, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:00.574175: step 7651, loss 0.125078, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:00.755273: step 7652, loss 0.189255, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:00.938229: step 7653, loss 0.196695, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:01.132568: step 7654, loss 0.126046, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:01.324193: step 7655, loss 0.111983, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:01.518673: step 7656, loss 0.136109, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:01.713747: step 7657, loss 0.182236, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:01.895270: step 7658, loss 0.197416, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:02.074843: step 7659, loss 0.0673576, acc 1, learning_rate 0.0001
2017-09-29T11:23:02.263602: step 7660, loss 0.116127, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:02.447226: step 7661, loss 0.171845, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:02.630302: step 7662, loss 0.148572, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:02.812928: step 7663, loss 0.198713, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:03.004601: step 7664, loss 0.151379, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:03.194966: step 7665, loss 0.189123, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:03.378979: step 7666, loss 0.145227, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:03.565054: step 7667, loss 0.129813, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:03.749964: step 7668, loss 0.187592, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:03.932023: step 7669, loss 0.125792, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:04.114011: step 7670, loss 0.0681808, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:04.301019: step 7671, loss 0.183171, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:04.489767: step 7672, loss 0.182766, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:04.674378: step 7673, loss 0.137329, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:04.857422: step 7674, loss 0.195455, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:05.049075: step 7675, loss 0.28383, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:05.233827: step 7676, loss 0.212038, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:05.418865: step 7677, loss 0.280243, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:05.600294: step 7678, loss 0.111706, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:05.785235: step 7679, loss 0.0797633, acc 1, learning_rate 0.0001
2017-09-29T11:23:05.965168: step 7680, loss 0.162238, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-09-29T11:23:06.526292: step 7680, loss 0.262416, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7680

2017-09-29T11:23:07.234322: step 7681, loss 0.106238, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:07.422873: step 7682, loss 0.139836, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:07.604837: step 7683, loss 0.247399, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:07.790047: step 7684, loss 0.229866, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:07.974259: step 7685, loss 0.163506, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:08.157759: step 7686, loss 0.185348, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:08.339936: step 7687, loss 0.1117, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:08.540025: step 7688, loss 0.0992286, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:08.737627: step 7689, loss 0.169409, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:08.920103: step 7690, loss 0.136296, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:09.103864: step 7691, loss 0.0908044, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:09.301029: step 7692, loss 0.23503, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:09.496833: step 7693, loss 0.193341, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:09.682046: step 7694, loss 0.128181, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:09.868295: step 7695, loss 0.137039, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:10.051750: step 7696, loss 0.191192, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:10.237674: step 7697, loss 0.142141, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:10.424513: step 7698, loss 0.114509, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:10.611359: step 7699, loss 0.286927, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:10.793634: step 7700, loss 0.140046, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:10.978544: step 7701, loss 0.122319, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:11.168782: step 7702, loss 0.115933, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:11.363455: step 7703, loss 0.0930519, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:11.547653: step 7704, loss 0.0986569, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:11.732698: step 7705, loss 0.176086, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:11.927458: step 7706, loss 0.199853, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:12.108717: step 7707, loss 0.0856908, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:12.304604: step 7708, loss 0.070618, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:12.507988: step 7709, loss 0.108754, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:12.689822: step 7710, loss 0.0878934, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:12.872749: step 7711, loss 0.16487, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:13.053716: step 7712, loss 0.195821, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:13.236853: step 7713, loss 0.162723, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:13.423944: step 7714, loss 0.105065, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:13.608491: step 7715, loss 0.146485, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:13.801203: step 7716, loss 0.145727, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:13.985343: step 7717, loss 0.0962132, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:14.183820: step 7718, loss 0.0476689, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:14.372768: step 7719, loss 0.0960229, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:14.557805: step 7720, loss 0.0947201, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-09-29T11:23:15.101581: step 7720, loss 0.265836, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7720

2017-09-29T11:23:15.893736: step 7721, loss 0.188636, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:16.076450: step 7722, loss 0.0674206, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:16.262986: step 7723, loss 0.177573, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:16.456150: step 7724, loss 0.206933, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:16.652076: step 7725, loss 0.223362, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:16.837331: step 7726, loss 0.14736, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:17.025630: step 7727, loss 0.161305, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:17.208014: step 7728, loss 0.0786114, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:17.405655: step 7729, loss 0.112632, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:17.589229: step 7730, loss 0.210586, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:17.781621: step 7731, loss 0.0792162, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:17.966820: step 7732, loss 0.0694223, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:18.151270: step 7733, loss 0.108652, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:18.336524: step 7734, loss 0.174787, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:18.532727: step 7735, loss 0.144232, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:18.716999: step 7736, loss 0.162779, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:18.901679: step 7737, loss 0.138483, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:19.085154: step 7738, loss 0.0782927, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:19.269854: step 7739, loss 0.155493, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:19.455445: step 7740, loss 0.202346, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:19.641930: step 7741, loss 0.0886255, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:19.793024: step 7742, loss 0.22781, acc 0.921569, learning_rate 0.0001
2017-09-29T11:23:19.980181: step 7743, loss 0.108262, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:20.162698: step 7744, loss 0.0695552, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:20.345056: step 7745, loss 0.088862, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:20.528256: step 7746, loss 0.132523, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:20.722563: step 7747, loss 0.155876, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:20.905203: step 7748, loss 0.0697604, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:21.087401: step 7749, loss 0.169499, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:21.272007: step 7750, loss 0.189355, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:21.463440: step 7751, loss 0.0949628, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:21.645019: step 7752, loss 0.200742, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:21.838384: step 7753, loss 0.193847, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:22.020833: step 7754, loss 0.12523, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:22.205854: step 7755, loss 0.12837, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:22.401794: step 7756, loss 0.192135, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:22.593841: step 7757, loss 0.171747, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:22.783180: step 7758, loss 0.0933233, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:22.965090: step 7759, loss 0.130945, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:23.143110: step 7760, loss 0.176601, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:23:23.687171: step 7760, loss 0.261101, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7760

2017-09-29T11:23:24.317673: step 7761, loss 0.162479, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:24.515244: step 7762, loss 0.0919027, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:24.699881: step 7763, loss 0.342778, acc 0.890625, learning_rate 0.0001
2017-09-29T11:23:24.884312: step 7764, loss 0.251903, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:25.067293: step 7765, loss 0.123349, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:25.253209: step 7766, loss 0.0829303, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:25.445160: step 7767, loss 0.081279, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:25.625710: step 7768, loss 0.174276, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:25.810064: step 7769, loss 0.141024, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:25.992730: step 7770, loss 0.142326, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:26.174235: step 7771, loss 0.139736, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:26.359744: step 7772, loss 0.127021, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:26.549229: step 7773, loss 0.116671, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:26.731645: step 7774, loss 0.174665, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:26.910357: step 7775, loss 0.198445, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:27.096587: step 7776, loss 0.0751154, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:27.292855: step 7777, loss 0.182649, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:27.479991: step 7778, loss 0.0954714, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:27.664605: step 7779, loss 0.15474, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:27.851890: step 7780, loss 0.145168, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:28.036167: step 7781, loss 0.208061, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:28.238144: step 7782, loss 0.209374, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:28.423448: step 7783, loss 0.219481, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:28.609826: step 7784, loss 0.113276, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:28.799541: step 7785, loss 0.126516, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:28.994329: step 7786, loss 0.204515, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:29.180921: step 7787, loss 0.144895, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:29.375545: step 7788, loss 0.135759, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:29.562369: step 7789, loss 0.0655158, acc 1, learning_rate 0.0001
2017-09-29T11:23:29.746663: step 7790, loss 0.163423, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:29.932167: step 7791, loss 0.161563, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:30.115135: step 7792, loss 0.220807, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:30.300880: step 7793, loss 0.189037, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:30.495756: step 7794, loss 0.314483, acc 0.875, learning_rate 0.0001
2017-09-29T11:23:30.681014: step 7795, loss 0.0982497, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:30.868772: step 7796, loss 0.137645, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:31.054822: step 7797, loss 0.111237, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:31.240231: step 7798, loss 0.209719, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:31.427977: step 7799, loss 0.117311, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:31.626132: step 7800, loss 0.115973, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-09-29T11:23:32.240706: step 7800, loss 0.265044, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7800

2017-09-29T11:23:32.940140: step 7801, loss 0.0930346, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:33.144750: step 7802, loss 0.141637, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:33.350858: step 7803, loss 0.17893, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:33.548700: step 7804, loss 0.0339242, acc 1, learning_rate 0.0001
2017-09-29T11:23:33.733885: step 7805, loss 0.0893448, acc 1, learning_rate 0.0001
2017-09-29T11:23:33.914035: step 7806, loss 0.175592, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:34.100219: step 7807, loss 0.1298, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:34.283061: step 7808, loss 0.185766, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:34.477160: step 7809, loss 0.0919989, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:34.677466: step 7810, loss 0.203563, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:34.881703: step 7811, loss 0.236129, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:35.083210: step 7812, loss 0.163452, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:35.272284: step 7813, loss 0.113279, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:35.455354: step 7814, loss 0.235669, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:35.644852: step 7815, loss 0.125059, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:35.832494: step 7816, loss 0.119145, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:36.014799: step 7817, loss 0.106618, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:36.217107: step 7818, loss 0.119182, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:36.432252: step 7819, loss 0.155124, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:36.653739: step 7820, loss 0.160337, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:36.864036: step 7821, loss 0.0878786, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:37.069001: step 7822, loss 0.105759, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:37.266577: step 7823, loss 0.0510198, acc 1, learning_rate 0.0001
2017-09-29T11:23:37.455452: step 7824, loss 0.129536, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:37.646579: step 7825, loss 0.137484, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:37.831448: step 7826, loss 0.0906984, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:38.023643: step 7827, loss 0.236656, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:38.209110: step 7828, loss 0.102743, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:38.392397: step 7829, loss 0.135547, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:38.574184: step 7830, loss 0.1332, acc 0.953125, learning_rate 0.0001
2017-09-29T11:23:38.760851: step 7831, loss 0.198523, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:38.945682: step 7832, loss 0.196011, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:39.128890: step 7833, loss 0.125072, acc 0.984375, learning_rate 0.0001
2017-09-29T11:23:39.313433: step 7834, loss 0.171147, acc 0.921875, learning_rate 0.0001
2017-09-29T11:23:39.499344: step 7835, loss 0.143948, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:39.682063: step 7836, loss 0.13742, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:39.868341: step 7837, loss 0.191057, acc 0.9375, learning_rate 0.0001
2017-09-29T11:23:40.055751: step 7838, loss 0.235817, acc 0.90625, learning_rate 0.0001
2017-09-29T11:23:40.246201: step 7839, loss 0.109871, acc 0.96875, learning_rate 0.0001
2017-09-29T11:23:40.401444: step 7840, loss 0.139429, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-09-29T11:23:40.944482: step 7840, loss 0.264127, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1506700537/checkpoints/model-7840

