
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507664623

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T14:43:48.921112: step 1, loss 10.1694, acc 0.109375, learning_rate 0.005
2017-10-10T14:43:49.205182: step 2, loss 7.08942, acc 0.109375, learning_rate 0.00498
2017-10-10T14:43:49.522498: step 3, loss 4.9326, acc 0.359375, learning_rate 0.00496008
2017-10-10T14:43:49.849389: step 4, loss 5.47455, acc 0.453125, learning_rate 0.00494024
2017-10-10T14:43:50.166061: step 5, loss 6.84567, acc 0.28125, learning_rate 0.00492049
2017-10-10T14:43:50.418965: step 6, loss 5.09621, acc 0.328125, learning_rate 0.00490081
2017-10-10T14:43:50.719727: step 7, loss 4.33352, acc 0.421875, learning_rate 0.00488121
2017-10-10T14:43:51.021636: step 8, loss 4.66326, acc 0.40625, learning_rate 0.0048617
2017-10-10T14:43:51.384878: step 9, loss 4.45056, acc 0.46875, learning_rate 0.00484226
2017-10-10T14:43:51.609003: step 10, loss 4.78289, acc 0.34375, learning_rate 0.00482291
2017-10-10T14:43:51.916577: step 11, loss 3.55127, acc 0.46875, learning_rate 0.00480363
2017-10-10T14:43:52.252976: step 12, loss 4.55657, acc 0.3125, learning_rate 0.00478443
2017-10-10T14:43:52.542962: step 13, loss 4.40839, acc 0.359375, learning_rate 0.00476531
2017-10-10T14:43:52.824866: step 14, loss 3.99859, acc 0.453125, learning_rate 0.00474627
2017-10-10T14:43:53.166112: step 15, loss 3.39645, acc 0.46875, learning_rate 0.0047273
2017-10-10T14:43:53.439621: step 16, loss 3.15711, acc 0.453125, learning_rate 0.00470841
2017-10-10T14:43:53.722008: step 17, loss 2.45443, acc 0.53125, learning_rate 0.0046896
2017-10-10T14:43:54.018409: step 18, loss 3.37746, acc 0.515625, learning_rate 0.00467087
2017-10-10T14:43:54.351109: step 19, loss 2.46935, acc 0.59375, learning_rate 0.00465221
2017-10-10T14:43:54.652468: step 20, loss 2.43805, acc 0.578125, learning_rate 0.00463363
2017-10-10T14:43:54.936979: step 21, loss 2.92384, acc 0.484375, learning_rate 0.00461513
2017-10-10T14:43:55.231470: step 22, loss 2.46649, acc 0.59375, learning_rate 0.0045967
2017-10-10T14:43:55.596825: step 23, loss 2.08044, acc 0.65625, learning_rate 0.00457834
2017-10-10T14:43:55.892833: step 24, loss 2.62819, acc 0.515625, learning_rate 0.00456006
2017-10-10T14:43:56.140867: step 25, loss 2.67387, acc 0.546875, learning_rate 0.00454186
2017-10-10T14:43:56.434081: step 26, loss 2.2047, acc 0.625, learning_rate 0.00452373
2017-10-10T14:43:56.732980: step 27, loss 1.65357, acc 0.671875, learning_rate 0.00450567
2017-10-10T14:43:57.004972: step 28, loss 1.85468, acc 0.609375, learning_rate 0.00448769
2017-10-10T14:43:57.337955: step 29, loss 1.85621, acc 0.546875, learning_rate 0.00446978
2017-10-10T14:43:57.648987: step 30, loss 1.61601, acc 0.625, learning_rate 0.00445194
2017-10-10T14:43:57.986008: step 31, loss 1.35302, acc 0.65625, learning_rate 0.00443418
2017-10-10T14:43:58.289189: step 32, loss 1.84301, acc 0.625, learning_rate 0.00441649
2017-10-10T14:43:58.620913: step 33, loss 1.64663, acc 0.59375, learning_rate 0.00439887
2017-10-10T14:43:59.004076: step 34, loss 1.9424, acc 0.625, learning_rate 0.00438132
2017-10-10T14:43:59.315660: step 35, loss 1.16445, acc 0.6875, learning_rate 0.00436385
2017-10-10T14:43:59.584656: step 36, loss 1.09015, acc 0.734375, learning_rate 0.00434644
2017-10-10T14:43:59.848000: step 37, loss 1.28431, acc 0.71875, learning_rate 0.00432911
2017-10-10T14:44:00.134550: step 38, loss 1.42103, acc 0.671875, learning_rate 0.00431185
2017-10-10T14:44:00.483488: step 39, loss 0.726824, acc 0.796875, learning_rate 0.00429465
2017-10-10T14:44:00.665319: step 40, loss 1.04839, acc 0.75, learning_rate 0.00427753

Evaluation:
2017-10-10T14:44:00.975300: step 40, loss 0.404484, acc 0.840288

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-40

2017-10-10T14:44:01.897029: step 41, loss 1.49174, acc 0.703125, learning_rate 0.00426048
2017-10-10T14:44:02.232864: step 42, loss 0.984349, acc 0.796875, learning_rate 0.0042435
2017-10-10T14:44:02.501326: step 43, loss 0.93795, acc 0.703125, learning_rate 0.00422659
2017-10-10T14:44:02.729832: step 44, loss 1.16328, acc 0.71875, learning_rate 0.00420974
2017-10-10T14:44:03.043813: step 45, loss 1.36469, acc 0.71875, learning_rate 0.00419297
2017-10-10T14:44:03.314884: step 46, loss 2.4093, acc 0.578125, learning_rate 0.00417626
2017-10-10T14:44:03.608902: step 47, loss 1.24105, acc 0.75, learning_rate 0.00415962
2017-10-10T14:44:03.966516: step 48, loss 0.980831, acc 0.734375, learning_rate 0.00414305
2017-10-10T14:44:04.282666: step 49, loss 0.990296, acc 0.765625, learning_rate 0.00412655
2017-10-10T14:44:04.556760: step 50, loss 1.27366, acc 0.625, learning_rate 0.00411011
2017-10-10T14:44:04.925108: step 51, loss 0.761909, acc 0.796875, learning_rate 0.00409375
2017-10-10T14:44:05.233066: step 52, loss 1.13863, acc 0.71875, learning_rate 0.00407744
2017-10-10T14:44:05.490066: step 53, loss 0.772516, acc 0.765625, learning_rate 0.00406121
2017-10-10T14:44:05.811961: step 54, loss 0.785729, acc 0.765625, learning_rate 0.00404504
2017-10-10T14:44:06.140507: step 55, loss 1.021, acc 0.828125, learning_rate 0.00402894
2017-10-10T14:44:06.417050: step 56, loss 0.939838, acc 0.734375, learning_rate 0.0040129
2017-10-10T14:44:06.728390: step 57, loss 1.2373, acc 0.6875, learning_rate 0.00399693
2017-10-10T14:44:07.104075: step 58, loss 1.08998, acc 0.6875, learning_rate 0.00398102
2017-10-10T14:44:07.350207: step 59, loss 0.68961, acc 0.703125, learning_rate 0.00396518
2017-10-10T14:44:07.632189: step 60, loss 1.14819, acc 0.734375, learning_rate 0.00394941
2017-10-10T14:44:07.871717: step 61, loss 1.24025, acc 0.78125, learning_rate 0.00393369
2017-10-10T14:44:08.164829: step 62, loss 0.742405, acc 0.84375, learning_rate 0.00391804
2017-10-10T14:44:08.496953: step 63, loss 0.870583, acc 0.75, learning_rate 0.00390246
2017-10-10T14:44:08.824841: step 64, loss 1.00517, acc 0.796875, learning_rate 0.00388694
2017-10-10T14:44:09.184078: step 65, loss 0.624519, acc 0.828125, learning_rate 0.00387148
2017-10-10T14:44:09.459369: step 66, loss 1.01417, acc 0.796875, learning_rate 0.00385609
2017-10-10T14:44:09.733285: step 67, loss 0.64775, acc 0.8125, learning_rate 0.00384076
2017-10-10T14:44:10.047409: step 68, loss 0.806953, acc 0.71875, learning_rate 0.00382549
2017-10-10T14:44:10.313333: step 69, loss 0.728672, acc 0.796875, learning_rate 0.00381028
2017-10-10T14:44:10.580525: step 70, loss 0.845212, acc 0.796875, learning_rate 0.00379514
2017-10-10T14:44:10.911819: step 71, loss 0.689505, acc 0.75, learning_rate 0.00378005
2017-10-10T14:44:11.236988: step 72, loss 0.376786, acc 0.84375, learning_rate 0.00376503
2017-10-10T14:44:11.519377: step 73, loss 0.907588, acc 0.734375, learning_rate 0.00375007
2017-10-10T14:44:11.828724: step 74, loss 0.553492, acc 0.859375, learning_rate 0.00373517
2017-10-10T14:44:12.187045: step 75, loss 0.663589, acc 0.84375, learning_rate 0.00372034
2017-10-10T14:44:12.520847: step 76, loss 0.861323, acc 0.765625, learning_rate 0.00370556
2017-10-10T14:44:12.794527: step 77, loss 0.698471, acc 0.78125, learning_rate 0.00369084
2017-10-10T14:44:13.061087: step 78, loss 0.309971, acc 0.90625, learning_rate 0.00367619
2017-10-10T14:44:13.375420: step 79, loss 0.523884, acc 0.796875, learning_rate 0.00366159
2017-10-10T14:44:13.687820: step 80, loss 0.690684, acc 0.78125, learning_rate 0.00364705

Evaluation:
2017-10-10T14:44:14.262378: step 80, loss 0.363497, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-80

2017-10-10T14:44:15.281117: step 81, loss 0.896699, acc 0.765625, learning_rate 0.00363257
2017-10-10T14:44:15.642537: step 82, loss 1.1579, acc 0.71875, learning_rate 0.00361815
2017-10-10T14:44:15.933996: step 83, loss 0.763343, acc 0.78125, learning_rate 0.00360379
2017-10-10T14:44:16.217504: step 84, loss 0.837907, acc 0.78125, learning_rate 0.00358949
2017-10-10T14:44:16.520603: step 85, loss 0.718124, acc 0.71875, learning_rate 0.00357525
2017-10-10T14:44:16.908825: step 86, loss 0.581092, acc 0.8125, learning_rate 0.00356106
2017-10-10T14:44:17.283527: step 87, loss 0.806102, acc 0.75, learning_rate 0.00354694
2017-10-10T14:44:17.543864: step 88, loss 0.693875, acc 0.8125, learning_rate 0.00353287
2017-10-10T14:44:17.728817: step 89, loss 0.556118, acc 0.796875, learning_rate 0.00351885
2017-10-10T14:44:18.004013: step 90, loss 0.556796, acc 0.859375, learning_rate 0.0035049
2017-10-10T14:44:18.152006: step 91, loss 0.672016, acc 0.796875, learning_rate 0.003491
2017-10-10T14:44:18.327129: step 92, loss 0.385568, acc 0.859375, learning_rate 0.00347716
2017-10-10T14:44:18.559577: step 93, loss 0.541603, acc 0.828125, learning_rate 0.00346338
2017-10-10T14:44:18.814191: step 94, loss 0.437604, acc 0.828125, learning_rate 0.00344965
2017-10-10T14:44:19.157547: step 95, loss 0.769004, acc 0.75, learning_rate 0.00343597
2017-10-10T14:44:19.440941: step 96, loss 0.550884, acc 0.859375, learning_rate 0.00342236
2017-10-10T14:44:19.709120: step 97, loss 1.2326, acc 0.640625, learning_rate 0.0034088
2017-10-10T14:44:19.915373: step 98, loss 1.09085, acc 0.72549, learning_rate 0.00339529
2017-10-10T14:44:20.200870: step 99, loss 0.756572, acc 0.78125, learning_rate 0.00338184
2017-10-10T14:44:20.521731: step 100, loss 0.760697, acc 0.78125, learning_rate 0.00336844
2017-10-10T14:44:20.813673: step 101, loss 0.687578, acc 0.8125, learning_rate 0.0033551
2017-10-10T14:44:21.098767: step 102, loss 0.601495, acc 0.75, learning_rate 0.00334182
2017-10-10T14:44:21.376869: step 103, loss 0.399507, acc 0.875, learning_rate 0.00332858
2017-10-10T14:44:21.661903: step 104, loss 0.425537, acc 0.875, learning_rate 0.00331541
2017-10-10T14:44:21.900947: step 105, loss 0.643585, acc 0.78125, learning_rate 0.00330228
2017-10-10T14:44:22.188491: step 106, loss 0.482397, acc 0.8125, learning_rate 0.00328921
2017-10-10T14:44:22.458469: step 107, loss 0.60432, acc 0.84375, learning_rate 0.00327619
2017-10-10T14:44:22.738842: step 108, loss 0.603641, acc 0.765625, learning_rate 0.00326323
2017-10-10T14:44:23.036986: step 109, loss 0.487313, acc 0.859375, learning_rate 0.00325032
2017-10-10T14:44:23.342618: step 110, loss 0.621822, acc 0.78125, learning_rate 0.00323746
2017-10-10T14:44:23.690356: step 111, loss 0.66456, acc 0.828125, learning_rate 0.00322465
2017-10-10T14:44:24.060927: step 112, loss 0.489972, acc 0.84375, learning_rate 0.0032119
2017-10-10T14:44:24.329250: step 113, loss 0.472276, acc 0.859375, learning_rate 0.0031992
2017-10-10T14:44:24.652884: step 114, loss 0.715562, acc 0.8125, learning_rate 0.00318655
2017-10-10T14:44:24.914377: step 115, loss 0.541879, acc 0.8125, learning_rate 0.00317395
2017-10-10T14:44:25.224802: step 116, loss 1.14235, acc 0.703125, learning_rate 0.0031614
2017-10-10T14:44:25.530115: step 117, loss 0.476885, acc 0.890625, learning_rate 0.0031489
2017-10-10T14:44:25.797249: step 118, loss 0.637247, acc 0.8125, learning_rate 0.00313646
2017-10-10T14:44:26.061020: step 119, loss 0.530807, acc 0.890625, learning_rate 0.00312407
2017-10-10T14:44:26.386397: step 120, loss 0.619594, acc 0.796875, learning_rate 0.00311172

Evaluation:
2017-10-10T14:44:26.884969: step 120, loss 0.319685, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-120

2017-10-10T14:44:27.988954: step 121, loss 0.229362, acc 0.890625, learning_rate 0.00309943
2017-10-10T14:44:28.307931: step 122, loss 0.379652, acc 0.828125, learning_rate 0.00308719
2017-10-10T14:44:28.638267: step 123, loss 0.402575, acc 0.8125, learning_rate 0.00307499
2017-10-10T14:44:28.932957: step 124, loss 0.515484, acc 0.828125, learning_rate 0.00306285
2017-10-10T14:44:29.249756: step 125, loss 0.321458, acc 0.859375, learning_rate 0.00305076
2017-10-10T14:44:29.508135: step 126, loss 0.324887, acc 0.875, learning_rate 0.00303871
2017-10-10T14:44:29.896965: step 127, loss 0.467501, acc 0.859375, learning_rate 0.00302672
2017-10-10T14:44:30.174530: step 128, loss 0.420182, acc 0.78125, learning_rate 0.00301477
2017-10-10T14:44:30.458457: step 129, loss 0.475845, acc 0.84375, learning_rate 0.00300287
2017-10-10T14:44:30.753203: step 130, loss 0.468617, acc 0.84375, learning_rate 0.00299102
2017-10-10T14:44:31.051334: step 131, loss 0.346663, acc 0.875, learning_rate 0.00297922
2017-10-10T14:44:31.333049: step 132, loss 0.574363, acc 0.828125, learning_rate 0.00296747
2017-10-10T14:44:31.635435: step 133, loss 0.245087, acc 0.890625, learning_rate 0.00295577
2017-10-10T14:44:31.976931: step 134, loss 0.238622, acc 0.90625, learning_rate 0.00294411
2017-10-10T14:44:32.374627: step 135, loss 0.425215, acc 0.8125, learning_rate 0.0029325
2017-10-10T14:44:32.712515: step 136, loss 0.520403, acc 0.859375, learning_rate 0.00292094
2017-10-10T14:44:33.013333: step 137, loss 0.30679, acc 0.90625, learning_rate 0.00290943
2017-10-10T14:44:33.355427: step 138, loss 0.75256, acc 0.78125, learning_rate 0.00289796
2017-10-10T14:44:33.622625: step 139, loss 0.701363, acc 0.796875, learning_rate 0.00288654
2017-10-10T14:44:33.897989: step 140, loss 0.255716, acc 0.921875, learning_rate 0.00287516
2017-10-10T14:44:34.222084: step 141, loss 0.548216, acc 0.8125, learning_rate 0.00286384
2017-10-10T14:44:34.502663: step 142, loss 0.422432, acc 0.828125, learning_rate 0.00285256
2017-10-10T14:44:34.812108: step 143, loss 0.539314, acc 0.78125, learning_rate 0.00284132
2017-10-10T14:44:35.142332: step 144, loss 0.274401, acc 0.90625, learning_rate 0.00283013
2017-10-10T14:44:35.435147: step 145, loss 0.221315, acc 0.921875, learning_rate 0.00281899
2017-10-10T14:44:35.744049: step 146, loss 0.63035, acc 0.78125, learning_rate 0.00280789
2017-10-10T14:44:36.218439: step 147, loss 0.517867, acc 0.8125, learning_rate 0.00279684
2017-10-10T14:44:36.429299: step 148, loss 0.329274, acc 0.90625, learning_rate 0.00278583
2017-10-10T14:44:36.644835: step 149, loss 0.355332, acc 0.90625, learning_rate 0.00277486
2017-10-10T14:44:36.955111: step 150, loss 0.482356, acc 0.828125, learning_rate 0.00276395
2017-10-10T14:44:37.297185: step 151, loss 0.42401, acc 0.875, learning_rate 0.00275307
2017-10-10T14:44:37.576079: step 152, loss 0.493397, acc 0.875, learning_rate 0.00274224
2017-10-10T14:44:37.854696: step 153, loss 0.292186, acc 0.890625, learning_rate 0.00273146
2017-10-10T14:44:38.211028: step 154, loss 0.439681, acc 0.859375, learning_rate 0.00272072
2017-10-10T14:44:38.534952: step 155, loss 0.129363, acc 0.96875, learning_rate 0.00271002
2017-10-10T14:44:38.796984: step 156, loss 0.542391, acc 0.8125, learning_rate 0.00269937
2017-10-10T14:44:39.055667: step 157, loss 0.343597, acc 0.875, learning_rate 0.00268876
2017-10-10T14:44:39.411919: step 158, loss 0.283314, acc 0.890625, learning_rate 0.00267819
2017-10-10T14:44:39.712839: step 159, loss 0.551431, acc 0.8125, learning_rate 0.00266767
2017-10-10T14:44:40.030258: step 160, loss 0.444732, acc 0.875, learning_rate 0.00265719

Evaluation:
2017-10-10T14:44:40.580284: step 160, loss 0.316422, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-160

2017-10-10T14:44:41.584629: step 161, loss 0.288979, acc 0.875, learning_rate 0.00264675
2017-10-10T14:44:41.881903: step 162, loss 0.437634, acc 0.84375, learning_rate 0.00263635
2017-10-10T14:44:42.133782: step 163, loss 0.60104, acc 0.765625, learning_rate 0.002626
2017-10-10T14:44:42.432916: step 164, loss 0.462174, acc 0.796875, learning_rate 0.00261569
2017-10-10T14:44:42.691017: step 165, loss 0.338692, acc 0.875, learning_rate 0.00260542
2017-10-10T14:44:43.023154: step 166, loss 0.483716, acc 0.84375, learning_rate 0.0025952
2017-10-10T14:44:43.344932: step 167, loss 0.206237, acc 0.921875, learning_rate 0.00258501
2017-10-10T14:44:43.708930: step 168, loss 0.544816, acc 0.84375, learning_rate 0.00257487
2017-10-10T14:44:44.011720: step 169, loss 0.175295, acc 0.9375, learning_rate 0.00256477
2017-10-10T14:44:44.315041: step 170, loss 0.355277, acc 0.875, learning_rate 0.0025547
2017-10-10T14:44:44.635058: step 171, loss 0.368623, acc 0.859375, learning_rate 0.00254469
2017-10-10T14:44:44.932912: step 172, loss 0.561524, acc 0.859375, learning_rate 0.00253471
2017-10-10T14:44:45.277029: step 173, loss 0.321697, acc 0.921875, learning_rate 0.00252477
2017-10-10T14:44:45.567337: step 174, loss 0.449746, acc 0.875, learning_rate 0.00251487
2017-10-10T14:44:45.924026: step 175, loss 0.327094, acc 0.921875, learning_rate 0.00250501
2017-10-10T14:44:46.234989: step 176, loss 0.260186, acc 0.875, learning_rate 0.0024952
2017-10-10T14:44:46.564487: step 177, loss 0.222651, acc 0.9375, learning_rate 0.00248542
2017-10-10T14:44:46.856448: step 178, loss 0.354288, acc 0.875, learning_rate 0.00247568
2017-10-10T14:44:47.160969: step 179, loss 0.397296, acc 0.84375, learning_rate 0.00246599
2017-10-10T14:44:47.447496: step 180, loss 0.434791, acc 0.875, learning_rate 0.00245633
2017-10-10T14:44:47.786636: step 181, loss 0.648389, acc 0.796875, learning_rate 0.00244671
2017-10-10T14:44:48.072861: step 182, loss 0.58789, acc 0.8125, learning_rate 0.00243713
2017-10-10T14:44:48.410295: step 183, loss 0.411221, acc 0.828125, learning_rate 0.00242759
2017-10-10T14:44:48.712928: step 184, loss 0.701256, acc 0.75, learning_rate 0.00241809
2017-10-10T14:44:49.032855: step 185, loss 0.46109, acc 0.859375, learning_rate 0.00240863
2017-10-10T14:44:49.292544: step 186, loss 0.158571, acc 0.921875, learning_rate 0.00239921
2017-10-10T14:44:49.597181: step 187, loss 0.336895, acc 0.90625, learning_rate 0.00238982
2017-10-10T14:44:49.912285: step 188, loss 0.23638, acc 0.90625, learning_rate 0.00238048
2017-10-10T14:44:50.260806: step 189, loss 0.195334, acc 0.9375, learning_rate 0.00237117
2017-10-10T14:44:50.503756: step 190, loss 0.336109, acc 0.875, learning_rate 0.0023619
2017-10-10T14:44:50.797267: step 191, loss 0.59721, acc 0.84375, learning_rate 0.00235267
2017-10-10T14:44:51.110906: step 192, loss 0.488805, acc 0.890625, learning_rate 0.00234347
2017-10-10T14:44:51.453466: step 193, loss 0.26867, acc 0.9375, learning_rate 0.00233431
2017-10-10T14:44:51.751775: step 194, loss 0.349417, acc 0.859375, learning_rate 0.00232519
2017-10-10T14:44:51.993840: step 195, loss 0.347341, acc 0.875, learning_rate 0.00231611
2017-10-10T14:44:52.269856: step 196, loss 0.536828, acc 0.843137, learning_rate 0.00230707
2017-10-10T14:44:52.643372: step 197, loss 0.222605, acc 0.890625, learning_rate 0.00229806
2017-10-10T14:44:53.017222: step 198, loss 0.26012, acc 0.921875, learning_rate 0.00228908
2017-10-10T14:44:53.337160: step 199, loss 0.374766, acc 0.890625, learning_rate 0.00228015
2017-10-10T14:44:53.624756: step 200, loss 0.395496, acc 0.828125, learning_rate 0.00227125

Evaluation:
2017-10-10T14:44:54.031064: step 200, loss 0.320266, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-200

2017-10-10T14:44:54.892899: step 201, loss 0.278725, acc 0.90625, learning_rate 0.00226239
2017-10-10T14:44:55.218499: step 202, loss 0.434111, acc 0.84375, learning_rate 0.00225356
2017-10-10T14:44:55.548828: step 203, loss 0.315544, acc 0.875, learning_rate 0.00224477
2017-10-10T14:44:55.877442: step 204, loss 0.316947, acc 0.890625, learning_rate 0.00223602
2017-10-10T14:44:56.216953: step 205, loss 0.208356, acc 0.90625, learning_rate 0.0022273
2017-10-10T14:44:56.588076: step 206, loss 0.328522, acc 0.859375, learning_rate 0.00221862
2017-10-10T14:44:56.893102: step 207, loss 0.280949, acc 0.921875, learning_rate 0.00220997
2017-10-10T14:44:57.160526: step 208, loss 0.162095, acc 0.9375, learning_rate 0.00220136
2017-10-10T14:44:57.521992: step 209, loss 0.203519, acc 0.90625, learning_rate 0.00219278
2017-10-10T14:44:57.794789: step 210, loss 0.25744, acc 0.9375, learning_rate 0.00218424
2017-10-10T14:44:58.093942: step 211, loss 0.239492, acc 0.875, learning_rate 0.00217573
2017-10-10T14:44:58.389929: step 212, loss 0.260977, acc 0.90625, learning_rate 0.00216726
2017-10-10T14:44:58.708837: step 213, loss 0.181989, acc 0.921875, learning_rate 0.00215882
2017-10-10T14:44:59.000329: step 214, loss 0.530521, acc 0.859375, learning_rate 0.00215041
2017-10-10T14:44:59.316924: step 215, loss 0.281887, acc 0.890625, learning_rate 0.00214204
2017-10-10T14:44:59.660867: step 216, loss 0.307856, acc 0.875, learning_rate 0.00213371
2017-10-10T14:44:59.897550: step 217, loss 0.209765, acc 0.921875, learning_rate 0.00212541
2017-10-10T14:45:00.201079: step 218, loss 0.408895, acc 0.859375, learning_rate 0.00211714
2017-10-10T14:45:00.565548: step 219, loss 0.412397, acc 0.890625, learning_rate 0.00210891
2017-10-10T14:45:00.881406: step 220, loss 0.37186, acc 0.875, learning_rate 0.00210071
2017-10-10T14:45:01.172883: step 221, loss 0.280111, acc 0.9375, learning_rate 0.00209254
2017-10-10T14:45:01.401147: step 222, loss 0.183368, acc 0.953125, learning_rate 0.00208441
2017-10-10T14:45:01.664290: step 223, loss 0.411617, acc 0.90625, learning_rate 0.00207631
2017-10-10T14:45:01.949294: step 224, loss 0.400311, acc 0.859375, learning_rate 0.00206824
2017-10-10T14:45:02.316501: step 225, loss 0.277211, acc 0.890625, learning_rate 0.00206021
2017-10-10T14:45:02.589010: step 226, loss 0.205381, acc 0.9375, learning_rate 0.00205221
2017-10-10T14:45:02.820879: step 227, loss 0.258962, acc 0.90625, learning_rate 0.00204424
2017-10-10T14:45:03.124824: step 228, loss 0.423244, acc 0.78125, learning_rate 0.0020363
2017-10-10T14:45:03.456844: step 229, loss 0.29584, acc 0.890625, learning_rate 0.0020284
2017-10-10T14:45:03.772370: step 230, loss 0.302809, acc 0.875, learning_rate 0.00202053
2017-10-10T14:45:04.016917: step 231, loss 0.387207, acc 0.859375, learning_rate 0.00201269
2017-10-10T14:45:04.347289: step 232, loss 0.280359, acc 0.90625, learning_rate 0.00200488
2017-10-10T14:45:04.644842: step 233, loss 0.38235, acc 0.859375, learning_rate 0.00199711
2017-10-10T14:45:04.959363: step 234, loss 0.310703, acc 0.890625, learning_rate 0.00198936
2017-10-10T14:45:05.264635: step 235, loss 0.217967, acc 0.953125, learning_rate 0.00198165
2017-10-10T14:45:05.590127: step 236, loss 0.239829, acc 0.90625, learning_rate 0.00197397
2017-10-10T14:45:05.837043: step 237, loss 0.364167, acc 0.84375, learning_rate 0.00196632
2017-10-10T14:45:06.195144: step 238, loss 0.373676, acc 0.875, learning_rate 0.0019587
2017-10-10T14:45:06.519142: step 239, loss 0.354518, acc 0.859375, learning_rate 0.00195112
2017-10-10T14:45:06.788163: step 240, loss 0.297864, acc 0.90625, learning_rate 0.00194356

Evaluation:
2017-10-10T14:45:07.252815: step 240, loss 0.288302, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-240

2017-10-10T14:45:08.368136: step 241, loss 0.201686, acc 0.921875, learning_rate 0.00193604
2017-10-10T14:45:08.680779: step 242, loss 0.384391, acc 0.84375, learning_rate 0.00192854
2017-10-10T14:45:09.026447: step 243, loss 0.455986, acc 0.84375, learning_rate 0.00192108
2017-10-10T14:45:09.409592: step 244, loss 0.256749, acc 0.921875, learning_rate 0.00191364
2017-10-10T14:45:09.655558: step 245, loss 0.455987, acc 0.84375, learning_rate 0.00190624
2017-10-10T14:45:09.884380: step 246, loss 0.204408, acc 0.96875, learning_rate 0.00189887
2017-10-10T14:45:10.127371: step 247, loss 0.54357, acc 0.84375, learning_rate 0.00189153
2017-10-10T14:45:10.405575: step 248, loss 0.352215, acc 0.84375, learning_rate 0.00188421
2017-10-10T14:45:10.656862: step 249, loss 0.328463, acc 0.859375, learning_rate 0.00187693
2017-10-10T14:45:11.033010: step 250, loss 0.363, acc 0.84375, learning_rate 0.00186968
2017-10-10T14:45:11.361041: step 251, loss 0.278917, acc 0.90625, learning_rate 0.00186245
2017-10-10T14:45:11.586163: step 252, loss 0.118528, acc 0.96875, learning_rate 0.00185526
2017-10-10T14:45:11.955327: step 253, loss 0.473224, acc 0.796875, learning_rate 0.0018481
2017-10-10T14:45:12.328226: step 254, loss 0.318854, acc 0.875, learning_rate 0.00184096
2017-10-10T14:45:12.577644: step 255, loss 0.169958, acc 0.890625, learning_rate 0.00183385
2017-10-10T14:45:12.848967: step 256, loss 0.440419, acc 0.84375, learning_rate 0.00182678
2017-10-10T14:45:13.126171: step 257, loss 0.343819, acc 0.875, learning_rate 0.00181973
2017-10-10T14:45:13.410397: step 258, loss 0.324304, acc 0.890625, learning_rate 0.00181271
2017-10-10T14:45:13.763876: step 259, loss 0.217709, acc 0.9375, learning_rate 0.00180572
2017-10-10T14:45:14.162899: step 260, loss 0.204532, acc 0.96875, learning_rate 0.00179876
2017-10-10T14:45:14.444877: step 261, loss 0.537769, acc 0.859375, learning_rate 0.00179182
2017-10-10T14:45:14.768869: step 262, loss 0.320895, acc 0.890625, learning_rate 0.00178492
2017-10-10T14:45:15.045076: step 263, loss 0.37086, acc 0.84375, learning_rate 0.00177804
2017-10-10T14:45:15.312952: step 264, loss 0.414227, acc 0.90625, learning_rate 0.00177119
2017-10-10T14:45:15.589744: step 265, loss 0.198634, acc 0.953125, learning_rate 0.00176437
2017-10-10T14:45:15.944058: step 266, loss 0.231331, acc 0.921875, learning_rate 0.00175758
2017-10-10T14:45:16.276905: step 267, loss 0.296968, acc 0.90625, learning_rate 0.00175081
2017-10-10T14:45:16.525293: step 268, loss 0.22399, acc 0.953125, learning_rate 0.00174407
2017-10-10T14:45:16.801050: step 269, loss 0.629749, acc 0.765625, learning_rate 0.00173736
2017-10-10T14:45:17.155248: step 270, loss 0.212571, acc 0.921875, learning_rate 0.00173068
2017-10-10T14:45:17.492845: step 271, loss 0.222845, acc 0.9375, learning_rate 0.00172402
2017-10-10T14:45:17.747247: step 272, loss 0.321561, acc 0.890625, learning_rate 0.00171739
2017-10-10T14:45:18.040834: step 273, loss 0.186361, acc 0.9375, learning_rate 0.00171079
2017-10-10T14:45:18.377342: step 274, loss 0.518757, acc 0.8125, learning_rate 0.00170422
2017-10-10T14:45:18.530417: step 275, loss 0.376328, acc 0.875, learning_rate 0.00169767
2017-10-10T14:45:18.780745: step 276, loss 0.424446, acc 0.828125, learning_rate 0.00169115
2017-10-10T14:45:19.023052: step 277, loss 0.411632, acc 0.828125, learning_rate 0.00168465
2017-10-10T14:45:19.299343: step 278, loss 0.374356, acc 0.890625, learning_rate 0.00167818
2017-10-10T14:45:19.613005: step 279, loss 0.368381, acc 0.828125, learning_rate 0.00167174
2017-10-10T14:45:19.882582: step 280, loss 0.227023, acc 0.921875, learning_rate 0.00166533

Evaluation:
2017-10-10T14:45:20.360894: step 280, loss 0.28666, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-280

2017-10-10T14:45:21.544886: step 281, loss 0.426816, acc 0.84375, learning_rate 0.00165894
2017-10-10T14:45:21.831873: step 282, loss 0.450249, acc 0.796875, learning_rate 0.00165257
2017-10-10T14:45:22.124701: step 283, loss 0.352724, acc 0.890625, learning_rate 0.00164624
2017-10-10T14:45:22.518059: step 284, loss 0.359514, acc 0.890625, learning_rate 0.00163993
2017-10-10T14:45:22.857828: step 285, loss 0.294556, acc 0.90625, learning_rate 0.00163364
2017-10-10T14:45:23.216968: step 286, loss 0.210199, acc 0.90625, learning_rate 0.00162738
2017-10-10T14:45:23.514602: step 287, loss 0.289562, acc 0.90625, learning_rate 0.00162115
2017-10-10T14:45:23.846586: step 288, loss 0.335099, acc 0.890625, learning_rate 0.00161494
2017-10-10T14:45:24.154558: step 289, loss 0.39051, acc 0.859375, learning_rate 0.00160875
2017-10-10T14:45:24.381010: step 290, loss 0.401492, acc 0.8125, learning_rate 0.00160259
2017-10-10T14:45:24.639773: step 291, loss 0.247672, acc 0.90625, learning_rate 0.00159646
2017-10-10T14:45:24.936857: step 292, loss 0.18281, acc 0.9375, learning_rate 0.00159035
2017-10-10T14:45:25.235724: step 293, loss 0.221131, acc 0.9375, learning_rate 0.00158427
2017-10-10T14:45:25.461309: step 294, loss 0.202733, acc 0.960784, learning_rate 0.00157821
2017-10-10T14:45:25.791218: step 295, loss 0.224853, acc 0.9375, learning_rate 0.00157218
2017-10-10T14:45:26.088995: step 296, loss 0.276247, acc 0.90625, learning_rate 0.00156617
2017-10-10T14:45:26.507671: step 297, loss 0.356298, acc 0.890625, learning_rate 0.00156018
2017-10-10T14:45:26.775419: step 298, loss 0.116806, acc 0.96875, learning_rate 0.00155422
2017-10-10T14:45:27.060926: step 299, loss 0.444935, acc 0.84375, learning_rate 0.00154829
2017-10-10T14:45:27.262954: step 300, loss 0.15044, acc 0.953125, learning_rate 0.00154238
2017-10-10T14:45:27.549852: step 301, loss 0.326253, acc 0.90625, learning_rate 0.00153649
2017-10-10T14:45:27.852909: step 302, loss 0.211578, acc 0.9375, learning_rate 0.00153063
2017-10-10T14:45:28.127865: step 303, loss 0.299159, acc 0.9375, learning_rate 0.00152479
2017-10-10T14:45:28.454629: step 304, loss 0.294795, acc 0.890625, learning_rate 0.00151897
2017-10-10T14:45:28.768958: step 305, loss 0.202293, acc 0.9375, learning_rate 0.00151318
2017-10-10T14:45:28.974849: step 306, loss 0.445516, acc 0.859375, learning_rate 0.00150741
2017-10-10T14:45:29.304715: step 307, loss 0.343542, acc 0.875, learning_rate 0.00150167
2017-10-10T14:45:29.603496: step 308, loss 0.296354, acc 0.90625, learning_rate 0.00149594
2017-10-10T14:45:29.867914: step 309, loss 0.276984, acc 0.90625, learning_rate 0.00149025
2017-10-10T14:45:30.241700: step 310, loss 0.0923696, acc 0.984375, learning_rate 0.00148457
2017-10-10T14:45:30.593024: step 311, loss 0.230691, acc 0.90625, learning_rate 0.00147892
2017-10-10T14:45:30.881792: step 312, loss 0.233579, acc 0.9375, learning_rate 0.00147329
2017-10-10T14:45:31.127857: step 313, loss 0.195621, acc 0.953125, learning_rate 0.00146769
2017-10-10T14:45:31.373125: step 314, loss 0.45204, acc 0.859375, learning_rate 0.0014621
2017-10-10T14:45:31.658163: step 315, loss 0.1761, acc 0.921875, learning_rate 0.00145654
2017-10-10T14:45:31.985022: step 316, loss 0.380854, acc 0.875, learning_rate 0.00145101
2017-10-10T14:45:32.268996: step 317, loss 0.27649, acc 0.921875, learning_rate 0.00144549
2017-10-10T14:45:32.589746: step 318, loss 0.391887, acc 0.8125, learning_rate 0.00144
2017-10-10T14:45:32.894027: step 319, loss 0.343593, acc 0.84375, learning_rate 0.00143453
2017-10-10T14:45:33.216950: step 320, loss 0.277834, acc 0.890625, learning_rate 0.00142908

Evaluation:
2017-10-10T14:45:33.733008: step 320, loss 0.284857, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-320

2017-10-10T14:45:34.895683: step 321, loss 0.430487, acc 0.859375, learning_rate 0.00142366
2017-10-10T14:45:35.164115: step 322, loss 0.109275, acc 0.984375, learning_rate 0.00141826
2017-10-10T14:45:35.449511: step 323, loss 0.322429, acc 0.875, learning_rate 0.00141288
2017-10-10T14:45:35.746203: step 324, loss 0.309155, acc 0.90625, learning_rate 0.00140752
2017-10-10T14:45:35.993004: step 325, loss 0.23648, acc 0.90625, learning_rate 0.00140218
2017-10-10T14:45:36.274399: step 326, loss 0.190636, acc 0.90625, learning_rate 0.00139686
2017-10-10T14:45:36.568691: step 327, loss 0.132226, acc 0.953125, learning_rate 0.00139157
2017-10-10T14:45:36.825239: step 328, loss 0.26494, acc 0.921875, learning_rate 0.0013863
2017-10-10T14:45:37.148219: step 329, loss 0.221441, acc 0.921875, learning_rate 0.00138105
2017-10-10T14:45:37.493000: step 330, loss 0.289145, acc 0.921875, learning_rate 0.00137582
2017-10-10T14:45:37.812869: step 331, loss 0.389333, acc 0.859375, learning_rate 0.00137061
2017-10-10T14:45:38.073241: step 332, loss 0.191786, acc 0.921875, learning_rate 0.00136543
2017-10-10T14:45:38.395234: step 333, loss 0.211164, acc 0.96875, learning_rate 0.00136026
2017-10-10T14:45:38.700836: step 334, loss 0.27103, acc 0.921875, learning_rate 0.00135512
2017-10-10T14:45:38.989042: step 335, loss 0.224732, acc 0.921875, learning_rate 0.00134999
2017-10-10T14:45:39.288869: step 336, loss 0.265348, acc 0.921875, learning_rate 0.00134489
2017-10-10T14:45:39.604454: step 337, loss 0.210195, acc 0.9375, learning_rate 0.00133981
2017-10-10T14:45:39.921038: step 338, loss 0.302508, acc 0.890625, learning_rate 0.00133475
2017-10-10T14:45:40.205004: step 339, loss 0.250975, acc 0.890625, learning_rate 0.00132971
2017-10-10T14:45:40.552817: step 340, loss 0.422639, acc 0.890625, learning_rate 0.00132469
2017-10-10T14:45:40.817364: step 341, loss 0.183726, acc 0.9375, learning_rate 0.00131969
2017-10-10T14:45:41.117028: step 342, loss 0.270312, acc 0.890625, learning_rate 0.00131471
2017-10-10T14:45:41.397139: step 343, loss 0.106739, acc 0.984375, learning_rate 0.00130975
2017-10-10T14:45:41.748931: step 344, loss 0.202593, acc 0.9375, learning_rate 0.00130482
2017-10-10T14:45:42.093213: step 345, loss 0.277142, acc 0.921875, learning_rate 0.0012999
2017-10-10T14:45:42.396842: step 346, loss 0.245299, acc 0.875, learning_rate 0.001295
2017-10-10T14:45:42.778157: step 347, loss 0.218416, acc 0.953125, learning_rate 0.00129012
2017-10-10T14:45:43.049022: step 348, loss 0.316483, acc 0.890625, learning_rate 0.00128527
2017-10-10T14:45:43.219849: step 349, loss 0.15382, acc 0.9375, learning_rate 0.00128043
2017-10-10T14:45:43.440158: step 350, loss 0.200835, acc 0.9375, learning_rate 0.00127561
2017-10-10T14:45:43.642317: step 351, loss 0.180341, acc 0.953125, learning_rate 0.00127081
2017-10-10T14:45:43.868936: step 352, loss 0.263859, acc 0.921875, learning_rate 0.00126603
2017-10-10T14:45:44.148897: step 353, loss 0.401578, acc 0.875, learning_rate 0.00126127
2017-10-10T14:45:44.487279: step 354, loss 0.368685, acc 0.859375, learning_rate 0.00125653
2017-10-10T14:45:44.855191: step 355, loss 0.211197, acc 0.921875, learning_rate 0.00125181
2017-10-10T14:45:45.172945: step 356, loss 0.165888, acc 0.9375, learning_rate 0.00124711
2017-10-10T14:45:45.459620: step 357, loss 0.299131, acc 0.921875, learning_rate 0.00124243
2017-10-10T14:45:45.785128: step 358, loss 0.391172, acc 0.875, learning_rate 0.00123777
2017-10-10T14:45:46.102272: step 359, loss 0.219112, acc 0.9375, learning_rate 0.00123312
2017-10-10T14:45:46.332334: step 360, loss 0.328303, acc 0.890625, learning_rate 0.0012285

Evaluation:
2017-10-10T14:45:46.890429: step 360, loss 0.279177, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-360

2017-10-10T14:45:47.844826: step 361, loss 0.171358, acc 0.953125, learning_rate 0.00122389
2017-10-10T14:45:48.168228: step 362, loss 0.249463, acc 0.875, learning_rate 0.0012193
2017-10-10T14:45:48.503287: step 363, loss 0.291273, acc 0.890625, learning_rate 0.00121473
2017-10-10T14:45:48.848109: step 364, loss 0.344709, acc 0.875, learning_rate 0.00121018
2017-10-10T14:45:49.145467: step 365, loss 0.199994, acc 0.90625, learning_rate 0.00120565
2017-10-10T14:45:49.404797: step 366, loss 0.435473, acc 0.828125, learning_rate 0.00120114
2017-10-10T14:45:49.666195: step 367, loss 0.274205, acc 0.90625, learning_rate 0.00119664
2017-10-10T14:45:49.976395: step 368, loss 0.149398, acc 0.96875, learning_rate 0.00119217
2017-10-10T14:45:50.272914: step 369, loss 0.164832, acc 0.9375, learning_rate 0.00118771
2017-10-10T14:45:50.551336: step 370, loss 0.382147, acc 0.90625, learning_rate 0.00118327
2017-10-10T14:45:50.828983: step 371, loss 0.196737, acc 0.96875, learning_rate 0.00117885
2017-10-10T14:45:51.154898: step 372, loss 0.253383, acc 0.90625, learning_rate 0.00117445
2017-10-10T14:45:51.481255: step 373, loss 0.241134, acc 0.921875, learning_rate 0.00117006
2017-10-10T14:45:51.720455: step 374, loss 0.40838, acc 0.875, learning_rate 0.00116569
2017-10-10T14:45:52.051053: step 375, loss 0.256169, acc 0.9375, learning_rate 0.00116134
2017-10-10T14:45:52.385179: step 376, loss 0.192892, acc 0.9375, learning_rate 0.00115701
2017-10-10T14:45:52.705001: step 377, loss 0.352133, acc 0.921875, learning_rate 0.0011527
2017-10-10T14:45:53.021514: step 378, loss 0.139593, acc 0.953125, learning_rate 0.0011484
2017-10-10T14:45:53.316918: step 379, loss 0.0897616, acc 0.984375, learning_rate 0.00114412
2017-10-10T14:45:53.589047: step 380, loss 0.136934, acc 0.953125, learning_rate 0.00113986
2017-10-10T14:45:53.892843: step 381, loss 0.268496, acc 0.890625, learning_rate 0.00113561
2017-10-10T14:45:54.282349: step 382, loss 0.294997, acc 0.90625, learning_rate 0.00113139
2017-10-10T14:45:54.604834: step 383, loss 0.240415, acc 0.921875, learning_rate 0.00112718
2017-10-10T14:45:54.916824: step 384, loss 0.211586, acc 0.90625, learning_rate 0.00112298
2017-10-10T14:45:55.229020: step 385, loss 0.131763, acc 0.953125, learning_rate 0.00111881
2017-10-10T14:45:55.510422: step 386, loss 0.226513, acc 0.890625, learning_rate 0.00111465
2017-10-10T14:45:55.820259: step 387, loss 0.144211, acc 0.9375, learning_rate 0.00111051
2017-10-10T14:45:56.118080: step 388, loss 0.262159, acc 0.890625, learning_rate 0.00110638
2017-10-10T14:45:56.399601: step 389, loss 0.370458, acc 0.890625, learning_rate 0.00110228
2017-10-10T14:45:56.715751: step 390, loss 0.28918, acc 0.84375, learning_rate 0.00109818
2017-10-10T14:45:56.992702: step 391, loss 0.148664, acc 0.953125, learning_rate 0.00109411
2017-10-10T14:45:57.252831: step 392, loss 0.329535, acc 0.843137, learning_rate 0.00109005
2017-10-10T14:45:57.497154: step 393, loss 0.33076, acc 0.890625, learning_rate 0.00108601
2017-10-10T14:45:57.780346: step 394, loss 0.122363, acc 0.984375, learning_rate 0.00108199
2017-10-10T14:45:58.104939: step 395, loss 0.121493, acc 0.9375, learning_rate 0.00107798
2017-10-10T14:45:58.392988: step 396, loss 0.190247, acc 0.953125, learning_rate 0.00107399
2017-10-10T14:45:58.654140: step 397, loss 0.195426, acc 0.953125, learning_rate 0.00107001
2017-10-10T14:45:58.932938: step 398, loss 0.120027, acc 0.953125, learning_rate 0.00106605
2017-10-10T14:45:59.230497: step 399, loss 0.172346, acc 0.96875, learning_rate 0.00106211
2017-10-10T14:45:59.545038: step 400, loss 0.258271, acc 0.890625, learning_rate 0.00105818

Evaluation:
2017-10-10T14:46:00.216859: step 400, loss 0.27076, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-400

2017-10-10T14:46:01.300326: step 401, loss 0.164681, acc 0.96875, learning_rate 0.00105427
2017-10-10T14:46:01.530537: step 402, loss 0.321411, acc 0.859375, learning_rate 0.00105037
2017-10-10T14:46:01.831953: step 403, loss 0.394498, acc 0.875, learning_rate 0.0010465
2017-10-10T14:46:02.189070: step 404, loss 0.221357, acc 0.90625, learning_rate 0.00104263
2017-10-10T14:46:02.510840: step 405, loss 0.352784, acc 0.90625, learning_rate 0.00103878
2017-10-10T14:46:02.853519: step 406, loss 0.263894, acc 0.890625, learning_rate 0.00103495
2017-10-10T14:46:03.090455: step 407, loss 0.367271, acc 0.875, learning_rate 0.00103114
2017-10-10T14:46:03.434498: step 408, loss 0.126961, acc 0.96875, learning_rate 0.00102734
2017-10-10T14:46:03.765088: step 409, loss 0.271141, acc 0.90625, learning_rate 0.00102355
2017-10-10T14:46:04.104965: step 410, loss 0.270264, acc 0.890625, learning_rate 0.00101978
2017-10-10T14:46:04.353373: step 411, loss 0.227045, acc 0.921875, learning_rate 0.00101603
2017-10-10T14:46:04.623731: step 412, loss 0.21032, acc 0.921875, learning_rate 0.00101229
2017-10-10T14:46:04.949398: step 413, loss 0.249052, acc 0.890625, learning_rate 0.00100856
2017-10-10T14:46:05.292828: step 414, loss 0.210624, acc 0.953125, learning_rate 0.00100486
2017-10-10T14:46:05.602611: step 415, loss 0.147881, acc 0.96875, learning_rate 0.00100116
2017-10-10T14:46:05.917141: step 416, loss 0.20717, acc 0.921875, learning_rate 0.000997483
2017-10-10T14:46:06.260106: step 417, loss 0.173411, acc 0.9375, learning_rate 0.00099382
2017-10-10T14:46:06.551882: step 418, loss 0.276521, acc 0.890625, learning_rate 0.000990172
2017-10-10T14:46:06.873140: step 419, loss 0.141628, acc 0.953125, learning_rate 0.000986538
2017-10-10T14:46:07.214075: step 420, loss 0.270002, acc 0.890625, learning_rate 0.00098292
2017-10-10T14:46:07.516851: step 421, loss 0.168635, acc 0.921875, learning_rate 0.000979316
2017-10-10T14:46:07.731265: step 422, loss 0.187824, acc 0.953125, learning_rate 0.000975727
2017-10-10T14:46:07.960458: step 423, loss 0.198094, acc 0.90625, learning_rate 0.000972152
2017-10-10T14:46:08.209200: step 424, loss 0.202043, acc 0.953125, learning_rate 0.000968592
2017-10-10T14:46:08.525074: step 425, loss 0.255313, acc 0.890625, learning_rate 0.000965047
2017-10-10T14:46:08.779793: step 426, loss 0.22566, acc 0.90625, learning_rate 0.000961516
2017-10-10T14:46:09.102112: step 427, loss 0.153191, acc 0.953125, learning_rate 0.000958
2017-10-10T14:46:09.350903: step 428, loss 0.224323, acc 0.9375, learning_rate 0.000954497
2017-10-10T14:46:09.676887: step 429, loss 0.343394, acc 0.875, learning_rate 0.00095101
2017-10-10T14:46:09.986711: step 430, loss 0.150055, acc 0.9375, learning_rate 0.000947536
2017-10-10T14:46:10.200896: step 431, loss 0.248811, acc 0.953125, learning_rate 0.000944076
2017-10-10T14:46:10.512597: step 432, loss 0.198219, acc 0.90625, learning_rate 0.000940631
2017-10-10T14:46:10.884849: step 433, loss 0.140732, acc 0.9375, learning_rate 0.0009372
2017-10-10T14:46:11.221076: step 434, loss 0.231934, acc 0.9375, learning_rate 0.000933783
2017-10-10T14:46:11.462771: step 435, loss 0.253469, acc 0.90625, learning_rate 0.000930379
2017-10-10T14:46:11.800881: step 436, loss 0.172834, acc 0.953125, learning_rate 0.00092699
2017-10-10T14:46:12.134611: step 437, loss 0.175388, acc 0.90625, learning_rate 0.000923614
2017-10-10T14:46:12.453320: step 438, loss 0.103533, acc 0.984375, learning_rate 0.000920253
2017-10-10T14:46:12.660967: step 439, loss 0.143044, acc 0.921875, learning_rate 0.000916905
2017-10-10T14:46:12.976886: step 440, loss 0.229593, acc 0.953125, learning_rate 0.00091357

Evaluation:
2017-10-10T14:46:13.514639: step 440, loss 0.270582, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-440

2017-10-10T14:46:14.751634: step 441, loss 0.155599, acc 0.96875, learning_rate 0.000910249
2017-10-10T14:46:15.104771: step 442, loss 0.205301, acc 0.890625, learning_rate 0.000906942
2017-10-10T14:46:15.408891: step 443, loss 0.215161, acc 0.90625, learning_rate 0.000903648
2017-10-10T14:46:15.653156: step 444, loss 0.252769, acc 0.921875, learning_rate 0.000900368
2017-10-10T14:46:15.934647: step 445, loss 0.324644, acc 0.90625, learning_rate 0.000897101
2017-10-10T14:46:16.248532: step 446, loss 0.238895, acc 0.921875, learning_rate 0.000893848
2017-10-10T14:46:16.481879: step 447, loss 0.184691, acc 0.9375, learning_rate 0.000890607
2017-10-10T14:46:16.793985: step 448, loss 0.23097, acc 0.9375, learning_rate 0.00088738
2017-10-10T14:46:17.135000: step 449, loss 0.272615, acc 0.890625, learning_rate 0.000884166
2017-10-10T14:46:17.361280: step 450, loss 0.210397, acc 0.890625, learning_rate 0.000880966
2017-10-10T14:46:17.634544: step 451, loss 0.151253, acc 0.953125, learning_rate 0.000877778
2017-10-10T14:46:17.879865: step 452, loss 0.247605, acc 0.890625, learning_rate 0.000874603
2017-10-10T14:46:18.141134: step 453, loss 0.301238, acc 0.9375, learning_rate 0.000871441
2017-10-10T14:46:18.499315: step 454, loss 0.129173, acc 0.984375, learning_rate 0.000868293
2017-10-10T14:46:18.763344: step 455, loss 0.337557, acc 0.859375, learning_rate 0.000865157
2017-10-10T14:46:19.013043: step 456, loss 0.231847, acc 0.90625, learning_rate 0.000862033
2017-10-10T14:46:19.300907: step 457, loss 0.19505, acc 0.9375, learning_rate 0.000858923
2017-10-10T14:46:19.665277: step 458, loss 0.270008, acc 0.90625, learning_rate 0.000855825
2017-10-10T14:46:19.948894: step 459, loss 0.178672, acc 0.96875, learning_rate 0.00085274
2017-10-10T14:46:20.243838: step 460, loss 0.165153, acc 0.96875, learning_rate 0.000849668
2017-10-10T14:46:20.503622: step 461, loss 0.231098, acc 0.9375, learning_rate 0.000846608
2017-10-10T14:46:20.845774: step 462, loss 0.230843, acc 0.921875, learning_rate 0.00084356
2017-10-10T14:46:21.169135: step 463, loss 0.301315, acc 0.90625, learning_rate 0.000840525
2017-10-10T14:46:21.442693: step 464, loss 0.151745, acc 0.96875, learning_rate 0.000837502
2017-10-10T14:46:21.697350: step 465, loss 0.245414, acc 0.9375, learning_rate 0.000834492
2017-10-10T14:46:21.972861: step 466, loss 0.278445, acc 0.890625, learning_rate 0.000831494
2017-10-10T14:46:22.320824: step 467, loss 0.253288, acc 0.90625, learning_rate 0.000828508
2017-10-10T14:46:22.646748: step 468, loss 0.198742, acc 0.90625, learning_rate 0.000825535
2017-10-10T14:46:22.980982: step 469, loss 0.188115, acc 0.96875, learning_rate 0.000822573
2017-10-10T14:46:23.268154: step 470, loss 0.0702102, acc 0.984375, learning_rate 0.000819624
2017-10-10T14:46:23.577018: step 471, loss 0.187583, acc 0.96875, learning_rate 0.000816687
2017-10-10T14:46:23.899178: step 472, loss 0.0826215, acc 0.984375, learning_rate 0.000813761
2017-10-10T14:46:24.171357: step 473, loss 0.166156, acc 0.921875, learning_rate 0.000810848
2017-10-10T14:46:24.418545: step 474, loss 0.171836, acc 0.953125, learning_rate 0.000807946
2017-10-10T14:46:24.711788: step 475, loss 0.288099, acc 0.921875, learning_rate 0.000805057
2017-10-10T14:46:25.117777: step 476, loss 0.171019, acc 0.9375, learning_rate 0.000802179
2017-10-10T14:46:25.389107: step 477, loss 0.143503, acc 0.9375, learning_rate 0.000799313
2017-10-10T14:46:25.682826: step 478, loss 0.206497, acc 0.921875, learning_rate 0.000796458
2017-10-10T14:46:25.911763: step 479, loss 0.185549, acc 0.9375, learning_rate 0.000793616
2017-10-10T14:46:26.188655: step 480, loss 0.259329, acc 0.890625, learning_rate 0.000790784

Evaluation:
2017-10-10T14:46:26.736982: step 480, loss 0.263697, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-480

2017-10-10T14:46:27.892766: step 481, loss 0.252589, acc 0.921875, learning_rate 0.000787965
2017-10-10T14:46:28.123104: step 482, loss 0.323101, acc 0.859375, learning_rate 0.000785157
2017-10-10T14:46:28.429451: step 483, loss 0.216718, acc 0.90625, learning_rate 0.00078236
2017-10-10T14:46:28.719775: step 484, loss 0.0956261, acc 0.984375, learning_rate 0.000779575
2017-10-10T14:46:28.951768: step 485, loss 0.109841, acc 0.953125, learning_rate 0.000776801
2017-10-10T14:46:29.228783: step 486, loss 0.241846, acc 0.921875, learning_rate 0.000774038
2017-10-10T14:46:29.616914: step 487, loss 0.177881, acc 0.921875, learning_rate 0.000771287
2017-10-10T14:46:29.840792: step 488, loss 0.206888, acc 0.9375, learning_rate 0.000768547
2017-10-10T14:46:30.132962: step 489, loss 0.287802, acc 0.90625, learning_rate 0.000765818
2017-10-10T14:46:30.400954: step 490, loss 0.210335, acc 0.941176, learning_rate 0.000763101
2017-10-10T14:46:30.713054: step 491, loss 0.110007, acc 0.984375, learning_rate 0.000760394
2017-10-10T14:46:31.048620: step 492, loss 0.157469, acc 0.9375, learning_rate 0.000757698
2017-10-10T14:46:31.345167: step 493, loss 0.139066, acc 0.953125, learning_rate 0.000755014
2017-10-10T14:46:31.715150: step 494, loss 0.203335, acc 0.953125, learning_rate 0.00075234
2017-10-10T14:46:31.978998: step 495, loss 0.2526, acc 0.90625, learning_rate 0.000749677
2017-10-10T14:46:32.253563: step 496, loss 0.196346, acc 0.953125, learning_rate 0.000747026
2017-10-10T14:46:32.556844: step 497, loss 0.310002, acc 0.875, learning_rate 0.000744385
2017-10-10T14:46:32.820825: step 498, loss 0.208371, acc 0.90625, learning_rate 0.000741754
2017-10-10T14:46:33.097760: step 499, loss 0.119465, acc 0.984375, learning_rate 0.000739135
2017-10-10T14:46:33.440903: step 500, loss 0.0712574, acc 0.984375, learning_rate 0.000736526
2017-10-10T14:46:33.744812: step 501, loss 0.132427, acc 0.9375, learning_rate 0.000733928
2017-10-10T14:46:34.073358: step 502, loss 0.38939, acc 0.890625, learning_rate 0.00073134
2017-10-10T14:46:34.288780: step 503, loss 0.295236, acc 0.90625, learning_rate 0.000728763
2017-10-10T14:46:34.515164: step 504, loss 0.190291, acc 0.9375, learning_rate 0.000726197
2017-10-10T14:46:34.785203: step 505, loss 0.152422, acc 0.953125, learning_rate 0.000723641
2017-10-10T14:46:34.996371: step 506, loss 0.178285, acc 0.953125, learning_rate 0.000721095
2017-10-10T14:46:35.298530: step 507, loss 0.226976, acc 0.953125, learning_rate 0.00071856
2017-10-10T14:46:35.572982: step 508, loss 0.142574, acc 0.953125, learning_rate 0.000716036
2017-10-10T14:46:35.869638: step 509, loss 0.234086, acc 0.953125, learning_rate 0.000713521
2017-10-10T14:46:36.217078: step 510, loss 0.250233, acc 0.96875, learning_rate 0.000711017
2017-10-10T14:46:36.530004: step 511, loss 0.233533, acc 0.890625, learning_rate 0.000708523
2017-10-10T14:46:36.814541: step 512, loss 0.206745, acc 0.9375, learning_rate 0.000706039
2017-10-10T14:46:37.089318: step 513, loss 0.304013, acc 0.90625, learning_rate 0.000703565
2017-10-10T14:46:37.420325: step 514, loss 0.107641, acc 0.984375, learning_rate 0.000701102
2017-10-10T14:46:37.770119: step 515, loss 0.310431, acc 0.890625, learning_rate 0.000698648
2017-10-10T14:46:38.124887: step 516, loss 0.168617, acc 0.9375, learning_rate 0.000696204
2017-10-10T14:46:38.440253: step 517, loss 0.16408, acc 0.921875, learning_rate 0.000693771
2017-10-10T14:46:38.674031: step 518, loss 0.19501, acc 0.90625, learning_rate 0.000691347
2017-10-10T14:46:38.989348: step 519, loss 0.148453, acc 0.921875, learning_rate 0.000688934
2017-10-10T14:46:39.327473: step 520, loss 0.252952, acc 0.96875, learning_rate 0.00068653

Evaluation:
2017-10-10T14:46:39.904891: step 520, loss 0.260283, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-520

2017-10-10T14:46:40.864987: step 521, loss 0.184277, acc 0.953125, learning_rate 0.000684136
2017-10-10T14:46:41.107177: step 522, loss 0.1617, acc 0.9375, learning_rate 0.000681751
2017-10-10T14:46:41.436805: step 523, loss 0.283409, acc 0.90625, learning_rate 0.000679377
2017-10-10T14:46:41.713169: step 524, loss 0.182891, acc 0.921875, learning_rate 0.000677012
2017-10-10T14:46:42.003723: step 525, loss 0.167338, acc 0.921875, learning_rate 0.000674657
2017-10-10T14:46:42.327236: step 526, loss 0.27838, acc 0.90625, learning_rate 0.000672311
2017-10-10T14:46:42.654696: step 527, loss 0.116574, acc 0.96875, learning_rate 0.000669975
2017-10-10T14:46:43.008375: step 528, loss 0.236932, acc 0.90625, learning_rate 0.000667648
2017-10-10T14:46:43.304792: step 529, loss 0.270975, acc 0.890625, learning_rate 0.000665331
2017-10-10T14:46:43.568347: step 530, loss 0.171605, acc 0.953125, learning_rate 0.000663024
2017-10-10T14:46:43.832893: step 531, loss 0.133075, acc 0.9375, learning_rate 0.000660726
2017-10-10T14:46:44.024492: step 532, loss 0.37098, acc 0.921875, learning_rate 0.000658437
2017-10-10T14:46:44.310245: step 533, loss 0.199635, acc 0.921875, learning_rate 0.000656158
2017-10-10T14:46:44.616934: step 534, loss 0.323957, acc 0.859375, learning_rate 0.000653888
2017-10-10T14:46:44.941737: step 535, loss 0.17838, acc 0.953125, learning_rate 0.000651627
2017-10-10T14:46:45.304768: step 536, loss 0.170289, acc 0.921875, learning_rate 0.000649375
2017-10-10T14:46:45.644933: step 537, loss 0.258652, acc 0.890625, learning_rate 0.000647133
2017-10-10T14:46:45.932934: step 538, loss 0.205807, acc 0.921875, learning_rate 0.000644899
2017-10-10T14:46:46.253233: step 539, loss 0.1953, acc 0.9375, learning_rate 0.000642675
2017-10-10T14:46:46.507059: step 540, loss 0.12354, acc 0.96875, learning_rate 0.00064046
2017-10-10T14:46:46.848089: step 541, loss 0.202864, acc 0.90625, learning_rate 0.000638254
2017-10-10T14:46:47.168825: step 542, loss 0.236111, acc 0.90625, learning_rate 0.000636057
2017-10-10T14:46:47.423828: step 543, loss 0.0792649, acc 0.984375, learning_rate 0.000633869
2017-10-10T14:46:47.708955: step 544, loss 0.288386, acc 0.90625, learning_rate 0.00063169
2017-10-10T14:46:48.063090: step 545, loss 0.155351, acc 0.953125, learning_rate 0.00062952
2017-10-10T14:46:48.366570: step 546, loss 0.20322, acc 0.921875, learning_rate 0.000627358
2017-10-10T14:46:48.672707: step 547, loss 0.216999, acc 0.921875, learning_rate 0.000625206
2017-10-10T14:46:49.016867: step 548, loss 0.192581, acc 0.921875, learning_rate 0.000623062
2017-10-10T14:46:49.373444: step 549, loss 0.309064, acc 0.90625, learning_rate 0.000620927
2017-10-10T14:46:49.651406: step 550, loss 0.0827758, acc 0.984375, learning_rate 0.000618801
2017-10-10T14:46:49.928907: step 551, loss 0.165951, acc 0.953125, learning_rate 0.000616683
2017-10-10T14:46:50.308514: step 552, loss 0.112933, acc 0.953125, learning_rate 0.000614574
2017-10-10T14:46:50.589222: step 553, loss 0.196404, acc 0.921875, learning_rate 0.000612474
2017-10-10T14:46:50.876840: step 554, loss 0.180632, acc 0.890625, learning_rate 0.000610382
2017-10-10T14:46:51.128862: step 555, loss 0.129171, acc 0.9375, learning_rate 0.000608299
2017-10-10T14:46:51.427909: step 556, loss 0.195677, acc 0.9375, learning_rate 0.000606224
2017-10-10T14:46:51.753736: step 557, loss 0.16262, acc 0.96875, learning_rate 0.000604158
2017-10-10T14:46:51.995239: step 558, loss 0.0875012, acc 1, learning_rate 0.0006021
2017-10-10T14:46:52.295218: step 559, loss 0.23968, acc 0.921875, learning_rate 0.00060005
2017-10-10T14:46:52.584042: step 560, loss 0.13576, acc 0.953125, learning_rate 0.000598009

Evaluation:
2017-10-10T14:46:53.133130: step 560, loss 0.260837, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-560

2017-10-10T14:46:54.216773: step 561, loss 0.200587, acc 0.9375, learning_rate 0.000595977
2017-10-10T14:46:54.568972: step 562, loss 0.235355, acc 0.890625, learning_rate 0.000593952
2017-10-10T14:46:54.866006: step 563, loss 0.161456, acc 0.96875, learning_rate 0.000591936
2017-10-10T14:46:55.079703: step 564, loss 0.107285, acc 0.96875, learning_rate 0.000589928
2017-10-10T14:46:55.376530: step 565, loss 0.283375, acc 0.9375, learning_rate 0.000587928
2017-10-10T14:46:55.672797: step 566, loss 0.228905, acc 0.9375, learning_rate 0.000585937
2017-10-10T14:46:55.952928: step 567, loss 0.234066, acc 0.921875, learning_rate 0.000583953
2017-10-10T14:46:56.277460: step 568, loss 0.256478, acc 0.890625, learning_rate 0.000581978
2017-10-10T14:46:56.545009: step 569, loss 0.211274, acc 0.90625, learning_rate 0.00058001
2017-10-10T14:46:56.815804: step 570, loss 0.272576, acc 0.890625, learning_rate 0.000578051
2017-10-10T14:46:57.071319: step 571, loss 0.138226, acc 0.96875, learning_rate 0.0005761
2017-10-10T14:46:57.390786: step 572, loss 0.214234, acc 0.90625, learning_rate 0.000574157
2017-10-10T14:46:57.663254: step 573, loss 0.182388, acc 0.953125, learning_rate 0.000572221
2017-10-10T14:46:57.955952: step 574, loss 0.0862957, acc 0.984375, learning_rate 0.000570294
2017-10-10T14:46:58.272055: step 575, loss 0.163102, acc 0.953125, learning_rate 0.000568374
2017-10-10T14:46:58.587238: step 576, loss 0.25375, acc 0.890625, learning_rate 0.000566462
2017-10-10T14:46:58.880050: step 577, loss 0.27074, acc 0.890625, learning_rate 0.000564558
2017-10-10T14:46:59.218804: step 578, loss 0.189984, acc 0.921875, learning_rate 0.000562662
2017-10-10T14:46:59.536355: step 579, loss 0.156067, acc 0.953125, learning_rate 0.000560774
2017-10-10T14:46:59.859278: step 580, loss 0.266094, acc 0.875, learning_rate 0.000558893
2017-10-10T14:47:00.113102: step 581, loss 0.252055, acc 0.9375, learning_rate 0.00055702
2017-10-10T14:47:00.410014: step 582, loss 0.116025, acc 0.953125, learning_rate 0.000555154
2017-10-10T14:47:00.780866: step 583, loss 0.209112, acc 0.921875, learning_rate 0.000553296
2017-10-10T14:47:01.153173: step 584, loss 0.232451, acc 0.9375, learning_rate 0.000551446
2017-10-10T14:47:01.445309: step 585, loss 0.174577, acc 0.9375, learning_rate 0.000549604
2017-10-10T14:47:01.632832: step 586, loss 0.201999, acc 0.90625, learning_rate 0.000547768
2017-10-10T14:47:01.863103: step 587, loss 0.103901, acc 0.984375, learning_rate 0.000545941
2017-10-10T14:47:02.124317: step 588, loss 0.310171, acc 0.921569, learning_rate 0.00054412
2017-10-10T14:47:02.353121: step 589, loss 0.20506, acc 0.9375, learning_rate 0.000542308
2017-10-10T14:47:02.612845: step 590, loss 0.137962, acc 0.9375, learning_rate 0.000540502
2017-10-10T14:47:02.934201: step 591, loss 0.165361, acc 0.9375, learning_rate 0.000538704
2017-10-10T14:47:03.240185: step 592, loss 0.187184, acc 0.9375, learning_rate 0.000536914
2017-10-10T14:47:03.504550: step 593, loss 0.280708, acc 0.90625, learning_rate 0.00053513
2017-10-10T14:47:03.761152: step 594, loss 0.150427, acc 0.9375, learning_rate 0.000533354
2017-10-10T14:47:04.060201: step 595, loss 0.168057, acc 0.96875, learning_rate 0.000531585
2017-10-10T14:47:04.366489: step 596, loss 0.191172, acc 0.9375, learning_rate 0.000529824
2017-10-10T14:47:04.659383: step 597, loss 0.212996, acc 0.9375, learning_rate 0.000528069
2017-10-10T14:47:04.948949: step 598, loss 0.18155, acc 0.9375, learning_rate 0.000526322
2017-10-10T14:47:05.262379: step 599, loss 0.238896, acc 0.90625, learning_rate 0.000524582
2017-10-10T14:47:05.585117: step 600, loss 0.237744, acc 0.921875, learning_rate 0.000522849

Evaluation:
2017-10-10T14:47:06.081262: step 600, loss 0.256038, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-600

2017-10-10T14:47:07.326206: step 601, loss 0.152802, acc 0.9375, learning_rate 0.000521123
2017-10-10T14:47:07.614719: step 602, loss 0.262561, acc 0.921875, learning_rate 0.000519404
2017-10-10T14:47:07.921217: step 603, loss 0.0978638, acc 0.96875, learning_rate 0.000517692
2017-10-10T14:47:08.283185: step 604, loss 0.264758, acc 0.90625, learning_rate 0.000515987
2017-10-10T14:47:08.579697: step 605, loss 0.264565, acc 0.921875, learning_rate 0.000514289
2017-10-10T14:47:08.889029: step 606, loss 0.210767, acc 0.90625, learning_rate 0.000512598
2017-10-10T14:47:09.166299: step 607, loss 0.0980723, acc 0.96875, learning_rate 0.000510914
2017-10-10T14:47:09.480556: step 608, loss 0.112407, acc 0.953125, learning_rate 0.000509237
2017-10-10T14:47:09.824948: step 609, loss 0.218084, acc 0.90625, learning_rate 0.000507566
2017-10-10T14:47:10.087824: step 610, loss 0.179895, acc 0.953125, learning_rate 0.000505903
2017-10-10T14:47:10.385017: step 611, loss 0.139393, acc 0.9375, learning_rate 0.000504246
2017-10-10T14:47:10.699729: step 612, loss 0.261906, acc 0.890625, learning_rate 0.000502596
2017-10-10T14:47:10.959996: step 613, loss 0.20099, acc 0.953125, learning_rate 0.000500953
2017-10-10T14:47:11.243621: step 614, loss 0.281074, acc 0.890625, learning_rate 0.000499316
2017-10-10T14:47:11.566626: step 615, loss 0.236719, acc 0.9375, learning_rate 0.000497686
2017-10-10T14:47:11.851191: step 616, loss 0.135986, acc 0.953125, learning_rate 0.000496063
2017-10-10T14:47:12.156851: step 617, loss 0.264446, acc 0.875, learning_rate 0.000494446
2017-10-10T14:47:12.514370: step 618, loss 0.164921, acc 0.96875, learning_rate 0.000492836
2017-10-10T14:47:12.836854: step 619, loss 0.151267, acc 0.9375, learning_rate 0.000491233
2017-10-10T14:47:13.129780: step 620, loss 0.200987, acc 0.921875, learning_rate 0.000489636
2017-10-10T14:47:13.395316: step 621, loss 0.184328, acc 0.9375, learning_rate 0.000488045
2017-10-10T14:47:13.760311: step 622, loss 0.170136, acc 0.9375, learning_rate 0.000486461
2017-10-10T14:47:14.073544: step 623, loss 0.171646, acc 0.921875, learning_rate 0.000484884
2017-10-10T14:47:14.287489: step 624, loss 0.0788614, acc 0.984375, learning_rate 0.000483313
2017-10-10T14:47:14.528918: step 625, loss 0.115409, acc 0.953125, learning_rate 0.000481748
2017-10-10T14:47:14.837634: step 626, loss 0.238783, acc 0.890625, learning_rate 0.00048019
2017-10-10T14:47:15.119500: step 627, loss 0.237213, acc 0.890625, learning_rate 0.000478638
2017-10-10T14:47:15.390663: step 628, loss 0.157512, acc 0.9375, learning_rate 0.000477093
2017-10-10T14:47:15.656709: step 629, loss 0.107177, acc 0.953125, learning_rate 0.000475554
2017-10-10T14:47:15.997080: step 630, loss 0.136317, acc 0.984375, learning_rate 0.000474021
2017-10-10T14:47:16.288911: step 631, loss 0.183422, acc 0.921875, learning_rate 0.000472494
2017-10-10T14:47:16.569056: step 632, loss 0.175243, acc 0.9375, learning_rate 0.000470974
2017-10-10T14:47:16.904970: step 633, loss 0.222684, acc 0.9375, learning_rate 0.000469459
2017-10-10T14:47:17.208865: step 634, loss 0.140947, acc 0.96875, learning_rate 0.000467951
2017-10-10T14:47:17.517363: step 635, loss 0.342655, acc 0.90625, learning_rate 0.000466449
2017-10-10T14:47:17.814210: step 636, loss 0.118148, acc 0.984375, learning_rate 0.000464954
2017-10-10T14:47:18.169172: step 637, loss 0.292968, acc 0.921875, learning_rate 0.000463464
2017-10-10T14:47:18.465047: step 638, loss 0.0647631, acc 1, learning_rate 0.00046198
2017-10-10T14:47:18.784309: step 639, loss 0.168668, acc 0.9375, learning_rate 0.000460503
2017-10-10T14:47:19.112864: step 640, loss 0.182924, acc 0.921875, learning_rate 0.000459031

Evaluation:
2017-10-10T14:47:20.001854: step 640, loss 0.256012, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-640

2017-10-10T14:47:21.308695: step 641, loss 0.0975544, acc 0.96875, learning_rate 0.000457566
2017-10-10T14:47:21.581059: step 642, loss 0.168503, acc 0.953125, learning_rate 0.000456106
2017-10-10T14:47:21.847962: step 643, loss 0.21997, acc 0.921875, learning_rate 0.000454653
2017-10-10T14:47:22.155064: step 644, loss 0.200905, acc 0.921875, learning_rate 0.000453205
2017-10-10T14:47:22.454474: step 645, loss 0.288159, acc 0.875, learning_rate 0.000451764
2017-10-10T14:47:22.772309: step 646, loss 0.210273, acc 0.9375, learning_rate 0.000450328
2017-10-10T14:47:23.085222: step 647, loss 0.15004, acc 0.9375, learning_rate 0.000448898
2017-10-10T14:47:23.408952: step 648, loss 0.268835, acc 0.921875, learning_rate 0.000447474
2017-10-10T14:47:23.713611: step 649, loss 0.227803, acc 0.921875, learning_rate 0.000446055
2017-10-10T14:47:23.996856: step 650, loss 0.145993, acc 0.96875, learning_rate 0.000444643
2017-10-10T14:47:24.258340: step 651, loss 0.338962, acc 0.90625, learning_rate 0.000443236
2017-10-10T14:47:24.544084: step 652, loss 0.192353, acc 0.921875, learning_rate 0.000441835
2017-10-10T14:47:24.767824: step 653, loss 0.255994, acc 0.90625, learning_rate 0.00044044
2017-10-10T14:47:25.070367: step 654, loss 0.120153, acc 0.96875, learning_rate 0.00043905
2017-10-10T14:47:25.320257: step 655, loss 0.157452, acc 0.953125, learning_rate 0.000437666
2017-10-10T14:47:25.569876: step 656, loss 0.156667, acc 0.96875, learning_rate 0.000436288
2017-10-10T14:47:25.893178: step 657, loss 0.100256, acc 0.96875, learning_rate 0.000434915
2017-10-10T14:47:26.226960: step 658, loss 0.192357, acc 0.9375, learning_rate 0.000433548
2017-10-10T14:47:26.494287: step 659, loss 0.181274, acc 0.9375, learning_rate 0.000432187
2017-10-10T14:47:26.802032: step 660, loss 0.286654, acc 0.875, learning_rate 0.000430831
2017-10-10T14:47:27.120897: step 661, loss 0.140466, acc 0.984375, learning_rate 0.000429481
2017-10-10T14:47:27.416976: step 662, loss 0.134405, acc 0.984375, learning_rate 0.000428136
2017-10-10T14:47:27.725171: step 663, loss 0.227527, acc 0.9375, learning_rate 0.000426796
2017-10-10T14:47:28.021192: step 664, loss 0.26692, acc 0.875, learning_rate 0.000425463
2017-10-10T14:47:28.369143: step 665, loss 0.117546, acc 0.984375, learning_rate 0.000424134
2017-10-10T14:47:28.666394: step 666, loss 0.152568, acc 0.9375, learning_rate 0.000422811
2017-10-10T14:47:28.969004: step 667, loss 0.189575, acc 0.9375, learning_rate 0.000421493
2017-10-10T14:47:29.293130: step 668, loss 0.222364, acc 0.90625, learning_rate 0.000420181
2017-10-10T14:47:29.624903: step 669, loss 0.202071, acc 0.890625, learning_rate 0.000418874
2017-10-10T14:47:29.914396: step 670, loss 0.119127, acc 0.96875, learning_rate 0.000417573
2017-10-10T14:47:30.208966: step 671, loss 0.227046, acc 0.90625, learning_rate 0.000416276
2017-10-10T14:47:30.515760: step 672, loss 0.171743, acc 0.953125, learning_rate 0.000414985
2017-10-10T14:47:30.813119: step 673, loss 0.193293, acc 0.96875, learning_rate 0.0004137
2017-10-10T14:47:31.122603: step 674, loss 0.227283, acc 0.9375, learning_rate 0.000412419
2017-10-10T14:47:31.441177: step 675, loss 0.196804, acc 0.921875, learning_rate 0.000411144
2017-10-10T14:47:31.725291: step 676, loss 0.141522, acc 0.984375, learning_rate 0.000409874
2017-10-10T14:47:32.069224: step 677, loss 0.147422, acc 0.953125, learning_rate 0.000408609
2017-10-10T14:47:32.435052: step 678, loss 0.168275, acc 0.96875, learning_rate 0.00040735
2017-10-10T14:47:32.748976: step 679, loss 0.353659, acc 0.90625, learning_rate 0.000406095
2017-10-10T14:47:33.089118: step 680, loss 0.196882, acc 0.90625, learning_rate 0.000404846

Evaluation:
2017-10-10T14:47:33.560981: step 680, loss 0.253037, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-680

2017-10-10T14:47:34.496563: step 681, loss 0.158837, acc 0.9375, learning_rate 0.000403601
2017-10-10T14:47:34.758133: step 682, loss 0.151898, acc 0.953125, learning_rate 0.000402362
2017-10-10T14:47:35.034289: step 683, loss 0.140499, acc 0.96875, learning_rate 0.000401128
2017-10-10T14:47:35.372086: step 684, loss 0.196455, acc 0.96875, learning_rate 0.000399899
2017-10-10T14:47:35.685482: step 685, loss 0.15623, acc 0.953125, learning_rate 0.000398675
2017-10-10T14:47:35.912840: step 686, loss 0.160522, acc 0.921569, learning_rate 0.000397456
2017-10-10T14:47:36.213061: step 687, loss 0.137971, acc 0.96875, learning_rate 0.000396241
2017-10-10T14:47:36.544816: step 688, loss 0.103841, acc 0.96875, learning_rate 0.000395032
2017-10-10T14:47:36.791628: step 689, loss 0.142814, acc 0.9375, learning_rate 0.000393828
2017-10-10T14:47:37.081175: step 690, loss 0.133093, acc 0.921875, learning_rate 0.000392629
2017-10-10T14:47:37.341043: step 691, loss 0.187684, acc 0.921875, learning_rate 0.000391434
2017-10-10T14:47:37.655487: step 692, loss 0.151532, acc 0.953125, learning_rate 0.000390245
2017-10-10T14:47:38.024827: step 693, loss 0.221473, acc 0.90625, learning_rate 0.00038906
2017-10-10T14:47:38.385255: step 694, loss 0.138553, acc 0.953125, learning_rate 0.00038788
2017-10-10T14:47:38.642204: step 695, loss 0.156793, acc 0.953125, learning_rate 0.000386705
2017-10-10T14:47:38.924673: step 696, loss 0.194333, acc 0.90625, learning_rate 0.000385535
2017-10-10T14:47:39.177117: step 697, loss 0.108639, acc 0.953125, learning_rate 0.000384369
2017-10-10T14:47:39.501768: step 698, loss 0.25313, acc 0.953125, learning_rate 0.000383209
2017-10-10T14:47:39.809077: step 699, loss 0.249864, acc 0.921875, learning_rate 0.000382053
2017-10-10T14:47:40.101065: step 700, loss 0.284232, acc 0.90625, learning_rate 0.000380901
2017-10-10T14:47:40.414713: step 701, loss 0.113626, acc 0.96875, learning_rate 0.000379755
2017-10-10T14:47:40.670255: step 702, loss 0.191467, acc 0.953125, learning_rate 0.000378613
2017-10-10T14:47:40.956627: step 703, loss 0.252902, acc 0.90625, learning_rate 0.000377476
2017-10-10T14:47:41.208939: step 704, loss 0.145799, acc 0.96875, learning_rate 0.000376343
2017-10-10T14:47:41.584948: step 705, loss 0.181545, acc 0.953125, learning_rate 0.000375215
2017-10-10T14:47:41.895442: step 706, loss 0.257198, acc 0.9375, learning_rate 0.000374092
2017-10-10T14:47:42.221118: step 707, loss 0.192036, acc 0.9375, learning_rate 0.000372973
2017-10-10T14:47:42.541094: step 708, loss 0.164429, acc 0.96875, learning_rate 0.000371859
2017-10-10T14:47:42.874592: step 709, loss 0.138352, acc 0.96875, learning_rate 0.000370749
2017-10-10T14:47:43.174129: step 710, loss 0.185091, acc 0.921875, learning_rate 0.000369644
2017-10-10T14:47:43.431293: step 711, loss 0.188806, acc 0.953125, learning_rate 0.000368543
2017-10-10T14:47:43.779762: step 712, loss 0.251697, acc 0.875, learning_rate 0.000367447
2017-10-10T14:47:44.057007: step 713, loss 0.174802, acc 0.9375, learning_rate 0.000366356
2017-10-10T14:47:44.383863: step 714, loss 0.232371, acc 0.921875, learning_rate 0.000365268
2017-10-10T14:47:44.649099: step 715, loss 0.214745, acc 0.90625, learning_rate 0.000364186
2017-10-10T14:47:44.980594: step 716, loss 0.29315, acc 0.875, learning_rate 0.000363107
2017-10-10T14:47:45.325127: step 717, loss 0.146525, acc 0.9375, learning_rate 0.000362033
2017-10-10T14:47:45.593618: step 718, loss 0.282525, acc 0.828125, learning_rate 0.000360964
2017-10-10T14:47:45.852993: step 719, loss 0.163402, acc 0.9375, learning_rate 0.000359899
2017-10-10T14:47:46.164048: step 720, loss 0.134403, acc 0.9375, learning_rate 0.000358838

Evaluation:
2017-10-10T14:47:46.668987: step 720, loss 0.254415, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-720

2017-10-10T14:47:47.911108: step 721, loss 0.192748, acc 0.921875, learning_rate 0.000357781
2017-10-10T14:47:48.268916: step 722, loss 0.265144, acc 0.875, learning_rate 0.000356729
2017-10-10T14:47:48.510475: step 723, loss 0.276483, acc 0.875, learning_rate 0.000355681
2017-10-10T14:47:48.815910: step 724, loss 0.0665682, acc 0.984375, learning_rate 0.000354637
2017-10-10T14:47:49.106566: step 725, loss 0.298925, acc 0.890625, learning_rate 0.000353598
2017-10-10T14:47:49.444153: step 726, loss 0.169101, acc 0.953125, learning_rate 0.000352563
2017-10-10T14:47:49.728029: step 727, loss 0.228878, acc 0.953125, learning_rate 0.000351532
2017-10-10T14:47:50.032806: step 728, loss 0.246244, acc 0.953125, learning_rate 0.000350505
2017-10-10T14:47:50.359467: step 729, loss 0.162408, acc 0.9375, learning_rate 0.000349483
2017-10-10T14:47:50.664411: step 730, loss 0.125572, acc 0.953125, learning_rate 0.000348465
2017-10-10T14:47:50.984571: step 731, loss 0.16421, acc 0.96875, learning_rate 0.00034745
2017-10-10T14:47:51.241042: step 732, loss 0.122979, acc 0.96875, learning_rate 0.00034644
2017-10-10T14:47:51.556563: step 733, loss 0.222632, acc 0.9375, learning_rate 0.000345434
2017-10-10T14:47:51.884816: step 734, loss 0.144507, acc 0.9375, learning_rate 0.000344433
2017-10-10T14:47:52.217774: step 735, loss 0.140557, acc 0.953125, learning_rate 0.000343435
2017-10-10T14:47:52.512068: step 736, loss 0.148684, acc 0.96875, learning_rate 0.000342441
2017-10-10T14:47:52.805156: step 737, loss 0.11828, acc 0.984375, learning_rate 0.000341452
2017-10-10T14:47:53.100205: step 738, loss 0.170764, acc 0.953125, learning_rate 0.000340466
2017-10-10T14:47:53.361088: step 739, loss 0.126487, acc 0.96875, learning_rate 0.000339485
2017-10-10T14:47:53.653227: step 740, loss 0.210805, acc 0.9375, learning_rate 0.000338507
2017-10-10T14:47:53.966053: step 741, loss 0.179288, acc 0.9375, learning_rate 0.000337534
2017-10-10T14:47:54.236446: step 742, loss 0.150719, acc 0.921875, learning_rate 0.000336564
2017-10-10T14:47:54.499430: step 743, loss 0.289744, acc 0.90625, learning_rate 0.000335598
2017-10-10T14:47:54.812582: step 744, loss 0.214214, acc 0.9375, learning_rate 0.000334637
2017-10-10T14:47:55.113482: step 745, loss 0.2043, acc 0.9375, learning_rate 0.000333679
2017-10-10T14:47:55.402097: step 746, loss 0.116752, acc 0.96875, learning_rate 0.000332725
2017-10-10T14:47:55.669777: step 747, loss 0.172811, acc 0.953125, learning_rate 0.000331775
2017-10-10T14:47:55.995196: step 748, loss 0.123862, acc 0.96875, learning_rate 0.000330829
2017-10-10T14:47:56.313003: step 749, loss 0.141005, acc 0.96875, learning_rate 0.000329887
2017-10-10T14:47:56.645213: step 750, loss 0.1134, acc 0.96875, learning_rate 0.000328949
2017-10-10T14:47:56.917128: step 751, loss 0.292745, acc 0.890625, learning_rate 0.000328014
2017-10-10T14:47:57.202210: step 752, loss 0.0656683, acc 0.984375, learning_rate 0.000327083
2017-10-10T14:47:57.428535: step 753, loss 0.199467, acc 0.90625, learning_rate 0.000326157
2017-10-10T14:47:57.644928: step 754, loss 0.348065, acc 0.890625, learning_rate 0.000325233
2017-10-10T14:47:57.959523: step 755, loss 0.246528, acc 0.921875, learning_rate 0.000324314
2017-10-10T14:47:58.128851: step 756, loss 0.182363, acc 0.921875, learning_rate 0.000323399
2017-10-10T14:47:58.411883: step 757, loss 0.153345, acc 0.9375, learning_rate 0.000322487
2017-10-10T14:47:58.677017: step 758, loss 0.220081, acc 0.9375, learning_rate 0.000321579
2017-10-10T14:47:58.940934: step 759, loss 0.187078, acc 0.890625, learning_rate 0.000320674
2017-10-10T14:47:59.287912: step 760, loss 0.145147, acc 0.9375, learning_rate 0.000319773

Evaluation:
2017-10-10T14:47:59.812240: step 760, loss 0.258673, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-760

2017-10-10T14:48:00.988620: step 761, loss 0.15451, acc 0.9375, learning_rate 0.000318876
2017-10-10T14:48:01.284893: step 762, loss 0.0994386, acc 0.984375, learning_rate 0.000317983
2017-10-10T14:48:01.540726: step 763, loss 0.167773, acc 0.96875, learning_rate 0.000317093
2017-10-10T14:48:01.854786: step 764, loss 0.079694, acc 0.984375, learning_rate 0.000316207
2017-10-10T14:48:02.139520: step 765, loss 0.140241, acc 0.96875, learning_rate 0.000315325
2017-10-10T14:48:02.425079: step 766, loss 0.168013, acc 0.953125, learning_rate 0.000314446
2017-10-10T14:48:02.708680: step 767, loss 0.0696481, acc 0.984375, learning_rate 0.00031357
2017-10-10T14:48:03.009616: step 768, loss 0.174597, acc 0.9375, learning_rate 0.000312699
2017-10-10T14:48:03.289128: step 769, loss 0.23156, acc 0.890625, learning_rate 0.00031183
2017-10-10T14:48:03.560816: step 770, loss 0.330458, acc 0.875, learning_rate 0.000310966
2017-10-10T14:48:03.913988: step 771, loss 0.208961, acc 0.921875, learning_rate 0.000310105
2017-10-10T14:48:04.245630: step 772, loss 0.199375, acc 0.953125, learning_rate 0.000309247
2017-10-10T14:48:04.494568: step 773, loss 0.198936, acc 0.921875, learning_rate 0.000308393
2017-10-10T14:48:04.787572: step 774, loss 0.343068, acc 0.890625, learning_rate 0.000307542
2017-10-10T14:48:05.121223: step 775, loss 0.222197, acc 0.921875, learning_rate 0.000306695
2017-10-10T14:48:05.372369: step 776, loss 0.0719734, acc 0.96875, learning_rate 0.000305852
2017-10-10T14:48:05.708425: step 777, loss 0.143205, acc 0.953125, learning_rate 0.000305011
2017-10-10T14:48:05.967564: step 778, loss 0.11377, acc 0.96875, learning_rate 0.000304174
2017-10-10T14:48:06.320940: step 779, loss 0.21842, acc 0.9375, learning_rate 0.000303341
2017-10-10T14:48:06.624002: step 780, loss 0.153022, acc 0.9375, learning_rate 0.000302511
2017-10-10T14:48:06.921533: step 781, loss 0.166126, acc 0.953125, learning_rate 0.000301684
2017-10-10T14:48:07.210525: step 782, loss 0.275874, acc 0.90625, learning_rate 0.000300861
2017-10-10T14:48:07.516021: step 783, loss 0.0987634, acc 0.96875, learning_rate 0.000300041
2017-10-10T14:48:07.755415: step 784, loss 0.176167, acc 0.941176, learning_rate 0.000299225
2017-10-10T14:48:08.093375: step 785, loss 0.222547, acc 0.953125, learning_rate 0.000298412
2017-10-10T14:48:08.407109: step 786, loss 0.179668, acc 0.984375, learning_rate 0.000297602
2017-10-10T14:48:08.669103: step 787, loss 0.102643, acc 0.984375, learning_rate 0.000296795
2017-10-10T14:48:09.050687: step 788, loss 0.231449, acc 0.921875, learning_rate 0.000295992
2017-10-10T14:48:09.358022: step 789, loss 0.248752, acc 0.90625, learning_rate 0.000295192
2017-10-10T14:48:09.689253: step 790, loss 0.161167, acc 0.953125, learning_rate 0.000294395
2017-10-10T14:48:09.969037: step 791, loss 0.100515, acc 0.953125, learning_rate 0.000293602
2017-10-10T14:48:10.292821: step 792, loss 0.269738, acc 0.921875, learning_rate 0.000292812
2017-10-10T14:48:10.565710: step 793, loss 0.285114, acc 0.890625, learning_rate 0.000292025
2017-10-10T14:48:10.803990: step 794, loss 0.239799, acc 0.921875, learning_rate 0.000291241
2017-10-10T14:48:11.139082: step 795, loss 0.188861, acc 0.953125, learning_rate 0.00029046
2017-10-10T14:48:11.468859: step 796, loss 0.178871, acc 0.9375, learning_rate 0.000289683
2017-10-10T14:48:11.812317: step 797, loss 0.121015, acc 0.953125, learning_rate 0.000288908
2017-10-10T14:48:12.085274: step 798, loss 0.230991, acc 0.890625, learning_rate 0.000288137
2017-10-10T14:48:12.332024: step 799, loss 0.254619, acc 0.875, learning_rate 0.000287369
2017-10-10T14:48:12.586245: step 800, loss 0.149006, acc 0.953125, learning_rate 0.000286605

Evaluation:
2017-10-10T14:48:13.131064: step 800, loss 0.256716, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-800

2017-10-10T14:48:14.404890: step 801, loss 0.317222, acc 0.921875, learning_rate 0.000285843
2017-10-10T14:48:14.690590: step 802, loss 0.107084, acc 0.984375, learning_rate 0.000285084
2017-10-10T14:48:14.973160: step 803, loss 0.150024, acc 0.953125, learning_rate 0.000284329
2017-10-10T14:48:15.215978: step 804, loss 0.204311, acc 0.953125, learning_rate 0.000283577
2017-10-10T14:48:15.425637: step 805, loss 0.319106, acc 0.890625, learning_rate 0.000282827
2017-10-10T14:48:15.725090: step 806, loss 0.148466, acc 0.96875, learning_rate 0.000282081
2017-10-10T14:48:16.072896: step 807, loss 0.100188, acc 0.96875, learning_rate 0.000281338
2017-10-10T14:48:16.371760: step 808, loss 0.214389, acc 0.890625, learning_rate 0.000280598
2017-10-10T14:48:16.710750: step 809, loss 0.245336, acc 0.921875, learning_rate 0.00027986
2017-10-10T14:48:17.024951: step 810, loss 0.156271, acc 0.9375, learning_rate 0.000279126
2017-10-10T14:48:17.275824: step 811, loss 0.10267, acc 0.96875, learning_rate 0.000278395
2017-10-10T14:48:17.556169: step 812, loss 0.222807, acc 0.921875, learning_rate 0.000277667
2017-10-10T14:48:17.811311: step 813, loss 0.173995, acc 0.9375, learning_rate 0.000276942
2017-10-10T14:48:18.099077: step 814, loss 0.17048, acc 0.9375, learning_rate 0.00027622
2017-10-10T14:48:18.375847: step 815, loss 0.179313, acc 0.9375, learning_rate 0.0002755
2017-10-10T14:48:18.668974: step 816, loss 0.220611, acc 0.90625, learning_rate 0.000274784
2017-10-10T14:48:18.968122: step 817, loss 0.0889326, acc 0.984375, learning_rate 0.000274071
2017-10-10T14:48:19.293168: step 818, loss 0.177074, acc 0.921875, learning_rate 0.00027336
2017-10-10T14:48:19.625698: step 819, loss 0.199677, acc 0.9375, learning_rate 0.000272652
2017-10-10T14:48:19.923924: step 820, loss 0.265108, acc 0.859375, learning_rate 0.000271948
2017-10-10T14:48:20.242698: step 821, loss 0.303006, acc 0.890625, learning_rate 0.000271246
2017-10-10T14:48:20.523349: step 822, loss 0.183696, acc 0.90625, learning_rate 0.000270547
2017-10-10T14:48:20.820816: step 823, loss 0.224432, acc 0.9375, learning_rate 0.000269851
2017-10-10T14:48:21.113160: step 824, loss 0.138541, acc 0.984375, learning_rate 0.000269157
2017-10-10T14:48:21.435326: step 825, loss 0.127781, acc 0.953125, learning_rate 0.000268467
2017-10-10T14:48:21.788269: step 826, loss 0.342927, acc 0.890625, learning_rate 0.000267779
2017-10-10T14:48:22.103019: step 827, loss 0.274194, acc 0.875, learning_rate 0.000267094
2017-10-10T14:48:22.354776: step 828, loss 0.0995656, acc 0.953125, learning_rate 0.000266412
2017-10-10T14:48:22.577445: step 829, loss 0.10191, acc 0.96875, learning_rate 0.000265733
2017-10-10T14:48:22.882368: step 830, loss 0.169167, acc 0.953125, learning_rate 0.000265057
2017-10-10T14:48:23.185701: step 831, loss 0.283081, acc 0.9375, learning_rate 0.000264383
2017-10-10T14:48:23.460461: step 832, loss 0.0782852, acc 0.984375, learning_rate 0.000263712
2017-10-10T14:48:23.727768: step 833, loss 0.167453, acc 0.9375, learning_rate 0.000263044
2017-10-10T14:48:24.083462: step 834, loss 0.181054, acc 0.953125, learning_rate 0.000262378
2017-10-10T14:48:24.418427: step 835, loss 0.157311, acc 0.953125, learning_rate 0.000261715
2017-10-10T14:48:24.647279: step 836, loss 0.238746, acc 0.953125, learning_rate 0.000261055
2017-10-10T14:48:24.976834: step 837, loss 0.20189, acc 0.953125, learning_rate 0.000260398
2017-10-10T14:48:25.233626: step 838, loss 0.214527, acc 0.90625, learning_rate 0.000259743
2017-10-10T14:48:25.577160: step 839, loss 0.136509, acc 0.96875, learning_rate 0.000259091
2017-10-10T14:48:25.869821: step 840, loss 0.19746, acc 0.921875, learning_rate 0.000258442

Evaluation:
2017-10-10T14:48:26.399463: step 840, loss 0.251348, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-840

2017-10-10T14:48:27.325065: step 841, loss 0.182563, acc 0.9375, learning_rate 0.000257795
2017-10-10T14:48:27.660080: step 842, loss 0.255282, acc 0.921875, learning_rate 0.000257151
2017-10-10T14:48:28.031696: step 843, loss 0.219031, acc 0.953125, learning_rate 0.00025651
2017-10-10T14:48:28.320872: step 844, loss 0.131128, acc 0.953125, learning_rate 0.000255871
2017-10-10T14:48:28.584928: step 845, loss 0.0849603, acc 0.984375, learning_rate 0.000255235
2017-10-10T14:48:28.892953: step 846, loss 0.247697, acc 0.953125, learning_rate 0.000254601
2017-10-10T14:48:29.225932: step 847, loss 0.148804, acc 0.953125, learning_rate 0.00025397
2017-10-10T14:48:29.544910: step 848, loss 0.352508, acc 0.875, learning_rate 0.000253341
2017-10-10T14:48:29.796921: step 849, loss 0.0844921, acc 0.984375, learning_rate 0.000252716
2017-10-10T14:48:30.218466: step 850, loss 0.166396, acc 0.953125, learning_rate 0.000252092
2017-10-10T14:48:30.523012: step 851, loss 0.224233, acc 0.921875, learning_rate 0.000251471
2017-10-10T14:48:30.756453: step 852, loss 0.280342, acc 0.90625, learning_rate 0.000250853
2017-10-10T14:48:31.015174: step 853, loss 0.243416, acc 0.9375, learning_rate 0.000250237
2017-10-10T14:48:31.320911: step 854, loss 0.0636296, acc 1, learning_rate 0.000249624
2017-10-10T14:48:31.621427: step 855, loss 0.118877, acc 0.984375, learning_rate 0.000249013
2017-10-10T14:48:31.963626: step 856, loss 0.063488, acc 0.984375, learning_rate 0.000248405
2017-10-10T14:48:32.232828: step 857, loss 0.234193, acc 0.921875, learning_rate 0.000247799
2017-10-10T14:48:32.588970: step 858, loss 0.162614, acc 0.9375, learning_rate 0.000247196
2017-10-10T14:48:32.872263: step 859, loss 0.0527747, acc 0.984375, learning_rate 0.000246595
2017-10-10T14:48:33.141550: step 860, loss 0.134337, acc 0.953125, learning_rate 0.000245997
2017-10-10T14:48:33.400945: step 861, loss 0.220468, acc 0.890625, learning_rate 0.000245401
2017-10-10T14:48:33.633214: step 862, loss 0.230761, acc 0.890625, learning_rate 0.000244808
2017-10-10T14:48:33.922385: step 863, loss 0.160453, acc 0.953125, learning_rate 0.000244216
2017-10-10T14:48:34.243140: step 864, loss 0.260539, acc 0.90625, learning_rate 0.000243628
2017-10-10T14:48:34.516503: step 865, loss 0.161655, acc 0.921875, learning_rate 0.000243042
2017-10-10T14:48:34.724972: step 866, loss 0.210281, acc 0.9375, learning_rate 0.000242458
2017-10-10T14:48:35.041653: step 867, loss 0.152337, acc 0.9375, learning_rate 0.000241876
2017-10-10T14:48:35.333011: step 868, loss 0.11247, acc 0.96875, learning_rate 0.000241297
2017-10-10T14:48:35.611438: step 869, loss 0.0760526, acc 0.96875, learning_rate 0.00024072
2017-10-10T14:48:35.928241: step 870, loss 0.215788, acc 0.921875, learning_rate 0.000240146
2017-10-10T14:48:36.256852: step 871, loss 0.135763, acc 0.9375, learning_rate 0.000239574
2017-10-10T14:48:36.564877: step 872, loss 0.0972358, acc 0.96875, learning_rate 0.000239004
2017-10-10T14:48:36.902896: step 873, loss 0.21492, acc 0.90625, learning_rate 0.000238437
2017-10-10T14:48:37.248892: step 874, loss 0.0627471, acc 0.96875, learning_rate 0.000237872
2017-10-10T14:48:37.549249: step 875, loss 0.306667, acc 0.875, learning_rate 0.000237309
2017-10-10T14:48:37.828939: step 876, loss 0.122348, acc 0.953125, learning_rate 0.000236749
2017-10-10T14:48:38.080909: step 877, loss 0.142309, acc 0.953125, learning_rate 0.00023619
2017-10-10T14:48:38.420949: step 878, loss 0.179518, acc 0.9375, learning_rate 0.000235635
2017-10-10T14:48:38.706221: step 879, loss 0.180001, acc 0.9375, learning_rate 0.000235081
2017-10-10T14:48:38.964315: step 880, loss 0.214232, acc 0.9375, learning_rate 0.00023453

Evaluation:
2017-10-10T14:48:39.564893: step 880, loss 0.251166, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-880

2017-10-10T14:48:40.714575: step 881, loss 0.107176, acc 0.984375, learning_rate 0.00023398
2017-10-10T14:48:40.979860: step 882, loss 0.220856, acc 0.941176, learning_rate 0.000233434
2017-10-10T14:48:41.306864: step 883, loss 0.131744, acc 0.953125, learning_rate 0.000232889
2017-10-10T14:48:41.648942: step 884, loss 0.257311, acc 0.9375, learning_rate 0.000232346
2017-10-10T14:48:41.953058: step 885, loss 0.133885, acc 0.984375, learning_rate 0.000231806
2017-10-10T14:48:42.201415: step 886, loss 0.125161, acc 0.96875, learning_rate 0.000231268
2017-10-10T14:48:42.496537: step 887, loss 0.082156, acc 1, learning_rate 0.000230732
2017-10-10T14:48:42.736919: step 888, loss 0.281988, acc 0.9375, learning_rate 0.000230199
2017-10-10T14:48:43.055847: step 889, loss 0.193796, acc 0.90625, learning_rate 0.000229667
2017-10-10T14:48:43.356457: step 890, loss 0.120001, acc 0.96875, learning_rate 0.000229138
2017-10-10T14:48:43.651949: step 891, loss 0.142072, acc 0.9375, learning_rate 0.000228611
2017-10-10T14:48:43.996974: step 892, loss 0.428172, acc 0.84375, learning_rate 0.000228086
2017-10-10T14:48:44.299968: step 893, loss 0.205453, acc 0.953125, learning_rate 0.000227563
2017-10-10T14:48:44.621768: step 894, loss 0.132917, acc 0.96875, learning_rate 0.000227043
2017-10-10T14:48:44.929421: step 895, loss 0.159521, acc 0.953125, learning_rate 0.000226524
2017-10-10T14:48:45.194891: step 896, loss 0.0859875, acc 0.96875, learning_rate 0.000226008
2017-10-10T14:48:45.492066: step 897, loss 0.226367, acc 0.921875, learning_rate 0.000225493
2017-10-10T14:48:45.816905: step 898, loss 0.128842, acc 0.953125, learning_rate 0.000224981
2017-10-10T14:48:46.134019: step 899, loss 0.302333, acc 0.890625, learning_rate 0.000224471
2017-10-10T14:48:46.581193: step 900, loss 0.200841, acc 0.953125, learning_rate 0.000223963
2017-10-10T14:48:46.888480: step 901, loss 0.110289, acc 0.96875, learning_rate 0.000223457
2017-10-10T14:48:47.078355: step 902, loss 0.11257, acc 0.96875, learning_rate 0.000222953
2017-10-10T14:48:47.293172: step 903, loss 0.204978, acc 0.90625, learning_rate 0.000222451
2017-10-10T14:48:47.572078: step 904, loss 0.195859, acc 0.9375, learning_rate 0.000221951
2017-10-10T14:48:47.792941: step 905, loss 0.165517, acc 0.953125, learning_rate 0.000221453
2017-10-10T14:48:48.085317: step 906, loss 0.158336, acc 0.921875, learning_rate 0.000220958
2017-10-10T14:48:48.368981: step 907, loss 0.161426, acc 0.9375, learning_rate 0.000220464
2017-10-10T14:48:48.688991: step 908, loss 0.176842, acc 0.90625, learning_rate 0.000219972
2017-10-10T14:48:49.085533: step 909, loss 0.134111, acc 0.96875, learning_rate 0.000219483
2017-10-10T14:48:49.359145: step 910, loss 0.205036, acc 0.953125, learning_rate 0.000218995
2017-10-10T14:48:49.697008: step 911, loss 0.185096, acc 0.953125, learning_rate 0.000218509
2017-10-10T14:48:50.002562: step 912, loss 0.149179, acc 0.921875, learning_rate 0.000218025
2017-10-10T14:48:50.309056: step 913, loss 0.158428, acc 0.953125, learning_rate 0.000217544
2017-10-10T14:48:50.682376: step 914, loss 0.186599, acc 0.9375, learning_rate 0.000217064
2017-10-10T14:48:51.028359: step 915, loss 0.161559, acc 0.921875, learning_rate 0.000216586
2017-10-10T14:48:51.331029: step 916, loss 0.145918, acc 0.96875, learning_rate 0.00021611
2017-10-10T14:48:51.594069: step 917, loss 0.182451, acc 0.9375, learning_rate 0.000215636
2017-10-10T14:48:51.812919: step 918, loss 0.11192, acc 0.953125, learning_rate 0.000215164
2017-10-10T14:48:52.059939: step 919, loss 0.133487, acc 0.96875, learning_rate 0.000214694
2017-10-10T14:48:52.372900: step 920, loss 0.140894, acc 0.96875, learning_rate 0.000214226

Evaluation:
2017-10-10T14:48:52.879238: step 920, loss 0.251648, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-920

2017-10-10T14:48:53.896367: step 921, loss 0.16694, acc 0.9375, learning_rate 0.00021376
2017-10-10T14:48:54.181475: step 922, loss 0.411451, acc 0.90625, learning_rate 0.000213295
2017-10-10T14:48:54.499725: step 923, loss 0.211631, acc 0.90625, learning_rate 0.000212833
2017-10-10T14:48:54.812034: step 924, loss 0.157123, acc 0.9375, learning_rate 0.000212372
2017-10-10T14:48:55.135836: step 925, loss 0.146161, acc 0.953125, learning_rate 0.000211914
2017-10-10T14:48:55.401173: step 926, loss 0.088143, acc 0.984375, learning_rate 0.000211457
2017-10-10T14:48:55.720978: step 927, loss 0.127551, acc 0.921875, learning_rate 0.000211002
2017-10-10T14:48:56.048967: step 928, loss 0.145083, acc 0.9375, learning_rate 0.000210549
2017-10-10T14:48:56.408358: step 929, loss 0.172833, acc 0.953125, learning_rate 0.000210098
2017-10-10T14:48:56.746696: step 930, loss 0.143292, acc 0.953125, learning_rate 0.000209648
2017-10-10T14:48:57.178094: step 931, loss 0.240691, acc 0.90625, learning_rate 0.000209201
2017-10-10T14:48:57.433073: step 932, loss 0.172731, acc 0.953125, learning_rate 0.000208755
2017-10-10T14:48:57.728851: step 933, loss 0.178414, acc 0.9375, learning_rate 0.000208311
2017-10-10T14:48:58.037813: step 934, loss 0.226206, acc 0.90625, learning_rate 0.000207869
2017-10-10T14:48:58.350667: step 935, loss 0.216594, acc 0.9375, learning_rate 0.000207429
2017-10-10T14:48:58.599455: step 936, loss 0.238465, acc 0.90625, learning_rate 0.00020699
2017-10-10T14:48:58.973122: step 937, loss 0.0380345, acc 1, learning_rate 0.000206554
2017-10-10T14:48:59.264916: step 938, loss 0.101587, acc 0.953125, learning_rate 0.000206119
2017-10-10T14:48:59.537053: step 939, loss 0.245883, acc 0.9375, learning_rate 0.000205685
2017-10-10T14:48:59.791450: step 940, loss 0.263414, acc 0.90625, learning_rate 0.000205254
2017-10-10T14:49:00.093366: step 941, loss 0.138038, acc 0.953125, learning_rate 0.000204824
2017-10-10T14:49:00.351780: step 942, loss 0.239096, acc 0.9375, learning_rate 0.000204397
2017-10-10T14:49:00.678289: step 943, loss 0.153058, acc 0.953125, learning_rate 0.00020397
2017-10-10T14:49:01.039977: step 944, loss 0.206638, acc 0.90625, learning_rate 0.000203546
2017-10-10T14:49:01.319114: step 945, loss 0.174501, acc 0.953125, learning_rate 0.000203123
2017-10-10T14:49:01.587745: step 946, loss 0.254347, acc 0.890625, learning_rate 0.000202702
2017-10-10T14:49:01.897180: step 947, loss 0.144085, acc 0.984375, learning_rate 0.000202283
2017-10-10T14:49:02.207773: step 948, loss 0.0726657, acc 0.984375, learning_rate 0.000201866
2017-10-10T14:49:02.544315: step 949, loss 0.122953, acc 0.96875, learning_rate 0.00020145
2017-10-10T14:49:02.845697: step 950, loss 0.188839, acc 0.953125, learning_rate 0.000201036
2017-10-10T14:49:03.144966: step 951, loss 0.174007, acc 0.921875, learning_rate 0.000200623
2017-10-10T14:49:03.460818: step 952, loss 0.0995461, acc 0.984375, learning_rate 0.000200213
2017-10-10T14:49:03.738075: step 953, loss 0.171768, acc 0.9375, learning_rate 0.000199804
2017-10-10T14:49:04.032259: step 954, loss 0.187514, acc 0.96875, learning_rate 0.000199396
2017-10-10T14:49:04.334656: step 955, loss 0.134276, acc 0.9375, learning_rate 0.000198991
2017-10-10T14:49:04.558760: step 956, loss 0.17065, acc 0.96875, learning_rate 0.000198587
2017-10-10T14:49:04.868883: step 957, loss 0.181226, acc 0.9375, learning_rate 0.000198184
2017-10-10T14:49:05.144819: step 958, loss 0.121785, acc 0.984375, learning_rate 0.000197783
2017-10-10T14:49:05.464150: step 959, loss 0.176839, acc 0.921875, learning_rate 0.000197384
2017-10-10T14:49:05.802058: step 960, loss 0.316441, acc 0.953125, learning_rate 0.000196987

Evaluation:
2017-10-10T14:49:06.336969: step 960, loss 0.251142, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-960

2017-10-10T14:49:07.331701: step 961, loss 0.149469, acc 0.953125, learning_rate 0.000196591
2017-10-10T14:49:07.651106: step 962, loss 0.177716, acc 0.9375, learning_rate 0.000196197
2017-10-10T14:49:07.958496: step 963, loss 0.146777, acc 0.953125, learning_rate 0.000195804
2017-10-10T14:49:08.256815: step 964, loss 0.231352, acc 0.9375, learning_rate 0.000195413
2017-10-10T14:49:08.580062: step 965, loss 0.199621, acc 0.9375, learning_rate 0.000195023
2017-10-10T14:49:08.937898: step 966, loss 0.150481, acc 0.96875, learning_rate 0.000194636
2017-10-10T14:49:09.259920: step 967, loss 0.144591, acc 0.953125, learning_rate 0.000194249
2017-10-10T14:49:09.588421: step 968, loss 0.189958, acc 0.953125, learning_rate 0.000193865
2017-10-10T14:49:09.853618: step 969, loss 0.202606, acc 0.9375, learning_rate 0.000193482
2017-10-10T14:49:10.113233: step 970, loss 0.201019, acc 0.9375, learning_rate 0.0001931
2017-10-10T14:49:10.400938: step 971, loss 0.137889, acc 0.953125, learning_rate 0.00019272
2017-10-10T14:49:10.684195: step 972, loss 0.11957, acc 0.96875, learning_rate 0.000192341
2017-10-10T14:49:10.996988: step 973, loss 0.19457, acc 0.9375, learning_rate 0.000191965
2017-10-10T14:49:11.349038: step 974, loss 0.151668, acc 0.9375, learning_rate 0.000191589
2017-10-10T14:49:11.612682: step 975, loss 0.12933, acc 0.9375, learning_rate 0.000191215
2017-10-10T14:49:11.884329: step 976, loss 0.164843, acc 0.96875, learning_rate 0.000190843
2017-10-10T14:49:12.222057: step 977, loss 0.0818899, acc 0.96875, learning_rate 0.000190472
2017-10-10T14:49:12.559466: step 978, loss 0.158596, acc 0.953125, learning_rate 0.000190103
2017-10-10T14:49:12.828912: step 979, loss 0.121637, acc 0.96875, learning_rate 0.000189735
2017-10-10T14:49:13.130525: step 980, loss 0.150745, acc 0.960784, learning_rate 0.000189369
2017-10-10T14:49:13.388828: step 981, loss 0.180216, acc 0.921875, learning_rate 0.000189004
2017-10-10T14:49:13.692284: step 982, loss 0.240134, acc 0.921875, learning_rate 0.000188641
2017-10-10T14:49:13.986812: step 983, loss 0.108777, acc 0.96875, learning_rate 0.000188279
2017-10-10T14:49:14.250586: step 984, loss 0.107458, acc 0.984375, learning_rate 0.000187919
2017-10-10T14:49:14.555554: step 985, loss 0.0957662, acc 0.984375, learning_rate 0.00018756
2017-10-10T14:49:14.901977: step 986, loss 0.150999, acc 0.9375, learning_rate 0.000187202
2017-10-10T14:49:15.208911: step 987, loss 0.193952, acc 0.921875, learning_rate 0.000186846
2017-10-10T14:49:15.492924: step 988, loss 0.169019, acc 0.9375, learning_rate 0.000186492
2017-10-10T14:49:15.813867: step 989, loss 0.137215, acc 0.953125, learning_rate 0.000186139
2017-10-10T14:49:16.091042: step 990, loss 0.168517, acc 0.9375, learning_rate 0.000185787
2017-10-10T14:49:16.395270: step 991, loss 0.223677, acc 0.921875, learning_rate 0.000185437
2017-10-10T14:49:16.711794: step 992, loss 0.249849, acc 0.921875, learning_rate 0.000185088
2017-10-10T14:49:16.985463: step 993, loss 0.156505, acc 0.96875, learning_rate 0.000184741
2017-10-10T14:49:17.301666: step 994, loss 0.118908, acc 0.96875, learning_rate 0.000184395
2017-10-10T14:49:17.646231: step 995, loss 0.217871, acc 0.9375, learning_rate 0.000184051
2017-10-10T14:49:17.949292: step 996, loss 0.130644, acc 0.953125, learning_rate 0.000183708
2017-10-10T14:49:18.229258: step 997, loss 0.193091, acc 0.9375, learning_rate 0.000183366
2017-10-10T14:49:18.540962: step 998, loss 0.254122, acc 0.921875, learning_rate 0.000183026
2017-10-10T14:49:18.819314: step 999, loss 0.209356, acc 0.921875, learning_rate 0.000182687
2017-10-10T14:49:19.110901: step 1000, loss 0.10716, acc 0.96875, learning_rate 0.000182349

Evaluation:
2017-10-10T14:49:19.938545: step 1000, loss 0.249222, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1000

2017-10-10T14:49:20.972910: step 1001, loss 0.0647615, acc 1, learning_rate 0.000182013
2017-10-10T14:49:21.200504: step 1002, loss 0.140562, acc 0.953125, learning_rate 0.000181678
2017-10-10T14:49:21.528782: step 1003, loss 0.179843, acc 0.9375, learning_rate 0.000181345
2017-10-10T14:49:21.853330: step 1004, loss 0.143308, acc 0.953125, learning_rate 0.000181013
2017-10-10T14:49:22.113235: step 1005, loss 0.145811, acc 0.96875, learning_rate 0.000180682
2017-10-10T14:49:22.414825: step 1006, loss 0.203811, acc 0.9375, learning_rate 0.000180353
2017-10-10T14:49:22.703071: step 1007, loss 0.257997, acc 0.90625, learning_rate 0.000180025
2017-10-10T14:49:22.992842: step 1008, loss 0.182003, acc 0.953125, learning_rate 0.000179698
2017-10-10T14:49:23.332823: step 1009, loss 0.074319, acc 0.984375, learning_rate 0.000179373
2017-10-10T14:49:23.642953: step 1010, loss 0.153873, acc 0.953125, learning_rate 0.000179049
2017-10-10T14:49:23.921371: step 1011, loss 0.13643, acc 0.953125, learning_rate 0.000178726
2017-10-10T14:49:24.194568: step 1012, loss 0.102866, acc 0.96875, learning_rate 0.000178405
2017-10-10T14:49:24.487185: step 1013, loss 0.163922, acc 0.953125, learning_rate 0.000178085
2017-10-10T14:49:24.785392: step 1014, loss 0.135862, acc 0.9375, learning_rate 0.000177766
2017-10-10T14:49:25.072376: step 1015, loss 0.198024, acc 0.9375, learning_rate 0.000177449
2017-10-10T14:49:25.408904: step 1016, loss 0.178863, acc 0.953125, learning_rate 0.000177133
2017-10-10T14:49:25.729965: step 1017, loss 0.111968, acc 0.96875, learning_rate 0.000176818
2017-10-10T14:49:26.032955: step 1018, loss 0.1256, acc 0.96875, learning_rate 0.000176504
2017-10-10T14:49:26.342675: step 1019, loss 0.121294, acc 0.984375, learning_rate 0.000176192
2017-10-10T14:49:26.596022: step 1020, loss 0.12027, acc 0.984375, learning_rate 0.000175881
2017-10-10T14:49:26.972862: step 1021, loss 0.135932, acc 0.96875, learning_rate 0.000175571
2017-10-10T14:49:27.313090: step 1022, loss 0.217608, acc 0.90625, learning_rate 0.000175263
2017-10-10T14:49:27.570156: step 1023, loss 0.101418, acc 0.96875, learning_rate 0.000174956
2017-10-10T14:49:27.720604: step 1024, loss 0.112196, acc 0.96875, learning_rate 0.00017465
2017-10-10T14:49:27.989176: step 1025, loss 0.142794, acc 0.9375, learning_rate 0.000174345
2017-10-10T14:49:28.299234: step 1026, loss 0.0972488, acc 0.984375, learning_rate 0.000174042
2017-10-10T14:49:28.540823: step 1027, loss 0.138739, acc 0.96875, learning_rate 0.000173739
2017-10-10T14:49:28.837427: step 1028, loss 0.127433, acc 0.953125, learning_rate 0.000173438
2017-10-10T14:49:29.143343: step 1029, loss 0.0985016, acc 0.953125, learning_rate 0.000173139
2017-10-10T14:49:29.524571: step 1030, loss 0.263342, acc 0.90625, learning_rate 0.00017284
2017-10-10T14:49:29.862806: step 1031, loss 0.195229, acc 0.921875, learning_rate 0.000172543
2017-10-10T14:49:30.214996: step 1032, loss 0.195034, acc 0.9375, learning_rate 0.000172247
2017-10-10T14:49:30.549367: step 1033, loss 0.135372, acc 0.953125, learning_rate 0.000171952
2017-10-10T14:49:30.828974: step 1034, loss 0.0990819, acc 0.984375, learning_rate 0.000171658
2017-10-10T14:49:31.093140: step 1035, loss 0.300595, acc 0.890625, learning_rate 0.000171366
2017-10-10T14:49:31.407035: step 1036, loss 0.210102, acc 0.953125, learning_rate 0.000171074
2017-10-10T14:49:31.700908: step 1037, loss 0.166667, acc 0.953125, learning_rate 0.000170784
2017-10-10T14:49:32.016870: step 1038, loss 0.17331, acc 0.9375, learning_rate 0.000170495
2017-10-10T14:49:32.318453: step 1039, loss 0.189426, acc 0.9375, learning_rate 0.000170208
2017-10-10T14:49:32.608916: step 1040, loss 0.275481, acc 0.90625, learning_rate 0.000169921

Evaluation:
2017-10-10T14:49:33.176835: step 1040, loss 0.25078, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1040

2017-10-10T14:49:34.380886: step 1041, loss 0.124437, acc 0.953125, learning_rate 0.000169636
2017-10-10T14:49:34.676817: step 1042, loss 0.113002, acc 0.953125, learning_rate 0.000169351
2017-10-10T14:49:35.016675: step 1043, loss 0.191599, acc 0.96875, learning_rate 0.000169068
2017-10-10T14:49:35.296999: step 1044, loss 0.100354, acc 0.984375, learning_rate 0.000168786
2017-10-10T14:49:35.644249: step 1045, loss 0.195214, acc 0.96875, learning_rate 0.000168506
2017-10-10T14:49:35.911465: step 1046, loss 0.189357, acc 0.953125, learning_rate 0.000168226
2017-10-10T14:49:36.172371: step 1047, loss 0.181752, acc 0.953125, learning_rate 0.000167947
2017-10-10T14:49:36.470638: step 1048, loss 0.150945, acc 0.921875, learning_rate 0.00016767
2017-10-10T14:49:36.820326: step 1049, loss 0.28247, acc 0.921875, learning_rate 0.000167394
2017-10-10T14:49:37.075608: step 1050, loss 0.137998, acc 0.96875, learning_rate 0.000167119
2017-10-10T14:49:37.311109: step 1051, loss 0.132101, acc 0.984375, learning_rate 0.000166845
2017-10-10T14:49:37.569647: step 1052, loss 0.135735, acc 0.96875, learning_rate 0.000166572
2017-10-10T14:49:37.842072: step 1053, loss 0.207805, acc 0.890625, learning_rate 0.0001663
2017-10-10T14:49:38.008632: step 1054, loss 0.0669255, acc 1, learning_rate 0.00016603
2017-10-10T14:49:38.284449: step 1055, loss 0.120083, acc 0.9375, learning_rate 0.00016576
2017-10-10T14:49:38.536874: step 1056, loss 0.162681, acc 0.96875, learning_rate 0.000165492
2017-10-10T14:49:38.861096: step 1057, loss 0.116995, acc 0.984375, learning_rate 0.000165224
2017-10-10T14:49:39.172918: step 1058, loss 0.17198, acc 0.921875, learning_rate 0.000164958
2017-10-10T14:49:39.481111: step 1059, loss 0.172666, acc 0.953125, learning_rate 0.000164693
2017-10-10T14:49:39.740745: step 1060, loss 0.182033, acc 0.9375, learning_rate 0.000164429
2017-10-10T14:49:40.025912: step 1061, loss 0.263754, acc 0.9375, learning_rate 0.000164166
2017-10-10T14:49:40.354662: step 1062, loss 0.134802, acc 0.953125, learning_rate 0.000163904
2017-10-10T14:49:40.703958: step 1063, loss 0.240857, acc 0.90625, learning_rate 0.000163643
2017-10-10T14:49:40.957191: step 1064, loss 0.0981553, acc 0.984375, learning_rate 0.000163383
2017-10-10T14:49:41.233010: step 1065, loss 0.0964373, acc 0.96875, learning_rate 0.000163125
2017-10-10T14:49:41.527425: step 1066, loss 0.122849, acc 0.984375, learning_rate 0.000162867
2017-10-10T14:49:41.793280: step 1067, loss 0.176799, acc 0.9375, learning_rate 0.00016261
2017-10-10T14:49:42.045117: step 1068, loss 0.203967, acc 0.9375, learning_rate 0.000162355
2017-10-10T14:49:42.334719: step 1069, loss 0.060002, acc 0.984375, learning_rate 0.0001621
2017-10-10T14:49:42.673838: step 1070, loss 0.180919, acc 0.921875, learning_rate 0.000161847
2017-10-10T14:49:42.962402: step 1071, loss 0.154559, acc 0.96875, learning_rate 0.000161594
2017-10-10T14:49:43.228860: step 1072, loss 0.151703, acc 0.96875, learning_rate 0.000161343
2017-10-10T14:49:43.543548: step 1073, loss 0.169536, acc 0.9375, learning_rate 0.000161093
2017-10-10T14:49:43.848414: step 1074, loss 0.165787, acc 0.953125, learning_rate 0.000160843
2017-10-10T14:49:44.144987: step 1075, loss 0.209798, acc 0.9375, learning_rate 0.000160595
2017-10-10T14:49:44.441644: step 1076, loss 0.151101, acc 0.9375, learning_rate 0.000160348
2017-10-10T14:49:44.728968: step 1077, loss 0.0987657, acc 0.953125, learning_rate 0.000160101
2017-10-10T14:49:44.960111: step 1078, loss 0.210205, acc 0.941176, learning_rate 0.000159856
2017-10-10T14:49:45.278930: step 1079, loss 0.115527, acc 0.984375, learning_rate 0.000159612
2017-10-10T14:49:45.625899: step 1080, loss 0.113843, acc 0.96875, learning_rate 0.000159368

Evaluation:
2017-10-10T14:49:46.262544: step 1080, loss 0.247211, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1080

2017-10-10T14:49:47.409408: step 1081, loss 0.134858, acc 0.96875, learning_rate 0.000159126
2017-10-10T14:49:47.648824: step 1082, loss 0.105371, acc 0.984375, learning_rate 0.000158885
2017-10-10T14:49:47.973081: step 1083, loss 0.174626, acc 0.90625, learning_rate 0.000158644
2017-10-10T14:49:48.248887: step 1084, loss 0.0748843, acc 0.984375, learning_rate 0.000158405
2017-10-10T14:49:48.610494: step 1085, loss 0.135538, acc 0.984375, learning_rate 0.000158167
2017-10-10T14:49:48.888849: step 1086, loss 0.100197, acc 0.96875, learning_rate 0.000157929
2017-10-10T14:49:49.182861: step 1087, loss 0.121229, acc 0.953125, learning_rate 0.000157693
2017-10-10T14:49:49.461226: step 1088, loss 0.186703, acc 0.953125, learning_rate 0.000157457
2017-10-10T14:49:49.698439: step 1089, loss 0.170051, acc 0.96875, learning_rate 0.000157223
2017-10-10T14:49:49.971778: step 1090, loss 0.308694, acc 0.921875, learning_rate 0.000156989
2017-10-10T14:49:50.282515: step 1091, loss 0.219086, acc 0.953125, learning_rate 0.000156757
2017-10-10T14:49:50.585192: step 1092, loss 0.314819, acc 0.90625, learning_rate 0.000156525
2017-10-10T14:49:50.896876: step 1093, loss 0.122016, acc 0.96875, learning_rate 0.000156294
2017-10-10T14:49:51.177035: step 1094, loss 0.230256, acc 0.90625, learning_rate 0.000156064
2017-10-10T14:49:51.428891: step 1095, loss 0.165178, acc 0.921875, learning_rate 0.000155836
2017-10-10T14:49:51.743594: step 1096, loss 0.178166, acc 0.9375, learning_rate 0.000155608
2017-10-10T14:49:52.118478: step 1097, loss 0.0830009, acc 0.96875, learning_rate 0.000155381
2017-10-10T14:49:52.449147: step 1098, loss 0.244198, acc 0.90625, learning_rate 0.000155155
2017-10-10T14:49:52.764837: step 1099, loss 0.11723, acc 0.96875, learning_rate 0.000154929
2017-10-10T14:49:53.028620: step 1100, loss 0.332871, acc 0.921875, learning_rate 0.000154705
2017-10-10T14:49:53.368805: step 1101, loss 0.20214, acc 0.9375, learning_rate 0.000154482
2017-10-10T14:49:53.721055: step 1102, loss 0.161188, acc 0.953125, learning_rate 0.00015426
2017-10-10T14:49:54.088829: step 1103, loss 0.336891, acc 0.890625, learning_rate 0.000154038
2017-10-10T14:49:54.325077: step 1104, loss 0.143199, acc 0.953125, learning_rate 0.000153818
2017-10-10T14:49:54.581016: step 1105, loss 0.144807, acc 0.9375, learning_rate 0.000153598
2017-10-10T14:49:54.867769: step 1106, loss 0.225966, acc 0.921875, learning_rate 0.000153379
2017-10-10T14:49:55.223025: step 1107, loss 0.218357, acc 0.90625, learning_rate 0.000153161
2017-10-10T14:49:55.576976: step 1108, loss 0.148442, acc 0.953125, learning_rate 0.000152944
2017-10-10T14:49:55.918659: step 1109, loss 0.0817488, acc 0.96875, learning_rate 0.000152728
2017-10-10T14:49:56.232937: step 1110, loss 0.0874947, acc 1, learning_rate 0.000152513
2017-10-10T14:49:56.508956: step 1111, loss 0.114072, acc 0.96875, learning_rate 0.000152299
2017-10-10T14:49:56.848900: step 1112, loss 0.206469, acc 0.90625, learning_rate 0.000152085
2017-10-10T14:49:57.168738: step 1113, loss 0.127725, acc 0.953125, learning_rate 0.000151872
2017-10-10T14:49:57.457030: step 1114, loss 0.224259, acc 0.921875, learning_rate 0.000151661
2017-10-10T14:49:57.755307: step 1115, loss 0.179467, acc 0.9375, learning_rate 0.00015145
2017-10-10T14:49:58.082098: step 1116, loss 0.189073, acc 0.9375, learning_rate 0.00015124
2017-10-10T14:49:58.384951: step 1117, loss 0.117095, acc 0.9375, learning_rate 0.000151031
2017-10-10T14:49:58.632210: step 1118, loss 0.21067, acc 0.90625, learning_rate 0.000150822
2017-10-10T14:49:58.956998: step 1119, loss 0.194396, acc 0.921875, learning_rate 0.000150615
2017-10-10T14:49:59.294079: step 1120, loss 0.225253, acc 0.90625, learning_rate 0.000150408

Evaluation:
2017-10-10T14:49:59.787812: step 1120, loss 0.248476, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1120

2017-10-10T14:50:00.747444: step 1121, loss 0.139625, acc 0.96875, learning_rate 0.000150203
2017-10-10T14:50:01.028138: step 1122, loss 0.0631005, acc 1, learning_rate 0.000149998
2017-10-10T14:50:01.322148: step 1123, loss 0.075236, acc 0.984375, learning_rate 0.000149794
2017-10-10T14:50:01.648820: step 1124, loss 0.24913, acc 0.921875, learning_rate 0.00014959
2017-10-10T14:50:01.980938: step 1125, loss 0.120807, acc 0.96875, learning_rate 0.000149388
2017-10-10T14:50:02.232821: step 1126, loss 0.217378, acc 0.953125, learning_rate 0.000149186
2017-10-10T14:50:02.535575: step 1127, loss 0.190068, acc 0.96875, learning_rate 0.000148986
2017-10-10T14:50:02.814107: step 1128, loss 0.238174, acc 0.890625, learning_rate 0.000148786
2017-10-10T14:50:03.184862: step 1129, loss 0.275607, acc 0.875, learning_rate 0.000148587
2017-10-10T14:50:03.508837: step 1130, loss 0.161956, acc 0.953125, learning_rate 0.000148388
2017-10-10T14:50:03.853076: step 1131, loss 0.215595, acc 0.90625, learning_rate 0.000148191
2017-10-10T14:50:04.167702: step 1132, loss 0.278733, acc 0.9375, learning_rate 0.000147994
2017-10-10T14:50:04.433030: step 1133, loss 0.100382, acc 0.96875, learning_rate 0.000147798
2017-10-10T14:50:04.714288: step 1134, loss 0.206231, acc 0.921875, learning_rate 0.000147603
2017-10-10T14:50:05.015691: step 1135, loss 0.116431, acc 0.984375, learning_rate 0.000147409
2017-10-10T14:50:05.326246: step 1136, loss 0.158563, acc 0.921875, learning_rate 0.000147215
2017-10-10T14:50:05.664851: step 1137, loss 0.171365, acc 0.9375, learning_rate 0.000147022
2017-10-10T14:50:05.983728: step 1138, loss 0.138915, acc 0.953125, learning_rate 0.000146831
2017-10-10T14:50:06.280768: step 1139, loss 0.173235, acc 0.953125, learning_rate 0.000146639
2017-10-10T14:50:06.568782: step 1140, loss 0.240521, acc 0.921875, learning_rate 0.000146449
2017-10-10T14:50:06.860803: step 1141, loss 0.210664, acc 0.890625, learning_rate 0.000146259
2017-10-10T14:50:07.174146: step 1142, loss 0.115286, acc 0.96875, learning_rate 0.000146071
2017-10-10T14:50:07.526687: step 1143, loss 0.150501, acc 0.921875, learning_rate 0.000145883
2017-10-10T14:50:07.845675: step 1144, loss 0.126932, acc 0.9375, learning_rate 0.000145695
2017-10-10T14:50:08.128919: step 1145, loss 0.32195, acc 0.890625, learning_rate 0.000145509
2017-10-10T14:50:08.414474: step 1146, loss 0.131868, acc 0.953125, learning_rate 0.000145323
2017-10-10T14:50:08.737684: step 1147, loss 0.259615, acc 0.921875, learning_rate 0.000145138
2017-10-10T14:50:09.032891: step 1148, loss 0.120211, acc 0.953125, learning_rate 0.000144954
2017-10-10T14:50:09.349749: step 1149, loss 0.171689, acc 0.9375, learning_rate 0.00014477
2017-10-10T14:50:09.696848: step 1150, loss 0.144374, acc 0.9375, learning_rate 0.000144588
2017-10-10T14:50:10.145062: step 1151, loss 0.121912, acc 0.96875, learning_rate 0.000144406
2017-10-10T14:50:10.401867: step 1152, loss 0.16996, acc 0.953125, learning_rate 0.000144224
2017-10-10T14:50:10.660946: step 1153, loss 0.128381, acc 0.953125, learning_rate 0.000144044
2017-10-10T14:50:11.004724: step 1154, loss 0.183819, acc 0.9375, learning_rate 0.000143864
2017-10-10T14:50:11.181744: step 1155, loss 0.100991, acc 0.96875, learning_rate 0.000143685
2017-10-10T14:50:11.474285: step 1156, loss 0.215617, acc 0.90625, learning_rate 0.000143507
2017-10-10T14:50:11.743562: step 1157, loss 0.107877, acc 0.953125, learning_rate 0.000143329
2017-10-10T14:50:12.100938: step 1158, loss 0.0966683, acc 0.96875, learning_rate 0.000143152
2017-10-10T14:50:12.389974: step 1159, loss 0.116803, acc 0.984375, learning_rate 0.000142976
2017-10-10T14:50:12.644005: step 1160, loss 0.10243, acc 0.953125, learning_rate 0.000142801

Evaluation:
2017-10-10T14:50:13.157623: step 1160, loss 0.247774, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1160

2017-10-10T14:50:14.197012: step 1161, loss 0.18752, acc 0.96875, learning_rate 0.000142626
2017-10-10T14:50:14.510398: step 1162, loss 0.0579369, acc 1, learning_rate 0.000142452
2017-10-10T14:50:14.824141: step 1163, loss 0.277422, acc 0.890625, learning_rate 0.000142279
2017-10-10T14:50:15.151503: step 1164, loss 0.0765064, acc 0.984375, learning_rate 0.000142106
2017-10-10T14:50:15.452596: step 1165, loss 0.0846287, acc 0.96875, learning_rate 0.000141934
2017-10-10T14:50:15.764837: step 1166, loss 0.134368, acc 0.953125, learning_rate 0.000141763
2017-10-10T14:50:16.054786: step 1167, loss 0.133605, acc 0.953125, learning_rate 0.000141593
2017-10-10T14:50:16.400847: step 1168, loss 0.17054, acc 0.953125, learning_rate 0.000141423
2017-10-10T14:50:16.720633: step 1169, loss 0.12446, acc 0.953125, learning_rate 0.000141254
2017-10-10T14:50:17.103884: step 1170, loss 0.107716, acc 0.953125, learning_rate 0.000141085
2017-10-10T14:50:17.390888: step 1171, loss 0.0878621, acc 0.984375, learning_rate 0.000140918
2017-10-10T14:50:17.769062: step 1172, loss 0.16963, acc 0.9375, learning_rate 0.000140751
2017-10-10T14:50:18.100311: step 1173, loss 0.105691, acc 0.96875, learning_rate 0.000140584
2017-10-10T14:50:18.381295: step 1174, loss 0.242922, acc 0.921875, learning_rate 0.000140419
2017-10-10T14:50:18.665229: step 1175, loss 0.089987, acc 1, learning_rate 0.000140254
2017-10-10T14:50:19.006458: step 1176, loss 0.0913014, acc 0.980392, learning_rate 0.000140089
2017-10-10T14:50:19.261580: step 1177, loss 0.121831, acc 0.953125, learning_rate 0.000139926
2017-10-10T14:50:19.566362: step 1178, loss 0.185388, acc 0.9375, learning_rate 0.000139763
2017-10-10T14:50:19.888958: step 1179, loss 0.0986818, acc 0.984375, learning_rate 0.0001396
2017-10-10T14:50:20.146050: step 1180, loss 0.150635, acc 0.953125, learning_rate 0.000139439
2017-10-10T14:50:20.448597: step 1181, loss 0.093342, acc 0.96875, learning_rate 0.000139278
2017-10-10T14:50:20.767267: step 1182, loss 0.196071, acc 0.9375, learning_rate 0.000139118
2017-10-10T14:50:21.092991: step 1183, loss 0.0686963, acc 1, learning_rate 0.000138958
2017-10-10T14:50:21.317579: step 1184, loss 0.131575, acc 0.953125, learning_rate 0.000138799
2017-10-10T14:50:21.673393: step 1185, loss 0.145043, acc 0.953125, learning_rate 0.00013864
2017-10-10T14:50:21.941465: step 1186, loss 0.163557, acc 0.953125, learning_rate 0.000138483
2017-10-10T14:50:22.185444: step 1187, loss 0.341534, acc 0.84375, learning_rate 0.000138326
2017-10-10T14:50:22.469354: step 1188, loss 0.296032, acc 0.9375, learning_rate 0.000138169
2017-10-10T14:50:22.691797: step 1189, loss 0.160589, acc 0.953125, learning_rate 0.000138013
2017-10-10T14:50:22.968855: step 1190, loss 0.095655, acc 0.953125, learning_rate 0.000137858
2017-10-10T14:50:23.204986: step 1191, loss 0.113464, acc 0.984375, learning_rate 0.000137704
2017-10-10T14:50:23.508555: step 1192, loss 0.376844, acc 0.890625, learning_rate 0.00013755
2017-10-10T14:50:23.798768: step 1193, loss 0.196151, acc 0.90625, learning_rate 0.000137397
2017-10-10T14:50:24.068446: step 1194, loss 0.18377, acc 0.921875, learning_rate 0.000137244
2017-10-10T14:50:24.353488: step 1195, loss 0.196395, acc 0.9375, learning_rate 0.000137092
2017-10-10T14:50:24.623730: step 1196, loss 0.136126, acc 0.9375, learning_rate 0.000136941
2017-10-10T14:50:24.954890: step 1197, loss 0.165789, acc 0.9375, learning_rate 0.00013679
2017-10-10T14:50:25.207396: step 1198, loss 0.236635, acc 0.921875, learning_rate 0.00013664
2017-10-10T14:50:25.477270: step 1199, loss 0.19643, acc 0.9375, learning_rate 0.00013649
2017-10-10T14:50:25.768822: step 1200, loss 0.167914, acc 0.890625, learning_rate 0.000136341

Evaluation:
2017-10-10T14:50:26.236924: step 1200, loss 0.24743, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1200

2017-10-10T14:50:27.329515: step 1201, loss 0.204226, acc 0.921875, learning_rate 0.000136193
2017-10-10T14:50:27.527603: step 1202, loss 0.124485, acc 0.9375, learning_rate 0.000136045
2017-10-10T14:50:27.810796: step 1203, loss 0.0690591, acc 0.984375, learning_rate 0.000135898
2017-10-10T14:50:28.140537: step 1204, loss 0.0870346, acc 0.984375, learning_rate 0.000135751
2017-10-10T14:50:28.412896: step 1205, loss 0.211121, acc 0.90625, learning_rate 0.000135605
2017-10-10T14:50:28.740936: step 1206, loss 0.0803833, acc 0.96875, learning_rate 0.00013546
2017-10-10T14:50:29.086229: step 1207, loss 0.325323, acc 0.9375, learning_rate 0.000135315
2017-10-10T14:50:29.372845: step 1208, loss 0.223337, acc 0.9375, learning_rate 0.000135171
2017-10-10T14:50:29.634720: step 1209, loss 0.130658, acc 0.953125, learning_rate 0.000135028
2017-10-10T14:50:29.967903: step 1210, loss 0.122165, acc 0.96875, learning_rate 0.000134885
2017-10-10T14:50:30.253660: step 1211, loss 0.0979094, acc 0.984375, learning_rate 0.000134742
2017-10-10T14:50:30.556823: step 1212, loss 0.207685, acc 0.953125, learning_rate 0.0001346
2017-10-10T14:50:30.852465: step 1213, loss 0.121153, acc 0.96875, learning_rate 0.000134459
2017-10-10T14:50:31.145056: step 1214, loss 0.138875, acc 0.96875, learning_rate 0.000134319
2017-10-10T14:50:31.456224: step 1215, loss 0.184323, acc 0.90625, learning_rate 0.000134178
2017-10-10T14:50:31.776910: step 1216, loss 0.105271, acc 0.96875, learning_rate 0.000134039
2017-10-10T14:50:32.068855: step 1217, loss 0.233567, acc 0.90625, learning_rate 0.0001339
2017-10-10T14:50:32.392379: step 1218, loss 0.1962, acc 0.9375, learning_rate 0.000133762
2017-10-10T14:50:32.733057: step 1219, loss 0.174581, acc 0.953125, learning_rate 0.000133624
2017-10-10T14:50:33.052738: step 1220, loss 0.171571, acc 0.96875, learning_rate 0.000133487
2017-10-10T14:50:33.333476: step 1221, loss 0.10303, acc 0.984375, learning_rate 0.00013335
2017-10-10T14:50:33.578446: step 1222, loss 0.0956003, acc 0.96875, learning_rate 0.000133214
2017-10-10T14:50:33.886749: step 1223, loss 0.157885, acc 0.953125, learning_rate 0.000133078
2017-10-10T14:50:34.168405: step 1224, loss 0.140234, acc 0.953125, learning_rate 0.000132943
2017-10-10T14:50:34.478395: step 1225, loss 0.126743, acc 0.953125, learning_rate 0.000132809
2017-10-10T14:50:34.752982: step 1226, loss 0.0745138, acc 1, learning_rate 0.000132675
2017-10-10T14:50:35.081067: step 1227, loss 0.123577, acc 0.984375, learning_rate 0.000132541
2017-10-10T14:50:35.398514: step 1228, loss 0.0861777, acc 0.984375, learning_rate 0.000132409
2017-10-10T14:50:35.679302: step 1229, loss 0.0734722, acc 0.96875, learning_rate 0.000132276
2017-10-10T14:50:35.984953: step 1230, loss 0.299392, acc 0.890625, learning_rate 0.000132145
2017-10-10T14:50:36.320589: step 1231, loss 0.0894824, acc 0.984375, learning_rate 0.000132013
2017-10-10T14:50:36.671928: step 1232, loss 0.205164, acc 0.9375, learning_rate 0.000131883
2017-10-10T14:50:36.981080: step 1233, loss 0.278532, acc 0.921875, learning_rate 0.000131753
2017-10-10T14:50:37.244826: step 1234, loss 0.0419921, acc 1, learning_rate 0.000131623
2017-10-10T14:50:37.541359: step 1235, loss 0.0904501, acc 0.96875, learning_rate 0.000131494
2017-10-10T14:50:37.823403: step 1236, loss 0.160451, acc 0.9375, learning_rate 0.000131365
2017-10-10T14:50:38.119058: step 1237, loss 0.122156, acc 0.96875, learning_rate 0.000131237
2017-10-10T14:50:38.451956: step 1238, loss 0.139458, acc 0.953125, learning_rate 0.00013111
2017-10-10T14:50:38.784823: step 1239, loss 0.221242, acc 0.953125, learning_rate 0.000130983
2017-10-10T14:50:39.148893: step 1240, loss 0.114458, acc 0.984375, learning_rate 0.000130856

Evaluation:
2017-10-10T14:50:39.731874: step 1240, loss 0.24572, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1240

2017-10-10T14:50:40.737051: step 1241, loss 0.19709, acc 0.9375, learning_rate 0.00013073
2017-10-10T14:50:41.060603: step 1242, loss 0.139088, acc 0.9375, learning_rate 0.000130605
2017-10-10T14:50:41.344007: step 1243, loss 0.131657, acc 0.953125, learning_rate 0.00013048
2017-10-10T14:50:41.589058: step 1244, loss 0.105709, acc 0.953125, learning_rate 0.000130356
2017-10-10T14:50:41.836911: step 1245, loss 0.298323, acc 0.890625, learning_rate 0.000130232
2017-10-10T14:50:42.125044: step 1246, loss 0.127867, acc 0.96875, learning_rate 0.000130108
2017-10-10T14:50:42.425141: step 1247, loss 0.0761487, acc 1, learning_rate 0.000129985
2017-10-10T14:50:42.712061: step 1248, loss 0.233734, acc 0.921875, learning_rate 0.000129863
2017-10-10T14:50:43.055749: step 1249, loss 0.161139, acc 0.953125, learning_rate 0.000129741
2017-10-10T14:50:43.486586: step 1250, loss 0.0888687, acc 0.96875, learning_rate 0.00012962
2017-10-10T14:50:43.729064: step 1251, loss 0.226933, acc 0.90625, learning_rate 0.000129499
2017-10-10T14:50:44.013935: step 1252, loss 0.204465, acc 0.90625, learning_rate 0.000129378
2017-10-10T14:50:44.282437: step 1253, loss 0.102671, acc 0.953125, learning_rate 0.000129259
2017-10-10T14:50:44.493916: step 1254, loss 0.0961411, acc 1, learning_rate 0.000129139
2017-10-10T14:50:44.784569: step 1255, loss 0.0496217, acc 0.984375, learning_rate 0.00012902
2017-10-10T14:50:45.131505: step 1256, loss 0.157346, acc 0.953125, learning_rate 0.000128902
2017-10-10T14:50:45.380077: step 1257, loss 0.113754, acc 0.96875, learning_rate 0.000128784
2017-10-10T14:50:45.689118: step 1258, loss 0.0893612, acc 0.96875, learning_rate 0.000128666
2017-10-10T14:50:46.013259: step 1259, loss 0.158224, acc 0.953125, learning_rate 0.000128549
2017-10-10T14:50:46.332967: step 1260, loss 0.235637, acc 0.9375, learning_rate 0.000128433
2017-10-10T14:50:46.627172: step 1261, loss 0.186477, acc 0.921875, learning_rate 0.000128317
2017-10-10T14:50:46.913535: step 1262, loss 0.115861, acc 0.96875, learning_rate 0.000128201
2017-10-10T14:50:47.214962: step 1263, loss 0.13491, acc 0.9375, learning_rate 0.000128086
2017-10-10T14:50:47.501087: step 1264, loss 0.183797, acc 0.921875, learning_rate 0.000127971
2017-10-10T14:50:47.816929: step 1265, loss 0.0530707, acc 0.984375, learning_rate 0.000127857
2017-10-10T14:50:48.135141: step 1266, loss 0.355899, acc 0.859375, learning_rate 0.000127743
2017-10-10T14:50:48.454656: step 1267, loss 0.212866, acc 0.90625, learning_rate 0.00012763
2017-10-10T14:50:48.694522: step 1268, loss 0.222773, acc 0.9375, learning_rate 0.000127517
2017-10-10T14:50:49.019010: step 1269, loss 0.114214, acc 0.953125, learning_rate 0.000127405
2017-10-10T14:50:49.352885: step 1270, loss 0.159017, acc 0.953125, learning_rate 0.000127293
2017-10-10T14:50:49.636435: step 1271, loss 0.186982, acc 0.90625, learning_rate 0.000127182
2017-10-10T14:50:49.860990: step 1272, loss 0.253621, acc 0.90625, learning_rate 0.000127071
2017-10-10T14:50:50.148205: step 1273, loss 0.17403, acc 0.953125, learning_rate 0.00012696
2017-10-10T14:50:50.476827: step 1274, loss 0.242722, acc 0.941176, learning_rate 0.00012685
2017-10-10T14:50:50.824859: step 1275, loss 0.159826, acc 0.9375, learning_rate 0.000126741
2017-10-10T14:50:51.107747: step 1276, loss 0.110045, acc 0.953125, learning_rate 0.000126632
2017-10-10T14:50:51.377925: step 1277, loss 0.102415, acc 0.96875, learning_rate 0.000126523
2017-10-10T14:50:51.636879: step 1278, loss 0.149523, acc 0.9375, learning_rate 0.000126415
2017-10-10T14:50:51.948986: step 1279, loss 0.141658, acc 0.90625, learning_rate 0.000126307
2017-10-10T14:50:52.287826: step 1280, loss 0.0672287, acc 1, learning_rate 0.000126199

Evaluation:
2017-10-10T14:50:52.776547: step 1280, loss 0.247459, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1280

2017-10-10T14:50:53.894834: step 1281, loss 0.10005, acc 0.953125, learning_rate 0.000126093
2017-10-10T14:50:54.159223: step 1282, loss 0.118759, acc 0.96875, learning_rate 0.000125986
2017-10-10T14:50:54.461185: step 1283, loss 0.157769, acc 0.953125, learning_rate 0.00012588
2017-10-10T14:50:54.789963: step 1284, loss 0.125668, acc 0.953125, learning_rate 0.000125774
2017-10-10T14:50:55.064822: step 1285, loss 0.100605, acc 0.984375, learning_rate 0.000125669
2017-10-10T14:50:55.350419: step 1286, loss 0.0988227, acc 0.9375, learning_rate 0.000125564
2017-10-10T14:50:55.623598: step 1287, loss 0.0518983, acc 1, learning_rate 0.00012546
2017-10-10T14:50:55.927802: step 1288, loss 0.208537, acc 0.921875, learning_rate 0.000125356
2017-10-10T14:50:56.282513: step 1289, loss 0.230611, acc 0.921875, learning_rate 0.000125253
2017-10-10T14:50:56.608363: step 1290, loss 0.121684, acc 0.96875, learning_rate 0.00012515
2017-10-10T14:50:56.878073: step 1291, loss 0.205015, acc 0.921875, learning_rate 0.000125047
2017-10-10T14:50:57.215923: step 1292, loss 0.242473, acc 0.890625, learning_rate 0.000124945
2017-10-10T14:50:57.510850: step 1293, loss 0.231955, acc 0.921875, learning_rate 0.000124843
2017-10-10T14:50:57.798605: step 1294, loss 0.0888738, acc 0.984375, learning_rate 0.000124741
2017-10-10T14:50:58.109468: step 1295, loss 0.175438, acc 0.9375, learning_rate 0.00012464
2017-10-10T14:50:58.456864: step 1296, loss 0.179567, acc 0.953125, learning_rate 0.00012454
2017-10-10T14:50:58.805003: step 1297, loss 0.116023, acc 0.96875, learning_rate 0.00012444
2017-10-10T14:50:59.173006: step 1298, loss 0.176914, acc 0.953125, learning_rate 0.00012434
2017-10-10T14:50:59.401233: step 1299, loss 0.105222, acc 0.984375, learning_rate 0.000124241
2017-10-10T14:50:59.668939: step 1300, loss 0.0530153, acc 1, learning_rate 0.000124142
2017-10-10T14:51:00.008910: step 1301, loss 0.11238, acc 0.96875, learning_rate 0.000124043
2017-10-10T14:51:00.299942: step 1302, loss 0.0822272, acc 0.984375, learning_rate 0.000123945
2017-10-10T14:51:00.577654: step 1303, loss 0.15934, acc 0.953125, learning_rate 0.000123847
2017-10-10T14:51:00.859925: step 1304, loss 0.14822, acc 0.953125, learning_rate 0.00012375
2017-10-10T14:51:01.116867: step 1305, loss 0.128752, acc 0.96875, learning_rate 0.000123653
2017-10-10T14:51:01.440076: step 1306, loss 0.0941821, acc 0.953125, learning_rate 0.000123556
2017-10-10T14:51:01.747110: step 1307, loss 0.0919486, acc 0.96875, learning_rate 0.00012346
2017-10-10T14:51:02.112863: step 1308, loss 0.146857, acc 0.96875, learning_rate 0.000123364
2017-10-10T14:51:02.397025: step 1309, loss 0.0755749, acc 0.96875, learning_rate 0.000123269
2017-10-10T14:51:02.713111: step 1310, loss 0.25063, acc 0.921875, learning_rate 0.000123174
2017-10-10T14:51:02.980914: step 1311, loss 0.245663, acc 0.9375, learning_rate 0.00012308
2017-10-10T14:51:03.292108: step 1312, loss 0.179841, acc 0.921875, learning_rate 0.000122985
2017-10-10T14:51:03.608440: step 1313, loss 0.156202, acc 0.921875, learning_rate 0.000122892
2017-10-10T14:51:03.879974: step 1314, loss 0.164601, acc 0.9375, learning_rate 0.000122798
2017-10-10T14:51:04.185089: step 1315, loss 0.122954, acc 0.953125, learning_rate 0.000122705
2017-10-10T14:51:04.513427: step 1316, loss 0.133552, acc 0.953125, learning_rate 0.000122612
2017-10-10T14:51:04.829849: step 1317, loss 0.0941259, acc 0.984375, learning_rate 0.00012252
2017-10-10T14:51:05.100865: step 1318, loss 0.147488, acc 0.953125, learning_rate 0.000122428
2017-10-10T14:51:05.417118: step 1319, loss 0.251902, acc 0.921875, learning_rate 0.000122337
2017-10-10T14:51:05.727324: step 1320, loss 0.0917816, acc 0.96875, learning_rate 0.000122245

Evaluation:
2017-10-10T14:51:06.219389: step 1320, loss 0.249084, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1320

2017-10-10T14:51:07.784263: step 1321, loss 0.187344, acc 0.921875, learning_rate 0.000122155
2017-10-10T14:51:08.072834: step 1322, loss 0.24444, acc 0.890625, learning_rate 0.000122064
2017-10-10T14:51:08.349325: step 1323, loss 0.152927, acc 0.921875, learning_rate 0.000121974
2017-10-10T14:51:08.662611: step 1324, loss 0.16525, acc 0.9375, learning_rate 0.000121884
2017-10-10T14:51:08.947794: step 1325, loss 0.200639, acc 0.90625, learning_rate 0.000121795
2017-10-10T14:51:09.249552: step 1326, loss 0.177289, acc 0.9375, learning_rate 0.000121706
2017-10-10T14:51:09.571295: step 1327, loss 0.0829124, acc 0.984375, learning_rate 0.000121618
2017-10-10T14:51:09.873018: step 1328, loss 0.108894, acc 0.96875, learning_rate 0.000121529
2017-10-10T14:51:10.204635: step 1329, loss 0.176198, acc 0.9375, learning_rate 0.000121441
2017-10-10T14:51:10.496938: step 1330, loss 0.131332, acc 0.953125, learning_rate 0.000121354
2017-10-10T14:51:10.790987: step 1331, loss 0.178822, acc 0.953125, learning_rate 0.000121267
2017-10-10T14:51:11.077017: step 1332, loss 0.136485, acc 0.9375, learning_rate 0.00012118
2017-10-10T14:51:11.405133: step 1333, loss 0.170141, acc 0.953125, learning_rate 0.000121093
2017-10-10T14:51:11.748420: step 1334, loss 0.140192, acc 0.96875, learning_rate 0.000121007
2017-10-10T14:51:12.025293: step 1335, loss 0.307905, acc 0.921875, learning_rate 0.000120922
2017-10-10T14:51:12.334246: step 1336, loss 0.160552, acc 0.953125, learning_rate 0.000120836
2017-10-10T14:51:12.677038: step 1337, loss 0.0940549, acc 0.96875, learning_rate 0.000120751
2017-10-10T14:51:13.008590: step 1338, loss 0.212651, acc 0.875, learning_rate 0.000120666
2017-10-10T14:51:13.312901: step 1339, loss 0.13806, acc 0.921875, learning_rate 0.000120582
2017-10-10T14:51:13.610087: step 1340, loss 0.347962, acc 0.84375, learning_rate 0.000120498
2017-10-10T14:51:13.887192: step 1341, loss 0.212264, acc 0.90625, learning_rate 0.000120414
2017-10-10T14:51:14.209651: step 1342, loss 0.0988652, acc 0.984375, learning_rate 0.000120331
2017-10-10T14:51:14.541772: step 1343, loss 0.0985361, acc 0.984375, learning_rate 0.000120248
2017-10-10T14:51:14.846311: step 1344, loss 0.0877157, acc 0.96875, learning_rate 0.000120165
2017-10-10T14:51:15.136865: step 1345, loss 0.100465, acc 0.953125, learning_rate 0.000120083
2017-10-10T14:51:15.464964: step 1346, loss 0.15569, acc 0.96875, learning_rate 0.000120001
2017-10-10T14:51:15.708932: step 1347, loss 0.0949657, acc 0.96875, learning_rate 0.00011992
2017-10-10T14:51:16.054609: step 1348, loss 0.265222, acc 0.9375, learning_rate 0.000119838
2017-10-10T14:51:16.387936: step 1349, loss 0.137357, acc 0.96875, learning_rate 0.000119757
2017-10-10T14:51:16.601756: step 1350, loss 0.231704, acc 0.921875, learning_rate 0.000119677
2017-10-10T14:52:38.016408: step 1351, loss 0.106976, acc 0.96875, learning_rate 0.000119596
2017-10-10T14:53:47.100496: step 1352, loss 0.160083, acc 0.953125, learning_rate 0.000119516
2017-10-10T14:53:53.213469: step 1353, loss 0.119755, acc 0.984375, learning_rate 0.000119437
2017-10-10T14:54:02.436627: step 1354, loss 0.155704, acc 0.90625, learning_rate 0.000119357
2017-10-10T14:54:10.434883: step 1355, loss 0.145135, acc 0.953125, learning_rate 0.000119278
2017-10-10T14:54:16.450000: step 1356, loss 0.118747, acc 0.96875, learning_rate 0.0001192
2017-10-10T14:54:19.640562: step 1357, loss 0.110452, acc 0.96875, learning_rate 0.000119121
2017-10-10T14:54:24.012862: step 1358, loss 0.100026, acc 0.96875, learning_rate 0.000119043
2017-10-10T14:54:26.261800: step 1359, loss 0.264199, acc 0.90625, learning_rate 0.000118965
2017-10-10T14:54:32.248774: step 1360, loss 0.132981, acc 0.9375, learning_rate 0.000118888

Evaluation:
2017-10-10T14:54:32.615869: step 1360, loss 0.247761, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1360

2017-10-10T14:56:46.144983: step 1361, loss 0.222984, acc 0.921875, learning_rate 0.000118811
2017-10-10T14:56:47.834411: step 1362, loss 0.191729, acc 0.921875, learning_rate 0.000118734
2017-10-10T14:56:51.626776: step 1363, loss 0.225192, acc 0.921875, learning_rate 0.000118658
2017-10-10T14:56:54.015775: step 1364, loss 0.156987, acc 0.953125, learning_rate 0.000118582
2017-10-10T14:56:55.382770: step 1365, loss 0.223962, acc 0.953125, learning_rate 0.000118506
2017-10-10T14:56:57.798136: step 1366, loss 0.117747, acc 0.953125, learning_rate 0.00011843
2017-10-10T14:56:59.798368: step 1367, loss 0.257793, acc 0.890625, learning_rate 0.000118355
2017-10-10T14:57:03.419500: step 1368, loss 0.105369, acc 0.96875, learning_rate 0.00011828
2017-10-10T14:57:05.398023: step 1369, loss 0.268566, acc 0.890625, learning_rate 0.000118205
2017-10-10T14:57:07.454014: step 1370, loss 0.247289, acc 0.9375, learning_rate 0.000118131
2017-10-10T14:57:12.502838: step 1371, loss 0.077911, acc 0.984375, learning_rate 0.000118057
2017-10-10T14:57:13.816147: step 1372, loss 0.100066, acc 0.960784, learning_rate 0.000117983
2017-10-10T14:57:18.285761: step 1373, loss 0.1314, acc 0.96875, learning_rate 0.00011791
2017-10-10T14:57:19.233717: step 1374, loss 0.0980374, acc 0.96875, learning_rate 0.000117837
2017-10-10T14:57:25.023697: step 1375, loss 0.103694, acc 0.96875, learning_rate 0.000117764
2017-10-10T14:57:29.703608: step 1376, loss 0.151808, acc 0.9375, learning_rate 0.000117692
2017-10-10T14:57:30.467363: step 1377, loss 0.081451, acc 0.984375, learning_rate 0.000117619
2017-10-10T14:57:31.432842: step 1378, loss 0.0896437, acc 0.984375, learning_rate 0.000117547
2017-10-10T14:57:32.436776: step 1379, loss 0.158716, acc 0.9375, learning_rate 0.000117476
2017-10-10T14:57:35.997209: step 1380, loss 0.124777, acc 0.921875, learning_rate 0.000117404
2017-10-10T14:57:37.146998: step 1381, loss 0.0878872, acc 0.96875, learning_rate 0.000117333
2017-10-10T14:57:39.587321: step 1382, loss 0.0903123, acc 0.984375, learning_rate 0.000117263
2017-10-10T14:57:40.879144: step 1383, loss 0.119483, acc 0.96875, learning_rate 0.000117192
2017-10-10T14:57:41.756841: step 1384, loss 0.0819716, acc 1, learning_rate 0.000117122
2017-10-10T14:57:42.692450: step 1385, loss 0.181113, acc 0.9375, learning_rate 0.000117052
2017-10-10T14:57:43.714325: step 1386, loss 0.142318, acc 0.96875, learning_rate 0.000116983
2017-10-10T14:57:44.701006: step 1387, loss 0.284798, acc 0.890625, learning_rate 0.000116913
2017-10-10T14:57:46.055789: step 1388, loss 0.0838516, acc 0.984375, learning_rate 0.000116844
2017-10-10T14:57:46.954860: step 1389, loss 0.143723, acc 0.9375, learning_rate 0.000116775
2017-10-10T14:57:48.550498: step 1390, loss 0.141806, acc 0.953125, learning_rate 0.000116707
2017-10-10T14:57:49.366245: step 1391, loss 0.180263, acc 0.953125, learning_rate 0.000116639
2017-10-10T14:57:50.808311: step 1392, loss 0.210296, acc 0.921875, learning_rate 0.000116571
2017-10-10T14:57:52.492839: step 1393, loss 0.142218, acc 0.96875, learning_rate 0.000116503
2017-10-10T14:57:53.218630: step 1394, loss 0.284679, acc 0.921875, learning_rate 0.000116436
2017-10-10T14:57:54.376826: step 1395, loss 0.29035, acc 0.859375, learning_rate 0.000116369
2017-10-10T14:57:55.900046: step 1396, loss 0.135336, acc 0.953125, learning_rate 0.000116302
2017-10-10T14:57:56.779700: step 1397, loss 0.100424, acc 0.96875, learning_rate 0.000116235
2017-10-10T14:57:57.247307: step 1398, loss 0.207492, acc 0.90625, learning_rate 0.000116169
2017-10-10T14:57:57.761096: step 1399, loss 0.186913, acc 0.9375, learning_rate 0.000116103
2017-10-10T14:57:58.491618: step 1400, loss 0.185562, acc 0.953125, learning_rate 0.000116037

Evaluation:
2017-10-10T14:57:58.807352: step 1400, loss 0.246789, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1400

2017-10-10T14:58:01.013808: step 1401, loss 0.165429, acc 0.9375, learning_rate 0.000115972
2017-10-10T14:58:01.593000: step 1402, loss 0.117435, acc 0.953125, learning_rate 0.000115907
2017-10-10T14:58:02.036519: step 1403, loss 0.177722, acc 0.953125, learning_rate 0.000115842
2017-10-10T14:58:02.529356: step 1404, loss 0.111004, acc 0.984375, learning_rate 0.000115777
2017-10-10T14:58:02.813160: step 1405, loss 0.137933, acc 0.953125, learning_rate 0.000115713
2017-10-10T14:58:03.454546: step 1406, loss 0.115466, acc 0.96875, learning_rate 0.000115649
2017-10-10T14:58:03.952857: step 1407, loss 0.119132, acc 0.984375, learning_rate 0.000115585
2017-10-10T14:58:04.429249: step 1408, loss 0.242674, acc 0.890625, learning_rate 0.000115521
2017-10-10T14:58:05.133563: step 1409, loss 0.124132, acc 0.953125, learning_rate 0.000115458
2017-10-10T14:58:05.711239: step 1410, loss 0.121097, acc 0.9375, learning_rate 0.000115395
2017-10-10T14:58:06.268289: step 1411, loss 0.15287, acc 0.953125, learning_rate 0.000115332
2017-10-10T14:58:06.825319: step 1412, loss 0.167933, acc 0.9375, learning_rate 0.000115269
2017-10-10T14:58:07.162792: step 1413, loss 0.0870657, acc 0.984375, learning_rate 0.000115207
2017-10-10T14:58:07.622345: step 1414, loss 0.110047, acc 0.96875, learning_rate 0.000115145
2017-10-10T14:58:07.995823: step 1415, loss 0.126565, acc 0.96875, learning_rate 0.000115083
2017-10-10T14:58:08.329091: step 1416, loss 0.12589, acc 0.96875, learning_rate 0.000115022
2017-10-10T14:58:08.668983: step 1417, loss 0.155828, acc 0.9375, learning_rate 0.00011496
2017-10-10T14:58:09.130969: step 1418, loss 0.0563074, acc 1, learning_rate 0.000114899
2017-10-10T14:58:09.525212: step 1419, loss 0.306917, acc 0.90625, learning_rate 0.000114838
2017-10-10T14:58:10.078692: step 1420, loss 0.169723, acc 0.953125, learning_rate 0.000114778
2017-10-10T14:58:10.392779: step 1421, loss 0.130241, acc 0.953125, learning_rate 0.000114717
2017-10-10T14:58:10.698802: step 1422, loss 0.113427, acc 0.953125, learning_rate 0.000114657
2017-10-10T14:58:11.038510: step 1423, loss 0.0422237, acc 1, learning_rate 0.000114598
2017-10-10T14:58:11.396243: step 1424, loss 0.152218, acc 0.953125, learning_rate 0.000114538
2017-10-10T14:58:11.855987: step 1425, loss 0.213109, acc 0.953125, learning_rate 0.000114479
2017-10-10T14:58:11.992324: step 1426, loss 0.133572, acc 0.96875, learning_rate 0.00011442
2017-10-10T14:58:12.669036: step 1427, loss 0.164184, acc 0.953125, learning_rate 0.000114361
2017-10-10T14:58:13.110154: step 1428, loss 0.066583, acc 0.984375, learning_rate 0.000114302
2017-10-10T14:58:13.552480: step 1429, loss 0.146829, acc 0.953125, learning_rate 0.000114244
2017-10-10T14:58:13.868782: step 1430, loss 0.0888302, acc 0.984375, learning_rate 0.000114186
2017-10-10T14:58:14.307553: step 1431, loss 0.270625, acc 0.90625, learning_rate 0.000114128
2017-10-10T14:58:14.604214: step 1432, loss 0.0845461, acc 0.984375, learning_rate 0.00011407
2017-10-10T14:58:15.094869: step 1433, loss 0.234203, acc 0.890625, learning_rate 0.000114013
2017-10-10T14:58:15.416131: step 1434, loss 0.313714, acc 0.859375, learning_rate 0.000113955
2017-10-10T14:58:15.560898: step 1435, loss 0.27551, acc 0.875, learning_rate 0.000113898
2017-10-10T14:58:16.099840: step 1436, loss 0.0944264, acc 0.984375, learning_rate 0.000113842
2017-10-10T14:58:16.648829: step 1437, loss 0.269713, acc 0.890625, learning_rate 0.000113785
2017-10-10T14:58:16.968806: step 1438, loss 0.210117, acc 0.9375, learning_rate 0.000113729
2017-10-10T14:58:17.219446: step 1439, loss 0.289901, acc 0.859375, learning_rate 0.000113673
2017-10-10T14:58:17.544830: step 1440, loss 0.0856398, acc 0.984375, learning_rate 0.000113617

Evaluation:
2017-10-10T14:58:17.921001: step 1440, loss 0.245908, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1440

2017-10-10T14:58:19.096895: step 1441, loss 0.0996008, acc 0.96875, learning_rate 0.000113561
2017-10-10T14:58:19.377120: step 1442, loss 0.176676, acc 0.953125, learning_rate 0.000113506
2017-10-10T14:58:19.795765: step 1443, loss 0.284214, acc 0.921875, learning_rate 0.000113451
2017-10-10T14:58:20.161646: step 1444, loss 0.104844, acc 0.984375, learning_rate 0.000113396
2017-10-10T14:58:20.550490: step 1445, loss 0.17419, acc 0.9375, learning_rate 0.000113341
2017-10-10T14:58:20.818667: step 1446, loss 0.196446, acc 0.921875, learning_rate 0.000113287
2017-10-10T14:58:21.113824: step 1447, loss 0.185891, acc 0.953125, learning_rate 0.000113233
2017-10-10T14:58:21.381187: step 1448, loss 0.256754, acc 0.9375, learning_rate 0.000113179
2017-10-10T14:58:21.657700: step 1449, loss 0.144644, acc 0.953125, learning_rate 0.000113125
2017-10-10T14:58:21.967843: step 1450, loss 0.127787, acc 0.984375, learning_rate 0.000113071
2017-10-10T14:58:22.288451: step 1451, loss 0.162329, acc 0.96875, learning_rate 0.000113018
2017-10-10T14:58:22.550583: step 1452, loss 0.156845, acc 0.953125, learning_rate 0.000112965
2017-10-10T14:58:22.892670: step 1453, loss 0.129453, acc 0.96875, learning_rate 0.000112912
2017-10-10T14:58:23.181001: step 1454, loss 0.181495, acc 0.953125, learning_rate 0.000112859
2017-10-10T14:58:23.425141: step 1455, loss 0.130659, acc 0.96875, learning_rate 0.000112807
2017-10-10T14:58:23.756117: step 1456, loss 0.0752473, acc 1, learning_rate 0.000112754
2017-10-10T14:58:24.037254: step 1457, loss 0.12096, acc 0.96875, learning_rate 0.000112702
2017-10-10T14:58:24.401804: step 1458, loss 0.166958, acc 0.953125, learning_rate 0.000112651
2017-10-10T14:58:24.709236: step 1459, loss 0.137543, acc 0.953125, learning_rate 0.000112599
2017-10-10T14:58:25.037500: step 1460, loss 0.240516, acc 0.921875, learning_rate 0.000112547
2017-10-10T14:58:25.228955: step 1461, loss 0.279727, acc 0.875, learning_rate 0.000112496
2017-10-10T14:58:25.441993: step 1462, loss 0.164466, acc 0.953125, learning_rate 0.000112445
2017-10-10T14:58:25.759198: step 1463, loss 0.177323, acc 0.9375, learning_rate 0.000112394
2017-10-10T14:58:26.113155: step 1464, loss 0.231232, acc 0.90625, learning_rate 0.000112344
2017-10-10T14:58:26.388950: step 1465, loss 0.176244, acc 0.9375, learning_rate 0.000112293
2017-10-10T14:58:26.586939: step 1466, loss 0.177296, acc 0.90625, learning_rate 0.000112243
2017-10-10T14:58:26.874394: step 1467, loss 0.192051, acc 0.953125, learning_rate 0.000112193
2017-10-10T14:58:27.076872: step 1468, loss 0.075525, acc 0.96875, learning_rate 0.000112144
2017-10-10T14:58:27.234142: step 1469, loss 0.121637, acc 0.953125, learning_rate 0.000112094
2017-10-10T14:58:27.417001: step 1470, loss 0.0940247, acc 0.960784, learning_rate 0.000112045
2017-10-10T14:58:27.644755: step 1471, loss 0.165835, acc 0.96875, learning_rate 0.000111995
2017-10-10T14:58:27.969255: step 1472, loss 0.0603921, acc 0.984375, learning_rate 0.000111946
2017-10-10T14:58:28.241778: step 1473, loss 0.163163, acc 0.9375, learning_rate 0.000111898
2017-10-10T14:58:28.487890: step 1474, loss 0.227443, acc 0.953125, learning_rate 0.000111849
2017-10-10T14:58:28.751412: step 1475, loss 0.138947, acc 0.9375, learning_rate 0.000111801
2017-10-10T14:58:29.075650: step 1476, loss 0.0990622, acc 0.96875, learning_rate 0.000111753
2017-10-10T14:58:29.393126: step 1477, loss 0.21367, acc 0.90625, learning_rate 0.000111705
2017-10-10T14:58:29.625475: step 1478, loss 0.176904, acc 0.96875, learning_rate 0.000111657
2017-10-10T14:58:29.800540: step 1479, loss 0.130849, acc 0.953125, learning_rate 0.000111609
2017-10-10T14:58:29.937700: step 1480, loss 0.146076, acc 0.953125, learning_rate 0.000111562

Evaluation:
2017-10-10T14:58:30.263880: step 1480, loss 0.245138, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1480

2017-10-10T14:58:31.281772: step 1481, loss 0.174385, acc 0.9375, learning_rate 0.000111515
2017-10-10T14:58:31.514649: step 1482, loss 0.219602, acc 0.921875, learning_rate 0.000111468
2017-10-10T14:58:31.690560: step 1483, loss 0.149635, acc 0.921875, learning_rate 0.000111421
2017-10-10T14:58:31.830304: step 1484, loss 0.0544456, acc 0.984375, learning_rate 0.000111374
2017-10-10T14:58:31.956723: step 1485, loss 0.240969, acc 0.921875, learning_rate 0.000111328
2017-10-10T14:58:32.129463: step 1486, loss 0.178966, acc 0.953125, learning_rate 0.000111282
2017-10-10T14:58:32.272881: step 1487, loss 0.133302, acc 0.9375, learning_rate 0.000111236
2017-10-10T14:58:32.596927: step 1488, loss 0.0627599, acc 1, learning_rate 0.00011119
2017-10-10T14:58:32.906624: step 1489, loss 0.166767, acc 0.921875, learning_rate 0.000111144
2017-10-10T14:58:33.105087: step 1490, loss 0.183929, acc 0.9375, learning_rate 0.000111099
2017-10-10T14:58:33.423006: step 1491, loss 0.150893, acc 0.921875, learning_rate 0.000111053
2017-10-10T14:58:33.725077: step 1492, loss 0.113891, acc 0.96875, learning_rate 0.000111008
2017-10-10T14:58:34.004778: step 1493, loss 0.149769, acc 0.953125, learning_rate 0.000110963
2017-10-10T14:58:34.340701: step 1494, loss 0.164602, acc 0.953125, learning_rate 0.000110918
2017-10-10T14:58:34.680825: step 1495, loss 0.151986, acc 0.9375, learning_rate 0.000110874
2017-10-10T14:58:34.948886: step 1496, loss 0.211422, acc 0.9375, learning_rate 0.00011083
2017-10-10T14:58:35.147361: step 1497, loss 0.153173, acc 0.953125, learning_rate 0.000110785
2017-10-10T14:58:35.463188: step 1498, loss 0.0881021, acc 1, learning_rate 0.000110741
2017-10-10T14:58:35.784796: step 1499, loss 0.0715463, acc 0.984375, learning_rate 0.000110697
2017-10-10T14:58:36.025380: step 1500, loss 0.217911, acc 0.921875, learning_rate 0.000110654
2017-10-10T14:58:36.234472: step 1501, loss 0.163547, acc 0.921875, learning_rate 0.00011061
2017-10-10T14:58:36.521305: step 1502, loss 0.127183, acc 0.96875, learning_rate 0.000110567
2017-10-10T14:58:36.858828: step 1503, loss 0.0910801, acc 0.984375, learning_rate 0.000110524
2017-10-10T14:58:37.120883: step 1504, loss 0.11024, acc 0.96875, learning_rate 0.000110481
2017-10-10T14:58:37.351005: step 1505, loss 0.109458, acc 0.9375, learning_rate 0.000110438
2017-10-10T14:58:37.521993: step 1506, loss 0.127395, acc 0.96875, learning_rate 0.000110396
2017-10-10T14:58:37.752893: step 1507, loss 0.12217, acc 0.96875, learning_rate 0.000110353
2017-10-10T14:58:38.056278: step 1508, loss 0.137988, acc 0.953125, learning_rate 0.000110311
2017-10-10T14:58:38.368870: step 1509, loss 0.0860811, acc 0.96875, learning_rate 0.000110269
2017-10-10T14:58:38.629176: step 1510, loss 0.111986, acc 0.96875, learning_rate 0.000110227
2017-10-10T14:58:38.869025: step 1511, loss 0.21957, acc 0.90625, learning_rate 0.000110185
2017-10-10T14:58:39.116139: step 1512, loss 0.159021, acc 0.9375, learning_rate 0.000110144
2017-10-10T14:58:39.315815: step 1513, loss 0.0655037, acc 1, learning_rate 0.000110102
2017-10-10T14:58:39.530626: step 1514, loss 0.178024, acc 0.9375, learning_rate 0.000110061
2017-10-10T14:58:39.768457: step 1515, loss 0.203025, acc 0.9375, learning_rate 0.00011002
2017-10-10T14:58:40.029812: step 1516, loss 0.146661, acc 0.9375, learning_rate 0.000109979
2017-10-10T14:58:40.316171: step 1517, loss 0.244455, acc 0.859375, learning_rate 0.000109938
2017-10-10T14:58:40.566876: step 1518, loss 0.211737, acc 0.921875, learning_rate 0.000109898
2017-10-10T14:58:40.741602: step 1519, loss 0.24284, acc 0.9375, learning_rate 0.000109857
2017-10-10T14:58:41.008942: step 1520, loss 0.160689, acc 0.953125, learning_rate 0.000109817

Evaluation:
2017-10-10T14:58:41.376808: step 1520, loss 0.246724, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1520

2017-10-10T14:58:42.222246: step 1521, loss 0.205012, acc 0.96875, learning_rate 0.000109777
2017-10-10T14:58:42.447565: step 1522, loss 0.11259, acc 0.96875, learning_rate 0.000109737
2017-10-10T14:58:42.701093: step 1523, loss 0.0714232, acc 0.984375, learning_rate 0.000109697
2017-10-10T14:58:42.929149: step 1524, loss 0.1723, acc 0.9375, learning_rate 0.000109658
2017-10-10T14:58:43.261298: step 1525, loss 0.198572, acc 0.921875, learning_rate 0.000109618
2017-10-10T14:58:43.579242: step 1526, loss 0.12074, acc 0.96875, learning_rate 0.000109579
2017-10-10T14:58:43.899560: step 1527, loss 0.276251, acc 0.90625, learning_rate 0.00010954
2017-10-10T14:58:44.109716: step 1528, loss 0.0969628, acc 0.984375, learning_rate 0.000109501
2017-10-10T14:58:44.303901: step 1529, loss 0.128097, acc 0.96875, learning_rate 0.000109462
2017-10-10T14:58:44.555842: step 1530, loss 0.24487, acc 0.921875, learning_rate 0.000109424
2017-10-10T14:58:44.832823: step 1531, loss 0.149902, acc 0.96875, learning_rate 0.000109385
2017-10-10T14:58:45.196872: step 1532, loss 0.108789, acc 0.953125, learning_rate 0.000109347
2017-10-10T14:58:45.496881: step 1533, loss 0.133603, acc 0.953125, learning_rate 0.000109309
2017-10-10T14:58:45.800934: step 1534, loss 0.233464, acc 0.953125, learning_rate 0.000109271
2017-10-10T14:58:46.147966: step 1535, loss 0.112779, acc 0.953125, learning_rate 0.000109233
2017-10-10T14:58:46.380852: step 1536, loss 0.228586, acc 0.953125, learning_rate 0.000109195
2017-10-10T14:58:46.679011: step 1537, loss 0.19668, acc 0.921875, learning_rate 0.000109158
2017-10-10T14:58:46.912527: step 1538, loss 0.161307, acc 0.953125, learning_rate 0.00010912
2017-10-10T14:58:47.189043: step 1539, loss 0.129474, acc 0.96875, learning_rate 0.000109083
2017-10-10T14:58:47.513166: step 1540, loss 0.0730792, acc 0.984375, learning_rate 0.000109046
2017-10-10T14:58:47.849224: step 1541, loss 0.169928, acc 0.921875, learning_rate 0.000109009
2017-10-10T14:58:48.171715: step 1542, loss 0.192989, acc 0.9375, learning_rate 0.000108972
2017-10-10T14:58:48.455494: step 1543, loss 0.11034, acc 0.96875, learning_rate 0.000108936
2017-10-10T14:58:48.689933: step 1544, loss 0.142264, acc 0.953125, learning_rate 0.000108899
2017-10-10T14:58:49.038157: step 1545, loss 0.122401, acc 0.96875, learning_rate 0.000108863
2017-10-10T14:58:49.363581: step 1546, loss 0.162466, acc 0.9375, learning_rate 0.000108827
2017-10-10T14:58:49.682719: step 1547, loss 0.183883, acc 0.9375, learning_rate 0.000108791
2017-10-10T14:58:49.934965: step 1548, loss 0.169502, acc 0.9375, learning_rate 0.000108755
2017-10-10T14:58:50.159079: step 1549, loss 0.123825, acc 0.96875, learning_rate 0.000108719
2017-10-10T14:58:50.503974: step 1550, loss 0.158064, acc 0.921875, learning_rate 0.000108683
2017-10-10T14:58:50.791828: step 1551, loss 0.116633, acc 0.96875, learning_rate 0.000108648
2017-10-10T14:58:51.089370: step 1552, loss 0.123708, acc 0.984375, learning_rate 0.000108613
2017-10-10T14:58:51.348394: step 1553, loss 0.125396, acc 0.96875, learning_rate 0.000108577
2017-10-10T14:58:51.651509: step 1554, loss 0.260959, acc 0.921875, learning_rate 0.000108542
2017-10-10T14:58:51.945415: step 1555, loss 0.137866, acc 0.96875, learning_rate 0.000108508
2017-10-10T14:58:52.232906: step 1556, loss 0.0775557, acc 0.984375, learning_rate 0.000108473
2017-10-10T14:58:52.542544: step 1557, loss 0.0869925, acc 0.953125, learning_rate 0.000108438
2017-10-10T14:58:52.862053: step 1558, loss 0.0793086, acc 0.96875, learning_rate 0.000108404
2017-10-10T14:58:53.220950: step 1559, loss 0.125232, acc 0.953125, learning_rate 0.00010837
2017-10-10T14:58:53.493019: step 1560, loss 0.0813883, acc 0.984375, learning_rate 0.000108335

Evaluation:
2017-10-10T14:58:54.100851: step 1560, loss 0.245124, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1560

2017-10-10T14:58:55.289008: step 1561, loss 0.0561765, acc 0.984375, learning_rate 0.000108301
2017-10-10T14:58:55.554863: step 1562, loss 0.122541, acc 0.953125, learning_rate 0.000108267
2017-10-10T14:58:55.829548: step 1563, loss 0.152748, acc 0.96875, learning_rate 0.000108234
2017-10-10T14:58:56.084927: step 1564, loss 0.230528, acc 0.90625, learning_rate 0.0001082
2017-10-10T14:58:56.324115: step 1565, loss 0.0947929, acc 0.984375, learning_rate 0.000108167
2017-10-10T14:58:56.664913: step 1566, loss 0.140382, acc 0.96875, learning_rate 0.000108133
2017-10-10T14:58:56.952970: step 1567, loss 0.15111, acc 0.9375, learning_rate 0.0001081
2017-10-10T14:58:57.212872: step 1568, loss 0.107031, acc 0.980392, learning_rate 0.000108067
2017-10-10T14:58:57.516969: step 1569, loss 0.0830996, acc 0.96875, learning_rate 0.000108034
2017-10-10T14:58:57.885049: step 1570, loss 0.11699, acc 0.953125, learning_rate 0.000108001
2017-10-10T14:58:58.175466: step 1571, loss 0.147866, acc 0.9375, learning_rate 0.000107969
2017-10-10T14:58:58.405313: step 1572, loss 0.23785, acc 0.921875, learning_rate 0.000107936
2017-10-10T14:58:58.711432: step 1573, loss 0.19242, acc 0.953125, learning_rate 0.000107904
2017-10-10T14:58:59.021028: step 1574, loss 0.0758995, acc 0.984375, learning_rate 0.000107871
2017-10-10T14:58:59.280987: step 1575, loss 0.133638, acc 0.96875, learning_rate 0.000107839
2017-10-10T14:58:59.607880: step 1576, loss 0.253545, acc 0.890625, learning_rate 0.000107807
2017-10-10T14:58:59.947105: step 1577, loss 0.240396, acc 0.921875, learning_rate 0.000107775
2017-10-10T14:59:00.200629: step 1578, loss 0.0855377, acc 0.984375, learning_rate 0.000107744
2017-10-10T14:59:00.555993: step 1579, loss 0.12998, acc 0.96875, learning_rate 0.000107712
2017-10-10T14:59:00.844865: step 1580, loss 0.13737, acc 0.953125, learning_rate 0.000107681
2017-10-10T14:59:01.104339: step 1581, loss 0.0886274, acc 0.984375, learning_rate 0.000107649
2017-10-10T14:59:01.365620: step 1582, loss 0.125409, acc 0.96875, learning_rate 0.000107618
2017-10-10T14:59:01.628276: step 1583, loss 0.21366, acc 0.9375, learning_rate 0.000107587
2017-10-10T14:59:01.931282: step 1584, loss 0.0989536, acc 0.984375, learning_rate 0.000107556
2017-10-10T14:59:02.127377: step 1585, loss 0.21729, acc 0.9375, learning_rate 0.000107525
2017-10-10T14:59:02.447340: step 1586, loss 0.0869703, acc 0.984375, learning_rate 0.000107494
2017-10-10T14:59:02.809938: step 1587, loss 0.191024, acc 0.9375, learning_rate 0.000107464
2017-10-10T14:59:03.084274: step 1588, loss 0.0999807, acc 0.96875, learning_rate 0.000107433
2017-10-10T14:59:03.346940: step 1589, loss 0.256092, acc 0.90625, learning_rate 0.000107403
2017-10-10T14:59:03.616358: step 1590, loss 0.0898908, acc 0.984375, learning_rate 0.000107373
2017-10-10T14:59:03.884881: step 1591, loss 0.163425, acc 0.9375, learning_rate 0.000107343
2017-10-10T14:59:04.290713: step 1592, loss 0.243587, acc 0.921875, learning_rate 0.000107313
2017-10-10T14:59:04.636464: step 1593, loss 0.202099, acc 0.90625, learning_rate 0.000107283
2017-10-10T14:59:04.920863: step 1594, loss 0.151118, acc 0.953125, learning_rate 0.000107253
2017-10-10T14:59:05.044268: step 1595, loss 0.136602, acc 0.984375, learning_rate 0.000107224
2017-10-10T14:59:05.129719: step 1596, loss 0.0843198, acc 0.984375, learning_rate 0.000107194
2017-10-10T14:59:05.216008: step 1597, loss 0.151305, acc 0.953125, learning_rate 0.000107165
2017-10-10T14:59:05.301750: step 1598, loss 0.120436, acc 0.96875, learning_rate 0.000107136
2017-10-10T14:59:05.381708: step 1599, loss 0.182056, acc 0.921875, learning_rate 0.000107106
2017-10-10T14:59:05.461635: step 1600, loss 0.173128, acc 0.921875, learning_rate 0.000107077

Evaluation:
2017-10-10T14:59:06.003556: step 1600, loss 0.243381, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1600

2017-10-10T14:59:07.093115: step 1601, loss 0.207593, acc 0.953125, learning_rate 0.000107048
2017-10-10T14:59:07.392987: step 1602, loss 0.0845525, acc 0.96875, learning_rate 0.00010702
2017-10-10T14:59:07.682533: step 1603, loss 0.143587, acc 0.96875, learning_rate 0.000106991
2017-10-10T14:59:07.979885: step 1604, loss 0.204497, acc 0.90625, learning_rate 0.000106963
2017-10-10T14:59:08.307631: step 1605, loss 0.125573, acc 0.953125, learning_rate 0.000106934
2017-10-10T14:59:08.629091: step 1606, loss 0.121275, acc 0.984375, learning_rate 0.000106906
2017-10-10T14:59:08.906049: step 1607, loss 0.193881, acc 0.921875, learning_rate 0.000106878
2017-10-10T14:59:09.224484: step 1608, loss 0.117329, acc 0.953125, learning_rate 0.00010685
2017-10-10T14:59:09.495281: step 1609, loss 0.164252, acc 0.9375, learning_rate 0.000106822
2017-10-10T14:59:09.851479: step 1610, loss 0.102444, acc 0.96875, learning_rate 0.000106794
2017-10-10T14:59:10.148877: step 1611, loss 0.0795678, acc 0.984375, learning_rate 0.000106766
2017-10-10T14:59:10.476387: step 1612, loss 0.150445, acc 0.953125, learning_rate 0.000106738
2017-10-10T14:59:10.751035: step 1613, loss 0.140777, acc 0.96875, learning_rate 0.000106711
2017-10-10T14:59:11.029894: step 1614, loss 0.17222, acc 0.953125, learning_rate 0.000106684
2017-10-10T14:59:11.363408: step 1615, loss 0.099355, acc 0.984375, learning_rate 0.000106656
2017-10-10T14:59:11.646858: step 1616, loss 0.14584, acc 0.921875, learning_rate 0.000106629
2017-10-10T14:59:11.919010: step 1617, loss 0.130347, acc 0.953125, learning_rate 0.000106602
2017-10-10T14:59:12.238074: step 1618, loss 0.168505, acc 0.953125, learning_rate 0.000106575
2017-10-10T14:59:12.589224: step 1619, loss 0.143216, acc 0.96875, learning_rate 0.000106548
2017-10-10T14:59:12.880100: step 1620, loss 0.182683, acc 0.921875, learning_rate 0.000106521
2017-10-10T14:59:13.153241: step 1621, loss 0.126067, acc 0.984375, learning_rate 0.000106495
2017-10-10T14:59:13.439335: step 1622, loss 0.225954, acc 0.9375, learning_rate 0.000106468
2017-10-10T14:59:13.696596: step 1623, loss 0.093003, acc 0.953125, learning_rate 0.000106442
2017-10-10T14:59:13.985326: step 1624, loss 0.168963, acc 0.953125, learning_rate 0.000106416
2017-10-10T14:59:14.320831: step 1625, loss 0.174822, acc 0.9375, learning_rate 0.000106389
2017-10-10T14:59:14.668966: step 1626, loss 0.123363, acc 0.953125, learning_rate 0.000106363
2017-10-10T14:59:14.944878: step 1627, loss 0.135444, acc 0.984375, learning_rate 0.000106337
2017-10-10T14:59:15.261948: step 1628, loss 0.219409, acc 0.953125, learning_rate 0.000106312
2017-10-10T14:59:15.580832: step 1629, loss 0.11137, acc 0.984375, learning_rate 0.000106286
2017-10-10T14:59:15.848390: step 1630, loss 0.0736454, acc 0.984375, learning_rate 0.00010626
2017-10-10T14:59:16.147500: step 1631, loss 0.283379, acc 0.90625, learning_rate 0.000106235
2017-10-10T14:59:16.489078: step 1632, loss 0.178256, acc 0.953125, learning_rate 0.000106209
2017-10-10T14:59:16.823603: step 1633, loss 0.196876, acc 0.90625, learning_rate 0.000106184
2017-10-10T14:59:17.105020: step 1634, loss 0.0954338, acc 0.984375, learning_rate 0.000106159
2017-10-10T14:59:17.465222: step 1635, loss 0.0678861, acc 0.96875, learning_rate 0.000106133
2017-10-10T14:59:17.798919: step 1636, loss 0.115354, acc 0.953125, learning_rate 0.000106108
2017-10-10T14:59:18.081772: step 1637, loss 0.146516, acc 0.921875, learning_rate 0.000106083
2017-10-10T14:59:18.283672: step 1638, loss 0.166436, acc 0.9375, learning_rate 0.000106059
2017-10-10T14:59:18.519876: step 1639, loss 0.136484, acc 0.96875, learning_rate 0.000106034
2017-10-10T14:59:18.814017: step 1640, loss 0.0808172, acc 0.96875, learning_rate 0.000106009

Evaluation:
2017-10-10T14:59:19.301982: step 1640, loss 0.24263, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1640

2017-10-10T14:59:20.465083: step 1641, loss 0.131027, acc 0.96875, learning_rate 0.000105985
2017-10-10T14:59:20.763157: step 1642, loss 0.0964545, acc 0.96875, learning_rate 0.00010596
2017-10-10T14:59:21.080922: step 1643, loss 0.177073, acc 0.9375, learning_rate 0.000105936
2017-10-10T14:59:21.431298: step 1644, loss 0.172229, acc 0.9375, learning_rate 0.000105912
2017-10-10T14:59:21.792947: step 1645, loss 0.157182, acc 0.953125, learning_rate 0.000105888
2017-10-10T14:59:22.039947: step 1646, loss 0.244681, acc 0.921875, learning_rate 0.000105864
2017-10-10T14:59:22.357084: step 1647, loss 0.0487117, acc 1, learning_rate 0.00010584
2017-10-10T14:59:22.713614: step 1648, loss 0.238543, acc 0.921875, learning_rate 0.000105816
2017-10-10T14:59:23.052850: step 1649, loss 0.172382, acc 0.9375, learning_rate 0.000105792
2017-10-10T14:59:23.350438: step 1650, loss 0.140444, acc 0.921875, learning_rate 0.000105768
2017-10-10T14:59:23.564828: step 1651, loss 0.0620261, acc 0.96875, learning_rate 0.000105745
2017-10-10T14:59:23.868051: step 1652, loss 0.0962667, acc 0.984375, learning_rate 0.000105721
2017-10-10T14:59:24.145603: step 1653, loss 0.0566619, acc 1, learning_rate 0.000105698
2017-10-10T14:59:24.498519: step 1654, loss 0.221376, acc 0.90625, learning_rate 0.000105675
2017-10-10T14:59:24.811453: step 1655, loss 0.163136, acc 0.953125, learning_rate 0.000105652
2017-10-10T14:59:25.140322: step 1656, loss 0.217051, acc 0.90625, learning_rate 0.000105629
2017-10-10T14:59:25.401983: step 1657, loss 0.0817022, acc 0.984375, learning_rate 0.000105606
2017-10-10T14:59:25.668822: step 1658, loss 0.231253, acc 0.921875, learning_rate 0.000105583
2017-10-10T14:59:25.916403: step 1659, loss 0.251751, acc 0.921875, learning_rate 0.00010556
2017-10-10T14:59:26.207436: step 1660, loss 0.23246, acc 0.921875, learning_rate 0.000105537
2017-10-10T14:59:26.511155: step 1661, loss 0.126903, acc 0.953125, learning_rate 0.000105515
2017-10-10T14:59:26.836925: step 1662, loss 0.119034, acc 0.953125, learning_rate 0.000105492
2017-10-10T14:59:27.150134: step 1663, loss 0.153983, acc 0.921875, learning_rate 0.00010547
2017-10-10T14:59:27.488911: step 1664, loss 0.193474, acc 0.953125, learning_rate 0.000105447
2017-10-10T14:59:27.834834: step 1665, loss 0.0902686, acc 0.96875, learning_rate 0.000105425
2017-10-10T14:59:28.057042: step 1666, loss 0.152506, acc 0.941176, learning_rate 0.000105403
2017-10-10T14:59:28.364869: step 1667, loss 0.0679289, acc 0.984375, learning_rate 0.000105381
2017-10-10T14:59:28.676029: step 1668, loss 0.275728, acc 0.90625, learning_rate 0.000105359
2017-10-10T14:59:29.011942: step 1669, loss 0.181598, acc 0.953125, learning_rate 0.000105337
2017-10-10T14:59:29.352920: step 1670, loss 0.121986, acc 0.96875, learning_rate 0.000105315
2017-10-10T14:59:29.571236: step 1671, loss 0.127281, acc 0.984375, learning_rate 0.000105294
2017-10-10T14:59:29.838048: step 1672, loss 0.134912, acc 0.953125, learning_rate 0.000105272
2017-10-10T14:59:30.152840: step 1673, loss 0.176046, acc 0.9375, learning_rate 0.000105251
2017-10-10T14:59:30.439464: step 1674, loss 0.199769, acc 0.890625, learning_rate 0.000105229
2017-10-10T14:59:30.765789: step 1675, loss 0.131311, acc 0.96875, learning_rate 0.000105208
2017-10-10T14:59:31.057337: step 1676, loss 0.134467, acc 0.984375, learning_rate 0.000105186
2017-10-10T14:59:31.404954: step 1677, loss 0.0849682, acc 0.984375, learning_rate 0.000105165
2017-10-10T14:59:31.704869: step 1678, loss 0.133142, acc 0.953125, learning_rate 0.000105144
2017-10-10T14:59:32.023975: step 1679, loss 0.0809984, acc 0.984375, learning_rate 0.000105123
2017-10-10T14:59:32.303150: step 1680, loss 0.159971, acc 0.921875, learning_rate 0.000105102

Evaluation:
2017-10-10T14:59:32.839310: step 1680, loss 0.244815, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1680

2017-10-10T14:59:33.784367: step 1681, loss 0.186656, acc 0.9375, learning_rate 0.000105081
2017-10-10T14:59:34.064900: step 1682, loss 0.0573972, acc 0.984375, learning_rate 0.000105061
2017-10-10T14:59:34.305011: step 1683, loss 0.0720854, acc 0.96875, learning_rate 0.00010504
2017-10-10T14:59:34.655984: step 1684, loss 0.173191, acc 0.96875, learning_rate 0.00010502
2017-10-10T14:59:35.015935: step 1685, loss 0.117366, acc 0.984375, learning_rate 0.000104999
2017-10-10T14:59:35.304018: step 1686, loss 0.121856, acc 0.953125, learning_rate 0.000104979
2017-10-10T14:59:35.549083: step 1687, loss 0.134161, acc 0.96875, learning_rate 0.000104958
2017-10-10T14:59:35.832921: step 1688, loss 0.155855, acc 0.921875, learning_rate 0.000104938
2017-10-10T14:59:36.139003: step 1689, loss 0.151992, acc 0.9375, learning_rate 0.000104918
2017-10-10T14:59:36.458296: step 1690, loss 0.124617, acc 0.96875, learning_rate 0.000104898
2017-10-10T14:59:36.781085: step 1691, loss 0.0868908, acc 0.96875, learning_rate 0.000104878
2017-10-10T14:59:37.081360: step 1692, loss 0.192286, acc 0.953125, learning_rate 0.000104858
2017-10-10T14:59:37.399857: step 1693, loss 0.23222, acc 0.953125, learning_rate 0.000104838
2017-10-10T14:59:37.717662: step 1694, loss 0.0849805, acc 0.984375, learning_rate 0.000104818
2017-10-10T14:59:37.936542: step 1695, loss 0.066916, acc 1, learning_rate 0.000104799
2017-10-10T14:59:38.232140: step 1696, loss 0.131288, acc 0.953125, learning_rate 0.000104779
2017-10-10T14:59:38.548874: step 1697, loss 0.116908, acc 0.984375, learning_rate 0.00010476
2017-10-10T14:59:38.856912: step 1698, loss 0.125576, acc 0.96875, learning_rate 0.00010474
2017-10-10T14:59:39.153592: step 1699, loss 0.0976847, acc 0.96875, learning_rate 0.000104721
2017-10-10T14:59:39.411833: step 1700, loss 0.0663373, acc 1, learning_rate 0.000104702
2017-10-10T14:59:39.703177: step 1701, loss 0.257866, acc 0.875, learning_rate 0.000104682
2017-10-10T14:59:40.083423: step 1702, loss 0.131174, acc 0.9375, learning_rate 0.000104663
2017-10-10T14:59:40.306406: step 1703, loss 0.0978448, acc 0.96875, learning_rate 0.000104644
2017-10-10T14:59:40.688841: step 1704, loss 0.258597, acc 0.890625, learning_rate 0.000104625
2017-10-10T14:59:41.022943: step 1705, loss 0.238778, acc 0.90625, learning_rate 0.000104606
2017-10-10T14:59:41.294878: step 1706, loss 0.0478375, acc 1, learning_rate 0.000104588
2017-10-10T14:59:41.555104: step 1707, loss 0.162011, acc 0.953125, learning_rate 0.000104569
2017-10-10T14:59:41.815048: step 1708, loss 0.134252, acc 0.96875, learning_rate 0.00010455
2017-10-10T14:59:42.141822: step 1709, loss 0.130172, acc 0.9375, learning_rate 0.000104532
2017-10-10T14:59:42.438620: step 1710, loss 0.126028, acc 0.9375, learning_rate 0.000104513
2017-10-10T14:59:42.722788: step 1711, loss 0.158969, acc 0.9375, learning_rate 0.000104495
2017-10-10T14:59:43.004982: step 1712, loss 0.168988, acc 0.953125, learning_rate 0.000104476
2017-10-10T14:59:43.315676: step 1713, loss 0.0930324, acc 0.96875, learning_rate 0.000104458
2017-10-10T14:59:43.600067: step 1714, loss 0.0808795, acc 1, learning_rate 0.00010444
2017-10-10T14:59:43.896852: step 1715, loss 0.179174, acc 0.953125, learning_rate 0.000104422
2017-10-10T14:59:44.220311: step 1716, loss 0.105083, acc 0.953125, learning_rate 0.000104404
2017-10-10T14:59:44.526464: step 1717, loss 0.118691, acc 0.984375, learning_rate 0.000104386
2017-10-10T14:59:44.822886: step 1718, loss 0.176814, acc 0.953125, learning_rate 0.000104368
2017-10-10T14:59:45.162040: step 1719, loss 0.198134, acc 0.921875, learning_rate 0.00010435
2017-10-10T14:59:45.434357: step 1720, loss 0.100435, acc 0.96875, learning_rate 0.000104332

Evaluation:
2017-10-10T14:59:45.976790: step 1720, loss 0.24708, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1720

2017-10-10T14:59:47.060095: step 1721, loss 0.387271, acc 0.90625, learning_rate 0.000104315
2017-10-10T14:59:47.366397: step 1722, loss 0.185645, acc 0.9375, learning_rate 0.000104297
2017-10-10T14:59:47.660859: step 1723, loss 0.264251, acc 0.921875, learning_rate 0.000104279
2017-10-10T14:59:47.928814: step 1724, loss 0.162213, acc 0.953125, learning_rate 0.000104262
2017-10-10T14:59:48.220984: step 1725, loss 0.171505, acc 0.921875, learning_rate 0.000104245
2017-10-10T14:59:48.560698: step 1726, loss 0.101018, acc 0.96875, learning_rate 0.000104227
2017-10-10T14:59:48.847801: step 1727, loss 0.148315, acc 0.953125, learning_rate 0.00010421
2017-10-10T14:59:49.156958: step 1728, loss 0.197193, acc 0.921875, learning_rate 0.000104193
2017-10-10T14:59:49.544618: step 1729, loss 0.160891, acc 0.921875, learning_rate 0.000104176
2017-10-10T14:59:49.830494: step 1730, loss 0.147687, acc 0.9375, learning_rate 0.000104159
2017-10-10T14:59:50.148830: step 1731, loss 0.101281, acc 0.96875, learning_rate 0.000104142
2017-10-10T14:59:50.414402: step 1732, loss 0.148091, acc 0.9375, learning_rate 0.000104125
2017-10-10T14:59:50.585096: step 1733, loss 0.103523, acc 0.96875, learning_rate 0.000104108
2017-10-10T14:59:50.965991: step 1734, loss 0.195106, acc 0.9375, learning_rate 0.000104091
2017-10-10T14:59:51.238532: step 1735, loss 0.261388, acc 0.890625, learning_rate 0.000104074
2017-10-10T14:59:51.520847: step 1736, loss 0.13031, acc 0.953125, learning_rate 0.000104058
2017-10-10T14:59:51.869931: step 1737, loss 0.0621243, acc 1, learning_rate 0.000104041
2017-10-10T14:59:52.172464: step 1738, loss 0.198875, acc 0.90625, learning_rate 0.000104025
2017-10-10T14:59:52.438502: step 1739, loss 0.245927, acc 0.9375, learning_rate 0.000104008
2017-10-10T14:59:52.720618: step 1740, loss 0.223847, acc 0.90625, learning_rate 0.000103992
2017-10-10T14:59:53.009204: step 1741, loss 0.123325, acc 0.96875, learning_rate 0.000103976
2017-10-10T14:59:53.332963: step 1742, loss 0.178428, acc 0.9375, learning_rate 0.000103959
2017-10-10T14:59:53.559036: step 1743, loss 0.17924, acc 0.9375, learning_rate 0.000103943
2017-10-10T14:59:53.832491: step 1744, loss 0.179922, acc 0.9375, learning_rate 0.000103927
2017-10-10T14:59:54.088939: step 1745, loss 0.191041, acc 0.953125, learning_rate 0.000103911
2017-10-10T14:59:54.449123: step 1746, loss 0.114087, acc 0.96875, learning_rate 0.000103895
2017-10-10T14:59:54.751720: step 1747, loss 0.194198, acc 0.953125, learning_rate 0.000103879
2017-10-10T14:59:55.044383: step 1748, loss 0.0837173, acc 0.984375, learning_rate 0.000103863
2017-10-10T14:59:55.360841: step 1749, loss 0.0565527, acc 0.984375, learning_rate 0.000103848
2017-10-10T14:59:55.679803: step 1750, loss 0.119613, acc 0.953125, learning_rate 0.000103832
2017-10-10T14:59:55.932378: step 1751, loss 0.188864, acc 0.9375, learning_rate 0.000103816
2017-10-10T14:59:56.276117: step 1752, loss 0.160065, acc 0.953125, learning_rate 0.000103801
2017-10-10T14:59:56.636834: step 1753, loss 0.127956, acc 0.96875, learning_rate 0.000103785
2017-10-10T14:59:56.948517: step 1754, loss 0.120762, acc 0.96875, learning_rate 0.00010377
2017-10-10T14:59:57.245727: step 1755, loss 0.138133, acc 0.953125, learning_rate 0.000103754
2017-10-10T14:59:57.584704: step 1756, loss 0.101016, acc 0.96875, learning_rate 0.000103739
2017-10-10T14:59:57.871525: step 1757, loss 0.104422, acc 0.953125, learning_rate 0.000103724
2017-10-10T14:59:58.138289: step 1758, loss 0.105487, acc 0.96875, learning_rate 0.000103709
2017-10-10T14:59:58.444910: step 1759, loss 0.113661, acc 0.96875, learning_rate 0.000103694
2017-10-10T14:59:58.798226: step 1760, loss 0.11688, acc 0.953125, learning_rate 0.000103678

Evaluation:
2017-10-10T14:59:59.338526: step 1760, loss 0.241722, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1760

2017-10-10T15:00:00.404634: step 1761, loss 0.108358, acc 0.984375, learning_rate 0.000103663
2017-10-10T15:00:00.663136: step 1762, loss 0.049349, acc 1, learning_rate 0.000103648
2017-10-10T15:00:01.009188: step 1763, loss 0.119122, acc 0.96875, learning_rate 0.000103634
2017-10-10T15:00:01.269441: step 1764, loss 0.115716, acc 0.980392, learning_rate 0.000103619
2017-10-10T15:00:01.541235: step 1765, loss 0.176774, acc 0.953125, learning_rate 0.000103604
2017-10-10T15:00:01.801163: step 1766, loss 0.123736, acc 0.96875, learning_rate 0.000103589
2017-10-10T15:00:02.108855: step 1767, loss 0.162654, acc 0.96875, learning_rate 0.000103575
2017-10-10T15:00:02.388912: step 1768, loss 0.0769369, acc 0.984375, learning_rate 0.00010356
2017-10-10T15:00:02.667481: step 1769, loss 0.180886, acc 0.9375, learning_rate 0.000103545
2017-10-10T15:00:02.969942: step 1770, loss 0.0998931, acc 0.953125, learning_rate 0.000103531
2017-10-10T15:00:03.263338: step 1771, loss 0.284089, acc 0.875, learning_rate 0.000103517
2017-10-10T15:00:03.600901: step 1772, loss 0.172419, acc 0.90625, learning_rate 0.000103502
2017-10-10T15:00:03.929110: step 1773, loss 0.0995506, acc 0.96875, learning_rate 0.000103488
2017-10-10T15:00:04.244924: step 1774, loss 0.170647, acc 0.96875, learning_rate 0.000103474
2017-10-10T15:00:04.575130: step 1775, loss 0.145004, acc 0.953125, learning_rate 0.00010346
2017-10-10T15:00:04.877592: step 1776, loss 0.114768, acc 0.96875, learning_rate 0.000103445
2017-10-10T15:00:05.133262: step 1777, loss 0.0615537, acc 0.96875, learning_rate 0.000103431
2017-10-10T15:00:05.447096: step 1778, loss 0.21557, acc 0.890625, learning_rate 0.000103417
2017-10-10T15:00:05.746418: step 1779, loss 0.117241, acc 0.9375, learning_rate 0.000103403
2017-10-10T15:00:06.060893: step 1780, loss 0.110826, acc 0.984375, learning_rate 0.00010339
2017-10-10T15:00:06.345119: step 1781, loss 0.0738968, acc 0.96875, learning_rate 0.000103376
2017-10-10T15:00:06.674752: step 1782, loss 0.233813, acc 0.921875, learning_rate 0.000103362
2017-10-10T15:00:06.991633: step 1783, loss 0.170599, acc 0.90625, learning_rate 0.000103348
2017-10-10T15:00:07.308067: step 1784, loss 0.228082, acc 0.90625, learning_rate 0.000103335
2017-10-10T15:00:07.568936: step 1785, loss 0.134896, acc 0.9375, learning_rate 0.000103321
2017-10-10T15:00:07.885296: step 1786, loss 0.259473, acc 0.921875, learning_rate 0.000103307
2017-10-10T15:00:08.256426: step 1787, loss 0.114574, acc 0.984375, learning_rate 0.000103294
2017-10-10T15:00:08.593360: step 1788, loss 0.119318, acc 0.96875, learning_rate 0.00010328
2017-10-10T15:00:08.938278: step 1789, loss 0.107519, acc 0.953125, learning_rate 0.000103267
2017-10-10T15:00:09.193523: step 1790, loss 0.105599, acc 0.96875, learning_rate 0.000103254
2017-10-10T15:00:09.416782: step 1791, loss 0.166394, acc 0.953125, learning_rate 0.00010324
2017-10-10T15:00:09.676830: step 1792, loss 0.122956, acc 0.984375, learning_rate 0.000103227
2017-10-10T15:00:10.029398: step 1793, loss 0.128095, acc 0.953125, learning_rate 0.000103214
2017-10-10T15:00:10.320845: step 1794, loss 0.21799, acc 0.9375, learning_rate 0.000103201
2017-10-10T15:00:10.512228: step 1795, loss 0.254479, acc 0.953125, learning_rate 0.000103188
2017-10-10T15:00:11.469244: step 1796, loss 0.103713, acc 0.984375, learning_rate 0.000103175
2017-10-10T15:00:11.814765: step 1797, loss 0.123251, acc 0.96875, learning_rate 0.000103162
2017-10-10T15:00:12.075330: step 1798, loss 0.137477, acc 0.953125, learning_rate 0.000103149
2017-10-10T15:00:12.367679: step 1799, loss 0.20805, acc 0.90625, learning_rate 0.000103136
2017-10-10T15:00:12.644874: step 1800, loss 0.0904454, acc 0.96875, learning_rate 0.000103123

Evaluation:
2017-10-10T15:00:13.109665: step 1800, loss 0.243409, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1800

2017-10-10T15:00:14.228206: step 1801, loss 0.171239, acc 0.953125, learning_rate 0.000103111
2017-10-10T15:00:14.601095: step 1802, loss 0.263172, acc 0.9375, learning_rate 0.000103098
2017-10-10T15:00:14.877817: step 1803, loss 0.130896, acc 0.984375, learning_rate 0.000103085
2017-10-10T15:00:15.155882: step 1804, loss 0.110464, acc 0.96875, learning_rate 0.000103073
2017-10-10T15:00:15.428830: step 1805, loss 0.10722, acc 0.96875, learning_rate 0.00010306
2017-10-10T15:00:15.776653: step 1806, loss 0.149754, acc 0.953125, learning_rate 0.000103048
2017-10-10T15:00:16.144838: step 1807, loss 0.258626, acc 0.890625, learning_rate 0.000103035
2017-10-10T15:00:16.476862: step 1808, loss 0.153222, acc 0.9375, learning_rate 0.000103023
2017-10-10T15:00:16.765684: step 1809, loss 0.0716002, acc 0.984375, learning_rate 0.00010301
2017-10-10T15:00:17.083229: step 1810, loss 0.147966, acc 0.953125, learning_rate 0.000102998
2017-10-10T15:00:17.408768: step 1811, loss 0.104471, acc 0.96875, learning_rate 0.000102986
2017-10-10T15:00:17.716912: step 1812, loss 0.200896, acc 0.953125, learning_rate 0.000102974
2017-10-10T15:00:18.014995: step 1813, loss 0.16963, acc 0.9375, learning_rate 0.000102962
2017-10-10T15:00:18.311632: step 1814, loss 0.1272, acc 0.953125, learning_rate 0.000102949
2017-10-10T15:00:18.640837: step 1815, loss 0.153334, acc 0.9375, learning_rate 0.000102937
2017-10-10T15:00:18.934549: step 1816, loss 0.0725257, acc 0.96875, learning_rate 0.000102925
2017-10-10T15:00:19.298810: step 1817, loss 0.196865, acc 0.953125, learning_rate 0.000102913
2017-10-10T15:00:19.596762: step 1818, loss 0.205684, acc 0.921875, learning_rate 0.000102902
2017-10-10T15:00:19.968794: step 1819, loss 0.284881, acc 0.90625, learning_rate 0.00010289
2017-10-10T15:00:20.267166: step 1820, loss 0.132413, acc 0.96875, learning_rate 0.000102878
2017-10-10T15:00:20.582230: step 1821, loss 0.234643, acc 0.921875, learning_rate 0.000102866
2017-10-10T15:00:20.863320: step 1822, loss 0.0816463, acc 0.984375, learning_rate 0.000102855
2017-10-10T15:00:21.144848: step 1823, loss 0.163856, acc 0.953125, learning_rate 0.000102843
2017-10-10T15:00:21.528884: step 1824, loss 0.171785, acc 0.921875, learning_rate 0.000102831
2017-10-10T15:00:21.845457: step 1825, loss 0.0931228, acc 0.96875, learning_rate 0.00010282
2017-10-10T15:00:22.130115: step 1826, loss 0.233099, acc 0.921875, learning_rate 0.000102808
2017-10-10T15:00:22.387619: step 1827, loss 0.135771, acc 0.96875, learning_rate 0.000102797
2017-10-10T15:00:22.719144: step 1828, loss 0.173644, acc 0.921875, learning_rate 0.000102785
2017-10-10T15:00:22.963457: step 1829, loss 0.0731391, acc 1, learning_rate 0.000102774
2017-10-10T15:00:23.269856: step 1830, loss 0.114729, acc 0.984375, learning_rate 0.000102763
2017-10-10T15:00:23.570977: step 1831, loss 0.119713, acc 0.96875, learning_rate 0.000102751
2017-10-10T15:00:23.909086: step 1832, loss 0.0946886, acc 0.953125, learning_rate 0.00010274
2017-10-10T15:00:24.258639: step 1833, loss 0.117912, acc 0.953125, learning_rate 0.000102729
2017-10-10T15:00:24.561281: step 1834, loss 0.125922, acc 0.9375, learning_rate 0.000102718
2017-10-10T15:00:24.872883: step 1835, loss 0.0784752, acc 0.984375, learning_rate 0.000102707
2017-10-10T15:00:25.185100: step 1836, loss 0.291427, acc 0.875, learning_rate 0.000102696
2017-10-10T15:00:25.538750: step 1837, loss 0.262095, acc 0.9375, learning_rate 0.000102685
2017-10-10T15:00:25.868873: step 1838, loss 0.123269, acc 0.984375, learning_rate 0.000102674
2017-10-10T15:00:26.140927: step 1839, loss 0.162837, acc 0.9375, learning_rate 0.000102663
2017-10-10T15:00:26.393674: step 1840, loss 0.152016, acc 0.953125, learning_rate 0.000102652

Evaluation:
2017-10-10T15:00:26.856187: step 1840, loss 0.241954, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1840

2017-10-10T15:00:27.711718: step 1841, loss 0.136176, acc 0.9375, learning_rate 0.000102641
2017-10-10T15:00:28.039620: step 1842, loss 0.213414, acc 0.921875, learning_rate 0.00010263
2017-10-10T15:00:28.343173: step 1843, loss 0.11868, acc 0.984375, learning_rate 0.00010262
2017-10-10T15:00:28.638224: step 1844, loss 0.158922, acc 0.953125, learning_rate 0.000102609
2017-10-10T15:00:28.967223: step 1845, loss 0.234613, acc 0.890625, learning_rate 0.000102598
2017-10-10T15:00:29.274018: step 1846, loss 0.166688, acc 0.921875, learning_rate 0.000102588
2017-10-10T15:00:29.565020: step 1847, loss 0.187851, acc 0.953125, learning_rate 0.000102577
2017-10-10T15:00:29.783976: step 1848, loss 0.0805505, acc 0.96875, learning_rate 0.000102567
2017-10-10T15:00:30.106393: step 1849, loss 0.148547, acc 0.953125, learning_rate 0.000102556
2017-10-10T15:00:30.446694: step 1850, loss 0.119631, acc 0.953125, learning_rate 0.000102546
2017-10-10T15:00:30.727220: step 1851, loss 0.127785, acc 0.953125, learning_rate 0.000102535
2017-10-10T15:00:31.008157: step 1852, loss 0.138932, acc 0.96875, learning_rate 0.000102525
2017-10-10T15:00:31.314404: step 1853, loss 0.120313, acc 0.96875, learning_rate 0.000102515
2017-10-10T15:00:31.637000: step 1854, loss 0.0774907, acc 0.984375, learning_rate 0.000102504
2017-10-10T15:00:31.931262: step 1855, loss 0.0911015, acc 0.984375, learning_rate 0.000102494
2017-10-10T15:00:32.236960: step 1856, loss 0.1724, acc 0.921875, learning_rate 0.000102484
2017-10-10T15:00:32.556867: step 1857, loss 0.132532, acc 0.96875, learning_rate 0.000102474
2017-10-10T15:00:32.861084: step 1858, loss 0.137196, acc 0.953125, learning_rate 0.000102464
2017-10-10T15:00:33.199353: step 1859, loss 0.147119, acc 0.96875, learning_rate 0.000102454
2017-10-10T15:00:33.463713: step 1860, loss 0.163306, acc 0.9375, learning_rate 0.000102444
2017-10-10T15:00:33.792811: step 1861, loss 0.0596316, acc 0.984375, learning_rate 0.000102434
2017-10-10T15:00:34.009395: step 1862, loss 0.275044, acc 0.960784, learning_rate 0.000102424
2017-10-10T15:00:34.346100: step 1863, loss 0.0598139, acc 0.984375, learning_rate 0.000102414
2017-10-10T15:00:34.673508: step 1864, loss 0.28264, acc 0.921875, learning_rate 0.000102404
2017-10-10T15:00:35.037631: step 1865, loss 0.129205, acc 0.953125, learning_rate 0.000102394
2017-10-10T15:00:35.428893: step 1866, loss 0.0794791, acc 0.984375, learning_rate 0.000102384
2017-10-10T15:00:35.696508: step 1867, loss 0.191319, acc 0.921875, learning_rate 0.000102375
2017-10-10T15:00:35.894024: step 1868, loss 0.104877, acc 0.96875, learning_rate 0.000102365
2017-10-10T15:00:36.063501: step 1869, loss 0.153649, acc 0.953125, learning_rate 0.000102355
2017-10-10T15:00:36.312814: step 1870, loss 0.0825474, acc 1, learning_rate 0.000102346
2017-10-10T15:00:36.652877: step 1871, loss 0.187063, acc 0.9375, learning_rate 0.000102336
2017-10-10T15:00:36.908885: step 1872, loss 0.173, acc 0.953125, learning_rate 0.000102327
2017-10-10T15:00:37.194695: step 1873, loss 0.101326, acc 0.96875, learning_rate 0.000102317
2017-10-10T15:00:37.469834: step 1874, loss 0.286364, acc 0.890625, learning_rate 0.000102308
2017-10-10T15:00:37.755296: step 1875, loss 0.141515, acc 0.9375, learning_rate 0.000102298
2017-10-10T15:00:38.047670: step 1876, loss 0.148717, acc 0.953125, learning_rate 0.000102289
2017-10-10T15:00:38.393159: step 1877, loss 0.100613, acc 0.96875, learning_rate 0.000102279
2017-10-10T15:00:38.668487: step 1878, loss 0.266367, acc 0.90625, learning_rate 0.00010227
2017-10-10T15:00:38.992919: step 1879, loss 0.0868668, acc 0.984375, learning_rate 0.000102261
2017-10-10T15:00:39.294403: step 1880, loss 0.123556, acc 0.96875, learning_rate 0.000102252

Evaluation:
2017-10-10T15:00:39.774841: step 1880, loss 0.242643, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1880

2017-10-10T15:00:40.890301: step 1881, loss 0.108773, acc 0.984375, learning_rate 0.000102242
2017-10-10T15:00:41.230292: step 1882, loss 0.238965, acc 0.90625, learning_rate 0.000102233
2017-10-10T15:00:41.556042: step 1883, loss 0.0922738, acc 0.984375, learning_rate 0.000102224
2017-10-10T15:00:41.850646: step 1884, loss 0.177823, acc 0.9375, learning_rate 0.000102215
2017-10-10T15:00:42.173166: step 1885, loss 0.0935603, acc 1, learning_rate 0.000102206
2017-10-10T15:00:42.529923: step 1886, loss 0.12109, acc 0.953125, learning_rate 0.000102197
2017-10-10T15:00:42.833042: step 1887, loss 0.201237, acc 0.921875, learning_rate 0.000102188
2017-10-10T15:00:43.096326: step 1888, loss 0.128157, acc 0.9375, learning_rate 0.000102179
2017-10-10T15:00:43.327894: step 1889, loss 0.171811, acc 0.96875, learning_rate 0.00010217
2017-10-10T15:00:43.564688: step 1890, loss 0.134789, acc 0.984375, learning_rate 0.000102161
2017-10-10T15:00:43.840839: step 1891, loss 0.12521, acc 0.9375, learning_rate 0.000102153
2017-10-10T15:00:44.165270: step 1892, loss 0.140439, acc 0.953125, learning_rate 0.000102144
2017-10-10T15:00:44.493363: step 1893, loss 0.168784, acc 0.96875, learning_rate 0.000102135
2017-10-10T15:00:44.732286: step 1894, loss 0.130637, acc 0.953125, learning_rate 0.000102126
2017-10-10T15:00:45.028830: step 1895, loss 0.0807077, acc 0.984375, learning_rate 0.000102118
2017-10-10T15:00:45.308964: step 1896, loss 0.107421, acc 0.984375, learning_rate 0.000102109
2017-10-10T15:00:45.573232: step 1897, loss 0.213867, acc 0.921875, learning_rate 0.0001021
2017-10-10T15:00:45.864912: step 1898, loss 0.125226, acc 0.953125, learning_rate 0.000102092
2017-10-10T15:00:46.153784: step 1899, loss 0.18186, acc 0.890625, learning_rate 0.000102083
2017-10-10T15:00:46.428590: step 1900, loss 0.162166, acc 0.96875, learning_rate 0.000102075
2017-10-10T15:00:46.748895: step 1901, loss 0.240513, acc 0.953125, learning_rate 0.000102066
2017-10-10T15:00:47.029068: step 1902, loss 0.0838514, acc 0.96875, learning_rate 0.000102058
2017-10-10T15:00:47.319652: step 1903, loss 0.158986, acc 0.953125, learning_rate 0.00010205
2017-10-10T15:00:47.674364: step 1904, loss 0.138468, acc 0.96875, learning_rate 0.000102041
2017-10-10T15:00:47.988509: step 1905, loss 0.12415, acc 0.953125, learning_rate 0.000102033
2017-10-10T15:00:48.304808: step 1906, loss 0.0716636, acc 0.984375, learning_rate 0.000102025
2017-10-10T15:00:48.596888: step 1907, loss 0.121548, acc 0.96875, learning_rate 0.000102016
2017-10-10T15:00:48.892836: step 1908, loss 0.0878943, acc 0.96875, learning_rate 0.000102008
2017-10-10T15:00:49.246789: step 1909, loss 0.149128, acc 0.96875, learning_rate 0.000102
2017-10-10T15:00:49.546776: step 1910, loss 0.090284, acc 0.984375, learning_rate 0.000101992
2017-10-10T15:00:49.858016: step 1911, loss 0.164064, acc 0.9375, learning_rate 0.000101984
2017-10-10T15:00:50.156904: step 1912, loss 0.0789124, acc 0.984375, learning_rate 0.000101975
2017-10-10T15:00:50.407281: step 1913, loss 0.136408, acc 0.953125, learning_rate 0.000101967
2017-10-10T15:00:50.746492: step 1914, loss 0.0491783, acc 0.984375, learning_rate 0.000101959
2017-10-10T15:00:51.047622: step 1915, loss 0.175345, acc 0.90625, learning_rate 0.000101951
2017-10-10T15:00:51.407611: step 1916, loss 0.1343, acc 0.96875, learning_rate 0.000101943
2017-10-10T15:00:51.718980: step 1917, loss 0.139817, acc 0.953125, learning_rate 0.000101935
2017-10-10T15:00:52.004958: step 1918, loss 0.155583, acc 0.953125, learning_rate 0.000101928
2017-10-10T15:00:52.205051: step 1919, loss 0.150155, acc 0.953125, learning_rate 0.00010192
2017-10-10T15:00:52.527695: step 1920, loss 0.249077, acc 0.875, learning_rate 0.000101912

Evaluation:
2017-10-10T15:00:53.013456: step 1920, loss 0.242613, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1920

2017-10-10T15:00:54.068593: step 1921, loss 0.225548, acc 0.9375, learning_rate 0.000101904
2017-10-10T15:00:54.350837: step 1922, loss 0.150358, acc 0.953125, learning_rate 0.000101896
2017-10-10T15:00:54.601892: step 1923, loss 0.250319, acc 0.875, learning_rate 0.000101889
2017-10-10T15:00:54.871665: step 1924, loss 0.215044, acc 0.921875, learning_rate 0.000101881
2017-10-10T15:00:55.150109: step 1925, loss 0.101065, acc 0.96875, learning_rate 0.000101873
2017-10-10T15:00:55.392272: step 1926, loss 0.185275, acc 0.9375, learning_rate 0.000101865
2017-10-10T15:00:55.750034: step 1927, loss 0.181529, acc 0.9375, learning_rate 0.000101858
2017-10-10T15:00:56.048723: step 1928, loss 0.143402, acc 0.953125, learning_rate 0.00010185
2017-10-10T15:00:56.322911: step 1929, loss 0.10265, acc 0.984375, learning_rate 0.000101843
2017-10-10T15:00:56.650507: step 1930, loss 0.0805346, acc 0.984375, learning_rate 0.000101835
2017-10-10T15:00:56.943746: step 1931, loss 0.0654512, acc 0.984375, learning_rate 0.000101828
2017-10-10T15:00:57.231360: step 1932, loss 0.122154, acc 0.9375, learning_rate 0.00010182
2017-10-10T15:00:57.540927: step 1933, loss 0.133171, acc 0.96875, learning_rate 0.000101813
2017-10-10T15:00:57.801011: step 1934, loss 0.15495, acc 0.9375, learning_rate 0.000101805
2017-10-10T15:00:58.147413: step 1935, loss 0.151737, acc 0.953125, learning_rate 0.000101798
2017-10-10T15:00:58.456051: step 1936, loss 0.161929, acc 0.953125, learning_rate 0.000101791
2017-10-10T15:00:58.733858: step 1937, loss 0.0899595, acc 0.984375, learning_rate 0.000101783
2017-10-10T15:00:58.999634: step 1938, loss 0.102121, acc 0.96875, learning_rate 0.000101776
2017-10-10T15:00:59.322869: step 1939, loss 0.126013, acc 0.953125, learning_rate 0.000101769
2017-10-10T15:00:59.656835: step 1940, loss 0.153805, acc 0.953125, learning_rate 0.000101762
2017-10-10T15:00:59.936863: step 1941, loss 0.0659756, acc 0.984375, learning_rate 0.000101754
2017-10-10T15:01:00.192493: step 1942, loss 0.165817, acc 0.90625, learning_rate 0.000101747
2017-10-10T15:01:00.438923: step 1943, loss 0.154443, acc 0.96875, learning_rate 0.00010174
2017-10-10T15:01:00.752895: step 1944, loss 0.138143, acc 0.9375, learning_rate 0.000101733
2017-10-10T15:01:01.044854: step 1945, loss 0.0711777, acc 0.984375, learning_rate 0.000101726
2017-10-10T15:01:01.408989: step 1946, loss 0.116586, acc 0.96875, learning_rate 0.000101719
2017-10-10T15:01:01.675667: step 1947, loss 0.153919, acc 0.9375, learning_rate 0.000101712
2017-10-10T15:01:01.941040: step 1948, loss 0.130763, acc 0.953125, learning_rate 0.000101705
2017-10-10T15:01:02.258938: step 1949, loss 0.196072, acc 0.96875, learning_rate 0.000101698
2017-10-10T15:01:02.528355: step 1950, loss 0.116706, acc 0.96875, learning_rate 0.000101691
2017-10-10T15:01:02.844782: step 1951, loss 0.129334, acc 0.953125, learning_rate 0.000101684
2017-10-10T15:01:03.115602: step 1952, loss 0.259892, acc 0.921875, learning_rate 0.000101677
2017-10-10T15:01:03.444780: step 1953, loss 0.0736234, acc 0.96875, learning_rate 0.00010167
2017-10-10T15:01:03.736555: step 1954, loss 0.120756, acc 0.9375, learning_rate 0.000101664
2017-10-10T15:01:04.175405: step 1955, loss 0.203063, acc 0.9375, learning_rate 0.000101657
2017-10-10T15:01:04.423796: step 1956, loss 0.181204, acc 0.9375, learning_rate 0.00010165
2017-10-10T15:01:04.704624: step 1957, loss 0.147824, acc 0.90625, learning_rate 0.000101643
2017-10-10T15:01:04.924312: step 1958, loss 0.155924, acc 0.9375, learning_rate 0.000101637
2017-10-10T15:01:05.184952: step 1959, loss 0.104079, acc 0.984375, learning_rate 0.00010163
2017-10-10T15:01:05.463603: step 1960, loss 0.107735, acc 0.960784, learning_rate 0.000101623

Evaluation:
2017-10-10T15:01:05.957161: step 1960, loss 0.241675, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-1960

2017-10-10T15:01:07.119146: step 1961, loss 0.0867174, acc 0.96875, learning_rate 0.000101617
2017-10-10T15:01:07.433172: step 1962, loss 0.139302, acc 0.9375, learning_rate 0.00010161
2017-10-10T15:01:07.799493: step 1963, loss 0.132526, acc 0.953125, learning_rate 0.000101604
2017-10-10T15:01:08.086814: step 1964, loss 0.208429, acc 0.890625, learning_rate 0.000101597
2017-10-10T15:01:08.367341: step 1965, loss 0.152986, acc 0.921875, learning_rate 0.00010159
2017-10-10T15:01:08.693871: step 1966, loss 0.160463, acc 0.96875, learning_rate 0.000101584
2017-10-10T15:01:08.957041: step 1967, loss 0.176338, acc 0.953125, learning_rate 0.000101577
2017-10-10T15:01:09.238595: step 1968, loss 0.1508, acc 0.9375, learning_rate 0.000101571
2017-10-10T15:01:09.549186: step 1969, loss 0.215117, acc 0.90625, learning_rate 0.000101565
2017-10-10T15:01:09.793107: step 1970, loss 0.0705999, acc 0.984375, learning_rate 0.000101558
2017-10-10T15:01:10.085035: step 1971, loss 0.101252, acc 0.984375, learning_rate 0.000101552
2017-10-10T15:01:10.392915: step 1972, loss 0.14698, acc 0.9375, learning_rate 0.000101546
2017-10-10T15:01:10.692942: step 1973, loss 0.257056, acc 0.953125, learning_rate 0.000101539
2017-10-10T15:01:11.002755: step 1974, loss 0.0878706, acc 0.96875, learning_rate 0.000101533
2017-10-10T15:01:11.287415: step 1975, loss 0.182334, acc 0.953125, learning_rate 0.000101527
2017-10-10T15:01:11.673101: step 1976, loss 0.13619, acc 0.96875, learning_rate 0.00010152
2017-10-10T15:01:12.048604: step 1977, loss 0.0930825, acc 0.96875, learning_rate 0.000101514
2017-10-10T15:01:12.343614: step 1978, loss 0.076245, acc 0.984375, learning_rate 0.000101508
2017-10-10T15:01:13.602211: step 1979, loss 0.132007, acc 0.953125, learning_rate 0.000101502
2017-10-10T15:01:13.922722: step 1980, loss 0.159462, acc 0.953125, learning_rate 0.000101496
2017-10-10T15:01:14.236298: step 1981, loss 0.126191, acc 0.96875, learning_rate 0.00010149
2017-10-10T15:01:14.475834: step 1982, loss 0.163602, acc 0.953125, learning_rate 0.000101484
2017-10-10T15:01:14.820831: step 1983, loss 0.153208, acc 0.921875, learning_rate 0.000101478
2017-10-10T15:01:15.104925: step 1984, loss 0.169821, acc 0.9375, learning_rate 0.000101472
2017-10-10T15:01:15.397859: step 1985, loss 0.13868, acc 0.96875, learning_rate 0.000101466
2017-10-10T15:01:15.722612: step 1986, loss 0.142045, acc 0.96875, learning_rate 0.00010146
2017-10-10T15:01:16.001073: step 1987, loss 0.0849286, acc 0.953125, learning_rate 0.000101454
2017-10-10T15:01:16.354303: step 1988, loss 0.220921, acc 0.921875, learning_rate 0.000101448
2017-10-10T15:01:16.613429: step 1989, loss 0.227033, acc 0.890625, learning_rate 0.000101442
2017-10-10T15:01:16.915532: step 1990, loss 0.134717, acc 0.953125, learning_rate 0.000101436
2017-10-10T15:01:17.247121: step 1991, loss 0.143032, acc 0.9375, learning_rate 0.00010143
2017-10-10T15:01:17.585015: step 1992, loss 0.128696, acc 0.9375, learning_rate 0.000101424
2017-10-10T15:01:17.841378: step 1993, loss 0.21257, acc 0.921875, learning_rate 0.000101418
2017-10-10T15:01:18.077240: step 1994, loss 0.129982, acc 0.96875, learning_rate 0.000101413
2017-10-10T15:01:18.336046: step 1995, loss 0.0769357, acc 0.984375, learning_rate 0.000101407
2017-10-10T15:01:18.629041: step 1996, loss 0.13253, acc 0.953125, learning_rate 0.000101401
2017-10-10T15:01:18.879251: step 1997, loss 0.115173, acc 0.953125, learning_rate 0.000101395
2017-10-10T15:01:19.136177: step 1998, loss 0.217426, acc 0.875, learning_rate 0.00010139
2017-10-10T15:01:19.448206: step 1999, loss 0.121512, acc 0.96875, learning_rate 0.000101384
2017-10-10T15:01:19.720434: step 2000, loss 0.155148, acc 0.921875, learning_rate 0.000101378

Evaluation:
2017-10-10T15:01:20.263046: step 2000, loss 0.243017, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2000

2017-10-10T15:01:21.135672: step 2001, loss 0.0863645, acc 0.984375, learning_rate 0.000101373
2017-10-10T15:01:21.405209: step 2002, loss 0.152715, acc 0.9375, learning_rate 0.000101367
2017-10-10T15:01:21.753590: step 2003, loss 0.13942, acc 0.953125, learning_rate 0.000101362
2017-10-10T15:01:22.020427: step 2004, loss 0.155487, acc 0.953125, learning_rate 0.000101356
2017-10-10T15:01:22.328775: step 2005, loss 0.141514, acc 0.953125, learning_rate 0.00010135
2017-10-10T15:01:22.592865: step 2006, loss 0.163276, acc 0.9375, learning_rate 0.000101345
2017-10-10T15:01:22.934423: step 2007, loss 0.0742152, acc 1, learning_rate 0.000101339
2017-10-10T15:01:23.182529: step 2008, loss 0.172449, acc 0.9375, learning_rate 0.000101334
2017-10-10T15:01:23.497110: step 2009, loss 0.0722079, acc 0.984375, learning_rate 0.000101328
2017-10-10T15:01:23.840173: step 2010, loss 0.271557, acc 0.90625, learning_rate 0.000101323
2017-10-10T15:01:24.132926: step 2011, loss 0.114141, acc 0.953125, learning_rate 0.000101318
2017-10-10T15:01:24.432451: step 2012, loss 0.159299, acc 0.953125, learning_rate 0.000101312
2017-10-10T15:01:24.700962: step 2013, loss 0.156713, acc 0.921875, learning_rate 0.000101307
2017-10-10T15:01:25.039380: step 2014, loss 0.0833346, acc 0.984375, learning_rate 0.000101302
2017-10-10T15:01:25.347322: step 2015, loss 0.158606, acc 0.96875, learning_rate 0.000101296
2017-10-10T15:01:25.656965: step 2016, loss 0.129307, acc 0.96875, learning_rate 0.000101291
2017-10-10T15:01:25.928836: step 2017, loss 0.191734, acc 0.90625, learning_rate 0.000101286
2017-10-10T15:01:26.205039: step 2018, loss 0.0976681, acc 0.953125, learning_rate 0.00010128
2017-10-10T15:01:26.474838: step 2019, loss 0.0400373, acc 0.984375, learning_rate 0.000101275
2017-10-10T15:01:26.802961: step 2020, loss 0.174109, acc 0.96875, learning_rate 0.00010127
2017-10-10T15:01:27.075195: step 2021, loss 0.179063, acc 0.953125, learning_rate 0.000101265
2017-10-10T15:01:27.407680: step 2022, loss 0.243921, acc 0.9375, learning_rate 0.00010126
2017-10-10T15:01:27.724797: step 2023, loss 0.107499, acc 0.984375, learning_rate 0.000101255
2017-10-10T15:01:28.015247: step 2024, loss 0.109621, acc 0.96875, learning_rate 0.000101249
2017-10-10T15:01:28.314903: step 2025, loss 0.117953, acc 0.953125, learning_rate 0.000101244
2017-10-10T15:01:28.600925: step 2026, loss 0.122351, acc 0.953125, learning_rate 0.000101239
2017-10-10T15:01:28.903120: step 2027, loss 0.193989, acc 0.921875, learning_rate 0.000101234
2017-10-10T15:01:29.225154: step 2028, loss 0.117537, acc 0.953125, learning_rate 0.000101229
2017-10-10T15:01:29.499389: step 2029, loss 0.220391, acc 0.9375, learning_rate 0.000101224
2017-10-10T15:01:29.824823: step 2030, loss 0.0807678, acc 1, learning_rate 0.000101219
2017-10-10T15:01:30.160804: step 2031, loss 0.123878, acc 0.953125, learning_rate 0.000101214
2017-10-10T15:01:30.582898: step 2032, loss 0.0981947, acc 0.96875, learning_rate 0.000101209
2017-10-10T15:01:30.856863: step 2033, loss 0.107109, acc 0.96875, learning_rate 0.000101204
2017-10-10T15:01:31.148840: step 2034, loss 0.0883044, acc 0.96875, learning_rate 0.000101199
2017-10-10T15:01:31.414325: step 2035, loss 0.0729633, acc 0.984375, learning_rate 0.000101194
2017-10-10T15:01:31.622776: step 2036, loss 0.0961959, acc 0.96875, learning_rate 0.00010119
2017-10-10T15:01:31.968919: step 2037, loss 0.145862, acc 0.953125, learning_rate 0.000101185
2017-10-10T15:01:32.228829: step 2038, loss 0.172691, acc 0.953125, learning_rate 0.00010118
2017-10-10T15:01:32.580995: step 2039, loss 0.140599, acc 0.96875, learning_rate 0.000101175
2017-10-10T15:01:32.897639: step 2040, loss 0.15982, acc 0.953125, learning_rate 0.00010117

Evaluation:
2017-10-10T15:01:33.332789: step 2040, loss 0.242351, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2040

2017-10-10T15:01:34.392897: step 2041, loss 0.130867, acc 0.96875, learning_rate 0.000101166
2017-10-10T15:01:34.674524: step 2042, loss 0.175658, acc 0.953125, learning_rate 0.000101161
2017-10-10T15:01:34.947816: step 2043, loss 0.172903, acc 0.9375, learning_rate 0.000101156
2017-10-10T15:01:35.192929: step 2044, loss 0.115194, acc 0.984375, learning_rate 0.000101151
2017-10-10T15:01:35.536982: step 2045, loss 0.109515, acc 0.96875, learning_rate 0.000101147
2017-10-10T15:01:35.851522: step 2046, loss 0.10753, acc 0.984375, learning_rate 0.000101142
2017-10-10T15:01:36.199141: step 2047, loss 0.265771, acc 0.90625, learning_rate 0.000101137
2017-10-10T15:01:36.505032: step 2048, loss 0.147016, acc 0.953125, learning_rate 0.000101133
2017-10-10T15:01:36.764934: step 2049, loss 0.14313, acc 0.9375, learning_rate 0.000101128
2017-10-10T15:01:37.053049: step 2050, loss 0.116395, acc 0.984375, learning_rate 0.000101123
2017-10-10T15:01:37.387169: step 2051, loss 0.100899, acc 0.96875, learning_rate 0.000101119
2017-10-10T15:01:37.745560: step 2052, loss 0.101783, acc 0.984375, learning_rate 0.000101114
2017-10-10T15:01:38.045404: step 2053, loss 0.137987, acc 0.984375, learning_rate 0.00010111
2017-10-10T15:01:38.392829: step 2054, loss 0.160927, acc 0.953125, learning_rate 0.000101105
2017-10-10T15:01:38.705037: step 2055, loss 0.176539, acc 0.9375, learning_rate 0.000101101
2017-10-10T15:01:39.013583: step 2056, loss 0.0816155, acc 0.984375, learning_rate 0.000101096
2017-10-10T15:01:39.316410: step 2057, loss 0.113079, acc 0.9375, learning_rate 0.000101092
2017-10-10T15:01:39.586238: step 2058, loss 0.298473, acc 0.901961, learning_rate 0.000101087
2017-10-10T15:01:39.895032: step 2059, loss 0.0684631, acc 0.96875, learning_rate 0.000101083
2017-10-10T15:01:40.216204: step 2060, loss 0.0756735, acc 0.96875, learning_rate 0.000101078
2017-10-10T15:01:40.565016: step 2061, loss 0.215389, acc 0.921875, learning_rate 0.000101074
2017-10-10T15:01:40.843071: step 2062, loss 0.274488, acc 0.921875, learning_rate 0.00010107
2017-10-10T15:01:41.167942: step 2063, loss 0.0848707, acc 0.96875, learning_rate 0.000101065
2017-10-10T15:01:41.512844: step 2064, loss 0.0893291, acc 0.984375, learning_rate 0.000101061
2017-10-10T15:01:41.813795: step 2065, loss 0.172129, acc 0.953125, learning_rate 0.000101057
2017-10-10T15:01:42.090231: step 2066, loss 0.183471, acc 0.9375, learning_rate 0.000101052
2017-10-10T15:01:42.263935: step 2067, loss 0.18828, acc 0.9375, learning_rate 0.000101048
2017-10-10T15:01:42.576875: step 2068, loss 0.0691926, acc 0.984375, learning_rate 0.000101044
2017-10-10T15:01:42.891800: step 2069, loss 0.062703, acc 0.96875, learning_rate 0.000101039
2017-10-10T15:01:43.184066: step 2070, loss 0.140823, acc 0.953125, learning_rate 0.000101035
2017-10-10T15:01:43.426275: step 2071, loss 0.25526, acc 0.921875, learning_rate 0.000101031
2017-10-10T15:01:43.758471: step 2072, loss 0.0934948, acc 0.984375, learning_rate 0.000101027
2017-10-10T15:01:44.009220: step 2073, loss 0.207132, acc 0.9375, learning_rate 0.000101023
2017-10-10T15:01:44.282264: step 2074, loss 0.197588, acc 0.953125, learning_rate 0.000101018
2017-10-10T15:01:44.549733: step 2075, loss 0.157375, acc 0.96875, learning_rate 0.000101014
2017-10-10T15:01:44.825147: step 2076, loss 0.251651, acc 0.9375, learning_rate 0.00010101
2017-10-10T15:01:45.137104: step 2077, loss 0.165, acc 0.9375, learning_rate 0.000101006
2017-10-10T15:01:45.459620: step 2078, loss 0.320687, acc 0.890625, learning_rate 0.000101002
2017-10-10T15:01:45.804984: step 2079, loss 0.198659, acc 0.9375, learning_rate 0.000100998
2017-10-10T15:01:46.060723: step 2080, loss 0.235824, acc 0.921875, learning_rate 0.000100994

Evaluation:
2017-10-10T15:01:46.550225: step 2080, loss 0.241956, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2080

2017-10-10T15:01:47.657013: step 2081, loss 0.125552, acc 0.96875, learning_rate 0.00010099
2017-10-10T15:01:48.027253: step 2082, loss 0.175885, acc 0.9375, learning_rate 0.000100986
2017-10-10T15:01:48.376692: step 2083, loss 0.253426, acc 0.890625, learning_rate 0.000100982
2017-10-10T15:01:48.680959: step 2084, loss 0.168559, acc 0.984375, learning_rate 0.000100978
2017-10-10T15:01:49.099490: step 2085, loss 0.124784, acc 0.953125, learning_rate 0.000100974
2017-10-10T15:01:49.346971: step 2086, loss 0.132391, acc 0.9375, learning_rate 0.00010097
2017-10-10T15:01:49.627128: step 2087, loss 0.0955123, acc 0.96875, learning_rate 0.000100966
2017-10-10T15:01:49.843818: step 2088, loss 0.179468, acc 0.953125, learning_rate 0.000100962
2017-10-10T15:01:50.073814: step 2089, loss 0.2199, acc 0.9375, learning_rate 0.000100958
2017-10-10T15:01:50.435718: step 2090, loss 0.0959185, acc 0.96875, learning_rate 0.000100954
2017-10-10T15:01:50.784836: step 2091, loss 0.192687, acc 0.90625, learning_rate 0.00010095
2017-10-10T15:01:51.039243: step 2092, loss 0.0978121, acc 0.96875, learning_rate 0.000100946
2017-10-10T15:01:51.306486: step 2093, loss 0.114679, acc 0.953125, learning_rate 0.000100942
2017-10-10T15:01:51.516918: step 2094, loss 0.258128, acc 0.875, learning_rate 0.000100938
2017-10-10T15:01:51.828935: step 2095, loss 0.181013, acc 0.9375, learning_rate 0.000100935
2017-10-10T15:01:52.199447: step 2096, loss 0.125444, acc 0.96875, learning_rate 0.000100931
2017-10-10T15:01:52.508963: step 2097, loss 0.0917272, acc 0.96875, learning_rate 0.000100927
2017-10-10T15:01:52.809895: step 2098, loss 0.186523, acc 0.921875, learning_rate 0.000100923
2017-10-10T15:01:53.149066: step 2099, loss 0.0854189, acc 1, learning_rate 0.000100919
2017-10-10T15:01:53.468837: step 2100, loss 0.224244, acc 0.9375, learning_rate 0.000100916
2017-10-10T15:01:53.808537: step 2101, loss 0.053482, acc 0.984375, learning_rate 0.000100912
2017-10-10T15:01:54.136962: step 2102, loss 0.0848276, acc 0.96875, learning_rate 0.000100908
2017-10-10T15:01:54.327382: step 2103, loss 0.158365, acc 0.9375, learning_rate 0.000100904
2017-10-10T15:01:54.677734: step 2104, loss 0.151972, acc 0.9375, learning_rate 0.000100901
2017-10-10T15:01:54.938237: step 2105, loss 0.210739, acc 0.921875, learning_rate 0.000100897
2017-10-10T15:01:55.212742: step 2106, loss 0.110822, acc 0.96875, learning_rate 0.000100893
2017-10-10T15:01:55.477199: step 2107, loss 0.0946215, acc 0.984375, learning_rate 0.00010089
2017-10-10T15:01:55.796924: step 2108, loss 0.135694, acc 0.953125, learning_rate 0.000100886
2017-10-10T15:01:56.126225: step 2109, loss 0.0679118, acc 0.984375, learning_rate 0.000100883
2017-10-10T15:01:56.420892: step 2110, loss 0.104766, acc 0.953125, learning_rate 0.000100879
2017-10-10T15:01:56.734900: step 2111, loss 0.163495, acc 0.953125, learning_rate 0.000100875
2017-10-10T15:01:57.028838: step 2112, loss 0.250829, acc 0.9375, learning_rate 0.000100872
2017-10-10T15:01:57.284940: step 2113, loss 0.0875856, acc 0.953125, learning_rate 0.000100868
2017-10-10T15:01:57.587898: step 2114, loss 0.177982, acc 0.9375, learning_rate 0.000100865
2017-10-10T15:01:57.964917: step 2115, loss 0.150622, acc 0.9375, learning_rate 0.000100861
2017-10-10T15:01:58.312897: step 2116, loss 0.0492763, acc 1, learning_rate 0.000100858
2017-10-10T15:01:58.627678: step 2117, loss 0.090868, acc 0.96875, learning_rate 0.000100854
2017-10-10T15:01:58.867968: step 2118, loss 0.0713748, acc 0.984375, learning_rate 0.000100851
2017-10-10T15:01:59.133424: step 2119, loss 0.242066, acc 0.90625, learning_rate 0.000100847
2017-10-10T15:01:59.429627: step 2120, loss 0.132319, acc 0.953125, learning_rate 0.000100844

Evaluation:
2017-10-10T15:01:59.977080: step 2120, loss 0.243452, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2120

2017-10-10T15:02:01.119323: step 2121, loss 0.0622914, acc 1, learning_rate 0.00010084
2017-10-10T15:02:01.428774: step 2122, loss 0.190242, acc 0.921875, learning_rate 0.000100837
2017-10-10T15:02:01.704824: step 2123, loss 0.107296, acc 0.953125, learning_rate 0.000100833
2017-10-10T15:02:02.004986: step 2124, loss 0.0729341, acc 0.984375, learning_rate 0.00010083
2017-10-10T15:02:02.356699: step 2125, loss 0.0614748, acc 1, learning_rate 0.000100827
2017-10-10T15:02:02.682153: step 2126, loss 0.150672, acc 0.953125, learning_rate 0.000100823
2017-10-10T15:02:02.976850: step 2127, loss 0.194834, acc 0.9375, learning_rate 0.00010082
2017-10-10T15:02:03.264445: step 2128, loss 0.105798, acc 0.96875, learning_rate 0.000100817
2017-10-10T15:02:03.551399: step 2129, loss 0.18165, acc 0.921875, learning_rate 0.000100813
2017-10-10T15:02:03.873335: step 2130, loss 0.158343, acc 0.9375, learning_rate 0.00010081
2017-10-10T15:02:04.224601: step 2131, loss 0.150137, acc 0.9375, learning_rate 0.000100807
2017-10-10T15:02:04.586739: step 2132, loss 0.167497, acc 0.9375, learning_rate 0.000100803
2017-10-10T15:02:04.833030: step 2133, loss 0.213331, acc 0.9375, learning_rate 0.0001008
2017-10-10T15:02:05.173247: step 2134, loss 0.147389, acc 0.921875, learning_rate 0.000100797
2017-10-10T15:02:05.446338: step 2135, loss 0.0802769, acc 0.96875, learning_rate 0.000100793
2017-10-10T15:02:05.749929: step 2136, loss 0.120542, acc 0.953125, learning_rate 0.00010079
2017-10-10T15:02:06.144930: step 2137, loss 0.216537, acc 0.921875, learning_rate 0.000100787
2017-10-10T15:02:06.386127: step 2138, loss 0.107606, acc 0.984375, learning_rate 0.000100784
2017-10-10T15:02:06.660837: step 2139, loss 0.174199, acc 0.96875, learning_rate 0.000100781
2017-10-10T15:02:06.979294: step 2140, loss 0.225105, acc 0.890625, learning_rate 0.000100777
2017-10-10T15:02:07.315170: step 2141, loss 0.0898814, acc 0.96875, learning_rate 0.000100774
2017-10-10T15:02:07.692070: step 2142, loss 0.130973, acc 0.953125, learning_rate 0.000100771
2017-10-10T15:02:07.897260: step 2143, loss 0.115039, acc 0.96875, learning_rate 0.000100768
2017-10-10T15:02:08.091175: step 2144, loss 0.131576, acc 0.9375, learning_rate 0.000100765
2017-10-10T15:02:08.299965: step 2145, loss 0.134098, acc 0.96875, learning_rate 0.000100762
2017-10-10T15:02:08.568899: step 2146, loss 0.14968, acc 0.96875, learning_rate 0.000100759
2017-10-10T15:02:08.902973: step 2147, loss 0.0758701, acc 1, learning_rate 0.000100755
2017-10-10T15:02:09.239546: step 2148, loss 0.131384, acc 0.96875, learning_rate 0.000100752
2017-10-10T15:02:09.557427: step 2149, loss 0.195203, acc 0.953125, learning_rate 0.000100749
2017-10-10T15:02:09.854912: step 2150, loss 0.176168, acc 0.921875, learning_rate 0.000100746
2017-10-10T15:02:10.133187: step 2151, loss 0.225413, acc 0.953125, learning_rate 0.000100743
2017-10-10T15:02:10.456142: step 2152, loss 0.216035, acc 0.9375, learning_rate 0.00010074
2017-10-10T15:02:10.750656: step 2153, loss 0.100298, acc 0.984375, learning_rate 0.000100737
2017-10-10T15:02:11.074959: step 2154, loss 0.129353, acc 0.96875, learning_rate 0.000100734
2017-10-10T15:02:11.342416: step 2155, loss 0.21326, acc 0.9375, learning_rate 0.000100731
2017-10-10T15:02:11.574049: step 2156, loss 0.123125, acc 0.960784, learning_rate 0.000100728
2017-10-10T15:02:11.894166: step 2157, loss 0.117818, acc 0.953125, learning_rate 0.000100725
2017-10-10T15:02:12.165364: step 2158, loss 0.0966853, acc 0.96875, learning_rate 0.000100722
2017-10-10T15:02:12.452817: step 2159, loss 0.149857, acc 0.96875, learning_rate 0.000100719
2017-10-10T15:02:12.768102: step 2160, loss 0.12532, acc 0.96875, learning_rate 0.000100716

Evaluation:
2017-10-10T15:02:13.254960: step 2160, loss 0.242348, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2160

2017-10-10T15:02:14.272917: step 2161, loss 0.132838, acc 0.984375, learning_rate 0.000100713
2017-10-10T15:02:14.573239: step 2162, loss 0.068199, acc 0.984375, learning_rate 0.000100711
2017-10-10T15:02:14.868958: step 2163, loss 0.0738389, acc 0.96875, learning_rate 0.000100708
2017-10-10T15:02:15.128909: step 2164, loss 0.246803, acc 0.90625, learning_rate 0.000100705
2017-10-10T15:02:15.392903: step 2165, loss 0.1902, acc 0.9375, learning_rate 0.000100702
2017-10-10T15:02:15.722001: step 2166, loss 0.212626, acc 0.921875, learning_rate 0.000100699
2017-10-10T15:02:16.040878: step 2167, loss 0.0650498, acc 1, learning_rate 0.000100696
2017-10-10T15:02:16.272366: step 2168, loss 0.112597, acc 0.96875, learning_rate 0.000100693
2017-10-10T15:02:16.578719: step 2169, loss 0.10704, acc 0.96875, learning_rate 0.00010069
2017-10-10T15:02:16.827314: step 2170, loss 0.138727, acc 0.953125, learning_rate 0.000100688
2017-10-10T15:02:17.092887: step 2171, loss 0.213484, acc 0.9375, learning_rate 0.000100685
2017-10-10T15:02:17.344069: step 2172, loss 0.207453, acc 0.921875, learning_rate 0.000100682
2017-10-10T15:02:17.586253: step 2173, loss 0.208712, acc 0.921875, learning_rate 0.000100679
2017-10-10T15:02:17.788880: step 2174, loss 0.186993, acc 0.921875, learning_rate 0.000100677
2017-10-10T15:02:17.988846: step 2175, loss 0.130228, acc 0.96875, learning_rate 0.000100674
2017-10-10T15:02:18.307664: step 2176, loss 0.117982, acc 0.96875, learning_rate 0.000100671
2017-10-10T15:02:18.471882: step 2177, loss 0.0721749, acc 1, learning_rate 0.000100668
2017-10-10T15:02:18.678651: step 2178, loss 0.193339, acc 0.9375, learning_rate 0.000100666
2017-10-10T15:02:18.925710: step 2179, loss 0.142792, acc 0.953125, learning_rate 0.000100663
2017-10-10T15:02:19.211737: step 2180, loss 0.169331, acc 0.921875, learning_rate 0.00010066
2017-10-10T15:02:19.451249: step 2181, loss 0.275477, acc 0.875, learning_rate 0.000100657
2017-10-10T15:02:19.710078: step 2182, loss 0.151712, acc 0.953125, learning_rate 0.000100655
2017-10-10T15:02:19.947506: step 2183, loss 0.111803, acc 0.96875, learning_rate 0.000100652
2017-10-10T15:02:20.242513: step 2184, loss 0.160707, acc 0.96875, learning_rate 0.000100649
2017-10-10T15:02:20.557273: step 2185, loss 0.0719434, acc 1, learning_rate 0.000100647
2017-10-10T15:02:20.836736: step 2186, loss 0.124904, acc 0.953125, learning_rate 0.000100644
2017-10-10T15:02:21.112915: step 2187, loss 0.130983, acc 0.9375, learning_rate 0.000100641
2017-10-10T15:02:21.398460: step 2188, loss 0.238404, acc 0.875, learning_rate 0.000100639
2017-10-10T15:02:21.669475: step 2189, loss 0.0743442, acc 0.96875, learning_rate 0.000100636
2017-10-10T15:02:21.955652: step 2190, loss 0.0645918, acc 1, learning_rate 0.000100634
2017-10-10T15:02:22.256465: step 2191, loss 0.154781, acc 0.9375, learning_rate 0.000100631
2017-10-10T15:02:22.525169: step 2192, loss 0.16326, acc 0.9375, learning_rate 0.000100628
2017-10-10T15:02:22.903280: step 2193, loss 0.185414, acc 0.9375, learning_rate 0.000100626
2017-10-10T15:02:23.259744: step 2194, loss 0.0749415, acc 0.984375, learning_rate 0.000100623
2017-10-10T15:02:23.542521: step 2195, loss 0.0891535, acc 0.984375, learning_rate 0.000100621
2017-10-10T15:02:23.832811: step 2196, loss 0.182379, acc 0.96875, learning_rate 0.000100618
2017-10-10T15:02:24.132873: step 2197, loss 0.107864, acc 0.953125, learning_rate 0.000100616
2017-10-10T15:02:24.437002: step 2198, loss 0.0601812, acc 1, learning_rate 0.000100613
2017-10-10T15:02:24.805045: step 2199, loss 0.0768417, acc 0.984375, learning_rate 0.000100611
2017-10-10T15:02:25.068213: step 2200, loss 0.223416, acc 0.875, learning_rate 0.000100608

Evaluation:
2017-10-10T15:02:25.509592: step 2200, loss 0.24016, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2200

2017-10-10T15:02:26.602781: step 2201, loss 0.0732208, acc 1, learning_rate 0.000100606
2017-10-10T15:02:26.941345: step 2202, loss 0.210014, acc 0.890625, learning_rate 0.000100603
2017-10-10T15:02:27.295396: step 2203, loss 0.123149, acc 0.96875, learning_rate 0.000100601
2017-10-10T15:02:27.591004: step 2204, loss 0.0841686, acc 0.984375, learning_rate 0.000100598
2017-10-10T15:02:27.984909: step 2205, loss 0.106494, acc 0.96875, learning_rate 0.000100596
2017-10-10T15:02:28.316101: step 2206, loss 0.107494, acc 0.96875, learning_rate 0.000100594
2017-10-10T15:02:28.649916: step 2207, loss 0.125984, acc 0.9375, learning_rate 0.000100591
2017-10-10T15:02:28.891783: step 2208, loss 0.132092, acc 0.96875, learning_rate 0.000100589
2017-10-10T15:02:29.172818: step 2209, loss 0.132393, acc 0.96875, learning_rate 0.000100586
2017-10-10T15:02:29.517398: step 2210, loss 0.109314, acc 0.984375, learning_rate 0.000100584
2017-10-10T15:02:29.832209: step 2211, loss 0.0656631, acc 0.984375, learning_rate 0.000100581
2017-10-10T15:02:30.105483: step 2212, loss 0.129512, acc 0.96875, learning_rate 0.000100579
2017-10-10T15:02:30.383392: step 2213, loss 0.0888856, acc 0.953125, learning_rate 0.000100577
2017-10-10T15:02:30.635566: step 2214, loss 0.0939056, acc 0.96875, learning_rate 0.000100574
2017-10-10T15:02:30.928377: step 2215, loss 0.168669, acc 0.921875, learning_rate 0.000100572
2017-10-10T15:02:31.301338: step 2216, loss 0.150455, acc 0.953125, learning_rate 0.00010057
2017-10-10T15:02:31.624891: step 2217, loss 0.176907, acc 0.953125, learning_rate 0.000100567
2017-10-10T15:02:31.915715: step 2218, loss 0.255181, acc 0.921875, learning_rate 0.000100565
2017-10-10T15:02:32.184834: step 2219, loss 0.0821077, acc 0.984375, learning_rate 0.000100563
2017-10-10T15:02:32.491318: step 2220, loss 0.116273, acc 0.953125, learning_rate 0.00010056
2017-10-10T15:02:32.802591: step 2221, loss 0.0923937, acc 0.96875, learning_rate 0.000100558
2017-10-10T15:02:33.067584: step 2222, loss 0.0993887, acc 0.984375, learning_rate 0.000100556
2017-10-10T15:02:33.388519: step 2223, loss 0.300193, acc 0.921875, learning_rate 0.000100554
2017-10-10T15:02:33.693106: step 2224, loss 0.170538, acc 0.9375, learning_rate 0.000100551
2017-10-10T15:02:33.968996: step 2225, loss 0.110373, acc 0.96875, learning_rate 0.000100549
2017-10-10T15:02:34.263841: step 2226, loss 0.232033, acc 0.9375, learning_rate 0.000100547
2017-10-10T15:02:34.491087: step 2227, loss 0.140683, acc 0.96875, learning_rate 0.000100545
2017-10-10T15:02:34.803448: step 2228, loss 0.121309, acc 0.953125, learning_rate 0.000100542
2017-10-10T15:02:35.133752: step 2229, loss 0.151672, acc 0.953125, learning_rate 0.00010054
2017-10-10T15:02:35.434814: step 2230, loss 0.093963, acc 0.953125, learning_rate 0.000100538
2017-10-10T15:02:35.747027: step 2231, loss 0.103787, acc 0.953125, learning_rate 0.000100536
2017-10-10T15:02:36.033343: step 2232, loss 0.120653, acc 0.953125, learning_rate 0.000100534
2017-10-10T15:02:36.395813: step 2233, loss 0.154856, acc 0.953125, learning_rate 0.000100531
2017-10-10T15:02:36.672817: step 2234, loss 0.224636, acc 0.953125, learning_rate 0.000100529
2017-10-10T15:02:37.025086: step 2235, loss 0.149043, acc 0.921875, learning_rate 0.000100527
2017-10-10T15:02:37.261817: step 2236, loss 0.161838, acc 0.9375, learning_rate 0.000100525
2017-10-10T15:02:37.612888: step 2237, loss 0.100195, acc 0.984375, learning_rate 0.000100523
2017-10-10T15:02:37.966580: step 2238, loss 0.195492, acc 0.953125, learning_rate 0.000100521
2017-10-10T15:02:38.262581: step 2239, loss 0.108204, acc 0.984375, learning_rate 0.000100519
2017-10-10T15:02:38.580901: step 2240, loss 0.189704, acc 0.890625, learning_rate 0.000100516

Evaluation:
2017-10-10T15:02:39.096814: step 2240, loss 0.240934, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2240

2017-10-10T15:02:40.188830: step 2241, loss 0.163148, acc 0.953125, learning_rate 0.000100514
2017-10-10T15:02:40.474988: step 2242, loss 0.101022, acc 1, learning_rate 0.000100512
2017-10-10T15:02:40.756274: step 2243, loss 0.1839, acc 0.921875, learning_rate 0.00010051
2017-10-10T15:02:40.993019: step 2244, loss 0.0755201, acc 0.96875, learning_rate 0.000100508
2017-10-10T15:02:41.284833: step 2245, loss 0.0839172, acc 0.984375, learning_rate 0.000100506
2017-10-10T15:02:41.618924: step 2246, loss 0.24523, acc 0.921875, learning_rate 0.000100504
2017-10-10T15:02:41.920913: step 2247, loss 0.122775, acc 0.9375, learning_rate 0.000100502
2017-10-10T15:02:42.280070: step 2248, loss 0.180588, acc 0.9375, learning_rate 0.0001005
2017-10-10T15:02:42.597816: step 2249, loss 0.0885158, acc 0.984375, learning_rate 0.000100498
2017-10-10T15:02:42.860897: step 2250, loss 0.135474, acc 0.9375, learning_rate 0.000100496
2017-10-10T15:02:43.189010: step 2251, loss 0.202908, acc 0.875, learning_rate 0.000100494
2017-10-10T15:02:43.408821: step 2252, loss 0.125786, acc 0.921875, learning_rate 0.000100492
2017-10-10T15:02:43.712199: step 2253, loss 0.137964, acc 0.953125, learning_rate 0.00010049
2017-10-10T15:02:44.062886: step 2254, loss 0.175002, acc 0.960784, learning_rate 0.000100488
2017-10-10T15:02:44.330581: step 2255, loss 0.206721, acc 0.9375, learning_rate 0.000100486
2017-10-10T15:02:44.618265: step 2256, loss 0.185091, acc 0.9375, learning_rate 0.000100484
2017-10-10T15:02:44.911988: step 2257, loss 0.147135, acc 0.9375, learning_rate 0.000100482
2017-10-10T15:02:45.208923: step 2258, loss 0.0758262, acc 1, learning_rate 0.00010048
2017-10-10T15:02:45.546237: step 2259, loss 0.0707639, acc 0.984375, learning_rate 0.000100478
2017-10-10T15:02:45.800729: step 2260, loss 0.0933823, acc 0.96875, learning_rate 0.000100476
2017-10-10T15:02:46.116857: step 2261, loss 0.054522, acc 0.984375, learning_rate 0.000100474
2017-10-10T15:02:46.405448: step 2262, loss 0.123988, acc 0.953125, learning_rate 0.000100472
2017-10-10T15:02:46.727193: step 2263, loss 0.159872, acc 0.9375, learning_rate 0.00010047
2017-10-10T15:02:47.072975: step 2264, loss 0.0688908, acc 0.984375, learning_rate 0.000100468
2017-10-10T15:02:47.364951: step 2265, loss 0.173519, acc 0.890625, learning_rate 0.000100466
2017-10-10T15:02:47.624883: step 2266, loss 0.178695, acc 0.9375, learning_rate 0.000100464
2017-10-10T15:02:47.945198: step 2267, loss 0.169818, acc 0.9375, learning_rate 0.000100462
2017-10-10T15:02:48.276285: step 2268, loss 0.0963241, acc 0.96875, learning_rate 0.000100461
2017-10-10T15:02:48.561887: step 2269, loss 0.116342, acc 0.96875, learning_rate 0.000100459
2017-10-10T15:02:48.866765: step 2270, loss 0.125167, acc 0.984375, learning_rate 0.000100457
2017-10-10T15:02:49.112892: step 2271, loss 0.115477, acc 0.953125, learning_rate 0.000100455
2017-10-10T15:02:49.372091: step 2272, loss 0.126677, acc 0.953125, learning_rate 0.000100453
2017-10-10T15:02:49.722508: step 2273, loss 0.198928, acc 0.921875, learning_rate 0.000100451
2017-10-10T15:02:50.003135: step 2274, loss 0.173596, acc 0.9375, learning_rate 0.000100449
2017-10-10T15:02:50.311565: step 2275, loss 0.196071, acc 0.921875, learning_rate 0.000100448
2017-10-10T15:02:50.617499: step 2276, loss 0.143904, acc 0.953125, learning_rate 0.000100446
2017-10-10T15:02:50.901368: step 2277, loss 0.181179, acc 0.921875, learning_rate 0.000100444
2017-10-10T15:02:51.213006: step 2278, loss 0.153242, acc 0.953125, learning_rate 0.000100442
2017-10-10T15:02:51.484850: step 2279, loss 0.198807, acc 0.921875, learning_rate 0.00010044
2017-10-10T15:02:51.813614: step 2280, loss 0.166043, acc 0.9375, learning_rate 0.000100439

Evaluation:
2017-10-10T15:02:52.313187: step 2280, loss 0.240318, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2280

2017-10-10T15:02:53.501095: step 2281, loss 0.16614, acc 0.96875, learning_rate 0.000100437
2017-10-10T15:02:53.837304: step 2282, loss 0.172944, acc 0.96875, learning_rate 0.000100435
2017-10-10T15:02:54.205526: step 2283, loss 0.169112, acc 0.953125, learning_rate 0.000100433
2017-10-10T15:02:54.501863: step 2284, loss 0.135279, acc 0.953125, learning_rate 0.000100431
2017-10-10T15:02:54.728253: step 2285, loss 0.191276, acc 0.96875, learning_rate 0.00010043
2017-10-10T15:02:55.016120: step 2286, loss 0.161138, acc 0.9375, learning_rate 0.000100428
2017-10-10T15:02:55.328923: step 2287, loss 0.160153, acc 0.90625, learning_rate 0.000100426
2017-10-10T15:02:55.589030: step 2288, loss 0.108024, acc 0.984375, learning_rate 0.000100424
2017-10-10T15:02:55.861823: step 2289, loss 0.126676, acc 0.953125, learning_rate 0.000100423
2017-10-10T15:02:56.211997: step 2290, loss 0.167991, acc 0.953125, learning_rate 0.000100421
2017-10-10T15:02:56.580881: step 2291, loss 0.153491, acc 0.953125, learning_rate 0.000100419
2017-10-10T15:02:56.963286: step 2292, loss 0.0322853, acc 1, learning_rate 0.000100418
2017-10-10T15:02:57.212315: step 2293, loss 0.166966, acc 0.9375, learning_rate 0.000100416
2017-10-10T15:02:57.492824: step 2294, loss 0.0911729, acc 0.984375, learning_rate 0.000100414
2017-10-10T15:02:57.755838: step 2295, loss 0.0757423, acc 0.984375, learning_rate 0.000100412
2017-10-10T15:02:58.008989: step 2296, loss 0.119736, acc 0.953125, learning_rate 0.000100411
2017-10-10T15:02:58.283869: step 2297, loss 0.1452, acc 0.953125, learning_rate 0.000100409
2017-10-10T15:02:58.579616: step 2298, loss 0.249705, acc 0.890625, learning_rate 0.000100407
2017-10-10T15:02:58.881276: step 2299, loss 0.169054, acc 0.9375, learning_rate 0.000100406
2017-10-10T15:02:59.181075: step 2300, loss 0.162327, acc 0.96875, learning_rate 0.000100404
2017-10-10T15:02:59.503152: step 2301, loss 0.207157, acc 0.9375, learning_rate 0.000100402
2017-10-10T15:02:59.824999: step 2302, loss 0.18667, acc 0.9375, learning_rate 0.000100401
2017-10-10T15:03:00.088553: step 2303, loss 0.0834657, acc 0.96875, learning_rate 0.000100399
2017-10-10T15:03:00.389139: step 2304, loss 0.104397, acc 0.984375, learning_rate 0.000100398
2017-10-10T15:03:00.670505: step 2305, loss 0.0881256, acc 0.96875, learning_rate 0.000100396
2017-10-10T15:03:01.016957: step 2306, loss 0.0764669, acc 0.984375, learning_rate 0.000100394
2017-10-10T15:03:01.413362: step 2307, loss 0.106364, acc 0.96875, learning_rate 0.000100393
2017-10-10T15:03:01.713101: step 2308, loss 0.202989, acc 0.9375, learning_rate 0.000100391
2017-10-10T15:03:01.923102: step 2309, loss 0.193692, acc 0.921875, learning_rate 0.000100389
2017-10-10T15:03:02.168888: step 2310, loss 0.104485, acc 0.96875, learning_rate 0.000100388
2017-10-10T15:03:02.435779: step 2311, loss 0.0463544, acc 1, learning_rate 0.000100386
2017-10-10T15:03:02.720995: step 2312, loss 0.099016, acc 0.96875, learning_rate 0.000100385
2017-10-10T15:03:03.028856: step 2313, loss 0.0852044, acc 0.96875, learning_rate 0.000100383
2017-10-10T15:03:03.333481: step 2314, loss 0.109027, acc 0.96875, learning_rate 0.000100382
2017-10-10T15:03:03.607755: step 2315, loss 0.140479, acc 0.96875, learning_rate 0.00010038
2017-10-10T15:03:03.839654: step 2316, loss 0.175452, acc 0.953125, learning_rate 0.000100378
2017-10-10T15:03:04.129157: step 2317, loss 0.111002, acc 0.984375, learning_rate 0.000100377
2017-10-10T15:03:04.404300: step 2318, loss 0.086227, acc 0.984375, learning_rate 0.000100375
2017-10-10T15:03:04.719343: step 2319, loss 0.132588, acc 0.984375, learning_rate 0.000100374
2017-10-10T15:03:05.065819: step 2320, loss 0.148743, acc 0.9375, learning_rate 0.000100372

Evaluation:
2017-10-10T15:03:05.516130: step 2320, loss 0.241931, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2320

2017-10-10T15:03:06.492814: step 2321, loss 0.162703, acc 0.953125, learning_rate 0.000100371
2017-10-10T15:03:06.800856: step 2322, loss 0.13125, acc 0.9375, learning_rate 0.000100369
2017-10-10T15:03:07.008749: step 2323, loss 0.103691, acc 0.96875, learning_rate 0.000100368
2017-10-10T15:03:07.259894: step 2324, loss 0.0739742, acc 0.984375, learning_rate 0.000100366
2017-10-10T15:03:07.541167: step 2325, loss 0.146359, acc 0.9375, learning_rate 0.000100365
2017-10-10T15:03:07.861000: step 2326, loss 0.0829348, acc 0.96875, learning_rate 0.000100363
2017-10-10T15:03:08.133043: step 2327, loss 0.262187, acc 0.953125, learning_rate 0.000100362
2017-10-10T15:03:08.449331: step 2328, loss 0.0687427, acc 0.984375, learning_rate 0.00010036
2017-10-10T15:03:08.746824: step 2329, loss 0.183194, acc 0.9375, learning_rate 0.000100359
2017-10-10T15:03:09.099931: step 2330, loss 0.209021, acc 0.90625, learning_rate 0.000100357
2017-10-10T15:03:09.409348: step 2331, loss 0.156544, acc 0.953125, learning_rate 0.000100356
2017-10-10T15:03:09.797117: step 2332, loss 0.0471867, acc 1, learning_rate 0.000100354
2017-10-10T15:03:10.093426: step 2333, loss 0.189602, acc 0.9375, learning_rate 0.000100353
2017-10-10T15:03:10.416880: step 2334, loss 0.215971, acc 0.90625, learning_rate 0.000100352
2017-10-10T15:03:10.693021: step 2335, loss 0.0890404, acc 0.984375, learning_rate 0.00010035
2017-10-10T15:03:10.998572: step 2336, loss 0.209485, acc 0.90625, learning_rate 0.000100349
2017-10-10T15:03:11.297244: step 2337, loss 0.20437, acc 0.921875, learning_rate 0.000100347
2017-10-10T15:03:11.609623: step 2338, loss 0.14104, acc 0.96875, learning_rate 0.000100346
2017-10-10T15:03:11.860961: step 2339, loss 0.230061, acc 0.9375, learning_rate 0.000100344
2017-10-10T15:03:12.141137: step 2340, loss 0.101896, acc 0.984375, learning_rate 0.000100343
2017-10-10T15:03:12.464699: step 2341, loss 0.140721, acc 0.96875, learning_rate 0.000100342
2017-10-10T15:03:12.801434: step 2342, loss 0.111121, acc 0.953125, learning_rate 0.00010034
2017-10-10T15:03:13.112959: step 2343, loss 0.144456, acc 0.953125, learning_rate 0.000100339
2017-10-10T15:03:13.513125: step 2344, loss 0.112943, acc 0.96875, learning_rate 0.000100338
2017-10-10T15:03:13.792721: step 2345, loss 0.165862, acc 0.953125, learning_rate 0.000100336
2017-10-10T15:03:14.055565: step 2346, loss 0.325187, acc 0.890625, learning_rate 0.000100335
2017-10-10T15:03:14.375325: step 2347, loss 0.167636, acc 0.9375, learning_rate 0.000100333
2017-10-10T15:03:14.644861: step 2348, loss 0.122211, acc 0.984375, learning_rate 0.000100332
2017-10-10T15:03:15.013067: step 2349, loss 0.123883, acc 0.96875, learning_rate 0.000100331
2017-10-10T15:03:15.314172: step 2350, loss 0.140211, acc 0.96875, learning_rate 0.000100329
2017-10-10T15:03:15.664890: step 2351, loss 0.222695, acc 0.9375, learning_rate 0.000100328
2017-10-10T15:03:15.935006: step 2352, loss 0.0696937, acc 0.980392, learning_rate 0.000100327
2017-10-10T15:03:16.176681: step 2353, loss 0.0757661, acc 0.96875, learning_rate 0.000100325
2017-10-10T15:03:16.441348: step 2354, loss 0.0982369, acc 0.953125, learning_rate 0.000100324
2017-10-10T15:03:16.816881: step 2355, loss 0.0879525, acc 1, learning_rate 0.000100323
2017-10-10T15:03:17.152461: step 2356, loss 0.137037, acc 0.953125, learning_rate 0.000100321
2017-10-10T15:03:17.455267: step 2357, loss 0.105127, acc 0.953125, learning_rate 0.00010032
2017-10-10T15:03:17.782818: step 2358, loss 0.194892, acc 0.921875, learning_rate 0.000100319
2017-10-10T15:03:18.084864: step 2359, loss 0.106142, acc 0.984375, learning_rate 0.000100317
2017-10-10T15:03:18.418393: step 2360, loss 0.111459, acc 0.96875, learning_rate 0.000100316

Evaluation:
2017-10-10T15:03:18.987335: step 2360, loss 0.239413, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2360

2017-10-10T15:03:20.033101: step 2361, loss 0.15214, acc 0.953125, learning_rate 0.000100315
2017-10-10T15:03:20.309109: step 2362, loss 0.104409, acc 0.96875, learning_rate 0.000100314
2017-10-10T15:03:20.557205: step 2363, loss 0.2023, acc 0.953125, learning_rate 0.000100312
2017-10-10T15:03:20.877149: step 2364, loss 0.267383, acc 0.9375, learning_rate 0.000100311
2017-10-10T15:03:21.260995: step 2365, loss 0.0762801, acc 0.984375, learning_rate 0.00010031
2017-10-10T15:03:21.560693: step 2366, loss 0.184249, acc 0.90625, learning_rate 0.000100308
2017-10-10T15:03:21.888952: step 2367, loss 0.146572, acc 0.96875, learning_rate 0.000100307
2017-10-10T15:03:22.148863: step 2368, loss 0.128748, acc 0.96875, learning_rate 0.000100306
2017-10-10T15:03:22.423645: step 2369, loss 0.0505294, acc 1, learning_rate 0.000100305
2017-10-10T15:03:22.776940: step 2370, loss 0.148136, acc 0.953125, learning_rate 0.000100303
2017-10-10T15:03:23.090467: step 2371, loss 0.111216, acc 0.984375, learning_rate 0.000100302
2017-10-10T15:03:23.336974: step 2372, loss 0.0510198, acc 1, learning_rate 0.000100301
2017-10-10T15:03:23.667436: step 2373, loss 0.163935, acc 0.96875, learning_rate 0.0001003
2017-10-10T15:03:24.013579: step 2374, loss 0.0869372, acc 0.953125, learning_rate 0.000100299
2017-10-10T15:03:24.316926: step 2375, loss 0.231513, acc 0.90625, learning_rate 0.000100297
2017-10-10T15:03:24.688981: step 2376, loss 0.0689432, acc 0.984375, learning_rate 0.000100296
2017-10-10T15:03:24.976941: step 2377, loss 0.124463, acc 0.96875, learning_rate 0.000100295
2017-10-10T15:03:25.258279: step 2378, loss 0.169317, acc 0.953125, learning_rate 0.000100294
2017-10-10T15:03:25.563428: step 2379, loss 0.143299, acc 0.953125, learning_rate 0.000100292
2017-10-10T15:03:25.869048: step 2380, loss 0.0928047, acc 0.96875, learning_rate 0.000100291
2017-10-10T15:03:26.105727: step 2381, loss 0.161745, acc 0.953125, learning_rate 0.00010029
2017-10-10T15:03:26.384963: step 2382, loss 0.0518541, acc 1, learning_rate 0.000100289
2017-10-10T15:03:26.681103: step 2383, loss 0.235976, acc 0.921875, learning_rate 0.000100288
2017-10-10T15:03:26.924167: step 2384, loss 0.157507, acc 0.9375, learning_rate 0.000100287
2017-10-10T15:03:27.203926: step 2385, loss 0.0752758, acc 0.984375, learning_rate 0.000100285
2017-10-10T15:03:27.472839: step 2386, loss 0.0677285, acc 0.984375, learning_rate 0.000100284
2017-10-10T15:03:27.792972: step 2387, loss 0.167166, acc 0.9375, learning_rate 0.000100283
2017-10-10T15:03:28.161020: step 2388, loss 0.170162, acc 0.9375, learning_rate 0.000100282
2017-10-10T15:03:28.464097: step 2389, loss 0.0931758, acc 0.96875, learning_rate 0.000100281
2017-10-10T15:03:28.751914: step 2390, loss 0.159733, acc 0.9375, learning_rate 0.00010028
2017-10-10T15:03:29.076910: step 2391, loss 0.187406, acc 0.90625, learning_rate 0.000100278
2017-10-10T15:03:29.340943: step 2392, loss 0.135133, acc 0.9375, learning_rate 0.000100277
2017-10-10T15:03:29.624962: step 2393, loss 0.0987693, acc 0.96875, learning_rate 0.000100276
2017-10-10T15:03:29.891495: step 2394, loss 0.122216, acc 0.96875, learning_rate 0.000100275
2017-10-10T15:03:30.255418: step 2395, loss 0.0856461, acc 1, learning_rate 0.000100274
2017-10-10T15:03:30.587313: step 2396, loss 0.107578, acc 0.953125, learning_rate 0.000100273
2017-10-10T15:03:30.840878: step 2397, loss 0.140352, acc 0.953125, learning_rate 0.000100272
2017-10-10T15:03:31.096700: step 2398, loss 0.091717, acc 0.96875, learning_rate 0.000100271
2017-10-10T15:03:31.344879: step 2399, loss 0.0629358, acc 0.984375, learning_rate 0.00010027
2017-10-10T15:03:31.557001: step 2400, loss 0.124289, acc 0.9375, learning_rate 0.000100268

Evaluation:
2017-10-10T15:03:32.072700: step 2400, loss 0.238438, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2400

2017-10-10T15:03:33.237165: step 2401, loss 0.116164, acc 0.96875, learning_rate 0.000100267
2017-10-10T15:03:33.513041: step 2402, loss 0.16765, acc 0.953125, learning_rate 0.000100266
2017-10-10T15:03:33.835837: step 2403, loss 0.0994965, acc 0.984375, learning_rate 0.000100265
2017-10-10T15:03:34.163877: step 2404, loss 0.130583, acc 0.953125, learning_rate 0.000100264
2017-10-10T15:03:34.473844: step 2405, loss 0.102389, acc 0.96875, learning_rate 0.000100263
2017-10-10T15:03:34.779429: step 2406, loss 0.26932, acc 0.90625, learning_rate 0.000100262
2017-10-10T15:03:35.104138: step 2407, loss 0.094676, acc 0.96875, learning_rate 0.000100261
2017-10-10T15:03:35.350222: step 2408, loss 0.120905, acc 0.953125, learning_rate 0.00010026
2017-10-10T15:03:35.652842: step 2409, loss 0.153458, acc 0.921875, learning_rate 0.000100259
2017-10-10T15:03:36.020512: step 2410, loss 0.149785, acc 0.9375, learning_rate 0.000100258
2017-10-10T15:03:36.351848: step 2411, loss 0.146043, acc 0.921875, learning_rate 0.000100257
2017-10-10T15:03:36.653129: step 2412, loss 0.173408, acc 0.921875, learning_rate 0.000100256
2017-10-10T15:03:36.886671: step 2413, loss 0.224876, acc 0.9375, learning_rate 0.000100255
2017-10-10T15:03:37.169043: step 2414, loss 0.0851265, acc 0.96875, learning_rate 0.000100253
2017-10-10T15:03:37.514602: step 2415, loss 0.136233, acc 0.9375, learning_rate 0.000100252
2017-10-10T15:03:37.908382: step 2416, loss 0.204643, acc 0.953125, learning_rate 0.000100251
2017-10-10T15:03:38.235169: step 2417, loss 0.115632, acc 0.953125, learning_rate 0.00010025
2017-10-10T15:03:38.436967: step 2418, loss 0.138829, acc 0.984375, learning_rate 0.000100249
2017-10-10T15:03:38.638069: step 2419, loss 0.166634, acc 0.9375, learning_rate 0.000100248
2017-10-10T15:03:38.894671: step 2420, loss 0.215054, acc 0.9375, learning_rate 0.000100247
2017-10-10T15:03:39.120363: step 2421, loss 0.0950904, acc 0.953125, learning_rate 0.000100246
2017-10-10T15:03:39.317662: step 2422, loss 0.0668291, acc 1, learning_rate 0.000100245
2017-10-10T15:03:39.575308: step 2423, loss 0.197886, acc 0.90625, learning_rate 0.000100244
2017-10-10T15:03:39.905135: step 2424, loss 0.105315, acc 0.984375, learning_rate 0.000100243
2017-10-10T15:03:40.216221: step 2425, loss 0.14237, acc 0.953125, learning_rate 0.000100242
2017-10-10T15:03:40.514423: step 2426, loss 0.0921113, acc 0.96875, learning_rate 0.000100241
2017-10-10T15:03:40.799225: step 2427, loss 0.134098, acc 0.96875, learning_rate 0.00010024
2017-10-10T15:03:41.112285: step 2428, loss 0.139114, acc 0.953125, learning_rate 0.000100239
2017-10-10T15:03:41.394041: step 2429, loss 0.200884, acc 0.9375, learning_rate 0.000100238
2017-10-10T15:03:41.696849: step 2430, loss 0.0684105, acc 0.96875, learning_rate 0.000100237
2017-10-10T15:03:42.001186: step 2431, loss 0.10319, acc 0.953125, learning_rate 0.000100236
2017-10-10T15:03:42.285066: step 2432, loss 0.0749535, acc 0.984375, learning_rate 0.000100235
2017-10-10T15:03:42.596405: step 2433, loss 0.1507, acc 0.9375, learning_rate 0.000100235
2017-10-10T15:03:42.880446: step 2434, loss 0.147069, acc 0.96875, learning_rate 0.000100234
2017-10-10T15:03:43.156910: step 2435, loss 0.194787, acc 0.9375, learning_rate 0.000100233
2017-10-10T15:03:43.453235: step 2436, loss 0.110772, acc 0.96875, learning_rate 0.000100232
2017-10-10T15:03:43.717635: step 2437, loss 0.09893, acc 0.984375, learning_rate 0.000100231
2017-10-10T15:03:44.040887: step 2438, loss 0.15907, acc 0.953125, learning_rate 0.00010023
2017-10-10T15:03:44.316067: step 2439, loss 0.134721, acc 0.953125, learning_rate 0.000100229
2017-10-10T15:03:44.576968: step 2440, loss 0.183174, acc 0.921875, learning_rate 0.000100228

Evaluation:
2017-10-10T15:03:45.113036: step 2440, loss 0.240553, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2440

2017-10-10T15:03:46.259371: step 2441, loss 0.122927, acc 0.96875, learning_rate 0.000100227
2017-10-10T15:03:46.576529: step 2442, loss 0.199754, acc 0.890625, learning_rate 0.000100226
2017-10-10T15:03:46.966143: step 2443, loss 0.100812, acc 0.953125, learning_rate 0.000100225
2017-10-10T15:03:47.215980: step 2444, loss 0.18582, acc 0.953125, learning_rate 0.000100224
2017-10-10T15:03:47.460706: step 2445, loss 0.206254, acc 0.953125, learning_rate 0.000100223
2017-10-10T15:03:47.752800: step 2446, loss 0.183054, acc 0.90625, learning_rate 0.000100222
2017-10-10T15:03:47.987156: step 2447, loss 0.15366, acc 0.953125, learning_rate 0.000100221
2017-10-10T15:03:48.324842: step 2448, loss 0.105199, acc 0.96875, learning_rate 0.000100221
2017-10-10T15:03:48.672881: step 2449, loss 0.166753, acc 0.921875, learning_rate 0.00010022
2017-10-10T15:03:48.991716: step 2450, loss 0.10776, acc 0.960784, learning_rate 0.000100219
2017-10-10T15:03:49.265571: step 2451, loss 0.183419, acc 0.9375, learning_rate 0.000100218
2017-10-10T15:03:49.525247: step 2452, loss 0.126144, acc 0.96875, learning_rate 0.000100217
2017-10-10T15:03:49.779965: step 2453, loss 0.15005, acc 0.953125, learning_rate 0.000100216
2017-10-10T15:03:50.022713: step 2454, loss 0.145891, acc 0.984375, learning_rate 0.000100215
2017-10-10T15:03:50.338982: step 2455, loss 0.105403, acc 0.984375, learning_rate 0.000100214
2017-10-10T15:03:50.643631: step 2456, loss 0.126535, acc 0.96875, learning_rate 0.000100213
2017-10-10T15:03:50.916461: step 2457, loss 0.163556, acc 0.9375, learning_rate 0.000100213
2017-10-10T15:03:51.239433: step 2458, loss 0.0964692, acc 0.96875, learning_rate 0.000100212
2017-10-10T15:03:51.469031: step 2459, loss 0.0711114, acc 0.984375, learning_rate 0.000100211
2017-10-10T15:03:51.796873: step 2460, loss 0.217237, acc 0.921875, learning_rate 0.00010021
2017-10-10T15:03:52.138738: step 2461, loss 0.206667, acc 0.953125, learning_rate 0.000100209
2017-10-10T15:03:52.464901: step 2462, loss 0.174079, acc 0.9375, learning_rate 0.000100208
2017-10-10T15:03:52.762217: step 2463, loss 0.093056, acc 0.984375, learning_rate 0.000100207
2017-10-10T15:03:53.081976: step 2464, loss 0.17756, acc 0.953125, learning_rate 0.000100207
2017-10-10T15:03:53.345018: step 2465, loss 0.216425, acc 0.9375, learning_rate 0.000100206
2017-10-10T15:03:53.612546: step 2466, loss 0.217117, acc 0.9375, learning_rate 0.000100205
2017-10-10T15:03:53.868892: step 2467, loss 0.101662, acc 0.96875, learning_rate 0.000100204
2017-10-10T15:03:54.139450: step 2468, loss 0.0920511, acc 1, learning_rate 0.000100203
2017-10-10T15:03:54.435570: step 2469, loss 0.142318, acc 0.96875, learning_rate 0.000100202
2017-10-10T15:03:54.732716: step 2470, loss 0.153806, acc 0.921875, learning_rate 0.000100202
2017-10-10T15:03:55.132867: step 2471, loss 0.103747, acc 0.984375, learning_rate 0.000100201
2017-10-10T15:03:55.362792: step 2472, loss 0.111022, acc 0.96875, learning_rate 0.0001002
2017-10-10T15:03:55.611489: step 2473, loss 0.196604, acc 0.921875, learning_rate 0.000100199
2017-10-10T15:03:55.878935: step 2474, loss 0.142128, acc 0.953125, learning_rate 0.000100198
2017-10-10T15:03:56.224849: step 2475, loss 0.173282, acc 0.9375, learning_rate 0.000100198
2017-10-10T15:03:56.544966: step 2476, loss 0.0890595, acc 0.953125, learning_rate 0.000100197
2017-10-10T15:03:56.900151: step 2477, loss 0.146918, acc 0.96875, learning_rate 0.000100196
2017-10-10T15:03:57.184936: step 2478, loss 0.161435, acc 0.921875, learning_rate 0.000100195
2017-10-10T15:03:57.452846: step 2479, loss 0.143355, acc 0.96875, learning_rate 0.000100194
2017-10-10T15:03:57.696877: step 2480, loss 0.0600737, acc 1, learning_rate 0.000100194

Evaluation:
2017-10-10T15:03:58.278023: step 2480, loss 0.23914, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2480

2017-10-10T15:03:59.188353: step 2481, loss 0.148984, acc 0.9375, learning_rate 0.000100193
2017-10-10T15:03:59.488822: step 2482, loss 0.172681, acc 0.953125, learning_rate 0.000100192
2017-10-10T15:03:59.845271: step 2483, loss 0.116525, acc 0.96875, learning_rate 0.000100191
2017-10-10T15:04:00.201358: step 2484, loss 0.0729138, acc 0.96875, learning_rate 0.00010019
2017-10-10T15:04:00.555145: step 2485, loss 0.177455, acc 0.96875, learning_rate 0.00010019
2017-10-10T15:04:00.856177: step 2486, loss 0.146699, acc 0.953125, learning_rate 0.000100189
2017-10-10T15:04:01.147745: step 2487, loss 0.119038, acc 0.984375, learning_rate 0.000100188
2017-10-10T15:04:01.442714: step 2488, loss 0.152714, acc 0.921875, learning_rate 0.000100187
2017-10-10T15:04:01.759933: step 2489, loss 0.0961068, acc 0.96875, learning_rate 0.000100187
2017-10-10T15:04:02.046126: step 2490, loss 0.192592, acc 0.921875, learning_rate 0.000100186
2017-10-10T15:04:02.337089: step 2491, loss 0.166056, acc 0.953125, learning_rate 0.000100185
2017-10-10T15:04:02.688874: step 2492, loss 0.124112, acc 0.96875, learning_rate 0.000100184
2017-10-10T15:04:03.029743: step 2493, loss 0.093435, acc 0.96875, learning_rate 0.000100183
2017-10-10T15:04:03.358284: step 2494, loss 0.242808, acc 0.890625, learning_rate 0.000100183
2017-10-10T15:04:03.652023: step 2495, loss 0.104413, acc 0.96875, learning_rate 0.000100182
2017-10-10T15:04:03.938738: step 2496, loss 0.0875807, acc 0.984375, learning_rate 0.000100181
2017-10-10T15:04:04.194303: step 2497, loss 0.0989218, acc 0.9375, learning_rate 0.000100181
2017-10-10T15:04:04.473797: step 2498, loss 0.327283, acc 0.84375, learning_rate 0.00010018
2017-10-10T15:04:04.791204: step 2499, loss 0.188341, acc 0.953125, learning_rate 0.000100179
2017-10-10T15:04:05.126711: step 2500, loss 0.130829, acc 0.9375, learning_rate 0.000100178
2017-10-10T15:04:05.428375: step 2501, loss 0.128797, acc 0.953125, learning_rate 0.000100178
2017-10-10T15:04:05.696370: step 2502, loss 0.085776, acc 0.96875, learning_rate 0.000100177
2017-10-10T15:04:06.004890: step 2503, loss 0.11696, acc 0.96875, learning_rate 0.000100176
2017-10-10T15:04:06.267431: step 2504, loss 0.0993015, acc 0.953125, learning_rate 0.000100175
2017-10-10T15:04:06.552873: step 2505, loss 0.0966928, acc 0.953125, learning_rate 0.000100175
2017-10-10T15:04:06.845048: step 2506, loss 0.0840937, acc 0.96875, learning_rate 0.000100174
2017-10-10T15:04:07.212894: step 2507, loss 0.151338, acc 0.953125, learning_rate 0.000100173
2017-10-10T15:04:07.501947: step 2508, loss 0.14103, acc 0.953125, learning_rate 0.000100173
2017-10-10T15:04:07.783723: step 2509, loss 0.286922, acc 0.90625, learning_rate 0.000100172
2017-10-10T15:04:08.064842: step 2510, loss 0.094261, acc 0.96875, learning_rate 0.000100171
2017-10-10T15:04:08.349382: step 2511, loss 0.120668, acc 0.96875, learning_rate 0.00010017
2017-10-10T15:04:08.583714: step 2512, loss 0.121015, acc 0.96875, learning_rate 0.00010017
2017-10-10T15:04:08.895643: step 2513, loss 0.113218, acc 0.96875, learning_rate 0.000100169
2017-10-10T15:04:09.197091: step 2514, loss 0.093986, acc 0.96875, learning_rate 0.000100168
2017-10-10T15:04:09.462335: step 2515, loss 0.0602318, acc 0.984375, learning_rate 0.000100168
2017-10-10T15:04:09.728340: step 2516, loss 0.129973, acc 0.9375, learning_rate 0.000100167
2017-10-10T15:04:10.048033: step 2517, loss 0.20959, acc 0.953125, learning_rate 0.000100166
2017-10-10T15:04:10.328873: step 2518, loss 0.0776507, acc 0.984375, learning_rate 0.000100166
2017-10-10T15:04:10.616513: step 2519, loss 0.135814, acc 0.953125, learning_rate 0.000100165
2017-10-10T15:04:10.920402: step 2520, loss 0.177952, acc 0.9375, learning_rate 0.000100164

Evaluation:
2017-10-10T15:04:11.488909: step 2520, loss 0.239021, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2520

2017-10-10T15:04:12.553048: step 2521, loss 0.15945, acc 0.953125, learning_rate 0.000100164
2017-10-10T15:04:12.836999: step 2522, loss 0.140095, acc 0.953125, learning_rate 0.000100163
2017-10-10T15:04:13.136600: step 2523, loss 0.10527, acc 0.953125, learning_rate 0.000100162
2017-10-10T15:04:13.416883: step 2524, loss 0.0802688, acc 0.96875, learning_rate 0.000100162
2017-10-10T15:04:13.780254: step 2525, loss 0.166233, acc 0.96875, learning_rate 0.000100161
2017-10-10T15:04:14.111424: step 2526, loss 0.186234, acc 0.9375, learning_rate 0.00010016
2017-10-10T15:04:14.504777: step 2527, loss 0.125178, acc 0.921875, learning_rate 0.00010016
2017-10-10T15:04:14.760264: step 2528, loss 0.06073, acc 0.984375, learning_rate 0.000100159
2017-10-10T15:04:15.045644: step 2529, loss 0.263543, acc 0.921875, learning_rate 0.000100158
2017-10-10T15:04:15.325220: step 2530, loss 0.149996, acc 0.96875, learning_rate 0.000100158
2017-10-10T15:04:15.672921: step 2531, loss 0.160403, acc 0.96875, learning_rate 0.000100157
2017-10-10T15:04:15.992901: step 2532, loss 0.195461, acc 0.953125, learning_rate 0.000100156
2017-10-10T15:04:16.300930: step 2533, loss 0.0747632, acc 0.96875, learning_rate 0.000100156
2017-10-10T15:04:16.641200: step 2534, loss 0.171484, acc 0.953125, learning_rate 0.000100155
2017-10-10T15:04:16.950274: step 2535, loss 0.199151, acc 0.921875, learning_rate 0.000100155
2017-10-10T15:04:17.281039: step 2536, loss 0.0618781, acc 0.984375, learning_rate 0.000100154
2017-10-10T15:04:17.617183: step 2537, loss 0.161874, acc 0.953125, learning_rate 0.000100153
2017-10-10T15:04:17.876934: step 2538, loss 0.125068, acc 0.9375, learning_rate 0.000100153
2017-10-10T15:04:18.217028: step 2539, loss 0.0485091, acc 1, learning_rate 0.000100152
2017-10-10T15:04:18.496923: step 2540, loss 0.145046, acc 0.9375, learning_rate 0.000100151
2017-10-10T15:04:18.838293: step 2541, loss 0.157054, acc 0.9375, learning_rate 0.000100151
2017-10-10T15:04:19.112163: step 2542, loss 0.105395, acc 0.96875, learning_rate 0.00010015
2017-10-10T15:04:19.418813: step 2543, loss 0.172841, acc 0.9375, learning_rate 0.00010015
2017-10-10T15:04:19.777127: step 2544, loss 0.0747193, acc 0.984375, learning_rate 0.000100149
2017-10-10T15:04:20.116901: step 2545, loss 0.262936, acc 0.90625, learning_rate 0.000100148
2017-10-10T15:04:20.384516: step 2546, loss 0.137725, acc 0.953125, learning_rate 0.000100148
2017-10-10T15:04:20.664839: step 2547, loss 0.116061, acc 0.953125, learning_rate 0.000100147
2017-10-10T15:04:20.883293: step 2548, loss 0.149543, acc 0.960784, learning_rate 0.000100147
2017-10-10T15:04:21.139363: step 2549, loss 0.1497, acc 0.9375, learning_rate 0.000100146
2017-10-10T15:04:21.498125: step 2550, loss 0.105703, acc 0.984375, learning_rate 0.000100145
2017-10-10T15:04:21.825946: step 2551, loss 0.15877, acc 0.96875, learning_rate 0.000100145
2017-10-10T15:04:22.128299: step 2552, loss 0.107501, acc 0.96875, learning_rate 0.000100144
2017-10-10T15:04:22.464992: step 2553, loss 0.075925, acc 0.96875, learning_rate 0.000100144
2017-10-10T15:04:22.720892: step 2554, loss 0.178064, acc 0.921875, learning_rate 0.000100143
2017-10-10T15:04:23.049550: step 2555, loss 0.0560006, acc 1, learning_rate 0.000100142
2017-10-10T15:04:23.320872: step 2556, loss 0.138947, acc 0.9375, learning_rate 0.000100142
2017-10-10T15:04:23.595643: step 2557, loss 0.177628, acc 0.921875, learning_rate 0.000100141
2017-10-10T15:04:23.870535: step 2558, loss 0.0847926, acc 0.953125, learning_rate 0.000100141
2017-10-10T15:04:24.184658: step 2559, loss 0.130575, acc 0.96875, learning_rate 0.00010014
2017-10-10T15:04:24.537165: step 2560, loss 0.223066, acc 0.96875, learning_rate 0.00010014

Evaluation:
2017-10-10T15:04:25.035855: step 2560, loss 0.240615, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2560

2017-10-10T15:04:26.168967: step 2561, loss 0.157244, acc 0.9375, learning_rate 0.000100139
2017-10-10T15:04:26.540957: step 2562, loss 0.0833928, acc 0.96875, learning_rate 0.000100138
2017-10-10T15:04:26.856854: step 2563, loss 0.124785, acc 0.953125, learning_rate 0.000100138
2017-10-10T15:04:27.149232: step 2564, loss 0.160993, acc 0.953125, learning_rate 0.000100137
2017-10-10T15:04:27.468324: step 2565, loss 0.0726787, acc 0.984375, learning_rate 0.000100137
2017-10-10T15:04:27.771877: step 2566, loss 0.207876, acc 0.9375, learning_rate 0.000100136
2017-10-10T15:04:28.073546: step 2567, loss 0.157623, acc 0.953125, learning_rate 0.000100136
2017-10-10T15:04:28.336492: step 2568, loss 0.171725, acc 0.953125, learning_rate 0.000100135
2017-10-10T15:04:28.632831: step 2569, loss 0.0976615, acc 1, learning_rate 0.000100134
2017-10-10T15:04:28.952361: step 2570, loss 0.0793207, acc 1, learning_rate 0.000100134
2017-10-10T15:04:29.247296: step 2571, loss 0.0794731, acc 0.96875, learning_rate 0.000100133
2017-10-10T15:04:29.616992: step 2572, loss 0.1765, acc 0.90625, learning_rate 0.000100133
2017-10-10T15:04:29.827852: step 2573, loss 0.235112, acc 0.90625, learning_rate 0.000100132
2017-10-10T15:04:30.152201: step 2574, loss 0.121481, acc 0.9375, learning_rate 0.000100132
2017-10-10T15:04:30.465683: step 2575, loss 0.197884, acc 0.9375, learning_rate 0.000100131
2017-10-10T15:04:30.792884: step 2576, loss 0.0944828, acc 0.96875, learning_rate 0.000100131
2017-10-10T15:04:31.100972: step 2577, loss 0.0905119, acc 0.96875, learning_rate 0.00010013
2017-10-10T15:04:31.448989: step 2578, loss 0.185701, acc 0.96875, learning_rate 0.00010013
2017-10-10T15:04:31.909777: step 2579, loss 0.144621, acc 0.953125, learning_rate 0.000100129
2017-10-10T15:04:32.187031: step 2580, loss 0.0484784, acc 1, learning_rate 0.000100129
2017-10-10T15:04:32.441108: step 2581, loss 0.240461, acc 0.890625, learning_rate 0.000100128
2017-10-10T15:04:32.706258: step 2582, loss 0.0872754, acc 0.96875, learning_rate 0.000100128
2017-10-10T15:04:32.993764: step 2583, loss 0.300913, acc 0.90625, learning_rate 0.000100127
2017-10-10T15:04:33.276843: step 2584, loss 0.197259, acc 0.921875, learning_rate 0.000100126
2017-10-10T15:04:33.636896: step 2585, loss 0.0865991, acc 1, learning_rate 0.000100126
2017-10-10T15:04:33.940869: step 2586, loss 0.0706279, acc 1, learning_rate 0.000100125
2017-10-10T15:04:34.197885: step 2587, loss 0.0595519, acc 1, learning_rate 0.000100125
2017-10-10T15:04:34.437234: step 2588, loss 0.126283, acc 0.96875, learning_rate 0.000100124
2017-10-10T15:04:34.776953: step 2589, loss 0.19789, acc 0.9375, learning_rate 0.000100124
2017-10-10T15:04:35.120095: step 2590, loss 0.114705, acc 0.984375, learning_rate 0.000100123
2017-10-10T15:04:35.417065: step 2591, loss 0.15349, acc 0.953125, learning_rate 0.000100123
2017-10-10T15:04:35.715495: step 2592, loss 0.0784658, acc 0.96875, learning_rate 0.000100122
2017-10-10T15:04:36.032940: step 2593, loss 0.120789, acc 0.96875, learning_rate 0.000100122
2017-10-10T15:04:36.412906: step 2594, loss 0.117328, acc 0.953125, learning_rate 0.000100121
2017-10-10T15:04:36.748045: step 2595, loss 0.155566, acc 0.96875, learning_rate 0.000100121
2017-10-10T15:04:37.003269: step 2596, loss 0.127624, acc 0.953125, learning_rate 0.00010012
2017-10-10T15:04:37.273001: step 2597, loss 0.0341981, acc 1, learning_rate 0.00010012
2017-10-10T15:04:37.528898: step 2598, loss 0.207655, acc 0.96875, learning_rate 0.000100119
2017-10-10T15:04:37.838887: step 2599, loss 0.20923, acc 0.90625, learning_rate 0.000100119
2017-10-10T15:04:38.161091: step 2600, loss 0.0810818, acc 0.984375, learning_rate 0.000100118

Evaluation:
2017-10-10T15:04:38.663345: step 2600, loss 0.239838, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2600

2017-10-10T15:04:39.629750: step 2601, loss 0.144604, acc 0.953125, learning_rate 0.000100118
2017-10-10T15:04:39.845381: step 2602, loss 0.204081, acc 0.9375, learning_rate 0.000100117
2017-10-10T15:04:40.162302: step 2603, loss 0.123937, acc 0.984375, learning_rate 0.000100117
2017-10-10T15:04:40.476592: step 2604, loss 0.112578, acc 0.96875, learning_rate 0.000100117
2017-10-10T15:04:40.764977: step 2605, loss 0.0916317, acc 0.984375, learning_rate 0.000100116
2017-10-10T15:04:41.065380: step 2606, loss 0.0764554, acc 0.953125, learning_rate 0.000100116
2017-10-10T15:04:41.427896: step 2607, loss 0.140355, acc 0.9375, learning_rate 0.000100115
2017-10-10T15:04:41.729572: step 2608, loss 0.139968, acc 0.953125, learning_rate 0.000100115
2017-10-10T15:04:42.041027: step 2609, loss 0.338345, acc 0.90625, learning_rate 0.000100114
2017-10-10T15:04:42.348886: step 2610, loss 0.110275, acc 0.96875, learning_rate 0.000100114
2017-10-10T15:04:42.672899: step 2611, loss 0.148169, acc 0.96875, learning_rate 0.000100113
2017-10-10T15:04:43.016877: step 2612, loss 0.0324179, acc 1, learning_rate 0.000100113
2017-10-10T15:04:43.333027: step 2613, loss 0.220639, acc 0.921875, learning_rate 0.000100112
2017-10-10T15:04:43.600317: step 2614, loss 0.272112, acc 0.875, learning_rate 0.000100112
2017-10-10T15:04:43.947113: step 2615, loss 0.138401, acc 0.9375, learning_rate 0.000100111
2017-10-10T15:04:44.279954: step 2616, loss 0.192655, acc 0.953125, learning_rate 0.000100111
2017-10-10T15:04:44.612413: step 2617, loss 0.0905283, acc 1, learning_rate 0.000100111
2017-10-10T15:04:44.877062: step 2618, loss 0.057174, acc 0.984375, learning_rate 0.00010011
2017-10-10T15:04:45.156842: step 2619, loss 0.157866, acc 0.953125, learning_rate 0.00010011
2017-10-10T15:04:45.468812: step 2620, loss 0.199703, acc 0.9375, learning_rate 0.000100109
2017-10-10T15:04:45.832235: step 2621, loss 0.108908, acc 0.953125, learning_rate 0.000100109
2017-10-10T15:04:46.140684: step 2622, loss 0.105998, acc 0.96875, learning_rate 0.000100108
2017-10-10T15:04:46.443401: step 2623, loss 0.150788, acc 0.953125, learning_rate 0.000100108
2017-10-10T15:04:46.725081: step 2624, loss 0.201717, acc 0.90625, learning_rate 0.000100107
2017-10-10T15:04:47.071488: step 2625, loss 0.0506856, acc 0.984375, learning_rate 0.000100107
2017-10-10T15:04:47.352501: step 2626, loss 0.0609958, acc 1, learning_rate 0.000100107
2017-10-10T15:04:47.672961: step 2627, loss 0.116435, acc 0.96875, learning_rate 0.000100106
2017-10-10T15:04:47.968846: step 2628, loss 0.081911, acc 0.96875, learning_rate 0.000100106
2017-10-10T15:04:48.239515: step 2629, loss 0.0874339, acc 0.984375, learning_rate 0.000100105
2017-10-10T15:04:48.600809: step 2630, loss 0.325769, acc 0.890625, learning_rate 0.000100105
2017-10-10T15:04:48.928361: step 2631, loss 0.17284, acc 0.9375, learning_rate 0.000100104
2017-10-10T15:04:49.240443: step 2632, loss 0.183882, acc 0.9375, learning_rate 0.000100104
2017-10-10T15:04:49.588893: step 2633, loss 0.139204, acc 0.953125, learning_rate 0.000100104
2017-10-10T15:04:50.034963: step 2634, loss 0.158868, acc 0.953125, learning_rate 0.000100103
2017-10-10T15:04:50.351614: step 2635, loss 0.0612192, acc 0.984375, learning_rate 0.000100103
2017-10-10T15:04:50.600934: step 2636, loss 0.168332, acc 0.953125, learning_rate 0.000100102
2017-10-10T15:04:50.889993: step 2637, loss 0.0778731, acc 0.96875, learning_rate 0.000100102
2017-10-10T15:04:51.123925: step 2638, loss 0.0710835, acc 0.984375, learning_rate 0.000100101
2017-10-10T15:04:51.467582: step 2639, loss 0.162389, acc 0.953125, learning_rate 0.000100101
2017-10-10T15:04:51.762852: step 2640, loss 0.146953, acc 0.953125, learning_rate 0.000100101

Evaluation:
2017-10-10T15:04:52.242051: step 2640, loss 0.238787, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2640

2017-10-10T15:04:53.354871: step 2641, loss 0.100552, acc 0.96875, learning_rate 0.0001001
2017-10-10T15:04:53.612928: step 2642, loss 0.0839575, acc 0.984375, learning_rate 0.0001001
2017-10-10T15:04:53.847245: step 2643, loss 0.18003, acc 0.9375, learning_rate 0.000100099
2017-10-10T15:04:54.142901: step 2644, loss 0.093848, acc 0.953125, learning_rate 0.000100099
2017-10-10T15:04:54.407606: step 2645, loss 0.103047, acc 0.96875, learning_rate 0.000100099
2017-10-10T15:04:54.714920: step 2646, loss 0.0975134, acc 0.980392, learning_rate 0.000100098
2017-10-10T15:04:54.989740: step 2647, loss 0.145606, acc 0.953125, learning_rate 0.000100098
2017-10-10T15:04:55.274861: step 2648, loss 0.0474785, acc 1, learning_rate 0.000100097
2017-10-10T15:04:55.596886: step 2649, loss 0.172805, acc 0.96875, learning_rate 0.000100097
2017-10-10T15:04:55.964867: step 2650, loss 0.125457, acc 0.96875, learning_rate 0.000100097
2017-10-10T15:04:56.328794: step 2651, loss 0.140904, acc 0.953125, learning_rate 0.000100096
2017-10-10T15:04:56.540400: step 2652, loss 0.158963, acc 0.953125, learning_rate 0.000100096
2017-10-10T15:04:56.812678: step 2653, loss 0.107227, acc 0.96875, learning_rate 0.000100095
2017-10-10T15:04:57.056963: step 2654, loss 0.101817, acc 0.96875, learning_rate 0.000100095
2017-10-10T15:04:57.376860: step 2655, loss 0.0952317, acc 0.984375, learning_rate 0.000100095
2017-10-10T15:04:57.705388: step 2656, loss 0.136805, acc 0.953125, learning_rate 0.000100094
2017-10-10T15:04:57.990375: step 2657, loss 0.0843421, acc 0.984375, learning_rate 0.000100094
2017-10-10T15:04:58.309250: step 2658, loss 0.214721, acc 0.9375, learning_rate 0.000100093
2017-10-10T15:04:58.596545: step 2659, loss 0.0942852, acc 0.96875, learning_rate 0.000100093
2017-10-10T15:04:58.888333: step 2660, loss 0.0890487, acc 0.953125, learning_rate 0.000100093
2017-10-10T15:04:59.183651: step 2661, loss 0.186523, acc 0.953125, learning_rate 0.000100092
2017-10-10T15:04:59.480879: step 2662, loss 0.213893, acc 0.9375, learning_rate 0.000100092
2017-10-10T15:04:59.768277: step 2663, loss 0.0526017, acc 0.984375, learning_rate 0.000100092
2017-10-10T15:05:00.073557: step 2664, loss 0.123977, acc 0.953125, learning_rate 0.000100091
2017-10-10T15:05:00.337435: step 2665, loss 0.120774, acc 0.9375, learning_rate 0.000100091
2017-10-10T15:05:00.608843: step 2666, loss 0.195429, acc 0.953125, learning_rate 0.00010009
2017-10-10T15:05:00.925001: step 2667, loss 0.116952, acc 0.953125, learning_rate 0.00010009
2017-10-10T15:05:01.271091: step 2668, loss 0.152079, acc 0.953125, learning_rate 0.00010009
2017-10-10T15:05:01.548881: step 2669, loss 0.159168, acc 0.953125, learning_rate 0.000100089
2017-10-10T15:05:01.835519: step 2670, loss 0.132294, acc 0.953125, learning_rate 0.000100089
2017-10-10T15:05:02.139093: step 2671, loss 0.199724, acc 0.953125, learning_rate 0.000100089
2017-10-10T15:05:02.444842: step 2672, loss 0.139932, acc 0.921875, learning_rate 0.000100088
2017-10-10T15:05:02.769599: step 2673, loss 0.110852, acc 0.953125, learning_rate 0.000100088
2017-10-10T15:05:03.073983: step 2674, loss 0.0581301, acc 0.984375, learning_rate 0.000100088
2017-10-10T15:05:03.383435: step 2675, loss 0.113554, acc 0.953125, learning_rate 0.000100087
2017-10-10T15:05:03.696940: step 2676, loss 0.22361, acc 0.921875, learning_rate 0.000100087
2017-10-10T15:05:03.949368: step 2677, loss 0.213691, acc 0.90625, learning_rate 0.000100086
2017-10-10T15:05:04.240717: step 2678, loss 0.111628, acc 0.953125, learning_rate 0.000100086
2017-10-10T15:05:04.552897: step 2679, loss 0.0872039, acc 0.984375, learning_rate 0.000100086
2017-10-10T15:05:04.928843: step 2680, loss 0.0785351, acc 0.984375, learning_rate 0.000100085

Evaluation:
2017-10-10T15:05:05.404956: step 2680, loss 0.237192, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2680

2017-10-10T15:05:06.537453: step 2681, loss 0.0680789, acc 1, learning_rate 0.000100085
2017-10-10T15:05:06.881027: step 2682, loss 0.124453, acc 0.96875, learning_rate 0.000100085
2017-10-10T15:05:07.170524: step 2683, loss 0.0549009, acc 0.984375, learning_rate 0.000100084
2017-10-10T15:05:07.459436: step 2684, loss 0.13343, acc 0.9375, learning_rate 0.000100084
2017-10-10T15:05:07.787344: step 2685, loss 0.180204, acc 0.9375, learning_rate 0.000100084
2017-10-10T15:05:08.167851: step 2686, loss 0.120716, acc 0.953125, learning_rate 0.000100083
2017-10-10T15:05:08.528911: step 2687, loss 0.156051, acc 0.9375, learning_rate 0.000100083
2017-10-10T15:05:08.770495: step 2688, loss 0.109707, acc 0.96875, learning_rate 0.000100083
2017-10-10T15:05:08.967249: step 2689, loss 0.0603616, acc 1, learning_rate 0.000100082
2017-10-10T15:05:09.203603: step 2690, loss 0.171524, acc 0.921875, learning_rate 0.000100082
2017-10-10T15:05:09.412837: step 2691, loss 0.182973, acc 0.953125, learning_rate 0.000100082
2017-10-10T15:05:09.847425: step 2692, loss 0.093992, acc 0.984375, learning_rate 0.000100081
2017-10-10T15:05:10.100526: step 2693, loss 0.185771, acc 0.953125, learning_rate 0.000100081
2017-10-10T15:05:10.329333: step 2694, loss 0.122179, acc 0.921875, learning_rate 0.000100081
2017-10-10T15:05:10.614223: step 2695, loss 0.231037, acc 0.921875, learning_rate 0.00010008
2017-10-10T15:05:10.865527: step 2696, loss 0.122597, acc 0.96875, learning_rate 0.00010008
2017-10-10T15:05:11.137572: step 2697, loss 0.164994, acc 0.921875, learning_rate 0.00010008
2017-10-10T15:05:11.471633: step 2698, loss 0.229647, acc 0.921875, learning_rate 0.000100079
2017-10-10T15:05:11.803107: step 2699, loss 0.0710541, acc 0.984375, learning_rate 0.000100079
2017-10-10T15:05:12.154859: step 2700, loss 0.0797821, acc 1, learning_rate 0.000100079
2017-10-10T15:05:12.431834: step 2701, loss 0.149692, acc 0.953125, learning_rate 0.000100078
2017-10-10T15:05:12.744172: step 2702, loss 0.148213, acc 0.96875, learning_rate 0.000100078
2017-10-10T15:05:13.060573: step 2703, loss 0.169587, acc 0.9375, learning_rate 0.000100078
2017-10-10T15:05:13.364876: step 2704, loss 0.0941973, acc 0.96875, learning_rate 0.000100077
2017-10-10T15:05:13.671268: step 2705, loss 0.16169, acc 0.953125, learning_rate 0.000100077
2017-10-10T15:05:13.977309: step 2706, loss 0.0884541, acc 1, learning_rate 0.000100077
2017-10-10T15:05:14.320777: step 2707, loss 0.0972888, acc 0.96875, learning_rate 0.000100076
2017-10-10T15:05:14.585079: step 2708, loss 0.11446, acc 0.953125, learning_rate 0.000100076
2017-10-10T15:05:14.912014: step 2709, loss 0.0665762, acc 0.984375, learning_rate 0.000100076
2017-10-10T15:05:15.167020: step 2710, loss 0.0498858, acc 0.984375, learning_rate 0.000100076
2017-10-10T15:05:15.468899: step 2711, loss 0.182682, acc 0.921875, learning_rate 0.000100075
2017-10-10T15:05:15.804876: step 2712, loss 0.191675, acc 0.96875, learning_rate 0.000100075
2017-10-10T15:05:16.104945: step 2713, loss 0.110179, acc 0.984375, learning_rate 0.000100075
2017-10-10T15:05:16.402057: step 2714, loss 0.156372, acc 0.9375, learning_rate 0.000100074
2017-10-10T15:05:16.778396: step 2715, loss 0.0495057, acc 1, learning_rate 0.000100074
2017-10-10T15:05:17.037110: step 2716, loss 0.200752, acc 0.953125, learning_rate 0.000100074
2017-10-10T15:05:17.327860: step 2717, loss 0.0773312, acc 0.96875, learning_rate 0.000100073
2017-10-10T15:05:17.630818: step 2718, loss 0.107035, acc 0.984375, learning_rate 0.000100073
2017-10-10T15:05:17.973342: step 2719, loss 0.268889, acc 0.890625, learning_rate 0.000100073
2017-10-10T15:05:18.258036: step 2720, loss 0.102259, acc 0.96875, learning_rate 0.000100073

Evaluation:
2017-10-10T15:05:18.747691: step 2720, loss 0.237598, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2720

2017-10-10T15:05:20.100842: step 2721, loss 0.122682, acc 0.96875, learning_rate 0.000100072
2017-10-10T15:05:20.407343: step 2722, loss 0.0915034, acc 0.984375, learning_rate 0.000100072
2017-10-10T15:05:20.817367: step 2723, loss 0.134674, acc 0.96875, learning_rate 0.000100072
2017-10-10T15:05:21.099717: step 2724, loss 0.0960747, acc 0.96875, learning_rate 0.000100071
2017-10-10T15:05:21.392612: step 2725, loss 0.0927318, acc 0.984375, learning_rate 0.000100071
2017-10-10T15:05:21.674399: step 2726, loss 0.109977, acc 0.953125, learning_rate 0.000100071
2017-10-10T15:05:21.960853: step 2727, loss 0.0902668, acc 0.984375, learning_rate 0.00010007
2017-10-10T15:05:22.273389: step 2728, loss 0.102107, acc 0.96875, learning_rate 0.00010007
2017-10-10T15:05:22.548495: step 2729, loss 0.231847, acc 0.921875, learning_rate 0.00010007
2017-10-10T15:05:22.848948: step 2730, loss 0.290663, acc 0.875, learning_rate 0.00010007
2017-10-10T15:05:23.178566: step 2731, loss 0.16158, acc 0.9375, learning_rate 0.000100069
2017-10-10T15:05:23.521475: step 2732, loss 0.153936, acc 0.9375, learning_rate 0.000100069
2017-10-10T15:05:23.802860: step 2733, loss 0.080239, acc 0.953125, learning_rate 0.000100069
2017-10-10T15:05:24.104168: step 2734, loss 0.0843438, acc 0.96875, learning_rate 0.000100068
2017-10-10T15:05:24.453628: step 2735, loss 0.171599, acc 0.953125, learning_rate 0.000100068
2017-10-10T15:05:24.728307: step 2736, loss 0.089784, acc 0.984375, learning_rate 0.000100068
2017-10-10T15:05:25.058928: step 2737, loss 0.121531, acc 0.96875, learning_rate 0.000100068
2017-10-10T15:05:25.334105: step 2738, loss 0.150369, acc 0.953125, learning_rate 0.000100067
2017-10-10T15:05:25.677021: step 2739, loss 0.139825, acc 0.96875, learning_rate 0.000100067
2017-10-10T15:05:25.984953: step 2740, loss 0.123958, acc 0.9375, learning_rate 0.000100067
2017-10-10T15:05:26.370922: step 2741, loss 0.113558, acc 0.953125, learning_rate 0.000100067
2017-10-10T15:05:26.743319: step 2742, loss 0.111421, acc 0.953125, learning_rate 0.000100066
2017-10-10T15:05:26.936958: step 2743, loss 0.206893, acc 0.953125, learning_rate 0.000100066
2017-10-10T15:05:27.084937: step 2744, loss 0.1472, acc 0.941176, learning_rate 0.000100066
2017-10-10T15:05:27.272810: step 2745, loss 0.08039, acc 0.984375, learning_rate 0.000100065
2017-10-10T15:05:27.482217: step 2746, loss 0.167794, acc 0.9375, learning_rate 0.000100065
2017-10-10T15:05:27.685400: step 2747, loss 0.12781, acc 0.9375, learning_rate 0.000100065
2017-10-10T15:05:28.033099: step 2748, loss 0.0767217, acc 0.984375, learning_rate 0.000100065
2017-10-10T15:05:28.348851: step 2749, loss 0.0821963, acc 0.96875, learning_rate 0.000100064
2017-10-10T15:05:28.636890: step 2750, loss 0.100054, acc 0.984375, learning_rate 0.000100064
2017-10-10T15:05:28.973806: step 2751, loss 0.126839, acc 0.96875, learning_rate 0.000100064
2017-10-10T15:05:29.185072: step 2752, loss 0.151342, acc 0.953125, learning_rate 0.000100064
2017-10-10T15:05:29.488709: step 2753, loss 0.111104, acc 0.953125, learning_rate 0.000100063
2017-10-10T15:05:29.832866: step 2754, loss 0.090043, acc 0.984375, learning_rate 0.000100063
2017-10-10T15:05:30.073356: step 2755, loss 0.105752, acc 0.96875, learning_rate 0.000100063
2017-10-10T15:05:30.369287: step 2756, loss 0.221763, acc 0.921875, learning_rate 0.000100063
2017-10-10T15:05:30.705547: step 2757, loss 0.121988, acc 0.953125, learning_rate 0.000100062
2017-10-10T15:05:31.021644: step 2758, loss 0.122975, acc 0.96875, learning_rate 0.000100062
2017-10-10T15:05:31.288896: step 2759, loss 0.146966, acc 0.953125, learning_rate 0.000100062
2017-10-10T15:05:31.592900: step 2760, loss 0.078957, acc 0.984375, learning_rate 0.000100062

Evaluation:
2017-10-10T15:05:32.112001: step 2760, loss 0.236033, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2760

2017-10-10T15:05:33.092932: step 2761, loss 0.0975948, acc 0.953125, learning_rate 0.000100061
2017-10-10T15:05:33.421904: step 2762, loss 0.0652712, acc 0.984375, learning_rate 0.000100061
2017-10-10T15:05:33.700942: step 2763, loss 0.123496, acc 0.9375, learning_rate 0.000100061
2017-10-10T15:05:34.031637: step 2764, loss 0.152126, acc 0.953125, learning_rate 0.000100061
2017-10-10T15:05:34.353900: step 2765, loss 0.081682, acc 0.96875, learning_rate 0.00010006
2017-10-10T15:05:34.601024: step 2766, loss 0.106101, acc 0.96875, learning_rate 0.00010006
2017-10-10T15:05:34.929233: step 2767, loss 0.195055, acc 0.953125, learning_rate 0.00010006
2017-10-10T15:05:35.272874: step 2768, loss 0.16429, acc 0.921875, learning_rate 0.00010006
2017-10-10T15:05:35.623589: step 2769, loss 0.114165, acc 0.984375, learning_rate 0.000100059
2017-10-10T15:05:35.962171: step 2770, loss 0.190117, acc 0.921875, learning_rate 0.000100059
2017-10-10T15:05:36.183955: step 2771, loss 0.14785, acc 0.953125, learning_rate 0.000100059
2017-10-10T15:05:36.505268: step 2772, loss 0.0539618, acc 1, learning_rate 0.000100059
2017-10-10T15:05:36.790322: step 2773, loss 0.116619, acc 0.9375, learning_rate 0.000100058
2017-10-10T15:05:37.112479: step 2774, loss 0.107451, acc 0.96875, learning_rate 0.000100058
2017-10-10T15:05:37.376950: step 2775, loss 0.0916596, acc 0.984375, learning_rate 0.000100058
2017-10-10T15:05:37.655970: step 2776, loss 0.074658, acc 1, learning_rate 0.000100058
2017-10-10T15:05:37.963672: step 2777, loss 0.144109, acc 0.953125, learning_rate 0.000100057
2017-10-10T15:05:38.257850: step 2778, loss 0.122521, acc 0.96875, learning_rate 0.000100057
2017-10-10T15:05:38.557549: step 2779, loss 0.1665, acc 0.921875, learning_rate 0.000100057
2017-10-10T15:05:38.789060: step 2780, loss 0.110091, acc 0.96875, learning_rate 0.000100057
2017-10-10T15:05:39.105130: step 2781, loss 0.21682, acc 0.90625, learning_rate 0.000100056
2017-10-10T15:05:39.461514: step 2782, loss 0.138344, acc 0.953125, learning_rate 0.000100056
2017-10-10T15:05:39.838293: step 2783, loss 0.103214, acc 0.984375, learning_rate 0.000100056
2017-10-10T15:05:40.105372: step 2784, loss 0.0991981, acc 0.96875, learning_rate 0.000100056
2017-10-10T15:05:40.388873: step 2785, loss 0.0585697, acc 1, learning_rate 0.000100056
2017-10-10T15:05:40.664173: step 2786, loss 0.122926, acc 0.953125, learning_rate 0.000100055
2017-10-10T15:05:41.020845: step 2787, loss 0.113518, acc 0.953125, learning_rate 0.000100055
2017-10-10T15:05:41.296858: step 2788, loss 0.129598, acc 0.9375, learning_rate 0.000100055
2017-10-10T15:05:41.649239: step 2789, loss 0.139251, acc 0.9375, learning_rate 0.000100055
2017-10-10T15:05:42.016405: step 2790, loss 0.15173, acc 0.9375, learning_rate 0.000100054
2017-10-10T15:05:42.352066: step 2791, loss 0.240209, acc 0.921875, learning_rate 0.000100054
2017-10-10T15:05:42.650275: step 2792, loss 0.0484956, acc 1, learning_rate 0.000100054
2017-10-10T15:05:42.919069: step 2793, loss 0.136734, acc 0.9375, learning_rate 0.000100054
2017-10-10T15:05:43.262294: step 2794, loss 0.173885, acc 0.953125, learning_rate 0.000100054
2017-10-10T15:05:43.587934: step 2795, loss 0.121273, acc 0.953125, learning_rate 0.000100053
2017-10-10T15:05:43.868813: step 2796, loss 0.0976391, acc 0.96875, learning_rate 0.000100053
2017-10-10T15:05:44.071684: step 2797, loss 0.0493467, acc 0.984375, learning_rate 0.000100053
2017-10-10T15:05:44.396944: step 2798, loss 0.188862, acc 0.9375, learning_rate 0.000100053
2017-10-10T15:05:44.777225: step 2799, loss 0.151153, acc 0.96875, learning_rate 0.000100052
2017-10-10T15:05:45.043953: step 2800, loss 0.092818, acc 0.984375, learning_rate 0.000100052

Evaluation:
2017-10-10T15:05:45.551080: step 2800, loss 0.23548, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2800

2017-10-10T15:05:46.708870: step 2801, loss 0.155769, acc 0.953125, learning_rate 0.000100052
2017-10-10T15:05:46.977086: step 2802, loss 0.120073, acc 0.953125, learning_rate 0.000100052
2017-10-10T15:05:47.279457: step 2803, loss 0.114544, acc 0.9375, learning_rate 0.000100052
2017-10-10T15:05:47.617496: step 2804, loss 0.0700176, acc 0.984375, learning_rate 0.000100051
2017-10-10T15:05:47.925047: step 2805, loss 0.210845, acc 0.9375, learning_rate 0.000100051
2017-10-10T15:05:48.201049: step 2806, loss 0.105885, acc 0.984375, learning_rate 0.000100051
2017-10-10T15:05:48.470062: step 2807, loss 0.145431, acc 0.96875, learning_rate 0.000100051
2017-10-10T15:05:48.783091: step 2808, loss 0.059059, acc 1, learning_rate 0.000100051
2017-10-10T15:05:49.105049: step 2809, loss 0.115796, acc 0.984375, learning_rate 0.00010005
2017-10-10T15:05:49.434156: step 2810, loss 0.151729, acc 0.953125, learning_rate 0.00010005
2017-10-10T15:05:49.699333: step 2811, loss 0.0737047, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:05:50.009862: step 2812, loss 0.190652, acc 0.90625, learning_rate 0.00010005
2017-10-10T15:05:50.290771: step 2813, loss 0.0985924, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:05:50.609534: step 2814, loss 0.221117, acc 0.921875, learning_rate 0.000100049
2017-10-10T15:05:50.946292: step 2815, loss 0.122213, acc 0.9375, learning_rate 0.000100049
2017-10-10T15:05:51.226477: step 2816, loss 0.130477, acc 0.96875, learning_rate 0.000100049
2017-10-10T15:05:51.422179: step 2817, loss 0.0655402, acc 1, learning_rate 0.000100049
2017-10-10T15:05:51.724934: step 2818, loss 0.197514, acc 0.9375, learning_rate 0.000100049
2017-10-10T15:05:52.036207: step 2819, loss 0.0649397, acc 0.984375, learning_rate 0.000100048
2017-10-10T15:05:52.308339: step 2820, loss 0.046335, acc 1, learning_rate 0.000100048
2017-10-10T15:05:52.591594: step 2821, loss 0.179746, acc 0.953125, learning_rate 0.000100048
2017-10-10T15:05:52.877803: step 2822, loss 0.158369, acc 0.953125, learning_rate 0.000100048
2017-10-10T15:05:53.245738: step 2823, loss 0.0753378, acc 0.984375, learning_rate 0.000100048
2017-10-10T15:05:53.556915: step 2824, loss 0.147993, acc 0.96875, learning_rate 0.000100047
2017-10-10T15:05:53.848468: step 2825, loss 0.0787864, acc 0.984375, learning_rate 0.000100047
2017-10-10T15:05:54.204953: step 2826, loss 0.151354, acc 0.96875, learning_rate 0.000100047
2017-10-10T15:05:54.528598: step 2827, loss 0.111447, acc 0.96875, learning_rate 0.000100047
2017-10-10T15:05:54.786529: step 2828, loss 0.0679018, acc 0.984375, learning_rate 0.000100047
2017-10-10T15:05:55.096932: step 2829, loss 0.134218, acc 0.984375, learning_rate 0.000100046
2017-10-10T15:05:55.404135: step 2830, loss 0.157968, acc 0.9375, learning_rate 0.000100046
2017-10-10T15:05:55.688852: step 2831, loss 0.135259, acc 0.96875, learning_rate 0.000100046
2017-10-10T15:05:55.952943: step 2832, loss 0.116103, acc 0.953125, learning_rate 0.000100046
2017-10-10T15:05:56.245244: step 2833, loss 0.196087, acc 0.96875, learning_rate 0.000100046
2017-10-10T15:05:56.560873: step 2834, loss 0.163644, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:05:56.769849: step 2835, loss 0.0639809, acc 1, learning_rate 0.000100045
2017-10-10T15:05:57.096072: step 2836, loss 0.17659, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:05:57.420948: step 2837, loss 0.140993, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:05:57.716920: step 2838, loss 0.151144, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:05:58.041001: step 2839, loss 0.161269, acc 0.9375, learning_rate 0.000100045
2017-10-10T15:05:58.370383: step 2840, loss 0.0766626, acc 0.96875, learning_rate 0.000100044

Evaluation:
2017-10-10T15:05:58.857503: step 2840, loss 0.237629, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2840

2017-10-10T15:06:00.013165: step 2841, loss 0.159843, acc 0.953125, learning_rate 0.000100044
2017-10-10T15:06:00.312901: step 2842, loss 0.209564, acc 0.960784, learning_rate 0.000100044
2017-10-10T15:06:00.569016: step 2843, loss 0.105732, acc 0.96875, learning_rate 0.000100044
2017-10-10T15:06:00.813132: step 2844, loss 0.0920318, acc 1, learning_rate 0.000100044
2017-10-10T15:06:01.105902: step 2845, loss 0.16375, acc 0.96875, learning_rate 0.000100043
2017-10-10T15:06:01.397413: step 2846, loss 0.0488131, acc 1, learning_rate 0.000100043
2017-10-10T15:06:01.688670: step 2847, loss 0.197767, acc 0.953125, learning_rate 0.000100043
2017-10-10T15:06:01.975610: step 2848, loss 0.149333, acc 0.953125, learning_rate 0.000100043
2017-10-10T15:06:02.286434: step 2849, loss 0.16891, acc 0.96875, learning_rate 0.000100043
2017-10-10T15:06:02.648985: step 2850, loss 0.211848, acc 0.9375, learning_rate 0.000100043
2017-10-10T15:06:02.961098: step 2851, loss 0.103459, acc 0.96875, learning_rate 0.000100042
2017-10-10T15:06:03.209721: step 2852, loss 0.0883987, acc 0.984375, learning_rate 0.000100042
2017-10-10T15:06:03.440812: step 2853, loss 0.141894, acc 0.953125, learning_rate 0.000100042
2017-10-10T15:06:03.677294: step 2854, loss 0.082641, acc 0.984375, learning_rate 0.000100042
2017-10-10T15:06:03.976984: step 2855, loss 0.228118, acc 0.921875, learning_rate 0.000100042
2017-10-10T15:06:04.273835: step 2856, loss 0.195671, acc 0.9375, learning_rate 0.000100042
2017-10-10T15:06:04.577193: step 2857, loss 0.0762818, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:06:04.872503: step 2858, loss 0.132258, acc 0.96875, learning_rate 0.000100041
2017-10-10T15:06:05.148991: step 2859, loss 0.115213, acc 0.953125, learning_rate 0.000100041
2017-10-10T15:06:05.477035: step 2860, loss 0.0759997, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:06:05.808983: step 2861, loss 0.13523, acc 0.953125, learning_rate 0.000100041
2017-10-10T15:06:06.184895: step 2862, loss 0.0960208, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:06:06.476970: step 2863, loss 0.176163, acc 0.921875, learning_rate 0.00010004
2017-10-10T15:06:06.722310: step 2864, loss 0.216264, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:06:07.075267: step 2865, loss 0.0605169, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:06:07.378702: step 2866, loss 0.106745, acc 0.953125, learning_rate 0.00010004
2017-10-10T15:06:07.655900: step 2867, loss 0.133791, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:06:07.945001: step 2868, loss 0.0898298, acc 1, learning_rate 0.00010004
2017-10-10T15:06:08.240007: step 2869, loss 0.173321, acc 0.953125, learning_rate 0.000100039
2017-10-10T15:06:08.526078: step 2870, loss 0.0628732, acc 1, learning_rate 0.000100039
2017-10-10T15:06:08.894293: step 2871, loss 0.116441, acc 0.953125, learning_rate 0.000100039
2017-10-10T15:06:09.229474: step 2872, loss 0.0726969, acc 1, learning_rate 0.000100039
2017-10-10T15:06:09.521071: step 2873, loss 0.13378, acc 0.953125, learning_rate 0.000100039
2017-10-10T15:06:09.861191: step 2874, loss 0.121289, acc 0.953125, learning_rate 0.000100039
2017-10-10T15:06:10.177294: step 2875, loss 0.181016, acc 0.9375, learning_rate 0.000100038
2017-10-10T15:06:10.491184: step 2876, loss 0.077894, acc 0.984375, learning_rate 0.000100038
2017-10-10T15:06:10.756839: step 2877, loss 0.0600339, acc 0.96875, learning_rate 0.000100038
2017-10-10T15:06:11.049025: step 2878, loss 0.146053, acc 0.953125, learning_rate 0.000100038
2017-10-10T15:06:11.284966: step 2879, loss 0.116324, acc 0.984375, learning_rate 0.000100038
2017-10-10T15:06:11.635410: step 2880, loss 0.161117, acc 0.921875, learning_rate 0.000100038

Evaluation:
2017-10-10T15:06:12.016357: step 2880, loss 0.237099, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2880

2017-10-10T15:06:13.318206: step 2881, loss 0.132509, acc 0.96875, learning_rate 0.000100038
2017-10-10T15:06:13.573249: step 2882, loss 0.141945, acc 0.984375, learning_rate 0.000100037
2017-10-10T15:06:13.920445: step 2883, loss 0.143468, acc 0.9375, learning_rate 0.000100037
2017-10-10T15:06:14.197539: step 2884, loss 0.0963835, acc 0.96875, learning_rate 0.000100037
2017-10-10T15:06:14.487885: step 2885, loss 0.0664178, acc 0.984375, learning_rate 0.000100037
2017-10-10T15:06:14.815628: step 2886, loss 0.161872, acc 0.90625, learning_rate 0.000100037
2017-10-10T15:06:15.131611: step 2887, loss 0.172273, acc 0.9375, learning_rate 0.000100037
2017-10-10T15:06:15.411752: step 2888, loss 0.219073, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:06:15.778701: step 2889, loss 0.0906288, acc 0.984375, learning_rate 0.000100036
2017-10-10T15:06:16.061009: step 2890, loss 0.0992683, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:06:16.442099: step 2891, loss 0.178774, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:06:16.788860: step 2892, loss 0.107366, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:06:17.061048: step 2893, loss 0.193689, acc 0.9375, learning_rate 0.000100036
2017-10-10T15:06:17.346981: step 2894, loss 0.131032, acc 0.96875, learning_rate 0.000100036
2017-10-10T15:06:17.649262: step 2895, loss 0.084856, acc 0.984375, learning_rate 0.000100035
2017-10-10T15:06:17.940852: step 2896, loss 0.160368, acc 0.9375, learning_rate 0.000100035
2017-10-10T15:06:18.274447: step 2897, loss 0.160164, acc 0.921875, learning_rate 0.000100035
2017-10-10T15:06:18.604844: step 2898, loss 0.161128, acc 0.921875, learning_rate 0.000100035
2017-10-10T15:06:18.909032: step 2899, loss 0.157966, acc 0.9375, learning_rate 0.000100035
2017-10-10T15:06:19.163671: step 2900, loss 0.190794, acc 0.921875, learning_rate 0.000100035
2017-10-10T15:06:19.479593: step 2901, loss 0.07602, acc 0.984375, learning_rate 0.000100035
2017-10-10T15:06:19.777387: step 2902, loss 0.0669982, acc 1, learning_rate 0.000100034
2017-10-10T15:06:20.065224: step 2903, loss 0.109034, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:06:20.427514: step 2904, loss 0.0784542, acc 0.984375, learning_rate 0.000100034
2017-10-10T15:06:20.738503: step 2905, loss 0.14198, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:06:21.080371: step 2906, loss 0.244437, acc 0.921875, learning_rate 0.000100034
2017-10-10T15:06:21.418896: step 2907, loss 0.237981, acc 0.890625, learning_rate 0.000100034
2017-10-10T15:06:21.710169: step 2908, loss 0.0879239, acc 0.984375, learning_rate 0.000100034
2017-10-10T15:06:21.984995: step 2909, loss 0.15832, acc 0.96875, learning_rate 0.000100033
2017-10-10T15:06:22.277053: step 2910, loss 0.122061, acc 0.96875, learning_rate 0.000100033
2017-10-10T15:06:22.596943: step 2911, loss 0.197697, acc 0.921875, learning_rate 0.000100033
2017-10-10T15:06:22.888294: step 2912, loss 0.25143, acc 0.921875, learning_rate 0.000100033
2017-10-10T15:06:23.263232: step 2913, loss 0.216786, acc 0.953125, learning_rate 0.000100033
2017-10-10T15:06:23.517516: step 2914, loss 0.0825092, acc 0.96875, learning_rate 0.000100033
2017-10-10T15:06:23.849009: step 2915, loss 0.174117, acc 0.9375, learning_rate 0.000100033
2017-10-10T15:06:24.124838: step 2916, loss 0.128014, acc 0.953125, learning_rate 0.000100033
2017-10-10T15:06:24.482761: step 2917, loss 0.11274, acc 0.96875, learning_rate 0.000100032
2017-10-10T15:06:24.740322: step 2918, loss 0.108347, acc 0.984375, learning_rate 0.000100032
2017-10-10T15:06:25.052102: step 2919, loss 0.13637, acc 0.953125, learning_rate 0.000100032
2017-10-10T15:06:25.376734: step 2920, loss 0.162259, acc 0.953125, learning_rate 0.000100032

Evaluation:
2017-10-10T15:06:25.935692: step 2920, loss 0.23595, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2920

2017-10-10T15:06:27.056917: step 2921, loss 0.0796121, acc 0.984375, learning_rate 0.000100032
2017-10-10T15:06:27.336823: step 2922, loss 0.153534, acc 0.96875, learning_rate 0.000100032
2017-10-10T15:06:27.641915: step 2923, loss 0.0920215, acc 0.953125, learning_rate 0.000100032
2017-10-10T15:06:27.944991: step 2924, loss 0.0506999, acc 1, learning_rate 0.000100031
2017-10-10T15:06:28.213136: step 2925, loss 0.213686, acc 0.9375, learning_rate 0.000100031
2017-10-10T15:06:28.504245: step 2926, loss 0.185622, acc 0.9375, learning_rate 0.000100031
2017-10-10T15:06:28.780656: step 2927, loss 0.0934757, acc 0.953125, learning_rate 0.000100031
2017-10-10T15:06:29.128947: step 2928, loss 0.171241, acc 0.9375, learning_rate 0.000100031
2017-10-10T15:06:29.451788: step 2929, loss 0.0941285, acc 0.96875, learning_rate 0.000100031
2017-10-10T15:06:29.741538: step 2930, loss 0.140278, acc 0.953125, learning_rate 0.000100031
2017-10-10T15:06:30.040879: step 2931, loss 0.147677, acc 0.9375, learning_rate 0.000100031
2017-10-10T15:06:30.384967: step 2932, loss 0.287055, acc 0.890625, learning_rate 0.00010003
2017-10-10T15:06:30.703165: step 2933, loss 0.0876917, acc 1, learning_rate 0.00010003
2017-10-10T15:06:30.982543: step 2934, loss 0.127887, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:06:31.277727: step 2935, loss 0.127081, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:06:31.572961: step 2936, loss 0.106289, acc 0.96875, learning_rate 0.00010003
2017-10-10T15:06:31.892890: step 2937, loss 0.160892, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:06:32.216854: step 2938, loss 0.0471671, acc 0.984375, learning_rate 0.00010003
2017-10-10T15:06:32.556837: step 2939, loss 0.180817, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:06:32.774790: step 2940, loss 0.130198, acc 0.980392, learning_rate 0.000100029
2017-10-10T15:06:33.123650: step 2941, loss 0.124675, acc 0.984375, learning_rate 0.000100029
2017-10-10T15:06:33.480815: step 2942, loss 0.246426, acc 0.9375, learning_rate 0.000100029
2017-10-10T15:06:33.727257: step 2943, loss 0.085026, acc 1, learning_rate 0.000100029
2017-10-10T15:06:33.980845: step 2944, loss 0.227616, acc 0.953125, learning_rate 0.000100029
2017-10-10T15:06:34.321859: step 2945, loss 0.064604, acc 0.984375, learning_rate 0.000100029
2017-10-10T15:06:34.616881: step 2946, loss 0.157974, acc 0.953125, learning_rate 0.000100029
2017-10-10T15:06:34.900968: step 2947, loss 0.0723038, acc 0.984375, learning_rate 0.000100029
2017-10-10T15:06:35.095948: step 2948, loss 0.106251, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:06:35.360516: step 2949, loss 0.106283, acc 0.984375, learning_rate 0.000100028
2017-10-10T15:06:35.645409: step 2950, loss 0.165824, acc 0.953125, learning_rate 0.000100028
2017-10-10T15:06:36.007774: step 2951, loss 0.156435, acc 0.921875, learning_rate 0.000100028
2017-10-10T15:06:36.324954: step 2952, loss 0.145314, acc 0.953125, learning_rate 0.000100028
2017-10-10T15:06:36.637210: step 2953, loss 0.147376, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:06:36.943562: step 2954, loss 0.086358, acc 0.953125, learning_rate 0.000100028
2017-10-10T15:06:37.227816: step 2955, loss 0.099372, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:06:37.525352: step 2956, loss 0.168951, acc 0.90625, learning_rate 0.000100028
2017-10-10T15:06:37.847053: step 2957, loss 0.103002, acc 0.984375, learning_rate 0.000100028
2017-10-10T15:06:38.113058: step 2958, loss 0.128049, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:06:38.374752: step 2959, loss 0.171104, acc 0.9375, learning_rate 0.000100027
2017-10-10T15:06:38.721496: step 2960, loss 0.085939, acc 0.96875, learning_rate 0.000100027

Evaluation:
2017-10-10T15:06:39.313625: step 2960, loss 0.235983, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-2960

2017-10-10T15:06:40.450417: step 2961, loss 0.156009, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:06:40.768430: step 2962, loss 0.114649, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:06:41.036103: step 2963, loss 0.0611292, acc 1, learning_rate 0.000100027
2017-10-10T15:06:41.337649: step 2964, loss 0.0627939, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:06:41.617107: step 2965, loss 0.168901, acc 0.9375, learning_rate 0.000100027
2017-10-10T15:06:41.949262: step 2966, loss 0.123419, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:06:42.306221: step 2967, loss 0.230729, acc 0.921875, learning_rate 0.000100026
2017-10-10T15:06:42.598756: step 2968, loss 0.0967512, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:06:42.896124: step 2969, loss 0.122029, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:06:43.173241: step 2970, loss 0.0953841, acc 0.96875, learning_rate 0.000100026
2017-10-10T15:06:43.480299: step 2971, loss 0.0815314, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:06:43.780893: step 2972, loss 0.241938, acc 0.890625, learning_rate 0.000100026
2017-10-10T15:06:44.045069: step 2973, loss 0.0668882, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:06:44.353043: step 2974, loss 0.088032, acc 0.953125, learning_rate 0.000100026
2017-10-10T15:06:44.637145: step 2975, loss 0.150009, acc 0.9375, learning_rate 0.000100026
2017-10-10T15:06:44.926568: step 2976, loss 0.0463521, acc 1, learning_rate 0.000100025
2017-10-10T15:06:45.183745: step 2977, loss 0.179879, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:06:45.533306: step 2978, loss 0.134414, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:06:45.807098: step 2979, loss 0.0962645, acc 0.9375, learning_rate 0.000100025
2017-10-10T15:06:46.049941: step 2980, loss 0.173443, acc 0.96875, learning_rate 0.000100025
2017-10-10T15:06:46.405954: step 2981, loss 0.150041, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:06:46.705444: step 2982, loss 0.105103, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:06:47.099955: step 2983, loss 0.333543, acc 0.90625, learning_rate 0.000100025
2017-10-10T15:06:47.352467: step 2984, loss 0.148247, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:06:47.689677: step 2985, loss 0.0977157, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:06:48.011496: step 2986, loss 0.100275, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:06:48.348912: step 2987, loss 0.173199, acc 0.9375, learning_rate 0.000100024
2017-10-10T15:06:48.671397: step 2988, loss 0.173391, acc 0.921875, learning_rate 0.000100024
2017-10-10T15:06:49.015893: step 2989, loss 0.222429, acc 0.9375, learning_rate 0.000100024
2017-10-10T15:06:49.348940: step 2990, loss 0.0811603, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:06:49.594932: step 2991, loss 0.0819088, acc 0.96875, learning_rate 0.000100024
2017-10-10T15:06:49.834540: step 2992, loss 0.206974, acc 0.921875, learning_rate 0.000100024
2017-10-10T15:06:50.112844: step 2993, loss 0.0442118, acc 1, learning_rate 0.000100024
2017-10-10T15:06:50.349613: step 2994, loss 0.0382948, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:06:50.685337: step 2995, loss 0.0430555, acc 1, learning_rate 0.000100024
2017-10-10T15:06:50.945941: step 2996, loss 0.104559, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:06:51.253150: step 2997, loss 0.0608209, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:06:51.592873: step 2998, loss 0.132788, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:06:51.921093: step 2999, loss 0.139749, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:06:52.227905: step 3000, loss 0.100389, acc 0.984375, learning_rate 0.000100023

Evaluation:
2017-10-10T15:06:52.758923: step 3000, loss 0.23629, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3000

2017-10-10T15:06:53.937529: step 3001, loss 0.155151, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:06:54.220388: step 3002, loss 0.111448, acc 0.9375, learning_rate 0.000100023
2017-10-10T15:06:54.556508: step 3003, loss 0.153738, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:06:54.872875: step 3004, loss 0.109654, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:06:55.136891: step 3005, loss 0.101606, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:06:55.410460: step 3006, loss 0.162145, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:06:55.703424: step 3007, loss 0.182602, acc 0.9375, learning_rate 0.000100022
2017-10-10T15:06:56.001097: step 3008, loss 0.14355, acc 0.9375, learning_rate 0.000100022
2017-10-10T15:06:56.308899: step 3009, loss 0.14058, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:06:56.613312: step 3010, loss 0.20156, acc 0.9375, learning_rate 0.000100022
2017-10-10T15:06:56.880842: step 3011, loss 0.0977892, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:06:57.172887: step 3012, loss 0.108923, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:06:57.465039: step 3013, loss 0.0790856, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:06:57.703853: step 3014, loss 0.07635, acc 1, learning_rate 0.000100022
2017-10-10T15:06:57.990614: step 3015, loss 0.132189, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:06:58.242200: step 3016, loss 0.132267, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:06:58.488848: step 3017, loss 0.0988108, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:06:58.782263: step 3018, loss 0.100402, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:06:59.050404: step 3019, loss 0.127976, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:06:59.368382: step 3020, loss 0.0796499, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:06:59.658997: step 3021, loss 0.0910202, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:06:59.888460: step 3022, loss 0.100188, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:07:00.142506: step 3023, loss 0.139519, acc 0.9375, learning_rate 0.000100021
2017-10-10T15:07:00.411457: step 3024, loss 0.137178, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:07:00.698916: step 3025, loss 0.171361, acc 0.921875, learning_rate 0.000100021
2017-10-10T15:07:00.997385: step 3026, loss 0.256409, acc 0.90625, learning_rate 0.000100021
2017-10-10T15:07:01.329804: step 3027, loss 0.0874125, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:07:01.655896: step 3028, loss 0.0907322, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:07:01.936939: step 3029, loss 0.110688, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:07:02.202119: step 3030, loss 0.0823812, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:07:02.460921: step 3031, loss 0.0965132, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:07:02.764723: step 3032, loss 0.108195, acc 0.9375, learning_rate 0.00010002
2017-10-10T15:07:03.088749: step 3033, loss 0.109495, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:07:03.453149: step 3034, loss 0.285734, acc 0.875, learning_rate 0.00010002
2017-10-10T15:07:03.667703: step 3035, loss 0.139907, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:07:03.961914: step 3036, loss 0.171892, acc 0.921875, learning_rate 0.00010002
2017-10-10T15:07:04.256702: step 3037, loss 0.0239954, acc 1, learning_rate 0.00010002
2017-10-10T15:07:04.516396: step 3038, loss 0.228526, acc 0.941176, learning_rate 0.00010002
2017-10-10T15:07:04.794169: step 3039, loss 0.0914742, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:07:05.085189: step 3040, loss 0.110334, acc 0.984375, learning_rate 0.00010002

Evaluation:
2017-10-10T15:07:05.590929: step 3040, loss 0.234027, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3040

2017-10-10T15:07:06.576839: step 3041, loss 0.0815043, acc 1, learning_rate 0.00010002
2017-10-10T15:07:06.816672: step 3042, loss 0.104856, acc 0.953125, learning_rate 0.000100019
2017-10-10T15:07:07.057718: step 3043, loss 0.0900881, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:07:07.443953: step 3044, loss 0.125369, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:07:07.705860: step 3045, loss 0.0597304, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:07:07.935259: step 3046, loss 0.0498481, acc 1, learning_rate 0.000100019
2017-10-10T15:07:08.211363: step 3047, loss 0.146005, acc 0.953125, learning_rate 0.000100019
2017-10-10T15:07:08.504718: step 3048, loss 0.0740767, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:07:08.793035: step 3049, loss 0.153047, acc 0.9375, learning_rate 0.000100019
2017-10-10T15:07:09.071953: step 3050, loss 0.0627566, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:07:09.379257: step 3051, loss 0.0713363, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:07:09.664968: step 3052, loss 0.135977, acc 0.9375, learning_rate 0.000100019
2017-10-10T15:07:10.024981: step 3053, loss 0.0734419, acc 0.984375, learning_rate 0.000100019
2017-10-10T15:07:10.323678: step 3054, loss 0.223808, acc 0.90625, learning_rate 0.000100018
2017-10-10T15:07:10.681042: step 3055, loss 0.0475622, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:07:10.940997: step 3056, loss 0.0948555, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:07:11.225676: step 3057, loss 0.0798054, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:07:11.557046: step 3058, loss 0.191545, acc 0.9375, learning_rate 0.000100018
2017-10-10T15:07:11.905502: step 3059, loss 0.130334, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:07:12.593595: step 3060, loss 0.159618, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:07:12.816925: step 3061, loss 0.0744087, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:07:13.115786: step 3062, loss 0.182545, acc 0.921875, learning_rate 0.000100018
2017-10-10T15:07:13.461092: step 3063, loss 0.105105, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:07:13.769051: step 3064, loss 0.107944, acc 0.9375, learning_rate 0.000100018
2017-10-10T15:07:14.087636: step 3065, loss 0.180987, acc 0.921875, learning_rate 0.000100018
2017-10-10T15:07:14.348145: step 3066, loss 0.131825, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:07:14.688928: step 3067, loss 0.166284, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:07:14.989009: step 3068, loss 0.0474824, acc 1, learning_rate 0.000100017
2017-10-10T15:07:15.248809: step 3069, loss 0.157532, acc 0.9375, learning_rate 0.000100017
2017-10-10T15:07:15.565494: step 3070, loss 0.122402, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:07:15.874461: step 3071, loss 0.113434, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:07:16.247056: step 3072, loss 0.164233, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:07:16.604858: step 3073, loss 0.107601, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:07:16.865034: step 3074, loss 0.149859, acc 0.9375, learning_rate 0.000100017
2017-10-10T15:07:17.116587: step 3075, loss 0.103873, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:07:17.461136: step 3076, loss 0.169735, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:07:17.732967: step 3077, loss 0.114523, acc 0.9375, learning_rate 0.000100017
2017-10-10T15:07:17.996133: step 3078, loss 0.219628, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:07:18.344867: step 3079, loss 0.110954, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:07:18.700875: step 3080, loss 0.120878, acc 0.953125, learning_rate 0.000100017

Evaluation:
2017-10-10T15:07:19.196194: step 3080, loss 0.235468, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3080

2017-10-10T15:07:20.278068: step 3081, loss 0.143961, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:07:20.552409: step 3082, loss 0.10719, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:07:20.855530: step 3083, loss 0.0490172, acc 1, learning_rate 0.000100016
2017-10-10T15:07:21.159394: step 3084, loss 0.139172, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:07:21.463079: step 3085, loss 0.110854, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:07:21.749035: step 3086, loss 0.126845, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:07:22.024873: step 3087, loss 0.142328, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:07:22.336980: step 3088, loss 0.136101, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:07:22.652894: step 3089, loss 0.0978403, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:07:22.986505: step 3090, loss 0.0725999, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:07:23.325382: step 3091, loss 0.170159, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:07:23.570313: step 3092, loss 0.154165, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:07:23.800804: step 3093, loss 0.116141, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:07:24.131756: step 3094, loss 0.107905, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:07:24.449585: step 3095, loss 0.234007, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:07:24.716411: step 3096, loss 0.143652, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:07:24.988359: step 3097, loss 0.112539, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:07:25.293454: step 3098, loss 0.149982, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:07:25.623471: step 3099, loss 0.190841, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:07:25.945497: step 3100, loss 0.180332, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:07:26.224882: step 3101, loss 0.100838, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:07:26.517580: step 3102, loss 0.0659095, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:07:26.785057: step 3103, loss 0.0866388, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:07:27.052791: step 3104, loss 0.118961, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:07:27.392993: step 3105, loss 0.109692, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:07:27.722122: step 3106, loss 0.116924, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:07:28.012993: step 3107, loss 0.111661, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:07:28.312968: step 3108, loss 0.0644888, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:07:28.618911: step 3109, loss 0.10385, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:07:28.887136: step 3110, loss 0.166186, acc 0.921875, learning_rate 0.000100015
2017-10-10T15:07:29.173002: step 3111, loss 0.123467, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:07:29.493688: step 3112, loss 0.141712, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:07:29.815148: step 3113, loss 0.101307, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:07:30.157270: step 3114, loss 0.106266, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:07:30.417798: step 3115, loss 0.151989, acc 0.90625, learning_rate 0.000100014
2017-10-10T15:07:30.673835: step 3116, loss 0.110111, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:07:30.932890: step 3117, loss 0.180128, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:07:31.262225: step 3118, loss 0.0993238, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:07:31.563971: step 3119, loss 0.0730467, acc 1, learning_rate 0.000100014
2017-10-10T15:07:31.831057: step 3120, loss 0.275123, acc 0.890625, learning_rate 0.000100014

Evaluation:
2017-10-10T15:07:32.301499: step 3120, loss 0.234359, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3120

2017-10-10T15:07:33.650860: step 3121, loss 0.11637, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:07:33.953134: step 3122, loss 0.0900533, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:07:34.298666: step 3123, loss 0.165367, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:07:34.612000: step 3124, loss 0.127105, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:07:34.896862: step 3125, loss 0.0980459, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:07:35.163969: step 3126, loss 0.163305, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:07:35.436985: step 3127, loss 0.0777814, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:07:35.693007: step 3128, loss 0.0968462, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:07:36.024984: step 3129, loss 0.131364, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:07:36.383322: step 3130, loss 0.0993424, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:07:36.634140: step 3131, loss 0.0947618, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:07:36.887376: step 3132, loss 0.0602097, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:07:37.181796: step 3133, loss 0.121778, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:07:37.688632: step 3134, loss 0.13047, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:07:38.013497: step 3135, loss 0.150317, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:07:38.256240: step 3136, loss 0.109666, acc 0.960784, learning_rate 0.000100013
2017-10-10T15:07:38.578484: step 3137, loss 0.130075, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:07:38.861072: step 3138, loss 0.158043, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:07:39.179316: step 3139, loss 0.107604, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:07:39.504939: step 3140, loss 0.132111, acc 0.921875, learning_rate 0.000100013
2017-10-10T15:07:39.761543: step 3141, loss 0.148108, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:07:40.073011: step 3142, loss 0.0872647, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:07:40.304464: step 3143, loss 0.138419, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:07:40.561668: step 3144, loss 0.0949831, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:07:40.865164: step 3145, loss 0.0360185, acc 1, learning_rate 0.000100013
2017-10-10T15:07:41.152819: step 3146, loss 0.109382, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:07:41.480878: step 3147, loss 0.126827, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:07:41.794646: step 3148, loss 0.0807428, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:07:42.019044: step 3149, loss 0.0670795, acc 1, learning_rate 0.000100013
2017-10-10T15:07:42.299176: step 3150, loss 0.160481, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:07:42.584986: step 3151, loss 0.0869827, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:07:42.901155: step 3152, loss 0.145633, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:07:43.204681: step 3153, loss 0.131869, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:07:43.547780: step 3154, loss 0.229621, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:07:43.849014: step 3155, loss 0.0987876, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:07:44.153116: step 3156, loss 0.0973533, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:07:44.452925: step 3157, loss 0.125619, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:07:44.768027: step 3158, loss 0.101563, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:07:45.048933: step 3159, loss 0.126309, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:07:45.328117: step 3160, loss 0.122728, acc 0.953125, learning_rate 0.000100012

Evaluation:
2017-10-10T15:07:45.844585: step 3160, loss 0.234646, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3160

2017-10-10T15:07:46.808931: step 3161, loss 0.149554, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:07:47.096848: step 3162, loss 0.0761754, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:07:47.430120: step 3163, loss 0.133905, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:07:47.745025: step 3164, loss 0.131757, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:07:47.997037: step 3165, loss 0.126679, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:07:48.343560: step 3166, loss 0.153744, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:07:48.704835: step 3167, loss 0.289432, acc 0.890625, learning_rate 0.000100012
2017-10-10T15:07:48.985174: step 3168, loss 0.159009, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:07:49.240833: step 3169, loss 0.154124, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:07:49.556940: step 3170, loss 0.10087, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:07:49.856379: step 3171, loss 0.148576, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:07:50.107542: step 3172, loss 0.0651488, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:07:50.427463: step 3173, loss 0.167478, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:07:50.729979: step 3174, loss 0.120448, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:07:50.981740: step 3175, loss 0.100022, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:07:51.282942: step 3176, loss 0.106363, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:07:51.615579: step 3177, loss 0.113617, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:07:51.895155: step 3178, loss 0.163582, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:07:52.200939: step 3179, loss 0.0944168, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:07:52.464800: step 3180, loss 0.128271, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:07:52.796835: step 3181, loss 0.0554612, acc 1, learning_rate 0.000100011
2017-10-10T15:07:53.156821: step 3182, loss 0.241356, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:07:53.482190: step 3183, loss 0.126272, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:07:53.683929: step 3184, loss 0.150814, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:07:53.951283: step 3185, loss 0.134692, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:07:54.197557: step 3186, loss 0.121361, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:07:54.525187: step 3187, loss 0.178329, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:07:54.852019: step 3188, loss 0.119299, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:07:55.143149: step 3189, loss 0.126904, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:07:55.504850: step 3190, loss 0.15406, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:07:55.808870: step 3191, loss 0.163583, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:07:56.089287: step 3192, loss 0.18801, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:07:56.388746: step 3193, loss 0.111703, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:07:56.657620: step 3194, loss 0.109231, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:07:56.920981: step 3195, loss 0.126264, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:07:57.236841: step 3196, loss 0.1042, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:07:57.546475: step 3197, loss 0.121775, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:07:57.869107: step 3198, loss 0.183111, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:07:58.131501: step 3199, loss 0.0528601, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:07:58.423767: step 3200, loss 0.115782, acc 0.953125, learning_rate 0.00010001

Evaluation:
2017-10-10T15:07:58.971394: step 3200, loss 0.233924, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3200

2017-10-10T15:08:00.090940: step 3201, loss 0.140479, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:08:00.425048: step 3202, loss 0.123319, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:08:00.622802: step 3203, loss 0.0961277, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:08:00.996272: step 3204, loss 0.150812, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:08:01.314947: step 3205, loss 0.0657023, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:08:01.544945: step 3206, loss 0.215136, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:08:01.838371: step 3207, loss 0.16313, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:08:02.170113: step 3208, loss 0.0804396, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:08:02.462265: step 3209, loss 0.258598, acc 0.875, learning_rate 0.00010001
2017-10-10T15:08:02.802050: step 3210, loss 0.0916734, acc 1, learning_rate 0.00010001
2017-10-10T15:08:03.132282: step 3211, loss 0.184915, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:08:03.437236: step 3212, loss 0.098144, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:08:03.737560: step 3213, loss 0.145687, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:08:04.020808: step 3214, loss 0.18159, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:08:04.296266: step 3215, loss 0.103748, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:08:04.648862: step 3216, loss 0.091057, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:08:04.961367: step 3217, loss 0.176802, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:08:05.246356: step 3218, loss 0.149787, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:08:05.533148: step 3219, loss 0.107153, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:08:05.844877: step 3220, loss 0.107486, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:08:06.202400: step 3221, loss 0.162789, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:08:06.544591: step 3222, loss 0.097967, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:08:06.878996: step 3223, loss 0.0512338, acc 1, learning_rate 0.000100009
2017-10-10T15:08:07.218219: step 3224, loss 0.0685093, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:08:07.431757: step 3225, loss 0.171395, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:08:07.787246: step 3226, loss 0.0413791, acc 1, learning_rate 0.000100009
2017-10-10T15:08:08.124226: step 3227, loss 0.173438, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:08:08.476842: step 3228, loss 0.115871, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:08:08.720093: step 3229, loss 0.0924621, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:08:09.035224: step 3230, loss 0.160514, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:08:09.340610: step 3231, loss 0.0967977, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:08:09.570622: step 3232, loss 0.165579, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:08:09.870448: step 3233, loss 0.113391, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:08:10.118182: step 3234, loss 0.157945, acc 0.941176, learning_rate 0.000100009
2017-10-10T15:08:10.374702: step 3235, loss 0.0920713, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:08:10.689207: step 3236, loss 0.162136, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:08:11.043968: step 3237, loss 0.120213, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:08:11.412441: step 3238, loss 0.0708316, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:08:11.726575: step 3239, loss 0.073753, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:08:11.962998: step 3240, loss 0.0972474, acc 0.953125, learning_rate 0.000100009

Evaluation:
2017-10-10T15:08:12.480827: step 3240, loss 0.232079, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3240

2017-10-10T15:08:13.617712: step 3241, loss 0.0839954, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:08:13.942498: step 3242, loss 0.105665, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:08:14.299720: step 3243, loss 0.136162, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:08:14.577001: step 3244, loss 0.0783725, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:08:14.843090: step 3245, loss 0.0915078, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:08:15.120741: step 3246, loss 0.0642235, acc 1, learning_rate 0.000100008
2017-10-10T15:08:15.412891: step 3247, loss 0.216042, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:08:15.661301: step 3248, loss 0.106259, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:08:15.976756: step 3249, loss 0.152822, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:08:16.290352: step 3250, loss 0.0966879, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:08:16.630033: step 3251, loss 0.0964703, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:08:16.911936: step 3252, loss 0.27531, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:08:17.186342: step 3253, loss 0.0879576, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:08:17.528890: step 3254, loss 0.111833, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:08:17.828798: step 3255, loss 0.134044, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:08:18.157689: step 3256, loss 0.0697235, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:08:18.468856: step 3257, loss 0.0988947, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:08:18.815015: step 3258, loss 0.19159, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:08:19.088239: step 3259, loss 0.218051, acc 0.90625, learning_rate 0.000100008
2017-10-10T15:08:19.409145: step 3260, loss 0.172101, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:08:19.727243: step 3261, loss 0.166758, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:08:20.055819: step 3262, loss 0.212289, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:08:20.341019: step 3263, loss 0.107396, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:08:20.611967: step 3264, loss 0.149724, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:08:20.944876: step 3265, loss 0.122449, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:08:21.249472: step 3266, loss 0.13213, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:08:21.587036: step 3267, loss 0.150153, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:08:21.888558: step 3268, loss 0.0878266, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:08:22.204661: step 3269, loss 0.11056, acc 1, learning_rate 0.000100008
2017-10-10T15:08:22.497283: step 3270, loss 0.127918, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:08:22.730302: step 3271, loss 0.0912625, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:08:23.008791: step 3272, loss 0.0823176, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:08:23.322915: step 3273, loss 0.0642314, acc 1, learning_rate 0.000100008
2017-10-10T15:08:23.650549: step 3274, loss 0.0931605, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:08:23.976945: step 3275, loss 0.0965801, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:24.296977: step 3276, loss 0.0642516, acc 1, learning_rate 0.000100007
2017-10-10T15:08:24.632976: step 3277, loss 0.162774, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:24.896419: step 3278, loss 0.104522, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:08:25.285069: step 3279, loss 0.0911848, acc 1, learning_rate 0.000100007
2017-10-10T15:08:25.615591: step 3280, loss 0.110383, acc 0.953125, learning_rate 0.000100007

Evaluation:
2017-10-10T15:08:26.015353: step 3280, loss 0.234946, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3280

2017-10-10T15:08:27.118923: step 3281, loss 0.122037, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:08:27.377337: step 3282, loss 0.0434209, acc 1, learning_rate 0.000100007
2017-10-10T15:08:27.676148: step 3283, loss 0.180329, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:08:27.992294: step 3284, loss 0.0859019, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:28.293805: step 3285, loss 0.108424, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:08:28.570915: step 3286, loss 0.139401, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:28.869153: step 3287, loss 0.0832811, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:08:29.243360: step 3288, loss 0.126991, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:08:29.553268: step 3289, loss 0.146978, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:29.824048: step 3290, loss 0.131953, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:30.039079: step 3291, loss 0.10551, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:30.259908: step 3292, loss 0.103031, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:30.466851: step 3293, loss 0.14287, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:30.706211: step 3294, loss 0.141139, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:31.069091: step 3295, loss 0.0764237, acc 1, learning_rate 0.000100007
2017-10-10T15:08:31.377626: step 3296, loss 0.114162, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:08:31.672865: step 3297, loss 0.138532, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:08:31.980844: step 3298, loss 0.166052, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:08:32.312852: step 3299, loss 0.0636247, acc 1, learning_rate 0.000100007
2017-10-10T15:08:32.650037: step 3300, loss 0.132896, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:08:32.947442: step 3301, loss 0.149025, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:08:33.231866: step 3302, loss 0.126157, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:08:33.522286: step 3303, loss 0.143194, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:08:33.801412: step 3304, loss 0.0760645, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:08:34.057297: step 3305, loss 0.203023, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:08:34.341768: step 3306, loss 0.057, acc 1, learning_rate 0.000100007
2017-10-10T15:08:34.575955: step 3307, loss 0.123856, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:08:34.860238: step 3308, loss 0.143265, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:08:35.149878: step 3309, loss 0.209, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:08:35.449563: step 3310, loss 0.147431, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:35.784129: step 3311, loss 0.0878962, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:36.045054: step 3312, loss 0.15253, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:36.324873: step 3313, loss 0.15728, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:36.662898: step 3314, loss 0.113092, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:36.962673: step 3315, loss 0.0899267, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:37.316885: step 3316, loss 0.108647, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:37.597258: step 3317, loss 0.110246, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:37.829129: step 3318, loss 0.147468, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:38.117005: step 3319, loss 0.041824, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:38.443467: step 3320, loss 0.136464, acc 0.96875, learning_rate 0.000100006

Evaluation:
2017-10-10T15:08:38.944339: step 3320, loss 0.233288, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3320

2017-10-10T15:08:39.913231: step 3321, loss 0.111408, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:40.201220: step 3322, loss 0.128793, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:08:40.512997: step 3323, loss 0.0677871, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:40.798606: step 3324, loss 0.186385, acc 0.90625, learning_rate 0.000100006
2017-10-10T15:08:41.113043: step 3325, loss 0.105422, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:41.432259: step 3326, loss 0.164166, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:41.741156: step 3327, loss 0.15592, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:42.084863: step 3328, loss 0.109918, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:42.373823: step 3329, loss 0.128349, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:42.668998: step 3330, loss 0.167546, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:08:43.008901: step 3331, loss 0.117487, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:43.223171: step 3332, loss 0.221293, acc 0.941176, learning_rate 0.000100006
2017-10-10T15:08:43.545288: step 3333, loss 0.102717, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:43.840231: step 3334, loss 0.174835, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:08:44.165133: step 3335, loss 0.103574, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:44.441221: step 3336, loss 0.156267, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:44.758182: step 3337, loss 0.108152, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:45.074531: step 3338, loss 0.0740742, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:45.404482: step 3339, loss 0.0647962, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:45.756135: step 3340, loss 0.139041, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:46.148839: step 3341, loss 0.0576871, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:46.457556: step 3342, loss 0.0828592, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:46.744523: step 3343, loss 0.13863, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:47.029290: step 3344, loss 0.123318, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:47.419227: step 3345, loss 0.0647791, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:47.814489: step 3346, loss 0.169814, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:48.048083: step 3347, loss 0.147349, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:48.260791: step 3348, loss 0.104099, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:08:48.453414: step 3349, loss 0.144263, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:08:48.703569: step 3350, loss 0.107023, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:08:48.996840: step 3351, loss 0.128283, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:08:49.285236: step 3352, loss 0.149552, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:08:49.665115: step 3353, loss 0.0576374, acc 1, learning_rate 0.000100005
2017-10-10T15:08:49.942798: step 3354, loss 0.0814223, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:08:50.242105: step 3355, loss 0.0838407, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:08:50.527998: step 3356, loss 0.110157, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:08:50.845072: step 3357, loss 0.0643792, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:08:51.168225: step 3358, loss 0.141628, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:08:51.456844: step 3359, loss 0.112427, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:08:51.738900: step 3360, loss 0.092619, acc 0.984375, learning_rate 0.000100005

Evaluation:
2017-10-10T15:08:52.278709: step 3360, loss 0.233012, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3360

2017-10-10T15:08:53.283685: step 3361, loss 0.0752733, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:08:53.596130: step 3362, loss 0.108517, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:08:53.923710: step 3363, loss 0.0569224, acc 1, learning_rate 0.000100005
2017-10-10T15:08:54.260816: step 3364, loss 0.176153, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:08:54.533049: step 3365, loss 0.113696, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:08:54.860863: step 3366, loss 0.129764, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:08:55.181770: step 3367, loss 0.136397, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:08:55.543190: step 3368, loss 0.0866722, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:08:55.836969: step 3369, loss 0.0748727, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:08:56.112979: step 3370, loss 0.0491561, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:08:56.447620: step 3371, loss 0.21478, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:08:56.704826: step 3372, loss 0.0656126, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:08:57.000938: step 3373, loss 0.22704, acc 0.890625, learning_rate 0.000100005
2017-10-10T15:08:57.369719: step 3374, loss 0.170033, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:08:57.674206: step 3375, loss 0.102208, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:08:57.957674: step 3376, loss 0.133849, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:08:58.329024: step 3377, loss 0.0897044, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:08:58.661874: step 3378, loss 0.188386, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:08:58.949579: step 3379, loss 0.158374, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:08:59.222570: step 3380, loss 0.154751, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:08:59.504699: step 3381, loss 0.156928, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:08:59.767202: step 3382, loss 0.0946364, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:09:00.066012: step 3383, loss 0.114768, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:09:00.442373: step 3384, loss 0.224599, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:09:00.763334: step 3385, loss 0.0984056, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:09:01.033444: step 3386, loss 0.209662, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:09:01.326359: step 3387, loss 0.0588488, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:09:01.682362: step 3388, loss 0.0628228, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:09:01.951359: step 3389, loss 0.117767, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:09:02.197126: step 3390, loss 0.130914, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:09:02.508853: step 3391, loss 0.103615, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:09:02.816938: step 3392, loss 0.146563, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:09:03.084948: step 3393, loss 0.102564, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:09:03.370350: step 3394, loss 0.177741, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:09:04.494996: step 3395, loss 0.0559165, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:09:04.816841: step 3396, loss 0.14879, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:09:05.157064: step 3397, loss 0.115857, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:09:05.480907: step 3398, loss 0.171319, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:09:05.757650: step 3399, loss 0.0442902, acc 1, learning_rate 0.000100005
2017-10-10T15:09:06.176915: step 3400, loss 0.211242, acc 0.9375, learning_rate 0.000100004

Evaluation:
2017-10-10T15:09:06.771542: step 3400, loss 0.233978, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3400

2017-10-10T15:09:07.848870: step 3401, loss 0.0741077, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:08.148864: step 3402, loss 0.154344, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:08.460940: step 3403, loss 0.110656, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:09:08.770405: step 3404, loss 0.0859353, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:09.097011: step 3405, loss 0.122064, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:09.342328: step 3406, loss 0.174596, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:09:09.644903: step 3407, loss 0.16461, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:10.007480: step 3408, loss 0.0646372, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:10.344791: step 3409, loss 0.0665692, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:10.647873: step 3410, loss 0.160004, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:11.014407: step 3411, loss 0.164959, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:11.312905: step 3412, loss 0.0923014, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:11.628822: step 3413, loss 0.0822077, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:11.948884: step 3414, loss 0.080714, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:12.248827: step 3415, loss 0.0730476, acc 1, learning_rate 0.000100004
2017-10-10T15:09:12.553037: step 3416, loss 0.234209, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:12.829199: step 3417, loss 0.0780856, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:13.104987: step 3418, loss 0.105101, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:13.391002: step 3419, loss 0.153352, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:13.708839: step 3420, loss 0.115778, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:13.988932: step 3421, loss 0.15289, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:09:14.335927: step 3422, loss 0.141673, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:14.685010: step 3423, loss 0.118756, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:14.989287: step 3424, loss 0.0500551, acc 1, learning_rate 0.000100004
2017-10-10T15:09:15.292812: step 3425, loss 0.141239, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:15.562252: step 3426, loss 0.155714, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:15.858036: step 3427, loss 0.132224, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:16.176716: step 3428, loss 0.0752178, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:16.464932: step 3429, loss 0.191404, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:09:16.724902: step 3430, loss 0.200767, acc 0.941176, learning_rate 0.000100004
2017-10-10T15:09:17.070707: step 3431, loss 0.0921604, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:17.400831: step 3432, loss 0.145824, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:17.706098: step 3433, loss 0.0856247, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:18.000738: step 3434, loss 0.0716535, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:18.298929: step 3435, loss 0.244972, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:18.623509: step 3436, loss 0.13287, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:18.965000: step 3437, loss 0.112277, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:19.238646: step 3438, loss 0.159478, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:19.640955: step 3439, loss 0.0512341, acc 1, learning_rate 0.000100004
2017-10-10T15:09:19.947528: step 3440, loss 0.130944, acc 0.96875, learning_rate 0.000100004

Evaluation:
2017-10-10T15:09:20.483120: step 3440, loss 0.232931, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3440

2017-10-10T15:09:21.608915: step 3441, loss 0.217904, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:21.928273: step 3442, loss 0.106365, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:22.167919: step 3443, loss 0.120492, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:22.447578: step 3444, loss 0.172477, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:09:22.780557: step 3445, loss 0.104718, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:23.116954: step 3446, loss 0.310445, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:09:23.341899: step 3447, loss 0.140706, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:23.655867: step 3448, loss 0.231114, acc 0.890625, learning_rate 0.000100004
2017-10-10T15:09:23.960011: step 3449, loss 0.0849549, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:09:24.325970: step 3450, loss 0.105303, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:24.644894: step 3451, loss 0.0460028, acc 1, learning_rate 0.000100004
2017-10-10T15:09:24.960819: step 3452, loss 0.0869995, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:25.256917: step 3453, loss 0.074855, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:25.561373: step 3454, loss 0.129647, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:09:25.826725: step 3455, loss 0.0508178, acc 1, learning_rate 0.000100004
2017-10-10T15:09:26.131910: step 3456, loss 0.121336, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:26.370967: step 3457, loss 0.0722839, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:26.724831: step 3458, loss 0.115764, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:27.026237: step 3459, loss 0.0619366, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:09:27.328864: step 3460, loss 0.185427, acc 0.890625, learning_rate 0.000100004
2017-10-10T15:09:27.556226: step 3461, loss 0.146783, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:09:27.859692: step 3462, loss 0.161397, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:28.176358: step 3463, loss 0.11996, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:28.530548: step 3464, loss 0.166493, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:28.851453: step 3465, loss 0.0912577, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:29.141899: step 3466, loss 0.198597, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:29.465731: step 3467, loss 0.146083, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:29.715029: step 3468, loss 0.0803543, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:29.979414: step 3469, loss 0.0556444, acc 1, learning_rate 0.000100003
2017-10-10T15:09:30.276810: step 3470, loss 0.140328, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:30.581004: step 3471, loss 0.114787, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:30.908033: step 3472, loss 0.0910164, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:31.188716: step 3473, loss 0.0844069, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:31.494800: step 3474, loss 0.191178, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:31.754851: step 3475, loss 0.0734505, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:32.043578: step 3476, loss 0.0744701, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:32.367775: step 3477, loss 0.132621, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:32.676942: step 3478, loss 0.0940723, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:32.996450: step 3479, loss 0.116283, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:33.263310: step 3480, loss 0.189799, acc 0.921875, learning_rate 0.000100003

Evaluation:
2017-10-10T15:09:33.751747: step 3480, loss 0.230704, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3480

2017-10-10T15:09:34.661015: step 3481, loss 0.100715, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:34.940867: step 3482, loss 0.0937023, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:35.281061: step 3483, loss 0.126365, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:35.608878: step 3484, loss 0.1219, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:35.895058: step 3485, loss 0.107798, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:36.241091: step 3486, loss 0.0540708, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:36.523484: step 3487, loss 0.0723928, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:36.793485: step 3488, loss 0.126537, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:37.027905: step 3489, loss 0.0713164, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:37.272860: step 3490, loss 0.0583859, acc 1, learning_rate 0.000100003
2017-10-10T15:09:37.592836: step 3491, loss 0.118378, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:37.933035: step 3492, loss 0.0653707, acc 1, learning_rate 0.000100003
2017-10-10T15:09:38.228992: step 3493, loss 0.110879, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:38.551802: step 3494, loss 0.10278, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:38.867524: step 3495, loss 0.129428, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:39.162945: step 3496, loss 0.0806784, acc 1, learning_rate 0.000100003
2017-10-10T15:09:39.492939: step 3497, loss 0.0602213, acc 1, learning_rate 0.000100003
2017-10-10T15:09:39.721522: step 3498, loss 0.244247, acc 0.890625, learning_rate 0.000100003
2017-10-10T15:09:40.016254: step 3499, loss 0.0563627, acc 1, learning_rate 0.000100003
2017-10-10T15:09:40.372845: step 3500, loss 0.143514, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:40.769014: step 3501, loss 0.104815, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:41.062141: step 3502, loss 0.114301, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:41.316211: step 3503, loss 0.122851, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:41.631064: step 3504, loss 0.15275, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:09:41.964551: step 3505, loss 0.0445822, acc 1, learning_rate 0.000100003
2017-10-10T15:09:42.284731: step 3506, loss 0.21297, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:42.591138: step 3507, loss 0.153342, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:42.917398: step 3508, loss 0.096898, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:43.156917: step 3509, loss 0.126238, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:43.401187: step 3510, loss 0.136117, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:09:43.621481: step 3511, loss 0.237734, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:43.904834: step 3512, loss 0.135675, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:44.221064: step 3513, loss 0.0863255, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:44.524913: step 3514, loss 0.14328, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:09:44.792999: step 3515, loss 0.150233, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:45.076951: step 3516, loss 0.1185, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:45.440947: step 3517, loss 0.100318, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:45.775361: step 3518, loss 0.0970681, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:46.076905: step 3519, loss 0.112337, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:46.416533: step 3520, loss 0.121966, acc 0.96875, learning_rate 0.000100003

Evaluation:
2017-10-10T15:09:46.949657: step 3520, loss 0.232109, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3520

2017-10-10T15:09:48.035045: step 3521, loss 0.198103, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:48.301293: step 3522, loss 0.102614, acc 1, learning_rate 0.000100003
2017-10-10T15:09:48.659497: step 3523, loss 0.0910007, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:48.948944: step 3524, loss 0.168072, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:09:49.290645: step 3525, loss 0.139933, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:49.568972: step 3526, loss 0.0897502, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:49.885254: step 3527, loss 0.0719724, acc 1, learning_rate 0.000100003
2017-10-10T15:09:50.156981: step 3528, loss 0.0213819, acc 1, learning_rate 0.000100003
2017-10-10T15:09:50.395542: step 3529, loss 0.23181, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:09:50.741396: step 3530, loss 0.0747625, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:51.074623: step 3531, loss 0.142043, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:51.476855: step 3532, loss 0.0834412, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:51.780486: step 3533, loss 0.149351, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:52.055889: step 3534, loss 0.172741, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:09:52.336875: step 3535, loss 0.111435, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:52.644422: step 3536, loss 0.105203, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:52.922318: step 3537, loss 0.0734267, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:53.304322: step 3538, loss 0.099996, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:53.584831: step 3539, loss 0.142154, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:53.862095: step 3540, loss 0.128439, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:09:54.092676: step 3541, loss 0.142278, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:09:54.377008: step 3542, loss 0.101297, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:09:54.743665: step 3543, loss 0.07145, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:09:55.084885: step 3544, loss 0.122855, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:55.407356: step 3545, loss 0.110031, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:55.684898: step 3546, loss 0.233282, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:09:56.015243: step 3547, loss 0.173255, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:56.309143: step 3548, loss 0.0642129, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:09:56.619568: step 3549, loss 0.068315, acc 1, learning_rate 0.000100002
2017-10-10T15:09:56.918420: step 3550, loss 0.163246, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:09:57.165551: step 3551, loss 0.1061, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:57.492424: step 3552, loss 0.136858, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:09:57.800266: step 3553, loss 0.145411, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:09:58.030122: step 3554, loss 0.0461886, acc 1, learning_rate 0.000100002
2017-10-10T15:09:58.345218: step 3555, loss 0.0903553, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:58.692965: step 3556, loss 0.0718923, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:58.973046: step 3557, loss 0.124435, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:59.312251: step 3558, loss 0.0955907, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:59.616853: step 3559, loss 0.114891, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:09:59.924508: step 3560, loss 0.083343, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T15:10:00.441276: step 3560, loss 0.23407, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3560

2017-10-10T15:10:01.575099: step 3561, loss 0.118265, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:01.889107: step 3562, loss 0.183945, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:02.160618: step 3563, loss 0.104564, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:02.504891: step 3564, loss 0.0966549, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:02.844832: step 3565, loss 0.101912, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:03.189119: step 3566, loss 0.099234, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:03.565482: step 3567, loss 0.0800214, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:03.865241: step 3568, loss 0.0911189, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:04.165650: step 3569, loss 0.157113, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:04.479454: step 3570, loss 0.156162, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:04.725029: step 3571, loss 0.147729, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:05.044985: step 3572, loss 0.0507422, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:05.357756: step 3573, loss 0.123031, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:05.620933: step 3574, loss 0.181629, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:05.968800: step 3575, loss 0.10375, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:06.230672: step 3576, loss 0.174284, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:06.506476: step 3577, loss 0.203733, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:10:06.797281: step 3578, loss 0.167011, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:07.077707: step 3579, loss 0.0952766, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:07.375983: step 3580, loss 0.0742581, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:07.652192: step 3581, loss 0.133831, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:07.941649: step 3582, loss 0.0447869, acc 1, learning_rate 0.000100002
2017-10-10T15:10:08.277215: step 3583, loss 0.207192, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:10:08.612853: step 3584, loss 0.169315, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:08.884881: step 3585, loss 0.0526092, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:09.238391: step 3586, loss 0.151832, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:09.565088: step 3587, loss 0.0677166, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:09.843631: step 3588, loss 0.0702766, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:10.078171: step 3589, loss 0.153072, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:10.400817: step 3590, loss 0.210858, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:10.686355: step 3591, loss 0.063188, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:10.934941: step 3592, loss 0.103045, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:11.251101: step 3593, loss 0.176927, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:11.572917: step 3594, loss 0.0982146, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:11.838560: step 3595, loss 0.0890647, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:12.121651: step 3596, loss 0.0905513, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:12.477010: step 3597, loss 0.213493, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:12.760545: step 3598, loss 0.0820914, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:13.104901: step 3599, loss 0.0901151, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:13.392281: step 3600, loss 0.136659, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T15:10:13.935199: step 3600, loss 0.232611, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3600

2017-10-10T15:10:15.146603: step 3601, loss 0.148688, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:15.464995: step 3602, loss 0.116351, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:15.732888: step 3603, loss 0.179884, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:16.141590: step 3604, loss 0.116874, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:16.416987: step 3605, loss 0.130206, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:16.730985: step 3606, loss 0.162384, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:17.004844: step 3607, loss 0.243666, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:10:17.270001: step 3608, loss 0.211424, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:17.539755: step 3609, loss 0.108164, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:17.818101: step 3610, loss 0.11067, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:18.137186: step 3611, loss 0.112132, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:18.472912: step 3612, loss 0.0450929, acc 1, learning_rate 0.000100002
2017-10-10T15:10:18.824112: step 3613, loss 0.219104, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:19.120493: step 3614, loss 0.127316, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:19.389939: step 3615, loss 0.174834, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:19.636963: step 3616, loss 0.0830147, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:19.890250: step 3617, loss 0.0637096, acc 1, learning_rate 0.000100002
2017-10-10T15:10:20.184853: step 3618, loss 0.0823608, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:20.490926: step 3619, loss 0.166073, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:20.815174: step 3620, loss 0.179897, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:21.098527: step 3621, loss 0.129712, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:21.396974: step 3622, loss 0.133335, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:21.695997: step 3623, loss 0.074822, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:22.029545: step 3624, loss 0.156139, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:22.340888: step 3625, loss 0.089733, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:22.581952: step 3626, loss 0.199407, acc 0.882353, learning_rate 0.000100002
2017-10-10T15:10:22.926796: step 3627, loss 0.247685, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:23.236421: step 3628, loss 0.169748, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:23.539783: step 3629, loss 0.0592437, acc 1, learning_rate 0.000100002
2017-10-10T15:10:23.801225: step 3630, loss 0.189782, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:24.095044: step 3631, loss 0.16993, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:24.415154: step 3632, loss 0.148836, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:24.736848: step 3633, loss 0.123065, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:25.023094: step 3634, loss 0.12457, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:25.310555: step 3635, loss 0.101199, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:25.640020: step 3636, loss 0.241546, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:25.957461: step 3637, loss 0.0787895, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:26.302847: step 3638, loss 0.102851, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:26.553671: step 3639, loss 0.0842431, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:26.824817: step 3640, loss 0.0488108, acc 1, learning_rate 0.000100002

Evaluation:
2017-10-10T15:10:27.343753: step 3640, loss 0.231634, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3640

2017-10-10T15:10:28.765789: step 3641, loss 0.169815, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:29.069383: step 3642, loss 0.170263, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:10:29.363545: step 3643, loss 0.130983, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:29.636924: step 3644, loss 0.122098, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:29.910684: step 3645, loss 0.0940225, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:30.232116: step 3646, loss 0.0529848, acc 1, learning_rate 0.000100002
2017-10-10T15:10:30.527023: step 3647, loss 0.112475, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:30.792134: step 3648, loss 0.044023, acc 1, learning_rate 0.000100002
2017-10-10T15:10:31.131730: step 3649, loss 0.12489, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:31.404972: step 3650, loss 0.104777, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:31.690200: step 3651, loss 0.0943841, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:31.993588: step 3652, loss 0.123967, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:32.321759: step 3653, loss 0.0816694, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:32.608132: step 3654, loss 0.122081, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:32.892882: step 3655, loss 0.178355, acc 0.890625, learning_rate 0.000100002
2017-10-10T15:10:33.232970: step 3656, loss 0.0798317, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:10:33.466519: step 3657, loss 0.0711423, acc 1, learning_rate 0.000100002
2017-10-10T15:10:33.784349: step 3658, loss 0.112464, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:34.093013: step 3659, loss 0.105906, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:34.325961: step 3660, loss 0.135555, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:34.635982: step 3661, loss 0.107966, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:34.963212: step 3662, loss 0.194391, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:35.292545: step 3663, loss 0.127081, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:35.571138: step 3664, loss 0.127473, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:35.853760: step 3665, loss 0.123784, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:10:36.166625: step 3666, loss 0.184452, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:10:36.469106: step 3667, loss 0.0616505, acc 1, learning_rate 0.000100002
2017-10-10T15:10:36.752079: step 3668, loss 0.12738, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:10:37.062243: step 3669, loss 0.0733353, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:37.421994: step 3670, loss 0.122435, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:37.693131: step 3671, loss 0.122524, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:37.992921: step 3672, loss 0.125679, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:38.248340: step 3673, loss 0.189627, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:38.498134: step 3674, loss 0.100831, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:38.896836: step 3675, loss 0.111445, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:39.264839: step 3676, loss 0.137901, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:39.521713: step 3677, loss 0.0999768, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:39.784979: step 3678, loss 0.0984888, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:40.084813: step 3679, loss 0.115811, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:40.410664: step 3680, loss 0.171117, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:10:40.976336: step 3680, loss 0.230145, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3680

2017-10-10T15:10:42.009602: step 3681, loss 0.0715286, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:42.327965: step 3682, loss 0.0452323, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:42.650073: step 3683, loss 0.0550584, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:42.888864: step 3684, loss 0.126846, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:43.196150: step 3685, loss 0.1575, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:43.479512: step 3686, loss 0.0545569, acc 1, learning_rate 0.000100001
2017-10-10T15:10:43.753054: step 3687, loss 0.234208, acc 0.890625, learning_rate 0.000100001
2017-10-10T15:10:44.068877: step 3688, loss 0.12664, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:44.416849: step 3689, loss 0.240761, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:44.769718: step 3690, loss 0.0895908, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:45.105949: step 3691, loss 0.219336, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:45.385331: step 3692, loss 0.0852618, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:45.700065: step 3693, loss 0.0931282, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:46.009020: step 3694, loss 0.189801, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:46.336917: step 3695, loss 0.121505, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:46.631693: step 3696, loss 0.1459, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:46.877619: step 3697, loss 0.228757, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:47.181049: step 3698, loss 0.104789, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:47.492841: step 3699, loss 0.149405, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:47.792742: step 3700, loss 0.0960822, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:48.035355: step 3701, loss 0.156985, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:48.364082: step 3702, loss 0.269697, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:10:48.666474: step 3703, loss 0.174784, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:48.953490: step 3704, loss 0.182866, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:49.255351: step 3705, loss 0.126162, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:49.527058: step 3706, loss 0.0727204, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:49.836893: step 3707, loss 0.0481643, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:50.209039: step 3708, loss 0.0946664, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:50.440283: step 3709, loss 0.0990888, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:50.744757: step 3710, loss 0.0874486, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:51.101340: step 3711, loss 0.104479, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:51.417622: step 3712, loss 0.157729, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:10:51.711266: step 3713, loss 0.0460133, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:52.022148: step 3714, loss 0.126349, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:52.337041: step 3715, loss 0.0794125, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:52.668953: step 3716, loss 0.137001, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:52.964302: step 3717, loss 0.0765745, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:53.285183: step 3718, loss 0.0446464, acc 1, learning_rate 0.000100001
2017-10-10T15:10:53.556668: step 3719, loss 0.0631347, acc 1, learning_rate 0.000100001
2017-10-10T15:10:53.838934: step 3720, loss 0.132216, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T15:10:54.295633: step 3720, loss 0.229283, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3720

2017-10-10T15:10:55.301096: step 3721, loss 0.0809135, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:55.559121: step 3722, loss 0.107026, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:55.760809: step 3723, loss 0.0925404, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:56.005494: step 3724, loss 0.0722652, acc 0.980392, learning_rate 0.000100001
2017-10-10T15:10:56.351238: step 3725, loss 0.0877433, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:56.643414: step 3726, loss 0.0872758, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:56.940926: step 3727, loss 0.0877582, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:57.233307: step 3728, loss 0.0499735, acc 1, learning_rate 0.000100001
2017-10-10T15:10:57.630182: step 3729, loss 0.0902233, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:57.929919: step 3730, loss 0.134785, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:10:58.264100: step 3731, loss 0.0940365, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:10:58.580180: step 3732, loss 0.177653, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:58.945006: step 3733, loss 0.0836945, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:59.296890: step 3734, loss 0.0674052, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:10:59.517288: step 3735, loss 0.137017, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:10:59.778980: step 3736, loss 0.058775, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:00.055216: step 3737, loss 0.129523, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:00.349916: step 3738, loss 0.109083, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:00.684736: step 3739, loss 0.0674709, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:01.039163: step 3740, loss 0.0986602, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:01.291015: step 3741, loss 0.227297, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:11:01.577135: step 3742, loss 0.151723, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:01.839290: step 3743, loss 0.117454, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:02.098006: step 3744, loss 0.116861, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:02.374373: step 3745, loss 0.129499, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:02.722412: step 3746, loss 0.116994, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:03.020565: step 3747, loss 0.128084, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:03.297396: step 3748, loss 0.101427, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:03.616802: step 3749, loss 0.102518, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:03.900845: step 3750, loss 0.077629, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:04.213105: step 3751, loss 0.138596, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:04.527355: step 3752, loss 0.153817, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:04.837019: step 3753, loss 0.0591871, acc 1, learning_rate 0.000100001
2017-10-10T15:11:05.185280: step 3754, loss 0.109461, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:05.468874: step 3755, loss 0.0566682, acc 1, learning_rate 0.000100001
2017-10-10T15:11:05.723988: step 3756, loss 0.116497, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:06.017022: step 3757, loss 0.105897, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:06.332839: step 3758, loss 0.0841708, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:06.588182: step 3759, loss 0.161536, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:06.869995: step 3760, loss 0.0842283, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T15:11:07.419879: step 3760, loss 0.231651, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3760

2017-10-10T15:11:08.581307: step 3761, loss 0.157406, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:08.924861: step 3762, loss 0.159832, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:09.248941: step 3763, loss 0.103337, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:09.538878: step 3764, loss 0.0942795, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:09.800016: step 3765, loss 0.0605525, acc 1, learning_rate 0.000100001
2017-10-10T15:11:10.082927: step 3766, loss 0.0761473, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:10.396937: step 3767, loss 0.0489225, acc 1, learning_rate 0.000100001
2017-10-10T15:11:10.688887: step 3768, loss 0.110652, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:11.025308: step 3769, loss 0.125012, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:11.348985: step 3770, loss 0.0883509, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:11.677683: step 3771, loss 0.0940192, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:11.997625: step 3772, loss 0.120381, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:12.289334: step 3773, loss 0.0902615, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:12.642842: step 3774, loss 0.16077, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:12.977755: step 3775, loss 0.0680905, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:13.261230: step 3776, loss 0.0832333, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:13.539689: step 3777, loss 0.0446762, acc 1, learning_rate 0.000100001
2017-10-10T15:11:13.764654: step 3778, loss 0.116924, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:13.997249: step 3779, loss 0.097651, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:14.300980: step 3780, loss 0.100721, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:14.596747: step 3781, loss 0.0509043, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:14.926983: step 3782, loss 0.0816665, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:15.232836: step 3783, loss 0.180876, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:15.517609: step 3784, loss 0.0656873, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:15.833951: step 3785, loss 0.126478, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:16.184997: step 3786, loss 0.0548793, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:16.388017: step 3787, loss 0.113493, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:16.610571: step 3788, loss 0.133103, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:16.924270: step 3789, loss 0.215361, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:17.212888: step 3790, loss 0.133715, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:17.492950: step 3791, loss 0.0594746, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:17.796816: step 3792, loss 0.153725, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:18.145115: step 3793, loss 0.204852, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:18.488378: step 3794, loss 0.108063, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:18.689007: step 3795, loss 0.100344, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:18.978285: step 3796, loss 0.146651, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:19.290954: step 3797, loss 0.137084, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:19.575177: step 3798, loss 0.0429817, acc 1, learning_rate 0.000100001
2017-10-10T15:11:19.889133: step 3799, loss 0.102014, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:20.161873: step 3800, loss 0.0571034, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-10T15:11:20.704984: step 3800, loss 0.231216, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3800

2017-10-10T15:11:21.684875: step 3801, loss 0.0907586, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:22.009431: step 3802, loss 0.122249, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:22.245758: step 3803, loss 0.050626, acc 1, learning_rate 0.000100001
2017-10-10T15:11:22.542254: step 3804, loss 0.0954286, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:22.956050: step 3805, loss 0.0678979, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:23.197103: step 3806, loss 0.105122, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:23.512969: step 3807, loss 0.104995, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:23.826985: step 3808, loss 0.136533, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:24.124220: step 3809, loss 0.154311, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:24.433098: step 3810, loss 0.170906, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:24.710343: step 3811, loss 0.145743, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:25.015592: step 3812, loss 0.141829, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:25.304993: step 3813, loss 0.123591, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:25.596981: step 3814, loss 0.157238, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:25.923739: step 3815, loss 0.0837477, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:26.245063: step 3816, loss 0.106391, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:26.541139: step 3817, loss 0.164204, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:26.835908: step 3818, loss 0.13146, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:27.161454: step 3819, loss 0.0999903, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:27.384128: step 3820, loss 0.114779, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:27.693658: step 3821, loss 0.143241, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:28.032852: step 3822, loss 0.0892978, acc 0.980392, learning_rate 0.000100001
2017-10-10T15:11:28.340841: step 3823, loss 0.131551, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:28.676863: step 3824, loss 0.180121, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:28.940889: step 3825, loss 0.0673252, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:29.224205: step 3826, loss 0.112833, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:29.560622: step 3827, loss 0.143071, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:29.866717: step 3828, loss 0.0686185, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:30.177081: step 3829, loss 0.0552201, acc 1, learning_rate 0.000100001
2017-10-10T15:11:30.524876: step 3830, loss 0.0516671, acc 1, learning_rate 0.000100001
2017-10-10T15:11:30.887707: step 3831, loss 0.0263236, acc 1, learning_rate 0.000100001
2017-10-10T15:11:31.175438: step 3832, loss 0.0884844, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:31.488799: step 3833, loss 0.0561865, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:31.726088: step 3834, loss 0.0562383, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:31.960805: step 3835, loss 0.125695, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:32.162798: step 3836, loss 0.103938, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:32.572862: step 3837, loss 0.157886, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:32.897200: step 3838, loss 0.0526893, acc 1, learning_rate 0.000100001
2017-10-10T15:11:33.132875: step 3839, loss 0.052206, acc 1, learning_rate 0.000100001
2017-10-10T15:11:33.412969: step 3840, loss 0.0558162, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-10T15:11:33.893391: step 3840, loss 0.232655, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3840

2017-10-10T15:11:35.064931: step 3841, loss 0.0685496, acc 1, learning_rate 0.000100001
2017-10-10T15:11:35.401656: step 3842, loss 0.156375, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:35.647571: step 3843, loss 0.110462, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:35.959779: step 3844, loss 0.0730969, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:36.263029: step 3845, loss 0.109543, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:36.567078: step 3846, loss 0.11943, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:36.842682: step 3847, loss 0.127697, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:37.119807: step 3848, loss 0.105689, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:37.380867: step 3849, loss 0.0566564, acc 1, learning_rate 0.000100001
2017-10-10T15:11:37.687880: step 3850, loss 0.114171, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:38.001588: step 3851, loss 0.189129, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:38.305054: step 3852, loss 0.169485, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:38.595913: step 3853, loss 0.103662, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:38.900012: step 3854, loss 0.138873, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:39.170601: step 3855, loss 0.12342, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:39.424200: step 3856, loss 0.161794, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:11:39.719401: step 3857, loss 0.121265, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:40.050454: step 3858, loss 0.198964, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:40.397106: step 3859, loss 0.0811275, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:40.673424: step 3860, loss 0.0354178, acc 1, learning_rate 0.000100001
2017-10-10T15:11:40.985240: step 3861, loss 0.0971709, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:41.317912: step 3862, loss 0.168261, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:41.604324: step 3863, loss 0.0760841, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:41.895108: step 3864, loss 0.240279, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:11:42.225722: step 3865, loss 0.134273, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:42.576877: step 3866, loss 0.0915038, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:42.921686: step 3867, loss 0.191744, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:43.221040: step 3868, loss 0.11289, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:43.521827: step 3869, loss 0.146802, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:43.803817: step 3870, loss 0.0740822, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:44.081258: step 3871, loss 0.120362, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:44.354846: step 3872, loss 0.0980175, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:44.664249: step 3873, loss 0.0837949, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:44.932130: step 3874, loss 0.0995142, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:45.230638: step 3875, loss 0.0684153, acc 1, learning_rate 0.000100001
2017-10-10T15:11:45.516522: step 3876, loss 0.0919375, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:45.817065: step 3877, loss 0.114113, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:46.101551: step 3878, loss 0.143841, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:46.446819: step 3879, loss 0.186211, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:46.756879: step 3880, loss 0.0564212, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:11:47.244915: step 3880, loss 0.232855, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3880

2017-10-10T15:11:48.359364: step 3881, loss 0.0853374, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:48.649116: step 3882, loss 0.162084, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:49.016366: step 3883, loss 0.110367, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:49.441336: step 3884, loss 0.102909, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:49.724613: step 3885, loss 0.194865, acc 0.890625, learning_rate 0.000100001
2017-10-10T15:11:49.958421: step 3886, loss 0.0258269, acc 1, learning_rate 0.000100001
2017-10-10T15:11:50.154022: step 3887, loss 0.111831, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:50.369772: step 3888, loss 0.0984299, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:50.637005: step 3889, loss 0.167102, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:50.955409: step 3890, loss 0.0830523, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:51.210188: step 3891, loss 0.123683, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:51.532975: step 3892, loss 0.124795, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:51.819513: step 3893, loss 0.0480534, acc 1, learning_rate 0.000100001
2017-10-10T15:11:52.110466: step 3894, loss 0.0843379, acc 1, learning_rate 0.000100001
2017-10-10T15:11:52.431940: step 3895, loss 0.249117, acc 0.890625, learning_rate 0.000100001
2017-10-10T15:11:52.701319: step 3896, loss 0.152559, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:53.084310: step 3897, loss 0.130737, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:53.372467: step 3898, loss 0.0882109, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:53.661985: step 3899, loss 0.141111, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:54.023791: step 3900, loss 0.142079, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:54.281599: step 3901, loss 0.140158, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:54.576535: step 3902, loss 0.174831, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:54.842753: step 3903, loss 0.0916607, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:55.192886: step 3904, loss 0.0988898, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:55.500803: step 3905, loss 0.0969903, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:55.765250: step 3906, loss 0.121518, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:56.027130: step 3907, loss 0.228949, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:11:56.328526: step 3908, loss 0.143368, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:56.633308: step 3909, loss 0.0387589, acc 1, learning_rate 0.000100001
2017-10-10T15:11:56.980458: step 3910, loss 0.0788052, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:57.280920: step 3911, loss 0.148013, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:57.558816: step 3912, loss 0.0594666, acc 1, learning_rate 0.000100001
2017-10-10T15:11:57.841405: step 3913, loss 0.123699, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:11:58.142678: step 3914, loss 0.127648, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:11:58.428891: step 3915, loss 0.160942, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:11:58.724877: step 3916, loss 0.0519668, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:59.045118: step 3917, loss 0.113218, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:59.349225: step 3918, loss 0.090577, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:11:59.644311: step 3919, loss 0.0761478, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:11:59.896868: step 3920, loss 0.0754306, acc 0.980392, learning_rate 0.000100001

Evaluation:
2017-10-10T15:12:00.382092: step 3920, loss 0.229749, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3920

2017-10-10T15:12:01.548878: step 3921, loss 0.13276, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:12:01.805160: step 3922, loss 0.154332, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:12:02.116011: step 3923, loss 0.0918703, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:12:02.417060: step 3924, loss 0.168032, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:12:02.725083: step 3925, loss 0.121232, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:12:03.000884: step 3926, loss 0.124153, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:12:03.336904: step 3927, loss 0.105135, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:12:03.629216: step 3928, loss 0.144641, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:12:03.896890: step 3929, loss 0.145, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:12:04.194758: step 3930, loss 0.111548, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:12:04.508273: step 3931, loss 0.0744555, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:12:04.799999: step 3932, loss 0.173991, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:12:05.121343: step 3933, loss 0.0876122, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:12:05.449012: step 3934, loss 0.13078, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:12:05.860862: step 3935, loss 0.132422, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:12:06.144933: step 3936, loss 0.133946, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:12:06.472607: step 3937, loss 0.0971395, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:06.714445: step 3938, loss 0.119843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:07.016872: step 3939, loss 0.110858, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:07.337618: step 3940, loss 0.171878, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:07.580997: step 3941, loss 0.124572, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:07.904833: step 3942, loss 0.103905, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:08.242821: step 3943, loss 0.142645, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:08.503677: step 3944, loss 0.0865644, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:08.802104: step 3945, loss 0.136121, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:09.052825: step 3946, loss 0.0714902, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:09.273749: step 3947, loss 0.134837, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:09.598280: step 3948, loss 0.121085, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:09.892178: step 3949, loss 0.106774, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:10.178924: step 3950, loss 0.127602, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:10.501202: step 3951, loss 0.115654, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:10.826680: step 3952, loss 0.117927, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:11.103931: step 3953, loss 0.0758386, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:11.383321: step 3954, loss 0.120729, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:11.723737: step 3955, loss 0.151123, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:12.041126: step 3956, loss 0.0904231, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:12.294942: step 3957, loss 0.107671, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:12.680031: step 3958, loss 0.100018, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:12.973176: step 3959, loss 0.126217, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:13.323465: step 3960, loss 0.105098, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:13.829080: step 3960, loss 0.230621, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-3960

2017-10-10T15:12:14.752731: step 3961, loss 0.195935, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:15.085773: step 3962, loss 0.03726, acc 1, learning_rate 0.0001
2017-10-10T15:12:15.378389: step 3963, loss 0.0386908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:15.689770: step 3964, loss 0.150003, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:16.012605: step 3965, loss 0.143692, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:16.325234: step 3966, loss 0.0696845, acc 1, learning_rate 0.0001
2017-10-10T15:12:16.635955: step 3967, loss 0.074557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:16.952622: step 3968, loss 0.0356437, acc 1, learning_rate 0.0001
2017-10-10T15:12:17.245193: step 3969, loss 0.156755, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:17.509024: step 3970, loss 0.137826, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:17.848661: step 3971, loss 0.19499, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:18.211797: step 3972, loss 0.0670165, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:18.501738: step 3973, loss 0.0901966, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:18.752937: step 3974, loss 0.175662, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:19.043300: step 3975, loss 0.0600622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:19.332836: step 3976, loss 0.108319, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:19.625181: step 3977, loss 0.0578, acc 1, learning_rate 0.0001
2017-10-10T15:12:19.964758: step 3978, loss 0.0868622, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:20.256857: step 3979, loss 0.067879, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:20.536825: step 3980, loss 0.0867783, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:20.844970: step 3981, loss 0.038812, acc 1, learning_rate 0.0001
2017-10-10T15:12:21.180853: step 3982, loss 0.0502225, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:21.469155: step 3983, loss 0.138108, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:21.897038: step 3984, loss 0.166203, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:22.200978: step 3985, loss 0.142397, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:22.565373: step 3986, loss 0.0635718, acc 1, learning_rate 0.0001
2017-10-10T15:12:22.867728: step 3987, loss 0.09652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:23.148170: step 3988, loss 0.141611, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:23.412812: step 3989, loss 0.138263, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:23.689035: step 3990, loss 0.130452, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:24.000132: step 3991, loss 0.0585421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:24.317865: step 3992, loss 0.0855467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:24.628143: step 3993, loss 0.178726, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:24.868840: step 3994, loss 0.214912, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:25.160027: step 3995, loss 0.144532, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:25.424324: step 3996, loss 0.17855, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:25.718731: step 3997, loss 0.121807, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:26.032952: step 3998, loss 0.136472, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:26.389250: step 3999, loss 0.171877, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:26.719802: step 4000, loss 0.0494227, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:27.183938: step 4000, loss 0.229811, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4000

2017-10-10T15:12:28.248341: step 4001, loss 0.109237, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:28.526314: step 4002, loss 0.123073, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:28.844091: step 4003, loss 0.111329, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:29.120918: step 4004, loss 0.0525424, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:29.448851: step 4005, loss 0.0905197, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:29.756849: step 4006, loss 0.0780521, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:30.003813: step 4007, loss 0.133128, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:30.299482: step 4008, loss 0.127729, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:30.611223: step 4009, loss 0.0814371, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:30.885084: step 4010, loss 0.15751, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:31.272909: step 4011, loss 0.161067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:31.522494: step 4012, loss 0.0953437, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:31.856907: step 4013, loss 0.0953197, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:32.196846: step 4014, loss 0.181768, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:32.422728: step 4015, loss 0.196603, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:32.690916: step 4016, loss 0.119078, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:32.943228: step 4017, loss 0.130431, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:33.228916: step 4018, loss 0.118667, acc 0.941176, learning_rate 0.0001
2017-10-10T15:12:33.526184: step 4019, loss 0.070643, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:33.887126: step 4020, loss 0.0677382, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:34.159139: step 4021, loss 0.180539, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:34.433958: step 4022, loss 0.261285, acc 0.890625, learning_rate 0.0001
2017-10-10T15:12:34.755119: step 4023, loss 0.0192092, acc 1, learning_rate 0.0001
2017-10-10T15:12:35.061115: step 4024, loss 0.134292, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:35.427096: step 4025, loss 0.0925341, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:35.691541: step 4026, loss 0.125393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:35.979705: step 4027, loss 0.0462744, acc 1, learning_rate 0.0001
2017-10-10T15:12:36.251455: step 4028, loss 0.127252, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:36.652986: step 4029, loss 0.099468, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:36.961079: step 4030, loss 0.0675614, acc 1, learning_rate 0.0001
2017-10-10T15:12:37.240828: step 4031, loss 0.0539869, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:37.564874: step 4032, loss 0.0601376, acc 1, learning_rate 0.0001
2017-10-10T15:12:37.872917: step 4033, loss 0.105151, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:38.189014: step 4034, loss 0.0734074, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:38.465298: step 4035, loss 0.152352, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:38.812257: step 4036, loss 0.0779687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:39.277102: step 4037, loss 0.109474, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:39.510933: step 4038, loss 0.0779591, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:39.765081: step 4039, loss 0.137459, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:40.003420: step 4040, loss 0.183278, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:40.545678: step 4040, loss 0.229868, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4040

2017-10-10T15:12:41.758476: step 4041, loss 0.0980038, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:42.096895: step 4042, loss 0.0984088, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:42.409796: step 4043, loss 0.160585, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:42.741041: step 4044, loss 0.0829584, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:43.028937: step 4045, loss 0.103645, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:43.374684: step 4046, loss 0.20798, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:43.648856: step 4047, loss 0.16282, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:44.005687: step 4048, loss 0.160892, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:44.335734: step 4049, loss 0.0476711, acc 1, learning_rate 0.0001
2017-10-10T15:12:44.657656: step 4050, loss 0.181995, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:44.817093: step 4051, loss 0.0721265, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:45.075126: step 4052, loss 0.173586, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:45.299618: step 4053, loss 0.0835376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:45.539711: step 4054, loss 0.0477636, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:45.821251: step 4055, loss 0.12378, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:46.084868: step 4056, loss 0.0532582, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:46.398160: step 4057, loss 0.0763796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:46.696874: step 4058, loss 0.0763989, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:47.012933: step 4059, loss 0.123287, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:47.309328: step 4060, loss 0.0887597, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:47.612459: step 4061, loss 0.120882, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:47.952917: step 4062, loss 0.139059, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:48.233396: step 4063, loss 0.0612129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:48.528778: step 4064, loss 0.151131, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:48.828339: step 4065, loss 0.0648612, acc 1, learning_rate 0.0001
2017-10-10T15:12:49.105086: step 4066, loss 0.106045, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:49.388574: step 4067, loss 0.122747, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:49.715564: step 4068, loss 0.0527892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:50.049247: step 4069, loss 0.0896629, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:50.304544: step 4070, loss 0.185569, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:50.584990: step 4071, loss 0.180141, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:50.900824: step 4072, loss 0.0922474, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:51.185211: step 4073, loss 0.160169, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:51.396937: step 4074, loss 0.14617, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:51.751293: step 4075, loss 0.175268, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:52.015352: step 4076, loss 0.115765, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:52.269077: step 4077, loss 0.0678739, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:52.524572: step 4078, loss 0.124481, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:52.844943: step 4079, loss 0.145231, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:53.102899: step 4080, loss 0.135586, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:53.622224: step 4080, loss 0.229967, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4080

2017-10-10T15:12:54.908924: step 4081, loss 0.162404, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:55.240842: step 4082, loss 0.0407266, acc 1, learning_rate 0.0001
2017-10-10T15:12:55.546009: step 4083, loss 0.0680031, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:55.942886: step 4084, loss 0.0913379, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:56.237405: step 4085, loss 0.0952204, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:56.461525: step 4086, loss 0.041169, acc 1, learning_rate 0.0001
2017-10-10T15:12:56.740331: step 4087, loss 0.0910943, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:57.044842: step 4088, loss 0.072371, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:57.363240: step 4089, loss 0.057558, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:57.602358: step 4090, loss 0.17354, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:57.911710: step 4091, loss 0.103001, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:58.241147: step 4092, loss 0.061755, acc 1, learning_rate 0.0001
2017-10-10T15:12:58.545245: step 4093, loss 0.0884603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:58.875811: step 4094, loss 0.0521897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:59.217337: step 4095, loss 0.111133, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:59.497109: step 4096, loss 0.106273, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:59.794879: step 4097, loss 0.0407771, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:00.073029: step 4098, loss 0.188268, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:00.388950: step 4099, loss 0.0660272, acc 1, learning_rate 0.0001
2017-10-10T15:13:00.630439: step 4100, loss 0.0742555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:00.861066: step 4101, loss 0.145634, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:01.174038: step 4102, loss 0.101641, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:01.424744: step 4103, loss 0.0752708, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:01.736816: step 4104, loss 0.100234, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:02.044881: step 4105, loss 0.160749, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:02.324863: step 4106, loss 0.0960891, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:02.712992: step 4107, loss 0.0942184, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:03.038715: step 4108, loss 0.0879812, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:03.231621: step 4109, loss 0.110607, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:03.458500: step 4110, loss 0.1117, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:03.672841: step 4111, loss 0.147279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:03.880579: step 4112, loss 0.132235, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:04.121968: step 4113, loss 0.0925318, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:04.411374: step 4114, loss 0.160302, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:04.748967: step 4115, loss 0.141447, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:05.042770: step 4116, loss 0.101373, acc 0.960784, learning_rate 0.0001
2017-10-10T15:13:05.319110: step 4117, loss 0.107336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:05.670109: step 4118, loss 0.0489807, acc 1, learning_rate 0.0001
2017-10-10T15:13:05.983391: step 4119, loss 0.078873, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:06.314068: step 4120, loss 0.0578833, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:06.797897: step 4120, loss 0.228775, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4120

2017-10-10T15:13:07.813930: step 4121, loss 0.0855876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:08.160794: step 4122, loss 0.070095, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:08.468999: step 4123, loss 0.177837, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:08.810109: step 4124, loss 0.138968, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:09.079291: step 4125, loss 0.117017, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:09.332866: step 4126, loss 0.0749538, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:09.561006: step 4127, loss 0.117225, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:09.911352: step 4128, loss 0.218918, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:10.241279: step 4129, loss 0.14332, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:10.572892: step 4130, loss 0.121489, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:10.876899: step 4131, loss 0.084787, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:11.156841: step 4132, loss 0.0732293, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:11.521376: step 4133, loss 0.276606, acc 0.875, learning_rate 0.0001
2017-10-10T15:13:11.785088: step 4134, loss 0.094424, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:12.161019: step 4135, loss 0.126707, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:12.456904: step 4136, loss 0.0945922, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:12.799374: step 4137, loss 0.150091, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:13.016722: step 4138, loss 0.108205, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:13.260518: step 4139, loss 0.0997528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:13.572824: step 4140, loss 0.144488, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:13.868225: step 4141, loss 0.0912542, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:14.196936: step 4142, loss 0.140142, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:14.484299: step 4143, loss 0.0826862, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:14.768903: step 4144, loss 0.0599014, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:15.045509: step 4145, loss 0.14204, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:15.245572: step 4146, loss 0.104621, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:15.567221: step 4147, loss 0.083745, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:15.881065: step 4148, loss 0.132154, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:16.256869: step 4149, loss 0.073812, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:16.543988: step 4150, loss 0.0694313, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:16.833098: step 4151, loss 0.0452559, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:17.152553: step 4152, loss 0.113086, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:17.460965: step 4153, loss 0.0596181, acc 1, learning_rate 0.0001
2017-10-10T15:13:17.754341: step 4154, loss 0.0779484, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:18.080892: step 4155, loss 0.097716, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:18.372884: step 4156, loss 0.0571619, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:18.601077: step 4157, loss 0.0834619, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:18.952929: step 4158, loss 0.101718, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:19.243859: step 4159, loss 0.194152, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:19.597424: step 4160, loss 0.110645, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:20.071156: step 4160, loss 0.230108, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4160

2017-10-10T15:13:21.216195: step 4161, loss 0.0978627, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:21.548780: step 4162, loss 0.0852994, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:21.816895: step 4163, loss 0.0846153, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:22.001012: step 4164, loss 0.089208, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:22.249657: step 4165, loss 0.0887154, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:22.620662: step 4166, loss 0.11475, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:22.914941: step 4167, loss 0.0643924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:23.169112: step 4168, loss 0.110341, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:23.457654: step 4169, loss 0.134062, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:23.772500: step 4170, loss 0.0533311, acc 1, learning_rate 0.0001
2017-10-10T15:13:24.103307: step 4171, loss 0.0949157, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:24.384125: step 4172, loss 0.127748, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:24.655172: step 4173, loss 0.0735928, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:24.964679: step 4174, loss 0.092147, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:25.288552: step 4175, loss 0.095105, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:25.608018: step 4176, loss 0.247282, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:25.908587: step 4177, loss 0.0726524, acc 1, learning_rate 0.0001
2017-10-10T15:13:26.212992: step 4178, loss 0.161119, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:26.473013: step 4179, loss 0.151898, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:26.773062: step 4180, loss 0.145805, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:27.056966: step 4181, loss 0.104744, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:27.286592: step 4182, loss 0.10302, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:27.572339: step 4183, loss 0.0627813, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:27.938547: step 4184, loss 0.132665, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:28.191326: step 4185, loss 0.0660826, acc 1, learning_rate 0.0001
2017-10-10T15:13:28.409006: step 4186, loss 0.0956313, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:28.739289: step 4187, loss 0.105373, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:29.101327: step 4188, loss 0.086387, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:29.308374: step 4189, loss 0.0501694, acc 1, learning_rate 0.0001
2017-10-10T15:13:29.561300: step 4190, loss 0.281251, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:29.788195: step 4191, loss 0.0684771, acc 1, learning_rate 0.0001
2017-10-10T15:13:30.012220: step 4192, loss 0.220067, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:30.348982: step 4193, loss 0.137794, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:30.571285: step 4194, loss 0.203929, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:30.865627: step 4195, loss 0.0746361, acc 1, learning_rate 0.0001
2017-10-10T15:13:31.181354: step 4196, loss 0.0364656, acc 1, learning_rate 0.0001
2017-10-10T15:13:31.410790: step 4197, loss 0.168034, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:31.712940: step 4198, loss 0.09139, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:32.032858: step 4199, loss 0.165121, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:32.379198: step 4200, loss 0.090344, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:32.958920: step 4200, loss 0.22787, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4200

2017-10-10T15:13:34.176855: step 4201, loss 0.0400692, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:34.492909: step 4202, loss 0.160429, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:34.865346: step 4203, loss 0.103628, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:35.151071: step 4204, loss 0.0561839, acc 1, learning_rate 0.0001
2017-10-10T15:13:35.469772: step 4205, loss 0.341838, acc 0.875, learning_rate 0.0001
2017-10-10T15:13:35.792256: step 4206, loss 0.0763998, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:36.057141: step 4207, loss 0.126903, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:36.352978: step 4208, loss 0.0539444, acc 1, learning_rate 0.0001
2017-10-10T15:13:36.600982: step 4209, loss 0.0831337, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:36.991590: step 4210, loss 0.0766871, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:37.225819: step 4211, loss 0.0981747, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:37.510100: step 4212, loss 0.0494955, acc 1, learning_rate 0.0001
2017-10-10T15:13:37.765086: step 4213, loss 0.0806367, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:38.068781: step 4214, loss 0.115536, acc 0.941176, learning_rate 0.0001
2017-10-10T15:13:38.348921: step 4215, loss 0.104029, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:38.744557: step 4216, loss 0.0710019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:39.044159: step 4217, loss 0.107308, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:39.348229: step 4218, loss 0.0889052, acc 1, learning_rate 0.0001
2017-10-10T15:13:39.593043: step 4219, loss 0.173234, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:39.797240: step 4220, loss 0.0773512, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:40.084903: step 4221, loss 0.120721, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:40.391295: step 4222, loss 0.0354217, acc 1, learning_rate 0.0001
2017-10-10T15:13:40.695984: step 4223, loss 0.0535081, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:40.985728: step 4224, loss 0.199077, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:41.311328: step 4225, loss 0.175226, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:41.619599: step 4226, loss 0.113304, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:41.928906: step 4227, loss 0.0888475, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:42.203654: step 4228, loss 0.221599, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:42.475774: step 4229, loss 0.110925, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:42.833028: step 4230, loss 0.114069, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:43.138900: step 4231, loss 0.098479, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:43.494005: step 4232, loss 0.116117, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:43.813599: step 4233, loss 0.0717281, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:44.016003: step 4234, loss 0.029795, acc 1, learning_rate 0.0001
2017-10-10T15:13:44.348960: step 4235, loss 0.0342224, acc 1, learning_rate 0.0001
2017-10-10T15:13:44.701334: step 4236, loss 0.0803151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:44.965334: step 4237, loss 0.0663891, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:45.325785: step 4238, loss 0.102825, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:45.664989: step 4239, loss 0.0841124, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:45.986472: step 4240, loss 0.142116, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:46.426775: step 4240, loss 0.227091, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4240

2017-10-10T15:13:47.351148: step 4241, loss 0.118265, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:47.737609: step 4242, loss 0.0905004, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:48.034660: step 4243, loss 0.166224, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:48.272389: step 4244, loss 0.143374, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:48.548405: step 4245, loss 0.138579, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:48.870545: step 4246, loss 0.233451, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:49.105421: step 4247, loss 0.110542, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:49.397325: step 4248, loss 0.106914, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:49.744341: step 4249, loss 0.176505, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:50.067411: step 4250, loss 0.0753643, acc 1, learning_rate 0.0001
2017-10-10T15:13:50.384945: step 4251, loss 0.118913, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:50.697080: step 4252, loss 0.0253303, acc 1, learning_rate 0.0001
2017-10-10T15:13:50.982554: step 4253, loss 0.134895, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:51.267121: step 4254, loss 0.167805, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:51.562809: step 4255, loss 0.0928788, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:51.886086: step 4256, loss 0.115656, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:52.168961: step 4257, loss 0.111179, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:52.401595: step 4258, loss 0.126319, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:52.744960: step 4259, loss 0.114861, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:53.072982: step 4260, loss 0.169686, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:53.407473: step 4261, loss 0.0454268, acc 1, learning_rate 0.0001
2017-10-10T15:13:53.688843: step 4262, loss 0.139634, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:54.033042: step 4263, loss 0.125345, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:54.338562: step 4264, loss 0.144031, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:54.601052: step 4265, loss 0.0491378, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:54.912812: step 4266, loss 0.0853512, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:55.255663: step 4267, loss 0.153963, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:55.585628: step 4268, loss 0.0623486, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:55.896874: step 4269, loss 0.0925485, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:56.223832: step 4270, loss 0.103375, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:56.520825: step 4271, loss 0.122574, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:56.850768: step 4272, loss 0.100707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:57.234642: step 4273, loss 0.150112, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:57.600947: step 4274, loss 0.0899143, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:57.870547: step 4275, loss 0.0961251, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:58.180872: step 4276, loss 0.0445946, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:58.555204: step 4277, loss 0.151909, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:58.853195: step 4278, loss 0.0655932, acc 1, learning_rate 0.0001
2017-10-10T15:13:59.160998: step 4279, loss 0.0960042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:59.504391: step 4280, loss 0.183027, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:00.034777: step 4280, loss 0.230579, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4280

2017-10-10T15:14:01.147630: step 4281, loss 0.145508, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:01.409625: step 4282, loss 0.0652975, acc 1, learning_rate 0.0001
2017-10-10T15:14:01.720818: step 4283, loss 0.0826931, acc 1, learning_rate 0.0001
2017-10-10T15:14:02.076963: step 4284, loss 0.132105, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:02.386844: step 4285, loss 0.0865539, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:02.628963: step 4286, loss 0.058208, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:02.904839: step 4287, loss 0.0785132, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:03.180896: step 4288, loss 0.156786, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:03.529076: step 4289, loss 0.0967373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:03.837016: step 4290, loss 0.102616, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:04.137236: step 4291, loss 0.125474, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:04.441059: step 4292, loss 0.0587218, acc 1, learning_rate 0.0001
2017-10-10T15:14:04.789176: step 4293, loss 0.088792, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:05.072890: step 4294, loss 0.135406, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:05.382504: step 4295, loss 0.136451, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:05.673403: step 4296, loss 0.0621954, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:06.011147: step 4297, loss 0.0430476, acc 1, learning_rate 0.0001
2017-10-10T15:14:06.273987: step 4298, loss 0.0762418, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:06.583412: step 4299, loss 0.123382, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:06.871416: step 4300, loss 0.0956631, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:07.244180: step 4301, loss 0.101836, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:07.593035: step 4302, loss 0.112035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:07.908906: step 4303, loss 0.0999432, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:08.179990: step 4304, loss 0.169575, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:08.528838: step 4305, loss 0.0570535, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.846365: step 4306, loss 0.122958, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:09.132568: step 4307, loss 0.175069, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:09.439806: step 4308, loss 0.0698488, acc 1, learning_rate 0.0001
2017-10-10T15:14:09.764245: step 4309, loss 0.123781, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:10.069032: step 4310, loss 0.056905, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:10.325267: step 4311, loss 0.0748745, acc 1, learning_rate 0.0001
2017-10-10T15:14:10.625445: step 4312, loss 0.128032, acc 0.941176, learning_rate 0.0001
2017-10-10T15:14:10.912847: step 4313, loss 0.0697537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:11.141099: step 4314, loss 0.124154, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:11.429999: step 4315, loss 0.154347, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:11.732769: step 4316, loss 0.0444298, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:12.036903: step 4317, loss 0.0818403, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:12.348794: step 4318, loss 0.106959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:12.620895: step 4319, loss 0.0873298, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:12.858602: step 4320, loss 0.123634, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:13.421844: step 4320, loss 0.228636, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4320

2017-10-10T15:14:14.577179: step 4321, loss 0.14318, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:14.879149: step 4322, loss 0.0815941, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:15.182215: step 4323, loss 0.168045, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:15.478147: step 4324, loss 0.243942, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:15.801003: step 4325, loss 0.146096, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:15.986617: step 4326, loss 0.0878591, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:16.243957: step 4327, loss 0.067693, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:16.492487: step 4328, loss 0.0486143, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:16.805075: step 4329, loss 0.074004, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:17.198015: step 4330, loss 0.0840368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:17.521955: step 4331, loss 0.130137, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:17.865220: step 4332, loss 0.0838525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:18.136850: step 4333, loss 0.102868, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:18.492906: step 4334, loss 0.105189, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:18.812908: step 4335, loss 0.0869955, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:19.137715: step 4336, loss 0.133381, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:19.332983: step 4337, loss 0.108742, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:19.583924: step 4338, loss 0.231013, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:19.802122: step 4339, loss 0.0456394, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:20.111411: step 4340, loss 0.0924746, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:20.440899: step 4341, loss 0.130821, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:20.739803: step 4342, loss 0.0984457, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:21.070664: step 4343, loss 0.040966, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:21.391053: step 4344, loss 0.142455, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:21.650005: step 4345, loss 0.0567368, acc 1, learning_rate 0.0001
2017-10-10T15:14:21.932913: step 4346, loss 0.133391, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:22.225017: step 4347, loss 0.0821178, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:22.460887: step 4348, loss 0.181668, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:22.702177: step 4349, loss 0.142708, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:22.976820: step 4350, loss 0.152872, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:23.308281: step 4351, loss 0.128771, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:23.628936: step 4352, loss 0.095171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:23.920863: step 4353, loss 0.145427, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:24.248959: step 4354, loss 0.0635196, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:24.540140: step 4355, loss 0.128667, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:24.805183: step 4356, loss 0.111827, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:25.101158: step 4357, loss 0.100037, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:25.442998: step 4358, loss 0.0813051, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:25.698514: step 4359, loss 0.13109, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:26.040557: step 4360, loss 0.0949897, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:26.609855: step 4360, loss 0.226501, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4360

2017-10-10T15:14:27.730896: step 4361, loss 0.226549, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:28.064137: step 4362, loss 0.124892, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:28.412961: step 4363, loss 0.115029, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:28.695361: step 4364, loss 0.0758301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:28.967826: step 4365, loss 0.108361, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:29.333184: step 4366, loss 0.114263, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:29.624777: step 4367, loss 0.0627539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:29.891243: step 4368, loss 0.140624, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:30.203851: step 4369, loss 0.0402583, acc 1, learning_rate 0.0001
2017-10-10T15:14:30.507693: step 4370, loss 0.200577, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:30.796162: step 4371, loss 0.0526553, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:31.050735: step 4372, loss 0.121311, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:31.390361: step 4373, loss 0.073087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:31.681349: step 4374, loss 0.0458376, acc 1, learning_rate 0.0001
2017-10-10T15:14:32.001182: step 4375, loss 0.147579, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:32.337011: step 4376, loss 0.100984, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:32.611237: step 4377, loss 0.0505954, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:32.942794: step 4378, loss 0.0774567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:33.219069: step 4379, loss 0.0890647, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:33.519923: step 4380, loss 0.0580193, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:33.842078: step 4381, loss 0.118577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:34.197134: step 4382, loss 0.119987, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:34.518421: step 4383, loss 0.0553361, acc 1, learning_rate 0.0001
2017-10-10T15:14:34.760836: step 4384, loss 0.0417576, acc 1, learning_rate 0.0001
2017-10-10T15:14:34.989880: step 4385, loss 0.0573356, acc 1, learning_rate 0.0001
2017-10-10T15:14:35.240041: step 4386, loss 0.0714177, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:35.565138: step 4387, loss 0.192376, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:35.823596: step 4388, loss 0.187346, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:36.097200: step 4389, loss 0.0322728, acc 1, learning_rate 0.0001
2017-10-10T15:14:36.374436: step 4390, loss 0.0999353, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:36.640812: step 4391, loss 0.0877671, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:36.893576: step 4392, loss 0.0945063, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:37.208876: step 4393, loss 0.1563, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:37.537682: step 4394, loss 0.126159, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:37.841073: step 4395, loss 0.144277, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:38.143431: step 4396, loss 0.102531, acc 1, learning_rate 0.0001
2017-10-10T15:14:38.459197: step 4397, loss 0.0618927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:38.763139: step 4398, loss 0.133574, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:39.055101: step 4399, loss 0.101382, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:39.399726: step 4400, loss 0.133365, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:39.887193: step 4400, loss 0.228648, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4400

2017-10-10T15:14:40.829108: step 4401, loss 0.0667937, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.081893: step 4402, loss 0.099041, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:41.428327: step 4403, loss 0.0697376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.734083: step 4404, loss 0.0547017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:42.017039: step 4405, loss 0.0573313, acc 1, learning_rate 0.0001
2017-10-10T15:14:42.279169: step 4406, loss 0.0798112, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:42.596843: step 4407, loss 0.0709694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:42.909737: step 4408, loss 0.101829, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:43.149782: step 4409, loss 0.0634868, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:43.412747: step 4410, loss 0.240248, acc 0.941176, learning_rate 0.0001
2017-10-10T15:14:43.724951: step 4411, loss 0.170927, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:44.061052: step 4412, loss 0.117959, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:44.386134: step 4413, loss 0.159344, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:44.663572: step 4414, loss 0.17703, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:44.915380: step 4415, loss 0.186944, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:45.184637: step 4416, loss 0.0680266, acc 1, learning_rate 0.0001
2017-10-10T15:14:45.464626: step 4417, loss 0.0614527, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:45.784818: step 4418, loss 0.113124, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:46.079841: step 4419, loss 0.116815, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:46.319461: step 4420, loss 0.0703658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:46.662472: step 4421, loss 0.0881421, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:46.976870: step 4422, loss 0.0571928, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:47.233004: step 4423, loss 0.129587, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:47.509033: step 4424, loss 0.0616069, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:47.804518: step 4425, loss 0.0340466, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:48.137540: step 4426, loss 0.0683307, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:48.465410: step 4427, loss 0.0482807, acc 1, learning_rate 0.0001
2017-10-10T15:14:48.696540: step 4428, loss 0.141086, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:49.008927: step 4429, loss 0.144884, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:49.314341: step 4430, loss 0.127083, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:49.604428: step 4431, loss 0.0752101, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:49.924948: step 4432, loss 0.0847138, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:50.197033: step 4433, loss 0.088631, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:50.530119: step 4434, loss 0.118203, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:50.860879: step 4435, loss 0.108654, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:51.171665: step 4436, loss 0.10451, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:51.469080: step 4437, loss 0.0708574, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:51.833061: step 4438, loss 0.0386184, acc 1, learning_rate 0.0001
2017-10-10T15:14:52.245684: step 4439, loss 0.141874, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:52.570748: step 4440, loss 0.16801, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:52.948078: step 4440, loss 0.229663, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4440

2017-10-10T15:14:53.931134: step 4441, loss 0.0462918, acc 1, learning_rate 0.0001
2017-10-10T15:14:54.208854: step 4442, loss 0.0920864, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:54.472870: step 4443, loss 0.0467166, acc 1, learning_rate 0.0001
2017-10-10T15:14:54.771917: step 4444, loss 0.151373, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:55.136862: step 4445, loss 0.129781, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:55.444801: step 4446, loss 0.093024, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:55.798122: step 4447, loss 0.194095, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:56.111130: step 4448, loss 0.152296, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:56.429424: step 4449, loss 0.109467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:56.796940: step 4450, loss 0.0407803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:57.086179: step 4451, loss 0.090083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:57.335724: step 4452, loss 0.0622575, acc 1, learning_rate 0.0001
2017-10-10T15:14:57.680926: step 4453, loss 0.0792157, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:57.949584: step 4454, loss 0.128953, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:58.269414: step 4455, loss 0.0808784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:58.667157: step 4456, loss 0.0877865, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:58.971292: step 4457, loss 0.181042, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:59.227815: step 4458, loss 0.0759615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:59.537282: step 4459, loss 0.086335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:59.830141: step 4460, loss 0.0386694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:00.150010: step 4461, loss 0.142126, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:00.453385: step 4462, loss 0.0895932, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:00.732843: step 4463, loss 0.0572555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:01.093003: step 4464, loss 0.115396, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:01.384532: step 4465, loss 0.101355, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:01.689062: step 4466, loss 0.0946749, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:01.962753: step 4467, loss 0.0535385, acc 1, learning_rate 0.0001
2017-10-10T15:15:02.268991: step 4468, loss 0.103932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:02.574974: step 4469, loss 0.0351536, acc 1, learning_rate 0.0001
2017-10-10T15:15:02.805184: step 4470, loss 0.0890476, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:03.072694: step 4471, loss 0.083303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:03.385027: step 4472, loss 0.100152, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:03.668743: step 4473, loss 0.0580348, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:03.999952: step 4474, loss 0.176018, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:04.316984: step 4475, loss 0.0996833, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:04.651594: step 4476, loss 0.107514, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:04.891174: step 4477, loss 0.0725159, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:05.224175: step 4478, loss 0.131113, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:05.519706: step 4479, loss 0.0727615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:05.843047: step 4480, loss 0.0658742, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:06.347432: step 4480, loss 0.228411, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4480

2017-10-10T15:15:07.483394: step 4481, loss 0.0987807, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:07.776861: step 4482, loss 0.116793, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:08.101125: step 4483, loss 0.0702248, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:08.394055: step 4484, loss 0.178244, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:08.716852: step 4485, loss 0.0720697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:09.039183: step 4486, loss 0.104682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:09.324815: step 4487, loss 0.141521, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:09.591711: step 4488, loss 0.0788153, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:09.842552: step 4489, loss 0.114623, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:10.119541: step 4490, loss 0.0657323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:10.424477: step 4491, loss 0.16228, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:10.642703: step 4492, loss 0.0630579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:10.898687: step 4493, loss 0.0817154, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:11.252324: step 4494, loss 0.105646, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:11.524790: step 4495, loss 0.0977124, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:11.845831: step 4496, loss 0.0842411, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:12.125727: step 4497, loss 0.126959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:12.425946: step 4498, loss 0.0999284, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:12.746943: step 4499, loss 0.0319641, acc 1, learning_rate 0.0001
2017-10-10T15:15:13.044775: step 4500, loss 0.0357093, acc 1, learning_rate 0.0001
2017-10-10T15:15:13.338383: step 4501, loss 0.137857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:13.622818: step 4502, loss 0.156285, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:13.893929: step 4503, loss 0.140067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.225010: step 4504, loss 0.0655808, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.549544: step 4505, loss 0.0665096, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.865635: step 4506, loss 0.202476, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:15.133660: step 4507, loss 0.128665, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:15.389730: step 4508, loss 0.0403223, acc 1, learning_rate 0.0001
2017-10-10T15:15:15.730868: step 4509, loss 0.0965334, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:16.065173: step 4510, loss 0.0715024, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:16.404812: step 4511, loss 0.0688029, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:16.735739: step 4512, loss 0.0884356, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:17.068414: step 4513, loss 0.0956459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:17.388960: step 4514, loss 0.0665996, acc 1, learning_rate 0.0001
2017-10-10T15:15:17.678419: step 4515, loss 0.041359, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:18.025209: step 4516, loss 0.13609, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:18.296879: step 4517, loss 0.22859, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:18.555225: step 4518, loss 0.144144, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:18.824859: step 4519, loss 0.068732, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:19.126436: step 4520, loss 0.088923, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:19.633083: step 4520, loss 0.229346, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4520

2017-10-10T15:15:20.574687: step 4521, loss 0.0995197, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:20.842994: step 4522, loss 0.124704, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:21.122794: step 4523, loss 0.130601, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:21.483017: step 4524, loss 0.0250314, acc 1, learning_rate 0.0001
2017-10-10T15:15:21.781026: step 4525, loss 0.104119, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:22.050507: step 4526, loss 0.20574, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:22.332826: step 4527, loss 0.122761, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:22.634252: step 4528, loss 0.042718, acc 1, learning_rate 0.0001
2017-10-10T15:15:22.968871: step 4529, loss 0.161531, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:23.325806: step 4530, loss 0.185769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:23.646553: step 4531, loss 0.143737, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:23.930741: step 4532, loss 0.115973, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:24.167897: step 4533, loss 0.122902, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:24.420879: step 4534, loss 0.160393, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:24.735838: step 4535, loss 0.110126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:25.058178: step 4536, loss 0.109204, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:25.363226: step 4537, loss 0.108944, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:25.661027: step 4538, loss 0.0786311, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:26.005616: step 4539, loss 0.173098, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:26.244824: step 4540, loss 0.164836, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:26.501248: step 4541, loss 0.0683484, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:26.737721: step 4542, loss 0.105606, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:27.107566: step 4543, loss 0.0376146, acc 1, learning_rate 0.0001
2017-10-10T15:15:27.424056: step 4544, loss 0.0718903, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:27.813680: step 4545, loss 0.13078, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:28.073515: step 4546, loss 0.107211, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:28.336804: step 4547, loss 0.130854, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:28.625129: step 4548, loss 0.145027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:28.836613: step 4549, loss 0.146891, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:29.223693: step 4550, loss 0.0588421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:29.508351: step 4551, loss 0.107039, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:29.845480: step 4552, loss 0.136008, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:30.095542: step 4553, loss 0.120864, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:30.420744: step 4554, loss 0.0879197, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:30.740975: step 4555, loss 0.167406, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:31.032451: step 4556, loss 0.0891499, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:31.345601: step 4557, loss 0.172415, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:31.628807: step 4558, loss 0.0993108, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:31.915956: step 4559, loss 0.113832, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:32.204877: step 4560, loss 0.193304, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:32.728738: step 4560, loss 0.228696, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4560

2017-10-10T15:15:33.943330: step 4561, loss 0.0714455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:34.250951: step 4562, loss 0.137576, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:34.553588: step 4563, loss 0.101199, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:34.866070: step 4564, loss 0.0914206, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:35.136966: step 4565, loss 0.112447, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:35.422146: step 4566, loss 0.0670917, acc 1, learning_rate 0.0001
2017-10-10T15:15:35.743740: step 4567, loss 0.0451625, acc 1, learning_rate 0.0001
2017-10-10T15:15:36.065022: step 4568, loss 0.053436, acc 1, learning_rate 0.0001
2017-10-10T15:15:36.301211: step 4569, loss 0.132823, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:36.558637: step 4570, loss 0.0910295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:36.831597: step 4571, loss 0.182258, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:37.151580: step 4572, loss 0.0546848, acc 1, learning_rate 0.0001
2017-10-10T15:15:37.425590: step 4573, loss 0.0570813, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:37.756916: step 4574, loss 0.109472, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:38.024946: step 4575, loss 0.115965, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:38.359132: step 4576, loss 0.184991, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:38.634600: step 4577, loss 0.0967173, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:38.868986: step 4578, loss 0.106817, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:39.158708: step 4579, loss 0.0834298, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:39.420879: step 4580, loss 0.0915535, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:39.708575: step 4581, loss 0.0815905, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:40.048947: step 4582, loss 0.0981298, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:40.360405: step 4583, loss 0.0992781, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:40.581048: step 4584, loss 0.0567987, acc 1, learning_rate 0.0001
2017-10-10T15:15:40.838599: step 4585, loss 0.0974539, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:41.085063: step 4586, loss 0.105678, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:41.355255: step 4587, loss 0.139171, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:41.687482: step 4588, loss 0.137094, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:42.029035: step 4589, loss 0.17097, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:42.345042: step 4590, loss 0.0797855, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:42.631925: step 4591, loss 0.0859454, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:42.914267: step 4592, loss 0.0843068, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:43.146119: step 4593, loss 0.0691459, acc 1, learning_rate 0.0001
2017-10-10T15:15:43.373911: step 4594, loss 0.0776897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:43.726476: step 4595, loss 0.138133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:44.012857: step 4596, loss 0.164062, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:44.373390: step 4597, loss 0.0676329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:44.676866: step 4598, loss 0.0799815, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:45.028879: step 4599, loss 0.15209, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:45.336489: step 4600, loss 0.0837527, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:45.868479: step 4600, loss 0.229274, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4600

2017-10-10T15:15:47.017445: step 4601, loss 0.142716, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:47.284855: step 4602, loss 0.0504886, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:47.578662: step 4603, loss 0.0750383, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:47.849004: step 4604, loss 0.088023, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:48.185090: step 4605, loss 0.0960063, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:48.484253: step 4606, loss 0.0601965, acc 0.980392, learning_rate 0.0001
2017-10-10T15:15:48.732870: step 4607, loss 0.0841458, acc 1, learning_rate 0.0001
2017-10-10T15:15:48.984990: step 4608, loss 0.0911956, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:49.295090: step 4609, loss 0.130206, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:49.573392: step 4610, loss 0.0856036, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:49.892838: step 4611, loss 0.161404, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:50.214402: step 4612, loss 0.0980648, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:50.460926: step 4613, loss 0.0333162, acc 1, learning_rate 0.0001
2017-10-10T15:15:50.772490: step 4614, loss 0.0658062, acc 1, learning_rate 0.0001
2017-10-10T15:15:51.094818: step 4615, loss 0.103004, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:51.407188: step 4616, loss 0.159361, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:51.668863: step 4617, loss 0.0897215, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:51.892984: step 4618, loss 0.166078, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:52.206641: step 4619, loss 0.121738, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:52.485022: step 4620, loss 0.0803588, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:52.794440: step 4621, loss 0.111896, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:53.105130: step 4622, loss 0.111839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:53.420466: step 4623, loss 0.0987077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:53.745013: step 4624, loss 0.0869213, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:54.038832: step 4625, loss 0.0734786, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:54.368874: step 4626, loss 0.192413, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:54.696845: step 4627, loss 0.0693224, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:54.969077: step 4628, loss 0.0869085, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:55.280326: step 4629, loss 0.142432, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:55.574503: step 4630, loss 0.113956, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:55.880887: step 4631, loss 0.131552, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:56.240940: step 4632, loss 0.125446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:56.511178: step 4633, loss 0.188689, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:56.786526: step 4634, loss 0.086362, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:57.140092: step 4635, loss 0.0338914, acc 1, learning_rate 0.0001
2017-10-10T15:15:57.422876: step 4636, loss 0.0623149, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:57.711627: step 4637, loss 0.0818633, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:57.992666: step 4638, loss 0.142877, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:58.320838: step 4639, loss 0.0665079, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:58.698707: step 4640, loss 0.0758258, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:59.178680: step 4640, loss 0.228195, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4640

2017-10-10T15:16:00.269143: step 4641, loss 0.196799, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:00.540459: step 4642, loss 0.0659144, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:00.792831: step 4643, loss 0.146349, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:01.118344: step 4644, loss 0.0306458, acc 1, learning_rate 0.0001
2017-10-10T15:16:01.421558: step 4645, loss 0.100714, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:01.677677: step 4646, loss 0.10405, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:01.963107: step 4647, loss 0.0625394, acc 1, learning_rate 0.0001
2017-10-10T15:16:02.301182: step 4648, loss 0.0939408, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:02.563764: step 4649, loss 0.0552436, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:02.798920: step 4650, loss 0.124087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:03.040945: step 4651, loss 0.112671, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:03.329188: step 4652, loss 0.11016, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:03.668992: step 4653, loss 0.0899875, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:04.026912: step 4654, loss 0.0901697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:04.373960: step 4655, loss 0.0456816, acc 1, learning_rate 0.0001
2017-10-10T15:16:04.700594: step 4656, loss 0.0566486, acc 1, learning_rate 0.0001
2017-10-10T15:16:04.932928: step 4657, loss 0.139913, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:05.210807: step 4658, loss 0.0844211, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:05.472821: step 4659, loss 0.216233, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:05.816860: step 4660, loss 0.0199421, acc 1, learning_rate 0.0001
2017-10-10T15:16:06.160962: step 4661, loss 0.113632, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:06.400858: step 4662, loss 0.0910872, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:06.716403: step 4663, loss 0.0957442, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:07.041547: step 4664, loss 0.224213, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:07.316943: step 4665, loss 0.0975147, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:07.630357: step 4666, loss 0.0920034, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:07.988105: step 4667, loss 0.165168, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:08.365288: step 4668, loss 0.173006, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:08.663444: step 4669, loss 0.11215, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:08.976567: step 4670, loss 0.0830269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:09.294560: step 4671, loss 0.126451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:09.580862: step 4672, loss 0.093883, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:09.904875: step 4673, loss 0.121307, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:10.197081: step 4674, loss 0.0928799, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:10.535116: step 4675, loss 0.120959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:10.844558: step 4676, loss 0.0666481, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:11.152992: step 4677, loss 0.0758767, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:11.441169: step 4678, loss 0.127327, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:11.800982: step 4679, loss 0.124903, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:12.144850: step 4680, loss 0.105146, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:12.613205: step 4680, loss 0.228583, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4680

2017-10-10T15:16:13.493174: step 4681, loss 0.0997673, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:13.763944: step 4682, loss 0.130756, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:14.047276: step 4683, loss 0.0808612, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:14.363715: step 4684, loss 0.0814318, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:14.657133: step 4685, loss 0.0732258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:14.940161: step 4686, loss 0.0454834, acc 1, learning_rate 0.0001
2017-10-10T15:16:15.248973: step 4687, loss 0.16076, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:15.677868: step 4688, loss 0.0454434, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:15.956929: step 4689, loss 0.187288, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:16.261009: step 4690, loss 0.0735285, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:16.508092: step 4691, loss 0.0421204, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:16.777117: step 4692, loss 0.138439, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:17.137751: step 4693, loss 0.124259, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:17.439788: step 4694, loss 0.104009, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:17.724911: step 4695, loss 0.107975, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:18.072865: step 4696, loss 0.0902968, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:18.392954: step 4697, loss 0.0918481, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:18.720841: step 4698, loss 0.316828, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:18.996875: step 4699, loss 0.103591, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:19.281271: step 4700, loss 0.107195, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:19.596562: step 4701, loss 0.0217077, acc 1, learning_rate 0.0001
2017-10-10T15:16:19.886955: step 4702, loss 0.109971, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:20.233108: step 4703, loss 0.0922458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:20.484955: step 4704, loss 0.114069, acc 0.941176, learning_rate 0.0001
2017-10-10T15:16:20.869027: step 4705, loss 0.145976, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:21.170471: step 4706, loss 0.0770604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:21.486644: step 4707, loss 0.0659582, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:21.801980: step 4708, loss 0.108598, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:22.096862: step 4709, loss 0.172924, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:22.450930: step 4710, loss 0.116669, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:22.846927: step 4711, loss 0.123159, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:23.131479: step 4712, loss 0.0703101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:23.402297: step 4713, loss 0.0927877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:23.656884: step 4714, loss 0.141877, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:23.887528: step 4715, loss 0.0722213, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:24.203556: step 4716, loss 0.146594, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:24.536863: step 4717, loss 0.126127, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:24.862681: step 4718, loss 0.0781442, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:25.090103: step 4719, loss 0.194795, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:25.358667: step 4720, loss 0.0720879, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:25.945082: step 4720, loss 0.228079, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4720

2017-10-10T15:16:27.074678: step 4721, loss 0.0782816, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:27.374677: step 4722, loss 0.0468707, acc 1, learning_rate 0.0001
2017-10-10T15:16:27.648892: step 4723, loss 0.0684186, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:27.910547: step 4724, loss 0.169066, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:28.289155: step 4725, loss 0.0829059, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:28.632882: step 4726, loss 0.0637428, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:28.936855: step 4727, loss 0.0821693, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:29.184975: step 4728, loss 0.0629208, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:29.505291: step 4729, loss 0.0955697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:29.805502: step 4730, loss 0.126465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:30.109270: step 4731, loss 0.0680211, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:30.380770: step 4732, loss 0.0651377, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:30.669020: step 4733, loss 0.107529, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:30.974553: step 4734, loss 0.0705714, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:31.264899: step 4735, loss 0.189614, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:31.620847: step 4736, loss 0.0629626, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:31.944479: step 4737, loss 0.107304, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:32.248819: step 4738, loss 0.0977361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:32.604170: step 4739, loss 0.0530795, acc 1, learning_rate 0.0001
2017-10-10T15:16:32.871846: step 4740, loss 0.0644484, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:33.100077: step 4741, loss 0.0531933, acc 1, learning_rate 0.0001
2017-10-10T15:16:33.333427: step 4742, loss 0.124826, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:33.660695: step 4743, loss 0.0710727, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:33.936146: step 4744, loss 0.111052, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:34.313018: step 4745, loss 0.0734766, acc 1, learning_rate 0.0001
2017-10-10T15:16:34.545037: step 4746, loss 0.179642, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:34.892891: step 4747, loss 0.0744615, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:35.212730: step 4748, loss 0.0407861, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:35.488687: step 4749, loss 0.121708, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:35.740223: step 4750, loss 0.115116, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:35.992986: step 4751, loss 0.0554379, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:36.250651: step 4752, loss 0.122451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:36.602575: step 4753, loss 0.0538807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:36.963712: step 4754, loss 0.203398, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:37.261165: step 4755, loss 0.10914, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:37.539618: step 4756, loss 0.158233, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:37.847729: step 4757, loss 0.0970899, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:38.169190: step 4758, loss 0.0671854, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:38.396955: step 4759, loss 0.203809, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:38.708046: step 4760, loss 0.132798, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:39.215639: step 4760, loss 0.227471, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4760

2017-10-10T15:16:40.479331: step 4761, loss 0.083948, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:40.784609: step 4762, loss 0.0903532, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:41.108960: step 4763, loss 0.152276, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:41.398861: step 4764, loss 0.147745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:41.733172: step 4765, loss 0.0961374, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:41.981001: step 4766, loss 0.0377097, acc 1, learning_rate 0.0001
2017-10-10T15:16:42.617071: step 4767, loss 0.156663, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:42.876889: step 4768, loss 0.103584, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:43.134265: step 4769, loss 0.151022, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:43.432294: step 4770, loss 0.0974229, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:43.748221: step 4771, loss 0.163491, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:44.064664: step 4772, loss 0.097161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:44.374383: step 4773, loss 0.1264, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:44.712760: step 4774, loss 0.128107, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:44.964955: step 4775, loss 0.108372, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:45.279566: step 4776, loss 0.0790845, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:45.619908: step 4777, loss 0.149433, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:45.907177: step 4778, loss 0.0852114, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:46.237161: step 4779, loss 0.169293, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:46.566692: step 4780, loss 0.11997, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:46.904819: step 4781, loss 0.0590879, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:47.158687: step 4782, loss 0.182259, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:47.437062: step 4783, loss 0.127585, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:47.679233: step 4784, loss 0.124455, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:47.954958: step 4785, loss 0.158326, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:48.281973: step 4786, loss 0.0715811, acc 1, learning_rate 0.0001
2017-10-10T15:16:48.615480: step 4787, loss 0.054769, acc 1, learning_rate 0.0001
2017-10-10T15:16:48.901828: step 4788, loss 0.0655111, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:49.175532: step 4789, loss 0.0914799, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:49.430655: step 4790, loss 0.112093, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:49.642594: step 4791, loss 0.0533475, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:49.844860: step 4792, loss 0.0581249, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:50.183881: step 4793, loss 0.0889404, acc 1, learning_rate 0.0001
2017-10-10T15:16:50.517435: step 4794, loss 0.0423797, acc 1, learning_rate 0.0001
2017-10-10T15:16:50.854381: step 4795, loss 0.104554, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:51.164807: step 4796, loss 0.0696404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:51.512630: step 4797, loss 0.125937, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:51.838812: step 4798, loss 0.0813585, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:52.144102: step 4799, loss 0.0820907, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:52.431309: step 4800, loss 0.0467178, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:52.887934: step 4800, loss 0.225294, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4800

2017-10-10T15:16:53.940546: step 4801, loss 0.0703742, acc 1, learning_rate 0.0001
2017-10-10T15:16:54.173901: step 4802, loss 0.10977, acc 0.980392, learning_rate 0.0001
2017-10-10T15:16:54.485047: step 4803, loss 0.0564871, acc 1, learning_rate 0.0001
2017-10-10T15:16:54.808838: step 4804, loss 0.180234, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:55.125379: step 4805, loss 0.158168, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:55.433153: step 4806, loss 0.0523082, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:55.688583: step 4807, loss 0.0876676, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:56.008431: step 4808, loss 0.0564162, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:56.308842: step 4809, loss 0.148432, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:56.631799: step 4810, loss 0.0718503, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:56.944820: step 4811, loss 0.0279421, acc 1, learning_rate 0.0001
2017-10-10T15:16:57.284288: step 4812, loss 0.0957076, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:57.564922: step 4813, loss 0.0816895, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:57.831992: step 4814, loss 0.0865553, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:58.257122: step 4815, loss 0.0972722, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:58.559876: step 4816, loss 0.130925, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:58.838774: step 4817, loss 0.0908373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:59.144419: step 4818, loss 0.0932778, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:59.449259: step 4819, loss 0.109745, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:59.736215: step 4820, loss 0.146486, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:00.040944: step 4821, loss 0.0813821, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:00.376873: step 4822, loss 0.0975311, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:00.716884: step 4823, loss 0.106396, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:01.006278: step 4824, loss 0.0905767, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:01.355713: step 4825, loss 0.0939463, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:01.596869: step 4826, loss 0.0864138, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:01.876201: step 4827, loss 0.117554, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:02.099338: step 4828, loss 0.0952097, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:02.401320: step 4829, loss 0.091556, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:02.723057: step 4830, loss 0.0796985, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:03.002767: step 4831, loss 0.168578, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:03.256566: step 4832, loss 0.0582519, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:03.604052: step 4833, loss 0.138898, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:03.834433: step 4834, loss 0.104702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:04.098290: step 4835, loss 0.0614242, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:04.416902: step 4836, loss 0.156067, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:04.673577: step 4837, loss 0.0434296, acc 1, learning_rate 0.0001
2017-10-10T15:17:04.941325: step 4838, loss 0.0664528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:05.261228: step 4839, loss 0.134436, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:05.631006: step 4840, loss 0.126601, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:06.137608: step 4840, loss 0.227459, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4840

2017-10-10T15:17:07.268785: step 4841, loss 0.127014, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:07.586122: step 4842, loss 0.0834631, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:07.911161: step 4843, loss 0.0937157, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:08.219673: step 4844, loss 0.0391609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:08.500995: step 4845, loss 0.138457, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:08.849210: step 4846, loss 0.19082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:09.163882: step 4847, loss 0.08471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:09.364840: step 4848, loss 0.0683751, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:09.680971: step 4849, loss 0.164529, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:09.967943: step 4850, loss 0.113806, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:10.292955: step 4851, loss 0.0636249, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:10.685687: step 4852, loss 0.0780302, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:10.934953: step 4853, loss 0.171042, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:11.217157: step 4854, loss 0.0553962, acc 1, learning_rate 0.0001
2017-10-10T15:17:11.532942: step 4855, loss 0.0546964, acc 1, learning_rate 0.0001
2017-10-10T15:17:11.860943: step 4856, loss 0.0504273, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:12.149071: step 4857, loss 0.183718, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:12.449558: step 4858, loss 0.10397, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:12.769207: step 4859, loss 0.0264041, acc 1, learning_rate 0.0001
2017-10-10T15:17:13.076959: step 4860, loss 0.099477, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:13.410919: step 4861, loss 0.0527499, acc 1, learning_rate 0.0001
2017-10-10T15:17:13.697448: step 4862, loss 0.0877308, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:13.956964: step 4863, loss 0.0809129, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:14.277023: step 4864, loss 0.0918697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:14.579524: step 4865, loss 0.0840994, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:14.903252: step 4866, loss 0.124375, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:15.210852: step 4867, loss 0.11906, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:15.512974: step 4868, loss 0.0297058, acc 1, learning_rate 0.0001
2017-10-10T15:17:15.763574: step 4869, loss 0.0663531, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:16.013218: step 4870, loss 0.0901983, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:16.288594: step 4871, loss 0.112814, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:16.602005: step 4872, loss 0.105853, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:16.838383: step 4873, loss 0.0277179, acc 1, learning_rate 0.0001
2017-10-10T15:17:17.111888: step 4874, loss 0.183514, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:17.464834: step 4875, loss 0.0889519, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:17.776892: step 4876, loss 0.115342, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:18.039575: step 4877, loss 0.0335227, acc 1, learning_rate 0.0001
2017-10-10T15:17:18.441043: step 4878, loss 0.0838233, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:18.791031: step 4879, loss 0.0567733, acc 1, learning_rate 0.0001
2017-10-10T15:17:19.013482: step 4880, loss 0.138921, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:19.512115: step 4880, loss 0.227492, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4880

2017-10-10T15:17:20.629919: step 4881, loss 0.0738808, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:20.940736: step 4882, loss 0.175724, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:21.213047: step 4883, loss 0.0707992, acc 1, learning_rate 0.0001
2017-10-10T15:17:21.492363: step 4884, loss 0.0800638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:21.751495: step 4885, loss 0.0743174, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:22.131312: step 4886, loss 0.0623337, acc 1, learning_rate 0.0001
2017-10-10T15:17:22.483052: step 4887, loss 0.106548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:22.774078: step 4888, loss 0.0424873, acc 1, learning_rate 0.0001
2017-10-10T15:17:23.034010: step 4889, loss 0.170015, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:23.292318: step 4890, loss 0.0801458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:23.585970: step 4891, loss 0.0883302, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:23.889866: step 4892, loss 0.0872348, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:24.228866: step 4893, loss 0.060509, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:24.515251: step 4894, loss 0.112852, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:24.804954: step 4895, loss 0.121782, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:25.110752: step 4896, loss 0.0450798, acc 1, learning_rate 0.0001
2017-10-10T15:17:25.464054: step 4897, loss 0.0785769, acc 1, learning_rate 0.0001
2017-10-10T15:17:25.774753: step 4898, loss 0.0569349, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:26.048833: step 4899, loss 0.149546, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:26.279932: step 4900, loss 0.113915, acc 0.941176, learning_rate 0.0001
2017-10-10T15:17:26.553934: step 4901, loss 0.0682191, acc 1, learning_rate 0.0001
2017-10-10T15:17:26.810908: step 4902, loss 0.050741, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:27.140858: step 4903, loss 0.11615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:27.417004: step 4904, loss 0.102388, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:27.693397: step 4905, loss 0.111155, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:27.977087: step 4906, loss 0.116204, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:28.298706: step 4907, loss 0.122643, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:28.541149: step 4908, loss 0.0758615, acc 1, learning_rate 0.0001
2017-10-10T15:17:28.825708: step 4909, loss 0.124533, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:29.181657: step 4910, loss 0.0508351, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:29.472956: step 4911, loss 0.0929092, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:29.768635: step 4912, loss 0.0885486, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:30.085161: step 4913, loss 0.110604, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:30.428830: step 4914, loss 0.0532402, acc 1, learning_rate 0.0001
2017-10-10T15:17:30.736845: step 4915, loss 0.109131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:31.019037: step 4916, loss 0.137156, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:31.315915: step 4917, loss 0.0614069, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:31.685179: step 4918, loss 0.0430562, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:32.020784: step 4919, loss 0.0949191, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:32.307504: step 4920, loss 0.0849341, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:32.760891: step 4920, loss 0.226607, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4920

2017-10-10T15:17:33.917090: step 4921, loss 0.046857, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:34.189293: step 4922, loss 0.0980054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:34.536898: step 4923, loss 0.183041, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:34.852824: step 4924, loss 0.0743462, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:35.157154: step 4925, loss 0.122385, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:35.446396: step 4926, loss 0.0333447, acc 1, learning_rate 0.0001
2017-10-10T15:17:35.744840: step 4927, loss 0.0684081, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:36.053097: step 4928, loss 0.0692327, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:36.320508: step 4929, loss 0.12006, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:36.659672: step 4930, loss 0.112619, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:36.969344: step 4931, loss 0.224114, acc 0.890625, learning_rate 0.0001
2017-10-10T15:17:37.260265: step 4932, loss 0.103834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:37.480842: step 4933, loss 0.0935099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:37.765077: step 4934, loss 0.0822067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:38.021000: step 4935, loss 0.0911637, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:38.378465: step 4936, loss 0.055047, acc 1, learning_rate 0.0001
2017-10-10T15:17:38.667827: step 4937, loss 0.061324, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:39.021157: step 4938, loss 0.114257, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:39.273736: step 4939, loss 0.0817094, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:39.526730: step 4940, loss 0.0333876, acc 1, learning_rate 0.0001
2017-10-10T15:17:39.787637: step 4941, loss 0.109555, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:40.099756: step 4942, loss 0.081272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:40.447863: step 4943, loss 0.0890003, acc 1, learning_rate 0.0001
2017-10-10T15:17:40.703500: step 4944, loss 0.0937049, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:41.036958: step 4945, loss 0.194593, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:41.326412: step 4946, loss 0.12581, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:41.653029: step 4947, loss 0.14425, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:41.941067: step 4948, loss 0.0667701, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:42.210757: step 4949, loss 0.0610889, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:42.503515: step 4950, loss 0.0978752, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:42.795554: step 4951, loss 0.132309, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:43.136879: step 4952, loss 0.0840232, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:43.493050: step 4953, loss 0.0704135, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:43.787023: step 4954, loss 0.144869, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:44.029259: step 4955, loss 0.0810023, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:44.363265: step 4956, loss 0.0665547, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:44.655143: step 4957, loss 0.0573033, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:44.958480: step 4958, loss 0.121318, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:45.247989: step 4959, loss 0.133011, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:45.576487: step 4960, loss 0.101627, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:46.091746: step 4960, loss 0.225784, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-4960

2017-10-10T15:17:47.090706: step 4961, loss 0.08313, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:47.348718: step 4962, loss 0.0976886, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:47.638787: step 4963, loss 0.0820394, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:48.013763: step 4964, loss 0.0525996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:48.297030: step 4965, loss 0.111216, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:48.633090: step 4966, loss 0.12467, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:48.947777: step 4967, loss 0.13545, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:49.231279: step 4968, loss 0.0526369, acc 1, learning_rate 0.0001
2017-10-10T15:17:49.520894: step 4969, loss 0.121707, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:49.928935: step 4970, loss 0.151273, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:50.175070: step 4971, loss 0.23198, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:50.461169: step 4972, loss 0.0693715, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:50.760872: step 4973, loss 0.0701091, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:51.047494: step 4974, loss 0.112941, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:51.334028: step 4975, loss 0.156178, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:51.591554: step 4976, loss 0.0451651, acc 1, learning_rate 0.0001
2017-10-10T15:17:51.872376: step 4977, loss 0.0720624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:52.183634: step 4978, loss 0.0589401, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:52.465063: step 4979, loss 0.113902, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:52.768868: step 4980, loss 0.118633, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:53.055214: step 4981, loss 0.0967592, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:53.375637: step 4982, loss 0.127515, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:53.663877: step 4983, loss 0.0654656, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:53.961218: step 4984, loss 0.0594799, acc 1, learning_rate 0.0001
2017-10-10T15:17:54.291443: step 4985, loss 0.0745074, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:54.551646: step 4986, loss 0.0604491, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:54.886377: step 4987, loss 0.0645059, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:55.151002: step 4988, loss 0.0991288, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:55.506784: step 4989, loss 0.0952128, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:55.715388: step 4990, loss 0.126456, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:55.954961: step 4991, loss 0.126934, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:56.141619: step 4992, loss 0.110599, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:56.348832: step 4993, loss 0.0552463, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:56.640910: step 4994, loss 0.155745, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:56.873074: step 4995, loss 0.0784361, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:57.186135: step 4996, loss 0.0516294, acc 1, learning_rate 0.0001
2017-10-10T15:17:57.537799: step 4997, loss 0.083709, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:57.800915: step 4998, loss 0.0929889, acc 0.980392, learning_rate 0.0001
2017-10-10T15:17:58.040863: step 4999, loss 0.130099, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:58.408835: step 5000, loss 0.160163, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:59.024107: step 5000, loss 0.227039, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5000

2017-10-10T15:18:00.100757: step 5001, loss 0.114007, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:00.445292: step 5002, loss 0.096251, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:00.763767: step 5003, loss 0.115303, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:01.011632: step 5004, loss 0.140302, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:01.281072: step 5005, loss 0.0863778, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:01.620993: step 5006, loss 0.122898, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:01.877006: step 5007, loss 0.126451, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:02.259128: step 5008, loss 0.070327, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:02.512896: step 5009, loss 0.126998, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:02.795218: step 5010, loss 0.0598146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:03.105468: step 5011, loss 0.0639636, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:03.353699: step 5012, loss 0.146778, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:03.690582: step 5013, loss 0.0852444, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:04.008677: step 5014, loss 0.0405059, acc 1, learning_rate 0.0001
2017-10-10T15:18:04.372877: step 5015, loss 0.0784855, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:04.694716: step 5016, loss 0.0978391, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:04.984889: step 5017, loss 0.188905, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:05.293937: step 5018, loss 0.102587, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:05.571484: step 5019, loss 0.113446, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:05.900147: step 5020, loss 0.11073, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:06.221069: step 5021, loss 0.109971, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:06.525969: step 5022, loss 0.0416837, acc 1, learning_rate 0.0001
2017-10-10T15:18:06.794341: step 5023, loss 0.048292, acc 1, learning_rate 0.0001
2017-10-10T15:18:07.114118: step 5024, loss 0.117348, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:07.417146: step 5025, loss 0.0674181, acc 1, learning_rate 0.0001
2017-10-10T15:18:07.748830: step 5026, loss 0.121306, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:08.070222: step 5027, loss 0.140651, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:08.389127: step 5028, loss 0.0828455, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:08.610553: step 5029, loss 0.108087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:08.913943: step 5030, loss 0.0984547, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:09.165058: step 5031, loss 0.158342, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:09.489075: step 5032, loss 0.0453695, acc 1, learning_rate 0.0001
2017-10-10T15:18:09.783171: step 5033, loss 0.103919, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:10.138628: step 5034, loss 0.0804753, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:10.421030: step 5035, loss 0.0881979, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:10.698711: step 5036, loss 0.0847929, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:11.047055: step 5037, loss 0.0655136, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:11.366978: step 5038, loss 0.091938, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:11.649303: step 5039, loss 0.0650908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:11.951913: step 5040, loss 0.0555162, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:12.510842: step 5040, loss 0.226388, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5040

2017-10-10T15:18:13.608877: step 5041, loss 0.149615, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:13.897781: step 5042, loss 0.0636022, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:14.069746: step 5043, loss 0.0324516, acc 1, learning_rate 0.0001
2017-10-10T15:18:14.348009: step 5044, loss 0.0717067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:14.614390: step 5045, loss 0.0548749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:14.832601: step 5046, loss 0.058332, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:15.157284: step 5047, loss 0.128922, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:15.497430: step 5048, loss 0.123559, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:15.796876: step 5049, loss 0.0663625, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:16.025079: step 5050, loss 0.103221, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:16.308883: step 5051, loss 0.0598557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:16.607938: step 5052, loss 0.179773, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:16.882818: step 5053, loss 0.047725, acc 1, learning_rate 0.0001
2017-10-10T15:18:17.184854: step 5054, loss 0.0732575, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:17.498153: step 5055, loss 0.0778307, acc 1, learning_rate 0.0001
2017-10-10T15:18:17.785515: step 5056, loss 0.0623857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:18.056867: step 5057, loss 0.0704561, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:18.344995: step 5058, loss 0.0697973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:18.570997: step 5059, loss 0.0734079, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:18.864542: step 5060, loss 0.121101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:19.193065: step 5061, loss 0.0411051, acc 1, learning_rate 0.0001
2017-10-10T15:18:19.593795: step 5062, loss 0.0612389, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:19.924919: step 5063, loss 0.100694, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:20.282806: step 5064, loss 0.0986684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:20.540975: step 5065, loss 0.0774132, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:20.841047: step 5066, loss 0.167674, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:21.100491: step 5067, loss 0.0928991, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:21.428816: step 5068, loss 0.0552709, acc 1, learning_rate 0.0001
2017-10-10T15:18:21.695122: step 5069, loss 0.0506951, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:21.961092: step 5070, loss 0.0957053, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:22.223611: step 5071, loss 0.12138, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:22.528927: step 5072, loss 0.116797, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:22.852912: step 5073, loss 0.0547735, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:23.121017: step 5074, loss 0.118739, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:23.504861: step 5075, loss 0.138214, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:23.849081: step 5076, loss 0.064269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:24.211747: step 5077, loss 0.0701361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:24.535062: step 5078, loss 0.0594374, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:24.833270: step 5079, loss 0.0331162, acc 1, learning_rate 0.0001
2017-10-10T15:18:25.129574: step 5080, loss 0.09174, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:25.686337: step 5080, loss 0.226932, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5080

2017-10-10T15:18:26.916429: step 5081, loss 0.0542003, acc 1, learning_rate 0.0001
2017-10-10T15:18:27.215413: step 5082, loss 0.0472136, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:27.450354: step 5083, loss 0.173558, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:27.649104: step 5084, loss 0.0933503, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:27.916903: step 5085, loss 0.127603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:28.248879: step 5086, loss 0.0895009, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:28.505818: step 5087, loss 0.0720966, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:28.783341: step 5088, loss 0.0991864, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:29.087963: step 5089, loss 0.0707309, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:29.412845: step 5090, loss 0.155274, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:29.790668: step 5091, loss 0.0684244, acc 1, learning_rate 0.0001
2017-10-10T15:18:30.069498: step 5092, loss 0.116335, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:30.324848: step 5093, loss 0.0946349, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:30.562055: step 5094, loss 0.0739598, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:30.808892: step 5095, loss 0.163398, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:31.105053: step 5096, loss 0.0741784, acc 0.980392, learning_rate 0.0001
2017-10-10T15:18:31.488910: step 5097, loss 0.111494, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:31.781730: step 5098, loss 0.0852285, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:32.148311: step 5099, loss 0.0970016, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:32.427692: step 5100, loss 0.153055, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:32.669030: step 5101, loss 0.05963, acc 1, learning_rate 0.0001
2017-10-10T15:18:32.980896: step 5102, loss 0.116051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:33.202833: step 5103, loss 0.136698, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:33.540463: step 5104, loss 0.061515, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:33.833777: step 5105, loss 0.14247, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:34.192863: step 5106, loss 0.224821, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:34.450652: step 5107, loss 0.12394, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:34.756835: step 5108, loss 0.153067, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:35.056914: step 5109, loss 0.0686013, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:35.344846: step 5110, loss 0.12563, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:35.625174: step 5111, loss 0.13418, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:35.913535: step 5112, loss 0.046087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:36.191752: step 5113, loss 0.153401, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:36.492463: step 5114, loss 0.103442, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:36.802841: step 5115, loss 0.0426306, acc 1, learning_rate 0.0001
2017-10-10T15:18:37.113032: step 5116, loss 0.0631578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:37.408922: step 5117, loss 0.154495, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:37.733547: step 5118, loss 0.126728, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:38.124902: step 5119, loss 0.122911, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:38.435622: step 5120, loss 0.0896745, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:39.059709: step 5120, loss 0.226035, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5120

2017-10-10T15:18:39.968598: step 5121, loss 0.0533631, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:40.276813: step 5122, loss 0.0979979, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:40.608765: step 5123, loss 0.0701348, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:40.962989: step 5124, loss 0.0803018, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:41.270200: step 5125, loss 0.143252, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:41.632868: step 5126, loss 0.225197, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:41.963737: step 5127, loss 0.153552, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:42.229066: step 5128, loss 0.0681243, acc 1, learning_rate 0.0001
2017-10-10T15:18:42.540370: step 5129, loss 0.132154, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:42.828671: step 5130, loss 0.0299178, acc 1, learning_rate 0.0001
2017-10-10T15:18:43.095169: step 5131, loss 0.0935814, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:43.414931: step 5132, loss 0.194545, acc 0.90625, learning_rate 0.0001
2017-10-10T15:18:43.709020: step 5133, loss 0.0517329, acc 1, learning_rate 0.0001
2017-10-10T15:18:44.001220: step 5134, loss 0.0796822, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:44.316987: step 5135, loss 0.0878841, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:44.628400: step 5136, loss 0.136365, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:44.965932: step 5137, loss 0.062255, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:45.235901: step 5138, loss 0.155348, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:45.459859: step 5139, loss 0.121386, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:46.209425: step 5140, loss 0.0386842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:46.455178: step 5141, loss 0.0852287, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:46.784682: step 5142, loss 0.0724454, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:47.117054: step 5143, loss 0.115039, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:47.331612: step 5144, loss 0.158456, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:47.584729: step 5145, loss 0.0424927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:47.860986: step 5146, loss 0.111021, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:48.111657: step 5147, loss 0.127805, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:48.428911: step 5148, loss 0.105745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:48.716946: step 5149, loss 0.0893954, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:49.021271: step 5150, loss 0.0491342, acc 1, learning_rate 0.0001
2017-10-10T15:18:49.292047: step 5151, loss 0.0507516, acc 1, learning_rate 0.0001
2017-10-10T15:18:49.679124: step 5152, loss 0.0914267, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:49.974566: step 5153, loss 0.0657508, acc 1, learning_rate 0.0001
2017-10-10T15:18:50.257770: step 5154, loss 0.0901793, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:50.534453: step 5155, loss 0.084334, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:50.822594: step 5156, loss 0.0861054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:51.068632: step 5157, loss 0.0583003, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:51.281026: step 5158, loss 0.0886248, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:51.522396: step 5159, loss 0.0478468, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:51.789455: step 5160, loss 0.0369738, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:52.473085: step 5160, loss 0.227065, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5160

2017-10-10T15:18:53.624438: step 5161, loss 0.0952189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:53.948873: step 5162, loss 0.0603801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:54.231316: step 5163, loss 0.024036, acc 1, learning_rate 0.0001
2017-10-10T15:18:54.528476: step 5164, loss 0.0780892, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:54.821914: step 5165, loss 0.231862, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:55.129086: step 5166, loss 0.0620942, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:55.416033: step 5167, loss 0.0686256, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:55.735592: step 5168, loss 0.103218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:56.028213: step 5169, loss 0.102897, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:56.344933: step 5170, loss 0.133822, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:56.656614: step 5171, loss 0.108832, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:56.992993: step 5172, loss 0.0490763, acc 1, learning_rate 0.0001
2017-10-10T15:18:57.340844: step 5173, loss 0.181165, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:57.630235: step 5174, loss 0.0739545, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:57.925116: step 5175, loss 0.189643, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:58.252383: step 5176, loss 0.0615019, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:58.549414: step 5177, loss 0.144359, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:58.918887: step 5178, loss 0.0524395, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:59.205874: step 5179, loss 0.133194, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:59.532229: step 5180, loss 0.143759, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:59.860973: step 5181, loss 0.118267, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:00.141864: step 5182, loss 0.0944216, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:00.506104: step 5183, loss 0.0939508, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:00.792877: step 5184, loss 0.0612868, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:01.113701: step 5185, loss 0.100183, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:01.420151: step 5186, loss 0.150057, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:01.688309: step 5187, loss 0.067063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:01.995967: step 5188, loss 0.0480599, acc 1, learning_rate 0.0001
2017-10-10T15:19:02.284848: step 5189, loss 0.154102, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:02.580564: step 5190, loss 0.122135, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:02.825260: step 5191, loss 0.0913003, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:03.166008: step 5192, loss 0.100091, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:03.509927: step 5193, loss 0.08684, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:03.778390: step 5194, loss 0.0864821, acc 0.980392, learning_rate 0.0001
2017-10-10T15:19:04.068140: step 5195, loss 0.0816016, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:04.305354: step 5196, loss 0.0820782, acc 1, learning_rate 0.0001
2017-10-10T15:19:04.552044: step 5197, loss 0.106911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:04.827735: step 5198, loss 0.117205, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:05.137149: step 5199, loss 0.113749, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:05.381182: step 5200, loss 0.0470238, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:06.037224: step 5200, loss 0.226834, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5200

2017-10-10T15:19:07.188412: step 5201, loss 0.114578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:07.493580: step 5202, loss 0.0566582, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:07.790735: step 5203, loss 0.0744658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:08.073052: step 5204, loss 0.174333, acc 0.90625, learning_rate 0.0001
2017-10-10T15:19:08.396010: step 5205, loss 0.096591, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:08.680875: step 5206, loss 0.163505, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:08.948817: step 5207, loss 0.0720165, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:09.242090: step 5208, loss 0.0660637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:09.580082: step 5209, loss 0.0892974, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:09.890219: step 5210, loss 0.0669595, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:10.111508: step 5211, loss 0.104304, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:10.336868: step 5212, loss 0.0768009, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:10.616926: step 5213, loss 0.126565, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:10.933268: step 5214, loss 0.116193, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:11.242949: step 5215, loss 0.0786662, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:11.584314: step 5216, loss 0.146609, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:11.897212: step 5217, loss 0.0746617, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:12.173154: step 5218, loss 0.146622, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:12.447702: step 5219, loss 0.0837682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:12.732671: step 5220, loss 0.103709, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:13.015097: step 5221, loss 0.0904707, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:13.332985: step 5222, loss 0.0539636, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:13.660975: step 5223, loss 0.100874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:13.925763: step 5224, loss 0.113131, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:14.232679: step 5225, loss 0.193119, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:14.536859: step 5226, loss 0.11323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:14.807591: step 5227, loss 0.0720896, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:15.112937: step 5228, loss 0.0573121, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:15.384936: step 5229, loss 0.15071, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:15.712420: step 5230, loss 0.0739535, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:16.076829: step 5231, loss 0.0521976, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:16.397060: step 5232, loss 0.0536564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:16.640883: step 5233, loss 0.0495693, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:16.946136: step 5234, loss 0.12889, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:17.273469: step 5235, loss 0.116347, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:17.632224: step 5236, loss 0.135575, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:17.853419: step 5237, loss 0.0312324, acc 1, learning_rate 0.0001
2017-10-10T15:19:18.141507: step 5238, loss 0.0395495, acc 1, learning_rate 0.0001
2017-10-10T15:19:18.464959: step 5239, loss 0.0897178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:18.763911: step 5240, loss 0.123359, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:19.317740: step 5240, loss 0.224465, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5240

2017-10-10T15:19:20.441085: step 5241, loss 0.118101, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:20.735439: step 5242, loss 0.0712063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:21.004944: step 5243, loss 0.0916169, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:21.274117: step 5244, loss 0.0521005, acc 1, learning_rate 0.0001
2017-10-10T15:19:21.554535: step 5245, loss 0.140993, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:21.808992: step 5246, loss 0.0532764, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:22.108966: step 5247, loss 0.0880555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:22.516358: step 5248, loss 0.0929046, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:22.770265: step 5249, loss 0.0420828, acc 1, learning_rate 0.0001
2017-10-10T15:19:23.015543: step 5250, loss 0.108422, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:23.326488: step 5251, loss 0.104742, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:23.656903: step 5252, loss 0.0639224, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:23.920753: step 5253, loss 0.0546346, acc 1, learning_rate 0.0001
2017-10-10T15:19:24.226281: step 5254, loss 0.0694177, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:24.545771: step 5255, loss 0.0873595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:24.864954: step 5256, loss 0.12738, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:25.158938: step 5257, loss 0.0813907, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:25.476818: step 5258, loss 0.0265291, acc 1, learning_rate 0.0001
2017-10-10T15:19:25.778966: step 5259, loss 0.0248901, acc 1, learning_rate 0.0001
2017-10-10T15:19:26.008521: step 5260, loss 0.0599036, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:26.307772: step 5261, loss 0.071893, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:26.612937: step 5262, loss 0.0792097, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:26.905974: step 5263, loss 0.181152, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:27.210957: step 5264, loss 0.0916576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:27.534919: step 5265, loss 0.11235, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:27.847910: step 5266, loss 0.121755, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:28.192861: step 5267, loss 0.0764325, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:28.454167: step 5268, loss 0.100514, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:28.684929: step 5269, loss 0.137503, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:28.935042: step 5270, loss 0.157587, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:29.188988: step 5271, loss 0.119937, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:29.533053: step 5272, loss 0.0725892, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:29.829430: step 5273, loss 0.097105, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:30.074599: step 5274, loss 0.125562, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:30.385002: step 5275, loss 0.0955601, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:30.676836: step 5276, loss 0.115103, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:30.972404: step 5277, loss 0.112656, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:31.241008: step 5278, loss 0.116963, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:31.565615: step 5279, loss 0.0910651, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:31.864512: step 5280, loss 0.114595, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:32.477009: step 5280, loss 0.227762, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5280

2017-10-10T15:19:33.616819: step 5281, loss 0.0718066, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:33.794552: step 5282, loss 0.129403, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:33.882285: step 5283, loss 0.0613653, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:33.968139: step 5284, loss 0.0747606, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:34.268736: step 5285, loss 0.0495266, acc 1, learning_rate 0.0001
2017-10-10T15:19:34.528958: step 5286, loss 0.0937561, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:34.836977: step 5287, loss 0.174182, acc 0.90625, learning_rate 0.0001
2017-10-10T15:19:35.175148: step 5288, loss 0.0693045, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:35.504570: step 5289, loss 0.0673315, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:35.846995: step 5290, loss 0.14905, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:36.097576: step 5291, loss 0.0523006, acc 1, learning_rate 0.0001
2017-10-10T15:19:36.370224: step 5292, loss 0.0573539, acc 1, learning_rate 0.0001
2017-10-10T15:19:36.648917: step 5293, loss 0.275583, acc 0.890625, learning_rate 0.0001
2017-10-10T15:19:37.000712: step 5294, loss 0.0515054, acc 1, learning_rate 0.0001
2017-10-10T15:19:37.364519: step 5295, loss 0.127491, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:37.660918: step 5296, loss 0.0688439, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:37.944933: step 5297, loss 0.0648349, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:38.196939: step 5298, loss 0.0523119, acc 1, learning_rate 0.0001
2017-10-10T15:19:38.464834: step 5299, loss 0.0917582, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:38.789890: step 5300, loss 0.101662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:39.114742: step 5301, loss 0.038252, acc 1, learning_rate 0.0001
2017-10-10T15:19:39.399610: step 5302, loss 0.0523921, acc 1, learning_rate 0.0001
2017-10-10T15:19:39.672600: step 5303, loss 0.10347, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:39.972878: step 5304, loss 0.187105, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:40.346508: step 5305, loss 0.0778644, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:40.585759: step 5306, loss 0.0715047, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:40.867408: step 5307, loss 0.113546, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:41.209941: step 5308, loss 0.0668956, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:41.498653: step 5309, loss 0.202082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:41.840868: step 5310, loss 0.0499504, acc 1, learning_rate 0.0001
2017-10-10T15:19:42.190609: step 5311, loss 0.0572904, acc 1, learning_rate 0.0001
2017-10-10T15:19:42.517074: step 5312, loss 0.0661971, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:42.745505: step 5313, loss 0.0989623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:42.981799: step 5314, loss 0.0783159, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:43.346401: step 5315, loss 0.0309358, acc 1, learning_rate 0.0001
2017-10-10T15:19:43.678113: step 5316, loss 0.0311698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:43.933135: step 5317, loss 0.163009, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:44.285172: step 5318, loss 0.0976123, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:44.584866: step 5319, loss 0.0926343, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:44.874886: step 5320, loss 0.0869238, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:45.407926: step 5320, loss 0.228061, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5320

2017-10-10T15:19:46.480821: step 5321, loss 0.0602015, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:46.824783: step 5322, loss 0.179012, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:47.111985: step 5323, loss 0.0745345, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:47.369011: step 5324, loss 0.0748811, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:47.590208: step 5325, loss 0.0891115, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:47.881185: step 5326, loss 0.0201613, acc 1, learning_rate 0.0001
2017-10-10T15:19:48.177232: step 5327, loss 0.14268, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:48.532836: step 5328, loss 0.0523352, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:48.852819: step 5329, loss 0.119283, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:49.061725: step 5330, loss 0.181859, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:49.380863: step 5331, loss 0.0277138, acc 1, learning_rate 0.0001
2017-10-10T15:19:49.700833: step 5332, loss 0.0922307, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:50.012958: step 5333, loss 0.0642924, acc 1, learning_rate 0.0001
2017-10-10T15:19:50.333635: step 5334, loss 0.0844267, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:50.636847: step 5335, loss 0.104063, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:50.952902: step 5336, loss 0.0856669, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:51.225260: step 5337, loss 0.0889899, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:51.513091: step 5338, loss 0.0879893, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:51.879237: step 5339, loss 0.0906562, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:52.158618: step 5340, loss 0.0862218, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:52.441642: step 5341, loss 0.1105, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:52.770893: step 5342, loss 0.0973915, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:53.069017: step 5343, loss 0.14423, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:53.373604: step 5344, loss 0.0824023, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:53.677962: step 5345, loss 0.190407, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:54.016988: step 5346, loss 0.077481, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:54.373094: step 5347, loss 0.0950145, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:54.616816: step 5348, loss 0.0886738, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:54.856062: step 5349, loss 0.0420906, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:55.078505: step 5350, loss 0.0897512, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:55.375356: step 5351, loss 0.148574, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:55.697011: step 5352, loss 0.107039, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:56.040912: step 5353, loss 0.0784395, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:56.297344: step 5354, loss 0.090077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:56.589000: step 5355, loss 0.114976, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:56.895362: step 5356, loss 0.124681, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:57.155997: step 5357, loss 0.0731938, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:57.432617: step 5358, loss 0.0508275, acc 1, learning_rate 0.0001
2017-10-10T15:19:57.752899: step 5359, loss 0.070115, acc 1, learning_rate 0.0001
2017-10-10T15:19:58.113118: step 5360, loss 0.132793, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:58.729017: step 5360, loss 0.225073, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5360

2017-10-10T15:20:00.186173: step 5361, loss 0.128178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:00.420731: step 5362, loss 0.0524838, acc 1, learning_rate 0.0001
2017-10-10T15:20:00.723342: step 5363, loss 0.170981, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:01.001077: step 5364, loss 0.0526407, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:01.296989: step 5365, loss 0.0572279, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:01.599027: step 5366, loss 0.0831526, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:01.908818: step 5367, loss 0.0516276, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:02.252773: step 5368, loss 0.175343, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:02.517816: step 5369, loss 0.102772, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:02.860870: step 5370, loss 0.113101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:03.138724: step 5371, loss 0.0302563, acc 1, learning_rate 0.0001
2017-10-10T15:20:03.482864: step 5372, loss 0.0738584, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:03.762478: step 5373, loss 0.124999, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:04.045302: step 5374, loss 0.0943597, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:04.396842: step 5375, loss 0.0843735, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:04.740802: step 5376, loss 0.0793977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:05.020995: step 5377, loss 0.0881633, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:05.376990: step 5378, loss 0.0601341, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:05.736239: step 5379, loss 0.0764702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:05.992609: step 5380, loss 0.028409, acc 1, learning_rate 0.0001
2017-10-10T15:20:06.299049: step 5381, loss 0.331624, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:06.612824: step 5382, loss 0.069299, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:06.824891: step 5383, loss 0.0749616, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:07.051039: step 5384, loss 0.115765, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:07.344955: step 5385, loss 0.0835998, acc 1, learning_rate 0.0001
2017-10-10T15:20:07.597759: step 5386, loss 0.0865003, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:07.866612: step 5387, loss 0.0719033, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:08.190803: step 5388, loss 0.0633694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:08.529528: step 5389, loss 0.0565227, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:08.804853: step 5390, loss 0.105838, acc 0.960784, learning_rate 0.0001
2017-10-10T15:20:09.118425: step 5391, loss 0.1086, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:09.433682: step 5392, loss 0.118287, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:09.736218: step 5393, loss 0.0427184, acc 1, learning_rate 0.0001
2017-10-10T15:20:10.056158: step 5394, loss 0.172674, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:10.370684: step 5395, loss 0.102965, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:10.606496: step 5396, loss 0.0405365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:10.872822: step 5397, loss 0.133806, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:11.169457: step 5398, loss 0.118428, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:11.497677: step 5399, loss 0.137763, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:11.756005: step 5400, loss 0.0664308, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:12.377178: step 5400, loss 0.224343, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5400

2017-10-10T15:20:13.476993: step 5401, loss 0.0330171, acc 1, learning_rate 0.0001
2017-10-10T15:20:13.753496: step 5402, loss 0.0439911, acc 1, learning_rate 0.0001
2017-10-10T15:20:14.104842: step 5403, loss 0.0997179, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:14.389254: step 5404, loss 0.0966676, acc 1, learning_rate 0.0001
2017-10-10T15:20:14.682706: step 5405, loss 0.0489346, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:14.963831: step 5406, loss 0.0716788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:15.240439: step 5407, loss 0.0264678, acc 1, learning_rate 0.0001
2017-10-10T15:20:15.494590: step 5408, loss 0.0628741, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:15.811529: step 5409, loss 0.112325, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:16.120413: step 5410, loss 0.0595661, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:16.358399: step 5411, loss 0.114181, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:16.644612: step 5412, loss 0.123458, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:16.981046: step 5413, loss 0.140064, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:17.302784: step 5414, loss 0.143127, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:17.598839: step 5415, loss 0.042246, acc 1, learning_rate 0.0001
2017-10-10T15:20:17.860814: step 5416, loss 0.0547345, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:18.073547: step 5417, loss 0.0550507, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:18.346092: step 5418, loss 0.102652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:18.656978: step 5419, loss 0.0667657, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:18.984096: step 5420, loss 0.0197843, acc 1, learning_rate 0.0001
2017-10-10T15:20:19.280900: step 5421, loss 0.0487932, acc 1, learning_rate 0.0001
2017-10-10T15:20:19.546361: step 5422, loss 0.113427, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:19.815517: step 5423, loss 0.0661301, acc 1, learning_rate 0.0001
2017-10-10T15:20:20.104985: step 5424, loss 0.098808, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:20.388340: step 5425, loss 0.165277, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:20.694259: step 5426, loss 0.0509897, acc 1, learning_rate 0.0001
2017-10-10T15:20:20.961130: step 5427, loss 0.159321, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:21.240890: step 5428, loss 0.211124, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:21.577977: step 5429, loss 0.131845, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:21.945906: step 5430, loss 0.0534841, acc 1, learning_rate 0.0001
2017-10-10T15:20:22.200701: step 5431, loss 0.215886, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:22.482728: step 5432, loss 0.180185, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:22.832562: step 5433, loss 0.167154, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:23.157919: step 5434, loss 0.0523882, acc 1, learning_rate 0.0001
2017-10-10T15:20:23.456989: step 5435, loss 0.0923596, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:23.764323: step 5436, loss 0.0973166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:24.111095: step 5437, loss 0.127255, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:24.377615: step 5438, loss 0.10064, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.600347: step 5439, loss 0.0701723, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.840894: step 5440, loss 0.100629, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:25.389599: step 5440, loss 0.224521, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5440

2017-10-10T15:20:26.319379: step 5441, loss 0.0543756, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:26.660755: step 5442, loss 0.0506272, acc 1, learning_rate 0.0001
2017-10-10T15:20:27.004906: step 5443, loss 0.0961541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:27.297051: step 5444, loss 0.158545, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:27.586735: step 5445, loss 0.127463, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:27.879876: step 5446, loss 0.113781, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:28.112042: step 5447, loss 0.0398279, acc 1, learning_rate 0.0001
2017-10-10T15:20:28.377584: step 5448, loss 0.0768876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:28.690797: step 5449, loss 0.0240878, acc 1, learning_rate 0.0001
2017-10-10T15:20:28.924429: step 5450, loss 0.0933527, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:29.152862: step 5451, loss 0.0742655, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:29.469827: step 5452, loss 0.0918366, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:29.765622: step 5453, loss 0.0835576, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:30.089025: step 5454, loss 0.141719, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:30.416867: step 5455, loss 0.162018, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:30.707263: step 5456, loss 0.0765326, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:31.002568: step 5457, loss 0.297089, acc 0.875, learning_rate 0.0001
2017-10-10T15:20:31.299867: step 5458, loss 0.0969686, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:31.630839: step 5459, loss 0.0766419, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:31.893047: step 5460, loss 0.200057, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:32.124834: step 5461, loss 0.0833569, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:32.456027: step 5462, loss 0.113264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:32.780938: step 5463, loss 0.0696949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:33.072966: step 5464, loss 0.0425499, acc 1, learning_rate 0.0001
2017-10-10T15:20:33.384992: step 5465, loss 0.0710387, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:33.648868: step 5466, loss 0.0592733, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:33.976853: step 5467, loss 0.0828935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:34.221001: step 5468, loss 0.0479652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:34.493241: step 5469, loss 0.0833534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:34.818685: step 5470, loss 0.0836595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:35.130599: step 5471, loss 0.150141, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:35.398329: step 5472, loss 0.106136, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:35.712457: step 5473, loss 0.068377, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.027051: step 5474, loss 0.0622798, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.324849: step 5475, loss 0.080496, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.565066: step 5476, loss 0.103976, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:36.904274: step 5477, loss 0.0473178, acc 1, learning_rate 0.0001
2017-10-10T15:20:37.208826: step 5478, loss 0.0454866, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:37.540947: step 5479, loss 0.0757353, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:37.905060: step 5480, loss 0.194104, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:38.486154: step 5480, loss 0.225534, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5480

2017-10-10T15:20:39.616862: step 5481, loss 0.0499192, acc 1, learning_rate 0.0001
2017-10-10T15:20:39.957005: step 5482, loss 0.136598, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:40.177100: step 5483, loss 0.0621969, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:40.475146: step 5484, loss 0.111323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:40.783859: step 5485, loss 0.0706573, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:41.071320: step 5486, loss 0.0562432, acc 1, learning_rate 0.0001
2017-10-10T15:20:41.348961: step 5487, loss 0.0743454, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:41.628911: step 5488, loss 0.074856, acc 0.980392, learning_rate 0.0001
2017-10-10T15:20:41.993048: step 5489, loss 0.136525, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:42.316167: step 5490, loss 0.0833101, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:42.552834: step 5491, loss 0.120683, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:42.831044: step 5492, loss 0.0695511, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:43.068843: step 5493, loss 0.0986241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:43.311886: step 5494, loss 0.0650789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:43.615454: step 5495, loss 0.177416, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:43.939356: step 5496, loss 0.135326, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:44.303238: step 5497, loss 0.0675984, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:44.576883: step 5498, loss 0.0955331, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:44.778743: step 5499, loss 0.10678, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:44.988840: step 5500, loss 0.0607769, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:45.258066: step 5501, loss 0.186686, acc 0.90625, learning_rate 0.0001
2017-10-10T15:20:45.605544: step 5502, loss 0.0572796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:45.891090: step 5503, loss 0.0892263, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:46.225126: step 5504, loss 0.109219, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:46.481449: step 5505, loss 0.0373802, acc 1, learning_rate 0.0001
2017-10-10T15:20:46.831878: step 5506, loss 0.149081, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:47.169043: step 5507, loss 0.0492612, acc 1, learning_rate 0.0001
2017-10-10T15:20:47.457178: step 5508, loss 0.0262649, acc 1, learning_rate 0.0001
2017-10-10T15:20:47.748305: step 5509, loss 0.0658727, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:48.042472: step 5510, loss 0.152986, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:48.325069: step 5511, loss 0.178197, acc 0.90625, learning_rate 0.0001
2017-10-10T15:20:48.598077: step 5512, loss 0.114358, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:48.895050: step 5513, loss 0.0516723, acc 1, learning_rate 0.0001
2017-10-10T15:20:49.172629: step 5514, loss 0.0535465, acc 1, learning_rate 0.0001
2017-10-10T15:20:49.472970: step 5515, loss 0.123874, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:49.761837: step 5516, loss 0.0690071, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:50.060866: step 5517, loss 0.0706649, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:50.341495: step 5518, loss 0.0403951, acc 1, learning_rate 0.0001
2017-10-10T15:20:50.666388: step 5519, loss 0.142734, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:51.023966: step 5520, loss 0.101984, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:51.615139: step 5520, loss 0.225361, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5520

2017-10-10T15:20:52.707794: step 5521, loss 0.068817, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:53.063184: step 5522, loss 0.119253, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:53.350288: step 5523, loss 0.0279292, acc 1, learning_rate 0.0001
2017-10-10T15:20:53.585990: step 5524, loss 0.110836, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:53.907459: step 5525, loss 0.0818965, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:54.177133: step 5526, loss 0.0712548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:54.464046: step 5527, loss 0.064634, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:54.745100: step 5528, loss 0.116138, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.051940: step 5529, loss 0.0731874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:55.432911: step 5530, loss 0.138852, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:55.756859: step 5531, loss 0.133252, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:56.052997: step 5532, loss 0.113902, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:56.319087: step 5533, loss 0.078801, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:56.680794: step 5534, loss 0.0208836, acc 1, learning_rate 0.0001
2017-10-10T15:20:56.959517: step 5535, loss 0.148271, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:57.183137: step 5536, loss 0.0854835, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:57.465018: step 5537, loss 0.066146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:57.768809: step 5538, loss 0.0925708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:58.076954: step 5539, loss 0.0551561, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:58.359609: step 5540, loss 0.108718, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:58.655937: step 5541, loss 0.0291469, acc 1, learning_rate 0.0001
2017-10-10T15:20:58.908704: step 5542, loss 0.067995, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:59.167612: step 5543, loss 0.117621, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:59.478987: step 5544, loss 0.087212, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:59.709043: step 5545, loss 0.0666398, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:00.047919: step 5546, loss 0.143743, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:00.328861: step 5547, loss 0.0662588, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:00.655349: step 5548, loss 0.0391833, acc 1, learning_rate 0.0001
2017-10-10T15:21:00.827596: step 5549, loss 0.130577, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:01.144614: step 5550, loss 0.0974984, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:01.458737: step 5551, loss 0.330338, acc 0.90625, learning_rate 0.0001
2017-10-10T15:21:01.640905: step 5552, loss 0.0830939, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:01.977450: step 5553, loss 0.0746094, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:02.266367: step 5554, loss 0.0241116, acc 1, learning_rate 0.0001
2017-10-10T15:21:02.564485: step 5555, loss 0.0709051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:02.808144: step 5556, loss 0.0583664, acc 1, learning_rate 0.0001
2017-10-10T15:21:03.056870: step 5557, loss 0.0576732, acc 1, learning_rate 0.0001
2017-10-10T15:21:03.384840: step 5558, loss 0.0864368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:03.751588: step 5559, loss 0.242336, acc 0.875, learning_rate 0.0001
2017-10-10T15:21:04.073177: step 5560, loss 0.0418579, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:04.677828: step 5560, loss 0.227157, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5560

2017-10-10T15:21:05.844874: step 5561, loss 0.0441749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:06.145141: step 5562, loss 0.11517, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:06.440374: step 5563, loss 0.152105, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:06.700824: step 5564, loss 0.121983, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:07.030017: step 5565, loss 0.149926, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:07.326342: step 5566, loss 0.0783757, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:07.632837: step 5567, loss 0.114442, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:07.938312: step 5568, loss 0.11117, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:08.238769: step 5569, loss 0.0942882, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:08.519420: step 5570, loss 0.103669, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:08.812817: step 5571, loss 0.141793, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:09.147505: step 5572, loss 0.0911936, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:09.404827: step 5573, loss 0.0800449, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:09.653052: step 5574, loss 0.0858963, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:09.991350: step 5575, loss 0.0636852, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:10.261940: step 5576, loss 0.0564285, acc 1, learning_rate 0.0001
2017-10-10T15:21:10.576629: step 5577, loss 0.088211, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:10.857104: step 5578, loss 0.144681, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:11.217600: step 5579, loss 0.0857683, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:11.572829: step 5580, loss 0.0342991, acc 1, learning_rate 0.0001
2017-10-10T15:21:11.871499: step 5581, loss 0.144934, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:12.204797: step 5582, loss 0.0671886, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:12.504327: step 5583, loss 0.0575782, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:12.764839: step 5584, loss 0.0733333, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:13.128908: step 5585, loss 0.093025, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:13.327531: step 5586, loss 0.0736677, acc 0.980392, learning_rate 0.0001
2017-10-10T15:21:13.671888: step 5587, loss 0.0544542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:13.989697: step 5588, loss 0.207059, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:14.288885: step 5589, loss 0.155169, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:14.564906: step 5590, loss 0.0654931, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:14.836301: step 5591, loss 0.109305, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:15.196871: step 5592, loss 0.0725424, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:15.533432: step 5593, loss 0.136333, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:15.815017: step 5594, loss 0.0515226, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:16.066273: step 5595, loss 0.18212, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:16.370853: step 5596, loss 0.0622715, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:16.675555: step 5597, loss 0.0674522, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:17.032969: step 5598, loss 0.094496, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:17.278869: step 5599, loss 0.0629334, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:17.597010: step 5600, loss 0.119516, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:18.806868: step 5600, loss 0.226697, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5600

2017-10-10T15:21:19.708696: step 5601, loss 0.11771, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:20.019614: step 5602, loss 0.103734, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:20.441855: step 5603, loss 0.0551609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:20.807536: step 5604, loss 0.0313433, acc 1, learning_rate 0.0001
2017-10-10T15:21:21.124987: step 5605, loss 0.120161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:21.408294: step 5606, loss 0.200388, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:21.619080: step 5607, loss 0.0622633, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:21.939746: step 5608, loss 0.1246, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:22.312851: step 5609, loss 0.0536334, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:22.620738: step 5610, loss 0.042641, acc 1, learning_rate 0.0001
2017-10-10T15:21:22.890802: step 5611, loss 0.122954, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:23.184881: step 5612, loss 0.0909039, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:23.497011: step 5613, loss 0.204473, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:23.841772: step 5614, loss 0.102707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:24.140763: step 5615, loss 0.022956, acc 1, learning_rate 0.0001
2017-10-10T15:21:24.478647: step 5616, loss 0.0884569, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:24.806746: step 5617, loss 0.0570769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:25.140205: step 5618, loss 0.167043, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:25.460992: step 5619, loss 0.173121, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:25.713114: step 5620, loss 0.105835, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:26.040034: step 5621, loss 0.0644516, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:26.328916: step 5622, loss 0.0881765, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:26.603628: step 5623, loss 0.114384, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:26.977133: step 5624, loss 0.184038, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:27.280894: step 5625, loss 0.0678655, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:27.630533: step 5626, loss 0.161027, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:27.937544: step 5627, loss 0.104697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:28.280101: step 5628, loss 0.0862283, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:28.543249: step 5629, loss 0.0988731, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:28.861041: step 5630, loss 0.129659, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:29.143060: step 5631, loss 0.160994, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:29.424943: step 5632, loss 0.0640296, acc 1, learning_rate 0.0001
2017-10-10T15:21:29.727757: step 5633, loss 0.0272724, acc 1, learning_rate 0.0001
2017-10-10T15:21:30.003470: step 5634, loss 0.0820777, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:30.296954: step 5635, loss 0.151034, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:30.600935: step 5636, loss 0.0711211, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:30.914061: step 5637, loss 0.0518201, acc 1, learning_rate 0.0001
2017-10-10T15:21:31.176352: step 5638, loss 0.0554808, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:31.433954: step 5639, loss 0.0279966, acc 1, learning_rate 0.0001
2017-10-10T15:21:31.757298: step 5640, loss 0.138185, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:32.253048: step 5640, loss 0.225161, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5640

2017-10-10T15:21:33.346308: step 5641, loss 0.0889209, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:33.631469: step 5642, loss 0.0413895, acc 1, learning_rate 0.0001
2017-10-10T15:21:33.872995: step 5643, loss 0.0849145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:34.196888: step 5644, loss 0.118924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:34.492951: step 5645, loss 0.0429328, acc 1, learning_rate 0.0001
2017-10-10T15:21:34.877218: step 5646, loss 0.0969257, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:35.170119: step 5647, loss 0.157635, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:35.381220: step 5648, loss 0.0759007, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:35.688840: step 5649, loss 0.0693962, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:35.992921: step 5650, loss 0.140969, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:36.332655: step 5651, loss 0.0440646, acc 1, learning_rate 0.0001
2017-10-10T15:21:36.603762: step 5652, loss 0.135445, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:36.897981: step 5653, loss 0.0587539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:37.192813: step 5654, loss 0.118684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:37.428867: step 5655, loss 0.078875, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:37.747128: step 5656, loss 0.0898664, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:38.059443: step 5657, loss 0.0599704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:38.358209: step 5658, loss 0.0590351, acc 1, learning_rate 0.0001
2017-10-10T15:21:38.675917: step 5659, loss 0.170084, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:39.001631: step 5660, loss 0.118243, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:39.370996: step 5661, loss 0.0456497, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.621632: step 5662, loss 0.0682583, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:39.941891: step 5663, loss 0.0981551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:40.251300: step 5664, loss 0.110307, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:40.573010: step 5665, loss 0.0465326, acc 1, learning_rate 0.0001
2017-10-10T15:21:40.797174: step 5666, loss 0.124879, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:41.137045: step 5667, loss 0.12637, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:41.472862: step 5668, loss 0.0336359, acc 1, learning_rate 0.0001
2017-10-10T15:21:41.781801: step 5669, loss 0.0810695, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:42.040783: step 5670, loss 0.0624917, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:42.359656: step 5671, loss 0.0748629, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:42.649113: step 5672, loss 0.141496, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:43.013006: step 5673, loss 0.070517, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:43.382824: step 5674, loss 0.0529472, acc 1, learning_rate 0.0001
2017-10-10T15:21:43.714405: step 5675, loss 0.092277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:44.013671: step 5676, loss 0.0968866, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:44.265447: step 5677, loss 0.143195, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:44.559975: step 5678, loss 0.115669, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:44.880826: step 5679, loss 0.0975774, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:45.247040: step 5680, loss 0.0886008, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:45.692932: step 5680, loss 0.223556, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5680

2017-10-10T15:21:46.705482: step 5681, loss 0.0248672, acc 1, learning_rate 0.0001
2017-10-10T15:21:46.983986: step 5682, loss 0.0344734, acc 1, learning_rate 0.0001
2017-10-10T15:21:47.350820: step 5683, loss 0.0605835, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:47.616933: step 5684, loss 0.0960741, acc 0.980392, learning_rate 0.0001
2017-10-10T15:21:47.943591: step 5685, loss 0.198269, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:48.224130: step 5686, loss 0.0924552, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:48.536884: step 5687, loss 0.103571, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:48.839026: step 5688, loss 0.0630014, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:49.128828: step 5689, loss 0.127116, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:49.407089: step 5690, loss 0.0544438, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:49.684902: step 5691, loss 0.0858082, acc 1, learning_rate 0.0001
2017-10-10T15:21:49.993140: step 5692, loss 0.057644, acc 1, learning_rate 0.0001
2017-10-10T15:21:50.326687: step 5693, loss 0.154495, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:50.600019: step 5694, loss 0.0441849, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:50.861888: step 5695, loss 0.183753, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:51.165970: step 5696, loss 0.0899234, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:51.545059: step 5697, loss 0.139691, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:51.909538: step 5698, loss 0.107484, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:52.255364: step 5699, loss 0.108881, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:52.501121: step 5700, loss 0.0452837, acc 1, learning_rate 0.0001
2017-10-10T15:21:52.773937: step 5701, loss 0.0664219, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:53.073671: step 5702, loss 0.13523, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:53.297373: step 5703, loss 0.142846, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:53.620505: step 5704, loss 0.0402646, acc 1, learning_rate 0.0001
2017-10-10T15:21:53.940828: step 5705, loss 0.0556184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:54.248882: step 5706, loss 0.11532, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:54.656007: step 5707, loss 0.143683, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:54.934563: step 5708, loss 0.0811953, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:55.213155: step 5709, loss 0.177884, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:55.477986: step 5710, loss 0.0721169, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:55.719848: step 5711, loss 0.0946476, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:56.017116: step 5712, loss 0.0977199, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:56.326977: step 5713, loss 0.0658339, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:56.652836: step 5714, loss 0.208542, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:56.984942: step 5715, loss 0.0371818, acc 1, learning_rate 0.0001
2017-10-10T15:21:57.413046: step 5716, loss 0.0799971, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:57.656916: step 5717, loss 0.0770825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:57.855103: step 5718, loss 0.0663102, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:58.123983: step 5719, loss 0.111215, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:58.407579: step 5720, loss 0.202072, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:58.897407: step 5720, loss 0.224207, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5720

2017-10-10T15:22:00.361121: step 5721, loss 0.0626102, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:00.694127: step 5722, loss 0.0446827, acc 1, learning_rate 0.0001
2017-10-10T15:22:01.024832: step 5723, loss 0.081212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:01.387512: step 5724, loss 0.0663613, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:01.680864: step 5725, loss 0.101558, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:02.001148: step 5726, loss 0.0806227, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:02.368850: step 5727, loss 0.0376404, acc 1, learning_rate 0.0001
2017-10-10T15:22:02.712920: step 5728, loss 0.0598466, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:03.073029: step 5729, loss 0.167568, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:03.400505: step 5730, loss 0.135487, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:03.682789: step 5731, loss 0.198428, acc 0.90625, learning_rate 0.0001
2017-10-10T15:22:03.985099: step 5732, loss 0.0302223, acc 1, learning_rate 0.0001
2017-10-10T15:22:04.288903: step 5733, loss 0.0488805, acc 1, learning_rate 0.0001
2017-10-10T15:22:04.636844: step 5734, loss 0.0493953, acc 1, learning_rate 0.0001
2017-10-10T15:22:04.937033: step 5735, loss 0.0552999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:05.181311: step 5736, loss 0.125336, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:05.544925: step 5737, loss 0.0337905, acc 1, learning_rate 0.0001
2017-10-10T15:22:05.872228: step 5738, loss 0.133935, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:06.165691: step 5739, loss 0.0928173, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:06.421806: step 5740, loss 0.150842, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:06.787566: step 5741, loss 0.0817675, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:07.043103: step 5742, loss 0.0716134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:07.363937: step 5743, loss 0.0762316, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:07.660261: step 5744, loss 0.216586, acc 0.90625, learning_rate 0.0001
2017-10-10T15:22:07.981057: step 5745, loss 0.110013, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:08.326074: step 5746, loss 0.0754048, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:08.614118: step 5747, loss 0.147228, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:08.962017: step 5748, loss 0.160608, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:09.212497: step 5749, loss 0.0758708, acc 1, learning_rate 0.0001
2017-10-10T15:22:09.492824: step 5750, loss 0.0375022, acc 1, learning_rate 0.0001
2017-10-10T15:22:09.779712: step 5751, loss 0.14683, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:10.097007: step 5752, loss 0.116677, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:10.376811: step 5753, loss 0.0612796, acc 1, learning_rate 0.0001
2017-10-10T15:22:10.688948: step 5754, loss 0.0819591, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:10.986990: step 5755, loss 0.166986, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:11.270606: step 5756, loss 0.0509454, acc 1, learning_rate 0.0001
2017-10-10T15:22:11.588883: step 5757, loss 0.105592, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:11.861288: step 5758, loss 0.0729831, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:12.177247: step 5759, loss 0.0813283, acc 1, learning_rate 0.0001
2017-10-10T15:22:12.515166: step 5760, loss 0.0619968, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:13.193093: step 5760, loss 0.224532, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5760

2017-10-10T15:22:14.015939: step 5761, loss 0.0837829, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:14.310619: step 5762, loss 0.0788314, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:14.619816: step 5763, loss 0.0873798, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:14.957624: step 5764, loss 0.120192, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:15.257203: step 5765, loss 0.0895026, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:15.565078: step 5766, loss 0.12647, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:15.846336: step 5767, loss 0.121741, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:16.156303: step 5768, loss 0.107496, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:16.477498: step 5769, loss 0.0343719, acc 1, learning_rate 0.0001
2017-10-10T15:22:16.780916: step 5770, loss 0.110852, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:17.048876: step 5771, loss 0.0677912, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:17.384281: step 5772, loss 0.0524049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:17.720146: step 5773, loss 0.0886641, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:18.003259: step 5774, loss 0.062898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:18.257177: step 5775, loss 0.0637787, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:18.576588: step 5776, loss 0.0458016, acc 1, learning_rate 0.0001
2017-10-10T15:22:18.919703: step 5777, loss 0.0675206, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:19.265098: step 5778, loss 0.0591839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:19.572997: step 5779, loss 0.0779403, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:19.852092: step 5780, loss 0.127879, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:20.114649: step 5781, loss 0.104051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:20.376902: step 5782, loss 0.0667102, acc 0.980392, learning_rate 0.0001
2017-10-10T15:22:20.668897: step 5783, loss 0.0709685, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:20.993033: step 5784, loss 0.0724058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:21.294641: step 5785, loss 0.0611243, acc 1, learning_rate 0.0001
2017-10-10T15:22:21.571355: step 5786, loss 0.112797, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:21.932873: step 5787, loss 0.0470551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:22.241905: step 5788, loss 0.117776, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:22.448918: step 5789, loss 0.166781, acc 0.90625, learning_rate 0.0001
2017-10-10T15:22:22.734271: step 5790, loss 0.0800123, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:23.055515: step 5791, loss 0.163226, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:23.356860: step 5792, loss 0.0748807, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:23.665324: step 5793, loss 0.0603505, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:23.969102: step 5794, loss 0.156783, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:24.276966: step 5795, loss 0.0488269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:24.520924: step 5796, loss 0.100855, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:24.888857: step 5797, loss 0.0406772, acc 1, learning_rate 0.0001
2017-10-10T15:22:25.216480: step 5798, loss 0.0699563, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:25.450000: step 5799, loss 0.0959816, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:25.696530: step 5800, loss 0.0476185, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:26.192615: step 5800, loss 0.225042, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5800

2017-10-10T15:22:27.287658: step 5801, loss 0.092369, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:27.653909: step 5802, loss 0.117873, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:27.938785: step 5803, loss 0.10062, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:28.180968: step 5804, loss 0.0875665, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.501675: step 5805, loss 0.0374126, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.787857: step 5806, loss 0.0485269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:29.068963: step 5807, loss 0.182919, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:29.334718: step 5808, loss 0.0964718, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:29.687649: step 5809, loss 0.0897159, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:29.957572: step 5810, loss 0.131236, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:30.280503: step 5811, loss 0.0901166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:30.558144: step 5812, loss 0.0447567, acc 1, learning_rate 0.0001
2017-10-10T15:22:30.877233: step 5813, loss 0.0689915, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:31.199707: step 5814, loss 0.0675879, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:31.488843: step 5815, loss 0.0596048, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:31.805033: step 5816, loss 0.133952, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:32.040849: step 5817, loss 0.0427854, acc 1, learning_rate 0.0001
2017-10-10T15:22:32.303519: step 5818, loss 0.128192, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:32.634417: step 5819, loss 0.065829, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:32.976905: step 5820, loss 0.184858, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:33.281156: step 5821, loss 0.0765374, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:33.540541: step 5822, loss 0.187452, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:33.853132: step 5823, loss 0.0966687, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:34.157079: step 5824, loss 0.0898663, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:34.508531: step 5825, loss 0.108813, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:34.778385: step 5826, loss 0.0530986, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:35.104939: step 5827, loss 0.0350131, acc 1, learning_rate 0.0001
2017-10-10T15:22:35.373636: step 5828, loss 0.10252, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:35.756847: step 5829, loss 0.104166, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:36.048904: step 5830, loss 0.0724491, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:36.299434: step 5831, loss 0.106525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:36.640829: step 5832, loss 0.0765768, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:36.905550: step 5833, loss 0.15893, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:37.177380: step 5834, loss 0.109305, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:37.502241: step 5835, loss 0.0808596, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:37.804810: step 5836, loss 0.099855, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:38.091475: step 5837, loss 0.0836959, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:38.400833: step 5838, loss 0.0630347, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:38.709989: step 5839, loss 0.0836364, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:39.073461: step 5840, loss 0.0693171, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:39.580819: step 5840, loss 0.224835, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5840

2017-10-10T15:22:40.696690: step 5841, loss 0.0394928, acc 1, learning_rate 0.0001
2017-10-10T15:22:40.978595: step 5842, loss 0.0447414, acc 1, learning_rate 0.0001
2017-10-10T15:22:41.352928: step 5843, loss 0.181945, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:41.745021: step 5844, loss 0.148878, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:41.957890: step 5845, loss 0.0553696, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:42.204146: step 5846, loss 0.0530649, acc 1, learning_rate 0.0001
2017-10-10T15:22:42.444298: step 5847, loss 0.0491911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:42.686290: step 5848, loss 0.137955, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:42.949038: step 5849, loss 0.0341576, acc 1, learning_rate 0.0001
2017-10-10T15:22:43.160980: step 5850, loss 0.0907789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:43.423353: step 5851, loss 0.0513564, acc 1, learning_rate 0.0001
2017-10-10T15:22:43.709926: step 5852, loss 0.266052, acc 0.890625, learning_rate 0.0001
2017-10-10T15:22:43.958388: step 5853, loss 0.0836, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:44.227706: step 5854, loss 0.0776539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:44.506154: step 5855, loss 0.138221, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:44.718643: step 5856, loss 0.0360809, acc 1, learning_rate 0.0001
2017-10-10T15:22:44.940606: step 5857, loss 0.107273, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:45.246823: step 5858, loss 0.137659, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:45.532850: step 5859, loss 0.0425058, acc 1, learning_rate 0.0001
2017-10-10T15:22:45.781886: step 5860, loss 0.1633, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:46.023193: step 5861, loss 0.112048, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:46.271453: step 5862, loss 0.103093, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:46.496874: step 5863, loss 0.0814743, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:46.717744: step 5864, loss 0.160645, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:46.977601: step 5865, loss 0.027143, acc 1, learning_rate 0.0001
2017-10-10T15:22:47.207271: step 5866, loss 0.112699, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:47.496521: step 5867, loss 0.0736704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:47.716067: step 5868, loss 0.0742958, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:48.005028: step 5869, loss 0.049303, acc 1, learning_rate 0.0001
2017-10-10T15:22:48.311698: step 5870, loss 0.0424136, acc 1, learning_rate 0.0001
2017-10-10T15:22:48.527146: step 5871, loss 0.118823, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:48.752801: step 5872, loss 0.099465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:48.988255: step 5873, loss 0.138537, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:49.232610: step 5874, loss 0.123601, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:49.426908: step 5875, loss 0.105184, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:49.682357: step 5876, loss 0.145272, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:49.940380: step 5877, loss 0.0440624, acc 1, learning_rate 0.0001
2017-10-10T15:22:50.165296: step 5878, loss 0.0618133, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:50.447409: step 5879, loss 0.0646138, acc 1, learning_rate 0.0001
2017-10-10T15:22:50.657704: step 5880, loss 0.127302, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:51.103058: step 5880, loss 0.225631, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5880

2017-10-10T15:22:51.921119: step 5881, loss 0.0550152, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:52.148961: step 5882, loss 0.0656088, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:52.434531: step 5883, loss 0.0709988, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:52.724830: step 5884, loss 0.0680994, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:53.009593: step 5885, loss 0.0490125, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:53.268882: step 5886, loss 0.0742478, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:53.496203: step 5887, loss 0.0218754, acc 1, learning_rate 0.0001
2017-10-10T15:22:53.739272: step 5888, loss 0.100164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:53.976136: step 5889, loss 0.0532417, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:54.234786: step 5890, loss 0.0517116, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:54.489903: step 5891, loss 0.051858, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:54.764891: step 5892, loss 0.0611939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:55.000934: step 5893, loss 0.0581878, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:55.228884: step 5894, loss 0.0245723, acc 1, learning_rate 0.0001
2017-10-10T15:22:55.560967: step 5895, loss 0.02854, acc 1, learning_rate 0.0001
2017-10-10T15:22:55.850894: step 5896, loss 0.0404799, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:56.034459: step 5897, loss 0.102469, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:56.257068: step 5898, loss 0.0500641, acc 1, learning_rate 0.0001
2017-10-10T15:22:56.462511: step 5899, loss 0.0446495, acc 1, learning_rate 0.0001
2017-10-10T15:22:56.600495: step 5900, loss 0.0693196, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:56.797017: step 5901, loss 0.12974, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:57.047763: step 5902, loss 0.0988342, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:57.324250: step 5903, loss 0.0822527, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:57.552119: step 5904, loss 0.1374, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:57.776853: step 5905, loss 0.10392, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:58.058157: step 5906, loss 0.0510828, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:58.329293: step 5907, loss 0.0576573, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:58.536353: step 5908, loss 0.0820446, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:58.781523: step 5909, loss 0.0617112, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:59.040758: step 5910, loss 0.0879236, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:59.300841: step 5911, loss 0.0923389, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:59.528842: step 5912, loss 0.0891783, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:59.782449: step 5913, loss 0.0306327, acc 1, learning_rate 0.0001
2017-10-10T15:23:00.064273: step 5914, loss 0.111183, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:00.316956: step 5915, loss 0.0937538, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:00.515891: step 5916, loss 0.0978606, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:00.778635: step 5917, loss 0.0742082, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:01.026023: step 5918, loss 0.142929, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:01.309069: step 5919, loss 0.103003, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:01.522117: step 5920, loss 0.0932381, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:01.988945: step 5920, loss 0.225231, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5920

2017-10-10T15:23:03.066156: step 5921, loss 0.0785478, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:03.276097: step 5922, loss 0.0764917, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:03.470743: step 5923, loss 0.102822, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:03.681042: step 5924, loss 0.0491676, acc 1, learning_rate 0.0001
2017-10-10T15:23:03.868610: step 5925, loss 0.0777691, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:04.053041: step 5926, loss 0.204956, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:04.353012: step 5927, loss 0.114955, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:04.636139: step 5928, loss 0.0653999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:04.913194: step 5929, loss 0.0571851, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:05.159404: step 5930, loss 0.119729, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:05.406092: step 5931, loss 0.0520099, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:05.629474: step 5932, loss 0.125009, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:05.909702: step 5933, loss 0.111246, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:06.165909: step 5934, loss 0.0440273, acc 1, learning_rate 0.0001
2017-10-10T15:23:06.441124: step 5935, loss 0.127814, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:06.740607: step 5936, loss 0.0814837, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:06.947998: step 5937, loss 0.132041, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:07.160820: step 5938, loss 0.101886, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:07.367334: step 5939, loss 0.15503, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:07.620846: step 5940, loss 0.0751617, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:07.919223: step 5941, loss 0.170914, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:08.165137: step 5942, loss 0.079306, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:08.373754: step 5943, loss 0.0523937, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:08.622823: step 5944, loss 0.0285585, acc 1, learning_rate 0.0001
2017-10-10T15:23:08.924875: step 5945, loss 0.126056, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:09.197132: step 5946, loss 0.0605347, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:09.420843: step 5947, loss 0.0594548, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:09.613283: step 5948, loss 0.148448, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:09.850145: step 5949, loss 0.0781558, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:10.028897: step 5950, loss 0.0727568, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:10.198185: step 5951, loss 0.076567, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:10.482139: step 5952, loss 0.0716889, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:10.790499: step 5953, loss 0.0539227, acc 1, learning_rate 0.0001
2017-10-10T15:23:11.077159: step 5954, loss 0.134238, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:11.261038: step 5955, loss 0.0827446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:11.512086: step 5956, loss 0.0749513, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:11.768955: step 5957, loss 0.13334, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:11.993138: step 5958, loss 0.111275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:12.248822: step 5959, loss 0.0897006, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:12.549002: step 5960, loss 0.0383927, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:13.017042: step 5960, loss 0.225245, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-5960

2017-10-10T15:23:14.111943: step 5961, loss 0.105594, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:14.380857: step 5962, loss 0.0765063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:14.624354: step 5963, loss 0.101611, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:14.916237: step 5964, loss 0.0980362, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:15.191521: step 5965, loss 0.23028, acc 0.90625, learning_rate 0.0001
2017-10-10T15:23:15.437914: step 5966, loss 0.0524573, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:15.657100: step 5967, loss 0.0919252, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:15.900948: step 5968, loss 0.0617038, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:16.154560: step 5969, loss 0.0651171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:16.384897: step 5970, loss 0.0305094, acc 1, learning_rate 0.0001
2017-10-10T15:23:16.600337: step 5971, loss 0.0320542, acc 1, learning_rate 0.0001
2017-10-10T15:23:16.879321: step 5972, loss 0.102443, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:17.157037: step 5973, loss 0.042936, acc 1, learning_rate 0.0001
2017-10-10T15:23:17.340453: step 5974, loss 0.0441921, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:17.634495: step 5975, loss 0.0853317, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:17.918341: step 5976, loss 0.206272, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:18.126216: step 5977, loss 0.133965, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:18.318827: step 5978, loss 0.0484587, acc 0.980392, learning_rate 0.0001
2017-10-10T15:23:18.467018: step 5979, loss 0.103703, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:18.640793: step 5980, loss 0.0430175, acc 1, learning_rate 0.0001
2017-10-10T15:23:18.839685: step 5981, loss 0.187245, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:19.148975: step 5982, loss 0.0645254, acc 1, learning_rate 0.0001
2017-10-10T15:23:19.413408: step 5983, loss 0.0607916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:19.601140: step 5984, loss 0.0942583, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:19.884942: step 5985, loss 0.06204, acc 1, learning_rate 0.0001
2017-10-10T15:23:20.144862: step 5986, loss 0.157312, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:20.432801: step 5987, loss 0.1068, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:20.710632: step 5988, loss 0.0547419, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:20.929010: step 5989, loss 0.186313, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:21.156936: step 5990, loss 0.0747291, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:21.401318: step 5991, loss 0.0700175, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:21.684102: step 5992, loss 0.0783091, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:21.943315: step 5993, loss 0.110936, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:22.245338: step 5994, loss 0.0313072, acc 1, learning_rate 0.0001
2017-10-10T15:23:22.484849: step 5995, loss 0.114198, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:22.785603: step 5996, loss 0.147032, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:22.957280: step 5997, loss 0.0565157, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:23.139984: step 5998, loss 0.0596391, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:23.328441: step 5999, loss 0.116151, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:23.505442: step 6000, loss 0.0704188, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:23.855002: step 6000, loss 0.228643, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6000

2017-10-10T15:23:24.914710: step 6001, loss 0.0777676, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:25.137110: step 6002, loss 0.091639, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:25.361057: step 6003, loss 0.111422, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:25.643622: step 6004, loss 0.163905, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:25.873849: step 6005, loss 0.0772365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:26.112617: step 6006, loss 0.0767772, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:26.386332: step 6007, loss 0.0367436, acc 1, learning_rate 0.0001
2017-10-10T15:23:26.648214: step 6008, loss 0.0536164, acc 1, learning_rate 0.0001
2017-10-10T15:23:26.873196: step 6009, loss 0.0955863, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:27.188359: step 6010, loss 0.0751324, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:27.424762: step 6011, loss 0.109155, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:27.645359: step 6012, loss 0.0819191, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:27.859188: step 6013, loss 0.0952531, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:28.076497: step 6014, loss 0.0570206, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:28.356541: step 6015, loss 0.0518172, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:28.640892: step 6016, loss 0.0718551, acc 1, learning_rate 0.0001
2017-10-10T15:23:28.887996: step 6017, loss 0.140167, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.114131: step 6018, loss 0.09599, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.393642: step 6019, loss 0.0792821, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:29.658185: step 6020, loss 0.073385, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.932900: step 6021, loss 0.086784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.147286: step 6022, loss 0.0415645, acc 1, learning_rate 0.0001
2017-10-10T15:23:30.383699: step 6023, loss 0.0752909, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.668830: step 6024, loss 0.0473889, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.897075: step 6025, loss 0.0820243, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:31.167830: step 6026, loss 0.0583442, acc 1, learning_rate 0.0001
2017-10-10T15:23:31.408396: step 6027, loss 0.0932691, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:31.689654: step 6028, loss 0.12615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:31.966805: step 6029, loss 0.0985349, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:32.175745: step 6030, loss 0.0949253, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:32.524867: step 6031, loss 0.0719176, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:32.844221: step 6032, loss 0.13114, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:33.024814: step 6033, loss 0.129377, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:33.203492: step 6034, loss 0.0437571, acc 1, learning_rate 0.0001
2017-10-10T15:23:33.392958: step 6035, loss 0.0426913, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:33.572900: step 6036, loss 0.0978872, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:33.760853: step 6037, loss 0.0325913, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:33.988414: step 6038, loss 0.0984337, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:34.252109: step 6039, loss 0.0849214, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:34.536003: step 6040, loss 0.151041, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:34.975590: step 6040, loss 0.226202, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6040

2017-10-10T15:23:35.852851: step 6041, loss 0.0770777, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:36.197110: step 6042, loss 0.0655994, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:36.460590: step 6043, loss 0.0812427, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:36.701928: step 6044, loss 0.0777229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:36.922935: step 6045, loss 0.0889807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:37.070229: step 6046, loss 0.0762651, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:37.265767: step 6047, loss 0.0558887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:37.569355: step 6048, loss 0.0592985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:37.809187: step 6049, loss 0.0721639, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:38.019002: step 6050, loss 0.0882375, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:38.232921: step 6051, loss 0.0204764, acc 1, learning_rate 0.0001
2017-10-10T15:23:38.487085: step 6052, loss 0.0527816, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:38.737262: step 6053, loss 0.118646, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:38.954417: step 6054, loss 0.0939762, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:39.212537: step 6055, loss 0.0411079, acc 1, learning_rate 0.0001
2017-10-10T15:23:39.487926: step 6056, loss 0.0911566, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:39.734976: step 6057, loss 0.0377124, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:39.968806: step 6058, loss 0.0776992, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:40.253084: step 6059, loss 0.0361729, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:40.516402: step 6060, loss 0.0874122, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:40.774218: step 6061, loss 0.0424662, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:41.033158: step 6062, loss 0.130567, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:41.264853: step 6063, loss 0.108871, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:41.532834: step 6064, loss 0.185923, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:41.808828: step 6065, loss 0.115719, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.088841: step 6066, loss 0.0624029, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:42.344163: step 6067, loss 0.0829109, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.540983: step 6068, loss 0.0867731, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:42.828324: step 6069, loss 0.0507905, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:43.110511: step 6070, loss 0.0675473, acc 1, learning_rate 0.0001
2017-10-10T15:23:43.353199: step 6071, loss 0.0722805, acc 1, learning_rate 0.0001
2017-10-10T15:23:43.582704: step 6072, loss 0.133427, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:43.844353: step 6073, loss 0.0549529, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:44.106841: step 6074, loss 0.0689698, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:44.404937: step 6075, loss 0.0633156, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:44.648860: step 6076, loss 0.0685566, acc 0.980392, learning_rate 0.0001
2017-10-10T15:23:44.853700: step 6077, loss 0.131992, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:45.126266: step 6078, loss 0.0397026, acc 1, learning_rate 0.0001
2017-10-10T15:23:45.389095: step 6079, loss 0.0549075, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:45.644193: step 6080, loss 0.0431066, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:46.052811: step 6080, loss 0.224845, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6080

2017-10-10T15:23:47.118551: step 6081, loss 0.0957497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:47.399184: step 6082, loss 0.0366377, acc 1, learning_rate 0.0001
2017-10-10T15:23:47.632854: step 6083, loss 0.188151, acc 0.90625, learning_rate 0.0001
2017-10-10T15:23:47.895764: step 6084, loss 0.0862412, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:48.053237: step 6085, loss 0.0888566, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:48.214202: step 6086, loss 0.111116, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:48.457208: step 6087, loss 0.0941055, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:48.696852: step 6088, loss 0.0270451, acc 1, learning_rate 0.0001
2017-10-10T15:23:48.936857: step 6089, loss 0.022606, acc 1, learning_rate 0.0001
2017-10-10T15:23:49.208475: step 6090, loss 0.0971188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:49.460854: step 6091, loss 0.0979246, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:49.694074: step 6092, loss 0.097056, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:49.976897: step 6093, loss 0.146328, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:50.313018: step 6094, loss 0.0987508, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:50.543967: step 6095, loss 0.11011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:50.760859: step 6096, loss 0.0698127, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:50.952560: step 6097, loss 0.169405, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:51.172821: step 6098, loss 0.0949594, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:51.461571: step 6099, loss 0.116593, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:51.763058: step 6100, loss 0.0316016, acc 1, learning_rate 0.0001
2017-10-10T15:23:52.038491: step 6101, loss 0.0308331, acc 1, learning_rate 0.0001
2017-10-10T15:23:52.245194: step 6102, loss 0.142128, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:52.450615: step 6103, loss 0.104813, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:52.730649: step 6104, loss 0.0522926, acc 1, learning_rate 0.0001
2017-10-10T15:23:52.955145: step 6105, loss 0.045393, acc 1, learning_rate 0.0001
2017-10-10T15:23:53.212991: step 6106, loss 0.0311506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:53.442083: step 6107, loss 0.104954, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:53.732314: step 6108, loss 0.0735095, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:53.948933: step 6109, loss 0.0908277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:54.222066: step 6110, loss 0.0759863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:54.468923: step 6111, loss 0.0706918, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:54.704255: step 6112, loss 0.145155, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:54.946130: step 6113, loss 0.020951, acc 1, learning_rate 0.0001
2017-10-10T15:23:55.198102: step 6114, loss 0.0996403, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:55.473000: step 6115, loss 0.099667, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:55.744538: step 6116, loss 0.0999859, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:55.928848: step 6117, loss 0.106133, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:56.173689: step 6118, loss 0.153906, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:56.391501: step 6119, loss 0.103644, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:56.601601: step 6120, loss 0.0971522, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:57.060813: step 6120, loss 0.225413, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6120

2017-10-10T15:23:58.152960: step 6121, loss 0.0587747, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:58.369821: step 6122, loss 0.0694665, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:58.589295: step 6123, loss 0.0740873, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:58.826598: step 6124, loss 0.0653982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:59.113043: step 6125, loss 0.118497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:59.323592: step 6126, loss 0.0983188, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:59.561705: step 6127, loss 0.12585, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:59.807810: step 6128, loss 0.0313585, acc 1, learning_rate 0.0001
2017-10-10T15:24:00.083504: step 6129, loss 0.0691627, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:00.302794: step 6130, loss 0.0829176, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:00.571567: step 6131, loss 0.0848973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:00.828379: step 6132, loss 0.101832, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:01.085543: step 6133, loss 0.122506, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:01.305840: step 6134, loss 0.120217, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:01.558042: step 6135, loss 0.0736122, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:01.845203: step 6136, loss 0.0940149, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:02.165108: step 6137, loss 0.173126, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:02.409847: step 6138, loss 0.0782787, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:02.613143: step 6139, loss 0.0766849, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:02.796852: step 6140, loss 0.0727018, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:03.005395: step 6141, loss 0.0754739, acc 1, learning_rate 0.0001
2017-10-10T15:24:03.238241: step 6142, loss 0.0922179, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:03.572906: step 6143, loss 0.0727451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:03.866077: step 6144, loss 0.181744, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:04.031810: step 6145, loss 0.147179, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:04.244907: step 6146, loss 0.131835, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:04.465092: step 6147, loss 0.154751, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:04.640126: step 6148, loss 0.0495455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:04.894894: step 6149, loss 0.0639423, acc 1, learning_rate 0.0001
2017-10-10T15:24:05.169158: step 6150, loss 0.0799044, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:05.432223: step 6151, loss 0.0766162, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:05.668460: step 6152, loss 0.0692727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:05.920810: step 6153, loss 0.0745438, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:06.204852: step 6154, loss 0.0802368, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:06.476122: step 6155, loss 0.0397009, acc 1, learning_rate 0.0001
2017-10-10T15:24:06.668098: step 6156, loss 0.0519514, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:06.905755: step 6157, loss 0.0439389, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:07.169415: step 6158, loss 0.0700401, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:07.392360: step 6159, loss 0.0724771, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:07.640638: step 6160, loss 0.07736, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:08.018225: step 6160, loss 0.223867, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6160

2017-10-10T15:24:08.913535: step 6161, loss 0.0590303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:09.178887: step 6162, loss 0.0474138, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:09.448866: step 6163, loss 0.0203982, acc 1, learning_rate 0.0001
2017-10-10T15:24:09.671468: step 6164, loss 0.121011, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:09.943254: step 6165, loss 0.152223, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:10.191791: step 6166, loss 0.0671766, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:10.470160: step 6167, loss 0.0581607, acc 1, learning_rate 0.0001
2017-10-10T15:24:10.694045: step 6168, loss 0.0236348, acc 1, learning_rate 0.0001
2017-10-10T15:24:10.938933: step 6169, loss 0.0778537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:11.194282: step 6170, loss 0.0814026, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:11.470172: step 6171, loss 0.154538, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:11.708918: step 6172, loss 0.100643, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:11.943508: step 6173, loss 0.0476969, acc 1, learning_rate 0.0001
2017-10-10T15:24:12.176544: step 6174, loss 0.0561027, acc 0.980392, learning_rate 0.0001
2017-10-10T15:24:12.453524: step 6175, loss 0.0962755, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:12.728839: step 6176, loss 0.0710842, acc 1, learning_rate 0.0001
2017-10-10T15:24:12.927953: step 6177, loss 0.0600404, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:13.180855: step 6178, loss 0.116516, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:13.472975: step 6179, loss 0.114973, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:13.744851: step 6180, loss 0.13673, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:14.000974: step 6181, loss 0.0903227, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:14.248020: step 6182, loss 0.0425985, acc 1, learning_rate 0.0001
2017-10-10T15:24:14.507304: step 6183, loss 0.104138, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:14.816892: step 6184, loss 0.114998, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:15.041743: step 6185, loss 0.190612, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:15.263283: step 6186, loss 0.0972635, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:15.538596: step 6187, loss 0.180887, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:15.773042: step 6188, loss 0.100163, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:15.984881: step 6189, loss 0.0652115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:16.245503: step 6190, loss 0.0925921, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:16.540271: step 6191, loss 0.0470306, acc 1, learning_rate 0.0001
2017-10-10T15:24:16.776709: step 6192, loss 0.0450933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:17.047041: step 6193, loss 0.051597, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:17.360763: step 6194, loss 0.143911, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:17.603788: step 6195, loss 0.0773768, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:17.761698: step 6196, loss 0.0949129, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:17.905577: step 6197, loss 0.0820976, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:18.041356: step 6198, loss 0.0553062, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:18.170679: step 6199, loss 0.0518695, acc 1, learning_rate 0.0001
2017-10-10T15:24:18.440919: step 6200, loss 0.0550183, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:18.836970: step 6200, loss 0.223845, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6200

2017-10-10T15:24:19.837009: step 6201, loss 0.0952379, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:20.123424: step 6202, loss 0.0861467, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:20.384439: step 6203, loss 0.0835205, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:20.641465: step 6204, loss 0.0701219, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:20.904899: step 6205, loss 0.0536906, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:21.165065: step 6206, loss 0.0719761, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:21.416437: step 6207, loss 0.124496, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:21.631109: step 6208, loss 0.0717608, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:21.916330: step 6209, loss 0.110322, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:22.208122: step 6210, loss 0.0643028, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:22.452624: step 6211, loss 0.0839494, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:22.704867: step 6212, loss 0.0382442, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:22.924809: step 6213, loss 0.130389, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:23.169541: step 6214, loss 0.0859791, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:23.418912: step 6215, loss 0.0826543, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:23.647519: step 6216, loss 0.126192, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:23.907638: step 6217, loss 0.101539, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:24.155706: step 6218, loss 0.0563645, acc 1, learning_rate 0.0001
2017-10-10T15:24:24.412006: step 6219, loss 0.0623849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:24.666614: step 6220, loss 0.0594212, acc 1, learning_rate 0.0001
2017-10-10T15:24:24.948942: step 6221, loss 0.112519, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:25.232452: step 6222, loss 0.0562106, acc 1, learning_rate 0.0001
2017-10-10T15:24:25.475607: step 6223, loss 0.0741805, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:25.676959: step 6224, loss 0.0505364, acc 1, learning_rate 0.0001
2017-10-10T15:24:25.968785: step 6225, loss 0.0762437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:26.200865: step 6226, loss 0.0621987, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:26.388853: step 6227, loss 0.0650723, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:26.690578: step 6228, loss 0.0991172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:26.968913: step 6229, loss 0.147917, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:27.223155: step 6230, loss 0.0682673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:27.446969: step 6231, loss 0.14839, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:27.690425: step 6232, loss 0.0636629, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:27.948626: step 6233, loss 0.111461, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:28.196069: step 6234, loss 0.0414374, acc 1, learning_rate 0.0001
2017-10-10T15:24:28.452838: step 6235, loss 0.0360685, acc 1, learning_rate 0.0001
2017-10-10T15:24:28.691541: step 6236, loss 0.0960664, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:29.005919: step 6237, loss 0.0657298, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:29.212400: step 6238, loss 0.0602198, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:29.442930: step 6239, loss 0.0296295, acc 1, learning_rate 0.0001
2017-10-10T15:24:29.684433: step 6240, loss 0.0997914, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:30.111185: step 6240, loss 0.223602, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6240

2017-10-10T15:24:31.212451: step 6241, loss 0.0381407, acc 1, learning_rate 0.0001
2017-10-10T15:24:31.421605: step 6242, loss 0.104853, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:31.654703: step 6243, loss 0.0740104, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:31.912866: step 6244, loss 0.0903004, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:32.205046: step 6245, loss 0.0643131, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:32.422865: step 6246, loss 0.0521141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:32.632989: step 6247, loss 0.0958815, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:32.848874: step 6248, loss 0.105163, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:33.013774: step 6249, loss 0.100156, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:33.211906: step 6250, loss 0.11649, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:33.506413: step 6251, loss 0.0460998, acc 1, learning_rate 0.0001
2017-10-10T15:24:33.749121: step 6252, loss 0.074319, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:34.025030: step 6253, loss 0.0652795, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:34.260864: step 6254, loss 0.155516, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:34.496815: step 6255, loss 0.149903, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:34.836914: step 6256, loss 0.15127, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:35.036414: step 6257, loss 0.0718757, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:35.256195: step 6258, loss 0.0339633, acc 1, learning_rate 0.0001
2017-10-10T15:24:35.536334: step 6259, loss 0.0258076, acc 1, learning_rate 0.0001
2017-10-10T15:24:35.812837: step 6260, loss 0.0405479, acc 1, learning_rate 0.0001
2017-10-10T15:24:36.032922: step 6261, loss 0.100534, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:36.284866: step 6262, loss 0.10372, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:36.555740: step 6263, loss 0.0386649, acc 1, learning_rate 0.0001
2017-10-10T15:24:36.812866: step 6264, loss 0.0950969, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:37.076414: step 6265, loss 0.120026, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:37.316960: step 6266, loss 0.101219, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:37.556850: step 6267, loss 0.0698205, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:37.840896: step 6268, loss 0.0635952, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:38.097004: step 6269, loss 0.161053, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:38.301122: step 6270, loss 0.122927, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:38.579235: step 6271, loss 0.0895008, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:38.804413: step 6272, loss 0.0808658, acc 0.980392, learning_rate 0.0001
2017-10-10T15:24:39.053032: step 6273, loss 0.0509161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:39.312963: step 6274, loss 0.11007, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:39.574224: step 6275, loss 0.0702504, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:39.789766: step 6276, loss 0.0713676, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:39.977706: step 6277, loss 0.0903871, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:40.236851: step 6278, loss 0.135603, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:40.511972: step 6279, loss 0.0924738, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:40.771089: step 6280, loss 0.127622, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:41.173419: step 6280, loss 0.223854, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6280

2017-10-10T15:24:42.228891: step 6281, loss 0.138455, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:42.461315: step 6282, loss 0.0472288, acc 1, learning_rate 0.0001
2017-10-10T15:24:42.689099: step 6283, loss 0.106631, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:42.960874: step 6284, loss 0.0381629, acc 1, learning_rate 0.0001
2017-10-10T15:24:43.236885: step 6285, loss 0.098217, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:43.467671: step 6286, loss 0.0375066, acc 1, learning_rate 0.0001
2017-10-10T15:24:43.758034: step 6287, loss 0.058087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:44.062318: step 6288, loss 0.160719, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:44.312934: step 6289, loss 0.0759436, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:44.589086: step 6290, loss 0.0733276, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:44.844520: step 6291, loss 0.0715852, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:45.032824: step 6292, loss 0.0442069, acc 1, learning_rate 0.0001
2017-10-10T15:24:45.208500: step 6293, loss 0.0369957, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:45.428205: step 6294, loss 0.112233, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:45.639019: step 6295, loss 0.0364396, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:45.884855: step 6296, loss 0.0615449, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:46.088775: step 6297, loss 0.120926, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:46.342323: step 6298, loss 0.0680823, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:46.605731: step 6299, loss 0.143871, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:46.871310: step 6300, loss 0.0738027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:47.169044: step 6301, loss 0.0983582, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:47.461772: step 6302, loss 0.032738, acc 1, learning_rate 0.0001
2017-10-10T15:24:47.648542: step 6303, loss 0.0681335, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:47.854439: step 6304, loss 0.0370162, acc 1, learning_rate 0.0001
2017-10-10T15:24:48.060846: step 6305, loss 0.0553334, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:48.272869: step 6306, loss 0.137442, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:48.508483: step 6307, loss 0.0881308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:48.728828: step 6308, loss 0.0622087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:49.008816: step 6309, loss 0.0652773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:49.322503: step 6310, loss 0.0529231, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:49.587773: step 6311, loss 0.0573417, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:49.793227: step 6312, loss 0.0524051, acc 1, learning_rate 0.0001
2017-10-10T15:24:50.010716: step 6313, loss 0.0256827, acc 1, learning_rate 0.0001
2017-10-10T15:24:50.228938: step 6314, loss 0.0964612, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:50.516968: step 6315, loss 0.149446, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:50.782277: step 6316, loss 0.0891712, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:51.021396: step 6317, loss 0.130134, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:51.257518: step 6318, loss 0.0763063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:51.528365: step 6319, loss 0.0290455, acc 1, learning_rate 0.0001
2017-10-10T15:24:51.824755: step 6320, loss 0.0234971, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:52.277727: step 6320, loss 0.224468, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6320

2017-10-10T15:24:53.224932: step 6321, loss 0.0349497, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:53.452247: step 6322, loss 0.0919608, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:53.656843: step 6323, loss 0.0940179, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:53.899969: step 6324, loss 0.132785, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:54.180869: step 6325, loss 0.0529688, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:54.416755: step 6326, loss 0.0919864, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:54.688650: step 6327, loss 0.11425, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:54.940670: step 6328, loss 0.0675551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:55.149174: step 6329, loss 0.0484042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:55.397274: step 6330, loss 0.055222, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:55.642257: step 6331, loss 0.101885, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:55.875722: step 6332, loss 0.124276, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.199109: step 6333, loss 0.115586, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:56.437021: step 6334, loss 0.106896, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.705053: step 6335, loss 0.0700469, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:56.972892: step 6336, loss 0.0429938, acc 1, learning_rate 0.0001
2017-10-10T15:24:57.212838: step 6337, loss 0.0766115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:57.486590: step 6338, loss 0.108939, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:57.772831: step 6339, loss 0.0697996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:58.017108: step 6340, loss 0.0742273, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:58.285940: step 6341, loss 0.0432986, acc 1, learning_rate 0.0001
2017-10-10T15:24:58.580303: step 6342, loss 0.0706289, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:58.795981: step 6343, loss 0.138223, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:59.037651: step 6344, loss 0.0722139, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:59.206103: step 6345, loss 0.0367624, acc 1, learning_rate 0.0001
2017-10-10T15:24:59.441103: step 6346, loss 0.0555618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:59.720764: step 6347, loss 0.0788376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:59.968812: step 6348, loss 0.0906495, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:00.199885: step 6349, loss 0.0782331, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:00.397446: step 6350, loss 0.122846, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:00.600076: step 6351, loss 0.132741, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:00.858981: step 6352, loss 0.10404, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:01.100077: step 6353, loss 0.133695, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:01.328831: step 6354, loss 0.136098, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:01.596949: step 6355, loss 0.0803176, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:01.896925: step 6356, loss 0.172794, acc 0.890625, learning_rate 0.0001
2017-10-10T15:25:02.145048: step 6357, loss 0.0641279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:02.437534: step 6358, loss 0.0940774, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:02.640257: step 6359, loss 0.0852731, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:02.853806: step 6360, loss 0.0957039, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:03.257353: step 6360, loss 0.225285, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6360

2017-10-10T15:25:04.384843: step 6361, loss 0.18806, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:04.689131: step 6362, loss 0.0553054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:04.969902: step 6363, loss 0.184243, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:05.264894: step 6364, loss 0.11926, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:05.461444: step 6365, loss 0.106972, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:05.705090: step 6366, loss 0.0252677, acc 1, learning_rate 0.0001
2017-10-10T15:25:05.946446: step 6367, loss 0.076258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:06.166223: step 6368, loss 0.0643578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:06.432437: step 6369, loss 0.0959783, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:06.625001: step 6370, loss 0.042988, acc 0.980392, learning_rate 0.0001
2017-10-10T15:25:06.839243: step 6371, loss 0.14017, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:07.107361: step 6372, loss 0.0952055, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:07.372824: step 6373, loss 0.0414988, acc 1, learning_rate 0.0001
2017-10-10T15:25:07.586951: step 6374, loss 0.0595446, acc 1, learning_rate 0.0001
2017-10-10T15:25:07.832087: step 6375, loss 0.114446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:08.118167: step 6376, loss 0.144715, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:08.390482: step 6377, loss 0.131798, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:08.579155: step 6378, loss 0.0677073, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:08.826636: step 6379, loss 0.0555704, acc 1, learning_rate 0.0001
2017-10-10T15:25:09.120820: step 6380, loss 0.0968718, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:09.381913: step 6381, loss 0.093808, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:09.589137: step 6382, loss 0.0977302, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:09.826812: step 6383, loss 0.0715727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:10.100901: step 6384, loss 0.0539125, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:10.406318: step 6385, loss 0.122739, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:10.628035: step 6386, loss 0.0475413, acc 1, learning_rate 0.0001
2017-10-10T15:25:10.822968: step 6387, loss 0.11587, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:11.048000: step 6388, loss 0.0974845, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:11.293586: step 6389, loss 0.0615468, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:11.530234: step 6390, loss 0.109285, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:11.828858: step 6391, loss 0.0422129, acc 1, learning_rate 0.0001
2017-10-10T15:25:12.153637: step 6392, loss 0.0988154, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:12.359373: step 6393, loss 0.0242321, acc 1, learning_rate 0.0001
2017-10-10T15:25:12.540152: step 6394, loss 0.0501994, acc 1, learning_rate 0.0001
2017-10-10T15:25:12.772122: step 6395, loss 0.0987834, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:12.992806: step 6396, loss 0.357188, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:13.233885: step 6397, loss 0.0658065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:13.441745: step 6398, loss 0.0705373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:13.703335: step 6399, loss 0.0430788, acc 1, learning_rate 0.0001
2017-10-10T15:25:13.975942: step 6400, loss 0.150604, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:14.384270: step 6400, loss 0.226017, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6400

2017-10-10T15:25:15.390883: step 6401, loss 0.145, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:15.659671: step 6402, loss 0.0736801, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:15.889026: step 6403, loss 0.15013, acc 0.90625, learning_rate 0.0001
2017-10-10T15:25:16.151542: step 6404, loss 0.0576778, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:16.410391: step 6405, loss 0.137737, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:16.724906: step 6406, loss 0.0511647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:17.033053: step 6407, loss 0.169154, acc 0.90625, learning_rate 0.0001
2017-10-10T15:25:17.207474: step 6408, loss 0.0557057, acc 1, learning_rate 0.0001
2017-10-10T15:25:17.426261: step 6409, loss 0.0263505, acc 1, learning_rate 0.0001
2017-10-10T15:25:17.622533: step 6410, loss 0.105281, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:17.848891: step 6411, loss 0.0744645, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:18.076129: step 6412, loss 0.0180804, acc 1, learning_rate 0.0001
2017-10-10T15:25:18.288773: step 6413, loss 0.111548, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:18.565323: step 6414, loss 0.0358413, acc 1, learning_rate 0.0001
2017-10-10T15:25:18.767555: step 6415, loss 0.107587, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:19.030104: step 6416, loss 0.0951947, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:19.300897: step 6417, loss 0.0473896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:19.506308: step 6418, loss 0.267151, acc 0.90625, learning_rate 0.0001
2017-10-10T15:25:19.740249: step 6419, loss 0.0598532, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:20.004682: step 6420, loss 0.117825, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:20.205955: step 6421, loss 0.0366031, acc 1, learning_rate 0.0001
2017-10-10T15:25:20.461680: step 6422, loss 0.0803384, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:20.786624: step 6423, loss 0.0759088, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:21.047470: step 6424, loss 0.0346903, acc 1, learning_rate 0.0001
2017-10-10T15:25:21.271129: step 6425, loss 0.156345, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:21.465396: step 6426, loss 0.080816, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:21.721144: step 6427, loss 0.0323383, acc 1, learning_rate 0.0001
2017-10-10T15:25:21.964503: step 6428, loss 0.0438247, acc 1, learning_rate 0.0001
2017-10-10T15:25:22.200954: step 6429, loss 0.0275585, acc 1, learning_rate 0.0001
2017-10-10T15:25:22.468863: step 6430, loss 0.0419976, acc 1, learning_rate 0.0001
2017-10-10T15:25:22.748355: step 6431, loss 0.171452, acc 0.90625, learning_rate 0.0001
2017-10-10T15:25:23.001068: step 6432, loss 0.0329427, acc 1, learning_rate 0.0001
2017-10-10T15:25:23.205005: step 6433, loss 0.036029, acc 1, learning_rate 0.0001
2017-10-10T15:25:23.475795: step 6434, loss 0.0243536, acc 1, learning_rate 0.0001
2017-10-10T15:25:23.733345: step 6435, loss 0.0975391, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:23.961322: step 6436, loss 0.0964057, acc 1, learning_rate 0.0001
2017-10-10T15:25:24.208834: step 6437, loss 0.13439, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:24.481732: step 6438, loss 0.0714886, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:24.680420: step 6439, loss 0.0476922, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:24.889772: step 6440, loss 0.0684507, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:25.293696: step 6440, loss 0.222397, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6440

2017-10-10T15:25:26.190372: step 6441, loss 0.0590449, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:26.403262: step 6442, loss 0.0881964, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:26.568887: step 6443, loss 0.0995615, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:26.760848: step 6444, loss 0.111013, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:27.034169: step 6445, loss 0.10998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:27.337104: step 6446, loss 0.0547531, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:27.598247: step 6447, loss 0.0241773, acc 1, learning_rate 0.0001
2017-10-10T15:25:27.813112: step 6448, loss 0.0648983, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:28.045177: step 6449, loss 0.0691902, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:28.317745: step 6450, loss 0.0995803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:28.543269: step 6451, loss 0.0854053, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:28.822521: step 6452, loss 0.0426558, acc 1, learning_rate 0.0001
2017-10-10T15:25:29.091873: step 6453, loss 0.0896392, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:29.373589: step 6454, loss 0.0583083, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:29.644194: step 6455, loss 0.112752, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:29.881076: step 6456, loss 0.0433014, acc 1, learning_rate 0.0001
2017-10-10T15:25:30.157048: step 6457, loss 0.0717339, acc 1, learning_rate 0.0001
2017-10-10T15:25:30.402737: step 6458, loss 0.042717, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:30.692359: step 6459, loss 0.0947094, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:30.955327: step 6460, loss 0.0851959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:31.185052: step 6461, loss 0.133155, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:31.421077: step 6462, loss 0.105635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:31.683333: step 6463, loss 0.0729679, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:31.932865: step 6464, loss 0.0888467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:32.231299: step 6465, loss 0.067789, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:32.437796: step 6466, loss 0.0549693, acc 1, learning_rate 0.0001
2017-10-10T15:25:32.628301: step 6467, loss 0.0612565, acc 1, learning_rate 0.0001
2017-10-10T15:25:32.796849: step 6468, loss 0.0411689, acc 0.980392, learning_rate 0.0001
2017-10-10T15:25:32.995977: step 6469, loss 0.0492586, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:33.248855: step 6470, loss 0.167288, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:33.535701: step 6471, loss 0.0488236, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:33.776820: step 6472, loss 0.0890944, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:34.061518: step 6473, loss 0.0576401, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:34.304740: step 6474, loss 0.0557647, acc 1, learning_rate 0.0001
2017-10-10T15:25:34.518404: step 6475, loss 0.0907158, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:34.751539: step 6476, loss 0.0668402, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:35.046269: step 6477, loss 0.130475, acc 0.890625, learning_rate 0.0001
2017-10-10T15:25:35.319464: step 6478, loss 0.0610867, acc 1, learning_rate 0.0001
2017-10-10T15:25:35.533010: step 6479, loss 0.0453246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:35.772835: step 6480, loss 0.229487, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:36.198723: step 6480, loss 0.223126, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6480

2017-10-10T15:25:37.303620: step 6481, loss 0.0988795, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:37.568197: step 6482, loss 0.106991, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:37.811298: step 6483, loss 0.135761, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:38.069144: step 6484, loss 0.044071, acc 1, learning_rate 0.0001
2017-10-10T15:25:38.328919: step 6485, loss 0.0675803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:38.579493: step 6486, loss 0.12658, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:38.834717: step 6487, loss 0.0933457, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:39.104265: step 6488, loss 0.103327, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:39.340276: step 6489, loss 0.13776, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:39.602025: step 6490, loss 0.0737242, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:39.812880: step 6491, loss 0.10683, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:40.010924: step 6492, loss 0.0781243, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:40.222750: step 6493, loss 0.0900141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:40.426390: step 6494, loss 0.103426, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:40.685132: step 6495, loss 0.0733932, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:40.919364: step 6496, loss 0.208411, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:41.193616: step 6497, loss 0.0552461, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:41.496880: step 6498, loss 0.0780988, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:41.784222: step 6499, loss 0.0931484, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:41.957139: step 6500, loss 0.0315366, acc 1, learning_rate 0.0001
2017-10-10T15:25:42.222377: step 6501, loss 0.063345, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:42.432950: step 6502, loss 0.0818804, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:42.653240: step 6503, loss 0.0734642, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:42.889308: step 6504, loss 0.0972898, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:43.097717: step 6505, loss 0.137643, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:43.364993: step 6506, loss 0.0995999, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:43.632232: step 6507, loss 0.102762, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:43.820122: step 6508, loss 0.0984944, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:44.061110: step 6509, loss 0.109371, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:44.332853: step 6510, loss 0.114923, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:44.572517: step 6511, loss 0.097245, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:44.868980: step 6512, loss 0.11359, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:45.139663: step 6513, loss 0.105394, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:45.395945: step 6514, loss 0.0763597, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:45.670024: step 6515, loss 0.0716356, acc 1, learning_rate 0.0001
2017-10-10T15:25:45.903546: step 6516, loss 0.0303356, acc 1, learning_rate 0.0001
2017-10-10T15:25:46.152063: step 6517, loss 0.143395, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:46.371958: step 6518, loss 0.103558, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:46.648926: step 6519, loss 0.0483849, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:46.950839: step 6520, loss 0.0967727, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:47.352391: step 6520, loss 0.22193, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6520

2017-10-10T15:25:48.284739: step 6521, loss 0.0746109, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:48.562099: step 6522, loss 0.104131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:48.824693: step 6523, loss 0.049836, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:49.126569: step 6524, loss 0.15934, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:49.369279: step 6525, loss 0.0901752, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:49.636414: step 6526, loss 0.122632, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:49.869046: step 6527, loss 0.0744708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:50.095862: step 6528, loss 0.0580463, acc 1, learning_rate 0.0001
2017-10-10T15:25:50.313665: step 6529, loss 0.0414655, acc 1, learning_rate 0.0001
2017-10-10T15:25:50.626663: step 6530, loss 0.0550668, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:50.892622: step 6531, loss 0.0657692, acc 1, learning_rate 0.0001
2017-10-10T15:25:51.125145: step 6532, loss 0.131211, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:51.342979: step 6533, loss 0.0935857, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:51.598385: step 6534, loss 0.0847116, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:51.864860: step 6535, loss 0.0433803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:52.138169: step 6536, loss 0.0766418, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:52.336898: step 6537, loss 0.041786, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:52.561073: step 6538, loss 0.0489457, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:52.854283: step 6539, loss 0.0391885, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:53.137006: step 6540, loss 0.10638, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:53.360147: step 6541, loss 0.0464287, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:53.591448: step 6542, loss 0.0480294, acc 1, learning_rate 0.0001
2017-10-10T15:25:53.828165: step 6543, loss 0.0759555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:54.008906: step 6544, loss 0.0547537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:54.276842: step 6545, loss 0.0777862, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:54.516381: step 6546, loss 0.0333373, acc 1, learning_rate 0.0001
2017-10-10T15:25:54.761663: step 6547, loss 0.0403948, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:55.005184: step 6548, loss 0.0339196, acc 1, learning_rate 0.0001
2017-10-10T15:25:55.232877: step 6549, loss 0.0711469, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:55.514786: step 6550, loss 0.0710031, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:55.804897: step 6551, loss 0.0554158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:56.000895: step 6552, loss 0.153724, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:56.249030: step 6553, loss 0.0705329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:56.509080: step 6554, loss 0.0362951, acc 1, learning_rate 0.0001
2017-10-10T15:25:56.769250: step 6555, loss 0.0571889, acc 1, learning_rate 0.0001
2017-10-10T15:25:57.072837: step 6556, loss 0.0468075, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:57.316573: step 6557, loss 0.0500597, acc 1, learning_rate 0.0001
2017-10-10T15:25:57.590266: step 6558, loss 0.0433704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:57.843661: step 6559, loss 0.0469761, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:58.084977: step 6560, loss 0.0659214, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:58.549002: step 6560, loss 0.223308, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6560

2017-10-10T15:25:59.623295: step 6561, loss 0.0674087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:59.859584: step 6562, loss 0.160218, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:00.116913: step 6563, loss 0.0757249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:00.384865: step 6564, loss 0.137738, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:00.632577: step 6565, loss 0.0920051, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:00.835143: step 6566, loss 0.0543315, acc 1, learning_rate 0.0001
2017-10-10T15:26:01.097406: step 6567, loss 0.12537, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:01.356839: step 6568, loss 0.100671, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:01.663829: step 6569, loss 0.0787004, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:02.013572: step 6570, loss 0.0933833, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:02.180843: step 6571, loss 0.0969051, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:02.333066: step 6572, loss 0.0489235, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:02.476798: step 6573, loss 0.108257, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:02.659801: step 6574, loss 0.0621571, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:02.864926: step 6575, loss 0.0498993, acc 1, learning_rate 0.0001
2017-10-10T15:26:03.132164: step 6576, loss 0.0673151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:03.399389: step 6577, loss 0.105567, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:03.671597: step 6578, loss 0.0750206, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:03.904994: step 6579, loss 0.0690528, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:04.126120: step 6580, loss 0.110968, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:04.399887: step 6581, loss 0.0721698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:04.656839: step 6582, loss 0.0827964, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:04.904991: step 6583, loss 0.0607641, acc 1, learning_rate 0.0001
2017-10-10T15:26:05.162492: step 6584, loss 0.0513923, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:05.460842: step 6585, loss 0.0671673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:05.759326: step 6586, loss 0.0495922, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:06.024863: step 6587, loss 0.0570058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:06.251016: step 6588, loss 0.0916699, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:06.529034: step 6589, loss 0.0759618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:06.802343: step 6590, loss 0.0558242, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:06.958586: step 6591, loss 0.162578, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:07.172903: step 6592, loss 0.0895114, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:07.377218: step 6593, loss 0.0321552, acc 1, learning_rate 0.0001
2017-10-10T15:26:07.554173: step 6594, loss 0.0871643, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:07.796936: step 6595, loss 0.123293, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:08.026062: step 6596, loss 0.0790965, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:08.291646: step 6597, loss 0.0879367, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:08.537257: step 6598, loss 0.0617328, acc 1, learning_rate 0.0001
2017-10-10T15:26:08.744980: step 6599, loss 0.0988239, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:09.013040: step 6600, loss 0.0458675, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:09.439599: step 6600, loss 0.222979, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6600

2017-10-10T15:26:10.345941: step 6601, loss 0.0394057, acc 1, learning_rate 0.0001
2017-10-10T15:26:10.622109: step 6602, loss 0.0581238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:10.817098: step 6603, loss 0.075042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:11.024911: step 6604, loss 0.0744388, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:11.304816: step 6605, loss 0.0460668, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:11.599254: step 6606, loss 0.135691, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:11.843825: step 6607, loss 0.0447264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:12.104877: step 6608, loss 0.0547476, acc 1, learning_rate 0.0001
2017-10-10T15:26:12.411384: step 6609, loss 0.0901969, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:12.579799: step 6610, loss 0.0841158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:12.806464: step 6611, loss 0.106875, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:13.058660: step 6612, loss 0.065788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:13.225093: step 6613, loss 0.200426, acc 0.90625, learning_rate 0.0001
2017-10-10T15:26:13.505006: step 6614, loss 0.0879311, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:13.741126: step 6615, loss 0.124957, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:14.007542: step 6616, loss 0.0622384, acc 1, learning_rate 0.0001
2017-10-10T15:26:14.271190: step 6617, loss 0.117249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:14.485711: step 6618, loss 0.0260252, acc 1, learning_rate 0.0001
2017-10-10T15:26:14.726195: step 6619, loss 0.0580387, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:14.991097: step 6620, loss 0.0491745, acc 1, learning_rate 0.0001
2017-10-10T15:26:15.233077: step 6621, loss 0.0441771, acc 1, learning_rate 0.0001
2017-10-10T15:26:15.462653: step 6622, loss 0.0420748, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:15.726852: step 6623, loss 0.0576887, acc 1, learning_rate 0.0001
2017-10-10T15:26:16.024687: step 6624, loss 0.121909, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:16.272855: step 6625, loss 0.0527551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:16.612311: step 6626, loss 0.111077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:16.868911: step 6627, loss 0.0573301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:17.053218: step 6628, loss 0.0676318, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:17.253098: step 6629, loss 0.0425846, acc 1, learning_rate 0.0001
2017-10-10T15:26:17.446694: step 6630, loss 0.0833356, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:17.648022: step 6631, loss 0.0764586, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:17.905815: step 6632, loss 0.107605, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:18.125183: step 6633, loss 0.0865019, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:18.360853: step 6634, loss 0.116049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:18.646013: step 6635, loss 0.120622, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:18.909671: step 6636, loss 0.105093, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:19.099678: step 6637, loss 0.0540881, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:19.339141: step 6638, loss 0.124765, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:19.578837: step 6639, loss 0.0494478, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:19.822440: step 6640, loss 0.0348353, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:20.241112: step 6640, loss 0.221199, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6640

2017-10-10T15:26:21.252595: step 6641, loss 0.102875, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:21.472851: step 6642, loss 0.0992457, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:21.715858: step 6643, loss 0.106295, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:21.967336: step 6644, loss 0.0459564, acc 1, learning_rate 0.0001
2017-10-10T15:26:22.250118: step 6645, loss 0.0476191, acc 1, learning_rate 0.0001
2017-10-10T15:26:22.511004: step 6646, loss 0.0338219, acc 1, learning_rate 0.0001
2017-10-10T15:26:22.724653: step 6647, loss 0.283504, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:22.957727: step 6648, loss 0.0646289, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:23.213928: step 6649, loss 0.160552, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:23.460808: step 6650, loss 0.0900815, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:23.659167: step 6651, loss 0.0932073, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:23.906054: step 6652, loss 0.0943647, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:24.182193: step 6653, loss 0.0661414, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:24.460819: step 6654, loss 0.0817294, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:24.698994: step 6655, loss 0.0865721, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:24.938619: step 6656, loss 0.0728631, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:25.219091: step 6657, loss 0.0745427, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:25.468832: step 6658, loss 0.0765249, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:25.722988: step 6659, loss 0.0773942, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:25.948837: step 6660, loss 0.0466515, acc 1, learning_rate 0.0001
2017-10-10T15:26:26.189236: step 6661, loss 0.108271, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:26.444556: step 6662, loss 0.0421808, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:26.689009: step 6663, loss 0.0815045, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:26.842010: step 6664, loss 0.120653, acc 0.960784, learning_rate 0.0001
2017-10-10T15:26:27.119275: step 6665, loss 0.0879885, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:27.376898: step 6666, loss 0.0933867, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:27.643873: step 6667, loss 0.0321813, acc 1, learning_rate 0.0001
2017-10-10T15:26:27.894517: step 6668, loss 0.088864, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:28.192842: step 6669, loss 0.0719624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:28.420895: step 6670, loss 0.106793, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:28.660921: step 6671, loss 0.0476799, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:28.855853: step 6672, loss 0.0880327, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:29.100406: step 6673, loss 0.0219429, acc 1, learning_rate 0.0001
2017-10-10T15:26:29.309131: step 6674, loss 0.0930688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:29.569464: step 6675, loss 0.0745019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:29.837105: step 6676, loss 0.050367, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:30.123208: step 6677, loss 0.0949136, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:30.388814: step 6678, loss 0.108192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:30.619543: step 6679, loss 0.0653646, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:30.871993: step 6680, loss 0.100293, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:31.521207: step 6680, loss 0.223829, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6680

2017-10-10T15:26:32.424851: step 6681, loss 0.171536, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:32.737062: step 6682, loss 0.15608, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:32.995696: step 6683, loss 0.0972496, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:33.213301: step 6684, loss 0.100587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:33.405038: step 6685, loss 0.109177, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:33.683499: step 6686, loss 0.106553, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:33.983084: step 6687, loss 0.0955484, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:34.219423: step 6688, loss 0.059652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:34.448867: step 6689, loss 0.07914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:34.680030: step 6690, loss 0.026025, acc 1, learning_rate 0.0001
2017-10-10T15:26:34.902468: step 6691, loss 0.113834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:35.085089: step 6692, loss 0.142047, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:35.337165: step 6693, loss 0.0242947, acc 1, learning_rate 0.0001
2017-10-10T15:26:35.615076: step 6694, loss 0.0782834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:35.898095: step 6695, loss 0.0424219, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:36.167681: step 6696, loss 0.0916105, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:36.442792: step 6697, loss 0.105168, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:36.696400: step 6698, loss 0.126479, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:36.920243: step 6699, loss 0.0952455, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:37.176881: step 6700, loss 0.0663689, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:37.408547: step 6701, loss 0.0584955, acc 1, learning_rate 0.0001
2017-10-10T15:26:37.649093: step 6702, loss 0.0743911, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:37.918256: step 6703, loss 0.0758331, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:38.176850: step 6704, loss 0.113795, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:38.444996: step 6705, loss 0.0365568, acc 1, learning_rate 0.0001
2017-10-10T15:26:38.705075: step 6706, loss 0.0704427, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:38.948064: step 6707, loss 0.0296222, acc 1, learning_rate 0.0001
2017-10-10T15:26:39.204183: step 6708, loss 0.105247, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:39.488549: step 6709, loss 0.0976914, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:39.726781: step 6710, loss 0.0401606, acc 1, learning_rate 0.0001
2017-10-10T15:26:39.989096: step 6711, loss 0.0338523, acc 1, learning_rate 0.0001
2017-10-10T15:26:40.233098: step 6712, loss 0.0652927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:40.504349: step 6713, loss 0.0338202, acc 1, learning_rate 0.0001
2017-10-10T15:26:40.724910: step 6714, loss 0.102468, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:40.984401: step 6715, loss 0.104635, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:41.249111: step 6716, loss 0.0836862, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:41.528871: step 6717, loss 0.0865754, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:41.733974: step 6718, loss 0.0419337, acc 1, learning_rate 0.0001
2017-10-10T15:26:42.004970: step 6719, loss 0.0576815, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:42.275139: step 6720, loss 0.153699, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:42.725041: step 6720, loss 0.222863, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6720

2017-10-10T15:26:43.663825: step 6721, loss 0.0458894, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:43.900385: step 6722, loss 0.0715803, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:44.188870: step 6723, loss 0.0325186, acc 1, learning_rate 0.0001
2017-10-10T15:26:44.446913: step 6724, loss 0.224861, acc 0.90625, learning_rate 0.0001
2017-10-10T15:26:44.656965: step 6725, loss 0.0631897, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:44.907056: step 6726, loss 0.0581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:45.149640: step 6727, loss 0.0280626, acc 1, learning_rate 0.0001
2017-10-10T15:26:45.378591: step 6728, loss 0.0537075, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:45.653021: step 6729, loss 0.074594, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:45.942024: step 6730, loss 0.0941755, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:46.266521: step 6731, loss 0.160528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:46.452733: step 6732, loss 0.0756995, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:46.653245: step 6733, loss 0.121012, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:46.803754: step 6734, loss 0.128403, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:47.024844: step 6735, loss 0.141186, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:47.249207: step 6736, loss 0.0936289, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:47.531408: step 6737, loss 0.0889888, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:47.903255: step 6738, loss 0.0726368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:48.081444: step 6739, loss 0.0453294, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:48.288825: step 6740, loss 0.081679, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:48.512840: step 6741, loss 0.0307514, acc 1, learning_rate 0.0001
2017-10-10T15:26:48.700834: step 6742, loss 0.0648691, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:48.907457: step 6743, loss 0.0674555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:49.183461: step 6744, loss 0.0781839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:49.434455: step 6745, loss 0.099467, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:49.645964: step 6746, loss 0.0616771, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:49.920379: step 6747, loss 0.113155, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:50.195116: step 6748, loss 0.0632461, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:50.456986: step 6749, loss 0.0677832, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:50.709023: step 6750, loss 0.161359, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:50.971559: step 6751, loss 0.0767165, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:51.173091: step 6752, loss 0.147576, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:51.452475: step 6753, loss 0.0414727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:51.706319: step 6754, loss 0.103689, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:51.984062: step 6755, loss 0.0244566, acc 1, learning_rate 0.0001
2017-10-10T15:26:52.221023: step 6756, loss 0.0811649, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:52.460990: step 6757, loss 0.0733627, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:52.710569: step 6758, loss 0.0495314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:52.978014: step 6759, loss 0.0887466, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:53.276865: step 6760, loss 0.0609881, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:53.632949: step 6760, loss 0.223271, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6760

2017-10-10T15:26:54.470470: step 6761, loss 0.0364206, acc 1, learning_rate 0.0001
2017-10-10T15:26:54.704335: step 6762, loss 0.138821, acc 0.960784, learning_rate 0.0001
2017-10-10T15:26:54.958359: step 6763, loss 0.0263229, acc 1, learning_rate 0.0001
2017-10-10T15:26:55.190879: step 6764, loss 0.112828, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:55.416169: step 6765, loss 0.0651518, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:55.658456: step 6766, loss 0.0696861, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:55.903622: step 6767, loss 0.0542131, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:56.153662: step 6768, loss 0.0728548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:56.393429: step 6769, loss 0.0982097, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:56.649007: step 6770, loss 0.0553014, acc 1, learning_rate 0.0001
2017-10-10T15:26:56.869099: step 6771, loss 0.0653856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:57.165030: step 6772, loss 0.051142, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:57.455215: step 6773, loss 0.0696089, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:57.720889: step 6774, loss 0.0482206, acc 1, learning_rate 0.0001
2017-10-10T15:26:57.908974: step 6775, loss 0.0553637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:58.194168: step 6776, loss 0.074311, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:58.453518: step 6777, loss 0.053685, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:58.713001: step 6778, loss 0.0296244, acc 1, learning_rate 0.0001
2017-10-10T15:26:58.966954: step 6779, loss 0.0708181, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:59.230566: step 6780, loss 0.103228, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:59.444868: step 6781, loss 0.0951805, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:59.664897: step 6782, loss 0.0575667, acc 1, learning_rate 0.0001
2017-10-10T15:26:59.959551: step 6783, loss 0.10066, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:00.228860: step 6784, loss 0.0224094, acc 1, learning_rate 0.0001
2017-10-10T15:27:00.445385: step 6785, loss 0.0500296, acc 1, learning_rate 0.0001
2017-10-10T15:27:00.670824: step 6786, loss 0.0622153, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:00.920866: step 6787, loss 0.0462527, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:01.204030: step 6788, loss 0.0688051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:01.496305: step 6789, loss 0.116669, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:01.728820: step 6790, loss 0.0732477, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:01.872856: step 6791, loss 0.0867765, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:02.030926: step 6792, loss 0.0809186, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:02.204741: step 6793, loss 0.0978391, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:02.385642: step 6794, loss 0.0645446, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:02.670782: step 6795, loss 0.0821481, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:02.904516: step 6796, loss 0.0740748, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:03.156937: step 6797, loss 0.0732999, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:03.433333: step 6798, loss 0.0549511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:03.742919: step 6799, loss 0.0770703, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:03.978184: step 6800, loss 0.0502804, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:04.404145: step 6800, loss 0.222172, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6800

2017-10-10T15:27:05.411900: step 6801, loss 0.0827272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:05.700914: step 6802, loss 0.113663, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:05.960140: step 6803, loss 0.0764429, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:06.144888: step 6804, loss 0.102155, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:06.400753: step 6805, loss 0.0439991, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:06.622345: step 6806, loss 0.100228, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:06.828928: step 6807, loss 0.137362, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:07.081721: step 6808, loss 0.0554069, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:07.341739: step 6809, loss 0.121756, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:07.586171: step 6810, loss 0.0957149, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:07.814282: step 6811, loss 0.0828558, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:08.074166: step 6812, loss 0.115596, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:08.321008: step 6813, loss 0.0539431, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:08.575039: step 6814, loss 0.0902218, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:08.813654: step 6815, loss 0.0446444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:09.091758: step 6816, loss 0.0618326, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:09.350876: step 6817, loss 0.071048, acc 1, learning_rate 0.0001
2017-10-10T15:27:09.600503: step 6818, loss 0.0451447, acc 1, learning_rate 0.0001
2017-10-10T15:27:09.821594: step 6819, loss 0.176794, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:10.113048: step 6820, loss 0.0337726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:10.403465: step 6821, loss 0.152855, acc 0.90625, learning_rate 0.0001
2017-10-10T15:27:10.609107: step 6822, loss 0.0524678, acc 1, learning_rate 0.0001
2017-10-10T15:27:10.875293: step 6823, loss 0.111029, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:11.140968: step 6824, loss 0.164602, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:11.336674: step 6825, loss 0.0703934, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:11.587744: step 6826, loss 0.0671097, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:11.847741: step 6827, loss 0.129157, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:12.118445: step 6828, loss 0.0349142, acc 1, learning_rate 0.0001
2017-10-10T15:27:12.331266: step 6829, loss 0.032831, acc 1, learning_rate 0.0001
2017-10-10T15:27:12.629386: step 6830, loss 0.0349094, acc 1, learning_rate 0.0001
2017-10-10T15:27:12.889619: step 6831, loss 0.0723933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:13.142507: step 6832, loss 0.0840881, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:13.377737: step 6833, loss 0.0729767, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:13.655675: step 6834, loss 0.115966, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:13.954539: step 6835, loss 0.0907325, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:14.121706: step 6836, loss 0.0423857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:14.352809: step 6837, loss 0.170862, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:14.609657: step 6838, loss 0.0287546, acc 1, learning_rate 0.0001
2017-10-10T15:27:14.789886: step 6839, loss 0.0970357, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:15.053022: step 6840, loss 0.0710278, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:15.534561: step 6840, loss 0.220965, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6840

2017-10-10T15:27:16.430771: step 6841, loss 0.0648011, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:16.628354: step 6842, loss 0.0361541, acc 1, learning_rate 0.0001
2017-10-10T15:27:16.800869: step 6843, loss 0.0776899, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:16.984830: step 6844, loss 0.0943681, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:17.231846: step 6845, loss 0.122334, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:17.451013: step 6846, loss 0.103366, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:17.703137: step 6847, loss 0.0610622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:17.967579: step 6848, loss 0.0925256, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:18.239622: step 6849, loss 0.0819813, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:18.479235: step 6850, loss 0.168492, acc 0.921875, learning_rate 0.0001
2017-10-10T15:27:18.756342: step 6851, loss 0.0882271, acc 1, learning_rate 0.0001
2017-10-10T15:27:18.975274: step 6852, loss 0.0687473, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:19.232569: step 6853, loss 0.0567788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:19.508854: step 6854, loss 0.0649116, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:19.739872: step 6855, loss 0.152412, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:19.942402: step 6856, loss 0.0666001, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:20.196331: step 6857, loss 0.0754069, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:20.448972: step 6858, loss 0.0507716, acc 1, learning_rate 0.0001
2017-10-10T15:27:20.677111: step 6859, loss 0.138452, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:20.944869: step 6860, loss 0.0361738, acc 1, learning_rate 0.0001
2017-10-10T15:27:21.192500: step 6861, loss 0.0876523, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:21.419823: step 6862, loss 0.0950302, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:21.677319: step 6863, loss 0.117956, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:21.912831: step 6864, loss 0.112439, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:22.205049: step 6865, loss 0.0530974, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:22.395681: step 6866, loss 0.0873501, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:22.644136: step 6867, loss 0.0704027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:22.936234: step 6868, loss 0.0691306, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:23.205033: step 6869, loss 0.0257441, acc 1, learning_rate 0.0001
2017-10-10T15:27:23.390280: step 6870, loss 0.0386147, acc 1, learning_rate 0.0001
2017-10-10T15:27:23.634798: step 6871, loss 0.0772735, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:23.860930: step 6872, loss 0.0850907, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:24.145190: step 6873, loss 0.0686764, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:24.365594: step 6874, loss 0.114465, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:24.589643: step 6875, loss 0.0506263, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:24.751984: step 6876, loss 0.0445896, acc 1, learning_rate 0.0001
2017-10-10T15:27:24.966144: step 6877, loss 0.0704337, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:25.236827: step 6878, loss 0.0972061, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:25.479660: step 6879, loss 0.117661, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:25.724530: step 6880, loss 0.0861188, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:26.154967: step 6880, loss 0.220419, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6880

2017-10-10T15:27:27.252897: step 6881, loss 0.0545579, acc 1, learning_rate 0.0001
2017-10-10T15:27:27.485103: step 6882, loss 0.0883307, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:27.724242: step 6883, loss 0.0945148, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:27.980347: step 6884, loss 0.134703, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:28.241029: step 6885, loss 0.0831704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:28.464853: step 6886, loss 0.117818, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:28.740879: step 6887, loss 0.142157, acc 0.921875, learning_rate 0.0001
2017-10-10T15:27:29.036714: step 6888, loss 0.126689, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:29.301042: step 6889, loss 0.147875, acc 0.921875, learning_rate 0.0001
2017-10-10T15:27:29.500482: step 6890, loss 0.178886, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:29.698852: step 6891, loss 0.0898828, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:29.915461: step 6892, loss 0.0784275, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:30.095431: step 6893, loss 0.0414006, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:30.354979: step 6894, loss 0.0919724, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:30.656930: step 6895, loss 0.0348721, acc 1, learning_rate 0.0001
2017-10-10T15:27:31.005054: step 6896, loss 0.0469718, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:31.195264: step 6897, loss 0.06012, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:31.386988: step 6898, loss 0.0696571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:31.603341: step 6899, loss 0.0678232, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:31.817173: step 6900, loss 0.0561277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:32.072891: step 6901, loss 0.0681121, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:32.365014: step 6902, loss 0.118926, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:32.641653: step 6903, loss 0.169553, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:32.845384: step 6904, loss 0.0399111, acc 1, learning_rate 0.0001
2017-10-10T15:27:33.129133: step 6905, loss 0.122012, acc 0.921875, learning_rate 0.0001
2017-10-10T15:27:33.396993: step 6906, loss 0.0919635, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:33.663769: step 6907, loss 0.0950506, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:33.905909: step 6908, loss 0.0775236, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:34.203561: step 6909, loss 0.0571195, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:34.395076: step 6910, loss 0.0958898, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:34.616877: step 6911, loss 0.0453553, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:34.859037: step 6912, loss 0.0393159, acc 1, learning_rate 0.0001
2017-10-10T15:27:35.064914: step 6913, loss 0.104751, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:35.263294: step 6914, loss 0.0327069, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:35.525667: step 6915, loss 0.0711754, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:35.776525: step 6916, loss 0.0627585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:36.002138: step 6917, loss 0.0659702, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:36.236993: step 6918, loss 0.0205268, acc 1, learning_rate 0.0001
2017-10-10T15:27:36.530661: step 6919, loss 0.0644858, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:36.808899: step 6920, loss 0.0888827, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:37.270255: step 6920, loss 0.220749, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6920

2017-10-10T15:27:38.185312: step 6921, loss 0.0939358, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:38.434459: step 6922, loss 0.110719, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:38.663681: step 6923, loss 0.134363, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:38.927718: step 6924, loss 0.0688371, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:39.164389: step 6925, loss 0.0696845, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:39.418629: step 6926, loss 0.174595, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:39.682595: step 6927, loss 0.0870063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:39.944876: step 6928, loss 0.110555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:40.233523: step 6929, loss 0.0580118, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:40.509513: step 6930, loss 0.0848682, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:40.764940: step 6931, loss 0.0609282, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:40.987031: step 6932, loss 0.0687443, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:41.250836: step 6933, loss 0.087911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:41.529046: step 6934, loss 0.0495518, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:41.747963: step 6935, loss 0.12239, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:41.990170: step 6936, loss 0.0646727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:42.295380: step 6937, loss 0.0988907, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:42.589347: step 6938, loss 0.0729749, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:42.896112: step 6939, loss 0.0265697, acc 1, learning_rate 0.0001
2017-10-10T15:27:43.048956: step 6940, loss 0.06708, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:43.264904: step 6941, loss 0.051491, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:43.456911: step 6942, loss 0.0552119, acc 1, learning_rate 0.0001
2017-10-10T15:27:43.643996: step 6943, loss 0.102496, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:43.844885: step 6944, loss 0.0541839, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:44.111690: step 6945, loss 0.11328, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:44.428837: step 6946, loss 0.0584994, acc 1, learning_rate 0.0001
2017-10-10T15:27:44.701806: step 6947, loss 0.100283, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:44.949018: step 6948, loss 0.0372002, acc 1, learning_rate 0.0001
2017-10-10T15:27:45.160817: step 6949, loss 0.103833, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:45.468863: step 6950, loss 0.0694752, acc 1, learning_rate 0.0001
2017-10-10T15:27:45.802330: step 6951, loss 0.132126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:46.045744: step 6952, loss 0.0753215, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:46.260812: step 6953, loss 0.0614821, acc 1, learning_rate 0.0001
2017-10-10T15:27:46.476003: step 6954, loss 0.0491777, acc 1, learning_rate 0.0001
2017-10-10T15:27:46.658959: step 6955, loss 0.0460009, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:46.860970: step 6956, loss 0.0471815, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:47.118108: step 6957, loss 0.102413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:47.368972: step 6958, loss 0.092933, acc 0.960784, learning_rate 0.0001
2017-10-10T15:27:47.573973: step 6959, loss 0.0467554, acc 1, learning_rate 0.0001
2017-10-10T15:27:47.796848: step 6960, loss 0.0862121, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:48.323470: step 6960, loss 0.219619, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-6960

2017-10-10T15:27:49.365103: step 6961, loss 0.0199155, acc 1, learning_rate 0.0001
2017-10-10T15:27:49.557308: step 6962, loss 0.0315917, acc 1, learning_rate 0.0001
2017-10-10T15:27:49.841216: step 6963, loss 0.0372708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:50.113120: step 6964, loss 0.077049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:50.316965: step 6965, loss 0.110495, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:50.548947: step 6966, loss 0.123812, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:50.836306: step 6967, loss 0.0775321, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:51.127670: step 6968, loss 0.0209894, acc 1, learning_rate 0.0001
2017-10-10T15:27:51.328926: step 6969, loss 0.0612807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:51.571214: step 6970, loss 0.109901, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:51.834826: step 6971, loss 0.139248, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:52.102408: step 6972, loss 0.0683433, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:52.347530: step 6973, loss 0.0855044, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:52.609269: step 6974, loss 0.0795066, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:52.874288: step 6975, loss 0.0619535, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:53.119184: step 6976, loss 0.0378574, acc 1, learning_rate 0.0001
2017-10-10T15:27:53.312951: step 6977, loss 0.0566658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:53.574635: step 6978, loss 0.0592904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:53.869262: step 6979, loss 0.0911764, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:54.156635: step 6980, loss 0.0781728, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:54.396928: step 6981, loss 0.0607195, acc 1, learning_rate 0.0001
2017-10-10T15:27:54.686255: step 6982, loss 0.0556446, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:54.894320: step 6983, loss 0.127126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:55.144482: step 6984, loss 0.10215, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:55.344864: step 6985, loss 0.0848152, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:55.580978: step 6986, loss 0.0689564, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:55.842633: step 6987, loss 0.0225266, acc 1, learning_rate 0.0001
2017-10-10T15:27:56.161269: step 6988, loss 0.0820589, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:56.408947: step 6989, loss 0.0667776, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:56.568256: step 6990, loss 0.106854, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:56.776842: step 6991, loss 0.101424, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:56.972827: step 6992, loss 0.0678843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:57.199024: step 6993, loss 0.0350732, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:57.463689: step 6994, loss 0.0303073, acc 1, learning_rate 0.0001
2017-10-10T15:27:57.720852: step 6995, loss 0.0708078, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:57.925161: step 6996, loss 0.0406688, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:58.176616: step 6997, loss 0.153017, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:58.435841: step 6998, loss 0.0284013, acc 1, learning_rate 0.0001
2017-10-10T15:27:58.708837: step 6999, loss 0.14193, acc 0.921875, learning_rate 0.0001
2017-10-10T15:27:58.975538: step 7000, loss 0.0738418, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:59.423277: step 7000, loss 0.221945, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7000

2017-10-10T15:28:00.472927: step 7001, loss 0.0464434, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:00.761476: step 7002, loss 0.0403763, acc 1, learning_rate 0.0001
2017-10-10T15:28:00.940558: step 7003, loss 0.0889131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:01.178629: step 7004, loss 0.122602, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:01.404720: step 7005, loss 0.0561039, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:01.620302: step 7006, loss 0.0812726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:01.849163: step 7007, loss 0.0525296, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:02.090595: step 7008, loss 0.0685759, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:02.386475: step 7009, loss 0.189389, acc 0.921875, learning_rate 0.0001
2017-10-10T15:28:02.614993: step 7010, loss 0.0979894, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:02.856841: step 7011, loss 0.0586558, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:03.117810: step 7012, loss 0.076145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:03.394974: step 7013, loss 0.0441848, acc 1, learning_rate 0.0001
2017-10-10T15:28:03.605482: step 7014, loss 0.0637027, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:03.876809: step 7015, loss 0.0447822, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:04.140346: step 7016, loss 0.110268, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:04.388923: step 7017, loss 0.0736066, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:04.640968: step 7018, loss 0.0739037, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:04.941085: step 7019, loss 0.0805762, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:05.183704: step 7020, loss 0.0770519, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:05.388966: step 7021, loss 0.0609833, acc 1, learning_rate 0.0001
2017-10-10T15:28:05.569211: step 7022, loss 0.108276, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:05.803435: step 7023, loss 0.0579555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:06.072027: step 7024, loss 0.104521, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:06.294699: step 7025, loss 0.159318, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:06.525076: step 7026, loss 0.0348912, acc 1, learning_rate 0.0001
2017-10-10T15:28:06.755420: step 7027, loss 0.12638, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:07.016925: step 7028, loss 0.129099, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:07.257471: step 7029, loss 0.0524706, acc 1, learning_rate 0.0001
2017-10-10T15:28:07.500929: step 7030, loss 0.0493532, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:07.737097: step 7031, loss 0.0553361, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:07.936726: step 7032, loss 0.0416882, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:08.212377: step 7033, loss 0.108926, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:08.449006: step 7034, loss 0.0738042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:08.681040: step 7035, loss 0.0969113, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:08.932996: step 7036, loss 0.0475587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:09.216821: step 7037, loss 0.0348115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:09.565044: step 7038, loss 0.0472787, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:09.865020: step 7039, loss 0.0646325, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:10.100920: step 7040, loss 0.0582346, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:10.520807: step 7040, loss 0.219448, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7040

2017-10-10T15:28:11.504960: step 7041, loss 0.0495576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:11.767597: step 7042, loss 0.160379, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:12.005562: step 7043, loss 0.0419924, acc 1, learning_rate 0.0001
2017-10-10T15:28:12.250848: step 7044, loss 0.108801, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:12.533350: step 7045, loss 0.0992781, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:12.782003: step 7046, loss 0.0484727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:13.053038: step 7047, loss 0.0778276, acc 1, learning_rate 0.0001
2017-10-10T15:28:13.329405: step 7048, loss 0.0451823, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:13.545375: step 7049, loss 0.100571, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:13.802701: step 7050, loss 0.0294072, acc 1, learning_rate 0.0001
2017-10-10T15:28:14.087100: step 7051, loss 0.125223, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:14.390464: step 7052, loss 0.0570554, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:14.629742: step 7053, loss 0.0618745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:14.850376: step 7054, loss 0.0903883, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:15.127458: step 7055, loss 0.0673562, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:15.411524: step 7056, loss 0.0793489, acc 0.980392, learning_rate 0.0001
2017-10-10T15:28:15.716790: step 7057, loss 0.0237258, acc 1, learning_rate 0.0001
2017-10-10T15:28:15.872924: step 7058, loss 0.0628766, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:16.024803: step 7059, loss 0.105004, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:16.210546: step 7060, loss 0.0974898, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:16.413058: step 7061, loss 0.0287342, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:16.666869: step 7062, loss 0.0597011, acc 1, learning_rate 0.0001
2017-10-10T15:28:16.918869: step 7063, loss 0.0247392, acc 1, learning_rate 0.0001
2017-10-10T15:28:17.159332: step 7064, loss 0.0491468, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:17.327930: step 7065, loss 0.0366685, acc 1, learning_rate 0.0001
2017-10-10T15:28:17.611124: step 7066, loss 0.0405039, acc 1, learning_rate 0.0001
2017-10-10T15:28:17.877351: step 7067, loss 0.162217, acc 0.921875, learning_rate 0.0001
2017-10-10T15:28:18.133088: step 7068, loss 0.0430274, acc 1, learning_rate 0.0001
2017-10-10T15:28:18.344918: step 7069, loss 0.13206, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:18.618622: step 7070, loss 0.0600612, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:18.840910: step 7071, loss 0.0841456, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:19.114594: step 7072, loss 0.0286372, acc 1, learning_rate 0.0001
2017-10-10T15:28:19.372774: step 7073, loss 0.111219, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:19.624844: step 7074, loss 0.10712, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:19.827987: step 7075, loss 0.0484919, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:20.076855: step 7076, loss 0.0318636, acc 1, learning_rate 0.0001
2017-10-10T15:28:20.355948: step 7077, loss 0.0262975, acc 1, learning_rate 0.0001
2017-10-10T15:28:20.609171: step 7078, loss 0.0801419, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:20.857534: step 7079, loss 0.137461, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:21.128731: step 7080, loss 0.0952661, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:21.537284: step 7080, loss 0.219111, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7080

2017-10-10T15:28:22.341015: step 7081, loss 0.0720046, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:22.600442: step 7082, loss 0.0486269, acc 1, learning_rate 0.0001
2017-10-10T15:28:22.860909: step 7083, loss 0.112711, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:23.181121: step 7084, loss 0.0345658, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:23.408194: step 7085, loss 0.0508424, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:23.616836: step 7086, loss 0.0505295, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:23.837006: step 7087, loss 0.0797745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:24.025687: step 7088, loss 0.070598, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:24.232966: step 7089, loss 0.104494, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:24.480867: step 7090, loss 0.0924076, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:24.741143: step 7091, loss 0.0378592, acc 1, learning_rate 0.0001
2017-10-10T15:28:25.006741: step 7092, loss 0.0702713, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:25.254167: step 7093, loss 0.0757925, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:25.509107: step 7094, loss 0.0407435, acc 1, learning_rate 0.0001
2017-10-10T15:28:25.801157: step 7095, loss 0.0613661, acc 1, learning_rate 0.0001
2017-10-10T15:28:26.024863: step 7096, loss 0.0441124, acc 1, learning_rate 0.0001
2017-10-10T15:28:26.282132: step 7097, loss 0.115313, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:26.490115: step 7098, loss 0.0444221, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:26.674579: step 7099, loss 0.128049, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:26.880836: step 7100, loss 0.0729963, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:27.129674: step 7101, loss 0.117092, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:27.331718: step 7102, loss 0.111408, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:27.519970: step 7103, loss 0.109328, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:27.728221: step 7104, loss 0.0939923, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:27.948823: step 7105, loss 0.0220396, acc 1, learning_rate 0.0001
2017-10-10T15:28:28.176873: step 7106, loss 0.200987, acc 0.921875, learning_rate 0.0001
2017-10-10T15:28:28.384155: step 7107, loss 0.0502752, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:28.587860: step 7108, loss 0.0744827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:28.754851: step 7109, loss 0.050349, acc 1, learning_rate 0.0001
2017-10-10T15:28:28.976844: step 7110, loss 0.0690202, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:29.184856: step 7111, loss 0.100001, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:29.382418: step 7112, loss 0.0779149, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:29.571220: step 7113, loss 0.101031, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:29.816976: step 7114, loss 0.132072, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:30.065122: step 7115, loss 0.0790379, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:30.250663: step 7116, loss 0.0511595, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:30.420251: step 7117, loss 0.0498578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:30.582771: step 7118, loss 0.0736491, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:30.730057: step 7119, loss 0.0972085, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:30.841278: step 7120, loss 0.0673837, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:31.185736: step 7120, loss 0.218578, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7120

2017-10-10T15:28:32.136383: step 7121, loss 0.12014, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:32.360358: step 7122, loss 0.0607555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:32.552654: step 7123, loss 0.111081, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:32.746634: step 7124, loss 0.0223273, acc 1, learning_rate 0.0001
2017-10-10T15:28:32.914166: step 7125, loss 0.0934457, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:33.114532: step 7126, loss 0.0802789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:33.339228: step 7127, loss 0.0419128, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:33.544847: step 7128, loss 0.0527077, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:33.708261: step 7129, loss 0.0471777, acc 1, learning_rate 0.0001
2017-10-10T15:28:33.899652: step 7130, loss 0.0455985, acc 1, learning_rate 0.0001
2017-10-10T15:28:34.125433: step 7131, loss 0.0448411, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:34.318520: step 7132, loss 0.159791, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:34.459544: step 7133, loss 0.0555757, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:34.713570: step 7134, loss 0.0769576, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:34.937122: step 7135, loss 0.141089, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:35.076496: step 7136, loss 0.14429, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:35.242758: step 7137, loss 0.083231, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:35.397576: step 7138, loss 0.0545359, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:35.530334: step 7139, loss 0.0898077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:35.688175: step 7140, loss 0.0586469, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:35.873256: step 7141, loss 0.071617, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:36.048844: step 7142, loss 0.0612733, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:36.251950: step 7143, loss 0.0864104, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:36.493562: step 7144, loss 0.0611038, acc 1, learning_rate 0.0001
2017-10-10T15:28:36.709207: step 7145, loss 0.113603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:36.890822: step 7146, loss 0.111028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:37.047091: step 7147, loss 0.118347, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:37.274315: step 7148, loss 0.0651508, acc 1, learning_rate 0.0001
2017-10-10T15:28:37.476848: step 7149, loss 0.120608, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:37.688438: step 7150, loss 0.0659874, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:37.848864: step 7151, loss 0.0745751, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:38.069060: step 7152, loss 0.15087, acc 0.90625, learning_rate 0.0001
2017-10-10T15:28:38.272879: step 7153, loss 0.0528805, acc 1, learning_rate 0.0001
2017-10-10T15:28:38.437333: step 7154, loss 0.0853624, acc 0.960784, learning_rate 0.0001
2017-10-10T15:28:38.614922: step 7155, loss 0.0858986, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:38.836663: step 7156, loss 0.024183, acc 1, learning_rate 0.0001
2017-10-10T15:28:39.074992: step 7157, loss 0.0743896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:39.270047: step 7158, loss 0.0483073, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:39.437119: step 7159, loss 0.130956, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:39.647006: step 7160, loss 0.0871158, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:39.999566: step 7160, loss 0.220157, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7160

2017-10-10T15:28:40.959777: step 7161, loss 0.0503486, acc 1, learning_rate 0.0001
2017-10-10T15:28:41.184534: step 7162, loss 0.132005, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:41.333086: step 7163, loss 0.109044, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:41.522977: step 7164, loss 0.0604719, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:41.797000: step 7165, loss 0.0919313, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:42.044084: step 7166, loss 0.0955029, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:42.153490: step 7167, loss 0.0487409, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:42.310703: step 7168, loss 0.0516362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:42.474929: step 7169, loss 0.0861886, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:42.636823: step 7170, loss 0.0298171, acc 1, learning_rate 0.0001
2017-10-10T15:28:42.752927: step 7171, loss 0.0585644, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:42.896836: step 7172, loss 0.0156124, acc 1, learning_rate 0.0001
2017-10-10T15:28:43.096715: step 7173, loss 0.0587145, acc 1, learning_rate 0.0001
2017-10-10T15:28:43.275389: step 7174, loss 0.0653653, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:43.467473: step 7175, loss 0.0580526, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:43.730614: step 7176, loss 0.0376621, acc 1, learning_rate 0.0001
2017-10-10T15:28:43.897247: step 7177, loss 0.0370742, acc 1, learning_rate 0.0001
2017-10-10T15:28:44.076416: step 7178, loss 0.0685089, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:44.318002: step 7179, loss 0.037543, acc 1, learning_rate 0.0001
2017-10-10T15:28:44.539452: step 7180, loss 0.110527, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:44.740771: step 7181, loss 0.110896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:44.928865: step 7182, loss 0.0517026, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:45.129864: step 7183, loss 0.0769391, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:45.336850: step 7184, loss 0.0671291, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:45.520864: step 7185, loss 0.0868901, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:45.828959: step 7186, loss 0.04399, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:46.034410: step 7187, loss 0.076396, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:46.200759: step 7188, loss 0.0560642, acc 1, learning_rate 0.0001
2017-10-10T15:28:46.332832: step 7189, loss 0.0375978, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:46.464824: step 7190, loss 0.108051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:46.630924: step 7191, loss 0.112042, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:46.796184: step 7192, loss 0.165514, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:46.957320: step 7193, loss 0.154061, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:47.140976: step 7194, loss 0.0241453, acc 1, learning_rate 0.0001
2017-10-10T15:28:47.370914: step 7195, loss 0.0406892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:47.561248: step 7196, loss 0.0736651, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:47.816839: step 7197, loss 0.0763732, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:47.992032: step 7198, loss 0.0993017, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:48.196865: step 7199, loss 0.0247225, acc 1, learning_rate 0.0001
2017-10-10T15:28:48.401007: step 7200, loss 0.049289, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:48.697828: step 7200, loss 0.219525, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7200

2017-10-10T15:28:49.701627: step 7201, loss 0.104693, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:49.854792: step 7202, loss 0.155022, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:50.069148: step 7203, loss 0.178162, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:50.290876: step 7204, loss 0.142251, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:50.503651: step 7205, loss 0.0307327, acc 1, learning_rate 0.0001
2017-10-10T15:28:50.741026: step 7206, loss 0.0428673, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:50.924848: step 7207, loss 0.0339938, acc 1, learning_rate 0.0001
2017-10-10T15:28:51.144496: step 7208, loss 0.0700836, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:51.376188: step 7209, loss 0.0384617, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:51.596858: step 7210, loss 0.08301, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:51.788781: step 7211, loss 0.100803, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:51.939599: step 7212, loss 0.0490548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:52.148324: step 7213, loss 0.089555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:52.363231: step 7214, loss 0.0586623, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:52.574348: step 7215, loss 0.0574648, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:52.736098: step 7216, loss 0.0600765, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:52.948854: step 7217, loss 0.101702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:53.140919: step 7218, loss 0.125093, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:53.334077: step 7219, loss 0.079432, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:53.566092: step 7220, loss 0.093489, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:53.792838: step 7221, loss 0.0362819, acc 1, learning_rate 0.0001
2017-10-10T15:28:54.049148: step 7222, loss 0.0574039, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:54.212854: step 7223, loss 0.0429036, acc 1, learning_rate 0.0001
2017-10-10T15:28:54.364709: step 7224, loss 0.0945609, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:54.539280: step 7225, loss 0.0225116, acc 1, learning_rate 0.0001
2017-10-10T15:28:54.697086: step 7226, loss 0.0677527, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:54.862478: step 7227, loss 0.0739585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:54.978821: step 7228, loss 0.102286, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:55.202133: step 7229, loss 0.0484117, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:55.375906: step 7230, loss 0.102591, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:55.556807: step 7231, loss 0.0366883, acc 1, learning_rate 0.0001
2017-10-10T15:28:55.792827: step 7232, loss 0.0589859, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:55.984390: step 7233, loss 0.0454856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:56.132009: step 7234, loss 0.109837, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:56.348670: step 7235, loss 0.145333, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:56.592886: step 7236, loss 0.125728, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:56.864203: step 7237, loss 0.186715, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:56.990541: step 7238, loss 0.0758784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:57.163480: step 7239, loss 0.0564429, acc 1, learning_rate 0.0001
2017-10-10T15:28:57.358804: step 7240, loss 0.0710725, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:57.626364: step 7240, loss 0.22217, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7240

2017-10-10T15:28:58.382234: step 7241, loss 0.0272154, acc 1, learning_rate 0.0001
2017-10-10T15:28:58.562266: step 7242, loss 0.0585065, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:58.804479: step 7243, loss 0.0541075, acc 1, learning_rate 0.0001
2017-10-10T15:28:59.038576: step 7244, loss 0.131111, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:59.232984: step 7245, loss 0.0683455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:59.410053: step 7246, loss 0.0308842, acc 1, learning_rate 0.0001
2017-10-10T15:28:59.617356: step 7247, loss 0.0912019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:59.846764: step 7248, loss 0.0769917, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:00.056016: step 7249, loss 0.0511029, acc 1, learning_rate 0.0001
2017-10-10T15:29:00.240864: step 7250, loss 0.067564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:00.434500: step 7251, loss 0.0441127, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:00.625419: step 7252, loss 0.0808064, acc 0.980392, learning_rate 0.0001
2017-10-10T15:29:00.817964: step 7253, loss 0.102889, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:01.007722: step 7254, loss 0.125133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:01.252802: step 7255, loss 0.107503, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:01.483075: step 7256, loss 0.095538, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:01.667561: step 7257, loss 0.168149, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:01.823129: step 7258, loss 0.0430235, acc 1, learning_rate 0.0001
2017-10-10T15:29:02.046593: step 7259, loss 0.0440663, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:02.229000: step 7260, loss 0.101303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:02.389863: step 7261, loss 0.0564372, acc 1, learning_rate 0.0001
2017-10-10T15:29:02.633200: step 7262, loss 0.0809123, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:02.853746: step 7263, loss 0.115774, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:03.048907: step 7264, loss 0.0346785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:03.256983: step 7265, loss 0.0449997, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:03.452885: step 7266, loss 0.0607119, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:03.644764: step 7267, loss 0.0311515, acc 1, learning_rate 0.0001
2017-10-10T15:29:03.885109: step 7268, loss 0.114956, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:04.106767: step 7269, loss 0.0354732, acc 1, learning_rate 0.0001
2017-10-10T15:29:04.297003: step 7270, loss 0.052547, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:04.475520: step 7271, loss 0.0186517, acc 1, learning_rate 0.0001
2017-10-10T15:29:04.698273: step 7272, loss 0.0750705, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:04.908555: step 7273, loss 0.072238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:05.075203: step 7274, loss 0.0923287, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:05.275456: step 7275, loss 0.123492, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:05.493987: step 7276, loss 0.0652971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:05.729071: step 7277, loss 0.0937771, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:05.991553: step 7278, loss 0.0780851, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:06.156603: step 7279, loss 0.0170522, acc 1, learning_rate 0.0001
2017-10-10T15:29:06.311483: step 7280, loss 0.193537, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:06.625701: step 7280, loss 0.22184, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7280

2017-10-10T15:29:07.624942: step 7281, loss 0.0610739, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:07.846498: step 7282, loss 0.0738922, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:07.994457: step 7283, loss 0.0578901, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:08.157804: step 7284, loss 0.0340563, acc 1, learning_rate 0.0001
2017-10-10T15:29:08.329944: step 7285, loss 0.0338893, acc 1, learning_rate 0.0001
2017-10-10T15:29:08.467750: step 7286, loss 0.190101, acc 0.90625, learning_rate 0.0001
2017-10-10T15:29:08.592823: step 7287, loss 0.0541673, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:08.823054: step 7288, loss 0.117871, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:09.088869: step 7289, loss 0.0393497, acc 1, learning_rate 0.0001
2017-10-10T15:29:09.314452: step 7290, loss 0.132628, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:09.497022: step 7291, loss 0.076774, acc 1, learning_rate 0.0001
2017-10-10T15:29:09.650578: step 7292, loss 0.133997, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:09.897178: step 7293, loss 0.0924924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:10.103989: step 7294, loss 0.0184774, acc 1, learning_rate 0.0001
2017-10-10T15:29:10.282558: step 7295, loss 0.0833767, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:10.516432: step 7296, loss 0.0319301, acc 1, learning_rate 0.0001
2017-10-10T15:29:10.657377: step 7297, loss 0.0475946, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:10.840149: step 7298, loss 0.0555404, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:11.065859: step 7299, loss 0.103746, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:11.305098: step 7300, loss 0.0943516, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:11.458573: step 7301, loss 0.0817634, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:11.637849: step 7302, loss 0.0769105, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:11.873320: step 7303, loss 0.064153, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:12.093387: step 7304, loss 0.0720646, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:12.292922: step 7305, loss 0.0895668, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:12.455896: step 7306, loss 0.18032, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:12.662193: step 7307, loss 0.0306088, acc 1, learning_rate 0.0001
2017-10-10T15:29:12.888813: step 7308, loss 0.0422244, acc 1, learning_rate 0.0001
2017-10-10T15:29:13.138300: step 7309, loss 0.0318028, acc 1, learning_rate 0.0001
2017-10-10T15:29:13.357338: step 7310, loss 0.0358512, acc 1, learning_rate 0.0001
2017-10-10T15:29:13.550714: step 7311, loss 0.0975777, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:13.723788: step 7312, loss 0.0461989, acc 1, learning_rate 0.0001
2017-10-10T15:29:13.925355: step 7313, loss 0.090941, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:14.144820: step 7314, loss 0.0949951, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:14.383982: step 7315, loss 0.08397, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:14.580774: step 7316, loss 0.0327914, acc 1, learning_rate 0.0001
2017-10-10T15:29:14.770425: step 7317, loss 0.110709, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:15.014623: step 7318, loss 0.0632662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:15.221098: step 7319, loss 0.0713571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:15.397206: step 7320, loss 0.0557346, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:15.733273: step 7320, loss 0.221474, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7320

2017-10-10T15:29:16.708490: step 7321, loss 0.0567175, acc 1, learning_rate 0.0001
2017-10-10T15:29:16.920984: step 7322, loss 0.0377692, acc 1, learning_rate 0.0001
2017-10-10T15:29:17.152846: step 7323, loss 0.158468, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:17.463863: step 7324, loss 0.0893727, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:17.706914: step 7325, loss 0.048543, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:17.844728: step 7326, loss 0.0583824, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:17.987847: step 7327, loss 0.0884816, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:18.120867: step 7328, loss 0.0445747, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:18.267604: step 7329, loss 0.106745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:18.494711: step 7330, loss 0.0949701, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:18.744809: step 7331, loss 0.0795272, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:18.894551: step 7332, loss 0.139752, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:19.032277: step 7333, loss 0.0659691, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:19.181231: step 7334, loss 0.061672, acc 1, learning_rate 0.0001
2017-10-10T15:29:19.355940: step 7335, loss 0.0441047, acc 1, learning_rate 0.0001
2017-10-10T15:29:19.504574: step 7336, loss 0.0917118, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:19.711195: step 7337, loss 0.0650251, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:19.915536: step 7338, loss 0.165576, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:20.120833: step 7339, loss 0.100722, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:20.325158: step 7340, loss 0.0433838, acc 1, learning_rate 0.0001
2017-10-10T15:29:20.477807: step 7341, loss 0.0340242, acc 1, learning_rate 0.0001
2017-10-10T15:29:20.719081: step 7342, loss 0.123172, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:20.928033: step 7343, loss 0.0713879, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:21.156872: step 7344, loss 0.035316, acc 1, learning_rate 0.0001
2017-10-10T15:29:21.321094: step 7345, loss 0.076979, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:21.499852: step 7346, loss 0.0850637, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:21.734802: step 7347, loss 0.12046, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:21.911566: step 7348, loss 0.0642003, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:22.055382: step 7349, loss 0.0256183, acc 1, learning_rate 0.0001
2017-10-10T15:29:22.268187: step 7350, loss 0.0352495, acc 1, learning_rate 0.0001
2017-10-10T15:29:22.410054: step 7351, loss 0.0560908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:22.597048: step 7352, loss 0.155418, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:22.834498: step 7353, loss 0.0295031, acc 1, learning_rate 0.0001
2017-10-10T15:29:23.009547: step 7354, loss 0.122556, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:23.155292: step 7355, loss 0.0635598, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:23.367580: step 7356, loss 0.0601063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:23.580320: step 7357, loss 0.135464, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:23.723249: step 7358, loss 0.0525298, acc 1, learning_rate 0.0001
2017-10-10T15:29:23.951633: step 7359, loss 0.0825529, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:24.168946: step 7360, loss 0.0320628, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:24.557564: step 7360, loss 0.221691, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7360

2017-10-10T15:29:25.479529: step 7361, loss 0.0598646, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:25.649094: step 7362, loss 0.0237667, acc 1, learning_rate 0.0001
2017-10-10T15:29:25.808920: step 7363, loss 0.0419303, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:26.021981: step 7364, loss 0.113293, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:26.236970: step 7365, loss 0.0434582, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:26.486796: step 7366, loss 0.073446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:26.672969: step 7367, loss 0.0879338, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:26.825033: step 7368, loss 0.0566329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:27.046171: step 7369, loss 0.0248779, acc 1, learning_rate 0.0001
2017-10-10T15:29:27.286668: step 7370, loss 0.0303853, acc 1, learning_rate 0.0001
2017-10-10T15:29:27.496841: step 7371, loss 0.0876657, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:27.658394: step 7372, loss 0.0694809, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:27.852753: step 7373, loss 0.0491054, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:28.087636: step 7374, loss 0.107862, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:28.315861: step 7375, loss 0.0425331, acc 1, learning_rate 0.0001
2017-10-10T15:29:28.471721: step 7376, loss 0.0574145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:28.644524: step 7377, loss 0.0528671, acc 1, learning_rate 0.0001
2017-10-10T15:29:28.904020: step 7378, loss 0.0797485, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:29.135401: step 7379, loss 0.0800491, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:29.393953: step 7380, loss 0.0379686, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:29.573044: step 7381, loss 0.0608518, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:29.699805: step 7382, loss 0.0297339, acc 1, learning_rate 0.0001
2017-10-10T15:29:29.916921: step 7383, loss 0.0954095, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:30.101996: step 7384, loss 0.0466344, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:30.200359: step 7385, loss 0.0301081, acc 1, learning_rate 0.0001
2017-10-10T15:29:30.348673: step 7386, loss 0.0573775, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:30.488916: step 7387, loss 0.0483369, acc 1, learning_rate 0.0001
2017-10-10T15:29:30.641457: step 7388, loss 0.118814, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:30.815342: step 7389, loss 0.0536393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:30.966496: step 7390, loss 0.0375012, acc 1, learning_rate 0.0001
2017-10-10T15:29:31.143767: step 7391, loss 0.0650773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:31.385334: step 7392, loss 0.227515, acc 0.90625, learning_rate 0.0001
2017-10-10T15:29:31.606493: step 7393, loss 0.0789925, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:31.784972: step 7394, loss 0.0749041, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:32.006858: step 7395, loss 0.0301969, acc 1, learning_rate 0.0001
2017-10-10T15:29:32.185455: step 7396, loss 0.0954143, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:32.403909: step 7397, loss 0.069751, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:32.628225: step 7398, loss 0.0851135, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:32.852826: step 7399, loss 0.0738904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:33.061580: step 7400, loss 0.0638765, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:33.368115: step 7400, loss 0.22167, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7400

2017-10-10T15:29:34.183009: step 7401, loss 0.0852628, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:34.395826: step 7402, loss 0.0551224, acc 1, learning_rate 0.0001
2017-10-10T15:29:34.617918: step 7403, loss 0.108334, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:34.811157: step 7404, loss 0.0690258, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:35.003223: step 7405, loss 0.0683902, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:35.226880: step 7406, loss 0.0429773, acc 1, learning_rate 0.0001
2017-10-10T15:29:35.420912: step 7407, loss 0.0661617, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:35.589535: step 7408, loss 0.0315867, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:35.790779: step 7409, loss 0.0425653, acc 1, learning_rate 0.0001
2017-10-10T15:29:36.025112: step 7410, loss 0.0503003, acc 1, learning_rate 0.0001
2017-10-10T15:29:36.214988: step 7411, loss 0.0558983, acc 1, learning_rate 0.0001
2017-10-10T15:29:36.402363: step 7412, loss 0.0673146, acc 1, learning_rate 0.0001
2017-10-10T15:29:36.582797: step 7413, loss 0.0386399, acc 1, learning_rate 0.0001
2017-10-10T15:29:36.804836: step 7414, loss 0.143539, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:37.020797: step 7415, loss 0.0345325, acc 1, learning_rate 0.0001
2017-10-10T15:29:37.218783: step 7416, loss 0.0655216, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:37.432079: step 7417, loss 0.0776448, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:37.658658: step 7418, loss 0.0302043, acc 1, learning_rate 0.0001
2017-10-10T15:29:37.821054: step 7419, loss 0.044839, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:37.989901: step 7420, loss 0.0393227, acc 1, learning_rate 0.0001
2017-10-10T15:29:38.192828: step 7421, loss 0.0680002, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:38.349236: step 7422, loss 0.0767986, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:38.536839: step 7423, loss 0.157245, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:38.736252: step 7424, loss 0.0786104, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:38.952925: step 7425, loss 0.0395556, acc 1, learning_rate 0.0001
2017-10-10T15:29:39.111800: step 7426, loss 0.0467604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:39.327221: step 7427, loss 0.0657695, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:39.529315: step 7428, loss 0.0407906, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:39.680824: step 7429, loss 0.0909375, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:39.909243: step 7430, loss 0.0484133, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:40.126329: step 7431, loss 0.0981866, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:40.266966: step 7432, loss 0.0217413, acc 1, learning_rate 0.0001
2017-10-10T15:29:40.467729: step 7433, loss 0.0865849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:40.705167: step 7434, loss 0.101804, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:40.924026: step 7435, loss 0.056825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:41.209042: step 7436, loss 0.16308, acc 0.921875, learning_rate 0.0001
2017-10-10T15:29:41.410670: step 7437, loss 0.100541, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:41.513091: step 7438, loss 0.0692083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:41.619163: step 7439, loss 0.109236, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:41.719736: step 7440, loss 0.0442858, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:41.934019: step 7440, loss 0.220792, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7440

2017-10-10T15:29:42.722871: step 7441, loss 0.0256479, acc 1, learning_rate 0.0001
2017-10-10T15:29:42.932297: step 7442, loss 0.0417991, acc 1, learning_rate 0.0001
2017-10-10T15:29:43.155262: step 7443, loss 0.047803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:43.359587: step 7444, loss 0.099155, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:43.569383: step 7445, loss 0.063784, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:43.804511: step 7446, loss 0.0867355, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:44.004962: step 7447, loss 0.0942337, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:44.122987: step 7448, loss 0.10524, acc 0.960784, learning_rate 0.0001
2017-10-10T15:29:44.353571: step 7449, loss 0.0330375, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:44.520276: step 7450, loss 0.0933572, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:44.692888: step 7451, loss 0.0538703, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:44.918003: step 7452, loss 0.0499298, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:45.140829: step 7453, loss 0.0551132, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:45.312196: step 7454, loss 0.0459734, acc 1, learning_rate 0.0001
2017-10-10T15:29:45.480979: step 7455, loss 0.09849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:45.708594: step 7456, loss 0.0576687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:45.897446: step 7457, loss 0.0815021, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:46.069396: step 7458, loss 0.065857, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:46.260485: step 7459, loss 0.0557523, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:46.500010: step 7460, loss 0.0188125, acc 1, learning_rate 0.0001
2017-10-10T15:29:46.704883: step 7461, loss 0.0667408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:46.889689: step 7462, loss 0.126116, acc 0.921875, learning_rate 0.0001
2017-10-10T15:29:47.068252: step 7463, loss 0.0752968, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:47.285408: step 7464, loss 0.0872755, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:47.484918: step 7465, loss 0.122213, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:47.639202: step 7466, loss 0.0582082, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:47.836964: step 7467, loss 0.0994476, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:48.067327: step 7468, loss 0.102056, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:48.298783: step 7469, loss 0.0959083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:48.493100: step 7470, loss 0.028915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:48.718123: step 7471, loss 0.0530347, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:48.891949: step 7472, loss 0.0433061, acc 1, learning_rate 0.0001
2017-10-10T15:29:49.111933: step 7473, loss 0.0477806, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:49.325026: step 7474, loss 0.0577012, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:49.568180: step 7475, loss 0.0760946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:49.789952: step 7476, loss 0.0484774, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:50.007674: step 7477, loss 0.0345631, acc 1, learning_rate 0.0001
2017-10-10T15:29:50.199616: step 7478, loss 0.212319, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:50.412826: step 7479, loss 0.0548296, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:50.647905: step 7480, loss 0.0810288, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:50.996597: step 7480, loss 0.221931, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7480

2017-10-10T15:29:51.916858: step 7481, loss 0.0882467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:52.176441: step 7482, loss 0.0416713, acc 1, learning_rate 0.0001
2017-10-10T15:29:52.429669: step 7483, loss 0.0454558, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:52.588833: step 7484, loss 0.0958462, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:52.812866: step 7485, loss 0.0704099, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:52.966440: step 7486, loss 0.0705554, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:53.057495: step 7487, loss 0.0530615, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:53.150634: step 7488, loss 0.10834, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:53.324189: step 7489, loss 0.0568446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:53.497768: step 7490, loss 0.0308504, acc 1, learning_rate 0.0001
2017-10-10T15:29:53.655111: step 7491, loss 0.0563107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:53.779584: step 7492, loss 0.189522, acc 0.9375, learning_rate 0.0001
2017-10-10T15:29:54.026386: step 7493, loss 0.0391825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:54.237141: step 7494, loss 0.0866262, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:54.443809: step 7495, loss 0.086999, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:54.636007: step 7496, loss 0.0717246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:54.836829: step 7497, loss 0.0353023, acc 1, learning_rate 0.0001
2017-10-10T15:29:55.052880: step 7498, loss 0.11579, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:55.178926: step 7499, loss 0.0332713, acc 1, learning_rate 0.0001
2017-10-10T15:29:55.379181: step 7500, loss 0.129266, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:55.628029: step 7501, loss 0.0895995, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:55.856827: step 7502, loss 0.0855298, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:56.016988: step 7503, loss 0.0912662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:56.200367: step 7504, loss 0.0478001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:56.412484: step 7505, loss 0.0298997, acc 1, learning_rate 0.0001
2017-10-10T15:29:56.644829: step 7506, loss 0.1912, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:56.862766: step 7507, loss 0.11981, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:57.053805: step 7508, loss 0.0926015, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:57.223022: step 7509, loss 0.054399, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:57.437994: step 7510, loss 0.0868656, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:57.624956: step 7511, loss 0.0637148, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:57.828829: step 7512, loss 0.125351, acc 0.953125, learning_rate 0.0001
2017-10-10T15:29:58.057218: step 7513, loss 0.0427914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:58.293946: step 7514, loss 0.0917438, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:58.497973: step 7515, loss 0.0618823, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:58.671999: step 7516, loss 0.0637317, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:58.883065: step 7517, loss 0.0826857, acc 0.96875, learning_rate 0.0001
2017-10-10T15:29:59.108235: step 7518, loss 0.0528405, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:59.308852: step 7519, loss 0.0245529, acc 0.984375, learning_rate 0.0001
2017-10-10T15:29:59.503055: step 7520, loss 0.0705109, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:29:59.843558: step 7520, loss 0.220874, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7520

2017-10-10T15:30:00.666537: step 7521, loss 0.0343752, acc 1, learning_rate 0.0001
2017-10-10T15:30:00.899792: step 7522, loss 0.0734539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:01.073065: step 7523, loss 0.150803, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:01.320993: step 7524, loss 0.130146, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:01.501258: step 7525, loss 0.094376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:01.729031: step 7526, loss 0.0663911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:01.980847: step 7527, loss 0.0909338, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:02.132424: step 7528, loss 0.027689, acc 1, learning_rate 0.0001
2017-10-10T15:30:02.324880: step 7529, loss 0.1235, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:02.557210: step 7530, loss 0.113529, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:02.808950: step 7531, loss 0.117224, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:03.076853: step 7532, loss 0.0689285, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:03.211484: step 7533, loss 0.0250495, acc 1, learning_rate 0.0001
2017-10-10T15:30:03.374576: step 7534, loss 0.0973106, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:03.559592: step 7535, loss 0.119989, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:03.724820: step 7536, loss 0.10657, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:03.872831: step 7537, loss 0.0732564, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:04.068834: step 7538, loss 0.146317, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:04.232850: step 7539, loss 0.040009, acc 1, learning_rate 0.0001
2017-10-10T15:30:04.458693: step 7540, loss 0.119538, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:04.719481: step 7541, loss 0.0611647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:04.943196: step 7542, loss 0.100011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:05.096831: step 7543, loss 0.0771459, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:05.232866: step 7544, loss 0.0743108, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:05.364613: step 7545, loss 0.113177, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:05.506517: step 7546, loss 0.0744647, acc 0.980392, learning_rate 0.0001
2017-10-10T15:30:05.668428: step 7547, loss 0.0805607, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:05.864232: step 7548, loss 0.0528951, acc 1, learning_rate 0.0001
2017-10-10T15:30:06.004898: step 7549, loss 0.067765, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:06.239391: step 7550, loss 0.0493306, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:06.445285: step 7551, loss 0.0257776, acc 1, learning_rate 0.0001
2017-10-10T15:30:06.605904: step 7552, loss 0.109938, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:06.789918: step 7553, loss 0.0855337, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:07.019582: step 7554, loss 0.0669744, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:07.241655: step 7555, loss 0.115247, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:07.394751: step 7556, loss 0.0422352, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:07.576874: step 7557, loss 0.127186, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:07.793921: step 7558, loss 0.0479638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:08.003933: step 7559, loss 0.0414504, acc 1, learning_rate 0.0001
2017-10-10T15:30:08.193173: step 7560, loss 0.0984974, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:08.532265: step 7560, loss 0.219131, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7560

2017-10-10T15:30:09.489519: step 7561, loss 0.0729595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:09.692957: step 7562, loss 0.0445836, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:09.929301: step 7563, loss 0.0359635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:10.123900: step 7564, loss 0.110927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:10.339493: step 7565, loss 0.07279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:10.538807: step 7566, loss 0.0931603, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:10.736922: step 7567, loss 0.0373628, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:10.935882: step 7568, loss 0.0405571, acc 1, learning_rate 0.0001
2017-10-10T15:30:11.125307: step 7569, loss 0.0664293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:11.336991: step 7570, loss 0.0535844, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:11.481006: step 7571, loss 0.0862133, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:11.681377: step 7572, loss 0.0752427, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:11.892862: step 7573, loss 0.0279619, acc 1, learning_rate 0.0001
2017-10-10T15:30:12.073275: step 7574, loss 0.0215429, acc 1, learning_rate 0.0001
2017-10-10T15:30:12.307283: step 7575, loss 0.0387537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:12.508334: step 7576, loss 0.0906932, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:12.724939: step 7577, loss 0.0549669, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:12.887106: step 7578, loss 0.0897616, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:13.062955: step 7579, loss 0.0471129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:13.309848: step 7580, loss 0.112091, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:13.487156: step 7581, loss 0.0524564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:13.692955: step 7582, loss 0.0275669, acc 1, learning_rate 0.0001
2017-10-10T15:30:13.988228: step 7583, loss 0.0683935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:14.170704: step 7584, loss 0.0524446, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:14.306255: step 7585, loss 0.145413, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:14.432705: step 7586, loss 0.140424, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:14.610358: step 7587, loss 0.0596202, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:14.774792: step 7588, loss 0.0528945, acc 1, learning_rate 0.0001
2017-10-10T15:30:14.960985: step 7589, loss 0.0426674, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:15.116814: step 7590, loss 0.0791982, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:15.343099: step 7591, loss 0.0553522, acc 1, learning_rate 0.0001
2017-10-10T15:30:15.536885: step 7592, loss 0.0481162, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:15.705465: step 7593, loss 0.098855, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:15.908258: step 7594, loss 0.0854835, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:16.122900: step 7595, loss 0.10504, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:16.329832: step 7596, loss 0.0827473, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:16.608924: step 7597, loss 0.0770308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:16.835929: step 7598, loss 0.0701521, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:16.952689: step 7599, loss 0.0412013, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:17.117069: step 7600, loss 0.0378997, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:17.408508: step 7600, loss 0.220023, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7600

2017-10-10T15:30:18.321244: step 7601, loss 0.039116, acc 1, learning_rate 0.0001
2017-10-10T15:30:18.553209: step 7602, loss 0.109917, acc 0.921875, learning_rate 0.0001
2017-10-10T15:30:18.710287: step 7603, loss 0.0708661, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:18.887245: step 7604, loss 0.0806725, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:19.112671: step 7605, loss 0.0261855, acc 1, learning_rate 0.0001
2017-10-10T15:30:19.334400: step 7606, loss 0.109463, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:19.540957: step 7607, loss 0.0468852, acc 1, learning_rate 0.0001
2017-10-10T15:30:19.700994: step 7608, loss 0.0528263, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:19.917582: step 7609, loss 0.110567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:20.107624: step 7610, loss 0.0598297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:20.334816: step 7611, loss 0.064773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:20.522408: step 7612, loss 0.0272198, acc 1, learning_rate 0.0001
2017-10-10T15:30:20.733144: step 7613, loss 0.0766184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:20.978709: step 7614, loss 0.0713067, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:21.140896: step 7615, loss 0.0673298, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:21.348928: step 7616, loss 0.110356, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:21.563075: step 7617, loss 0.0538408, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:21.796229: step 7618, loss 0.118116, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:22.018166: step 7619, loss 0.0539296, acc 1, learning_rate 0.0001
2017-10-10T15:30:22.155450: step 7620, loss 0.065206, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:22.366078: step 7621, loss 0.068444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:22.597127: step 7622, loss 0.0941316, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:22.823298: step 7623, loss 0.107144, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:23.013006: step 7624, loss 0.0416723, acc 1, learning_rate 0.0001
2017-10-10T15:30:23.156406: step 7625, loss 0.0242753, acc 1, learning_rate 0.0001
2017-10-10T15:30:23.356471: step 7626, loss 0.0874721, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:23.560901: step 7627, loss 0.0842343, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:23.743013: step 7628, loss 0.0772577, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:23.913488: step 7629, loss 0.0619482, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:24.141544: step 7630, loss 0.055292, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:24.372839: step 7631, loss 0.0417144, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:24.567350: step 7632, loss 0.0375944, acc 1, learning_rate 0.0001
2017-10-10T15:30:24.720872: step 7633, loss 0.11693, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:24.976126: step 7634, loss 0.108864, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:25.196751: step 7635, loss 0.0964039, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:25.331922: step 7636, loss 0.0988567, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:25.503074: step 7637, loss 0.086883, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:25.680185: step 7638, loss 0.0338948, acc 1, learning_rate 0.0001
2017-10-10T15:30:25.816855: step 7639, loss 0.0936223, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:25.962666: step 7640, loss 0.0896509, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:26.308851: step 7640, loss 0.219639, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7640

2017-10-10T15:30:27.248902: step 7641, loss 0.0965854, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:27.464873: step 7642, loss 0.0635153, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:27.661058: step 7643, loss 0.0572701, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:27.821878: step 7644, loss 0.130041, acc 0.941176, learning_rate 0.0001
2017-10-10T15:30:28.056933: step 7645, loss 0.0565132, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:28.300987: step 7646, loss 0.0942999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:28.552973: step 7647, loss 0.123374, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:28.728855: step 7648, loss 0.0239716, acc 1, learning_rate 0.0001
2017-10-10T15:30:28.841255: step 7649, loss 0.109449, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:29.010243: step 7650, loss 0.0922023, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:29.174998: step 7651, loss 0.0653931, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:29.332662: step 7652, loss 0.038068, acc 1, learning_rate 0.0001
2017-10-10T15:30:29.491763: step 7653, loss 0.0687883, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:29.728835: step 7654, loss 0.0794707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:29.976856: step 7655, loss 0.0608139, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:30.174740: step 7656, loss 0.031409, acc 1, learning_rate 0.0001
2017-10-10T15:30:30.360847: step 7657, loss 0.0469734, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:30.578647: step 7658, loss 0.0545471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:30.801194: step 7659, loss 0.0382804, acc 1, learning_rate 0.0001
2017-10-10T15:30:31.016271: step 7660, loss 0.0361099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:31.202380: step 7661, loss 0.031053, acc 1, learning_rate 0.0001
2017-10-10T15:30:31.394412: step 7662, loss 0.0331074, acc 1, learning_rate 0.0001
2017-10-10T15:30:31.632444: step 7663, loss 0.0449416, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:31.848841: step 7664, loss 0.0358658, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:31.996939: step 7665, loss 0.147129, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:32.191509: step 7666, loss 0.0365095, acc 1, learning_rate 0.0001
2017-10-10T15:30:32.428671: step 7667, loss 0.0297959, acc 1, learning_rate 0.0001
2017-10-10T15:30:32.648438: step 7668, loss 0.0301442, acc 1, learning_rate 0.0001
2017-10-10T15:30:32.859342: step 7669, loss 0.0413763, acc 1, learning_rate 0.0001
2017-10-10T15:30:33.058164: step 7670, loss 0.0615242, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:33.232962: step 7671, loss 0.0653301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:33.420857: step 7672, loss 0.168434, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:33.607648: step 7673, loss 0.123324, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:33.816929: step 7674, loss 0.0575963, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:34.014535: step 7675, loss 0.0106684, acc 1, learning_rate 0.0001
2017-10-10T15:30:34.195224: step 7676, loss 0.0551144, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:34.355725: step 7677, loss 0.0503792, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:34.563332: step 7678, loss 0.0836024, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:34.798487: step 7679, loss 0.0929862, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:34.980997: step 7680, loss 0.103246, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:35.320331: step 7680, loss 0.220624, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7680

2017-10-10T15:30:36.126550: step 7681, loss 0.0905181, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:36.264867: step 7682, loss 0.0629342, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:36.385271: step 7683, loss 0.0873744, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:36.540924: step 7684, loss 0.0706831, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:36.685584: step 7685, loss 0.0254657, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:36.897517: step 7686, loss 0.0402834, acc 1, learning_rate 0.0001
2017-10-10T15:30:37.100164: step 7687, loss 0.0306535, acc 1, learning_rate 0.0001
2017-10-10T15:30:37.292875: step 7688, loss 0.0434001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:37.455766: step 7689, loss 0.1078, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:37.656854: step 7690, loss 0.0484508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:37.892060: step 7691, loss 0.15458, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:38.145657: step 7692, loss 0.0358985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:38.355607: step 7693, loss 0.126039, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:38.557039: step 7694, loss 0.146546, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:38.757704: step 7695, loss 0.0774902, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:38.992913: step 7696, loss 0.122338, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:39.231228: step 7697, loss 0.0484335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:39.400673: step 7698, loss 0.0771777, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:39.574673: step 7699, loss 0.0687182, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:39.776637: step 7700, loss 0.0801035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:40.000161: step 7701, loss 0.05261, acc 1, learning_rate 0.0001
2017-10-10T15:30:40.252867: step 7702, loss 0.0941188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:40.564335: step 7703, loss 0.0978196, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:40.746348: step 7704, loss 0.0885957, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:40.888834: step 7705, loss 0.203984, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:41.036860: step 7706, loss 0.0785852, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:41.165109: step 7707, loss 0.0342251, acc 1, learning_rate 0.0001
2017-10-10T15:30:41.313034: step 7708, loss 0.0879446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:41.555739: step 7709, loss 0.0860924, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:41.767183: step 7710, loss 0.0579311, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:41.901019: step 7711, loss 0.0139778, acc 1, learning_rate 0.0001
2017-10-10T15:30:42.111826: step 7712, loss 0.0537312, acc 1, learning_rate 0.0001
2017-10-10T15:30:42.333206: step 7713, loss 0.045117, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:42.548039: step 7714, loss 0.0640184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:42.781142: step 7715, loss 0.116903, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:42.988422: step 7716, loss 0.0455008, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:43.200811: step 7717, loss 0.0514671, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:43.444970: step 7718, loss 0.0385522, acc 1, learning_rate 0.0001
2017-10-10T15:30:43.668844: step 7719, loss 0.0549502, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:43.829118: step 7720, loss 0.0767109, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:44.196999: step 7720, loss 0.220447, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7720

2017-10-10T15:30:45.169388: step 7721, loss 0.0688062, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:45.382784: step 7722, loss 0.0620314, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:45.532976: step 7723, loss 0.0168936, acc 1, learning_rate 0.0001
2017-10-10T15:30:45.724679: step 7724, loss 0.147906, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:45.921033: step 7725, loss 0.0358153, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:46.108838: step 7726, loss 0.0601352, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:46.344955: step 7727, loss 0.0969567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:46.592999: step 7728, loss 0.0596831, acc 1, learning_rate 0.0001
2017-10-10T15:30:46.801722: step 7729, loss 0.0413859, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:46.933018: step 7730, loss 0.0241286, acc 1, learning_rate 0.0001
2017-10-10T15:30:47.086733: step 7731, loss 0.101335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:47.248887: step 7732, loss 0.0507653, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:47.424936: step 7733, loss 0.0588487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:47.600878: step 7734, loss 0.106843, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:47.792943: step 7735, loss 0.136972, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:48.004941: step 7736, loss 0.0930739, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:48.215761: step 7737, loss 0.0570106, acc 1, learning_rate 0.0001
2017-10-10T15:30:48.417330: step 7738, loss 0.0506927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:48.566601: step 7739, loss 0.0245672, acc 1, learning_rate 0.0001
2017-10-10T15:30:48.769791: step 7740, loss 0.0573297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:49.007038: step 7741, loss 0.0465034, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:49.154057: step 7742, loss 0.0310763, acc 1, learning_rate 0.0001
2017-10-10T15:30:49.330685: step 7743, loss 0.0855806, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:49.537151: step 7744, loss 0.0365717, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:49.736937: step 7745, loss 0.0320742, acc 1, learning_rate 0.0001
2017-10-10T15:30:49.944841: step 7746, loss 0.0413666, acc 1, learning_rate 0.0001
2017-10-10T15:30:50.173946: step 7747, loss 0.0770018, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:50.400804: step 7748, loss 0.0726581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:50.549423: step 7749, loss 0.0889103, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:50.748117: step 7750, loss 0.0828804, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:50.979422: step 7751, loss 0.105539, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:51.188918: step 7752, loss 0.0722994, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:51.359476: step 7753, loss 0.0926214, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:51.597759: step 7754, loss 0.0341491, acc 1, learning_rate 0.0001
2017-10-10T15:30:51.814076: step 7755, loss 0.0405712, acc 1, learning_rate 0.0001
2017-10-10T15:30:52.004991: step 7756, loss 0.0579633, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:52.248044: step 7757, loss 0.114993, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:52.482618: step 7758, loss 0.11253, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:52.651219: step 7759, loss 0.0594745, acc 1, learning_rate 0.0001
2017-10-10T15:30:52.802587: step 7760, loss 0.0987615, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:30:53.087067: step 7760, loss 0.219198, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7760

2017-10-10T15:30:53.937213: step 7761, loss 0.0426002, acc 1, learning_rate 0.0001
2017-10-10T15:30:54.140688: step 7762, loss 0.0465391, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:54.356846: step 7763, loss 0.056145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:54.536912: step 7764, loss 0.0384601, acc 1, learning_rate 0.0001
2017-10-10T15:30:54.729799: step 7765, loss 0.0758908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:54.939251: step 7766, loss 0.11495, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:55.176850: step 7767, loss 0.0744763, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:55.380833: step 7768, loss 0.0406826, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:55.605132: step 7769, loss 0.0793355, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:55.807788: step 7770, loss 0.0534932, acc 1, learning_rate 0.0001
2017-10-10T15:30:56.008893: step 7771, loss 0.0455424, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:56.205411: step 7772, loss 0.0419604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:56.401040: step 7773, loss 0.0885213, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:56.652843: step 7774, loss 0.0850797, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:56.872914: step 7775, loss 0.111947, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:57.095481: step 7776, loss 0.116744, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:57.344916: step 7777, loss 0.135723, acc 0.9375, learning_rate 0.0001
2017-10-10T15:30:57.620892: step 7778, loss 0.0987483, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:57.797018: step 7779, loss 0.101153, acc 0.953125, learning_rate 0.0001
2017-10-10T15:30:57.945173: step 7780, loss 0.0746398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:58.066296: step 7781, loss 0.0634893, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:58.261085: step 7782, loss 0.0458174, acc 1, learning_rate 0.0001
2017-10-10T15:30:58.499065: step 7783, loss 0.0637444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:58.720832: step 7784, loss 0.0481319, acc 1, learning_rate 0.0001
2017-10-10T15:30:58.861372: step 7785, loss 0.0306356, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:59.072187: step 7786, loss 0.0503398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:59.293780: step 7787, loss 0.0749756, acc 0.984375, learning_rate 0.0001
2017-10-10T15:30:59.516851: step 7788, loss 0.0883402, acc 0.96875, learning_rate 0.0001
2017-10-10T15:30:59.709095: step 7789, loss 0.0356585, acc 1, learning_rate 0.0001
2017-10-10T15:30:59.881549: step 7790, loss 0.0444462, acc 1, learning_rate 0.0001
2017-10-10T15:31:00.064843: step 7791, loss 0.0684486, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:00.299849: step 7792, loss 0.0568482, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:00.510252: step 7793, loss 0.0361807, acc 1, learning_rate 0.0001
2017-10-10T15:31:00.713110: step 7794, loss 0.108006, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:00.905128: step 7795, loss 0.055927, acc 1, learning_rate 0.0001
2017-10-10T15:31:01.152822: step 7796, loss 0.116397, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:01.351324: step 7797, loss 0.0558465, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:01.495675: step 7798, loss 0.112601, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:01.727104: step 7799, loss 0.073616, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:01.978040: step 7800, loss 0.106721, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:02.329244: step 7800, loss 0.219112, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7800

2017-10-10T15:31:03.209224: step 7801, loss 0.0820017, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:03.400896: step 7802, loss 0.0425891, acc 1, learning_rate 0.0001
2017-10-10T15:31:03.599932: step 7803, loss 0.0448462, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:03.852880: step 7804, loss 0.0404289, acc 1, learning_rate 0.0001
2017-10-10T15:31:04.109004: step 7805, loss 0.0293856, acc 1, learning_rate 0.0001
2017-10-10T15:31:04.229263: step 7806, loss 0.0835336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:04.394880: step 7807, loss 0.0643487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:04.552756: step 7808, loss 0.0363136, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:04.713476: step 7809, loss 0.115777, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:04.875230: step 7810, loss 0.068809, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:05.076845: step 7811, loss 0.16545, acc 0.9375, learning_rate 0.0001
2017-10-10T15:31:05.248870: step 7812, loss 0.092995, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:05.428733: step 7813, loss 0.0339907, acc 1, learning_rate 0.0001
2017-10-10T15:31:05.647712: step 7814, loss 0.0645534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:05.865654: step 7815, loss 0.0826026, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:06.092461: step 7816, loss 0.0801132, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:06.313136: step 7817, loss 0.0401027, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:06.540845: step 7818, loss 0.0676705, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:06.778937: step 7819, loss 0.0806449, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:06.951171: step 7820, loss 0.100241, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:07.103692: step 7821, loss 0.0286471, acc 1, learning_rate 0.0001
2017-10-10T15:31:07.293399: step 7822, loss 0.0831614, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:07.484226: step 7823, loss 0.0434288, acc 1, learning_rate 0.0001
2017-10-10T15:31:07.701837: step 7824, loss 0.0625709, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:07.949611: step 7825, loss 0.0562735, acc 1, learning_rate 0.0001
2017-10-10T15:31:08.219724: step 7826, loss 0.0400264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:08.360853: step 7827, loss 0.0294874, acc 1, learning_rate 0.0001
2017-10-10T15:31:08.527140: step 7828, loss 0.0409688, acc 1, learning_rate 0.0001
2017-10-10T15:31:08.703291: step 7829, loss 0.0262308, acc 1, learning_rate 0.0001
2017-10-10T15:31:08.868818: step 7830, loss 0.0497535, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:08.980172: step 7831, loss 0.0933051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:09.213353: step 7832, loss 0.055666, acc 1, learning_rate 0.0001
2017-10-10T15:31:09.440673: step 7833, loss 0.0848746, acc 0.984375, learning_rate 0.0001
2017-10-10T15:31:09.640450: step 7834, loss 0.0636684, acc 1, learning_rate 0.0001
2017-10-10T15:31:09.831959: step 7835, loss 0.0608748, acc 1, learning_rate 0.0001
2017-10-10T15:31:10.038820: step 7836, loss 0.0806311, acc 0.96875, learning_rate 0.0001
2017-10-10T15:31:10.256842: step 7837, loss 0.0500314, acc 1, learning_rate 0.0001
2017-10-10T15:31:10.473717: step 7838, loss 0.106727, acc 0.953125, learning_rate 0.0001
2017-10-10T15:31:10.673665: step 7839, loss 0.04223, acc 1, learning_rate 0.0001
2017-10-10T15:31:10.804825: step 7840, loss 0.172737, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-10T15:31:11.204822: step 7840, loss 0.221296, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664623/checkpoints/model-7840

