
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=32

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507653916

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T11:45:23.520882: step 1, loss 5.81322, acc 0.21875, learning_rate 0.005
2017-10-10T11:45:23.765142: step 2, loss 5.4228, acc 0.3125, learning_rate 0.00498
2017-10-10T11:45:23.895030: step 3, loss 4.4888, acc 0.296875, learning_rate 0.00496008
2017-10-10T11:45:24.013409: step 4, loss 4.86932, acc 0.34375, learning_rate 0.00494024
2017-10-10T11:45:24.137727: step 5, loss 5.16012, acc 0.3125, learning_rate 0.00492049
2017-10-10T11:45:24.285768: step 6, loss 4.76356, acc 0.359375, learning_rate 0.00490081
2017-10-10T11:45:24.422344: step 7, loss 3.53273, acc 0.359375, learning_rate 0.00488121
2017-10-10T11:45:24.544270: step 8, loss 3.88678, acc 0.25, learning_rate 0.0048617
2017-10-10T11:45:24.695336: step 9, loss 2.85503, acc 0.46875, learning_rate 0.00484226
2017-10-10T11:45:24.839499: step 10, loss 3.67422, acc 0.421875, learning_rate 0.00482291
2017-10-10T11:45:24.965385: step 11, loss 3.08928, acc 0.46875, learning_rate 0.00480363
2017-10-10T11:45:25.220924: step 12, loss 3.37782, acc 0.484375, learning_rate 0.00478443
2017-10-10T11:45:25.443960: step 13, loss 3.36286, acc 0.5, learning_rate 0.00476531
2017-10-10T11:45:25.631655: step 14, loss 2.73612, acc 0.5, learning_rate 0.00474627
2017-10-10T11:45:25.778749: step 15, loss 3.93629, acc 0.34375, learning_rate 0.0047273
2017-10-10T11:45:25.897644: step 16, loss 2.24636, acc 0.546875, learning_rate 0.00470841
2017-10-10T11:45:26.018594: step 17, loss 2.60146, acc 0.40625, learning_rate 0.0046896
2017-10-10T11:45:26.160971: step 18, loss 2.22919, acc 0.4375, learning_rate 0.00467087
2017-10-10T11:45:26.293664: step 19, loss 2.72216, acc 0.46875, learning_rate 0.00465221
2017-10-10T11:45:26.424140: step 20, loss 2.26842, acc 0.515625, learning_rate 0.00463363
2017-10-10T11:45:26.614189: step 21, loss 2.51649, acc 0.5, learning_rate 0.00461513
2017-10-10T11:45:26.822580: step 22, loss 1.85304, acc 0.5, learning_rate 0.0045967
2017-10-10T11:45:27.034378: step 23, loss 1.88053, acc 0.5625, learning_rate 0.00457834
2017-10-10T11:45:27.212900: step 24, loss 2.10093, acc 0.453125, learning_rate 0.00456006
2017-10-10T11:45:27.365390: step 25, loss 1.96683, acc 0.5, learning_rate 0.00454186
2017-10-10T11:45:27.555913: step 26, loss 1.34293, acc 0.5625, learning_rate 0.00452373
2017-10-10T11:45:27.748911: step 27, loss 1.40981, acc 0.640625, learning_rate 0.00450567
2017-10-10T11:45:27.870006: step 28, loss 1.60544, acc 0.640625, learning_rate 0.00448769
2017-10-10T11:45:28.059705: step 29, loss 1.51885, acc 0.578125, learning_rate 0.00446978
2017-10-10T11:45:28.244912: step 30, loss 1.52784, acc 0.609375, learning_rate 0.00445194
2017-10-10T11:45:28.389439: step 31, loss 1.54651, acc 0.625, learning_rate 0.00443418
2017-10-10T11:45:28.583366: step 32, loss 1.65379, acc 0.59375, learning_rate 0.00441649
2017-10-10T11:45:28.779201: step 33, loss 1.5651, acc 0.71875, learning_rate 0.00439887
2017-10-10T11:45:28.934285: step 34, loss 1.00626, acc 0.71875, learning_rate 0.00438132
2017-10-10T11:45:29.104595: step 35, loss 1.11116, acc 0.671875, learning_rate 0.00436385
2017-10-10T11:45:29.318687: step 36, loss 1.43948, acc 0.671875, learning_rate 0.00434644
2017-10-10T11:45:29.529606: step 37, loss 1.19927, acc 0.609375, learning_rate 0.00432911
2017-10-10T11:45:29.653340: step 38, loss 1.42294, acc 0.546875, learning_rate 0.00431185
2017-10-10T11:45:29.840823: step 39, loss 0.95562, acc 0.71875, learning_rate 0.00429465
2017-10-10T11:45:30.046651: step 40, loss 1.26515, acc 0.65625, learning_rate 0.00427753

Evaluation:
2017-10-10T11:45:30.458193: step 40, loss 0.403159, acc 0.854676

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-40

2017-10-10T11:45:31.607285: step 41, loss 1.08135, acc 0.71875, learning_rate 0.00426048
2017-10-10T11:45:31.815978: step 42, loss 1.33547, acc 0.640625, learning_rate 0.0042435
2017-10-10T11:45:31.981012: step 43, loss 1.06817, acc 0.59375, learning_rate 0.00422659
2017-10-10T11:45:32.149092: step 44, loss 1.13065, acc 0.671875, learning_rate 0.00420974
2017-10-10T11:45:32.336867: step 45, loss 0.842012, acc 0.734375, learning_rate 0.00419297
2017-10-10T11:45:32.574087: step 46, loss 1.65846, acc 0.546875, learning_rate 0.00417626
2017-10-10T11:45:32.732224: step 47, loss 1.23707, acc 0.65625, learning_rate 0.00415962
2017-10-10T11:45:32.928311: step 48, loss 1.74833, acc 0.65625, learning_rate 0.00414305
2017-10-10T11:45:33.105044: step 49, loss 0.841142, acc 0.71875, learning_rate 0.00412655
2017-10-10T11:45:33.268854: step 50, loss 0.805399, acc 0.75, learning_rate 0.00411011
2017-10-10T11:45:33.449252: step 51, loss 0.768514, acc 0.765625, learning_rate 0.00409375
2017-10-10T11:45:33.633411: step 52, loss 0.779816, acc 0.78125, learning_rate 0.00407744
2017-10-10T11:45:33.801782: step 53, loss 1.04616, acc 0.734375, learning_rate 0.00406121
2017-10-10T11:45:33.936977: step 54, loss 0.620187, acc 0.8125, learning_rate 0.00404504
2017-10-10T11:45:34.153434: step 55, loss 0.875623, acc 0.765625, learning_rate 0.00402894
2017-10-10T11:45:34.380929: step 56, loss 0.581383, acc 0.8125, learning_rate 0.0040129
2017-10-10T11:45:34.608889: step 57, loss 0.523977, acc 0.828125, learning_rate 0.00399693
2017-10-10T11:45:34.785170: step 58, loss 0.735785, acc 0.78125, learning_rate 0.00398102
2017-10-10T11:45:34.919409: step 59, loss 0.888167, acc 0.75, learning_rate 0.00396518
2017-10-10T11:45:35.058815: step 60, loss 0.908548, acc 0.625, learning_rate 0.00394941
2017-10-10T11:45:35.189845: step 61, loss 0.879957, acc 0.6875, learning_rate 0.00393369
2017-10-10T11:45:35.307728: step 62, loss 0.895444, acc 0.71875, learning_rate 0.00391804
2017-10-10T11:45:35.444694: step 63, loss 0.878669, acc 0.734375, learning_rate 0.00390246
2017-10-10T11:45:35.578623: step 64, loss 0.963369, acc 0.71875, learning_rate 0.00388694
2017-10-10T11:45:35.713115: step 65, loss 0.438189, acc 0.859375, learning_rate 0.00387148
2017-10-10T11:45:35.861616: step 66, loss 1.00148, acc 0.71875, learning_rate 0.00385609
2017-10-10T11:45:36.056875: step 67, loss 0.844901, acc 0.734375, learning_rate 0.00384076
2017-10-10T11:45:36.279318: step 68, loss 1.06412, acc 0.703125, learning_rate 0.00382549
2017-10-10T11:45:36.424865: step 69, loss 0.668531, acc 0.734375, learning_rate 0.00381028
2017-10-10T11:45:36.569487: step 70, loss 0.853971, acc 0.65625, learning_rate 0.00379514
2017-10-10T11:45:36.717786: step 71, loss 0.879999, acc 0.75, learning_rate 0.00378005
2017-10-10T11:45:36.852864: step 72, loss 0.469935, acc 0.84375, learning_rate 0.00376503
2017-10-10T11:45:36.981047: step 73, loss 0.945483, acc 0.765625, learning_rate 0.00375007
2017-10-10T11:45:37.141898: step 74, loss 0.519224, acc 0.8125, learning_rate 0.00373517
2017-10-10T11:45:37.278083: step 75, loss 0.305854, acc 0.890625, learning_rate 0.00372034
2017-10-10T11:45:37.483319: step 76, loss 0.440938, acc 0.828125, learning_rate 0.00370556
2017-10-10T11:45:37.656830: step 77, loss 0.765065, acc 0.75, learning_rate 0.00369084
2017-10-10T11:45:37.846653: step 78, loss 0.558761, acc 0.78125, learning_rate 0.00367619
2017-10-10T11:45:38.052880: step 79, loss 0.491928, acc 0.84375, learning_rate 0.00366159
2017-10-10T11:45:38.188388: step 80, loss 0.812061, acc 0.75, learning_rate 0.00364705

Evaluation:
2017-10-10T11:45:38.600874: step 80, loss 0.346689, acc 0.873381

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-80

2017-10-10T11:45:39.748512: step 81, loss 0.755971, acc 0.71875, learning_rate 0.00363257
2017-10-10T11:45:39.942812: step 82, loss 0.733404, acc 0.78125, learning_rate 0.00361815
2017-10-10T11:45:40.088998: step 83, loss 0.503668, acc 0.734375, learning_rate 0.00360379
2017-10-10T11:45:40.268727: step 84, loss 0.884109, acc 0.71875, learning_rate 0.00358949
2017-10-10T11:45:40.480892: step 85, loss 0.777274, acc 0.796875, learning_rate 0.00357525
2017-10-10T11:45:40.654652: step 86, loss 0.748297, acc 0.78125, learning_rate 0.00356106
2017-10-10T11:45:40.792847: step 87, loss 0.628066, acc 0.78125, learning_rate 0.00354694
2017-10-10T11:45:41.003812: step 88, loss 0.651492, acc 0.859375, learning_rate 0.00353287
2017-10-10T11:45:41.151711: step 89, loss 0.620008, acc 0.78125, learning_rate 0.00351885
2017-10-10T11:45:41.306696: step 90, loss 0.535966, acc 0.828125, learning_rate 0.0035049
2017-10-10T11:45:41.500000: step 91, loss 0.581443, acc 0.765625, learning_rate 0.003491
2017-10-10T11:45:41.713298: step 92, loss 0.391747, acc 0.859375, learning_rate 0.00347716
2017-10-10T11:45:41.877040: step 93, loss 0.485363, acc 0.859375, learning_rate 0.00346338
2017-10-10T11:45:42.024913: step 94, loss 0.407715, acc 0.890625, learning_rate 0.00344965
2017-10-10T11:45:42.221509: step 95, loss 0.542517, acc 0.78125, learning_rate 0.00343597
2017-10-10T11:45:42.395236: step 96, loss 0.692942, acc 0.78125, learning_rate 0.00342236
2017-10-10T11:45:42.552573: step 97, loss 0.733145, acc 0.796875, learning_rate 0.0034088
2017-10-10T11:45:42.694662: step 98, loss 0.60957, acc 0.745098, learning_rate 0.00339529
2017-10-10T11:45:42.860071: step 99, loss 0.667047, acc 0.78125, learning_rate 0.00338184
2017-10-10T11:45:43.043660: step 100, loss 0.832252, acc 0.75, learning_rate 0.00336844
2017-10-10T11:45:43.250145: step 101, loss 0.738157, acc 0.75, learning_rate 0.0033551
2017-10-10T11:45:43.395818: step 102, loss 0.629406, acc 0.765625, learning_rate 0.00334182
2017-10-10T11:45:43.582983: step 103, loss 0.563406, acc 0.8125, learning_rate 0.00332858
2017-10-10T11:45:43.784324: step 104, loss 0.457434, acc 0.828125, learning_rate 0.00331541
2017-10-10T11:45:44.010448: step 105, loss 0.485974, acc 0.84375, learning_rate 0.00330228
2017-10-10T11:45:44.160896: step 106, loss 0.504465, acc 0.84375, learning_rate 0.00328921
2017-10-10T11:45:44.325101: step 107, loss 0.508236, acc 0.8125, learning_rate 0.00327619
2017-10-10T11:45:44.518553: step 108, loss 0.551501, acc 0.765625, learning_rate 0.00326323
2017-10-10T11:45:44.735704: step 109, loss 0.366577, acc 0.875, learning_rate 0.00325032
2017-10-10T11:45:44.918006: step 110, loss 0.664575, acc 0.8125, learning_rate 0.00323746
2017-10-10T11:45:45.072921: step 111, loss 0.499996, acc 0.828125, learning_rate 0.00322465
2017-10-10T11:45:45.245605: step 112, loss 0.369246, acc 0.890625, learning_rate 0.0032119
2017-10-10T11:45:45.502065: step 113, loss 0.439773, acc 0.8125, learning_rate 0.0031992
2017-10-10T11:45:45.718905: step 114, loss 0.548084, acc 0.75, learning_rate 0.00318655
2017-10-10T11:45:45.841955: step 115, loss 0.363545, acc 0.875, learning_rate 0.00317395
2017-10-10T11:45:45.988397: step 116, loss 0.677043, acc 0.75, learning_rate 0.0031614
2017-10-10T11:45:46.107011: step 117, loss 0.444036, acc 0.90625, learning_rate 0.0031489
2017-10-10T11:45:46.230625: step 118, loss 0.61431, acc 0.796875, learning_rate 0.00313646
2017-10-10T11:45:46.392527: step 119, loss 0.662372, acc 0.8125, learning_rate 0.00312407
2017-10-10T11:45:46.553466: step 120, loss 0.467122, acc 0.828125, learning_rate 0.00311172

Evaluation:
2017-10-10T11:45:46.903298: step 120, loss 0.322799, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-120

2017-10-10T11:45:47.783305: step 121, loss 0.404495, acc 0.875, learning_rate 0.00309943
2017-10-10T11:45:47.860282: step 122, loss 0.352644, acc 0.84375, learning_rate 0.00308719
2017-10-10T11:45:47.937748: step 123, loss 0.391488, acc 0.828125, learning_rate 0.00307499
2017-10-10T11:45:48.012915: step 124, loss 0.467282, acc 0.84375, learning_rate 0.00306285
2017-10-10T11:45:48.087776: step 125, loss 0.356006, acc 0.875, learning_rate 0.00305076
2017-10-10T11:45:48.163114: step 126, loss 0.335533, acc 0.90625, learning_rate 0.00303871
2017-10-10T11:45:48.238617: step 127, loss 0.42802, acc 0.828125, learning_rate 0.00302672
2017-10-10T11:45:48.366555: step 128, loss 0.411192, acc 0.8125, learning_rate 0.00301477
2017-10-10T11:45:48.655395: step 129, loss 0.499477, acc 0.796875, learning_rate 0.00300287
2017-10-10T11:45:48.795477: step 130, loss 0.517529, acc 0.828125, learning_rate 0.00299102
2017-10-10T11:45:48.873815: step 131, loss 0.273269, acc 0.890625, learning_rate 0.00297922
2017-10-10T11:45:48.949929: step 132, loss 0.448596, acc 0.796875, learning_rate 0.00296747
2017-10-10T11:45:49.025965: step 133, loss 0.47502, acc 0.84375, learning_rate 0.00295577
2017-10-10T11:45:49.102398: step 134, loss 0.430456, acc 0.84375, learning_rate 0.00294411
2017-10-10T11:45:49.177704: step 135, loss 0.401845, acc 0.828125, learning_rate 0.0029325
2017-10-10T11:45:49.253311: step 136, loss 0.352385, acc 0.90625, learning_rate 0.00292094
2017-10-10T11:45:49.328784: step 137, loss 0.60466, acc 0.796875, learning_rate 0.00290943
2017-10-10T11:45:49.404810: step 138, loss 0.524072, acc 0.8125, learning_rate 0.00289796
2017-10-10T11:45:49.480534: step 139, loss 0.416235, acc 0.875, learning_rate 0.00288654
2017-10-10T11:45:49.556357: step 140, loss 0.186623, acc 0.921875, learning_rate 0.00287516
2017-10-10T11:45:49.668888: step 141, loss 0.480494, acc 0.796875, learning_rate 0.00286384
2017-10-10T11:45:49.780845: step 142, loss 0.674238, acc 0.765625, learning_rate 0.00285256
2017-10-10T11:45:49.913442: step 143, loss 0.601818, acc 0.78125, learning_rate 0.00284132
2017-10-10T11:45:50.058623: step 144, loss 0.388123, acc 0.859375, learning_rate 0.00283013
2017-10-10T11:45:50.218130: step 145, loss 0.277716, acc 0.953125, learning_rate 0.00281899
2017-10-10T11:45:50.368039: step 146, loss 0.787952, acc 0.8125, learning_rate 0.00280789
2017-10-10T11:45:50.453671: step 147, loss 0.478465, acc 0.84375, learning_rate 0.00279684
2017-10-10T11:45:50.536769: step 148, loss 0.377704, acc 0.828125, learning_rate 0.00278583
2017-10-10T11:45:50.654040: step 149, loss 0.329121, acc 0.875, learning_rate 0.00277486
2017-10-10T11:45:50.776841: step 150, loss 0.665871, acc 0.78125, learning_rate 0.00276395
2017-10-10T11:45:50.918798: step 151, loss 0.315096, acc 0.890625, learning_rate 0.00275307
2017-10-10T11:45:51.076383: step 152, loss 0.425789, acc 0.90625, learning_rate 0.00274224
2017-10-10T11:45:51.283831: step 153, loss 0.447711, acc 0.84375, learning_rate 0.00273146
2017-10-10T11:45:51.468822: step 154, loss 0.361473, acc 0.90625, learning_rate 0.00272072
2017-10-10T11:45:51.621355: step 155, loss 0.294893, acc 0.875, learning_rate 0.00271002
2017-10-10T11:45:51.816838: step 156, loss 0.607365, acc 0.859375, learning_rate 0.00269937
2017-10-10T11:45:52.019456: step 157, loss 0.247376, acc 0.921875, learning_rate 0.00268876
2017-10-10T11:45:52.212465: step 158, loss 0.326131, acc 0.875, learning_rate 0.00267819
2017-10-10T11:45:52.381534: step 159, loss 0.709645, acc 0.75, learning_rate 0.00266767
2017-10-10T11:45:52.558264: step 160, loss 0.488033, acc 0.8125, learning_rate 0.00265719

Evaluation:
2017-10-10T11:45:52.944818: step 160, loss 0.306004, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-160

2017-10-10T11:45:54.089458: step 161, loss 0.31393, acc 0.859375, learning_rate 0.00264675
2017-10-10T11:45:54.208913: step 162, loss 0.295709, acc 0.921875, learning_rate 0.00263635
2017-10-10T11:45:54.407119: step 163, loss 0.539019, acc 0.8125, learning_rate 0.002626
2017-10-10T11:45:54.614289: step 164, loss 0.403982, acc 0.8125, learning_rate 0.00261569
2017-10-10T11:45:54.716950: step 165, loss 0.331926, acc 0.875, learning_rate 0.00260542
2017-10-10T11:45:54.937340: step 166, loss 0.438074, acc 0.828125, learning_rate 0.0025952
2017-10-10T11:45:55.081132: step 167, loss 0.248441, acc 0.90625, learning_rate 0.00258501
2017-10-10T11:45:55.240866: step 168, loss 0.416607, acc 0.859375, learning_rate 0.00257487
2017-10-10T11:45:55.452855: step 169, loss 0.349273, acc 0.84375, learning_rate 0.00256477
2017-10-10T11:45:55.640859: step 170, loss 0.408927, acc 0.875, learning_rate 0.0025547
2017-10-10T11:45:55.829647: step 171, loss 0.423185, acc 0.84375, learning_rate 0.00254469
2017-10-10T11:45:56.008772: step 172, loss 0.564282, acc 0.84375, learning_rate 0.00253471
2017-10-10T11:45:56.208316: step 173, loss 0.33478, acc 0.859375, learning_rate 0.00252477
2017-10-10T11:45:56.361015: step 174, loss 0.545407, acc 0.859375, learning_rate 0.00251487
2017-10-10T11:45:56.564940: step 175, loss 0.481097, acc 0.84375, learning_rate 0.00250501
2017-10-10T11:45:56.748907: step 176, loss 0.243835, acc 0.921875, learning_rate 0.0024952
2017-10-10T11:45:56.976974: step 177, loss 0.636824, acc 0.796875, learning_rate 0.00248542
2017-10-10T11:45:57.186126: step 178, loss 0.432525, acc 0.828125, learning_rate 0.00247568
2017-10-10T11:45:57.359506: step 179, loss 0.368941, acc 0.859375, learning_rate 0.00246599
2017-10-10T11:45:57.511181: step 180, loss 0.481534, acc 0.828125, learning_rate 0.00245633
2017-10-10T11:45:57.661608: step 181, loss 0.500131, acc 0.828125, learning_rate 0.00244671
2017-10-10T11:45:57.772832: step 182, loss 0.743064, acc 0.78125, learning_rate 0.00243713
2017-10-10T11:45:57.933746: step 183, loss 0.311683, acc 0.921875, learning_rate 0.00242759
2017-10-10T11:45:58.092871: step 184, loss 0.546364, acc 0.828125, learning_rate 0.00241809
2017-10-10T11:45:58.276982: step 185, loss 0.406634, acc 0.8125, learning_rate 0.00240863
2017-10-10T11:45:58.412859: step 186, loss 0.255218, acc 0.921875, learning_rate 0.00239921
2017-10-10T11:45:58.614170: step 187, loss 0.444082, acc 0.875, learning_rate 0.00238982
2017-10-10T11:45:58.826218: step 188, loss 0.219926, acc 0.953125, learning_rate 0.00238048
2017-10-10T11:45:59.021952: step 189, loss 0.279473, acc 0.890625, learning_rate 0.00237117
2017-10-10T11:45:59.171397: step 190, loss 0.368209, acc 0.890625, learning_rate 0.0023619
2017-10-10T11:45:59.350185: step 191, loss 0.483325, acc 0.875, learning_rate 0.00235267
2017-10-10T11:45:59.568434: step 192, loss 0.519527, acc 0.828125, learning_rate 0.00234347
2017-10-10T11:45:59.780724: step 193, loss 0.233049, acc 0.890625, learning_rate 0.00233431
2017-10-10T11:45:59.958737: step 194, loss 0.266656, acc 0.9375, learning_rate 0.00232519
2017-10-10T11:46:00.108882: step 195, loss 0.354908, acc 0.84375, learning_rate 0.00231611
2017-10-10T11:46:00.276146: step 196, loss 0.455232, acc 0.921569, learning_rate 0.00230707
2017-10-10T11:46:00.495371: step 197, loss 0.179275, acc 0.9375, learning_rate 0.00229806
2017-10-10T11:46:00.688270: step 198, loss 0.328541, acc 0.921875, learning_rate 0.00228908
2017-10-10T11:46:00.845065: step 199, loss 0.32776, acc 0.90625, learning_rate 0.00228015
2017-10-10T11:46:01.056872: step 200, loss 0.462641, acc 0.875, learning_rate 0.00227125

Evaluation:
2017-10-10T11:46:01.502440: step 200, loss 0.310522, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-200

2017-10-10T11:46:02.662383: step 201, loss 0.424384, acc 0.828125, learning_rate 0.00226239
2017-10-10T11:46:02.862793: step 202, loss 0.468099, acc 0.84375, learning_rate 0.00225356
2017-10-10T11:46:03.044967: step 203, loss 0.299272, acc 0.90625, learning_rate 0.00224477
2017-10-10T11:46:03.173389: step 204, loss 0.591685, acc 0.765625, learning_rate 0.00223602
2017-10-10T11:46:03.392298: step 205, loss 0.264923, acc 0.859375, learning_rate 0.0022273
2017-10-10T11:46:03.560898: step 206, loss 0.292848, acc 0.90625, learning_rate 0.00221862
2017-10-10T11:46:03.708135: step 207, loss 0.270998, acc 0.90625, learning_rate 0.00220997
2017-10-10T11:46:03.921747: step 208, loss 0.174335, acc 0.9375, learning_rate 0.00220136
2017-10-10T11:46:04.134433: step 209, loss 0.3203, acc 0.890625, learning_rate 0.00219278
2017-10-10T11:46:04.285092: step 210, loss 0.259354, acc 0.90625, learning_rate 0.00218424
2017-10-10T11:46:04.446621: step 211, loss 0.36472, acc 0.828125, learning_rate 0.00217573
2017-10-10T11:46:04.642515: step 212, loss 0.200562, acc 0.953125, learning_rate 0.00216726
2017-10-10T11:46:04.785177: step 213, loss 0.26923, acc 0.90625, learning_rate 0.00215882
2017-10-10T11:46:04.943009: step 214, loss 0.407567, acc 0.859375, learning_rate 0.00215041
2017-10-10T11:46:05.157788: step 215, loss 0.206353, acc 0.953125, learning_rate 0.00214204
2017-10-10T11:46:05.375310: step 216, loss 0.203978, acc 0.921875, learning_rate 0.00213371
2017-10-10T11:46:05.521231: step 217, loss 0.296305, acc 0.890625, learning_rate 0.00212541
2017-10-10T11:46:05.693781: step 218, loss 0.313259, acc 0.890625, learning_rate 0.00211714
2017-10-10T11:46:05.889509: step 219, loss 0.498928, acc 0.859375, learning_rate 0.00210891
2017-10-10T11:46:06.092635: step 220, loss 0.401319, acc 0.84375, learning_rate 0.00210071
2017-10-10T11:46:06.264947: step 221, loss 0.346857, acc 0.875, learning_rate 0.00209254
2017-10-10T11:46:06.420883: step 222, loss 0.197296, acc 0.921875, learning_rate 0.00208441
2017-10-10T11:46:06.645649: step 223, loss 0.35431, acc 0.859375, learning_rate 0.00207631
2017-10-10T11:46:06.831849: step 224, loss 0.394977, acc 0.890625, learning_rate 0.00206824
2017-10-10T11:46:06.971539: step 225, loss 0.342112, acc 0.875, learning_rate 0.00206021
2017-10-10T11:46:07.149843: step 226, loss 0.259319, acc 0.921875, learning_rate 0.00205221
2017-10-10T11:46:07.336476: step 227, loss 0.457656, acc 0.84375, learning_rate 0.00204424
2017-10-10T11:46:07.490697: step 228, loss 0.537462, acc 0.859375, learning_rate 0.0020363
2017-10-10T11:46:07.701603: step 229, loss 0.29219, acc 0.921875, learning_rate 0.0020284
2017-10-10T11:46:07.944250: step 230, loss 0.412204, acc 0.84375, learning_rate 0.00202053
2017-10-10T11:46:08.072495: step 231, loss 0.538274, acc 0.828125, learning_rate 0.00201269
2017-10-10T11:46:08.191219: step 232, loss 0.334077, acc 0.890625, learning_rate 0.00200488
2017-10-10T11:46:08.344171: step 233, loss 0.161078, acc 0.953125, learning_rate 0.00199711
2017-10-10T11:46:08.476857: step 234, loss 0.292749, acc 0.890625, learning_rate 0.00198936
2017-10-10T11:46:08.605570: step 235, loss 0.197905, acc 0.953125, learning_rate 0.00198165
2017-10-10T11:46:08.761397: step 236, loss 0.265381, acc 0.890625, learning_rate 0.00197397
2017-10-10T11:46:08.916298: step 237, loss 0.424676, acc 0.828125, learning_rate 0.00196632
2017-10-10T11:46:09.046219: step 238, loss 0.425872, acc 0.90625, learning_rate 0.0019587
2017-10-10T11:46:09.217299: step 239, loss 0.330547, acc 0.859375, learning_rate 0.00195112
2017-10-10T11:46:09.299519: step 240, loss 0.283668, acc 0.890625, learning_rate 0.00194356

Evaluation:
2017-10-10T11:46:09.477445: step 240, loss 0.288885, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-240

2017-10-10T11:46:10.988069: step 241, loss 0.203401, acc 0.921875, learning_rate 0.00193604
2017-10-10T11:46:11.182542: step 242, loss 0.344289, acc 0.890625, learning_rate 0.00192854
2017-10-10T11:46:11.344919: step 243, loss 0.219583, acc 0.9375, learning_rate 0.00192108
2017-10-10T11:46:11.525081: step 244, loss 0.355985, acc 0.875, learning_rate 0.00191364
2017-10-10T11:46:11.736977: step 245, loss 0.380461, acc 0.8125, learning_rate 0.00190624
2017-10-10T11:46:11.900714: step 246, loss 0.286468, acc 0.84375, learning_rate 0.00189887
2017-10-10T11:46:12.104888: step 247, loss 0.547646, acc 0.84375, learning_rate 0.00189153
2017-10-10T11:46:12.356870: step 248, loss 0.347364, acc 0.875, learning_rate 0.00188421
2017-10-10T11:46:12.556824: step 249, loss 0.324955, acc 0.875, learning_rate 0.00187693
2017-10-10T11:46:12.688993: step 250, loss 0.43567, acc 0.84375, learning_rate 0.00186968
2017-10-10T11:46:12.824991: step 251, loss 0.436371, acc 0.84375, learning_rate 0.00186245
2017-10-10T11:46:12.964839: step 252, loss 0.237423, acc 0.875, learning_rate 0.00185526
2017-10-10T11:46:13.109076: step 253, loss 0.283213, acc 0.875, learning_rate 0.0018481
2017-10-10T11:46:13.197755: step 254, loss 0.359707, acc 0.859375, learning_rate 0.00184096
2017-10-10T11:46:13.345920: step 255, loss 0.262652, acc 0.859375, learning_rate 0.00183385
2017-10-10T11:46:13.516243: step 256, loss 0.351823, acc 0.84375, learning_rate 0.00182678
2017-10-10T11:46:13.710129: step 257, loss 0.397935, acc 0.84375, learning_rate 0.00181973
2017-10-10T11:46:13.920562: step 258, loss 0.320486, acc 0.90625, learning_rate 0.00181271
2017-10-10T11:46:14.096889: step 259, loss 0.363971, acc 0.90625, learning_rate 0.00180572
2017-10-10T11:46:14.280836: step 260, loss 0.276531, acc 0.90625, learning_rate 0.00179876
2017-10-10T11:46:14.450602: step 261, loss 0.370207, acc 0.84375, learning_rate 0.00179182
2017-10-10T11:46:14.654736: step 262, loss 0.326943, acc 0.859375, learning_rate 0.00178492
2017-10-10T11:46:14.812923: step 263, loss 0.365903, acc 0.859375, learning_rate 0.00177804
2017-10-10T11:46:14.986507: step 264, loss 0.337303, acc 0.9375, learning_rate 0.00177119
2017-10-10T11:46:15.172594: step 265, loss 0.234192, acc 0.921875, learning_rate 0.00176437
2017-10-10T11:46:15.374052: step 266, loss 0.21988, acc 0.96875, learning_rate 0.00175758
2017-10-10T11:46:15.522225: step 267, loss 0.332622, acc 0.9375, learning_rate 0.00175081
2017-10-10T11:46:15.681732: step 268, loss 0.274324, acc 0.890625, learning_rate 0.00174407
2017-10-10T11:46:15.874318: step 269, loss 0.513902, acc 0.828125, learning_rate 0.00173736
2017-10-10T11:46:16.028941: step 270, loss 0.274738, acc 0.875, learning_rate 0.00173068
2017-10-10T11:46:16.177873: step 271, loss 0.203064, acc 0.96875, learning_rate 0.00172402
2017-10-10T11:46:16.373222: step 272, loss 0.374336, acc 0.875, learning_rate 0.00171739
2017-10-10T11:46:16.569140: step 273, loss 0.236891, acc 0.890625, learning_rate 0.00171079
2017-10-10T11:46:16.732909: step 274, loss 0.357887, acc 0.84375, learning_rate 0.00170422
2017-10-10T11:46:16.913744: step 275, loss 0.278093, acc 0.921875, learning_rate 0.00169767
2017-10-10T11:46:17.114101: step 276, loss 0.298444, acc 0.90625, learning_rate 0.00169115
2017-10-10T11:46:17.290329: step 277, loss 0.497471, acc 0.890625, learning_rate 0.00168465
2017-10-10T11:46:17.480858: step 278, loss 0.285874, acc 0.90625, learning_rate 0.00167818
2017-10-10T11:46:17.700888: step 279, loss 0.463912, acc 0.859375, learning_rate 0.00167174
2017-10-10T11:46:17.880359: step 280, loss 0.171775, acc 0.953125, learning_rate 0.00166533

Evaluation:
2017-10-10T11:46:18.219147: step 280, loss 0.283443, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-280

2017-10-10T11:46:19.282183: step 281, loss 0.348964, acc 0.875, learning_rate 0.00165894
2017-10-10T11:46:19.385837: step 282, loss 0.610572, acc 0.75, learning_rate 0.00165257
2017-10-10T11:46:19.522339: step 283, loss 0.233999, acc 0.9375, learning_rate 0.00164624
2017-10-10T11:46:19.714173: step 284, loss 0.408736, acc 0.84375, learning_rate 0.00163993
2017-10-10T11:46:19.871653: step 285, loss 0.281965, acc 0.875, learning_rate 0.00163364
2017-10-10T11:46:20.043680: step 286, loss 0.21661, acc 0.921875, learning_rate 0.00162738
2017-10-10T11:46:20.252895: step 287, loss 0.197369, acc 0.921875, learning_rate 0.00162115
2017-10-10T11:46:20.453650: step 288, loss 0.448764, acc 0.8125, learning_rate 0.00161494
2017-10-10T11:46:20.647426: step 289, loss 0.492209, acc 0.8125, learning_rate 0.00160875
2017-10-10T11:46:20.795194: step 290, loss 0.452004, acc 0.84375, learning_rate 0.00160259
2017-10-10T11:46:20.952253: step 291, loss 0.286, acc 0.90625, learning_rate 0.00159646
2017-10-10T11:46:21.124427: step 292, loss 0.312629, acc 0.890625, learning_rate 0.00159035
2017-10-10T11:46:21.264624: step 293, loss 0.428312, acc 0.875, learning_rate 0.00158427
2017-10-10T11:46:21.439632: step 294, loss 0.324504, acc 0.843137, learning_rate 0.00157821
2017-10-10T11:46:21.626142: step 295, loss 0.292613, acc 0.890625, learning_rate 0.00157218
2017-10-10T11:46:21.804430: step 296, loss 0.317586, acc 0.875, learning_rate 0.00156617
2017-10-10T11:46:21.964698: step 297, loss 0.300991, acc 0.9375, learning_rate 0.00156018
2017-10-10T11:46:22.170463: step 298, loss 0.253251, acc 0.921875, learning_rate 0.00155422
2017-10-10T11:46:22.392841: step 299, loss 0.302169, acc 0.890625, learning_rate 0.00154829
2017-10-10T11:46:22.525188: step 300, loss 0.198896, acc 0.921875, learning_rate 0.00154238
2017-10-10T11:46:22.697046: step 301, loss 0.395753, acc 0.84375, learning_rate 0.00153649
2017-10-10T11:46:22.902918: step 302, loss 0.21915, acc 0.9375, learning_rate 0.00153063
2017-10-10T11:46:23.024992: step 303, loss 0.302322, acc 0.90625, learning_rate 0.00152479
2017-10-10T11:46:23.321178: step 304, loss 0.256391, acc 0.90625, learning_rate 0.00151897
2017-10-10T11:46:23.548299: step 305, loss 0.220845, acc 0.9375, learning_rate 0.00151318
2017-10-10T11:46:23.704727: step 306, loss 0.480877, acc 0.8125, learning_rate 0.00150741
2017-10-10T11:46:23.858793: step 307, loss 0.325164, acc 0.875, learning_rate 0.00150167
2017-10-10T11:46:24.024226: step 308, loss 0.174715, acc 0.96875, learning_rate 0.00149594
2017-10-10T11:46:24.148852: step 309, loss 0.23583, acc 0.90625, learning_rate 0.00149025
2017-10-10T11:46:24.260823: step 310, loss 0.182513, acc 0.96875, learning_rate 0.00148457
2017-10-10T11:46:24.416541: step 311, loss 0.266018, acc 0.921875, learning_rate 0.00147892
2017-10-10T11:46:24.631269: step 312, loss 0.227545, acc 0.953125, learning_rate 0.00147329
2017-10-10T11:46:24.777306: step 313, loss 0.204274, acc 0.9375, learning_rate 0.00146769
2017-10-10T11:46:24.950690: step 314, loss 0.473679, acc 0.890625, learning_rate 0.0014621
2017-10-10T11:46:25.163545: step 315, loss 0.312056, acc 0.890625, learning_rate 0.00145654
2017-10-10T11:46:25.322340: step 316, loss 0.299857, acc 0.890625, learning_rate 0.00145101
2017-10-10T11:46:25.464985: step 317, loss 0.293802, acc 0.890625, learning_rate 0.00144549
2017-10-10T11:46:25.664431: step 318, loss 0.543856, acc 0.765625, learning_rate 0.00144
2017-10-10T11:46:25.865591: step 319, loss 0.366819, acc 0.859375, learning_rate 0.00143453
2017-10-10T11:46:26.009176: step 320, loss 0.373959, acc 0.859375, learning_rate 0.00142908

Evaluation:
2017-10-10T11:46:26.388376: step 320, loss 0.274358, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-320

2017-10-10T11:46:27.560168: step 321, loss 0.313913, acc 0.859375, learning_rate 0.00142366
2017-10-10T11:46:27.765609: step 322, loss 0.165658, acc 0.953125, learning_rate 0.00141826
2017-10-10T11:46:27.901273: step 323, loss 0.260605, acc 0.90625, learning_rate 0.00141288
2017-10-10T11:46:28.081314: step 324, loss 0.26695, acc 0.890625, learning_rate 0.00140752
2017-10-10T11:46:28.310089: step 325, loss 0.224739, acc 0.90625, learning_rate 0.00140218
2017-10-10T11:46:28.501058: step 326, loss 0.232526, acc 0.953125, learning_rate 0.00139686
2017-10-10T11:46:28.658951: step 327, loss 0.250912, acc 0.921875, learning_rate 0.00139157
2017-10-10T11:46:28.808339: step 328, loss 0.299061, acc 0.890625, learning_rate 0.0013863
2017-10-10T11:46:29.032851: step 329, loss 0.232486, acc 0.875, learning_rate 0.00138105
2017-10-10T11:46:29.282236: step 330, loss 0.268886, acc 0.90625, learning_rate 0.00137582
2017-10-10T11:46:29.402376: step 331, loss 0.351333, acc 0.921875, learning_rate 0.00137061
2017-10-10T11:46:29.553265: step 332, loss 0.193428, acc 0.953125, learning_rate 0.00136543
2017-10-10T11:46:29.696856: step 333, loss 0.292801, acc 0.890625, learning_rate 0.00136026
2017-10-10T11:46:29.828374: step 334, loss 0.250307, acc 0.953125, learning_rate 0.00135512
2017-10-10T11:46:29.957314: step 335, loss 0.212381, acc 0.921875, learning_rate 0.00134999
2017-10-10T11:46:30.121174: step 336, loss 0.476695, acc 0.859375, learning_rate 0.00134489
2017-10-10T11:46:30.300885: step 337, loss 0.383389, acc 0.875, learning_rate 0.00133981
2017-10-10T11:46:30.473612: step 338, loss 0.454072, acc 0.828125, learning_rate 0.00133475
2017-10-10T11:46:30.648832: step 339, loss 0.229605, acc 0.90625, learning_rate 0.00132971
2017-10-10T11:46:30.840900: step 340, loss 0.442413, acc 0.90625, learning_rate 0.00132469
2017-10-10T11:46:30.972632: step 341, loss 0.249195, acc 0.90625, learning_rate 0.00131969
2017-10-10T11:46:31.141986: step 342, loss 0.298095, acc 0.921875, learning_rate 0.00131471
2017-10-10T11:46:31.359937: step 343, loss 0.171164, acc 0.96875, learning_rate 0.00130975
2017-10-10T11:46:31.577142: step 344, loss 0.22892, acc 0.90625, learning_rate 0.00130482
2017-10-10T11:46:31.728967: step 345, loss 0.445604, acc 0.828125, learning_rate 0.0012999
2017-10-10T11:46:31.893779: step 346, loss 0.397197, acc 0.828125, learning_rate 0.001295
2017-10-10T11:46:32.091072: step 347, loss 0.299906, acc 0.90625, learning_rate 0.00129012
2017-10-10T11:46:32.301369: step 348, loss 0.199212, acc 0.921875, learning_rate 0.00128527
2017-10-10T11:46:32.504067: step 349, loss 0.297909, acc 0.890625, learning_rate 0.00128043
2017-10-10T11:46:32.661007: step 350, loss 0.189884, acc 0.9375, learning_rate 0.00127561
2017-10-10T11:46:32.833247: step 351, loss 0.135382, acc 0.953125, learning_rate 0.00127081
2017-10-10T11:46:33.068897: step 352, loss 0.185935, acc 0.9375, learning_rate 0.00126603
2017-10-10T11:46:33.256853: step 353, loss 0.496293, acc 0.84375, learning_rate 0.00126127
2017-10-10T11:46:33.408920: step 354, loss 0.302028, acc 0.890625, learning_rate 0.00125653
2017-10-10T11:46:33.596699: step 355, loss 0.289015, acc 0.890625, learning_rate 0.00125181
2017-10-10T11:46:33.802180: step 356, loss 0.165435, acc 0.9375, learning_rate 0.00124711
2017-10-10T11:46:34.013947: step 357, loss 0.294852, acc 0.90625, learning_rate 0.00124243
2017-10-10T11:46:34.215906: step 358, loss 0.255386, acc 0.890625, learning_rate 0.00123777
2017-10-10T11:46:34.484277: step 359, loss 0.252881, acc 0.921875, learning_rate 0.00123312
2017-10-10T11:46:34.684878: step 360, loss 0.387673, acc 0.890625, learning_rate 0.0012285

Evaluation:
2017-10-10T11:46:34.986120: step 360, loss 0.273111, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-360

2017-10-10T11:46:36.012861: step 361, loss 0.191499, acc 0.953125, learning_rate 0.00122389
2017-10-10T11:46:36.197240: step 362, loss 0.279628, acc 0.875, learning_rate 0.0012193
2017-10-10T11:46:36.354879: step 363, loss 0.31335, acc 0.90625, learning_rate 0.00121473
2017-10-10T11:46:36.570012: step 364, loss 0.549432, acc 0.796875, learning_rate 0.00121018
2017-10-10T11:46:36.752939: step 365, loss 0.222896, acc 0.90625, learning_rate 0.00120565
2017-10-10T11:46:36.899964: step 366, loss 0.445308, acc 0.875, learning_rate 0.00120114
2017-10-10T11:46:37.094863: step 367, loss 0.322496, acc 0.890625, learning_rate 0.00119664
2017-10-10T11:46:37.284884: step 368, loss 0.15623, acc 0.953125, learning_rate 0.00119217
2017-10-10T11:46:37.446289: step 369, loss 0.233202, acc 0.90625, learning_rate 0.00118771
2017-10-10T11:46:37.612823: step 370, loss 0.454151, acc 0.828125, learning_rate 0.00118327
2017-10-10T11:46:37.797026: step 371, loss 0.144375, acc 0.9375, learning_rate 0.00117885
2017-10-10T11:46:37.991332: step 372, loss 0.295543, acc 0.875, learning_rate 0.00117445
2017-10-10T11:46:38.172857: step 373, loss 0.227356, acc 0.921875, learning_rate 0.00117006
2017-10-10T11:46:38.383840: step 374, loss 0.295982, acc 0.875, learning_rate 0.00116569
2017-10-10T11:46:38.512463: step 375, loss 0.205731, acc 0.9375, learning_rate 0.00116134
2017-10-10T11:46:38.697204: step 376, loss 0.181361, acc 0.96875, learning_rate 0.00115701
2017-10-10T11:46:38.909211: step 377, loss 0.285105, acc 0.921875, learning_rate 0.0011527
2017-10-10T11:46:39.100381: step 378, loss 0.191163, acc 0.9375, learning_rate 0.0011484
2017-10-10T11:46:39.224855: step 379, loss 0.237308, acc 0.921875, learning_rate 0.00114412
2017-10-10T11:46:39.426587: step 380, loss 0.257112, acc 0.921875, learning_rate 0.00113986
2017-10-10T11:46:39.665071: step 381, loss 0.276853, acc 0.875, learning_rate 0.00113561
2017-10-10T11:46:39.804506: step 382, loss 0.23879, acc 0.953125, learning_rate 0.00113139
2017-10-10T11:46:39.942897: step 383, loss 0.358637, acc 0.875, learning_rate 0.00112718
2017-10-10T11:46:40.110889: step 384, loss 0.169791, acc 0.953125, learning_rate 0.00112298
2017-10-10T11:46:40.277171: step 385, loss 0.299515, acc 0.890625, learning_rate 0.00111881
2017-10-10T11:46:40.442308: step 386, loss 0.200193, acc 0.9375, learning_rate 0.00111465
2017-10-10T11:46:40.569873: step 387, loss 0.212703, acc 0.921875, learning_rate 0.00111051
2017-10-10T11:46:40.700885: step 388, loss 0.242781, acc 0.890625, learning_rate 0.00110638
2017-10-10T11:46:40.906404: step 389, loss 0.275961, acc 0.875, learning_rate 0.00110228
2017-10-10T11:46:41.116862: step 390, loss 0.204096, acc 0.921875, learning_rate 0.00109818
2017-10-10T11:46:41.261039: step 391, loss 0.238545, acc 0.953125, learning_rate 0.00109411
2017-10-10T11:46:41.420859: step 392, loss 0.316324, acc 0.921569, learning_rate 0.00109005
2017-10-10T11:46:41.635252: step 393, loss 0.254493, acc 0.9375, learning_rate 0.00108601
2017-10-10T11:46:41.840844: step 394, loss 0.193909, acc 0.90625, learning_rate 0.00108199
2017-10-10T11:46:42.030359: step 395, loss 0.277504, acc 0.90625, learning_rate 0.00107798
2017-10-10T11:46:42.149170: step 396, loss 0.24847, acc 0.9375, learning_rate 0.00107399
2017-10-10T11:46:42.354999: step 397, loss 0.217746, acc 0.9375, learning_rate 0.00107001
2017-10-10T11:46:42.559560: step 398, loss 0.186504, acc 0.9375, learning_rate 0.00106605
2017-10-10T11:46:42.742044: step 399, loss 0.149992, acc 0.984375, learning_rate 0.00106211
2017-10-10T11:46:42.864525: step 400, loss 0.303851, acc 0.90625, learning_rate 0.00105818

Evaluation:
2017-10-10T11:46:43.238059: step 400, loss 0.265448, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-400

2017-10-10T11:46:44.309098: step 401, loss 0.278186, acc 0.921875, learning_rate 0.00105427
2017-10-10T11:46:44.522020: step 402, loss 0.192523, acc 0.9375, learning_rate 0.00105037
2017-10-10T11:46:44.674053: step 403, loss 0.216952, acc 0.921875, learning_rate 0.0010465
2017-10-10T11:46:44.873165: step 404, loss 0.314639, acc 0.890625, learning_rate 0.00104263
2017-10-10T11:46:45.035344: step 405, loss 0.408368, acc 0.890625, learning_rate 0.00103878
2017-10-10T11:46:45.200840: step 406, loss 0.241644, acc 0.953125, learning_rate 0.00103495
2017-10-10T11:46:45.417301: step 407, loss 0.295926, acc 0.890625, learning_rate 0.00103114
2017-10-10T11:46:45.620807: step 408, loss 0.122251, acc 0.96875, learning_rate 0.00102734
2017-10-10T11:46:45.803369: step 409, loss 0.244747, acc 0.90625, learning_rate 0.00102355
2017-10-10T11:46:45.958062: step 410, loss 0.285599, acc 0.890625, learning_rate 0.00101978
2017-10-10T11:46:46.084841: step 411, loss 0.281823, acc 0.921875, learning_rate 0.00101603
2017-10-10T11:46:46.240838: step 412, loss 0.258358, acc 0.90625, learning_rate 0.00101229
2017-10-10T11:46:46.355075: step 413, loss 0.18909, acc 0.96875, learning_rate 0.00100856
2017-10-10T11:46:46.498978: step 414, loss 0.238056, acc 0.9375, learning_rate 0.00100486
2017-10-10T11:46:46.667409: step 415, loss 0.172627, acc 0.953125, learning_rate 0.00100116
2017-10-10T11:46:46.826434: step 416, loss 0.219281, acc 0.9375, learning_rate 0.000997483
2017-10-10T11:46:47.016863: step 417, loss 0.182005, acc 0.953125, learning_rate 0.00099382
2017-10-10T11:46:47.228855: step 418, loss 0.350149, acc 0.90625, learning_rate 0.000990172
2017-10-10T11:46:47.435560: step 419, loss 0.182034, acc 0.953125, learning_rate 0.000986538
2017-10-10T11:46:47.624212: step 420, loss 0.315471, acc 0.90625, learning_rate 0.00098292
2017-10-10T11:46:47.796976: step 421, loss 0.0867395, acc 1, learning_rate 0.000979316
2017-10-10T11:46:47.947535: step 422, loss 0.196434, acc 0.921875, learning_rate 0.000975727
2017-10-10T11:46:48.130131: step 423, loss 0.248213, acc 0.875, learning_rate 0.000972152
2017-10-10T11:46:48.337901: step 424, loss 0.258362, acc 0.875, learning_rate 0.000968592
2017-10-10T11:46:48.527516: step 425, loss 0.31579, acc 0.90625, learning_rate 0.000965047
2017-10-10T11:46:48.728441: step 426, loss 0.3286, acc 0.921875, learning_rate 0.000961516
2017-10-10T11:46:48.903947: step 427, loss 0.210938, acc 0.9375, learning_rate 0.000958
2017-10-10T11:46:49.030491: step 428, loss 0.21433, acc 0.953125, learning_rate 0.000954497
2017-10-10T11:46:49.224341: step 429, loss 0.262228, acc 0.90625, learning_rate 0.00095101
2017-10-10T11:46:49.416847: step 430, loss 0.26493, acc 0.9375, learning_rate 0.000947536
2017-10-10T11:46:49.581124: step 431, loss 0.133375, acc 0.96875, learning_rate 0.000944076
2017-10-10T11:46:49.743595: step 432, loss 0.152997, acc 0.96875, learning_rate 0.000940631
2017-10-10T11:46:49.987220: step 433, loss 0.166256, acc 0.9375, learning_rate 0.0009372
2017-10-10T11:46:50.221161: step 434, loss 0.220488, acc 0.921875, learning_rate 0.000933783
2017-10-10T11:46:50.344436: step 435, loss 0.192518, acc 0.921875, learning_rate 0.000930379
2017-10-10T11:46:50.472466: step 436, loss 0.197398, acc 0.9375, learning_rate 0.00092699
2017-10-10T11:46:50.606794: step 437, loss 0.163109, acc 0.90625, learning_rate 0.000923614
2017-10-10T11:46:50.738914: step 438, loss 0.207158, acc 0.953125, learning_rate 0.000920253
2017-10-10T11:46:50.840892: step 439, loss 0.206838, acc 0.90625, learning_rate 0.000916905
2017-10-10T11:46:51.004211: step 440, loss 0.313467, acc 0.875, learning_rate 0.00091357

Evaluation:
2017-10-10T11:46:51.287792: step 440, loss 0.263389, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-440

2017-10-10T11:46:52.344720: step 441, loss 0.27349, acc 0.921875, learning_rate 0.000910249
2017-10-10T11:46:52.548865: step 442, loss 0.275433, acc 0.921875, learning_rate 0.000906942
2017-10-10T11:46:52.741069: step 443, loss 0.181366, acc 0.9375, learning_rate 0.000903648
2017-10-10T11:46:52.888861: step 444, loss 0.284469, acc 0.890625, learning_rate 0.000900368
2017-10-10T11:46:53.092256: step 445, loss 0.269676, acc 0.921875, learning_rate 0.000897101
2017-10-10T11:46:53.268810: step 446, loss 0.318822, acc 0.859375, learning_rate 0.000893848
2017-10-10T11:46:53.412197: step 447, loss 0.17881, acc 0.953125, learning_rate 0.000890607
2017-10-10T11:46:53.596381: step 448, loss 0.357502, acc 0.890625, learning_rate 0.00088738
2017-10-10T11:46:53.784895: step 449, loss 0.323827, acc 0.890625, learning_rate 0.000884166
2017-10-10T11:46:53.924713: step 450, loss 0.20605, acc 0.9375, learning_rate 0.000880966
2017-10-10T11:46:54.097759: step 451, loss 0.13383, acc 0.953125, learning_rate 0.000877778
2017-10-10T11:46:54.267653: step 452, loss 0.314816, acc 0.890625, learning_rate 0.000874603
2017-10-10T11:46:54.419905: step 453, loss 0.342987, acc 0.890625, learning_rate 0.000871441
2017-10-10T11:46:54.609199: step 454, loss 0.204816, acc 0.890625, learning_rate 0.000868293
2017-10-10T11:46:54.828063: step 455, loss 0.335484, acc 0.875, learning_rate 0.000865157
2017-10-10T11:46:55.025434: step 456, loss 0.225747, acc 0.875, learning_rate 0.000862033
2017-10-10T11:46:55.135776: step 457, loss 0.217116, acc 0.890625, learning_rate 0.000858923
2017-10-10T11:46:55.338259: step 458, loss 0.417208, acc 0.875, learning_rate 0.000855825
2017-10-10T11:46:55.464142: step 459, loss 0.283573, acc 0.921875, learning_rate 0.00085274
2017-10-10T11:46:55.642384: step 460, loss 0.102257, acc 0.984375, learning_rate 0.000849668
2017-10-10T11:46:55.863626: step 461, loss 0.204433, acc 0.9375, learning_rate 0.000846608
2017-10-10T11:46:56.057510: step 462, loss 0.323122, acc 0.890625, learning_rate 0.00084356
2017-10-10T11:46:56.311015: step 463, loss 0.413079, acc 0.875, learning_rate 0.000840525
2017-10-10T11:46:56.479310: step 464, loss 0.140914, acc 0.953125, learning_rate 0.000837502
2017-10-10T11:46:56.617341: step 465, loss 0.283317, acc 0.890625, learning_rate 0.000834492
2017-10-10T11:46:56.761271: step 466, loss 0.280325, acc 0.90625, learning_rate 0.000831494
2017-10-10T11:46:56.882739: step 467, loss 0.309836, acc 0.875, learning_rate 0.000828508
2017-10-10T11:46:57.012845: step 468, loss 0.309766, acc 0.90625, learning_rate 0.000825535
2017-10-10T11:46:57.149039: step 469, loss 0.244538, acc 0.90625, learning_rate 0.000822573
2017-10-10T11:46:57.299789: step 470, loss 0.117069, acc 0.953125, learning_rate 0.000819624
2017-10-10T11:46:57.440674: step 471, loss 0.315376, acc 0.890625, learning_rate 0.000816687
2017-10-10T11:46:57.664844: step 472, loss 0.154674, acc 0.9375, learning_rate 0.000813761
2017-10-10T11:46:57.865456: step 473, loss 0.26033, acc 0.90625, learning_rate 0.000810848
2017-10-10T11:46:57.988995: step 474, loss 0.196154, acc 0.9375, learning_rate 0.000807946
2017-10-10T11:46:58.186176: step 475, loss 0.426971, acc 0.828125, learning_rate 0.000805057
2017-10-10T11:46:58.403044: step 476, loss 0.245266, acc 0.890625, learning_rate 0.000802179
2017-10-10T11:46:58.557196: step 477, loss 0.171233, acc 0.9375, learning_rate 0.000799313
2017-10-10T11:46:58.710754: step 478, loss 0.24426, acc 0.9375, learning_rate 0.000796458
2017-10-10T11:46:58.921934: step 479, loss 0.42291, acc 0.859375, learning_rate 0.000793616
2017-10-10T11:46:59.123366: step 480, loss 0.312323, acc 0.890625, learning_rate 0.000790784

Evaluation:
2017-10-10T11:46:59.453578: step 480, loss 0.264549, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-480

2017-10-10T11:47:00.776368: step 481, loss 0.305253, acc 0.875, learning_rate 0.000787965
2017-10-10T11:47:00.969041: step 482, loss 0.287821, acc 0.90625, learning_rate 0.000785157
2017-10-10T11:47:01.217372: step 483, loss 0.231724, acc 0.9375, learning_rate 0.00078236
2017-10-10T11:47:01.388666: step 484, loss 0.231147, acc 0.9375, learning_rate 0.000779575
2017-10-10T11:47:01.561039: step 485, loss 0.116124, acc 0.96875, learning_rate 0.000776801
2017-10-10T11:47:01.711868: step 486, loss 0.284817, acc 0.890625, learning_rate 0.000774038
2017-10-10T11:47:01.864839: step 487, loss 0.294, acc 0.890625, learning_rate 0.000771287
2017-10-10T11:47:02.003910: step 488, loss 0.229695, acc 0.890625, learning_rate 0.000768547
2017-10-10T11:47:02.148895: step 489, loss 0.278763, acc 0.890625, learning_rate 0.000765818
2017-10-10T11:47:02.339333: step 490, loss 0.191258, acc 0.921569, learning_rate 0.000763101
2017-10-10T11:47:02.515699: step 491, loss 0.150715, acc 0.96875, learning_rate 0.000760394
2017-10-10T11:47:02.680872: step 492, loss 0.230941, acc 0.9375, learning_rate 0.000757698
2017-10-10T11:47:02.856903: step 493, loss 0.135164, acc 0.96875, learning_rate 0.000755014
2017-10-10T11:47:03.052854: step 494, loss 0.261552, acc 0.921875, learning_rate 0.00075234
2017-10-10T11:47:03.227306: step 495, loss 0.228397, acc 0.953125, learning_rate 0.000749677
2017-10-10T11:47:03.420769: step 496, loss 0.198411, acc 0.9375, learning_rate 0.000747026
2017-10-10T11:47:03.628985: step 497, loss 0.398998, acc 0.828125, learning_rate 0.000744385
2017-10-10T11:47:03.848844: step 498, loss 0.257142, acc 0.921875, learning_rate 0.000741754
2017-10-10T11:47:04.002770: step 499, loss 0.135212, acc 0.984375, learning_rate 0.000739135
2017-10-10T11:47:04.176735: step 500, loss 0.132302, acc 0.96875, learning_rate 0.000736526
2017-10-10T11:47:04.394822: step 501, loss 0.153015, acc 0.96875, learning_rate 0.000733928
2017-10-10T11:47:04.581429: step 502, loss 0.232363, acc 0.90625, learning_rate 0.00073134
2017-10-10T11:47:04.721574: step 503, loss 0.262435, acc 0.90625, learning_rate 0.000728763
2017-10-10T11:47:04.893074: step 504, loss 0.223336, acc 0.90625, learning_rate 0.000726197
2017-10-10T11:47:05.096524: step 505, loss 0.19687, acc 0.9375, learning_rate 0.000723641
2017-10-10T11:47:05.261086: step 506, loss 0.192443, acc 0.921875, learning_rate 0.000721095
2017-10-10T11:47:05.426813: step 507, loss 0.300544, acc 0.921875, learning_rate 0.00071856
2017-10-10T11:47:05.628127: step 508, loss 0.24854, acc 0.9375, learning_rate 0.000716036
2017-10-10T11:47:05.822265: step 509, loss 0.285631, acc 0.90625, learning_rate 0.000713521
2017-10-10T11:47:06.016831: step 510, loss 0.232202, acc 0.921875, learning_rate 0.000711017
2017-10-10T11:47:06.199482: step 511, loss 0.218947, acc 0.921875, learning_rate 0.000708523
2017-10-10T11:47:06.375222: step 512, loss 0.232986, acc 0.9375, learning_rate 0.000706039
2017-10-10T11:47:06.580630: step 513, loss 0.318963, acc 0.859375, learning_rate 0.000703565
2017-10-10T11:47:06.741001: step 514, loss 0.168028, acc 0.9375, learning_rate 0.000701102
2017-10-10T11:47:06.929060: step 515, loss 0.162121, acc 0.9375, learning_rate 0.000698648
2017-10-10T11:47:07.112255: step 516, loss 0.200762, acc 0.90625, learning_rate 0.000696204
2017-10-10T11:47:07.292885: step 517, loss 0.185823, acc 0.9375, learning_rate 0.000693771
2017-10-10T11:47:07.492664: step 518, loss 0.33474, acc 0.859375, learning_rate 0.000691347
2017-10-10T11:47:07.699160: step 519, loss 0.203011, acc 0.9375, learning_rate 0.000688934
2017-10-10T11:47:07.862013: step 520, loss 0.216339, acc 0.953125, learning_rate 0.00068653

Evaluation:
2017-10-10T11:47:08.141662: step 520, loss 0.256575, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-520

2017-10-10T11:47:09.101304: step 521, loss 0.193722, acc 0.953125, learning_rate 0.000684136
2017-10-10T11:47:09.324630: step 522, loss 0.23356, acc 0.921875, learning_rate 0.000681751
2017-10-10T11:47:09.540460: step 523, loss 0.262429, acc 0.890625, learning_rate 0.000679377
2017-10-10T11:47:09.712989: step 524, loss 0.209069, acc 0.921875, learning_rate 0.000677012
2017-10-10T11:47:09.854405: step 525, loss 0.220574, acc 0.9375, learning_rate 0.000674657
2017-10-10T11:47:10.048187: step 526, loss 0.346846, acc 0.875, learning_rate 0.000672311
2017-10-10T11:47:10.242538: step 527, loss 0.276116, acc 0.90625, learning_rate 0.000669975
2017-10-10T11:47:10.386150: step 528, loss 0.378286, acc 0.859375, learning_rate 0.000667648
2017-10-10T11:47:10.565828: step 529, loss 0.231574, acc 0.890625, learning_rate 0.000665331
2017-10-10T11:47:10.788912: step 530, loss 0.208822, acc 0.921875, learning_rate 0.000663024
2017-10-10T11:47:10.980474: step 531, loss 0.21511, acc 0.921875, learning_rate 0.000660726
2017-10-10T11:47:11.150448: step 532, loss 0.3601, acc 0.921875, learning_rate 0.000658437
2017-10-10T11:47:11.364894: step 533, loss 0.271311, acc 0.921875, learning_rate 0.000656158
2017-10-10T11:47:11.631593: step 534, loss 0.306355, acc 0.90625, learning_rate 0.000653888
2017-10-10T11:47:11.764567: step 535, loss 0.331984, acc 0.890625, learning_rate 0.000651627
2017-10-10T11:47:11.878154: step 536, loss 0.299489, acc 0.90625, learning_rate 0.000649375
2017-10-10T11:47:12.023461: step 537, loss 0.28431, acc 0.875, learning_rate 0.000647133
2017-10-10T11:47:12.183529: step 538, loss 0.22461, acc 0.90625, learning_rate 0.000644899
2017-10-10T11:47:12.324830: step 539, loss 0.332027, acc 0.875, learning_rate 0.000642675
2017-10-10T11:47:12.435479: step 540, loss 0.209008, acc 0.9375, learning_rate 0.00064046
2017-10-10T11:47:12.580100: step 541, loss 0.210671, acc 0.921875, learning_rate 0.000638254
2017-10-10T11:47:12.740529: step 542, loss 0.238911, acc 0.890625, learning_rate 0.000636057
2017-10-10T11:47:12.896881: step 543, loss 0.13157, acc 0.953125, learning_rate 0.000633869
2017-10-10T11:47:13.092747: step 544, loss 0.452711, acc 0.84375, learning_rate 0.00063169
2017-10-10T11:47:13.254377: step 545, loss 0.231991, acc 0.9375, learning_rate 0.00062952
2017-10-10T11:47:13.397602: step 546, loss 0.358093, acc 0.859375, learning_rate 0.000627358
2017-10-10T11:47:13.603220: step 547, loss 0.192607, acc 0.9375, learning_rate 0.000625206
2017-10-10T11:47:13.820493: step 548, loss 0.192637, acc 0.921875, learning_rate 0.000623062
2017-10-10T11:47:14.037806: step 549, loss 0.383337, acc 0.84375, learning_rate 0.000620927
2017-10-10T11:47:14.217012: step 550, loss 0.0730656, acc 1, learning_rate 0.000618801
2017-10-10T11:47:14.377207: step 551, loss 0.22704, acc 0.921875, learning_rate 0.000616683
2017-10-10T11:47:14.567581: step 552, loss 0.162843, acc 0.96875, learning_rate 0.000614574
2017-10-10T11:47:14.758313: step 553, loss 0.170674, acc 0.953125, learning_rate 0.000612474
2017-10-10T11:47:14.936859: step 554, loss 0.269746, acc 0.921875, learning_rate 0.000610382
2017-10-10T11:47:15.107721: step 555, loss 0.159097, acc 0.96875, learning_rate 0.000608299
2017-10-10T11:47:15.290817: step 556, loss 0.168516, acc 0.921875, learning_rate 0.000606224
2017-10-10T11:47:15.498059: step 557, loss 0.0899439, acc 0.984375, learning_rate 0.000604158
2017-10-10T11:47:15.670284: step 558, loss 0.247593, acc 0.9375, learning_rate 0.0006021
2017-10-10T11:47:15.868113: step 559, loss 0.275088, acc 0.890625, learning_rate 0.00060005
2017-10-10T11:47:16.064880: step 560, loss 0.232749, acc 0.90625, learning_rate 0.000598009

Evaluation:
2017-10-10T11:47:16.449269: step 560, loss 0.255486, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-560

2017-10-10T11:47:17.789272: step 561, loss 0.25596, acc 0.890625, learning_rate 0.000595977
2017-10-10T11:47:17.971648: step 562, loss 0.424327, acc 0.8125, learning_rate 0.000593952
2017-10-10T11:47:18.136206: step 563, loss 0.151731, acc 0.953125, learning_rate 0.000591936
2017-10-10T11:47:18.344893: step 564, loss 0.232367, acc 0.921875, learning_rate 0.000589928
2017-10-10T11:47:18.573069: step 565, loss 0.281961, acc 0.90625, learning_rate 0.000587928
2017-10-10T11:47:18.789837: step 566, loss 0.292529, acc 0.890625, learning_rate 0.000585937
2017-10-10T11:47:18.941982: step 567, loss 0.319218, acc 0.921875, learning_rate 0.000583953
2017-10-10T11:47:19.060975: step 568, loss 0.228235, acc 0.9375, learning_rate 0.000581978
2017-10-10T11:47:19.198725: step 569, loss 0.165832, acc 0.96875, learning_rate 0.00058001
2017-10-10T11:47:19.340855: step 570, loss 0.325039, acc 0.859375, learning_rate 0.000578051
2017-10-10T11:47:19.499391: step 571, loss 0.180656, acc 0.921875, learning_rate 0.0005761
2017-10-10T11:47:19.716613: step 572, loss 0.146181, acc 0.953125, learning_rate 0.000574157
2017-10-10T11:47:19.880887: step 573, loss 0.219996, acc 0.90625, learning_rate 0.000572221
2017-10-10T11:47:20.088798: step 574, loss 0.11678, acc 0.96875, learning_rate 0.000570294
2017-10-10T11:47:20.287258: step 575, loss 0.164137, acc 0.9375, learning_rate 0.000568374
2017-10-10T11:47:20.448890: step 576, loss 0.267991, acc 0.921875, learning_rate 0.000566462
2017-10-10T11:47:20.634913: step 577, loss 0.307556, acc 0.890625, learning_rate 0.000564558
2017-10-10T11:47:20.819724: step 578, loss 0.166072, acc 0.96875, learning_rate 0.000562662
2017-10-10T11:47:21.018652: step 579, loss 0.202432, acc 0.90625, learning_rate 0.000560774
2017-10-10T11:47:21.161011: step 580, loss 0.331226, acc 0.875, learning_rate 0.000558893
2017-10-10T11:47:21.336809: step 581, loss 0.207537, acc 0.921875, learning_rate 0.00055702
2017-10-10T11:47:21.549242: step 582, loss 0.251531, acc 0.890625, learning_rate 0.000555154
2017-10-10T11:47:21.763335: step 583, loss 0.178569, acc 0.953125, learning_rate 0.000553296
2017-10-10T11:47:22.003443: step 584, loss 0.231812, acc 0.921875, learning_rate 0.000551446
2017-10-10T11:47:22.176805: step 585, loss 0.242211, acc 0.921875, learning_rate 0.000549604
2017-10-10T11:47:22.314263: step 586, loss 0.230795, acc 0.921875, learning_rate 0.000547768
2017-10-10T11:47:22.474823: step 587, loss 0.173352, acc 0.9375, learning_rate 0.000545941
2017-10-10T11:47:22.584834: step 588, loss 0.407468, acc 0.843137, learning_rate 0.00054412
2017-10-10T11:47:22.722172: step 589, loss 0.209562, acc 0.890625, learning_rate 0.000542308
2017-10-10T11:47:22.868764: step 590, loss 0.156382, acc 0.9375, learning_rate 0.000540502
2017-10-10T11:47:23.026962: step 591, loss 0.11822, acc 0.953125, learning_rate 0.000538704
2017-10-10T11:47:23.162046: step 592, loss 0.234708, acc 0.921875, learning_rate 0.000536914
2017-10-10T11:47:23.280822: step 593, loss 0.329373, acc 0.875, learning_rate 0.00053513
2017-10-10T11:47:23.494974: step 594, loss 0.180006, acc 0.96875, learning_rate 0.000533354
2017-10-10T11:47:23.658938: step 595, loss 0.227171, acc 0.9375, learning_rate 0.000531585
2017-10-10T11:47:23.800176: step 596, loss 0.28148, acc 0.921875, learning_rate 0.000529824
2017-10-10T11:47:24.021009: step 597, loss 0.234065, acc 0.953125, learning_rate 0.000528069
2017-10-10T11:47:24.228135: step 598, loss 0.312131, acc 0.890625, learning_rate 0.000526322
2017-10-10T11:47:24.352960: step 599, loss 0.176388, acc 0.9375, learning_rate 0.000524582
2017-10-10T11:47:24.563415: step 600, loss 0.100127, acc 0.96875, learning_rate 0.000522849

Evaluation:
2017-10-10T11:47:24.962306: step 600, loss 0.251876, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-600

2017-10-10T11:47:26.782352: step 601, loss 0.136667, acc 0.953125, learning_rate 0.000521123
2017-10-10T11:47:26.947480: step 602, loss 0.236604, acc 0.890625, learning_rate 0.000519404
2017-10-10T11:47:27.142924: step 603, loss 0.179145, acc 0.9375, learning_rate 0.000517692
2017-10-10T11:47:27.344812: step 604, loss 0.252104, acc 0.921875, learning_rate 0.000515987
2017-10-10T11:47:27.469823: step 605, loss 0.208558, acc 0.953125, learning_rate 0.000514289
2017-10-10T11:47:27.663494: step 606, loss 0.289476, acc 0.90625, learning_rate 0.000512598
2017-10-10T11:47:27.879704: step 607, loss 0.196191, acc 0.9375, learning_rate 0.000510914
2017-10-10T11:47:28.068932: step 608, loss 0.3701, acc 0.890625, learning_rate 0.000509237
2017-10-10T11:47:28.228952: step 609, loss 0.271351, acc 0.90625, learning_rate 0.000507566
2017-10-10T11:47:28.406873: step 610, loss 0.1963, acc 0.9375, learning_rate 0.000505903
2017-10-10T11:47:28.623735: step 611, loss 0.193385, acc 0.921875, learning_rate 0.000504246
2017-10-10T11:47:28.819348: step 612, loss 0.335067, acc 0.90625, learning_rate 0.000502596
2017-10-10T11:47:28.960943: step 613, loss 0.243111, acc 0.875, learning_rate 0.000500953
2017-10-10T11:47:29.216871: step 614, loss 0.351117, acc 0.859375, learning_rate 0.000499316
2017-10-10T11:47:29.459442: step 615, loss 0.113094, acc 0.984375, learning_rate 0.000497686
2017-10-10T11:47:29.592821: step 616, loss 0.222906, acc 0.90625, learning_rate 0.000496063
2017-10-10T11:47:29.709707: step 617, loss 0.315795, acc 0.875, learning_rate 0.000494446
2017-10-10T11:47:29.837702: step 618, loss 0.159477, acc 0.96875, learning_rate 0.000492836
2017-10-10T11:47:29.993068: step 619, loss 0.220825, acc 0.90625, learning_rate 0.000491233
2017-10-10T11:47:30.149076: step 620, loss 0.290778, acc 0.90625, learning_rate 0.000489636
2017-10-10T11:47:30.298630: step 621, loss 0.133104, acc 0.96875, learning_rate 0.000488045
2017-10-10T11:47:30.456261: step 622, loss 0.234316, acc 0.875, learning_rate 0.000486461
2017-10-10T11:47:30.667354: step 623, loss 0.207179, acc 0.96875, learning_rate 0.000484884
2017-10-10T11:47:30.823570: step 624, loss 0.137297, acc 0.953125, learning_rate 0.000483313
2017-10-10T11:47:30.994867: step 625, loss 0.265776, acc 0.90625, learning_rate 0.000481748
2017-10-10T11:47:31.195535: step 626, loss 0.221744, acc 0.9375, learning_rate 0.00048019
2017-10-10T11:47:31.360880: step 627, loss 0.174372, acc 0.9375, learning_rate 0.000478638
2017-10-10T11:47:31.510706: step 628, loss 0.261632, acc 0.90625, learning_rate 0.000477093
2017-10-10T11:47:31.705812: step 629, loss 0.130635, acc 0.984375, learning_rate 0.000475554
2017-10-10T11:47:31.839796: step 630, loss 0.155808, acc 0.953125, learning_rate 0.000474021
2017-10-10T11:47:32.010058: step 631, loss 0.191186, acc 0.921875, learning_rate 0.000472494
2017-10-10T11:47:32.213469: step 632, loss 0.279876, acc 0.875, learning_rate 0.000470974
2017-10-10T11:47:32.385525: step 633, loss 0.273016, acc 0.921875, learning_rate 0.000469459
2017-10-10T11:47:32.585725: step 634, loss 0.175229, acc 0.953125, learning_rate 0.000467951
2017-10-10T11:47:32.808094: step 635, loss 0.270753, acc 0.96875, learning_rate 0.000466449
2017-10-10T11:47:32.944571: step 636, loss 0.1805, acc 0.953125, learning_rate 0.000464954
2017-10-10T11:47:33.054154: step 637, loss 0.258674, acc 0.890625, learning_rate 0.000463464
2017-10-10T11:47:33.211372: step 638, loss 0.150739, acc 0.96875, learning_rate 0.00046198
2017-10-10T11:47:33.336820: step 639, loss 0.326008, acc 0.890625, learning_rate 0.000460503
2017-10-10T11:47:33.455499: step 640, loss 0.258013, acc 0.90625, learning_rate 0.000459031

Evaluation:
2017-10-10T11:47:33.791893: step 640, loss 0.25041, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-640

2017-10-10T11:47:34.913045: step 641, loss 0.209895, acc 0.921875, learning_rate 0.000457566
2017-10-10T11:47:35.100909: step 642, loss 0.246221, acc 0.875, learning_rate 0.000456106
2017-10-10T11:47:35.272403: step 643, loss 0.338817, acc 0.890625, learning_rate 0.000454653
2017-10-10T11:47:35.441557: step 644, loss 0.209041, acc 0.90625, learning_rate 0.000453205
2017-10-10T11:47:35.588853: step 645, loss 0.390508, acc 0.890625, learning_rate 0.000451764
2017-10-10T11:47:35.756512: step 646, loss 0.317247, acc 0.890625, learning_rate 0.000450328
2017-10-10T11:47:35.973562: step 647, loss 0.220908, acc 0.953125, learning_rate 0.000448898
2017-10-10T11:47:36.198131: step 648, loss 0.246868, acc 0.9375, learning_rate 0.000447474
2017-10-10T11:47:36.327671: step 649, loss 0.31173, acc 0.90625, learning_rate 0.000446055
2017-10-10T11:47:36.500861: step 650, loss 0.180243, acc 0.90625, learning_rate 0.000444643
2017-10-10T11:47:36.730383: step 651, loss 0.315596, acc 0.921875, learning_rate 0.000443236
2017-10-10T11:47:36.884075: step 652, loss 0.185124, acc 0.953125, learning_rate 0.000441835
2017-10-10T11:47:37.069703: step 653, loss 0.213869, acc 0.921875, learning_rate 0.00044044
2017-10-10T11:47:37.251808: step 654, loss 0.207494, acc 0.9375, learning_rate 0.00043905
2017-10-10T11:47:37.443579: step 655, loss 0.204503, acc 0.9375, learning_rate 0.000437666
2017-10-10T11:47:37.627099: step 656, loss 0.20159, acc 0.9375, learning_rate 0.000436288
2017-10-10T11:47:37.793026: step 657, loss 0.161869, acc 0.9375, learning_rate 0.000434915
2017-10-10T11:47:37.988542: step 658, loss 0.243977, acc 0.9375, learning_rate 0.000433548
2017-10-10T11:47:38.164167: step 659, loss 0.248122, acc 0.921875, learning_rate 0.000432187
2017-10-10T11:47:38.339818: step 660, loss 0.265028, acc 0.875, learning_rate 0.000430831
2017-10-10T11:47:38.525012: step 661, loss 0.187446, acc 0.9375, learning_rate 0.000429481
2017-10-10T11:47:38.742411: step 662, loss 0.168478, acc 0.984375, learning_rate 0.000428136
2017-10-10T11:47:38.936917: step 663, loss 0.125892, acc 0.96875, learning_rate 0.000426796
2017-10-10T11:47:39.076442: step 664, loss 0.240585, acc 0.90625, learning_rate 0.000425463
2017-10-10T11:47:39.262534: step 665, loss 0.119418, acc 0.984375, learning_rate 0.000424134
2017-10-10T11:47:39.463866: step 666, loss 0.191849, acc 0.9375, learning_rate 0.000422811
2017-10-10T11:47:39.650749: step 667, loss 0.197928, acc 0.953125, learning_rate 0.000421493
2017-10-10T11:47:39.798044: step 668, loss 0.222088, acc 0.921875, learning_rate 0.000420181
2017-10-10T11:47:39.983247: step 669, loss 0.174176, acc 0.9375, learning_rate 0.000418874
2017-10-10T11:47:40.150548: step 670, loss 0.22342, acc 0.921875, learning_rate 0.000417573
2017-10-10T11:47:40.329141: step 671, loss 0.215315, acc 0.90625, learning_rate 0.000416276
2017-10-10T11:47:40.572877: step 672, loss 0.185578, acc 0.9375, learning_rate 0.000414985
2017-10-10T11:47:40.769027: step 673, loss 0.168395, acc 0.953125, learning_rate 0.0004137
2017-10-10T11:47:40.948894: step 674, loss 0.168337, acc 0.953125, learning_rate 0.000412419
2017-10-10T11:47:41.106730: step 675, loss 0.16668, acc 0.921875, learning_rate 0.000411144
2017-10-10T11:47:41.240846: step 676, loss 0.180534, acc 0.90625, learning_rate 0.000409874
2017-10-10T11:47:41.383487: step 677, loss 0.199015, acc 0.921875, learning_rate 0.000408609
2017-10-10T11:47:41.489156: step 678, loss 0.3419, acc 0.921875, learning_rate 0.00040735
2017-10-10T11:47:41.633703: step 679, loss 0.351358, acc 0.890625, learning_rate 0.000406095
2017-10-10T11:47:41.790938: step 680, loss 0.150648, acc 0.953125, learning_rate 0.000404846

Evaluation:
2017-10-10T11:47:42.112677: step 680, loss 0.25166, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-680

2017-10-10T11:47:43.135610: step 681, loss 0.263876, acc 0.875, learning_rate 0.000403601
2017-10-10T11:47:43.289588: step 682, loss 0.25044, acc 0.90625, learning_rate 0.000402362
2017-10-10T11:47:43.443292: step 683, loss 0.164757, acc 0.921875, learning_rate 0.000401128
2017-10-10T11:47:43.554700: step 684, loss 0.254942, acc 0.90625, learning_rate 0.000399899
2017-10-10T11:47:43.693421: step 685, loss 0.304728, acc 0.875, learning_rate 0.000398675
2017-10-10T11:47:43.863378: step 686, loss 0.298489, acc 0.921569, learning_rate 0.000397456
2017-10-10T11:47:44.044984: step 687, loss 0.236302, acc 0.9375, learning_rate 0.000396241
2017-10-10T11:47:44.215718: step 688, loss 0.213017, acc 0.921875, learning_rate 0.000395032
2017-10-10T11:47:44.433282: step 689, loss 0.165935, acc 0.921875, learning_rate 0.000393828
2017-10-10T11:47:44.637801: step 690, loss 0.12482, acc 0.9375, learning_rate 0.000392629
2017-10-10T11:47:44.769326: step 691, loss 0.252972, acc 0.921875, learning_rate 0.000391434
2017-10-10T11:47:44.959970: step 692, loss 0.192645, acc 0.9375, learning_rate 0.000390245
2017-10-10T11:47:45.153058: step 693, loss 0.183788, acc 0.953125, learning_rate 0.00038906
2017-10-10T11:47:45.292896: step 694, loss 0.250341, acc 0.890625, learning_rate 0.00038788
2017-10-10T11:47:45.539262: step 695, loss 0.222443, acc 0.921875, learning_rate 0.000386705
2017-10-10T11:47:45.723682: step 696, loss 0.250037, acc 0.90625, learning_rate 0.000385535
2017-10-10T11:47:45.942185: step 697, loss 0.0953585, acc 0.984375, learning_rate 0.000384369
2017-10-10T11:47:46.120899: step 698, loss 0.155747, acc 0.9375, learning_rate 0.000383209
2017-10-10T11:47:46.332884: step 699, loss 0.225067, acc 0.90625, learning_rate 0.000382053
2017-10-10T11:47:46.533061: step 700, loss 0.297167, acc 0.890625, learning_rate 0.000380901
2017-10-10T11:47:46.696364: step 701, loss 0.139526, acc 0.96875, learning_rate 0.000379755
2017-10-10T11:47:46.920547: step 702, loss 0.205031, acc 0.90625, learning_rate 0.000378613
2017-10-10T11:47:47.069505: step 703, loss 0.260176, acc 0.90625, learning_rate 0.000377476
2017-10-10T11:47:47.232603: step 704, loss 0.13589, acc 0.96875, learning_rate 0.000376343
2017-10-10T11:47:47.429023: step 705, loss 0.26758, acc 0.90625, learning_rate 0.000375215
2017-10-10T11:47:47.645822: step 706, loss 0.211049, acc 0.9375, learning_rate 0.000374092
2017-10-10T11:47:47.840734: step 707, loss 0.180873, acc 0.9375, learning_rate 0.000372973
2017-10-10T11:47:48.000469: step 708, loss 0.167685, acc 0.9375, learning_rate 0.000371859
2017-10-10T11:47:48.163316: step 709, loss 0.139903, acc 0.984375, learning_rate 0.000370749
2017-10-10T11:47:48.325004: step 710, loss 0.264889, acc 0.9375, learning_rate 0.000369644
2017-10-10T11:47:48.501099: step 711, loss 0.18674, acc 0.953125, learning_rate 0.000368543
2017-10-10T11:47:48.696296: step 712, loss 0.176076, acc 0.96875, learning_rate 0.000367447
2017-10-10T11:47:48.888925: step 713, loss 0.214633, acc 0.90625, learning_rate 0.000366356
2017-10-10T11:47:49.087395: step 714, loss 0.260967, acc 0.9375, learning_rate 0.000365268
2017-10-10T11:47:49.238311: step 715, loss 0.239818, acc 0.890625, learning_rate 0.000364186
2017-10-10T11:47:49.420380: step 716, loss 0.324274, acc 0.875, learning_rate 0.000363107
2017-10-10T11:47:49.632479: step 717, loss 0.272334, acc 0.90625, learning_rate 0.000362033
2017-10-10T11:47:49.786818: step 718, loss 0.192001, acc 0.9375, learning_rate 0.000360964
2017-10-10T11:47:49.955085: step 719, loss 0.265725, acc 0.890625, learning_rate 0.000359899
2017-10-10T11:47:50.165039: step 720, loss 0.180631, acc 0.921875, learning_rate 0.000358838

Evaluation:
2017-10-10T11:47:50.502138: step 720, loss 0.248574, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-720

2017-10-10T11:47:51.788878: step 721, loss 0.442672, acc 0.84375, learning_rate 0.000357781
2017-10-10T11:47:51.985408: step 722, loss 0.214507, acc 0.921875, learning_rate 0.000356729
2017-10-10T11:47:52.161269: step 723, loss 0.332941, acc 0.859375, learning_rate 0.000355681
2017-10-10T11:47:52.306932: step 724, loss 0.190904, acc 0.953125, learning_rate 0.000354637
2017-10-10T11:47:52.426690: step 725, loss 0.274301, acc 0.890625, learning_rate 0.000353598
2017-10-10T11:47:52.596845: step 726, loss 0.235025, acc 0.953125, learning_rate 0.000352563
2017-10-10T11:47:52.716859: step 727, loss 0.213024, acc 0.96875, learning_rate 0.000351532
2017-10-10T11:47:52.920821: step 728, loss 0.316465, acc 0.921875, learning_rate 0.000350505
2017-10-10T11:47:53.062983: step 729, loss 0.148149, acc 0.96875, learning_rate 0.000349483
2017-10-10T11:47:53.150509: step 730, loss 0.22603, acc 0.875, learning_rate 0.000348465
2017-10-10T11:47:53.288827: step 731, loss 0.171815, acc 0.953125, learning_rate 0.00034745
2017-10-10T11:47:53.412939: step 732, loss 0.151478, acc 0.9375, learning_rate 0.00034644
2017-10-10T11:47:53.563148: step 733, loss 0.158309, acc 0.953125, learning_rate 0.000345434
2017-10-10T11:47:53.728693: step 734, loss 0.114196, acc 0.96875, learning_rate 0.000344433
2017-10-10T11:47:53.856226: step 735, loss 0.13432, acc 0.984375, learning_rate 0.000343435
2017-10-10T11:47:53.974050: step 736, loss 0.174519, acc 0.96875, learning_rate 0.000342441
2017-10-10T11:47:54.157206: step 737, loss 0.265097, acc 0.9375, learning_rate 0.000341452
2017-10-10T11:47:54.337052: step 738, loss 0.138015, acc 0.953125, learning_rate 0.000340466
2017-10-10T11:47:54.521069: step 739, loss 0.251205, acc 0.921875, learning_rate 0.000339485
2017-10-10T11:47:54.715502: step 740, loss 0.170423, acc 0.953125, learning_rate 0.000338507
2017-10-10T11:47:54.932564: step 741, loss 0.228457, acc 0.90625, learning_rate 0.000337534
2017-10-10T11:47:55.142816: step 742, loss 0.195723, acc 0.96875, learning_rate 0.000336564
2017-10-10T11:47:55.285150: step 743, loss 0.28682, acc 0.859375, learning_rate 0.000335598
2017-10-10T11:47:55.448535: step 744, loss 0.158017, acc 0.953125, learning_rate 0.000334637
2017-10-10T11:47:55.645331: step 745, loss 0.236613, acc 0.90625, learning_rate 0.000333679
2017-10-10T11:47:55.837821: step 746, loss 0.124193, acc 1, learning_rate 0.000332725
2017-10-10T11:47:56.013890: step 747, loss 0.183648, acc 0.9375, learning_rate 0.000331775
2017-10-10T11:47:56.176402: step 748, loss 0.209608, acc 0.96875, learning_rate 0.000330829
2017-10-10T11:47:56.377605: step 749, loss 0.191882, acc 0.96875, learning_rate 0.000329887
2017-10-10T11:47:56.572846: step 750, loss 0.126233, acc 0.984375, learning_rate 0.000328949
2017-10-10T11:47:56.744991: step 751, loss 0.175178, acc 0.9375, learning_rate 0.000328014
2017-10-10T11:47:56.936532: step 752, loss 0.147954, acc 0.96875, learning_rate 0.000327083
2017-10-10T11:47:57.085112: step 753, loss 0.211377, acc 0.9375, learning_rate 0.000326157
2017-10-10T11:47:57.289737: step 754, loss 0.359349, acc 0.890625, learning_rate 0.000325233
2017-10-10T11:47:57.494531: step 755, loss 0.156799, acc 0.9375, learning_rate 0.000324314
2017-10-10T11:47:57.729338: step 756, loss 0.129687, acc 0.96875, learning_rate 0.000323399
2017-10-10T11:47:57.956305: step 757, loss 0.225091, acc 0.90625, learning_rate 0.000322487
2017-10-10T11:47:58.140850: step 758, loss 0.256941, acc 0.921875, learning_rate 0.000321579
2017-10-10T11:47:58.331613: step 759, loss 0.222067, acc 0.90625, learning_rate 0.000320674
2017-10-10T11:47:58.486998: step 760, loss 0.155281, acc 0.96875, learning_rate 0.000319773

Evaluation:
2017-10-10T11:47:58.826232: step 760, loss 0.249225, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-760

2017-10-10T11:47:59.837838: step 761, loss 0.193643, acc 0.921875, learning_rate 0.000318876
2017-10-10T11:48:00.005631: step 762, loss 0.114856, acc 0.984375, learning_rate 0.000317983
2017-10-10T11:48:00.198608: step 763, loss 0.138348, acc 0.96875, learning_rate 0.000317093
2017-10-10T11:48:00.353139: step 764, loss 0.198826, acc 0.921875, learning_rate 0.000316207
2017-10-10T11:48:00.513469: step 765, loss 0.296282, acc 0.921875, learning_rate 0.000315325
2017-10-10T11:48:00.698923: step 766, loss 0.222528, acc 0.953125, learning_rate 0.000314446
2017-10-10T11:48:00.899760: step 767, loss 0.123974, acc 0.96875, learning_rate 0.00031357
2017-10-10T11:48:01.086654: step 768, loss 0.23535, acc 0.9375, learning_rate 0.000312699
2017-10-10T11:48:01.273164: step 769, loss 0.332222, acc 0.890625, learning_rate 0.00031183
2017-10-10T11:48:01.417184: step 770, loss 0.272982, acc 0.921875, learning_rate 0.000310966
2017-10-10T11:48:01.608870: step 771, loss 0.364468, acc 0.875, learning_rate 0.000310105
2017-10-10T11:48:01.794377: step 772, loss 0.264677, acc 0.921875, learning_rate 0.000309247
2017-10-10T11:48:01.998726: step 773, loss 0.256719, acc 0.921875, learning_rate 0.000308393
2017-10-10T11:48:02.173034: step 774, loss 0.223517, acc 0.921875, learning_rate 0.000307542
2017-10-10T11:48:02.326506: step 775, loss 0.328179, acc 0.875, learning_rate 0.000306695
2017-10-10T11:48:02.521857: step 776, loss 0.212647, acc 0.921875, learning_rate 0.000305852
2017-10-10T11:48:02.684836: step 777, loss 0.167487, acc 0.9375, learning_rate 0.000305011
2017-10-10T11:48:02.902389: step 778, loss 0.246324, acc 0.90625, learning_rate 0.000304174
2017-10-10T11:48:03.103430: step 779, loss 0.195265, acc 0.921875, learning_rate 0.000303341
2017-10-10T11:48:03.270939: step 780, loss 0.211028, acc 0.9375, learning_rate 0.000302511
2017-10-10T11:48:03.444938: step 781, loss 0.190627, acc 0.9375, learning_rate 0.000301684
2017-10-10T11:48:03.649251: step 782, loss 0.239811, acc 0.9375, learning_rate 0.000300861
2017-10-10T11:48:03.738801: step 783, loss 0.136443, acc 0.984375, learning_rate 0.000300041
2017-10-10T11:48:03.814958: step 784, loss 0.123063, acc 0.960784, learning_rate 0.000299225
2017-10-10T11:48:03.905258: step 785, loss 0.282069, acc 0.890625, learning_rate 0.000298412
2017-10-10T11:48:03.994412: step 786, loss 0.147268, acc 0.9375, learning_rate 0.000297602
2017-10-10T11:48:04.082984: step 787, loss 0.0984627, acc 0.96875, learning_rate 0.000296795
2017-10-10T11:48:04.251407: step 788, loss 0.27286, acc 0.921875, learning_rate 0.000295992
2017-10-10T11:48:04.412847: step 789, loss 0.189647, acc 0.96875, learning_rate 0.000295192
2017-10-10T11:48:04.623245: step 790, loss 0.148454, acc 0.96875, learning_rate 0.000294395
2017-10-10T11:48:04.801938: step 791, loss 0.174335, acc 0.9375, learning_rate 0.000293602
2017-10-10T11:48:04.955038: step 792, loss 0.220505, acc 0.953125, learning_rate 0.000292812
2017-10-10T11:48:05.131032: step 793, loss 0.288754, acc 0.890625, learning_rate 0.000292025
2017-10-10T11:48:05.342204: step 794, loss 0.12047, acc 0.96875, learning_rate 0.000291241
2017-10-10T11:48:05.540863: step 795, loss 0.128248, acc 0.984375, learning_rate 0.00029046
2017-10-10T11:48:05.682592: step 796, loss 0.158079, acc 0.953125, learning_rate 0.000289683
2017-10-10T11:48:05.853911: step 797, loss 0.191646, acc 0.90625, learning_rate 0.000288908
2017-10-10T11:48:06.052842: step 798, loss 0.193325, acc 0.9375, learning_rate 0.000288137
2017-10-10T11:48:06.264464: step 799, loss 0.344176, acc 0.90625, learning_rate 0.000287369
2017-10-10T11:48:06.436969: step 800, loss 0.15401, acc 0.953125, learning_rate 0.000286605

Evaluation:
2017-10-10T11:48:06.831820: step 800, loss 0.249708, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-800

2017-10-10T11:48:07.961547: step 801, loss 0.213545, acc 0.984375, learning_rate 0.000285843
2017-10-10T11:48:08.172720: step 802, loss 0.134225, acc 0.953125, learning_rate 0.000285084
2017-10-10T11:48:08.379596: step 803, loss 0.136948, acc 0.953125, learning_rate 0.000284329
2017-10-10T11:48:08.533134: step 804, loss 0.214326, acc 0.921875, learning_rate 0.000283577
2017-10-10T11:48:08.733017: step 805, loss 0.388736, acc 0.859375, learning_rate 0.000282827
2017-10-10T11:48:08.928840: step 806, loss 0.165469, acc 0.953125, learning_rate 0.000282081
2017-10-10T11:48:09.136367: step 807, loss 0.148468, acc 0.9375, learning_rate 0.000281338
2017-10-10T11:48:09.325046: step 808, loss 0.230738, acc 0.890625, learning_rate 0.000280598
2017-10-10T11:48:09.503144: step 809, loss 0.252634, acc 0.9375, learning_rate 0.00027986
2017-10-10T11:48:09.671180: step 810, loss 0.212864, acc 0.890625, learning_rate 0.000279126
2017-10-10T11:48:09.883626: step 811, loss 0.126299, acc 0.96875, learning_rate 0.000278395
2017-10-10T11:48:10.104945: step 812, loss 0.236553, acc 0.921875, learning_rate 0.000277667
2017-10-10T11:48:10.271354: step 813, loss 0.357295, acc 0.875, learning_rate 0.000276942
2017-10-10T11:48:10.429743: step 814, loss 0.210673, acc 0.9375, learning_rate 0.00027622
2017-10-10T11:48:10.629990: step 815, loss 0.161775, acc 0.9375, learning_rate 0.0002755
2017-10-10T11:48:10.824355: step 816, loss 0.161924, acc 0.921875, learning_rate 0.000274784
2017-10-10T11:48:10.969990: step 817, loss 0.138951, acc 0.953125, learning_rate 0.000274071
2017-10-10T11:48:11.144849: step 818, loss 0.231524, acc 0.90625, learning_rate 0.00027336
2017-10-10T11:48:11.349666: step 819, loss 0.252346, acc 0.921875, learning_rate 0.000272652
2017-10-10T11:48:11.528934: step 820, loss 0.190191, acc 0.9375, learning_rate 0.000271948
2017-10-10T11:48:11.717200: step 821, loss 0.275035, acc 0.90625, learning_rate 0.000271246
2017-10-10T11:48:11.881113: step 822, loss 0.246954, acc 0.90625, learning_rate 0.000270547
2017-10-10T11:48:12.062073: step 823, loss 0.230237, acc 0.921875, learning_rate 0.000269851
2017-10-10T11:48:12.245688: step 824, loss 0.219762, acc 0.953125, learning_rate 0.000269157
2017-10-10T11:48:12.429466: step 825, loss 0.196435, acc 0.90625, learning_rate 0.000268467
2017-10-10T11:48:12.585213: step 826, loss 0.276612, acc 0.921875, learning_rate 0.000267779
2017-10-10T11:48:12.783692: step 827, loss 0.289111, acc 0.921875, learning_rate 0.000267094
2017-10-10T11:48:12.988891: step 828, loss 0.188145, acc 0.953125, learning_rate 0.000266412
2017-10-10T11:48:13.124405: step 829, loss 0.090627, acc 0.984375, learning_rate 0.000265733
2017-10-10T11:48:13.277007: step 830, loss 0.211099, acc 0.921875, learning_rate 0.000265057
2017-10-10T11:48:13.491256: step 831, loss 0.255265, acc 0.9375, learning_rate 0.000264383
2017-10-10T11:48:13.702627: step 832, loss 0.188749, acc 0.921875, learning_rate 0.000263712
2017-10-10T11:48:13.846579: step 833, loss 0.123021, acc 0.96875, learning_rate 0.000263044
2017-10-10T11:48:14.251223: step 834, loss 0.164198, acc 0.9375, learning_rate 0.000262378
2017-10-10T11:48:14.621243: step 835, loss 0.162075, acc 0.953125, learning_rate 0.000261715
2017-10-10T11:48:14.997565: step 836, loss 0.259119, acc 0.90625, learning_rate 0.000261055
2017-10-10T11:48:15.489237: step 837, loss 0.223502, acc 0.90625, learning_rate 0.000260398
2017-10-10T11:48:15.686347: step 838, loss 0.282099, acc 0.890625, learning_rate 0.000259743
2017-10-10T11:48:15.762749: step 839, loss 0.177672, acc 0.921875, learning_rate 0.000259091
2017-10-10T11:48:15.839979: step 840, loss 0.234781, acc 0.9375, learning_rate 0.000258442

Evaluation:
2017-10-10T11:48:37.235705: step 840, loss 0.246912, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-840

2017-10-10T11:48:45.260256: step 841, loss 0.22945, acc 0.90625, learning_rate 0.000257795
2017-10-10T11:48:45.500983: step 842, loss 0.250371, acc 0.9375, learning_rate 0.000257151
2017-10-10T11:48:45.689017: step 843, loss 0.257789, acc 0.875, learning_rate 0.00025651
2017-10-10T11:48:45.837238: step 844, loss 0.137738, acc 0.96875, learning_rate 0.000255871
2017-10-10T11:48:46.043784: step 845, loss 0.152558, acc 0.953125, learning_rate 0.000255235
2017-10-10T11:48:46.235055: step 846, loss 0.235872, acc 0.921875, learning_rate 0.000254601
2017-10-10T11:48:46.386780: step 847, loss 0.124327, acc 0.96875, learning_rate 0.00025397
2017-10-10T11:48:46.554175: step 848, loss 0.286492, acc 0.890625, learning_rate 0.000253341
2017-10-10T11:48:46.753999: step 849, loss 0.121971, acc 0.96875, learning_rate 0.000252716
2017-10-10T11:48:46.894527: step 850, loss 0.206025, acc 0.9375, learning_rate 0.000252092
2017-10-10T11:48:47.061690: step 851, loss 0.242255, acc 0.953125, learning_rate 0.000251471
2017-10-10T11:48:47.259137: step 852, loss 0.143478, acc 0.96875, learning_rate 0.000250853
2017-10-10T11:48:47.372885: step 853, loss 0.172284, acc 0.953125, learning_rate 0.000250237
2017-10-10T11:48:47.573396: step 854, loss 0.100399, acc 0.984375, learning_rate 0.000249624
2017-10-10T11:48:47.780847: step 855, loss 0.150155, acc 0.96875, learning_rate 0.000249013
2017-10-10T11:48:47.953036: step 856, loss 0.0993802, acc 0.96875, learning_rate 0.000248405
2017-10-10T11:48:48.104814: step 857, loss 0.311418, acc 0.875, learning_rate 0.000247799
2017-10-10T11:48:48.304588: step 858, loss 0.22775, acc 0.9375, learning_rate 0.000247196
2017-10-10T11:48:48.552892: step 859, loss 0.139323, acc 0.9375, learning_rate 0.000246595
2017-10-10T11:48:48.729469: step 860, loss 0.225933, acc 0.90625, learning_rate 0.000245997
2017-10-10T11:48:48.905563: step 861, loss 0.184054, acc 0.90625, learning_rate 0.000245401
2017-10-10T11:48:49.123022: step 862, loss 0.310415, acc 0.859375, learning_rate 0.000244808
2017-10-10T11:48:49.324834: step 863, loss 0.0861506, acc 0.984375, learning_rate 0.000244216
2017-10-10T11:48:49.480368: step 864, loss 0.199488, acc 0.9375, learning_rate 0.000243628
2017-10-10T11:48:49.656535: step 865, loss 0.193403, acc 0.90625, learning_rate 0.000243042
2017-10-10T11:48:49.847998: step 866, loss 0.264284, acc 0.921875, learning_rate 0.000242458
2017-10-10T11:48:50.003026: step 867, loss 0.135333, acc 0.96875, learning_rate 0.000241876
2017-10-10T11:48:50.160422: step 868, loss 0.132189, acc 0.953125, learning_rate 0.000241297
2017-10-10T11:48:50.358268: step 869, loss 0.108466, acc 1, learning_rate 0.00024072
2017-10-10T11:48:50.498178: step 870, loss 0.363042, acc 0.875, learning_rate 0.000240146
2017-10-10T11:48:50.672892: step 871, loss 0.208334, acc 0.9375, learning_rate 0.000239574
2017-10-10T11:48:50.878338: step 872, loss 0.13482, acc 0.9375, learning_rate 0.000239004
2017-10-10T11:48:51.034577: step 873, loss 0.259216, acc 0.921875, learning_rate 0.000238437
2017-10-10T11:48:51.201033: step 874, loss 0.0887778, acc 0.984375, learning_rate 0.000237872
2017-10-10T11:48:51.390775: step 875, loss 0.286261, acc 0.890625, learning_rate 0.000237309
2017-10-10T11:48:51.611600: step 876, loss 0.11502, acc 0.96875, learning_rate 0.000236749
2017-10-10T11:48:51.821831: step 877, loss 0.208904, acc 0.90625, learning_rate 0.00023619
2017-10-10T11:48:51.993612: step 878, loss 0.223388, acc 0.90625, learning_rate 0.000235635
2017-10-10T11:48:52.167066: step 879, loss 0.316469, acc 0.859375, learning_rate 0.000235081
2017-10-10T11:48:52.380874: step 880, loss 0.312693, acc 0.890625, learning_rate 0.00023453

Evaluation:
2017-10-10T11:48:52.721539: step 880, loss 0.247664, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-880

2017-10-10T11:48:53.770891: step 881, loss 0.199034, acc 0.953125, learning_rate 0.00023398
2017-10-10T11:48:53.938307: step 882, loss 0.138527, acc 0.980392, learning_rate 0.000233434
2017-10-10T11:48:54.445909: step 883, loss 0.118446, acc 0.984375, learning_rate 0.000232889
2017-10-10T11:48:54.664606: step 884, loss 0.271988, acc 0.921875, learning_rate 0.000232346
2017-10-10T11:48:54.896793: step 885, loss 0.173546, acc 0.9375, learning_rate 0.000231806
2017-10-10T11:48:55.008906: step 886, loss 0.114488, acc 0.984375, learning_rate 0.000231268
2017-10-10T11:48:55.236849: step 887, loss 0.150058, acc 0.96875, learning_rate 0.000230732
2017-10-10T11:48:55.419148: step 888, loss 0.300426, acc 0.90625, learning_rate 0.000230199
2017-10-10T11:48:55.509402: step 889, loss 0.334867, acc 0.90625, learning_rate 0.000229667
2017-10-10T11:48:55.609657: step 890, loss 0.149216, acc 0.953125, learning_rate 0.000229138
2017-10-10T11:48:55.699404: step 891, loss 0.229709, acc 0.921875, learning_rate 0.000228611
2017-10-10T11:48:55.789549: step 892, loss 0.298674, acc 0.90625, learning_rate 0.000228086
2017-10-10T11:48:55.961296: step 893, loss 0.229759, acc 0.921875, learning_rate 0.000227563
2017-10-10T11:48:56.111370: step 894, loss 0.152099, acc 0.953125, learning_rate 0.000227043
2017-10-10T11:48:56.224882: step 895, loss 0.299495, acc 0.84375, learning_rate 0.000226524
2017-10-10T11:48:56.367558: step 896, loss 0.0745234, acc 1, learning_rate 0.000226008
2017-10-10T11:48:56.570659: step 897, loss 0.113264, acc 0.953125, learning_rate 0.000225493
2017-10-10T11:48:56.771576: step 898, loss 0.213825, acc 0.9375, learning_rate 0.000224981
2017-10-10T11:48:56.929357: step 899, loss 0.212479, acc 0.921875, learning_rate 0.000224471
2017-10-10T11:48:57.104852: step 900, loss 0.325071, acc 0.859375, learning_rate 0.000223963
2017-10-10T11:48:57.314175: step 901, loss 0.176132, acc 0.9375, learning_rate 0.000223457
2017-10-10T11:48:57.475278: step 902, loss 0.131206, acc 0.984375, learning_rate 0.000222953
2017-10-10T11:48:57.619676: step 903, loss 0.104562, acc 0.984375, learning_rate 0.000222451
2017-10-10T11:48:57.847665: step 904, loss 0.239473, acc 0.90625, learning_rate 0.000221951
2017-10-10T11:48:58.082853: step 905, loss 0.327548, acc 0.890625, learning_rate 0.000221453
2017-10-10T11:48:58.271946: step 906, loss 0.233723, acc 0.9375, learning_rate 0.000220958
2017-10-10T11:48:58.468861: step 907, loss 0.125614, acc 0.984375, learning_rate 0.000220464
2017-10-10T11:48:58.680006: step 908, loss 0.18618, acc 0.9375, learning_rate 0.000219972
2017-10-10T11:48:58.900189: step 909, loss 0.232035, acc 0.90625, learning_rate 0.000219483
2017-10-10T11:48:59.102658: step 910, loss 0.233556, acc 0.9375, learning_rate 0.000218995
2017-10-10T11:48:59.272072: step 911, loss 0.182188, acc 0.9375, learning_rate 0.000218509
2017-10-10T11:48:59.477005: step 912, loss 0.176836, acc 0.984375, learning_rate 0.000218025
2017-10-10T11:48:59.692691: step 913, loss 0.13741, acc 0.96875, learning_rate 0.000217544
2017-10-10T11:48:59.899717: step 914, loss 0.235575, acc 0.921875, learning_rate 0.000217064
2017-10-10T11:49:00.093327: step 915, loss 0.140275, acc 0.9375, learning_rate 0.000216586
2017-10-10T11:49:00.260861: step 916, loss 0.095588, acc 0.96875, learning_rate 0.00021611
2017-10-10T11:49:00.445175: step 917, loss 0.160301, acc 0.953125, learning_rate 0.000215636
2017-10-10T11:49:00.629054: step 918, loss 0.152331, acc 0.953125, learning_rate 0.000215164
2017-10-10T11:49:00.792847: step 919, loss 0.11211, acc 0.984375, learning_rate 0.000214694
2017-10-10T11:49:00.965080: step 920, loss 0.256007, acc 0.921875, learning_rate 0.000214226

Evaluation:
2017-10-10T11:49:01.313179: step 920, loss 0.243543, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-920

2017-10-10T11:49:02.471658: step 921, loss 0.206042, acc 0.9375, learning_rate 0.00021376
2017-10-10T11:49:02.661768: step 922, loss 0.302009, acc 0.921875, learning_rate 0.000213295
2017-10-10T11:49:02.819022: step 923, loss 0.199809, acc 0.90625, learning_rate 0.000212833
2017-10-10T11:49:02.998330: step 924, loss 0.206572, acc 0.921875, learning_rate 0.000212372
2017-10-10T11:49:03.198078: step 925, loss 0.180323, acc 0.90625, learning_rate 0.000211914
2017-10-10T11:49:03.417532: step 926, loss 0.0807153, acc 1, learning_rate 0.000211457
2017-10-10T11:49:03.616549: step 927, loss 0.141398, acc 0.953125, learning_rate 0.000211002
2017-10-10T11:49:03.749072: step 928, loss 0.165186, acc 0.9375, learning_rate 0.000210549
2017-10-10T11:49:03.937238: step 929, loss 0.123583, acc 0.96875, learning_rate 0.000210098
2017-10-10T11:49:04.112900: step 930, loss 0.216869, acc 0.921875, learning_rate 0.000209648
2017-10-10T11:49:04.303570: step 931, loss 0.303833, acc 0.875, learning_rate 0.000209201
2017-10-10T11:49:04.476857: step 932, loss 0.190756, acc 0.921875, learning_rate 0.000208755
2017-10-10T11:49:04.664599: step 933, loss 0.146866, acc 0.953125, learning_rate 0.000208311
2017-10-10T11:49:04.861271: step 934, loss 0.203167, acc 0.9375, learning_rate 0.000207869
2017-10-10T11:49:05.052872: step 935, loss 0.298965, acc 0.890625, learning_rate 0.000207429
2017-10-10T11:49:05.299139: step 936, loss 0.312939, acc 0.90625, learning_rate 0.00020699
2017-10-10T11:49:05.518674: step 937, loss 0.125422, acc 0.96875, learning_rate 0.000206554
2017-10-10T11:49:05.679883: step 938, loss 0.212828, acc 0.9375, learning_rate 0.000206119
2017-10-10T11:49:05.853445: step 939, loss 0.253457, acc 0.890625, learning_rate 0.000205685
2017-10-10T11:49:05.988207: step 940, loss 0.261484, acc 0.90625, learning_rate 0.000205254
2017-10-10T11:49:06.112939: step 941, loss 0.21437, acc 0.90625, learning_rate 0.000204824
2017-10-10T11:49:06.320818: step 942, loss 0.204694, acc 0.921875, learning_rate 0.000204397
2017-10-10T11:49:06.508244: step 943, loss 0.18021, acc 0.96875, learning_rate 0.00020397
2017-10-10T11:49:06.648395: step 944, loss 0.187044, acc 0.921875, learning_rate 0.000203546
2017-10-10T11:49:06.780920: step 945, loss 0.20563, acc 0.9375, learning_rate 0.000203123
2017-10-10T11:49:06.910124: step 946, loss 0.248269, acc 0.875, learning_rate 0.000202702
2017-10-10T11:49:07.042709: step 947, loss 0.182303, acc 0.921875, learning_rate 0.000202283
2017-10-10T11:49:07.204209: step 948, loss 0.0711074, acc 0.96875, learning_rate 0.000201866
2017-10-10T11:49:07.353147: step 949, loss 0.163193, acc 0.953125, learning_rate 0.00020145
2017-10-10T11:49:07.519901: step 950, loss 0.201539, acc 0.921875, learning_rate 0.000201036
2017-10-10T11:49:07.750055: step 951, loss 0.191856, acc 0.921875, learning_rate 0.000200623
2017-10-10T11:49:07.901087: step 952, loss 0.173192, acc 0.953125, learning_rate 0.000200213
2017-10-10T11:49:08.061729: step 953, loss 0.266646, acc 0.875, learning_rate 0.000199804
2017-10-10T11:49:08.250612: step 954, loss 0.25166, acc 0.875, learning_rate 0.000199396
2017-10-10T11:49:08.440366: step 955, loss 0.207772, acc 0.9375, learning_rate 0.000198991
2017-10-10T11:49:08.613230: step 956, loss 0.239006, acc 0.9375, learning_rate 0.000198587
2017-10-10T11:49:08.778368: step 957, loss 0.248955, acc 0.9375, learning_rate 0.000198184
2017-10-10T11:49:08.953398: step 958, loss 0.102723, acc 1, learning_rate 0.000197783
2017-10-10T11:49:09.128475: step 959, loss 0.22943, acc 0.890625, learning_rate 0.000197384
2017-10-10T11:49:09.345928: step 960, loss 0.351678, acc 0.890625, learning_rate 0.000196987

Evaluation:
2017-10-10T11:49:09.680235: step 960, loss 0.243309, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-960

2017-10-10T11:49:10.723941: step 961, loss 0.150115, acc 0.9375, learning_rate 0.000196591
2017-10-10T11:49:10.919326: step 962, loss 0.205734, acc 0.90625, learning_rate 0.000196197
2017-10-10T11:49:11.080798: step 963, loss 0.201885, acc 0.953125, learning_rate 0.000195804
2017-10-10T11:49:11.245023: step 964, loss 0.225044, acc 0.921875, learning_rate 0.000195413
2017-10-10T11:49:11.442811: step 965, loss 0.106795, acc 0.96875, learning_rate 0.000195023
2017-10-10T11:49:11.655451: step 966, loss 0.172161, acc 0.9375, learning_rate 0.000194636
2017-10-10T11:49:11.830972: step 967, loss 0.205503, acc 0.9375, learning_rate 0.000194249
2017-10-10T11:49:11.975204: step 968, loss 0.344297, acc 0.890625, learning_rate 0.000193865
2017-10-10T11:49:12.154031: step 969, loss 0.236041, acc 0.921875, learning_rate 0.000193482
2017-10-10T11:49:12.350269: step 970, loss 0.168126, acc 0.921875, learning_rate 0.0001931
2017-10-10T11:49:12.508984: step 971, loss 0.136277, acc 0.96875, learning_rate 0.00019272
2017-10-10T11:49:12.755930: step 972, loss 0.166235, acc 0.921875, learning_rate 0.000192341
2017-10-10T11:49:12.968879: step 973, loss 0.138616, acc 0.96875, learning_rate 0.000191965
2017-10-10T11:49:13.126476: step 974, loss 0.226489, acc 0.953125, learning_rate 0.000191589
2017-10-10T11:49:13.290437: step 975, loss 0.204625, acc 0.921875, learning_rate 0.000191215
2017-10-10T11:49:13.484870: step 976, loss 0.128308, acc 0.9375, learning_rate 0.000190843
2017-10-10T11:49:13.691905: step 977, loss 0.17369, acc 0.953125, learning_rate 0.000190472
2017-10-10T11:49:13.892867: step 978, loss 0.301685, acc 0.90625, learning_rate 0.000190103
2017-10-10T11:49:14.043911: step 979, loss 0.152902, acc 0.9375, learning_rate 0.000189735
2017-10-10T11:49:14.183257: step 980, loss 0.129056, acc 0.960784, learning_rate 0.000189369
2017-10-10T11:49:14.393836: step 981, loss 0.298682, acc 0.90625, learning_rate 0.000189004
2017-10-10T11:49:14.528419: step 982, loss 0.354517, acc 0.90625, learning_rate 0.000188641
2017-10-10T11:49:14.723995: step 983, loss 0.112418, acc 0.953125, learning_rate 0.000188279
2017-10-10T11:49:14.918046: step 984, loss 0.132894, acc 0.96875, learning_rate 0.000187919
2017-10-10T11:49:15.128563: step 985, loss 0.0954002, acc 0.984375, learning_rate 0.00018756
2017-10-10T11:49:15.279357: step 986, loss 0.169318, acc 0.953125, learning_rate 0.000187202
2017-10-10T11:49:15.436806: step 987, loss 0.22137, acc 0.890625, learning_rate 0.000186846
2017-10-10T11:49:15.624493: step 988, loss 0.151093, acc 0.984375, learning_rate 0.000186492
2017-10-10T11:49:15.805047: step 989, loss 0.163611, acc 0.953125, learning_rate 0.000186139
2017-10-10T11:49:16.041152: step 990, loss 0.137013, acc 0.953125, learning_rate 0.000185787
2017-10-10T11:49:16.216471: step 991, loss 0.256585, acc 0.921875, learning_rate 0.000185437
2017-10-10T11:49:16.333128: step 992, loss 0.165178, acc 0.9375, learning_rate 0.000185088
2017-10-10T11:49:16.440907: step 993, loss 0.192145, acc 0.9375, learning_rate 0.000184741
2017-10-10T11:49:16.594683: step 994, loss 0.114968, acc 0.984375, learning_rate 0.000184395
2017-10-10T11:49:16.749892: step 995, loss 0.231129, acc 0.890625, learning_rate 0.000184051
2017-10-10T11:49:16.865543: step 996, loss 0.191715, acc 0.9375, learning_rate 0.000183708
2017-10-10T11:49:16.990238: step 997, loss 0.187668, acc 0.953125, learning_rate 0.000183366
2017-10-10T11:49:17.208844: step 998, loss 0.235848, acc 0.9375, learning_rate 0.000183026
2017-10-10T11:49:17.481317: step 999, loss 0.261634, acc 0.90625, learning_rate 0.000182687
2017-10-10T11:49:17.616889: step 1000, loss 0.155925, acc 0.9375, learning_rate 0.000182349

Evaluation:
2017-10-10T11:49:17.911773: step 1000, loss 0.242331, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1000

2017-10-10T11:49:19.099255: step 1001, loss 0.169916, acc 0.921875, learning_rate 0.000182013
2017-10-10T11:49:19.284889: step 1002, loss 0.180429, acc 0.9375, learning_rate 0.000181678
2017-10-10T11:49:19.469029: step 1003, loss 0.152838, acc 0.953125, learning_rate 0.000181345
2017-10-10T11:49:19.640890: step 1004, loss 0.281451, acc 0.921875, learning_rate 0.000181013
2017-10-10T11:49:19.790219: step 1005, loss 0.182497, acc 0.921875, learning_rate 0.000180682
2017-10-10T11:49:19.965723: step 1006, loss 0.186858, acc 0.953125, learning_rate 0.000180353
2017-10-10T11:49:20.179573: step 1007, loss 0.248484, acc 0.9375, learning_rate 0.000180025
2017-10-10T11:49:20.362736: step 1008, loss 0.196672, acc 0.9375, learning_rate 0.000179698
2017-10-10T11:49:20.493484: step 1009, loss 0.200397, acc 0.953125, learning_rate 0.000179373
2017-10-10T11:49:20.694296: step 1010, loss 0.131878, acc 0.953125, learning_rate 0.000179049
2017-10-10T11:49:20.884850: step 1011, loss 0.108238, acc 0.984375, learning_rate 0.000178726
2017-10-10T11:49:21.018430: step 1012, loss 0.121296, acc 0.953125, learning_rate 0.000178405
2017-10-10T11:49:21.225387: step 1013, loss 0.249489, acc 0.9375, learning_rate 0.000178085
2017-10-10T11:49:21.377109: step 1014, loss 0.162277, acc 0.96875, learning_rate 0.000177766
2017-10-10T11:49:21.536600: step 1015, loss 0.214912, acc 0.921875, learning_rate 0.000177449
2017-10-10T11:49:21.736868: step 1016, loss 0.156898, acc 0.953125, learning_rate 0.000177133
2017-10-10T11:49:21.888933: step 1017, loss 0.124172, acc 0.953125, learning_rate 0.000176818
2017-10-10T11:49:22.040680: step 1018, loss 0.143759, acc 0.9375, learning_rate 0.000176504
2017-10-10T11:49:22.244628: step 1019, loss 0.16881, acc 0.96875, learning_rate 0.000176192
2017-10-10T11:49:22.424932: step 1020, loss 0.164088, acc 0.953125, learning_rate 0.000175881
2017-10-10T11:49:22.592909: step 1021, loss 0.145551, acc 0.953125, learning_rate 0.000175571
2017-10-10T11:49:22.779395: step 1022, loss 0.261898, acc 0.953125, learning_rate 0.000175263
2017-10-10T11:49:22.982287: step 1023, loss 0.140693, acc 0.96875, learning_rate 0.000174956
2017-10-10T11:49:23.154935: step 1024, loss 0.217274, acc 0.9375, learning_rate 0.00017465
2017-10-10T11:49:23.286992: step 1025, loss 0.136695, acc 0.96875, learning_rate 0.000174345
2017-10-10T11:49:23.458528: step 1026, loss 0.107188, acc 0.984375, learning_rate 0.000174042
2017-10-10T11:49:23.676234: step 1027, loss 0.190178, acc 0.9375, learning_rate 0.000173739
2017-10-10T11:49:23.848884: step 1028, loss 0.158878, acc 0.953125, learning_rate 0.000173438
2017-10-10T11:49:24.005523: step 1029, loss 0.0790769, acc 1, learning_rate 0.000173139
2017-10-10T11:49:24.176345: step 1030, loss 0.253655, acc 0.90625, learning_rate 0.00017284
2017-10-10T11:49:24.384841: step 1031, loss 0.206486, acc 0.921875, learning_rate 0.000172543
2017-10-10T11:49:24.549083: step 1032, loss 0.287581, acc 0.921875, learning_rate 0.000172247
2017-10-10T11:49:24.712873: step 1033, loss 0.227318, acc 0.90625, learning_rate 0.000171952
2017-10-10T11:49:24.903180: step 1034, loss 0.134027, acc 0.96875, learning_rate 0.000171658
2017-10-10T11:49:25.053028: step 1035, loss 0.346022, acc 0.921875, learning_rate 0.000171366
2017-10-10T11:49:25.193096: step 1036, loss 0.217786, acc 0.953125, learning_rate 0.000171074
2017-10-10T11:49:25.392387: step 1037, loss 0.136154, acc 0.96875, learning_rate 0.000170784
2017-10-10T11:49:25.602077: step 1038, loss 0.194119, acc 0.921875, learning_rate 0.000170495
2017-10-10T11:49:25.753195: step 1039, loss 0.246323, acc 0.90625, learning_rate 0.000170208
2017-10-10T11:49:25.929076: step 1040, loss 0.256387, acc 0.90625, learning_rate 0.000169921

Evaluation:
2017-10-10T11:49:26.312858: step 1040, loss 0.242291, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1040

2017-10-10T11:49:27.228846: step 1041, loss 0.137239, acc 0.953125, learning_rate 0.000169636
2017-10-10T11:49:27.380171: step 1042, loss 0.171326, acc 0.953125, learning_rate 0.000169351
2017-10-10T11:49:27.475083: step 1043, loss 0.248674, acc 0.9375, learning_rate 0.000169068
2017-10-10T11:49:27.625085: step 1044, loss 0.194445, acc 0.921875, learning_rate 0.000168786
2017-10-10T11:49:27.794238: step 1045, loss 0.226506, acc 0.921875, learning_rate 0.000168506
2017-10-10T11:49:27.932849: step 1046, loss 0.271177, acc 0.921875, learning_rate 0.000168226
2017-10-10T11:49:28.179147: step 1047, loss 0.195852, acc 0.921875, learning_rate 0.000167947
2017-10-10T11:49:28.412444: step 1048, loss 0.160518, acc 0.953125, learning_rate 0.00016767
2017-10-10T11:49:28.552869: step 1049, loss 0.333436, acc 0.875, learning_rate 0.000167394
2017-10-10T11:49:28.688889: step 1050, loss 0.171135, acc 0.9375, learning_rate 0.000167119
2017-10-10T11:49:28.812871: step 1051, loss 0.163473, acc 0.921875, learning_rate 0.000166845
2017-10-10T11:49:28.962956: step 1052, loss 0.337447, acc 0.90625, learning_rate 0.000166572
2017-10-10T11:49:29.108709: step 1053, loss 0.245621, acc 0.921875, learning_rate 0.0001663
2017-10-10T11:49:29.271318: step 1054, loss 0.141507, acc 0.96875, learning_rate 0.00016603
2017-10-10T11:49:29.419995: step 1055, loss 0.215792, acc 0.921875, learning_rate 0.00016576
2017-10-10T11:49:29.560844: step 1056, loss 0.21146, acc 0.890625, learning_rate 0.000165492
2017-10-10T11:49:29.744299: step 1057, loss 0.175621, acc 0.9375, learning_rate 0.000165224
2017-10-10T11:49:29.896833: step 1058, loss 0.294731, acc 0.875, learning_rate 0.000164958
2017-10-10T11:49:30.082297: step 1059, loss 0.245503, acc 0.921875, learning_rate 0.000164693
2017-10-10T11:49:30.298673: step 1060, loss 0.254868, acc 0.90625, learning_rate 0.000164429
2017-10-10T11:49:30.487309: step 1061, loss 0.233809, acc 0.9375, learning_rate 0.000164166
2017-10-10T11:49:30.632935: step 1062, loss 0.155616, acc 0.9375, learning_rate 0.000163904
2017-10-10T11:49:30.808932: step 1063, loss 0.185987, acc 0.9375, learning_rate 0.000163643
2017-10-10T11:49:30.994416: step 1064, loss 0.134442, acc 0.96875, learning_rate 0.000163383
2017-10-10T11:49:31.136636: step 1065, loss 0.104058, acc 0.96875, learning_rate 0.000163125
2017-10-10T11:49:31.322431: step 1066, loss 0.13994, acc 0.953125, learning_rate 0.000162867
2017-10-10T11:49:31.515820: step 1067, loss 0.177113, acc 0.984375, learning_rate 0.00016261
2017-10-10T11:49:31.663058: step 1068, loss 0.148326, acc 0.953125, learning_rate 0.000162355
2017-10-10T11:49:31.851062: step 1069, loss 0.0658514, acc 0.984375, learning_rate 0.0001621
2017-10-10T11:49:32.063740: step 1070, loss 0.123217, acc 0.953125, learning_rate 0.000161847
2017-10-10T11:49:32.192985: step 1071, loss 0.216208, acc 0.96875, learning_rate 0.000161594
2017-10-10T11:49:32.367182: step 1072, loss 0.16668, acc 0.9375, learning_rate 0.000161343
2017-10-10T11:49:32.610515: step 1073, loss 0.19514, acc 0.9375, learning_rate 0.000161093
2017-10-10T11:49:32.824737: step 1074, loss 0.242914, acc 0.953125, learning_rate 0.000160843
2017-10-10T11:49:33.013708: step 1075, loss 0.17101, acc 0.9375, learning_rate 0.000160595
2017-10-10T11:49:33.185177: step 1076, loss 0.198414, acc 0.90625, learning_rate 0.000160348
2017-10-10T11:49:33.349239: step 1077, loss 0.127863, acc 0.96875, learning_rate 0.000160101
2017-10-10T11:49:33.536832: step 1078, loss 0.111513, acc 0.980392, learning_rate 0.000159856
2017-10-10T11:49:33.670768: step 1079, loss 0.207442, acc 0.921875, learning_rate 0.000159612
2017-10-10T11:49:33.845733: step 1080, loss 0.163412, acc 0.953125, learning_rate 0.000159368

Evaluation:
2017-10-10T11:49:34.246840: step 1080, loss 0.243378, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1080

2017-10-10T11:49:35.400523: step 1081, loss 0.269345, acc 0.921875, learning_rate 0.000159126
2017-10-10T11:49:35.572122: step 1082, loss 0.178781, acc 0.9375, learning_rate 0.000158885
2017-10-10T11:49:35.756371: step 1083, loss 0.161811, acc 0.921875, learning_rate 0.000158644
2017-10-10T11:49:35.964015: step 1084, loss 0.0969829, acc 0.96875, learning_rate 0.000158405
2017-10-10T11:49:36.185163: step 1085, loss 0.203404, acc 0.921875, learning_rate 0.000158167
2017-10-10T11:49:36.364826: step 1086, loss 0.133586, acc 0.984375, learning_rate 0.000157929
2017-10-10T11:49:36.484297: step 1087, loss 0.135154, acc 0.953125, learning_rate 0.000157693
2017-10-10T11:49:36.686492: step 1088, loss 0.203464, acc 0.921875, learning_rate 0.000157457
2017-10-10T11:49:36.861071: step 1089, loss 0.136478, acc 0.9375, learning_rate 0.000157223
2017-10-10T11:49:37.064825: step 1090, loss 0.171042, acc 0.9375, learning_rate 0.000156989
2017-10-10T11:49:37.261445: step 1091, loss 0.177447, acc 0.921875, learning_rate 0.000156757
2017-10-10T11:49:37.388840: step 1092, loss 0.365872, acc 0.890625, learning_rate 0.000156525
2017-10-10T11:49:37.515286: step 1093, loss 0.110447, acc 0.96875, learning_rate 0.000156294
2017-10-10T11:49:37.662174: step 1094, loss 0.24951, acc 0.90625, learning_rate 0.000156064
2017-10-10T11:49:37.823784: step 1095, loss 0.216911, acc 0.9375, learning_rate 0.000155836
2017-10-10T11:49:37.954615: step 1096, loss 0.312973, acc 0.84375, learning_rate 0.000155608
2017-10-10T11:49:38.072824: step 1097, loss 0.0968513, acc 1, learning_rate 0.000155381
2017-10-10T11:49:38.231606: step 1098, loss 0.173724, acc 0.953125, learning_rate 0.000155155
2017-10-10T11:49:38.420435: step 1099, loss 0.194907, acc 0.9375, learning_rate 0.000154929
2017-10-10T11:49:38.566319: step 1100, loss 0.278984, acc 0.875, learning_rate 0.000154705
2017-10-10T11:49:38.742191: step 1101, loss 0.289886, acc 0.921875, learning_rate 0.000154482
2017-10-10T11:49:38.953174: step 1102, loss 0.148169, acc 0.96875, learning_rate 0.00015426
2017-10-10T11:49:39.109467: step 1103, loss 0.318238, acc 0.859375, learning_rate 0.000154038
2017-10-10T11:49:39.275314: step 1104, loss 0.16812, acc 0.9375, learning_rate 0.000153818
2017-10-10T11:49:39.484517: step 1105, loss 0.161799, acc 0.9375, learning_rate 0.000153598
2017-10-10T11:49:39.705982: step 1106, loss 0.192618, acc 0.9375, learning_rate 0.000153379
2017-10-10T11:49:39.904920: step 1107, loss 0.295811, acc 0.875, learning_rate 0.000153161
2017-10-10T11:49:40.017857: step 1108, loss 0.251636, acc 0.953125, learning_rate 0.000152944
2017-10-10T11:49:40.427354: step 1109, loss 0.112673, acc 0.96875, learning_rate 0.000152728
2017-10-10T11:49:40.588964: step 1110, loss 0.119634, acc 0.96875, learning_rate 0.000152513
2017-10-10T11:49:40.722147: step 1111, loss 0.150455, acc 0.9375, learning_rate 0.000152299
2017-10-10T11:49:40.917333: step 1112, loss 0.202193, acc 0.9375, learning_rate 0.000152085
2017-10-10T11:49:41.103505: step 1113, loss 0.111335, acc 0.96875, learning_rate 0.000151872
2017-10-10T11:49:41.230514: step 1114, loss 0.374801, acc 0.890625, learning_rate 0.000151661
2017-10-10T11:49:41.421049: step 1115, loss 0.196416, acc 0.96875, learning_rate 0.00015145
2017-10-10T11:49:41.629118: step 1116, loss 0.180796, acc 0.984375, learning_rate 0.00015124
2017-10-10T11:49:41.833072: step 1117, loss 0.243129, acc 0.890625, learning_rate 0.000151031
2017-10-10T11:49:41.986045: step 1118, loss 0.228882, acc 0.875, learning_rate 0.000150822
2017-10-10T11:49:42.216246: step 1119, loss 0.312952, acc 0.84375, learning_rate 0.000150615
2017-10-10T11:49:42.412979: step 1120, loss 0.339362, acc 0.890625, learning_rate 0.000150408

Evaluation:
2017-10-10T11:49:42.759246: step 1120, loss 0.242557, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1120

2017-10-10T11:49:44.098902: step 1121, loss 0.166274, acc 0.953125, learning_rate 0.000150203
2017-10-10T11:49:44.311045: step 1122, loss 0.113681, acc 0.96875, learning_rate 0.000149998
2017-10-10T11:49:44.469787: step 1123, loss 0.265811, acc 0.890625, learning_rate 0.000149794
2017-10-10T11:49:44.614834: step 1124, loss 0.184046, acc 0.9375, learning_rate 0.00014959
2017-10-10T11:49:44.803359: step 1125, loss 0.165379, acc 0.953125, learning_rate 0.000149388
2017-10-10T11:49:45.020837: step 1126, loss 0.109727, acc 0.96875, learning_rate 0.000149186
2017-10-10T11:49:45.214829: step 1127, loss 0.230782, acc 0.90625, learning_rate 0.000148986
2017-10-10T11:49:45.339426: step 1128, loss 0.205087, acc 0.9375, learning_rate 0.000148786
2017-10-10T11:49:45.415694: step 1129, loss 0.190258, acc 0.921875, learning_rate 0.000148587
2017-10-10T11:49:45.493705: step 1130, loss 0.184144, acc 0.953125, learning_rate 0.000148388
2017-10-10T11:49:45.570133: step 1131, loss 0.241377, acc 0.9375, learning_rate 0.000148191
2017-10-10T11:49:45.645655: step 1132, loss 0.214451, acc 0.90625, learning_rate 0.000147994
2017-10-10T11:49:45.837443: step 1133, loss 0.116343, acc 0.96875, learning_rate 0.000147798
2017-10-10T11:49:45.965867: step 1134, loss 0.153811, acc 0.96875, learning_rate 0.000147603
2017-10-10T11:49:46.168325: step 1135, loss 0.17031, acc 0.9375, learning_rate 0.000147409
2017-10-10T11:49:46.393225: step 1136, loss 0.13424, acc 0.984375, learning_rate 0.000147215
2017-10-10T11:49:46.540903: step 1137, loss 0.262973, acc 0.90625, learning_rate 0.000147022
2017-10-10T11:49:46.702772: step 1138, loss 0.136988, acc 0.96875, learning_rate 0.000146831
2017-10-10T11:49:46.898979: step 1139, loss 0.197622, acc 0.953125, learning_rate 0.000146639
2017-10-10T11:49:47.094497: step 1140, loss 0.250632, acc 0.9375, learning_rate 0.000146449
2017-10-10T11:49:47.230067: step 1141, loss 0.196602, acc 0.90625, learning_rate 0.000146259
2017-10-10T11:49:47.425833: step 1142, loss 0.175405, acc 0.953125, learning_rate 0.000146071
2017-10-10T11:49:47.617182: step 1143, loss 0.153865, acc 0.96875, learning_rate 0.000145883
2017-10-10T11:49:47.759446: step 1144, loss 0.0971681, acc 0.96875, learning_rate 0.000145695
2017-10-10T11:49:47.956984: step 1145, loss 0.321264, acc 0.890625, learning_rate 0.000145509
2017-10-10T11:49:48.136947: step 1146, loss 0.180783, acc 0.921875, learning_rate 0.000145323
2017-10-10T11:49:48.340905: step 1147, loss 0.180856, acc 0.9375, learning_rate 0.000145138
2017-10-10T11:49:48.549400: step 1148, loss 0.0905543, acc 0.984375, learning_rate 0.000144954
2017-10-10T11:49:48.712503: step 1149, loss 0.167495, acc 0.953125, learning_rate 0.00014477
2017-10-10T11:49:48.838190: step 1150, loss 0.201989, acc 0.953125, learning_rate 0.000144588
2017-10-10T11:49:48.953856: step 1151, loss 0.196231, acc 0.9375, learning_rate 0.000144406
2017-10-10T11:49:49.122959: step 1152, loss 0.226767, acc 0.9375, learning_rate 0.000144224
2017-10-10T11:49:49.305895: step 1153, loss 0.181396, acc 0.9375, learning_rate 0.000144044
2017-10-10T11:49:49.504503: step 1154, loss 0.13152, acc 0.96875, learning_rate 0.000143864
2017-10-10T11:49:49.678012: step 1155, loss 0.186329, acc 0.953125, learning_rate 0.000143685
2017-10-10T11:49:49.863254: step 1156, loss 0.192931, acc 0.9375, learning_rate 0.000143507
2017-10-10T11:49:50.064841: step 1157, loss 0.133169, acc 0.9375, learning_rate 0.000143329
2017-10-10T11:49:50.240181: step 1158, loss 0.177798, acc 0.9375, learning_rate 0.000143152
2017-10-10T11:49:50.413683: step 1159, loss 0.0905796, acc 1, learning_rate 0.000142976
2017-10-10T11:49:50.579586: step 1160, loss 0.125081, acc 0.96875, learning_rate 0.000142801

Evaluation:
2017-10-10T11:49:51.056898: step 1160, loss 0.241488, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1160

2017-10-10T11:49:52.004865: step 1161, loss 0.147671, acc 0.953125, learning_rate 0.000142626
2017-10-10T11:49:52.205011: step 1162, loss 0.096385, acc 0.984375, learning_rate 0.000142452
2017-10-10T11:49:52.433906: step 1163, loss 0.166615, acc 0.9375, learning_rate 0.000142279
2017-10-10T11:49:52.626098: step 1164, loss 0.193738, acc 0.9375, learning_rate 0.000142106
2017-10-10T11:49:52.816902: step 1165, loss 0.175437, acc 0.9375, learning_rate 0.000141934
2017-10-10T11:49:53.008170: step 1166, loss 0.162147, acc 0.953125, learning_rate 0.000141763
2017-10-10T11:49:53.135439: step 1167, loss 0.213091, acc 0.921875, learning_rate 0.000141593
2017-10-10T11:49:53.321496: step 1168, loss 0.220546, acc 0.921875, learning_rate 0.000141423
2017-10-10T11:49:53.534321: step 1169, loss 0.12524, acc 0.96875, learning_rate 0.000141254
2017-10-10T11:49:53.709060: step 1170, loss 0.188968, acc 0.921875, learning_rate 0.000141085
2017-10-10T11:49:53.865908: step 1171, loss 0.0854604, acc 0.984375, learning_rate 0.000140918
2017-10-10T11:49:54.068892: step 1172, loss 0.137635, acc 0.953125, learning_rate 0.000140751
2017-10-10T11:49:54.264557: step 1173, loss 0.159457, acc 0.96875, learning_rate 0.000140584
2017-10-10T11:49:54.405042: step 1174, loss 0.236408, acc 0.921875, learning_rate 0.000140419
2017-10-10T11:49:54.576865: step 1175, loss 0.161773, acc 0.953125, learning_rate 0.000140254
2017-10-10T11:49:54.754246: step 1176, loss 0.145301, acc 0.921569, learning_rate 0.000140089
2017-10-10T11:49:54.940385: step 1177, loss 0.112477, acc 0.9375, learning_rate 0.000139926
2017-10-10T11:49:55.076113: step 1178, loss 0.15607, acc 0.9375, learning_rate 0.000139763
2017-10-10T11:49:55.267662: step 1179, loss 0.110604, acc 0.96875, learning_rate 0.0001396
2017-10-10T11:49:55.462808: step 1180, loss 0.180716, acc 0.921875, learning_rate 0.000139439
2017-10-10T11:49:55.619708: step 1181, loss 0.151991, acc 0.9375, learning_rate 0.000139278
2017-10-10T11:49:55.775105: step 1182, loss 0.246297, acc 0.890625, learning_rate 0.000139118
2017-10-10T11:49:55.988554: step 1183, loss 0.203977, acc 0.921875, learning_rate 0.000138958
2017-10-10T11:49:56.173769: step 1184, loss 0.121977, acc 0.96875, learning_rate 0.000138799
2017-10-10T11:49:56.340454: step 1185, loss 0.280429, acc 0.90625, learning_rate 0.00013864
2017-10-10T11:49:56.491687: step 1186, loss 0.140812, acc 0.9375, learning_rate 0.000138483
2017-10-10T11:49:56.625152: step 1187, loss 0.243583, acc 0.90625, learning_rate 0.000138326
2017-10-10T11:49:56.823648: step 1188, loss 0.392366, acc 0.90625, learning_rate 0.000138169
2017-10-10T11:49:57.020883: step 1189, loss 0.22773, acc 0.890625, learning_rate 0.000138013
2017-10-10T11:49:57.152879: step 1190, loss 0.135625, acc 0.96875, learning_rate 0.000137858
2017-10-10T11:49:57.361940: step 1191, loss 0.169916, acc 0.96875, learning_rate 0.000137704
2017-10-10T11:49:57.560825: step 1192, loss 0.261101, acc 0.90625, learning_rate 0.00013755
2017-10-10T11:49:57.705608: step 1193, loss 0.127637, acc 0.984375, learning_rate 0.000137397
2017-10-10T11:49:57.880304: step 1194, loss 0.237693, acc 0.890625, learning_rate 0.000137244
2017-10-10T11:49:58.079919: step 1195, loss 0.179585, acc 0.9375, learning_rate 0.000137092
2017-10-10T11:49:58.296889: step 1196, loss 0.163205, acc 0.953125, learning_rate 0.000136941
2017-10-10T11:49:58.505236: step 1197, loss 0.122053, acc 0.96875, learning_rate 0.00013679
2017-10-10T11:49:58.673582: step 1198, loss 0.242205, acc 0.921875, learning_rate 0.00013664
2017-10-10T11:49:58.800408: step 1199, loss 0.209694, acc 0.921875, learning_rate 0.00013649
2017-10-10T11:49:58.957205: step 1200, loss 0.169519, acc 0.984375, learning_rate 0.000136341

Evaluation:
2017-10-10T11:49:59.244832: step 1200, loss 0.241105, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1200

2017-10-10T11:50:00.302353: step 1201, loss 0.140242, acc 0.9375, learning_rate 0.000136193
2017-10-10T11:50:00.511414: step 1202, loss 0.136612, acc 0.96875, learning_rate 0.000136045
2017-10-10T11:50:00.689244: step 1203, loss 0.179919, acc 0.921875, learning_rate 0.000135898
2017-10-10T11:50:00.821036: step 1204, loss 0.242801, acc 0.921875, learning_rate 0.000135751
2017-10-10T11:50:01.028313: step 1205, loss 0.420323, acc 0.828125, learning_rate 0.000135605
2017-10-10T11:50:01.216909: step 1206, loss 0.149238, acc 0.96875, learning_rate 0.00013546
2017-10-10T11:50:01.380568: step 1207, loss 0.161735, acc 0.96875, learning_rate 0.000135315
2017-10-10T11:50:01.591595: step 1208, loss 0.276311, acc 0.875, learning_rate 0.000135171
2017-10-10T11:50:01.828873: step 1209, loss 0.178365, acc 0.9375, learning_rate 0.000135028
2017-10-10T11:50:02.031817: step 1210, loss 0.137464, acc 0.96875, learning_rate 0.000134885
2017-10-10T11:50:02.215766: step 1211, loss 0.205199, acc 0.921875, learning_rate 0.000134742
2017-10-10T11:50:02.373171: step 1212, loss 0.184973, acc 0.9375, learning_rate 0.0001346
2017-10-10T11:50:02.536183: step 1213, loss 0.162948, acc 0.9375, learning_rate 0.000134459
2017-10-10T11:50:02.664376: step 1214, loss 0.269281, acc 0.90625, learning_rate 0.000134319
2017-10-10T11:50:02.779997: step 1215, loss 0.295508, acc 0.890625, learning_rate 0.000134178
2017-10-10T11:50:02.927817: step 1216, loss 0.0878448, acc 0.984375, learning_rate 0.000134039
2017-10-10T11:50:03.072805: step 1217, loss 0.348313, acc 0.84375, learning_rate 0.0001339
2017-10-10T11:50:03.204952: step 1218, loss 0.240123, acc 0.90625, learning_rate 0.000133762
2017-10-10T11:50:03.405332: step 1219, loss 0.153562, acc 0.953125, learning_rate 0.000133624
2017-10-10T11:50:03.568879: step 1220, loss 0.129506, acc 0.96875, learning_rate 0.000133487
2017-10-10T11:50:03.744048: step 1221, loss 0.140158, acc 0.984375, learning_rate 0.00013335
2017-10-10T11:50:03.948470: step 1222, loss 0.0993227, acc 0.984375, learning_rate 0.000133214
2017-10-10T11:50:04.156835: step 1223, loss 0.170503, acc 0.9375, learning_rate 0.000133078
2017-10-10T11:50:04.291184: step 1224, loss 0.236736, acc 0.953125, learning_rate 0.000132943
2017-10-10T11:50:04.460178: step 1225, loss 0.145372, acc 0.953125, learning_rate 0.000132809
2017-10-10T11:50:04.655983: step 1226, loss 0.147375, acc 0.90625, learning_rate 0.000132675
2017-10-10T11:50:04.817346: step 1227, loss 0.202981, acc 0.9375, learning_rate 0.000132541
2017-10-10T11:50:04.978824: step 1228, loss 0.0866086, acc 0.984375, learning_rate 0.000132409
2017-10-10T11:50:05.143320: step 1229, loss 0.140192, acc 0.953125, learning_rate 0.000132276
2017-10-10T11:50:05.321133: step 1230, loss 0.251794, acc 0.890625, learning_rate 0.000132145
2017-10-10T11:50:05.540984: step 1231, loss 0.135513, acc 0.953125, learning_rate 0.000132013
2017-10-10T11:50:05.698737: step 1232, loss 0.111899, acc 0.984375, learning_rate 0.000131883
2017-10-10T11:50:05.890055: step 1233, loss 0.239741, acc 0.9375, learning_rate 0.000131753
2017-10-10T11:50:06.028532: step 1234, loss 0.0808791, acc 0.96875, learning_rate 0.000131623
2017-10-10T11:50:06.230488: step 1235, loss 0.156017, acc 0.9375, learning_rate 0.000131494
2017-10-10T11:50:06.416953: step 1236, loss 0.205377, acc 0.921875, learning_rate 0.000131365
2017-10-10T11:50:06.597047: step 1237, loss 0.127614, acc 0.96875, learning_rate 0.000131237
2017-10-10T11:50:06.761023: step 1238, loss 0.158209, acc 0.9375, learning_rate 0.00013111
2017-10-10T11:50:06.952383: step 1239, loss 0.1191, acc 0.953125, learning_rate 0.000130983
2017-10-10T11:50:07.176741: step 1240, loss 0.0748193, acc 0.984375, learning_rate 0.000130856

Evaluation:
2017-10-10T11:50:07.516830: step 1240, loss 0.240486, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1240

2017-10-10T11:50:08.840970: step 1241, loss 0.193907, acc 0.90625, learning_rate 0.00013073
2017-10-10T11:50:08.991952: step 1242, loss 0.256246, acc 0.921875, learning_rate 0.000130605
2017-10-10T11:50:09.122622: step 1243, loss 0.17629, acc 0.90625, learning_rate 0.00013048
2017-10-10T11:50:09.237201: step 1244, loss 0.126939, acc 0.96875, learning_rate 0.000130356
2017-10-10T11:50:09.407829: step 1245, loss 0.287122, acc 0.890625, learning_rate 0.000130232
2017-10-10T11:50:09.517106: step 1246, loss 0.246767, acc 0.9375, learning_rate 0.000130108
2017-10-10T11:50:09.696972: step 1247, loss 0.115409, acc 0.953125, learning_rate 0.000129985
2017-10-10T11:50:09.857090: step 1248, loss 0.295815, acc 0.90625, learning_rate 0.000129863
2017-10-10T11:50:10.052598: step 1249, loss 0.121998, acc 0.96875, learning_rate 0.000129741
2017-10-10T11:50:10.244828: step 1250, loss 0.103672, acc 1, learning_rate 0.00012962
2017-10-10T11:50:10.391513: step 1251, loss 0.153872, acc 0.96875, learning_rate 0.000129499
2017-10-10T11:50:10.568800: step 1252, loss 0.257215, acc 0.90625, learning_rate 0.000129378
2017-10-10T11:50:10.788771: step 1253, loss 0.115651, acc 0.96875, learning_rate 0.000129259
2017-10-10T11:50:10.945641: step 1254, loss 0.121397, acc 0.96875, learning_rate 0.000129139
2017-10-10T11:50:11.108909: step 1255, loss 0.0918982, acc 0.984375, learning_rate 0.00012902
2017-10-10T11:50:11.314575: step 1256, loss 0.188035, acc 0.9375, learning_rate 0.000128902
2017-10-10T11:50:11.510040: step 1257, loss 0.151176, acc 0.96875, learning_rate 0.000128784
2017-10-10T11:50:11.624606: step 1258, loss 0.136042, acc 0.96875, learning_rate 0.000128666
2017-10-10T11:50:11.820576: step 1259, loss 0.165333, acc 0.921875, learning_rate 0.000128549
2017-10-10T11:50:12.021221: step 1260, loss 0.287385, acc 0.890625, learning_rate 0.000128433
2017-10-10T11:50:12.204635: step 1261, loss 0.150552, acc 0.96875, learning_rate 0.000128317
2017-10-10T11:50:12.355820: step 1262, loss 0.188267, acc 0.9375, learning_rate 0.000128201
2017-10-10T11:50:12.532869: step 1263, loss 0.199038, acc 0.90625, learning_rate 0.000128086
2017-10-10T11:50:12.740346: step 1264, loss 0.212112, acc 0.9375, learning_rate 0.000127971
2017-10-10T11:50:12.913063: step 1265, loss 0.14052, acc 0.9375, learning_rate 0.000127857
2017-10-10T11:50:13.176898: step 1266, loss 0.404942, acc 0.875, learning_rate 0.000127743
2017-10-10T11:50:13.396805: step 1267, loss 0.136515, acc 0.9375, learning_rate 0.00012763
2017-10-10T11:50:13.536868: step 1268, loss 0.155905, acc 0.953125, learning_rate 0.000127517
2017-10-10T11:50:13.693110: step 1269, loss 0.202285, acc 0.921875, learning_rate 0.000127405
2017-10-10T11:50:13.820995: step 1270, loss 0.149121, acc 0.984375, learning_rate 0.000127293
2017-10-10T11:50:13.967183: step 1271, loss 0.226981, acc 0.9375, learning_rate 0.000127182
2017-10-10T11:50:14.119144: step 1272, loss 0.11985, acc 0.953125, learning_rate 0.000127071
2017-10-10T11:50:14.325266: step 1273, loss 0.174553, acc 0.921875, learning_rate 0.00012696
2017-10-10T11:50:14.437039: step 1274, loss 0.31101, acc 0.882353, learning_rate 0.00012685
2017-10-10T11:50:14.628345: step 1275, loss 0.0853278, acc 0.96875, learning_rate 0.000126741
2017-10-10T11:50:14.810853: step 1276, loss 0.142901, acc 0.953125, learning_rate 0.000126632
2017-10-10T11:50:14.960922: step 1277, loss 0.150681, acc 0.953125, learning_rate 0.000126523
2017-10-10T11:50:15.149856: step 1278, loss 0.121025, acc 0.96875, learning_rate 0.000126415
2017-10-10T11:50:15.348176: step 1279, loss 0.25283, acc 0.9375, learning_rate 0.000126307
2017-10-10T11:50:15.552549: step 1280, loss 0.065472, acc 1, learning_rate 0.000126199

Evaluation:
2017-10-10T11:50:15.976837: step 1280, loss 0.240815, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1280

2017-10-10T11:50:16.908852: step 1281, loss 0.19223, acc 0.890625, learning_rate 0.000126093
2017-10-10T11:50:17.106361: step 1282, loss 0.288645, acc 0.90625, learning_rate 0.000125986
2017-10-10T11:50:17.304855: step 1283, loss 0.246642, acc 0.921875, learning_rate 0.00012588
2017-10-10T11:50:17.508855: step 1284, loss 0.143483, acc 0.96875, learning_rate 0.000125774
2017-10-10T11:50:17.696757: step 1285, loss 0.163238, acc 0.96875, learning_rate 0.000125669
2017-10-10T11:50:17.864966: step 1286, loss 0.207619, acc 0.90625, learning_rate 0.000125564
2017-10-10T11:50:18.048809: step 1287, loss 0.110781, acc 0.96875, learning_rate 0.00012546
2017-10-10T11:50:18.209215: step 1288, loss 0.272431, acc 0.921875, learning_rate 0.000125356
2017-10-10T11:50:18.403193: step 1289, loss 0.192912, acc 0.921875, learning_rate 0.000125253
2017-10-10T11:50:18.625761: step 1290, loss 0.255449, acc 0.90625, learning_rate 0.00012515
2017-10-10T11:50:18.836312: step 1291, loss 0.285911, acc 0.90625, learning_rate 0.000125047
2017-10-10T11:50:18.945468: step 1292, loss 0.293214, acc 0.859375, learning_rate 0.000124945
2017-10-10T11:50:19.180594: step 1293, loss 0.156544, acc 0.953125, learning_rate 0.000124843
2017-10-10T11:50:19.396377: step 1294, loss 0.142401, acc 0.90625, learning_rate 0.000124741
2017-10-10T11:50:19.528304: step 1295, loss 0.212427, acc 0.921875, learning_rate 0.00012464
2017-10-10T11:50:19.686529: step 1296, loss 0.243654, acc 0.9375, learning_rate 0.00012454
2017-10-10T11:50:19.833441: step 1297, loss 0.199602, acc 0.9375, learning_rate 0.00012444
2017-10-10T11:50:19.964816: step 1298, loss 0.301597, acc 0.921875, learning_rate 0.00012434
2017-10-10T11:50:20.092346: step 1299, loss 0.0854834, acc 0.984375, learning_rate 0.000124241
2017-10-10T11:50:20.242730: step 1300, loss 0.0479406, acc 1, learning_rate 0.000124142
2017-10-10T11:50:20.436833: step 1301, loss 0.112997, acc 0.953125, learning_rate 0.000124043
2017-10-10T11:50:20.600633: step 1302, loss 0.200708, acc 0.9375, learning_rate 0.000123945
2017-10-10T11:50:20.764871: step 1303, loss 0.147639, acc 0.953125, learning_rate 0.000123847
2017-10-10T11:50:20.964955: step 1304, loss 0.233484, acc 0.890625, learning_rate 0.00012375
2017-10-10T11:50:21.140815: step 1305, loss 0.258897, acc 0.9375, learning_rate 0.000123653
2017-10-10T11:50:21.320844: step 1306, loss 0.109362, acc 0.96875, learning_rate 0.000123556
2017-10-10T11:50:21.514130: step 1307, loss 0.150796, acc 0.96875, learning_rate 0.00012346
2017-10-10T11:50:21.728435: step 1308, loss 0.0980262, acc 0.96875, learning_rate 0.000123364
2017-10-10T11:50:21.903102: step 1309, loss 0.0964827, acc 0.96875, learning_rate 0.000123269
2017-10-10T11:50:22.047716: step 1310, loss 0.305075, acc 0.890625, learning_rate 0.000123174
2017-10-10T11:50:22.232833: step 1311, loss 0.173099, acc 0.921875, learning_rate 0.00012308
2017-10-10T11:50:22.446544: step 1312, loss 0.193231, acc 0.921875, learning_rate 0.000122985
2017-10-10T11:50:22.646553: step 1313, loss 0.148255, acc 0.9375, learning_rate 0.000122892
2017-10-10T11:50:22.782254: step 1314, loss 0.256277, acc 0.90625, learning_rate 0.000122798
2017-10-10T11:50:22.967317: step 1315, loss 0.241047, acc 0.921875, learning_rate 0.000122705
2017-10-10T11:50:23.180611: step 1316, loss 0.19341, acc 0.921875, learning_rate 0.000122612
2017-10-10T11:50:23.391089: step 1317, loss 0.140368, acc 0.953125, learning_rate 0.00012252
2017-10-10T11:50:23.588876: step 1318, loss 0.173909, acc 0.953125, learning_rate 0.000122428
2017-10-10T11:50:23.732567: step 1319, loss 0.308466, acc 0.921875, learning_rate 0.000122337
2017-10-10T11:50:23.980902: step 1320, loss 0.154764, acc 0.953125, learning_rate 0.000122245

Evaluation:
2017-10-10T11:50:24.418049: step 1320, loss 0.238994, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1320

2017-10-10T11:50:25.335776: step 1321, loss 0.229886, acc 0.921875, learning_rate 0.000122155
2017-10-10T11:50:25.517101: step 1322, loss 0.209626, acc 0.921875, learning_rate 0.000122064
2017-10-10T11:50:25.720871: step 1323, loss 0.199548, acc 0.9375, learning_rate 0.000121974
2017-10-10T11:50:25.902620: step 1324, loss 0.201933, acc 0.953125, learning_rate 0.000121884
2017-10-10T11:50:26.069008: step 1325, loss 0.156698, acc 0.96875, learning_rate 0.000121795
2017-10-10T11:50:26.264899: step 1326, loss 0.208128, acc 0.953125, learning_rate 0.000121706
2017-10-10T11:50:26.448557: step 1327, loss 0.186076, acc 0.953125, learning_rate 0.000121618
2017-10-10T11:50:26.630316: step 1328, loss 0.168526, acc 0.9375, learning_rate 0.000121529
2017-10-10T11:50:26.785500: step 1329, loss 0.175778, acc 0.9375, learning_rate 0.000121441
2017-10-10T11:50:26.984111: step 1330, loss 0.162657, acc 0.921875, learning_rate 0.000121354
2017-10-10T11:50:27.147469: step 1331, loss 0.178793, acc 0.953125, learning_rate 0.000121267
2017-10-10T11:50:27.308883: step 1332, loss 0.16282, acc 0.9375, learning_rate 0.00012118
2017-10-10T11:50:27.486460: step 1333, loss 0.189829, acc 0.953125, learning_rate 0.000121093
2017-10-10T11:50:27.646426: step 1334, loss 0.231405, acc 0.90625, learning_rate 0.000121007
2017-10-10T11:50:27.815520: step 1335, loss 0.277026, acc 0.921875, learning_rate 0.000120922
2017-10-10T11:50:28.030896: step 1336, loss 0.117035, acc 0.96875, learning_rate 0.000120836
2017-10-10T11:50:28.177103: step 1337, loss 0.226466, acc 0.90625, learning_rate 0.000120751
2017-10-10T11:50:28.331189: step 1338, loss 0.123708, acc 0.96875, learning_rate 0.000120666
2017-10-10T11:50:28.553207: step 1339, loss 0.104993, acc 0.96875, learning_rate 0.000120582
2017-10-10T11:50:28.752835: step 1340, loss 0.280281, acc 0.859375, learning_rate 0.000120498
2017-10-10T11:50:28.868834: step 1341, loss 0.223443, acc 0.953125, learning_rate 0.000120414
2017-10-10T11:50:29.079827: step 1342, loss 0.189669, acc 0.9375, learning_rate 0.000120331
2017-10-10T11:50:29.280045: step 1343, loss 0.152903, acc 0.96875, learning_rate 0.000120248
2017-10-10T11:50:29.470945: step 1344, loss 0.123177, acc 0.953125, learning_rate 0.000120165
2017-10-10T11:50:29.654193: step 1345, loss 0.14909, acc 0.9375, learning_rate 0.000120083
2017-10-10T11:50:29.813494: step 1346, loss 0.195147, acc 0.9375, learning_rate 0.000120001
2017-10-10T11:50:29.972095: step 1347, loss 0.134452, acc 0.96875, learning_rate 0.00011992
2017-10-10T11:50:30.080838: step 1348, loss 0.177951, acc 0.9375, learning_rate 0.000119838
2017-10-10T11:50:30.237867: step 1349, loss 0.25469, acc 0.921875, learning_rate 0.000119757
2017-10-10T11:50:30.397726: step 1350, loss 0.22091, acc 0.890625, learning_rate 0.000119677
2017-10-10T11:50:30.549534: step 1351, loss 0.154927, acc 0.96875, learning_rate 0.000119596
2017-10-10T11:50:30.720922: step 1352, loss 0.115244, acc 0.984375, learning_rate 0.000119516
2017-10-10T11:50:30.929559: step 1353, loss 0.114069, acc 0.953125, learning_rate 0.000119437
2017-10-10T11:50:31.091212: step 1354, loss 0.129504, acc 0.953125, learning_rate 0.000119357
2017-10-10T11:50:31.236728: step 1355, loss 0.287856, acc 0.875, learning_rate 0.000119278
2017-10-10T11:50:31.428883: step 1356, loss 0.138024, acc 0.984375, learning_rate 0.0001192
2017-10-10T11:50:31.645944: step 1357, loss 0.111106, acc 0.96875, learning_rate 0.000119121
2017-10-10T11:50:31.858145: step 1358, loss 0.111214, acc 0.96875, learning_rate 0.000119043
2017-10-10T11:50:32.060148: step 1359, loss 0.237311, acc 0.921875, learning_rate 0.000118965
2017-10-10T11:50:32.205826: step 1360, loss 0.220459, acc 0.90625, learning_rate 0.000118888

Evaluation:
2017-10-10T11:50:32.624121: step 1360, loss 0.23953, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1360

2017-10-10T11:50:34.180375: step 1361, loss 0.326527, acc 0.859375, learning_rate 0.000118811
2017-10-10T11:50:34.368710: step 1362, loss 0.197462, acc 0.921875, learning_rate 0.000118734
2017-10-10T11:50:34.503970: step 1363, loss 0.196808, acc 0.921875, learning_rate 0.000118658
2017-10-10T11:50:34.688858: step 1364, loss 0.150312, acc 0.9375, learning_rate 0.000118582
2017-10-10T11:50:34.937114: step 1365, loss 0.165055, acc 0.953125, learning_rate 0.000118506
2017-10-10T11:50:35.197286: step 1366, loss 0.260855, acc 0.90625, learning_rate 0.00011843
2017-10-10T11:50:35.357879: step 1367, loss 0.223191, acc 0.9375, learning_rate 0.000118355
2017-10-10T11:50:35.479676: step 1368, loss 0.146392, acc 0.953125, learning_rate 0.00011828
2017-10-10T11:50:35.604878: step 1369, loss 0.326325, acc 0.921875, learning_rate 0.000118205
2017-10-10T11:50:35.741374: step 1370, loss 0.22077, acc 0.96875, learning_rate 0.000118131
2017-10-10T11:50:35.878037: step 1371, loss 0.141019, acc 0.953125, learning_rate 0.000118057
2017-10-10T11:50:35.985039: step 1372, loss 0.165232, acc 0.960784, learning_rate 0.000117983
2017-10-10T11:50:36.111106: step 1373, loss 0.189162, acc 0.9375, learning_rate 0.00011791
2017-10-10T11:50:36.266173: step 1374, loss 0.124606, acc 0.96875, learning_rate 0.000117837
2017-10-10T11:50:36.460901: step 1375, loss 0.129302, acc 0.984375, learning_rate 0.000117764
2017-10-10T11:50:36.605064: step 1376, loss 0.201471, acc 0.890625, learning_rate 0.000117692
2017-10-10T11:50:36.796848: step 1377, loss 0.123688, acc 0.953125, learning_rate 0.000117619
2017-10-10T11:50:36.998958: step 1378, loss 0.138868, acc 0.984375, learning_rate 0.000117547
2017-10-10T11:50:37.177277: step 1379, loss 0.12037, acc 0.984375, learning_rate 0.000117476
2017-10-10T11:50:37.360867: step 1380, loss 0.116749, acc 0.96875, learning_rate 0.000117404
2017-10-10T11:50:37.521038: step 1381, loss 0.134367, acc 0.921875, learning_rate 0.000117333
2017-10-10T11:50:37.716439: step 1382, loss 0.162186, acc 0.953125, learning_rate 0.000117263
2017-10-10T11:50:37.943549: step 1383, loss 0.165114, acc 0.921875, learning_rate 0.000117192
2017-10-10T11:50:38.165885: step 1384, loss 0.1283, acc 0.96875, learning_rate 0.000117122
2017-10-10T11:50:38.372738: step 1385, loss 0.144535, acc 0.953125, learning_rate 0.000117052
2017-10-10T11:50:38.556938: step 1386, loss 0.177877, acc 0.921875, learning_rate 0.000116983
2017-10-10T11:50:38.741003: step 1387, loss 0.21715, acc 0.921875, learning_rate 0.000116913
2017-10-10T11:50:38.941802: step 1388, loss 0.166843, acc 0.953125, learning_rate 0.000116844
2017-10-10T11:50:39.138046: step 1389, loss 0.126376, acc 0.96875, learning_rate 0.000116775
2017-10-10T11:50:39.330815: step 1390, loss 0.287973, acc 0.890625, learning_rate 0.000116707
2017-10-10T11:50:39.556804: step 1391, loss 0.117419, acc 0.984375, learning_rate 0.000116639
2017-10-10T11:50:39.751107: step 1392, loss 0.128519, acc 0.953125, learning_rate 0.000116571
2017-10-10T11:50:39.884824: step 1393, loss 0.148937, acc 0.984375, learning_rate 0.000116503
2017-10-10T11:50:40.004944: step 1394, loss 0.257917, acc 0.90625, learning_rate 0.000116436
2017-10-10T11:50:40.146152: step 1395, loss 0.248243, acc 0.921875, learning_rate 0.000116369
2017-10-10T11:50:40.306262: step 1396, loss 0.113984, acc 0.96875, learning_rate 0.000116302
2017-10-10T11:50:40.460816: step 1397, loss 0.134087, acc 0.953125, learning_rate 0.000116235
2017-10-10T11:50:40.627488: step 1398, loss 0.179564, acc 0.90625, learning_rate 0.000116169
2017-10-10T11:50:40.764846: step 1399, loss 0.154524, acc 0.953125, learning_rate 0.000116103
2017-10-10T11:50:40.946632: step 1400, loss 0.116017, acc 0.96875, learning_rate 0.000116037

Evaluation:
2017-10-10T11:50:41.326634: step 1400, loss 0.238782, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1400

2017-10-10T11:50:42.603959: step 1401, loss 0.265532, acc 0.9375, learning_rate 0.000115972
2017-10-10T11:50:42.776885: step 1402, loss 0.157005, acc 0.90625, learning_rate 0.000115907
2017-10-10T11:50:42.963235: step 1403, loss 0.209106, acc 0.921875, learning_rate 0.000115842
2017-10-10T11:50:43.132664: step 1404, loss 0.169784, acc 0.90625, learning_rate 0.000115777
2017-10-10T11:50:43.332994: step 1405, loss 0.132232, acc 0.953125, learning_rate 0.000115713
2017-10-10T11:50:43.488994: step 1406, loss 0.107231, acc 0.96875, learning_rate 0.000115649
2017-10-10T11:50:43.664956: step 1407, loss 0.123912, acc 1, learning_rate 0.000115585
2017-10-10T11:50:43.840107: step 1408, loss 0.221579, acc 0.90625, learning_rate 0.000115521
2017-10-10T11:50:43.996984: step 1409, loss 0.227734, acc 0.90625, learning_rate 0.000115458
2017-10-10T11:50:44.213259: step 1410, loss 0.160109, acc 0.953125, learning_rate 0.000115395
2017-10-10T11:50:44.367651: step 1411, loss 0.148017, acc 0.921875, learning_rate 0.000115332
2017-10-10T11:50:44.568155: step 1412, loss 0.103676, acc 0.984375, learning_rate 0.000115269
2017-10-10T11:50:44.768970: step 1413, loss 0.140603, acc 0.96875, learning_rate 0.000115207
2017-10-10T11:50:44.917283: step 1414, loss 0.23893, acc 0.90625, learning_rate 0.000115145
2017-10-10T11:50:45.108127: step 1415, loss 0.128795, acc 0.96875, learning_rate 0.000115083
2017-10-10T11:50:45.292980: step 1416, loss 0.171578, acc 0.9375, learning_rate 0.000115022
2017-10-10T11:50:45.480206: step 1417, loss 0.174162, acc 0.9375, learning_rate 0.00011496
2017-10-10T11:50:45.673860: step 1418, loss 0.112042, acc 0.96875, learning_rate 0.000114899
2017-10-10T11:50:45.884933: step 1419, loss 0.252563, acc 0.890625, learning_rate 0.000114838
2017-10-10T11:50:46.093720: step 1420, loss 0.165559, acc 0.9375, learning_rate 0.000114778
2017-10-10T11:50:46.249976: step 1421, loss 0.181675, acc 0.9375, learning_rate 0.000114717
2017-10-10T11:50:46.395150: step 1422, loss 0.117112, acc 0.96875, learning_rate 0.000114657
2017-10-10T11:50:46.532865: step 1423, loss 0.125955, acc 0.953125, learning_rate 0.000114598
2017-10-10T11:50:46.676818: step 1424, loss 0.0925881, acc 0.96875, learning_rate 0.000114538
2017-10-10T11:50:46.802318: step 1425, loss 0.184441, acc 0.9375, learning_rate 0.000114479
2017-10-10T11:50:46.931569: step 1426, loss 0.196204, acc 0.96875, learning_rate 0.00011442
2017-10-10T11:50:47.155693: step 1427, loss 0.132953, acc 0.953125, learning_rate 0.000114361
2017-10-10T11:50:47.336851: step 1428, loss 0.115947, acc 0.96875, learning_rate 0.000114302
2017-10-10T11:50:47.481185: step 1429, loss 0.221091, acc 0.96875, learning_rate 0.000114244
2017-10-10T11:50:47.668980: step 1430, loss 0.137547, acc 0.953125, learning_rate 0.000114186
2017-10-10T11:50:47.871013: step 1431, loss 0.251227, acc 0.921875, learning_rate 0.000114128
2017-10-10T11:50:48.025059: step 1432, loss 0.126334, acc 0.984375, learning_rate 0.00011407
2017-10-10T11:50:48.196932: step 1433, loss 0.163885, acc 0.9375, learning_rate 0.000114013
2017-10-10T11:50:48.417615: step 1434, loss 0.295314, acc 0.875, learning_rate 0.000113955
2017-10-10T11:50:48.597977: step 1435, loss 0.275279, acc 0.875, learning_rate 0.000113898
2017-10-10T11:50:48.744828: step 1436, loss 0.0983934, acc 0.984375, learning_rate 0.000113842
2017-10-10T11:50:48.948670: step 1437, loss 0.2326, acc 0.921875, learning_rate 0.000113785
2017-10-10T11:50:49.163170: step 1438, loss 0.135066, acc 0.96875, learning_rate 0.000113729
2017-10-10T11:50:49.299824: step 1439, loss 0.213535, acc 0.921875, learning_rate 0.000113673
2017-10-10T11:50:49.470505: step 1440, loss 0.22321, acc 0.90625, learning_rate 0.000113617

Evaluation:
2017-10-10T11:50:49.884851: step 1440, loss 0.239444, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1440

2017-10-10T11:50:50.843410: step 1441, loss 0.25444, acc 0.890625, learning_rate 0.000113561
2017-10-10T11:50:51.020149: step 1442, loss 0.139201, acc 0.96875, learning_rate 0.000113506
2017-10-10T11:50:51.174240: step 1443, loss 0.203414, acc 0.953125, learning_rate 0.000113451
2017-10-10T11:50:51.361887: step 1444, loss 0.259349, acc 0.890625, learning_rate 0.000113396
2017-10-10T11:50:51.565015: step 1445, loss 0.15139, acc 0.953125, learning_rate 0.000113341
2017-10-10T11:50:51.736867: step 1446, loss 0.193911, acc 0.890625, learning_rate 0.000113287
2017-10-10T11:50:51.900930: step 1447, loss 0.157547, acc 0.9375, learning_rate 0.000113233
2017-10-10T11:50:52.098248: step 1448, loss 0.232512, acc 0.9375, learning_rate 0.000113179
2017-10-10T11:50:52.300626: step 1449, loss 0.156503, acc 0.96875, learning_rate 0.000113125
2017-10-10T11:50:52.473158: step 1450, loss 0.195564, acc 0.921875, learning_rate 0.000113071
2017-10-10T11:50:52.641141: step 1451, loss 0.188167, acc 0.9375, learning_rate 0.000113018
2017-10-10T11:50:52.848902: step 1452, loss 0.176565, acc 0.953125, learning_rate 0.000112965
2017-10-10T11:50:53.014277: step 1453, loss 0.237889, acc 0.875, learning_rate 0.000112912
2017-10-10T11:50:53.195646: step 1454, loss 0.190296, acc 0.9375, learning_rate 0.000112859
2017-10-10T11:50:53.360814: step 1455, loss 0.270784, acc 0.9375, learning_rate 0.000112807
2017-10-10T11:50:53.564860: step 1456, loss 0.121587, acc 0.984375, learning_rate 0.000112754
2017-10-10T11:50:53.710315: step 1457, loss 0.191357, acc 0.921875, learning_rate 0.000112702
2017-10-10T11:50:53.872458: step 1458, loss 0.231751, acc 0.90625, learning_rate 0.000112651
2017-10-10T11:50:54.079347: step 1459, loss 0.258553, acc 0.921875, learning_rate 0.000112599
2017-10-10T11:50:54.257014: step 1460, loss 0.237591, acc 0.890625, learning_rate 0.000112547
2017-10-10T11:50:54.437093: step 1461, loss 0.237321, acc 0.90625, learning_rate 0.000112496
2017-10-10T11:50:54.596358: step 1462, loss 0.077473, acc 0.984375, learning_rate 0.000112445
2017-10-10T11:50:54.787477: step 1463, loss 0.215844, acc 0.953125, learning_rate 0.000112394
2017-10-10T11:50:54.931227: step 1464, loss 0.293174, acc 0.90625, learning_rate 0.000112344
2017-10-10T11:50:55.096850: step 1465, loss 0.203643, acc 0.9375, learning_rate 0.000112293
2017-10-10T11:50:55.324194: step 1466, loss 0.240388, acc 0.890625, learning_rate 0.000112243
2017-10-10T11:50:55.543550: step 1467, loss 0.224943, acc 0.921875, learning_rate 0.000112193
2017-10-10T11:50:55.680013: step 1468, loss 0.107233, acc 0.984375, learning_rate 0.000112144
2017-10-10T11:50:55.868879: step 1469, loss 0.110054, acc 0.96875, learning_rate 0.000112094
2017-10-10T11:50:56.055152: step 1470, loss 0.135928, acc 0.941176, learning_rate 0.000112045
2017-10-10T11:50:56.228960: step 1471, loss 0.249367, acc 0.9375, learning_rate 0.000111995
2017-10-10T11:50:56.428276: step 1472, loss 0.20423, acc 0.921875, learning_rate 0.000111946
2017-10-10T11:50:56.600881: step 1473, loss 0.142282, acc 0.9375, learning_rate 0.000111898
2017-10-10T11:50:56.839687: step 1474, loss 0.293014, acc 0.90625, learning_rate 0.000111849
2017-10-10T11:50:57.065092: step 1475, loss 0.136386, acc 0.96875, learning_rate 0.000111801
2017-10-10T11:50:57.221925: step 1476, loss 0.291155, acc 0.921875, learning_rate 0.000111753
2017-10-10T11:50:57.387686: step 1477, loss 0.22678, acc 0.9375, learning_rate 0.000111705
2017-10-10T11:50:57.500230: step 1478, loss 0.251337, acc 0.90625, learning_rate 0.000111657
2017-10-10T11:50:57.624308: step 1479, loss 0.132082, acc 0.96875, learning_rate 0.000111609
2017-10-10T11:50:57.768630: step 1480, loss 0.241594, acc 0.890625, learning_rate 0.000111562

Evaluation:
2017-10-10T11:50:58.061020: step 1480, loss 0.238144, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1480

2017-10-10T11:50:59.360839: step 1481, loss 0.131294, acc 0.9375, learning_rate 0.000111515
2017-10-10T11:50:59.581531: step 1482, loss 0.184654, acc 0.9375, learning_rate 0.000111468
2017-10-10T11:50:59.744414: step 1483, loss 0.197291, acc 0.9375, learning_rate 0.000111421
2017-10-10T11:50:59.888836: step 1484, loss 0.118488, acc 0.96875, learning_rate 0.000111374
2017-10-10T11:51:00.132881: step 1485, loss 0.271958, acc 0.9375, learning_rate 0.000111328
2017-10-10T11:51:00.391494: step 1486, loss 0.21257, acc 0.9375, learning_rate 0.000111282
2017-10-10T11:51:00.504889: step 1487, loss 0.180447, acc 0.953125, learning_rate 0.000111236
2017-10-10T11:51:00.635770: step 1488, loss 0.163184, acc 0.9375, learning_rate 0.00011119
2017-10-10T11:51:00.783976: step 1489, loss 0.182008, acc 0.9375, learning_rate 0.000111144
2017-10-10T11:51:00.887229: step 1490, loss 0.228571, acc 0.9375, learning_rate 0.000111099
2017-10-10T11:51:01.040514: step 1491, loss 0.149395, acc 0.953125, learning_rate 0.000111053
2017-10-10T11:51:01.208832: step 1492, loss 0.146057, acc 0.96875, learning_rate 0.000111008
2017-10-10T11:51:01.344688: step 1493, loss 0.178862, acc 0.9375, learning_rate 0.000110963
2017-10-10T11:51:01.511846: step 1494, loss 0.34667, acc 0.90625, learning_rate 0.000110918
2017-10-10T11:51:01.694053: step 1495, loss 0.0981495, acc 0.984375, learning_rate 0.000110874
2017-10-10T11:51:01.828848: step 1496, loss 0.139543, acc 0.953125, learning_rate 0.00011083
2017-10-10T11:51:02.016802: step 1497, loss 0.237979, acc 0.953125, learning_rate 0.000110785
2017-10-10T11:51:02.214562: step 1498, loss 0.150568, acc 0.96875, learning_rate 0.000110741
2017-10-10T11:51:02.420678: step 1499, loss 0.105181, acc 0.96875, learning_rate 0.000110697
2017-10-10T11:51:02.553388: step 1500, loss 0.166133, acc 0.9375, learning_rate 0.000110654
2017-10-10T11:51:02.745894: step 1501, loss 0.192196, acc 0.953125, learning_rate 0.00011061
2017-10-10T11:51:02.956873: step 1502, loss 0.102593, acc 0.984375, learning_rate 0.000110567
2017-10-10T11:51:03.183051: step 1503, loss 0.166801, acc 0.9375, learning_rate 0.000110524
2017-10-10T11:51:03.380913: step 1504, loss 0.170156, acc 0.890625, learning_rate 0.000110481
2017-10-10T11:51:03.540898: step 1505, loss 0.119395, acc 0.96875, learning_rate 0.000110438
2017-10-10T11:51:03.745813: step 1506, loss 0.206102, acc 0.90625, learning_rate 0.000110396
2017-10-10T11:51:03.896433: step 1507, loss 0.121424, acc 0.96875, learning_rate 0.000110353
2017-10-10T11:51:04.056869: step 1508, loss 0.0624103, acc 1, learning_rate 0.000110311
2017-10-10T11:51:04.274016: step 1509, loss 0.101835, acc 0.984375, learning_rate 0.000110269
2017-10-10T11:51:04.465197: step 1510, loss 0.087528, acc 0.984375, learning_rate 0.000110227
2017-10-10T11:51:04.608939: step 1511, loss 0.121779, acc 0.953125, learning_rate 0.000110185
2017-10-10T11:51:04.782855: step 1512, loss 0.151178, acc 0.9375, learning_rate 0.000110144
2017-10-10T11:51:04.964163: step 1513, loss 0.199071, acc 0.90625, learning_rate 0.000110102
2017-10-10T11:51:05.169160: step 1514, loss 0.176809, acc 0.921875, learning_rate 0.000110061
2017-10-10T11:51:05.360932: step 1515, loss 0.276161, acc 0.921875, learning_rate 0.00011002
2017-10-10T11:51:05.568013: step 1516, loss 0.210236, acc 0.890625, learning_rate 0.000109979
2017-10-10T11:51:05.772833: step 1517, loss 0.298266, acc 0.90625, learning_rate 0.000109938
2017-10-10T11:51:06.528257: step 1518, loss 0.274659, acc 0.921875, learning_rate 0.000109898
2017-10-10T11:51:06.709007: step 1519, loss 0.200796, acc 0.921875, learning_rate 0.000109857
2017-10-10T11:51:06.849184: step 1520, loss 0.198983, acc 0.890625, learning_rate 0.000109817

Evaluation:
2017-10-10T11:51:07.260886: step 1520, loss 0.237942, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1520

2017-10-10T11:51:08.473970: step 1521, loss 0.224792, acc 0.90625, learning_rate 0.000109777
2017-10-10T11:51:08.628527: step 1522, loss 0.1199, acc 0.96875, learning_rate 0.000109737
2017-10-10T11:51:08.756841: step 1523, loss 0.0844738, acc 1, learning_rate 0.000109697
2017-10-10T11:51:08.959693: step 1524, loss 0.201029, acc 0.9375, learning_rate 0.000109658
2017-10-10T11:51:09.157172: step 1525, loss 0.186741, acc 0.953125, learning_rate 0.000109618
2017-10-10T11:51:09.332349: step 1526, loss 0.0856061, acc 0.984375, learning_rate 0.000109579
2017-10-10T11:51:09.538635: step 1527, loss 0.174336, acc 0.953125, learning_rate 0.00010954
2017-10-10T11:51:09.720841: step 1528, loss 0.130126, acc 0.953125, learning_rate 0.000109501
2017-10-10T11:51:09.936881: step 1529, loss 0.15756, acc 0.953125, learning_rate 0.000109462
2017-10-10T11:51:10.081438: step 1530, loss 0.193142, acc 0.921875, learning_rate 0.000109424
2017-10-10T11:51:10.248681: step 1531, loss 0.0909332, acc 0.984375, learning_rate 0.000109385
2017-10-10T11:51:10.468578: step 1532, loss 0.171869, acc 0.953125, learning_rate 0.000109347
2017-10-10T11:51:10.672859: step 1533, loss 0.157291, acc 0.96875, learning_rate 0.000109309
2017-10-10T11:51:10.903558: step 1534, loss 0.233375, acc 0.921875, learning_rate 0.000109271
2017-10-10T11:51:11.074015: step 1535, loss 0.148776, acc 0.953125, learning_rate 0.000109233
2017-10-10T11:51:11.223501: step 1536, loss 0.310566, acc 0.9375, learning_rate 0.000109195
2017-10-10T11:51:11.353036: step 1537, loss 0.201151, acc 0.90625, learning_rate 0.000109158
2017-10-10T11:51:11.489770: step 1538, loss 0.239089, acc 0.90625, learning_rate 0.00010912
2017-10-10T11:51:11.653556: step 1539, loss 0.225816, acc 0.921875, learning_rate 0.000109083
2017-10-10T11:51:11.806624: step 1540, loss 0.109945, acc 1, learning_rate 0.000109046
2017-10-10T11:51:11.906780: step 1541, loss 0.16977, acc 0.984375, learning_rate 0.000109009
2017-10-10T11:51:12.113839: step 1542, loss 0.264481, acc 0.90625, learning_rate 0.000108972
2017-10-10T11:51:12.303046: step 1543, loss 0.202424, acc 0.921875, learning_rate 0.000108936
2017-10-10T11:51:12.446038: step 1544, loss 0.0981309, acc 0.984375, learning_rate 0.000108899
2017-10-10T11:51:12.637420: step 1545, loss 0.207962, acc 0.9375, learning_rate 0.000108863
2017-10-10T11:51:12.816834: step 1546, loss 0.210597, acc 0.9375, learning_rate 0.000108827
2017-10-10T11:51:12.956836: step 1547, loss 0.275228, acc 0.921875, learning_rate 0.000108791
2017-10-10T11:51:13.160979: step 1548, loss 0.143415, acc 0.953125, learning_rate 0.000108755
2017-10-10T11:51:13.368318: step 1549, loss 0.166362, acc 0.953125, learning_rate 0.000108719
2017-10-10T11:51:13.499245: step 1550, loss 0.183615, acc 0.9375, learning_rate 0.000108683
2017-10-10T11:51:13.699508: step 1551, loss 0.155625, acc 0.953125, learning_rate 0.000108648
2017-10-10T11:51:13.839320: step 1552, loss 0.115406, acc 0.984375, learning_rate 0.000108613
2017-10-10T11:51:13.990432: step 1553, loss 0.184286, acc 0.921875, learning_rate 0.000108577
2017-10-10T11:51:14.191773: step 1554, loss 0.278892, acc 0.890625, learning_rate 0.000108542
2017-10-10T11:51:14.384835: step 1555, loss 0.190707, acc 0.9375, learning_rate 0.000108508
2017-10-10T11:51:14.521228: step 1556, loss 0.0876223, acc 0.984375, learning_rate 0.000108473
2017-10-10T11:51:14.708874: step 1557, loss 0.112172, acc 0.96875, learning_rate 0.000108438
2017-10-10T11:51:14.931784: step 1558, loss 0.129103, acc 0.953125, learning_rate 0.000108404
2017-10-10T11:51:15.136865: step 1559, loss 0.146154, acc 0.96875, learning_rate 0.00010837
2017-10-10T11:51:15.304931: step 1560, loss 0.116254, acc 0.953125, learning_rate 0.000108335

Evaluation:
2017-10-10T11:51:15.666172: step 1560, loss 0.238368, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1560

2017-10-10T11:51:16.755681: step 1561, loss 0.0972807, acc 0.96875, learning_rate 0.000108301
2017-10-10T11:51:16.921202: step 1562, loss 0.147542, acc 0.953125, learning_rate 0.000108267
2017-10-10T11:51:17.064905: step 1563, loss 0.152772, acc 0.953125, learning_rate 0.000108234
2017-10-10T11:51:17.259395: step 1564, loss 0.226945, acc 0.9375, learning_rate 0.0001082
2017-10-10T11:51:17.468859: step 1565, loss 0.188229, acc 0.96875, learning_rate 0.000108167
2017-10-10T11:51:17.608336: step 1566, loss 0.227247, acc 0.953125, learning_rate 0.000108133
2017-10-10T11:51:17.770325: step 1567, loss 0.165887, acc 0.96875, learning_rate 0.0001081
2017-10-10T11:51:17.982952: step 1568, loss 0.135197, acc 0.941176, learning_rate 0.000108067
2017-10-10T11:51:18.154332: step 1569, loss 0.123997, acc 0.96875, learning_rate 0.000108034
2017-10-10T11:51:18.304866: step 1570, loss 0.274428, acc 0.890625, learning_rate 0.000108001
2017-10-10T11:51:18.513596: step 1571, loss 0.296096, acc 0.890625, learning_rate 0.000107969
2017-10-10T11:51:18.684862: step 1572, loss 0.191903, acc 0.9375, learning_rate 0.000107936
2017-10-10T11:51:18.905068: step 1573, loss 0.181441, acc 0.9375, learning_rate 0.000107904
2017-10-10T11:51:19.133870: step 1574, loss 0.120439, acc 0.984375, learning_rate 0.000107871
2017-10-10T11:51:19.275027: step 1575, loss 0.0907474, acc 0.96875, learning_rate 0.000107839
2017-10-10T11:51:19.427210: step 1576, loss 0.194247, acc 0.953125, learning_rate 0.000107807
2017-10-10T11:51:19.582824: step 1577, loss 0.179439, acc 0.921875, learning_rate 0.000107775
2017-10-10T11:51:19.735622: step 1578, loss 0.24848, acc 0.921875, learning_rate 0.000107744
2017-10-10T11:51:19.883010: step 1579, loss 0.168021, acc 0.96875, learning_rate 0.000107712
2017-10-10T11:51:20.045777: step 1580, loss 0.182557, acc 0.90625, learning_rate 0.000107681
2017-10-10T11:51:20.204736: step 1581, loss 0.16448, acc 0.9375, learning_rate 0.000107649
2017-10-10T11:51:20.415443: step 1582, loss 0.160828, acc 0.953125, learning_rate 0.000107618
2017-10-10T11:51:20.608329: step 1583, loss 0.242314, acc 0.890625, learning_rate 0.000107587
2017-10-10T11:51:20.768980: step 1584, loss 0.165184, acc 0.953125, learning_rate 0.000107556
2017-10-10T11:51:20.948846: step 1585, loss 0.209839, acc 0.9375, learning_rate 0.000107525
2017-10-10T11:51:21.127333: step 1586, loss 0.193609, acc 0.921875, learning_rate 0.000107494
2017-10-10T11:51:21.377403: step 1587, loss 0.142048, acc 0.96875, learning_rate 0.000107464
2017-10-10T11:51:21.562247: step 1588, loss 0.118548, acc 0.953125, learning_rate 0.000107433
2017-10-10T11:51:21.706357: step 1589, loss 0.277471, acc 0.890625, learning_rate 0.000107403
2017-10-10T11:51:21.820950: step 1590, loss 0.0949695, acc 0.984375, learning_rate 0.000107373
2017-10-10T11:51:21.948603: step 1591, loss 0.204482, acc 0.953125, learning_rate 0.000107343
2017-10-10T11:51:22.106694: step 1592, loss 0.201081, acc 0.90625, learning_rate 0.000107313
2017-10-10T11:51:22.261930: step 1593, loss 0.145317, acc 0.953125, learning_rate 0.000107283
2017-10-10T11:51:22.384148: step 1594, loss 0.161591, acc 0.9375, learning_rate 0.000107253
2017-10-10T11:51:22.500867: step 1595, loss 0.114679, acc 0.96875, learning_rate 0.000107224
2017-10-10T11:51:22.705217: step 1596, loss 0.198342, acc 0.9375, learning_rate 0.000107194
2017-10-10T11:51:22.879473: step 1597, loss 0.137784, acc 0.9375, learning_rate 0.000107165
2017-10-10T11:51:23.016515: step 1598, loss 0.0669464, acc 0.984375, learning_rate 0.000107136
2017-10-10T11:51:23.236847: step 1599, loss 0.133918, acc 0.921875, learning_rate 0.000107106
2017-10-10T11:51:23.435154: step 1600, loss 0.0931412, acc 1, learning_rate 0.000107077

Evaluation:
2017-10-10T11:51:23.843171: step 1600, loss 0.237282, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1600

2017-10-10T11:51:25.007141: step 1601, loss 0.253646, acc 0.90625, learning_rate 0.000107048
2017-10-10T11:51:25.171058: step 1602, loss 0.119624, acc 0.96875, learning_rate 0.00010702
2017-10-10T11:51:25.379022: step 1603, loss 0.119403, acc 0.984375, learning_rate 0.000106991
2017-10-10T11:51:25.592423: step 1604, loss 0.183339, acc 0.953125, learning_rate 0.000106963
2017-10-10T11:51:25.733080: step 1605, loss 0.188882, acc 0.953125, learning_rate 0.000106934
2017-10-10T11:51:25.898492: step 1606, loss 0.122616, acc 0.953125, learning_rate 0.000106906
2017-10-10T11:51:26.106406: step 1607, loss 0.276259, acc 0.9375, learning_rate 0.000106878
2017-10-10T11:51:26.308882: step 1608, loss 0.1763, acc 0.953125, learning_rate 0.00010685
2017-10-10T11:51:26.452179: step 1609, loss 0.14211, acc 0.96875, learning_rate 0.000106822
2017-10-10T11:51:26.622815: step 1610, loss 0.250443, acc 0.921875, learning_rate 0.000106794
2017-10-10T11:51:26.821998: step 1611, loss 0.115669, acc 0.984375, learning_rate 0.000106766
2017-10-10T11:51:27.028917: step 1612, loss 0.207491, acc 0.921875, learning_rate 0.000106738
2017-10-10T11:51:27.190201: step 1613, loss 0.122467, acc 0.953125, learning_rate 0.000106711
2017-10-10T11:51:27.359607: step 1614, loss 0.118522, acc 0.953125, learning_rate 0.000106684
2017-10-10T11:51:27.556070: step 1615, loss 0.180945, acc 0.953125, learning_rate 0.000106656
2017-10-10T11:51:27.733112: step 1616, loss 0.167547, acc 0.9375, learning_rate 0.000106629
2017-10-10T11:51:27.908872: step 1617, loss 0.105014, acc 0.984375, learning_rate 0.000106602
2017-10-10T11:51:28.075839: step 1618, loss 0.267079, acc 0.875, learning_rate 0.000106575
2017-10-10T11:51:28.270859: step 1619, loss 0.174823, acc 0.953125, learning_rate 0.000106548
2017-10-10T11:51:28.414373: step 1620, loss 0.175572, acc 0.9375, learning_rate 0.000106521
2017-10-10T11:51:28.614970: step 1621, loss 0.0993162, acc 0.96875, learning_rate 0.000106495
2017-10-10T11:51:28.789900: step 1622, loss 0.282292, acc 0.9375, learning_rate 0.000106468
2017-10-10T11:51:28.961459: step 1623, loss 0.174688, acc 0.953125, learning_rate 0.000106442
2017-10-10T11:51:29.128714: step 1624, loss 0.143975, acc 0.953125, learning_rate 0.000106416
2017-10-10T11:51:29.302534: step 1625, loss 0.14319, acc 0.953125, learning_rate 0.000106389
2017-10-10T11:51:29.484886: step 1626, loss 0.145359, acc 0.9375, learning_rate 0.000106363
2017-10-10T11:51:29.744918: step 1627, loss 0.167047, acc 0.953125, learning_rate 0.000106337
2017-10-10T11:51:29.932548: step 1628, loss 0.211253, acc 0.921875, learning_rate 0.000106312
2017-10-10T11:51:30.077142: step 1629, loss 0.176708, acc 0.9375, learning_rate 0.000106286
2017-10-10T11:51:30.228183: step 1630, loss 0.0902454, acc 0.96875, learning_rate 0.00010626
2017-10-10T11:51:30.371348: step 1631, loss 0.248097, acc 0.890625, learning_rate 0.000106235
2017-10-10T11:51:30.479011: step 1632, loss 0.163567, acc 0.9375, learning_rate 0.000106209
2017-10-10T11:51:30.615628: step 1633, loss 0.194013, acc 0.921875, learning_rate 0.000106184
2017-10-10T11:51:30.773817: step 1634, loss 0.114463, acc 0.96875, learning_rate 0.000106159
2017-10-10T11:51:30.892053: step 1635, loss 0.122417, acc 0.96875, learning_rate 0.000106133
2017-10-10T11:51:31.125216: step 1636, loss 0.14063, acc 0.953125, learning_rate 0.000106108
2017-10-10T11:51:31.307653: step 1637, loss 0.185202, acc 0.953125, learning_rate 0.000106083
2017-10-10T11:51:31.512677: step 1638, loss 0.184869, acc 0.921875, learning_rate 0.000106059
2017-10-10T11:51:31.776886: step 1639, loss 0.194777, acc 0.921875, learning_rate 0.000106034
2017-10-10T11:51:32.011091: step 1640, loss 0.126415, acc 0.953125, learning_rate 0.000106009

Evaluation:
2017-10-10T11:51:32.313043: step 1640, loss 0.237677, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1640

2017-10-10T11:51:33.483893: step 1641, loss 0.164697, acc 0.953125, learning_rate 0.000105985
2017-10-10T11:51:33.689964: step 1642, loss 0.195181, acc 0.953125, learning_rate 0.00010596
2017-10-10T11:51:33.896894: step 1643, loss 0.320931, acc 0.875, learning_rate 0.000105936
2017-10-10T11:51:34.023523: step 1644, loss 0.130694, acc 0.96875, learning_rate 0.000105912
2017-10-10T11:51:34.215370: step 1645, loss 0.205454, acc 0.921875, learning_rate 0.000105888
2017-10-10T11:51:34.416832: step 1646, loss 0.291826, acc 0.890625, learning_rate 0.000105864
2017-10-10T11:51:34.603583: step 1647, loss 0.068229, acc 0.984375, learning_rate 0.00010584
2017-10-10T11:51:34.806075: step 1648, loss 0.225202, acc 0.921875, learning_rate 0.000105816
2017-10-10T11:51:34.986971: step 1649, loss 0.214671, acc 0.921875, learning_rate 0.000105792
2017-10-10T11:51:35.156628: step 1650, loss 0.132723, acc 0.96875, learning_rate 0.000105768
2017-10-10T11:51:35.344984: step 1651, loss 0.095294, acc 0.953125, learning_rate 0.000105745
2017-10-10T11:51:35.489433: step 1652, loss 0.194354, acc 0.921875, learning_rate 0.000105721
2017-10-10T11:51:35.693025: step 1653, loss 0.129857, acc 0.96875, learning_rate 0.000105698
2017-10-10T11:51:35.892997: step 1654, loss 0.145142, acc 0.96875, learning_rate 0.000105675
2017-10-10T11:51:36.107418: step 1655, loss 0.17198, acc 0.953125, learning_rate 0.000105652
2017-10-10T11:51:36.270557: step 1656, loss 0.19517, acc 0.921875, learning_rate 0.000105629
2017-10-10T11:51:36.461975: step 1657, loss 0.154811, acc 0.953125, learning_rate 0.000105606
2017-10-10T11:51:36.608863: step 1658, loss 0.197219, acc 0.9375, learning_rate 0.000105583
2017-10-10T11:51:36.796926: step 1659, loss 0.170832, acc 0.9375, learning_rate 0.00010556
2017-10-10T11:51:36.966612: step 1660, loss 0.175043, acc 0.953125, learning_rate 0.000105537
2017-10-10T11:51:37.168435: step 1661, loss 0.203366, acc 0.9375, learning_rate 0.000105515
2017-10-10T11:51:37.356861: step 1662, loss 0.133331, acc 0.984375, learning_rate 0.000105492
2017-10-10T11:51:37.552935: step 1663, loss 0.182888, acc 0.9375, learning_rate 0.00010547
2017-10-10T11:51:37.739282: step 1664, loss 0.220701, acc 0.953125, learning_rate 0.000105447
2017-10-10T11:51:37.897330: step 1665, loss 0.0847636, acc 1, learning_rate 0.000105425
2017-10-10T11:51:38.073770: step 1666, loss 0.196305, acc 0.960784, learning_rate 0.000105403
2017-10-10T11:51:38.258877: step 1667, loss 0.142127, acc 0.9375, learning_rate 0.000105381
2017-10-10T11:51:38.473837: step 1668, loss 0.262545, acc 0.890625, learning_rate 0.000105359
2017-10-10T11:51:38.633053: step 1669, loss 0.213034, acc 0.9375, learning_rate 0.000105337
2017-10-10T11:51:38.816371: step 1670, loss 0.121238, acc 0.96875, learning_rate 0.000105315
2017-10-10T11:51:39.015204: step 1671, loss 0.112471, acc 0.984375, learning_rate 0.000105294
2017-10-10T11:51:39.184393: step 1672, loss 0.202069, acc 0.921875, learning_rate 0.000105272
2017-10-10T11:51:39.388841: step 1673, loss 0.224389, acc 0.9375, learning_rate 0.000105251
2017-10-10T11:51:39.580994: step 1674, loss 0.198635, acc 0.9375, learning_rate 0.000105229
2017-10-10T11:51:39.783744: step 1675, loss 0.265933, acc 0.921875, learning_rate 0.000105208
2017-10-10T11:51:39.931367: step 1676, loss 0.218708, acc 0.9375, learning_rate 0.000105186
2017-10-10T11:51:40.172992: step 1677, loss 0.160282, acc 0.96875, learning_rate 0.000105165
2017-10-10T11:51:40.406966: step 1678, loss 0.246565, acc 0.9375, learning_rate 0.000105144
2017-10-10T11:51:40.555852: step 1679, loss 0.106525, acc 0.984375, learning_rate 0.000105123
2017-10-10T11:51:40.716369: step 1680, loss 0.129532, acc 0.953125, learning_rate 0.000105102

Evaluation:
2017-10-10T11:51:41.003928: step 1680, loss 0.239217, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1680

2017-10-10T11:51:41.943418: step 1681, loss 0.2388, acc 0.90625, learning_rate 0.000105081
2017-10-10T11:51:42.163111: step 1682, loss 0.184937, acc 0.953125, learning_rate 0.000105061
2017-10-10T11:51:42.413407: step 1683, loss 0.175425, acc 0.953125, learning_rate 0.00010504
2017-10-10T11:51:42.533988: step 1684, loss 0.171906, acc 0.921875, learning_rate 0.00010502
2017-10-10T11:51:42.676073: step 1685, loss 0.169803, acc 0.9375, learning_rate 0.000104999
2017-10-10T11:51:42.806073: step 1686, loss 0.0949944, acc 0.953125, learning_rate 0.000104979
2017-10-10T11:51:42.921676: step 1687, loss 0.183623, acc 0.9375, learning_rate 0.000104958
2017-10-10T11:51:43.074023: step 1688, loss 0.25127, acc 0.90625, learning_rate 0.000104938
2017-10-10T11:51:43.230822: step 1689, loss 0.256455, acc 0.9375, learning_rate 0.000104918
2017-10-10T11:51:43.360931: step 1690, loss 0.220755, acc 0.90625, learning_rate 0.000104898
2017-10-10T11:51:43.509168: step 1691, loss 0.105826, acc 0.96875, learning_rate 0.000104878
2017-10-10T11:51:43.644876: step 1692, loss 0.278823, acc 0.875, learning_rate 0.000104858
2017-10-10T11:51:43.859700: step 1693, loss 0.259412, acc 0.9375, learning_rate 0.000104838
2017-10-10T11:51:44.068784: step 1694, loss 0.144312, acc 0.953125, learning_rate 0.000104818
2017-10-10T11:51:44.272863: step 1695, loss 0.180985, acc 0.9375, learning_rate 0.000104799
2017-10-10T11:51:44.456048: step 1696, loss 0.110785, acc 0.953125, learning_rate 0.000104779
2017-10-10T11:51:44.624688: step 1697, loss 0.178283, acc 0.921875, learning_rate 0.00010476
2017-10-10T11:51:44.854311: step 1698, loss 0.154419, acc 0.984375, learning_rate 0.00010474
2017-10-10T11:51:45.041173: step 1699, loss 0.0778739, acc 1, learning_rate 0.000104721
2017-10-10T11:51:45.196920: step 1700, loss 0.19237, acc 0.953125, learning_rate 0.000104702
2017-10-10T11:51:45.404844: step 1701, loss 0.194487, acc 0.921875, learning_rate 0.000104682
2017-10-10T11:51:45.652893: step 1702, loss 0.166447, acc 0.921875, learning_rate 0.000104663
2017-10-10T11:51:45.824156: step 1703, loss 0.0947965, acc 0.984375, learning_rate 0.000104644
2017-10-10T11:51:46.019790: step 1704, loss 0.19543, acc 0.921875, learning_rate 0.000104625
2017-10-10T11:51:46.231166: step 1705, loss 0.325833, acc 0.875, learning_rate 0.000104606
2017-10-10T11:51:46.389085: step 1706, loss 0.102774, acc 0.96875, learning_rate 0.000104588
2017-10-10T11:51:46.564115: step 1707, loss 0.145223, acc 0.96875, learning_rate 0.000104569
2017-10-10T11:51:46.742691: step 1708, loss 0.168997, acc 0.953125, learning_rate 0.00010455
2017-10-10T11:51:46.923997: step 1709, loss 0.156732, acc 0.96875, learning_rate 0.000104532
2017-10-10T11:51:47.121063: step 1710, loss 0.233445, acc 0.90625, learning_rate 0.000104513
2017-10-10T11:51:47.268865: step 1711, loss 0.168975, acc 0.9375, learning_rate 0.000104495
2017-10-10T11:51:47.482875: step 1712, loss 0.129809, acc 0.984375, learning_rate 0.000104476
2017-10-10T11:51:47.678421: step 1713, loss 0.148307, acc 0.9375, learning_rate 0.000104458
2017-10-10T11:51:47.840306: step 1714, loss 0.132435, acc 0.96875, learning_rate 0.00010444
2017-10-10T11:51:47.991455: step 1715, loss 0.134842, acc 0.953125, learning_rate 0.000104422
2017-10-10T11:51:48.193329: step 1716, loss 0.0833067, acc 1, learning_rate 0.000104404
2017-10-10T11:51:48.385821: step 1717, loss 0.145743, acc 0.9375, learning_rate 0.000104386
2017-10-10T11:51:48.562770: step 1718, loss 0.238882, acc 0.9375, learning_rate 0.000104368
2017-10-10T11:51:48.716982: step 1719, loss 0.129604, acc 0.96875, learning_rate 0.00010435
2017-10-10T11:51:48.916992: step 1720, loss 0.160051, acc 0.953125, learning_rate 0.000104332

Evaluation:
2017-10-10T11:51:49.276032: step 1720, loss 0.24033, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1720

2017-10-10T11:51:50.486602: step 1721, loss 0.179105, acc 0.953125, learning_rate 0.000104315
2017-10-10T11:51:50.607508: step 1722, loss 0.189687, acc 0.9375, learning_rate 0.000104297
2017-10-10T11:51:50.823117: step 1723, loss 0.267241, acc 0.921875, learning_rate 0.000104279
2017-10-10T11:51:51.083524: step 1724, loss 0.121875, acc 0.984375, learning_rate 0.000104262
2017-10-10T11:51:51.236701: step 1725, loss 0.32349, acc 0.921875, learning_rate 0.000104245
2017-10-10T11:51:51.357841: step 1726, loss 0.0641026, acc 0.984375, learning_rate 0.000104227
2017-10-10T11:51:51.504919: step 1727, loss 0.149871, acc 0.953125, learning_rate 0.00010421
2017-10-10T11:51:51.632860: step 1728, loss 0.239785, acc 0.9375, learning_rate 0.000104193
2017-10-10T11:51:51.775940: step 1729, loss 0.225741, acc 0.921875, learning_rate 0.000104176
2017-10-10T11:51:51.932859: step 1730, loss 0.208422, acc 0.9375, learning_rate 0.000104159
2017-10-10T11:51:52.084295: step 1731, loss 0.137546, acc 0.953125, learning_rate 0.000104142
2017-10-10T11:51:52.230825: step 1732, loss 0.237388, acc 0.890625, learning_rate 0.000104125
2017-10-10T11:51:52.399762: step 1733, loss 0.0924577, acc 0.984375, learning_rate 0.000104108
2017-10-10T11:51:52.664843: step 1734, loss 0.15518, acc 0.96875, learning_rate 0.000104091
2017-10-10T11:51:52.904613: step 1735, loss 0.352747, acc 0.890625, learning_rate 0.000104074
2017-10-10T11:51:53.036905: step 1736, loss 0.156487, acc 0.953125, learning_rate 0.000104058
2017-10-10T11:51:53.180920: step 1737, loss 0.0923608, acc 0.984375, learning_rate 0.000104041
2017-10-10T11:51:53.324982: step 1738, loss 0.285799, acc 0.921875, learning_rate 0.000104025
2017-10-10T11:51:53.470491: step 1739, loss 0.252048, acc 0.921875, learning_rate 0.000104008
2017-10-10T11:51:53.629261: step 1740, loss 0.189251, acc 0.953125, learning_rate 0.000103992
2017-10-10T11:51:53.749908: step 1741, loss 0.0938985, acc 0.984375, learning_rate 0.000103976
2017-10-10T11:51:53.872197: step 1742, loss 0.224344, acc 0.890625, learning_rate 0.000103959
2017-10-10T11:51:54.058976: step 1743, loss 0.181138, acc 0.921875, learning_rate 0.000103943
2017-10-10T11:51:54.201199: step 1744, loss 0.262668, acc 0.875, learning_rate 0.000103927
2017-10-10T11:51:54.384135: step 1745, loss 0.193845, acc 0.921875, learning_rate 0.000103911
2017-10-10T11:51:54.592742: step 1746, loss 0.122554, acc 0.953125, learning_rate 0.000103895
2017-10-10T11:51:54.792635: step 1747, loss 0.127101, acc 0.9375, learning_rate 0.000103879
2017-10-10T11:51:54.929035: step 1748, loss 0.187055, acc 0.953125, learning_rate 0.000103863
2017-10-10T11:51:55.109763: step 1749, loss 0.151386, acc 0.953125, learning_rate 0.000103848
2017-10-10T11:51:55.291450: step 1750, loss 0.165238, acc 0.953125, learning_rate 0.000103832
2017-10-10T11:51:55.438932: step 1751, loss 0.221321, acc 0.9375, learning_rate 0.000103816
2017-10-10T11:51:55.613056: step 1752, loss 0.222716, acc 0.921875, learning_rate 0.000103801
2017-10-10T11:51:55.832609: step 1753, loss 0.183714, acc 0.953125, learning_rate 0.000103785
2017-10-10T11:51:55.996920: step 1754, loss 0.101596, acc 0.984375, learning_rate 0.00010377
2017-10-10T11:51:56.140408: step 1755, loss 0.150128, acc 0.953125, learning_rate 0.000103754
2017-10-10T11:51:56.336347: step 1756, loss 0.11519, acc 0.96875, learning_rate 0.000103739
2017-10-10T11:51:56.549522: step 1757, loss 0.179019, acc 0.921875, learning_rate 0.000103724
2017-10-10T11:51:56.734743: step 1758, loss 0.193278, acc 0.9375, learning_rate 0.000103709
2017-10-10T11:51:56.881380: step 1759, loss 0.147903, acc 0.953125, learning_rate 0.000103694
2017-10-10T11:51:57.062136: step 1760, loss 0.135845, acc 0.953125, learning_rate 0.000103678

Evaluation:
2017-10-10T11:51:57.411269: step 1760, loss 0.237043, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1760

2017-10-10T11:51:58.725075: step 1761, loss 0.187662, acc 0.9375, learning_rate 0.000103663
2017-10-10T11:51:58.883422: step 1762, loss 0.107032, acc 0.96875, learning_rate 0.000103648
2017-10-10T11:51:59.042537: step 1763, loss 0.196612, acc 0.90625, learning_rate 0.000103634
2017-10-10T11:51:59.223715: step 1764, loss 0.180795, acc 0.921569, learning_rate 0.000103619
2017-10-10T11:51:59.430922: step 1765, loss 0.251084, acc 0.9375, learning_rate 0.000103604
2017-10-10T11:51:59.617221: step 1766, loss 0.195178, acc 0.96875, learning_rate 0.000103589
2017-10-10T11:51:59.773895: step 1767, loss 0.118929, acc 0.953125, learning_rate 0.000103575
2017-10-10T11:51:59.939630: step 1768, loss 0.107871, acc 0.953125, learning_rate 0.00010356
2017-10-10T11:52:00.110332: step 1769, loss 0.151372, acc 0.953125, learning_rate 0.000103545
2017-10-10T11:52:00.316827: step 1770, loss 0.14428, acc 0.9375, learning_rate 0.000103531
2017-10-10T11:52:00.520163: step 1771, loss 0.289217, acc 0.90625, learning_rate 0.000103517
2017-10-10T11:52:00.704821: step 1772, loss 0.130491, acc 0.953125, learning_rate 0.000103502
2017-10-10T11:52:00.836953: step 1773, loss 0.115893, acc 0.984375, learning_rate 0.000103488
2017-10-10T11:52:01.038345: step 1774, loss 0.10968, acc 0.96875, learning_rate 0.000103474
2017-10-10T11:52:01.258896: step 1775, loss 0.134183, acc 0.953125, learning_rate 0.00010346
2017-10-10T11:52:01.399883: step 1776, loss 0.167447, acc 0.953125, learning_rate 0.000103445
2017-10-10T11:52:01.560993: step 1777, loss 0.0736242, acc 0.984375, learning_rate 0.000103431
2017-10-10T11:52:01.772873: step 1778, loss 0.243862, acc 0.921875, learning_rate 0.000103417
2017-10-10T11:52:02.033244: step 1779, loss 0.17473, acc 0.90625, learning_rate 0.000103403
2017-10-10T11:52:02.173247: step 1780, loss 0.128053, acc 0.953125, learning_rate 0.00010339
2017-10-10T11:52:02.360871: step 1781, loss 0.179254, acc 0.9375, learning_rate 0.000103376
2017-10-10T11:52:02.489038: step 1782, loss 0.159276, acc 0.953125, learning_rate 0.000103362
2017-10-10T11:52:02.607397: step 1783, loss 0.235112, acc 0.9375, learning_rate 0.000103348
2017-10-10T11:52:02.768825: step 1784, loss 0.242732, acc 0.890625, learning_rate 0.000103335
2017-10-10T11:52:02.922504: step 1785, loss 0.0977344, acc 1, learning_rate 0.000103321
2017-10-10T11:52:03.056985: step 1786, loss 0.24566, acc 0.921875, learning_rate 0.000103307
2017-10-10T11:52:03.284054: step 1787, loss 0.166188, acc 0.953125, learning_rate 0.000103294
2017-10-10T11:52:03.487352: step 1788, loss 0.118909, acc 0.96875, learning_rate 0.00010328
2017-10-10T11:52:03.604989: step 1789, loss 0.172547, acc 0.9375, learning_rate 0.000103267
2017-10-10T11:52:03.760868: step 1790, loss 0.0906602, acc 0.96875, learning_rate 0.000103254
2017-10-10T11:52:03.896792: step 1791, loss 0.220658, acc 0.921875, learning_rate 0.00010324
2017-10-10T11:52:04.053718: step 1792, loss 0.170082, acc 0.953125, learning_rate 0.000103227
2017-10-10T11:52:04.195470: step 1793, loss 0.118608, acc 0.96875, learning_rate 0.000103214
2017-10-10T11:52:04.320871: step 1794, loss 0.199768, acc 0.953125, learning_rate 0.000103201
2017-10-10T11:52:04.514097: step 1795, loss 0.133127, acc 0.953125, learning_rate 0.000103188
2017-10-10T11:52:04.724861: step 1796, loss 0.187702, acc 0.953125, learning_rate 0.000103175
2017-10-10T11:52:04.838965: step 1797, loss 0.128057, acc 0.984375, learning_rate 0.000103162
2017-10-10T11:52:05.050173: step 1798, loss 0.221952, acc 0.953125, learning_rate 0.000103149
2017-10-10T11:52:05.253694: step 1799, loss 0.0885023, acc 1, learning_rate 0.000103136
2017-10-10T11:52:05.389540: step 1800, loss 0.0642485, acc 0.984375, learning_rate 0.000103123

Evaluation:
2017-10-10T11:52:05.754515: step 1800, loss 0.236201, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1800

2017-10-10T11:52:06.901274: step 1801, loss 0.18458, acc 0.953125, learning_rate 0.000103111
2017-10-10T11:52:07.113926: step 1802, loss 0.281855, acc 0.9375, learning_rate 0.000103098
2017-10-10T11:52:07.323704: step 1803, loss 0.165943, acc 0.921875, learning_rate 0.000103085
2017-10-10T11:52:07.508872: step 1804, loss 0.129227, acc 0.96875, learning_rate 0.000103073
2017-10-10T11:52:07.677112: step 1805, loss 0.0984764, acc 0.984375, learning_rate 0.00010306
2017-10-10T11:52:07.834266: step 1806, loss 0.165105, acc 0.96875, learning_rate 0.000103048
2017-10-10T11:52:08.038550: step 1807, loss 0.200831, acc 0.921875, learning_rate 0.000103035
2017-10-10T11:52:08.216938: step 1808, loss 0.163216, acc 0.921875, learning_rate 0.000103023
2017-10-10T11:52:08.382382: step 1809, loss 0.10613, acc 0.953125, learning_rate 0.00010301
2017-10-10T11:52:08.584124: step 1810, loss 0.196262, acc 0.921875, learning_rate 0.000102998
2017-10-10T11:52:08.799910: step 1811, loss 0.135558, acc 0.96875, learning_rate 0.000102986
2017-10-10T11:52:08.977053: step 1812, loss 0.204285, acc 0.921875, learning_rate 0.000102974
2017-10-10T11:52:09.117660: step 1813, loss 0.207968, acc 0.921875, learning_rate 0.000102962
2017-10-10T11:52:09.292230: step 1814, loss 0.191224, acc 0.9375, learning_rate 0.000102949
2017-10-10T11:52:09.416917: step 1815, loss 0.149955, acc 0.9375, learning_rate 0.000102937
2017-10-10T11:52:09.620874: step 1816, loss 0.222731, acc 0.921875, learning_rate 0.000102925
2017-10-10T11:52:09.834950: step 1817, loss 0.142211, acc 0.96875, learning_rate 0.000102913
2017-10-10T11:52:10.037856: step 1818, loss 0.299864, acc 0.890625, learning_rate 0.000102902
2017-10-10T11:52:10.178070: step 1819, loss 0.408053, acc 0.921875, learning_rate 0.00010289
2017-10-10T11:52:10.368091: step 1820, loss 0.196325, acc 0.890625, learning_rate 0.000102878
2017-10-10T11:52:10.585171: step 1821, loss 0.274131, acc 0.90625, learning_rate 0.000102866
2017-10-10T11:52:10.778633: step 1822, loss 0.183349, acc 0.9375, learning_rate 0.000102855
2017-10-10T11:52:10.912840: step 1823, loss 0.170094, acc 0.9375, learning_rate 0.000102843
2017-10-10T11:52:11.118690: step 1824, loss 0.213973, acc 0.90625, learning_rate 0.000102831
2017-10-10T11:52:11.312962: step 1825, loss 0.191854, acc 0.921875, learning_rate 0.00010282
2017-10-10T11:52:11.483545: step 1826, loss 0.257438, acc 0.90625, learning_rate 0.000102808
2017-10-10T11:52:11.878724: step 1827, loss 0.128405, acc 0.921875, learning_rate 0.000102797
2017-10-10T11:52:11.979000: step 1828, loss 0.254145, acc 0.90625, learning_rate 0.000102785
2017-10-10T11:52:12.165587: step 1829, loss 0.145683, acc 0.953125, learning_rate 0.000102774
2017-10-10T11:52:12.337034: step 1830, loss 0.152128, acc 0.9375, learning_rate 0.000102763
2017-10-10T11:52:12.542128: step 1831, loss 0.107171, acc 0.953125, learning_rate 0.000102751
2017-10-10T11:52:12.752909: step 1832, loss 0.0970319, acc 0.96875, learning_rate 0.00010274
2017-10-10T11:52:12.948854: step 1833, loss 0.131092, acc 0.953125, learning_rate 0.000102729
2017-10-10T11:52:13.101051: step 1834, loss 0.129826, acc 0.953125, learning_rate 0.000102718
2017-10-10T11:52:13.333002: step 1835, loss 0.108215, acc 0.953125, learning_rate 0.000102707
2017-10-10T11:52:13.571700: step 1836, loss 0.219157, acc 0.921875, learning_rate 0.000102696
2017-10-10T11:52:13.712984: step 1837, loss 0.232566, acc 0.90625, learning_rate 0.000102685
2017-10-10T11:52:13.848908: step 1838, loss 0.29613, acc 0.890625, learning_rate 0.000102674
2017-10-10T11:52:13.972526: step 1839, loss 0.233646, acc 0.953125, learning_rate 0.000102663
2017-10-10T11:52:14.164818: step 1840, loss 0.22635, acc 0.921875, learning_rate 0.000102652

Evaluation:
2017-10-10T11:52:14.466169: step 1840, loss 0.234702, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1840

2017-10-10T11:52:15.449438: step 1841, loss 0.321528, acc 0.921875, learning_rate 0.000102641
2017-10-10T11:52:15.662250: step 1842, loss 0.0920897, acc 1, learning_rate 0.00010263
2017-10-10T11:52:15.833083: step 1843, loss 0.132542, acc 0.953125, learning_rate 0.00010262
2017-10-10T11:52:15.972853: step 1844, loss 0.0891604, acc 0.984375, learning_rate 0.000102609
2017-10-10T11:52:16.169435: step 1845, loss 0.142307, acc 0.953125, learning_rate 0.000102598
2017-10-10T11:52:16.336912: step 1846, loss 0.224418, acc 0.90625, learning_rate 0.000102588
2017-10-10T11:52:16.481440: step 1847, loss 0.239851, acc 0.90625, learning_rate 0.000102577
2017-10-10T11:52:16.687511: step 1848, loss 0.0906745, acc 0.984375, learning_rate 0.000102567
2017-10-10T11:52:16.881557: step 1849, loss 0.17293, acc 0.9375, learning_rate 0.000102556
2017-10-10T11:52:17.069212: step 1850, loss 0.131137, acc 0.9375, learning_rate 0.000102546
2017-10-10T11:52:17.258585: step 1851, loss 0.157282, acc 0.9375, learning_rate 0.000102535
2017-10-10T11:52:17.436973: step 1852, loss 0.25434, acc 0.890625, learning_rate 0.000102525
2017-10-10T11:52:17.612879: step 1853, loss 0.218665, acc 0.9375, learning_rate 0.000102515
2017-10-10T11:52:17.777709: step 1854, loss 0.204616, acc 0.921875, learning_rate 0.000102504
2017-10-10T11:52:17.978001: step 1855, loss 0.125443, acc 0.984375, learning_rate 0.000102494
2017-10-10T11:52:18.182556: step 1856, loss 0.130984, acc 0.9375, learning_rate 0.000102484
2017-10-10T11:52:18.333930: step 1857, loss 0.138014, acc 0.9375, learning_rate 0.000102474
2017-10-10T11:52:18.487306: step 1858, loss 0.241693, acc 0.953125, learning_rate 0.000102464
2017-10-10T11:52:18.682254: step 1859, loss 0.15134, acc 0.953125, learning_rate 0.000102454
2017-10-10T11:52:18.855548: step 1860, loss 0.258481, acc 0.9375, learning_rate 0.000102444
2017-10-10T11:52:19.040742: step 1861, loss 0.0856622, acc 0.984375, learning_rate 0.000102434
2017-10-10T11:52:19.203771: step 1862, loss 0.270834, acc 0.901961, learning_rate 0.000102424
2017-10-10T11:52:19.391334: step 1863, loss 0.120292, acc 0.984375, learning_rate 0.000102414
2017-10-10T11:52:19.571616: step 1864, loss 0.300318, acc 0.921875, learning_rate 0.000102404
2017-10-10T11:52:19.761746: step 1865, loss 0.0861202, acc 0.984375, learning_rate 0.000102394
2017-10-10T11:52:19.977817: step 1866, loss 0.137991, acc 0.9375, learning_rate 0.000102384
2017-10-10T11:52:20.220294: step 1867, loss 0.138015, acc 0.984375, learning_rate 0.000102375
2017-10-10T11:52:20.425864: step 1868, loss 0.178794, acc 0.9375, learning_rate 0.000102365
2017-10-10T11:52:20.580840: step 1869, loss 0.208479, acc 0.9375, learning_rate 0.000102355
2017-10-10T11:52:20.743835: step 1870, loss 0.108841, acc 0.96875, learning_rate 0.000102346
2017-10-10T11:52:20.956307: step 1871, loss 0.176026, acc 0.96875, learning_rate 0.000102336
2017-10-10T11:52:21.165242: step 1872, loss 0.229166, acc 0.953125, learning_rate 0.000102327
2017-10-10T11:52:21.338449: step 1873, loss 0.0722952, acc 0.96875, learning_rate 0.000102317
2017-10-10T11:52:21.504929: step 1874, loss 0.323925, acc 0.859375, learning_rate 0.000102308
2017-10-10T11:52:21.699668: step 1875, loss 0.0952751, acc 0.984375, learning_rate 0.000102298
2017-10-10T11:52:21.860916: step 1876, loss 0.234925, acc 0.96875, learning_rate 0.000102289
2017-10-10T11:52:22.051568: step 1877, loss 0.139162, acc 0.9375, learning_rate 0.000102279
2017-10-10T11:52:22.238998: step 1878, loss 0.367588, acc 0.859375, learning_rate 0.00010227
2017-10-10T11:52:22.424480: step 1879, loss 0.0896641, acc 0.96875, learning_rate 0.000102261
2017-10-10T11:52:22.630566: step 1880, loss 0.131709, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-10-10T11:52:23.006410: step 1880, loss 0.235815, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1880

2017-10-10T11:52:24.296995: step 1881, loss 0.0982715, acc 0.96875, learning_rate 0.000102242
2017-10-10T11:52:24.514893: step 1882, loss 0.190038, acc 0.921875, learning_rate 0.000102233
2017-10-10T11:52:24.720714: step 1883, loss 0.239716, acc 0.921875, learning_rate 0.000102224
2017-10-10T11:52:24.816719: step 1884, loss 0.261113, acc 0.921875, learning_rate 0.000102215
2017-10-10T11:52:24.906370: step 1885, loss 0.0983796, acc 0.96875, learning_rate 0.000102206
2017-10-10T11:52:24.994409: step 1886, loss 0.113043, acc 0.96875, learning_rate 0.000102197
2017-10-10T11:52:25.092288: step 1887, loss 0.134964, acc 0.96875, learning_rate 0.000102188
2017-10-10T11:52:25.180171: step 1888, loss 0.149315, acc 0.9375, learning_rate 0.000102179
2017-10-10T11:52:25.268313: step 1889, loss 0.246351, acc 0.90625, learning_rate 0.00010217
2017-10-10T11:52:25.355195: step 1890, loss 0.188, acc 0.9375, learning_rate 0.000102161
2017-10-10T11:52:25.461121: step 1891, loss 0.197547, acc 0.921875, learning_rate 0.000102153
2017-10-10T11:52:25.611687: step 1892, loss 0.0634682, acc 0.984375, learning_rate 0.000102144
2017-10-10T11:52:25.788714: step 1893, loss 0.151701, acc 0.9375, learning_rate 0.000102135
2017-10-10T11:52:25.921883: step 1894, loss 0.106034, acc 0.984375, learning_rate 0.000102126
2017-10-10T11:52:26.104826: step 1895, loss 0.131917, acc 0.984375, learning_rate 0.000102118
2017-10-10T11:52:26.280946: step 1896, loss 0.143941, acc 0.96875, learning_rate 0.000102109
2017-10-10T11:52:26.426389: step 1897, loss 0.27381, acc 0.890625, learning_rate 0.0001021
2017-10-10T11:52:26.623959: step 1898, loss 0.247325, acc 0.90625, learning_rate 0.000102092
2017-10-10T11:52:26.837271: step 1899, loss 0.252955, acc 0.875, learning_rate 0.000102083
2017-10-10T11:52:27.048868: step 1900, loss 0.231859, acc 0.921875, learning_rate 0.000102075
2017-10-10T11:52:27.202175: step 1901, loss 0.186723, acc 0.90625, learning_rate 0.000102066
2017-10-10T11:52:27.360482: step 1902, loss 0.154048, acc 0.9375, learning_rate 0.000102058
2017-10-10T11:52:27.570215: step 1903, loss 0.177882, acc 0.9375, learning_rate 0.00010205
2017-10-10T11:52:27.695076: step 1904, loss 0.175336, acc 0.953125, learning_rate 0.000102041
2017-10-10T11:52:27.872541: step 1905, loss 0.135476, acc 0.953125, learning_rate 0.000102033
2017-10-10T11:52:28.079485: step 1906, loss 0.150607, acc 0.984375, learning_rate 0.000102025
2017-10-10T11:52:28.268865: step 1907, loss 0.0830701, acc 1, learning_rate 0.000102016
2017-10-10T11:52:28.433037: step 1908, loss 0.174469, acc 0.921875, learning_rate 0.000102008
2017-10-10T11:52:28.588035: step 1909, loss 0.136437, acc 0.96875, learning_rate 0.000102
2017-10-10T11:52:28.809273: step 1910, loss 0.158432, acc 0.96875, learning_rate 0.000101992
2017-10-10T11:52:28.941057: step 1911, loss 0.269655, acc 0.90625, learning_rate 0.000101984
2017-10-10T11:52:29.103662: step 1912, loss 0.0728108, acc 0.984375, learning_rate 0.000101975
2017-10-10T11:52:29.300735: step 1913, loss 0.190457, acc 0.953125, learning_rate 0.000101967
2017-10-10T11:52:29.477940: step 1914, loss 0.142777, acc 0.953125, learning_rate 0.000101959
2017-10-10T11:52:29.657038: step 1915, loss 0.181235, acc 0.90625, learning_rate 0.000101951
2017-10-10T11:52:29.818788: step 1916, loss 0.165792, acc 0.953125, learning_rate 0.000101943
2017-10-10T11:52:29.993157: step 1917, loss 0.148352, acc 0.96875, learning_rate 0.000101935
2017-10-10T11:52:30.172861: step 1918, loss 0.173673, acc 0.953125, learning_rate 0.000101928
2017-10-10T11:52:30.380534: step 1919, loss 0.214535, acc 0.921875, learning_rate 0.00010192
2017-10-10T11:52:30.560140: step 1920, loss 0.219032, acc 0.90625, learning_rate 0.000101912

Evaluation:
2017-10-10T11:52:30.899539: step 1920, loss 0.235391, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1920

2017-10-10T11:52:31.839862: step 1921, loss 0.151186, acc 0.953125, learning_rate 0.000101904
2017-10-10T11:52:32.041064: step 1922, loss 0.151471, acc 0.953125, learning_rate 0.000101896
2017-10-10T11:52:32.204978: step 1923, loss 0.273084, acc 0.890625, learning_rate 0.000101889
2017-10-10T11:52:32.373875: step 1924, loss 0.281709, acc 0.875, learning_rate 0.000101881
2017-10-10T11:52:32.571864: step 1925, loss 0.163106, acc 0.921875, learning_rate 0.000101873
2017-10-10T11:52:32.759150: step 1926, loss 0.197681, acc 0.9375, learning_rate 0.000101865
2017-10-10T11:52:32.939617: step 1927, loss 0.207204, acc 0.921875, learning_rate 0.000101858
2017-10-10T11:52:33.113994: step 1928, loss 0.187123, acc 0.9375, learning_rate 0.00010185
2017-10-10T11:52:33.313718: step 1929, loss 0.151836, acc 0.953125, learning_rate 0.000101843
2017-10-10T11:52:33.512826: step 1930, loss 0.14884, acc 0.96875, learning_rate 0.000101835
2017-10-10T11:52:33.628884: step 1931, loss 0.122791, acc 0.96875, learning_rate 0.000101828
2017-10-10T11:52:33.819522: step 1932, loss 0.127949, acc 0.953125, learning_rate 0.00010182
2017-10-10T11:52:34.039400: step 1933, loss 0.199938, acc 0.90625, learning_rate 0.000101813
2017-10-10T11:52:34.220975: step 1934, loss 0.230874, acc 0.921875, learning_rate 0.000101805
2017-10-10T11:52:34.369919: step 1935, loss 0.241618, acc 0.90625, learning_rate 0.000101798
2017-10-10T11:52:34.576870: step 1936, loss 0.123268, acc 0.96875, learning_rate 0.000101791
2017-10-10T11:52:34.776473: step 1937, loss 0.0879071, acc 0.984375, learning_rate 0.000101783
2017-10-10T11:52:34.949766: step 1938, loss 0.123848, acc 0.96875, learning_rate 0.000101776
2017-10-10T11:52:35.068387: step 1939, loss 0.145652, acc 0.953125, learning_rate 0.000101769
2017-10-10T11:52:35.289063: step 1940, loss 0.241681, acc 0.9375, learning_rate 0.000101762
2017-10-10T11:52:35.492952: step 1941, loss 0.189687, acc 0.9375, learning_rate 0.000101754
2017-10-10T11:52:35.745189: step 1942, loss 0.186791, acc 0.921875, learning_rate 0.000101747
2017-10-10T11:52:35.880427: step 1943, loss 0.191911, acc 0.9375, learning_rate 0.00010174
2017-10-10T11:52:35.977687: step 1944, loss 0.247129, acc 0.9375, learning_rate 0.000101733
2017-10-10T11:52:36.084679: step 1945, loss 0.190024, acc 0.953125, learning_rate 0.000101726
2017-10-10T11:52:36.175863: step 1946, loss 0.257006, acc 0.890625, learning_rate 0.000101719
2017-10-10T11:52:36.261721: step 1947, loss 0.217841, acc 0.90625, learning_rate 0.000101712
2017-10-10T11:52:36.348740: step 1948, loss 0.276079, acc 0.9375, learning_rate 0.000101705
2017-10-10T11:52:36.434990: step 1949, loss 0.14264, acc 0.96875, learning_rate 0.000101698
2017-10-10T11:52:36.653242: step 1950, loss 0.172825, acc 0.921875, learning_rate 0.000101691
2017-10-10T11:52:36.801216: step 1951, loss 0.150704, acc 0.921875, learning_rate 0.000101684
2017-10-10T11:52:36.967767: step 1952, loss 0.169342, acc 0.953125, learning_rate 0.000101677
2017-10-10T11:52:37.191693: step 1953, loss 0.12502, acc 0.953125, learning_rate 0.00010167
2017-10-10T11:52:37.406122: step 1954, loss 0.127374, acc 0.9375, learning_rate 0.000101664
2017-10-10T11:52:37.596868: step 1955, loss 0.224874, acc 0.921875, learning_rate 0.000101657
2017-10-10T11:52:37.762301: step 1956, loss 0.169112, acc 0.9375, learning_rate 0.00010165
2017-10-10T11:52:37.949556: step 1957, loss 0.0989238, acc 0.984375, learning_rate 0.000101643
2017-10-10T11:52:38.139319: step 1958, loss 0.127106, acc 0.96875, learning_rate 0.000101637
2017-10-10T11:52:38.332862: step 1959, loss 0.163089, acc 0.96875, learning_rate 0.00010163
2017-10-10T11:52:38.438726: step 1960, loss 0.129503, acc 0.960784, learning_rate 0.000101623

Evaluation:
2017-10-10T11:52:38.821548: step 1960, loss 0.234694, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-1960

2017-10-10T11:52:39.979205: step 1961, loss 0.134469, acc 0.921875, learning_rate 0.000101617
2017-10-10T11:52:40.168869: step 1962, loss 0.10471, acc 0.953125, learning_rate 0.00010161
2017-10-10T11:52:40.332842: step 1963, loss 0.243392, acc 0.921875, learning_rate 0.000101604
2017-10-10T11:52:40.546793: step 1964, loss 0.245488, acc 0.921875, learning_rate 0.000101597
2017-10-10T11:52:40.757676: step 1965, loss 0.133138, acc 0.96875, learning_rate 0.00010159
2017-10-10T11:52:40.914585: step 1966, loss 0.107792, acc 0.984375, learning_rate 0.000101584
2017-10-10T11:52:41.066183: step 1967, loss 0.147256, acc 0.96875, learning_rate 0.000101577
2017-10-10T11:52:41.281253: step 1968, loss 0.257221, acc 0.953125, learning_rate 0.000101571
2017-10-10T11:52:41.495708: step 1969, loss 0.147796, acc 0.953125, learning_rate 0.000101565
2017-10-10T11:52:41.692900: step 1970, loss 0.127805, acc 0.953125, learning_rate 0.000101558
2017-10-10T11:52:41.857038: step 1971, loss 0.157949, acc 0.9375, learning_rate 0.000101552
2017-10-10T11:52:42.028825: step 1972, loss 0.200529, acc 0.9375, learning_rate 0.000101546
2017-10-10T11:52:42.220879: step 1973, loss 0.244523, acc 0.90625, learning_rate 0.000101539
2017-10-10T11:52:42.360840: step 1974, loss 0.111595, acc 0.984375, learning_rate 0.000101533
2017-10-10T11:52:42.532625: step 1975, loss 0.19237, acc 0.953125, learning_rate 0.000101527
2017-10-10T11:52:42.740578: step 1976, loss 0.215125, acc 0.921875, learning_rate 0.00010152
2017-10-10T11:52:42.912785: step 1977, loss 0.0965113, acc 1, learning_rate 0.000101514
2017-10-10T11:52:43.084245: step 1978, loss 0.109515, acc 0.96875, learning_rate 0.000101508
2017-10-10T11:52:43.304870: step 1979, loss 0.178865, acc 0.921875, learning_rate 0.000101502
2017-10-10T11:52:43.514109: step 1980, loss 0.18901, acc 0.9375, learning_rate 0.000101496
2017-10-10T11:52:43.716866: step 1981, loss 0.258988, acc 0.921875, learning_rate 0.00010149
2017-10-10T11:52:43.882968: step 1982, loss 0.168856, acc 0.9375, learning_rate 0.000101484
2017-10-10T11:52:44.065389: step 1983, loss 0.224413, acc 0.875, learning_rate 0.000101478
2017-10-10T11:52:44.277069: step 1984, loss 0.167504, acc 0.9375, learning_rate 0.000101472
2017-10-10T11:52:44.468853: step 1985, loss 0.139949, acc 0.9375, learning_rate 0.000101466
2017-10-10T11:52:44.633090: step 1986, loss 0.111748, acc 0.96875, learning_rate 0.00010146
2017-10-10T11:52:44.817597: step 1987, loss 0.130765, acc 0.953125, learning_rate 0.000101454
2017-10-10T11:52:44.999886: step 1988, loss 0.178765, acc 0.953125, learning_rate 0.000101448
2017-10-10T11:52:45.215899: step 1989, loss 0.220745, acc 0.921875, learning_rate 0.000101442
2017-10-10T11:52:45.380653: step 1990, loss 0.140674, acc 0.96875, learning_rate 0.000101436
2017-10-10T11:52:45.511832: step 1991, loss 0.0813493, acc 1, learning_rate 0.00010143
2017-10-10T11:52:45.708821: step 1992, loss 0.101236, acc 0.96875, learning_rate 0.000101424
2017-10-10T11:52:45.907688: step 1993, loss 0.202375, acc 0.9375, learning_rate 0.000101418
2017-10-10T11:52:46.115976: step 1994, loss 0.174053, acc 0.96875, learning_rate 0.000101413
2017-10-10T11:52:46.294795: step 1995, loss 0.0698649, acc 0.984375, learning_rate 0.000101407
2017-10-10T11:52:46.468850: step 1996, loss 0.106688, acc 0.96875, learning_rate 0.000101401
2017-10-10T11:52:46.646979: step 1997, loss 0.20406, acc 0.90625, learning_rate 0.000101395
2017-10-10T11:52:46.816945: step 1998, loss 0.213499, acc 0.953125, learning_rate 0.00010139
2017-10-10T11:52:46.908571: step 1999, loss 0.103378, acc 0.984375, learning_rate 0.000101384
2017-10-10T11:52:47.004962: step 2000, loss 0.247656, acc 0.90625, learning_rate 0.000101378

Evaluation:
2017-10-10T11:52:47.289958: step 2000, loss 0.234635, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2000

2017-10-10T11:52:48.379168: step 2001, loss 0.114936, acc 0.953125, learning_rate 0.000101373
2017-10-10T11:52:48.582049: step 2002, loss 0.197778, acc 0.90625, learning_rate 0.000101367
2017-10-10T11:52:48.793990: step 2003, loss 0.174491, acc 0.9375, learning_rate 0.000101362
2017-10-10T11:52:48.958014: step 2004, loss 0.198725, acc 0.9375, learning_rate 0.000101356
2017-10-10T11:52:49.107923: step 2005, loss 0.226615, acc 0.9375, learning_rate 0.00010135
2017-10-10T11:52:49.315584: step 2006, loss 0.163428, acc 0.9375, learning_rate 0.000101345
2017-10-10T11:52:49.476890: step 2007, loss 0.221899, acc 0.90625, learning_rate 0.000101339
2017-10-10T11:52:49.618228: step 2008, loss 0.206264, acc 0.9375, learning_rate 0.000101334
2017-10-10T11:52:49.803397: step 2009, loss 0.173527, acc 0.9375, learning_rate 0.000101328
2017-10-10T11:52:50.004869: step 2010, loss 0.260053, acc 0.890625, learning_rate 0.000101323
2017-10-10T11:52:50.208736: step 2011, loss 0.138175, acc 0.96875, learning_rate 0.000101318
2017-10-10T11:52:50.416993: step 2012, loss 0.19237, acc 0.984375, learning_rate 0.000101312
2017-10-10T11:52:50.608424: step 2013, loss 0.257943, acc 0.90625, learning_rate 0.000101307
2017-10-10T11:52:50.797015: step 2014, loss 0.0744046, acc 0.984375, learning_rate 0.000101302
2017-10-10T11:52:50.988383: step 2015, loss 0.212282, acc 0.9375, learning_rate 0.000101296
2017-10-10T11:52:51.176332: step 2016, loss 0.120003, acc 0.953125, learning_rate 0.000101291
2017-10-10T11:52:51.394123: step 2017, loss 0.237935, acc 0.953125, learning_rate 0.000101286
2017-10-10T11:52:51.572858: step 2018, loss 0.124928, acc 0.96875, learning_rate 0.00010128
2017-10-10T11:52:51.712293: step 2019, loss 0.0838263, acc 0.984375, learning_rate 0.000101275
2017-10-10T11:52:51.904517: step 2020, loss 0.0840147, acc 0.984375, learning_rate 0.00010127
2017-10-10T11:52:52.118884: step 2021, loss 0.18913, acc 0.9375, learning_rate 0.000101265
2017-10-10T11:52:52.271197: step 2022, loss 0.263398, acc 0.921875, learning_rate 0.00010126
2017-10-10T11:52:52.431630: step 2023, loss 0.143119, acc 0.9375, learning_rate 0.000101255
2017-10-10T11:52:52.642279: step 2024, loss 0.158361, acc 0.9375, learning_rate 0.000101249
2017-10-10T11:52:52.774380: step 2025, loss 0.22765, acc 0.90625, learning_rate 0.000101244
2017-10-10T11:52:52.931856: step 2026, loss 0.0948552, acc 1, learning_rate 0.000101239
2017-10-10T11:52:53.139342: step 2027, loss 0.179218, acc 0.953125, learning_rate 0.000101234
2017-10-10T11:52:53.332918: step 2028, loss 0.134719, acc 0.96875, learning_rate 0.000101229
2017-10-10T11:52:53.535436: step 2029, loss 0.19791, acc 0.921875, learning_rate 0.000101224
2017-10-10T11:52:53.686166: step 2030, loss 0.134503, acc 0.9375, learning_rate 0.000101219
2017-10-10T11:52:53.878822: step 2031, loss 0.165428, acc 0.9375, learning_rate 0.000101214
2017-10-10T11:52:54.053033: step 2032, loss 0.142406, acc 0.9375, learning_rate 0.000101209
2017-10-10T11:52:54.251248: step 2033, loss 0.106397, acc 0.96875, learning_rate 0.000101204
2017-10-10T11:52:54.447007: step 2034, loss 0.179339, acc 0.9375, learning_rate 0.000101199
2017-10-10T11:52:54.669153: step 2035, loss 0.119434, acc 0.953125, learning_rate 0.000101194
2017-10-10T11:52:54.818750: step 2036, loss 0.119265, acc 0.9375, learning_rate 0.00010119
2017-10-10T11:52:55.032928: step 2037, loss 0.144807, acc 0.96875, learning_rate 0.000101185
2017-10-10T11:52:55.203241: step 2038, loss 0.213278, acc 0.921875, learning_rate 0.00010118
2017-10-10T11:52:55.419331: step 2039, loss 0.158643, acc 0.96875, learning_rate 0.000101175
2017-10-10T11:52:55.602609: step 2040, loss 0.184953, acc 0.9375, learning_rate 0.00010117

Evaluation:
2017-10-10T11:52:55.991058: step 2040, loss 0.234309, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2040

2017-10-10T11:52:57.334744: step 2041, loss 0.161343, acc 0.96875, learning_rate 0.000101166
2017-10-10T11:52:57.472692: step 2042, loss 0.135873, acc 0.9375, learning_rate 0.000101161
2017-10-10T11:52:57.629165: step 2043, loss 0.177102, acc 0.921875, learning_rate 0.000101156
2017-10-10T11:52:57.764860: step 2044, loss 0.115677, acc 0.96875, learning_rate 0.000101151
2017-10-10T11:52:57.896905: step 2045, loss 0.113271, acc 0.953125, learning_rate 0.000101147
2017-10-10T11:52:58.028856: step 2046, loss 0.154744, acc 0.953125, learning_rate 0.000101142
2017-10-10T11:52:58.176879: step 2047, loss 0.216187, acc 0.9375, learning_rate 0.000101137
2017-10-10T11:52:58.330776: step 2048, loss 0.120403, acc 0.953125, learning_rate 0.000101133
2017-10-10T11:52:58.492387: step 2049, loss 0.217119, acc 0.953125, learning_rate 0.000101128
2017-10-10T11:52:58.616506: step 2050, loss 0.147221, acc 0.96875, learning_rate 0.000101123
2017-10-10T11:52:58.779110: step 2051, loss 0.177172, acc 0.96875, learning_rate 0.000101119
2017-10-10T11:52:58.999760: step 2052, loss 0.1053, acc 0.953125, learning_rate 0.000101114
2017-10-10T11:52:59.143277: step 2053, loss 0.149662, acc 0.921875, learning_rate 0.00010111
2017-10-10T11:52:59.360872: step 2054, loss 0.161536, acc 0.953125, learning_rate 0.000101105
2017-10-10T11:52:59.528836: step 2055, loss 0.21036, acc 0.921875, learning_rate 0.000101101
2017-10-10T11:52:59.722506: step 2056, loss 0.129056, acc 0.953125, learning_rate 0.000101096
2017-10-10T11:52:59.890423: step 2057, loss 0.0916862, acc 0.96875, learning_rate 0.000101092
2017-10-10T11:53:00.076827: step 2058, loss 0.181059, acc 0.901961, learning_rate 0.000101087
2017-10-10T11:53:00.224406: step 2059, loss 0.142841, acc 0.953125, learning_rate 0.000101083
2017-10-10T11:53:00.390881: step 2060, loss 0.0510336, acc 1, learning_rate 0.000101078
2017-10-10T11:53:00.597484: step 2061, loss 0.267337, acc 0.921875, learning_rate 0.000101074
2017-10-10T11:53:00.745083: step 2062, loss 0.248216, acc 0.90625, learning_rate 0.00010107
2017-10-10T11:53:00.936370: step 2063, loss 0.16328, acc 0.96875, learning_rate 0.000101065
2017-10-10T11:53:01.122118: step 2064, loss 0.0820413, acc 0.96875, learning_rate 0.000101061
2017-10-10T11:53:01.313869: step 2065, loss 0.179928, acc 0.921875, learning_rate 0.000101057
2017-10-10T11:53:01.453010: step 2066, loss 0.190271, acc 0.953125, learning_rate 0.000101052
2017-10-10T11:53:01.623494: step 2067, loss 0.241526, acc 0.875, learning_rate 0.000101048
2017-10-10T11:53:01.826050: step 2068, loss 0.0709316, acc 0.984375, learning_rate 0.000101044
2017-10-10T11:53:02.004856: step 2069, loss 0.0974763, acc 1, learning_rate 0.000101039
2017-10-10T11:53:02.181068: step 2070, loss 0.272133, acc 0.859375, learning_rate 0.000101035
2017-10-10T11:53:02.332864: step 2071, loss 0.23186, acc 0.9375, learning_rate 0.000101031
2017-10-10T11:53:02.527189: step 2072, loss 0.205088, acc 0.9375, learning_rate 0.000101027
2017-10-10T11:53:02.733458: step 2073, loss 0.0976175, acc 0.96875, learning_rate 0.000101023
2017-10-10T11:53:02.856065: step 2074, loss 0.188766, acc 0.9375, learning_rate 0.000101018
2017-10-10T11:53:03.041119: step 2075, loss 0.228377, acc 0.953125, learning_rate 0.000101014
2017-10-10T11:53:03.240688: step 2076, loss 0.147165, acc 0.953125, learning_rate 0.00010101
2017-10-10T11:53:03.400992: step 2077, loss 0.281098, acc 0.921875, learning_rate 0.000101006
2017-10-10T11:53:03.544004: step 2078, loss 0.249606, acc 0.921875, learning_rate 0.000101002
2017-10-10T11:53:03.762466: step 2079, loss 0.197067, acc 0.9375, learning_rate 0.000100998
2017-10-10T11:53:03.979272: step 2080, loss 0.210187, acc 0.9375, learning_rate 0.000100994

Evaluation:
2017-10-10T11:53:04.373103: step 2080, loss 0.233016, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2080

2017-10-10T11:53:05.367734: step 2081, loss 0.221299, acc 0.9375, learning_rate 0.00010099
2017-10-10T11:53:05.537493: step 2082, loss 0.224709, acc 0.875, learning_rate 0.000100986
2017-10-10T11:53:05.712931: step 2083, loss 0.208629, acc 0.921875, learning_rate 0.000100982
2017-10-10T11:53:05.911085: step 2084, loss 0.0683526, acc 0.984375, learning_rate 0.000100978
2017-10-10T11:53:06.106654: step 2085, loss 0.146274, acc 0.9375, learning_rate 0.000100974
2017-10-10T11:53:06.328976: step 2086, loss 0.164935, acc 0.9375, learning_rate 0.00010097
2017-10-10T11:53:06.560991: step 2087, loss 0.154996, acc 0.921875, learning_rate 0.000100966
2017-10-10T11:53:06.700853: step 2088, loss 0.162358, acc 0.953125, learning_rate 0.000100962
2017-10-10T11:53:06.860174: step 2089, loss 0.146544, acc 0.96875, learning_rate 0.000100958
2017-10-10T11:53:07.002052: step 2090, loss 0.206105, acc 0.953125, learning_rate 0.000100954
2017-10-10T11:53:07.152824: step 2091, loss 0.274303, acc 0.875, learning_rate 0.00010095
2017-10-10T11:53:07.274873: step 2092, loss 0.190006, acc 0.9375, learning_rate 0.000100946
2017-10-10T11:53:07.416741: step 2093, loss 0.109096, acc 0.953125, learning_rate 0.000100942
2017-10-10T11:53:07.600861: step 2094, loss 0.141701, acc 0.96875, learning_rate 0.000100938
2017-10-10T11:53:07.780590: step 2095, loss 0.222957, acc 0.90625, learning_rate 0.000100935
2017-10-10T11:53:07.941110: step 2096, loss 0.115007, acc 0.953125, learning_rate 0.000100931
2017-10-10T11:53:08.146091: step 2097, loss 0.15362, acc 0.9375, learning_rate 0.000100927
2017-10-10T11:53:08.289194: step 2098, loss 0.203524, acc 0.9375, learning_rate 0.000100923
2017-10-10T11:53:08.520844: step 2099, loss 0.183739, acc 0.953125, learning_rate 0.000100919
2017-10-10T11:53:08.760341: step 2100, loss 0.261699, acc 0.921875, learning_rate 0.000100916
2017-10-10T11:53:08.911462: step 2101, loss 0.167773, acc 0.921875, learning_rate 0.000100912
2017-10-10T11:53:09.032867: step 2102, loss 0.0618827, acc 0.984375, learning_rate 0.000100908
2017-10-10T11:53:09.164857: step 2103, loss 0.0930999, acc 0.96875, learning_rate 0.000100904
2017-10-10T11:53:09.313079: step 2104, loss 0.202217, acc 0.890625, learning_rate 0.000100901
2017-10-10T11:53:09.465650: step 2105, loss 0.191263, acc 0.9375, learning_rate 0.000100897
2017-10-10T11:53:09.642272: step 2106, loss 0.141479, acc 0.953125, learning_rate 0.000100893
2017-10-10T11:53:09.815904: step 2107, loss 0.165235, acc 0.90625, learning_rate 0.00010089
2017-10-10T11:53:09.995417: step 2108, loss 0.186109, acc 0.953125, learning_rate 0.000100886
2017-10-10T11:53:10.157040: step 2109, loss 0.123724, acc 0.953125, learning_rate 0.000100883
2017-10-10T11:53:10.349152: step 2110, loss 0.161234, acc 0.9375, learning_rate 0.000100879
2017-10-10T11:53:10.540819: step 2111, loss 0.257668, acc 0.921875, learning_rate 0.000100875
2017-10-10T11:53:10.756847: step 2112, loss 0.236839, acc 0.921875, learning_rate 0.000100872
2017-10-10T11:53:10.908794: step 2113, loss 0.215853, acc 0.921875, learning_rate 0.000100868
2017-10-10T11:53:11.065842: step 2114, loss 0.230906, acc 0.90625, learning_rate 0.000100865
2017-10-10T11:53:11.259567: step 2115, loss 0.146386, acc 0.953125, learning_rate 0.000100861
2017-10-10T11:53:11.410269: step 2116, loss 0.0635944, acc 0.984375, learning_rate 0.000100858
2017-10-10T11:53:11.572853: step 2117, loss 0.114542, acc 0.96875, learning_rate 0.000100854
2017-10-10T11:53:11.772365: step 2118, loss 0.113492, acc 0.96875, learning_rate 0.000100851
2017-10-10T11:53:11.965858: step 2119, loss 0.269694, acc 0.90625, learning_rate 0.000100847
2017-10-10T11:53:12.118426: step 2120, loss 0.169144, acc 0.9375, learning_rate 0.000100844

Evaluation:
2017-10-10T11:53:12.470800: step 2120, loss 0.234405, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2120

2017-10-10T11:53:13.606287: step 2121, loss 0.110085, acc 0.96875, learning_rate 0.00010084
2017-10-10T11:53:13.753352: step 2122, loss 0.194984, acc 0.953125, learning_rate 0.000100837
2017-10-10T11:53:13.928615: step 2123, loss 0.110784, acc 0.9375, learning_rate 0.000100833
2017-10-10T11:53:14.131041: step 2124, loss 0.178364, acc 0.921875, learning_rate 0.00010083
2017-10-10T11:53:14.329308: step 2125, loss 0.0942519, acc 0.984375, learning_rate 0.000100827
2017-10-10T11:53:14.492932: step 2126, loss 0.159315, acc 0.9375, learning_rate 0.000100823
2017-10-10T11:53:14.659775: step 2127, loss 0.139687, acc 0.953125, learning_rate 0.00010082
2017-10-10T11:53:14.870772: step 2128, loss 0.132621, acc 0.953125, learning_rate 0.000100817
2017-10-10T11:53:15.091512: step 2129, loss 0.149802, acc 0.96875, learning_rate 0.000100813
2017-10-10T11:53:15.282263: step 2130, loss 0.207642, acc 0.90625, learning_rate 0.00010081
2017-10-10T11:53:15.414323: step 2131, loss 0.191784, acc 0.9375, learning_rate 0.000100807
2017-10-10T11:53:15.610831: step 2132, loss 0.151841, acc 0.96875, learning_rate 0.000100803
2017-10-10T11:53:15.830065: step 2133, loss 0.194138, acc 0.9375, learning_rate 0.0001008
2017-10-10T11:53:16.020770: step 2134, loss 0.110127, acc 0.96875, learning_rate 0.000100797
2017-10-10T11:53:16.160563: step 2135, loss 0.141767, acc 0.96875, learning_rate 0.000100793
2017-10-10T11:53:16.356797: step 2136, loss 0.146932, acc 0.96875, learning_rate 0.00010079
2017-10-10T11:53:16.574651: step 2137, loss 0.281991, acc 0.90625, learning_rate 0.000100787
2017-10-10T11:53:16.760794: step 2138, loss 0.0807327, acc 0.984375, learning_rate 0.000100784
2017-10-10T11:53:17.017286: step 2139, loss 0.211682, acc 0.921875, learning_rate 0.000100781
2017-10-10T11:53:17.173182: step 2140, loss 0.138103, acc 0.953125, learning_rate 0.000100777
2017-10-10T11:53:17.331728: step 2141, loss 0.113063, acc 0.953125, learning_rate 0.000100774
2017-10-10T11:53:17.464843: step 2142, loss 0.149531, acc 0.9375, learning_rate 0.000100771
2017-10-10T11:53:17.592862: step 2143, loss 0.18472, acc 0.9375, learning_rate 0.000100768
2017-10-10T11:53:17.730937: step 2144, loss 0.0942783, acc 0.984375, learning_rate 0.000100765
2017-10-10T11:53:17.868854: step 2145, loss 0.18181, acc 0.90625, learning_rate 0.000100762
2017-10-10T11:53:17.997837: step 2146, loss 0.121029, acc 0.984375, learning_rate 0.000100759
2017-10-10T11:53:18.173674: step 2147, loss 0.195985, acc 0.9375, learning_rate 0.000100755
2017-10-10T11:53:18.383353: step 2148, loss 0.162546, acc 0.96875, learning_rate 0.000100752
2017-10-10T11:53:18.575489: step 2149, loss 0.269722, acc 0.921875, learning_rate 0.000100749
2017-10-10T11:53:18.748895: step 2150, loss 0.114315, acc 0.96875, learning_rate 0.000100746
2017-10-10T11:53:18.920886: step 2151, loss 0.184579, acc 0.921875, learning_rate 0.000100743
2017-10-10T11:53:19.140844: step 2152, loss 0.267208, acc 0.953125, learning_rate 0.00010074
2017-10-10T11:53:19.412880: step 2153, loss 0.229595, acc 0.90625, learning_rate 0.000100737
2017-10-10T11:53:19.647025: step 2154, loss 0.25784, acc 0.890625, learning_rate 0.000100734
2017-10-10T11:53:19.743466: step 2155, loss 0.169398, acc 0.953125, learning_rate 0.000100731
2017-10-10T11:53:19.874108: step 2156, loss 0.21466, acc 0.921569, learning_rate 0.000100728
2017-10-10T11:53:20.018277: step 2157, loss 0.121142, acc 0.953125, learning_rate 0.000100725
2017-10-10T11:53:20.168832: step 2158, loss 0.18116, acc 0.921875, learning_rate 0.000100722
2017-10-10T11:53:20.275392: step 2159, loss 0.189387, acc 0.890625, learning_rate 0.000100719
2017-10-10T11:53:20.415568: step 2160, loss 0.122726, acc 0.984375, learning_rate 0.000100716

Evaluation:
2017-10-10T11:53:20.780898: step 2160, loss 0.234511, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2160

2017-10-10T11:53:22.080988: step 2161, loss 0.172068, acc 0.953125, learning_rate 0.000100713
2017-10-10T11:53:22.252966: step 2162, loss 0.188071, acc 0.921875, learning_rate 0.000100711
2017-10-10T11:53:22.442355: step 2163, loss 0.0835153, acc 0.96875, learning_rate 0.000100708
2017-10-10T11:53:22.648369: step 2164, loss 0.198073, acc 0.953125, learning_rate 0.000100705
2017-10-10T11:53:22.791146: step 2165, loss 0.182777, acc 0.9375, learning_rate 0.000100702
2017-10-10T11:53:22.946944: step 2166, loss 0.205067, acc 0.9375, learning_rate 0.000100699
2017-10-10T11:53:23.179063: step 2167, loss 0.228362, acc 0.96875, learning_rate 0.000100696
2017-10-10T11:53:23.318231: step 2168, loss 0.151719, acc 0.953125, learning_rate 0.000100693
2017-10-10T11:53:23.487365: step 2169, loss 0.174905, acc 0.921875, learning_rate 0.00010069
2017-10-10T11:53:23.692218: step 2170, loss 0.152287, acc 0.953125, learning_rate 0.000100688
2017-10-10T11:53:23.880880: step 2171, loss 0.185273, acc 0.9375, learning_rate 0.000100685
2017-10-10T11:53:24.021320: step 2172, loss 0.193536, acc 0.9375, learning_rate 0.000100682
2017-10-10T11:53:24.195995: step 2173, loss 0.112502, acc 0.953125, learning_rate 0.000100679
2017-10-10T11:53:24.417529: step 2174, loss 0.213905, acc 0.9375, learning_rate 0.000100677
2017-10-10T11:53:24.620848: step 2175, loss 0.119976, acc 0.953125, learning_rate 0.000100674
2017-10-10T11:53:24.772847: step 2176, loss 0.0983518, acc 0.984375, learning_rate 0.000100671
2017-10-10T11:53:24.943323: step 2177, loss 0.0548909, acc 1, learning_rate 0.000100668
2017-10-10T11:53:25.143217: step 2178, loss 0.235218, acc 0.890625, learning_rate 0.000100666
2017-10-10T11:53:25.324989: step 2179, loss 0.210346, acc 0.90625, learning_rate 0.000100663
2017-10-10T11:53:25.482740: step 2180, loss 0.115794, acc 0.953125, learning_rate 0.00010066
2017-10-10T11:53:25.688920: step 2181, loss 0.36143, acc 0.890625, learning_rate 0.000100657
2017-10-10T11:53:25.893488: step 2182, loss 0.192412, acc 0.9375, learning_rate 0.000100655
2017-10-10T11:53:26.060995: step 2183, loss 0.179629, acc 0.9375, learning_rate 0.000100652
2017-10-10T11:53:26.244449: step 2184, loss 0.197744, acc 0.9375, learning_rate 0.000100649
2017-10-10T11:53:26.445855: step 2185, loss 0.0856642, acc 1, learning_rate 0.000100647
2017-10-10T11:53:26.644890: step 2186, loss 0.175424, acc 0.96875, learning_rate 0.000100644
2017-10-10T11:53:26.816863: step 2187, loss 0.265696, acc 0.9375, learning_rate 0.000100641
2017-10-10T11:53:26.988848: step 2188, loss 0.212657, acc 0.921875, learning_rate 0.000100639
2017-10-10T11:53:27.236927: step 2189, loss 0.127067, acc 0.953125, learning_rate 0.000100636
2017-10-10T11:53:27.426597: step 2190, loss 0.107331, acc 0.96875, learning_rate 0.000100634
2017-10-10T11:53:27.580227: step 2191, loss 0.218279, acc 0.953125, learning_rate 0.000100631
2017-10-10T11:53:27.742424: step 2192, loss 0.274658, acc 0.875, learning_rate 0.000100628
2017-10-10T11:53:27.898332: step 2193, loss 0.209306, acc 0.921875, learning_rate 0.000100626
2017-10-10T11:53:28.057839: step 2194, loss 0.0989222, acc 0.984375, learning_rate 0.000100623
2017-10-10T11:53:28.179037: step 2195, loss 0.098526, acc 0.984375, learning_rate 0.000100621
2017-10-10T11:53:28.296040: step 2196, loss 0.214525, acc 0.9375, learning_rate 0.000100618
2017-10-10T11:53:28.496865: step 2197, loss 0.0865258, acc 1, learning_rate 0.000100616
2017-10-10T11:53:28.657975: step 2198, loss 0.0625549, acc 1, learning_rate 0.000100613
2017-10-10T11:53:28.813102: step 2199, loss 0.10878, acc 0.96875, learning_rate 0.000100611
2017-10-10T11:53:29.028976: step 2200, loss 0.153899, acc 0.921875, learning_rate 0.000100608

Evaluation:
2017-10-10T11:53:29.370032: step 2200, loss 0.233342, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2200

2017-10-10T11:53:30.420840: step 2201, loss 0.147087, acc 0.96875, learning_rate 0.000100606
2017-10-10T11:53:30.649104: step 2202, loss 0.222009, acc 0.9375, learning_rate 0.000100603
2017-10-10T11:53:30.844856: step 2203, loss 0.103923, acc 0.96875, learning_rate 0.000100601
2017-10-10T11:53:30.971432: step 2204, loss 0.0984404, acc 0.96875, learning_rate 0.000100598
2017-10-10T11:53:31.100426: step 2205, loss 0.108051, acc 1, learning_rate 0.000100596
2017-10-10T11:53:31.241225: step 2206, loss 0.134146, acc 0.96875, learning_rate 0.000100594
2017-10-10T11:53:31.384349: step 2207, loss 0.151269, acc 0.96875, learning_rate 0.000100591
2017-10-10T11:53:31.524988: step 2208, loss 0.096504, acc 0.953125, learning_rate 0.000100589
2017-10-10T11:53:31.650906: step 2209, loss 0.273398, acc 0.9375, learning_rate 0.000100586
2017-10-10T11:53:31.840836: step 2210, loss 0.14821, acc 0.96875, learning_rate 0.000100584
2017-10-10T11:53:32.020880: step 2211, loss 0.0954526, acc 0.96875, learning_rate 0.000100581
2017-10-10T11:53:32.171810: step 2212, loss 0.153162, acc 0.953125, learning_rate 0.000100579
2017-10-10T11:53:32.356850: step 2213, loss 0.150443, acc 0.953125, learning_rate 0.000100577
2017-10-10T11:53:32.560832: step 2214, loss 0.0691358, acc 0.984375, learning_rate 0.000100574
2017-10-10T11:53:32.712998: step 2215, loss 0.217653, acc 0.921875, learning_rate 0.000100572
2017-10-10T11:53:32.871011: step 2216, loss 0.235701, acc 0.921875, learning_rate 0.00010057
2017-10-10T11:53:33.061640: step 2217, loss 0.0979557, acc 0.953125, learning_rate 0.000100567
2017-10-10T11:53:33.252906: step 2218, loss 0.248453, acc 0.9375, learning_rate 0.000100565
2017-10-10T11:53:33.418196: step 2219, loss 0.21144, acc 0.9375, learning_rate 0.000100563
2017-10-10T11:53:33.623140: step 2220, loss 0.0808115, acc 1, learning_rate 0.00010056
2017-10-10T11:53:33.792237: step 2221, loss 0.0955881, acc 0.96875, learning_rate 0.000100558
2017-10-10T11:53:33.992855: step 2222, loss 0.188718, acc 0.953125, learning_rate 0.000100556
2017-10-10T11:53:34.178968: step 2223, loss 0.24658, acc 0.921875, learning_rate 0.000100554
2017-10-10T11:53:34.368825: step 2224, loss 0.211289, acc 0.9375, learning_rate 0.000100551
2017-10-10T11:53:34.568829: step 2225, loss 0.17978, acc 0.953125, learning_rate 0.000100549
2017-10-10T11:53:34.765193: step 2226, loss 0.163055, acc 0.9375, learning_rate 0.000100547
2017-10-10T11:53:34.949161: step 2227, loss 0.175425, acc 0.9375, learning_rate 0.000100545
2017-10-10T11:53:35.121347: step 2228, loss 0.133036, acc 0.953125, learning_rate 0.000100542
2017-10-10T11:53:35.312850: step 2229, loss 0.0904173, acc 0.96875, learning_rate 0.00010054
2017-10-10T11:53:35.520431: step 2230, loss 0.140496, acc 0.9375, learning_rate 0.000100538
2017-10-10T11:53:35.702754: step 2231, loss 0.097892, acc 1, learning_rate 0.000100536
2017-10-10T11:53:35.896210: step 2232, loss 0.103923, acc 0.96875, learning_rate 0.000100534
2017-10-10T11:53:36.072809: step 2233, loss 0.189612, acc 0.921875, learning_rate 0.000100531
2017-10-10T11:53:36.245030: step 2234, loss 0.201122, acc 0.921875, learning_rate 0.000100529
2017-10-10T11:53:36.455170: step 2235, loss 0.23609, acc 0.953125, learning_rate 0.000100527
2017-10-10T11:53:36.623499: step 2236, loss 0.223518, acc 0.9375, learning_rate 0.000100525
2017-10-10T11:53:36.780856: step 2237, loss 0.162943, acc 0.9375, learning_rate 0.000100523
2017-10-10T11:53:36.989275: step 2238, loss 0.186614, acc 0.953125, learning_rate 0.000100521
2017-10-10T11:53:37.180853: step 2239, loss 0.194568, acc 0.90625, learning_rate 0.000100519
2017-10-10T11:53:37.316853: step 2240, loss 0.142223, acc 0.9375, learning_rate 0.000100516

Evaluation:
2017-10-10T11:53:37.684832: step 2240, loss 0.232492, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2240

2017-10-10T11:53:38.716819: step 2241, loss 0.19345, acc 0.9375, learning_rate 0.000100514
2017-10-10T11:53:38.887195: step 2242, loss 0.151865, acc 0.953125, learning_rate 0.000100512
2017-10-10T11:53:39.087002: step 2243, loss 0.138763, acc 0.96875, learning_rate 0.00010051
2017-10-10T11:53:39.290726: step 2244, loss 0.112939, acc 0.96875, learning_rate 0.000100508
2017-10-10T11:53:39.408791: step 2245, loss 0.133424, acc 0.984375, learning_rate 0.000100506
2017-10-10T11:53:39.600283: step 2246, loss 0.208312, acc 0.9375, learning_rate 0.000100504
2017-10-10T11:53:39.801262: step 2247, loss 0.236752, acc 0.890625, learning_rate 0.000100502
2017-10-10T11:53:39.969013: step 2248, loss 0.190408, acc 0.921875, learning_rate 0.0001005
2017-10-10T11:53:40.126582: step 2249, loss 0.167579, acc 0.96875, learning_rate 0.000100498
2017-10-10T11:53:40.341173: step 2250, loss 0.232329, acc 0.90625, learning_rate 0.000100496
2017-10-10T11:53:40.476450: step 2251, loss 0.195218, acc 0.890625, learning_rate 0.000100494
2017-10-10T11:53:40.645052: step 2252, loss 0.185518, acc 0.921875, learning_rate 0.000100492
2017-10-10T11:53:40.848949: step 2253, loss 0.15967, acc 0.9375, learning_rate 0.00010049
2017-10-10T11:53:41.004521: step 2254, loss 0.11619, acc 0.980392, learning_rate 0.000100488
2017-10-10T11:53:41.169157: step 2255, loss 0.161931, acc 0.96875, learning_rate 0.000100486
2017-10-10T11:53:41.406916: step 2256, loss 0.214364, acc 0.90625, learning_rate 0.000100484
2017-10-10T11:53:41.624983: step 2257, loss 0.275088, acc 0.90625, learning_rate 0.000100482
2017-10-10T11:53:41.793326: step 2258, loss 0.0969821, acc 0.984375, learning_rate 0.00010048
2017-10-10T11:53:41.951092: step 2259, loss 0.102282, acc 0.984375, learning_rate 0.000100478
2017-10-10T11:53:42.100768: step 2260, loss 0.104172, acc 1, learning_rate 0.000100476
2017-10-10T11:53:42.252832: step 2261, loss 0.0999597, acc 0.96875, learning_rate 0.000100474
2017-10-10T11:53:42.388832: step 2262, loss 0.160026, acc 0.9375, learning_rate 0.000100472
2017-10-10T11:53:42.529510: step 2263, loss 0.12812, acc 0.96875, learning_rate 0.00010047
2017-10-10T11:53:42.657788: step 2264, loss 0.0899568, acc 0.984375, learning_rate 0.000100468
2017-10-10T11:53:42.812358: step 2265, loss 0.216232, acc 0.90625, learning_rate 0.000100466
2017-10-10T11:53:43.018636: step 2266, loss 0.1471, acc 0.953125, learning_rate 0.000100464
2017-10-10T11:53:43.202923: step 2267, loss 0.188668, acc 0.9375, learning_rate 0.000100462
2017-10-10T11:53:43.341243: step 2268, loss 0.146719, acc 0.953125, learning_rate 0.000100461
2017-10-10T11:53:43.516571: step 2269, loss 0.154166, acc 0.9375, learning_rate 0.000100459
2017-10-10T11:53:43.735237: step 2270, loss 0.160872, acc 0.9375, learning_rate 0.000100457
2017-10-10T11:53:43.952838: step 2271, loss 0.162441, acc 0.90625, learning_rate 0.000100455
2017-10-10T11:53:44.129703: step 2272, loss 0.127416, acc 0.953125, learning_rate 0.000100453
2017-10-10T11:53:44.278426: step 2273, loss 0.206943, acc 0.921875, learning_rate 0.000100451
2017-10-10T11:53:44.472488: step 2274, loss 0.158001, acc 0.921875, learning_rate 0.000100449
2017-10-10T11:53:44.636903: step 2275, loss 0.130629, acc 0.953125, learning_rate 0.000100448
2017-10-10T11:53:44.776857: step 2276, loss 0.154267, acc 0.921875, learning_rate 0.000100446
2017-10-10T11:53:44.993916: step 2277, loss 0.213457, acc 0.90625, learning_rate 0.000100444
2017-10-10T11:53:45.204487: step 2278, loss 0.128293, acc 0.984375, learning_rate 0.000100442
2017-10-10T11:53:45.364095: step 2279, loss 0.161509, acc 0.953125, learning_rate 0.00010044
2017-10-10T11:53:45.568862: step 2280, loss 0.139212, acc 0.953125, learning_rate 0.000100439

Evaluation:
2017-10-10T11:53:45.921086: step 2280, loss 0.232118, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2280

2017-10-10T11:53:47.321708: step 2281, loss 0.325586, acc 0.90625, learning_rate 0.000100437
2017-10-10T11:53:47.530437: step 2282, loss 0.20778, acc 0.953125, learning_rate 0.000100435
2017-10-10T11:53:47.688864: step 2283, loss 0.118781, acc 0.984375, learning_rate 0.000100433
2017-10-10T11:53:47.905008: step 2284, loss 0.161676, acc 0.953125, learning_rate 0.000100431
2017-10-10T11:53:48.138283: step 2285, loss 0.270428, acc 0.890625, learning_rate 0.00010043
2017-10-10T11:53:48.314510: step 2286, loss 0.14874, acc 0.96875, learning_rate 0.000100428
2017-10-10T11:53:48.449941: step 2287, loss 0.140781, acc 0.984375, learning_rate 0.000100426
2017-10-10T11:53:48.580102: step 2288, loss 0.115001, acc 0.984375, learning_rate 0.000100424
2017-10-10T11:53:48.712756: step 2289, loss 0.187128, acc 0.9375, learning_rate 0.000100423
2017-10-10T11:53:48.871224: step 2290, loss 0.159315, acc 0.9375, learning_rate 0.000100421
2017-10-10T11:53:49.024860: step 2291, loss 0.136911, acc 0.96875, learning_rate 0.000100419
2017-10-10T11:53:49.184149: step 2292, loss 0.100843, acc 0.984375, learning_rate 0.000100418
2017-10-10T11:53:49.384608: step 2293, loss 0.153651, acc 0.9375, learning_rate 0.000100416
2017-10-10T11:53:49.599061: step 2294, loss 0.130677, acc 0.96875, learning_rate 0.000100414
2017-10-10T11:53:49.763099: step 2295, loss 0.102823, acc 0.96875, learning_rate 0.000100412
2017-10-10T11:53:49.919681: step 2296, loss 0.0919964, acc 0.984375, learning_rate 0.000100411
2017-10-10T11:53:50.131250: step 2297, loss 0.13241, acc 0.953125, learning_rate 0.000100409
2017-10-10T11:53:50.308853: step 2298, loss 0.208788, acc 0.90625, learning_rate 0.000100407
2017-10-10T11:53:50.501835: step 2299, loss 0.240265, acc 0.90625, learning_rate 0.000100406
2017-10-10T11:53:50.697736: step 2300, loss 0.202964, acc 0.9375, learning_rate 0.000100404
2017-10-10T11:53:50.878222: step 2301, loss 0.238832, acc 0.921875, learning_rate 0.000100402
2017-10-10T11:53:51.004921: step 2302, loss 0.185556, acc 0.9375, learning_rate 0.000100401
2017-10-10T11:53:51.215828: step 2303, loss 0.0940147, acc 0.96875, learning_rate 0.000100399
2017-10-10T11:53:51.444712: step 2304, loss 0.0835429, acc 0.984375, learning_rate 0.000100398
2017-10-10T11:53:51.627347: step 2305, loss 0.107063, acc 0.96875, learning_rate 0.000100396
2017-10-10T11:53:51.747898: step 2306, loss 0.177281, acc 0.953125, learning_rate 0.000100394
2017-10-10T11:53:51.950506: step 2307, loss 0.176414, acc 0.9375, learning_rate 0.000100393
2017-10-10T11:53:52.141318: step 2308, loss 0.162193, acc 0.96875, learning_rate 0.000100391
2017-10-10T11:53:52.291238: step 2309, loss 0.162935, acc 0.96875, learning_rate 0.000100389
2017-10-10T11:53:52.451678: step 2310, loss 0.161682, acc 0.90625, learning_rate 0.000100388
2017-10-10T11:53:52.684877: step 2311, loss 0.144064, acc 0.953125, learning_rate 0.000100386
2017-10-10T11:53:52.897003: step 2312, loss 0.0692617, acc 0.984375, learning_rate 0.000100385
2017-10-10T11:53:53.063226: step 2313, loss 0.175465, acc 0.9375, learning_rate 0.000100383
2017-10-10T11:53:53.212304: step 2314, loss 0.149491, acc 0.9375, learning_rate 0.000100382
2017-10-10T11:53:53.352826: step 2315, loss 0.205748, acc 0.953125, learning_rate 0.00010038
2017-10-10T11:53:53.481031: step 2316, loss 0.187086, acc 0.96875, learning_rate 0.000100378
2017-10-10T11:53:53.600622: step 2317, loss 0.0504993, acc 1, learning_rate 0.000100377
2017-10-10T11:53:53.753802: step 2318, loss 0.146693, acc 0.984375, learning_rate 0.000100375
2017-10-10T11:53:53.928846: step 2319, loss 0.152886, acc 0.96875, learning_rate 0.000100374
2017-10-10T11:53:54.067821: step 2320, loss 0.220327, acc 0.953125, learning_rate 0.000100372

Evaluation:
2017-10-10T11:53:54.484961: step 2320, loss 0.232998, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2320

2017-10-10T11:53:55.451609: step 2321, loss 0.150899, acc 0.953125, learning_rate 0.000100371
2017-10-10T11:53:55.624577: step 2322, loss 0.123388, acc 0.984375, learning_rate 0.000100369
2017-10-10T11:53:55.816034: step 2323, loss 0.11025, acc 0.96875, learning_rate 0.000100368
2017-10-10T11:53:55.965891: step 2324, loss 0.127146, acc 0.953125, learning_rate 0.000100366
2017-10-10T11:53:56.143710: step 2325, loss 0.17204, acc 0.9375, learning_rate 0.000100365
2017-10-10T11:53:56.337975: step 2326, loss 0.11666, acc 0.96875, learning_rate 0.000100363
2017-10-10T11:53:56.492942: step 2327, loss 0.125642, acc 0.9375, learning_rate 0.000100362
2017-10-10T11:53:56.696881: step 2328, loss 0.110758, acc 0.96875, learning_rate 0.00010036
2017-10-10T11:53:56.910506: step 2329, loss 0.157201, acc 0.953125, learning_rate 0.000100359
2017-10-10T11:53:57.115769: step 2330, loss 0.171258, acc 0.953125, learning_rate 0.000100357
2017-10-10T11:53:57.324840: step 2331, loss 0.132628, acc 0.9375, learning_rate 0.000100356
2017-10-10T11:53:57.500376: step 2332, loss 0.100425, acc 0.984375, learning_rate 0.000100354
2017-10-10T11:53:57.643381: step 2333, loss 0.192795, acc 0.96875, learning_rate 0.000100353
2017-10-10T11:53:57.856829: step 2334, loss 0.225454, acc 0.890625, learning_rate 0.000100352
2017-10-10T11:53:58.043845: step 2335, loss 0.170691, acc 0.9375, learning_rate 0.00010035
2017-10-10T11:53:58.244869: step 2336, loss 0.321939, acc 0.90625, learning_rate 0.000100349
2017-10-10T11:53:58.412868: step 2337, loss 0.232501, acc 0.921875, learning_rate 0.000100347
2017-10-10T11:53:58.632828: step 2338, loss 0.143671, acc 0.953125, learning_rate 0.000100346
2017-10-10T11:53:58.757553: step 2339, loss 0.135363, acc 0.953125, learning_rate 0.000100344
2017-10-10T11:53:58.877003: step 2340, loss 0.18854, acc 0.953125, learning_rate 0.000100343
2017-10-10T11:53:59.039907: step 2341, loss 0.093069, acc 0.984375, learning_rate 0.000100342
2017-10-10T11:53:59.207098: step 2342, loss 0.0683058, acc 0.984375, learning_rate 0.00010034
2017-10-10T11:53:59.348654: step 2343, loss 0.11253, acc 0.96875, learning_rate 0.000100339
2017-10-10T11:53:59.467958: step 2344, loss 0.168911, acc 0.96875, learning_rate 0.000100338
2017-10-10T11:53:59.677450: step 2345, loss 0.149477, acc 0.953125, learning_rate 0.000100336
2017-10-10T11:53:59.895579: step 2346, loss 0.21954, acc 0.921875, learning_rate 0.000100335
2017-10-10T11:54:00.034718: step 2347, loss 0.194287, acc 0.953125, learning_rate 0.000100333
2017-10-10T11:54:00.194258: step 2348, loss 0.141135, acc 0.96875, learning_rate 0.000100332
2017-10-10T11:54:00.419954: step 2349, loss 0.267626, acc 0.90625, learning_rate 0.000100331
2017-10-10T11:54:00.587802: step 2350, loss 0.132637, acc 0.9375, learning_rate 0.000100329
2017-10-10T11:54:00.751064: step 2351, loss 0.173621, acc 0.9375, learning_rate 0.000100328
2017-10-10T11:54:00.949126: step 2352, loss 0.0969531, acc 0.960784, learning_rate 0.000100327
2017-10-10T11:54:01.159517: step 2353, loss 0.205941, acc 0.90625, learning_rate 0.000100325
2017-10-10T11:54:01.354362: step 2354, loss 0.229372, acc 0.9375, learning_rate 0.000100324
2017-10-10T11:54:01.525081: step 2355, loss 0.119222, acc 0.984375, learning_rate 0.000100323
2017-10-10T11:54:01.719638: step 2356, loss 0.17126, acc 0.96875, learning_rate 0.000100321
2017-10-10T11:54:01.916856: step 2357, loss 0.112261, acc 0.96875, learning_rate 0.00010032
2017-10-10T11:54:02.061241: step 2358, loss 0.170256, acc 0.921875, learning_rate 0.000100319
2017-10-10T11:54:02.232027: step 2359, loss 0.198426, acc 0.953125, learning_rate 0.000100317
2017-10-10T11:54:02.430004: step 2360, loss 0.0886509, acc 0.953125, learning_rate 0.000100316

Evaluation:
2017-10-10T11:54:02.772117: step 2360, loss 0.231829, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2360

2017-10-10T11:54:04.005259: step 2361, loss 0.100222, acc 0.984375, learning_rate 0.000100315
2017-10-10T11:54:04.117220: step 2362, loss 0.229674, acc 0.9375, learning_rate 0.000100314
2017-10-10T11:54:04.228894: step 2363, loss 0.239525, acc 0.875, learning_rate 0.000100312
2017-10-10T11:54:04.372577: step 2364, loss 0.452539, acc 0.859375, learning_rate 0.000100311
2017-10-10T11:54:04.533522: step 2365, loss 0.168093, acc 0.921875, learning_rate 0.00010031
2017-10-10T11:54:04.684816: step 2366, loss 0.136627, acc 0.953125, learning_rate 0.000100308
2017-10-10T11:54:04.795368: step 2367, loss 0.094568, acc 0.96875, learning_rate 0.000100307
2017-10-10T11:54:04.936008: step 2368, loss 0.119981, acc 0.96875, learning_rate 0.000100306
2017-10-10T11:54:05.093774: step 2369, loss 0.0724062, acc 0.984375, learning_rate 0.000100305
2017-10-10T11:54:05.237419: step 2370, loss 0.113899, acc 0.953125, learning_rate 0.000100303
2017-10-10T11:54:05.413929: step 2371, loss 0.126864, acc 0.96875, learning_rate 0.000100302
2017-10-10T11:54:05.613075: step 2372, loss 0.0767717, acc 0.96875, learning_rate 0.000100301
2017-10-10T11:54:05.814417: step 2373, loss 0.155539, acc 0.9375, learning_rate 0.0001003
2017-10-10T11:54:06.002297: step 2374, loss 0.10757, acc 0.953125, learning_rate 0.000100299
2017-10-10T11:54:06.166936: step 2375, loss 0.292562, acc 0.890625, learning_rate 0.000100297
2017-10-10T11:54:06.373467: step 2376, loss 0.111798, acc 0.984375, learning_rate 0.000100296
2017-10-10T11:54:06.541041: step 2377, loss 0.152816, acc 0.953125, learning_rate 0.000100295
2017-10-10T11:54:06.683841: step 2378, loss 0.210219, acc 0.9375, learning_rate 0.000100294
2017-10-10T11:54:06.874960: step 2379, loss 0.231715, acc 0.9375, learning_rate 0.000100292
2017-10-10T11:54:07.028852: step 2380, loss 0.142949, acc 0.96875, learning_rate 0.000100291
2017-10-10T11:54:07.217761: step 2381, loss 0.11963, acc 0.96875, learning_rate 0.00010029
2017-10-10T11:54:07.416884: step 2382, loss 0.144305, acc 0.96875, learning_rate 0.000100289
2017-10-10T11:54:07.619000: step 2383, loss 0.121013, acc 0.96875, learning_rate 0.000100288
2017-10-10T11:54:07.808860: step 2384, loss 0.229993, acc 0.90625, learning_rate 0.000100287
2017-10-10T11:54:07.963354: step 2385, loss 0.119449, acc 0.984375, learning_rate 0.000100285
2017-10-10T11:54:08.124848: step 2386, loss 0.118927, acc 0.9375, learning_rate 0.000100284
2017-10-10T11:54:08.316888: step 2387, loss 0.247155, acc 0.9375, learning_rate 0.000100283
2017-10-10T11:54:08.488943: step 2388, loss 0.149639, acc 0.9375, learning_rate 0.000100282
2017-10-10T11:54:08.757801: step 2389, loss 0.132314, acc 0.9375, learning_rate 0.000100281
2017-10-10T11:54:08.916848: step 2390, loss 0.215984, acc 0.9375, learning_rate 0.00010028
2017-10-10T11:54:09.072958: step 2391, loss 0.119672, acc 0.96875, learning_rate 0.000100278
2017-10-10T11:54:09.219502: step 2392, loss 0.205717, acc 0.90625, learning_rate 0.000100277
2017-10-10T11:54:09.362884: step 2393, loss 0.0964587, acc 0.984375, learning_rate 0.000100276
2017-10-10T11:54:09.484843: step 2394, loss 0.0671382, acc 1, learning_rate 0.000100275
2017-10-10T11:54:09.635296: step 2395, loss 0.216017, acc 0.90625, learning_rate 0.000100274
2017-10-10T11:54:09.793206: step 2396, loss 0.121865, acc 0.96875, learning_rate 0.000100273
2017-10-10T11:54:09.948871: step 2397, loss 0.140341, acc 0.953125, learning_rate 0.000100272
2017-10-10T11:54:10.120885: step 2398, loss 0.137376, acc 0.96875, learning_rate 0.000100271
2017-10-10T11:54:10.324682: step 2399, loss 0.167289, acc 0.953125, learning_rate 0.00010027
2017-10-10T11:54:10.528836: step 2400, loss 0.121338, acc 0.96875, learning_rate 0.000100268

Evaluation:
2017-10-10T11:54:10.877264: step 2400, loss 0.231163, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2400

2017-10-10T11:54:12.025266: step 2401, loss 0.133257, acc 0.96875, learning_rate 0.000100267
2017-10-10T11:54:12.234658: step 2402, loss 0.194459, acc 0.921875, learning_rate 0.000100266
2017-10-10T11:54:12.427906: step 2403, loss 0.178116, acc 0.9375, learning_rate 0.000100265
2017-10-10T11:54:12.585153: step 2404, loss 0.167377, acc 0.9375, learning_rate 0.000100264
2017-10-10T11:54:12.742409: step 2405, loss 0.157084, acc 0.9375, learning_rate 0.000100263
2017-10-10T11:54:12.960688: step 2406, loss 0.245775, acc 0.9375, learning_rate 0.000100262
2017-10-10T11:54:13.103359: step 2407, loss 0.126117, acc 0.953125, learning_rate 0.000100261
2017-10-10T11:54:13.278024: step 2408, loss 0.188484, acc 0.921875, learning_rate 0.00010026
2017-10-10T11:54:13.433900: step 2409, loss 0.123956, acc 0.984375, learning_rate 0.000100259
2017-10-10T11:54:13.612605: step 2410, loss 0.176914, acc 0.90625, learning_rate 0.000100258
2017-10-10T11:54:13.815679: step 2411, loss 0.241777, acc 0.90625, learning_rate 0.000100257
2017-10-10T11:54:13.972051: step 2412, loss 0.164869, acc 0.9375, learning_rate 0.000100256
2017-10-10T11:54:14.144955: step 2413, loss 0.198215, acc 0.921875, learning_rate 0.000100255
2017-10-10T11:54:14.346838: step 2414, loss 0.121644, acc 0.953125, learning_rate 0.000100253
2017-10-10T11:54:14.560845: step 2415, loss 0.146822, acc 0.953125, learning_rate 0.000100252
2017-10-10T11:54:14.701449: step 2416, loss 0.22782, acc 0.921875, learning_rate 0.000100251
2017-10-10T11:54:14.870756: step 2417, loss 0.216827, acc 0.9375, learning_rate 0.00010025
2017-10-10T11:54:15.080975: step 2418, loss 0.0814732, acc 0.96875, learning_rate 0.000100249
2017-10-10T11:54:15.276938: step 2419, loss 0.194912, acc 0.921875, learning_rate 0.000100248
2017-10-10T11:54:15.478365: step 2420, loss 0.205377, acc 0.9375, learning_rate 0.000100247
2017-10-10T11:54:15.636097: step 2421, loss 0.0637536, acc 0.984375, learning_rate 0.000100246
2017-10-10T11:54:15.801019: step 2422, loss 0.180165, acc 0.953125, learning_rate 0.000100245
2017-10-10T11:54:15.924028: step 2423, loss 0.230562, acc 0.90625, learning_rate 0.000100244
2017-10-10T11:54:16.041282: step 2424, loss 0.170308, acc 0.953125, learning_rate 0.000100243
2017-10-10T11:54:16.182621: step 2425, loss 0.19603, acc 0.96875, learning_rate 0.000100242
2017-10-10T11:54:16.323422: step 2426, loss 0.0903863, acc 0.984375, learning_rate 0.000100241
2017-10-10T11:54:16.488439: step 2427, loss 0.261187, acc 0.90625, learning_rate 0.00010024
2017-10-10T11:54:16.639251: step 2428, loss 0.183613, acc 0.9375, learning_rate 0.000100239
2017-10-10T11:54:16.809033: step 2429, loss 0.272806, acc 0.90625, learning_rate 0.000100238
2017-10-10T11:54:16.999425: step 2430, loss 0.104647, acc 0.96875, learning_rate 0.000100237
2017-10-10T11:54:17.191773: step 2431, loss 0.195796, acc 0.953125, learning_rate 0.000100236
2017-10-10T11:54:17.362322: step 2432, loss 0.0902888, acc 0.984375, learning_rate 0.000100235
2017-10-10T11:54:17.580610: step 2433, loss 0.0634875, acc 0.984375, learning_rate 0.000100235
2017-10-10T11:54:17.721327: step 2434, loss 0.149526, acc 0.9375, learning_rate 0.000100234
2017-10-10T11:54:17.893965: step 2435, loss 0.1448, acc 0.9375, learning_rate 0.000100233
2017-10-10T11:54:18.111542: step 2436, loss 0.118657, acc 0.96875, learning_rate 0.000100232
2017-10-10T11:54:18.287209: step 2437, loss 0.130749, acc 0.953125, learning_rate 0.000100231
2017-10-10T11:54:18.418295: step 2438, loss 0.158251, acc 0.921875, learning_rate 0.00010023
2017-10-10T11:54:18.631361: step 2439, loss 0.160026, acc 0.96875, learning_rate 0.000100229
2017-10-10T11:54:18.843624: step 2440, loss 0.215209, acc 0.90625, learning_rate 0.000100228

Evaluation:
2017-10-10T11:54:19.239391: step 2440, loss 0.232544, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2440

2017-10-10T11:54:20.386293: step 2441, loss 0.137389, acc 0.953125, learning_rate 0.000100227
2017-10-10T11:54:20.576971: step 2442, loss 0.214578, acc 0.90625, learning_rate 0.000100226
2017-10-10T11:54:20.771024: step 2443, loss 0.0940643, acc 1, learning_rate 0.000100225
2017-10-10T11:54:20.912175: step 2444, loss 0.249103, acc 0.921875, learning_rate 0.000100224
2017-10-10T11:54:21.077482: step 2445, loss 0.253882, acc 0.90625, learning_rate 0.000100223
2017-10-10T11:54:21.281135: step 2446, loss 0.2611, acc 0.921875, learning_rate 0.000100222
2017-10-10T11:54:21.452760: step 2447, loss 0.109954, acc 0.984375, learning_rate 0.000100221
2017-10-10T11:54:21.602651: step 2448, loss 0.134259, acc 0.953125, learning_rate 0.000100221
2017-10-10T11:54:21.795263: step 2449, loss 0.189078, acc 0.90625, learning_rate 0.00010022
2017-10-10T11:54:21.971771: step 2450, loss 0.179485, acc 0.921569, learning_rate 0.000100219
2017-10-10T11:54:22.161061: step 2451, loss 0.182191, acc 0.921875, learning_rate 0.000100218
2017-10-10T11:54:22.317207: step 2452, loss 0.153525, acc 0.9375, learning_rate 0.000100217
2017-10-10T11:54:22.528160: step 2453, loss 0.229388, acc 0.90625, learning_rate 0.000100216
2017-10-10T11:54:22.696050: step 2454, loss 0.25605, acc 0.9375, learning_rate 0.000100215
2017-10-10T11:54:22.880848: step 2455, loss 0.176724, acc 0.9375, learning_rate 0.000100214
2017-10-10T11:54:23.037152: step 2456, loss 0.0714872, acc 1, learning_rate 0.000100213
2017-10-10T11:54:23.220870: step 2457, loss 0.23019, acc 0.921875, learning_rate 0.000100213
2017-10-10T11:54:23.422264: step 2458, loss 0.0837756, acc 0.953125, learning_rate 0.000100212
2017-10-10T11:54:23.555687: step 2459, loss 0.114109, acc 0.96875, learning_rate 0.000100211
2017-10-10T11:54:23.725723: step 2460, loss 0.210665, acc 0.953125, learning_rate 0.00010021
2017-10-10T11:54:23.928868: step 2461, loss 0.122879, acc 0.953125, learning_rate 0.000100209
2017-10-10T11:54:24.120986: step 2462, loss 0.163105, acc 0.9375, learning_rate 0.000100208
2017-10-10T11:54:24.260524: step 2463, loss 0.143022, acc 0.96875, learning_rate 0.000100207
2017-10-10T11:54:24.440947: step 2464, loss 0.139348, acc 0.953125, learning_rate 0.000100207
2017-10-10T11:54:24.620837: step 2465, loss 0.239459, acc 0.90625, learning_rate 0.000100206
2017-10-10T11:54:24.821016: step 2466, loss 0.148418, acc 0.953125, learning_rate 0.000100205
2017-10-10T11:54:25.022077: step 2467, loss 0.100463, acc 0.984375, learning_rate 0.000100204
2017-10-10T11:54:25.175872: step 2468, loss 0.104668, acc 0.96875, learning_rate 0.000100203
2017-10-10T11:54:25.358357: step 2469, loss 0.13192, acc 0.9375, learning_rate 0.000100202
2017-10-10T11:54:25.537480: step 2470, loss 0.29046, acc 0.890625, learning_rate 0.000100202
2017-10-10T11:54:25.684984: step 2471, loss 0.149971, acc 0.953125, learning_rate 0.000100201
2017-10-10T11:54:25.859544: step 2472, loss 0.112642, acc 0.96875, learning_rate 0.0001002
2017-10-10T11:54:26.059648: step 2473, loss 0.200313, acc 0.96875, learning_rate 0.000100199
2017-10-10T11:54:26.276974: step 2474, loss 0.16701, acc 0.953125, learning_rate 0.000100198
2017-10-10T11:54:26.504283: step 2475, loss 0.177054, acc 0.953125, learning_rate 0.000100198
2017-10-10T11:54:26.669039: step 2476, loss 0.194778, acc 0.96875, learning_rate 0.000100197
2017-10-10T11:54:26.811774: step 2477, loss 0.130047, acc 0.96875, learning_rate 0.000100196
2017-10-10T11:54:26.963772: step 2478, loss 0.174064, acc 0.921875, learning_rate 0.000100195
2017-10-10T11:54:27.112826: step 2479, loss 0.109647, acc 0.953125, learning_rate 0.000100194
2017-10-10T11:54:27.216903: step 2480, loss 0.0961232, acc 0.96875, learning_rate 0.000100194

Evaluation:
2017-10-10T11:54:27.566247: step 2480, loss 0.230564, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2480

2017-10-10T11:54:28.565122: step 2481, loss 0.124023, acc 0.96875, learning_rate 0.000100193
2017-10-10T11:54:28.742905: step 2482, loss 0.144691, acc 0.953125, learning_rate 0.000100192
2017-10-10T11:54:28.886374: step 2483, loss 0.21099, acc 0.921875, learning_rate 0.000100191
2017-10-10T11:54:29.059159: step 2484, loss 0.111438, acc 0.953125, learning_rate 0.00010019
2017-10-10T11:54:29.267405: step 2485, loss 0.234689, acc 0.9375, learning_rate 0.00010019
2017-10-10T11:54:29.468865: step 2486, loss 0.10309, acc 1, learning_rate 0.000100189
2017-10-10T11:54:29.699038: step 2487, loss 0.169742, acc 0.9375, learning_rate 0.000100188
2017-10-10T11:54:29.864825: step 2488, loss 0.170384, acc 0.953125, learning_rate 0.000100187
2017-10-10T11:54:30.025704: step 2489, loss 0.120691, acc 0.96875, learning_rate 0.000100187
2017-10-10T11:54:30.190420: step 2490, loss 0.245406, acc 0.90625, learning_rate 0.000100186
2017-10-10T11:54:30.325292: step 2491, loss 0.13948, acc 0.9375, learning_rate 0.000100185
2017-10-10T11:54:30.443162: step 2492, loss 0.161196, acc 0.9375, learning_rate 0.000100184
2017-10-10T11:54:30.594422: step 2493, loss 0.0858048, acc 1, learning_rate 0.000100183
2017-10-10T11:54:30.754810: step 2494, loss 0.176879, acc 0.921875, learning_rate 0.000100183
2017-10-10T11:54:30.942738: step 2495, loss 0.161603, acc 0.96875, learning_rate 0.000100182
2017-10-10T11:54:31.086888: step 2496, loss 0.0983581, acc 0.984375, learning_rate 0.000100181
2017-10-10T11:54:31.285376: step 2497, loss 0.11939, acc 0.9375, learning_rate 0.000100181
2017-10-10T11:54:31.482648: step 2498, loss 0.241395, acc 0.921875, learning_rate 0.00010018
2017-10-10T11:54:31.675091: step 2499, loss 0.182224, acc 0.921875, learning_rate 0.000100179
2017-10-10T11:54:31.813455: step 2500, loss 0.150081, acc 0.9375, learning_rate 0.000100178
2017-10-10T11:54:32.014041: step 2501, loss 0.287303, acc 0.875, learning_rate 0.000100178
2017-10-10T11:54:32.195723: step 2502, loss 0.22747, acc 0.921875, learning_rate 0.000100177
2017-10-10T11:54:32.352894: step 2503, loss 0.14327, acc 0.953125, learning_rate 0.000100176
2017-10-10T11:54:32.552478: step 2504, loss 0.149266, acc 0.96875, learning_rate 0.000100175
2017-10-10T11:54:32.744858: step 2505, loss 0.103945, acc 0.96875, learning_rate 0.000100175
2017-10-10T11:54:32.898777: step 2506, loss 0.101011, acc 0.96875, learning_rate 0.000100174
2017-10-10T11:54:33.080001: step 2507, loss 0.157455, acc 0.953125, learning_rate 0.000100173
2017-10-10T11:54:33.275177: step 2508, loss 0.223445, acc 0.921875, learning_rate 0.000100173
2017-10-10T11:54:33.464837: step 2509, loss 0.24713, acc 0.90625, learning_rate 0.000100172
2017-10-10T11:54:33.616892: step 2510, loss 0.0971378, acc 0.96875, learning_rate 0.000100171
2017-10-10T11:54:33.808987: step 2511, loss 0.182756, acc 0.9375, learning_rate 0.00010017
2017-10-10T11:54:34.013524: step 2512, loss 0.110321, acc 0.96875, learning_rate 0.00010017
2017-10-10T11:54:34.206824: step 2513, loss 0.146835, acc 0.984375, learning_rate 0.000100169
2017-10-10T11:54:34.339655: step 2514, loss 0.113856, acc 0.96875, learning_rate 0.000100168
2017-10-10T11:54:34.548836: step 2515, loss 0.083674, acc 0.984375, learning_rate 0.000100168
2017-10-10T11:54:34.769539: step 2516, loss 0.163303, acc 0.921875, learning_rate 0.000100167
2017-10-10T11:54:34.943946: step 2517, loss 0.226542, acc 0.921875, learning_rate 0.000100166
2017-10-10T11:54:35.112886: step 2518, loss 0.0659725, acc 1, learning_rate 0.000100166
2017-10-10T11:54:35.303856: step 2519, loss 0.132868, acc 0.96875, learning_rate 0.000100165
2017-10-10T11:54:35.488135: step 2520, loss 0.12311, acc 0.96875, learning_rate 0.000100164

Evaluation:
2017-10-10T11:54:35.831453: step 2520, loss 0.230708, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2520

2017-10-10T11:54:36.972629: step 2521, loss 0.112006, acc 0.984375, learning_rate 0.000100164
2017-10-10T11:54:37.188972: step 2522, loss 0.239396, acc 0.90625, learning_rate 0.000100163
2017-10-10T11:54:37.412514: step 2523, loss 0.105317, acc 0.96875, learning_rate 0.000100162
2017-10-10T11:54:37.546527: step 2524, loss 0.10378, acc 0.96875, learning_rate 0.000100162
2017-10-10T11:54:37.694468: step 2525, loss 0.178578, acc 0.953125, learning_rate 0.000100161
2017-10-10T11:54:37.816911: step 2526, loss 0.0911462, acc 0.984375, learning_rate 0.00010016
2017-10-10T11:54:37.944970: step 2527, loss 0.193976, acc 0.9375, learning_rate 0.00010016
2017-10-10T11:54:38.085967: step 2528, loss 0.11304, acc 0.96875, learning_rate 0.000100159
2017-10-10T11:54:38.235344: step 2529, loss 0.234071, acc 0.890625, learning_rate 0.000100158
2017-10-10T11:54:38.359055: step 2530, loss 0.154589, acc 0.953125, learning_rate 0.000100158
2017-10-10T11:54:38.567944: step 2531, loss 0.17268, acc 0.953125, learning_rate 0.000100157
2017-10-10T11:54:38.727868: step 2532, loss 0.101249, acc 0.96875, learning_rate 0.000100156
2017-10-10T11:54:38.901759: step 2533, loss 0.161904, acc 0.921875, learning_rate 0.000100156
2017-10-10T11:54:39.103156: step 2534, loss 0.075314, acc 1, learning_rate 0.000100155
2017-10-10T11:54:39.320140: step 2535, loss 0.209996, acc 0.921875, learning_rate 0.000100155
2017-10-10T11:54:39.462846: step 2536, loss 0.156163, acc 0.953125, learning_rate 0.000100154
2017-10-10T11:54:39.629703: step 2537, loss 0.154323, acc 0.953125, learning_rate 0.000100153
2017-10-10T11:54:39.840396: step 2538, loss 0.124815, acc 0.96875, learning_rate 0.000100153
2017-10-10T11:54:40.064843: step 2539, loss 0.108097, acc 0.9375, learning_rate 0.000100152
2017-10-10T11:54:40.295682: step 2540, loss 0.185274, acc 0.921875, learning_rate 0.000100151
2017-10-10T11:54:40.448350: step 2541, loss 0.16637, acc 0.953125, learning_rate 0.000100151
2017-10-10T11:54:40.620722: step 2542, loss 0.12345, acc 0.953125, learning_rate 0.00010015
2017-10-10T11:54:40.780487: step 2543, loss 0.161881, acc 0.953125, learning_rate 0.00010015
2017-10-10T11:54:40.903145: step 2544, loss 0.0957683, acc 0.953125, learning_rate 0.000100149
2017-10-10T11:54:41.030209: step 2545, loss 0.167513, acc 0.9375, learning_rate 0.000100148
2017-10-10T11:54:41.179831: step 2546, loss 0.279959, acc 0.890625, learning_rate 0.000100148
2017-10-10T11:54:41.340831: step 2547, loss 0.123652, acc 0.9375, learning_rate 0.000100147
2017-10-10T11:54:41.431711: step 2548, loss 0.0960982, acc 0.980392, learning_rate 0.000100147
2017-10-10T11:54:41.635044: step 2549, loss 0.183436, acc 0.9375, learning_rate 0.000100146
2017-10-10T11:54:41.831430: step 2550, loss 0.245286, acc 0.9375, learning_rate 0.000100145
2017-10-10T11:54:41.984253: step 2551, loss 0.189881, acc 0.9375, learning_rate 0.000100145
2017-10-10T11:54:42.148463: step 2552, loss 0.236678, acc 0.921875, learning_rate 0.000100144
2017-10-10T11:54:42.339847: step 2553, loss 0.157675, acc 0.953125, learning_rate 0.000100144
2017-10-10T11:54:42.498771: step 2554, loss 0.228637, acc 0.90625, learning_rate 0.000100143
2017-10-10T11:54:42.654116: step 2555, loss 0.112315, acc 0.96875, learning_rate 0.000100142
2017-10-10T11:54:42.863991: step 2556, loss 0.137467, acc 0.953125, learning_rate 0.000100142
2017-10-10T11:54:43.067134: step 2557, loss 0.196422, acc 0.921875, learning_rate 0.000100141
2017-10-10T11:54:43.244083: step 2558, loss 0.0837292, acc 0.96875, learning_rate 0.000100141
2017-10-10T11:54:43.416943: step 2559, loss 0.100261, acc 0.96875, learning_rate 0.00010014
2017-10-10T11:54:43.617731: step 2560, loss 0.127166, acc 0.9375, learning_rate 0.00010014

Evaluation:
2017-10-10T11:54:43.976891: step 2560, loss 0.231641, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2560

2017-10-10T11:54:45.254268: step 2561, loss 0.125458, acc 0.96875, learning_rate 0.000100139
2017-10-10T11:54:45.465748: step 2562, loss 0.0947132, acc 0.953125, learning_rate 0.000100138
2017-10-10T11:54:45.601242: step 2563, loss 0.127203, acc 0.9375, learning_rate 0.000100138
2017-10-10T11:54:45.774790: step 2564, loss 0.170476, acc 0.984375, learning_rate 0.000100137
2017-10-10T11:54:45.990119: step 2565, loss 0.124748, acc 0.984375, learning_rate 0.000100137
2017-10-10T11:54:46.160317: step 2566, loss 0.28701, acc 0.90625, learning_rate 0.000100136
2017-10-10T11:54:46.338014: step 2567, loss 0.173275, acc 0.9375, learning_rate 0.000100136
2017-10-10T11:54:46.516860: step 2568, loss 0.222088, acc 0.921875, learning_rate 0.000100135
2017-10-10T11:54:46.702434: step 2569, loss 0.247514, acc 0.875, learning_rate 0.000100134
2017-10-10T11:54:46.874800: step 2570, loss 0.125198, acc 0.953125, learning_rate 0.000100134
2017-10-10T11:54:47.036418: step 2571, loss 0.0852911, acc 0.96875, learning_rate 0.000100133
2017-10-10T11:54:47.219603: step 2572, loss 0.137221, acc 0.96875, learning_rate 0.000100133
2017-10-10T11:54:47.381060: step 2573, loss 0.209303, acc 0.90625, learning_rate 0.000100132
2017-10-10T11:54:47.584256: step 2574, loss 0.071316, acc 1, learning_rate 0.000100132
2017-10-10T11:54:47.765933: step 2575, loss 0.150773, acc 0.921875, learning_rate 0.000100131
2017-10-10T11:54:47.952501: step 2576, loss 0.0902137, acc 0.96875, learning_rate 0.000100131
2017-10-10T11:54:48.123802: step 2577, loss 0.159653, acc 0.921875, learning_rate 0.00010013
2017-10-10T11:54:48.337111: step 2578, loss 0.217836, acc 0.90625, learning_rate 0.00010013
2017-10-10T11:54:48.532208: step 2579, loss 0.138686, acc 0.96875, learning_rate 0.000100129
2017-10-10T11:54:48.678231: step 2580, loss 0.117562, acc 0.96875, learning_rate 0.000100129
2017-10-10T11:54:48.822448: step 2581, loss 0.198713, acc 0.890625, learning_rate 0.000100128
2017-10-10T11:54:48.940322: step 2582, loss 0.0585894, acc 1, learning_rate 0.000100128
2017-10-10T11:54:49.059053: step 2583, loss 0.330507, acc 0.90625, learning_rate 0.000100127
2017-10-10T11:54:49.218023: step 2584, loss 0.15492, acc 0.953125, learning_rate 0.000100126
2017-10-10T11:54:49.373016: step 2585, loss 0.214404, acc 0.921875, learning_rate 0.000100126
2017-10-10T11:54:49.502347: step 2586, loss 0.143027, acc 0.953125, learning_rate 0.000100125
2017-10-10T11:54:49.641973: step 2587, loss 0.159373, acc 0.984375, learning_rate 0.000100125
2017-10-10T11:54:49.762863: step 2588, loss 0.110809, acc 0.953125, learning_rate 0.000100124
2017-10-10T11:54:49.985327: step 2589, loss 0.140279, acc 0.953125, learning_rate 0.000100124
2017-10-10T11:54:50.127856: step 2590, loss 0.172089, acc 0.921875, learning_rate 0.000100123
2017-10-10T11:54:50.286303: step 2591, loss 0.239248, acc 0.875, learning_rate 0.000100123
2017-10-10T11:54:50.482301: step 2592, loss 0.177662, acc 0.953125, learning_rate 0.000100122
2017-10-10T11:54:50.637526: step 2593, loss 0.206964, acc 0.9375, learning_rate 0.000100122
2017-10-10T11:54:50.849081: step 2594, loss 0.0866848, acc 1, learning_rate 0.000100121
2017-10-10T11:54:51.062687: step 2595, loss 0.0993197, acc 0.984375, learning_rate 0.000100121
2017-10-10T11:54:51.224401: step 2596, loss 0.151675, acc 0.96875, learning_rate 0.00010012
2017-10-10T11:54:51.348798: step 2597, loss 0.112361, acc 0.96875, learning_rate 0.00010012
2017-10-10T11:54:51.477809: step 2598, loss 0.19246, acc 0.9375, learning_rate 0.000100119
2017-10-10T11:54:51.627241: step 2599, loss 0.127933, acc 0.953125, learning_rate 0.000100119
2017-10-10T11:54:51.756837: step 2600, loss 0.152126, acc 0.953125, learning_rate 0.000100118

Evaluation:
2017-10-10T11:54:52.128228: step 2600, loss 0.232279, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2600

2017-10-10T11:54:53.135743: step 2601, loss 0.143808, acc 0.921875, learning_rate 0.000100118
2017-10-10T11:54:53.349061: step 2602, loss 0.0912362, acc 0.984375, learning_rate 0.000100117
2017-10-10T11:54:53.519237: step 2603, loss 0.106463, acc 0.984375, learning_rate 0.000100117
2017-10-10T11:54:53.712898: step 2604, loss 0.139395, acc 0.96875, learning_rate 0.000100117
2017-10-10T11:54:53.924854: step 2605, loss 0.115304, acc 0.96875, learning_rate 0.000100116
2017-10-10T11:54:54.098328: step 2606, loss 0.0975697, acc 0.96875, learning_rate 0.000100116
2017-10-10T11:54:54.256902: step 2607, loss 0.160668, acc 0.953125, learning_rate 0.000100115
2017-10-10T11:54:54.447481: step 2608, loss 0.126771, acc 0.953125, learning_rate 0.000100115
2017-10-10T11:54:54.643084: step 2609, loss 0.143207, acc 0.9375, learning_rate 0.000100114
2017-10-10T11:54:54.781035: step 2610, loss 0.106626, acc 0.984375, learning_rate 0.000100114
2017-10-10T11:54:54.955430: step 2611, loss 0.111378, acc 0.953125, learning_rate 0.000100113
2017-10-10T11:54:55.148834: step 2612, loss 0.109362, acc 0.96875, learning_rate 0.000100113
2017-10-10T11:54:55.336250: step 2613, loss 0.139, acc 0.953125, learning_rate 0.000100112
2017-10-10T11:54:55.532131: step 2614, loss 0.213239, acc 0.890625, learning_rate 0.000100112
2017-10-10T11:54:55.705725: step 2615, loss 0.199348, acc 0.921875, learning_rate 0.000100111
2017-10-10T11:54:55.899773: step 2616, loss 0.205018, acc 0.921875, learning_rate 0.000100111
2017-10-10T11:54:56.113030: step 2617, loss 0.150527, acc 0.96875, learning_rate 0.000100111
2017-10-10T11:54:56.313916: step 2618, loss 0.0720873, acc 0.96875, learning_rate 0.00010011
2017-10-10T11:54:56.492988: step 2619, loss 0.188755, acc 0.953125, learning_rate 0.00010011
2017-10-10T11:54:56.668290: step 2620, loss 0.213374, acc 0.953125, learning_rate 0.000100109
2017-10-10T11:54:56.858226: step 2621, loss 0.156911, acc 0.921875, learning_rate 0.000100109
2017-10-10T11:54:57.051475: step 2622, loss 0.152374, acc 0.953125, learning_rate 0.000100108
2017-10-10T11:54:57.213209: step 2623, loss 0.137647, acc 0.953125, learning_rate 0.000100108
2017-10-10T11:54:57.364299: step 2624, loss 0.184912, acc 0.96875, learning_rate 0.000100107
2017-10-10T11:54:57.560838: step 2625, loss 0.0526285, acc 1, learning_rate 0.000100107
2017-10-10T11:54:57.731214: step 2626, loss 0.115111, acc 0.96875, learning_rate 0.000100107
2017-10-10T11:54:57.907228: step 2627, loss 0.151312, acc 0.953125, learning_rate 0.000100106
2017-10-10T11:54:58.101243: step 2628, loss 0.107777, acc 0.96875, learning_rate 0.000100106
2017-10-10T11:54:58.304836: step 2629, loss 0.115715, acc 0.9375, learning_rate 0.000100105
2017-10-10T11:54:58.463926: step 2630, loss 0.367359, acc 0.875, learning_rate 0.000100105
2017-10-10T11:54:58.700861: step 2631, loss 0.316743, acc 0.90625, learning_rate 0.000100104
2017-10-10T11:54:58.857001: step 2632, loss 0.153894, acc 0.9375, learning_rate 0.000100104
2017-10-10T11:54:59.052013: step 2633, loss 0.0988913, acc 0.953125, learning_rate 0.000100104
2017-10-10T11:54:59.189120: step 2634, loss 0.124218, acc 0.96875, learning_rate 0.000100103
2017-10-10T11:54:59.360509: step 2635, loss 0.158585, acc 0.953125, learning_rate 0.000100103
2017-10-10T11:54:59.576878: step 2636, loss 0.168434, acc 0.9375, learning_rate 0.000100102
2017-10-10T11:54:59.757100: step 2637, loss 0.10883, acc 0.953125, learning_rate 0.000100102
2017-10-10T11:54:59.963653: step 2638, loss 0.113971, acc 0.96875, learning_rate 0.000100101
2017-10-10T11:55:00.111247: step 2639, loss 0.15363, acc 0.953125, learning_rate 0.000100101
2017-10-10T11:55:00.264079: step 2640, loss 0.114329, acc 0.984375, learning_rate 0.000100101

Evaluation:
2017-10-10T11:55:00.542891: step 2640, loss 0.230797, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2640

2017-10-10T11:55:01.609858: step 2641, loss 0.196757, acc 0.953125, learning_rate 0.0001001
2017-10-10T11:55:01.746502: step 2642, loss 0.1226, acc 0.96875, learning_rate 0.0001001
2017-10-10T11:55:01.862328: step 2643, loss 0.0978236, acc 0.96875, learning_rate 0.000100099
2017-10-10T11:55:02.021514: step 2644, loss 0.134813, acc 0.9375, learning_rate 0.000100099
2017-10-10T11:55:02.177993: step 2645, loss 0.177577, acc 0.953125, learning_rate 0.000100099
2017-10-10T11:55:02.362172: step 2646, loss 0.109715, acc 0.960784, learning_rate 0.000100098
2017-10-10T11:55:02.520987: step 2647, loss 0.189945, acc 0.921875, learning_rate 0.000100098
2017-10-10T11:55:02.686847: step 2648, loss 0.0441204, acc 1, learning_rate 0.000100097
2017-10-10T11:55:02.878466: step 2649, loss 0.139868, acc 0.953125, learning_rate 0.000100097
2017-10-10T11:55:03.092845: step 2650, loss 0.14376, acc 0.9375, learning_rate 0.000100097
2017-10-10T11:55:03.269333: step 2651, loss 0.147225, acc 0.953125, learning_rate 0.000100096
2017-10-10T11:55:03.416871: step 2652, loss 0.237063, acc 0.9375, learning_rate 0.000100096
2017-10-10T11:55:03.610434: step 2653, loss 0.143722, acc 0.953125, learning_rate 0.000100095
2017-10-10T11:55:03.804859: step 2654, loss 0.092829, acc 1, learning_rate 0.000100095
2017-10-10T11:55:03.988982: step 2655, loss 0.114251, acc 0.96875, learning_rate 0.000100095
2017-10-10T11:55:04.167898: step 2656, loss 0.163274, acc 0.953125, learning_rate 0.000100094
2017-10-10T11:55:04.363512: step 2657, loss 0.120394, acc 0.921875, learning_rate 0.000100094
2017-10-10T11:55:04.564852: step 2658, loss 0.340682, acc 0.890625, learning_rate 0.000100093
2017-10-10T11:55:04.720792: step 2659, loss 0.0841046, acc 0.984375, learning_rate 0.000100093
2017-10-10T11:55:04.875346: step 2660, loss 0.189999, acc 0.9375, learning_rate 0.000100093
2017-10-10T11:55:05.081275: step 2661, loss 0.135298, acc 0.984375, learning_rate 0.000100092
2017-10-10T11:55:05.249056: step 2662, loss 0.169954, acc 0.953125, learning_rate 0.000100092
2017-10-10T11:55:05.390318: step 2663, loss 0.0780017, acc 0.984375, learning_rate 0.000100092
2017-10-10T11:55:05.564349: step 2664, loss 0.264964, acc 0.90625, learning_rate 0.000100091
2017-10-10T11:55:05.779887: step 2665, loss 0.176482, acc 0.96875, learning_rate 0.000100091
2017-10-10T11:55:05.967089: step 2666, loss 0.227803, acc 0.90625, learning_rate 0.00010009
2017-10-10T11:55:06.141138: step 2667, loss 0.225135, acc 0.90625, learning_rate 0.00010009
2017-10-10T11:55:06.304879: step 2668, loss 0.217391, acc 0.9375, learning_rate 0.00010009
2017-10-10T11:55:06.494576: step 2669, loss 0.136436, acc 0.953125, learning_rate 0.000100089
2017-10-10T11:55:06.702997: step 2670, loss 0.269507, acc 0.9375, learning_rate 0.000100089
2017-10-10T11:55:06.844884: step 2671, loss 0.141309, acc 0.953125, learning_rate 0.000100089
2017-10-10T11:55:07.016833: step 2672, loss 0.138225, acc 0.96875, learning_rate 0.000100088
2017-10-10T11:55:07.212353: step 2673, loss 0.188517, acc 0.9375, learning_rate 0.000100088
2017-10-10T11:55:07.381527: step 2674, loss 0.121108, acc 0.984375, learning_rate 0.000100088
2017-10-10T11:55:07.528880: step 2675, loss 0.131194, acc 0.96875, learning_rate 0.000100087
2017-10-10T11:55:07.724730: step 2676, loss 0.220165, acc 0.90625, learning_rate 0.000100087
2017-10-10T11:55:07.889378: step 2677, loss 0.242065, acc 0.90625, learning_rate 0.000100086
2017-10-10T11:55:08.097128: step 2678, loss 0.17923, acc 0.890625, learning_rate 0.000100086
2017-10-10T11:55:08.296847: step 2679, loss 0.11709, acc 0.953125, learning_rate 0.000100086
2017-10-10T11:55:08.512833: step 2680, loss 0.20707, acc 0.921875, learning_rate 0.000100085

Evaluation:
2017-10-10T11:55:08.934245: step 2680, loss 0.230037, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2680

2017-10-10T11:55:10.309231: step 2681, loss 0.0930917, acc 0.96875, learning_rate 0.000100085
2017-10-10T11:55:10.498497: step 2682, loss 0.130751, acc 0.953125, learning_rate 0.000100085
2017-10-10T11:55:10.645099: step 2683, loss 0.0959138, acc 0.953125, learning_rate 0.000100084
2017-10-10T11:55:10.788833: step 2684, loss 0.143717, acc 0.96875, learning_rate 0.000100084
2017-10-10T11:55:10.900834: step 2685, loss 0.161668, acc 0.9375, learning_rate 0.000100084
2017-10-10T11:55:11.045725: step 2686, loss 0.196752, acc 0.9375, learning_rate 0.000100083
2017-10-10T11:55:11.220826: step 2687, loss 0.171344, acc 0.9375, learning_rate 0.000100083
2017-10-10T11:55:11.432478: step 2688, loss 0.145416, acc 0.9375, learning_rate 0.000100083
2017-10-10T11:55:11.572497: step 2689, loss 0.093115, acc 0.984375, learning_rate 0.000100082
2017-10-10T11:55:11.676058: step 2690, loss 0.241861, acc 0.953125, learning_rate 0.000100082
2017-10-10T11:55:11.825154: step 2691, loss 0.151696, acc 0.953125, learning_rate 0.000100082
2017-10-10T11:55:11.983637: step 2692, loss 0.133315, acc 0.953125, learning_rate 0.000100081
2017-10-10T11:55:12.083523: step 2693, loss 0.19932, acc 0.921875, learning_rate 0.000100081
2017-10-10T11:55:12.235191: step 2694, loss 0.168281, acc 0.953125, learning_rate 0.000100081
2017-10-10T11:55:12.389156: step 2695, loss 0.200079, acc 0.9375, learning_rate 0.00010008
2017-10-10T11:55:12.491481: step 2696, loss 0.116966, acc 0.984375, learning_rate 0.00010008
2017-10-10T11:55:12.672268: step 2697, loss 0.234039, acc 0.875, learning_rate 0.00010008
2017-10-10T11:55:12.890461: step 2698, loss 0.209307, acc 0.890625, learning_rate 0.000100079
2017-10-10T11:55:13.011451: step 2699, loss 0.116058, acc 0.96875, learning_rate 0.000100079
2017-10-10T11:55:13.211923: step 2700, loss 0.151798, acc 0.96875, learning_rate 0.000100079
2017-10-10T11:55:13.402998: step 2701, loss 0.162918, acc 0.96875, learning_rate 0.000100078
2017-10-10T11:55:13.609016: step 2702, loss 0.153841, acc 0.953125, learning_rate 0.000100078
2017-10-10T11:55:13.758736: step 2703, loss 0.218688, acc 0.890625, learning_rate 0.000100078
2017-10-10T11:55:13.916711: step 2704, loss 0.0785471, acc 0.96875, learning_rate 0.000100077
2017-10-10T11:55:14.130088: step 2705, loss 0.138793, acc 0.953125, learning_rate 0.000100077
2017-10-10T11:55:14.329057: step 2706, loss 0.157525, acc 0.9375, learning_rate 0.000100077
2017-10-10T11:55:14.447194: step 2707, loss 0.0569425, acc 0.984375, learning_rate 0.000100076
2017-10-10T11:55:14.628969: step 2708, loss 0.102544, acc 0.953125, learning_rate 0.000100076
2017-10-10T11:55:14.827011: step 2709, loss 0.0980318, acc 1, learning_rate 0.000100076
2017-10-10T11:55:14.987300: step 2710, loss 0.144894, acc 0.9375, learning_rate 0.000100076
2017-10-10T11:55:15.134294: step 2711, loss 0.141061, acc 0.96875, learning_rate 0.000100075
2017-10-10T11:55:15.333593: step 2712, loss 0.162523, acc 0.96875, learning_rate 0.000100075
2017-10-10T11:55:15.526894: step 2713, loss 0.166624, acc 0.953125, learning_rate 0.000100075
2017-10-10T11:55:15.697311: step 2714, loss 0.182765, acc 0.90625, learning_rate 0.000100074
2017-10-10T11:55:15.915972: step 2715, loss 0.150322, acc 0.96875, learning_rate 0.000100074
2017-10-10T11:55:16.120845: step 2716, loss 0.152256, acc 0.953125, learning_rate 0.000100074
2017-10-10T11:55:16.263507: step 2717, loss 0.129597, acc 0.984375, learning_rate 0.000100073
2017-10-10T11:55:16.441836: step 2718, loss 0.162095, acc 0.953125, learning_rate 0.000100073
2017-10-10T11:55:16.622272: step 2719, loss 0.248996, acc 0.890625, learning_rate 0.000100073
2017-10-10T11:55:16.785083: step 2720, loss 0.113215, acc 0.96875, learning_rate 0.000100073

Evaluation:
2017-10-10T11:55:17.120954: step 2720, loss 0.230947, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2720

2017-10-10T11:55:18.090654: step 2721, loss 0.189462, acc 0.953125, learning_rate 0.000100072
2017-10-10T11:55:18.293687: step 2722, loss 0.213629, acc 0.90625, learning_rate 0.000100072
2017-10-10T11:55:18.479769: step 2723, loss 0.117997, acc 0.96875, learning_rate 0.000100072
2017-10-10T11:55:18.613126: step 2724, loss 0.100348, acc 0.984375, learning_rate 0.000100071
2017-10-10T11:55:18.800085: step 2725, loss 0.114519, acc 0.96875, learning_rate 0.000100071
2017-10-10T11:55:19.002825: step 2726, loss 0.11287, acc 0.96875, learning_rate 0.000100071
2017-10-10T11:55:19.155286: step 2727, loss 0.0813, acc 0.984375, learning_rate 0.00010007
2017-10-10T11:55:19.321167: step 2728, loss 0.0812643, acc 0.96875, learning_rate 0.00010007
2017-10-10T11:55:19.510898: step 2729, loss 0.122941, acc 0.9375, learning_rate 0.00010007
2017-10-10T11:55:19.677808: step 2730, loss 0.294291, acc 0.890625, learning_rate 0.00010007
2017-10-10T11:55:19.816872: step 2731, loss 0.136861, acc 0.9375, learning_rate 0.000100069
2017-10-10T11:55:20.023837: step 2732, loss 0.159347, acc 0.953125, learning_rate 0.000100069
2017-10-10T11:55:20.240358: step 2733, loss 0.172457, acc 0.953125, learning_rate 0.000100069
2017-10-10T11:55:20.425125: step 2734, loss 0.162768, acc 0.9375, learning_rate 0.000100068
2017-10-10T11:55:20.583948: step 2735, loss 0.130079, acc 0.96875, learning_rate 0.000100068
2017-10-10T11:55:20.791909: step 2736, loss 0.141033, acc 0.96875, learning_rate 0.000100068
2017-10-10T11:55:20.919899: step 2737, loss 0.139894, acc 0.9375, learning_rate 0.000100068
2017-10-10T11:55:21.114952: step 2738, loss 0.198932, acc 0.953125, learning_rate 0.000100067
2017-10-10T11:55:21.338394: step 2739, loss 0.157314, acc 0.953125, learning_rate 0.000100067
2017-10-10T11:55:21.564885: step 2740, loss 0.156117, acc 0.953125, learning_rate 0.000100067
2017-10-10T11:55:21.745042: step 2741, loss 0.120516, acc 0.96875, learning_rate 0.000100067
2017-10-10T11:55:21.888961: step 2742, loss 0.203412, acc 0.9375, learning_rate 0.000100066
2017-10-10T11:55:22.105611: step 2743, loss 0.120332, acc 0.96875, learning_rate 0.000100066
2017-10-10T11:55:22.249549: step 2744, loss 0.169154, acc 0.941176, learning_rate 0.000100066
2017-10-10T11:55:22.339772: step 2745, loss 0.17913, acc 0.9375, learning_rate 0.000100065
2017-10-10T11:55:22.427587: step 2746, loss 0.172875, acc 0.953125, learning_rate 0.000100065
2017-10-10T11:55:22.519138: step 2747, loss 0.17464, acc 0.90625, learning_rate 0.000100065
2017-10-10T11:55:22.617344: step 2748, loss 0.106586, acc 0.96875, learning_rate 0.000100065
2017-10-10T11:55:22.704605: step 2749, loss 0.136348, acc 0.953125, learning_rate 0.000100064
2017-10-10T11:55:22.844821: step 2750, loss 0.0747399, acc 0.984375, learning_rate 0.000100064
2017-10-10T11:55:23.024947: step 2751, loss 0.206722, acc 0.9375, learning_rate 0.000100064
2017-10-10T11:55:23.212597: step 2752, loss 0.213353, acc 0.90625, learning_rate 0.000100064
2017-10-10T11:55:23.404295: step 2753, loss 0.116237, acc 0.96875, learning_rate 0.000100063
2017-10-10T11:55:23.588917: step 2754, loss 0.161826, acc 0.96875, learning_rate 0.000100063
2017-10-10T11:55:23.749078: step 2755, loss 0.168931, acc 0.96875, learning_rate 0.000100063
2017-10-10T11:55:23.952408: step 2756, loss 0.267026, acc 0.9375, learning_rate 0.000100063
2017-10-10T11:55:24.122234: step 2757, loss 0.107027, acc 0.953125, learning_rate 0.000100062
2017-10-10T11:55:24.278279: step 2758, loss 0.0793555, acc 1, learning_rate 0.000100062
2017-10-10T11:55:24.480372: step 2759, loss 0.132447, acc 0.96875, learning_rate 0.000100062
2017-10-10T11:55:24.681527: step 2760, loss 0.0728163, acc 0.96875, learning_rate 0.000100062

Evaluation:
2017-10-10T11:55:25.016210: step 2760, loss 0.229601, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2760

2017-10-10T11:55:26.192062: step 2761, loss 0.155728, acc 0.9375, learning_rate 0.000100061
2017-10-10T11:55:26.360121: step 2762, loss 0.126404, acc 0.953125, learning_rate 0.000100061
2017-10-10T11:55:26.552268: step 2763, loss 0.109936, acc 0.96875, learning_rate 0.000100061
2017-10-10T11:55:26.693155: step 2764, loss 0.168537, acc 0.96875, learning_rate 0.000100061
2017-10-10T11:55:26.870723: step 2765, loss 0.132734, acc 0.921875, learning_rate 0.00010006
2017-10-10T11:55:27.085092: step 2766, loss 0.117602, acc 0.9375, learning_rate 0.00010006
2017-10-10T11:55:27.249022: step 2767, loss 0.258298, acc 0.90625, learning_rate 0.00010006
2017-10-10T11:55:27.392457: step 2768, loss 0.161117, acc 0.953125, learning_rate 0.00010006
2017-10-10T11:55:27.606229: step 2769, loss 0.195803, acc 0.953125, learning_rate 0.000100059
2017-10-10T11:55:27.807334: step 2770, loss 0.172237, acc 0.96875, learning_rate 0.000100059
2017-10-10T11:55:27.967281: step 2771, loss 0.160419, acc 0.96875, learning_rate 0.000100059
2017-10-10T11:55:28.127846: step 2772, loss 0.0554701, acc 1, learning_rate 0.000100059
2017-10-10T11:55:28.335195: step 2773, loss 0.0983126, acc 0.984375, learning_rate 0.000100058
2017-10-10T11:55:28.504032: step 2774, loss 0.13247, acc 0.9375, learning_rate 0.000100058
2017-10-10T11:55:28.691345: step 2775, loss 0.146772, acc 0.9375, learning_rate 0.000100058
2017-10-10T11:55:28.917280: step 2776, loss 0.165898, acc 0.953125, learning_rate 0.000100058
2017-10-10T11:55:29.050787: step 2777, loss 0.0661481, acc 0.984375, learning_rate 0.000100057
2017-10-10T11:55:29.229026: step 2778, loss 0.16187, acc 0.984375, learning_rate 0.000100057
2017-10-10T11:55:29.418255: step 2779, loss 0.105697, acc 0.984375, learning_rate 0.000100057
2017-10-10T11:55:29.551770: step 2780, loss 0.1277, acc 0.953125, learning_rate 0.000100057
2017-10-10T11:55:29.729804: step 2781, loss 0.311321, acc 0.859375, learning_rate 0.000100056
2017-10-10T11:55:29.920072: step 2782, loss 0.175049, acc 0.9375, learning_rate 0.000100056
2017-10-10T11:55:30.062463: step 2783, loss 0.0987921, acc 0.984375, learning_rate 0.000100056
2017-10-10T11:55:30.250891: step 2784, loss 0.100198, acc 0.96875, learning_rate 0.000100056
2017-10-10T11:55:30.452570: step 2785, loss 0.120981, acc 0.9375, learning_rate 0.000100056
2017-10-10T11:55:30.652862: step 2786, loss 0.200241, acc 0.921875, learning_rate 0.000100055
2017-10-10T11:55:30.827858: step 2787, loss 0.117304, acc 0.96875, learning_rate 0.000100055
2017-10-10T11:55:31.005220: step 2788, loss 0.142445, acc 0.96875, learning_rate 0.000100055
2017-10-10T11:55:31.200827: step 2789, loss 0.224251, acc 0.921875, learning_rate 0.000100055
2017-10-10T11:55:31.393048: step 2790, loss 0.149074, acc 0.9375, learning_rate 0.000100054
2017-10-10T11:55:31.581443: step 2791, loss 0.164715, acc 0.953125, learning_rate 0.000100054
2017-10-10T11:55:31.767005: step 2792, loss 0.125772, acc 0.953125, learning_rate 0.000100054
2017-10-10T11:55:31.952822: step 2793, loss 0.199787, acc 0.921875, learning_rate 0.000100054
2017-10-10T11:55:32.188343: step 2794, loss 0.102492, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:55:32.400980: step 2795, loss 0.176052, acc 0.9375, learning_rate 0.000100053
2017-10-10T11:55:32.564981: step 2796, loss 0.163619, acc 0.9375, learning_rate 0.000100053
2017-10-10T11:55:32.713485: step 2797, loss 0.0433884, acc 1, learning_rate 0.000100053
2017-10-10T11:55:32.921062: step 2798, loss 0.208779, acc 0.90625, learning_rate 0.000100053
2017-10-10T11:55:33.119642: step 2799, loss 0.222083, acc 0.921875, learning_rate 0.000100052
2017-10-10T11:55:33.215443: step 2800, loss 0.143646, acc 0.9375, learning_rate 0.000100052

Evaluation:
2017-10-10T11:55:33.500602: step 2800, loss 0.22883, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2800

2017-10-10T11:55:34.636924: step 2801, loss 0.160718, acc 0.890625, learning_rate 0.000100052
2017-10-10T11:55:34.836104: step 2802, loss 0.120099, acc 0.984375, learning_rate 0.000100052
2017-10-10T11:55:34.996774: step 2803, loss 0.152295, acc 0.9375, learning_rate 0.000100052
2017-10-10T11:55:35.205619: step 2804, loss 0.11947, acc 0.9375, learning_rate 0.000100051
2017-10-10T11:55:35.369161: step 2805, loss 0.128692, acc 0.984375, learning_rate 0.000100051
2017-10-10T11:55:35.524884: step 2806, loss 0.184786, acc 0.9375, learning_rate 0.000100051
2017-10-10T11:55:35.729204: step 2807, loss 0.154333, acc 0.953125, learning_rate 0.000100051
2017-10-10T11:55:35.935627: step 2808, loss 0.160157, acc 0.9375, learning_rate 0.000100051
2017-10-10T11:55:36.072269: step 2809, loss 0.143087, acc 0.953125, learning_rate 0.00010005
2017-10-10T11:55:36.268893: step 2810, loss 0.100197, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:55:36.469602: step 2811, loss 0.140547, acc 0.984375, learning_rate 0.00010005
2017-10-10T11:55:36.678605: step 2812, loss 0.127723, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:55:36.868957: step 2813, loss 0.0573661, acc 1, learning_rate 0.00010005
2017-10-10T11:55:36.984843: step 2814, loss 0.207978, acc 0.921875, learning_rate 0.000100049
2017-10-10T11:55:37.202104: step 2815, loss 0.111862, acc 0.9375, learning_rate 0.000100049
2017-10-10T11:55:37.405033: step 2816, loss 0.100458, acc 0.96875, learning_rate 0.000100049
2017-10-10T11:55:37.597075: step 2817, loss 0.0824571, acc 0.984375, learning_rate 0.000100049
2017-10-10T11:55:37.731629: step 2818, loss 0.171594, acc 0.90625, learning_rate 0.000100049
2017-10-10T11:55:37.933777: step 2819, loss 0.212404, acc 0.953125, learning_rate 0.000100048
2017-10-10T11:55:38.148021: step 2820, loss 0.0761369, acc 0.984375, learning_rate 0.000100048
2017-10-10T11:55:38.317077: step 2821, loss 0.399855, acc 0.859375, learning_rate 0.000100048
2017-10-10T11:55:38.475311: step 2822, loss 0.177925, acc 0.9375, learning_rate 0.000100048
2017-10-10T11:55:38.651519: step 2823, loss 0.0894187, acc 0.984375, learning_rate 0.000100048
2017-10-10T11:55:38.828833: step 2824, loss 0.163509, acc 0.9375, learning_rate 0.000100047
2017-10-10T11:55:39.024880: step 2825, loss 0.27199, acc 0.921875, learning_rate 0.000100047
2017-10-10T11:55:39.205517: step 2826, loss 0.174022, acc 0.953125, learning_rate 0.000100047
2017-10-10T11:55:39.364844: step 2827, loss 0.163455, acc 0.921875, learning_rate 0.000100047
2017-10-10T11:55:39.527177: step 2828, loss 0.20793, acc 0.90625, learning_rate 0.000100047
2017-10-10T11:55:39.739602: step 2829, loss 0.158014, acc 0.953125, learning_rate 0.000100046
2017-10-10T11:55:39.938937: step 2830, loss 0.17543, acc 0.953125, learning_rate 0.000100046
2017-10-10T11:55:40.072968: step 2831, loss 0.151032, acc 0.953125, learning_rate 0.000100046
2017-10-10T11:55:40.272404: step 2832, loss 0.10586, acc 0.984375, learning_rate 0.000100046
2017-10-10T11:55:40.454323: step 2833, loss 0.213486, acc 0.921875, learning_rate 0.000100046
2017-10-10T11:55:40.594416: step 2834, loss 0.134942, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:55:40.777538: step 2835, loss 0.1287, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:55:40.985613: step 2836, loss 0.0957093, acc 0.96875, learning_rate 0.000100045
2017-10-10T11:55:41.116967: step 2837, loss 0.180563, acc 0.9375, learning_rate 0.000100045
2017-10-10T11:55:41.334996: step 2838, loss 0.12485, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:55:41.559365: step 2839, loss 0.17052, acc 0.9375, learning_rate 0.000100045
2017-10-10T11:55:41.759337: step 2840, loss 0.187787, acc 0.953125, learning_rate 0.000100044

Evaluation:
2017-10-10T11:55:42.102695: step 2840, loss 0.229403, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2840

2017-10-10T11:55:43.041046: step 2841, loss 0.118234, acc 0.953125, learning_rate 0.000100044
2017-10-10T11:55:43.171186: step 2842, loss 0.154448, acc 0.960784, learning_rate 0.000100044
2017-10-10T11:55:43.323402: step 2843, loss 0.214, acc 0.9375, learning_rate 0.000100044
2017-10-10T11:55:43.455096: step 2844, loss 0.0777356, acc 1, learning_rate 0.000100044
2017-10-10T11:55:43.568595: step 2845, loss 0.339582, acc 0.890625, learning_rate 0.000100043
2017-10-10T11:55:43.704199: step 2846, loss 0.143048, acc 0.953125, learning_rate 0.000100043
2017-10-10T11:55:43.936864: step 2847, loss 0.163448, acc 0.953125, learning_rate 0.000100043
2017-10-10T11:55:44.172750: step 2848, loss 0.109976, acc 0.96875, learning_rate 0.000100043
2017-10-10T11:55:44.333355: step 2849, loss 0.216839, acc 0.9375, learning_rate 0.000100043
2017-10-10T11:55:44.485771: step 2850, loss 0.138604, acc 0.96875, learning_rate 0.000100043
2017-10-10T11:55:44.642551: step 2851, loss 0.105748, acc 0.984375, learning_rate 0.000100042
2017-10-10T11:55:44.780839: step 2852, loss 0.0954439, acc 0.984375, learning_rate 0.000100042
2017-10-10T11:55:44.900920: step 2853, loss 0.124615, acc 0.953125, learning_rate 0.000100042
2017-10-10T11:55:45.024526: step 2854, loss 0.146371, acc 0.921875, learning_rate 0.000100042
2017-10-10T11:55:45.168733: step 2855, loss 0.203304, acc 0.9375, learning_rate 0.000100042
2017-10-10T11:55:45.363489: step 2856, loss 0.143717, acc 0.9375, learning_rate 0.000100042
2017-10-10T11:55:45.503349: step 2857, loss 0.122293, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:55:45.677924: step 2858, loss 0.0979044, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:55:45.883279: step 2859, loss 0.17571, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:55:46.052903: step 2860, loss 0.0916015, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:55:46.204851: step 2861, loss 0.170956, acc 0.953125, learning_rate 0.000100041
2017-10-10T11:55:46.400339: step 2862, loss 0.173265, acc 0.953125, learning_rate 0.000100041
2017-10-10T11:55:46.589537: step 2863, loss 0.209899, acc 0.921875, learning_rate 0.00010004
2017-10-10T11:55:46.750786: step 2864, loss 0.180616, acc 0.9375, learning_rate 0.00010004
2017-10-10T11:55:46.929236: step 2865, loss 0.0755051, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:55:47.129889: step 2866, loss 0.129872, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:55:47.348871: step 2867, loss 0.161259, acc 0.9375, learning_rate 0.00010004
2017-10-10T11:55:47.524968: step 2868, loss 0.0999675, acc 0.953125, learning_rate 0.00010004
2017-10-10T11:55:47.662063: step 2869, loss 0.131844, acc 0.984375, learning_rate 0.000100039
2017-10-10T11:55:47.886737: step 2870, loss 0.119384, acc 0.96875, learning_rate 0.000100039
2017-10-10T11:55:48.058486: step 2871, loss 0.159323, acc 0.9375, learning_rate 0.000100039
2017-10-10T11:55:48.232853: step 2872, loss 0.113228, acc 0.953125, learning_rate 0.000100039
2017-10-10T11:55:48.439110: step 2873, loss 0.134234, acc 0.953125, learning_rate 0.000100039
2017-10-10T11:55:48.646570: step 2874, loss 0.126495, acc 0.9375, learning_rate 0.000100039
2017-10-10T11:55:48.824892: step 2875, loss 0.255374, acc 0.90625, learning_rate 0.000100038
2017-10-10T11:55:48.969139: step 2876, loss 0.110647, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:55:49.160236: step 2877, loss 0.137576, acc 0.9375, learning_rate 0.000100038
2017-10-10T11:55:49.351538: step 2878, loss 0.094582, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:55:49.494725: step 2879, loss 0.183576, acc 0.9375, learning_rate 0.000100038
2017-10-10T11:55:49.676590: step 2880, loss 0.163816, acc 0.9375, learning_rate 0.000100038

Evaluation:
2017-10-10T11:55:50.011012: step 2880, loss 0.231155, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2880

2017-10-10T11:55:51.163418: step 2881, loss 0.181766, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:55:51.340064: step 2882, loss 0.083633, acc 0.96875, learning_rate 0.000100037
2017-10-10T11:55:51.542613: step 2883, loss 0.185334, acc 0.9375, learning_rate 0.000100037
2017-10-10T11:55:51.756579: step 2884, loss 0.174484, acc 0.9375, learning_rate 0.000100037
2017-10-10T11:55:51.890397: step 2885, loss 0.105861, acc 0.96875, learning_rate 0.000100037
2017-10-10T11:55:52.071604: step 2886, loss 0.190854, acc 0.890625, learning_rate 0.000100037
2017-10-10T11:55:52.284982: step 2887, loss 0.181082, acc 0.9375, learning_rate 0.000100037
2017-10-10T11:55:52.492822: step 2888, loss 0.191006, acc 0.953125, learning_rate 0.000100036
2017-10-10T11:55:52.650029: step 2889, loss 0.14475, acc 0.953125, learning_rate 0.000100036
2017-10-10T11:55:52.812916: step 2890, loss 0.152561, acc 0.96875, learning_rate 0.000100036
2017-10-10T11:55:53.062972: step 2891, loss 0.291915, acc 0.875, learning_rate 0.000100036
2017-10-10T11:55:53.245503: step 2892, loss 0.167876, acc 0.921875, learning_rate 0.000100036
2017-10-10T11:55:53.383079: step 2893, loss 0.229424, acc 0.9375, learning_rate 0.000100036
2017-10-10T11:55:53.547456: step 2894, loss 0.127364, acc 0.953125, learning_rate 0.000100036
2017-10-10T11:55:53.710114: step 2895, loss 0.119209, acc 0.953125, learning_rate 0.000100035
2017-10-10T11:55:53.844828: step 2896, loss 0.189953, acc 0.9375, learning_rate 0.000100035
2017-10-10T11:55:53.984432: step 2897, loss 0.0987418, acc 0.984375, learning_rate 0.000100035
2017-10-10T11:55:54.192855: step 2898, loss 0.188268, acc 0.9375, learning_rate 0.000100035
2017-10-10T11:55:54.362661: step 2899, loss 0.0509265, acc 1, learning_rate 0.000100035
2017-10-10T11:55:54.552861: step 2900, loss 0.147864, acc 0.9375, learning_rate 0.000100035
2017-10-10T11:55:54.714272: step 2901, loss 0.0893998, acc 0.96875, learning_rate 0.000100035
2017-10-10T11:55:54.916200: step 2902, loss 0.13221, acc 0.96875, learning_rate 0.000100034
2017-10-10T11:55:55.136977: step 2903, loss 0.150014, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:55:55.330824: step 2904, loss 0.143541, acc 0.96875, learning_rate 0.000100034
2017-10-10T11:55:55.535595: step 2905, loss 0.161576, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:55:55.695157: step 2906, loss 0.286672, acc 0.921875, learning_rate 0.000100034
2017-10-10T11:55:56.009726: step 2907, loss 0.153414, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:55:56.144824: step 2908, loss 0.121408, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:55:56.276871: step 2909, loss 0.206487, acc 0.90625, learning_rate 0.000100033
2017-10-10T11:55:56.415576: step 2910, loss 0.150494, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:55:56.616850: step 2911, loss 0.154509, acc 0.953125, learning_rate 0.000100033
2017-10-10T11:55:56.822758: step 2912, loss 0.161876, acc 0.953125, learning_rate 0.000100033
2017-10-10T11:55:57.026602: step 2913, loss 0.259277, acc 0.890625, learning_rate 0.000100033
2017-10-10T11:55:57.195794: step 2914, loss 0.120786, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:55:57.360051: step 2915, loss 0.186165, acc 0.9375, learning_rate 0.000100033
2017-10-10T11:55:57.538183: step 2916, loss 0.112328, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:55:57.746256: step 2917, loss 0.106689, acc 0.953125, learning_rate 0.000100032
2017-10-10T11:55:57.968030: step 2918, loss 0.181876, acc 0.9375, learning_rate 0.000100032
2017-10-10T11:55:58.117529: step 2919, loss 0.150977, acc 0.984375, learning_rate 0.000100032
2017-10-10T11:55:58.274613: step 2920, loss 0.15127, acc 0.9375, learning_rate 0.000100032

Evaluation:
2017-10-10T11:55:58.653789: step 2920, loss 0.228564, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2920

2017-10-10T11:55:59.880089: step 2921, loss 0.0903031, acc 0.96875, learning_rate 0.000100032
2017-10-10T11:56:00.097488: step 2922, loss 0.169461, acc 0.9375, learning_rate 0.000100032
2017-10-10T11:56:00.303880: step 2923, loss 0.20053, acc 0.890625, learning_rate 0.000100032
2017-10-10T11:56:00.457204: step 2924, loss 0.0664494, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:56:00.626099: step 2925, loss 0.085106, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:56:00.852030: step 2926, loss 0.209414, acc 0.921875, learning_rate 0.000100031
2017-10-10T11:56:01.041551: step 2927, loss 0.212525, acc 0.90625, learning_rate 0.000100031
2017-10-10T11:56:01.164830: step 2928, loss 0.209799, acc 0.9375, learning_rate 0.000100031
2017-10-10T11:56:01.374156: step 2929, loss 0.106338, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:56:01.567436: step 2930, loss 0.244442, acc 0.921875, learning_rate 0.000100031
2017-10-10T11:56:01.705103: step 2931, loss 0.169043, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:56:01.902407: step 2932, loss 0.18052, acc 0.953125, learning_rate 0.00010003
2017-10-10T11:56:02.097871: step 2933, loss 0.125356, acc 0.953125, learning_rate 0.00010003
2017-10-10T11:56:02.313627: step 2934, loss 0.120302, acc 0.953125, learning_rate 0.00010003
2017-10-10T11:56:02.497300: step 2935, loss 0.274545, acc 0.90625, learning_rate 0.00010003
2017-10-10T11:56:02.621685: step 2936, loss 0.164062, acc 0.9375, learning_rate 0.00010003
2017-10-10T11:56:02.802333: step 2937, loss 0.111759, acc 0.96875, learning_rate 0.00010003
2017-10-10T11:56:02.992851: step 2938, loss 0.0862009, acc 0.984375, learning_rate 0.00010003
2017-10-10T11:56:03.128319: step 2939, loss 0.276694, acc 0.90625, learning_rate 0.00010003
2017-10-10T11:56:03.303317: step 2940, loss 0.08767, acc 0.980392, learning_rate 0.000100029
2017-10-10T11:56:03.537565: step 2941, loss 0.241066, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:56:03.772908: step 2942, loss 0.298103, acc 0.9375, learning_rate 0.000100029
2017-10-10T11:56:03.935808: step 2943, loss 0.149363, acc 0.921875, learning_rate 0.000100029
2017-10-10T11:56:04.084897: step 2944, loss 0.183086, acc 0.9375, learning_rate 0.000100029
2017-10-10T11:56:04.246820: step 2945, loss 0.200082, acc 0.921875, learning_rate 0.000100029
2017-10-10T11:56:04.392825: step 2946, loss 0.149248, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:56:04.520947: step 2947, loss 0.0986681, acc 0.96875, learning_rate 0.000100029
2017-10-10T11:56:04.669076: step 2948, loss 0.161586, acc 0.9375, learning_rate 0.000100029
2017-10-10T11:56:04.801366: step 2949, loss 0.239983, acc 0.9375, learning_rate 0.000100028
2017-10-10T11:56:05.018526: step 2950, loss 0.283357, acc 0.921875, learning_rate 0.000100028
2017-10-10T11:56:05.239548: step 2951, loss 0.127277, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:56:05.431984: step 2952, loss 0.163088, acc 0.9375, learning_rate 0.000100028
2017-10-10T11:56:05.588954: step 2953, loss 0.0856396, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:56:05.789560: step 2954, loss 0.126736, acc 0.984375, learning_rate 0.000100028
2017-10-10T11:56:05.992896: step 2955, loss 0.0983575, acc 0.984375, learning_rate 0.000100028
2017-10-10T11:56:06.205072: step 2956, loss 0.186615, acc 0.90625, learning_rate 0.000100028
2017-10-10T11:56:06.411447: step 2957, loss 0.128389, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:56:06.552873: step 2958, loss 0.0841527, acc 1, learning_rate 0.000100027
2017-10-10T11:56:06.694246: step 2959, loss 0.119052, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:56:06.855657: step 2960, loss 0.105374, acc 0.953125, learning_rate 0.000100027

Evaluation:
2017-10-10T11:56:07.133696: step 2960, loss 0.228848, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-2960

2017-10-10T11:56:08.165102: step 2961, loss 0.176431, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:56:08.324092: step 2962, loss 0.335468, acc 0.90625, learning_rate 0.000100027
2017-10-10T11:56:08.516885: step 2963, loss 0.13894, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:56:08.707999: step 2964, loss 0.113953, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:56:08.916565: step 2965, loss 0.127884, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:56:09.097000: step 2966, loss 0.189323, acc 0.9375, learning_rate 0.000100027
2017-10-10T11:56:09.249064: step 2967, loss 0.151447, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:56:09.413101: step 2968, loss 0.0881333, acc 0.96875, learning_rate 0.000100026
2017-10-10T11:56:09.620845: step 2969, loss 0.147347, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:56:09.772030: step 2970, loss 0.146518, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:56:09.935516: step 2971, loss 0.150993, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:56:10.122246: step 2972, loss 0.14487, acc 0.953125, learning_rate 0.000100026
2017-10-10T11:56:10.296948: step 2973, loss 0.110053, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:56:10.432970: step 2974, loss 0.0961995, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:56:10.630647: step 2975, loss 0.174024, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:56:10.835850: step 2976, loss 0.08313, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:56:11.011817: step 2977, loss 0.158473, acc 0.9375, learning_rate 0.000100025
2017-10-10T11:56:11.147993: step 2978, loss 0.169766, acc 0.9375, learning_rate 0.000100025
2017-10-10T11:56:11.340168: step 2979, loss 0.146623, acc 0.9375, learning_rate 0.000100025
2017-10-10T11:56:11.561769: step 2980, loss 0.132953, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:56:11.759676: step 2981, loss 0.205378, acc 0.921875, learning_rate 0.000100025
2017-10-10T11:56:11.905934: step 2982, loss 0.156653, acc 0.953125, learning_rate 0.000100025
2017-10-10T11:56:12.078030: step 2983, loss 0.223159, acc 0.921875, learning_rate 0.000100025
2017-10-10T11:56:12.276850: step 2984, loss 0.182541, acc 0.9375, learning_rate 0.000100025
2017-10-10T11:56:12.433076: step 2985, loss 0.0984381, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:56:12.603165: step 2986, loss 0.120017, acc 0.921875, learning_rate 0.000100024
2017-10-10T11:56:12.796503: step 2987, loss 0.152275, acc 0.9375, learning_rate 0.000100024
2017-10-10T11:56:12.949012: step 2988, loss 0.136987, acc 0.96875, learning_rate 0.000100024
2017-10-10T11:56:13.120885: step 2989, loss 0.270748, acc 0.90625, learning_rate 0.000100024
2017-10-10T11:56:13.326735: step 2990, loss 0.141302, acc 0.953125, learning_rate 0.000100024
2017-10-10T11:56:13.518797: step 2991, loss 0.0699427, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:56:13.745286: step 2992, loss 0.136441, acc 0.953125, learning_rate 0.000100024
2017-10-10T11:56:13.916988: step 2993, loss 0.0813599, acc 0.96875, learning_rate 0.000100024
2017-10-10T11:56:14.067329: step 2994, loss 0.0888895, acc 0.96875, learning_rate 0.000100024
2017-10-10T11:56:14.308869: step 2995, loss 0.0404207, acc 1, learning_rate 0.000100024
2017-10-10T11:56:14.521493: step 2996, loss 0.146446, acc 0.9375, learning_rate 0.000100023
2017-10-10T11:56:14.633946: step 2997, loss 0.0922362, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:14.797462: step 2998, loss 0.130624, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:14.944682: step 2999, loss 0.15985, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:15.104818: step 3000, loss 0.2037, acc 0.953125, learning_rate 0.000100023

Evaluation:
2017-10-10T11:56:15.399346: step 3000, loss 0.227525, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3000

2017-10-10T11:56:16.491128: step 3001, loss 0.119784, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:56:16.652873: step 3002, loss 0.154463, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:16.860244: step 3003, loss 0.126488, acc 0.9375, learning_rate 0.000100023
2017-10-10T11:56:17.063151: step 3004, loss 0.102639, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:56:17.248830: step 3005, loss 0.10723, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:56:17.377321: step 3006, loss 0.153572, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:56:17.492908: step 3007, loss 0.249858, acc 0.921875, learning_rate 0.000100022
2017-10-10T11:56:17.651714: step 3008, loss 0.178863, acc 0.921875, learning_rate 0.000100022
2017-10-10T11:56:17.801280: step 3009, loss 0.0737449, acc 0.984375, learning_rate 0.000100022
2017-10-10T11:56:17.914368: step 3010, loss 0.314315, acc 0.859375, learning_rate 0.000100022
2017-10-10T11:56:18.029477: step 3011, loss 0.114301, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:56:18.195572: step 3012, loss 0.168117, acc 0.9375, learning_rate 0.000100022
2017-10-10T11:56:18.351785: step 3013, loss 0.229839, acc 0.921875, learning_rate 0.000100022
2017-10-10T11:56:18.510138: step 3014, loss 0.120747, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:56:18.697470: step 3015, loss 0.206408, acc 0.90625, learning_rate 0.000100022
2017-10-10T11:56:18.896991: step 3016, loss 0.218383, acc 0.9375, learning_rate 0.000100022
2017-10-10T11:56:19.084862: step 3017, loss 0.123486, acc 0.984375, learning_rate 0.000100022
2017-10-10T11:56:19.301271: step 3018, loss 0.062938, acc 1, learning_rate 0.000100021
2017-10-10T11:56:19.464885: step 3019, loss 0.151502, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:56:19.637757: step 3020, loss 0.128106, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:56:19.840428: step 3021, loss 0.165237, acc 0.953125, learning_rate 0.000100021
2017-10-10T11:56:20.032842: step 3022, loss 0.0848756, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:56:20.170339: step 3023, loss 0.161575, acc 0.953125, learning_rate 0.000100021
2017-10-10T11:56:20.364950: step 3024, loss 0.0657675, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:56:20.580783: step 3025, loss 0.138594, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:56:20.784930: step 3026, loss 0.249675, acc 0.890625, learning_rate 0.000100021
2017-10-10T11:56:20.933278: step 3027, loss 0.100059, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:56:21.100564: step 3028, loss 0.170716, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:56:21.301512: step 3029, loss 0.0912873, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:56:21.481995: step 3030, loss 0.172897, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:56:21.664887: step 3031, loss 0.0967544, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:56:21.833355: step 3032, loss 0.0627686, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:56:22.011947: step 3033, loss 0.116622, acc 0.953125, learning_rate 0.00010002
2017-10-10T11:56:22.227452: step 3034, loss 0.247019, acc 0.9375, learning_rate 0.00010002
2017-10-10T11:56:22.382929: step 3035, loss 0.108112, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:56:22.591318: step 3036, loss 0.190253, acc 0.953125, learning_rate 0.00010002
2017-10-10T11:56:22.742694: step 3037, loss 0.0948137, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:56:22.892076: step 3038, loss 0.157465, acc 0.901961, learning_rate 0.00010002
2017-10-10T11:56:23.098360: step 3039, loss 0.152845, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:56:23.313587: step 3040, loss 0.13917, acc 0.96875, learning_rate 0.00010002

Evaluation:
2017-10-10T11:56:23.646614: step 3040, loss 0.226778, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3040

2017-10-10T11:56:24.767270: step 3041, loss 0.174275, acc 0.9375, learning_rate 0.00010002
2017-10-10T11:56:24.984938: step 3042, loss 0.125765, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:56:25.126468: step 3043, loss 0.164443, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:56:25.291637: step 3044, loss 0.118401, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:56:25.456342: step 3045, loss 0.121862, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:56:25.620143: step 3046, loss 0.0637528, acc 1, learning_rate 0.000100019
2017-10-10T11:56:25.751442: step 3047, loss 0.231386, acc 0.921875, learning_rate 0.000100019
2017-10-10T11:56:25.867478: step 3048, loss 0.0725961, acc 1, learning_rate 0.000100019
2017-10-10T11:56:26.024211: step 3049, loss 0.175352, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:56:26.156831: step 3050, loss 0.0837247, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:56:26.360864: step 3051, loss 0.141878, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:56:26.498802: step 3052, loss 0.0619038, acc 1, learning_rate 0.000100019
2017-10-10T11:56:26.684851: step 3053, loss 0.108666, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:56:26.904947: step 3054, loss 0.201851, acc 0.921875, learning_rate 0.000100018
2017-10-10T11:56:27.048840: step 3055, loss 0.0806891, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:56:27.264036: step 3056, loss 0.0719155, acc 1, learning_rate 0.000100018
2017-10-10T11:56:27.492506: step 3057, loss 0.0952748, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:56:27.633171: step 3058, loss 0.110573, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:56:27.805870: step 3059, loss 0.111228, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:56:28.036862: step 3060, loss 0.159507, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:56:28.264449: step 3061, loss 0.137025, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:56:28.479392: step 3062, loss 0.200774, acc 0.9375, learning_rate 0.000100018
2017-10-10T11:56:28.604540: step 3063, loss 0.167507, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:56:28.709675: step 3064, loss 0.11928, acc 0.9375, learning_rate 0.000100018
2017-10-10T11:56:28.884767: step 3065, loss 0.140366, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:56:29.043349: step 3066, loss 0.162571, acc 0.921875, learning_rate 0.000100018
2017-10-10T11:56:29.134892: step 3067, loss 0.229591, acc 0.890625, learning_rate 0.000100018
2017-10-10T11:56:29.275101: step 3068, loss 0.104289, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:56:29.496049: step 3069, loss 0.176774, acc 0.9375, learning_rate 0.000100017
2017-10-10T11:56:29.649143: step 3070, loss 0.0824943, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:56:29.816866: step 3071, loss 0.132386, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:56:30.019659: step 3072, loss 0.160541, acc 0.921875, learning_rate 0.000100017
2017-10-10T11:56:30.237444: step 3073, loss 0.16635, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:56:30.412040: step 3074, loss 0.235523, acc 0.921875, learning_rate 0.000100017
2017-10-10T11:56:30.564923: step 3075, loss 0.218971, acc 0.921875, learning_rate 0.000100017
2017-10-10T11:56:30.752845: step 3076, loss 0.144012, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:56:30.988908: step 3077, loss 0.101164, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:56:31.200831: step 3078, loss 0.184344, acc 0.9375, learning_rate 0.000100017
2017-10-10T11:56:31.408831: step 3079, loss 0.135615, acc 1, learning_rate 0.000100017
2017-10-10T11:56:31.525108: step 3080, loss 0.151265, acc 0.9375, learning_rate 0.000100017

Evaluation:
2017-10-10T11:56:31.907650: step 3080, loss 0.227431, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3080

2017-10-10T11:56:33.214733: step 3081, loss 0.193434, acc 0.921875, learning_rate 0.000100017
2017-10-10T11:56:33.396898: step 3082, loss 0.126589, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:33.583936: step 3083, loss 0.109223, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:56:33.747009: step 3084, loss 0.245455, acc 0.90625, learning_rate 0.000100016
2017-10-10T11:56:33.935141: step 3085, loss 0.124409, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:56:34.149312: step 3086, loss 0.185069, acc 0.9375, learning_rate 0.000100016
2017-10-10T11:56:34.357102: step 3087, loss 0.159432, acc 0.9375, learning_rate 0.000100016
2017-10-10T11:56:34.541031: step 3088, loss 0.195567, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:56:34.696390: step 3089, loss 0.207979, acc 0.90625, learning_rate 0.000100016
2017-10-10T11:56:34.901787: step 3090, loss 0.0749228, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:56:35.064024: step 3091, loss 0.266635, acc 0.890625, learning_rate 0.000100016
2017-10-10T11:56:35.244956: step 3092, loss 0.170352, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:56:35.456872: step 3093, loss 0.134402, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:56:35.678318: step 3094, loss 0.166322, acc 0.9375, learning_rate 0.000100016
2017-10-10T11:56:35.823189: step 3095, loss 0.278568, acc 0.90625, learning_rate 0.000100016
2017-10-10T11:56:35.986483: step 3096, loss 0.193352, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:36.120836: step 3097, loss 0.118345, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:56:36.239474: step 3098, loss 0.128731, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:36.384897: step 3099, loss 0.159463, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:56:36.537632: step 3100, loss 0.201303, acc 0.921875, learning_rate 0.000100015
2017-10-10T11:56:36.689764: step 3101, loss 0.150398, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:56:36.835594: step 3102, loss 0.0676157, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:56:37.042744: step 3103, loss 0.177153, acc 0.890625, learning_rate 0.000100015
2017-10-10T11:56:37.213029: step 3104, loss 0.142479, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:37.396374: step 3105, loss 0.164161, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:56:37.592877: step 3106, loss 0.144596, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:37.804202: step 3107, loss 0.179104, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:56:37.981033: step 3108, loss 0.0851824, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:56:38.149046: step 3109, loss 0.142652, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:38.323635: step 3110, loss 0.140434, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:56:38.528843: step 3111, loss 0.112111, acc 0.96875, learning_rate 0.000100015
2017-10-10T11:56:38.704913: step 3112, loss 0.209923, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:56:38.845942: step 3113, loss 0.0478842, acc 1, learning_rate 0.000100015
2017-10-10T11:56:39.072618: step 3114, loss 0.0919591, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:39.329556: step 3115, loss 0.122512, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:56:39.496865: step 3116, loss 0.158422, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:56:39.636860: step 3117, loss 0.193456, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:56:39.758612: step 3118, loss 0.154723, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:56:39.895114: step 3119, loss 0.06957, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:56:40.032249: step 3120, loss 0.23577, acc 0.890625, learning_rate 0.000100014

Evaluation:
2017-10-10T11:56:40.316770: step 3120, loss 0.227693, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3120

2017-10-10T11:56:41.269532: step 3121, loss 0.0762261, acc 1, learning_rate 0.000100014
2017-10-10T11:56:41.456830: step 3122, loss 0.102179, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:56:41.616212: step 3123, loss 0.271116, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:56:41.783740: step 3124, loss 0.239673, acc 0.875, learning_rate 0.000100014
2017-10-10T11:56:42.003086: step 3125, loss 0.0870054, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:42.223158: step 3126, loss 0.20391, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:56:42.440604: step 3127, loss 0.159623, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:56:42.600874: step 3128, loss 0.118801, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:56:42.776426: step 3129, loss 0.164887, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:56:42.957585: step 3130, loss 0.0936378, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:43.156852: step 3131, loss 0.0862221, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:56:43.309151: step 3132, loss 0.064708, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:56:43.484609: step 3133, loss 0.160684, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:56:43.697826: step 3134, loss 0.18287, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:56:43.841082: step 3135, loss 0.149701, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:56:44.034166: step 3136, loss 0.19164, acc 0.941176, learning_rate 0.000100013
2017-10-10T11:56:44.223938: step 3137, loss 0.20052, acc 0.90625, learning_rate 0.000100013
2017-10-10T11:56:44.434672: step 3138, loss 0.121389, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:44.575691: step 3139, loss 0.134296, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:56:44.753367: step 3140, loss 0.19665, acc 0.90625, learning_rate 0.000100013
2017-10-10T11:56:44.941206: step 3141, loss 0.155852, acc 0.921875, learning_rate 0.000100013
2017-10-10T11:56:45.136371: step 3142, loss 0.0887076, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:56:45.267362: step 3143, loss 0.0746956, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:56:45.464863: step 3144, loss 0.135013, acc 0.9375, learning_rate 0.000100013
2017-10-10T11:56:45.665995: step 3145, loss 0.0477878, acc 1, learning_rate 0.000100013
2017-10-10T11:56:45.872897: step 3146, loss 0.13749, acc 0.9375, learning_rate 0.000100013
2017-10-10T11:56:46.081533: step 3147, loss 0.135947, acc 0.9375, learning_rate 0.000100013
2017-10-10T11:56:46.271662: step 3148, loss 0.0852808, acc 1, learning_rate 0.000100013
2017-10-10T11:56:46.386258: step 3149, loss 0.144189, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:56:46.828548: step 3150, loss 0.100453, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:46.953686: step 3151, loss 0.120545, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:47.079596: step 3152, loss 0.0747512, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:47.243952: step 3153, loss 0.147254, acc 0.9375, learning_rate 0.000100012
2017-10-10T11:56:47.365366: step 3154, loss 0.179478, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:47.480811: step 3155, loss 0.173297, acc 0.9375, learning_rate 0.000100012
2017-10-10T11:56:47.636888: step 3156, loss 0.132436, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:47.818114: step 3157, loss 0.162252, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:47.999396: step 3158, loss 0.166101, acc 0.9375, learning_rate 0.000100012
2017-10-10T11:56:48.204243: step 3159, loss 0.130383, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:48.312862: step 3160, loss 0.149976, acc 0.9375, learning_rate 0.000100012

Evaluation:
2017-10-10T11:56:48.703685: step 3160, loss 0.227976, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3160

2017-10-10T11:56:49.832904: step 3161, loss 0.145754, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:50.091635: step 3162, loss 0.207236, acc 0.921875, learning_rate 0.000100012
2017-10-10T11:56:50.280795: step 3163, loss 0.183314, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:56:50.441451: step 3164, loss 0.109124, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:56:50.590108: step 3165, loss 0.220034, acc 0.90625, learning_rate 0.000100012
2017-10-10T11:56:50.728841: step 3166, loss 0.17758, acc 0.9375, learning_rate 0.000100012
2017-10-10T11:56:50.848672: step 3167, loss 0.211533, acc 0.890625, learning_rate 0.000100012
2017-10-10T11:56:50.991651: step 3168, loss 0.216978, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:56:51.161047: step 3169, loss 0.150166, acc 0.9375, learning_rate 0.000100012
2017-10-10T11:56:51.341757: step 3170, loss 0.0795842, acc 0.984375, learning_rate 0.000100012
2017-10-10T11:56:51.520309: step 3171, loss 0.122816, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:56:51.680277: step 3172, loss 0.0596626, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:51.874443: step 3173, loss 0.195326, acc 0.921875, learning_rate 0.000100011
2017-10-10T11:56:52.104223: step 3174, loss 0.257845, acc 0.875, learning_rate 0.000100011
2017-10-10T11:56:52.230296: step 3175, loss 0.185165, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:52.426381: step 3176, loss 0.168778, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:52.612598: step 3177, loss 0.0768556, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:52.833393: step 3178, loss 0.185067, acc 0.921875, learning_rate 0.000100011
2017-10-10T11:56:53.028945: step 3179, loss 0.123298, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:53.202207: step 3180, loss 0.130889, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:56:53.360922: step 3181, loss 0.0671937, acc 1, learning_rate 0.000100011
2017-10-10T11:56:53.564572: step 3182, loss 0.12324, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:53.719757: step 3183, loss 0.0863299, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:56:53.874553: step 3184, loss 0.118017, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:54.091588: step 3185, loss 0.189946, acc 0.90625, learning_rate 0.000100011
2017-10-10T11:56:54.292874: step 3186, loss 0.159648, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:54.450756: step 3187, loss 0.181746, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:54.609013: step 3188, loss 0.207778, acc 0.890625, learning_rate 0.000100011
2017-10-10T11:56:54.805079: step 3189, loss 0.156468, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:54.996858: step 3190, loss 0.164826, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:56:55.172975: step 3191, loss 0.194547, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:56:55.316788: step 3192, loss 0.17507, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:56:55.513915: step 3193, loss 0.212902, acc 0.90625, learning_rate 0.00010001
2017-10-10T11:56:55.732944: step 3194, loss 0.161489, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:56:55.882971: step 3195, loss 0.177184, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:56:56.087181: step 3196, loss 0.140171, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:56:56.286820: step 3197, loss 0.168065, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:56:56.429888: step 3198, loss 0.208588, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:56:56.592489: step 3199, loss 0.0470839, acc 1, learning_rate 0.00010001
2017-10-10T11:56:56.784904: step 3200, loss 0.183447, acc 0.921875, learning_rate 0.00010001

Evaluation:
2017-10-10T11:56:57.300999: step 3200, loss 0.228121, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3200

2017-10-10T11:56:58.493310: step 3201, loss 0.170228, acc 0.921875, learning_rate 0.00010001
2017-10-10T11:56:58.697160: step 3202, loss 0.103205, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:56:58.848593: step 3203, loss 0.0584428, acc 1, learning_rate 0.00010001
2017-10-10T11:56:59.078373: step 3204, loss 0.188548, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:56:59.275622: step 3205, loss 0.135462, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:56:59.448842: step 3206, loss 0.247253, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:56:59.619863: step 3207, loss 0.186428, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:56:59.794730: step 3208, loss 0.121485, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:56:59.953210: step 3209, loss 0.273618, acc 0.921875, learning_rate 0.00010001
2017-10-10T11:57:00.183675: step 3210, loss 0.166086, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:57:00.377718: step 3211, loss 0.168922, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:57:00.560875: step 3212, loss 0.121458, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:57:00.769113: step 3213, loss 0.129769, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:57:01.025988: step 3214, loss 0.129062, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:57:01.168331: step 3215, loss 0.116166, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:57:01.325148: step 3216, loss 0.0775753, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:57:01.473868: step 3217, loss 0.234535, acc 0.890625, learning_rate 0.000100009
2017-10-10T11:57:01.612844: step 3218, loss 0.151852, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:01.784815: step 3219, loss 0.0787643, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:01.967412: step 3220, loss 0.217543, acc 0.921875, learning_rate 0.000100009
2017-10-10T11:57:02.181929: step 3221, loss 0.130941, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:02.368904: step 3222, loss 0.0782005, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:02.520986: step 3223, loss 0.075309, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:02.712687: step 3224, loss 0.0772184, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:02.904829: step 3225, loss 0.196376, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:03.047474: step 3226, loss 0.0843312, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:03.227826: step 3227, loss 0.23318, acc 0.921875, learning_rate 0.000100009
2017-10-10T11:57:03.427363: step 3228, loss 0.103977, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:03.629126: step 3229, loss 0.142148, acc 0.921875, learning_rate 0.000100009
2017-10-10T11:57:03.796916: step 3230, loss 0.295055, acc 0.875, learning_rate 0.000100009
2017-10-10T11:57:03.935654: step 3231, loss 0.123305, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:57:04.126841: step 3232, loss 0.172692, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:57:04.342963: step 3233, loss 0.126455, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:04.527112: step 3234, loss 0.247035, acc 0.921569, learning_rate 0.000100009
2017-10-10T11:57:04.642618: step 3235, loss 0.069744, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:04.829422: step 3236, loss 0.143006, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:05.054084: step 3237, loss 0.0979359, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:05.209007: step 3238, loss 0.0653274, acc 1, learning_rate 0.000100009
2017-10-10T11:57:05.355753: step 3239, loss 0.0795549, acc 1, learning_rate 0.000100009
2017-10-10T11:57:05.558951: step 3240, loss 0.110876, acc 0.96875, learning_rate 0.000100009

Evaluation:
2017-10-10T11:57:05.918445: step 3240, loss 0.227248, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3240

2017-10-10T11:57:06.957668: step 3241, loss 0.195512, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:57:07.151249: step 3242, loss 0.0997852, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:07.353090: step 3243, loss 0.113962, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:07.559602: step 3244, loss 0.0693701, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:57:07.720353: step 3245, loss 0.176261, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:07.884665: step 3246, loss 0.0737621, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:08.028334: step 3247, loss 0.12624, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:08.147528: step 3248, loss 0.223036, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:57:08.282482: step 3249, loss 0.235522, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:08.484485: step 3250, loss 0.0803882, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:08.651902: step 3251, loss 0.177319, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:08.820052: step 3252, loss 0.161613, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:09.012889: step 3253, loss 0.111577, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:57:09.168581: step 3254, loss 0.210897, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:09.370889: step 3255, loss 0.14693, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:09.556566: step 3256, loss 0.123345, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:09.718420: step 3257, loss 0.132978, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:09.888856: step 3258, loss 0.131702, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:10.087107: step 3259, loss 0.285094, acc 0.890625, learning_rate 0.000100008
2017-10-10T11:57:10.291998: step 3260, loss 0.155396, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:10.440825: step 3261, loss 0.143015, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:10.597564: step 3262, loss 0.164359, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:10.812082: step 3263, loss 0.0681996, acc 1, learning_rate 0.000100008
2017-10-10T11:57:11.016858: step 3264, loss 0.123595, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:11.160468: step 3265, loss 0.148584, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:11.340684: step 3266, loss 0.0664228, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:11.543772: step 3267, loss 0.173613, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:57:11.784931: step 3268, loss 0.0740809, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:11.988534: step 3269, loss 0.213946, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:57:12.185115: step 3270, loss 0.17057, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:57:12.313253: step 3271, loss 0.0897135, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:12.471019: step 3272, loss 0.166041, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:12.627349: step 3273, loss 0.115933, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:57:12.776936: step 3274, loss 0.169014, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:57:12.908850: step 3275, loss 0.221244, acc 0.90625, learning_rate 0.000100007
2017-10-10T11:57:13.044416: step 3276, loss 0.092213, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:13.163556: step 3277, loss 0.0939007, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:13.329409: step 3278, loss 0.121234, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:13.492199: step 3279, loss 0.244363, acc 0.90625, learning_rate 0.000100007
2017-10-10T11:57:13.672777: step 3280, loss 0.153897, acc 0.921875, learning_rate 0.000100007

Evaluation:
2017-10-10T11:57:14.081718: step 3280, loss 0.226972, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3280

2017-10-10T11:57:15.238569: step 3281, loss 0.0869751, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:15.432581: step 3282, loss 0.130045, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:57:15.624981: step 3283, loss 0.161945, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:57:15.789544: step 3284, loss 0.138211, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:15.983831: step 3285, loss 0.122778, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:16.136935: step 3286, loss 0.236599, acc 0.90625, learning_rate 0.000100007
2017-10-10T11:57:16.292261: step 3287, loss 0.0861644, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:16.523492: step 3288, loss 0.13661, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:16.729271: step 3289, loss 0.145614, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:16.891417: step 3290, loss 0.149129, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:17.046325: step 3291, loss 0.15032, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:57:17.236827: step 3292, loss 0.183403, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:57:17.412905: step 3293, loss 0.102635, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:17.664892: step 3294, loss 0.174514, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:57:17.829343: step 3295, loss 0.0717631, acc 1, learning_rate 0.000100007
2017-10-10T11:57:17.960868: step 3296, loss 0.18243, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:57:18.099090: step 3297, loss 0.187864, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:57:18.242243: step 3298, loss 0.126915, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:18.398373: step 3299, loss 0.114711, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:18.549496: step 3300, loss 0.106385, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:18.663418: step 3301, loss 0.150374, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:18.850296: step 3302, loss 0.191953, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:57:19.054958: step 3303, loss 0.145849, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:19.241608: step 3304, loss 0.130486, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:19.363516: step 3305, loss 0.164649, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:57:19.560220: step 3306, loss 0.0984792, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:57:19.678316: step 3307, loss 0.0546273, acc 1, learning_rate 0.000100007
2017-10-10T11:57:19.853673: step 3308, loss 0.168882, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:57:20.045439: step 3309, loss 0.293376, acc 0.890625, learning_rate 0.000100007
2017-10-10T11:57:20.188022: step 3310, loss 0.209382, acc 0.90625, learning_rate 0.000100006
2017-10-10T11:57:20.376548: step 3311, loss 0.135925, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:20.559854: step 3312, loss 0.120479, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:20.784426: step 3313, loss 0.130329, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:20.957121: step 3314, loss 0.330079, acc 0.890625, learning_rate 0.000100006
2017-10-10T11:57:21.129062: step 3315, loss 0.108614, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:21.339364: step 3316, loss 0.120883, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:21.516881: step 3317, loss 0.155189, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:21.704843: step 3318, loss 0.161148, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:21.889168: step 3319, loss 0.0873055, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:22.091389: step 3320, loss 0.153821, acc 0.953125, learning_rate 0.000100006

Evaluation:
2017-10-10T11:57:22.546672: step 3320, loss 0.226498, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3320

2017-10-10T11:57:23.686462: step 3321, loss 0.0988118, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:23.828917: step 3322, loss 0.113575, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:23.986346: step 3323, loss 0.178375, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:24.154472: step 3324, loss 0.1339, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:24.336507: step 3325, loss 0.152972, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:24.505004: step 3326, loss 0.0866324, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:24.672353: step 3327, loss 0.0852148, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:24.877033: step 3328, loss 0.170492, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:25.053458: step 3329, loss 0.167515, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:25.203642: step 3330, loss 0.160509, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:25.381826: step 3331, loss 0.154005, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:25.556842: step 3332, loss 0.365997, acc 0.882353, learning_rate 0.000100006
2017-10-10T11:57:25.776838: step 3333, loss 0.170775, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:25.969895: step 3334, loss 0.151889, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:26.148411: step 3335, loss 0.0996039, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:26.354913: step 3336, loss 0.117452, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:26.564651: step 3337, loss 0.207992, acc 0.921875, learning_rate 0.000100006
2017-10-10T11:57:26.714666: step 3338, loss 0.127383, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:26.884494: step 3339, loss 0.122283, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:27.107437: step 3340, loss 0.162043, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:57:27.282554: step 3341, loss 0.0764282, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:27.475151: step 3342, loss 0.0673015, acc 1, learning_rate 0.000100006
2017-10-10T11:57:27.633048: step 3343, loss 0.13103, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:27.896854: step 3344, loss 0.111764, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:57:28.126650: step 3345, loss 0.0732934, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:28.254106: step 3346, loss 0.253922, acc 0.921875, learning_rate 0.000100006
2017-10-10T11:57:28.364975: step 3347, loss 0.201787, acc 0.921875, learning_rate 0.000100006
2017-10-10T11:57:28.512865: step 3348, loss 0.0976197, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:57:28.648975: step 3349, loss 0.0649062, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:57:28.804837: step 3350, loss 0.0668439, acc 1, learning_rate 0.000100006
2017-10-10T11:57:28.923193: step 3351, loss 0.217335, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:29.083184: step 3352, loss 0.208144, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:29.254607: step 3353, loss 0.0730839, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:29.465181: step 3354, loss 0.0711302, acc 1, learning_rate 0.000100005
2017-10-10T11:57:29.666125: step 3355, loss 0.155964, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:29.839199: step 3356, loss 0.113026, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:29.998527: step 3357, loss 0.132747, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:30.177829: step 3358, loss 0.176845, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:30.360852: step 3359, loss 0.153323, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:30.540134: step 3360, loss 0.123035, acc 0.9375, learning_rate 0.000100005

Evaluation:
2017-10-10T11:57:30.914578: step 3360, loss 0.22542, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3360

2017-10-10T11:57:31.937839: step 3361, loss 0.204656, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:32.134076: step 3362, loss 0.105353, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:32.301062: step 3363, loss 0.158863, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:32.452927: step 3364, loss 0.2208, acc 0.890625, learning_rate 0.000100005
2017-10-10T11:57:32.652793: step 3365, loss 0.111364, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:32.786921: step 3366, loss 0.140219, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:32.957659: step 3367, loss 0.180963, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:33.174377: step 3368, loss 0.180374, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:33.373950: step 3369, loss 0.113089, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:33.501150: step 3370, loss 0.102372, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:33.675740: step 3371, loss 0.192816, acc 0.90625, learning_rate 0.000100005
2017-10-10T11:57:33.885080: step 3372, loss 0.210302, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:34.124860: step 3373, loss 0.153499, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:34.311086: step 3374, loss 0.201258, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:34.464309: step 3375, loss 0.122034, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:34.622400: step 3376, loss 0.250433, acc 0.90625, learning_rate 0.000100005
2017-10-10T11:57:34.776291: step 3377, loss 0.121522, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:34.906363: step 3378, loss 0.17641, acc 0.90625, learning_rate 0.000100005
2017-10-10T11:57:35.014376: step 3379, loss 0.107833, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:35.162582: step 3380, loss 0.0783264, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:35.329968: step 3381, loss 0.160076, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:35.511181: step 3382, loss 0.174763, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:35.720874: step 3383, loss 0.136966, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:35.891971: step 3384, loss 0.1567, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:36.048845: step 3385, loss 0.134442, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:36.238736: step 3386, loss 0.12042, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:36.381903: step 3387, loss 0.0675524, acc 1, learning_rate 0.000100005
2017-10-10T11:57:36.575852: step 3388, loss 0.0835768, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:36.780404: step 3389, loss 0.197389, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:36.944078: step 3390, loss 0.0657734, acc 1, learning_rate 0.000100005
2017-10-10T11:57:37.142042: step 3391, loss 0.131089, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:37.324159: step 3392, loss 0.200678, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:57:37.461090: step 3393, loss 0.145941, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:37.631410: step 3394, loss 0.173661, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:37.850845: step 3395, loss 0.0902392, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:38.042649: step 3396, loss 0.144625, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:57:38.178925: step 3397, loss 0.142659, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:57:38.382289: step 3398, loss 0.204945, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:57:38.588863: step 3399, loss 0.0492012, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:57:38.836916: step 3400, loss 0.243769, acc 0.921875, learning_rate 0.000100004

Evaluation:
2017-10-10T11:57:39.170825: step 3400, loss 0.225809, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3400

2017-10-10T11:57:40.214101: step 3401, loss 0.142945, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:40.413690: step 3402, loss 0.164549, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:40.553478: step 3403, loss 0.0578157, acc 1, learning_rate 0.000100004
2017-10-10T11:57:40.718230: step 3404, loss 0.0974024, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:40.939072: step 3405, loss 0.121033, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:41.093092: step 3406, loss 0.234427, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:41.265007: step 3407, loss 0.267602, acc 0.875, learning_rate 0.000100004
2017-10-10T11:57:41.499958: step 3408, loss 0.137952, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:41.680931: step 3409, loss 0.144029, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:41.845042: step 3410, loss 0.250008, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:42.115830: step 3411, loss 0.095321, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:42.303932: step 3412, loss 0.100733, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:42.489479: step 3413, loss 0.0509546, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:42.644392: step 3414, loss 0.115038, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:42.853004: step 3415, loss 0.133927, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:43.011235: step 3416, loss 0.169065, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:43.229107: step 3417, loss 0.137515, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:43.416861: step 3418, loss 0.109859, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:43.629038: step 3419, loss 0.191468, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:43.854272: step 3420, loss 0.2378, acc 0.90625, learning_rate 0.000100004
2017-10-10T11:57:44.048904: step 3421, loss 0.171097, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:44.212140: step 3422, loss 0.124186, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:44.362154: step 3423, loss 0.223792, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:44.578144: step 3424, loss 0.111774, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:44.729031: step 3425, loss 0.15103, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:44.881906: step 3426, loss 0.202231, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:45.135563: step 3427, loss 0.157769, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:45.352844: step 3428, loss 0.178876, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:45.506903: step 3429, loss 0.145703, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:45.620951: step 3430, loss 0.225644, acc 0.901961, learning_rate 0.000100004
2017-10-10T11:57:45.760507: step 3431, loss 0.0971332, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:45.919050: step 3432, loss 0.131312, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:46.048910: step 3433, loss 0.186053, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:46.164820: step 3434, loss 0.108467, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:46.327101: step 3435, loss 0.265741, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:46.473171: step 3436, loss 0.0958106, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:46.621084: step 3437, loss 0.123328, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:46.795807: step 3438, loss 0.244744, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:46.984133: step 3439, loss 0.0987697, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:47.099298: step 3440, loss 0.141534, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-10T11:57:47.551683: step 3440, loss 0.225206, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3440

2017-10-10T11:57:48.648098: step 3441, loss 0.179982, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:48.852881: step 3442, loss 0.143169, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:49.005126: step 3443, loss 0.12344, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:49.228704: step 3444, loss 0.168947, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:49.439545: step 3445, loss 0.146468, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:57:49.592828: step 3446, loss 0.241079, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:49.710917: step 3447, loss 0.190197, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:49.858482: step 3448, loss 0.168328, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:57:50.010866: step 3449, loss 0.172653, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:50.116827: step 3450, loss 0.103382, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:50.270480: step 3451, loss 0.0742091, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:50.429643: step 3452, loss 0.239031, acc 0.875, learning_rate 0.000100004
2017-10-10T11:57:50.570407: step 3453, loss 0.0475698, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:50.727063: step 3454, loss 0.110819, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:50.932454: step 3455, loss 0.0767572, acc 1, learning_rate 0.000100004
2017-10-10T11:57:51.076920: step 3456, loss 0.14934, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:51.231078: step 3457, loss 0.154543, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:51.444951: step 3458, loss 0.132423, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:51.636614: step 3459, loss 0.0936403, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:57:51.808967: step 3460, loss 0.140805, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:57:51.964119: step 3461, loss 0.168101, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:57:52.160237: step 3462, loss 0.163377, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:57:52.383654: step 3463, loss 0.124363, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:57:52.560934: step 3464, loss 0.249098, acc 0.890625, learning_rate 0.000100003
2017-10-10T11:57:52.701270: step 3465, loss 0.170225, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:57:52.913062: step 3466, loss 0.234419, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:57:53.127560: step 3467, loss 0.185587, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:57:53.343653: step 3468, loss 0.219154, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:57:53.492475: step 3469, loss 0.0785844, acc 1, learning_rate 0.000100003
2017-10-10T11:57:53.696922: step 3470, loss 0.128508, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:57:53.892831: step 3471, loss 0.0787815, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:57:54.100292: step 3472, loss 0.0879318, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:57:54.253888: step 3473, loss 0.0822505, acc 1, learning_rate 0.000100003
2017-10-10T11:57:54.430139: step 3474, loss 0.198664, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:57:54.614517: step 3475, loss 0.0996973, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:57:54.780876: step 3476, loss 0.103171, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:57:54.924945: step 3477, loss 0.0679087, acc 1, learning_rate 0.000100003
2017-10-10T11:57:55.146208: step 3478, loss 0.128285, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:57:55.354299: step 3479, loss 0.134732, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:57:55.468112: step 3480, loss 0.221813, acc 0.921875, learning_rate 0.000100003

Evaluation:
2017-10-10T11:57:55.822101: step 3480, loss 0.223884, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3480

2017-10-10T11:57:57.088853: step 3481, loss 0.229521, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:57:57.232849: step 3482, loss 0.14251, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:57:57.372384: step 3483, loss 0.116338, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:57:57.533929: step 3484, loss 0.182952, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:57:57.672858: step 3485, loss 0.177717, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:57:57.860886: step 3486, loss 0.0748982, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:57:58.051410: step 3487, loss 0.0523693, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:57:58.228806: step 3488, loss 0.101914, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:57:58.416823: step 3489, loss 0.0708871, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:57:58.626367: step 3490, loss 0.17163, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:57:58.838135: step 3491, loss 0.177115, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:57:59.035926: step 3492, loss 0.1155, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:57:59.184881: step 3493, loss 0.157404, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:57:59.362763: step 3494, loss 0.128397, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:57:59.548415: step 3495, loss 0.160319, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:57:59.796903: step 3496, loss 0.104154, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:57:59.997523: step 3497, loss 0.139556, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:00.114449: step 3498, loss 0.266573, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:58:00.242053: step 3499, loss 0.13258, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:00.416737: step 3500, loss 0.209854, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:00.542303: step 3501, loss 0.134799, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:00.657568: step 3502, loss 0.244343, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:00.820568: step 3503, loss 0.164659, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:01.049648: step 3504, loss 0.143354, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:01.242808: step 3505, loss 0.159993, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:01.368827: step 3506, loss 0.130235, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:01.571946: step 3507, loss 0.13865, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:01.790809: step 3508, loss 0.0914617, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:01.992926: step 3509, loss 0.170931, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:02.136885: step 3510, loss 0.195237, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:02.339893: step 3511, loss 0.213537, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:02.552892: step 3512, loss 0.230863, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:02.721181: step 3513, loss 0.0940162, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:02.897027: step 3514, loss 0.21306, acc 0.890625, learning_rate 0.000100003
2017-10-10T11:58:03.094558: step 3515, loss 0.19056, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:03.284455: step 3516, loss 0.0970029, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:03.401941: step 3517, loss 0.0947885, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:03.593693: step 3518, loss 0.118889, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:03.788858: step 3519, loss 0.150501, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:03.939748: step 3520, loss 0.237699, acc 0.90625, learning_rate 0.000100003

Evaluation:
2017-10-10T11:58:04.371168: step 3520, loss 0.225703, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3520

2017-10-10T11:58:05.344839: step 3521, loss 0.222975, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:58:05.497369: step 3522, loss 0.224167, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:05.681047: step 3523, loss 0.113955, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:05.860046: step 3524, loss 0.129848, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:06.040952: step 3525, loss 0.168253, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:06.197117: step 3526, loss 0.0867202, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:58:06.396521: step 3527, loss 0.118379, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:06.539799: step 3528, loss 0.0492084, acc 1, learning_rate 0.000100003
2017-10-10T11:58:06.746816: step 3529, loss 0.279005, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:58:06.934631: step 3530, loss 0.103291, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:07.092520: step 3531, loss 0.309659, acc 0.875, learning_rate 0.000100003
2017-10-10T11:58:07.268855: step 3532, loss 0.0789511, acc 1, learning_rate 0.000100003
2017-10-10T11:58:07.517015: step 3533, loss 0.191333, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:07.737310: step 3534, loss 0.208589, acc 0.875, learning_rate 0.000100003
2017-10-10T11:58:07.952855: step 3535, loss 0.0914126, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:08.089586: step 3536, loss 0.095837, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:58:08.202489: step 3537, loss 0.104996, acc 1, learning_rate 0.000100003
2017-10-10T11:58:08.350171: step 3538, loss 0.23466, acc 0.890625, learning_rate 0.000100003
2017-10-10T11:58:08.492544: step 3539, loss 0.186985, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:08.628843: step 3540, loss 0.140614, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:08.746476: step 3541, loss 0.140524, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:08.933013: step 3542, loss 0.128363, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:58:09.109573: step 3543, loss 0.14193, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:58:09.316308: step 3544, loss 0.164346, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:09.504867: step 3545, loss 0.0650851, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:09.657007: step 3546, loss 0.214493, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:09.851061: step 3547, loss 0.110473, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:10.068406: step 3548, loss 0.105145, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:10.286671: step 3549, loss 0.189555, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:58:10.505171: step 3550, loss 0.12315, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:10.667050: step 3551, loss 0.0901356, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:10.826641: step 3552, loss 0.0757617, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:10.986865: step 3553, loss 0.161782, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:11.132849: step 3554, loss 0.141815, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:11.276983: step 3555, loss 0.144905, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:11.396425: step 3556, loss 0.0702586, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:11.559541: step 3557, loss 0.208088, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:11.736775: step 3558, loss 0.139997, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:11.880899: step 3559, loss 0.157869, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:12.078661: step 3560, loss 0.107467, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T11:58:12.532910: step 3560, loss 0.226287, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3560

2017-10-10T11:58:13.673927: step 3561, loss 0.0997846, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:13.872164: step 3562, loss 0.112716, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:14.017074: step 3563, loss 0.105162, acc 1, learning_rate 0.000100002
2017-10-10T11:58:14.189539: step 3564, loss 0.0989264, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:14.389369: step 3565, loss 0.0983036, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:14.600954: step 3566, loss 0.088937, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:14.748885: step 3567, loss 0.0831165, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:14.949135: step 3568, loss 0.171379, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:58:15.105451: step 3569, loss 0.102811, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:15.299853: step 3570, loss 0.209404, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:15.465054: step 3571, loss 0.178491, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:15.639875: step 3572, loss 0.214824, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:15.822872: step 3573, loss 0.201459, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:15.972866: step 3574, loss 0.206009, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:16.188424: step 3575, loss 0.0898026, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:16.403478: step 3576, loss 0.140809, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:16.600844: step 3577, loss 0.11983, acc 1, learning_rate 0.000100002
2017-10-10T11:58:16.738574: step 3578, loss 0.154314, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:16.923141: step 3579, loss 0.138511, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:17.132732: step 3580, loss 0.117123, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:17.312569: step 3581, loss 0.159067, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:17.466691: step 3582, loss 0.092821, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:17.631408: step 3583, loss 0.194035, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:17.822352: step 3584, loss 0.181151, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:58:17.968019: step 3585, loss 0.109505, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:18.139259: step 3586, loss 0.246846, acc 0.875, learning_rate 0.000100002
2017-10-10T11:58:18.343734: step 3587, loss 0.0818326, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:18.545492: step 3588, loss 0.172111, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:18.758394: step 3589, loss 0.0851926, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:18.953249: step 3590, loss 0.217823, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:19.120758: step 3591, loss 0.0773637, acc 1, learning_rate 0.000100002
2017-10-10T11:58:19.264860: step 3592, loss 0.142207, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:19.388997: step 3593, loss 0.115234, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:19.520818: step 3594, loss 0.0864576, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:19.668607: step 3595, loss 0.140345, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:19.816119: step 3596, loss 0.135455, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:19.944841: step 3597, loss 0.162325, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:20.156034: step 3598, loss 0.109508, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:20.320631: step 3599, loss 0.121734, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:20.492830: step 3600, loss 0.112965, acc 0.953125, learning_rate 0.000100002

Evaluation:
2017-10-10T11:58:20.960844: step 3600, loss 0.224677, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3600

2017-10-10T11:58:22.208892: step 3601, loss 0.142562, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:22.388847: step 3602, loss 0.0857539, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:22.609666: step 3603, loss 0.0910245, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:22.796890: step 3604, loss 0.0658011, acc 1, learning_rate 0.000100002
2017-10-10T11:58:22.916959: step 3605, loss 0.149905, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:23.097245: step 3606, loss 0.11779, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:23.267689: step 3607, loss 0.181941, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:23.444281: step 3608, loss 0.160344, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:23.631812: step 3609, loss 0.0781834, acc 1, learning_rate 0.000100002
2017-10-10T11:58:23.827243: step 3610, loss 0.0994512, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:24.034782: step 3611, loss 0.132007, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:24.200961: step 3612, loss 0.120687, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:24.422842: step 3613, loss 0.151259, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:24.552744: step 3614, loss 0.124837, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:24.747530: step 3615, loss 0.193632, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:24.928894: step 3616, loss 0.200969, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:25.072850: step 3617, loss 0.121537, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:25.267515: step 3618, loss 0.102616, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:25.452821: step 3619, loss 0.116879, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:25.629836: step 3620, loss 0.177461, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:58:25.770838: step 3621, loss 0.182684, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:25.975550: step 3622, loss 0.210875, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:26.178530: step 3623, loss 0.145214, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:26.386960: step 3624, loss 0.207891, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:26.584920: step 3625, loss 0.129399, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:26.685796: step 3626, loss 0.219777, acc 0.882353, learning_rate 0.000100002
2017-10-10T11:58:26.892807: step 3627, loss 0.214679, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:27.080963: step 3628, loss 0.113249, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:27.228071: step 3629, loss 0.0919923, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:27.401636: step 3630, loss 0.12992, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:27.622429: step 3631, loss 0.189735, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:27.816681: step 3632, loss 0.202773, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:28.003832: step 3633, loss 0.0704082, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:28.154693: step 3634, loss 0.204696, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:28.343974: step 3635, loss 0.174692, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:58:28.501716: step 3636, loss 0.22069, acc 0.890625, learning_rate 0.000100002
2017-10-10T11:58:28.661120: step 3637, loss 0.0761561, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:28.856959: step 3638, loss 0.157624, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:29.073233: step 3639, loss 0.0802547, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:29.310322: step 3640, loss 0.076628, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T11:58:29.780312: step 3640, loss 0.225417, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3640

2017-10-10T11:58:30.681294: step 3641, loss 0.1728, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:30.832273: step 3642, loss 0.216056, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:31.001002: step 3643, loss 0.160425, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:31.200537: step 3644, loss 0.173141, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:31.424362: step 3645, loss 0.0969828, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:31.576839: step 3646, loss 0.0866802, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:31.694429: step 3647, loss 0.158419, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:31.845510: step 3648, loss 0.111848, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:31.988329: step 3649, loss 0.133829, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:32.128064: step 3650, loss 0.10371, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:32.263374: step 3651, loss 0.142503, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:32.456843: step 3652, loss 0.137761, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:32.672768: step 3653, loss 0.0927336, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:32.864931: step 3654, loss 0.169498, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:58:33.049243: step 3655, loss 0.133893, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:33.200468: step 3656, loss 0.0940248, acc 1, learning_rate 0.000100002
2017-10-10T11:58:33.394212: step 3657, loss 0.109836, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:33.592873: step 3658, loss 0.124836, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:33.776871: step 3659, loss 0.124006, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:33.902671: step 3660, loss 0.135995, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:58:34.111701: step 3661, loss 0.125316, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:58:34.303705: step 3662, loss 0.195365, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:34.473116: step 3663, loss 0.215174, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:34.661577: step 3664, loss 0.183072, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:58:34.831046: step 3665, loss 0.117845, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:35.046207: step 3666, loss 0.172247, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:58:35.206967: step 3667, loss 0.0942756, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:35.368839: step 3668, loss 0.0948768, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:58:35.562871: step 3669, loss 0.0692412, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:35.776853: step 3670, loss 0.0681556, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:35.936322: step 3671, loss 0.187801, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:36.107654: step 3672, loss 0.230667, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:36.313280: step 3673, loss 0.158956, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:36.514409: step 3674, loss 0.182145, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:36.685022: step 3675, loss 0.10546, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:36.843130: step 3676, loss 0.0948877, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:37.032864: step 3677, loss 0.155098, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:37.235186: step 3678, loss 0.206496, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:37.410764: step 3679, loss 0.0418258, acc 1, learning_rate 0.000100001
2017-10-10T11:58:37.605077: step 3680, loss 0.109271, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T11:58:38.020939: step 3680, loss 0.223627, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3680

2017-10-10T11:58:39.160852: step 3681, loss 0.0610221, acc 1, learning_rate 0.000100001
2017-10-10T11:58:39.346508: step 3682, loss 0.0919883, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:39.524197: step 3683, loss 0.117272, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:39.680670: step 3684, loss 0.0851379, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:39.859594: step 3685, loss 0.112984, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:40.030939: step 3686, loss 0.105173, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:40.176474: step 3687, loss 0.234854, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:58:40.398654: step 3688, loss 0.102754, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:40.596870: step 3689, loss 0.169723, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:40.832883: step 3690, loss 0.151791, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:41.049285: step 3691, loss 0.209577, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:41.172772: step 3692, loss 0.0785987, acc 1, learning_rate 0.000100001
2017-10-10T11:58:41.308077: step 3693, loss 0.126523, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:41.445167: step 3694, loss 0.197771, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:41.635048: step 3695, loss 0.128912, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:41.788936: step 3696, loss 0.149417, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:41.922572: step 3697, loss 0.145493, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:42.094089: step 3698, loss 0.140577, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:42.255320: step 3699, loss 0.274431, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:42.400843: step 3700, loss 0.17227, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:42.528200: step 3701, loss 0.185896, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:42.657832: step 3702, loss 0.199026, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:58:42.815312: step 3703, loss 0.112305, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:42.972562: step 3704, loss 0.118643, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:43.117361: step 3705, loss 0.119523, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:43.292978: step 3706, loss 0.109859, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:43.505681: step 3707, loss 0.0814412, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:43.708854: step 3708, loss 0.0963388, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:43.850723: step 3709, loss 0.136077, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:44.019733: step 3710, loss 0.157055, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:44.218352: step 3711, loss 0.213836, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:44.424279: step 3712, loss 0.0820632, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:44.594891: step 3713, loss 0.0998086, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:44.744810: step 3714, loss 0.198568, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:44.929279: step 3715, loss 0.167641, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:45.128859: step 3716, loss 0.145792, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:45.292076: step 3717, loss 0.162981, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:45.473813: step 3718, loss 0.105619, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:45.661066: step 3719, loss 0.109016, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:45.836858: step 3720, loss 0.23745, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:58:46.260600: step 3720, loss 0.222433, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3720

2017-10-10T11:58:47.518545: step 3721, loss 0.0884509, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:47.653313: step 3722, loss 0.179684, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:58:47.825589: step 3723, loss 0.0683011, acc 1, learning_rate 0.000100001
2017-10-10T11:58:48.040596: step 3724, loss 0.177868, acc 0.901961, learning_rate 0.000100001
2017-10-10T11:58:48.218620: step 3725, loss 0.152162, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:48.339048: step 3726, loss 0.127812, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:48.544598: step 3727, loss 0.193004, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:48.689053: step 3728, loss 0.141197, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:48.862950: step 3729, loss 0.134933, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:49.065073: step 3730, loss 0.224823, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:49.244849: step 3731, loss 0.190531, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:49.424856: step 3732, loss 0.145995, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:49.576853: step 3733, loss 0.117523, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:49.793905: step 3734, loss 0.137649, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:49.995247: step 3735, loss 0.102614, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:50.213528: step 3736, loss 0.103488, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:50.411180: step 3737, loss 0.0876179, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:50.580957: step 3738, loss 0.124624, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:50.760931: step 3739, loss 0.172656, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:50.943118: step 3740, loss 0.175462, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:51.144818: step 3741, loss 0.222441, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:58:51.284899: step 3742, loss 0.26199, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:58:51.572927: step 3743, loss 0.0951423, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:51.786571: step 3744, loss 0.163834, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:51.943704: step 3745, loss 0.110419, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:52.107824: step 3746, loss 0.0674096, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:52.265451: step 3747, loss 0.0779819, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:52.420635: step 3748, loss 0.10779, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:52.624611: step 3749, loss 0.187996, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:58:52.772859: step 3750, loss 0.119599, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:52.921668: step 3751, loss 0.148001, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:53.056837: step 3752, loss 0.120386, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:53.180847: step 3753, loss 0.0827619, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:53.322454: step 3754, loss 0.25211, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:58:53.480940: step 3755, loss 0.114526, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:53.641118: step 3756, loss 0.113454, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:53.812887: step 3757, loss 0.182317, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:53.993615: step 3758, loss 0.0889431, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:54.184899: step 3759, loss 0.1568, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:54.335392: step 3760, loss 0.168015, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T11:58:54.768190: step 3760, loss 0.225143, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3760

2017-10-10T11:58:55.692980: step 3761, loss 0.176873, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:58:55.914766: step 3762, loss 0.10878, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:56.073075: step 3763, loss 0.101825, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:56.213554: step 3764, loss 0.212594, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:56.414364: step 3765, loss 0.0744579, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:56.600956: step 3766, loss 0.133341, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:56.760875: step 3767, loss 0.112098, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:56.957744: step 3768, loss 0.165649, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:57.137472: step 3769, loss 0.122798, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:57.301200: step 3770, loss 0.158352, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:57.468906: step 3771, loss 0.196863, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:58:57.646704: step 3772, loss 0.174929, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:57.856218: step 3773, loss 0.114365, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:58.032980: step 3774, loss 0.147328, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:58.218514: step 3775, loss 0.0792813, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:58.425551: step 3776, loss 0.153365, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:58:58.613739: step 3777, loss 0.0817244, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:58.758915: step 3778, loss 0.0878272, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:58.939800: step 3779, loss 0.135023, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:58:59.145027: step 3780, loss 0.12881, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:59.317857: step 3781, loss 0.0874784, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:59.461383: step 3782, loss 0.0964747, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:59.644826: step 3783, loss 0.138438, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:58:59.827814: step 3784, loss 0.0397279, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:58:59.995672: step 3785, loss 0.1587, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:00.217007: step 3786, loss 0.0917025, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:00.410757: step 3787, loss 0.167007, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:00.574593: step 3788, loss 0.102055, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:00.751720: step 3789, loss 0.143624, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:00.955790: step 3790, loss 0.275819, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:01.105566: step 3791, loss 0.0985877, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:01.265044: step 3792, loss 0.126117, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:01.489484: step 3793, loss 0.2499, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:01.701527: step 3794, loss 0.117628, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:01.834258: step 3795, loss 0.0898927, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:02.032884: step 3796, loss 0.116554, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:02.256344: step 3797, loss 0.188286, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:02.461000: step 3798, loss 0.0742189, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:02.636899: step 3799, loss 0.127084, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:02.899623: step 3800, loss 0.135687, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:03.442537: step 3800, loss 0.222927, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3800

2017-10-10T11:59:04.353723: step 3801, loss 0.0624575, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:04.524939: step 3802, loss 0.194381, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:04.728286: step 3803, loss 0.119764, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:04.900983: step 3804, loss 0.101995, acc 1, learning_rate 0.000100001
2017-10-10T11:59:05.096598: step 3805, loss 0.218026, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:05.323628: step 3806, loss 0.158976, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:05.473111: step 3807, loss 0.112403, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:05.632840: step 3808, loss 0.126566, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:05.845055: step 3809, loss 0.164383, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:06.059960: step 3810, loss 0.152786, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:06.197277: step 3811, loss 0.192513, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:06.363244: step 3812, loss 0.135271, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:06.577127: step 3813, loss 0.160191, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:06.792854: step 3814, loss 0.164918, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:06.945012: step 3815, loss 0.110462, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:07.108839: step 3816, loss 0.104833, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:07.310816: step 3817, loss 0.154931, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:07.527647: step 3818, loss 0.144856, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:07.706837: step 3819, loss 0.123735, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:07.853202: step 3820, loss 0.14403, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:08.062145: step 3821, loss 0.191276, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:08.263519: step 3822, loss 0.132029, acc 0.941176, learning_rate 0.000100001
2017-10-10T11:59:08.429032: step 3823, loss 0.212967, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:08.572423: step 3824, loss 0.142778, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:08.780791: step 3825, loss 0.104486, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:08.961841: step 3826, loss 0.118822, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:09.132973: step 3827, loss 0.13415, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:09.300972: step 3828, loss 0.0924984, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:09.504340: step 3829, loss 0.052563, acc 1, learning_rate 0.000100001
2017-10-10T11:59:09.690496: step 3830, loss 0.114869, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:09.866230: step 3831, loss 0.123451, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:10.026661: step 3832, loss 0.138626, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:10.206285: step 3833, loss 0.150896, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:10.375350: step 3834, loss 0.150576, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:10.556899: step 3835, loss 0.0851052, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:10.772340: step 3836, loss 0.162802, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:10.979410: step 3837, loss 0.189727, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:11.199526: step 3838, loss 0.114107, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:11.340917: step 3839, loss 0.0732135, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:11.536888: step 3840, loss 0.121646, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:11.991471: step 3840, loss 0.222284, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3840

2017-10-10T11:59:13.312862: step 3841, loss 0.10639, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:13.535220: step 3842, loss 0.331698, acc 0.875, learning_rate 0.000100001
2017-10-10T11:59:13.772879: step 3843, loss 0.122066, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:13.946646: step 3844, loss 0.0900603, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:14.144875: step 3845, loss 0.126396, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:14.330260: step 3846, loss 0.0836271, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:14.499825: step 3847, loss 0.309671, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:59:14.598225: step 3848, loss 0.100429, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:14.695038: step 3849, loss 0.174428, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:14.791112: step 3850, loss 0.136363, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:14.889167: step 3851, loss 0.195451, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:14.998471: step 3852, loss 0.114063, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:15.156833: step 3853, loss 0.0801155, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:15.352017: step 3854, loss 0.118969, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:15.544844: step 3855, loss 0.0880705, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:15.701071: step 3856, loss 0.189812, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:15.858414: step 3857, loss 0.109043, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:16.062111: step 3858, loss 0.0818122, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:16.268831: step 3859, loss 0.128832, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:16.416613: step 3860, loss 0.0576084, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:16.576739: step 3861, loss 0.115793, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:16.788201: step 3862, loss 0.186789, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:16.968959: step 3863, loss 0.123745, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:17.115671: step 3864, loss 0.260743, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:59:17.320860: step 3865, loss 0.135377, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:17.520856: step 3866, loss 0.0970537, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:17.730840: step 3867, loss 0.171648, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:17.928830: step 3868, loss 0.130024, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:18.093020: step 3869, loss 0.235418, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:18.263012: step 3870, loss 0.069958, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:18.474190: step 3871, loss 0.184784, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:18.630972: step 3872, loss 0.0884964, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:18.790661: step 3873, loss 0.0913838, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:18.997164: step 3874, loss 0.207634, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:19.213005: step 3875, loss 0.258707, acc 0.875, learning_rate 0.000100001
2017-10-10T11:59:19.420806: step 3876, loss 0.103688, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:19.593051: step 3877, loss 0.130989, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:19.763907: step 3878, loss 0.0993603, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:19.982346: step 3879, loss 0.196865, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:20.100951: step 3880, loss 0.0888222, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:20.570175: step 3880, loss 0.223969, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3880

2017-10-10T11:59:21.607357: step 3881, loss 0.103894, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:21.809282: step 3882, loss 0.145693, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:22.026426: step 3883, loss 0.147225, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:22.180703: step 3884, loss 0.0991216, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:22.368742: step 3885, loss 0.164894, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:22.554377: step 3886, loss 0.165839, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:22.734701: step 3887, loss 0.0610433, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:22.880816: step 3888, loss 0.103645, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:23.089305: step 3889, loss 0.117032, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:23.276994: step 3890, loss 0.0923715, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:23.444933: step 3891, loss 0.176622, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:23.625812: step 3892, loss 0.156151, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:23.835029: step 3893, loss 0.122305, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:24.026761: step 3894, loss 0.13883, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:24.194977: step 3895, loss 0.251665, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:59:24.420974: step 3896, loss 0.191515, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:24.629217: step 3897, loss 0.181337, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:24.820318: step 3898, loss 0.120354, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:25.058450: step 3899, loss 0.140604, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:25.263526: step 3900, loss 0.163186, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:25.369947: step 3901, loss 0.196444, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:25.494919: step 3902, loss 0.180265, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:25.616880: step 3903, loss 0.198869, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:25.733968: step 3904, loss 0.238941, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:25.891884: step 3905, loss 0.0866658, acc 1, learning_rate 0.000100001
2017-10-10T11:59:26.044719: step 3906, loss 0.128002, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:26.207056: step 3907, loss 0.213602, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:26.368981: step 3908, loss 0.12826, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:26.547100: step 3909, loss 0.0799001, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:26.725132: step 3910, loss 0.0921899, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:26.929783: step 3911, loss 0.192875, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:27.088776: step 3912, loss 0.123095, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:27.232942: step 3913, loss 0.14529, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:27.426632: step 3914, loss 0.182047, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:27.639354: step 3915, loss 0.131845, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:27.830187: step 3916, loss 0.0780841, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:27.965259: step 3917, loss 0.149644, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:28.152395: step 3918, loss 0.102248, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:28.339245: step 3919, loss 0.160768, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:28.498917: step 3920, loss 0.176728, acc 0.921569, learning_rate 0.000100001

Evaluation:
2017-10-10T11:59:28.944849: step 3920, loss 0.222018, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3920

2017-10-10T11:59:30.152896: step 3921, loss 0.171131, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:59:30.380845: step 3922, loss 0.196699, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:30.562722: step 3923, loss 0.155887, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:59:30.700697: step 3924, loss 0.147344, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:30.894362: step 3925, loss 0.114706, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:31.102527: step 3926, loss 0.149751, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:31.302912: step 3927, loss 0.122265, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:31.443382: step 3928, loss 0.173599, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:31.631965: step 3929, loss 0.0998505, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:31.830913: step 3930, loss 0.136605, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:32.032957: step 3931, loss 0.0901522, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:59:32.205117: step 3932, loss 0.131465, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:59:32.389041: step 3933, loss 0.0844356, acc 1, learning_rate 0.000100001
2017-10-10T11:59:32.607178: step 3934, loss 0.105304, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:59:32.756843: step 3935, loss 0.115322, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:32.952046: step 3936, loss 0.0850738, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:59:33.152459: step 3937, loss 0.198169, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:33.350175: step 3938, loss 0.228509, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:33.476887: step 3939, loss 0.219466, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:33.672767: step 3940, loss 0.0975964, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:33.886987: step 3941, loss 0.0790554, acc 1, learning_rate 0.0001
2017-10-10T11:59:34.036975: step 3942, loss 0.161759, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:34.208856: step 3943, loss 0.133562, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:34.398844: step 3944, loss 0.0557792, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:34.623002: step 3945, loss 0.0798179, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:34.833743: step 3946, loss 0.117534, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:35.001763: step 3947, loss 0.167387, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:35.167077: step 3948, loss 0.136377, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:35.331413: step 3949, loss 0.130455, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:35.472977: step 3950, loss 0.191354, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:35.617266: step 3951, loss 0.199326, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:35.748023: step 3952, loss 0.0861911, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:35.957787: step 3953, loss 0.0959396, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:36.188858: step 3954, loss 0.128842, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:36.433014: step 3955, loss 0.155642, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:36.593848: step 3956, loss 0.0854063, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:36.744047: step 3957, loss 0.0822939, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:36.907766: step 3958, loss 0.114587, acc 1, learning_rate 0.0001
2017-10-10T11:59:37.058941: step 3959, loss 0.139198, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:37.189325: step 3960, loss 0.123396, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:37.519989: step 3960, loss 0.221168, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-3960

2017-10-10T11:59:38.829336: step 3961, loss 0.249662, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:38.964458: step 3962, loss 0.0892406, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:39.134144: step 3963, loss 0.0904561, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:39.351192: step 3964, loss 0.216797, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:39.535138: step 3965, loss 0.181955, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:39.664347: step 3966, loss 0.147842, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:39.848797: step 3967, loss 0.128247, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:40.053027: step 3968, loss 0.0784371, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:40.236867: step 3969, loss 0.188829, acc 0.90625, learning_rate 0.0001
2017-10-10T11:59:40.416860: step 3970, loss 0.158587, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:40.592903: step 3971, loss 0.161202, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:40.772588: step 3972, loss 0.100194, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:40.960026: step 3973, loss 0.0673496, acc 1, learning_rate 0.0001
2017-10-10T11:59:41.153027: step 3974, loss 0.160008, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:41.372872: step 3975, loss 0.144695, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:41.582204: step 3976, loss 0.155146, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:41.770322: step 3977, loss 0.108334, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:41.883650: step 3978, loss 0.210857, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:42.072758: step 3979, loss 0.130264, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:42.218009: step 3980, loss 0.176438, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:42.374214: step 3981, loss 0.0590097, acc 1, learning_rate 0.0001
2017-10-10T11:59:42.596920: step 3982, loss 0.075738, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:42.771233: step 3983, loss 0.163398, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:42.933059: step 3984, loss 0.199901, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:43.158859: step 3985, loss 0.0686818, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:43.335005: step 3986, loss 0.233677, acc 0.90625, learning_rate 0.0001
2017-10-10T11:59:43.498609: step 3987, loss 0.11265, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:43.670737: step 3988, loss 0.226856, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:43.843702: step 3989, loss 0.200861, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:44.025093: step 3990, loss 0.222289, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:44.177072: step 3991, loss 0.109889, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:44.375158: step 3992, loss 0.192723, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:44.553561: step 3993, loss 0.133573, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:44.704836: step 3994, loss 0.160402, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:44.945637: step 3995, loss 0.183315, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:45.192366: step 3996, loss 0.141031, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:45.356163: step 3997, loss 0.246133, acc 0.90625, learning_rate 0.0001
2017-10-10T11:59:45.512678: step 3998, loss 0.111705, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:45.640853: step 3999, loss 0.133023, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:45.786398: step 4000, loss 0.152642, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:46.145033: step 4000, loss 0.221464, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4000

2017-10-10T11:59:47.148889: step 4001, loss 0.143506, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:47.380913: step 4002, loss 0.20417, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:47.582491: step 4003, loss 0.0901799, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:47.733003: step 4004, loss 0.0954947, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:47.875780: step 4005, loss 0.103406, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:48.034107: step 4006, loss 0.129057, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:48.186704: step 4007, loss 0.161002, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:48.336842: step 4008, loss 0.128363, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:48.481254: step 4009, loss 0.079913, acc 1, learning_rate 0.0001
2017-10-10T11:59:48.672915: step 4010, loss 0.184444, acc 0.90625, learning_rate 0.0001
2017-10-10T11:59:48.848073: step 4011, loss 0.158196, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:49.066877: step 4012, loss 0.15828, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:49.222599: step 4013, loss 0.147617, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:49.376634: step 4014, loss 0.126265, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:49.602522: step 4015, loss 0.211998, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:49.755239: step 4016, loss 0.127017, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:49.921297: step 4017, loss 0.159944, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:50.116139: step 4018, loss 0.151952, acc 0.941176, learning_rate 0.0001
2017-10-10T11:59:50.328680: step 4019, loss 0.0680446, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:50.499315: step 4020, loss 0.147136, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:50.652826: step 4021, loss 0.119805, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:50.835328: step 4022, loss 0.341105, acc 0.859375, learning_rate 0.0001
2017-10-10T11:59:50.998482: step 4023, loss 0.0625169, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:51.140837: step 4024, loss 0.12961, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:51.328818: step 4025, loss 0.109423, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:51.508559: step 4026, loss 0.215101, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:51.683503: step 4027, loss 0.099854, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:51.858957: step 4028, loss 0.185826, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:52.032836: step 4029, loss 0.131858, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:52.214411: step 4030, loss 0.115127, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:52.371415: step 4031, loss 0.115134, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:52.561079: step 4032, loss 0.193893, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:52.701664: step 4033, loss 0.0451964, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:52.867519: step 4034, loss 0.114921, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:53.043600: step 4035, loss 0.129009, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:53.228897: step 4036, loss 0.113196, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:53.398385: step 4037, loss 0.0802957, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:53.567116: step 4038, loss 0.0898814, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:53.752816: step 4039, loss 0.180233, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:53.921015: step 4040, loss 0.217143, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:54.339637: step 4040, loss 0.221285, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4040

2017-10-10T11:59:55.469268: step 4041, loss 0.103985, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:55.704862: step 4042, loss 0.0871342, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:55.908912: step 4043, loss 0.0719666, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:56.099191: step 4044, loss 0.0674067, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:56.216380: step 4045, loss 0.137907, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:56.370609: step 4046, loss 0.263705, acc 0.890625, learning_rate 0.0001
2017-10-10T11:59:56.536024: step 4047, loss 0.149421, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:56.653021: step 4048, loss 0.0890625, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:56.792821: step 4049, loss 0.0637313, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:56.972852: step 4050, loss 0.133117, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:57.164897: step 4051, loss 0.0637184, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:57.335966: step 4052, loss 0.202545, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:57.505885: step 4053, loss 0.0644314, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:57.704907: step 4054, loss 0.0935964, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:57.831126: step 4055, loss 0.206296, acc 0.90625, learning_rate 0.0001
2017-10-10T11:59:58.031859: step 4056, loss 0.0920407, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:58.252868: step 4057, loss 0.13862, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:58.463824: step 4058, loss 0.0899633, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:58.689109: step 4059, loss 0.163229, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:58.829072: step 4060, loss 0.0785758, acc 1, learning_rate 0.0001
2017-10-10T11:59:58.976219: step 4061, loss 0.179213, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:59.105260: step 4062, loss 0.156434, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:59.244136: step 4063, loss 0.120782, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:59.372876: step 4064, loss 0.215137, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:59.500783: step 4065, loss 0.100648, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:59.630171: step 4066, loss 0.149768, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:59.807868: step 4067, loss 0.169123, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:59.964826: step 4068, loss 0.0650601, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:00.169113: step 4069, loss 0.133786, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:00.352896: step 4070, loss 0.107197, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:00.576922: step 4071, loss 0.336356, acc 0.875, learning_rate 0.0001
2017-10-10T12:00:00.726444: step 4072, loss 0.100909, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:00.908826: step 4073, loss 0.13695, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:01.131670: step 4074, loss 0.112349, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:01.269173: step 4075, loss 0.190247, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:01.446857: step 4076, loss 0.153575, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:01.665514: step 4077, loss 0.0801975, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:01.870077: step 4078, loss 0.102836, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:02.044872: step 4079, loss 0.141216, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:02.195228: step 4080, loss 0.163473, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:02.680515: step 4080, loss 0.222286, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4080

2017-10-10T12:00:03.768868: step 4081, loss 0.189045, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:03.982373: step 4082, loss 0.138658, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:04.175011: step 4083, loss 0.102914, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:04.372192: step 4084, loss 0.102824, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:04.497460: step 4085, loss 0.15605, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:04.697391: step 4086, loss 0.0577696, acc 1, learning_rate 0.0001
2017-10-10T12:00:04.888877: step 4087, loss 0.0763643, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:05.101906: step 4088, loss 0.241484, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:05.265299: step 4089, loss 0.140365, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:05.440639: step 4090, loss 0.145863, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:05.657978: step 4091, loss 0.217697, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:05.856822: step 4092, loss 0.0708127, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:06.020986: step 4093, loss 0.200524, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:06.257680: step 4094, loss 0.0549647, acc 1, learning_rate 0.0001
2017-10-10T12:00:06.482918: step 4095, loss 0.195756, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:06.633575: step 4096, loss 0.0970207, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:06.738023: step 4097, loss 0.0458755, acc 1, learning_rate 0.0001
2017-10-10T12:00:06.873735: step 4098, loss 0.181264, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:07.028749: step 4099, loss 0.108863, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:07.188835: step 4100, loss 0.133801, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:07.329781: step 4101, loss 0.146705, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:07.486619: step 4102, loss 0.117361, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:07.654953: step 4103, loss 0.0837108, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:07.852875: step 4104, loss 0.133789, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:08.017242: step 4105, loss 0.231476, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:08.182569: step 4106, loss 0.152117, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:08.389309: step 4107, loss 0.108979, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:08.601540: step 4108, loss 0.208901, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:08.748969: step 4109, loss 0.192145, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:08.913016: step 4110, loss 0.0920934, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:09.120827: step 4111, loss 0.152615, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:09.322097: step 4112, loss 0.118586, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:09.568703: step 4113, loss 0.142579, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:09.765475: step 4114, loss 0.139471, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:09.923145: step 4115, loss 0.266094, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:10.049080: step 4116, loss 0.156782, acc 0.941176, learning_rate 0.0001
2017-10-10T12:00:10.199733: step 4117, loss 0.14594, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:10.339240: step 4118, loss 0.153537, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:10.499789: step 4119, loss 0.0973085, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:10.659310: step 4120, loss 0.100803, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:11.080863: step 4120, loss 0.219099, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4120

2017-10-10T12:00:12.509556: step 4121, loss 0.177166, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:12.690307: step 4122, loss 0.107659, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:12.833389: step 4123, loss 0.158845, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:13.018526: step 4124, loss 0.217244, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:13.215714: step 4125, loss 0.131884, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:13.437013: step 4126, loss 0.124632, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:13.604976: step 4127, loss 0.231391, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:13.761848: step 4128, loss 0.193548, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:13.952570: step 4129, loss 0.12404, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:14.140059: step 4130, loss 0.112638, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:14.306016: step 4131, loss 0.109689, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:14.491810: step 4132, loss 0.134958, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:14.668998: step 4133, loss 0.177392, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:14.832528: step 4134, loss 0.142607, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:15.018077: step 4135, loss 0.078799, acc 1, learning_rate 0.0001
2017-10-10T12:00:15.208839: step 4136, loss 0.11427, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:15.426238: step 4137, loss 0.0727684, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:15.572340: step 4138, loss 0.148214, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:15.745577: step 4139, loss 0.157672, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:15.946099: step 4140, loss 0.181087, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:16.157426: step 4141, loss 0.106803, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:16.332693: step 4142, loss 0.151057, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:16.544938: step 4143, loss 0.057952, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:16.733394: step 4144, loss 0.113034, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:16.879707: step 4145, loss 0.0983079, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:17.019809: step 4146, loss 0.0970431, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:17.173295: step 4147, loss 0.0556553, acc 1, learning_rate 0.0001
2017-10-10T12:00:17.289211: step 4148, loss 0.206096, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:17.428859: step 4149, loss 0.164665, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:17.582461: step 4150, loss 0.136724, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:17.736699: step 4151, loss 0.0510754, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:17.876894: step 4152, loss 0.116383, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:18.056862: step 4153, loss 0.134525, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:18.279246: step 4154, loss 0.15725, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:18.475909: step 4155, loss 0.104409, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:18.698210: step 4156, loss 0.0783432, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:18.868879: step 4157, loss 0.114022, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:19.043131: step 4158, loss 0.0806507, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:19.236181: step 4159, loss 0.251173, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:19.440202: step 4160, loss 0.120417, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:19.881112: step 4160, loss 0.219608, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4160

2017-10-10T12:00:20.894824: step 4161, loss 0.16046, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:21.054984: step 4162, loss 0.0854767, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:21.196827: step 4163, loss 0.0553015, acc 1, learning_rate 0.0001
2017-10-10T12:00:21.299401: step 4164, loss 0.0870319, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:21.442877: step 4165, loss 0.0321043, acc 1, learning_rate 0.0001
2017-10-10T12:00:21.589774: step 4166, loss 0.100537, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:21.793048: step 4167, loss 0.114476, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:21.969037: step 4168, loss 0.0941665, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:22.114170: step 4169, loss 0.200951, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:22.335890: step 4170, loss 0.110546, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:22.533123: step 4171, loss 0.070151, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:22.665339: step 4172, loss 0.15077, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:22.861947: step 4173, loss 0.112068, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:23.059055: step 4174, loss 0.154963, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:23.169178: step 4175, loss 0.122285, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:23.377922: step 4176, loss 0.218995, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:23.509128: step 4177, loss 0.151395, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:23.675971: step 4178, loss 0.108865, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:23.864053: step 4179, loss 0.203877, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:24.032979: step 4180, loss 0.252907, acc 0.859375, learning_rate 0.0001
2017-10-10T12:00:24.168581: step 4181, loss 0.112499, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:24.380312: step 4182, loss 0.135299, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:24.528944: step 4183, loss 0.0918278, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:24.692888: step 4184, loss 0.101004, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:24.898682: step 4185, loss 0.0895332, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:25.102629: step 4186, loss 0.10035, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:25.253405: step 4187, loss 0.175182, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:25.412673: step 4188, loss 0.204951, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:25.615375: step 4189, loss 0.0847016, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:25.822326: step 4190, loss 0.190963, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:26.000971: step 4191, loss 0.10645, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:26.152889: step 4192, loss 0.206152, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:26.355454: step 4193, loss 0.183321, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:26.564768: step 4194, loss 0.0981669, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:26.736853: step 4195, loss 0.201573, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:26.886678: step 4196, loss 0.125634, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:27.105099: step 4197, loss 0.138292, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:27.280569: step 4198, loss 0.113767, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:27.405258: step 4199, loss 0.179229, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:27.636888: step 4200, loss 0.119979, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:28.106902: step 4200, loss 0.219027, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4200

2017-10-10T12:00:29.198246: step 4201, loss 0.0945484, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:29.365757: step 4202, loss 0.179786, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:29.525851: step 4203, loss 0.133487, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:29.693288: step 4204, loss 0.124468, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:29.879950: step 4205, loss 0.267546, acc 0.890625, learning_rate 0.0001
2017-10-10T12:00:30.041395: step 4206, loss 0.120943, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:30.188121: step 4207, loss 0.166251, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:30.368805: step 4208, loss 0.07904, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:30.534789: step 4209, loss 0.115663, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:30.748939: step 4210, loss 0.107913, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:30.957811: step 4211, loss 0.149492, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:31.137073: step 4212, loss 0.0995198, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:31.279581: step 4213, loss 0.0897941, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:31.503708: step 4214, loss 0.142014, acc 0.960784, learning_rate 0.0001
2017-10-10T12:00:31.705053: step 4215, loss 0.208047, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:31.894238: step 4216, loss 0.130828, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:32.062307: step 4217, loss 0.123606, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:32.221182: step 4218, loss 0.168457, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:32.373708: step 4219, loss 0.224171, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:32.508868: step 4220, loss 0.0711591, acc 1, learning_rate 0.0001
2017-10-10T12:00:32.627615: step 4221, loss 0.117976, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:32.747680: step 4222, loss 0.130202, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:32.978971: step 4223, loss 0.0974689, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:33.145133: step 4224, loss 0.155865, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:33.335237: step 4225, loss 0.132551, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:33.505575: step 4226, loss 0.176799, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:33.713283: step 4227, loss 0.196223, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:33.913805: step 4228, loss 0.266857, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:34.077093: step 4229, loss 0.132557, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:34.224057: step 4230, loss 0.174023, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:34.439955: step 4231, loss 0.0593003, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:34.637777: step 4232, loss 0.17489, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:34.824554: step 4233, loss 0.0967744, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:35.020367: step 4234, loss 0.0477606, acc 1, learning_rate 0.0001
2017-10-10T12:00:35.171340: step 4235, loss 0.0726442, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:35.371458: step 4236, loss 0.147847, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:35.573577: step 4237, loss 0.146308, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:35.744884: step 4238, loss 0.21163, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:35.912860: step 4239, loss 0.0789175, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:36.104595: step 4240, loss 0.154167, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:36.555556: step 4240, loss 0.218024, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4240

2017-10-10T12:00:37.756897: step 4241, loss 0.11987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:37.968166: step 4242, loss 0.0755614, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:38.189649: step 4243, loss 0.0905642, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:38.406945: step 4244, loss 0.172491, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:38.575770: step 4245, loss 0.113187, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:38.723192: step 4246, loss 0.14688, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:38.851950: step 4247, loss 0.148764, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:38.971848: step 4248, loss 0.173279, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:39.128028: step 4249, loss 0.16245, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:39.284821: step 4250, loss 0.141663, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:39.422628: step 4251, loss 0.164773, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:39.563201: step 4252, loss 0.0632075, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:39.768840: step 4253, loss 0.104172, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:39.961217: step 4254, loss 0.227549, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:40.128026: step 4255, loss 0.076557, acc 1, learning_rate 0.0001
2017-10-10T12:00:40.297222: step 4256, loss 0.118527, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:40.474802: step 4257, loss 0.115608, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:40.668990: step 4258, loss 0.177107, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:40.825665: step 4259, loss 0.169121, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:40.985294: step 4260, loss 0.181162, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:41.158012: step 4261, loss 0.0806657, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:41.363525: step 4262, loss 0.147789, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:41.565996: step 4263, loss 0.145711, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:41.732914: step 4264, loss 0.136897, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:41.912821: step 4265, loss 0.0691996, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:42.068916: step 4266, loss 0.0806151, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:42.324877: step 4267, loss 0.114959, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:42.568561: step 4268, loss 0.0541307, acc 1, learning_rate 0.0001
2017-10-10T12:00:42.791930: step 4269, loss 0.102473, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:42.944484: step 4270, loss 0.163283, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:43.077091: step 4271, loss 0.0887752, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:43.216221: step 4272, loss 0.217273, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:43.360708: step 4273, loss 0.256494, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:43.528262: step 4274, loss 0.125586, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:43.674851: step 4275, loss 0.200635, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:43.838945: step 4276, loss 0.0748043, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:44.048861: step 4277, loss 0.279691, acc 0.890625, learning_rate 0.0001
2017-10-10T12:00:44.206236: step 4278, loss 0.222193, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:44.376173: step 4279, loss 0.248022, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:44.585956: step 4280, loss 0.268062, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:45.080812: step 4280, loss 0.218873, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4280

2017-10-10T12:00:46.056847: step 4281, loss 0.13781, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:46.241384: step 4282, loss 0.212321, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:46.409289: step 4283, loss 0.0858871, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:46.579613: step 4284, loss 0.178, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:46.776224: step 4285, loss 0.0729085, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:46.939653: step 4286, loss 0.218369, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:47.142685: step 4287, loss 0.110652, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:47.341992: step 4288, loss 0.186395, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:47.513124: step 4289, loss 0.126998, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:47.661191: step 4290, loss 0.129345, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:47.871231: step 4291, loss 0.135213, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:48.060588: step 4292, loss 0.0589375, acc 1, learning_rate 0.0001
2017-10-10T12:00:48.226065: step 4293, loss 0.115184, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:48.390987: step 4294, loss 0.085655, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:48.655036: step 4295, loss 0.138849, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:48.833113: step 4296, loss 0.0964407, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:49.016155: step 4297, loss 0.0484814, acc 1, learning_rate 0.0001
2017-10-10T12:00:49.191735: step 4298, loss 0.0529195, acc 1, learning_rate 0.0001
2017-10-10T12:00:49.370592: step 4299, loss 0.199942, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:49.530075: step 4300, loss 0.128994, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:49.658008: step 4301, loss 0.163489, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:49.778150: step 4302, loss 0.127687, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:49.934309: step 4303, loss 0.0349572, acc 1, learning_rate 0.0001
2017-10-10T12:00:50.124117: step 4304, loss 0.163253, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:50.294992: step 4305, loss 0.0649554, acc 1, learning_rate 0.0001
2017-10-10T12:00:50.461044: step 4306, loss 0.0951045, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:50.656897: step 4307, loss 0.0861627, acc 1, learning_rate 0.0001
2017-10-10T12:00:50.857055: step 4308, loss 0.208631, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:51.003010: step 4309, loss 0.141201, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:51.204889: step 4310, loss 0.0819595, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:51.361578: step 4311, loss 0.146961, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:51.511585: step 4312, loss 0.158004, acc 0.980392, learning_rate 0.0001
2017-10-10T12:00:51.720831: step 4313, loss 0.0376408, acc 1, learning_rate 0.0001
2017-10-10T12:00:51.882948: step 4314, loss 0.171607, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:52.037584: step 4315, loss 0.140534, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:52.250147: step 4316, loss 0.0668832, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:52.367470: step 4317, loss 0.177083, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:52.549179: step 4318, loss 0.140661, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:52.758793: step 4319, loss 0.106691, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:52.932846: step 4320, loss 0.137919, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:53.353171: step 4320, loss 0.220012, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4320

2017-10-10T12:00:54.456857: step 4321, loss 0.140644, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:54.575585: step 4322, loss 0.133911, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:54.709537: step 4323, loss 0.178173, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:54.863205: step 4324, loss 0.249937, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:54.989749: step 4325, loss 0.180059, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:55.161946: step 4326, loss 0.103427, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:55.345616: step 4327, loss 0.066409, acc 1, learning_rate 0.0001
2017-10-10T12:00:55.514850: step 4328, loss 0.0654684, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:55.714559: step 4329, loss 0.126762, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:55.914967: step 4330, loss 0.160819, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:56.084908: step 4331, loss 0.146155, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:56.277989: step 4332, loss 0.212044, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:56.453961: step 4333, loss 0.170559, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:56.660837: step 4334, loss 0.0944351, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:56.849314: step 4335, loss 0.143675, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:56.983720: step 4336, loss 0.146808, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:57.173786: step 4337, loss 0.0958672, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:57.356994: step 4338, loss 0.166447, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:57.540848: step 4339, loss 0.0668332, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:57.705149: step 4340, loss 0.0541773, acc 1, learning_rate 0.0001
2017-10-10T12:00:57.866033: step 4341, loss 0.183955, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:58.055233: step 4342, loss 0.0949873, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:58.245212: step 4343, loss 0.052856, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:58.443900: step 4344, loss 0.134705, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:58.634286: step 4345, loss 0.0853657, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:58.792880: step 4346, loss 0.0962908, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:58.978802: step 4347, loss 0.210862, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:59.177074: step 4348, loss 0.225107, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:59.360849: step 4349, loss 0.179259, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:59.578267: step 4350, loss 0.242884, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:59.696832: step 4351, loss 0.124594, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:59.851560: step 4352, loss 0.140905, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:00.017763: step 4353, loss 0.204293, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:00.148864: step 4354, loss 0.108524, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:00.272756: step 4355, loss 0.134443, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:00.412872: step 4356, loss 0.247526, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:00.568264: step 4357, loss 0.205752, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:00.729043: step 4358, loss 0.0570324, acc 1, learning_rate 0.0001
2017-10-10T12:01:00.896868: step 4359, loss 0.134843, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:01.096727: step 4360, loss 0.152167, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:01.569790: step 4360, loss 0.218569, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4360

2017-10-10T12:01:02.865413: step 4361, loss 0.1114, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:03.054880: step 4362, loss 0.0880094, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:03.208068: step 4363, loss 0.163959, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:03.424731: step 4364, loss 0.078113, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:03.569537: step 4365, loss 0.100726, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:03.731318: step 4366, loss 0.119339, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:03.937104: step 4367, loss 0.113684, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:04.145759: step 4368, loss 0.122563, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:04.285261: step 4369, loss 0.0671702, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:04.458266: step 4370, loss 0.0854951, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:04.676843: step 4371, loss 0.101221, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:04.895044: step 4372, loss 0.129537, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:05.128002: step 4373, loss 0.156561, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:05.267675: step 4374, loss 0.0981939, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:05.375341: step 4375, loss 0.143858, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:05.550122: step 4376, loss 0.0742509, acc 1, learning_rate 0.0001
2017-10-10T12:01:05.701163: step 4377, loss 0.0947534, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:05.820878: step 4378, loss 0.0789423, acc 1, learning_rate 0.0001
2017-10-10T12:01:05.944823: step 4379, loss 0.132758, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:06.099093: step 4380, loss 0.111931, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:06.298601: step 4381, loss 0.0974655, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:06.433094: step 4382, loss 0.181212, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:06.610401: step 4383, loss 0.0994379, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:06.808293: step 4384, loss 0.0686745, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:07.000910: step 4385, loss 0.168243, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:07.136908: step 4386, loss 0.108862, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:07.318939: step 4387, loss 0.144721, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:07.521949: step 4388, loss 0.108933, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:07.709154: step 4389, loss 0.0919925, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:07.895465: step 4390, loss 0.0723382, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:08.046376: step 4391, loss 0.139637, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:08.224290: step 4392, loss 0.101507, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:08.428491: step 4393, loss 0.11731, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:08.612998: step 4394, loss 0.0872473, acc 1, learning_rate 0.0001
2017-10-10T12:01:08.765941: step 4395, loss 0.0815021, acc 1, learning_rate 0.0001
2017-10-10T12:01:08.979851: step 4396, loss 0.117978, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:09.189855: step 4397, loss 0.129704, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:09.340943: step 4398, loss 0.136286, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:09.500100: step 4399, loss 0.104344, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:09.669959: step 4400, loss 0.121347, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:10.249429: step 4400, loss 0.220312, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4400

2017-10-10T12:01:11.156159: step 4401, loss 0.0911721, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:11.331876: step 4402, loss 0.110185, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:11.482912: step 4403, loss 0.115981, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:11.692854: step 4404, loss 0.0882709, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:11.880828: step 4405, loss 0.0797199, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:12.064950: step 4406, loss 0.126941, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:12.250787: step 4407, loss 0.134878, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:12.402770: step 4408, loss 0.107828, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:12.578063: step 4409, loss 0.0953829, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:12.767672: step 4410, loss 0.279021, acc 0.862745, learning_rate 0.0001
2017-10-10T12:01:12.977372: step 4411, loss 0.100638, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:13.110637: step 4412, loss 0.120345, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:13.290597: step 4413, loss 0.141056, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:13.500234: step 4414, loss 0.184006, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:13.671980: step 4415, loss 0.0909838, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:13.865312: step 4416, loss 0.110245, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:14.012821: step 4417, loss 0.0857395, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:14.223106: step 4418, loss 0.160838, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:14.358798: step 4419, loss 0.144296, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:14.514866: step 4420, loss 0.128706, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:14.725394: step 4421, loss 0.198531, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:14.905066: step 4422, loss 0.104694, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:15.068997: step 4423, loss 0.166749, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:15.258360: step 4424, loss 0.0853901, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:15.460847: step 4425, loss 0.0651254, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:15.617156: step 4426, loss 0.0693894, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:15.828859: step 4427, loss 0.160965, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:16.029154: step 4428, loss 0.141847, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:16.172573: step 4429, loss 0.174301, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:16.313703: step 4430, loss 0.114359, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:16.466878: step 4431, loss 0.166414, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:16.619981: step 4432, loss 0.0962725, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:16.774492: step 4433, loss 0.202768, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:16.928825: step 4434, loss 0.127829, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:17.088959: step 4435, loss 0.142183, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:17.269369: step 4436, loss 0.174662, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:17.443219: step 4437, loss 0.0641008, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:17.628991: step 4438, loss 0.0529908, acc 1, learning_rate 0.0001
2017-10-10T12:01:17.817302: step 4439, loss 0.265453, acc 0.90625, learning_rate 0.0001
2017-10-10T12:01:18.014982: step 4440, loss 0.149408, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:18.448863: step 4440, loss 0.22234, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4440

2017-10-10T12:01:19.592842: step 4441, loss 0.0692503, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:19.788971: step 4442, loss 0.128877, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:19.957571: step 4443, loss 0.0907651, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:20.104573: step 4444, loss 0.200306, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:20.318066: step 4445, loss 0.127926, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:20.551645: step 4446, loss 0.12887, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:20.759152: step 4447, loss 0.169467, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:20.862734: step 4448, loss 0.12833, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:21.020100: step 4449, loss 0.103361, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:21.168854: step 4450, loss 0.100375, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:21.320044: step 4451, loss 0.118319, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:21.434180: step 4452, loss 0.17872, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:21.584586: step 4453, loss 0.0907255, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:21.740893: step 4454, loss 0.143845, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:21.919735: step 4455, loss 0.120534, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:22.125499: step 4456, loss 0.0911961, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:22.309392: step 4457, loss 0.233233, acc 0.90625, learning_rate 0.0001
2017-10-10T12:01:22.497013: step 4458, loss 0.0695739, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:22.680124: step 4459, loss 0.148478, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:22.868942: step 4460, loss 0.106845, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:23.035235: step 4461, loss 0.269302, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:23.226480: step 4462, loss 0.0876594, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:23.452978: step 4463, loss 0.0836181, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:23.587749: step 4464, loss 0.191307, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:23.767662: step 4465, loss 0.0885568, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:23.968592: step 4466, loss 0.170119, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:24.148987: step 4467, loss 0.111771, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:24.296915: step 4468, loss 0.113858, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:24.502945: step 4469, loss 0.0390862, acc 1, learning_rate 0.0001
2017-10-10T12:01:24.694559: step 4470, loss 0.167146, acc 0.90625, learning_rate 0.0001
2017-10-10T12:01:24.887176: step 4471, loss 0.1478, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:25.071973: step 4472, loss 0.124279, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:25.236524: step 4473, loss 0.0553787, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:25.408675: step 4474, loss 0.108758, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:25.613629: step 4475, loss 0.0699735, acc 1, learning_rate 0.0001
2017-10-10T12:01:25.787865: step 4476, loss 0.178291, acc 0.90625, learning_rate 0.0001
2017-10-10T12:01:25.940889: step 4477, loss 0.104295, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:26.147963: step 4478, loss 0.183251, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:26.332858: step 4479, loss 0.09513, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:26.484982: step 4480, loss 0.117926, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:27.150238: step 4480, loss 0.218349, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4480

2017-10-10T12:01:28.669571: step 4481, loss 0.11138, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:28.852416: step 4482, loss 0.134471, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:29.060511: step 4483, loss 0.129487, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:29.248852: step 4484, loss 0.201336, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:29.441164: step 4485, loss 0.135875, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:29.621003: step 4486, loss 0.0816345, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:29.797685: step 4487, loss 0.141536, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:29.980866: step 4488, loss 0.158121, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:30.164247: step 4489, loss 0.150655, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:30.359948: step 4490, loss 0.179575, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:30.576465: step 4491, loss 0.141509, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:30.759223: step 4492, loss 0.138449, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:30.893392: step 4493, loss 0.150108, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:31.116253: step 4494, loss 0.156067, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:31.324301: step 4495, loss 0.180442, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:31.493795: step 4496, loss 0.122508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:31.638140: step 4497, loss 0.129179, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:31.770919: step 4498, loss 0.0824614, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:31.892868: step 4499, loss 0.066822, acc 1, learning_rate 0.0001
2017-10-10T12:01:32.049611: step 4500, loss 0.0828223, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:32.204578: step 4501, loss 0.161201, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:32.328872: step 4502, loss 0.186094, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:32.469119: step 4503, loss 0.216512, acc 0.90625, learning_rate 0.0001
2017-10-10T12:01:32.651884: step 4504, loss 0.16261, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:32.812238: step 4505, loss 0.0678879, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:33.014795: step 4506, loss 0.0983292, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:33.189052: step 4507, loss 0.123613, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:33.387711: step 4508, loss 0.119771, acc 0.960784, learning_rate 0.0001
2017-10-10T12:01:33.588138: step 4509, loss 0.0969296, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:33.741006: step 4510, loss 0.0739343, acc 1, learning_rate 0.0001
2017-10-10T12:01:33.894816: step 4511, loss 0.0544838, acc 1, learning_rate 0.0001
2017-10-10T12:01:34.109746: step 4512, loss 0.0922585, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:34.242925: step 4513, loss 0.111642, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:34.428846: step 4514, loss 0.127858, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:34.635091: step 4515, loss 0.0456874, acc 1, learning_rate 0.0001
2017-10-10T12:01:34.849476: step 4516, loss 0.105606, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:35.045913: step 4517, loss 0.129613, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:35.207598: step 4518, loss 0.132679, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:35.387430: step 4519, loss 0.127472, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:35.583587: step 4520, loss 0.0935899, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:36.032836: step 4520, loss 0.217604, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4520

2017-10-10T12:01:37.349193: step 4521, loss 0.147274, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:37.480148: step 4522, loss 0.126371, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:37.671861: step 4523, loss 0.114562, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:37.890005: step 4524, loss 0.0386437, acc 1, learning_rate 0.0001
2017-10-10T12:01:38.066106: step 4525, loss 0.201648, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:38.241897: step 4526, loss 0.146349, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:38.481540: step 4527, loss 0.194134, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:38.688408: step 4528, loss 0.13669, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:38.930859: step 4529, loss 0.160867, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:39.055280: step 4530, loss 0.137285, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:39.180848: step 4531, loss 0.10495, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:39.321478: step 4532, loss 0.0829625, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:39.485793: step 4533, loss 0.149319, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:39.651305: step 4534, loss 0.170262, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:39.822762: step 4535, loss 0.0894164, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:39.976501: step 4536, loss 0.135639, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:40.196888: step 4537, loss 0.0540017, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:40.381100: step 4538, loss 0.120811, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:40.536803: step 4539, loss 0.106565, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:40.739082: step 4540, loss 0.198886, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:40.940887: step 4541, loss 0.0783302, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:41.101114: step 4542, loss 0.120272, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:41.252399: step 4543, loss 0.15437, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:41.462812: step 4544, loss 0.107593, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:41.644842: step 4545, loss 0.182145, acc 0.90625, learning_rate 0.0001
2017-10-10T12:01:41.868967: step 4546, loss 0.074471, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:42.062877: step 4547, loss 0.206842, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:42.241545: step 4548, loss 0.137349, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:42.415012: step 4549, loss 0.0939712, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:42.580998: step 4550, loss 0.130363, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:42.724841: step 4551, loss 0.068982, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:42.865005: step 4552, loss 0.25208, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:43.005551: step 4553, loss 0.146569, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:43.208858: step 4554, loss 0.0923758, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:43.407795: step 4555, loss 0.143363, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:43.584019: step 4556, loss 0.148755, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:43.752871: step 4557, loss 0.186934, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:43.974883: step 4558, loss 0.11479, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:44.152620: step 4559, loss 0.0905905, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:44.309541: step 4560, loss 0.291488, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:44.815842: step 4560, loss 0.21654, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4560

2017-10-10T12:01:45.866543: step 4561, loss 0.105516, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:46.035867: step 4562, loss 0.0707487, acc 1, learning_rate 0.0001
2017-10-10T12:01:46.176951: step 4563, loss 0.163575, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:46.384370: step 4564, loss 0.10981, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:46.601513: step 4565, loss 0.141218, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:46.756990: step 4566, loss 0.0995171, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:46.930859: step 4567, loss 0.0698354, acc 1, learning_rate 0.0001
2017-10-10T12:01:47.118929: step 4568, loss 0.138394, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:47.321843: step 4569, loss 0.120858, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:47.501035: step 4570, loss 0.0769178, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:47.638137: step 4571, loss 0.133894, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:47.867931: step 4572, loss 0.104904, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:48.069602: step 4573, loss 0.191027, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:48.220855: step 4574, loss 0.153278, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:48.408891: step 4575, loss 0.133352, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:48.620574: step 4576, loss 0.122696, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:48.836843: step 4577, loss 0.174159, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:49.015424: step 4578, loss 0.224052, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:49.177852: step 4579, loss 0.113981, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:49.362461: step 4580, loss 0.0835568, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:49.580632: step 4581, loss 0.156276, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:49.796874: step 4582, loss 0.250052, acc 0.90625, learning_rate 0.0001
2017-10-10T12:01:49.977202: step 4583, loss 0.113568, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:50.134088: step 4584, loss 0.0930495, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:50.283836: step 4585, loss 0.198429, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:50.438417: step 4586, loss 0.202264, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:50.575415: step 4587, loss 0.129823, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:50.705078: step 4588, loss 0.175012, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:50.826982: step 4589, loss 0.163926, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:50.977703: step 4590, loss 0.063048, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:51.199925: step 4591, loss 0.121667, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:51.337821: step 4592, loss 0.0909011, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:51.496634: step 4593, loss 0.0939789, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:51.697198: step 4594, loss 0.140615, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:51.868983: step 4595, loss 0.170689, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:52.113110: step 4596, loss 0.253291, acc 0.890625, learning_rate 0.0001
2017-10-10T12:01:52.327223: step 4597, loss 0.138383, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:52.486815: step 4598, loss 0.105482, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:52.618586: step 4599, loss 0.217735, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:52.765155: step 4600, loss 0.107886, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:53.155990: step 4600, loss 0.218043, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4600

2017-10-10T12:01:54.311593: step 4601, loss 0.159859, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:54.516856: step 4602, loss 0.176282, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:54.686085: step 4603, loss 0.176688, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:54.858253: step 4604, loss 0.129932, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:55.057915: step 4605, loss 0.0497746, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:55.198344: step 4606, loss 0.109521, acc 0.980392, learning_rate 0.0001
2017-10-10T12:01:55.396678: step 4607, loss 0.141823, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:55.562223: step 4608, loss 0.072687, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:55.759271: step 4609, loss 0.109469, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:55.921121: step 4610, loss 0.115652, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:56.091684: step 4611, loss 0.0920406, acc 1, learning_rate 0.0001
2017-10-10T12:01:56.298076: step 4612, loss 0.103727, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:56.511543: step 4613, loss 0.0485199, acc 1, learning_rate 0.0001
2017-10-10T12:01:56.667442: step 4614, loss 0.11541, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:56.894897: step 4615, loss 0.0887264, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:57.068828: step 4616, loss 0.212229, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:57.269240: step 4617, loss 0.117426, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:57.443371: step 4618, loss 0.22638, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:57.589285: step 4619, loss 0.125102, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:57.784845: step 4620, loss 0.182271, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:57.932886: step 4621, loss 0.104563, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:58.133561: step 4622, loss 0.11883, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:58.346616: step 4623, loss 0.164546, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:58.544471: step 4624, loss 0.149699, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:58.713077: step 4625, loss 0.108771, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:58.898725: step 4626, loss 0.236938, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:59.129416: step 4627, loss 0.0699413, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:59.281049: step 4628, loss 0.130925, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:59.448127: step 4629, loss 0.133312, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:59.659082: step 4630, loss 0.133194, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:59.895521: step 4631, loss 0.178531, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:00.068339: step 4632, loss 0.152331, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:00.237078: step 4633, loss 0.113362, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:00.417790: step 4634, loss 0.206938, acc 0.890625, learning_rate 0.0001
2017-10-10T12:02:00.624377: step 4635, loss 0.0593446, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:00.776885: step 4636, loss 0.147659, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:00.961317: step 4637, loss 0.108889, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:01.189020: step 4638, loss 0.146537, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:01.441172: step 4639, loss 0.12084, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:01.623619: step 4640, loss 0.141636, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:02.036089: step 4640, loss 0.215954, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4640

2017-10-10T12:02:03.175576: step 4641, loss 0.118949, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:03.284771: step 4642, loss 0.154212, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:03.439940: step 4643, loss 0.209795, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:03.636018: step 4644, loss 0.084788, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:03.805124: step 4645, loss 0.147968, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:03.968863: step 4646, loss 0.178004, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:04.172781: step 4647, loss 0.156051, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:04.360956: step 4648, loss 0.0753965, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:04.529112: step 4649, loss 0.0635923, acc 1, learning_rate 0.0001
2017-10-10T12:02:04.694107: step 4650, loss 0.142387, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:04.892888: step 4651, loss 0.122185, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:05.044890: step 4652, loss 0.0929904, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:05.184820: step 4653, loss 0.0913361, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:05.392959: step 4654, loss 0.0531651, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:05.576906: step 4655, loss 0.12387, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:05.741194: step 4656, loss 0.0613004, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:05.918664: step 4657, loss 0.115377, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:06.144331: step 4658, loss 0.0889375, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:06.313014: step 4659, loss 0.136274, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:06.469198: step 4660, loss 0.143228, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:06.681889: step 4661, loss 0.0791959, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:06.863835: step 4662, loss 0.102063, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:07.024856: step 4663, loss 0.145778, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:07.181084: step 4664, loss 0.224409, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:07.399733: step 4665, loss 0.0725816, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:07.593032: step 4666, loss 0.0972484, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:07.768978: step 4667, loss 0.246539, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:07.942249: step 4668, loss 0.18498, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:08.140034: step 4669, loss 0.090736, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:08.341182: step 4670, loss 0.0914002, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:08.504902: step 4671, loss 0.188197, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:08.665162: step 4672, loss 0.117238, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:08.853429: step 4673, loss 0.146807, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:09.049466: step 4674, loss 0.0984629, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:09.256866: step 4675, loss 0.0925757, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:09.457522: step 4676, loss 0.0904729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:09.598931: step 4677, loss 0.105232, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:09.784411: step 4678, loss 0.263288, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:09.969483: step 4679, loss 0.220019, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:10.137272: step 4680, loss 0.0938673, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:10.636830: step 4680, loss 0.217762, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4680

2017-10-10T12:02:11.662791: step 4681, loss 0.0869369, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:11.776934: step 4682, loss 0.194351, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:11.972840: step 4683, loss 0.0802742, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:12.179204: step 4684, loss 0.0728735, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:12.416917: step 4685, loss 0.188872, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:12.606583: step 4686, loss 0.102744, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:12.763143: step 4687, loss 0.0976619, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:12.911833: step 4688, loss 0.115835, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:13.012850: step 4689, loss 0.216204, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:13.199566: step 4690, loss 0.123753, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:13.380923: step 4691, loss 0.0818514, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:13.503373: step 4692, loss 0.164812, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:13.650977: step 4693, loss 0.136281, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:13.797092: step 4694, loss 0.147323, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:13.936830: step 4695, loss 0.160908, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:14.075647: step 4696, loss 0.12685, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:14.228961: step 4697, loss 0.151124, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:14.414027: step 4698, loss 0.157578, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:14.619331: step 4699, loss 0.0968155, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:14.778702: step 4700, loss 0.100631, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:14.928832: step 4701, loss 0.059249, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:15.136169: step 4702, loss 0.103384, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:15.316141: step 4703, loss 0.101927, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:15.456884: step 4704, loss 0.269348, acc 0.901961, learning_rate 0.0001
2017-10-10T12:02:15.653193: step 4705, loss 0.187084, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:15.864152: step 4706, loss 0.149467, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:16.075867: step 4707, loss 0.0431502, acc 1, learning_rate 0.0001
2017-10-10T12:02:16.283932: step 4708, loss 0.115334, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:16.446587: step 4709, loss 0.19374, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:16.623610: step 4710, loss 0.102613, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:16.832504: step 4711, loss 0.067808, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:17.052901: step 4712, loss 0.182712, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:17.182134: step 4713, loss 0.097788, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:17.359688: step 4714, loss 0.153932, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:17.564153: step 4715, loss 0.122994, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:17.740868: step 4716, loss 0.112831, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:17.932899: step 4717, loss 0.113525, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:18.123559: step 4718, loss 0.098418, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:18.315821: step 4719, loss 0.14757, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:18.505901: step 4720, loss 0.187655, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:18.958752: step 4720, loss 0.217684, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4720

2017-10-10T12:02:20.132846: step 4721, loss 0.105733, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:20.280235: step 4722, loss 0.206191, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:20.440604: step 4723, loss 0.174222, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:20.646275: step 4724, loss 0.190359, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:20.821286: step 4725, loss 0.108903, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:20.955348: step 4726, loss 0.0641797, acc 1, learning_rate 0.0001
2017-10-10T12:02:21.149599: step 4727, loss 0.178135, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:21.352880: step 4728, loss 0.109678, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:21.507294: step 4729, loss 0.160425, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:21.677811: step 4730, loss 0.158008, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:21.876886: step 4731, loss 0.0986634, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:22.044751: step 4732, loss 0.0774997, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:22.200962: step 4733, loss 0.0941516, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:22.380856: step 4734, loss 0.0738038, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:22.568854: step 4735, loss 0.203631, acc 0.890625, learning_rate 0.0001
2017-10-10T12:02:22.733398: step 4736, loss 0.186137, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:22.924850: step 4737, loss 0.148804, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:23.191929: step 4738, loss 0.0747248, acc 1, learning_rate 0.0001
2017-10-10T12:02:23.422248: step 4739, loss 0.109201, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:23.635115: step 4740, loss 0.101464, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:23.794376: step 4741, loss 0.0760728, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:23.934351: step 4742, loss 0.148237, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:24.023626: step 4743, loss 0.205417, acc 0.890625, learning_rate 0.0001
2017-10-10T12:02:24.111487: step 4744, loss 0.131699, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:24.199542: step 4745, loss 0.13126, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:24.286336: step 4746, loss 0.319936, acc 0.890625, learning_rate 0.0001
2017-10-10T12:02:24.379124: step 4747, loss 0.1604, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:24.512855: step 4748, loss 0.0742871, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:24.648911: step 4749, loss 0.112815, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:24.852399: step 4750, loss 0.158744, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:25.083981: step 4751, loss 0.161258, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:25.223246: step 4752, loss 0.215262, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:25.391963: step 4753, loss 0.0678635, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:25.619032: step 4754, loss 0.108762, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:25.831786: step 4755, loss 0.135827, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:25.985162: step 4756, loss 0.269876, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:26.139670: step 4757, loss 0.0706686, acc 1, learning_rate 0.0001
2017-10-10T12:02:26.366266: step 4758, loss 0.0956381, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:26.552838: step 4759, loss 0.0966873, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:26.688974: step 4760, loss 0.147052, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:27.194822: step 4760, loss 0.217248, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4760

2017-10-10T12:02:28.529364: step 4761, loss 0.151284, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:28.693790: step 4762, loss 0.109978, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:28.857076: step 4763, loss 0.157727, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:29.021730: step 4764, loss 0.16066, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:29.190082: step 4765, loss 0.174069, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:29.380231: step 4766, loss 0.114945, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:29.565036: step 4767, loss 0.157465, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:29.767590: step 4768, loss 0.0986506, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:29.951785: step 4769, loss 0.138818, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:30.129296: step 4770, loss 0.141989, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:30.333058: step 4771, loss 0.156344, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:30.516268: step 4772, loss 0.123851, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:30.701155: step 4773, loss 0.170058, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:30.905730: step 4774, loss 0.0970416, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:31.083628: step 4775, loss 0.0969894, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:31.271153: step 4776, loss 0.125786, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:31.456842: step 4777, loss 0.123728, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:31.632860: step 4778, loss 0.0989175, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:31.815949: step 4779, loss 0.0869834, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:31.971446: step 4780, loss 0.0973096, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:32.159429: step 4781, loss 0.113814, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:32.356995: step 4782, loss 0.264937, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:32.548915: step 4783, loss 0.0696555, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:32.726339: step 4784, loss 0.0596412, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:32.883178: step 4785, loss 0.0830552, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:33.082453: step 4786, loss 0.079096, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:33.276846: step 4787, loss 0.0575251, acc 1, learning_rate 0.0001
2017-10-10T12:02:33.396882: step 4788, loss 0.0951077, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:33.605951: step 4789, loss 0.181922, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:33.792897: step 4790, loss 0.122295, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:33.971068: step 4791, loss 0.0591758, acc 1, learning_rate 0.0001
2017-10-10T12:02:34.204723: step 4792, loss 0.112963, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:34.433947: step 4793, loss 0.0931578, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:34.621961: step 4794, loss 0.0913284, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:34.715268: step 4795, loss 0.196369, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:34.792512: step 4796, loss 0.0682248, acc 1, learning_rate 0.0001
2017-10-10T12:02:34.869737: step 4797, loss 0.175575, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:34.950274: step 4798, loss 0.0737676, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:35.043485: step 4799, loss 0.0611105, acc 1, learning_rate 0.0001
2017-10-10T12:02:35.132115: step 4800, loss 0.16733, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:35.398860: step 4800, loss 0.215417, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4800

2017-10-10T12:02:36.168995: step 4801, loss 0.0814609, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:36.299591: step 4802, loss 0.111401, acc 0.941176, learning_rate 0.0001
2017-10-10T12:02:36.511660: step 4803, loss 0.114931, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:36.647353: step 4804, loss 0.224129, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:36.821725: step 4805, loss 0.222189, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:37.044199: step 4806, loss 0.0738657, acc 1, learning_rate 0.0001
2017-10-10T12:02:37.198814: step 4807, loss 0.0830446, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:37.359750: step 4808, loss 0.0854312, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:37.542477: step 4809, loss 0.205481, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:37.752510: step 4810, loss 0.103407, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:37.928933: step 4811, loss 0.103176, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:38.084858: step 4812, loss 0.151651, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:38.288118: step 4813, loss 0.125771, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:38.482002: step 4814, loss 0.0865485, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:38.626613: step 4815, loss 0.165754, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:38.827807: step 4816, loss 0.173724, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:39.006397: step 4817, loss 0.13259, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:39.165006: step 4818, loss 0.101897, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:39.352844: step 4819, loss 0.122247, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:39.556834: step 4820, loss 0.137535, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:39.768863: step 4821, loss 0.0894255, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:39.884633: step 4822, loss 0.0614782, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:40.081848: step 4823, loss 0.128256, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:40.297329: step 4824, loss 0.11185, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:40.452622: step 4825, loss 0.110133, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:40.614044: step 4826, loss 0.109911, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:40.814401: step 4827, loss 0.150822, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:41.009080: step 4828, loss 0.111809, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:41.199810: step 4829, loss 0.113443, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:41.342440: step 4830, loss 0.0754804, acc 1, learning_rate 0.0001
2017-10-10T12:02:41.538918: step 4831, loss 0.139396, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:41.744342: step 4832, loss 0.0873306, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:41.875890: step 4833, loss 0.22965, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:42.075038: step 4834, loss 0.138002, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:42.280892: step 4835, loss 0.138633, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:42.468974: step 4836, loss 0.101711, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:42.631583: step 4837, loss 0.125255, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:42.821506: step 4838, loss 0.0696077, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:43.019722: step 4839, loss 0.145174, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:43.218431: step 4840, loss 0.145572, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:43.633923: step 4840, loss 0.216298, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4840

2017-10-10T12:02:44.795030: step 4841, loss 0.118191, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:44.977178: step 4842, loss 0.0547027, acc 1, learning_rate 0.0001
2017-10-10T12:02:45.172116: step 4843, loss 0.109719, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:45.377145: step 4844, loss 0.0610947, acc 1, learning_rate 0.0001
2017-10-10T12:02:45.516873: step 4845, loss 0.147102, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:45.768849: step 4846, loss 0.189368, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:45.945444: step 4847, loss 0.176589, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:46.051144: step 4848, loss 0.111876, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:46.147358: step 4849, loss 0.132857, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:46.244945: step 4850, loss 0.178013, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:46.342638: step 4851, loss 0.163014, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:46.430396: step 4852, loss 0.0877487, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:46.581622: step 4853, loss 0.14722, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:46.725121: step 4854, loss 0.0809769, acc 1, learning_rate 0.0001
2017-10-10T12:02:46.906730: step 4855, loss 0.117763, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:47.113062: step 4856, loss 0.173233, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:47.326464: step 4857, loss 0.12319, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:47.496869: step 4858, loss 0.122324, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:47.663290: step 4859, loss 0.0477099, acc 1, learning_rate 0.0001
2017-10-10T12:02:47.863305: step 4860, loss 0.169605, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:47.985820: step 4861, loss 0.0769038, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:48.164150: step 4862, loss 0.178934, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:48.377184: step 4863, loss 0.0782478, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:48.565998: step 4864, loss 0.073214, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:48.712763: step 4865, loss 0.0765946, acc 1, learning_rate 0.0001
2017-10-10T12:02:48.891228: step 4866, loss 0.199558, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:49.102757: step 4867, loss 0.161382, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:49.238184: step 4868, loss 0.0739662, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:49.388667: step 4869, loss 0.086677, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:49.578576: step 4870, loss 0.170945, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:49.736338: step 4871, loss 0.137311, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:49.908827: step 4872, loss 0.0721743, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:50.117028: step 4873, loss 0.0640349, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:50.308890: step 4874, loss 0.214975, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:50.501012: step 4875, loss 0.190234, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:50.660827: step 4876, loss 0.174966, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:50.831188: step 4877, loss 0.0432841, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:51.020961: step 4878, loss 0.106685, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:51.220058: step 4879, loss 0.0839831, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:51.416962: step 4880, loss 0.0972212, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:51.858543: step 4880, loss 0.216762, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4880

2017-10-10T12:02:53.136494: step 4881, loss 0.0716196, acc 1, learning_rate 0.0001
2017-10-10T12:02:53.343521: step 4882, loss 0.213463, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:53.549107: step 4883, loss 0.172035, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:53.732968: step 4884, loss 0.0780728, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:53.916672: step 4885, loss 0.0470049, acc 1, learning_rate 0.0001
2017-10-10T12:02:54.083099: step 4886, loss 0.166784, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:54.284875: step 4887, loss 0.254505, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:54.454494: step 4888, loss 0.104215, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:54.665176: step 4889, loss 0.196915, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:54.883158: step 4890, loss 0.105916, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:55.055751: step 4891, loss 0.0835177, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:55.230435: step 4892, loss 0.107533, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:55.377145: step 4893, loss 0.105723, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:55.564822: step 4894, loss 0.128202, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:55.820885: step 4895, loss 0.142013, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:55.996877: step 4896, loss 0.0613683, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:56.175235: step 4897, loss 0.220222, acc 0.890625, learning_rate 0.0001
2017-10-10T12:02:56.324873: step 4898, loss 0.0717148, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:56.520829: step 4899, loss 0.184072, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:56.642474: step 4900, loss 0.130401, acc 0.941176, learning_rate 0.0001
2017-10-10T12:02:56.769436: step 4901, loss 0.0869185, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:56.862965: step 4902, loss 0.0875495, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:56.955783: step 4903, loss 0.0495721, acc 1, learning_rate 0.0001
2017-10-10T12:02:57.047299: step 4904, loss 0.0692743, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:57.145093: step 4905, loss 0.162522, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:57.283195: step 4906, loss 0.0595559, acc 1, learning_rate 0.0001
2017-10-10T12:02:57.457544: step 4907, loss 0.167816, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:57.663443: step 4908, loss 0.073178, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:57.864856: step 4909, loss 0.184197, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:57.990323: step 4910, loss 0.074902, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:58.175417: step 4911, loss 0.129361, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:58.389230: step 4912, loss 0.0870207, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:58.595593: step 4913, loss 0.137961, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:58.736837: step 4914, loss 0.0535451, acc 1, learning_rate 0.0001
2017-10-10T12:02:58.935691: step 4915, loss 0.132609, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:59.140461: step 4916, loss 0.156147, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:59.317367: step 4917, loss 0.0791241, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:59.483132: step 4918, loss 0.0776606, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:59.692450: step 4919, loss 0.0566441, acc 1, learning_rate 0.0001
2017-10-10T12:02:59.864868: step 4920, loss 0.103834, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:00.353082: step 4920, loss 0.216635, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4920

2017-10-10T12:03:01.303075: step 4921, loss 0.104304, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:01.492588: step 4922, loss 0.167814, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:01.662924: step 4923, loss 0.158208, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:01.828454: step 4924, loss 0.175466, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:02.006350: step 4925, loss 0.121562, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:02.213873: step 4926, loss 0.101271, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:02.389201: step 4927, loss 0.111557, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:02.547705: step 4928, loss 0.118403, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:02.744848: step 4929, loss 0.183816, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:02.952932: step 4930, loss 0.114973, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:03.106269: step 4931, loss 0.293022, acc 0.859375, learning_rate 0.0001
2017-10-10T12:03:03.302673: step 4932, loss 0.209063, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:03.471923: step 4933, loss 0.0639883, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:03.622787: step 4934, loss 0.0932586, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:03.813093: step 4935, loss 0.155525, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:03.999881: step 4936, loss 0.0475006, acc 1, learning_rate 0.0001
2017-10-10T12:03:04.152947: step 4937, loss 0.158618, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:04.300836: step 4938, loss 0.158602, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:04.500602: step 4939, loss 0.176181, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:04.710603: step 4940, loss 0.136828, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:04.909275: step 4941, loss 0.12726, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:05.062697: step 4942, loss 0.108738, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:05.240537: step 4943, loss 0.0940031, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:05.440179: step 4944, loss 0.125901, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:05.633613: step 4945, loss 0.243626, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:05.825236: step 4946, loss 0.153525, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:06.016934: step 4947, loss 0.109183, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:06.225310: step 4948, loss 0.106912, acc 1, learning_rate 0.0001
2017-10-10T12:03:06.466871: step 4949, loss 0.101354, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:06.681215: step 4950, loss 0.14759, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:06.823439: step 4951, loss 0.144628, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:06.960812: step 4952, loss 0.156715, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:07.108905: step 4953, loss 0.106648, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:07.292837: step 4954, loss 0.125684, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:07.460830: step 4955, loss 0.0718439, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:07.636231: step 4956, loss 0.121944, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:07.797392: step 4957, loss 0.0297842, acc 1, learning_rate 0.0001
2017-10-10T12:03:07.952576: step 4958, loss 0.12719, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:08.097000: step 4959, loss 0.224506, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:08.222035: step 4960, loss 0.102702, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:08.568921: step 4960, loss 0.216957, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-4960

2017-10-10T12:03:09.690499: step 4961, loss 0.0990001, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:09.904703: step 4962, loss 0.124196, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:10.066424: step 4963, loss 0.121917, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:10.222810: step 4964, loss 0.0989762, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:10.435848: step 4965, loss 0.0941328, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:10.612866: step 4966, loss 0.0880202, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:10.781300: step 4967, loss 0.168245, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:10.950238: step 4968, loss 0.0672124, acc 1, learning_rate 0.0001
2017-10-10T12:03:11.170005: step 4969, loss 0.125292, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:11.353032: step 4970, loss 0.157295, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:11.542887: step 4971, loss 0.105467, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:11.729097: step 4972, loss 0.0745579, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:11.914616: step 4973, loss 0.0747438, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:12.124024: step 4974, loss 0.153411, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:12.338883: step 4975, loss 0.185332, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:12.540826: step 4976, loss 0.138706, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:12.721139: step 4977, loss 0.146261, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:12.889345: step 4978, loss 0.126851, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:13.060555: step 4979, loss 0.266087, acc 0.890625, learning_rate 0.0001
2017-10-10T12:03:13.271222: step 4980, loss 0.141815, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:13.464839: step 4981, loss 0.0864461, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:13.620900: step 4982, loss 0.121809, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:13.801083: step 4983, loss 0.0771437, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:13.972840: step 4984, loss 0.0965202, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:14.163800: step 4985, loss 0.121506, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:14.337843: step 4986, loss 0.0854107, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:14.536897: step 4987, loss 0.124184, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:14.728909: step 4988, loss 0.0831866, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:14.867466: step 4989, loss 0.075208, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:15.071353: step 4990, loss 0.220895, acc 0.890625, learning_rate 0.0001
2017-10-10T12:03:15.270451: step 4991, loss 0.173089, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:15.424878: step 4992, loss 0.201112, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:15.608880: step 4993, loss 0.116256, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:15.810350: step 4994, loss 0.187623, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:16.018207: step 4995, loss 0.133935, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:16.153146: step 4996, loss 0.0858907, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:16.337603: step 4997, loss 0.0685886, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:16.535043: step 4998, loss 0.118646, acc 0.960784, learning_rate 0.0001
2017-10-10T12:03:16.743658: step 4999, loss 0.184469, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:16.970533: step 5000, loss 0.202617, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:17.405701: step 5000, loss 0.218226, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5000

2017-10-10T12:03:18.620278: step 5001, loss 0.114854, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:18.763043: step 5002, loss 0.0614919, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:18.915923: step 5003, loss 0.121173, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:19.052919: step 5004, loss 0.199329, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:19.167294: step 5005, loss 0.0822736, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:19.303415: step 5006, loss 0.153846, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:19.469036: step 5007, loss 0.102795, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:19.625210: step 5008, loss 0.108628, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:19.811650: step 5009, loss 0.143046, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:19.957071: step 5010, loss 0.0602248, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:20.145338: step 5011, loss 0.13723, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:20.361570: step 5012, loss 0.184979, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:20.552494: step 5013, loss 0.069125, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:20.724019: step 5014, loss 0.104668, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:20.916830: step 5015, loss 0.0710252, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:21.132955: step 5016, loss 0.0625199, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:21.355555: step 5017, loss 0.129446, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:21.537064: step 5018, loss 0.108776, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:21.696355: step 5019, loss 0.158972, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:21.891581: step 5020, loss 0.170042, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:22.097842: step 5021, loss 0.191145, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:22.300863: step 5022, loss 0.0832956, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:22.463157: step 5023, loss 0.118942, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:22.660956: step 5024, loss 0.112467, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:22.854014: step 5025, loss 0.17526, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:23.012657: step 5026, loss 0.11013, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:23.188201: step 5027, loss 0.153624, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:23.366735: step 5028, loss 0.122713, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:23.603124: step 5029, loss 0.104015, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:23.812874: step 5030, loss 0.112963, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:23.949087: step 5031, loss 0.234045, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:24.121785: step 5032, loss 0.0732581, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:24.321076: step 5033, loss 0.211488, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:24.477234: step 5034, loss 0.163463, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:24.636674: step 5035, loss 0.071401, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:24.849924: step 5036, loss 0.141482, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:25.055268: step 5037, loss 0.0638027, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:25.203345: step 5038, loss 0.167266, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:25.371470: step 5039, loss 0.0731568, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:25.573304: step 5040, loss 0.15194, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:26.038252: step 5040, loss 0.218387, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5040

2017-10-10T12:03:27.068689: step 5041, loss 0.147092, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:27.281112: step 5042, loss 0.114653, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:27.529256: step 5043, loss 0.0883557, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:27.694891: step 5044, loss 0.0963683, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:27.857884: step 5045, loss 0.18523, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:27.992811: step 5046, loss 0.0972134, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:28.118147: step 5047, loss 0.184628, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:28.262011: step 5048, loss 0.0869749, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:28.425966: step 5049, loss 0.0817894, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:28.640111: step 5050, loss 0.0846317, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:28.831521: step 5051, loss 0.071783, acc 1, learning_rate 0.0001
2017-10-10T12:03:29.009727: step 5052, loss 0.148262, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:29.224884: step 5053, loss 0.138243, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:29.412897: step 5054, loss 0.0799932, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:29.623550: step 5055, loss 0.0757509, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:29.767128: step 5056, loss 0.0835739, acc 1, learning_rate 0.0001
2017-10-10T12:03:29.899602: step 5057, loss 0.0425528, acc 1, learning_rate 0.0001
2017-10-10T12:03:30.009081: step 5058, loss 0.100584, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:30.157618: step 5059, loss 0.131851, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:30.305450: step 5060, loss 0.153364, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:30.462712: step 5061, loss 0.0540208, acc 1, learning_rate 0.0001
2017-10-10T12:03:30.683466: step 5062, loss 0.0511019, acc 1, learning_rate 0.0001
2017-10-10T12:03:30.886318: step 5063, loss 0.161046, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:31.047874: step 5064, loss 0.101791, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:31.214916: step 5065, loss 0.19532, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:31.419976: step 5066, loss 0.146585, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:31.564925: step 5067, loss 0.150567, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:31.772887: step 5068, loss 0.0481556, acc 1, learning_rate 0.0001
2017-10-10T12:03:31.941661: step 5069, loss 0.156237, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:32.142050: step 5070, loss 0.203855, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:32.289398: step 5071, loss 0.173536, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:32.485012: step 5072, loss 0.0941323, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:32.656710: step 5073, loss 0.148756, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:32.839372: step 5074, loss 0.154477, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:32.975503: step 5075, loss 0.158918, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:33.152332: step 5076, loss 0.0875061, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:33.365064: step 5077, loss 0.13401, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:33.548894: step 5078, loss 0.0995643, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:33.719206: step 5079, loss 0.0734306, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:33.884632: step 5080, loss 0.11188, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:34.299875: step 5080, loss 0.218785, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5080

2017-10-10T12:03:35.476866: step 5081, loss 0.0910214, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:35.676781: step 5082, loss 0.0502837, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:35.832965: step 5083, loss 0.275207, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:35.988592: step 5084, loss 0.165966, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:36.204296: step 5085, loss 0.146459, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:36.372513: step 5086, loss 0.0827024, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:36.514554: step 5087, loss 0.112891, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:36.716560: step 5088, loss 0.0783205, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:36.920888: step 5089, loss 0.0735529, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:37.065402: step 5090, loss 0.204482, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:37.238164: step 5091, loss 0.0611623, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:37.448349: step 5092, loss 0.0860272, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:37.612988: step 5093, loss 0.0870526, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:37.855472: step 5094, loss 0.08217, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:38.069705: step 5095, loss 0.17334, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:38.188727: step 5096, loss 0.129189, acc 0.941176, learning_rate 0.0001
2017-10-10T12:03:38.297995: step 5097, loss 0.0375265, acc 1, learning_rate 0.0001
2017-10-10T12:03:38.471345: step 5098, loss 0.110766, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:38.619768: step 5099, loss 0.0850706, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:38.760870: step 5100, loss 0.0945126, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:38.895401: step 5101, loss 0.0854019, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:39.024917: step 5102, loss 0.0630435, acc 1, learning_rate 0.0001
2017-10-10T12:03:39.240145: step 5103, loss 0.166308, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:39.380842: step 5104, loss 0.0808129, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:39.545121: step 5105, loss 0.0826851, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:39.733896: step 5106, loss 0.163594, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:39.939064: step 5107, loss 0.121614, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:40.050999: step 5108, loss 0.160773, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:40.261797: step 5109, loss 0.0887644, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:40.488230: step 5110, loss 0.217605, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:40.678805: step 5111, loss 0.127047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:40.800828: step 5112, loss 0.0984143, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:40.969181: step 5113, loss 0.192654, acc 0.890625, learning_rate 0.0001
2017-10-10T12:03:41.102745: step 5114, loss 0.257647, acc 0.890625, learning_rate 0.0001
2017-10-10T12:03:41.244826: step 5115, loss 0.0856506, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:41.368431: step 5116, loss 0.172543, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:41.488264: step 5117, loss 0.0864761, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:41.644872: step 5118, loss 0.0772369, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:41.840817: step 5119, loss 0.248454, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:41.972533: step 5120, loss 0.13355, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:42.465842: step 5120, loss 0.218992, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5120

2017-10-10T12:03:43.613051: step 5121, loss 0.140944, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:43.792866: step 5122, loss 0.160667, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:43.968985: step 5123, loss 0.0756587, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:44.153477: step 5124, loss 0.0625869, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:44.324405: step 5125, loss 0.109006, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:44.510186: step 5126, loss 0.244962, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:44.697367: step 5127, loss 0.0828769, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:44.861095: step 5128, loss 0.106292, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:45.010642: step 5129, loss 0.100378, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:45.190241: step 5130, loss 0.0836771, acc 1, learning_rate 0.0001
2017-10-10T12:03:45.357126: step 5131, loss 0.0609235, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:45.517076: step 5132, loss 0.135696, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:45.719843: step 5133, loss 0.0975103, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:45.935400: step 5134, loss 0.112302, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:46.112878: step 5135, loss 0.124472, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:46.251432: step 5136, loss 0.135177, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:46.461124: step 5137, loss 0.0779308, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:46.660855: step 5138, loss 0.185226, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:46.816574: step 5139, loss 0.110009, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:46.976825: step 5140, loss 0.0525091, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:47.180636: step 5141, loss 0.0688491, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:47.314520: step 5142, loss 0.0675346, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:47.476869: step 5143, loss 0.197008, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:47.682227: step 5144, loss 0.138866, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:47.889373: step 5145, loss 0.0739411, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:48.059833: step 5146, loss 0.170641, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:48.258486: step 5147, loss 0.106697, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:48.458449: step 5148, loss 0.0826242, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:48.692387: step 5149, loss 0.0750356, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:48.852945: step 5150, loss 0.0579466, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:48.991412: step 5151, loss 0.0636107, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:49.128840: step 5152, loss 0.0768091, acc 1, learning_rate 0.0001
2017-10-10T12:03:49.290654: step 5153, loss 0.105744, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:49.457120: step 5154, loss 0.122898, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:49.580894: step 5155, loss 0.0723602, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:49.737213: step 5156, loss 0.106131, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:49.849185: step 5157, loss 0.0927203, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:50.046644: step 5158, loss 0.109113, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:50.211846: step 5159, loss 0.10301, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:50.364809: step 5160, loss 0.0993344, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:50.820028: step 5160, loss 0.217321, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5160

2017-10-10T12:03:52.011618: step 5161, loss 0.121581, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:52.123464: step 5162, loss 0.0383959, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:52.265804: step 5163, loss 0.0661675, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:52.423358: step 5164, loss 0.0955735, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:52.585001: step 5165, loss 0.0884437, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:52.740807: step 5166, loss 0.0945464, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:52.952880: step 5167, loss 0.0740731, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:53.102323: step 5168, loss 0.0771732, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:53.305011: step 5169, loss 0.106753, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:53.527706: step 5170, loss 0.155907, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:53.680862: step 5171, loss 0.18331, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:53.912860: step 5172, loss 0.0863054, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:54.060235: step 5173, loss 0.292035, acc 0.890625, learning_rate 0.0001
2017-10-10T12:03:54.264262: step 5174, loss 0.101023, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:54.480164: step 5175, loss 0.153978, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:54.676848: step 5176, loss 0.112128, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:54.806810: step 5177, loss 0.161973, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:55.008830: step 5178, loss 0.120315, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:55.183999: step 5179, loss 0.156612, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:55.340866: step 5180, loss 0.147062, acc 0.90625, learning_rate 0.0001
2017-10-10T12:03:55.528557: step 5181, loss 0.112005, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:55.716829: step 5182, loss 0.0885743, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:55.941138: step 5183, loss 0.102117, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:56.156944: step 5184, loss 0.119983, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:56.338161: step 5185, loss 0.0957156, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:56.496099: step 5186, loss 0.250467, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:56.691074: step 5187, loss 0.110372, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:56.896240: step 5188, loss 0.103107, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:57.088224: step 5189, loss 0.0877395, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:57.225161: step 5190, loss 0.113735, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:57.412857: step 5191, loss 0.0700897, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:57.607111: step 5192, loss 0.136263, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:57.800902: step 5193, loss 0.111544, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:57.918585: step 5194, loss 0.112267, acc 0.960784, learning_rate 0.0001
2017-10-10T12:03:58.139219: step 5195, loss 0.086674, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:58.305019: step 5196, loss 0.113164, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:58.460573: step 5197, loss 0.0816861, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:58.648864: step 5198, loss 0.0804582, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:58.832186: step 5199, loss 0.174745, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:58.993422: step 5200, loss 0.0721446, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:59.532826: step 5200, loss 0.21752, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5200

2017-10-10T12:04:00.316849: step 5201, loss 0.0985657, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:00.515020: step 5202, loss 0.141757, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:00.704867: step 5203, loss 0.0740963, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:00.896861: step 5204, loss 0.0825429, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:01.060840: step 5205, loss 0.0999497, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:01.257791: step 5206, loss 0.17823, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:01.463643: step 5207, loss 0.05007, acc 1, learning_rate 0.0001
2017-10-10T12:04:01.627662: step 5208, loss 0.061702, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:01.801179: step 5209, loss 0.208888, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:01.976192: step 5210, loss 0.144964, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:02.192768: step 5211, loss 0.084299, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:02.373247: step 5212, loss 0.0962231, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:02.573194: step 5213, loss 0.152754, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:02.828978: step 5214, loss 0.122382, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:02.966926: step 5215, loss 0.255563, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:03.116128: step 5216, loss 0.165071, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:03.269396: step 5217, loss 0.110439, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:03.420509: step 5218, loss 0.179123, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:03.539260: step 5219, loss 0.0975382, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:03.681966: step 5220, loss 0.0735533, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:03.893003: step 5221, loss 0.1277, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:04.069047: step 5222, loss 0.0731516, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:04.296851: step 5223, loss 0.133816, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:04.438958: step 5224, loss 0.189497, acc 0.90625, learning_rate 0.0001
2017-10-10T12:04:04.616378: step 5225, loss 0.0966448, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:04.797025: step 5226, loss 0.19914, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:04.974986: step 5227, loss 0.167467, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:05.126500: step 5228, loss 0.053765, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:05.355670: step 5229, loss 0.186166, acc 0.890625, learning_rate 0.0001
2017-10-10T12:04:05.543466: step 5230, loss 0.0510544, acc 1, learning_rate 0.0001
2017-10-10T12:04:05.725258: step 5231, loss 0.0899936, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:05.928856: step 5232, loss 0.0920127, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:06.149344: step 5233, loss 0.0469111, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:06.343113: step 5234, loss 0.146016, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:06.483844: step 5235, loss 0.16421, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:06.656102: step 5236, loss 0.146408, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:06.868788: step 5237, loss 0.0869642, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:07.005015: step 5238, loss 0.128684, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:07.204881: step 5239, loss 0.0602744, acc 1, learning_rate 0.0001
2017-10-10T12:04:07.406788: step 5240, loss 0.156205, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:07.767192: step 5240, loss 0.216663, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5240

2017-10-10T12:04:08.833972: step 5241, loss 0.0973719, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:09.045003: step 5242, loss 0.130235, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:09.262563: step 5243, loss 0.116446, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:09.529194: step 5244, loss 0.115692, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:09.742956: step 5245, loss 0.161524, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:09.876111: step 5246, loss 0.0791172, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:10.030020: step 5247, loss 0.173522, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:10.150935: step 5248, loss 0.110244, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:10.279570: step 5249, loss 0.0923362, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:10.448883: step 5250, loss 0.114342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:10.586868: step 5251, loss 0.0887976, acc 1, learning_rate 0.0001
2017-10-10T12:04:10.731593: step 5252, loss 0.0827287, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:10.909101: step 5253, loss 0.0873879, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:11.107242: step 5254, loss 0.0855522, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:11.323036: step 5255, loss 0.126208, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:11.468312: step 5256, loss 0.203297, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:11.628818: step 5257, loss 0.103189, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:11.826089: step 5258, loss 0.0697172, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:12.038315: step 5259, loss 0.0446506, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:12.218683: step 5260, loss 0.0886097, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:12.365491: step 5261, loss 0.133232, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:12.541233: step 5262, loss 0.165914, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:12.704910: step 5263, loss 0.129268, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:12.891291: step 5264, loss 0.0930122, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:13.085118: step 5265, loss 0.151921, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:13.207429: step 5266, loss 0.0717033, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:13.389303: step 5267, loss 0.0632115, acc 1, learning_rate 0.0001
2017-10-10T12:04:13.624850: step 5268, loss 0.0736567, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:13.856144: step 5269, loss 0.136003, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:14.036761: step 5270, loss 0.232031, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:14.175494: step 5271, loss 0.192056, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:14.331208: step 5272, loss 0.217303, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:14.456910: step 5273, loss 0.139009, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:14.574070: step 5274, loss 0.111984, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:14.712638: step 5275, loss 0.163297, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:14.859409: step 5276, loss 0.130276, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:15.032975: step 5277, loss 0.174999, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:15.235277: step 5278, loss 0.13032, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:15.404936: step 5279, loss 0.102527, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:15.645205: step 5280, loss 0.117617, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:16.052583: step 5280, loss 0.219188, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5280

2017-10-10T12:04:17.333929: step 5281, loss 0.0860607, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:17.531048: step 5282, loss 0.127568, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:17.727536: step 5283, loss 0.0966386, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:17.869139: step 5284, loss 0.0927015, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:18.048896: step 5285, loss 0.106677, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:18.255178: step 5286, loss 0.114351, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:18.457651: step 5287, loss 0.193436, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:18.657008: step 5288, loss 0.11244, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:18.797073: step 5289, loss 0.120126, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:19.007287: step 5290, loss 0.145517, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:19.203868: step 5291, loss 0.066486, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:19.335454: step 5292, loss 0.113559, acc 0.980392, learning_rate 0.0001
2017-10-10T12:04:19.504814: step 5293, loss 0.319905, acc 0.890625, learning_rate 0.0001
2017-10-10T12:04:19.703504: step 5294, loss 0.0967916, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:19.929216: step 5295, loss 0.101003, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:20.124613: step 5296, loss 0.0704514, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:20.298939: step 5297, loss 0.112397, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:20.452864: step 5298, loss 0.0983026, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:20.609456: step 5299, loss 0.124032, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:20.761912: step 5300, loss 0.121319, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:20.920697: step 5301, loss 0.0526597, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:21.056666: step 5302, loss 0.136954, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:21.190022: step 5303, loss 0.119677, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:21.341000: step 5304, loss 0.101293, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:21.513693: step 5305, loss 0.0974249, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:21.716947: step 5306, loss 0.139408, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:21.876937: step 5307, loss 0.19699, acc 0.90625, learning_rate 0.0001
2017-10-10T12:04:22.081700: step 5308, loss 0.0883692, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:22.282332: step 5309, loss 0.102871, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:22.460859: step 5310, loss 0.147704, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:22.595418: step 5311, loss 0.101782, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:22.812891: step 5312, loss 0.128798, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:23.023193: step 5313, loss 0.0783591, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:23.199278: step 5314, loss 0.102598, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:23.350692: step 5315, loss 0.0192251, acc 1, learning_rate 0.0001
2017-10-10T12:04:23.552866: step 5316, loss 0.0570611, acc 1, learning_rate 0.0001
2017-10-10T12:04:23.739316: step 5317, loss 0.161107, acc 0.90625, learning_rate 0.0001
2017-10-10T12:04:23.944852: step 5318, loss 0.19611, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:24.093100: step 5319, loss 0.102244, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:24.306083: step 5320, loss 0.0622619, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:24.751748: step 5320, loss 0.218703, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5320

2017-10-10T12:04:25.603529: step 5321, loss 0.108156, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:25.785166: step 5322, loss 0.234151, acc 0.90625, learning_rate 0.0001
2017-10-10T12:04:26.000902: step 5323, loss 0.130436, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:26.178085: step 5324, loss 0.0669586, acc 1, learning_rate 0.0001
2017-10-10T12:04:26.336121: step 5325, loss 0.123632, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:26.542781: step 5326, loss 0.072875, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:26.728847: step 5327, loss 0.147915, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:26.861030: step 5328, loss 0.0443429, acc 1, learning_rate 0.0001
2017-10-10T12:04:27.045216: step 5329, loss 0.154828, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:27.222869: step 5330, loss 0.092018, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:27.373380: step 5331, loss 0.0512297, acc 1, learning_rate 0.0001
2017-10-10T12:04:27.561319: step 5332, loss 0.0857968, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:27.752002: step 5333, loss 0.0813669, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:27.974734: step 5334, loss 0.104371, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:28.135523: step 5335, loss 0.161819, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:28.316881: step 5336, loss 0.108102, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:28.550179: step 5337, loss 0.185091, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:28.728304: step 5338, loss 0.0940707, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:28.900956: step 5339, loss 0.0905934, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:29.093398: step 5340, loss 0.153641, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:29.275553: step 5341, loss 0.183461, acc 0.90625, learning_rate 0.0001
2017-10-10T12:04:29.464895: step 5342, loss 0.108233, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:29.603135: step 5343, loss 0.292959, acc 0.875, learning_rate 0.0001
2017-10-10T12:04:29.788767: step 5344, loss 0.101331, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:30.012688: step 5345, loss 0.119525, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:30.161085: step 5346, loss 0.175353, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:30.328946: step 5347, loss 0.106747, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:30.584879: step 5348, loss 0.135596, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:30.826569: step 5349, loss 0.0381903, acc 1, learning_rate 0.0001
2017-10-10T12:04:30.980834: step 5350, loss 0.139736, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:31.148828: step 5351, loss 0.121903, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:31.252865: step 5352, loss 0.159141, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:31.398879: step 5353, loss 0.0415239, acc 1, learning_rate 0.0001
2017-10-10T12:04:31.545425: step 5354, loss 0.0863351, acc 1, learning_rate 0.0001
2017-10-10T12:04:31.697023: step 5355, loss 0.064339, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:31.861272: step 5356, loss 0.102802, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:32.060785: step 5357, loss 0.0859468, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:32.272468: step 5358, loss 0.130125, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:32.437025: step 5359, loss 0.0814009, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:32.607431: step 5360, loss 0.208266, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:33.003893: step 5360, loss 0.2166, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5360

2017-10-10T12:04:34.108854: step 5361, loss 0.0649591, acc 1, learning_rate 0.0001
2017-10-10T12:04:34.260106: step 5362, loss 0.0986636, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:34.452176: step 5363, loss 0.0905467, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:34.614994: step 5364, loss 0.11938, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:34.772071: step 5365, loss 0.104434, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:34.963419: step 5366, loss 0.0646324, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:35.142842: step 5367, loss 0.0828502, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:35.376925: step 5368, loss 0.201695, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:35.554929: step 5369, loss 0.153124, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:35.771735: step 5370, loss 0.15918, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:35.900931: step 5371, loss 0.0514546, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:36.027812: step 5372, loss 0.226633, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:36.153066: step 5373, loss 0.142839, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:36.324081: step 5374, loss 0.136456, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:36.475988: step 5375, loss 0.118775, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:36.660854: step 5376, loss 0.162314, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:36.845673: step 5377, loss 0.151439, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:37.003117: step 5378, loss 0.0699838, acc 1, learning_rate 0.0001
2017-10-10T12:04:37.205831: step 5379, loss 0.125413, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:37.432103: step 5380, loss 0.102344, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:37.627924: step 5381, loss 0.189287, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:37.809160: step 5382, loss 0.0806139, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:38.000622: step 5383, loss 0.0956676, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:38.177017: step 5384, loss 0.0842591, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:38.353635: step 5385, loss 0.0931313, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:38.549367: step 5386, loss 0.112835, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:38.776852: step 5387, loss 0.135343, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:38.955014: step 5388, loss 0.121462, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:39.132865: step 5389, loss 0.0697888, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:39.281727: step 5390, loss 0.125924, acc 0.960784, learning_rate 0.0001
2017-10-10T12:04:39.493065: step 5391, loss 0.0777112, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:39.719819: step 5392, loss 0.177332, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:39.924970: step 5393, loss 0.104363, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:40.082809: step 5394, loss 0.216523, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:40.232909: step 5395, loss 0.147848, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:40.432821: step 5396, loss 0.0957948, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:40.628985: step 5397, loss 0.0798613, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:40.836942: step 5398, loss 0.111842, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:41.048558: step 5399, loss 0.136494, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:41.238950: step 5400, loss 0.120023, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:41.545179: step 5400, loss 0.216469, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5400

2017-10-10T12:04:42.718662: step 5401, loss 0.0811928, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:42.938232: step 5402, loss 0.0841596, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:43.145491: step 5403, loss 0.205346, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:43.322215: step 5404, loss 0.15722, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:43.459187: step 5405, loss 0.124067, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:43.666972: step 5406, loss 0.0696983, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:43.845971: step 5407, loss 0.0505962, acc 1, learning_rate 0.0001
2017-10-10T12:04:44.021513: step 5408, loss 0.101537, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:44.220852: step 5409, loss 0.0788838, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:44.429085: step 5410, loss 0.12054, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:44.568854: step 5411, loss 0.245221, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:44.761900: step 5412, loss 0.10069, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:44.966888: step 5413, loss 0.100292, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:45.157515: step 5414, loss 0.140634, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:45.292926: step 5415, loss 0.0606246, acc 1, learning_rate 0.0001
2017-10-10T12:04:45.488133: step 5416, loss 0.105411, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:45.632104: step 5417, loss 0.0686695, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:45.820853: step 5418, loss 0.13386, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:46.016968: step 5419, loss 0.0558155, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:46.252861: step 5420, loss 0.0487098, acc 1, learning_rate 0.0001
2017-10-10T12:04:46.464904: step 5421, loss 0.0707014, acc 1, learning_rate 0.0001
2017-10-10T12:04:46.650593: step 5422, loss 0.149696, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:46.776859: step 5423, loss 0.216755, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:46.905032: step 5424, loss 0.0298334, acc 1, learning_rate 0.0001
2017-10-10T12:04:47.044957: step 5425, loss 0.116079, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:47.188495: step 5426, loss 0.073872, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:47.341885: step 5427, loss 0.144366, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:47.488827: step 5428, loss 0.176408, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:47.641195: step 5429, loss 0.116427, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:47.789136: step 5430, loss 0.100809, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:47.986342: step 5431, loss 0.195016, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:48.190196: step 5432, loss 0.192829, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:48.353082: step 5433, loss 0.0660528, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:48.543217: step 5434, loss 0.0723358, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:48.740716: step 5435, loss 0.0758386, acc 1, learning_rate 0.0001
2017-10-10T12:04:48.941324: step 5436, loss 0.156334, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:49.093802: step 5437, loss 0.117665, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:49.263703: step 5438, loss 0.059034, acc 1, learning_rate 0.0001
2017-10-10T12:04:49.471826: step 5439, loss 0.0987323, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:49.620544: step 5440, loss 0.120481, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:50.015121: step 5440, loss 0.213377, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5440

2017-10-10T12:04:50.976972: step 5441, loss 0.0490252, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:51.187431: step 5442, loss 0.0893035, acc 1, learning_rate 0.0001
2017-10-10T12:04:51.417908: step 5443, loss 0.179334, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:51.575438: step 5444, loss 0.154409, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:51.698323: step 5445, loss 0.124141, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:51.842511: step 5446, loss 0.148819, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:51.999318: step 5447, loss 0.0487952, acc 1, learning_rate 0.0001
2017-10-10T12:04:52.144507: step 5448, loss 0.0986533, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:52.272910: step 5449, loss 0.0696564, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:52.396832: step 5450, loss 0.106237, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:52.554327: step 5451, loss 0.103754, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:52.765726: step 5452, loss 0.0795801, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:52.963828: step 5453, loss 0.0801962, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:53.128721: step 5454, loss 0.164652, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:53.313161: step 5455, loss 0.0679081, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:53.523127: step 5456, loss 0.158626, acc 0.90625, learning_rate 0.0001
2017-10-10T12:04:53.736589: step 5457, loss 0.176722, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:53.945105: step 5458, loss 0.166727, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:54.093187: step 5459, loss 0.0923934, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:54.291792: step 5460, loss 0.179046, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:54.445105: step 5461, loss 0.121765, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:54.662466: step 5462, loss 0.120453, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:54.851543: step 5463, loss 0.0796104, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:55.074992: step 5464, loss 0.0454956, acc 1, learning_rate 0.0001
2017-10-10T12:04:55.214270: step 5465, loss 0.131047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:55.367891: step 5466, loss 0.128959, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:55.570169: step 5467, loss 0.105943, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:55.748862: step 5468, loss 0.0318079, acc 1, learning_rate 0.0001
2017-10-10T12:04:55.935366: step 5469, loss 0.108528, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:56.106627: step 5470, loss 0.12737, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:56.273711: step 5471, loss 0.117523, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:56.464891: step 5472, loss 0.121036, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:56.637080: step 5473, loss 0.127099, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:56.826229: step 5474, loss 0.0907907, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:57.009275: step 5475, loss 0.18569, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:57.216574: step 5476, loss 0.188732, acc 0.90625, learning_rate 0.0001
2017-10-10T12:04:57.428458: step 5477, loss 0.0580457, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:57.684862: step 5478, loss 0.0918382, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:57.808977: step 5479, loss 0.163847, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:57.980868: step 5480, loss 0.109892, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:58.286560: step 5480, loss 0.212963, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5480

2017-10-10T12:04:59.309095: step 5481, loss 0.0631975, acc 1, learning_rate 0.0001
2017-10-10T12:04:59.495790: step 5482, loss 0.185538, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:59.692952: step 5483, loss 0.0980215, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:59.880851: step 5484, loss 0.134989, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:00.064743: step 5485, loss 0.0540582, acc 1, learning_rate 0.0001
2017-10-10T12:05:00.272980: step 5486, loss 0.0955905, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:00.437081: step 5487, loss 0.131507, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:00.590684: step 5488, loss 0.15421, acc 0.980392, learning_rate 0.0001
2017-10-10T12:05:00.733415: step 5489, loss 0.135158, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:00.900844: step 5490, loss 0.147502, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:01.110722: step 5491, loss 0.110357, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:01.316842: step 5492, loss 0.0928612, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:01.516891: step 5493, loss 0.149132, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:01.732993: step 5494, loss 0.142786, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:01.927010: step 5495, loss 0.114582, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:02.068192: step 5496, loss 0.168496, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:02.225001: step 5497, loss 0.0484545, acc 1, learning_rate 0.0001
2017-10-10T12:05:02.363212: step 5498, loss 0.136247, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:02.532957: step 5499, loss 0.0886565, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:02.697058: step 5500, loss 0.126787, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:02.852322: step 5501, loss 0.199657, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:03.043043: step 5502, loss 0.0699457, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:03.244889: step 5503, loss 0.184366, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:03.363626: step 5504, loss 0.247088, acc 0.890625, learning_rate 0.0001
2017-10-10T12:05:03.560996: step 5505, loss 0.0690575, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:03.736862: step 5506, loss 0.1752, acc 0.890625, learning_rate 0.0001
2017-10-10T12:05:03.891597: step 5507, loss 0.0803793, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:04.061593: step 5508, loss 0.0598869, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:04.288837: step 5509, loss 0.152493, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:04.507259: step 5510, loss 0.139771, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:04.714658: step 5511, loss 0.186606, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:04.861209: step 5512, loss 0.149487, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:05.028837: step 5513, loss 0.115101, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:05.239364: step 5514, loss 0.0977012, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:05.459156: step 5515, loss 0.214681, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:05.645049: step 5516, loss 0.19166, acc 0.890625, learning_rate 0.0001
2017-10-10T12:05:05.788663: step 5517, loss 0.0941061, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:05.984320: step 5518, loss 0.0540886, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:06.182209: step 5519, loss 0.135062, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:06.349675: step 5520, loss 0.0987624, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:06.672675: step 5520, loss 0.21334, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5520

2017-10-10T12:05:07.979773: step 5521, loss 0.109193, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:08.148873: step 5522, loss 0.0898686, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:08.332847: step 5523, loss 0.091211, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:08.562727: step 5524, loss 0.0748006, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:08.769118: step 5525, loss 0.140885, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:08.913738: step 5526, loss 0.137614, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:09.066160: step 5527, loss 0.141059, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:09.202863: step 5528, loss 0.204894, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:09.348832: step 5529, loss 0.165808, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:09.500827: step 5530, loss 0.0847336, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:09.650436: step 5531, loss 0.0668704, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:09.819334: step 5532, loss 0.140618, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:10.032846: step 5533, loss 0.133649, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:10.253672: step 5534, loss 0.070655, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:10.456865: step 5535, loss 0.179232, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:10.593102: step 5536, loss 0.0708551, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:10.772499: step 5537, loss 0.121312, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:10.979166: step 5538, loss 0.116119, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:11.176853: step 5539, loss 0.0760715, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:11.345200: step 5540, loss 0.0778875, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:11.504847: step 5541, loss 0.0390952, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:11.692866: step 5542, loss 0.0551661, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:11.948917: step 5543, loss 0.0815999, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:12.160315: step 5544, loss 0.0764385, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:12.303861: step 5545, loss 0.0619563, acc 1, learning_rate 0.0001
2017-10-10T12:05:12.456401: step 5546, loss 0.0774879, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:12.586017: step 5547, loss 0.120949, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:12.707400: step 5548, loss 0.0773064, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:12.851317: step 5549, loss 0.125684, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:13.004811: step 5550, loss 0.110923, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:13.169723: step 5551, loss 0.207426, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:13.334254: step 5552, loss 0.0811674, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:13.555445: step 5553, loss 0.0899272, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:13.720946: step 5554, loss 0.0835955, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:13.896562: step 5555, loss 0.0630623, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:14.076837: step 5556, loss 0.0836935, acc 1, learning_rate 0.0001
2017-10-10T12:05:14.276169: step 5557, loss 0.125019, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:14.484724: step 5558, loss 0.159159, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:14.637099: step 5559, loss 0.192753, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:14.796964: step 5560, loss 0.0378652, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:15.234571: step 5560, loss 0.215022, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5560

2017-10-10T12:05:16.552413: step 5561, loss 0.0778714, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:16.700850: step 5562, loss 0.141466, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:16.884941: step 5563, loss 0.0648339, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:17.072868: step 5564, loss 0.093842, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:17.299323: step 5565, loss 0.160593, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:17.514652: step 5566, loss 0.111204, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:17.654129: step 5567, loss 0.118033, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:17.822249: step 5568, loss 0.0823671, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:18.033361: step 5569, loss 0.12424, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:18.209846: step 5570, loss 0.0960053, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:18.354538: step 5571, loss 0.120722, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:18.547830: step 5572, loss 0.117635, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:18.745646: step 5573, loss 0.123102, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:18.902017: step 5574, loss 0.172379, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:19.063210: step 5575, loss 0.0609359, acc 1, learning_rate 0.0001
2017-10-10T12:05:19.262136: step 5576, loss 0.0836375, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:19.505242: step 5577, loss 0.0595105, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:19.693939: step 5578, loss 0.303551, acc 0.90625, learning_rate 0.0001
2017-10-10T12:05:19.803936: step 5579, loss 0.150055, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:19.958424: step 5580, loss 0.0946723, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:20.115088: step 5581, loss 0.121033, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:20.256827: step 5582, loss 0.120586, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:20.381078: step 5583, loss 0.0941703, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:20.510899: step 5584, loss 0.104358, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:20.719372: step 5585, loss 0.123739, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:20.846498: step 5586, loss 0.139523, acc 0.960784, learning_rate 0.0001
2017-10-10T12:05:21.013202: step 5587, loss 0.123368, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:21.240200: step 5588, loss 0.208502, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:21.448844: step 5589, loss 0.074012, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:21.633098: step 5590, loss 0.158066, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:21.810767: step 5591, loss 0.0674342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:22.005709: step 5592, loss 0.0758945, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:22.216845: step 5593, loss 0.120708, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:22.345797: step 5594, loss 0.0703097, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:22.554193: step 5595, loss 0.0927903, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:22.778421: step 5596, loss 0.109984, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:22.920888: step 5597, loss 0.0645728, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:23.044828: step 5598, loss 0.144374, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:23.209919: step 5599, loss 0.119716, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:23.357511: step 5600, loss 0.100091, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:23.641872: step 5600, loss 0.216347, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5600

2017-10-10T12:05:24.702075: step 5601, loss 0.0904197, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:24.884868: step 5602, loss 0.10882, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:25.057105: step 5603, loss 0.121179, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:25.225041: step 5604, loss 0.0541866, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:25.430960: step 5605, loss 0.145065, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:25.624877: step 5606, loss 0.25593, acc 0.90625, learning_rate 0.0001
2017-10-10T12:05:25.812975: step 5607, loss 0.055778, acc 1, learning_rate 0.0001
2017-10-10T12:05:25.960664: step 5608, loss 0.163597, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:26.161989: step 5609, loss 0.145831, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:26.360837: step 5610, loss 0.0837016, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:26.516670: step 5611, loss 0.134485, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:26.688149: step 5612, loss 0.0849131, acc 1, learning_rate 0.0001
2017-10-10T12:05:26.909124: step 5613, loss 0.192998, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:27.111864: step 5614, loss 0.0848904, acc 1, learning_rate 0.0001
2017-10-10T12:05:27.304986: step 5615, loss 0.0565421, acc 1, learning_rate 0.0001
2017-10-10T12:05:27.449055: step 5616, loss 0.0719727, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:27.660882: step 5617, loss 0.172288, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:27.848804: step 5618, loss 0.157218, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:28.008873: step 5619, loss 0.0808012, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:28.216419: step 5620, loss 0.137843, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:28.377695: step 5621, loss 0.101223, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:28.571049: step 5622, loss 0.150237, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:28.744987: step 5623, loss 0.183159, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:28.890404: step 5624, loss 0.219861, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:29.112986: step 5625, loss 0.0888917, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:29.317181: step 5626, loss 0.0867283, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:29.528167: step 5627, loss 0.0871718, acc 1, learning_rate 0.0001
2017-10-10T12:05:29.690744: step 5628, loss 0.107484, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:29.863251: step 5629, loss 0.15306, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:30.018532: step 5630, loss 0.0706497, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:30.244851: step 5631, loss 0.140972, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:30.481128: step 5632, loss 0.0974896, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:30.643816: step 5633, loss 0.100654, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:30.772872: step 5634, loss 0.0797888, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:30.919765: step 5635, loss 0.154112, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:31.067914: step 5636, loss 0.0729123, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:31.222382: step 5637, loss 0.100053, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:31.373862: step 5638, loss 0.0751311, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:31.497505: step 5639, loss 0.0934752, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:31.630235: step 5640, loss 0.135269, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:32.028918: step 5640, loss 0.214317, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5640

2017-10-10T12:05:33.151315: step 5641, loss 0.152622, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:33.333705: step 5642, loss 0.0986916, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:33.463372: step 5643, loss 0.0622512, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:33.581607: step 5644, loss 0.125716, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:33.750812: step 5645, loss 0.100903, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:33.904081: step 5646, loss 0.156039, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:34.068927: step 5647, loss 0.270964, acc 0.90625, learning_rate 0.0001
2017-10-10T12:05:34.211387: step 5648, loss 0.106626, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:34.415504: step 5649, loss 0.101355, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:34.588925: step 5650, loss 0.151464, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:34.768961: step 5651, loss 0.0985155, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:34.942611: step 5652, loss 0.12086, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:35.148135: step 5653, loss 0.078654, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:35.293067: step 5654, loss 0.192006, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:35.448802: step 5655, loss 0.106629, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:35.651281: step 5656, loss 0.107635, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:35.858921: step 5657, loss 0.129795, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:36.014498: step 5658, loss 0.126318, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:36.191650: step 5659, loss 0.132477, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:36.397157: step 5660, loss 0.177328, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:36.613951: step 5661, loss 0.0544958, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:36.788877: step 5662, loss 0.0917166, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:36.937250: step 5663, loss 0.126245, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:37.136871: step 5664, loss 0.108055, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:37.335357: step 5665, loss 0.117704, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:37.492123: step 5666, loss 0.124143, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:37.632838: step 5667, loss 0.0708833, acc 1, learning_rate 0.0001
2017-10-10T12:05:37.836236: step 5668, loss 0.0586203, acc 1, learning_rate 0.0001
2017-10-10T12:05:38.022941: step 5669, loss 0.0858972, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:38.140475: step 5670, loss 0.214011, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:38.347356: step 5671, loss 0.109785, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:38.549754: step 5672, loss 0.104353, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:38.748283: step 5673, loss 0.0775442, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:38.905014: step 5674, loss 0.0599679, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:39.072833: step 5675, loss 0.106713, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.290382: step 5676, loss 0.104067, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:39.496842: step 5677, loss 0.120003, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:39.680902: step 5678, loss 0.0591773, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:39.828975: step 5679, loss 0.0656695, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:39.996817: step 5680, loss 0.136847, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:40.384998: step 5680, loss 0.214913, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5680

2017-10-10T12:05:41.612836: step 5681, loss 0.0568608, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:41.718442: step 5682, loss 0.102436, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:41.841989: step 5683, loss 0.039458, acc 1, learning_rate 0.0001
2017-10-10T12:05:41.983617: step 5684, loss 0.0755708, acc 1, learning_rate 0.0001
2017-10-10T12:05:42.137748: step 5685, loss 0.187875, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:42.321014: step 5686, loss 0.0649819, acc 1, learning_rate 0.0001
2017-10-10T12:05:42.448827: step 5687, loss 0.104347, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:42.663852: step 5688, loss 0.0925711, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:42.851078: step 5689, loss 0.103804, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:43.004948: step 5690, loss 0.116042, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:43.177635: step 5691, loss 0.0995447, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:43.408881: step 5692, loss 0.0804905, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:43.661881: step 5693, loss 0.137517, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:43.790234: step 5694, loss 0.118288, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:43.919546: step 5695, loss 0.144435, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:44.074421: step 5696, loss 0.0468168, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:44.232895: step 5697, loss 0.14517, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:44.355093: step 5698, loss 0.0918761, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:44.531216: step 5699, loss 0.144946, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:44.719335: step 5700, loss 0.131653, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:44.872890: step 5701, loss 0.114425, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:45.064865: step 5702, loss 0.170671, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:45.264502: step 5703, loss 0.16639, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:45.462304: step 5704, loss 0.0720479, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:45.602741: step 5705, loss 0.0626326, acc 1, learning_rate 0.0001
2017-10-10T12:05:45.772533: step 5706, loss 0.155837, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:45.972868: step 5707, loss 0.129765, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:46.117402: step 5708, loss 0.0517577, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:46.303481: step 5709, loss 0.155193, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:46.533074: step 5710, loss 0.0967456, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:46.748532: step 5711, loss 0.126155, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:46.964942: step 5712, loss 0.176446, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:47.121243: step 5713, loss 0.123484, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:47.282983: step 5714, loss 0.134161, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:47.488850: step 5715, loss 0.0743735, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:47.645134: step 5716, loss 0.087704, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:47.797645: step 5717, loss 0.110635, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:48.016247: step 5718, loss 0.0768027, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:48.233279: step 5719, loss 0.169219, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:48.392674: step 5720, loss 0.113611, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:48.748934: step 5720, loss 0.214227, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5720

2017-10-10T12:05:49.726353: step 5721, loss 0.0813896, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:49.911664: step 5722, loss 0.0885247, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:50.108585: step 5723, loss 0.104464, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:50.301023: step 5724, loss 0.0624933, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:50.412968: step 5725, loss 0.0863492, acc 1, learning_rate 0.0001
2017-10-10T12:05:50.615177: step 5726, loss 0.162599, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:50.800873: step 5727, loss 0.0427779, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:50.985069: step 5728, loss 0.0738996, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:51.171560: step 5729, loss 0.113297, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:51.387999: step 5730, loss 0.0726406, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:51.584275: step 5731, loss 0.126938, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:51.744522: step 5732, loss 0.0712202, acc 1, learning_rate 0.0001
2017-10-10T12:05:51.892447: step 5733, loss 0.157106, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:52.132875: step 5734, loss 0.0826037, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:52.366729: step 5735, loss 0.0987333, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:52.488894: step 5736, loss 0.0928463, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:52.628738: step 5737, loss 0.0429209, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:52.791208: step 5738, loss 0.148395, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:52.946058: step 5739, loss 0.0922884, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:53.105484: step 5740, loss 0.142749, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:53.263078: step 5741, loss 0.0426924, acc 1, learning_rate 0.0001
2017-10-10T12:05:53.383458: step 5742, loss 0.0539975, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:53.538172: step 5743, loss 0.119338, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:53.760822: step 5744, loss 0.187593, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:54.005373: step 5745, loss 0.110636, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:54.163429: step 5746, loss 0.0497676, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:54.303611: step 5747, loss 0.202359, acc 0.90625, learning_rate 0.0001
2017-10-10T12:05:54.419077: step 5748, loss 0.092699, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:54.567706: step 5749, loss 0.126823, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:54.732530: step 5750, loss 0.0620579, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:54.864970: step 5751, loss 0.168836, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:55.065642: step 5752, loss 0.0920463, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:55.231225: step 5753, loss 0.0749459, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:55.436872: step 5754, loss 0.0523882, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:55.569343: step 5755, loss 0.10408, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:55.745149: step 5756, loss 0.109225, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:55.952380: step 5757, loss 0.132098, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:56.073544: step 5758, loss 0.041137, acc 1, learning_rate 0.0001
2017-10-10T12:05:56.259964: step 5759, loss 0.152698, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:56.453561: step 5760, loss 0.0617442, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:56.824051: step 5760, loss 0.214488, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5760

2017-10-10T12:05:57.971822: step 5761, loss 0.0853674, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:58.172859: step 5762, loss 0.0542163, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:58.300634: step 5763, loss 0.153768, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:58.516856: step 5764, loss 0.165316, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:58.700853: step 5765, loss 0.148465, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:58.890275: step 5766, loss 0.111931, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:59.075015: step 5767, loss 0.16213, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:59.277712: step 5768, loss 0.239648, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:59.460970: step 5769, loss 0.138204, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:59.617043: step 5770, loss 0.0951601, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:59.818555: step 5771, loss 0.0777886, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:00.025345: step 5772, loss 0.140999, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:00.232902: step 5773, loss 0.182216, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:00.405006: step 5774, loss 0.136992, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:00.561079: step 5775, loss 0.0538718, acc 1, learning_rate 0.0001
2017-10-10T12:06:00.765755: step 5776, loss 0.0910097, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:00.965192: step 5777, loss 0.0669159, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:01.118418: step 5778, loss 0.0713275, acc 1, learning_rate 0.0001
2017-10-10T12:06:01.278862: step 5779, loss 0.101016, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:01.492781: step 5780, loss 0.0908983, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:01.651210: step 5781, loss 0.134775, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:01.788272: step 5782, loss 0.0560795, acc 1, learning_rate 0.0001
2017-10-10T12:06:01.978512: step 5783, loss 0.0795671, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:02.152294: step 5784, loss 0.114919, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:02.318337: step 5785, loss 0.116552, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:02.496853: step 5786, loss 0.14165, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:02.668828: step 5787, loss 0.0966779, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:02.857676: step 5788, loss 0.0732124, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:03.017009: step 5789, loss 0.118517, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:03.229470: step 5790, loss 0.109345, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:03.443885: step 5791, loss 0.109223, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:03.648688: step 5792, loss 0.107957, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:03.784886: step 5793, loss 0.0689342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:03.914829: step 5794, loss 0.163693, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:04.045590: step 5795, loss 0.170449, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:04.262457: step 5796, loss 0.0568326, acc 1, learning_rate 0.0001
2017-10-10T12:06:04.440367: step 5797, loss 0.106612, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:04.543201: step 5798, loss 0.106008, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:04.687812: step 5799, loss 0.120559, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:04.845295: step 5800, loss 0.103065, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:05.129682: step 5800, loss 0.21417, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5800

2017-10-10T12:06:06.348880: step 5801, loss 0.111929, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:06.545202: step 5802, loss 0.113435, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:06.674452: step 5803, loss 0.119673, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:06.868488: step 5804, loss 0.0843829, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:07.059606: step 5805, loss 0.0443649, acc 1, learning_rate 0.0001
2017-10-10T12:06:07.248836: step 5806, loss 0.0866668, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:07.396814: step 5807, loss 0.0801059, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:07.560877: step 5808, loss 0.113943, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:07.759414: step 5809, loss 0.175038, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:07.941881: step 5810, loss 0.19127, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:08.118482: step 5811, loss 0.0810064, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:08.291351: step 5812, loss 0.143092, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:08.501710: step 5813, loss 0.0362463, acc 1, learning_rate 0.0001
2017-10-10T12:06:08.692857: step 5814, loss 0.0837485, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:08.882298: step 5815, loss 0.15466, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:09.067035: step 5816, loss 0.132965, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:09.233133: step 5817, loss 0.108642, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:09.455147: step 5818, loss 0.10611, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:09.584909: step 5819, loss 0.0673488, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:09.779765: step 5820, loss 0.225884, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:09.993824: step 5821, loss 0.0987819, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:10.133174: step 5822, loss 0.187869, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:10.312858: step 5823, loss 0.103598, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:10.516465: step 5824, loss 0.106797, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:10.692966: step 5825, loss 0.17179, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:10.849535: step 5826, loss 0.0429078, acc 1, learning_rate 0.0001
2017-10-10T12:06:11.021100: step 5827, loss 0.096994, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:11.214903: step 5828, loss 0.167002, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:11.361258: step 5829, loss 0.0680502, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:11.577110: step 5830, loss 0.0648345, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:11.734785: step 5831, loss 0.171737, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:11.943441: step 5832, loss 0.164776, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:12.152851: step 5833, loss 0.100303, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:12.305621: step 5834, loss 0.179816, acc 0.90625, learning_rate 0.0001
2017-10-10T12:06:12.468829: step 5835, loss 0.0537528, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:12.674272: step 5836, loss 0.172863, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:12.892817: step 5837, loss 0.0863086, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:13.087085: step 5838, loss 0.0707131, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:13.273231: step 5839, loss 0.0480796, acc 1, learning_rate 0.0001
2017-10-10T12:06:13.441478: step 5840, loss 0.0806773, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:13.820996: step 5840, loss 0.213889, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5840

2017-10-10T12:06:14.704279: step 5841, loss 0.0906818, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:14.865115: step 5842, loss 0.0600472, acc 1, learning_rate 0.0001
2017-10-10T12:06:15.002943: step 5843, loss 0.118031, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:15.100005: step 5844, loss 0.0939478, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:15.196798: step 5845, loss 0.145364, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:15.294060: step 5846, loss 0.0720712, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:15.452862: step 5847, loss 0.0930021, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:15.585952: step 5848, loss 0.0521786, acc 1, learning_rate 0.0001
2017-10-10T12:06:15.724047: step 5849, loss 0.0898723, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:15.927367: step 5850, loss 0.0830194, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:16.121067: step 5851, loss 0.123029, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:16.328558: step 5852, loss 0.164327, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:16.476526: step 5853, loss 0.102216, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:16.671553: step 5854, loss 0.108436, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:16.837169: step 5855, loss 0.106704, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:17.015199: step 5856, loss 0.0400022, acc 1, learning_rate 0.0001
2017-10-10T12:06:17.229433: step 5857, loss 0.116188, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:17.413688: step 5858, loss 0.114503, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:17.612870: step 5859, loss 0.0676988, acc 1, learning_rate 0.0001
2017-10-10T12:06:17.764890: step 5860, loss 0.118802, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:17.981815: step 5861, loss 0.0992076, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:18.144821: step 5862, loss 0.112892, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:18.335442: step 5863, loss 0.199194, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:18.479051: step 5864, loss 0.112841, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:18.662736: step 5865, loss 0.0486748, acc 1, learning_rate 0.0001
2017-10-10T12:06:18.878724: step 5866, loss 0.0885114, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.032803: step 5867, loss 0.0964941, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.192855: step 5868, loss 0.111949, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.405760: step 5869, loss 0.0407085, acc 1, learning_rate 0.0001
2017-10-10T12:06:19.617687: step 5870, loss 0.098328, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.782353: step 5871, loss 0.127565, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.947055: step 5872, loss 0.0903064, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:20.173730: step 5873, loss 0.0934786, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:20.387666: step 5874, loss 0.145704, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:20.552010: step 5875, loss 0.0720131, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:20.704891: step 5876, loss 0.176255, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:20.900385: step 5877, loss 0.0545429, acc 1, learning_rate 0.0001
2017-10-10T12:06:21.120068: step 5878, loss 0.0570805, acc 1, learning_rate 0.0001
2017-10-10T12:06:21.276185: step 5879, loss 0.13886, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:21.452938: step 5880, loss 0.0957541, acc 0.960784, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:21.856964: step 5880, loss 0.213135, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5880

2017-10-10T12:06:23.011620: step 5881, loss 0.0835619, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:23.227338: step 5882, loss 0.0898793, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:23.375691: step 5883, loss 0.0686578, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:23.527866: step 5884, loss 0.156246, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:23.725474: step 5885, loss 0.056806, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:23.946706: step 5886, loss 0.157673, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:24.128968: step 5887, loss 0.131939, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:24.317076: step 5888, loss 0.0917634, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:24.471762: step 5889, loss 0.0872682, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:24.670988: step 5890, loss 0.117512, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:24.884950: step 5891, loss 0.0582389, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:25.064873: step 5892, loss 0.170745, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:25.304874: step 5893, loss 0.0926751, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:25.541088: step 5894, loss 0.0512802, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:25.680784: step 5895, loss 0.0442349, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:25.769871: step 5896, loss 0.115764, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:25.860448: step 5897, loss 0.139624, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:25.956954: step 5898, loss 0.126221, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:26.044555: step 5899, loss 0.120409, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:26.135317: step 5900, loss 0.102483, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:26.222978: step 5901, loss 0.133194, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:26.330728: step 5902, loss 0.0952545, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:26.480946: step 5903, loss 0.0582835, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:26.677461: step 5904, loss 0.167594, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:26.878957: step 5905, loss 0.0895477, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:27.080853: step 5906, loss 0.112778, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:27.228887: step 5907, loss 0.0768541, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:27.401364: step 5908, loss 0.0623726, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:27.584857: step 5909, loss 0.0836719, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:27.797058: step 5910, loss 0.0756158, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:27.969028: step 5911, loss 0.0905932, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:28.136906: step 5912, loss 0.126308, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:28.309734: step 5913, loss 0.102329, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:28.507881: step 5914, loss 0.12221, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:28.629706: step 5915, loss 0.0838361, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:28.813631: step 5916, loss 0.133471, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:28.973044: step 5917, loss 0.0736694, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:29.118313: step 5918, loss 0.117388, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:29.324621: step 5919, loss 0.0861225, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:29.496861: step 5920, loss 0.147734, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:29.906577: step 5920, loss 0.213841, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5920

2017-10-10T12:06:31.188997: step 5921, loss 0.162368, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:31.378385: step 5922, loss 0.110952, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:31.554383: step 5923, loss 0.133122, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:31.665483: step 5924, loss 0.057613, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:31.885841: step 5925, loss 0.113012, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:32.078278: step 5926, loss 0.190075, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:32.284902: step 5927, loss 0.152778, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:32.436890: step 5928, loss 0.118119, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:32.621462: step 5929, loss 0.0971108, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:32.826513: step 5930, loss 0.223728, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:32.965349: step 5931, loss 0.108005, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:33.139729: step 5932, loss 0.143547, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:33.329980: step 5933, loss 0.139544, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:33.495697: step 5934, loss 0.058054, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:33.661037: step 5935, loss 0.168758, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:33.864940: step 5936, loss 0.167363, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:34.078540: step 5937, loss 0.148398, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:34.270019: step 5938, loss 0.164074, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:34.444575: step 5939, loss 0.103236, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:34.634730: step 5940, loss 0.0757847, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:34.840846: step 5941, loss 0.142049, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:35.009232: step 5942, loss 0.0786357, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:35.172658: step 5943, loss 0.270021, acc 0.890625, learning_rate 0.0001
2017-10-10T12:06:35.382854: step 5944, loss 0.0708037, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:35.536715: step 5945, loss 0.121712, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:35.748592: step 5946, loss 0.0899534, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:35.958106: step 5947, loss 0.114973, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:36.092871: step 5948, loss 0.1474, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:36.280897: step 5949, loss 0.117432, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:36.461586: step 5950, loss 0.129817, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:36.554302: step 5951, loss 0.0544656, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:36.643038: step 5952, loss 0.106122, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:36.731150: step 5953, loss 0.133114, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:36.819795: step 5954, loss 0.0790788, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:36.908310: step 5955, loss 0.12351, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:37.040809: step 5956, loss 0.113633, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:37.180904: step 5957, loss 0.15634, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:37.376435: step 5958, loss 0.126083, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:37.554573: step 5959, loss 0.120583, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:37.741247: step 5960, loss 0.0730852, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:38.104395: step 5960, loss 0.212009, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-5960

2017-10-10T12:06:39.252819: step 5961, loss 0.099324, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:39.428905: step 5962, loss 0.0871518, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:39.566523: step 5963, loss 0.165183, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:39.759628: step 5964, loss 0.195883, acc 0.890625, learning_rate 0.0001
2017-10-10T12:06:39.936548: step 5965, loss 0.152595, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:40.064900: step 5966, loss 0.073826, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:40.253609: step 5967, loss 0.135489, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:40.464909: step 5968, loss 0.0710912, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:40.605582: step 5969, loss 0.0578076, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:40.772841: step 5970, loss 0.0695687, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:40.979441: step 5971, loss 0.0684677, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:41.187446: step 5972, loss 0.189965, acc 0.90625, learning_rate 0.0001
2017-10-10T12:06:41.335777: step 5973, loss 0.0717674, acc 1, learning_rate 0.0001
2017-10-10T12:06:41.508834: step 5974, loss 0.0941651, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:41.712866: step 5975, loss 0.0584829, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:41.917413: step 5976, loss 0.153433, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:42.062316: step 5977, loss 0.173043, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:42.223240: step 5978, loss 0.0563036, acc 0.980392, learning_rate 0.0001
2017-10-10T12:06:42.429646: step 5979, loss 0.146712, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:42.548868: step 5980, loss 0.0749023, acc 1, learning_rate 0.0001
2017-10-10T12:06:42.747953: step 5981, loss 0.128471, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:42.943387: step 5982, loss 0.0445182, acc 1, learning_rate 0.0001
2017-10-10T12:06:43.128852: step 5983, loss 0.103092, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:43.319912: step 5984, loss 0.206217, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:43.483872: step 5985, loss 0.118064, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:43.693993: step 5986, loss 0.0888438, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:43.871124: step 5987, loss 0.216985, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:44.003572: step 5988, loss 0.0600549, acc 1, learning_rate 0.0001
2017-10-10T12:06:44.210500: step 5989, loss 0.156381, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:44.396875: step 5990, loss 0.10258, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:44.576838: step 5991, loss 0.0727101, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:44.772859: step 5992, loss 0.0468293, acc 1, learning_rate 0.0001
2017-10-10T12:06:44.964993: step 5993, loss 0.0742501, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:45.112984: step 5994, loss 0.0575092, acc 1, learning_rate 0.0001
2017-10-10T12:06:45.270416: step 5995, loss 0.231413, acc 0.90625, learning_rate 0.0001
2017-10-10T12:06:45.482156: step 5996, loss 0.101638, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:45.657113: step 5997, loss 0.0892571, acc 1, learning_rate 0.0001
2017-10-10T12:06:45.808633: step 5998, loss 0.0879881, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:45.994626: step 5999, loss 0.162428, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:46.176129: step 6000, loss 0.0723248, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:46.610849: step 6000, loss 0.214271, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6000

2017-10-10T12:06:47.615184: step 6001, loss 0.125571, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:47.740126: step 6002, loss 0.104807, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:47.859358: step 6003, loss 0.133844, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:48.001405: step 6004, loss 0.161109, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:48.136831: step 6005, loss 0.160074, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:48.265076: step 6006, loss 0.0475905, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:48.463792: step 6007, loss 0.0729727, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:48.659946: step 6008, loss 0.111505, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:48.794466: step 6009, loss 0.0696045, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:48.963037: step 6010, loss 0.094559, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:49.173501: step 6011, loss 0.147001, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:49.343066: step 6012, loss 0.129455, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:49.470150: step 6013, loss 0.181062, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:49.668771: step 6014, loss 0.0881448, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:49.818260: step 6015, loss 0.0625361, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:49.987113: step 6016, loss 0.11059, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:50.183010: step 6017, loss 0.145053, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:50.374754: step 6018, loss 0.092327, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:50.530876: step 6019, loss 0.136627, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:50.692055: step 6020, loss 0.0828223, acc 1, learning_rate 0.0001
2017-10-10T12:06:50.886674: step 6021, loss 0.0816163, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:51.018821: step 6022, loss 0.0710316, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:51.193526: step 6023, loss 0.226892, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:51.389809: step 6024, loss 0.0373066, acc 1, learning_rate 0.0001
2017-10-10T12:06:51.576116: step 6025, loss 0.0953095, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:51.723752: step 6026, loss 0.0683909, acc 1, learning_rate 0.0001
2017-10-10T12:06:51.942091: step 6027, loss 0.0985987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:52.121666: step 6028, loss 0.172563, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:52.260985: step 6029, loss 0.0612601, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:52.460208: step 6030, loss 0.125803, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:52.675484: step 6031, loss 0.117586, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:52.845213: step 6032, loss 0.136839, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:52.992324: step 6033, loss 0.134441, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:53.224477: step 6034, loss 0.09286, acc 1, learning_rate 0.0001
2017-10-10T12:06:53.402285: step 6035, loss 0.0757784, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:53.555659: step 6036, loss 0.129334, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:53.737035: step 6037, loss 0.0423706, acc 1, learning_rate 0.0001
2017-10-10T12:06:53.915692: step 6038, loss 0.163645, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:54.120864: step 6039, loss 0.211888, acc 0.90625, learning_rate 0.0001
2017-10-10T12:06:54.303748: step 6040, loss 0.119945, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:54.712913: step 6040, loss 0.212615, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6040

2017-10-10T12:06:56.045904: step 6041, loss 0.0707605, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:56.217058: step 6042, loss 0.125055, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:56.368818: step 6043, loss 0.122927, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:56.573533: step 6044, loss 0.151769, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:56.794952: step 6045, loss 0.125934, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:57.000953: step 6046, loss 0.101873, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:57.234173: step 6047, loss 0.0559186, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:57.378249: step 6048, loss 0.213495, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:57.530278: step 6049, loss 0.0788845, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:57.702563: step 6050, loss 0.123336, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:57.852864: step 6051, loss 0.0668404, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:58.028856: step 6052, loss 0.0850181, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:58.222486: step 6053, loss 0.16484, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:58.454123: step 6054, loss 0.195349, acc 0.921875, learning_rate 0.0001
2017-10-10T12:06:58.572418: step 6055, loss 0.131761, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:58.689978: step 6056, loss 0.0703103, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:58.840130: step 6057, loss 0.08542, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:58.995799: step 6058, loss 0.12226, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:59.136834: step 6059, loss 0.0473141, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:59.359249: step 6060, loss 0.0763483, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:59.495477: step 6061, loss 0.108994, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:59.683067: step 6062, loss 0.158773, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:59.900010: step 6063, loss 0.0786705, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:00.128862: step 6064, loss 0.202317, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:00.312876: step 6065, loss 0.262248, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:00.485121: step 6066, loss 0.0613221, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:00.655222: step 6067, loss 0.0584485, acc 1, learning_rate 0.0001
2017-10-10T12:07:00.883940: step 6068, loss 0.0631968, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:01.024399: step 6069, loss 0.0362518, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:01.240883: step 6070, loss 0.18243, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:01.425127: step 6071, loss 0.10455, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:01.600851: step 6072, loss 0.105398, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:01.808988: step 6073, loss 0.10086, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:02.016886: step 6074, loss 0.158319, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:02.212972: step 6075, loss 0.136674, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:02.360831: step 6076, loss 0.113217, acc 0.960784, learning_rate 0.0001
2017-10-10T12:07:02.552943: step 6077, loss 0.150193, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:02.748527: step 6078, loss 0.0799571, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:02.948864: step 6079, loss 0.0818065, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:03.130752: step 6080, loss 0.056241, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:03.474388: step 6080, loss 0.212882, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6080

2017-10-10T12:07:04.498963: step 6081, loss 0.0573485, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:04.672167: step 6082, loss 0.102997, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:04.885210: step 6083, loss 0.0702684, acc 1, learning_rate 0.0001
2017-10-10T12:07:05.076849: step 6084, loss 0.0834593, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:05.216366: step 6085, loss 0.153881, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:05.382577: step 6086, loss 0.0625319, acc 1, learning_rate 0.0001
2017-10-10T12:07:05.560890: step 6087, loss 0.151083, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:05.714222: step 6088, loss 0.0753608, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:05.883964: step 6089, loss 0.0220211, acc 1, learning_rate 0.0001
2017-10-10T12:07:06.044548: step 6090, loss 0.0849123, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:06.210242: step 6091, loss 0.0882391, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:06.373406: step 6092, loss 0.0616075, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:06.526524: step 6093, loss 0.114806, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:06.725188: step 6094, loss 0.105383, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:06.898208: step 6095, loss 0.12672, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:07.056835: step 6096, loss 0.0796012, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:07.244291: step 6097, loss 0.159741, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:07.464847: step 6098, loss 0.135919, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:07.711145: step 6099, loss 0.17839, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:07.833255: step 6100, loss 0.0602345, acc 1, learning_rate 0.0001
2017-10-10T12:07:07.955292: step 6101, loss 0.0679141, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:08.103910: step 6102, loss 0.109546, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:08.235696: step 6103, loss 0.12581, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:08.345064: step 6104, loss 0.0571638, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:08.501536: step 6105, loss 0.106433, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:08.637509: step 6106, loss 0.113641, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:08.752875: step 6107, loss 0.223784, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:08.940846: step 6108, loss 0.0966315, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:09.159591: step 6109, loss 0.169469, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:09.310138: step 6110, loss 0.111095, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:09.442658: step 6111, loss 0.143878, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:09.588838: step 6112, loss 0.139405, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:09.724846: step 6113, loss 0.0564427, acc 1, learning_rate 0.0001
2017-10-10T12:07:09.844839: step 6114, loss 0.172446, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:09.983214: step 6115, loss 0.202672, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:10.133194: step 6116, loss 0.121508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:10.300312: step 6117, loss 0.0541976, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:10.480996: step 6118, loss 0.114941, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:10.658271: step 6119, loss 0.157681, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:10.842905: step 6120, loss 0.0949784, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:11.215555: step 6120, loss 0.212965, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6120

2017-10-10T12:07:12.341988: step 6121, loss 0.161075, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:12.508411: step 6122, loss 0.0644964, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:12.680924: step 6123, loss 0.0829073, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:12.893055: step 6124, loss 0.107198, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:13.096818: step 6125, loss 0.0980155, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:13.312894: step 6126, loss 0.128078, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:13.510207: step 6127, loss 0.121637, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:13.654978: step 6128, loss 0.0329079, acc 1, learning_rate 0.0001
2017-10-10T12:07:13.837607: step 6129, loss 0.06395, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:14.036949: step 6130, loss 0.130246, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:14.221425: step 6131, loss 0.0810491, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:14.400862: step 6132, loss 0.211964, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:14.608052: step 6133, loss 0.11102, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:14.805802: step 6134, loss 0.146751, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:14.923956: step 6135, loss 0.146247, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:15.108455: step 6136, loss 0.0923298, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:15.311117: step 6137, loss 0.159375, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:15.441221: step 6138, loss 0.0599966, acc 1, learning_rate 0.0001
2017-10-10T12:07:15.615034: step 6139, loss 0.159893, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:15.808878: step 6140, loss 0.163147, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:15.998063: step 6141, loss 0.0650116, acc 1, learning_rate 0.0001
2017-10-10T12:07:16.119711: step 6142, loss 0.100797, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:16.309145: step 6143, loss 0.0658808, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:16.533176: step 6144, loss 0.137291, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:16.729020: step 6145, loss 0.0837644, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:16.881432: step 6146, loss 0.196936, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:17.060498: step 6147, loss 0.128799, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:17.244149: step 6148, loss 0.0941718, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:17.425301: step 6149, loss 0.0915754, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:17.584059: step 6150, loss 0.0484916, acc 1, learning_rate 0.0001
2017-10-10T12:07:17.805057: step 6151, loss 0.0714879, acc 1, learning_rate 0.0001
2017-10-10T12:07:17.987918: step 6152, loss 0.130298, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:18.260929: step 6153, loss 0.0586323, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:18.479021: step 6154, loss 0.0689061, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:18.616809: step 6155, loss 0.111721, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:18.744863: step 6156, loss 0.162498, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:18.895469: step 6157, loss 0.105174, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:19.051177: step 6158, loss 0.094093, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:19.206109: step 6159, loss 0.0997171, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:19.334518: step 6160, loss 0.0983472, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:19.632880: step 6160, loss 0.213784, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6160

2017-10-10T12:07:20.722049: step 6161, loss 0.129761, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:20.864134: step 6162, loss 0.0527649, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:21.033470: step 6163, loss 0.0274041, acc 1, learning_rate 0.0001
2017-10-10T12:07:21.231876: step 6164, loss 0.0782973, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:21.419769: step 6165, loss 0.116951, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:21.567804: step 6166, loss 0.130845, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:21.756806: step 6167, loss 0.0784844, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:21.957129: step 6168, loss 0.0394844, acc 1, learning_rate 0.0001
2017-10-10T12:07:22.168870: step 6169, loss 0.134866, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:22.285799: step 6170, loss 0.089022, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:22.495484: step 6171, loss 0.184196, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:22.671546: step 6172, loss 0.050915, acc 1, learning_rate 0.0001
2017-10-10T12:07:22.799135: step 6173, loss 0.0724837, acc 1, learning_rate 0.0001
2017-10-10T12:07:22.989513: step 6174, loss 0.0910199, acc 0.960784, learning_rate 0.0001
2017-10-10T12:07:23.176184: step 6175, loss 0.103904, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:23.331111: step 6176, loss 0.0830853, acc 1, learning_rate 0.0001
2017-10-10T12:07:23.508822: step 6177, loss 0.0609615, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:23.716279: step 6178, loss 0.11048, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:23.926847: step 6179, loss 0.104398, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:24.085186: step 6180, loss 0.129602, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:24.234534: step 6181, loss 0.0606735, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:24.439559: step 6182, loss 0.076326, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:24.581061: step 6183, loss 0.141439, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:24.743641: step 6184, loss 0.140496, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:24.954892: step 6185, loss 0.0952268, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:25.119983: step 6186, loss 0.0770198, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:25.305101: step 6187, loss 0.130383, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:25.485398: step 6188, loss 0.17553, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:25.659239: step 6189, loss 0.0929597, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:25.833835: step 6190, loss 0.107682, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:26.009415: step 6191, loss 0.0708284, acc 1, learning_rate 0.0001
2017-10-10T12:07:26.196152: step 6192, loss 0.03279, acc 1, learning_rate 0.0001
2017-10-10T12:07:26.416206: step 6193, loss 0.118367, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:26.609756: step 6194, loss 0.141322, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:26.751457: step 6195, loss 0.149099, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:26.928467: step 6196, loss 0.172653, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:27.149578: step 6197, loss 0.12486, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:27.354229: step 6198, loss 0.127668, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:27.493203: step 6199, loss 0.0616763, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:27.667879: step 6200, loss 0.0829274, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:27.982649: step 6200, loss 0.213813, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6200

2017-10-10T12:07:29.272045: step 6201, loss 0.0684845, acc 1, learning_rate 0.0001
2017-10-10T12:07:29.385405: step 6202, loss 0.0855496, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:29.524761: step 6203, loss 0.157975, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:29.650519: step 6204, loss 0.129606, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:29.819077: step 6205, loss 0.0525747, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:29.991374: step 6206, loss 0.117194, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:30.175705: step 6207, loss 0.0703366, acc 1, learning_rate 0.0001
2017-10-10T12:07:30.385200: step 6208, loss 0.137274, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:30.568907: step 6209, loss 0.0696638, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:30.831612: step 6210, loss 0.0786028, acc 1, learning_rate 0.0001
2017-10-10T12:07:31.012270: step 6211, loss 0.117609, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:31.167989: step 6212, loss 0.0924519, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:31.307383: step 6213, loss 0.113576, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:31.445663: step 6214, loss 0.0607496, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:31.564926: step 6215, loss 0.0948849, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:31.676870: step 6216, loss 0.0341696, acc 1, learning_rate 0.0001
2017-10-10T12:07:31.828770: step 6217, loss 0.115203, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:31.990191: step 6218, loss 0.087826, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:32.146329: step 6219, loss 0.135156, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:32.316414: step 6220, loss 0.0767846, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:32.486395: step 6221, loss 0.151987, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:32.676052: step 6222, loss 0.0751004, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:32.823583: step 6223, loss 0.0833632, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:32.999213: step 6224, loss 0.13265, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:33.197967: step 6225, loss 0.130127, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:33.384418: step 6226, loss 0.0793991, acc 1, learning_rate 0.0001
2017-10-10T12:07:33.524040: step 6227, loss 0.127461, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:33.712028: step 6228, loss 0.111558, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:33.917204: step 6229, loss 0.19023, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:34.041933: step 6230, loss 0.0720518, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:34.244279: step 6231, loss 0.171168, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:34.437177: step 6232, loss 0.108305, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:34.603237: step 6233, loss 0.066932, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:34.805523: step 6234, loss 0.0615077, acc 1, learning_rate 0.0001
2017-10-10T12:07:34.978017: step 6235, loss 0.0890167, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:35.156589: step 6236, loss 0.141143, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:35.300967: step 6237, loss 0.0495493, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:35.485645: step 6238, loss 0.160912, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:35.688778: step 6239, loss 0.0424286, acc 1, learning_rate 0.0001
2017-10-10T12:07:35.903891: step 6240, loss 0.165149, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:36.281592: step 6240, loss 0.212414, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6240

2017-10-10T12:07:37.289123: step 6241, loss 0.0672426, acc 1, learning_rate 0.0001
2017-10-10T12:07:37.472846: step 6242, loss 0.107379, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:37.619515: step 6243, loss 0.0697494, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:37.775366: step 6244, loss 0.125017, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:37.974400: step 6245, loss 0.122776, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:38.128281: step 6246, loss 0.0877983, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:38.282656: step 6247, loss 0.0798175, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:38.476038: step 6248, loss 0.137489, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:38.669741: step 6249, loss 0.116756, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:38.845051: step 6250, loss 0.111529, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:39.080866: step 6251, loss 0.0944673, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:39.277207: step 6252, loss 0.0842566, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:39.416835: step 6253, loss 0.148858, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:39.576868: step 6254, loss 0.103016, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:39.712160: step 6255, loss 0.105723, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:39.857181: step 6256, loss 0.154447, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:40.004402: step 6257, loss 0.112207, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:40.145978: step 6258, loss 0.0675972, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:40.333977: step 6259, loss 0.0748234, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:40.534064: step 6260, loss 0.0772976, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:40.705697: step 6261, loss 0.157329, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:40.859705: step 6262, loss 0.181069, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:41.021683: step 6263, loss 0.108284, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:41.196861: step 6264, loss 0.122506, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:41.338982: step 6265, loss 0.140554, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:41.536729: step 6266, loss 0.0559702, acc 1, learning_rate 0.0001
2017-10-10T12:07:41.763220: step 6267, loss 0.0803092, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:41.944867: step 6268, loss 0.102286, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:42.213013: step 6269, loss 0.112141, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:42.397754: step 6270, loss 0.151339, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:42.535462: step 6271, loss 0.0895314, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:42.676861: step 6272, loss 0.0899526, acc 1, learning_rate 0.0001
2017-10-10T12:07:42.838742: step 6273, loss 0.0842584, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:42.957512: step 6274, loss 0.0969116, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:43.100229: step 6275, loss 0.130646, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:43.235249: step 6276, loss 0.0960987, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:43.418046: step 6277, loss 0.133235, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:43.565069: step 6278, loss 0.121242, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:43.749028: step 6279, loss 0.0730508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:43.932293: step 6280, loss 0.132019, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:44.303024: step 6280, loss 0.212894, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6280

2017-10-10T12:07:45.466240: step 6281, loss 0.0790018, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:45.623194: step 6282, loss 0.0636497, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:45.785108: step 6283, loss 0.139409, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:45.971064: step 6284, loss 0.0525783, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:46.137134: step 6285, loss 0.141947, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:46.331281: step 6286, loss 0.0729324, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:46.517352: step 6287, loss 0.124314, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:46.739440: step 6288, loss 0.0970428, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:46.945897: step 6289, loss 0.132588, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:47.072831: step 6290, loss 0.0796506, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:47.272357: step 6291, loss 0.133662, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:47.448876: step 6292, loss 0.0469917, acc 1, learning_rate 0.0001
2017-10-10T12:07:47.600970: step 6293, loss 0.0727906, acc 1, learning_rate 0.0001
2017-10-10T12:07:47.779470: step 6294, loss 0.220023, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:47.980830: step 6295, loss 0.0437273, acc 1, learning_rate 0.0001
2017-10-10T12:07:48.125196: step 6296, loss 0.091262, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:48.295387: step 6297, loss 0.192495, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:48.487326: step 6298, loss 0.210245, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:48.665055: step 6299, loss 0.117687, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:48.836878: step 6300, loss 0.102393, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:49.026599: step 6301, loss 0.149506, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:49.204829: step 6302, loss 0.039369, acc 1, learning_rate 0.0001
2017-10-10T12:07:49.384231: step 6303, loss 0.122798, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:49.584885: step 6304, loss 0.0801777, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:49.792911: step 6305, loss 0.0508625, acc 1, learning_rate 0.0001
2017-10-10T12:07:49.988894: step 6306, loss 0.23151, acc 0.90625, learning_rate 0.0001
2017-10-10T12:07:50.135853: step 6307, loss 0.0714416, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:50.253235: step 6308, loss 0.102219, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:50.417799: step 6309, loss 0.0915624, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:50.544445: step 6310, loss 0.0773169, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:50.685013: step 6311, loss 0.134577, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:50.816941: step 6312, loss 0.101698, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:50.982374: step 6313, loss 0.0379897, acc 1, learning_rate 0.0001
2017-10-10T12:07:51.173836: step 6314, loss 0.0698832, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:51.320864: step 6315, loss 0.135446, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:51.485006: step 6316, loss 0.103646, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:51.701911: step 6317, loss 0.0603636, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:51.841308: step 6318, loss 0.0950237, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:51.994485: step 6319, loss 0.0755509, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:52.179172: step 6320, loss 0.0917999, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:52.532834: step 6320, loss 0.211105, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6320

2017-10-10T12:07:53.656834: step 6321, loss 0.116326, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:53.797657: step 6322, loss 0.0897839, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:53.944198: step 6323, loss 0.0970148, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:54.064847: step 6324, loss 0.0887074, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:54.312854: step 6325, loss 0.0719736, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:54.532258: step 6326, loss 0.0908753, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:54.736790: step 6327, loss 0.137022, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:54.909511: step 6328, loss 0.0823752, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:55.056953: step 6329, loss 0.0522418, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:55.250102: step 6330, loss 0.0593939, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:55.391861: step 6331, loss 0.130325, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:55.548844: step 6332, loss 0.0791267, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:55.747200: step 6333, loss 0.144156, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:55.942043: step 6334, loss 0.127343, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:56.077067: step 6335, loss 0.16558, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:56.272208: step 6336, loss 0.0752984, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:56.472953: step 6337, loss 0.0633514, acc 1, learning_rate 0.0001
2017-10-10T12:07:56.676847: step 6338, loss 0.0769827, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:56.849072: step 6339, loss 0.0869977, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:57.045918: step 6340, loss 0.082936, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:57.244835: step 6341, loss 0.0589888, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:57.473860: step 6342, loss 0.0924992, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:57.675997: step 6343, loss 0.126397, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:57.872873: step 6344, loss 0.128654, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:58.046541: step 6345, loss 0.0512317, acc 1, learning_rate 0.0001
2017-10-10T12:07:58.208969: step 6346, loss 0.108346, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:58.408166: step 6347, loss 0.0807244, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:58.588860: step 6348, loss 0.0804781, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:58.747368: step 6349, loss 0.0589638, acc 1, learning_rate 0.0001
2017-10-10T12:07:58.914510: step 6350, loss 0.0842716, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:59.128964: step 6351, loss 0.126415, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:59.332386: step 6352, loss 0.127508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:59.448962: step 6353, loss 0.0985259, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:59.640874: step 6354, loss 0.143468, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:59.840392: step 6355, loss 0.10267, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:00.081084: step 6356, loss 0.180138, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:00.297137: step 6357, loss 0.117883, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:00.455444: step 6358, loss 0.149622, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:00.593322: step 6359, loss 0.0716329, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:00.751637: step 6360, loss 0.157877, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:01.051256: step 6360, loss 0.212338, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6360

2017-10-10T12:08:02.062481: step 6361, loss 0.139132, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:02.252841: step 6362, loss 0.0273562, acc 1, learning_rate 0.0001
2017-10-10T12:08:02.421081: step 6363, loss 0.124879, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:02.551698: step 6364, loss 0.120302, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:02.769327: step 6365, loss 0.0773181, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:02.952826: step 6366, loss 0.0422678, acc 1, learning_rate 0.0001
2017-10-10T12:08:03.130909: step 6367, loss 0.0894477, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:03.338625: step 6368, loss 0.0730807, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:03.510763: step 6369, loss 0.104399, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:03.684849: step 6370, loss 0.0277792, acc 1, learning_rate 0.0001
2017-10-10T12:08:03.924836: step 6371, loss 0.0934155, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:04.152966: step 6372, loss 0.10934, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:04.300289: step 6373, loss 0.0432688, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:04.459634: step 6374, loss 0.115386, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:04.588799: step 6375, loss 0.0705878, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:04.709491: step 6376, loss 0.0844509, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:04.855281: step 6377, loss 0.12506, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:05.006347: step 6378, loss 0.0930925, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:05.172409: step 6379, loss 0.182775, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:05.340726: step 6380, loss 0.18738, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:05.536942: step 6381, loss 0.120055, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:05.728501: step 6382, loss 0.100612, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:05.912389: step 6383, loss 0.131367, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:06.061963: step 6384, loss 0.0541169, acc 1, learning_rate 0.0001
2017-10-10T12:08:06.242146: step 6385, loss 0.15803, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:06.430080: step 6386, loss 0.102263, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:06.609188: step 6387, loss 0.114239, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:06.757491: step 6388, loss 0.0781122, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:06.959137: step 6389, loss 0.154259, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:07.176687: step 6390, loss 0.0652225, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:07.380820: step 6391, loss 0.0828839, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:07.536844: step 6392, loss 0.110247, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:07.729686: step 6393, loss 0.0896815, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:07.936855: step 6394, loss 0.0892386, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.089308: step 6395, loss 0.0823467, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:08.252205: step 6396, loss 0.174261, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:08.464851: step 6397, loss 0.0781886, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.601023: step 6398, loss 0.0637969, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.760285: step 6399, loss 0.0777394, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.952679: step 6400, loss 0.104533, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:09.313358: step 6400, loss 0.212177, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6400

2017-10-10T12:08:10.472907: step 6401, loss 0.123335, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:10.641846: step 6402, loss 0.0848229, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:10.799409: step 6403, loss 0.134829, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:10.970340: step 6404, loss 0.201126, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:11.109297: step 6405, loss 0.0733993, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:11.241737: step 6406, loss 0.0517729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:11.368241: step 6407, loss 0.251277, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:11.559679: step 6408, loss 0.0993545, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:11.696546: step 6409, loss 0.0735487, acc 1, learning_rate 0.0001
2017-10-10T12:08:11.893244: step 6410, loss 0.106212, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:12.073271: step 6411, loss 0.175286, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:12.224995: step 6412, loss 0.0649335, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:12.418940: step 6413, loss 0.121213, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:12.586849: step 6414, loss 0.0557005, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:12.755139: step 6415, loss 0.133876, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:12.970173: step 6416, loss 0.1051, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:13.137132: step 6417, loss 0.0538742, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:13.306324: step 6418, loss 0.103161, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:13.478553: step 6419, loss 0.0604178, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:13.686937: step 6420, loss 0.156492, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:13.872855: step 6421, loss 0.126915, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:14.009063: step 6422, loss 0.0817449, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:14.213485: step 6423, loss 0.108981, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:14.431539: step 6424, loss 0.0463114, acc 1, learning_rate 0.0001
2017-10-10T12:08:14.622721: step 6425, loss 0.190139, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:14.768799: step 6426, loss 0.109508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:15.002143: step 6427, loss 0.0711182, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:15.223669: step 6428, loss 0.0579202, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:15.421464: step 6429, loss 0.0279621, acc 1, learning_rate 0.0001
2017-10-10T12:08:15.547749: step 6430, loss 0.0599457, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:15.659122: step 6431, loss 0.216263, acc 0.890625, learning_rate 0.0001
2017-10-10T12:08:15.805472: step 6432, loss 0.0380998, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:15.959121: step 6433, loss 0.0893913, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:16.108299: step 6434, loss 0.0526319, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:16.298307: step 6435, loss 0.244831, acc 0.90625, learning_rate 0.0001
2017-10-10T12:08:16.455118: step 6436, loss 0.12166, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:16.614829: step 6437, loss 0.11377, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:16.829271: step 6438, loss 0.0452968, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:17.028983: step 6439, loss 0.139709, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:17.201341: step 6440, loss 0.0945319, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:17.596409: step 6440, loss 0.211113, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6440

2017-10-10T12:08:18.875937: step 6441, loss 0.0765722, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:19.055884: step 6442, loss 0.0789785, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:19.271146: step 6443, loss 0.099721, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:19.449415: step 6444, loss 0.108396, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:19.617188: step 6445, loss 0.109208, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:19.774216: step 6446, loss 0.086343, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:19.977565: step 6447, loss 0.0789971, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:20.114373: step 6448, loss 0.0379629, acc 1, learning_rate 0.0001
2017-10-10T12:08:20.277359: step 6449, loss 0.173162, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:20.501605: step 6450, loss 0.065419, acc 1, learning_rate 0.0001
2017-10-10T12:08:20.772866: step 6451, loss 0.0708023, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:20.900550: step 6452, loss 0.0870019, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:21.031276: step 6453, loss 0.0613077, acc 1, learning_rate 0.0001
2017-10-10T12:08:21.196572: step 6454, loss 0.0682079, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:21.358676: step 6455, loss 0.136329, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:21.516822: step 6456, loss 0.0861413, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:21.656822: step 6457, loss 0.087378, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:21.780998: step 6458, loss 0.0298648, acc 1, learning_rate 0.0001
2017-10-10T12:08:21.977787: step 6459, loss 0.106671, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:22.168283: step 6460, loss 0.0883143, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:22.349219: step 6461, loss 0.206147, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:22.544926: step 6462, loss 0.0982141, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:22.689007: step 6463, loss 0.131685, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:22.884183: step 6464, loss 0.0972574, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:23.045849: step 6465, loss 0.0915425, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:23.186192: step 6466, loss 0.050905, acc 1, learning_rate 0.0001
2017-10-10T12:08:23.380936: step 6467, loss 0.133287, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:23.505330: step 6468, loss 0.0922835, acc 0.980392, learning_rate 0.0001
2017-10-10T12:08:23.681425: step 6469, loss 0.0796127, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:23.892378: step 6470, loss 0.195007, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:24.024975: step 6471, loss 0.0579429, acc 1, learning_rate 0.0001
2017-10-10T12:08:24.181813: step 6472, loss 0.146142, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:24.391521: step 6473, loss 0.0721146, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:24.545618: step 6474, loss 0.151017, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:24.704835: step 6475, loss 0.120775, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:24.880082: step 6476, loss 0.189881, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:25.073194: step 6477, loss 0.140756, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:25.300049: step 6478, loss 0.151688, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:25.485278: step 6479, loss 0.0991046, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:25.656853: step 6480, loss 0.110349, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:26.102520: step 6480, loss 0.211989, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6480

2017-10-10T12:08:27.143238: step 6481, loss 0.148925, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:27.332947: step 6482, loss 0.136412, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:27.451455: step 6483, loss 0.117528, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:27.649780: step 6484, loss 0.0868352, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:27.877591: step 6485, loss 0.0503946, acc 1, learning_rate 0.0001
2017-10-10T12:08:28.039754: step 6486, loss 0.0831374, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:28.243594: step 6487, loss 0.0491074, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:28.393032: step 6488, loss 0.0914633, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:28.616824: step 6489, loss 0.144101, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:28.784861: step 6490, loss 0.142818, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:28.924835: step 6491, loss 0.103473, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:29.131765: step 6492, loss 0.11185, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:29.285008: step 6493, loss 0.197728, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:29.452943: step 6494, loss 0.107208, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:29.616811: step 6495, loss 0.0888223, acc 1, learning_rate 0.0001
2017-10-10T12:08:29.804906: step 6496, loss 0.189496, acc 0.90625, learning_rate 0.0001
2017-10-10T12:08:29.971883: step 6497, loss 0.0806938, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:30.162809: step 6498, loss 0.0568538, acc 1, learning_rate 0.0001
2017-10-10T12:08:30.290355: step 6499, loss 0.182697, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:30.478530: step 6500, loss 0.0495025, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:30.709129: step 6501, loss 0.0415482, acc 1, learning_rate 0.0001
2017-10-10T12:08:30.900875: step 6502, loss 0.0929805, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:31.150392: step 6503, loss 0.0911615, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:31.319309: step 6504, loss 0.152789, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:31.484356: step 6505, loss 0.151947, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:31.659584: step 6506, loss 0.110405, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:31.808817: step 6507, loss 0.155425, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:31.936826: step 6508, loss 0.0922805, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:32.058749: step 6509, loss 0.192609, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:32.275790: step 6510, loss 0.0789062, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:32.492047: step 6511, loss 0.148121, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:32.652131: step 6512, loss 0.0984073, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:32.821163: step 6513, loss 0.121348, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:33.004402: step 6514, loss 0.115644, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:33.218840: step 6515, loss 0.156581, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:33.413931: step 6516, loss 0.101947, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:33.562565: step 6517, loss 0.142796, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:33.729590: step 6518, loss 0.147144, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:33.936205: step 6519, loss 0.054082, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:34.124933: step 6520, loss 0.0980786, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:34.521098: step 6520, loss 0.210643, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6520

2017-10-10T12:08:35.737486: step 6521, loss 0.115307, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:35.889455: step 6522, loss 0.141936, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:36.050395: step 6523, loss 0.10011, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:36.243345: step 6524, loss 0.0922185, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:36.422222: step 6525, loss 0.170473, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:36.562386: step 6526, loss 0.113505, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:36.737572: step 6527, loss 0.153024, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:36.945670: step 6528, loss 0.0531932, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:37.168958: step 6529, loss 0.097644, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:37.369089: step 6530, loss 0.0673373, acc 1, learning_rate 0.0001
2017-10-10T12:08:37.511421: step 6531, loss 0.101218, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:37.673580: step 6532, loss 0.127066, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:37.831693: step 6533, loss 0.0879319, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:37.976861: step 6534, loss 0.13651, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:38.112837: step 6535, loss 0.0875543, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:38.253400: step 6536, loss 0.0641885, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:38.368838: step 6537, loss 0.0825723, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:38.579462: step 6538, loss 0.0586015, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:38.776834: step 6539, loss 0.0750164, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:38.960855: step 6540, loss 0.121971, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:39.096839: step 6541, loss 0.0463103, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:39.299575: step 6542, loss 0.0325668, acc 1, learning_rate 0.0001
2017-10-10T12:08:39.496911: step 6543, loss 0.0948209, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:39.677019: step 6544, loss 0.0761945, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:39.802671: step 6545, loss 0.108605, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:40.008045: step 6546, loss 0.117372, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:40.212309: step 6547, loss 0.0777488, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:40.358322: step 6548, loss 0.0532216, acc 1, learning_rate 0.0001
2017-10-10T12:08:40.517525: step 6549, loss 0.165251, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:40.699437: step 6550, loss 0.17866, acc 0.90625, learning_rate 0.0001
2017-10-10T12:08:40.899795: step 6551, loss 0.13573, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:41.074321: step 6552, loss 0.157442, acc 0.90625, learning_rate 0.0001
2017-10-10T12:08:41.296863: step 6553, loss 0.141727, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:41.494106: step 6554, loss 0.056685, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:41.672763: step 6555, loss 0.0925534, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:41.814287: step 6556, loss 0.0562732, acc 1, learning_rate 0.0001
2017-10-10T12:08:41.972186: step 6557, loss 0.0504936, acc 1, learning_rate 0.0001
2017-10-10T12:08:42.087575: step 6558, loss 0.060374, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:42.215803: step 6559, loss 0.0295617, acc 1, learning_rate 0.0001
2017-10-10T12:08:42.382420: step 6560, loss 0.0349803, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:42.767993: step 6560, loss 0.212354, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6560

2017-10-10T12:08:43.904830: step 6561, loss 0.0987158, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:44.073011: step 6562, loss 0.135282, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:44.226850: step 6563, loss 0.0637807, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:44.427529: step 6564, loss 0.119253, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:44.596848: step 6565, loss 0.0690739, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:44.748889: step 6566, loss 0.0722913, acc 0.980392, learning_rate 0.0001
2017-10-10T12:08:44.929168: step 6567, loss 0.105529, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:45.125243: step 6568, loss 0.0994746, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:45.299447: step 6569, loss 0.0675515, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:45.476143: step 6570, loss 0.104276, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:45.693060: step 6571, loss 0.139963, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:45.899943: step 6572, loss 0.0725326, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:46.021548: step 6573, loss 0.105796, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:46.212923: step 6574, loss 0.0607929, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:46.432346: step 6575, loss 0.0246294, acc 1, learning_rate 0.0001
2017-10-10T12:08:46.612934: step 6576, loss 0.0954487, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:46.739566: step 6577, loss 0.165194, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:46.935080: step 6578, loss 0.155231, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:47.141516: step 6579, loss 0.0871416, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:47.289606: step 6580, loss 0.101626, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:47.445350: step 6581, loss 0.0420282, acc 1, learning_rate 0.0001
2017-10-10T12:08:47.661450: step 6582, loss 0.0997897, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:47.836864: step 6583, loss 0.100811, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:47.988079: step 6584, loss 0.161706, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:48.268878: step 6585, loss 0.0427085, acc 1, learning_rate 0.0001
2017-10-10T12:08:48.457108: step 6586, loss 0.0796254, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:48.658562: step 6587, loss 0.105385, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:48.816364: step 6588, loss 0.0997883, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:48.972670: step 6589, loss 0.0841666, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:49.104781: step 6590, loss 0.128907, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:49.230156: step 6591, loss 0.117679, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:49.360891: step 6592, loss 0.0841555, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:49.548840: step 6593, loss 0.0311391, acc 1, learning_rate 0.0001
2017-10-10T12:08:49.731111: step 6594, loss 0.120109, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:49.877047: step 6595, loss 0.119356, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:50.065080: step 6596, loss 0.0568146, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:50.254761: step 6597, loss 0.138204, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:50.439849: step 6598, loss 0.222674, acc 0.921875, learning_rate 0.0001
2017-10-10T12:08:50.604963: step 6599, loss 0.163039, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:50.744818: step 6600, loss 0.167671, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:51.191226: step 6600, loss 0.213763, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6600

2017-10-10T12:08:52.326300: step 6601, loss 0.0775496, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:52.462045: step 6602, loss 0.118217, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:52.613475: step 6603, loss 0.0358359, acc 1, learning_rate 0.0001
2017-10-10T12:08:52.749509: step 6604, loss 0.065019, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:52.880865: step 6605, loss 0.157512, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:53.044861: step 6606, loss 0.10642, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:53.241721: step 6607, loss 0.029614, acc 1, learning_rate 0.0001
2017-10-10T12:08:53.741470: step 6608, loss 0.0854076, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:53.933660: step 6609, loss 0.132393, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:54.096921: step 6610, loss 0.130832, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:54.280904: step 6611, loss 0.112735, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:54.468477: step 6612, loss 0.098208, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:54.645013: step 6613, loss 0.172013, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:54.870797: step 6614, loss 0.0721825, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:55.080846: step 6615, loss 0.107638, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:55.251425: step 6616, loss 0.119146, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:55.400371: step 6617, loss 0.0596229, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:55.580798: step 6618, loss 0.0524539, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:55.797021: step 6619, loss 0.130551, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:55.971442: step 6620, loss 0.0483167, acc 1, learning_rate 0.0001
2017-10-10T12:08:56.120823: step 6621, loss 0.0812889, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:56.324669: step 6622, loss 0.0850988, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:56.520839: step 6623, loss 0.0751024, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:56.691422: step 6624, loss 0.0958148, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:56.882398: step 6625, loss 0.111668, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:57.071041: step 6626, loss 0.125759, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:57.268834: step 6627, loss 0.0756884, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:57.426774: step 6628, loss 0.125438, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:57.597258: step 6629, loss 0.0578847, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:57.788672: step 6630, loss 0.067832, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:57.940900: step 6631, loss 0.157705, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:58.120384: step 6632, loss 0.143699, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:58.314721: step 6633, loss 0.109656, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:58.484852: step 6634, loss 0.168588, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:58.624201: step 6635, loss 0.140442, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:58.836633: step 6636, loss 0.147473, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:59.032202: step 6637, loss 0.0598219, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:59.184897: step 6638, loss 0.115329, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:59.388785: step 6639, loss 0.0658512, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:59.599096: step 6640, loss 0.0272352, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:00.034373: step 6640, loss 0.21211, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6640

2017-10-10T12:09:00.910678: step 6641, loss 0.116277, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:01.104867: step 6642, loss 0.136005, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:01.248106: step 6643, loss 0.100348, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:01.421056: step 6644, loss 0.0670754, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:01.668929: step 6645, loss 0.0618877, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:01.894823: step 6646, loss 0.111554, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:02.031016: step 6647, loss 0.233085, acc 0.90625, learning_rate 0.0001
2017-10-10T12:09:02.189243: step 6648, loss 0.0652275, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:02.352826: step 6649, loss 0.190123, acc 0.921875, learning_rate 0.0001
2017-10-10T12:09:02.508814: step 6650, loss 0.0806914, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:02.624004: step 6651, loss 0.134728, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:02.772820: step 6652, loss 0.0625695, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:02.962041: step 6653, loss 0.106664, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:03.114419: step 6654, loss 0.0768005, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:03.277021: step 6655, loss 0.0758629, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:03.470106: step 6656, loss 0.115052, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:03.689128: step 6657, loss 0.0674372, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:03.886561: step 6658, loss 0.0658045, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:04.033079: step 6659, loss 0.109408, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:04.207351: step 6660, loss 0.0443761, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:04.414455: step 6661, loss 0.0730894, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:04.620871: step 6662, loss 0.0240035, acc 1, learning_rate 0.0001
2017-10-10T12:09:04.773076: step 6663, loss 0.0811238, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:04.930511: step 6664, loss 0.106098, acc 0.980392, learning_rate 0.0001
2017-10-10T12:09:05.135132: step 6665, loss 0.119698, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:05.344113: step 6666, loss 0.122088, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:05.505192: step 6667, loss 0.0676992, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:05.670196: step 6668, loss 0.095579, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:05.881498: step 6669, loss 0.031293, acc 1, learning_rate 0.0001
2017-10-10T12:09:06.079434: step 6670, loss 0.1022, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:06.264945: step 6671, loss 0.137521, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:06.405029: step 6672, loss 0.114962, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:06.620183: step 6673, loss 0.053216, acc 1, learning_rate 0.0001
2017-10-10T12:09:06.832794: step 6674, loss 0.129776, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:07.007326: step 6675, loss 0.150403, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:07.149815: step 6676, loss 0.0620107, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:07.336934: step 6677, loss 0.0668818, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:07.518992: step 6678, loss 0.0806438, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:07.712970: step 6679, loss 0.0576724, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:07.862019: step 6680, loss 0.13779, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:08.260880: step 6680, loss 0.211382, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6680

2017-10-10T12:09:09.375210: step 6681, loss 0.16599, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:09.518222: step 6682, loss 0.147948, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:09.701454: step 6683, loss 0.101941, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:09.886083: step 6684, loss 0.121393, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:10.064416: step 6685, loss 0.0867028, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:10.220841: step 6686, loss 0.1509, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:10.436891: step 6687, loss 0.138946, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:10.692590: step 6688, loss 0.119654, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:10.897487: step 6689, loss 0.0803429, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:11.036060: step 6690, loss 0.0586888, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:11.172327: step 6691, loss 0.132527, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:11.282967: step 6692, loss 0.149001, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:11.423459: step 6693, loss 0.0985513, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:11.572801: step 6694, loss 0.0576515, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:11.678450: step 6695, loss 0.12086, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:11.807921: step 6696, loss 0.128904, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:11.991767: step 6697, loss 0.139283, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:12.225976: step 6698, loss 0.0795482, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:12.415971: step 6699, loss 0.0725993, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:12.583681: step 6700, loss 0.0998883, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:12.727149: step 6701, loss 0.159649, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:12.845412: step 6702, loss 0.0834329, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:12.996249: step 6703, loss 0.169265, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:13.165669: step 6704, loss 0.164387, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:13.320898: step 6705, loss 0.0708412, acc 1, learning_rate 0.0001
2017-10-10T12:09:13.520834: step 6706, loss 0.0914454, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:13.704950: step 6707, loss 0.0751646, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:13.905840: step 6708, loss 0.0821679, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:14.116840: step 6709, loss 0.144816, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:14.250965: step 6710, loss 0.0614762, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:14.426510: step 6711, loss 0.0585567, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:14.621677: step 6712, loss 0.0523161, acc 1, learning_rate 0.0001
2017-10-10T12:09:14.834013: step 6713, loss 0.0507872, acc 1, learning_rate 0.0001
2017-10-10T12:09:15.044841: step 6714, loss 0.136075, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:15.206563: step 6715, loss 0.132623, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:15.348221: step 6716, loss 0.137561, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:15.562435: step 6717, loss 0.156698, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:15.758932: step 6718, loss 0.0477991, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:15.915035: step 6719, loss 0.0379525, acc 1, learning_rate 0.0001
2017-10-10T12:09:16.106779: step 6720, loss 0.08868, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:16.552600: step 6720, loss 0.210043, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6720

2017-10-10T12:09:18.140445: step 6721, loss 0.101856, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:18.336079: step 6722, loss 0.0663955, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:18.530083: step 6723, loss 0.11046, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:18.701035: step 6724, loss 0.159954, acc 0.90625, learning_rate 0.0001
2017-10-10T12:09:18.857021: step 6725, loss 0.0164343, acc 1, learning_rate 0.0001
2017-10-10T12:09:19.054995: step 6726, loss 0.0449538, acc 1, learning_rate 0.0001
2017-10-10T12:09:19.216932: step 6727, loss 0.0419734, acc 1, learning_rate 0.0001
2017-10-10T12:09:19.426152: step 6728, loss 0.0524851, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:19.601934: step 6729, loss 0.0804389, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:19.785851: step 6730, loss 0.126921, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:19.996829: step 6731, loss 0.158222, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:20.179342: step 6732, loss 0.0852569, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:20.341540: step 6733, loss 0.161656, acc 0.921875, learning_rate 0.0001
2017-10-10T12:09:20.505021: step 6734, loss 0.0858486, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:20.688862: step 6735, loss 0.167616, acc 0.90625, learning_rate 0.0001
2017-10-10T12:09:20.834587: step 6736, loss 0.140694, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:21.029605: step 6737, loss 0.158777, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:21.296856: step 6738, loss 0.0822969, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:21.521038: step 6739, loss 0.0530179, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:21.680847: step 6740, loss 0.0522125, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:21.821849: step 6741, loss 0.0545651, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:21.977485: step 6742, loss 0.0742586, acc 1, learning_rate 0.0001
2017-10-10T12:09:22.100915: step 6743, loss 0.0742083, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:22.220836: step 6744, loss 0.11641, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:22.402749: step 6745, loss 0.12824, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:22.650255: step 6746, loss 0.0767162, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:22.796388: step 6747, loss 0.136263, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:22.916883: step 6748, loss 0.081458, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:23.069961: step 6749, loss 0.0983287, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:23.227419: step 6750, loss 0.119267, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:23.375193: step 6751, loss 0.109951, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:23.512859: step 6752, loss 0.0932449, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:23.655049: step 6753, loss 0.0693892, acc 1, learning_rate 0.0001
2017-10-10T12:09:23.791391: step 6754, loss 0.135302, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:24.004965: step 6755, loss 0.0730341, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:24.209298: step 6756, loss 0.0782067, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:24.326431: step 6757, loss 0.0994526, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:24.527187: step 6758, loss 0.100866, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:24.664204: step 6759, loss 0.189834, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:24.834625: step 6760, loss 0.0871181, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:25.287544: step 6760, loss 0.207826, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6760

2017-10-10T12:09:26.149856: step 6761, loss 0.0983064, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:26.316779: step 6762, loss 0.214387, acc 0.901961, learning_rate 0.0001
2017-10-10T12:09:26.484959: step 6763, loss 0.0531554, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:26.693011: step 6764, loss 0.161384, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:26.892910: step 6765, loss 0.0574265, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:27.072834: step 6766, loss 0.092952, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:27.247564: step 6767, loss 0.101541, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:27.416880: step 6768, loss 0.0530907, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:27.565028: step 6769, loss 0.0807619, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:27.733757: step 6770, loss 0.110456, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:27.937354: step 6771, loss 0.0461231, acc 1, learning_rate 0.0001
2017-10-10T12:09:28.124344: step 6772, loss 0.148351, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:28.265489: step 6773, loss 0.0636675, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:28.452235: step 6774, loss 0.0866545, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:28.663259: step 6775, loss 0.0893865, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:28.851346: step 6776, loss 0.0872359, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:29.036822: step 6777, loss 0.10785, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:29.202106: step 6778, loss 0.0484297, acc 1, learning_rate 0.0001
2017-10-10T12:09:29.394030: step 6779, loss 0.0869552, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:29.553781: step 6780, loss 0.154164, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:29.726188: step 6781, loss 0.0471205, acc 1, learning_rate 0.0001
2017-10-10T12:09:29.942608: step 6782, loss 0.11214, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:30.083712: step 6783, loss 0.111686, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:30.255042: step 6784, loss 0.0797046, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:30.450147: step 6785, loss 0.128898, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:30.647531: step 6786, loss 0.0849782, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:30.830458: step 6787, loss 0.0744239, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:31.022484: step 6788, loss 0.0680213, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:31.166006: step 6789, loss 0.110901, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:31.351808: step 6790, loss 0.145727, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:31.556879: step 6791, loss 0.0622629, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:31.745104: step 6792, loss 0.125685, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:31.907918: step 6793, loss 0.125251, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:32.104944: step 6794, loss 0.124886, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:32.309078: step 6795, loss 0.0733793, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:32.525046: step 6796, loss 0.0449694, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:32.676855: step 6797, loss 0.154117, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:32.825047: step 6798, loss 0.0746791, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:32.964844: step 6799, loss 0.151066, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:33.180845: step 6800, loss 0.0632215, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:33.498628: step 6800, loss 0.208728, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6800

2017-10-10T12:09:34.489144: step 6801, loss 0.0306624, acc 1, learning_rate 0.0001
2017-10-10T12:09:34.663374: step 6802, loss 0.133051, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:34.884880: step 6803, loss 0.100725, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:35.055475: step 6804, loss 0.0753372, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:35.251442: step 6805, loss 0.0356751, acc 1, learning_rate 0.0001
2017-10-10T12:09:35.423074: step 6806, loss 0.0883276, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:35.613561: step 6807, loss 0.0568728, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:35.753233: step 6808, loss 0.0821875, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:35.940882: step 6809, loss 0.074181, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:36.128362: step 6810, loss 0.0874225, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:36.297386: step 6811, loss 0.159606, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:36.508939: step 6812, loss 0.0858396, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:36.684885: step 6813, loss 0.069349, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:36.891791: step 6814, loss 0.0823831, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:37.057014: step 6815, loss 0.0562009, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:37.284700: step 6816, loss 0.0768771, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:37.476937: step 6817, loss 0.107283, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:37.615424: step 6818, loss 0.0601394, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:37.805147: step 6819, loss 0.132516, acc 0.921875, learning_rate 0.0001
2017-10-10T12:09:38.009218: step 6820, loss 0.074348, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:38.144985: step 6821, loss 0.0787642, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:38.302591: step 6822, loss 0.0660939, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:38.507151: step 6823, loss 0.169092, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:38.673082: step 6824, loss 0.0690603, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:38.815847: step 6825, loss 0.128125, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:38.993695: step 6826, loss 0.107684, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.180919: step 6827, loss 0.117614, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.369004: step 6828, loss 0.0444632, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:39.526160: step 6829, loss 0.0754039, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.721170: step 6830, loss 0.0600039, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:39.916839: step 6831, loss 0.0870217, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:40.089574: step 6832, loss 0.222937, acc 0.875, learning_rate 0.0001
2017-10-10T12:09:40.228654: step 6833, loss 0.125721, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:40.426974: step 6834, loss 0.09799, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:40.640824: step 6835, loss 0.0589993, acc 1, learning_rate 0.0001
2017-10-10T12:09:40.816887: step 6836, loss 0.0936852, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:40.969212: step 6837, loss 0.153802, acc 0.921875, learning_rate 0.0001
2017-10-10T12:09:41.175057: step 6838, loss 0.0589525, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:41.376915: step 6839, loss 0.111387, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:41.521767: step 6840, loss 0.202179, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:41.950694: step 6840, loss 0.210219, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6840

2017-10-10T12:09:43.224819: step 6841, loss 0.0776155, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:43.334247: step 6842, loss 0.073333, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:43.532843: step 6843, loss 0.118435, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:43.732888: step 6844, loss 0.0659809, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:43.841050: step 6845, loss 0.218352, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:43.948352: step 6846, loss 0.133654, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:44.064014: step 6847, loss 0.0565913, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:44.183402: step 6848, loss 0.0556504, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:44.335033: step 6849, loss 0.148146, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:44.490383: step 6850, loss 0.141883, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:44.607377: step 6851, loss 0.0795305, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:44.808824: step 6852, loss 0.0552246, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:45.012856: step 6853, loss 0.0350037, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:45.222203: step 6854, loss 0.133739, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:45.385604: step 6855, loss 0.109193, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:45.524875: step 6856, loss 0.124548, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:45.745014: step 6857, loss 0.100528, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:45.950319: step 6858, loss 0.125124, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:46.121016: step 6859, loss 0.0876473, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:46.256625: step 6860, loss 0.0864029, acc 0.980392, learning_rate 0.0001
2017-10-10T12:09:46.460164: step 6861, loss 0.122321, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:46.627924: step 6862, loss 0.120006, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:46.768860: step 6863, loss 0.119812, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:46.961471: step 6864, loss 0.0897846, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:47.178188: step 6865, loss 0.0681099, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:47.352935: step 6866, loss 0.0870404, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:47.500976: step 6867, loss 0.145368, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:47.688415: step 6868, loss 0.0584653, acc 1, learning_rate 0.0001
2017-10-10T12:09:47.894586: step 6869, loss 0.093808, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:48.080942: step 6870, loss 0.0964271, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:48.210654: step 6871, loss 0.107872, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:48.420560: step 6872, loss 0.121255, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:48.612900: step 6873, loss 0.110953, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:48.792974: step 6874, loss 0.20677, acc 0.90625, learning_rate 0.0001
2017-10-10T12:09:48.976837: step 6875, loss 0.0669379, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:49.157657: step 6876, loss 0.134287, acc 0.921875, learning_rate 0.0001
2017-10-10T12:09:49.332856: step 6877, loss 0.0687609, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:49.499900: step 6878, loss 0.197112, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:49.660832: step 6879, loss 0.133774, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:49.862030: step 6880, loss 0.073299, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:50.259535: step 6880, loss 0.210562, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6880

2017-10-10T12:09:51.228835: step 6881, loss 0.0988551, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:51.400690: step 6882, loss 0.0891479, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:51.564486: step 6883, loss 0.110724, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:51.716911: step 6884, loss 0.136311, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:51.892603: step 6885, loss 0.119961, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:52.112891: step 6886, loss 0.176345, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:52.308906: step 6887, loss 0.123893, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:52.439443: step 6888, loss 0.0820043, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:52.631688: step 6889, loss 0.111897, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:52.808883: step 6890, loss 0.108834, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:52.950651: step 6891, loss 0.069467, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:53.129868: step 6892, loss 0.0487706, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:53.334051: step 6893, loss 0.0602572, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:53.552262: step 6894, loss 0.100603, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:53.718571: step 6895, loss 0.0442003, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:53.880945: step 6896, loss 0.0201791, acc 1, learning_rate 0.0001
2017-10-10T12:09:54.120872: step 6897, loss 0.110631, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:54.344924: step 6898, loss 0.0647457, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:54.548873: step 6899, loss 0.106753, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:54.688284: step 6900, loss 0.0620957, acc 1, learning_rate 0.0001
2017-10-10T12:09:54.776608: step 6901, loss 0.0909315, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:54.867864: step 6902, loss 0.0619825, acc 1, learning_rate 0.0001
2017-10-10T12:09:54.975599: step 6903, loss 0.151009, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:55.066054: step 6904, loss 0.0968642, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:55.153408: step 6905, loss 0.0964137, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:55.242331: step 6906, loss 0.139059, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:55.372845: step 6907, loss 0.0798503, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:55.552848: step 6908, loss 0.0853587, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:55.768590: step 6909, loss 0.0686522, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:55.976628: step 6910, loss 0.0882806, acc 1, learning_rate 0.0001
2017-10-10T12:09:56.145126: step 6911, loss 0.075298, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:56.319642: step 6912, loss 0.0728408, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:56.489319: step 6913, loss 0.0791372, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:56.700855: step 6914, loss 0.0895001, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:56.849163: step 6915, loss 0.134516, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:57.011744: step 6916, loss 0.0670934, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:57.191450: step 6917, loss 0.0602003, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:57.373543: step 6918, loss 0.116333, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:57.532101: step 6919, loss 0.065439, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:57.723403: step 6920, loss 0.115198, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:58.163482: step 6920, loss 0.21025, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6920

2017-10-10T12:09:59.236972: step 6921, loss 0.0845731, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:59.391273: step 6922, loss 0.23369, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:59.584830: step 6923, loss 0.0943677, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:59.784387: step 6924, loss 0.065539, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:59.997414: step 6925, loss 0.164233, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:00.157105: step 6926, loss 0.132381, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:00.329754: step 6927, loss 0.11287, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:00.529032: step 6928, loss 0.0953368, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:00.706255: step 6929, loss 0.0362185, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:00.912831: step 6930, loss 0.0799432, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:01.093098: step 6931, loss 0.0452067, acc 1, learning_rate 0.0001
2017-10-10T12:10:01.240571: step 6932, loss 0.0666755, acc 1, learning_rate 0.0001
2017-10-10T12:10:01.388849: step 6933, loss 0.101283, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:01.585253: step 6934, loss 0.131827, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:01.804857: step 6935, loss 0.175534, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:02.017127: step 6936, loss 0.0884446, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:02.204890: step 6937, loss 0.0892194, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:02.408403: step 6938, loss 0.0657958, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:02.599991: step 6939, loss 0.0605905, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:02.740351: step 6940, loss 0.176266, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:02.910793: step 6941, loss 0.105488, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:03.116825: step 6942, loss 0.130088, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:03.304942: step 6943, loss 0.119355, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:03.446413: step 6944, loss 0.0778132, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:03.645243: step 6945, loss 0.101132, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:03.836384: step 6946, loss 0.141239, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:04.003755: step 6947, loss 0.10211, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:04.164482: step 6948, loss 0.0555772, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:04.368574: step 6949, loss 0.163524, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:04.609568: step 6950, loss 0.060192, acc 1, learning_rate 0.0001
2017-10-10T12:10:04.821061: step 6951, loss 0.131914, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:05.008917: step 6952, loss 0.110854, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:05.224974: step 6953, loss 0.0760507, acc 1, learning_rate 0.0001
2017-10-10T12:10:05.425340: step 6954, loss 0.110534, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:05.524434: step 6955, loss 0.0654889, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:05.626160: step 6956, loss 0.0308064, acc 1, learning_rate 0.0001
2017-10-10T12:10:05.713328: step 6957, loss 0.0589731, acc 1, learning_rate 0.0001
2017-10-10T12:10:05.789326: step 6958, loss 0.0796989, acc 0.980392, learning_rate 0.0001
2017-10-10T12:10:05.882362: step 6959, loss 0.0663536, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:05.978141: step 6960, loss 0.125981, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:06.234682: step 6960, loss 0.21006, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-6960

2017-10-10T12:10:07.476859: step 6961, loss 0.0305199, acc 1, learning_rate 0.0001
2017-10-10T12:10:07.683307: step 6962, loss 0.0539617, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:07.909907: step 6963, loss 0.0427359, acc 1, learning_rate 0.0001
2017-10-10T12:10:08.048233: step 6964, loss 0.186461, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:08.205740: step 6965, loss 0.19075, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:08.397990: step 6966, loss 0.103034, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:08.607639: step 6967, loss 0.147172, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:08.710873: step 6968, loss 0.0392939, acc 1, learning_rate 0.0001
2017-10-10T12:10:08.905209: step 6969, loss 0.115351, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:09.113288: step 6970, loss 0.113483, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:09.296889: step 6971, loss 0.114406, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:09.483981: step 6972, loss 0.154238, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:09.647526: step 6973, loss 0.102765, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:09.841008: step 6974, loss 0.0849305, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:10.056412: step 6975, loss 0.103651, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:10.201629: step 6976, loss 0.0518157, acc 1, learning_rate 0.0001
2017-10-10T12:10:10.374687: step 6977, loss 0.144722, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:10.561996: step 6978, loss 0.0513948, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:10.760052: step 6979, loss 0.121151, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:10.880258: step 6980, loss 0.127829, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:11.091295: step 6981, loss 0.103664, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:11.315960: step 6982, loss 0.159978, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:11.484724: step 6983, loss 0.0856912, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:11.613090: step 6984, loss 0.104727, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:11.795218: step 6985, loss 0.134575, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:12.019308: step 6986, loss 0.0776055, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:12.207609: step 6987, loss 0.0270846, acc 1, learning_rate 0.0001
2017-10-10T12:10:12.351711: step 6988, loss 0.0508657, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:12.515323: step 6989, loss 0.0887425, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:12.718879: step 6990, loss 0.0965401, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:12.912921: step 6991, loss 0.0515009, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:13.080872: step 6992, loss 0.1262, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:13.250935: step 6993, loss 0.032065, acc 1, learning_rate 0.0001
2017-10-10T12:10:13.429591: step 6994, loss 0.08809, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:13.609427: step 6995, loss 0.11611, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:13.773200: step 6996, loss 0.082263, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:13.991148: step 6997, loss 0.173211, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:14.190044: step 6998, loss 0.0421261, acc 1, learning_rate 0.0001
2017-10-10T12:10:14.340045: step 6999, loss 0.0710956, acc 1, learning_rate 0.0001
2017-10-10T12:10:14.495878: step 7000, loss 0.134739, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:14.989801: step 7000, loss 0.210692, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7000

2017-10-10T12:10:16.002377: step 7001, loss 0.0447484, acc 1, learning_rate 0.0001
2017-10-10T12:10:16.177246: step 7002, loss 0.0481191, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:16.298991: step 7003, loss 0.149971, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:16.399016: step 7004, loss 0.113272, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:16.500767: step 7005, loss 0.0767197, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:16.598255: step 7006, loss 0.0973158, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:16.736135: step 7007, loss 0.0465549, acc 1, learning_rate 0.0001
2017-10-10T12:10:16.889852: step 7008, loss 0.126386, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:17.041434: step 7009, loss 0.137529, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:17.189318: step 7010, loss 0.0533162, acc 1, learning_rate 0.0001
2017-10-10T12:10:17.374421: step 7011, loss 0.166083, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:17.513147: step 7012, loss 0.112508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:17.707424: step 7013, loss 0.0961967, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:17.908842: step 7014, loss 0.188836, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:18.073022: step 7015, loss 0.0930522, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:18.232936: step 7016, loss 0.0943255, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:18.423943: step 7017, loss 0.041044, acc 1, learning_rate 0.0001
2017-10-10T12:10:18.640992: step 7018, loss 0.105933, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:18.784935: step 7019, loss 0.0917676, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:18.961917: step 7020, loss 0.139253, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:19.147991: step 7021, loss 0.0957997, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:19.348833: step 7022, loss 0.0624232, acc 1, learning_rate 0.0001
2017-10-10T12:10:19.537235: step 7023, loss 0.108931, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:19.661058: step 7024, loss 0.120958, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:19.852721: step 7025, loss 0.133819, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:20.028875: step 7026, loss 0.0707388, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:20.222825: step 7027, loss 0.223966, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:20.404849: step 7028, loss 0.0813325, acc 1, learning_rate 0.0001
2017-10-10T12:10:20.604874: step 7029, loss 0.0852631, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:20.775368: step 7030, loss 0.101256, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:20.985043: step 7031, loss 0.0707276, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:21.159152: step 7032, loss 0.068766, acc 1, learning_rate 0.0001
2017-10-10T12:10:21.349993: step 7033, loss 0.113617, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:21.563716: step 7034, loss 0.13418, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:21.767804: step 7035, loss 0.112158, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:21.945030: step 7036, loss 0.0367232, acc 1, learning_rate 0.0001
2017-10-10T12:10:22.100905: step 7037, loss 0.0525739, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:22.272940: step 7038, loss 0.0745568, acc 1, learning_rate 0.0001
2017-10-10T12:10:22.472764: step 7039, loss 0.0792604, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:22.645343: step 7040, loss 0.0535452, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:23.140830: step 7040, loss 0.210648, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7040

2017-10-10T12:10:24.212865: step 7041, loss 0.0896508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:24.394708: step 7042, loss 0.0987137, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:24.578124: step 7043, loss 0.0969663, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:24.707827: step 7044, loss 0.112113, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:24.920837: step 7045, loss 0.135486, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:25.088914: step 7046, loss 0.102769, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:25.268855: step 7047, loss 0.0805964, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:25.504880: step 7048, loss 0.0758118, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:25.770424: step 7049, loss 0.161273, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:25.940853: step 7050, loss 0.0794541, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:26.089253: step 7051, loss 0.158847, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:26.211123: step 7052, loss 0.0541573, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:26.319724: step 7053, loss 0.0599235, acc 1, learning_rate 0.0001
2017-10-10T12:10:26.500412: step 7054, loss 0.0915352, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:26.666402: step 7055, loss 0.0811221, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:26.800831: step 7056, loss 0.0767228, acc 0.980392, learning_rate 0.0001
2017-10-10T12:10:27.000829: step 7057, loss 0.0222979, acc 1, learning_rate 0.0001
2017-10-10T12:10:27.197261: step 7058, loss 0.146548, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:27.348581: step 7059, loss 0.0702363, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:27.502956: step 7060, loss 0.0535292, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:27.652293: step 7061, loss 0.045224, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:27.784885: step 7062, loss 0.0272426, acc 1, learning_rate 0.0001
2017-10-10T12:10:27.900836: step 7063, loss 0.0452743, acc 1, learning_rate 0.0001
2017-10-10T12:10:28.048940: step 7064, loss 0.158733, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:28.270126: step 7065, loss 0.100534, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:28.432638: step 7066, loss 0.0531986, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:28.604874: step 7067, loss 0.096916, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:28.792982: step 7068, loss 0.0712306, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:28.969859: step 7069, loss 0.0404697, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:29.154769: step 7070, loss 0.131245, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:29.311264: step 7071, loss 0.230827, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:29.491081: step 7072, loss 0.0486726, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:29.643804: step 7073, loss 0.071039, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:29.829810: step 7074, loss 0.0586088, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:30.019916: step 7075, loss 0.128928, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:30.221081: step 7076, loss 0.0812189, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:30.373193: step 7077, loss 0.0667418, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:30.537812: step 7078, loss 0.0944706, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:30.744983: step 7079, loss 0.0984939, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:30.906005: step 7080, loss 0.241469, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:31.368989: step 7080, loss 0.210353, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7080

2017-10-10T12:10:32.764418: step 7081, loss 0.108282, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:32.895378: step 7082, loss 0.0999878, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:33.074429: step 7083, loss 0.0673728, acc 1, learning_rate 0.0001
2017-10-10T12:10:33.261168: step 7084, loss 0.0498312, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:33.407318: step 7085, loss 0.103116, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:33.576827: step 7086, loss 0.0492513, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:33.780472: step 7087, loss 0.052109, acc 1, learning_rate 0.0001
2017-10-10T12:10:33.956209: step 7088, loss 0.110146, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:34.084945: step 7089, loss 0.184917, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:34.277691: step 7090, loss 0.0921848, acc 1, learning_rate 0.0001
2017-10-10T12:10:34.480186: step 7091, loss 0.0550459, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:34.677964: step 7092, loss 0.0889445, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:34.861066: step 7093, loss 0.0878033, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:35.004855: step 7094, loss 0.0996306, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:35.202980: step 7095, loss 0.144135, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:35.383245: step 7096, loss 0.0944256, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:35.562236: step 7097, loss 0.0663709, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:35.723834: step 7098, loss 0.181084, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:35.900919: step 7099, loss 0.0632812, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:36.152839: step 7100, loss 0.0537194, acc 1, learning_rate 0.0001
2017-10-10T12:10:36.363154: step 7101, loss 0.0627279, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:36.523637: step 7102, loss 0.0992219, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:36.685027: step 7103, loss 0.0729335, acc 1, learning_rate 0.0001
2017-10-10T12:10:36.848198: step 7104, loss 0.155135, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:36.975101: step 7105, loss 0.0946482, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:37.085700: step 7106, loss 0.13537, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:37.237193: step 7107, loss 0.0429876, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:37.373373: step 7108, loss 0.0297039, acc 1, learning_rate 0.0001
2017-10-10T12:10:37.486263: step 7109, loss 0.100585, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:37.699046: step 7110, loss 0.133312, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:37.931870: step 7111, loss 0.0901644, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:38.161128: step 7112, loss 0.133354, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:38.285034: step 7113, loss 0.156876, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:38.406866: step 7114, loss 0.14963, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:38.561539: step 7115, loss 0.118971, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:38.711366: step 7116, loss 0.0955755, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:38.842731: step 7117, loss 0.078031, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:38.984664: step 7118, loss 0.0736248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:39.104872: step 7119, loss 0.144947, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:39.312202: step 7120, loss 0.0651019, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:39.732943: step 7120, loss 0.210006, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7120

2017-10-10T12:10:40.736949: step 7121, loss 0.0975918, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:40.914087: step 7122, loss 0.106935, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:41.067420: step 7123, loss 0.0773746, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:41.261986: step 7124, loss 0.0947586, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:41.468329: step 7125, loss 0.0730403, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:41.656869: step 7126, loss 0.0730545, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:41.805344: step 7127, loss 0.0907054, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:41.985651: step 7128, loss 0.0864399, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:42.193088: step 7129, loss 0.0806231, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:42.352998: step 7130, loss 0.110509, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:42.504370: step 7131, loss 0.0811457, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:42.703572: step 7132, loss 0.136446, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:42.884586: step 7133, loss 0.0676078, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:43.072214: step 7134, loss 0.0967249, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:43.249951: step 7135, loss 0.135656, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:43.404831: step 7136, loss 0.15026, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:43.579872: step 7137, loss 0.0522843, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:43.771436: step 7138, loss 0.0721351, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:43.984822: step 7139, loss 0.0576854, acc 1, learning_rate 0.0001
2017-10-10T12:10:44.168845: step 7140, loss 0.0378788, acc 1, learning_rate 0.0001
2017-10-10T12:10:44.325036: step 7141, loss 0.0559134, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:44.526024: step 7142, loss 0.132658, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:44.717644: step 7143, loss 0.095212, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:44.920893: step 7144, loss 0.0936181, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:45.136820: step 7145, loss 0.105694, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:45.336842: step 7146, loss 0.148538, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:45.492955: step 7147, loss 0.0693306, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:45.661912: step 7148, loss 0.0913502, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:45.858658: step 7149, loss 0.0838439, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:46.071874: step 7150, loss 0.0716476, acc 1, learning_rate 0.0001
2017-10-10T12:10:46.217078: step 7151, loss 0.0624305, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:46.382908: step 7152, loss 0.207242, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:46.612853: step 7153, loss 0.0736476, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:46.842365: step 7154, loss 0.0849031, acc 0.960784, learning_rate 0.0001
2017-10-10T12:10:47.064802: step 7155, loss 0.128493, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:47.202150: step 7156, loss 0.0350862, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:47.320850: step 7157, loss 0.126569, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:47.458833: step 7158, loss 0.0794125, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:47.588843: step 7159, loss 0.159342, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:47.733037: step 7160, loss 0.0664444, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:48.205022: step 7160, loss 0.211213, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7160

2017-10-10T12:10:49.291744: step 7161, loss 0.0349393, acc 1, learning_rate 0.0001
2017-10-10T12:10:49.411546: step 7162, loss 0.153025, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:49.531084: step 7163, loss 0.0901017, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:49.675797: step 7164, loss 0.0496031, acc 1, learning_rate 0.0001
2017-10-10T12:10:49.822197: step 7165, loss 0.0840586, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:49.978410: step 7166, loss 0.105453, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:50.129585: step 7167, loss 0.0559666, acc 1, learning_rate 0.0001
2017-10-10T12:10:50.320443: step 7168, loss 0.0996895, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:50.487270: step 7169, loss 0.118693, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:50.659848: step 7170, loss 0.060906, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:50.841003: step 7171, loss 0.203554, acc 0.875, learning_rate 0.0001
2017-10-10T12:10:51.026599: step 7172, loss 0.117615, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:51.227169: step 7173, loss 0.0425074, acc 1, learning_rate 0.0001
2017-10-10T12:10:51.411881: step 7174, loss 0.0615755, acc 1, learning_rate 0.0001
2017-10-10T12:10:51.593028: step 7175, loss 0.0524315, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:51.782083: step 7176, loss 0.0520545, acc 1, learning_rate 0.0001
2017-10-10T12:10:51.931301: step 7177, loss 0.0507188, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:52.107555: step 7178, loss 0.11896, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:52.320825: step 7179, loss 0.0822412, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:52.544990: step 7180, loss 0.0585401, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:52.703842: step 7181, loss 0.0886021, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:52.869143: step 7182, loss 0.0538849, acc 1, learning_rate 0.0001
2017-10-10T12:10:53.087673: step 7183, loss 0.0761582, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:53.288400: step 7184, loss 0.11085, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:53.436520: step 7185, loss 0.0885382, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:53.601657: step 7186, loss 0.142368, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:53.808873: step 7187, loss 0.0271531, acc 1, learning_rate 0.0001
2017-10-10T12:10:53.969123: step 7188, loss 0.0565646, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:54.131523: step 7189, loss 0.0402956, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:54.320067: step 7190, loss 0.0624921, acc 1, learning_rate 0.0001
2017-10-10T12:10:54.539389: step 7191, loss 0.138714, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:54.754193: step 7192, loss 0.0993968, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:54.902650: step 7193, loss 0.114447, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:55.064946: step 7194, loss 0.0361609, acc 1, learning_rate 0.0001
2017-10-10T12:10:55.265585: step 7195, loss 0.0476223, acc 1, learning_rate 0.0001
2017-10-10T12:10:55.463373: step 7196, loss 0.103587, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:55.610825: step 7197, loss 0.0532497, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:55.780736: step 7198, loss 0.0659669, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:55.976599: step 7199, loss 0.109161, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:56.152887: step 7200, loss 0.0663888, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:56.539969: step 7200, loss 0.211687, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7200

2017-10-10T12:10:57.597472: step 7201, loss 0.0893152, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:57.754121: step 7202, loss 0.140101, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:57.868723: step 7203, loss 0.0670336, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:57.991988: step 7204, loss 0.112879, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:58.147871: step 7205, loss 0.0831544, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:58.292460: step 7206, loss 0.134905, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:58.475888: step 7207, loss 0.139189, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:58.671585: step 7208, loss 0.0733474, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:58.832893: step 7209, loss 0.0628082, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:58.993707: step 7210, loss 0.0579554, acc 1, learning_rate 0.0001
2017-10-10T12:10:59.198125: step 7211, loss 0.0996881, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:59.374326: step 7212, loss 0.0769495, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:59.537320: step 7213, loss 0.0872239, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:59.715783: step 7214, loss 0.129358, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:59.940898: step 7215, loss 0.110966, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:00.173031: step 7216, loss 0.0683884, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:00.347294: step 7217, loss 0.0858333, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:00.481133: step 7218, loss 0.147273, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:00.627066: step 7219, loss 0.101318, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:00.767094: step 7220, loss 0.14437, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:00.916958: step 7221, loss 0.0624498, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:01.044910: step 7222, loss 0.0563899, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:01.172859: step 7223, loss 0.0775187, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:01.317471: step 7224, loss 0.0892046, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:01.479431: step 7225, loss 0.0307383, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:01.662595: step 7226, loss 0.0869498, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:01.814513: step 7227, loss 0.0767509, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:01.995920: step 7228, loss 0.0639105, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:02.113981: step 7229, loss 0.141215, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:02.297885: step 7230, loss 0.14066, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:02.515652: step 7231, loss 0.062228, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:02.720842: step 7232, loss 0.0845795, acc 1, learning_rate 0.0001
2017-10-10T12:11:02.912983: step 7233, loss 0.0804601, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:03.095431: step 7234, loss 0.0982521, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:03.277098: step 7235, loss 0.079209, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:03.442208: step 7236, loss 0.186041, acc 0.90625, learning_rate 0.0001
2017-10-10T12:11:03.627939: step 7237, loss 0.370552, acc 0.890625, learning_rate 0.0001
2017-10-10T12:11:03.816485: step 7238, loss 0.0499556, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:04.025161: step 7239, loss 0.0656945, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:04.204949: step 7240, loss 0.0671767, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:04.648969: step 7240, loss 0.212618, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7240

2017-10-10T12:11:05.976818: step 7241, loss 0.0579465, acc 1, learning_rate 0.0001
2017-10-10T12:11:06.210134: step 7242, loss 0.0702876, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.385182: step 7243, loss 0.0935439, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.560861: step 7244, loss 0.091448, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.736383: step 7245, loss 0.0928181, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.909102: step 7246, loss 0.0658158, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:07.124702: step 7247, loss 0.0794545, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:07.300946: step 7248, loss 0.112534, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:07.514961: step 7249, loss 0.070215, acc 1, learning_rate 0.0001
2017-10-10T12:11:07.668849: step 7250, loss 0.0761262, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:07.797357: step 7251, loss 0.0352276, acc 1, learning_rate 0.0001
2017-10-10T12:11:07.931809: step 7252, loss 0.160575, acc 0.941176, learning_rate 0.0001
2017-10-10T12:11:08.078746: step 7253, loss 0.144063, acc 0.921875, learning_rate 0.0001
2017-10-10T12:11:08.234475: step 7254, loss 0.151634, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:08.360885: step 7255, loss 0.0977548, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:08.488855: step 7256, loss 0.110172, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:08.647900: step 7257, loss 0.151135, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:08.860852: step 7258, loss 0.105936, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:09.008852: step 7259, loss 0.105378, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:09.173948: step 7260, loss 0.103055, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:09.360840: step 7261, loss 0.0660366, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:09.571507: step 7262, loss 0.043132, acc 1, learning_rate 0.0001
2017-10-10T12:11:09.742636: step 7263, loss 0.0877532, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:09.953115: step 7264, loss 0.0475916, acc 1, learning_rate 0.0001
2017-10-10T12:11:10.144516: step 7265, loss 0.168423, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:10.326441: step 7266, loss 0.176607, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:10.497008: step 7267, loss 0.0540839, acc 1, learning_rate 0.0001
2017-10-10T12:11:10.646245: step 7268, loss 0.154305, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:10.877131: step 7269, loss 0.0783434, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.084875: step 7270, loss 0.0565815, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.287858: step 7271, loss 0.0603309, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.425879: step 7272, loss 0.0799894, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:11.564282: step 7273, loss 0.0702031, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:11.715491: step 7274, loss 0.0934945, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.831038: step 7275, loss 0.105023, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:11.951688: step 7276, loss 0.0458477, acc 1, learning_rate 0.0001
2017-10-10T12:11:12.103394: step 7277, loss 0.119601, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:12.284724: step 7278, loss 0.0527319, acc 1, learning_rate 0.0001
2017-10-10T12:11:12.482167: step 7279, loss 0.0152639, acc 1, learning_rate 0.0001
2017-10-10T12:11:12.680937: step 7280, loss 0.166251, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:13.134175: step 7280, loss 0.211863, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7280

2017-10-10T12:11:14.106595: step 7281, loss 0.0850372, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:14.300808: step 7282, loss 0.125277, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:14.476920: step 7283, loss 0.0657074, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:14.644873: step 7284, loss 0.0790274, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:14.834379: step 7285, loss 0.0801776, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:15.028845: step 7286, loss 0.261716, acc 0.921875, learning_rate 0.0001
2017-10-10T12:11:15.187479: step 7287, loss 0.106051, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:15.354926: step 7288, loss 0.039238, acc 1, learning_rate 0.0001
2017-10-10T12:11:15.531775: step 7289, loss 0.125647, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:15.696869: step 7290, loss 0.0905184, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:15.908972: step 7291, loss 0.0920058, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:16.078418: step 7292, loss 0.0914519, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:16.285142: step 7293, loss 0.194594, acc 0.921875, learning_rate 0.0001
2017-10-10T12:11:16.457013: step 7294, loss 0.0494747, acc 1, learning_rate 0.0001
2017-10-10T12:11:16.668865: step 7295, loss 0.135575, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:16.874308: step 7296, loss 0.0609014, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:17.070302: step 7297, loss 0.0842208, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:17.243347: step 7298, loss 0.0774327, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:17.393123: step 7299, loss 0.116828, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:17.576180: step 7300, loss 0.0694407, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:17.785277: step 7301, loss 0.0956469, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:17.997419: step 7302, loss 0.0698941, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:18.171865: step 7303, loss 0.0605452, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:18.323780: step 7304, loss 0.0920872, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:18.468487: step 7305, loss 0.0689695, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:18.589028: step 7306, loss 0.168195, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:18.740867: step 7307, loss 0.0759588, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:18.877295: step 7308, loss 0.189702, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:19.041112: step 7309, loss 0.0941246, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:19.201916: step 7310, loss 0.0341574, acc 1, learning_rate 0.0001
2017-10-10T12:11:19.388114: step 7311, loss 0.100984, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:19.521067: step 7312, loss 0.0951725, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:19.676842: step 7313, loss 0.097807, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:19.864874: step 7314, loss 0.133193, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:20.065084: step 7315, loss 0.108014, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:20.244993: step 7316, loss 0.124398, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:20.418711: step 7317, loss 0.115167, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:20.624152: step 7318, loss 0.0866772, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:20.833320: step 7319, loss 0.0711686, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:21.060839: step 7320, loss 0.129547, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:21.553679: step 7320, loss 0.210731, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7320

2017-10-10T12:11:22.559571: step 7321, loss 0.0898297, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:22.787567: step 7322, loss 0.0763672, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:22.976844: step 7323, loss 0.131041, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:23.125263: step 7324, loss 0.156444, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:23.307314: step 7325, loss 0.0438531, acc 1, learning_rate 0.0001
2017-10-10T12:11:23.504819: step 7326, loss 0.0793087, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:23.704850: step 7327, loss 0.124115, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:23.905424: step 7328, loss 0.0699899, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:24.089084: step 7329, loss 0.108232, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:24.280899: step 7330, loss 0.0628818, acc 1, learning_rate 0.0001
2017-10-10T12:11:24.461005: step 7331, loss 0.102178, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:24.693031: step 7332, loss 0.115891, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:24.892869: step 7333, loss 0.0494469, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:25.011939: step 7334, loss 0.102564, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:25.204840: step 7335, loss 0.0998784, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:25.341057: step 7336, loss 0.114808, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:25.504779: step 7337, loss 0.0891059, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:25.689082: step 7338, loss 0.143434, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:25.912267: step 7339, loss 0.113129, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:26.089267: step 7340, loss 0.0569913, acc 1, learning_rate 0.0001
2017-10-10T12:11:26.248865: step 7341, loss 0.0656973, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:26.441187: step 7342, loss 0.224321, acc 0.921875, learning_rate 0.0001
2017-10-10T12:11:26.628458: step 7343, loss 0.0936538, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:26.780322: step 7344, loss 0.0733426, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:26.986737: step 7345, loss 0.106777, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:27.195845: step 7346, loss 0.0949417, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:27.412841: step 7347, loss 0.192842, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:27.581605: step 7348, loss 0.0473305, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:27.777159: step 7349, loss 0.0557493, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:27.926135: step 7350, loss 0.0681387, acc 0.960784, learning_rate 0.0001
2017-10-10T12:11:28.141770: step 7351, loss 0.0562159, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:28.360862: step 7352, loss 0.184059, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:28.545761: step 7353, loss 0.0382093, acc 1, learning_rate 0.0001
2017-10-10T12:11:28.719398: step 7354, loss 0.122736, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:28.888756: step 7355, loss 0.110268, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:29.027371: step 7356, loss 0.0761498, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:29.137062: step 7357, loss 0.0862257, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:29.306175: step 7358, loss 0.0765882, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:29.477088: step 7359, loss 0.197507, acc 0.921875, learning_rate 0.0001
2017-10-10T12:11:29.619056: step 7360, loss 0.0456915, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:30.083612: step 7360, loss 0.211437, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7360

2017-10-10T12:11:31.358584: step 7361, loss 0.130328, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:31.561228: step 7362, loss 0.103729, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:31.760111: step 7363, loss 0.0697266, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:31.939641: step 7364, loss 0.0973802, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:32.099780: step 7365, loss 0.0786053, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:32.308978: step 7366, loss 0.0454724, acc 1, learning_rate 0.0001
2017-10-10T12:11:32.509528: step 7367, loss 0.0688771, acc 1, learning_rate 0.0001
2017-10-10T12:11:32.736989: step 7368, loss 0.0462726, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:32.888872: step 7369, loss 0.0512195, acc 1, learning_rate 0.0001
2017-10-10T12:11:33.005106: step 7370, loss 0.0942041, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:33.136838: step 7371, loss 0.164321, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:33.280908: step 7372, loss 0.0601555, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:33.431810: step 7373, loss 0.167865, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:33.556837: step 7374, loss 0.0760942, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:33.676830: step 7375, loss 0.0491555, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:33.824837: step 7376, loss 0.0880748, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:34.023953: step 7377, loss 0.129744, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:34.176064: step 7378, loss 0.0903739, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:34.349547: step 7379, loss 0.0857801, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:34.536846: step 7380, loss 0.144172, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:34.717494: step 7381, loss 0.0873721, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:34.875446: step 7382, loss 0.0820989, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:35.073454: step 7383, loss 0.0848441, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:35.268029: step 7384, loss 0.0875412, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:35.397198: step 7385, loss 0.0661867, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:35.589814: step 7386, loss 0.0607245, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:35.792217: step 7387, loss 0.141652, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:35.945014: step 7388, loss 0.0589479, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:36.107808: step 7389, loss 0.041472, acc 1, learning_rate 0.0001
2017-10-10T12:11:36.326172: step 7390, loss 0.0330659, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:36.528874: step 7391, loss 0.104987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:36.705275: step 7392, loss 0.199477, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:36.866505: step 7393, loss 0.0961541, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:37.066899: step 7394, loss 0.057062, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:37.213098: step 7395, loss 0.0857268, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:37.372802: step 7396, loss 0.121727, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:37.563815: step 7397, loss 0.110458, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:37.777642: step 7398, loss 0.182188, acc 0.921875, learning_rate 0.0001
2017-10-10T12:11:37.973166: step 7399, loss 0.0948171, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:38.119159: step 7400, loss 0.0926759, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:38.598056: step 7400, loss 0.210204, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7400

2017-10-10T12:11:39.767148: step 7401, loss 0.0853014, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:39.957122: step 7402, loss 0.0771334, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:40.129138: step 7403, loss 0.108779, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:40.265030: step 7404, loss 0.0587688, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:40.424466: step 7405, loss 0.0765161, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:40.631697: step 7406, loss 0.0728505, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:40.843059: step 7407, loss 0.108522, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:40.992043: step 7408, loss 0.0644741, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:41.168807: step 7409, loss 0.0901281, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:41.380882: step 7410, loss 0.0366907, acc 1, learning_rate 0.0001
2017-10-10T12:11:41.537128: step 7411, loss 0.13323, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:41.714029: step 7412, loss 0.128421, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:41.902124: step 7413, loss 0.0477513, acc 1, learning_rate 0.0001
2017-10-10T12:11:42.118778: step 7414, loss 0.14762, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:42.311751: step 7415, loss 0.0577727, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:42.481226: step 7416, loss 0.0489298, acc 1, learning_rate 0.0001
2017-10-10T12:11:42.676083: step 7417, loss 0.0957815, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:42.820886: step 7418, loss 0.0398109, acc 1, learning_rate 0.0001
2017-10-10T12:11:43.008020: step 7419, loss 0.103693, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:43.172374: step 7420, loss 0.0553529, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:43.357472: step 7421, loss 0.103212, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:43.537127: step 7422, loss 0.0905928, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:43.770006: step 7423, loss 0.133138, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:44.012408: step 7424, loss 0.121742, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:44.153646: step 7425, loss 0.107737, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:44.305596: step 7426, loss 0.091029, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:44.456176: step 7427, loss 0.0699918, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:44.572837: step 7428, loss 0.0409184, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:44.763953: step 7429, loss 0.0652239, acc 1, learning_rate 0.0001
2017-10-10T12:11:44.972854: step 7430, loss 0.0410361, acc 1, learning_rate 0.0001
2017-10-10T12:11:45.115050: step 7431, loss 0.159955, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:45.284412: step 7432, loss 0.0646634, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:45.481890: step 7433, loss 0.117609, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:45.660631: step 7434, loss 0.217114, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:45.800846: step 7435, loss 0.0600847, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:45.988288: step 7436, loss 0.107749, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:46.177333: step 7437, loss 0.107611, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:46.358663: step 7438, loss 0.0744423, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:46.558421: step 7439, loss 0.128827, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:46.706099: step 7440, loss 0.137852, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:47.203064: step 7440, loss 0.209983, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7440

2017-10-10T12:11:48.412855: step 7441, loss 0.0412136, acc 1, learning_rate 0.0001
2017-10-10T12:11:48.636857: step 7442, loss 0.0663323, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:48.833038: step 7443, loss 0.119167, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:48.956159: step 7444, loss 0.0639836, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:49.104406: step 7445, loss 0.0340999, acc 1, learning_rate 0.0001
2017-10-10T12:11:49.262307: step 7446, loss 0.0972887, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:49.412120: step 7447, loss 0.0521657, acc 1, learning_rate 0.0001
2017-10-10T12:11:49.512851: step 7448, loss 0.236695, acc 0.921569, learning_rate 0.0001
2017-10-10T12:11:49.667363: step 7449, loss 0.0416254, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:49.831130: step 7450, loss 0.127044, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:50.037002: step 7451, loss 0.0282231, acc 1, learning_rate 0.0001
2017-10-10T12:11:50.196904: step 7452, loss 0.128278, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:50.361739: step 7453, loss 0.0846369, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:50.559895: step 7454, loss 0.0952795, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:50.717010: step 7455, loss 0.129527, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:50.916849: step 7456, loss 0.136039, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:51.106676: step 7457, loss 0.149763, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:51.272885: step 7458, loss 0.0808375, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:51.436227: step 7459, loss 0.0496452, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:51.636188: step 7460, loss 0.0870358, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:51.846584: step 7461, loss 0.0820956, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:52.013151: step 7462, loss 0.0517814, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:52.168879: step 7463, loss 0.0987018, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:52.359867: step 7464, loss 0.0689056, acc 1, learning_rate 0.0001
2017-10-10T12:11:52.563673: step 7465, loss 0.0993762, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:52.756851: step 7466, loss 0.0849736, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:52.935261: step 7467, loss 0.119708, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:53.113002: step 7468, loss 0.123295, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:53.308071: step 7469, loss 0.0313381, acc 1, learning_rate 0.0001
2017-10-10T12:11:53.452952: step 7470, loss 0.0262519, acc 1, learning_rate 0.0001
2017-10-10T12:11:53.653142: step 7471, loss 0.0502774, acc 1, learning_rate 0.0001
2017-10-10T12:11:53.856557: step 7472, loss 0.0254961, acc 1, learning_rate 0.0001
2017-10-10T12:11:54.064951: step 7473, loss 0.0973536, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:54.227355: step 7474, loss 0.0841362, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:54.381024: step 7475, loss 0.130911, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:54.612492: step 7476, loss 0.0499925, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:54.872809: step 7477, loss 0.0329672, acc 1, learning_rate 0.0001
2017-10-10T12:11:55.065801: step 7478, loss 0.12961, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:55.199147: step 7479, loss 0.0456206, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:55.327978: step 7480, loss 0.0902537, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:55.703291: step 7480, loss 0.211872, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7480

2017-10-10T12:11:56.735898: step 7481, loss 0.0623551, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:56.879844: step 7482, loss 0.123085, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:57.056950: step 7483, loss 0.0748758, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:57.251940: step 7484, loss 0.12581, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:57.430029: step 7485, loss 0.055267, acc 1, learning_rate 0.0001
2017-10-10T12:11:57.571138: step 7486, loss 0.0919093, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:57.752287: step 7487, loss 0.0572089, acc 1, learning_rate 0.0001
2017-10-10T12:11:57.921894: step 7488, loss 0.160246, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:58.107815: step 7489, loss 0.083938, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:58.297758: step 7490, loss 0.0398331, acc 1, learning_rate 0.0001
2017-10-10T12:11:58.492852: step 7491, loss 0.0980168, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:58.645034: step 7492, loss 0.144169, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:58.886684: step 7493, loss 0.131279, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:59.120061: step 7494, loss 0.0725046, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:59.296575: step 7495, loss 0.0587044, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:59.428866: step 7496, loss 0.0779317, acc 1, learning_rate 0.0001
2017-10-10T12:11:59.577118: step 7497, loss 0.0628933, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:59.703944: step 7498, loss 0.0493485, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:59.870045: step 7499, loss 0.122914, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:00.032840: step 7500, loss 0.0799473, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:00.218707: step 7501, loss 0.0937379, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:00.408273: step 7502, loss 0.125075, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:00.610713: step 7503, loss 0.186194, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:00.808901: step 7504, loss 0.044303, acc 1, learning_rate 0.0001
2017-10-10T12:12:00.980954: step 7505, loss 0.0665126, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:01.153012: step 7506, loss 0.0863204, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:01.338893: step 7507, loss 0.113675, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:01.518959: step 7508, loss 0.189376, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:01.657942: step 7509, loss 0.116257, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:01.828893: step 7510, loss 0.154461, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:02.048670: step 7511, loss 0.0962165, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:02.265370: step 7512, loss 0.0821925, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:02.424999: step 7513, loss 0.0851943, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:02.564850: step 7514, loss 0.0895435, acc 1, learning_rate 0.0001
2017-10-10T12:12:02.771297: step 7515, loss 0.105902, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:02.920886: step 7516, loss 0.0927299, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:03.068398: step 7517, loss 0.0702675, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:03.288906: step 7518, loss 0.0678212, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:03.425135: step 7519, loss 0.0385328, acc 1, learning_rate 0.0001
2017-10-10T12:12:03.597845: step 7520, loss 0.0638988, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:04.101454: step 7520, loss 0.211906, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7520

2017-10-10T12:12:05.594953: step 7521, loss 0.0780876, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:05.807104: step 7522, loss 0.160962, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:05.984939: step 7523, loss 0.0816618, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:06.204901: step 7524, loss 0.10854, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:06.397060: step 7525, loss 0.060944, acc 1, learning_rate 0.0001
2017-10-10T12:12:06.575332: step 7526, loss 0.0476633, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:06.720867: step 7527, loss 0.0905982, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:06.865643: step 7528, loss 0.0664547, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:06.992796: step 7529, loss 0.130275, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:07.128262: step 7530, loss 0.169689, acc 0.90625, learning_rate 0.0001
2017-10-10T12:12:07.254139: step 7531, loss 0.0908966, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:07.399703: step 7532, loss 0.0885154, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:07.520961: step 7533, loss 0.0462049, acc 1, learning_rate 0.0001
2017-10-10T12:12:07.708923: step 7534, loss 0.12368, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:07.856418: step 7535, loss 0.100501, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:08.057276: step 7536, loss 0.0657577, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:08.205747: step 7537, loss 0.0605974, acc 1, learning_rate 0.0001
2017-10-10T12:12:08.348856: step 7538, loss 0.0761292, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:08.570784: step 7539, loss 0.114312, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:08.725580: step 7540, loss 0.0897921, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:08.865273: step 7541, loss 0.0901214, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:09.051056: step 7542, loss 0.135702, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:09.214630: step 7543, loss 0.0738842, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:09.369716: step 7544, loss 0.0881816, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:09.547067: step 7545, loss 0.0889027, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:09.768886: step 7546, loss 0.126254, acc 0.941176, learning_rate 0.0001
2017-10-10T12:12:09.999972: step 7547, loss 0.147939, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:10.147014: step 7548, loss 0.0924898, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:10.292840: step 7549, loss 0.078656, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:10.412608: step 7550, loss 0.063431, acc 1, learning_rate 0.0001
2017-10-10T12:12:10.539696: step 7551, loss 0.0498181, acc 1, learning_rate 0.0001
2017-10-10T12:12:10.717872: step 7552, loss 0.0906313, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:10.872168: step 7553, loss 0.0663528, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:11.002023: step 7554, loss 0.0684135, acc 1, learning_rate 0.0001
2017-10-10T12:12:11.198848: step 7555, loss 0.174588, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:11.378770: step 7556, loss 0.0757952, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:11.572859: step 7557, loss 0.188053, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:11.706179: step 7558, loss 0.0311946, acc 1, learning_rate 0.0001
2017-10-10T12:12:11.898225: step 7559, loss 0.0720664, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:12.127137: step 7560, loss 0.143782, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:12.580940: step 7560, loss 0.211086, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7560

2017-10-10T12:12:13.676830: step 7561, loss 0.048469, acc 1, learning_rate 0.0001
2017-10-10T12:12:13.890332: step 7562, loss 0.112575, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:14.079237: step 7563, loss 0.0851765, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:14.257024: step 7564, loss 0.164164, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:14.418698: step 7565, loss 0.0620698, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:14.580865: step 7566, loss 0.0797436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:14.760893: step 7567, loss 0.0596679, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:14.946515: step 7568, loss 0.051345, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:15.119932: step 7569, loss 0.080815, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:15.293097: step 7570, loss 0.137407, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:15.459353: step 7571, loss 0.106226, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:15.664168: step 7572, loss 0.0747035, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:15.821096: step 7573, loss 0.0826363, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:15.978707: step 7574, loss 0.0582626, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:16.178877: step 7575, loss 0.0299674, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:16.325230: step 7576, loss 0.0991962, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:16.492940: step 7577, loss 0.0652328, acc 1, learning_rate 0.0001
2017-10-10T12:12:16.680825: step 7578, loss 0.0493856, acc 1, learning_rate 0.0001
2017-10-10T12:12:16.908824: step 7579, loss 0.0478594, acc 1, learning_rate 0.0001
2017-10-10T12:12:17.109243: step 7580, loss 0.10697, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:17.247490: step 7581, loss 0.0745585, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:17.486358: step 7582, loss 0.0474688, acc 1, learning_rate 0.0001
2017-10-10T12:12:17.676956: step 7583, loss 0.0995526, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:17.871015: step 7584, loss 0.129248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:17.976825: step 7585, loss 0.122334, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:18.117525: step 7586, loss 0.20915, acc 0.90625, learning_rate 0.0001
2017-10-10T12:12:18.282865: step 7587, loss 0.13458, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:18.437057: step 7588, loss 0.141074, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:18.558564: step 7589, loss 0.125447, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:18.720219: step 7590, loss 0.140847, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:18.889643: step 7591, loss 0.100201, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:19.072661: step 7592, loss 0.0616372, acc 1, learning_rate 0.0001
2017-10-10T12:12:19.268850: step 7593, loss 0.0401539, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:19.415831: step 7594, loss 0.0910737, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:19.590824: step 7595, loss 0.162333, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:19.800842: step 7596, loss 0.0911157, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:19.945395: step 7597, loss 0.124417, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:20.133760: step 7598, loss 0.111149, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:20.360913: step 7599, loss 0.0253494, acc 1, learning_rate 0.0001
2017-10-10T12:12:20.592927: step 7600, loss 0.0579116, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:20.997639: step 7600, loss 0.209312, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7600

2017-10-10T12:12:22.020440: step 7601, loss 0.0900502, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:22.223737: step 7602, loss 0.120025, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:22.369189: step 7603, loss 0.0797611, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:22.535663: step 7604, loss 0.156878, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:22.728820: step 7605, loss 0.0459417, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:22.916024: step 7606, loss 0.0625969, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:23.120012: step 7607, loss 0.0598, acc 1, learning_rate 0.0001
2017-10-10T12:12:23.289672: step 7608, loss 0.0438217, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:23.492772: step 7609, loss 0.0961003, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:23.690006: step 7610, loss 0.115544, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:23.821051: step 7611, loss 0.0842663, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:24.011596: step 7612, loss 0.094432, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:24.212848: step 7613, loss 0.185259, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:24.376078: step 7614, loss 0.0643347, acc 1, learning_rate 0.0001
2017-10-10T12:12:24.581246: step 7615, loss 0.0744959, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:24.768864: step 7616, loss 0.163323, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:24.936908: step 7617, loss 0.0203557, acc 1, learning_rate 0.0001
2017-10-10T12:12:25.148570: step 7618, loss 0.115918, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:25.358752: step 7619, loss 0.0477924, acc 1, learning_rate 0.0001
2017-10-10T12:12:25.501162: step 7620, loss 0.0759459, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:25.660566: step 7621, loss 0.0816006, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:25.848713: step 7622, loss 0.145233, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:26.001182: step 7623, loss 0.173092, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:26.162841: step 7624, loss 0.0748149, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:26.335226: step 7625, loss 0.128383, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:26.537009: step 7626, loss 0.182289, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:26.689158: step 7627, loss 0.116965, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:26.865226: step 7628, loss 0.0809626, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:27.051135: step 7629, loss 0.0869129, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:27.208890: step 7630, loss 0.0872865, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:27.359796: step 7631, loss 0.0820207, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:27.569099: step 7632, loss 0.0689931, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:27.784143: step 7633, loss 0.120719, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:27.897168: step 7634, loss 0.114101, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:28.120870: step 7635, loss 0.132223, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:28.312853: step 7636, loss 0.13644, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:28.495210: step 7637, loss 0.127287, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:28.639782: step 7638, loss 0.0413306, acc 1, learning_rate 0.0001
2017-10-10T12:12:28.795555: step 7639, loss 0.0838367, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:28.945146: step 7640, loss 0.109105, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:29.344256: step 7640, loss 0.20952, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7640

2017-10-10T12:12:30.531284: step 7641, loss 0.0712144, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:30.742061: step 7642, loss 0.0725331, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:30.988841: step 7643, loss 0.0641871, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:31.152026: step 7644, loss 0.238343, acc 0.901961, learning_rate 0.0001
2017-10-10T12:12:31.296519: step 7645, loss 0.0921526, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:31.457090: step 7646, loss 0.108765, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:31.584330: step 7647, loss 0.104731, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:31.704842: step 7648, loss 0.03607, acc 1, learning_rate 0.0001
2017-10-10T12:12:31.868765: step 7649, loss 0.120795, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:32.058822: step 7650, loss 0.0560486, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:32.221637: step 7651, loss 0.142644, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:32.373083: step 7652, loss 0.0646838, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:32.589585: step 7653, loss 0.0726126, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:32.776837: step 7654, loss 0.0828934, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:32.913023: step 7655, loss 0.0588637, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:33.107278: step 7656, loss 0.0991635, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:33.312009: step 7657, loss 0.0605858, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:33.484873: step 7658, loss 0.127949, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:33.649983: step 7659, loss 0.0997301, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:33.805683: step 7660, loss 0.0270246, acc 1, learning_rate 0.0001
2017-10-10T12:12:33.989178: step 7661, loss 0.0965116, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:34.135733: step 7662, loss 0.0622509, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:34.324912: step 7663, loss 0.103074, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:34.524850: step 7664, loss 0.0749612, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:34.688010: step 7665, loss 0.164065, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:34.883945: step 7666, loss 0.0526992, acc 1, learning_rate 0.0001
2017-10-10T12:12:35.077715: step 7667, loss 0.028302, acc 1, learning_rate 0.0001
2017-10-10T12:12:35.304616: step 7668, loss 0.113123, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:35.500802: step 7669, loss 0.159582, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:35.688928: step 7670, loss 0.100822, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:35.856594: step 7671, loss 0.0696279, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:36.039899: step 7672, loss 0.149318, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:36.220860: step 7673, loss 0.152822, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:36.352125: step 7674, loss 0.0757117, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:36.558858: step 7675, loss 0.108789, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:36.774961: step 7676, loss 0.048209, acc 1, learning_rate 0.0001
2017-10-10T12:12:36.992877: step 7677, loss 0.0716498, acc 1, learning_rate 0.0001
2017-10-10T12:12:37.131932: step 7678, loss 0.0483066, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:37.322180: step 7679, loss 0.142843, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:37.518739: step 7680, loss 0.0578723, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:38.004894: step 7680, loss 0.208484, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7680

2017-10-10T12:12:38.965439: step 7681, loss 0.0618739, acc 1, learning_rate 0.0001
2017-10-10T12:12:39.161717: step 7682, loss 0.138773, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:39.404868: step 7683, loss 0.058576, acc 1, learning_rate 0.0001
2017-10-10T12:12:39.596909: step 7684, loss 0.122088, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:39.746696: step 7685, loss 0.040708, acc 1, learning_rate 0.0001
2017-10-10T12:12:39.889636: step 7686, loss 0.0303814, acc 1, learning_rate 0.0001
2017-10-10T12:12:40.008987: step 7687, loss 0.0616053, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:40.125826: step 7688, loss 0.114219, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:40.264403: step 7689, loss 0.1133, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:40.421476: step 7690, loss 0.0909362, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:40.548854: step 7691, loss 0.116235, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:40.741188: step 7692, loss 0.0593778, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:40.944925: step 7693, loss 0.199683, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:41.128871: step 7694, loss 0.158849, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:41.384894: step 7695, loss 0.110846, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:41.577038: step 7696, loss 0.119224, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:41.765523: step 7697, loss 0.0250567, acc 1, learning_rate 0.0001
2017-10-10T12:12:41.924814: step 7698, loss 0.0895302, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:42.056969: step 7699, loss 0.129952, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:42.201089: step 7700, loss 0.157788, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:42.340397: step 7701, loss 0.103616, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:42.497922: step 7702, loss 0.124633, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:42.675829: step 7703, loss 0.0983119, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:42.824969: step 7704, loss 0.123815, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:43.032855: step 7705, loss 0.192462, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:43.231790: step 7706, loss 0.0786904, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:43.412901: step 7707, loss 0.141336, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:43.552443: step 7708, loss 0.107312, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:43.755333: step 7709, loss 0.138709, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:43.976971: step 7710, loss 0.0860098, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:44.171257: step 7711, loss 0.0292581, acc 1, learning_rate 0.0001
2017-10-10T12:12:44.330200: step 7712, loss 0.13507, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:44.481039: step 7713, loss 0.0376471, acc 1, learning_rate 0.0001
2017-10-10T12:12:44.687173: step 7714, loss 0.100857, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:44.896424: step 7715, loss 0.116598, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:45.074604: step 7716, loss 0.0663981, acc 1, learning_rate 0.0001
2017-10-10T12:12:45.198207: step 7717, loss 0.0530916, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:45.400408: step 7718, loss 0.0389346, acc 1, learning_rate 0.0001
2017-10-10T12:12:45.564858: step 7719, loss 0.0628548, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:45.692835: step 7720, loss 0.121147, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:46.168240: step 7720, loss 0.208093, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7720

2017-10-10T12:12:47.312146: step 7721, loss 0.0643962, acc 1, learning_rate 0.0001
2017-10-10T12:12:47.496982: step 7722, loss 0.097535, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:47.731260: step 7723, loss 0.0339495, acc 1, learning_rate 0.0001
2017-10-10T12:12:47.907339: step 7724, loss 0.122146, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:48.073210: step 7725, loss 0.056908, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:48.266150: step 7726, loss 0.125488, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:48.444680: step 7727, loss 0.0858811, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:48.577814: step 7728, loss 0.0868887, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:48.761373: step 7729, loss 0.031884, acc 1, learning_rate 0.0001
2017-10-10T12:12:48.968163: step 7730, loss 0.0570794, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:49.138831: step 7731, loss 0.116286, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:49.308386: step 7732, loss 0.11269, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:49.461199: step 7733, loss 0.0784413, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:49.611204: step 7734, loss 0.145535, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:49.825743: step 7735, loss 0.0534693, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:50.032855: step 7736, loss 0.156972, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:50.165322: step 7737, loss 0.0581364, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:50.398269: step 7738, loss 0.113097, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:50.633905: step 7739, loss 0.0592572, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:50.829517: step 7740, loss 0.0953289, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:50.988041: step 7741, loss 0.0421288, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:51.110412: step 7742, loss 0.106156, acc 0.980392, learning_rate 0.0001
2017-10-10T12:12:51.233148: step 7743, loss 0.0855643, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:51.366856: step 7744, loss 0.048352, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:51.507264: step 7745, loss 0.127151, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:51.672842: step 7746, loss 0.0776555, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:51.860136: step 7747, loss 0.118296, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:52.064697: step 7748, loss 0.102885, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:52.186105: step 7749, loss 0.101785, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:52.320974: step 7750, loss 0.0922755, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:52.479628: step 7751, loss 0.0544118, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:52.612633: step 7752, loss 0.0527537, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:52.720867: step 7753, loss 0.128191, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:52.865533: step 7754, loss 0.0713561, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:53.021610: step 7755, loss 0.100017, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:53.200860: step 7756, loss 0.0449624, acc 1, learning_rate 0.0001
2017-10-10T12:12:53.399455: step 7757, loss 0.195306, acc 0.90625, learning_rate 0.0001
2017-10-10T12:12:53.566888: step 7758, loss 0.173704, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:53.736880: step 7759, loss 0.0529603, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:53.895859: step 7760, loss 0.120015, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:54.364852: step 7760, loss 0.208915, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7760

2017-10-10T12:12:55.620045: step 7761, loss 0.109169, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:55.785041: step 7762, loss 0.0367304, acc 1, learning_rate 0.0001
2017-10-10T12:12:55.981546: step 7763, loss 0.0495185, acc 1, learning_rate 0.0001
2017-10-10T12:12:56.177417: step 7764, loss 0.0571287, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:56.329248: step 7765, loss 0.0985632, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:56.505179: step 7766, loss 0.17514, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:56.701046: step 7767, loss 0.14224, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:56.922455: step 7768, loss 0.0605392, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:57.108860: step 7769, loss 0.113163, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:57.267663: step 7770, loss 0.138716, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:57.432890: step 7771, loss 0.0524708, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:57.623692: step 7772, loss 0.0507217, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:57.800276: step 7773, loss 0.0916033, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:57.968529: step 7774, loss 0.119321, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:58.121108: step 7775, loss 0.140309, acc 0.921875, learning_rate 0.0001
2017-10-10T12:12:58.316319: step 7776, loss 0.098626, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:58.509074: step 7777, loss 0.116097, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:58.737426: step 7778, loss 0.126038, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:58.876887: step 7779, loss 0.0820779, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:59.056847: step 7780, loss 0.129613, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:59.256784: step 7781, loss 0.07239, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:59.467325: step 7782, loss 0.119694, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:59.656862: step 7783, loss 0.0381464, acc 1, learning_rate 0.0001
2017-10-10T12:12:59.808842: step 7784, loss 0.0698243, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:59.987957: step 7785, loss 0.0463712, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:00.172822: step 7786, loss 0.0850837, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:00.353273: step 7787, loss 0.100064, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:00.532893: step 7788, loss 0.0831766, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:00.683702: step 7789, loss 0.0535589, acc 1, learning_rate 0.0001
2017-10-10T12:13:00.878952: step 7790, loss 0.0878946, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:01.030521: step 7791, loss 0.0757668, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:01.224720: step 7792, loss 0.0629585, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:01.436904: step 7793, loss 0.0629641, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:01.625126: step 7794, loss 0.128395, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:01.841288: step 7795, loss 0.0458848, acc 1, learning_rate 0.0001
2017-10-10T12:13:02.017084: step 7796, loss 0.128905, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:02.164859: step 7797, loss 0.0351163, acc 1, learning_rate 0.0001
2017-10-10T12:13:02.352852: step 7798, loss 0.0618081, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:02.512887: step 7799, loss 0.122779, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:02.611104: step 7800, loss 0.0856576, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:02.964850: step 7800, loss 0.208128, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7800

2017-10-10T12:13:04.023458: step 7801, loss 0.0679928, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:04.168021: step 7802, loss 0.0574052, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:04.339729: step 7803, loss 0.122156, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:04.537204: step 7804, loss 0.0491856, acc 1, learning_rate 0.0001
2017-10-10T12:13:04.703501: step 7805, loss 0.0609676, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:04.853492: step 7806, loss 0.0853448, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:05.047253: step 7807, loss 0.0628481, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:05.195099: step 7808, loss 0.129669, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:05.345721: step 7809, loss 0.07957, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:05.557574: step 7810, loss 0.0316279, acc 1, learning_rate 0.0001
2017-10-10T12:13:05.786812: step 7811, loss 0.224161, acc 0.921875, learning_rate 0.0001
2017-10-10T12:13:05.930110: step 7812, loss 0.0890004, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:06.090142: step 7813, loss 0.0895506, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:06.289174: step 7814, loss 0.0783151, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:06.432994: step 7815, loss 0.0593499, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:06.595114: step 7816, loss 0.0711204, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:06.800182: step 7817, loss 0.105032, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:06.984568: step 7818, loss 0.0531459, acc 1, learning_rate 0.0001
2017-10-10T12:13:07.157918: step 7819, loss 0.195615, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:07.360604: step 7820, loss 0.11039, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:07.579082: step 7821, loss 0.10051, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:07.720949: step 7822, loss 0.0680791, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:07.888301: step 7823, loss 0.10059, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:08.098695: step 7824, loss 0.124209, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:08.300873: step 7825, loss 0.0805796, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:08.452836: step 7826, loss 0.129879, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:08.653864: step 7827, loss 0.122575, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:08.844511: step 7828, loss 0.076014, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:08.984877: step 7829, loss 0.0325615, acc 1, learning_rate 0.0001
2017-10-10T12:13:09.204944: step 7830, loss 0.0534475, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:09.380020: step 7831, loss 0.101723, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:09.574074: step 7832, loss 0.0417641, acc 1, learning_rate 0.0001
2017-10-10T12:13:09.771892: step 7833, loss 0.072166, acc 1, learning_rate 0.0001
2017-10-10T12:13:09.926881: step 7834, loss 0.0771902, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.097859: step 7835, loss 0.102627, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.279739: step 7836, loss 0.116004, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:10.458938: step 7837, loss 0.0619769, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:10.622245: step 7838, loss 0.096771, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:10.813053: step 7839, loss 0.0713729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.933035: step 7840, loss 0.145488, acc 0.921569, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:11.406943: step 7840, loss 0.207815, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653916/checkpoints/model-7840

